attend what you need:motion-appearance synergistic networks for video question answering.
ahjeong seo1, gi-cheon kang1,2, joonhan park3,∗, byoung-tak zhang1,21seoul national university2ai institute for seoul national university (aiis)3 hanyang university{ajseo,gckang,jhpark,btzhang}@bi.snu.ac.kr.
abstract.
video question answering is a task whichrequires an ai agentto answer questionsgrounded in video.
this task entails threekey challenges: (1) understand the intentionof various questions, (2) capturing various el-ements of the input video (e.g., object, ac-tion, causality), and (3) cross-modal ground-ing between language and vision information.
we propose motion-appearance synergisticnetworks (masn), which embed two cross-modal features grounded on motion and ap-pearance information and selectively utilizethem depending on the question’s intentions.
masn consists of a motion module, an ap-pearance module, and a motion-appearance fu-sion module.
the motion module computesthe action-oriented cross-modal joint represen-tations, while the appearance module focuseson the appearance aspect of the input video.
finally, the motion-appearance fusion moduletakes each output of the motion module andthe appearance module as input, and performsquestion-guided fusion.
as a result, masnachieves new state-of-the-art performance onthe tgif-qa and msvd-qa datasets.
wealso conduct qualitative analysis by visual-izing the inference results of masn.
thecode is available at https://github.com/ahjeongseo/masn-pytorch..1.introduction.
recently, research in natural language processingand computer vision has made signiﬁcant progressin artiﬁcial intelligence (ai).
thanks to this, vision-language tasks such as image captioning (xu et al.,2015), visual question answering (vqa) (antolet al., 2015; goyal et al., 2017), and visual com-monsense reasoning (vcr) (zellers et al., 2019)have been introduced to the research community,.
∗ work done during an internship at ai institute for seoul.
national university (aiis)..along with some benchmark datasets.
in particular,video question answering (video qa) tasks (xuet al., 2016; jang et al., 2017; lei et al., 2018; yuet al., 2019; choi et al., 2020) have been proposedwith the goal of reasoning over higher-level vision-language interactions.
in contrast to qa tasks basedon static images, the questions presented in thevideo qa dataset vary from frame-level questionsregarding the appearance of objects (e.g., what isthe color of the hat?)
to questions regarding ac-tion and causality (e.g., what does the man do afteropening a door?)..
there are three crucial challenges in video qa:(1) understand the intention of various questions,(2) capturing various elements of the input video(e.g., object, action, and causality), and (3) cross-modal grounding between language and vision in-formation.
to tackle these challenges, previousstudies (li et al., 2019; jiang et al., 2020; huanget al., 2020) have mainly explored this task byjointly embedding the features from the pre-trainedword embedding model (pennington et al., 2014)and the object detection models (he et al., 2016;ren et al., 2016).
however, as discussed in (gaoet al., 2018), the use of the visual features extractedfrom the object detection models suffers from mo-tion analysis since the object detection model lackstemporal modeling.
to enforce the motion analy-sis, a few approaches (xu et al., 2017; gao et al.,2018) have employed additional visual features(tran et al., 2015) (i.e., motion features) whichwere widely used in the action recognition domain,but their reasoning capability is still limited.
theytypically employed recurrent models (e.g., lstm)to embed a long sequence of the visual features.
due to the problem of long-term dependency in re-current models (bengio et al., 1993), their proposedmethods may fail to learn dependencies betweendistant features..in this paper, we propose motion-appearance.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6167–6177august1–6,2021.©2021associationforcomputationallinguistics6167figure 1: an overview of masn.
each extracted feature from resnet and i3d is fed into the appearance andmotion modules.
both modules have the same structure with a gcn and vq interaction submodule.
the resultsfrom each module are then concatenated and fused in the motion-appearance fusion module.
the output fromthe fusion module is used to derive answers.
for question features, the word-level representation fq is integratedwith the visual features in the vq interaction submodule.
the last hidden units q from the bi-lstm are used tocombine appearance and motion features..synergistic networks (masn) for video questionanswering which consist of three kinds of modules:the motion module, the appearance module, andthe motion-appearance fusion module.
as shownin figure 1, the motion module and the appear-ance module aim to embed rich cross-modal rep-resentations.
these two modules have the samearchitecture except that the motion module takesthe motion features extracted from i3d as visualfeatures and the appearance module utilizes theappearance features extracted from resnet.
eachof these modules ﬁrst constructs the object graphsvia graph convolutional networks (gcn) to com-pute the relationships among objects in each visualfeature.
then, the vision-question interaction mod-ule performs cross-modal grounding between theoutput of the gcns and the question features.
themotion module and the appearance module eachyield cross-modal representations of the motionand the appearance aspects of the input video re-spectively.
the motion-appearance fusion moduleﬁnally integrates these two features based on thequestion features..the main contributions of our paper are as fol-lows.
first, we propose motion-appearance syn-ergistic networks (masn) for video questionanswering based on three modules, the motionmodule, the appearance module, and the motion-appearance fusion module.
second, we validatemasn on the large-scale video question answeringdatasets tgif-qa, msvd-qa, and msrvtt-qa..masn achieves the new state-of-the-art perfor-mance on tgif-qa and msvd-qa.
we performablation studies to validate the effectiveness of ourproposed methods.
finally, we conduct a qualita-tive analysis of masn by visualizing inferenceresults..2 related work.
visual question answering (vqa) is a task thatrequires both understanding questions and ﬁndingclues from visual information.
vqa can be clas-siﬁed into two categories based on the type of thevisual source: image qa and video qa.
in imageqa, earlier works approach the task by applyingattention between the question and the spatial di-mensions of the image (yang et al., 2016; ander-son et al., 2018; kim et al., 2018a; kang et al.,2019).
in video qa, since a video is representedas a sequence of images over time, recognizing themovement of objects or causality in the temporaldimension should also be considered along withthe details from the spatial dimension (jang et al.,2017; on et al., 2020).
there have been some at-tempts (xu et al., 2017; gao et al., 2018; fan et al.,2019) to extract motion and appearance featuresand integrate them on a spatio-temporal dimensionvia memory networks.
li et al.
(2019), huang et al.
(2020), jiang et al.
(2020) proposed better perform-ing models using attention in order to overcomethe long-range dependency problem in memory net-works.
however, they do not represent motion in-.
6168formation sufﬁciently since they only use featurespre-trained on image or object classiﬁcation.
tobetter address this, we model spatio-temporal rea-soning on multiple visual information (i.e., resnet,i3d) while also solving the long-range dependencyproblem that occurred in previous studies.
action classiﬁcation is a task of recognizing ac-tions, which are composed of interactions betweenactors and objects.
therefore, this task has muchin common with video qa, in that the modelshould perform spatio-temporal reasoning.
for bet-ter spatio-temporal reasoning, tran et al.
(2015)introduced c3d, which extends the 2d cnn ﬁltersto the temporal dimension.
carreira and zisserman(2017) proposed i3d, which integrates 3d convo-lutions into a state-of-the-art 2d cnn architecture,which now acts as a baseline in action classiﬁcationtasks (murray et al., 2012; girdhar et al., 2018).
feichtenhofer et al.
(2019) introduced slowfast,a network which encodes images in two streamswith different frame rates and temporal resolutionsof convolution.
this study based on a two-streamarchitecture inspired us in terms of assigning dif-ferent inputs to each encoder module.
however,our method differs from the former studies in twoaspects: (1) we utilize language features as well asvision features, and (2) we expand the two-streamstructure to solve more than motion-oriented tasks.
attention mechanism explicitly calculates thecorrelation between two features (bahdanau et al.,2015; lin et al., 2017), and has been widely usedin a variety of ﬁelds.
for machine translation,the transformer architecture ﬁrst introduced byvaswani et al.
(2017), utilizes multi-head self-attention that captures diverse aspects in the inputfeatures (voita et al., 2019).
for video qa, kimet al.
(2018b); li et al.
(2019) use self and guided-attention to encode temporal dynamics in video andground them in the question.
for multi-modal align-ment, tsai et al.
(2019) apply the transformer tomerge cross-modal time series between vision, lan-guage, and audio features.
we utilize the attentionmechanism to capture various relations betweenappearance and motion and to aggregate them..3 model.
bine them with the question in section 3.2. finally,the motion-appearance fusion module modulatesthe amount of motion and appearance informationutilized and integrates them based on question con-text..3.1 visual and linguistic representation.
we ﬁrst extract appearance and motion featuresfrom the video frames.
for the appearance represen-tation, we use resnet (he et al., 2016) pre-trainedon an object and its attribute classiﬁcation task asa feature extractor.
for the motion representation,we use i3d (carreira and zisserman, 2017) pre-trained on the action classiﬁcation task.
we obtainlocal features representing object-level informationwithout background noise and global features rep-resenting each frame’s context for both appearanceand motion features..t,n, bt,n}t=t,n=n.
appearance representation.
for local features,given a video containing t frames, we obtainn objects from each frame using faster r-cnn(ren et al., 2016) that applies roialign to ex-tract the region of interest from resnet’s convolu-tional layer.
we denote the appearance-object setas ra = {oat=1,n=1 , where o, b indicateobject feature and bounding box location, respec-tively.
therefore, there are k = n × t objects in asingle video.
following previous works, we extractthe feature map from resnet-152’s conv5 layerand apply a linear projection (jiang et al., 2020;huang et al., 2020).
we denote global features asglobal ∈ rt ×d, where d is the size of the hiddenvadimension..motion representation.
we obtain a featuremap from the last convolutional layer in i3d (car-reira and zisserman, 2017) whose dimension is(cid:5), 7, 7, 2048).
(time, width, height, feature) = ((cid:4) t8that is, each set of 8 frames is represented as asingle feature map with dimension 7 × 7 × 2048.for local features, we apply roialign (he et al.,2017) on the feature map using object boundingbox location b. we deﬁne the motion-object setas rm = {omt=1,n=1 .
we apply averagepooling in the feature map and linear projection toobtain global features vm.
t,n, bt,n}t=t,n=n.
global ∈ rt ×d..in this section, we introduce a detailed descriptionof our masn network.
first, we explain how toobtain appearance and motion features in section3.1. then, we describe the appearance and motionmodules, which encode visual features and com-.
location encoding.
to reason about relationsbetween objects as in section 3.2, it is requiredto consider each object’s spatial and temporal lo-cation.
as appearance and motion features shareidentical operations until the motion-appearance.
6169fusion module, we combine superscript a and mfor simplicity.
following l-gcn (huang et al.,2020), we add a location encoding and deﬁne localfeatures as:.
(1).
va/mlocal = ffn([oa/m; ds; dt])where ds = ffn(b) and dt is obtained by po-sition encoding according to each frame’s index.
here oa/m denotes the object features mentionedabove while ffn denotes a feed-forward network.
analogous to local features, position encoding in-formation dt is added to global features as well.
we then concatenate object features with globalfeatures to reﬂect the frame-level context in objectsand obtain the visual representation va/m ∈ rk×d:.
va/m = ffn([va/m.
local; va/m.
global]).
(2).
linguistic representation.
we apply the pre-trained glove to convert each question word into a300-dimensional vector, following previous work(jang et al., 2017).
to represent contextual informa-tion in a sentence, we feed the word representationsinto a bidirectional lstm (bi-lstm).
word-levelfeatures and last hidden units from the bi-lstm aredenoted by f q ∈ rl×d, and q ∈ rd respectively.
l denotes the number of words in a question..3.2 motion and appearance module.
in this section, we explain the modules generatinghigh-level visual representations and integrate themwith linguistic representations.
each module con-sists of (1) an object graph: spatio-temporal rea-soning between object-level visual features, and (2)vq interaction: calculating correlations betweenobjects and words and obtaining cross-modal fea-ture embeddings.
since the modules share the samearchitecture, we describe each module’s compo-nents only once with a shared superscript to avoidredundancy..3.2.1 object graph constructionin this section, we deﬁne object graphs ga/m =(v a/m, e a/m) to capture spatio-temporal relationsbetween objects.
v, e denotes the node and edgeset of the graph.
as equation 2 provides visualfeatures va/m, we deﬁne these as the graph inputx a/m ∈ rk×d.
we denote the graph as ga/m.
thenodes of graph ga/m are given by va/m∈ x a/m,iand edges are given by (va/m), representingia relationship between the two nodes.
given theconstructed graph g, we perform graph convolution.
, va/mj.
(kipf and welling, 2016) to obtain the relation-aware object features.
we obtain the similarityscores of nodes by calculating the dot-product afterprojecting input features to the interaction spaceand deﬁne the adjacency matrix aa/m ∈ rk×k asfollows:.
aa/m = softmax((x a/mw1)(x a/mw2)(cid:62)) (3).
we denote the two-layer graph convolution on inputx with adjacency matrix a as:.
gcn(x; a) = relu(a relu(axw3) w4)f = layernorm(x + gcn(x; a)).
(4)we omit superscripts in the graph convolution equa-tion for simplicity.
we add a skip connection forresidual learning between self-information x andsmoothed-information with neighbor objects..3.2.2 vision-question (vq) interactionwe compute both appearance-question and motion-question interaction to obtain correlations betweenlanguage and each of the visual features.
as we en-code visual feature f a/m and question feature f qin equation 4 and section 3.1, we calculate everypair of relations between two modalities using thebilinear operation introduced in ban (kim et al.,2018a) as follows:.
hi = 1 · bani(hi−1, v ; ai)(cid:62) + hi−1.
(5).
where h0 = f q, 1 ∈ rl, 1 ≤ i ≤ g and a de-notes the attention map.
f a/m is substituted for vrespectively in our method.
in the equation above,calculating the result ban(h, v ; a) ∈ rd andadding it to the h is repeated in g times.
after-wards, h represents the combined visual and lan-guage features in the question space incorporatingdiverse aspects from the two modalities (yang et al.,2016)..3.3 motion-appearance fusion.
in thissection, we introduce the motion-appearance fusion module which is our key con-tribution.
depending on what the question ulti-mately asks about, the model is supposed to decidewhich features are more relevant among appear-ance and motion information, or a combination ofboth.
to do this, we produce appearance-centered,motion-centered, and all-mixed features and aggre-gate them depending on question context.
based onthe previous step, we obtain cross-modal combined.
6170where p ∈ r2l×d and z ∈ r2l×d..as in the ﬁrst line of the equation 8, we add pro-jected appearance features p a on each appearanceand motion feature to obtain za, since the matrixu is the concatenation of h a and h m. therefore,we argue that za contains appearance-centeredinformation.
similarly, zm/all contains motion-centered and all-mixed features, respectively.
weargue that the motion-appearance-centered atten-tion fuses appearance and motion features in vari-ous proportions and these three matrices work likemulti-head attention sharing the task of capturingdiverse information, and become synergistic whencombined..question-guided fusion.
for question-guidedfusion, we ﬁrst deﬁne za/m/all as the sum of matrixza/m/all ∈ r2l×d over sequence length 2l.
weobtain attention scores between each za/m/all andquestion context vector q:.
αa/m/all = softmax(.
q(za/m/all)(cid:62)√dz.
).
(9).
where q denotes the last hidden vector.
the atten-tion score αa/m/all can be interpreted as the impor-tance of each matrix z based on question context.
we obtain the question-guided fusion matrix o as:.
s = αaza + αmzm + αallzallo = layernorm(s + ffn(s)).
(10).
where o ∈ r2l×d is obtained by linear transfor-mation and a residual connection after weightedsum.
we aggregate information by attention overthe sequence length of o:.
βi = softmax(ffn(oi))2l(cid:88).
f =.
βioi.
i=1.
(11).
the ﬁnal output vector f ∈ rd is used for answerprediction..3.4 answer prediction and loss function.
the video qa task can be divided into counting,open-ended word, and multiple-choice tasks (janget al., 2017).
our method trains the model and pre-dicts the answer based on the three tasks similar toprevious work..the counting task is formulated as a linear re-gression of the ﬁnal output vector f .
we obtain the.
figure 2: motion-appearance fusion module.
theblue-colored elements in a matrix denote appearance-question, and the pink ones indicate motion-questioncombined features.
matrices above qk (cid:62) represent anattention score maps from each kind of attention.
theﬁnal output s in the ﬁgure is the weighted-sum matrixof all three attended features..features h a and h m in terms of appearance andmotion.
we concatenate these two matrices anddeﬁne u as:.
u =.
(cid:21).
(cid:20) h ah m., u ∈ r2l×d.
(6).
motion-appearance-centered attention.
weﬁrst deﬁne regular scaled dot-product attention toattend features to diverse aspects:.
attention(q, k, v ) = softmax(.
)v (7).
qk(cid:62)√dk.
where q, k, v denotes the query, key, andvalue, respectively.
to obtain motion-centered,appearance-centered and mixed attention, we sub-stitute u with the query, and h a, h m, u with thekey and value in the equation 7 as:.
p a = attention(u, h a, h a)p m = attention(u, h m, h m)p all = attention(u, u, u )za/m/all = layernorm(p a/m/all + u ).
(8).
6171ﬁnal answer by rounding the result and we mini-mize mean squared error (mse) loss..the open-ended word task is essentially a clas-siﬁcation task over the whole answer set.
we cal-culate a classiﬁcation score by applying a linearclassiﬁer and softmax function on the ﬁnal outputf and train the model by minimizing cross-entropyloss..for the multiple-choice task, like in previouswork (jang et al., 2017), we attach an answer tothe question and obtain m candidates.
then, weobtain the score for each of the m candidates bya linear transformation to the output vector f .
weminimize the hinge loss within every pair of can-didates, max(0, 1 + sn − sp), where sn and sp arescores from incorrect and correct answers respec-tively..4 experiments.
in this section, we evaluate our proposed model onthree video qa datasets: tgif-qa, msvd-qa,and msrvtt-qa.
we ﬁrst introduce each datasetand compare our results with the state-of-the-artmethods.
then, we report ablation studies and in-clude visualizations to show how each module inmasn works..4.1 datasets.
tgif-qa (jang et al., 2017) is a large-scaledataset that consists of 165k qa pairs collectedfrom 72k animated gifs.
the length of videoclips is very short, in general.
tgif-qa consistsof four types of tasks: count, action, statetransition (trans.
), and frameqa.
count is anopen-ended question to count how many timesan action repeats.
action is a task to ﬁnd actionrepeated at certain times, and transition aims toidentify a state transition over time.
both typesare multiple-choice tasks.
lastly, frameqa is anopen-ended question that can be solved from justone frame, similar to image qa..msvd-qa & msrvtt-qa (xu et al., 2017) areautomatically generated from video descriptions.
itconsists of 1,970 video clips and 50k and 243k qapairs, respectively.
the average video lengths are10 seconds and 15 seconds respectively.
questionsbelong to ﬁve types: what, who, how, when, andwhere.
the task is open-ended with a pre-deﬁnedanswer sets of size 1,000 and 4,000, respectively..methods count action trans.
frameqa60.84.28st-vqa68.2co-mem 4.1070.44.27psac72.34.25sta73.94.02hme75.44.09hga74.33.95l-gcn75.94.19quest75.03.82hcrn84.43.75masn.
49.351.555.756.653.855.156.359.755.959.5.
67.174.376.979.077.881.081.181.081.487.4.table 1: state-of-the-art comparison on the tgif-qadataset.
mean (cid:96)2 loss for count, and accuracy (%) forothers.
best results in bold, underlined results denotethe second best..methods msvd-qa msrvtt-qast-vqagraco-memhmehgaquesthcrnmasn.
30.932.532.033.035.534.635.635.2.
31.332.031.733.734.736.136.138.0.table 2: state-of-the-art comparison on the msvd-qaand msrvtt-qa datasets.
all values represent accu-racy (%).
best results in bold, underlined results denotethe second best..4.2.implementation details.
we ﬁrst extract frames with 6 fps for all datasets.
in the case of appearance features, we sample 1frame out of 4 to avoid information redundancy.
we apply faster r-cnn (ren et al., 2016) pre-trained on visual genome (krishna et al., 2017)to obtain local features.
the number of extractedobjects is n = 10. for global features, we useresnet-152 pre-trained on imagenet (deng et al.,2009).
in the the case of motion features, we applyi3d pre-trained on the kinetics action recognitiondataset (kay et al., 2017).
for the input of i3d, weconcatenate a set of 8 frames around the sampledframe mentioned above.
in terms of training details,we employ adam optimizer with learning rate as10−4.
the number of ban glimpse g is 4. we setthe batch size as 32 for the count and frameqatasks and 16 for action and trans.
tasks..6172methodsappr.
modulemotion moduleappr.
module + motion moduleappr.
module + motion module + fusion (ours).
single-att.
fusion.
dual-att.
fusion.
appr.
motionallappr.
+ motionappr.
+ allmotion + all.
count3.943.843.823.75.
3.783.793.783.773.773.80.action82.982.583.484.4.
82.883.183.683.683.684.1.trans.
86.286.286.887.486.387.087.487.487.586.5.frameqa58.651.058.659.5.
58.959.159.359.259.059.1.table 3: ablation study on the tgif-qa dataset.
mean (cid:96)2 loss for count, and accuracy (%) for others.
appr.
andatt.
stand for appearance and attention.
best results in bold, underlined results denote the second best..4.3 comparison with state-of-the-arts.
we compare masn with state-of-the-art (sota)models on the aforementioned datasets..tgif-qa.
compared with st-vqa (jang et al.,2017), co-mem (gao et al., 2018), psac (li et al.,2019), sta (gao et al., 2019), hme (fan et al.,2019), and recent sota models: hga, l-gcn,quest, hcrn (jiang and han, 2020; huang et al.,2020; jiang et al., 2020; le et al., 2020), masnshows the best results for three tasks: count, trans.,and action, outperforming the baseline methodsby a large margin as shown in table 1. in thecase of frameqa, the performance is similar toquest.
however, considering that there existssome tradeoff between the performance of countand frameqa since count focuses on identifyingtemporal information and frameqa focuses onspatial information, masn shows the best overallperformance on the entire task..msvd-qa & msrvtt-qa.
as shown in ta-ble 2, masn outperforms the best baseline meth-ods, quest and hcrn by approximately 2% onmsvd-qa, and shows competitive results onmsrvtt-qa.
since these datasets are composedof wh-questions, such as what or who, the questionsets seemingly resemble frameqa in tgif-qa,as they tend to focus on spatial appearance features.
this means that masn is able to capture spatialdetails well based on the spatiotemporally mixedfeatures..4.4 ablation study.
analyzing the impact of motion module and ap-pearance module.
we investigate the effect ofeach module as seen in figure 1. in table 3, the.
1st and 2nd row represent the result of using onlythe appearance and motion module, respectively.
the 3rd row shows the result of just concatenatingappearance and motion features from each moduleand ﬂattening them, by substituting the input xfor o in equation 11. most existing sota modelsutilize only resnet features for spatio-temporalreasoning based on the difference of vectors overtime.
using only the appearance module is simi-lar to most of these existing methods, which cancatch spatio-temporal relations relatively well.
onthe other hand, we found that the accuracy onframeqa when only using the motion module isabout 7% lower than when using the appearancemodule.
this means the motion module is lim-ited in its ability to capture the appearance details.
however, comparing the 1st and 3rd row in table3, the performance in the action and trans.
tasksincrease consistently when the motion module isadded compared to using only the appearance mod-ule.
this indicates that the motion module is ameaningful addition.
lastly, compared to the 1st,2nd and 3rd row, when integrating the output fromboth modules there is a further overall performanceimprovement.
this indicates a synergistic effectoccurs when integrating both the appearance andmotion feature after obtaining them as high-levelfeatures..analyzing the impact of fusion module.
weshow ablation studies inside the fusion modulerepresented in table 3. the 4th row indicates theperformance of our proposed masn architecture.
the results in the ‘single-attention fusion’ rowuse only one type of attention among appearance-centered, motion-centered, and all-mixed as seenin equation 8. the results in the ‘dual-attention.
6173figure 3: qualitative results on tgif-qa dataset.
from the left, count and frameqa are shown in 1st row andaction, trans.
in 2nd row.
each visualized attention map is log-scaled.
scores below attention maps represent αfrom the equation 9..fusion’ row utilize two among the three types ofattention mentioned above.
due to the nature ofvideo, when a question such as “how many timesdoes the man in the white shirt put his hand onthe head?” is given, the model is supposed to ﬁndthe motion information “put” while catching theappearance information “man in white shirt” or“hand on head”, and ﬁnally mixing them in differ-ent proportions depending on the context of ques-tion.
comparing the result of the 3rd (without fu-sion) row and masn ﬁrst, masn shows betterperformance across tasks.
this means mixing ap-pearance and motion features in various propor-tions using the motion-appearance-centered fu-sion module and computing the weighted fusion viathe question-guided fusion module contributes to.
the performance.
when comparing the general per-formance with the number of attention types in fu-sion module, using single, dual, and triple attention(masn) shows increasingly better performance inthe same order.
this indicates that focusing on dif-ferent aspects and integrating each attended featureperforms better than calculating attention at once.
additionally, comparing the result of using onlyappearance or motion-centered attention in ‘single’with both of them in ‘dual’, we ﬁnd that using bothfeatures shows better performance, which meansthey play complementary roles for each other.
sim-ilarly, we argue the reason for the performance in-crease in frameqa in the ‘motion’ row of ‘single-att.
fusion’ is due to the fact that the model canﬁnd relevant appearance information better based.
6174on motion information..references.
4.5 qualitative results.
we give examples of each attention score matrixfrom motion-appearance fusion module in figure3. we draw two conclusions from the figure: (1)each attention map catches different relations simi-larly to multi-head attention, (2) each attention mapis used to a different extend depending on the typeof task.
for example, in frameqa, the appearance-centered’s attention map captures which appear-ance trait to ﬁnd focusing on ‘how many’.
on theother hand, the motion-centered’s and all-mixed’sattention map attend on ‘waving’ or ‘hands’ tocatch motion-related information.
in action, simi-lar to frameqa, the appearance-centered’s atten-tion map attends on ‘head’ which is the object ofaction, while the motion-centered’s attention mapcatch ‘nod’ which is related to movement.
how-ever, in the case of the count task, the two atten-tion weights are not as sparse as scores in the othertasks.
we think this dense attention map causes theinconsistency in the performance increase betweencount task and action and trans.
task, althoughquestions for all of these three tasks ask for motioninformation..5 conclusion.
in this paper, we proposed a motion-appearancesynergistic networks to fuse and create a synergybetween motion and appearance features.
throughthe motion and appearance modules, masn man-ages to ﬁnd motion and appearance clues to solvethe question, while modulating the amount of in-formation used of each type through the fusionmodule.
experimental results on three benchmarkdatasets show the effectiveness of our proposedmasn architecture compared to other models..acknowledgement the authors would liketo thank ho-joon song, yu-jung heo, bjornbebensee, seonil son, kyoung-woon on, seonghochoi and woo-suk choifor helpful com-ments and editing.
this work was partly sup-ported by the institute of information & com-munications technology planning & evalua-tion (2015-0-00310-sw.starlab/25%, 2017-0-01772-vtt/25%, 2018-0-00622-rmi/25%, 2019-0-01371-babymind/25%) grant funded by the ko-rean government..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 6077–6086..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in proceedings of the ieee internationalconference on computer vision, pages 2425–2433..dzmitry bahdanau, kyung hyun cho, and yoshuabengio.
2015.neural machine translation byjointly learning to align and translate.
in 3rd inter-national conference on learning representations,iclr 2015..yoshua bengio, paolo frasconi, and patrice simard.
1993. the problem of learning long-term dependen-in ieee internationalcies in recurrent networks.
conference on neural networks, pages 1183–1188.
ieee..joao carreira and andrew zisserman.
2017. quo vadis,action recognition?
a new model and the kineticsin proceedings of the ieee conferencedataset.
on computer vision and pattern recognition, pages6299–6308..seongho choi, kyoung-woon on, yu-jung heo, ah-jeong seo, youwon jang, seungchan lee, minsulee, and byoung-tak zhang.
2020. dramaqa:character-centered video story understanding withhierarchical qa.
arxiv preprint arxiv:2005.03356..jia deng, wei dong, richard socher, li-jia li, kai li,and li fei-fei.
2009. imagenet: a large-scale hier-archical image database.
in 2009 ieee conferenceon computer vision and pattern recognition, pages248–255.
ieee..chenyou fan, xiaofan zhang, shu zhang, wenshengwang, chi zhang, and heng huang.
2019. het-erogeneous memory enhanced multimodal attentionmodel for video question answering.
in proceedingsof the ieee conference on computer vision and pat-tern recognition, pages 1999–2007..christoph feichtenhofer, haoqi fan, jitendra malik,and kaiming he.
2019. slowfast networks for videoin proceedings of the ieee/cvf in-recognition.
ternational conference on computer vision, pages6202–6211..jiyang gao, runzhou ge, kan chen, and ram neva-tia.
2018. motion-appearance co-memory networksfor video question answering.
in proceedings of theieee conference on computer vision and patternrecognition, pages 6576–6585..lianli gao, pengpeng zeng, jingkuan song, yuan-fang li, wu liu, tao mei, and heng tao shen.
2019..6175structured two-stream attention network for videoin proceedings of the aaaiquestion answering.
conference on artiﬁcial intelligence, volume 33,pages 6391–6398..rohit girdhar, joao carreira, carl doersch, and an-drew zisserman.
2018. a better baseline for ava.
arxiv preprint arxiv:1807.10066..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 6904–6913..kaiming he, georgia gkioxari, piotr doll´ar, and rossgirshick.
2017. mask r-cnn.
in proceedings of theieee international conference on computer vision,pages 2961–2969..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..deng huang, peihao chen, runhao zeng, qing du,mingkui tan, and chuang gan.
2020. location-aware graph convolutional networks for video ques-tion answering.
in aaai, pages 11021–11028..yunseok jang, yale song, youngjae yu, youngjin kim,and gunhee kim.
2017. tgif-qa: toward spatio-temporal reasoning in visual question answering.
inproceedings of the ieee conference on computervision and pattern recognition, pages 2758–2766..jianwen jiang, ziqiang chen, haojie lin, xibinzhao, and yue gao.
2020. divide and conquer:question-guided spatio-temporal contextual atten-tion for video question answering.
in aaai, pages11101–11108..pin jiang and yahong han.
2020. reasoning with het-erogeneous graph alignment for video question an-swering.
in aaai, pages 11109–11116..gi-cheon kang, jaeseo lim, and byoung-tak zhang.
2019. dual attention networks for visual referencein proceedings of theresolution in visual dialog.
2019 conference on empirical methods in naturallanguage processing, pages 2024–2033..will kay, joao carreira, karen simonyan, briansudheendra vijaya-zhang, chloe hillier,narasimhan, fabio viola, tim green, trevor back,paul natsev, et al.
2017. the kinetics human actionvideo dataset.
arxiv preprint arxiv:1705.06950..jin-hwa kim, jaehyun jun, and byoung-tak zhang.
2018a.
bilinear attention networks.
arxiv preprintarxiv:1805.07932..kyung-min kim, seong-ho choi, jin-hwa kim, andbyoung-tak zhang.
2018b.
multimodal dual atten-tion memory for video story question answering.
inproceedings of the european conference on com-puter vision (eccv), pages 673–688..thomas n kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
arxiv preprint arxiv:1609.02907..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a shamma, et al.
2017. visual genome: connecting language and vi-sion using crowdsourced dense image annotations.
international journal of computer vision, 123(1):32–73..thao minh le, vuong le, svetha venkatesh, andtruyen tran.
2020. hierarchical conditional relationnetworks for video question answering.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition, pages 9972–9981..jie lei, licheng yu, mohit bansal, and tamara l berg.
2018. tvqa: localized, compositional video ques-tion answering.
arxiv preprint arxiv:1809.01696..xiangpeng li, jingkuan song, lianli gao, xianglongliu, wenbing huang, xiangnan he, and chuanggan.
2019. beyond rnns: positional self-attentionwith co-attention for video question answering.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 8658–8665..zhouhan lin, minwei feng, cicero nogueira dos san-tos, mo yu, bing xiang, bowen zhou, and yoshuabengio.
2017. a structured self-attentive sentenceembedding.
arxiv preprint arxiv:1703.03130..naila murray, luca marchesotti, and florent perronnin.
2012. ava: a large-scale database for aesthetic vi-in 2012 ieee conference on com-sual analysis.
puter vision and pattern recognition, pages 2408–2415. ieee..kyoung-woon on, eun-sol kim, yu-jung heo, andbyoung-tak zhang.
2020. cut-based graph learn-ing networks to discover compositional structure ofsequential video data.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 34,pages 5315–5322..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..shaoqing ren, kaiming he, ross girshick, and jiansun.
2016. faster r-cnn: towards real-time objectieeedetection with region proposal networks.
transactions on pattern analysis and machine intelli-gence, 39(6):1137–1149..6176du tran, lubomir bourdev, rob fergus, lorenzo tor-resani, and manohar paluri.
2015. learning spa-tiotemporal features with 3d convolutional networks.
in proceedings of the ieee international conferenceon computer vision, pages 4489–4497..yao-hung hubert tsai, shaojie bai, paul pu liang,j zico kolter, louis-philippe morency, and ruslansalakhutdinov.
2019. multimodal transformer forunaligned multimodal language sequences.
in pro-ceedings of the conference.
association for com-putational linguistics.
meeting, volume 2019, page6558. nih public access..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-head self-attention: specialized heads do the heavyarxiv preprintlifting, the rest can be pruned.
arxiv:1905.09418..dejing xu, zhou zhao, jun xiao, fei wu, hanwangzhang, xiangnan he, and yueting zhuang.
2017.video question answering via gradually reﬁned at-tention over appearance and motion.
in proceedingsof the 25th acm international conference on multi-media, pages 1645–1653..jun xu, tao mei, ting yao, and yong rui.
2016. msr-vtt: a large video description dataset for bridgingvideo and language.
in proceedings of the ieee con-ference on computer vision and pattern recognition,pages 5288–5296..kelvin xu, jimmy lei ba, ryan kiros, kyunghyuncho, aaron courville, ruslan salakhutdinov,richard s zemel, and yoshua bengio.
2015. show,attend and tell: neural image caption generationwith visual attention.
in proceedings of the 32nd in-ternational conference on international conferenceon machine learning-volume 37, pages 2048–2057..zichao yang, xiaodong he, jianfeng gao, li deng,and alex smola.
2016. stacked attention networksin proceedings offor image question answering.
the ieee conference on computer vision and patternrecognition, pages 21–29..zhou yu, dejing xu, jun yu, ting yu, zhouzhao, yueting zhuang, and dacheng tao.
2019.activitynet-qa: a dataset for understanding complexweb videos via question answering.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 9127–9134..rowan zellers, yonatan bisk, ali farhadi, and yejinchoi.
2019.from recognition to cognition: vi-sual commonsense reasoning.
in proceedings of theieee/cvf conference on computer vision and pat-tern recognition, pages 6720–6731..6177