r2d2: recursive transformer based on differentiable treefor interpretable hierarchical language modeling.
xiang hu†∗ haitao mi†∗ zujie wen† yafang wang†yi su† jing zheng† gerard de melo‡ant financial services group†{aaron.hx, haitao.mi, zujie.wzj, yafang.wyf, yi.su, jing.zheng}@alibaba-inc.com†hasso plattner institute / university of potsdam‡gdm@demelo.org‡.
abstract.
human language understanding operates atmultiple levels of granularity (e.g., words,phrases, and sentences) with increasing levelsof abstraction that can be hierarchically com-bined.
however, existing deep models withstacked layers do not explicitly model any sortof hierarchical process.
this paper proposesa recursive transformer model based on dif-ferentiable cky style binary trees to emulatethe composition process.
we extend the bidi-rectional language model pre-training objec-tive to this architecture, attempting to predicteach word given its left and right abstractionnodes.
to scale up our approach, we also in-troduce an efﬁcient pruned tree induction algo-rithm to enable encoding in just a linear num-ber of composition steps.
experimental resultson language modeling and unsupervised pars-ing show the effectiveness of our approach.1.
1 introductionthe idea of devising a structural model of lan-guage capable of learning both representations andmeaningful syntactic structure without any human-annotated trees has been a long-standing but chal-lenging goal.
across a diverse range of linguistictheories, human language is assumed to possess arecursive hierarchical structure (chomsky, 1956,2014; de marneffe et al., 2006) such that lower-level meaning is combined to infer higher-levelsemantics.
humans possess notions of characters,words, phrases, and sentences, which children nat-urally learn to segment and combine..pretrained language models such as bert (de-vlin et al., 2019) have achieved substantial gains.
∗equal contribution.
1the code is available at: https://github.com/.
alipay/structuredlm_rtdt.
across a range of tasks.
however, they simply ap-ply layer-stacking with a ﬁxed depth to increasethe modeling power (bengio, 2009; salakhutdinov,2014).
moreover, as the core transformer compo-nent (vaswani et al., 2017) does not capture posi-tional information, one also needs to incorporateadditional positional embeddings.
thus, pretrainedlanguage models do not explicitly reﬂect the hier-archical structure of linguistic understanding..inspired by le and zuidema (2015), maillardet al.
(2017) proposed a fully differentiable ckyparser to model the hierarchical process explicitly.
to make their parser differentiable, they primar-ily introduce an energy function to combine allpossible derivations when constructing each cellrepresentation.
however, their model is based ontree-lstms (tai et al., 2015; zhu et al., 2015) andrequires o(n3) time complexity.
hence, it is hardto scale up to large training data..in this paper, we revisit these ideas, and proposea model applying recursive transformers along dif-ferentiable trees (r2d2).
to obtain differentiabil-ity, we adopt gumbel-softmax estimation (janget al., 2017) as an elegant solution.
our encoderparser operates in a bottom-up fashion akin to ckyparsing, yet runs in linear time with regard to thenumber of composition steps, thanks to a novelpruned tree induction algorithm.
as a training ob-jective, the model seeks to recover each word ina sentence given its left and right syntax nodes.
thus, our model does not require any positionalembedding and does not need to mask any wordsduring training.
figure 1 presents an example bi-nary tree induced by our method: without anysyntactic supervision, it acquires a model of hier-archical construction from the word-piece level towords, phrases, and ﬁnally the sentence level..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4897–4908august1–6,2021.©2021associationforcomputationallinguistics4897what ’ s more , such short - term cat #ac #ly #sms are sur #vi #vable and are no cause for panic selling ..figure 1: an example output tree emerging from our proposed method..we make the following contributions:.
• our novel cky-based recursive transformer ondifferentiable trees model is able to learn bothrepresentations and tree structure (section 2.1).
• we propose an efﬁcient optimization algorithmto scale up our approach to a linear number ofcomposition steps (section 2.2)..• we design an effective pre-training objective,which predicts each word given its left and rightsyntactic nodes (section 2.3)..for simplicity and efﬁciency reasons, in this pa-per we conduct experiments only on the tasks oflanguage modeling and unsupervised tree induc-tion.
the experimental results on language model-ing show that our model signiﬁcantly outperformsbaseline models with same parameter size even infewer training epochs.
at unsupervised parsing,our model as well obtains competitive results.
2 methodology.
2.1 model architecture.
the lowest level, we have terminal nodes ti,i withei,i initialized as embeddings of inputs si, whilepi,i and (cid:101)pi,i are set to one.
when j > i, the rep-resentation ei,j is a weighted sum of intermediatecombinations ck.
i,j, deﬁned as:.
cki,j, pki,j = f (ei,k, ek+1,j)i,j = pk(cid:101)pki,j (cid:101)pi,k (cid:101)pk+1,jαi,j = gumbel(log((cid:101)pi,j))i,j , ..., cj−1i,j, ci+1ei,j = [cii,j ]αi,j(cid:124)i,j[pi,j, (cid:101)pi,j][pi,j, (cid:101)pi,j] = α.
(1).
(2).
(3).
(4).
(5).
i,j and (cid:101)pk.
here, k is a split point from i to j − 1, f (·) is acomposition function that we shall further deﬁnelater on, pki,j denote the single step combi-nation probability and the subtree probability, re-spectively, at split point k, pi,j and (cid:101)pi,j are the con-catenation of all pki,j values, and gumbel isthe straight-through gumbel-softmax operationof jang et al.
(2017) with temperature set to one.
the [, ] notation denotes stacking of tensors..i,j or (cid:101)pk.
figure 2: chart data structure.
there are two alter-native ways of generating t1,3: combining either(t1,2, t3,3) or (t1,1, t2,3)..differentiable tree.
we follow maillard et al.
(2017) in deﬁning a differentiable binary parserusing a cky-style (cocke, 1969; kasami, 1966;younger, 1967) encoder.
informally, given a sen-tence s = {s1, s2, ..., sn} with n words or word-pieces, figure 2 shows the chart data structure t ,where each cell ti,j is a tuple (cid:104)ei,j, pi,j, (cid:101)pi,j(cid:105), ei,j isa vector representation, pi,j is the probability of asingle composition step, and (cid:101)pi,j is the probabilityof the subtree at span [i, j] over sub-string si:j. at.
figure 3: recursive transformer-based encoder..recursive transformer.
figure 3 provides aschematic overview of the composition functionf (·), comprising n transformer layers.
takingi,j and pkcki,j as an example, the input is a concate-nation of two special tokens [sum] and [cls],the left cell ei,k, and the right cell ek+1,j.
we also.
4898add role embeddings ([left] and [right]) tothe left and right inputs, respectively.
thus, theinput consists of four vectors in rd.
we denote ash[sum], h[cls], hi,k, hk+1,j ∈ rd the hidden stateof the output of n transformer layers.
this isfollowed by a linear layer over h[sum] to obtain.
pki,j = σ(wph[sum] + bp),.
(6).
where wp ∈ r1×d, bp ∈ r, and σ refers to sigmoidactivation.
then, ck.
i,j is computed as.
wki,j = softmax(wwh[cls] + bw)i,j = [hi,k, hk+1,j]wkck.
i,j,.
(7).
where ww ∈ r2×d with wki,j ∈ r2 capturing therespective weights of the left and right hidden stateshi,k and hk+1,j, and the ﬁnal cki,j is a weighted sumof hi,k and hk+1,j..the.
tree recovery.
asstraight-throughgumbel-softmax picks the optimal splitting pointk at each cell in practice, it is straightforward torecover the complete derivation tree, tree(t1,n),from the root node t1,n in a top-down mannerrecursively..2.2 complexity optimization.
for j ∈ i to n − 1 do.
(cid:46) create a new 2-d array.
(cid:46) find optimal merge point.
u ← find (t , m)n ← t .lent (cid:48) ← [n − 1][n − 1]for i ∈ 1 to n − 1 do.
i(cid:48) ← i ≥ u + 1 ?
i + 1 : ij(cid:48) ← j ≥ u ?
j + 1 : jt (cid:48)i,j ← ti(cid:48),j(cid:48) (cid:46) skip dark gray cells in fig.
4return t (cid:48).
algorithm 1 pruned tree induction algorithmrequire: t = 2-d array holding cell referencesrequire: m = pruning threshold1: function pruning(t , m)2:3:4:5:6:7:8:9:10:11: function treeinduction(t , m)12:13:14:15:16:17:18:19:20:21:.
l ← min(t + 1, m)for i ∈ 1 to t (cid:48).len − l + 1 do.
t (cid:48) ← tfor t ∈ 1 to t .len − 1 do.
i,j is empty thencompute cell t (cid:48).
j ← i + l − 1if t (cid:48).
t (cid:48) ← pruning (t (cid:48),m).
(cid:46) clamp the span length.
i,j with equation 1.if t ≥ m then.
return t.as the core computation comes from the compo-sition function f (·), our pruned tree induction al-gorithm aims to reduce the number of compositioncalls from o(n3) in the original cky algorithm tolinear..our intuition is based on the conjecture that lo-cally optimal compositions are likely to be retainedand participate in higher-level feature combination.
speciﬁcally, taking t 2 in figure 4 (c) as an exam-ple, we only pick locally optimal nodes from thesecond row of t 2. if t 24,5 is locally optimal andnon-splittable, then all the cells highlighted in darkgray in (d) may be pruned, as they break span [4, 5].
for any later encoding, including higher-level ones,we can merge the nodes and treat t 24,5 as a newnon-splittable terminal node (see (e) to (g))..(cid:46) create an array.
(cid:46) collect cells on the 2nd row.
(cid:46) iterate to m-th row.
τ ← ∅for i ∈ 1 to n − m + 1 do.
algorithm 2 find the best merge pointrequire: t = 2-d array holding cell referencesrequire: m = pruning threshold1: function find(t , m)n ← t .len2:l ← [n − 1]3:for i ∈ 1 to n − 1 do4:l[i] ← ti,i+15:6:7:8:9:10:11:12:13:14:15:16:17:.
i ← l.index(x)pl ← 1 − l[i − 1].ppr ← 1 − l[i + 1].p.
l ← new list()for cell x ∈ τ do.
l.append(x.p · pl · pr).
return argmaxi l[i].
j = i + m − 1τ ← τ ∪ {c | c ∈ tree(ti,j) ∧ c ∈ l}.
(cid:46) if index out of boundary then set to 0.figure 4 walks through the steps of processinga sentence of length 6, where si:j denotes a sub-string from si to sj.
algorithm 1 constructs ourchart table t sequentially row-by-row.
let t be thetime step and m be the pruning threshold.
first,we invoke treeinduction (t , m), and computea row of cells at each time step when t < m asin regular cky parsing, leading to result (b) infigure 4. when t ≥ m, we call pruning (t , m)in line 15. as mentioned, the pruning functionaims to ﬁnd the locally optimal combination nodein t , prunes some cells, and returns a new tableomitting the pruned cells.
algorithm 2 shows howwe find the locally optimal combination node.
again, the candidate set for the locally optimalnode is the second row of t , and we also takeadvantage of the subtrees derived from all nodesin the m-th row to limit the candidate set.
lines6 to 9 in algorithm 2 generate the candidate set.
each candidate must be in the second row of t andalso must be used in a subtree of any node in them-th row.
given the candidate set, we ﬁnd the leastambiguous one as the optimal selection (lines 11 to.
4899figure 4: example of encoding.
(a) initialized chart table.
(b) row-by-row encoding up to pruningthreshold m. (c) for each cell in the m-th row, recover its subtree and collect candidate nodes, each ofwhich must appear in the subtree and also must be in the 2nd row, e.g., the tree of t 23,5 is within the darkline, and the candidate node is t 24,5 here, and treat span s4:5as non-splittable.
thus, the dark gray cells become prunable.
(e) construct a new chart table t 3 treatingcell t 24,5 as a new terminal node and eliminating the prunable cells.
(f) compute empty cells in m-th row.
(g) keep pruning and growing the tree until no further empty cells remain.
(h) final discrete chart table..4,5.
(d) find locally optimal node, which is t 2.
17), i.e., the node with maximum own probabilitywhile adjacent bi-gram node probabilities (lines13 and 14 ) are as low as possible.
after selectingthe best merge point u, cells in {t ti,j | j = u} ∪{t t| i = u + 1} are pruned (highlighted ini,jdark gray in (d)), and we generate a new tablet t+1 by removing pruned nodes (lines 4 to 9 inalgorithm 1).
then we obtain (e), and compute theempty cells on the m-th row of t 3 to obtain (f).
wecontinue with the loop in line 13, trigger pruningagain, and obtain a new table t t+1, and then ﬁllempty cells on the m-th row t t+1.
continuingwith the process until all cells are computed, asshown in (g), we ﬁnally obtain a discrete charttable as given in (h)..in terms of the time complexity, when t ≥ m,there are at most m cells to update, so the com-plexity of each step is less than o(m2).
whent ≤ m, the complexity is o(t3) ≤ o(m2t).
thus,the overall times to call the composition functionis o(m2n), which is linear considering m is a con-stant..2.3 pretraining.
different from the masked language model trainingof bert, we directly minimize the sum of all neg-ative log probabilities of all words or word-pieces.
figure 5: the objective for our pretrained model..si given their left and right contexts..minθ.n(cid:88).
i=1.
− log pθ(si | s1:i−1, si+1:n).
(8).
as shown in figure 5, after invoking our recur-sive encoder on a sentence s, we directly use e1,i−1and ei+1,n as the left and right contexts, respec-tively, for each word si.
to distinguish from theencoding task, the input consists of a concatenationof a special token [mask], e1,i−1, and ei+1,n.
weapply the same composition function f (·) as in fig-ure 3, and feed h[mask] through an output softmaxto predict the distribution of si over the complete.
4900vocabulary.
finally, we compute the cross-entropyover the prediction and ground truth distributions.
in cases where e1,i−1 or ei+1,n is missing dueto the pruning algorithm in section 2.2, we simplyuse the left or right longest adjacent non-emptycell.
for example, tx,i−1 means the longest non-empty cell assuming we cannot ﬁnd any non-emptytx(cid:48),i−1 for all x(cid:48) < x. analogously, ti+1,y is de-ﬁned as the longest non-empty right cell.
notethat although the ﬁnal table is sparse, the sentencerepresentation e1,n is always established.
3 experimentsas our approach (r2d2) is able to learn both repre-sentations and intermediate structure, we evaluateits representation learning ability on bidirectionallanguage modeling and evaluate the intermediatestructures on unsupervised parsing..3.1 bidirectional language modeling.
3.1.1 setup.
baselines and evaluation.
as the objective ofour model is to predict each word with its left andright context, we use the pseudo-perplexity (pppl)metric of salazar et al.
(2020) to evaluate bidirec-tional language modeling..1n.n(cid:88).
i=1.
l(s) =.
logp (si | s1:i−1, si+1:n, θ).
pppl(s) = exp.
−.
.
.
l(sj).
1n.n(cid:88).
j=1.
pppl is a bidirectional version of perplexity, estab-lishing a macroscopic assessment of the model’sability to deal with diverse linguistic phenomena.
we compared our approach with sota autoen-coding and autoregressive language models ca-pable of capturing bidirectional contexts, includ-ing bert, xlnet (yang et al., 2019), and al-bert (lan et al., 2020).
for a fair apples to applescomparison, all models use the same vocabularyand are trained from scratch on a language model-ing corpus.
the models are all based on the opensource transformers library2.
to compute ppplfor models based on sequential transformers, foreach word si, we only mask si while others remainvisible to predict si.
when we evaluate our r2d2model, for each word si, we treat the left s1:i−1and right si+1:n as two complete sentences sepa-rately, then encode them separately, and pick the.
2https://github.com/huggingface/transformers.
bertxlnetalbertxlnetbertt-lstm (m=4)ours (m=4)ours (m=8)bertxlnetalbertxlnetbertours (m=4)ours (m=8).
#param #layer #epoch cplx331212121333312121233.pppl10 o(n2) 441.4210 o(n) 301.8710 o(n2) 219.2010 o(n) 127.7410 o(n2) 103.5410 o(n) 820.5710 o(n)83.1010 o(n)57.4060 o(n2) 112.1760 o(n) 105.6460 o(n2) 71.5260 o(n)59.7460 o(n2) 44.7060 o(n)55.7060 o(n)54.60.
46m46m46m116m109m46m45m45m46m46m46m116m109m45m45m.
table 1: comparison with state-of-the-art modelstrained from scratch on wikitext-2 with differentsettings (number of transformer layers and trainingepochs).
m is the pruning threshold..root nodes as the ﬁnal representations of left andright contexts.
in the end, we predict word si byrunning our transformers as in figure 5..data.
the english language wikitext-2 cor-pus (merity et al., 2017) serves as training data.
the dataset is split at the sentence level, and sen-tences longer than 128 after tokenization are dis-carded (about 0.03% of the original data).
the totalnumber of sentences is 68,634, and the average sen-tence length is 33.4..hyperparameters.
the tree encoder of ourmodel uses 3-layer transformers with 768-dimensional embeddings, 3,072-dimensional hid-den layer representations, and 12 attention heads.
other models based on the transformer share thesame setting but vary on the number of layers.
training is conducted using adam optimizationwith weight decay with a learning rate of 5 × 10−5.
the batch size is set to 8 for m=8 and 32 for m=4,though we also limit the maximum total length foreach batch, such that excess sentences are movedto the next batch.
the limit is set to 128 for m=8and 512 for m=4.
it takes about 43 hours for 10epochs of training with m = 8 and about 9 hourswith m=4, on 8 v100 gpus..3.1.2 results and discussion.
table 1 presents the results of all models with dif-ferent parameters.
our model outperforms othermodels with the same parameter size and number.
4901ours (m=4).
-w/o pruning-w/o pruning∗.
emb.
× hid.
× lay.
768 × 3072 × 312 × 12 × 1768 × 3072 × 3.training time7h1125h5 × 107h.
table 2: training time for one epoch on a singlev100 gpu, where emb.
and hid.
represent thedimensions of word embeddings and hidden staterespectively, and lay.
is the number of transformerlayers.
∗ means proportionally estimated time..of training epochs.
these results suggest that ourmodel architecture utilizes the training data moreefﬁciently.
comparing the different pruning thresh-olds m=4 and m=8 (last two rows), the two modelsactually converge to a similar place after 60 epochs,conﬁrming the effectiveness of the pruned tree in-duction algorithm.
we also replace transformerswith tree-lstms as in jang et al.
(2017), denotedas t-lstm, ﬁnding that the perplexity is signiﬁ-cantly higher compared to other models..the best score is from the bert model with 12layers at epoch 60. although our model has a lin-ear time complexity, it is still a sequential encodingmodel, and hence its training time is not compa-rable to that of fully parallelizable models.
thus,we do not have results of 12-layer transformersin table 1. the experimental results comparingmodels with the same parameter size suggest thatour model may perform even better with furtherdeep layers..table 2 shows the training time of our r2d2 withand without pruning.
the last row is proportionallyestimated by running the small setting (12×12×1).
it is clear that it is not feasible to run our r2d2without pruning..3.2 unsupervised constituency parsing.
we next assess to what extent the trees that nat-urally arise in our model bear similarities withhuman-speciﬁed parse trees..3.2.1 setup.
baselines and evaluation.
for comparison, wefurther include four recent strong models for un-supervised parsing with open source code: bertmasking (wu et al., 2020), ordered neurons (shenet al., 2019), diora (drozdov et al., 2019) andc-pcfg (kim et al., 2019a).
following htut et al.
(2018), we train all systems on a training set con-sisting of raw text, and evaluate and report theresults on an annotated test set.
as an evaluation.
metric, we adopt sentence-level unlabeled f1 com-puted using the script from kim et al.
(2019a).
wecompare against the non-binarized gold trees perconvention.
the best checkpoint for each system ispicked based on scores on the validation set..as our model is a pretrained model based onword-pieces, for a fair comparison, we test all mod-els with two types of input: word level (w) andword-piece level (wp)3. to support word-piecelevel evaluation, we convert gold trees to word-piece level trees by simply breaking each terminalnode into a non-terminal node with its word-piecesas terminals, e.g., (nn discrepancy) into (nn (wpdisc) (wp ##re) (wp ##pan) (wp ##cy).
we setthe pruning threshold m to 8 for our tree encoder.
to support a word-level evaluation, since ourmodel uses word-pieces, we force it to not pruneor select spans that conﬂict with word spans dur-ing prediction, and then merge word-pieces intowords in the ﬁnal output.
however, note that thisconstraint is only used for word-level prediction..for training, we use the same hyperparame-ters as in section 3.1.1. our model pretrained onwikitext-2 is ﬁnetuned on the training set with thesame unsupervised loss objective.
for chinese, weuse a subset of chinese wikipedia for pretraining,speciﬁcally the ﬁrst 100,000 sentences shorter than150 characters..data.
we test our approach on the penn treebank(ptb) (marcus et al., 1993) with the standard splits(2-21 for training, 22 for validation, 23 for test)and the same preprocessing as in recent work (kimet al., 2019a), where we discard punctuation andlower-case all tokens.
to explore the universalityof the model across languages, we also run exper-iments on chinese penn treebank (ctb) 8 (xueet al., 2005), on which we also remove punctuation.
note that in all settings, the training is conductedentirely on raw unannotated text..3.2.2 results and discussion.
table 3 provides the unlabeled f1 scores of all sys-tems on the wsj and ctb test sets.
it is clear thatall systems perform better than left/right branchingand random trees.
word-level c-pcfg (w) per-forms best on both the wsj and ctb test sets whenmeasuring against word-level gold standard trees.
our system performs better than on-lstm (w),but worse than diora (w) and c-pcfg (w).
still,.
3as diora relies on elmo word embeddings, to sup-port word-piece level inputs, we use bert word-piece em-beddings instead..4902wsjf1(m) f1.
ctbf1.
----.
model.
8.15 11.2839.62 27.5317.76 20.1737.39 33.2450.0† 47.72 24.73.cplxleft branching (w) o(n)right branching (w) o(n)random trees (w) o(n)bert-mask (wp) o(n4)o(n)o(n3) 58.9† 51.42o(n3) 60.1† 54.08 49.95o(n)48.11 44.85o(n3)43.94o(n3)49.76 60.34o(n)52.28 63.94.on-lstm (w)diora (w)c-pcfg (w)ours (wp)diora (wp)c-pcfg (wp)ours (wp).
----.
-.
-.
table 3: unsupervised parsing results with word(w) or word-piece (wp) as input.
values with † aretaken from kim et al.
(2019a).
f1(m) describesthe max.
score of 4 runs with different randomseeds.
the f1 column shows results of our runswith a random seed.
the bottom three systems takeword-pieces as input, and are also measured againstword-piece level golden trees..this is a remarkable result.
note that models suchas c-pcfg are specially designed for unsupervisedparsing, e.g., adopting 30 nonterminals, 60 preter-minals, and a training objective that is well-alignedwith unsupervised parsing.
in contrast, the objec-tive of our model is that of bi-directional languagemodeling, and the derived binary trees are merelya by-product of our model that happen to emergenaturally from the model’s preference for structuresthat are conducive to better language modeling..another factor is the mismatch between our train-ing and evaluation, where we train our model at theword-piece level, but evaluate against word-levelgold trees.
for comparison, we thus also consid-ered diora (wp), c-pcfg (wp), and our sys-tem all trained on word-piece inputs, and evaluatedagainst word-piece level gold trees.
the last threelines show the results, with our system achievingthe best f1.
as breaking words into word-piecesintroduces word boundaries as new spans, whileword boundaries are easier to recognize, the overallf1 score may increase, especially on chinese..analysis.
in order to better understand why ourmodel works better when evaluating on word-piecelevel golden trees, we compute the recall of con-stituents following kim et al.
(2019b) and drozdovet al.
(2020).
besides standard constituents, wealso compare the recall of word-piece chunks and.
(wp) wd nnp np.
sbardiora 81.65 77.83 71.24 17.30 22.16c-pcfg 74.26 66.44 65.01 23.63 40.4099.24 86.76 72.59 24.74 39.81.ours.
vp.
jsw.b c-pcfg 89.34t97.16c.ours.
--.
46.74 39.5361.26 37.90.
--.
table 4: recall of constituents and words at word-piece level.
wd means word..proper noun chunks.
proper noun chunks are ex-tracted by ﬁnding adjacent unary nodes with sameparent and tag nnp..table 4 reports the recall scores for constituentsand words on the wsj and ctb test sets.
ourmodel and diora perform better for small se-mantic units, while c-pcfg better matches largersemantic units such as vp and sbar.
the recall ofword chunks (wd) of our system is almost perfectand signiﬁcantly better than for other algorithms.
please note that all word-piece level models aretrained fairly without using any boundary informa-tion.
although it is trivial to recognize englishword boundaries among word-pieces using rules,this is non-trivial for chinese.
additionally, the re-call of proper noun segments is as well signiﬁcantlybetter for our model compared to other algorithms..3.3 dependency tree compatibility.
we compared examples of trees inferred by ourmodel with the corresponding ground truth con-stituency trees (see appendix), encountering rea-sonable structures that are different from the con-stituent structure posited by the manually deﬁnedgold trees.
experimental results of previous work(drozdov et al., 2020; kim et al., 2019a) also showsigniﬁcant variance with different random seeds.
thus, we hypothesize that an isomorphy-focusedf1 evaluation with respect to gold constituencytrees is insufﬁcient to evaluate how reasonable theinduced structures are.
in contrast, dependencygrammar encodes semantic and syntactic relationsdirectly, and has the best interlingual phrasal co-hesion properties (fox, 2002).
therefore, we in-troduce dependency compatibility as an additionalmetric and re-evaluate all system outputs..3.3.1 setup.
baselines and data.
as our approach is a word-piece level pretrained model, to enable a fair com-parison, we train all models on word-pieces and.
4903wsj.
ctb.
modelbert-mask (w)on-lstm (w)diora (w)c-pcfg (w)diora (wp)c-pcfg (wp)ours (wp).
%all53.5361.43†67.7672.74†54.7367.1869.29.
%n≤10 %n≤20 %n≤40 %all %n≤10 %n≤20 %n≤4036.6248.5677.0377.05†26.5936.48——78.0885.10†58.1664.4168.8083.0980.29.
55.4662.99†68.8074.65†55.6868.2070.29.
44.6655.94†64.1567.19†49.2261.0364.79.
68.8958.57—75.54.
47.2734.08—65.89.
—74.9874.42.
—62.2564.74.
—61.0463.86.
—52.5259.20.table 5: compatibility with dependency trees.
(w) denotes word level inputs, (wp) refers to word-piecelevel inputs.
%all denotes the accuracy on all test sentences, while %n≤x is the accuracy on sentences ofup to x words.
values with † are evaluated with predicted trees from kim et al.
(2019a).
figure 6: examples of compatible and incompatiblespans..learn models with the same settings as in the orig-inal papers.
evaluation at the word-piece levelreveals the model’s ability to learn structure froma smaller granularity.
in this section, we keep theword-level gold trees unchanged and invoke stan-ford corenlp (manning et al., 2014) to convertthe wsj and ctb into dependency trees..evaluation.
our metric is based on the notion ofquantifying the compatibility of a tree by countinghow many spans comply with dependency relationsin the gold dependency tree.
speciﬁcally, as illus-trated in figure 6, a span is deemed compatiblewith the ground truth if and only if this span formsan independent subtree..formally, given a gold dependency tree d, wedenote as s(d) the raw token sequence for d. con-sidering predicting a binary tree for word-level in-put, predicted spans in the binary tree are denotedas z. for any span z ∈ z, the subgraph of dincluding nodes in z and directional edges betweenthem is referred to as gz.
o(gz) is deﬁned as theset of nodes with parent nodes not in gz and i(gz)denotes the set of nodes whose child nodes arenot in gz.
thus, |o(gz)| and |i(gz)| are the out-degree and in-degree of the subgraph gz.
let i(z)denote whether z is valid, deﬁned as.
i(z).
(cid:26)1,0,.
|o(gz)| = 1 and i(gz) ⊆ o(gz)otherwise..(9).
for binary tree spans for word-piece level input,if z breaks word-piece spans, then i(z) = 0. oth-erwise, word-pieces are merged to words and theword-level logic is followed.
speciﬁcally, to makethe results at the word and word-piece levels com-parable, i(z) is forced to be zero if z only coversa single word.
the ﬁnal compatibility for z is(cid:80).
z∈z i(z).
|s(d)| − 1.
3.3.2 results and discussion.
table 5 lists system results on the wsj and ctbtest sets.
%all refers to the accuracy on all test sen-tences, while %n≤x is the accuracy on sentenceswith up to x words.
it is clear that the smaller granu-larity at the word-piece level makes this task harder.
our model performs better than other systems atthe word-piece level on both english and chineseand even outperforms the baselines in many casesat the word level.
it is worth noting that the resultis evaluated on the same binary predicted trees aswe use for unsupervised constituency parsing, yetour model outperforms baselines that perform bet-ter in table 3. one possible interpretation is thatour approach learns to prefer structures differentfrom human-deﬁned phrase structure grammar butself-consistent and compatible with a tree structure.
to further understand the strengths and weaknessesof each baseline, we analyzed the compatibility ofdifferent sentence length ranges.
interestingly, weﬁnd that our approach performs better on long sen-tences compared with c-pcfg at the word-piecelevel.
this shows that a bidirectional languagemodeling objective can learn to induce accuratestructures even on very long sentences, on whichcustom-tailored methods may not work as well..4904this makes it possible to train with backpropaga-tion.
however, their model runs in o(n3) and theyuse tree-lstms.
5 conclusion and outlookin this paper, we have proposed an efﬁcient cky-based recursive transformer to directly model hi-erarchical structure in linguistic utterances.
wehave ascertained the effectiveness of our approachon language modeling and unsupervised parsing.
with the help of our efﬁcient linear pruned treeinduction algorithm, our model quickly learns in-terpretable tree structures without any syntacticsupervision, which yet prove highly compatiblewith human-annotated trees.
as future work, weare investigating pre-training our model on billionword corpora as done for bert, and ﬁne-tuningour model on downstream tasks.
acknowledgementswe thank liqian sun, the wife of xiang hu, fortaking care of their newborn baby during criticalphases, which enabled xiang to polish the workand perform experiments..4 related work.
pre-trained models.
pre-trained models haveachieved signiﬁcant success across numerous tasks.
elmo (peters et al., 2018), pretrained on bidi-rectional language modeling based on bi-lstms,was the ﬁrst model to show signiﬁcant improve-ments across many downstream tasks.
gpt (rad-ford et al., 2018) replaces bi-lstms with a trans-former (vaswani et al., 2017).
as the global atten-tion mechanism may reveal contextual information,it uses a left-to-right transformer to predict thenext word given the previous context.
bert (de-vlin et al., 2019) proposes masked language model-ing (mlm) to enable bidirectional modeling whileavoiding contextual information leakage by directlymasking part of input tokens.
as masking inputtokens results in missing semantics, xlnet (yanget al., 2019) proposes permuted language model-ing (plm), where all bi-directional tokens are visi-ble when predicting masked tokens.
however, allaforementioned transformer based models do notnaturally capture positional information on theirown and do not have explicit interpretable struc-tural information, which is an essential feature ofnatural language.
to alleviate the above shortcom-ings, we extend pre-training and the transformermodel to structural language models..representation with structures.
in the line ofwork on learning a sentence representation withstructures, socher et al.
(2011) proposed the ﬁrstneural network model applying recursive autoen-coders to learn sentence representations, but theirapproach constructs trees in a greedy way, and it isstill unclear how autoencoders can perform againstlarge pre-trained models (e.g., bert).
yogatamaet al.
(2017) jointly train their shift-reduce parserand sentence embedding components.
as theirparser is not differentiable, they have to resort toreinforcement training, but the learned structurescollapse to trivial left/right branching trees.
thework of urnng (kim et al., 2019b) applies varia-tional inference over latent trees to perform unsu-pervised optimization of the rnng (dyer et al.,2016), an rnn model that estimates a joint dis-tribution over sentences and trees based on shift-reduce operations.
maillard et al.
(2017) proposean alternative approach, based on cky parsing.
the algorithm is made differentiable by using asoft-gating approach, which approximates discretecandidate selection by a probabilistic mixture ofthe constituents available in a given cell of the chart..4905referencesyoshua bengio.
2009. learning deep architectures for.
ai.
now publishers inc..noam chomsky.
1956. three models for the descrip-tion of language.
ire trans.
inf.
theory, 2(3):113–124..noam chomsky.
2014. aspects of the theory of syntax,.
volume 11. mit press..john cocke.
1969. programming languages and theircompilers: preliminary notes.
new york univer-sity, usa..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..andrew drozdov, subendhu rongali, yi-pei chen,tim o’gorman, mohit iyyer, and andrew mccal-lum.
2020. unsupervised parsing with s-diora:single tree encoding for deep inside-outside recur-sive autoencoders.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 4832–4845, online.
as-sociation for computational linguistics..andrew drozdov, patrick verga, mohit yadav, mohitiyyer, and andrew mccallum.
2019. unsupervisedlatent tree induction with deep inside-outside recur-sive auto-encoders.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1129–1141, minneapolis, minnesota.
association for computational linguistics..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 199–209, san diego, california.
association for computational linguistics..heidi fox.
2002. phrasal cohesion and statistical ma-chine translation.
in proceedings of the 2002 con-ference on empirical methods in natural languageprocessing (emnlp 2002), pages 304–3111.
asso-ciation for computational linguistics..phu mon htut, kyunghyun cho, and samuel bowman.
2018. grammar induction with neural languagemodels: an unusual replication.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4998–5003, brus-sels, belgium.
association for computational lin-guistics..eric jang, shixiang gu, and ben poole.
2017. categor-ical reparameterization with gumbel-softmax.
in 5thinternational conference on learning representa-tions, iclr 2017, toulon, france, april 24-26, 2017,conference track proceedings.
openreview.net..tadao kasami.
1966..recognitionand syntax-analysis algorithm for context-free lan-guages.
coordinated science laboratory report no.
r-257..an efﬁcient.
yoon kim, chris dyer, and alexander rush.
2019a.
compound probabilistic context-free grammars forgrammar induction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 2369–2385, florence, italy.
asso-ciation for computational linguistics..yoon kim, alexander rush, lei yu, adhiguna kun-coro, chris dyer, and g´abor melis.
2019b.
unsu-pervised recurrent neural network grammars.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 1105–1117,minneapolis, minnesota.
association for computa-tional linguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..phong le and willem zuidema.
2015. the forest con-volutional network: compositional distributional se-mantics with a neural chart and without binarization.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages1155–1164, lisbon, portugal.
association for com-putational linguistics..jean maillard, stephen clark, and dani yogatama.
2017.jointly learning sentence embeddingsand syntax with unsupervised tree-lstms.
corr,abs/1705.09189..christopher manning, mihai surdeanu, john bauer,jenny finkel, steven bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-in proceedings of 52nd annualcessing toolkit.
meeting of the association for computational lin-guistics: system demonstrations, pages 55–60, bal-timore, maryland.
association for computationallinguistics..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
computa-tional linguistics, 19(2):313–330..marie-catherine de marneffe, bill maccartney, andchristopher d. manning.
2006. generating typed.
4906ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..zhiyong wu, yun chen, ben kao, and qun liu.
2020.perturbed masking: parameter-free probing for ana-lyzing and interpreting bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4166–4176, online.
as-sociation for computational linguistics..naiwen xue, fei xia, fu-dong chiou, and martapalmer.
2005. the penn chinese treebank: phrasestructure annotation of a large corpus.
nat.
lang.
eng., 11(2):207–238..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 5754–5764..dani yogatama, phil blunsom, chris dyer, edwardgrefenstette, and wang ling.
2017. learning tocompose words into sentences with reinforcementlearning.
in 5th international conference on learn-ing representations, iclr 2017, toulon, france,april 24-26, 2017, conference track proceedings.
openreview.net..daniel h younger.
1967. recognition and parsing ofcontext-free languages in time n3.
information andcontrol, 10(2):189–208..xiao-dan zhu, parinaz sobhani, and hongyu guo.
2015. long short-term memory over recursive struc-tures.
in proceedings of the 32nd international con-ference on machine learning, icml 2015, lille,france, 6-11 july 2015, volume 37 of jmlr work-shop and conference proceedings, pages 1604–1612. jmlr.org..dependency parses from phrase structure parses.
inproceedings of the fifth international conferenceon language resources and evaluation (lrec’06),genoa, italy.
european language resources associ-ation (elra)..stephen merity, caiming xiong, james bradbury, andrichard socher.
2017. pointer sentinel mixture mod-in 5th international conference on learningels.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..ruslan salakhutdinov.
2014. deep learning..in the20th acm sigkdd international conference onknowledge discovery and data mining, kdd ’14,new york, ny, usa - august 24 - 27, 2014, page1973. acm..julian salazar, davis liang, toan q. nguyen, and ka-trin kirchhoff.
2020. masked language model scor-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 2699–2712, online.
association for compu-tational linguistics..yikang shen, shawn tan, alessandro sordoni, andaaron c. courville.
2019. ordered neurons: inte-grating tree structures into recurrent neural networks.
in 7th international conference on learning repre-sentations, iclr 2019, new orleans, la, usa, may6-9, 2019. openreview.net..richard socher, jeffrey pennington, eric h. huang,andrew y. ng, and christopher d. manning.
2011.semi-supervised recursive autoencoders for predict-in proceedings of theing sentiment distributions.
2011 conference on empirical methods in naturallanguage processing, pages 151–161, edinburgh,scotland, uk.
association for computational lin-guistics..kai sheng tai, richard socher, and christopher d.manning.
2015. improved semantic representationsfrom tree-structured long short-term memory net-in proceedings of the 53rd annual meet-works.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing (volume 1: long papers),pages 1556–1566, beijing, china.
association forcomputational linguistics..4907a appendix: tree examples.
system.
tree.
r2d2.
gold.
r2d2.
gold.
r2d2.
gold.
r2d2.
gold.
r2d2.
gold.
when the price of plastics took off in 1987 quantum chemical corp .
went along for the ride.
when the price of plastics took off in 1987 quantum chemical corp. went along for the ride.
pricing cycles to be sure are nothing new for plastics producers.
pricing cycles to be sure are nothing new for plastics producers.
we were all wonderful heroes last year says an executive at one of quantum ’ s competitors.
we were all wonderful heroes last year says an executive at one of quantum ’s competitors.
in the u .
s .
poly ##eth ##yle ##ne market quantum has claimed the largest share about 20 %.
in the u.s. polyethylene market quantum has claimed the largest share about 20 %.
noting others ’ estimates of when price increases can be sustained he remarks some say october.
noting others ’ estimates of when price increases can be sustained he remarks some say october.
4908