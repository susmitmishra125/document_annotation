explaining relationships between scientiﬁc documents.
kelvin luu1,∗ xinyi wu1,∗ rik koncel-kedziorski1.
kyle lo2.
isabel cachola3 noah a. smith1,2.
1university of washington3johns hopkins university2allen institute for ai{kellu,nasmith}@cs.washington.edu, {xywu,kedzior}@uw.edu,kylel@allenai.org, icachola@cs.jhu.edu.
abstract.
we address the task of explaining relation-ships between two scientiﬁc documents usingnatural language text.
this task requires mod-eling the complex content of long technicaldocuments, deducing a relationship betweenthese documents, and expressing that relation-ship in text.
successful solutions can help im-prove researcher efﬁciency in search and re-view.
in this paper, we operationalize this taskby using citing sentences as a proxy.
we es-tablish a large dataset for our task.
we pre-train a large language model to serve as thefoundation for autoregressive approaches tothe task.
we explore the impact of taking dif-ferent views on the two documents, includingthe use of dense representations extracted withscientiﬁc information extraction systems.
weprovide extensive automatic and human evalu-ations which show the promise of such models,and make clear the challenges for future work..1.introduction.
the output of the world’s scientists doubles roughlyevery nine years (bornmann and mutz, 2015).
con-sequently, researchers must devote signiﬁcant en-ergy to quickly understand how a new piece ofresearch ﬁts with a rapidly changing research land-scape..several lines of research seek to reduce this bur-den on scientists.
citation recommendation sys-tems suggest references to relevant published work(mcnee et al., 2002; bhagavatula et al., 2018).
in-tent classiﬁcation systems help determine the typeand importance of a citation in a work (valenzuelaet al., 2015; cohan et al., 2019).
summarizationsystems aim to help researchers more quickly un-derstand the basic ideas in a piece of research (co-han and goharian, 2015; yasunaga et al., 2019).
we draw inspiration from these works as well as.
∗equal contribution..broader challenges like explaining the connectionbetween concurrent works or relating a new paperto those a reader is already familiar with..automatically describing inter-document rela-tionships could decrease the time researchers de-vote to literature review.
for instance, explanationsfor a new paper can be personalized to a particu-lar reader by relating the new work to ones theyhave read before.
further, such technology couldbe incorporated into writing assistance systems tohelp less experienced or non-native writers betterarticulate the connection between their work andprior art.
additionally, users of citation recommen-dation systems can beneﬁt from natural languageexplanations of recommendation system choices..in addition to the utility of this task to scientists,it presents several interesting technical challenges.
these include effectively representing the impor-tant information in a document, generating froma long-tailed technical vocabulary, and expressingthe variety of connections between related scien-tiﬁc papers.
figure 1 illustrates how the same docu-ment is described differently in relation to differentdocuments..in this paper we use citing sentences to oper-ationalize the problem of generating natural lan-guage explanations of the relationships betweentwo scientiﬁc papers.
authors, when citing otherwork, oftentimes describe how their work relates tothe cited work.
to this end, we use in-text citationsentences as a naturally occurring proxy explana-tions for how two documents relate to each other.
however, we generate such sentences from generalrepresentations of document content rather than thespeciﬁc in-text locations where these sentences oc-cur, as this task formulation can better facilitate theapplications described above..we approximate the explanation objective byhaving a gpt2 language model generate sentencescontaining citations given a pair of documents..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2130–2144august1–6,2021.©2021associationforcomputationallinguistics2130document or a span of text (mcnee et al., 2002;nallapati et al., 2008; bhagavatula et al., 2018).
recently, researchers have sought to categorize ci-tations using various ontologies of citation intents.
teufel et al.
(2006) develop an annotation schemeand corresponding classiﬁcation model for citationfunctions.
valenzuela et al.
(2015) seek to discern“highly inﬂuential” citations from others.
jurgenset al.
(2018) use six categories including “moti-vation,” “uses,” and “future work” among others.
cohan et al.
(2019) condense this ontology to justthree: “background,” “method,” and “result com-parison.” intent classiﬁcation can identify relation-ships between documents; our relationship expla-nation task extends this in two ways.
first, data-driven freeform generation can express a widerarray of relationships compared to a manually-deﬁned label set.
further, our task framework couldbe used to describe relationships between workswhich do not actually cite each other, such as con-temporaneous works.
unlike categorization tech-niques, we require no task-speciﬁc annotated dataas we supervise with citing sentences that are read-ily available in scientiﬁc documents.
in practice,citation classiﬁcation is used to assist in suggest-ing relevant works to researchers; our work com-plements this goal by providing rationales for therecommendation and furthering progress towardexplainable ai..our work is also connected to a long historyof research on summarizing scientiﬁc documents(luhn, 1958; paice, 1980).
work in this area hasmostly used used abstracts or peer reviews as tar-gets (cachola et al., 2020; cohan et al., 2018;jaidka et al., 2017).
in particular, pilault et al.
(2020) show that using a simple extractive sum-mary as input for abstractive summarization ofscholarly texts work well.
researchers have alsoused citing sentences as part of the input for sum-marization, recognizing the explanatory power ofthese texts (nakov et al., 2004; cohan and gohar-ian, 2017; yasunaga et al., 2019).
ours is the ﬁrstwork to focus on learning to express the speciﬁcrelationship between two documents from such sen-tences..the closest work to our own is xing et al.
(2020),who pilot a task of in-line citation generation.
theirgoal is a model which can insert a citing sentenceinto a particular context within a document.
ourwork, on the other hand, aims to learn from cit-ing sentences how to describe general relationships.
figure 1: given two scientiﬁc documents, the goal is towrite the sentence describing the speciﬁc relationshipbetween them.
for a given document (in blue above),the output will vary depending the content of the other.
(this image is best viewed in color.).
this approach relies on providing dense but in-formative representations of documents to use asconditioning context for the generation model.
weexplore the use of sentence-based contexts as inputincluding document abstracts, introductions, andsampled sentences from the full document; we ﬁndthat using introductions and abstracts works well.
finally, we improve our model’s performance onautomated metrics by using informative entitiesand terms to both construct dense input and rankthe output relationship explanations..in addition to standard automatic metrics, weperform human evaluations of technical outputswith a pool of annotators.
in this work, we describea series of stages of model development, each withits own experiments that, together, informed thetask and our series of solutions..our contributions include: a novel dataset forthe relationship explanation task; a domain-adaptedgpt2 we release for left-to-right language mod-eling of scientiﬁc text; the scigen model for de-scribing document relationships; and an extensiveexpert evaluation and analysis of machine gener-ated technical text.1.
2 related work.
the current work builds on recent research in scien-tiﬁc document understanding, including citationrecommendation, intent categorization, and sci-entiﬁc document summarization.
citation recom-mendation systems suggest related works given a.
1https://github.com/kel-lu/scigen.
2131“bluebuilds on the method of red…”“greencollect 500k instances for training … ”“earlier techniques include yellow…”input documents+++explain relationshipbetween documents independent of particular in-document contexts.
while the xing et al.
(2020)method may facilitate writing assistance, our taskhas applications in search and summarization.
be-cause our task does not rely on a speciﬁc locationin a document where the citation will go, solutionscan be used at scale to provide users with generalexplanations of document relationships..our models rely heavily on recent advances intransfer learning in nlp.
large pretrained mod-els such as bert (devlin et al., 2018) and gpt2(radford et al., 2019) have made strong advanceson a number of tasks (wang et al., 2019).
it hasalso been shown that pretraining these models ondomain-speciﬁc data further improves results ondomain-speciﬁc tasks (beltagy et al., 2019; leeet al., 2019).
in this work, we apply that method-ology by adding a pretraining phase on in-domaindata before ﬁnetuning a gpt2 model toward theexplanation generation task.
a key challenge whenusing pretrained language models for document-level tasks is how to select document content to ﬁtwithin the limited context window of the model,which is a major focus of our work..3 task overview.
we aim to generate an explanation: a natural lan-guage sentence which expresses how one documentrelates to another.
explicit examples of such sen-tences are nontrivial to ﬁnd in corpora, especiallywhen annotation for a highly technical task is ex-pensive.
to this end, we use in-text citations ina scientiﬁc document to prior work as proxies forrelationship explanations.
we use these citing sen-tences as partial supervision for our task, and referto them as “explanations.”2.
we distinguish one document as the principaldocument, from which we will draw explanationsthat reference the cited document.
let t denote anexplanation drawn from principal document s, ands(cid:48) denote s without t. then let.
p (t | s(cid:48), c).
(1).
be the probability of t given s(cid:48) and the cited doc-ument c. a good generation technique shouldmaximize this probability across a large number of(cid:104)t, s, c(cid:105) triples, so that at inference time the modelis able to generate a sentence t∗ which accurately.
2future work might seek to ﬁlter or systematically alter in-text citations to be more explanation-like, without otherwisechanging our approach..documentstokensunique tokensexplanations.
total154k813m7.1m622k.
average/doc.
–5.3k1.3k4.0.table 1: dataset statistics, total and per document..describes the relationship between new documentsˆs and ˆc..optimizing equation 1 is made easier by modernrepresentation learning.
pretrained neural languagemodels like gpt2 have shown strong performancewhen generating sentences conditioned on a con-text.
however, existing implementations of gpt2limit the context window to 512 or 1024 tokens, farsmaller than scientiﬁc documents.
in this work, weexplore ways to represent the documents’ contentfor use with language models..data we use english-language computer sciencearticles and annotation from s2orc dataset (loet al., 2020).
s2orc is a large citation graphwhich includes full texts of 8.1 million scientiﬁcdocuments.
we use 154k connected computer sci-ence articles, from which we extract 622k expla-nations with a single reference that link back toother documents in our corpus.
we omit any sen-tences that cite more than one reference.
we hold5000 sentences for each of the validation and testsets.
detailed statistics can be found in table 1.information on dataset construction can be foundin appendix b..evaluation the most appropriate evaluation met-ric for this and many text generation tasks is humanjudgment by potential users of the system.
eval-uating explanations of the relationships betweenscientiﬁc documents requires human judges withscientiﬁc expertise whose time and effort can becostly.
while collecting human judgments in tech-nical domains is relatively rare, we believe it to bean important step in evaluating our systems for thistask.
thus, we conduct thorough human evalua-tions and analyses with expert judges.
we make useof both larger scale expert evaluations yielding hun-dreds of judgements as well as smaller scale, deeperevaluations where we can effect a higher degreeof quality control over fewer datapoints.
further,we make use of intermediate human evaluations inthe development of our models, and supplementthese evaluations with automatic metrics — bleu.
2132(papineni et al., 2002) and rouge (lin, 2004) thatare established in other generation tasks..4 models.
we develop several models for explaining docu-ment relationships.
following current work in neu-ral text generation, we ﬁnetune the predictions of alarge pretrained language model to our task (sec-tion 4.1).
in order to bring the language model intothe scientiﬁc text domain, we do additional lan-guage model pretraining over full scientiﬁc texts.
we also investigate approximate nearest neighbormethods to retrieve plausible human-authored ex-planations from the training data as a baseline (sec-tion 4.2)..4.1 neural text generation.
recent work has shown that ﬁnetuning large pre-trained language models to text generation tasksyields strong results (zellers et al., 2019).
to thisend, we construct scigen, a model based on gpt2(radford et al., 2019), a transformer model trainedon 40gb of internet text with a left-to-right lan-guage modeling objective (vaswani et al., 2017).
we do so by ﬁnetuning the predictions of the lan-guage model to generate explanations using differ-ent expressions of the principal and cited documentas context..to ﬁnetune gpt2 architectures for text gener-ation, it is typical to concatenate the condition-ing context x = x1 .
.
.
xn and target sentencey = y1 .
.
.
ym with a special separator token ξy.
to adapt this technique to our task, we construct theconditioning context x from the principal and citeddocuments and use the explanation as y .
we takej tokens from principal document s1, .
.
.
, sj alongwith k tokens from the cited document c1, .
.
.
, ck(which tokens to draw from the two documentsis an independent variable that we explore exper-imentally).
we then condition the generation ofexplanation y on x = s1, .
.
.
, sj, ξx, c1, .
.
.
, ck,where ξx is a token used to indicate the end of theprincipal document.
scigen is trained to predictthe explanation one token at a time as describedabove.
more details on training can be found inappendix a..at inference time, the model is provided withan unseen principal/cited document pair.
an ex-planation of their relationship is generated onetoken at a time using nucleus sampling (holtz-man et al., 2020).
at timestep t, output token ˆyt.
figure 2: overview of the construction of scigen.
wetake the pretrained gpt2 and continue pretraining onscientiﬁc texts.
we then ﬁnetune using data in table 1..is sampled from the top 90% of the distributionp (ˆyt | x, ξy, ˆy1, .
.
.
, ˆyt−1) (renormalized).
theselected ˆyt is used to condition the prediction ofsubsequent tokens..context the primary question we investigatewith the scigen model is what kind of input isbest for describing the relationship between theprincipal and cited documents accurately and in-formatively.
since models based on gpt2 havea small context window relative to the length ofscientiﬁc documents, we investigate the use of ab-stracts, introductions, or non-citing sentences sam-pled from throughout the document as conditioningcontext.
the effectiveness and description of theseapproaches is described in section 5. based onour ﬁndings with sentence-based contexts and in-formation retrieval systems, we then explore thepossibility of representing the cited document textas a list of important concepts rather than ﬂuenttext, in section 6..language model pretraining prior work hasshown that pretraining on in-domain data improvesthe performance of large language models ondomain-speciﬁc tasks (beltagy et al., 2019; gu-rurangan et al., 2020).
inspired by this, we con-tinue pretraining the gpt2 model in the sciencedomain to produce scigpt2, which we use asthe underlying language model for scigen de-scribed above.
scigpt2 starts from the standardpretrained gpt2-base model and is trained for anadditional 75k gradient updates at batch size of64 (effectively a single epoch over 4.8 million ab-stracts and body paragraphs) with a language mod-eling objective.
figure 2 illustrates the process..we observed signiﬁcant improvements in thequality of scigen outputs after replacing the un-derlying gpt2 language model with the domain-speciﬁc scigpt2 model.
we saw a perplexityimprovement in a held-out set and, in informal in-spections, qualitative improvements as well..when using pretrained language models, textfrom task-speciﬁc test data cannot be guaranteed.
2133web textgpt2pretrainscientiﬁc textscigpt2cont.
pretrainexplanation datascigenfinetuningto be absent from the large task-independent cor-pora upon which these models are trained, whichmay improve model performance compared tomodels without this exposure.
for the experi-ments described in this work, we train a versionof scigpt2 only on documents appearing in thetraining data, so that the principal documents andtarget sentences in the test data are unseen by thelanguage model.
we provide this and a full-corpusversion of scigpt2 as resources for future re-search.3.
4.2 retrieval with approximate nearest.
neighbors.
while neural text generation techniques have ad-vanced signiﬁcantly in recent years, their outputsare still inferior to human authored texts.
forsome tasks, it is better to retrieve a relevant human-authored text than to generate novel text automati-cally (fan et al., 2018).
is this also the case whengenerating explanations?.
to answer this question, we use an informationretrieval (ir) baseline.
we adapt an approximatenearest neighbor search algorithm to ﬁnd similarpairs of documents.
the basic search procedure isas follows: given a test instance input (s, c) forprincipal s and cited document c, we ﬁnd the setnc, the nearest neighbors to c in the training data.
for each document nc from nc, let ns be the setof documents that cite nc.
this means that eachns ∈ ns contains at least one citing sentence t(cid:48)which cites nc.
we use the t(cid:48) associated with the(ns, nc) pair from the training which is closest to(s, c) as the explanation of their relationship..we measure the closeness of two pairs of doc-uments using the cosine distances between vectorrepresentations of their abstracts.
the abstract ofeach document is encoded as a single dense vec-tor by averaging the contextualized embeddingsprovided by the scibert model of beltagy et al.
(2019) and normalizing.
the distance between(s, c) and neighbors (ns, nc) is computed as:.
α cos(s, ns) + β cos(c, nc).
(2).
where α and β control the relative contribution ofthe two document similarities.
we explore settingboth α and β to 1, or tuning them to optimizebleu on the validation data using mert (och,2003)..5 representing documents with.
sentence selection.
methods for the related task of citation recom-mendation have made use of abstracts, which per-haps act as sufﬁcient summaries of document con-tent.
building on this, we represent the principaland cited documents with the ﬁrst 450 tokens ofeither their abstracts, introductions, or sentencesrandomly sampled from throughout the full docu-ment.4 in this section, we answer two questions: 1)do neural generation models with sentence-basedcontext outperform the ir baseline and 2) does thetype of sentence-based context (abstract, introduc-tion, sampled) matter?
we answer these questionsby performing both automatic and human evalua-tions..5.1 automatic evaluation.
we compare the scigen and ir systems usingbleu (papineni et al., 2002) and rouge (specif-ically l; lin, 2004).
the “sentence-based” rowsof table 3 show the test set performance of their system and the best scigen models when pro-vided with the different sentence-based input con-text combinations.5 we assesss statistical signiﬁ-cance as well by bootstrapping with 1000 samplesin each of 100 iterations.
we ﬁnd that context doesmake a difference for scigen, and that a slight butstatistically signiﬁcant performance improvementcomes from using the introduction of the princi-pal document rather than the abstract.6 we do not,however, ﬁnd enough evidence to reject the nullhypothesis that any particular representation of thecited document’s content (abstract, intro, or ran-dom sample) is sufﬁcient..we ﬁnd that using the introduction of the princi-pal document paired with the abstract of the citeddocument performs best, and so we select these forhuman evaluation.
the ir systems perform well,obtaining slightly better scores in some settings.
we choose the mert-optimized version for humanevaluation..4we exclude any sentence with a citation from being sam-pled in all conditions.
this context type is also only used forthe cited document and not the principal document..5the performance of our best scigen models can befound in table 3 and the automatic test set evaluations of allsystems can be found in appendix f..3https://github.com/kel-lu/scigen.
6p < 0.01 after bonferroni correction..2134scigenirgoldagreement.
speciﬁc correct s&c agr70.577.583.8.
64.046.372.171.4.
55.040.068.063.1.
72.374.881.469.8.table 2: human evaluation of scigen (intro × abs)and ir (abs × abs) systems compared with gold ex-planations in percent.
s&c represents those that wereboth speciﬁc and correct.
all differences signiﬁcant atp < 0.01 except scigen vs. ir speciﬁc..5.2 human evaluation.
we conduct a human evaluation to determine, givena particular pair of principal and cited abstracts,how correct and speciﬁc the generated explanationof their relationship is.
by “correct” we mean:does the explanation correctly express the factualrelationship between the principal and cited doc-uments?
because generic explanations such as“this work extends the ideas of chomsky and halle(1968)”, while possibly factual, do not expressa detailed understanding of the documents’ rela-tionship, we ask judges whether the explanationdescribes a speciﬁc relationship between the twoworks.
an explanation can be speciﬁc even it isincorrect..we compare the principal intro × cited abs sci-gen setting against the tuned ir system.
for cali-bration, we also elicit judgments for the gold expla-nations extracted from principal documents alongwith the correct principal and cited abstracts.
in allthree cases, we ensure that the principal documentappeared in the acl anthology to ensure annotatorexpertise.
in total we solicit 37 nlp researchersand collect over 800 judgments, with over 100 foreach system/quality dimension combination..further details of our evaluation can be found inappendix d. we perform error analysis on thesejudgments as well as an additional study to validatehuman judgments; these are detailed in appendix eand appendix g. table 2 shows the percentage of“yes” judgments versus the total of “yes” and “no”judgements for each system/quality combination,along with pairwise agreement rates.7 gold textsreceived the highest scores for all dimensions oftext quality from the evaluators as well as the high-.
7that gold texts do not achieve perfect scores demonstratesa limitation of our evaluation setup, due in part to the factthat judgments are based on document abstracts rather thantheir full texts.
we take steps to resolve this limitation in oursubsequent analysis in section 6.2..est agreement rate.
we can also see that ir systemstend to produce incorrect explanations more oftenthan not..the scigen system performs quite well in thisanalysis, with a majority of outputs deemed cor-rect.
we observe a larger difference in speciﬁcitybetween scigen and gold texts, indicating thatscigen, like many neural text generation systems,often generates vague and generic sentences.
thesegenerations tended to be vacuous such as “(cited)this work is an extension of the paper.” speciﬁcityis key for future downstream applications such asautomated literature review and will need to beimproved for those tasks..6 using ie-extracted term lists.
compared to the gold explanations, we foundthat our generated explanations miss importantphrases such as unique model or dataset namesand other lower-frequency terms; generally, theylacked speciﬁcity.
the missing phrases typicallyappear in the cited document after the abstract andintroduction.8 na¨ıvely sampling from the full textdoes not capture them due to sparsity..to address this issue, we explore more sophis-ticated information extraction (ie) techniques forconstructing the conditioning context for scigen.
recent work has shown that pretrained languagemodels can adapt to disﬂuent inputs such as lin-earized trees and graphs (ribeiro et al., 2020).
in-spired by this, we investigate whether we can uselists of salient words and phrases to effect a denserepresentation of the cited document in the condi-tioning context.
speciﬁcally, we construct a list ofdocument-speciﬁc terms using tf-idf to score uni-grams and entities extracted with a state-of-the-artscientiﬁc ner system.
the paradigm is illustratedin figure 3..tf-idf tf-idf is a measure of the frequency of aterm in a document, normalized by the documentfrequency of that term.
in our use, we calculate thetf-idf score for each unigram in the cited document.
we keep the 100 highest scoring terms wi sortedin descending order of scores.
the terms of thislist are concatenated with a special token ξtf tosignal that this part of the input is structured as alist rather than conventional text.
the resulting con-text x tf = w1, ξtf , w2, ξtf , ..., ξtf , w100 is used torepresent the cited document to the scigen model..8a quantitative analysis of this phenomenon is available.
in appendix h..2135figure 3: overview of scigen using terms/entities.
we generate a list of candidates and rank them according tomean reciprocal rank to the input entities..entities we extract entities from abstracts withthe dygie++ information extraction framework(wadden et al., 2019) using the model trained onscierc (luan et al., 2018), a dataset of scientiﬁcdocument abstracts with entity and relation anno-tations.9 the extracted entities ei from the citeddocument are sorted by their tf-idf scores comparedto all entities in the corpus.
as above, a specialtoken ξe is used to concatenate entities and help thelanguage model distinguish this list from conven-tional text.
if there is additional room in the contextwindow we append the unigrams with the highest tf-idf to the end of the listed entities until the windowis full.
in that case, the cited document context x eis e1, ξe, e2, ..., ξe, en, ξtf , w1ξtf , ..., wm, where nis the number of entities and m is 100 − n..6.1 entity-based ranking.
maynez et al.
(2020) point out that summariza-tion systems frequently struggle with factuality andgenerate hallucinations unfaithful to input docu-ments.
we observe this problem with some gener-ated explanations as well: popular, topical termslike ‘cnn’ would appear in explanations of papersusing lstm models, for example.
to combat hallu-cinations and promote factual accuracy we includea ranking mechanism that rewards generated expla-nations with higher coverage of important entitiesfrom the conditioning context.10.
the process we use is as follows: ﬁrst, we gen-erate a large space of candidate explanations fora given input document pair from scigen vianucleus sampling.
we then extract the entitiesfrom each candidate using the dygie++ ie system.
where possible, we match entities from the can-didates with the entities extracted from the citeddocument.
to account for textual variation betweenthe explanations and the input documents, we usea similarity threshold to make soft alignments.11.
we then select the candidate that has the highestmean reciprocal rank of matched entities againstthe input as the explanation for this document pair..6.2 manual analysis.
we conducted a manual correctness analysis of thegenerated explanations from a sentence-based (in-tro × abs) and ie-based (intro × tﬁdf generate andrank) model.
two of the authors judged 50 data-points from each system using a similar setup tothat described in section 5.2, but with the singleobjective of judging correctness on a 3-way scale:correct; too vague (but not incorrect); and incor-rect.
additionally, the authors made use of the fulltext of the input documents to make decisions forcases where not enough information is availablein the abstract.
this resulted in a more accuratethough much more time-consuming evaluation pro-cess compared to the previous evaluation.
afterjudging all datapoints independently, the two au-thors discussed disagreements until a consensuswas reached..the results of this analysis are shown in table 4.we see a slight increase in correctness with theie-based model compared to the sentence-basedmodel, though the difference is small..6.3 automatic evaluation.
the “ie-based” rows of table 3 show the resultsof automatic metrics for the systems described inthis section.
we ﬁnd that these metrics improvesigniﬁcantly in the settings where the principal doc-ument is represented by its introduction and thecited document is represented either as a list ofterms or entities, with a slight advantage for enti-ties.
the models conditioned on intro × tﬁdf con-text outperform all other sentence-based, retrieval,and ie-based models..7 discussion.
9we found relation annotations to be noisy on inspection.
10an oracle ranking is shown in appendix i.
11we use difﬂib and a 0.7 similarity threshold for matching..example system outputs for selected test datapointsare shown in table 5. the ﬁrst example illustrates.
2136principalcitedintroentity/termsscigen input intro x entities/termsscigencandidate 1candidate 2candidate 20candidate 2rank  &  selectsentence-based.
ie-based.
method.
context.
scigen.
ir.
scigen.
+ranking.
principal abs × cited absprincipal intro × cited absprincipal intro × cited introprincipal intro × cited sampledsource abs × cited abs.
+mert.
principal intro × cited tﬁdfprincipal intro × cited entitiesprincipal intro × cited tﬁdfprincipal intro × cited entities.
bleu acl-bleu rouge-l10.409.8211.229.9210.549.8010.319.8110.509.9310.2910.2316.7513.1713.4213.4115.1013.5014.4713.16.
8.48.78.88.79.79.812.011.812.311.8.table 3: automatic test set evaluation of generated texts for a subset of our systems.
acl-bleu denotes thebleu scores of the subset of examples we use for human evaluation (see section 5.2).
the full results can befound in appendix f..correct vague.
sentence-basedie-based.
1113.
76.incorrect3231.table 4: results of manual analysis.
a case where the model identiﬁes a correct relation-ship between the two documents.
in this instance,they both use the pinyin representation for chinesecharacters in their transliteration models..output 2 demonstrates a failure of the explana-tion generation system.
the principal documentdeals with the topic of discourse relations, the au-tomatic identiﬁcation of which is a long-standingmachine learning task.
however, this particulardocument is an analysis paper, and does not in-volve any training..output 3 is an example of a “too vague (but notincorrect)” case from the analysis in section 6.2.here again the explanation generated by scigen istopical, dealing with the concept of “distant super-vision” that is key to both input documents.
how-ever, this sentence fails to capture the speciﬁc usethat the principal makes of the research describedin cited document..the ﬁnal example, output 4, showcases potentialfor our system to explain concurrent work.
thegenerated text summarizes the cited and impliesthat principal will build on that work.
however,selected papers are both concurrent generation pa-pers published in the same venue and do not citeeach other.
this appears to be a weakness in us-ing citation sentences as proxies for relationshipexplanations.
citations of contemporaneous workoccur less frequently, so these types of sentencesappear less often in training.
similarly, relationshipexplanations between papers with more distant con-nections (e.g., “multi-hop” in the citation graph).
principal:.
1 cited:.
scigen:.
principal:.
2 cited:.
scigen:.
principal:.
3 cited:.
scigen:.
principal:.
4 cited:.
scigen:.
a syllable-based name transliterationsystema joint source-channel model for ma-chine transliterationfollowing cited , chinese characters areconsidered as pinyin sequence..recovering discourse relations: varyinginﬂuence of discourse adverbialsthe beneﬁts of a model of annotationthe two text collections provided bycited were used for training, and theother two text collections were used forevaluation..coreference resolution for swedish andgerman using distant supervisioncollective cross-document relation ex-traction without labelled datait is one of the most widely used distantsupervision techniques and is inspired bytechniques proposed by cited..neural text generation in stories usingentity representations as contextdelete, retrieve, generate: a simple ap-proach to sentiment and style transferthe authors of cited proposed a modelthat combines neural generation withuser interaction to create an object-centric reading experience..table 5: example explanations.
the given texts are thedocument titles and the scigen outputs.
in the lastexample, the two documents do not cite each other..are missing in our training data..in addition to missing some relationships, notall citation sentences are useful as explanations.
as pointed out by other work, citation sentencescan often be simple summaries of the cited work(qazvinian and radev, 2008; cohan and goharian,2017).
alternatively, they can be too speciﬁc to beuseful, as seen in output 1, where a higher-levelsummary might be more useful.
future work couldfocus on curating better training sets for our task.
it is notable that the scigen model usually out-.
2137puts syntactically correct and topical explanations,even given the difﬁculty of the vocabulary in thisdomain.
this is consistent with many recent ﬁnd-ings using domain-speciﬁc language models..the ﬂuency and appropriateness of scigen’sgenerations shows the promise of generating ex-planations which accurately capture the relation-ship between two documents.
based on the resultsobtained here, we expect pretrained scientiﬁc lan-guage models to persist as a foundation.
futurework should focus on two complementary goals:ensuring the factual accuracy of the generated textand improved modeling of the cited document.
fac-tual accuracy is difﬁcult to enforce in languagemodel-based text generation systems, especiallywhere inference includes sampling procedures.
theuse of information extraction for contexts showedpromise in section 6; other methods of incorporat-ing information like grounding to knowledge basescould help prune false or irrelevant statements..combining knowledge graphs with languagemodels and generation is an active research areathat has shown promise in other domains (bosselutet al., 2019; koncel-kedziorski et al., 2019; peterset al., 2019).
applying this line of work to scientiﬁctext by modeling input documents as knowledgegraphs of their content may help algorithms betterunderstand the cited document, provide distant su-pervision for concurrent work, and result in betteroutputs..8 conclusion.
we have described a task of explaining the rela-tionship between two scientiﬁc texts and its con-nections to facilitating researcher productivity.
weemploy a large, publicly available dataset of sci-entiﬁc documents to train a domain-adapted left-to-right language model for use in text generationapplications and beyond.
we explore a collectionof techniques for representing document contentincluding using abstracts, introductions, sampledsentences, and lists of informative terms and enti-ties.
we conduct thorough human and automaticevaluations to determine the relative strengths ofeach representation for expressing document rela-tionships in natural language text..acknowledgements this research was sup-ported in part by the ofﬁce of naval researchunder the muri grant n00014-18-1-2670.
wethank the members of noah smith’s (ark), hannahajishirzi’s (h2lab), and luke zettlemoyer’s re-.
search groups for their participation in our study.
we thank members of noah’s ark for their helpfulcomments and the anonymous reviewers for theirfeedback..ethical considerations the authors received anirb approval for this human annotation project.
the project was classiﬁed as “no-risk.” all partici-pants in the study were volunteers and gave explicit,informed consent for the study..references.
iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: pretrained language model for scientiﬁc text.
inemnlp..chandra bhagavatula, sergey feldman, russell power,and waleed ammar.
2018. content-based citationrecommendation.
in naacl-hlt..lutz bornmann and r¨udiger mutz.
2015. growth ratesof modern science: a bibliometric analysis basedon the number of publications and cited references.
jasist..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli c¸ elikyilmaz, and yejin choi.
2019. comet: commonsense transformers for auto-matic knowledge graph construction.
in acl..isabel cachola, kyle lo, arman cohan, and daniel s.weld.
2020. tldr: extreme summarization of scien-tiﬁc documents.
in emnlp..arman cohan, waleed ammar, madeleine van zuylen,and field cady.
2019. structural scaffolds for cita-tion intent classiﬁcation in scientiﬁc publications.
innaacl-hlt..arman cohan, franck dernoncourt, doo soon kim,trung bui, seokhwan kim, walter chang, and nazligoharian.
2018. a discourse-aware attention modelfor abstractive summarization of long documents.
innaacl-hlt..arman cohan and nazli goharian.
2015. scientiﬁc ar-ticle summarization using citation-context and arti-cle’s discourse structure.
in emnlp..arman cohan and nazli goharian.
2017. contextualiz-ing citations for scientiﬁc summarization using wordembeddings and domain knowledge.
in sigir..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl-hlt..angela fan, mike lewis, and yann dauphin.
2018. hi-.
erarchical neural story generation.
in acl..2138suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
inacl..ari holtzman, jan buys, m. forbes, and yejin choi.
2020. the curious case of neural text degeneration.
in iclr..kokil jaidka, muthu kumar chandrasekaran, devan-shu jain, and min-yen kan. 2017. the cl-scisummshared task 2017: results and key insights.
inbirndl@sigir..david jurgens, srijan kumar, raine hoover, dan mc-farland, and dan jurafsky.
2018. measuring theevolution of a scientiﬁc ﬁeld through citation frames.
tacl..rik koncel-kedziorski, dhanush bekal, yi luan,mirella lapata, and hannaneh hajishirzi.
2019.text generation from knowledge graphs with graphtransformers.
in naacl..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2019. biobert: a pre-trainedbiomedicalforbiomedical text mining.
bioinformatics..language representation model.
chin-yew lin.
2004. rouge: a package for automatic.
evaluation of summaries.
in acl..kyle lo, lucy lu wang, mark neumann, rodney kin-ney, and daniel s. weld.
2020. s2orc: the seman-tic scholar open research corpus.
in proceedingsof acl..yi luan, luheng he, mari ostendorf, and hannanehhajishirzi.
2018. multi-task identiﬁcation of enti-ties, relations, and coreference for scientiﬁc knowl-edge graph construction.
in acl..hans peter luhn.
1958. the automatic creation of lit-erature abstracts.
ibm journal of research and de-velopment..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factual-ity in abstractive summarization.
in acl..sean m. mcnee, istvan albert, dan cosley, pra-teep gopalkrishnan, shyong k. lam, al mamunurrashid, joseph a. konstan, and john riedl.
2002.on the recommending of citations for research pa-pers.
in cscw..preslav i nakov, ariel s schwartz, and marti hearst.
2004. citances: citation sentences for semanticanalysis of bioscience text.
in sigir..franz josef och.
2003. minimum error rate training in.
statistical machine translation.
in acl..chris d. paice.
1980. the automatic generation of lit-erature abstracts: an approach based on the identiﬁ-cation of self-indicating phrases.
in sigir..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl..matthew e. peters, mark neumann, robert logan, royschwartz, vidur joshi, sameer singh, and noah a.smith.
2019. knowledge enhanced contextual wordrepresentations.
in emnlp..jonathan pilault, raymond li, sandeep subramanian,and christopher pal.
2020. on extractive and ab-stractive neural document summarization with trans-former language models.
in emnlp..vahed qazvinian and dragomir r. radev.
2008. sci-entiﬁc paper summarization using citation summarynetworks.
in proceedings of the 22nd internationalconference on computational linguistics (coling2008), pages 689–696, manchester, uk.
coling2008 organizing committee..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8)..leonardo fr ribeiro, martin schmitt, hinrich sch¨utze,and iryna gurevych.
2020. investigating pretrainedlanguage models for graph-to-text generation.
arxivpreprint arxiv:2007.08426..simone teufel, advaith siddharthan, and dan tidhar.
2006. automatic classiﬁcation of citation function.
in emnlp..marco valenzuela, vu a. ha, and oren etzioni.
2015.in aaai work-.
identifying meaningful citations.
shop: scholarly big data..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in neurips..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in emnlp..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r. bowman.
2019. superglue:a multi-task benchmark and analysis platform fornatural language understanding.
in neurips..ramesh nallapati, amr ahmed, eric p. xing, andwilliam w. cohen.
2008. joint latent topic modelsfor text and citations.
in kdd..xinyu xing, xiaosheng fan, and xiaojun wan.
2020.automatic generation of citation texts in scholarlypapers: a pilot study.
in acl..2139michihiro yasunaga, jungo kasai, rui zhang, alexan-der richard fabbri, irene li, dan friedman, anddragomir r. radev.
2019. scisummnet: a large an-notated corpus and content-impact models for scien-tiﬁc paper summarization with citation networks.
inaaai..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019. defending against neural fakenews.
in neurips..2140a training details.
we perform task-adaptive (continued) pretrain-ing and then ﬁnetuning on the task to constructscigpt2 and scigen respectively.
scigpt2starts from the standard pretrained gpt2-basemodel and is trained for an additional 75k stepsat batch size of 64 (effectively a single epoch over4.8 million abstracts and body paragraphs) witha language modeling objective.
we then ﬁnetunescigpt2 to build scigen for various contexts.
for all variants, we ﬁnetune the underlying lan-guage model for an additional 10 epochs, or ap-proximately 100k steps with batch size of 64.12.the hyper-parameters are in table 6. we pro-vide code for training and evaluating our modelas well.13 our code is based on huggingface’simplementation of gpt2-small (117m parameters).
we trained on ec2 p3.8x machines which had 4nvidia tesla v100 gpus each.
both models took24 hours to ﬁnish training..the only hyperparameter we tune is the learningrate.
we compared 1e-4 and 6e-5 for our learn-ing rates and used validation perplexity for modelselection..hyperparameterepochseffective batch sizelearning rateweight decaywarmup proportion.
pretrain1641e-40.000.05.finetune10641e-40.050.10.table 6: hyperparameters for the further pretrainingand ﬁnetuning..b dataset construction.
we use data from s2orc14 in both the additionalpretraining and ﬁnetuning.
in the former case, weuse s2orc’s text with a mask over all citationreferences.
for ﬁnetuning, we speciﬁcally train onprocessed data..we process our data by extracting principal con-text, cited context, and target sentence triplets.
foreach principal document, we extract (1) the cita-tion sentences, (2) the principal document context,and (3) the citation’s cited document context.
wetruncate the citation sentence to 100 tokens and thecontexts to 450 tokens.
any remaining space is.
12we use a triangular learning rate schedule with 10%.
warmup and a maximum learning rate of 0.0001..13https://github.com/kel-lu/scigen14https://github.com/allenai/s2orc.
padded with a special token.
the two contexts andthe target citation sentence are then concatenatedtogether with special separator tokens..figure 4 depicts our dataset construction.
toconstruct the data splits, we randomly select 500principal documents for both test and validationsets.
the citation sentences that occur in these prin-cipal documents are used as examples in the test(5310 examples) and validation (5164 examples)sets.
of the remaining examples where the prin-cipal documents were not in evaluation sets, wethrow out any citation sentences that use an evalua-tion document as the cited document.
the resultantexamples are used for training (609509 examples).
this construction allows us to ensure that the citeddocument is unseen at test time.15.
c examples.
see table 7..d human evaluation details.
to ensure no annotator sees the output of morethan one system on each datapoint, we randomlyselect 50 datapoints for each system (principal in-tro × cited abs, ir, and gold explanations) fromthe subset of our test data whose principal docu-ments appear in the acl anthology.
we collectjudgments from 37 nlp researchers with varyinglevels of expertise, the majority of whom are grad-uate students.
each judge is given 15 datapointsfor each of the speciﬁcity and correctness qualities.
judges are shown a table of datapoints asked tomark whether each meets (“yes”) or fails to meet(“no”) the condition.
judges are permitted to label“?” or skip examples they feel uncertain about orunqualiﬁed to judge, which we ignore..e validity of human judgments.
to test the validity of the human judgments in sec-tion 5.2, we conduct an additional human evalu-ation of gold explanations paired with differentkinds of mismatched inputs: (1) the correct princi-pal document and a random cited document, (2) thecorrect cited document but a random principal doc-ument (3) random principal and cited documentsselected from acl anthology.
conditions 1 and 2allow us to see whether human judges accept sen-tences which align with only one or the other ofthe input documents; condition 3 provides a lower.
15we also include code for data processing and splits in our.
repository; see footnote 13..2141principalmachine translation is important for eliminating language barriers in everyday life.
to train systems which can produce goodquality translations large parallel corpora are needed.
mining parallel sentences from various sources in order to train betterperforming mt systems is essential, especially for low resource languages.
.
.
..citedsimilarity search ﬁnds application in specialized database systems handling complex data such as images or videos, which aretypically represented by high-dimensional features and require speciﬁc indexing structures.
this paper tackles the problem ofbetter utilizing gpus for this task.
.
.
..ie-based representation of citedit (cid:104)|ent|(cid:105) lopq (cid:104)|ent|(cid:105) opq (cid:104)|ent|(cid:105) bucket selection (cid:104)|ent|(cid:105) gpu heap implementation .
.
.
(cid:104)|tfidf|(cid:105) quantizer (cid:104)|tfidf|(cid:105) memory (cid:104)|tfidf|(cid:105) gemm (cid:104)|tfidf|(cid:105) lane (cid:104)|tfidf|(cid:105) warp (cid:104)|tfidf|(cid:105) k-nn .
.
..sentence-based scigenfor this purpose, we followed the formulation of the greedy algorithm from (cited) for comparing similarity lists obtainedfrom n-best lists.
ie-based scigenin line with previous work (cited), we use a hash-based distance measure to calculate the similarity..citing sentencewe calculate sentence similarities of each possible pairs which can be done efﬁciently even for large inputs (cited).
principalwith the development of wireless technologies and popularization of smart phones, mobile trafﬁc grown 4000-fold over thepast 10 years and is expected to continue its growth at a compound annual growth rate of 53 percent from 2015 to 2020(cited).
the resulting problem of energy consumption on the information and communications technology (ict) hasbecome a serious issue.
.
.
..citedwe consider the problem of minimization of sum transmission energy in cellular networks where coupling occurs betweencells due to mutual interference.
the coupling relation is characterized by the signal-to-interference-and-noise-ratio (sinr)coupling model.
both cell load and transmission power, where cell load measures the average level of resource usage in thecell, interact via the coupling model.
.
.
..ie-based representation of citednon-linear power coupling equation -lrb- npce -rrb- (cid:104)|ent|(cid:105) non-linear load coupling equation -lrb- nlce -rrb- nlce (cid:104)|ent|(cid:105) average level of usage .
.
.
(cid:104)|tfidf|(cid:105) base (cid:104)|tfidf|(cid:105) r (cid:104)|tfidf|(cid:105) iap (cid:104)|tfidf|(cid:105) load (cid:104)|tfidf|(cid:105) cellular .
.
..sentence-based scigenbased on the hybrid beamforming design, the authors of (cited) studied the joint beamforming and power control of massivemimo cellular networks, and proposed a multi-stage energy-efﬁcient and fair power allocation algorithm in an ranarchitecture..ie-based scigenin (cited), the load-coupled problem was addressed and the authors derived the optimal power allocation policy for theworst-case load constrained system considering the two forms of load arrival and power consumption..citing sentenceproof: the proof is similar to that of theorem 1 in (cited).
principalour lives are increasingly reliant on multimodal conversations with others.
we email for business and personal purposes,attend meetings in person, chat online, and participate in blog or forum discussions.
while this growing amount of personaland public conversations represent a valuable source of information, going through such overwhelming amount of data, tosatisfy a particular information need, often leads to an information overload problem(cited).
.
.
..citedthe increasing complexity of summarization systems makes it difﬁcult to analyze exactly which modules make a differencein performance.
we carried out a principled comparison between the two most commonly used schemes for assigningimportance to words in the context of query focused multi-document summarization: raw frequency (word probability) andlog-likelihood ratio.
.
.
..ie-based representation of citedraw frequency (cid:104)|ent|(cid:105) log-likelihood weighting scheme (cid:104)|ent|(cid:105) log-likelihood ratio weighting (cid:104)|ent|(cid:105) focused summarizer.
.
.
(cid:104)|tfidf|(cid:105) 0.12717.these (cid:104)|tfidf|(cid:105) topicfocused (cid:104)|tfidf|(cid:105) summaries (cid:104)|tfidf|(cid:105) generic .
.
..sentence-based scigenfor generic single-query summaries, these measures often give better performance than the traditional measures (cited).
.
.
..ie-based scigenthis score is the same as that used by (cited) to generate query-focused summaries from document classiﬁcation..citing sentencein this work, we use log-likelihood ratio to extract the signature terms from chat logs, since log-likelihood ratio leads to betterresults(cited)..table 7: examples of system inputs and outputs from test set..2142figure 4: dataset construction from the cs subset of s2orc.
for the far right image, we the documents withcheckmarks represent the principal and those with a pencil represent the cited..correct.
random citedrandom principalboth random.
45.846.917.6.table 8: correctness judgements of incorrect citing sen-tences (percentages)..bound.
we collect 107 human evaluations of cor-rectness across these conditions, again allowingannotators to skip datapoints they are unsure of.
the results, shown in table 8, indicate that humanjudges will sometimes accept a explanationas longas one of the principal or cited documents is correct,but at a lower rate than seen in table 2 when bothdocuments are correct.
we note that both papers inthe mismatched cases are drawn from the acl an-thology, meaning there is some amount of topicalcoherence in their pairing.
there is no indicationfrom this experiment that either the principal orcited document is a stronger inﬂuence on a judge’scorrectness decision, although a larger sample sizemight make a clear determination..f further detail on automated metrics.
results.
we provide a more detailed report of performanceon the automated metrics in table 9 which includesall of our models.
the no principal × cited absmodel uses no information from the principal tomake its retrieval decision, demonstrating the im-portance of relational information..g error analysis.
we investigate the reasons why gold explanationsare marked incorrect in our ﬁrst human evaluationin section 5. one hypothesis for this gap couldbe that the grammar of the sentences inﬂuencedhuman judgment.
to test this claim, one authorannotated each gold example for grammatical cor-rectness and veriﬁed these annotations with a com-.
figure 5: upper and lower bounds of bleu for differ-ent choices of α..mercial writing assistant system.16 we ﬁnd thatgold explanations with errors are more likely to beclassiﬁed as incorrect (41.1%) than those withouterrors (25.4%).
these results may partially explainwhy evaluators rated a portion of the gold sentencesas incorrect..h analysis of token-wise overlap.
to get a straightforward idea of how much informa-tion useful for the relationship sentence is providedby each type of context representation, we calcu-late the averaged percentage of token-wise overlapof the input and the gold relationship sentence, asshown in table 10. while a larger overlap doesnot guarantee a better performance, we found thebest performing scigen systems, with or with-out ranking, among those using context showinglargest overlaps with gold sentences..i oracle study.
we conduct an oracle study to see the potential ofranking.
figure 5 shows the upper bound and lowerbound of the bleu score if we independently gen-erate α samples for each pair of (s, c) using sci-gen and optimally choose the one with the highestbleu and the one with the lowest.
with α = 20,an ideal ranking system could result in a bleuscore as high as 19.50. that provides evidence that.
16https://www.grammarly.com.
2143s2orc (cs)holdout docs145k+ documentstest 500 docsvalidation 500 docs render one keyframe for every ten frames for  dataset, which yields about 200,000 training samples.in  experiments,  use , which contains…… employ the generalized  to train the segmentation network…train (600k)valid (5k)test (5k)sentence-based.
ie-based.
method.
context.
principal abs × cited absprincipal abs × cited introprincipal abs × cited sampleprincipal intro × cited absprincipal intro × cited introprincipal intro × cited sampledprincipal abs × cited abs+ mert (bleu)no principal × cited absprincipal intro × cited tﬁdfprincipal abs × cited entitiesprincipal intro × cited entitiesprincipal intro × cited tﬁdfprincipal abs × cited entitiesprincipal intro × cited entities.
bleu rouge-1 rouge-2 rouge-l8.48.48.58.78.88.79.79.89.612.011.411.812.311.611.8.
9.829.399.609.929.809.819.9310.239.7913.1713.1013.4113.5013.2813.16.
10.710.710.711.111.110.914.214.314.115.014.314.715.514.715.0.
0.60.60.71.01.10.90.70.70.61.30.81.41.61.01.3.scigen.
retrieval.
scigen.
+ranking.
table 9: automatic evaluation of generated texts for all of our systems..noneprincipal abs xprincipal intro x.none cited abs cited intro cited tﬁdf cited entitiesn/a22.2832.61.
18.7432.2741.15.
22.0335.3543.24.
22.1635.4343.32.
22.9535.6942.81.table 10: token-wise overlap with gold relationship sentence..generate-and-rank systems have the potential to sur-pass generate-only systems regarding bleu.
ourproposed ranking mechanism manages to achievehigher bleu in some cases, though it performsfar below an ideal ranker.
that suggests potentialfuture work on further improvements of ranking..j auto-completion.
we notice the diversity of expressing the relation-ship even between the same pair of principal andcited documents.
we test whether our scigencould capture such diversity if provided with dif-ferent triggers.
our experiment shows that, if weprovide the ﬁrst three words of the relationship sen-tence to scigen and ask it to generate the restof the sentence, the bleu score of the generatedpart could be boosted to 21.38. that suggests ause case of scigen, where a more personalizedrelationship sentence could be generated given thebeginning of the sentence..k explaining without citations.
one direction of future work is the ability to pro-vide natural language relationship explanations topairs of papers without a direct citation link.
ta-ble 11 gives examples of scigen output for con-current papers at naacl 2018. none of thesepapers cited each other, and thus there was no su-pervision in generating the explanations..principal:.
cited:.
scigen:.
principal:.
cited:.
scigen:.
principal:.
cited:.
scigen:.
learning joint semantic parsers from dis-joint dataa transition-based algorithm for unre-stricted amr parsingfor principal’s task, principal will annotateeach graph g with semantic roles from a setof annotations m, that are ”required” in accor-dance with the amr graph grammar cited..looking beyond the surface: a challenge setfor reading comprehension over multiplesentencesclicr: a dataset of clinical case reportsfor machine reading comprehensionfor all of these datasets, principal focuses onperformance using a common reading com-prehension metric, like f1-score cited..attentive interaction model: modelingchanges in view in argumentationexploring the role of prior beliefs for argu-ment persuasioncited the primary strategy of this dataset isto focus on the important messages, which areimportant to people with different viewpoints..table 11: example relationship explanations for pairsof papers that appeared in the same track at naacl2018. these papers did not cite each other.
these ex-amples have some post-processing done that replacesﬁrst person pronouns with “principal”..2144