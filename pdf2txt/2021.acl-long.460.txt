controversy and conformity: from generalized to personalizedaggressiveness detectionkamil kanclerz1, alicja figas2, marcin gruza1, tomasz kajdanowicz1,jan koco ´n1, daria puchalska2, przemysław kazienko1wrocław university of science and technology27 wybrze˙ze wyspia´nskiego st.wrocław, poland1{kamil.kanclerz,marcin.gruza,tomasz.kajdanowicz,jan.kocon,kazienko}@pwr.edu.pl2{238442,234800}@student.pwr.edu.pl.
abstract.
there is content such as hate speech, offen-sive,toxic or aggressive documents, whichare perceived differently by their consumers.
they are commonly identiﬁed using classiﬁerssolely based on textual content that general-ize pre-agreed meanings of difﬁcult problems.
such models provide the same results for eachuser, which leads to high misclassiﬁcation rateobservable especially for contentious, aggres-sive documents.
both document controversyand user nonconformity require new solutions.
therefore, we propose novel personalized ap-proaches that respectindividual beliefs ex-pressed by either user conformity-based mea-sures or various embeddings of their previoustext annotations.
we found that only a fewannotations of most controversial documentsare enough for all our personalization meth-ods to signiﬁcantly outperform classic, gener-alized solutions.
the more controversial thecontent, the greater the gain.
the personalizedsolutions may be used to efﬁciently ﬁlter un-wanted aggressive content in the way adjustedto a given person..1.introduction.
unfortunately, in the pursuit of knowledge on theinternet, one may come across content that theyconsider inappropriate for various reasons, such asbeing too aggressive.
many users notoriously comeacross content that offends them while surﬁng theinternet.
this can cause discomfort and discouragefrom further expansion of knowledge.
to avoid this,it is important to effectively ﬁlter out content that agiven user may ﬁnd unwanted.
this poses a risk oferroneous assessment of whether a given text is con-sidered inappropriate by a given person.
for thatpurpose, we need to extend commonly applied gen-eralizing solutions and develop personalized meth-ods that take into account beliefs and preferencesof the individual user.
we expect this information.
can be obtained from the individual’s prior opin-ions about the offensiveness of some texts.
then,it is crucial to select the relevant texts that allowderiving as much information about users prefer-ences as possible.
our new idea is to use someknown, most controversial texts whose offensive-ness is very ambiguous and depends more on sub-jective personal judgment.
we examined how manydocuments has to be annotated by a given user toencapsulate their beliefs sufﬁciently and to improvepersonalized reasoning.
independently, we consid-ered personal measures quantifying conformity ofeach individual.
in other words, we measured towhat extent a person evaluates documents simi-larly to others, i.e.
"is a part of the mainstream".
the conformity measures are used as input fea-tures for the classiﬁer.
this way, it is possible toﬁnd out the user beliefs based on their opinionsregarding a relatively small number of texts.
in thispaper, we present novel methods of personalizedaggressive content detection based on the represen-tation of user opinion about aggressive texts.
wepropose: (1) conformity-based personalization, (2)class-based embeddings, and (3) annotation-basedembeddings (sec.
6).
our experiments were per-formed on the only relevant dataset wikipedia talklabels: aggression (sec.
3).
having deﬁned andcalculated controversy of documents and confor-mity of users (sec.
4), we validated our methods.
the results revealed that additional individualizedfeatures: simple user conformity measures com-puted on few texts or embeddings of even fourcontroversial texts signiﬁcantly boost our person-alized classiﬁcation (sec.
8).
the gain providedby our personalized methods is greater for morecontroversial documents.
this work is based on theresults obtained in the article (koco´n et al., 2021).
in addition, in paper (milkowski et al., 2021), weshowed that the personalized approach is also effec-tive for other subjective problems in nlp, such as.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5915–5926august1–6,2021.©2021associationforcomputationallinguistics5915recognizing emotions elicited by text.
the sourcecode we used to conduct experiments and evalua-tion is publicly available in clarin-pl githubrepository1..2 related work.
it is observable a steady increase in the number ofoffensive (levmore and nussbaum, 2010), hate(breckheimer, 2001; brown, 2018), aggressive,toxic, cyberbullying (chen et al., 2012), or simplysocially unacceptable online messages (ljubeši´cet al., 2019).
there are many deﬁnitions of offen-sive speech, which can be summarised as speechthat targets speciﬁc social groups in a way thatis harmful to them (jacobs, 2002).
some coun-tries, such as the usa, protect the rights to usethis type of speech as an acceptable form of po-litical expression (heyman, 2008).
in turn, thelaw prohibits hate speech in many eu countries(rosenfeld, 2002).
such laws pose a challenge foroperators of social networking sites and other on-line services to identify and moderate unacceptablecontent.
large companies such as facebook andgoogle are often accused of not doing enough to en-sure that their platforms are not used to attack otherpeople (ben-david and fernández, 2016).
on theother hand, attempts to automatically control con-tent often lead to the accidental blocking of contentthat was not intended to offend anyone..ambiguity of the deﬁnition of offensiveness isa serious problem.
this inconsistency is visible inmany reviews related to automatic detection of hatespeech (fortuna and nunes, 2018; schmidt andwiegand, 2017; alrehili, 2019; poletto et al., 2020)or more speciﬁcally on aggressiveness detection(sadiq et al., 2021; modha et al., 2020)..automatic recognition of offensive speech isthe subject of many nlp workshops, such as se-meval 2019 (zampieri et al., 2019b), germeval2018 (wiegand et al., 2018), fire/hasoc 2019(mandl et al., 2019) or poleval 2019 (ptaszy´nskiet al., 2019).
classic methods do not consider con-text and word order, e.g.
the bag-of-words model(zhang et al., 2010) or tf-idf (sahlgren et al.,2018).
the representation may be extended withadditional ontologies (bloehdorn and hotho, 2004)or wordnets (scott and matwin, 1998; piaseckiet al., 2009; misiaszek et al., 2014; janz et al., 2017;koco´n et al., 2019b) and used with svm (razavi.
1https://github.com/clarin-pl/.
controversy-conformity.
et al., 2010) or logistic regression models (waseemand hovy, 2016; sahlgren et al., 2018; koco´n et al.,2018; koco´n and maziarz, 2021).
new methodsoften use word embeddings (wiegand et al., 2018;bojanowski et al., 2017; łukasz augustyniak et al.,2021) (wiegand et al., 2018; bojanowski et al.,2017) mixed with character embeddings (augusty-niak et al., 2019), together with deep neural net-works, e.g.
cnn (zampieri et al., 2019a) or lstm(yenala et al., 2017).
the current state-of-the-artare transformer-based architectures such as bert(devlin et al., 2019), albert (lan et al., 2019),xlnet (yang et al., 2019) or roberta (liu et al.,2019).
nevertheless all these methods focus solelyon the text itself.
any wider context has been con-sidered very rarely, e.g.
as time, thread or author’ssocial network features (ziems et al., 2020)..in articles focused on detection of aggressive-ness (modha et al., 2018; risch and krestel, 2018;saﬁ samghabadi et al., 2020), the most often usedwere datasets shared at the workshops on trolling,aggression and cyberbullying (trac) (kumaret al., 2018, 2020) at lrec.
few others also usedthe wikipedia talk labels: aggression (wulczynet al., 2017b), where all individual annotations areavailable, not just the majority vote.
unfortunately,we have not found any other aggression dataset, forwhich this information would also be given.
more-over the authors focus mainly on the multilingualaspect of the aggression detection (modha et al.,2018; risch and krestel, 2018; saﬁ samghabadiet al., 2020).
in addition to deep neural models, lesscomplex methods such as logistic regression arealso used (modha et al., 2018; risch and krestel,2018)..to the best of our knowledge, there are no workthat dealt with the subjective problem of aggres-siveness detection in the personalized way.
thedisagreement between annotators is usually mea-sured by a single value, e.g.
using cohen’s kappaor krippendorf’s alpha, and not investigated fur-ther.
the researchers prefer a higher agreementlevel rather than controversy.
therefore, major-ity annotation is used in modeling, which to someextent leads to the loss of valuable information..there are several studies focusing on the prob-lem of the disagreement in data annotations.
thisprovides valuable information not only about theannotators, but also about the instances by re-ﬂecting their ambiguity (aroyo and welty, 2013).
there may be no single right label for every text..5916the disagreement was used to divide annotatorsinto polarized groups (akhtar et al., 2020) or toﬁlter out the spammers (raykar and yu, 2012;soberón et al., 2013).
in (gao et al., 2019), at-tention was also drawn to the problem of confor-mity bias, where the reviewers tend to issue similaropinions.
less frequently, the disagreement is ex-amined at the instance level, to measure its contro-versy or ambiguity, as in (aroyo and welty, 2013).
for example, (chklovski and mihalcea, 2003) usedconfusion matrices in word sense tagging task tocreate and explore coarse sense clusters..3 dataset: wikipedia talk labels.
we used the wikipedia talk labels: aggressiondata, gathered in the wikipedia detox project (wul-czyn et al., 2017b,a).
unlike other collections, itprovides information about all annotations given bycrowdﬂower workers (not only the majority vote)for 100k+ comments from english wikipedia.
theassigned aggression score ranged from very aggres-sive (-3), via neutral (0), to very friendly (3).
it wasbinarized to ’1 - aggressive’ for negative scores or’0 - nonaggressive’ for neutral or friendly annota-tions.
the dataset contained a suggested data splitinto train, dev and test set..to enable our experiments, we removed anno-tations assigned by workers with less than 100 an-notations in the train set, <20 in the dev set or<20 in the test set.
otherwise, we would not havedata to extract user beliefs from and to performpersonalization.
we also removed users who didnot assign any aggressive label in the dev set.
in-formation about at least one text, that a speciﬁcuser considered aggressive was crucial to modelhis individual perception of such content.
finally,there were 2,450 annotators left (tab.
1), so werandomly divided them into 10 equal-sized folds.
the train set is used to calculate the representa-tions (embeddings) of documents being classiﬁed.
this is the only data exploited in the classic, gen-eralizing approach (our baseline).
the dev set pro-vides information about user beliefs, i.e.
their pre-vious annotations.
individualized input features areextracted from dev data: (1) conformity measuresand (2) personal embeddings in class-based andannotation-based personalization.
personalization-related calculations on the dev set refer to bothtraining and testing procedure.
the documentsfrom the test set are embedded and classiﬁed bythe trained model for the validation purposes..figure 1: split of texts and users into train and test set.
the dev texts are solely used to quantify user beliefs:user conformity and personal embeddings.
each cell isa single text (comment) and its individual annotation..4 controversy and conformity measures.
for training and testing purposes, both contro-versy contr for documents and conformity gconf,wconf for users are calculated within the dev set..4.1 controversy.
controversy contr(d) ∈ [0, 1] of document d is anentropy-based measure expressed in the following.
description.
comments.
annotations (ann.).
annotators.
ann.
balance.
ann.
per comment.
ann.
per annotator.
traindevtesttraindevtestwhole setaggressivenonaggr.
meanstd.
dev.
meanstd.
dev.
beforeﬁltering69,52623,16023,178762,046253,589349,582405318.3%81.7%11.784.88336.84296.59.afterﬁltering69,52323,16023,178682,517226,996304,378245018.1%81.9%10.484.18495.47281.43.table 1: wikipedia talk labels dataset statistics..5917way:.
contr(d) =.
(cid:40)0, if n0− (cid:80).
d = nd ∨ n1nclog2dnd.
d = nd(cid:17)(cid:16) ncdnd.
c=0,1.
, otherwise.
d, n1.
where n0d is the number of negative and positiveannotations assigned to document d, respectively;nd is the total number of document d’s annota-tions, nd = n0approximates the proba-bility that annotation of document d is of class c.contr(d) = 0 means that all users annotated d thesame, contr(d) = 1 when 50% of users perceivedit aggressive and 50% not..d + n1d;.
ncdnd.
controversy contr(d) is used to rank docu-ments from the dev dataset.
the most controver-sial texts (top k) are embedded in class-based orannotation-based personalization.
independently,controversy is computed within the test data in or-der to investigate differences in reasoning qualityfor more and less controversial documents..4.2 general conformity.
general conformity gconf (a, c) ∈ [0, 1] of hu-man a quantiﬁes how often a belongs to the major-ity of annotators evaluating individual texts.
it canbe of different kind depending on the class c weconsider:.
gconf (a, c) =.
(cid:80).
d∈aa(cid:80).
1{ld∈c ∧ ld=ld,a}1{ld∈c}.
d∈aa.
,.
where aa is the set of documents annotated by a;c denotes the conformity type related to the consid-ered classes, i.e.
c = {0}, {1} or {0, 1}; ld,a is theclass label assigned by a to document d; ld is thed’s class label obtained by majority voting.
in caseof equal annotations for both classes document d isconsidered aggressive.
gconf (a, c) = 1 when aannotated all documents d ∈ aa the same like theothers and no one annotated it otherwise..note that depending on c, conformity can becalculated in three variants:for nonaggressive(c = {0}), aggressive (c = {1}) or any docu-ments (c = {0, 1}) annotated by a. such threeconformity values are used as input features inconformity-based personalization, sec.
7..4.3 weighted conformity.
weighted conformity w conf (a, c) ∈ [0, 1] issimilar to general conformity gconf (a, c) but itrespects the size of the group the annotator belongs.
to, while evaluating the document.
the larger thegroup with annotator a, the greater annotator aconformity:.
w conf (a, c) =.
(cid:80).
(cid:80).
d∈a(cid:80).
c∈c.
1{ld,a=c}.
ncdnd1{ld,a∈c}.
..d∈aa.
5 controversy analysis.
to have some insightinto our data, we cal-culated controversy contr(d) on each dataset(train/dev/test).
fig.
2 presents the distributionof annotations for controversy measure in the devand test set.
in both, the ratio of aggressive tononaggressive documents is increasing and reach-ing 0.5 for the most controversial documents, i.e.
contr(d) = 1 resulting from the same number ofaggressive and nonaggressive votes.
the examplesof such texts are following:.
"your behaviour is inappropriate and your reac-tion is ludicrous.
do they give out admin rights incornﬂake packets now?
", n0.
d = n1.
d = 5.."far from being ridiculous, it is the recom-mended approach to follow on wikipedia.
we don’tsimply state what either side claims, rather we re-port on how they are viewed by neutral 3rd partysources.
take it to wp:npovn if you don’t believeme, rather than indulging in your continued disrup-tive habit of always having the wp:lastword.
",d = n1n0.
d = 14..figure 2: distribution of controversy in documents cal-culated on a) the dev set, b) the test set.
we learned that classic methods based solely oncontent analysis (not personalized) perform worse,the more controversial the documents being tested,fig.
6. it was the main inspiration for our personal-ized methods..we also checked contribution of aggressive textsfor the consecutive most controversial documentsincluded in the personal user embeddings, fig.
3..5918the percentage of users who rated d as nonaggres-sive, (4) the rating of the given user (0/1), and (5)the information on whether this rating is consistentwith the the majority rating.
thus, we receive a rela-tively large number of input features: 300+k ∗304.
our general personalized aggressiveness detec-.
tion procedure is as follows:.
1. we ask users to annotate k most controversialdocuments from the pre-deﬁned set (here dev)..2. information from the ﬁrst step is used to ex-tract individually-speciﬁc features reﬂectingpersonal user beliefs, i.e.
conformity mea-sures or embeddings of these k texts (class-based and annotation-based methods)..3. a subset of the same users (upper rows in fig.
1) annotate next documents.
the data abouttheir following annotations (embeddings oftexts from train) together with data from step2. are used to train the classiﬁer..4. for some other users (lower rows in fig.
1),we also collect their annotations (the test set).
together with the information about their indi-vidual preferences (step 2.)
they are used forvalidation (testing) purposes only..7 experimental setup.
to validate our three personalized methods, we uti-lized wikipedia talk labels: aggression, see sec.
3. we applied 10-fold cross-validation based onusers.
the ﬁrst nine sets are used to train the model(upper rows in fig.
1), while the remaining 10thset for testing (lower rows in fig.
1).
the resultspresented in plots are averaged over all ten folds..since only dev texts with annotations are as-sumed to represent prior knowledge about users,they were used to test personalization scenar-ios for each of our three methods: class-based,annotation-based, and conformity-based.
the lastone was in three variants: only three gconf (a, c)measures (for c = {0}, {1}, {0, 1}), only threew conf (a, c) measures, all six conformity val-ues.
thus, we analyzed ﬁve methods in total.
for each of them, we considered: (1) differentnumber k=1,2,..20 of texts d previously annotatedby user a: d ∈ aa (for conformity-based methods|aa| = k, (2) different selection procedures fortexts d ∈ aa used to represent a’s beliefs (person-alization): (2a) k most controversial texts d ∈ aa,.
figure 3: contribution of aggressive texts in the follow-ing positions of the individual ranking of most contro-versial documents annotated by a given user..6 methods for personalizedaggressiveness detection.
we assume that personal beliefs can be expressedby user activity, i.e.
their individual annotations.
it means that we can use information about k docu-ments previously annotated by the user in the formof their embeddings or user conformity measures.
it leads us to three novel personalization meth-ods: (1) conformity-based, (2) text-based, and (3)annotation-based, fig.
4. according to our initialstudies, the most informative were user annotationsprovided for most controversial documents..in conformity-based personalization, we ex-ploited simple conformity measures that representthe beliefs of one user in the aggregated way:gconf and wconf.
each of them can deliver threeseparate values: for only aggressive, only nonag-gressive, and all texts.
finally, we examined inputfeature sets based on only gconf, only wconf, andon both, sec.
7..we also propose two versions of personal embed-dings for previously annotated texts: class-basedand annotation-based..the class-based embedding consists of two fast-text embeddings of k documents from the dev setthat the user rated as (1) nonaggressive and (2)separately as aggressive, fig.
4. each of the twoembeddings can aggregate any and different num-ber of previous user annotations; the embeddingsize is static for every k. if the user has not an-notated any texts of given class (e.g.
aggressive),the embedding represents an empty string (zeros).
overall, it is a very rare case in our experiments,mostly happening for k = 1..the annotation-based embeddings consider all kuser annotations individually.
for each such text d,we use the following features: (1) the embeddingof the d’s content, (2) its controversy contr(d), (3).
5919figure 4: a classic approach generalizing output based solely on textual content (the same decision for all users) –an upper ﬂow (our baseline).
three personalized methods proposed in the paper: (1) conformity-based – additionalinput features – personal conformity measures (gconf, wconf or both, each for aggressive, nonaggressive or anytexts); (2) class-based – two embeddings of k = 4 texts previously annotated by a given user, one embedding forone aggressive text and the second for three nonaggressive ones; (3) annotation-based – embeddings, classes andadditional features for each of k = 4 most controversial texts previously annotated by a given user..(2b) k class-balanced most controversial (like 2abut with class balancing), (2c) most aggressived ∈ aa (rank according to % of aggressive an-notations among all for d), (2d) random selectionof k texts d ∈ aa.
in total, we tested: 10 folds x(5 methods x 20 distinct k no.
of texts x 4 selection+ 1 baseline) = 4,010 models..the logistic regression models were optimizedduring the training process by using the l2 regu-larization and the early stopping mechanism.
bothof them aim to prevent overﬁtting and the earlystopping mechanism additionally ensures that themodel instance that achieved the best loss functionscore is preserved.
the models were run on intelxeon processor e5-2650 v4..we also compared our personalized methodswith the baseline, i.e.
the commonly investigatedapproach generalizing user perception.
it exploitedonly the evaluated text embeddings as the input..we considered classiﬁcation performance notonly for the whole test set but also in its break-down of 10 percentage buckets according to threeindependent rankings of test docs: (1) most con-troversial (contr(d)), (2) with least conformitygconf (a, {0, 1}), averaged over all a ∈ t est an-.
notating d, (3) least w conf (a, {0, 1}).
here, themeasures were computed for the test set only, notfor dev.
it was used to investigate where our modelsmore outperform the baseline.
in order to generatetext embeddings in each personalization method,we used the fasttext library (bojanowski et al.,2017; joulin et al., 2017).
it offers pre-trained wordvectors for 157 languages, based on the continuousbag of words (cbow) model in a 300-dimensionalspace, with character n-grams of length 5..8 validation of personalization methods.
both class-based and annotation-based methodswere tested using various rankings while selectingtexts for personal embeddings: most controversial,class-balanced most controversial, most aggressive,and random.
the conformity-based methods wereevaluated in terms of the measure variant used:general conformity, weighted conformity, and both,all with random selection of texts..8.1 conformity-based personalization.
the results for three conformity-based personal-ization methods, i.e.
three different sets of inputconformity features (sec.
7) and various number k.5920figure 5: performance of three personalized methods proposed in the paper, only for the aggression class:(a) conformity-based, in inset we inserted evaluation results for both classes; (b) class-based; (c) annotation-based;(d) comparison of the best method of each type.
both (b) and (c) were evaluated using various rankings whileselecting texts for personal embeddings: most controversial, class-balanced most controversial, most aggressive,and random.
macro f1 score for both classes have the same shapes by with different range for y: 0.68–0.73..of texts used to calculate user conformity are shownin fig.
5a.
the greater k results in more preciseevaluation of user conformity.
it also directly andpositively impacts on model performance, althoughgains for k > 15 are very small..additionally, we considered the performancefor more and less controversial documents in thetest set, fig.
6a.
it is clearly visible that the non-personalized method is completely lost for the mostcontroversial documents.
however, our conformity-based models lose relatively less.
it appears thattheir gain (smaller loss) is greater for 30% mostcontroversial texts.
in other words, the greater con-troversy, the greater gain from personalization..8.2 class-based embeddings.
fig.
5b describes evaluation of class-based em-beddings for various text selection approaches anddifferent number of previously annotated texts.
the performance was shown only for texts fromthe aggression class (the same plot shapes were formacro f1 and both classes).
the models using themost controversial texts for selection reached thebest results in 14 out of 20 cases (70%).
the high-est f1 score was achieved for only 4 texts repre-senting user beliefs.
it was greater than the model.
without any personalization by over 7pp..8.3 annotation-based embeddings.
annotation-based embeddings were tested for thesame rankings as in sec.
8.2, fig.
5c.
the mostcontroversial texts used to generate user representa-tions and feed the model provided the best resultsin 17 out of 20 cases (85%).
the best performancewas achieved while using 18 texts to represent userpersonal beliefs – then, the input consisted of 5,772features.
the f1 score of this model was greaterthan the baseline by over 10pp..the greater gain compared to the not personal-ized method is exposed for 50% of the most con-troversial texts in the test set; the greatest for 10%of the most controversial – even 22.7 percentagepoints (twice better: 44.0% vs. 21.3%), fig.
6b..8.4 comparison of personalization methods.
the best models from each personalization method,which were achieved for annotations of most con-troversial texts, are compared in fig.
5d.
modelsbased on annotation-based embeddings providedsigniﬁcantly better results than the others in 10out of 20 cases of k values (50%).
the conformity-based models performed better than other models in.
5921figure 6: performance of two personalized methods proposed in the paper, only for the aggression class: (a)conformity-based; (b) annotation-based.
both were evaluated on documents d in the test set, sorted in ascendingorder by contr(d) measure, 0-10 denotes 10% of the most controversial texts..3 out of 20 cases (15%); it referred to the smallestnumber of texts considered (k = 1 ÷ 3).
the high-est value of f1 score was achieved by the modelusing 18 texts to represent user personal beliefs.
however, this solution used 5,772 input features,whereas the much simpler conformity-based modelwith 306 input features was only 2.7 percentagepoints worse.
simultaneously, conformity-basedmodel training time was 38.6 times faster than theannotation-based one, fig.
7..practically, we would like to avoid bothering theuser with too many previous annotations, i.e.
wemay want to limit k to just a few, for example k = 4.then, we should select k most controversial textsand use either class-based or conformity-based per-sonalization.
they learn just as fast but keep thesame performance: 7.3 percentage points, 5.7 per-centage points greater f1 for class aggressive, re-spectively, and 3.9 percentage points, 3.2 percent-age points greater macro f1 (for both classes), re-spectively..the worst performance was observed for mod-els using class-based embeddings.
the results ofevaluation on all texts are presented in fig.
5d..random selection of k texts for personalizationis almost always worse than dedicated rankings,fig.
6b,c.
most controversial texts turned out to bethe best option that usually outperformed the mostaggressive and class-balanced most controversial..9 discussion.
a valuable observation from our experiments is thatalready one document used to valuate user beliefsis enough to signiﬁcantly improve reasoning, fig.
5d.
anyway, more texts in personalization keepboosting the performance, but about 4-5 previouslyannotated most controversial documents seem tobe a reasonable trade-off between reasoning quality.
figure 7: training computing time for personalizationmethods in reference to no personalization method..and user annoyance..annotation-based embeddings most preciselyexpress user opinions, but it comes at the cost oflinearly longer learning and demand for more sam-ples.
they also cannot easily adapt to differentnumber k of personalization documents..we decided to utilize very fast logistic regressionmodel with fasttext embeddings, since we wantedto examine thousands of models related to multiplescenarios, not all are presented here..we belief our personalization methods establisha new research direction: how to effectively andefﬁciently embed user beliefs?
we expect newmethods will be developed for that purpose..one of the most important postulate derivedfrom our research is the demand for new datasetscollections.
we need annotations of individual hu-mans rather than aggregated and agreed generalbeliefs received by majority voting, by annotatortraining, or by removal of controversial texts..besides, our personalization methods may beapplied to any nlp problem with inconsistenciesbetween people.
it especially refers to diverse emo-tions evoked by textual content, hate speech, detec-tion of cyberbullying or offensive, toxic, abusive,.
5922harmful, or socially unaccepted content..the common problem of imbalanced classes inaggressiveness detection (tab.
1, fig.
12) will beaddressed in future work..10 conclusions.
the main conclusion from our research is that thenatural controversies associated with individual per-ceptions of contents should not be overlooked orreduced but rather directly exploited in personal-ized solutions.
ultimately, this reﬂects the diversityin our societies..our three new personalization methods makeuse of texts previously annotated by a given userby means of conformity measures, class-based orannotation-based embeddings.
just a few docu-ments are able to capture individual user beliefs,the more so, the more controversial documents theyrelate to.
as a result, all our methods outperformclassic solutions that generalize offensiveness un-derstanding.
the gain is greater for more contro-versial documents..the personalization solutions can also be appliedto other nlp problems, where the content tends tobe subjectively perceived as hate speech, cyberbul-lying, abusive or offensive, as well as in predictionof emotions elicited by text (koco´n et al., 2019a;milkowski et al., 2021) and even in sentiment anal-ysis (koco´n et al., 2019; kanclerz et al., 2020)..we keep working on testing of our methods onmore resource-demanding but also more sota lan-guage representations: xlnet (yang et al., 2019),roberta (liu et al., 2019), and xlm-roberta(conneau et al., 2020)..acknowledgments.
this work was ﬁnanced by (1) the national sciencecentre, poland, project no.
2020/37/b/st6/03806;(2) the polish ministry of education and science,clarin-pl project; (3) the european regionaldevelopment fund as a part of the 2014-2020smart growth operational programme, clarin- common language resources and technol-ogy infrastructure, project no.
poir.04.02.00-00c002/19..references.
sohail akhtar, valerio basile, and viviana patti.
2020.modeling annotator perspective and polarized opin-ions to improve hate speech detection.
in proceed-.
ings of the eighth aaai conference on human com-putation and crowdsourcing, pages 151–154..a. alrehili.
2019. automatic hate speech detection onin 2019 ieee/acssocial media: a brief survey.
16th international conference on computer systemsand applications (aiccsa), pages 1–6..lora aroyo and chris welty.
2013. harnessing dis-agreement in crowdsourcing a relation extractiongold standard.
technical report, technical report..łukasz augustyniak, tomasz kajdanowicz, and prze-mysław kazienko.
2021. comprehensive analy-sis of aspect term extraction methods using varioustext embeddings.
computer speech & language,69:101217..łukasz augustyniak, tomasz kajdanowicz, and prze-mysław kazienko.
2019. aspect detection usingword and char embeddings with (bi) lstm and crf.
in 2019 ieee second international conference onartiﬁcial intelligence and knowledge engineering(aike), pages 43–50.
ieee..anat ben-david and ariadna matamoros fernández.
2016. hate speech and covert discrimination onsocial media: monitoring the facebook pages ofinterna-extreme-right political parties in spain.
tional journal of communication, 10:27..stephan bloehdorn and andreas hotho.
2004. textclassiﬁcation by boosting weak learners based onin fourth ieee internationalterms and concepts.
conference on data mining (icdm’04), pages 331–334. ieee..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..peter j breckheimer.
2001. a haven for hate: the for-eign and domestic implications of protecting inter-net hate speech under the ﬁrst amendment.
s. cal.
l.
rev., 75:1493..alexander brown.
2018. what is so special about on-line (as compared to ofﬂine) hate speech?
ethnici-ties, 18(3):297–326..ying chen, yilu zhou, sencun zhu, and heng xu.
2012. detecting offensive language in social mediain 2012 inter-to protect adolescent online safety.
national conference on privacy, security, risk andtrust and 2012 international confernece on socialcomputing, pages 71–80..timothy chklovski and rada mihalcea.
2003. exploit-ing agreement and disagreement of human annota-tors for word sense disambiguation.
in in proceed-ings of ranlp 2003..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, francisco.
5923guzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..p. fortuna and s. nunes.
2018. a survey on automaticdetection of hate speech in text.
acm computingsurveys (csur), 51:1 – 30..yang gao, steffen eger,.
irynagurevych, and yusuke miyao.
2019. does my rebut-tal matter?
insights from a major nlp conference.
corr, abs/1903.11367..ilia kuznetsov,.
steven j heyman.
2008. hate speech, public discourse,and the ﬁrst amendment.
oxford university press,forthcoming..james b jacobs.
2002. hate crime: criminal law andidentity politics: author’s summary.
theoreticalcriminology, 6(4):481–484..arkadiusz janz, jan koco´n, maciej piasecki, andza´sko-zieli´nska monika.
2017. plwordnet as a ba-sis for large emotive lexicons of polish.
in ltc’178th language and technology conference, pozna´n,poland.
fundacja uniwersytetu im.
adama mick-iewicza w poznaniu..armand joulin, édouard grave, piotr bojanowski, andtomáš mikolov.
2017. bag of tricks for efﬁcient textin proceedings of the 15th confer-classiﬁcation.
ence of the european chapter of the association forcomputational linguistics: volume 2, short papers,pages 427–431..kamil kanclerz, piotr miłkowski, and jan koco´n.
2020. cross-lingual deep neural transfer learningin sentiment analysis.
procedia computer science,176:128–137..jan koco´n, arkadiusz janz, and maciej piasecki.
2018.classiﬁer-based polarity propagation in a wordnet.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018)..jan koco´n and marek maziarz.
2021. mapping word-net onto human brain connectome in emotion pro-infor-cessing and semantic similarity recognition.
mation processing & management, 58(3):102530..jan koco´n, piotr miłkowski, and monika za´sko-zieli´nska.
2019. multi-level sentiment analysis ofpolemo 2.0: extended corpus of multi-domain con-sumer reviews.
in proceedings of the 23rd confer-ence on computational natural language learning(conll), pages 980–991..jan koco´n, alicja figas, marcin gruza, dariapuchalska, tomasz kajdanowicz, and przemysławkazienko.
2021. offensive, aggressive, and hatespeech analysis: from data-centric to human-centredapproach.
information processing & management..jan koco´n, arkadiusz janz, piotr miłkowski, monikariegel, małgorzata wierzba, artur marchewka, ag-nieszka czoska, damian grimling, barbara konat,konrad juszczyk, katarzyna klessa, and maciej pi-asecki.
2019a.
recognition of emotions, valenceand arousal in large-scale multi-domain text reviews.
in zygmunt vetulani and patrick paroubek, editors,human language technologies as a challenge forcomputer science and linguistics, pages 274–280.
wydawnictwo nauka i innowacje, pozna´n, poland..jan koco´n, arkadiusz janz, monika riegel, mał-gorzata wierzba, artur marchewka, agnieszkaczoska, damian grimling, barbara konat, konradjuszczyk, katarzyna klessa, and maciej piasecki.
2019b.
propagation of emotions, arousal and po-larity in wordnet using heterogeneous structuredsynset embeddings.
in proceedings of the 10th in-ternational global wordnet conference (gwc’19)..ritesh kumar, atul kr ojha, bornini lahiri, marcoszampieri, shervin malmasi, vanessa murdock, anddaniel kadar, editors.
2020. proceedings of thesecond workshop on trolling, aggression and cy-berbullying.
european language resources associ-ation (elra)..ritesh kumar, atul kr.
ojha, marcos zampieri, andshervin malmasi, editors.
2018. proceedings ofthe first workshop on trolling, aggression and cy-berbullying (trac-2018).
association for computa-tional linguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..saul levmore and martha craven nussbaum.
2010.the offensive internet: speech, privacy, and repu-tation.
harvard university press..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..nikola ljubeši´c, darja fišer, and tomaž erjavec.
2019.the frenk datasets of socially unacceptable dis-course in slovene and english.
in international con-ference on text, speech, and dialogue, pages 103–114. springer..5924thomas mandl, sandip modha, prasenjit majumder,daksh patel, mohana dave, chintak mandlia, andaditya patel.
2019. overview of the hasoc trackat fire 2019: hate speech and offensive contentin pro-identiﬁcation in indo-european languages.
ceedings of the 11th forum for information re-trieval evaluation, fire ’19, page 14–17, newyork, ny, usa.
association for computing machin-ery..piotr milkowski, marcin gruza, kamil kanclerz, prze-myslaw kazienko, damian grimling, and jan ko-co´n.
2021. personal bias in prediction of emotionselicited by textual opinions.
in proceedings of thejoint conference of the 59th annual meeting of theassociation for computational linguistics and the11th international joint conference on natural lan-guage processing (acl-ijcnlp 2021).
associationfor computational linguistics..andrzej misiaszek, przemysław kazienko, marcinkulisiewicz, łukasz augustyniak, włodzimierztuligłowicz, adrian popiel, and tomasz kajdanow-icz.
2014. belief propagation method for word sen-timent in wordnet 3.0. in asian conference on in-telligent information and database systems, pages263–272.
springer..sandip modha, prasenjit majumder, and thomasmandl.
2018. filtering aggression from the multilin-gual social media feed.
in proceedings of the firstworkshop on trolling, aggression and cyberbully-ing (trac-2018), pages 199–207, santa fe, newmexico, usa.
association for computational lin-guistics..sandip modha, prasenjit majumder, thomas mandl,and chintak mandalia.
2020. detecting and visual-izing hate speech in social media: a cyber watchdogfor surveillance.
expert systems with applications,161:113725..maciej piasecki, bernd broda, and stanislaw szpakow-icz.
2009. a wordnet from the ground up.
oﬁcynawydawnicza politechniki wrocławskiej wrocław..fabio poletto, valerio basile, m. sanguinetti, cristinabosco, and v. patti.
2020. resources and bench-mark corpora for hate speech detection: a systematicreview.
in lrec 2020..michal ptaszy´nski, agata pieciukiewicz, and pawełdybala.
2019. results of the poleval 2019 sharedtask 6: first dataset and open shared task forautomatic cyberbullying detection in polish twit-ter.
in proceedings of the poleval 2019 workshop,pages 89–110.
institute of computer science, polishacademy of sciences..vikas c. raykar and shipeng yu.
2012. eliminatingspammers and ranking annotators for crowdsourcedlabeling tasks.
j. mach.
learn.
res., 13(1):491–518..amir h. razavi, diana inkpen, sasha uritsky, andstan matwin.
2010. offensive language detection.
using multi-level classiﬁcation.
in advances in arti-ﬁcial intelligence, pages 16–27, berlin, heidelberg.
springer berlin heidelberg..julian risch and ralf krestel.
2018. aggression identi-ﬁcation using deep learning and data augmentation.
in proceedings of the first workshop on trolling,aggression and cyberbullying (trac-2018), pages150–158, santa fe, new mexico, usa.
associationfor computational linguistics..michel rosenfeld.
2002. hate speech in constitutionaljurisprudence: a comparative analysis.
cardozo l.
rev., 24:1523..saima sadiq, arif mehmood, saleem ullah, maqsoodahmad, gyu sang choi, and byung-won on.
2021.aggression detection through deep neural modelon twitter.
future generation computer systems,114:120 – 129..niloofar saﬁ samghabadi, parth patwa, srinivaspykl, prerana mukherjee, amitava das, andthamar solorio.
2020. aggression and misogynydetection using bert: a multi-task approach.
inproceedings of the second workshop on trolling, ag-gression and cyberbullying, pages 126–131, mar-seille, france.
european language resources asso-ciation (elra)..magnus sahlgren, tim isbister, and fredrik olsson.
2018. learning representations for detecting abu-sive language.
in proceedings of the 2nd workshopon abusive language online (alw2), pages 115–123, brussels, belgium.
association for computa-tional linguistics..anna schmidt and michael wiegand.
2017. a surveyon hate speech detection using natural language pro-in proceedings of the fifth internationalcessing.
workshop on natural language processing for so-cial media, pages 1–10, valencia, spain.
associa-tion for computational linguistics..sam scott and stan matwin.
1998. text classiﬁcationusing wordnet hypernyms.
in usage of wordnet innatural language processing systems..guillermo soberón, lora aroyo, chris welty, oanainel, hui lin, and manfred overmeen.
2013. mea-suring crowd truth: disagreement metrics combinedwith worker behavior ﬁlters.
in proceedings of the1st international conference on crowdsourcing thesemantic web - volume 1030, crowdsem’13, page45–58.
ceur-ws.org..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatespeech detection on twitter.
in proceedings of thenaacl student research workshop, pages 88–93,san diego, california.
association for computa-tional linguistics..michael wiegand, melanie siegel, and josef ruppen-hofer.
2018. overview of the germeval 2018 sharedtask on the identiﬁcation of offensive language.
in.
5925proceedings of germeval 2018, 14th conferenceon natural language processing (konvens 2018),pages 1–10..ellery wulczyn, nithum thain, and lucas dixon.
2017a.
ex machina: personal attacks seen at scale.
in proceedings of the 26th international conferenceon world wide web, pages 1391–1399..ellery wulczyn, nithum thain, and lucas dixon..2017b.
wikipedia talk labels: aggression..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..harish yenala, ashish jhanwar, manoj chinnakotla,and jay goyal.
2017. deep learning for detectinginappropriate content in text.
international journalof data science and analytics..marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019a.
predicting the type and target of offensivein proceedings of the 2019posts in social media.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1415–1420, minneapolis, minnesota.
association for computational linguistics..marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019b.
semeval-2019 task 6: identifying and cat-egorizing offensive language in social media (of-in proceedings of the 13th interna-fenseval).
tional workshop on semantic evaluation, pages 75–86, minneapolis, minnesota, usa.
association forcomputational linguistics..yin zhang, rong jin, and zhi-hua zhou.
2010. un-derstanding bag-of-words model: a statistical frame-international journal of machine learningwork.
and cybernetics, 1(1-4):43–52..caleb ziems, ymir vigfusson, and fred morstatter.
2020. aggressive, repetitive, intentional, visible,and imbalanced: reﬁning representations for cyber-bullying classiﬁcation.
proceedings of the interna-tional aaai conference on web and social media,14(1):808–819..5926