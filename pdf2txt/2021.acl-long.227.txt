ernie-doc: a retrospective long-document modeling transformer.
siyu ding∗, junyuan shang∗, shuohuan wang, yu sun, hao tian,hua wu and haifeng wangbaidu inc., china{dingsiyu, shangjunyuan, wangshuohuan, sunyu02,tianhao, wu hua, wanghaifeng}@baidu.com.
abstract.
transformers are not suited for process-ing long documents, due to their quadrati-cally increasing memory and time consump-tion.
simply truncating a long document orapplying the sparse attention mechanism willincur the context fragmentation problem orlead to an inferior modeling capability againstin this paper, wecomparable model sizes.
propose ernie-doc, a document-level lan-guage pretraining model based on recurrencetransformers (dai et al., 2019).
two well-designed techniques, namely the retrospectivefeed mechanism and the enhanced recurrencemechanism, enable ernie-doc 1, which hasa much longer effective context length,tocapture the contextual information of a com-plete document.
we pretrain ernie-docto explicitly learn the relationships amongsegments with an additional document-awaresegment-reordering objective.
various exper-iments were conducted on both english andchinese document-level tasks.
ernie-docimproved the state-of-the-art language model-ing result of perplexity to 16.8 on wikitext-103. moreover, it outperformed competitivepretraining models by a large margin on mostlanguage understanding tasks, such as textclassiﬁcation and question answering..1.introduction.
transformers (vaswani et al., 2017) have achievedremarkable improvements in a wide range of nat-ural language tasks, including language model-ing (dai et al., 2019), text classiﬁcation (yang et al.,2019), and question answering (devlin et al., 2018;radford et al., 2019).
this success is largely dueto the self-attention mechanism, which enables thenetwork to capture contextual information from the.
*indicates equal contribution.
1source code and pre-trained checkpoints can be foundat https://github.com/paddlepaddle/ernie/tree/repro/ernie-doc..figure 1: available contextual information utilized bytransformer variants, where a long document d is par-titioned into three segments si(i ∈ [1, 2, 3]).
whentraining on s2, (a) and (b) optimize the pretraining ob-jective depending only on the contextual informationfrom the current segment or segments in the forwardpass, whereas ernie-doc utilizes the contextual in-formation of the entire document for each segment..entire input sequence.
nevertheless, the memoryusage and computation complexity caused by theself-attention mechanism grows quadratically withthe sequence length, incurring excessive cost whenprocessing a long document on existing hardware.
currently, the most prominent pretrained mod-els, such as bert (devlin et al., 2018), are usedon ﬁxed-length input segments of a maximum of512 tokens owing to the aforementioned limita-tion.
thus, a long document input must be parti-tioned into smaller segments of manageable sizes.
however, this leads to the loss of important cross-segment information, that is, the context fragmen-tation problem (dai et al., 2019), as shown infig.
1(a).
to mitigate the problem of insufﬁcient in-teractions among the partitioned segments of longdocuments, recurrence transformers (dai et al.,2019; rae et al., 2019) permit the use of contextualinformation from previous segments in computingthe hidden states for a new segment by maintaininga memory component from the previous activation;.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2914–2927august1–6,2021.©2021associationforcomputationallinguistics2914s1s3s1s2(c) ernie-docs1s2s2recurrence transformers1s2vanilla or sparse transformerp( y | s2 )p( y | s1 , s2 )p( y | s1 , s2 , s3 )(cid:11)(cid:68)(cid:12)(cid:3)(b) s3s3layer-1layer-2layer-1layer-2this enables the modeling of long documents.
in ad-dition, sparse attention transformers (child et al.,2019; tay et al., 2020; beltagy et al., 2020; zaheeret al., 2020) focus on reducing the complexity ofself-attention operations to explicitly improve themodeling length, but only up to a restricted contextlength (4,096) due to resource limitations..we argue that existing strategies are not sufﬁ-ciently effective or reliable, because the contex-tual information of a complete document is stillnot available for each segment during the train-ing phase.
as depicted in fig.
1, when trainingon segment s2, the model is ideally optimizedby maximizing p (y | (s1, s2, s3)) conditioned onthe contextual information of the entire documentd = {s1, s2, s3}, in contrast to the following sub-optimal solutions: p (y | s2) for vanilla/sparsetransformers2 and p (y | (s1, s2)) for recurrencetransformers..to address this limitation, we propose ernie-doc (a retrospective long-document modelingtransformer) based on the recurrence transformerparadigm.
inspired by the human reading behaviorof skimming a document ﬁrst and then looking backupon it attentively, we design a retrospective feedmechanism in which segments from a documentare fed twice as input.
as a result, each segment inthe retrospective phase could explicitly fuse the se-mantic information of the entire document learnedin the skimming phase, which prevents contextfragmentation..however, simply incorporating the retrospectivefeed mechanism into recurrence transformers isinfeasible because the maximum effective contextlength is limited by the number of layers (dai et al.,2019), as shown in fig.
1 (b).
thus, we present anenhanced recurrence mechanism, a drop-in re-placement for a recurrence transformer, by chang-ing the shifting-one-layer-downwards recurrence tothe same-layer recurrence.
in this manner, the max-imum effective context length can be expanded, andpast higher-level representations can be exploitedto enrich future lower-level representations..moreover, we introduce a segment-reorderingobjective to pretrain a document-level model.
speciﬁcally, it is a document-aware task of pre-dicting the correct order of the permuted set ofsegments of a document, to model the relationshipamong segments directly.
this allows ernie-.
2for sparse transformers, the length of segment s2 couldbe up to 4,096 in beltagy et al.
(2020); zaheer et al.
(2020)..doc to build full document representations forprediction.
this is analogous to the sentence-reordering task in ernie 2.0 (sun et al., 2020b)but at a segment level of granularity, spanning(commonly) multiple training steps..we ﬁrst evaluate ernie-doc on autoregres-sive word-level language modeling using the en-hanced recurrence mechanism, which, in theory,allows the model to process a document with in-ﬁnite words.
ernie-doc achieves state-of-the-art (sota) results on the wikitext-103 bench-mark dataset, demonstrating its effectiveness inlong-document modeling.
then, to evaluate thepotential of ernie-doc on document-level nat-ural language understanding (nlu) tasks, we pre-trained the english ernie-doc on the text cor-pora utilized in bigbird (zaheer et al., 2020) fromthe roberta-released checkpoint, and the chi-nese ernie-doc on the text corpora utilized inernie 2.0 (sun et al., 2020b) from scratch.
afterpretraining, we ﬁne-tuned ernie-doc on a widerange of english and chinese downstream tasks, in-cluding text classiﬁcation, question answering andkeypharse extraction.
empirically, ernie-docconsistently outperformed roberta on variousbenchmarks and showed signiﬁcant improvementsover other high-performance long-text pretrainingmodels for most tasks..2 related work.
sparse attention transformers have been exten-sively explored (child et al., 2019; tay et al., 2020;beltagy et al., 2020; zaheer et al., 2020).
thekey idea is to sparsify the self-attention operation,which scales quadratically with the sequence length.
for instance, the sparse transformer (child et al.,2019) uses a dilated sliding window that reduces√l), where l is the se-the complexity to o(lquence length.
reformer (kitaev et al., 2020) fur-ther reduces the complexity to o(l log l) usinglocality-sensitive hashing attention to compute thenearest neighbors.
bp-transformers (ye et al.,2019) employs a binary partition for the inputsequence.
recently, longformer (beltagy et al.,2020) and bigbird (zaheer et al., 2020) have beenproposed, and both achieved state-of-the-art perfor-mance on a variety of long-document tasks.
theyreduce the complexity of self-attention to o(l)by combining random attention, window attention,and global attention.
however, it has been provenin zaheer et al.
(2020) that sparse attention mech-.
2915anisms cannot universally replace dense attentionmechanisms; moreover, solving the simple problemof ﬁnding the furthest vector requires ω(n)-layersof a sparse attention mechanism but only o(1)-layers of a dense attention mechanism.
in addition,the aforementioned methods require customizedcuda kernels or tvm programming to imple-ment sparse attention, which are not maintainableand are difﬁcult to use.
in this study, we adopt adifferent approach to adapting recurrence trans-formers for a pretraining-then-ﬁnetuning setting, tomodel a long document..recurrence transformers (dai et al., 2019; raeet al., 2019) have been successfully applied in gen-erative language modeling.
they employ the trans-former decoder as a parametric model for each con-ditional distribution in p(x) = (cid:81)lt=1 p(xt|x<t),where x denotes a text sequence.
to capture longdependencies, they process the text in segmentsfrom left to right based on the segment recurrencemechanism (dai et al., 2019).
this mechanismmaintains a memory bank of past activations ateach layer to preserve a history of context.
com-pressive transformer (rae et al., 2019) adds acompressive memory bank to sufﬁciently store oldactivations instead of discarding them, which fa-cilitates long-range sequence learning.
however,these methods operate from left to right, whichlimits their capacity for discriminative languageunderstanding tasks that require bidirectional in-formation.
xlnet (yang et al., 2019) proposed apermutation language modeling objective to con-struct bidirectional information and achieve supe-rior performance in multiple nlp tasks; however,its application to long-document modeling tasksremains largely unexplored.
ernie-doc buildson the ideas of the recurrence transformers to 1)tackle the limitation of recurrence transformersfor utilizing bidirectional contextual informationand 2) improve the behavior of the segment recur-rence mechanism to capture longer dependencies..hierarchical transformers (zhang et al., 2019;lin et al., 2020) have enabled signiﬁcant progresson numerous document-level tasks, such as docu-ment summarization (zhang et al., 2019) and docu-ment ranking (lin et al., 2020).
similar to vanillatransformers, hierarchical transformers also splitlong documents into shorter segments with man-ageable lengths and then feed them independentlyto produce corresponding segment-level semanticrepresentations.
unlike in vanilla transformers,.
however, separate transformer layers are used inhierarchical transformers to process the concate-nation of these representations.
hierarchical trans-formers ignore the contextual information from theremaining segments when processing each segmentof a long document, thus suffering from the contextfragmentation problem..3 proposed method.
in this section, we ﬁrst describe the background(sec.
3.1) that ernie-doc builds on.
then,we present the implementation of ernie-doc,including the retrospective feed mechanism insec.
3.2, the enhanced recurrence mechanism insec.
3.3, and the segment-reordering objective insec.
3.4..3.1 background.
formally, a long document d is sliced into tsequential segments, denoted as {s1, s2, ..., st },where sτ = {xτ,1, xτ,2, ..., xτ,l} is the τ -th seg-ment with l tokens; x denotes a single token.
vanilla, sparse, and recurrence transformers em-ploy different strategies to produce the hidden stateτ ∈ rl×d for segment sτ at the n-th layer:hn.
(cid:40)hn−1.
τ +1 , vanilla or sparse transformers.
(cid:101)hn−1τ +1 =.
τ.
[sg(hn−1.)
◦ hn−1qnτ +1w(cid:62)τ +1, knτ +1 = transformer-block (qnhn.
τ +1 = hn−1.
τ +1, vn.
τ +1 ], recurrence transformers,.
τ +1w(cid:62).
q , (cid:101)hn−1τ +1, kn.
k , (cid:101)hn−1τ +1)..τ +1, vn.
τ +1w(cid:62)v ..(1)where q ∈ rl×d, k, and v ∈ r(l+m)×d are thequery, key and value vectors, respectively withhidden dimension d and memory length m (notethat m = 0 for vanilla or sparse transform-ers); (cid:101)hn−1τ +1 ∈ r(l+m)×d is the extended context;w∗ ∈ rd∗×d represents learnable linear projec-tion parameters; the function sg(·) denotes thestop-gradient operation; and the notation [◦] de-notes the concatenation of two hidden states alongin contrast to vanilla orthe length dimension.
sparse transformers, where hnτ +1 is produced us-ing only itself, recurrence transformers introducea segment-level recurrence mechanism to promoteinteraction across segments.
the hidden state com-puted for the previous segment hn−1is cached asan auxiliary context to help process the current seg-ment hnτ .
however, from the concatenation part in) ◦ hn−1eq.
1, i.e., [sg(hn−1τ +1], there is apparentlya constraint that the current hidden state can onlyfuse information from the previous segments.
in.
τ.τ.
2916figure 2: illustrations of ernie-doc and recurrence transformers, where models with three layers take as inputa long document d which is sliced into four segments si, i ∈ [1, 2, 3, 4].
recurrence transformers (upper-right):when training on s4, it can only fuse the contextual information of the previous two consecutive segments s2, s3,since the largest effective context length grows linearly w.r.t the number of layers.
ernie-doc (lower):theeffective context length is much larger aided by the enhanced recurrence mechanism (sec.
3.3).
thus, s4 canfuse the information of s1 discarded by recurrence transformers.
moreover, segments in the retrospective phasecontains the contextual information of an entire document, powered by the retrospective feed mechanism (sec.
3.2)..other words, the contextual information of an entiredocument is not available for each segment..3.2 retrospective feed mechanism.
ernie-doc employs a retrospective feed mecha-nism to address the unavailability of the contextualinformation of a complete document for each seg-ment.
the segments from a long document aretwice fed as input.
mimicking the human readingbehavior, we refer to the ﬁrst and second input-taking phases as the skimming and retrospectivephases, respectively.
in the skimming phase, weemploy a recurrence mechanism to cache the hid-den states for each segment.
in the retrospectivephase, we reuse the cached hidden states from theskimming phase to enable bi-directional informa-tion ﬂow.
naively, we can rewrite eq.
1 to obtainthe contextual information of an entire documentin the skimming phase to be utilized in the retro-spective phase as follows,.
1:t ◦ (cid:98)h2.
(cid:98)h = [ (cid:98)h1(cid:101)hn−1τ +1 = [sg( (cid:98)h ◦ hn−1.
1:t · · · ◦ (cid:98)hn) ◦ hn−1.
1:t ], (skim.
phase)τ +1], (retro.
phase).
τ.
(2)where (cid:98)h ∈ r(l∗t ∗n )×d denotes the cached hid-den states in the skimming phase with t segments,l length of each segment and total n layers, and(cid:98)hit ] is the concatenation ofi-th layer’s hidden states of the skimming phase.
thus, the extended context (cid:101)hn−1τ +1 is guaranteed tocapture the bidirectional contextual information ofthe entire document.
however, it will incur massive.
1:t = [(cid:98)hi.
2 · · · ◦ (cid:98)hi.
1 ◦ (cid:98)hi.
memory and computation cost for directly employ-ing (cid:98)h in self-attention mechanism.
henceforth, themain issue is how (cid:98)h should be implemented in amemory- and computation-efﬁcient manner..by rethinking segment-level recurrence (daiet al., 2019), we observe that the largest possiblecontext dependency length increases linearly w.r.tthe number of layers (n ).
for instance, at i-thlayer, (cid:98)hiτ have the longest dependency to (cid:98)h1τ −(i−1).
thus, to minimize memory and computation con-sumption, hidden states from the n -th layer (top-layer) are included at a stride of n , which is suf-ﬁcient to build the contextual information of anentire document.
formally, (cid:98)h can be reduced to(cid:98)hr = [(cid:98)hn(cid:98)t /n (cid:99)∗n ] (note that whent is not evenly divisible by n , the last hidden state(cid:98)hnt need to be included).
however, for a long doc-ument input, the extra computational and memorycost of (cid:98)hr ∈ r(cid:100)t /n (cid:101)×d where t (cid:29) n is stillexcessive on existing hardware..2∗n · · · ◦ (cid:98)hn.
n ◦ (cid:98)hn.
3.3 enhanced recurrence mechanism.
τ.to effectively utilize the retrospective feed mech-anism in practice, an ideal strategy is to ensurethat the cached hidden state hn−1already containsthe contextual information of an entire documentwithout explicitly taking (cid:98)h or (cid:98)hr as input.
es-sentially, we should tackle the problem of limitedeffective context length in the segment-level re-currence mechanisms.
herein, we introduce theenhanced recurrence mechanism, a drop-in replace-ment for the segment-level recurrence mechanism,.
2917s1s2s3s4recurrence transformerss1s2s3s4larger eﬀective context lengths1s2s3s4ernie-doctransformer blockmemory concatenationhidden states inputeﬀective contextlarger eﬀective context lengthretrospective phaselayer-1layer-2layer-3layer-1layer-2layer-3the retrospective phaseby changing the shifting-one-layer-downwards re-currence to the same-layer recurrence as follows:.
(cid:101)hn−1τ +1 = [ sg(hn.
τ ) ◦ hn−1τ +1].
(3).
where the cached hidden state hn−1eq.
2 is replaced with hn.
ττ in eq.
3..in eq.
1 and.
as shown in fig.
2, when the retrospective feedmechanism is combined with the enhanced recur-rence mechanism, every segment in the retrospec-tive phase (shown in the box with a green dottedborder) has bidirectional contextual information ofthe entire text input.
we successfully modeled alarger effective context length (shown in the boxwith a orange dotted border) than traditional re-currence transformers can without extra memoryand computation costs.
another beneﬁt of the en-hanced recurrence scheme is that past higher-levelrepresentations can be exploited to enrich futurelower-level representations..3.4 segment-reordering objective.
in addition to the masked language model(mlm) objective (devlin et al., 2018), we intro-duce an additional document-aware task calledsegment-reordering objective for pretraining.
beneﬁtting from the much larger effective contextlength provided by the enhanced recurrence mecha-nism, the goal of the segment-reordering objectiveis to predict the correct order for the permutedset of segments of a long document, to explicitlylearn the relationships among segments.
duringthe pretraining process of this task, a long textinput d is ﬁrst randomly partitioned into 1 to mchunks; then, all the combinations are shufﬂed ina random order.
as shown in fig.
3, d is parti-tioned into three chunks and then permuted, thatis, d = {c1, c2, c3} =⇒ ˆd = {c2, c3, c1},where ci denotes the i-th chunk.
subsequently,the permuted long context ˆd is split into t se-quential segments as a common practice, denotedas ˆd = {s1, s2, ..., st }.
we let the pretrainedmodel reorganize these permuted segments, mod-eled as a k-class classiﬁcation problem, wherek = (cid:80)m.i=1 i!..
the pretraining objective is summarized as fol-.
lows for the τ -th input segment:.
log pθ(sτ | ˆsτ ) + 1τ =t log pθ(d| ˆd).
maxθ.where ˆsτ is the corrupted version of sτ , which isobtained by randomly setting a portion of tokens.
figure 3: illustrations of segment-reordering objective..to [mask]; ˆd is the permutated version of d; θ isthe model parameter; and 1τ =t indicates that thesegment-reordering objective is optimized only atthe t -th step..4 experiments.
4.1 autoregressive language modeling.
autoregressive language modeling aims to esti-mate the probability distribution of an existing to-ken/character based on previous tokens/charactersin an input sequence.
for comparison with pre-vious work, we conducted experiments on word-level lm, that is, wikitext-103 (merity et al.,2016), which is a document-level language model-ing dataset..4.1.1 experimental setup.
for autoregressive language modeling, we usea memory-enhanced transformer-xl (dai et al.,2019), that is, we employ our enhanced recurrencemechanism to replace the primitive one used inthe transformer-xl.
additionally, as proposedby segatron (bai et al., 2020), we introduce thesegment-aware mechanism into transformer-xl.
based on transformer-xl, we trained a base-sizemodel (l=16, h=410, a=10) and a large-sizemodel (l=18, h=1,024, a=16)3. the models weretrained for 200k/400k steps using a batch sizeof 64/128 for the base/large conﬁgurations.
dur-ing the training phase, the sequence length andmemory length were limited to 150 and 384 forthe base and the large model, respectively.
the re-maining hyper-parameters were identical to thoseof transformer-xl..3we denote the number of transformer layers as l, thehidden size as h, and the number of self-attention heads as a..2918s1s2stsegments(~512 tokens each)(cid:51)(cid:72)(cid:85)(cid:80)(cid:88)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3)(cid:38)(cid:75)(cid:88)(cid:81)(cid:78)(cid:86)(cid:3)(cid:82)(cid:73)(cid:68)(cid:3)(cid:47)(cid:82)(cid:81)(cid:74)(cid:3)(cid:55)(cid:72)(cid:91)(cid:87)(cid:3)(cid:44)(cid:81)(cid:83)(cid:88)(cid:87)modellabel = “c1c2c3”  c2:[related work] sparse attention based transformers are largely explored …c3:[proposed method] in this section, we ﬁrstly describe the background of proposed ernie-doc …c1:[introduction] transformers have achieved remarkable improvements …(cid:335)(cid:335)---.
#param.
ppl.
modelsresults of base modelslstm (grave et al., 2016)lstm+neural cache (grave et al., 2016)gcnn-14 (dauphin et al., 2017)qrnn (merity et al., 2018)transformer-xl base (dai et al., 2019)segatransformer-xl base (bai et al., 2020)ernie-doc baseresults of large models247m 18.7adaptive input (baevski and auli, 2018)247m 18.3transformer-xl large (dai et al., 2019)compressive transformer (rae et al., 2019)247m 17.1segatransformer-xl large (bai et al., 2020) 247m 17.1247m 16.8ernie-doc large.
48.740.837.2151m 33.0151m 24.0151m 22.5151m 21.0.table 1: comparison between transformer-xl andcompetitive baseline results on wikitext-103..4.1.2 resultstab.
1 summarizes the evaluation results forwikitext-103.
ernie-doc achieves an impres-sive improvement compared with transformer-xl:the perplexity (ppl) decreases by 3.0 for the basemodel and by 1.5 for the large model.
finally, weimprove the state-of-the-art result of ppl to 21.0(the base model) and 16.8 (the large model)..4.2 pretraining and finetuning.
4.2.1 pretraining text corpora.
datasetwikipediabookscorpuscc-newsstories.
# tokens avg len4802,0105601,891.
2.7b1.2b14b7.5b.
size8g3.5g42g22g.
table 2: english datasets used for pretraining..english data.
to allow ernie-doc to capturelong dependencies in pretraining, we compiled acorpus from four standard datasets: wikipedia,bookscorpus (zhu et al., 2015), cc-news4,and stories (trinh and le, 2018) (details listedin tab.
2).
we tokenized the corpus using theroberta wordpieces tokenizer (liu et al., 2019)and duplicated the pretraining data 10 times.
chinese data.
the chinese text corpora used inernie 2.0 (sun et al., 2020b) were adopted forpretraining ernie-doc..4.2.2 experimental setuppretraining.
we trained three sizes of models forenglish tasks: small (l=6, h=256, a=4), base(l=12, h=768, a=12), and large (l=24, h=1,024,.
4we used news-please to crawl english news articlespublished between september 2016 and february 2019 andadopted message digest algorithm5 (md5) for deduplication..models.
roberta (liu et al., 2019)longformer (beltagy et al., 2020)bigbird (zaheer et al., 2020)ernie-docxlnet-large (yang et al., 2019)ernie-doc-large.
imdb.
acc.
95.395.7-96.196.897.1.f195.0-95.296.1-97.1.hypf187.894.892.296.3-96.6.table 3: results on the imdb and hyp dataset forlong-text classiﬁcation..a=16).
for chinese tasks, we used only one size,i.e., base (l=12, h=768, a=12).
we limited thelength of the sentences in each mini-batch to512 tokens and the length of the memory to 128.the models were trained for 500k/400k/100ksteps using a batch size of 2,560/2,560/3,920sentences for the small/base/large conﬁgurations.
ernie-doc was optimized with the adam(kingma and ba, 2014) optimizer.
the learningrate was warmed up over the ﬁrst 4,000 steps to apeak value of 1e-4, and then it linearly decayed.
the remaining pretraining hyperparameters werethe same as those of roberta (liu et al., 2019)(see tab.
12).
additionally, we employed relativepositional embedding (shaw et al., 2018) in ourmodel pretraining because it is necessary forreusing hidden state without causing temporalconfusion (dai et al., 2019)..finetune.
in contrast to previous models, such asbert, roberta, and xlnet, the proposed modelemploys the retrospective feed mechanism and theenhanced recurrence mechanism during the ﬁne-tuning phase to fully utilize the advantages of thesetwo strategies..4.2.3 results on english tasks.
imdb reviews.
results on long-text classiﬁcation tasks.
we(maasconsider two datasets:et al., 2011) and hyperpartisan news detection(hyp) (kiesel et al., 2019).
the former is a widelyused sentiment analysis dataset containing 50,000movie reviews, labeled as positive or negative.
thelatter contains news that takes extreme left-wingor right-wing standpoints.
the documents in hypare extremely long (50% of the samples containmore than 537 tokens) and are thus suitable fortesting long-text classiﬁcation ability.
tab.
3 sum-marizes the results of the ernie-doc-base andernie-doc-large models for long-text classiﬁ-cation tasks, and ernie-doc achieves a sotaresult.
on imdb, we observed a modest perfor-.
2919models.
robertalongformerbigbirdernie-doclongformer-largebigbird-largeernie-doc-large.
tqaf174.375.279.580.177.8-82.5.hqaspan supp83.473.584.474.387.175.579.486.385.881.089.481.382.287.6.joint63.564.467.870.571.4-73.7.table 4: results on tqa and hqa dev dataset fordocument-level qa.
hqa metrics are f1..openkp datasetbling-kpe (xiong et al., 2019)jointkpe (sun et al., 2020a)etc (ainslie et al., 2020)ernie-doc.
f1@1 f1@3 f1@520.929.226.733.839.839.1-40.2-34.440.540.2.table 5: results on openkp dev dataset.
the baselineresults are obtained from corresponding papers underno-visual-features setting..on.
document-level.
mance gain compared with roberta.
this is be-cause nearly 90% of the samples in the datasetconsist of fewer than 569 tokens.
unlike on imdb,ernie-doc surpasses the baseline models onhyp by a substantial margin, demonstrating itscapability of utilizing information from a long doc-ument input.
note that we include xlnet-large,the previous sota pretraining model on the imdbdataset, as the baseline for a large model setting;ernie-doc achieves a result comparable to thatof xlnet-large.
question-resultsanswering tasks.
we utilized two document-level qa datasets (wikipedia setting of triviaqa(tqa) (joshi et al., 2017) and distractor setting ofhotpotqa (hqa) (yang et al., 2018)) to evaluatethe reasoning ability of the models over longdocuments.
tqa and hqa are extractive qatasks, and we follow the simple qa model ofbert (devlin et al., 2018) to predict an answerwith the maximum sum of start and end logitsacross multiple segments of a sample.
in addition,we use a modiﬁed cross-entropy loss (clark andgardner, 2017) for the tqa dataset and use atwo-stage model (groeneveld et al., 2020) with thebackbone of ernie-doc for the hqa dataset.
shows that ernie-doc outperformstab.
4.roberta and longformer by a considerablemargin on these two datasets, and is comparable tocurrent sota long-document model, i.e., bigbirdon hqa in large-size model setting.
results on the keyphrase extraction task.
weinclude openkp (xiong et al., 2019) dataset to eval-.
uate ernie-doc’s ability to extract keyphrasesfrom a long document.
each document containsup to three short keyphrases and we follow themodel setting of jointkpe (sun et al., 2020a) andetc (ainslie et al., 2020) by applying cnns onbert’s output to compose n-gram embeddingsfor classiﬁcation.
we report the results of base-size models in tab.
5 under no-visual-features set-ting for easy and fair comparison with baselines.
ernie-doc performs stably better on all metricson the openkp dataset..4.2.4 results on chinese taskswe conducted extensive experiments on sevenchinese natural language understanding (nlu)tasks, including machine reading comprehension(cmrc2018 (cui et al., 2018), drcd (shaoet al., 2018), dureader (he et al., 2017), c3 (sunet al., 2019a)), semantic similarity (cail2019-scm (cail) (xiao et al., 2019)), and long-textclassiﬁcation (iflytek (ifk) (xu et al., 2020),thucnews (thu)5 (sun et al., 2016)).
the docu-ments in all the aforementioned datasets are sufﬁ-ciently long to be used to evaluate the effectivenessof ernie-doc on long-context tasks (see detaileddatasets statistics in tab.
9).
we reported the meanresults with ﬁve runs for the seven chinese tasksin tab.
6, and summarized the hyperparameters intab.
16. ernie-doc outperforms previous mod-els across these chinese nlu tasks by a signiﬁcantmargin in the base-size model group..4.2.5 ablation studies.
no.
modelsiiiiiiivv.ernie-doci w/o segment-reorderingii w/o retrospective feediii w/o enhanced recurrenceiv w/o recurrence.
tqa hyp86.1064.5684.6063.5983.2763.3881.6761.0977.7258.35.table 7: performance of ernie-doc-small after ab-lating each proposed component (f1 result is reported).
effect of proposed components.
tab.
7 showsthe performance of ernie-doc-small on twoenglish tasks after ablating each proposed compo-nent.
all models were pretrained and ﬁne-tunedwith the same experimental setup, and we reportthe mean results of ﬁve runs.
we observed a stableperformance gain across these two tasks by incor-porating each proposed component.
by comparing.
5we usefound.
aat.
subset of thucnews which canhttps://github.com/gaussic/.
betext-classification-cnn-rnn..2920drcdem/f1.
cmrc2018 dureader cailacc..thuacc..ifkacc..c3acc..models.
dev.
test.
85.7/91.6 84.9/90.9bert (devlin et al., 2018)bert-wwm-ext∗85.0/91.2 83.6/90.4roberta-wwm-ext∗86.6/92.5 85.2/92.0macbert (cui et al., 2020a) 88.3/93.5 87.9/93.2xlnet-zh (cui et al., 2020b)83.2/92.0 82.8/91.8ernie 1.0 (sun et al., 2019b) 84.6/90.9 84.0/90.5ernie 2.0 (sun et al., 2020b) 88.5/93.8 88.0/93.490.5/95.2 90.5/95.1ernie-doc.
em/f1dev66.3/85.967.1/85.767.4/87.269.5/87.763.0/85.965.1/85.169.1/88.676.1/91.6.
em/f1dev.
dev test dev test dev dev test59.5/73.1 61.9 67.3 97.7 97.3 60.3 65.7 64.597.6 97.6 59.4 67.8 68.560.3 67.1 66.5--.
-/--/---/---/-57.9/72/197.7 97.3 59.0 65.5 64.161.3/74.9 64.9 67.9 98.0 97.5 61.7 72.3 73.265.8/77.9 65.6 68.8 98.3 97.7 62.4 76.5 76.5.
-----.
-----.
---.
---.
--.
table 6: results on seven chinese nlu tasks for ernie-doc-base model.the results of the models with ”∗” arefrom cui et al.
(2019).
the xlnet-zh is the abbreviation of chinese-xlnet.
notably, the result of bert on cailwas obtained from xiao et al.
(2019), where bert was post-pretrained with a legal dataset..no.iv and no.v, we see that segment-level recur-rence is necessary for modeling long documentsand produces 2.74 and 3.95 % points improvementon the tqa and hyp dateset, respectively.
more-over, a substantial improvement is achieved usingthe enhance recurrence mechanism (2.29% pointon tqa and 1.40% point on hyp, see no.iii - iv).
retrospective feed mechanism further improves0.21% point on tqa and 1.33% point on hyp(no.ii - no.iii).
considering different types oftasks, we observe that on hyp, an extremely longtext classiﬁcation dataset, a substantial improve-ment is achieved using the segment-reordering ob-jective (1.5% point).
this indicates that the [cls]token, pretrained using the segment-reordering ob-jective, is more adaptable to the document-leveltext classiﬁcation task..figure 4: acc.
(dotted line) and ppl (solid line) met-rics for variants of our small models with different max-imum sequence length during pretraining..effect of enhanced recurrence mechanism withregard to different maximum sequence lengths.
as depicted in fig.
4, the enhanced recurrencemechanism plays an important role in pretrainingan effective language model with lower ppl and.
higher accuracy under both the maximum sequenceinput lengths of 128 and 512. the effect of theenhanced recurrence mechanism is more signiﬁ-cant under a smaller maximum sequence length,even makes the ernie-doc-small (max-len:128)comparable to ernie-doc-small w/o en recur(max-len:512) w.r.t accuracy.
this intriguing prop-erty of the enhanced recurrence mechanism enablesmore efﬁcient model training and inference by re-ducing maximum sequence length while remainingcomparable modeling capability..5 conclusion.
in this paper, we proposed ernie-doc, adocument-level language pretraining model basedon the recurrence transformers paradigm.
twowell-designed mechanisms, namely the retrospec-tive feed mechanism and the enhanced recurrentmechanism, enable ernie-doc, which theoreti-cally has the longest possible dependency, to modelbidirectional contextual information of a completedocument.
additionally, ernie-doc is pre-trained with a document-aware segment-reorderingobjective to explicitly learn the relationship amongsegments of a long context.
experiments on var-ious downstream tasks demonstrate that ernie-doc outperforms existing strong pretraining mod-els such as roberta, longformer, and bigbirdand achieves sota results on several languagemodeling and language understanding benchmarks.
in future studies, we will evaluate ernie-doc onlanguage generation tasks, such as generative ques-tion answering and text summarization.
we willalso investigate its potential applicability in otherareas, such as computational biology.
another pos-sibility is to incorporate graph neural networks intoernie-doc to enhance its modeling capability.
292110203040steps (10k)3.23.43.63.8log(ppl)0.400.450.500.550.600.650.70accours_w/o_en_recur (max-len:128)ours_w/o_en_recur (max-len:512)ours (max-len:128)ours (max-len:512)for tasks that require multi-hop reasoning and long-document modeling ability..acknowledgments.
zihang dai, zhilin yang, yiming yang, jaime g. car-bonell, quoc v. le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyonda ﬁxed-length context.
corr, abs/1901.02860..this work was supported by the national key re-search and development project of china (no.
2018aaa0101900)..yann n dauphin, angela fan, michael auli, and davidgrangier.
2017. language modeling with gated con-volutional networks.
in international conference onmachine learning, pages 933–941.
pmlr..references.
joshua ainslie, santiago ontanon, chris alberti, va-clav cvicek, zachary fisher, philip pham, anirudhravula, sumit sanghai, qifan wang, and li yang.
2020. etc: encoding long and structured inputsin proceedings of the 2020 con-in transformers.
ference on empirical methods in natural languageprocessing (emnlp), pages 268–284..alexei baevski and michael auli.
2018. adaptive in-put representations for neural language modeling.
arxiv preprint arxiv:1809.10853..he bai, peng shi, jimmy lin, yuqing xie, luchen tan,kun xiong, wen gao, and ming li.
2020. segatron:segment-aware transformer for language modelingand understanding..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..rewon child, scott gray, alec radford, and ilyasutskever.
2019. generating long sequences withsparse transformers.
corr, abs/1904.10509..christopher clark and matt gardner.
2017. simpleand effective multi-paragraph reading comprehen-sion.
arxiv preprint arxiv:1710.10723..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020a.
revisiting pre-trained models for chinese natural language process-ing.
arxiv preprint arxiv:2004.13922..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020b.
revisiting pre-trained models for chinese natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:findings, pages 657–668, online.
association forcomputational linguistics..yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019.pre-training with whole word masking for chinesebert.
arxiv preprint arxiv:1906.08101..yiming cui, ting liu, wanxiang che, li xiao,zhipeng chen, wentao ma, shijin wang, and guop-ing hu.
2018. a span-extraction dataset for chinesearxiv preprintmachine reading comprehension.
arxiv:1810.07366..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..edouard grave, armand joulin, and nicolas usunier.
2016. improving neural language models with a con-tinuous cache.
arxiv preprint arxiv:1612.04426..dirk groeneveld, tushar khot, ashish sabharwal, et al.
2020. a simple yet strong pipeline for hotpotqa.
arxiv preprint arxiv:2004.06753..wei he, kai liu,.
jing liu, yajuan lyu, shiqizhao, xinyan xiao, yuan liu, yizhong wang,hua wu, qiaoqiao she, et al.
2017. dureader:a chinese machine reading comprehension datasetarxiv preprintfrom real-world applications.
arxiv:1711.05073..mandar joshi, eunsol choi, daniel s weld, and lukezettlemoyer.
2017. triviaqa: a large scale distantlysupervised challenge dataset for reading comprehen-sion.
arxiv preprint arxiv:1705.03551..johannes kiesel, maria mestre, rishabh shukla, em-manuel vincent, payam adineh, david corney,benno stein, and martin potthast.
2019. semeval-2019 task 4: hyperpartisan news detection.
in pro-ceedings of the 13th international workshop on se-mantic evaluation, pages 829–839..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in 8thinternational conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..jimmy lin, rodrigo nogueira, and andrew yates.
2020. pretrained transformers for text ranking: bertand beyond.
arxiv preprint arxiv:2010.06467..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..andrew maas, raymond e daly, peter t pham, danhuang, andrew y ng, and christopher potts.
2011.inlearning word vectors for sentiment analysis..2922proceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies, pages 142–150..stephen merity, nitish shirish keskar, and richardlanguagearxiv preprint.
socher.
2018. an analysis of neuralmodeling at multiple scales.
arxiv:1803.08240..stephen merity, caiming xiong, james bradbury, andrichard socher.
2016. pointer sentinel mixture mod-els.
corr, abs/1609.07843..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
advances in neural information process-ing systems, 30:5998–6008..johannes welbl, pontus stenetorp, and sebastianriedel.
2018. constructing datasets for multi-hopreading comprehension across documents.
transac-tions of the association for computational linguis-tics, 6:287–302..chaojun xiao, haoxi zhong, zhipeng guo, cunchaotu, zhiyuan liu, maosong sun, tianyang zhang,xianpei han, heng wang, jianfeng xu, et al.
2019.cail2019-scm: a dataset of similar case matching inlegal domain.
arxiv preprint arxiv:1911.08962..jack w. rae, anna potapenko, siddhant m. jayakumar,and timothy p. lillicrap.
2019. compressive trans-formers for long-range sequence modelling.
corr,abs/1911.05507..lee xiong, chuan hu, chenyan xiong, daniel cam-pos, and arnold overwijk.
2019. open domainweb keyphrase extraction beyond language model-ing.
arxiv preprint arxiv:1911.02671..chih chieh shao, trois liu, yuting lai, yiying tseng,and sam tsai.
2018. drcd: a chinese machinearxiv preprintreading comprehension dataset.
arxiv:1806.00920..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-tations.
arxiv preprint arxiv:1803.02155..kai sun, dian yu, dong yu, and claire cardie.
2019a.
probing prior knowledge needed in challeng-ing chinese machine reading comprehension.
corr,abs/1904.09679..m sun, j li, z guo, z yu, y zheng, x si, and z liu.
2016. thuctc: an efﬁcient chinese text classiﬁer.
github repository..si sun, chenyan xiong, zhenghao liu, zhiyuan liu,joint keyphrase chunkingarxiv preprint.
and jie bao.
2020a.
and salience ranking with bert.
arxiv:2004.13639..yu sun, shuohuan wang, yu-kun li, shikun feng,hao tian, hua wu, and haifeng wang.
2020b.
ernie 2.0: a continual pre-training framework forin aaai, pages 8968–language understanding.
8975..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019b.
ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..yi tay, dara bahri, liu yang, donald metzler, and da-cheng juan.
2020. sparse sinkhorn attention.
arxivpreprint arxiv:2002.11296..trieu h trinh and quoc v le.
2018. a simplemethod for commonsense reasoning.
arxiv preprintarxiv:1806.02847..liang xu, xuanwei zhang, lu li, hai hu, chen-jie cao, weitang liu, junyi li, yudong li, kaisun, yechen xu, et al.
2020. clue: a chinese lan-guage understanding evaluation benchmark.
arxivpreprint arxiv:2004.05986..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..zhilin yang, peng qi, saizheng zhang, yoshua ben-gio, william w cohen, ruslan salakhutdinov, andchristopher d manning.
2018. hotpotqa: a datasetfor diverse, explainable multi-hop question answer-ing.
arxiv preprint arxiv:1809.09600..zihao ye, qipeng guo, quan gan, xipeng qiu, andzheng zhang.
2019. bp-transformer: modellinglong-range context via binary partitioning.
arxivpreprint arxiv:1911.04070..manzil zaheer, guru guruganesh, avinava dubey,joshua ainslie, chris alberti, santiago ontanon,philip pham, anirudh ravula, qifan wang, li yang,et al.
2020. big bird: transformers for longer se-quences.
advances in neural information process-ing systems..xingxing zhang, furu wei, and ming zhou.
2019. hi-bert: document level pre-training of hierarchicalbidirectional transformers for document summariza-tion.
arxiv preprint arxiv:1905.06566..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee inter-national conference on computer vision, pages 19–27..2923a appendices.
a.1 tasks.
following previous work, we evaluate ernie-doc on various tasks that require the ability tomodel a long document..document-level language modeling task.
weemploy wikitext-103 (merity et al., 2016) inlanguage modeling experiments.
wikitext-103 isthe largest available word-level benchmark withlong-term dependency for language modeling,which consists of 28k articles, where each articlehas 3.6k tokens on average, thus 103m trainingtokens in total..long text classiﬁcation.
we consider two en-glish datasets: imdb reviews (maas et al., 2011)and hyperpartisan news detection (kiesel et al.,2019) (see tab.
8), and two chinese datasets: ifly-tek (xu et al., 2020) and thucnews (sun et al.,2016) (see tab.
9).
imdb is a widely used senti-ment analysis dataset containing 50,000 movie re-views labeled as positive or negative.
training anddev dataset is equally split.
hyperpartisan containsnews that takes an extreme left-wing or right-wingstandpoint.
documents are extremely long in hy-perpartisan which makes it a good test for long textclassiﬁcation.
we use the same split as longformerby dividing 654 documents into train/dev/test sets.
iflytek contains 17,332 app descriptions.
thetask is to assign each description into one of 119categories, such as food, car rental and education.
thucnews is generated by ﬁltering historical dataof sina news rss subscription channel from 2005to 2011, including 740,000 news documents and14 categories.
in this paper, we employ the subsetversion instead of the full one 6, which contains 10categories, each with 5,000 pieces of data..for the above four long text classiﬁcationdatasets, we concatenate [cls]token witheach segment and takes as input multiple seg-ments of a text sequentially.
each segmentis generated by slicing the text with a slidingwindow of 128 tokens.
we apply binary crossentropy loss on the [cls] token of the last segment..long text semantic similarity.
considering thatthere is no available long text semantic similaritydataset in english, we evaluate the effectiveness.
6the subset version is also released and can be downloaded.
from the ofﬁcial website of thuctc..of ernie-doc on semantic similarity task onlydepending on chinese dataset cail2019-scm.
according to xiao et al.
(2019), cail2019-scmis a sub-task of the chinese ai and law challenge(cail) competition in 2019, which contains8,964 triplets of legal documents collected fromchina judgments online.
every document in amajority of triplet has more than 512 characters,therefore, the total length of a triplet is quite long.
cail2019-scm requires researchers to decidewhich two cases are more similar in a triplet.
speciﬁcally, given a triplet (a, b, c), where a, b,c are fact descriptions of three cases.
the modelneeds to predict whether sim(a, b) > sim(a, c)or sim(a, c) > sim(a, b),in which simdenotes the similarity between two cases.
insteadof separately feeding the document a, b, cinto the model to get the feature h, we use thecombinations of (a, b) and (a, c) as input.
wegenerate multiple segments for (a, b) or (a, c)with a sliding window of 128 tokens and feed themas input sequentially.
the binary cross entropyloss is applied to the difference of [cls] tokenoutput of each segment..triviaqa is a large scale qa dataset.
document-level question answering.
we utilizetwo english question answering datasets (trivi-aqa (joshi et al., 2017), hotpotqa (yang et al.,2018)) (see tab.
8) and four chinese question an-swering datasets (cmrc2018 (cui et al., 2018),drcd (shao et al., 2018), dureader (he et al.,2017), c3 (sun et al., 2019a)) (see tab.
9) to evalu-ate models’ reasoning ability over long documents.
thatcontains over 650k question-answer pairs.
weevaluate models on its wikipedia setting wheredocuments are wikipedia articles, and answersare named entities mentioned in multiple docu-ments.
the dataset is distantly supervised mean-ing that there is no golden span, thus we ﬁndall superﬁcial identical answers in provided docu-ments7.
we use the following input format for eachsegment: “[cls] context [q] question[/q]” where context is generated by slicing multi-documents input with a sliding window of 128 to-kens.
we take as input multiple segments of a sam-ple sequentially and attach a linear layer to eachtoken in a segment to predict the answer span.
we.
7we use the same preprocessing code for triviaqasee https://github.com/.
datasettensorflow/models/blob/master/official/nlp/projects/triviaqa/preprocess.py.
as bigbird,.
2924imdb.
dev2,000.train25,000.hyperpartisandev64.datasetssplit# samples# tokens of context length in each percentile using roberta wordpiece tokenizer50%90%95%max.
8,68525,20732,018173,302.
6391,7721,9945,566.
5371,5191,9975,566.
5211,5391,9792,643.
2155697453,084.
2125507242,778.train61,888.triviaqa.
train516.
8,58624,82532,132146,012.dev7,993.test65.hotpotqa.
openkp.
train90,432.
1,2791,7251,8883,733.dev7,404.
1,3251,7851,9433,618.train134,894.dev6,616.
8943,4515,340105,548.
6812,7344,13043,609.table 8: english datasets statistics..cail.
iflytekdevtrain.
thucnewsdevtrain.
datasetssplittest# samples 12,133 2,599 50,000 5,000 5,102 1,500 10,121 3,219 15,763 1,628 11,869 3,816 26,936 3,524 3,493# tokens of context length in each percentile using bert tokenizer50%90%95%max.
5791,837 1,8342421,599 1,965 1,9625082,245 2,008 1,9955601,698 26,659 9,128 2,400 2,310.cmrc2018devtrain.
dureaderdevtrain.
1635506521,021.
965916971,534.
3976167091,678.
895546921,167.
2435075633,153.
6561,8212,455.
405626736950.
421666740989.
426771840961.
423745827970.
182567667854.drcddev.
train.
train.
train.
dev.
dev.
c3.
table 9: chinese datasets statistics..use a modiﬁed cross entropy loss (clark and gard-ner, 2017) assuming that each segment contains atleast one correct answer span.
the ﬁnal predictionfor each question is a span with the maximum sumof start and end logit across multiple segments..hotpotqa is a qa dataset where goldenspans of an answer and sentence-level supportingfacts are provided.
thus, it contains two tasksnamely, answer span prediction and support-ing facts prediction.
in the distractor setting,each question is associated with 10 documentswhere only 2 documents contain supportingfacts.
it requires the model to ﬁnd and reasonover multiple documents to ﬁnd answers, andexplain the predicted answers using predictedsupporting facts.
following groeneveld et al.
(2020), we implemented a two-stage model basedon ernie-doc and use the following input“[cls] title1format for each segment:[p] sent1,1 [sep] sent1,2 [sep] ...title2 [p] sent2,1 [sep] sent2,2[q] question [/q]”[sep] ...forevidence prediction, we apply 2 layer feedforwardnetworks over the special token [sep] and [p]representing a sentence and a paragraph separately.
then we use binary cross entropy loss to do binaryclassiﬁcation.
for answer span prediction, we trainthe model with a multi-task objective: 1) questiontype (yes/no/span) classiﬁcation on the [cls]token.
2) supporting evidence prediction on [sep]and [p].
3) span prediction on the start and endtoken of a golden span..cmrc2018, drcd and dureader are com-mon chinese qa datasets with same format, whichhave been evaluated in numerous popular pretrain-.
ing models, such as bert (devlin et al., 2018),ernie 1.0 (sun et al., 2019b), ernie 2.0 (sunet al., 2020b) and etc.
the detailed descriptionsof three datasets can refer to cui et al.
(2018),shao et al.
(2018) and he et al.
(2017).
weadopt the same input format as triviaqa for eachsegment, denotes as “[cls] context [sep]question [sep]“ where context is generatedby slicing multi-documents input with a slidingwindow of 128 tokens.
we take as input multiplesegments of a sample sequentially and attach a lin-ear layer to each token in a segment to predict theanswer span.
then, we apply a softmax and usethe cross entropy loss with the correct answer.
theﬁnal prediction for each question is a span withthe maximum sum of start and end logit acrossmultiple segments..the multiple choice chinese machine readingcomprehension dataset (c3) (sun et al., 2019a)isthe ﬁrst chinese free-form multi-choicedataset where each question is associated withat most four choices and a single document.
according to (sun et al., 2019a), m segments areconstructed for a question, in which m denotesthe number of choice for that question.
weuse the following input format for each seg-ment: “[cls] context [sep] question[sep] choicei [sep] ” where contextisgenerated by slicing document input with a slidingwindow of 128 tokens stride.
we take as inputmultiple segments of a sample in a single batchand attach a linear layer to [cls] that outputsan unnormalized logit.
then we obtain the ﬁnalprediction for a question by applying a softmaxlayer over the unnormalized logits of all choices.
2925qa.
classiﬁcation.
models\dataset#0 ernie-doc#1 w/o so#2 w/o so&retro#3 w/o so&retro&en-rec#4 w/o so&retro&recur.
triviaqa hotpotqa imdb hyperpartisan avg.
73.6693.1472.8593.1592.5672.2769.7292.0764.8091.60.
50.8550.0449.8744.0531.54.
86.1084.6083.2781.6777.72.
64.5663.5963.3861.0958.35.table 10: performance of ernie-doc-small after ablating each proposed component.
(so denotes the segment-reordering objective, re denotes the retrospective feed mechanism, en-rec denotes the enhanced recurrence mech-anism, and recur denotes the segment-level recurrence module.
we used the acc.
metric for imdb, f1 metric fortriviaqa and hyperpartisan, joint-f1 for hotpotqa.).
associated with it..we.
extraction..keyphraseincludeopenkp (xiong et al., 2019) dataset 8 to evaluateernie-doc’s ability to extract keyphrasesfrom a long document.
each document containsup to three short keyphrases and we follow themodel setting of jointkpe (sun et al., 2020a) andetc (ainslie et al., 2020) by applying cnns onbert’s output to compose n-gram embeddings forclassiﬁcation.
we clean the dataset by removingsome nonsense words such as the http links.
indetail, we apply ﬁve cnns on bert’s output withthe kernel size ranging from 1 to 5. since eachword is composed of several sub-tokens, we takethe ﬁrst token’s embedding as the input for cnns.
finally, we use the binary cross entropy loss as theoptimization objective..a.2 ablation studies.
tab.
10 shows the performance of ernie-doc-small on english tasks after ablating each proposedcomponent.
all models were pretrained and ﬁne-tuned with the same experimental setup, and wereport the mean results of ﬁve runs.
in the last col-umn in tab.
10, we see that the segment-reorderingobjective is improved ernie-doc by 0.81% onaverage (#1 - #0), the retrospective feed mecha-nism is improved ernie-doc by an average of0.58% (#2 - #1), and the enhanced recurrence mech-anism makes a large contribution of 2.55 percent-age points on average (#3 - #2).
by comparing #3with #4, we see that segment-level recurrence isnecessary for modeling long documents and pro-duces a 4.92 percentage point improvement on av-erage.
considering different types of tasks, weobserve that on hyperpartisan, an extremely longtext classiﬁcation dataset, a substantial improve-ment is achieved using the segment-reordering ob-.
8the dataset can be downloaded from https://.
github.com/thunlp/bert-kpe.
jective (1.5% point).
this indicates that the [cls]token, pretrained using the segment-reordering ob-jective, is more adaptable to the document-leveltext classiﬁcation task.
moreover, we observed astable performance gain across all tasks using theenhanced recurrence mechanism..a.3 hyperparameters for language.
modeling.
in tab.
11, we present the detailed hyperparametersused for our experiments, which are the same asthe hyperparameters employed in transformer-xl(dai et al., 2019)..hyperparameters.
layershidden sizeattention headstraining sequence lengthtraining memory lengthtesting sequence lengthtesting sequence lengthbatch sizelearning ratewarmup stepstraining steps.
wikitext-103base164101015015064640642.5e-40200k.
wikitext-103large181,024163843841281,6001282.5e-416,000400k.
table 11: hyperparameters used for wikitext-103..a.4 hyperparameters for pre-training.
as shown in tab.
12, we present the detailedhyperparameters adopted to pretraining ernie-doc on english text corpora and chinese text cor-pora.
for comparisons, we follow the same op-timization hyperparameters of robertabase orrobertalarge (liu et al., 2019) for base-size orlarge-size model in english domain.
as for chineseernie-doc, we follow the same optimization hy-perparameters of ernie 2.0base..a.5 hyperparameters for fine-tuning.
a.5.1 long text classiﬁcation tasksthe ﬁnetuning hyperparameters for imdb (maaset al., 2011) and hyperpartisan (kiesel et al., 2019).
2926hyperparameters.
english.
layershidden sizeattention headstraining stepsbatch sizelearning ratewarmup stepsadam (beta1,beta2)adam (epsilon)learning rate scheduleweight decaydropoutgpu (nvidia v100).
base1276812400k2,5601e-44,000(0.9, 0.999)1e-6linear0.010.140.large241,02416100k3,9201e-44,000(0.9, 0.999)1e-6linear0.010.180.chinesebase1276812300k2,5601e-44,000(0.9, 0.999)1e-6linear0.01040.table 12: hyperparameters used for ernie-doc pre-training..are presented in tab.
13..hyperparameters.
base.
large.
imdb hyp imdb hyp.
batch sizelearning rateepochslr schedulelayerwise lr decaywarmup proportionweight decay.
327e-53linear10.10.01.
321e-415linear0.70.10.01.
321e-53linear0.90.10.01.
164e-615linear10.10.01.table 13: hyperparameters used for ﬁnetuning onimdb and hyperpartisan (hyp)..a.5.2 document-level question answering.
tasks.
the ﬁnetuning hyperparameters for triviaqa(welbl et al., 2018) and hotpotqa (yang et al.,2018) are presented in tab.
14. hqa-sent.
is themodel for coarse-grained evidence prediction, andwe choose the evidence with the probability largerthan a pre-deﬁned threshold 1e-3 and 1e-5 for baseand large models, respectively.
hqa-span.
is themodel for span prediction..a.5.3 keyphrase extraction task.
the ﬁnetuning hyperparameters for the openkp(xiong et al., 2019) dataset are presented in tab.
15..base.
large.
tqa hqa-sent.
hqa-span.
tqa hqa-sent.
hqa-span..hyper..batch sizelearning rateepochslr schedulelayer-decaywarmup prop.
weight decay.
643e-55linear0.80.10.01.
1283e-56linear10.10.01.
1281.5e-46linear0.80.10.01.
645e-63linear0.90.10.01.
325e-65linear0.90.10.01.
321.5e-55linear0.90.10.01.table 14: finetuning hyperparameters on the tqa andhqa for base- and large-size ernie-doc..hyperparametersbatch sizelearning rateepochslr schedulelayerwise lr decaywarmup proportionweight decay.
openkp321.5e-45linear0.80.10.01.table 15: finetuning hyperparameters on the openkpfor base-size ernie-doc..a.5.4 chinese nlu taskstab.
16 lists the ﬁnetuning hyperparameters forchinese nlu tasks including iflytek (xu et al.,2020), thucnews (sun et al., 2016), cmrc2018(cui et al., 2018), drcd (shao et al., 2018),dureader he et al.
(2017), c3 (sun et al., 2019a)and cail2019-scm (xiao et al., 2019)..epochs dropout.
tasks.
drcdcmrc2018dureaderc3cailthuifk.
batchsize64646424481616.learningrate2.25-41.75e-42.75e-41e-45e-51.5e-41.5e-4.
555815165.
0.10.20.10.10.10.10.1.table 16: hyperparameters used for ﬁnetuning on chi-nese nlu tasks.
note that the warmup proportion areset to 0.1 and the layerwise learning rate decay rate areset to 0.8 for all tasks..b attention complexity.
given a long document with length l, longformerand bigbird usually applies a local attention witha window size of 512 tokens on the entire inputresulting in l ∗ 512 token-to-token calculations.
while the long document is fed twice as input andeach input is sliced with a sliding window sizeof 512 tokens in ernie-doc, which resultingin 2 ∗ l512 ∗ 512 ∗ (512 + m) token-to-token cal-culations where m is the memory length.
since512 (cid:28) l and m (cid:28) l, the attention complexityof ernie-doc is comparable to longformer andbigbird which scales linearly with respect to theinput length l, i.e., o(l).
notably, the segmentsproduced from the long document are fed one byone in ernie-doc, leading to the lower spatialcomplexity..2927