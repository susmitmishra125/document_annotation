learning span-level interactions for aspect sentiment triplet extraction.
lu xu* 1, 2, yew ken chia* 1, 2, lidong bing21 singapore university of technology and design2 damo academy, alibaba group{xu lu,yewken chia}@mymail.sutd.edu.sgl.bing@alibaba-inc.com.
abstract.
aspect sentiment triplet extraction (aste)is the most recent subtask of absa whichoutputs triplets of an aspect target, its asso-ciated sentiment, and the corresponding opin-ion term.
recent models perform the tripletextraction in an end-to-end manner but heav-ily rely on the interactions between each tar-get word and opinion word.
thereby, theycannot perform well on targets and opinionswhich contain multiple words.
our proposedspan-level approach explicitly considers theinteraction between the whole spans of tar-gets and opinions when predicting their sen-timent relation.
thus,it can make predic-tions with the semantics of whole spans, en-suring better sentiment consistency.
to easethe high computational cost caused by spanenumeration, we propose a dual-channel spanpruning strategy by incorporating supervisionfrom the aspect term extraction (ate) andopinion term extraction (ote) tasks.
thisstrategy not only improves computational efﬁ-ciency but also distinguishes the opinion andtarget spans more properly.
our framework si-multaneously achieves strong performance forthe aste as well as ate and ote tasks.
inparticular, our analysis shows that our span-level approach achieves more signiﬁcant im-provements over the baselines on triplets withmulti-word targets or opinions.
1.
1.introduction.
aspect-based sentiment analysis (absa) (liu,2012; pontiki et al., 2014) is an aggregation of sev-eral ﬁne-grained sentiment analysis tasks, and itsvarious subtasks are designed with the aspect tar-get as the fundamental item.
for the example in.
∗ equal contribution.
lu xu and yew ken chia areunder the joint phd program between alibaba and singaporeuniversity of technology and design..1we make our code publicly available at https://.
github.com/chiayewken/span-aste..figure 1: an example of aste.
the spans highlightedin orange are target terms, and the span in blue is opin-ion term.
the “-” on top of target terms indicates nega-tive sentiment..figure 1, the aspect targets are “windows 8” and“touchscreen functions”.
aspect sentiment classi-ﬁcation (asc) (dong et al., 2014; zhang et al.,2016; yang et al., 2017; li et al., 2018a; tang et al.,2019) is one of the most well-explored subtasks ofabsa and aims to predict the sentiment polarityof a given aspect target.
however, it is not alwayspractical to assume that the aspect target is pro-vided.
aspect term extraction (ate) (yin et al.,2016; li et al., 2018b; ma et al., 2019) focuseson extracting aspect targets, while opinion termextraction (ote) (yang and cardie, 2012; klingerand cimiano, 2013; yang and cardie, 2013) aims toextract the opinion terms which largely determinethe sentiment polarity of the sentence or the cor-responding target term.
aspect sentiment tripletextraction (aste) (peng et al., 2019) is the mostrecently proposed subtask of absa, which formsa more complete picture of the sentiment informa-tion through the triplet of an aspect target term, thecorresponding opinion term, and the expressed sen-timent.
for the example in figure 1, there are twotriplets: (“windows 8”, “not enjoy”, negative) and(“touchscreen functions”, “not enjoy”, negative).
the initial approach to aste (peng et al., 2019)was a two-stage pipeline.
the ﬁrst stage extractstarget terms and their sentiments via a joint labelingscheme 2, as well as the opinion terms with stan-.
2for example, the joint tag “b-pos” denotes the beginning.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4755–4766august1–6,2021.©2021associationforcomputationallinguistics4755--dard bioes 3 tags.
the second stage then couplesthe extracted target and opinion terms to determinetheir paired sentiment relation.
we know that inabsa, the aspect sentiment is mostly determinedby the opinion terms expressed on the aspect target(qiu et al., 2011; yang and cardie, 2012).
how-ever, this pipeline approach breaks the interactionwithin the triplet structure.
moreover, pipeline ap-proaches usually suffer from the error propagationproblem..recent end-to-end approaches (wu et al., 2020;xu et al., 2020b; zhang et al., 2020) can jointlyextract the target and opinion terms and classifytheir sentiment relation.
one drawback is that theyheavily rely on word-to-word interactions to pre-dict the sentiment relation for the target-opinionpair.
note that it is common for the aspect tar-gets and opinions to contain multiple words, whichaccounts for roughly one-third of triplets in thebenchmark datasets.
however, the previous meth-ods (wu et al., 2020; zhang et al., 2020) predict thesentiment polarity for each word-word pair inde-pendently, which cannot guarantee their sentimentconsistency when forming a triplet.
as a result,this prediction limitation on triplets that containmulti-word targets or opinions inevitably hurts theoverall aste performance.
for the example infigure 1, by only considering the word-to-word in-teractions, it is easy to wrongly predict that “enjoy”expresses a positive sentiment on “windows”.
xuet al.
(2020b) proposed a position-aware taggingscheme to allow the model to couple each word ina target span with all possible opinion spans, i.e.,aspect word to opinion span interactions (or viceversa, aspect span to opinion word interactions).
however, it still cannot directly model the span-to-span interactions between the whole target spansand opinion spans..in this paper, we propose a span-based modelfor aste (span-aste), which for the ﬁrst time di-rectly captures the span-to-span interactions whenpredicting the sentiment relation of an aspect tar-get and opinion pair.
of course, it can also con-sider the single-word aspects or opinions properly.
our model explicitly generates span representa-tions for all possible target and opinion spans, andtheir paired sentiment relation is independentlypredicted for all possible target and opinion pairs.
span-based methods have shown encouraging per-.
of a target span with positive sentiment polarity..3a common tagging scheme for sequence labeling, denot-.
ing “begin, inside, outside, end and single” respectively..formance on other tasks, such as coreference res-olution (lee et al., 2017), semantic role labeling(he et al., 2018a), and relation extraction (luanet al., 2019; wadden et al., 2019).
however, theycannot be directly applied to the aste task due todifferent task-speciﬁc characteristics..our contribution can be summarized as follows:.
• we tailor a span-level approach to explic-itly consider the span-to-span interactions forthe aste task and conduct extensive analy-sis to demonstrate its effectiveness.
our ap-proach signiﬁcantly improves performance,especially on triplets which contain multi-word targets or opinions..• we propose a dual-channel span pruning strat-egy by incorporating explicit supervision fromthe ate and ote tasks to ease the high com-putational cost caused by span enumerationand maximize the chances of pairing validtarget and opinion candidates together..• our proposed span-aste model outperformsthe previous methods signiﬁcantly not onlyfor the aste task, but also for the ate andote tasks on four benchmark datasets withboth bilstm and bert encoders..2 span-based aste.
2.1 task formulation.
let x = {x1, x2, ..., xn} denote a sentence of ntokens, let s = {s1,1, s1,2, ..., si,j, ..., sn,n} bethe set of all possible enumerated spans in x, with iand j indicating the start and end positions of a spanin the sentence.
we limit the span length as 0 ≤ j −i ≤ l. the objective of the aste task is to extractall possible triplets in x. each sentiment tripletis deﬁned as (target, opinion, sentiment) wheresentiment ∈ {p ositive, n egative, n eutral}..2.2 model architecture.
as shown in figure 2, span-aste consists of threemodules: sentence encoding, mention module, andtriplet module.
for the given example, the sentenceis ﬁrst input to the sentence encoding module toobtain the token-level representation, from whichwe derive the span-level representation for eachenumerated span, such as “did not enjoy”, “win-dows 8”.
we then adopt the ate and ote tasksto supervise our proposed dual-channel span prun-ing strategy which obtains the pruned target and.
4756figure 2: span-aste model structure..opinion candidates, such as “windows 8” and “notenjoy” respectively.
finally, each target candidateand opinion candidate are coupled to determine thesentiment relation between them..2.2.1 sentence encodingwe explore two encoding methods to obtain thecontextualized representation for each word in asentence: bilstm and bert..bilstm we ﬁrst obtain the word repre-sentations {e1, e2, ..., ei, ..., en} from the 300-dimension pre-trained glove (pennington et al.,2014) embeddings which are then contextualizedby a bidirectional lstm (hochreiter and schmid-huber, 1997) layer.
the ith token is represented as:.
hi = [.
−→hi;.
←−hi].
(1).
−→hi and.
whereward and backward lstms respectively..←−hi are the hidden states of the for-.
bert an alternative encoding method is to usea pre-trained language model such as bert (de-vlin et al., 2019) to obtain the contextualized wordrepresentations x = [x1, x2, ..., xn].
for wordsthat are tokenized as multiple word pieces, we usemean pooling to aggregate their representations ..span representation we deﬁne each span rep-resentation si,j ∈ s as:.
(cid:40).
si,j =.
[hi; hj; fwidth(i, j)][xi; xj; fwidth(i, j)].
if bilstmif bert.
(2).
where fwidth(i, j) produces a trainable feature em-bedding representing the span width (i.e., j − i + 1)..besides the concatenation of the start token, endtoken, and width representations, the span repre-sentation si,j can also be formed by max-poolingor mean-pooling across all token representationsof the span from position i to j. the experimentalresults can be found in the ablation study..2.2.2 mention moduleate & ote tasks we employ the absa sub-tasks of ate and ote to guide our dual-channelspan pruning strategy through the scores of thepredicted opinion and target span.
note thatthe target terms and opinion terms are not yetpaired together at this stage.
the mention mod-ule takes the representation of each enumeratedspan si,j as input and predicts the mention typesm ∈ {t arget, opinion, invalid}..p (m|si,j) = softmax(ffnnm(si,j)).
(3).
where ffnn denotes a feed-forward neural net-work with non-linear activation..pruned target and opinion for a sentence xof length n, the number of enumerated spans iso(n2), while the number of possible pairs betweenall opinion and target candidate spans is o(n4) atthe later stage (i.e., the triplet module).
as such,it is not computationally practical to consider allpossible pairwise interactions when using a span-based approach.
previous works (luan et al., 2019;wadden et al., 2019) employ a pruning strategy toreduce the number of spans, but they only prune thespans to a single pool which is a mix of differentmention types.
this strategy does not fully consider.
4757pp-span enumerator-all consecutive subsequences <= 8 words-span extractor-concat(start token (768), end token (768), width (20))-[bs, num_spans, 1556]-mention head-feedforward (1556 -> 150 -> 150 -> 3) {opinion, target, null}-crossentropyloss-relation head-pruner-feedforward (1556 -> 150 -> 150 -> 1)-keep top-k span embeds-k = sequence_length * 0.5-[bs, top_k, 1556]-pairwise embeds: -concat(span_a (1556), span_b (1556), multiply (1556)) -[bs, top_k, top_k, 4668]-feedforward (4668 -> 150 -> 150 -> 4) {pos, neg, neutral, null}-crossentropyloss-new relation head-opinion pruner -[bs, top_k, 1556]-mention loss-target pruner-[bs, top_k, 1556]-mention loss-pairwise embeds:-restrict to opinion-target pairsspan enumeratorbertspan embed extractormentionheadrelation headfinal lossinput sequence[bs=1, seq_len][bs, seq_len, 768][bs, num_spans, 2][bs, num_spans, 1556]weight=1.0weight=0.2opinion startopinion endtargetstarttarget end+pruned targetspruned opinionsφtripledfddfdlegend++++the koreandishesaretastybutcostly.the korean dishestastycostly.+++++contextual encoding (eq.
2)span enumerationthe korean dishes++are tasty but costlythe, tastythe, costlykorean dishes, tastykorean dishes, costlytarget & opinion pruning(eq.
3)<opinion><opinion><null><null>auxiliary span classification (eq.
10)<null><pos><neg><null>pair enumerationsentiment triplet classification(eq.
6, 7, 8)<null><target><null>loss(eq.
11)element-wise addition of mention scoreelement-wise addition of mention scoreconcatenate operationlegend+distance embeddingffnnffnn<null><opinion><target>the koreandishesaretastybutcostly.the korean dishestastycostly.the korean dishesare tasty but costly+++++++ffnnthecostlytastykorean disheskorean dishes, tastykorean dishes, costlykorean dishes, tastykorean dishes, costlythe ++++the koreandishesaretastyandcheapkorean dishestastycheap++++contextual encoding (eq.
2)span enumerationdishes++are tasty and cheapdishes, tastydishes, cheapkorean dishes, tastykorean dishes, cheaptwo-way span pruning(eq.
3)<positive><negative><neutral>opinion-target pair enumerationsentiment triplet classification(eq.
6, 7, 8)loss(eq.
11)concatenate operationlegend+distance embeddingffnnffnndisheskorean dishes<invalid>tastycheaptheare tasty and cheaptop target candidatestop opinion candidatesinvalid candidatessupervision by opinion and target co-extraction subtasksspan boundary embeddingcontextual embeddingspan width embeddingdisheskorean dishestastycheaptheare tasty and cheaptop target candidatestop opinion candidatesinvalid candidates++++++++kkk+legendffnnspan width embeddingconcatenate operation+start-end embeddingcontextual embeddingdistance embeddingtop-k select operationffnn+++++++++++++fffffffffffffp+fdataset.
rest 14.lap 14.rest 15.rest 16.
#s, # +, # 0, # -, #sw #mw #s, # +, # 0, # -, #sw #mw #s, # +, # 0, # -, #sw #mw #s, # +, # 0, # -, #sw #mw.
train 1266 1692 166 480 1586dev388404test657773.
54 11966 155.
310492.
752 906 817 126 517 824189 219 169 36 141 190337 328 364 63 116 291.
636 605 783 25 205156 148 185 1153252 322 317 25 143.
678165297.
50 329.
335 857 101584 210 252 1129188 326.
407.
91876 21678 344.
476123170.table 1: statistics of datasets.
#s denotes the number of sentences.
# +, # 0, and # - denote the numbers of positive,neutral, and negative sentiment triplets respectively.
#sw denotes the number of triplets where both target andopinion terms are single-word spans.
#mw denotes the number of triplets where at least one of the target oropinion terms are multi-word spans..the structure of an aspect sentiment triplet as it doesnot recognize the fundamental difference betweena target and an opinion term.
hence, we propose touse a dual-channel pruning strategy which resultsin two separate pruned pools of aspects and opin-ions.
this minimizes computational costs whilemaximizing the chance of pairing valid opinionand target spans together.
the opinion and targetcandidates are selected based on the scores of themention types for each span based on equation 3:.
the span pair representation gst.
sentiment relation classiﬁer then, we in-toputa feed-forward neural network to determinerelation r ∈the probability of sentimentr = {p ositive, n egative, n eutral, invalid}between the target st.a,b and the opinion so.
a,b,so.
c,d:.
c,d.
p (r|st.
a,b, so.
c,d) = softmax(ffnnr(gst.))
(6)invalid here indicates that the target and opinionpair has no valid sentiment relationship..a,b,so.
c,d.
φtarget(si,j) = p (m = target|si,j)φopinion(si,j) = p (m = opinion|si,j).
(4).
2.3 training.
we use the mention scores φtarget and φopinionto select the top candidates from the enumer-ated spans and obtain the target candidate poolst = {..., sta,b, ...} and the opinion candidate poolso = {..., soc,d, ...} respectively.
to consider a pro-portionate number of candidates for each sentence,the number of selected spans for both pruned targetand opinion candidates is nz, where n is the sen-tence length and z is a threshold hyper-parameter.
note that although the pruning operation preventsthe gradient ﬂow back to the ffnn in the mentionmodule, it is already receiving supervision fromthe ate and ote tasks.
hence, our model can betrained end-to-end without any issue or instability..2.2.3 triplet moduletarget opinion pair representation we ob-tain the target-opinion pair representation by cou-a,b ∈ stpling each target candidate representation stwith each opinion candidate representation soc,d ∈so:.
gst.
a,b,so.
c,d.
= [st.a,b; so.
c,d; fdistance(a, b, c, d)].
(5).
where fdistance(a, b, c, d) produces a trainablefeature embedding based on the distance (i.e.,min(|b − c|, |a − d|)) between the target and opin-ion spans, following (lee et al., 2017; he et al.,2018a; xu et al., 2020b)..the training objective is deﬁned as the sum of thenegative log-likelihood from both the mention mod-ule and triplet module..l = −.
log p (m∗.
i,j|si,j).
(cid:88).
si,j ∈s.
(cid:88).
−.
sta,b∈st,so.
c,d∈so.
log p (r∗|st.
a,b, so.
c,d).
(7).
where m∗i,j is the gold mention type of the span si,j,and r∗ is the gold sentiment relation of the targetand opinion span pair (stc,d).
s indicates theenumerated span pool; st and so are the prunedtarget and opinion span candidates..a,b, so.
3 experiment.
3.1 datasets.
our proposed span-aste model is evaluated onfour aste datasets released by xu et al.
(2020b),which include three datasets in the restaurant do-main and one dataset in the laptop domain.
theﬁrst version of the aste datasets are releasedby peng et al.
(2019).
however, it is found thatnot all triplets are explicitly annotated (xu et al.,2020b; wu et al., 2020).
xu et al.
(2020b) reﬁnedthe datasets with the missing triplets and removedtriplets with conﬂicting sentiments.
note that these.
4758model.
mtslb.i.cmla+ (wang et al., 2017)†rinante+ (dai and song, 2019)†li-uniﬁed-r (li et al., 2019)†peng et al.
(2019)†zhang et al.
(2020) ∗gts (wu et al., 2020)∗jeto.
m =6 (xu et al., 2020b)†.
39.1831.4241.0443.24.
62.7066.1361.50.
47.1339.3867.3563.66.
57.1057.9155.13.rest 14.lap 14.rest 15.rest 16.p..r..p..r..p..r..p..r..f1.
42.7934.9551.0051.46.
59.7161.7358.14.
30.0921.7140.5637.38.
49.6253.3553.03.
36.9218.6644.2850.38.
41.0740.9933.89.f1.
33.1620.0742.3442.87.
44.7846.3141.35.
34.5629.8844.7248.07.
55.6360.1064.37.
39.8430.0651.3957.51.
42.5146.8944.33.f1.
37.0129.9747.8252.32.
47.9452.6652.50.
41.3425.6837.3346.96.
60.9563.2870.94.
42.1022.3054.5164.24.
53.3558.5657.00.f1.
41.7223.8744.3154.21.
56.8260.7963.21.span-aste (ours)t gts (wu et al., 2020)∗reb.m =6 (xu et al., 2020b) †.
jeto.
span-aste (ours).
72.52.
62.43.
67.08.
59.85.
45.67.
51.80.
64.29.
52.12.
57.56.
67.25.
61.75.
64.37.
67.7670.56.
67.2955.94.
67.5062.40.
57.8255.39.
51.3247.33.
54.3651.04.
62.5964.45.
57.9451.96.
60.1557.53.
66.0870.42.
69.9158.37.
67.9363.83.
72.89.
70.89.
71.85.
63.44.
55.84.
59.38.
62.18.
64.45.
63.27.
69.45.
71.17.
70.26.table 2: results on the test set of the aste task.
†: the results are retrieved from xu et al.
(2020b).
∗: for afair comparison, we reproduce the results using their released implementation code and conﬁguration on the sameaste datasets released by xu et al.
(2020b)..four benchmark datasets are derived from the se-meval challenge (pontiki et al., 2014, 2015, 2016),and the opinion terms are retrieved from (fan et al.,2019).
table 1 shows the detailed statistics..3.2 experiment settings.
when using the bilstm encoder, the pre-trainedglove word embeddings are trainable.
the hid-den size of the bilstm encoder is 300 and thedropout rate is 0.5. in the second setting, we ﬁne-tune the pre-trained bert (devlin et al., 2019) toencode each sentence.
speciﬁcally, we use the un-cased version of bertbase.
the model is trainedfor 10 epochs with a linear warmup for 10% ofthe training steps followed by a linear decay ofthe learning rate to 0. we employ adamw as theoptimizer with the maximum learning rate of 5e-5for transformer weights and weight decay of 1e-2.
for other parameter groups, we use a learning rateof 1e-3 with no weight decay.
the maximum spanlength l is set as 8. the span pruning thresholdz is set as 0.5. we select the best model weightsbased on the f1 scores on the development set andthe reported results are the average of 5 runs withdifferent random seeds.
4.
3.3 baselines.
the baselines can be summarized as two groups:pipeline methods and end-to-end methods..opinion terms with bioes tags at the ﬁrst stage.
atthe second stage, the extracted targets and opinionsare then paired to determine if they can form a validtriplet.
note that these approaches employ differentmethods to obtain the features for the ﬁrst stage.
cmla+ (wang et al., 2017) employs an attentionmechanism to consider the interaction between as-pect terms and opinion terms.
rinante+ (daiand song, 2019) adopts a bilstm-crf modelwith mined rules to capture the dependency rela-tions.
li-uniﬁed-r (li et al., 2019) uses a uniﬁedtagging scheme to jointly extract the aspect termand associated sentiment.
peng et al.
(2019) in-cludes dependency relation information when con-sidering the interaction between the aspect andopinion terms..end-to-end the end-to-end methods aim tojointly extract full triplets in a single stage.
pre-vious work by zhang et al.
(2020) and wu et al.
(2020) independently predict the sentiment relationfor all possible word-word pairs, hence they requiredecoding heuristics to determine the overall senti-ment polarity of a triplet.
jet (xu et al., 2020b)models the aste task as a structured predictionproblem with a position-aware tagging scheme tocapture the interaction of the three elements in atriplet..3.4 experiment results.
pipeline for the pipeline approaches listed be-low, they are modiﬁed by peng et al.
(2019) toextract the aspect terms together with their asso-ciated sentiments via a joint labeling scheme, and.
4see appendix for more experimental settings, and also.
the dev results on the four datasets..table 2 compares span-aste with previous mod-els in terms of precision (p.), recall (r.), andf1 scores on four datasets.
under the f1 metric,our model consistently outperforms the previousworks for both bilstm and bert sentence en-coders.
in most cases, our model signiﬁcantly out-.
4759performs other end-to-end methods in both preci-sion and recall.
we also observe that the two strongpipeline methods (li et al., 2019; peng et al., 2019)achieved competitive recall results, but their overallperformance is much worse due to the low preci-sion.
speciﬁcally, using the bilstm encoder withglove embedding, our model outperforms the bestpipeline model (peng et al., 2019) by 15.62, 8.93,5.24, and 10.16 f1 points on the four datasets.
thisresult indicates that our end-to-end approach can ef-fectively encode the interaction between target andopinion spans, and also alleviates the error propa-gation.
in general, the other end-to-end methodsare also more competitive than the pipeline meth-ods.
however, due to the limitations of relying onword-level interactions, their performances are lessencouraging in a few cases, such as the results onlap 14 and rest 15. with the bert encoder, allthree end-to-end models achieve much stronger per-formance than their lstm-based versions, whichis consistent with previous ﬁndings (devlin et al.,2019).
our approach outperforms the previous bestresults gts (wu et al., 2020) by 4.35, 5.02, 3.12,and 2.33 f1 points on the four datasets..3.5 additional experiments.
as mentioned in section 2.2.2, we employ theabsa subtasks of ate and ote to guide our spanpruning strategy.
to examine if span-aste caneffectively extract target spans and opinion spans,we also evaluate our model on the ate and otetasks on the four datasets.
table 3 shows the com-parisons of our approach and the previous methodgts (wu et al., 2020).
5 without additional retrain-ing or tuning, our model can directly address theate and ote tasks, with signiﬁcant performanceimprovement than gts in terms of f1 scores onboth tasks.
even though gts shows a better re-call score on the rest 16 dataset, the low precisionscore results in worse f1 performance.
the betteroverall performance indicates that our span-levelmethod not only beneﬁts the sentiment triplet ex-traction, but also improves the extraction of targetand opinion terms by considering the semantics ofeach whole span rather than relying on decodingheuristics of tagging-based methods..5see appendix for the target and opinion data statistics.
note that the jet model (xu et al., 2020b) is not able todirectly solve the ate and ote tasks unless the evaluation isconducted based on the triplet predictions.
we include suchcomparisons in the appendix..dataset model.
rest 14.lap 14.rest 15.rest 16.gtsours.
gtsours.
gtsours.
gtsours.
ate.
r..85.6487.59.
82.6886.39.
81.5784.68.
89.4288.50.p..78.1283.56.
76.6381.48.
75.1378.97.
75.0679.78.p..r..f1.
81.6985.50.
79.5383.86.
78.2181.72.
81.6183.89.
81.1282.93.
76.1183.00.
74.9677.36.
78.9982.59.ote.
88.2489.67.
78.4482.28.
82.5284.86.
88.7190.91.f1.
84.5386.16.
77.2582.63.
78.4980.93.
83.5786.54.table 3: test results on the ate and ote tasks withbert encoder.
for reference, we include the resultsof the racl framework (chen and qian, 2020) in theappendix.
racl is the current state-of-the-art methodfor both tasks.
however, their framework does not con-sider the pairing relation between each target and opin-ion, therefore it is difﬁcult to have a completely faircomparison..4 analysis.
4.1 comparison of single-word and.
multi-word spans.
we compare the performance of span-aste withthe previous model gts (wu et al., 2020) for thefollowing two settings in table 4: single-word:both target and opinion terms in a triplet are single-word spans, multi-word: at least one of the targetor opinion terms in a triplet is a multi-word span.
for the single-word setting, our method shows con-sistent improvement in terms of both precisionand recall score on the four datasets, which re-sults in the improvement of f1 score.
when wecompare the evaluations for multi-word triplets,our model achieves more signiﬁcant improvementsfor f1 scores.
compared to precision, our recallshows greater improvement over the gts approach.
gts heavily relies on word-pair interactions to ex-tract triplets, while our methods explicitly considerthe span-to-span interactions.
our span enumera-tion also naturally beneﬁts the recall of multi-wordspans.
for both gts and our model, multi-wordtriplets pose challenges and their f1 results drop bymore than 10 points, even more than 20 points forrest 14. as shown in table 1, comparing with thesingle-word triplets, multi-word triplets are com-mon and account for one-third or even half of thedatasets.
therefore, a promising direction for fu-ture work is to further improve the model’s perfor-mance on such difﬁcult triplets..to identify further areas for improvement, weanalyze the results for the aste task based onwhether each sentiment triplet contains a multi-.
4760mode.
model.
treb.single-word.
multi-word.
gtsours∆.
gtsours∆.
rest 14.lap 14.rest 15.rest 16.p..r..f1.
p..r..f1.
p..r..f1.
p..r..f1.
79.1579.60.
66.5565.4774.9379.1270.2368.09+4.19 +0.46 +2.38 +2.62 +3.44 +3.04 +3.68.
63.9767.02.
62.5465.98.
76.9879.36.
65.6670.71+5.05.
69.6671.66.
73.0366.1070.4774.65+4.37 +2.00 +1.16 +1.62.
76.7477.91.
49.2655.79.
56.8555.9562.9761.64+4.79 +6.53 +5.78 +2.37 +3.17 +2.90 +0.42 +10.11 +5.10 +5.80 +8.24 +7.02.
46.1249.02.
48.7753.87.
52.2654.63.
41.2744.44.
52.7858.57.
50.2850.70.
55.2963.53.
47.3457.45.
56.6362.43.table 4: analysis with different evaluation modes on the aste task..dataset model.
multi-word target multi-word opinion.
p..r..p..r..rest 14.lap 14.rest 15.rest 16.gtsours.
gtsours.
gtsours.
gtsours.
56.5465.96.
55.1156.99.
51.0955.33.
62.6966.43.
49.8157.62.
44.0948.18.
51.0960.58.
65.1272.09.f1.
52.9661.51.
48.9952.22.
51.0957.84.
63.8869.14.
50.6749.43.
37.5034.62.
43.4037.18.
28.2636.73.
41.7647.25.
26.0926.09.
35.9445.31.
24.0733.33.f1.
45.7848.31.
30.7729.75.
39.3240.85.
26.0034.95.table 5: further comparison of test results for ourmodel and gts based on triplets of multi-word targetsand opinions for the aste task..word target or multi-word opinion term.
from ta-ble 5, the results show that the performance is lowerwhen the triplet contains a multi-word opinion term.
this trend can be attributed to the imbalanced datadistribution of triplets which contain multi-wordtarget or opinion terms..4.2 pruning efﬁciency.
to demonstrate the efﬁciency of the proposeddual-channel pruning strategy, we also compareit to a simpler strategy, denoted as single-channel(sc) which does not distinguish between opin-ion and target candidates.
figure 3 shows thecomparisons.
note the mention module underthis strategy does not explicitly solve the ateand ote tasks as it only predicts mention labelm ∈ {v alid, invalid}, where v alid means thespan is either a target or an opinion span andinvalid means the span does not belong to thetwo groups.
given sentence length n and pruningthreshold z, the number of candidates is limited tonz, and hence the computational cost scales withthe number of pairwise interactions, n2z2.
thedual-channel strategy considers each target-opinionpair where the pruned target and opinion candidatepools both have nz spans.
note that it is possiblefor the two pools to share some candidates.
in com-parison, the single-channel strategy considers each.
60.
).
%.
(.
1f.40.
20.dual-channelsingle-channel (sc)sc-adjusted.
0.0 6 2 5.
0.1 2 5.
0.2 5.
0.5 0.
1.0.figure 3: dev results with respect to pruning thresholdz which intuitively refers to the number of candidatespans to keep per word in the sentence..target-opinion pair where the target and opinioncandidates are drawn from the same single poolof nz spans.
in order to consider at least as manytarget and opinion candidates as the dual-channelstrategy, the single-channel strategy has to scale thethreshold z by two, which leads to 4 times morepairs and computational cost.
we denote this set-ting in figure 3 as sc-adjusted.
when controllingfor computational efﬁciency, there is a signiﬁcantperformance difference between dual-channel andsingle-channel in f1 score, especially for lowervalues of z. although the performance gap narrowswith increasing z, it is not practical for high values.
according to our experimental results, we selectthe dual-channel pruning strategy with z = 0.5 forthe reported model..4.3 qualitative analysis.
to illustrate the differences between the models,we present sample sentences from the aste testset with the gold labels as well as predictions fromgts (wu et al., 2020) and span-aste in figure 4.for the ﬁrst example, gts correctly extracts the tar-get term “windows 8” paired with the opinion term“not enjoy”, but the sentiment is incorrectly pre-dicted as positive.
when forming the triplet, theirdecoding heuristic considers the sentiment inde-.
4761figure 4: qualitative analysis.
the target and opinion terms are highlighted in orange and blue respectively.
each arc indicates the pairing relation between target and opinion terms.
the sentiment polarity of each triplet isindicated above the target terms..pendently for each word-word pair: {(“windows”,“not”, neutral), (“8”, “not”, neutral), (“windows”,“enjoy”, positive), (“8”, “enjoy”, positive)}.
theirheuristic votes the overall sentiment polarity as themost frequent label among the pairs.
in the caseof a tie (2 neutral and 2 positive), the heuristichas a predeﬁned bias to assign the sentiment polar-ity to positive.
similarly, the word-level methodfails to capture the negative sentiment expressed by“not enjoy” on the other target term “touchscreenfunctions”.
in the second example, it incompletelyextracts the target term “korean dishes”, resultingin the wrong triplet.
for both examples, our methodis able to accurately extract the target-opinion pairsand determine the overall sentiment even wheneach term has multiple words..4.4 ablation study.
we conduct an ablation study to examine the per-formance of different modules and span representa-tion methods, and the results are shown in table 6.the average f1 denotes the average dev results ofspan-aste on the four benchmark datasets over5 runs.
similar to the observation for coreferenceresolution (lee et al., 2017), we ﬁnd that the asteperformance is reduced when removing the spanwidth and distance embedding.
this indicates thatthe positional information is still useful for theaste task as targets and opinions which are farapart or too long are less likely to form a valid spanpair.
as mentioned in section 2.2.1, we exploretwo other methods (i.e., max pooling and meanpooling) to form span representations instead ofconcatenating the span boundary token represen-tations.
the negative results suggest that usingpooling to aggregate the span representation is dis-advantageous due to the loss of information that isuseful for distinguishing valid and invalid spans..model.
full model.
w/o width & distance embeddingmax poolingmean pooling.
average f1 ∆f1.
67.6966.4566.0966.19.
-1.24-1.60-1.53.table 6: ablation study on the development sets..5 related work.
sentiment analysis is a major natural languageunderstanding (nlu) task (wang et al., 2019) andhas been extensively studied as a classiﬁcationproblem at the sentence level (raffel et al., 2020;lan et al., 2020; yang et al., 2020).
aspect-basedsentiment analysis (absa) (pontiki et al., 2014)addresses various sentiment analysis tasks at a ﬁne-grained level.
as mentioned in the section 1, thesubtasks mainly include asc (dong et al., 2014;zhang et al., 2016; chen et al., 2017; he et al.,2018b; li et al., 2018a; peng et al., 2018; wangand lu, 2018; he et al., 2019; li and lu, 2019;xu et al., 2020a), ate (qiu et al., 2011; yin et al.,2016; li et al., 2018b; ma et al., 2019), ote (huand liu, 2004; yang and cardie, 2012; klinger andcimiano, 2013; yang and cardie, 2013).
there isalso another subtask named target-oriented opin-ion words extraction (towe) (fan et al., 2019),which aim to extract the corresponding opinionwords for a given target term.
another line ofresearch focuses on addressing different subtaskstogether.
aspect and opinion term co-extraction(aote) aiming to extract the aspect and opinionterms together (wang et al., 2017; ma et al., 2019;dai and song, 2019) and is often treated as a se-quence labeling problem.
note that aote doesnot consider the paired sentiment relationship be-tween each target and opinion term.
end-to-endabsa (li and lu, 2017; ma et al., 2018; li et al.,2019; he et al., 2019) jointly extracts each aspect.
4762+++----+term and its associated sentiment in an end-to-endmanner.
a few other methods are recently pro-posed to jointly solve three or more subtasks ofabsa.
chen and qian (2020) proposed a relationaware collaborative learning framework to unifythe three fundamental subtasks and achieved strongperformance on each subtask and combined task.
while wan et al.
(2020) focused more on aspectcategory related subtasks, such as aspect categoryextraction and aspect category and target jointextraction.
aste (peng et al., 2019; wu et al.,2020; xu et al., 2020b; zhang et al., 2020) is themost recent development of absa and its aim isto extract and form the aspect term, its associatedsentiment, and the corresponding opinion term intoa triplet..6 conclusions.
in this work, we propose a span-level approach -span-aste to learn the interactions between targetspans and opinion spans for the aste task.
it canaddress the limitation of the existing approachesthat only consider word-to-word interactions.
wealso propose to include the ate and ote tasksas supervision for our dual-channel pruning strat-egy to reduce the number of enumerated target andopinion candidates to increase the computational ef-ﬁciency and maximize the chances of pairing validtarget and opinion candidates together.
our methodsigniﬁcantly outperforms the previous methods foraste as well as ate and ote tasks and our analy-sis demonstrates the effectiveness of our approach.
while we achieve strong performance on the astetask, the performance can be mostly attributed tothe improvement on the multi-word triplets.
asdiscussed in section 4.1, there is still a signiﬁcantperformance gap between single-word and multi-word triplets, and this can be a potential area forfuture work..references.
peng chen, zhongqian sun, lidong bing, and weiyang.
2017. recurrent attention network on mem-in proc.
ofory for aspect sentiment analysis.
emnlp..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proc.
of naacl..li dong, furu wei, chuanqi tan, duyu tang, mingzhou, and ke xu.
2014. adaptive recursive neuralnetwork for target-dependent twitter sentiment clas-siﬁcation.
in proc.
of acl..zhifang fan, zhen wu, xinyu dai, shujian huang, andjiajun chen.
2019. target-oriented opinion wordsextraction with target-fused neural sequence label-ing.
in porc.
of naacl..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthewpeters, michael schmitz, and luke s. zettlemoyer.
2017. allennlp: a deep semantic natural languageprocessing platform..luheng he, kenton lee, omer levy, and luke zettle-moyer.
2018a.
jointly predicting predicates and ar-in proc.
guments in neural semantic role labeling.
of acl..ruidan he, wee sun lee, hwee tou ng, and danieldahlmeier.
2018b.
effective attention modeling forin proc.
ofaspect-level sentiment classiﬁcation.
coling..ruidan he, wee sun lee, hwee tou ng, and danieldahlmeier.
2019. an interactive multi-task learningnetwork for end-to-end aspect-based sentiment anal-ysis.
in proc.
of acl..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural computation, 9:1735–80..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proc.
of acm sigkdd..r. klinger and p. cimiano.
2013. joint and pipelineprobabilistic models for ﬁne-grained sentiment anal-ysis: extracting aspects, subjective phrases and theirrelations.
in 2013 ieee 13th international confer-ence on data mining workshops..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learn-ing of language representations.
in proc.
of iclr..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference resolu-tion.
in proc.
of emnlp..zhuang chen and tieyun qian.
2020. relation-awarecollaborative learning for uniﬁed aspect-based senti-ment analysis.
in proc.
of acl..hao li and wei lu.
2017. learning latent sentimentscopes for entity-level sentiment analysis.
in proc.
of aaai..hongliang dai and yangqiu song.
2019. neural as-pect and opinion term extraction with mined rules asweak supervision.
in proc.
of acl..hao li and wei lu.
2019. learning explicit and im-plicit structures for targeted sentiment analysis.
inproc.
of emnlp..4763xin li, lidong bing, wai lam, and bei shi.
2018a.
transformation networks for target-oriented senti-ment classiﬁcation.
in proc.
of acl..xin li, lidong bing, piji li, and wai lam.
2019. auniﬁed model for opinion target extraction and targetsentiment prediction.
in proc.
of aaai..xin li, lidong bing, piji li, wai lam, and zhimouyang.
2018b.
aspect term extraction with historyin proc.
ofattention and selective transformation.
ijcai..bing liu.
2012. sentiment analysis and opinion min-ing.
synthesis lectures on human language technolo-gies, 5(1):1–167..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..jialong tang, ziyao lu, jinsong su, yubin ge, linfengsong, le sun, and jiebo luo.
2019. progressive self-supervised attention learning for aspect-level senti-ment analysis.
in proc.
of acl..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proc.
of emnlp..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-namic span graphs.
in proc.
of naacl..hai wan, yufei yang, jianfeng du, yanan liu, kunxunqi, and jeff z. pan.
2020. target-aspect-sentimentjoint detection for aspect-based sentiment analysis.
in proc.
of aaai..dehong ma, sujian li, and houfeng wang.
2018. jointlearning for targeted sentiment analysis.
in proc.
ofemnlp..dehong ma, sujian li, fangzhao wu, xing xie,and houfeng wang.
2019. exploring sequence-to-sequence learning in aspect term extraction.
in proc.
of acl..haiyun peng, yukun ma, yang li, and erik cam-bria.
2018. learning multi-grained aspect target se-quence for chinese sentiment analysis.
knowledge-based systems, 148:167–176..haiyun peng, lu xu, lidong bing, fei huang, wei lu,and luo si.
2019. knowing what, how and why:a near complete solution for aspect-based sentimentanalysis.
in proc.
of aaai..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in proc.
of emnlp..maria pontiki, dimitris galanis, haris papageor-giou, ion androutsopoulos, suresh manandhar, mo-hammed al-smadi, mahmoud al-ayyoub, yanyanzhao, bing qin, orph´ee de clercq, et al.
2016.semeval-2016 task 5: aspect based sentiment anal-ysis.
in proc.
of semeval..maria pontiki, dimitris galanis, haris papageorgiou,suresh manandhar, and ion androutsopoulos.
2015.semeval-2015 task 12: aspect based sentimentanalysis.
in proc.
of semeval..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2019.glue: a multi-task benchmark and analysis plat-in proc.
form for natural language understanding.
of iclr..bailin wang and wei lu.
2018. learning latent opin-in.
ions for aspect-level sentiment classiﬁcation.
proc.
of aaai..wenya wang, sinno jialin pan, daniel dahlmeier, andxiaokui xiao.
2017. coupled multi-layer attentionsinfor co-extraction of aspect and opinion terms.
proc.
of aaai..zhen wu, chengcan ying, fei zhao, zhifang fan,xinyu dai, and rui xia.
2020. grid tagging schemefor aspect-oriented ﬁne-grained opinion extraction.
in findings of emnlp..lu xu, lidong bing, wei lu, and fei huang.
2020a.
aspect sentiment classiﬁcation with aspect-speciﬁcopinion spans.
in proc.
of emnlp..lu xu, hao li, w. lu, and lidong bing.
2020b.
position-aware tagging for aspect sentiment tripletextraction.
in proc.
of emnlp..bishan yang and claire cardie.
2012. extracting opin-ion expressions with semi-markov conditional ran-dom ﬁelds.
in proc.
of emnlp..bishan yang and claire cardie.
2013. joint inferencefor ﬁne-grained opinion extraction.
in proc.
of acl..maria pontiki, dimitris galanis, john pavlopoulos,ion androutsopoulos, andharris papageorgiou,suresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in proc.
of semeval..min yang, wenting tu, jingxuan wang, fei xu, andxiaojun chen.
2017. attention based lstm for tar-get dependent sentiment classiﬁcation.
in proc.
ofaaai..guang qiu, bing liu, jiajun bu, and chun chen.
2011. opinion word expansion and target extrac-tion through double propagation.
computationallinguistics, 37(1):9–27..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v. le.
2020.xlnet: generalized autoregressive pretraining forlanguage understanding.
in proc.
of neurips..4764yichun yin, furu wei, li dong, kaimeng xu, mingzhang, and ming zhou.
2016. unsupervised wordand dependency path embeddings for aspect term ex-traction.
in proc.
of ijcai..chen zhang, qiuchi li, dawei song, and benyouwang.
2020. a multi-task learning framework foropinion triplet extraction.
in findings of emnlp..meishan zhang, yue zhang, and duy-tin vo.
2016.gated neural networks for targeted sentiment anal-ysis.
in proc.
of aaai..a additional experimental settings.
we run our model experiments on a nvidia teslav100 gpu, with cuda version 10.2 and pytorchversion 1.6.0. the average run time for bert-based model is 157 sec/epoch, 115 sec/epoch, 87sec/epoch, and 111 sec/epoch for rest 14, lap 14,rest 15, and rest 16 respectively.
the total numberof parameters is 2.24m when glove is used, and is110m when bert base is used.
the feed-forwardneural networks in the mention module and tripletmodule have 2 hidden layers and hidden size of 150.we use relu activation and dropout of 0.4 aftereach hidden layer.
we use xavier normal weightinitialization for the feed-forward parameters.
thespan width and distance embeddings have 20 and128 dimensions respectively.
their input valuesare bucketed (gardner et al., 2017) before beingfed to an embedding matrix lookup: [0, 1, 2, 3, 4,5-7, 8-15, 16-31, 32-63, 64+].
during training, themodel parameters are updated after each sentencewhich results in a batch size of 1. for each inputtext sequence, we restrict it to a maximum of 512tokens..b additional data statistics.
table 9 shows the number of target terms and opin-ion terms on the four datasets..c dev results.
table 10 shows the results of our model on thedevelopment datasets..d additional comparisons.
as mentioned by footnote 5 in section 3.5, we can-not make a direct comparison with the jet model(xu et al., 2020b), as it is not able to directly solvethe ate and ote tasks unless the evaluation isconducted based on the triplet results.
table 7shows such comparisons.
our proposed method.
dataset model.
rest 14.lap 14.rest 15.rest 16.m =6.
m =6.
m =6.
m =6.
jetogtsoursjetogtsoursjetogtsoursjetogtsours.
ate.
r..66.0481.4980.31.
68.0373.6575.38.
65.7474.7778.01.
68.5885.6286.06.p..83.2183.2586.20.
83.3382.1787.69.
83.0480.9581.60.
83.3382.6984.20.p..r..f1.
73.6482.3683.15.
74.9177.6881.07.
73.3877.7479.76.
75.2484.1385.12.
83.7686.5587.20.
77.1681.6385.61.
81.3380.9680.09.
89.4483.3784.62.ote.
77.2886.6584.54.
75.5374.0576.58.
68.9876.5781.13.
80.2186.5388.00.f1.
80.3986.6085.85.
76.3477.6680.84.
74.6578.7080.61.
84.5784.9286.28.table 7: test results on the ate and ote tasks withsub-optimal evaluation method.
our method and gts(wu et al., 2020) allow for ate and ote tasks to bepredicted independently from the aste task.
however,jetom =6 (xu et al., 2020b) does not.
hence, we makeanother comparison here by extracting all opinion andtarget spans from the aste predictions..dataset model.
p..p..r..rest 14.lap 14.rest 15.rest 16.gts78.50racl 79.9083.56ours.
gts78.63racl 78.1181.48ours.
gts74.95racl 75.2278.97ours.
gts75.05racl 74.1279.78ours.
ate.
r..87.3887.7487.59.
81.8681.9986.39.
82.4181.9484.68.
89.1689.2088.50.f1.
82.7083.6385.50.
80.2179.9983.86.
78.5078.4381.72.
81.5080.9583.89.
82.0780.2682.93.
76.2775.1283.00.
74.7576.4177.36.
78.3679.2582.59.ote.
88.9987.9989.67.
79.3279.9282.28.
81.5682.5684.86.
88.4289.7790.91.f1.
85.3983.9486.16.
77.7777.4382.63.
78.0179.3580.93.
83.0984.1786.54.table 8: additional comparison of test results on theate and ote tasks.
note that racl (chen andqian, 2020) does not consider supervision from target-opinion pairs, but it includes the sentiment polarities onthe target terms..generally outperforms the previous two end-to-endapproaches on the four datasets..as mentioned in table 3, it is challenging tomake a fair comparison between the previousabsa framework racl (chen and qian, 2020),which also address the ate and ote tasks whilesolving other absa subtasks, and our approach aswell as the gts (wu et al., 2020).
this is becausethe mentioned approaches have different task set-tings.
the racl considers the sentiment polarityon the target terms when solving the ate and otetasks, but gts and our method both consider thepairing relation between target and opinion terms.
however, for reference, table 8 shows the compar-.
4765dataset.
traindevtest.
rest 14.lap 14.rest 15.rest 16.
# target.
# opinion.
# target.
# opinion.
# target.
# opinion.
# target.
# opinion.
205115001848.
208615031854.
128112961463.
126813041474.
186212131432.
194112361461.
119812961452.
130713191475.table 9: additional statistics.
# target denotes the number of target terms.
# opinion denotes the numbers ofopinion terms..model.
rest 14.lap 14.rest 15.rest 16.p..r..f1.
p..r..f1.
p..r..f1.
p..r..f1.
ours (bilstm)ours (bert).
66.7668.05.
53.9065.65.
59.6166.80.
60.7863.35.
49.3758.90.
54.4561.02.
69.1370.16.
60.0871.41.
64.2670.75.
71.5972.52.
61.9571.92.
66.4172.19.table 10: results on the development datasets..isons of the three methods on the ate and otetasks on the datasets released by xu et al.
(2020b)..4766