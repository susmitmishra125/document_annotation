sent: sentence-level distant relation extraction via negative training.
ruotian ma1, tao gui2∗, linyang li1, qi zhang1∗, xuanjing huang1 and yaqian zhou11school of computer science, fudan university, shanghai, china2institute of modern languages and linguistics, fudan university, shanghai, china{rtma19,tgui16,linyangli19,qz,xjhuang,yqzhou}@fudan.edu.cn.
abstract.
distant supervision for relation extraction pro-vides uniform bag labels for each sentenceinside the bag, while accurate sentence labelsare important for downstream applications thatneed the exact relation type.
directly usingbag labels for sentence-leveltraining willintroduce much noise, thus severely degradingin this work, we propose theperformance.
use of negative training (nt),in which amodel is trained using complementary labelsregarding that “the instance does not belongto these complementary labels”.
since theprobability of selecting a true label as ais low, nt providescomplementary labelless noisy information.
thefurthermore,model trained with nt is able to separate thenoisy data from the training data.
based onnt, we propose a sentence-level framework,sent, for distant relation extraction.
sentnot only ﬁlters the noisy data to constructa cleaner dataset, but also performs a re-labeling process to transform the noisy datainto useful training data, thus further beneﬁt-ing the model’s performance.
experimental re-sults show the signiﬁcant improvement of theproposed method over previous methods onsentence-level evaluation and de-noise effect..1.introduction.
relation extraction (re), which aims to extractthe relation between entity pairs from unstructuredtext, is a fundamental task in natural languageprocessing.
the extracted relation facts can beneﬁtvarious downstream applications, e.g., knowledgegraph completion (bordes et al., 2013; wang et al.,2014), information extraction (wu and weld, 2010)and question answering (yao and van durme,2014; fader et al., 2014)..a signiﬁcant challenge for relation extraction isthe lack of large-scale labeled data.
thus, distant.
∗∗ corresponding authors..figure 1: two types of noise exist in bag-level labels:1) multi-label noise: the exact label (“place of birth”or “employee of”) for each sentence is unclear; 2)wrong-label noise:the third sentence inside the bagactually expresses “live in” which is not included in thebag labels..supervision (mintz et al., 2009) is proposed togather training data through automatic alignmentbetween a database and plain text.
such annotationparadigm results in an inevitable noise problem,which is alleviated by previous studies using multi-instance learning (mil).
in mil, the training andtesting processes are performed at the bag level,where a bag contains noisy sentences mentioningthe same entity pair but possibly not describing thesame relation.
studies using mil can be broadlyclassiﬁed into two categories: 1) the soft de-noisemethods that leverage soft weights to differentiatethe inﬂuence of each sentence (lin et al., 2016;han et al., 2018c; li et al., 2020; hu et al., 2019a;ye and ling, 2019; yuan et al., 2019a,b); 2) thehard de-noise methods that remove noisy sentencesfrom the bag (zeng et al., 2015; qin et al., 2018;han et al., 2018a; shang, 2019)..however, these bag-level approaches fail to mapeach sentence inside bags with explicit sentencelabels.
this problem limits the application of rein some downstream tasks that require sentence-level relation type, e.g., yao and van durme (2014)and xu et al.
(2016) use sentence-level relation ex-traction to identify the relation between the answerand the entity in the question.
therefore, severalstudies (jia et al.
(2019); feng et al.
(2018)) havemade efforts on sentence-level (or instance-level).
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6201–6213august1–6,2021.©2021associationforcomputationallinguistics6201?
?obamais the 44thpresident of the united states.①place_of_birth②employee_of①place_of_birth②employee_oflived_in(unincludedlabel)the sentence bag of <obama, united-states>which  label  ?obamawas back to the united statesyesterday.obamaunitedstatesobamawas born in the  united states.bag labels(we need)sentencelabelsdistant re, empirically verifying the deﬁciencyof bag-level methods on sentence-level evaluation.
however, the instance selection approaches ofthese methods depend on rewards(feng et al., 2018)or frequent patterns(jia et al., 2019) determined bybag-level labels, which contain much noise.
forone thing, one bag might be assigned to multiplebag labels, leading to difﬁculties in one-to-onemapping between sentences and labels.
as shownin fig.1, we have no access to the exact relationbetween “place of birth” and “employee of” forthe sentence “obama was born in the unitedstates.”.
for another, the sentences inside a bagmight not express the bag relations.
in fig.1, thesentence “obama was back to the united statesyesterday” actually express the relation “live in”,which is not included in the bag labels..in this work, we propose the use of negativetraining (nt) (kim et al., 2019) for distant re.
different from positive training (pt), nt trains amodel by selecting the complementary labels ofthe given label, regarding that “the input sentencedoes not belong to this complementary label”.
since the probability of selecting a true label as acomplementary label is low, nt decreases the riskof providing noisy information and prevents themodel from overﬁtting the noisy data.
moreover,the model trained with nt is able to separate thenoisy data from the training data (a histogram infig.3 shows the separated data distribution duringnt).
based on nt, we propose sent, a sentence-level framework for distant re.
during senttraining, the noisy instances are not only ﬁlteredwith a noise-ﬁltering strategy, but also transformedinto useful training data with a re-labeling method.
we further design an iterative training algorithm totake full advantage of these data-reﬁning processes,which signiﬁcantly boost performance.
our codesare publicly available at github1..to summarize the contribution of this work:.
• we propose the use of negative trainingfor sentence-level distant re, which greatlyprotects the model from noisy information..• we present a sentence-level.
framework,sent, which includes a noise-ﬁltering and are-labeling strategy for re-ﬁning distant data..• the proposed method achieves signiﬁcantimprovement over previous methods in termsof both re performance and de-noise effect..1https://github.com/rtmaww/sent.
2 related work.
2.1 distant supervision for re.
supervised relation extraction (re) has beenconstrained by the lack of large-scale labeleddata.
therefore, distant supervision (ds) isintroduced by mintz et al.
(2009), which employsexisting knowledge bases (kbs) as source ofsupervision instead of annotated text.
riedelet al.
(2010) relaxes the ds assumption to theexpress-at-least-once assumption.
as a result,multi-instance learning is introduced (riedel et al.
(2010); hoffmann et al.
(2011); surdeanu et al.
(2012)) for this task, where the training andevaluating process are performed in bag-level,with potential noisy sentences existing in eachbag.
most following studies in distant re adoptthis paradigm, aiming to decrease the impact ofnoisy sentences in each bag.
these studies includethe attention-based methods to attend to usefulinformation ( lin et al.
(2016); han et al.
(2018c);li et al.
(2020); hu et al.
(2019a); ye and ling(2019); yuan et al.
(2019a); zhu et al.
(2019); yuanet al.
(2019b); wu et al.
(2017)), the selectionstrategies such as rl or adversarial training toremove noisy sentences from the bag (zeng et al.
(2015); shang (2019); qin et al.
(2018); hanet al.
(2018a)) and the incorporation with extrainformation such as kgs, multi-lingual corporaor other information (ji et al.
(2017); lei et al.
(2018); vashishth et al.
(2018); han et al.
(2018b);zhang et al.
(2019); qu et al.
(2019); verga et al.
(2016); lin et al.
(2017); wang et al.
(2018); dengand sun (2019); beltagy et al.
(2019)).
otherapproaches include soft-label strategy for denoising(liu et al.
(2017)), leveraging pre-trained lm (altet al.
(2019)), pattern-based method (zheng et al.
(2019)), structured learning method (bai and ritter(2019)) and so forth (luo et al.
(2017); chen et al.
(2019))..in this work, we focus on sentence-level relationextraction.
several previous studies also performdistant re on sentence-level.
feng et al.
(2018)proposes a reinforcement learning framework forsentence selecting, where the reward is given by theclassiﬁcation scores on bag labels.
jia et al.
(2019)builds an initial training set and further selectconﬁdent instances based on selected patterns.
thedifference between the proposed work and previousworks is that we do not rely on bag-level labelsfor sentence selecting.
furthermore, we leveragent to dynamically separate the noisy data from.
6202figure 2: an overview of the proposed framework, sent, for sentence-level distant re.
three steps are included:(1) negative training for separating the noisy data from the training data; (2) noise-ﬁltering and re-labeling; (3)iterative training to further boost the performance..the training data, thus can make use of diversiﬁedclean data..2.2 learning with noisy data.
learning with noisy data is a widely discussedproblem in deep learning, especially in the ﬁeldof computer vision.
existing approaches includerobust learning methods such as leveraging a robustloss function or regularization method(lyu andtsang, 2020; zhang and sabuncu, 2018; hu et al.,2019b; kim et al., 2019), re-weighting the loss ofpotential noisy samples (ren et al., 2018; jianget al., 2018), modeling the corruption probabilitywith a transition matrix (goldberger and ben-reuven, 2016; xia et al.)
and so on.
anotherline of research tries to recognize or even correctthe noisy instances from the training data(malachand shalev-shwartz, 2017; yu et al., 2019; arazoet al., 2019; li et al., 2019)..in this paper, we focus on the noisy labelproblem in distant re.
we ﬁrst leverage a robustnegative loss (kim et al., 2019) for model training.
then, we develop a new iterative training algorithmfor noise selection and correction..3 methodology.
in order to achieve sentence-level relation classi-ﬁcation using bag-level labels in distant re, wepropose a framework, sent, which contains threemain steps (as shown in fig.2): (1) separating thenoisy data from the training data with negativetraining (sec.3.1); (2) filtering the noisy data aswell as re-labeling a part of conﬁdent instances(sec.3.2); (3) leveraging an effective trainingalgorithm based on (1) and (2) to further boost.
the performance (sec.3.3)..1), .
.
.
, (sn , y∗.
speciﬁcally, we denote the input data in thistask as s∗ = {(s1, y∗n )}, wherei ∈ r = {1, .
.
.
, c} is the bag-level label ofy∗the ith input sentence si.
obviously, this is a noisydataset drawn from a noisy distribution d∗ becausethese bag-level labels y∗ come from the distantlabel of each entity bag.
for each si containinga pair of entities < e1, e2 >, y∗is one of theirelation facts2 that < e1, e2 > participates in inthe database.
such annotation method indicatesthat y∗is a potential noisy label for si.
here,iwe denote d as the real data distribution withoutnoise, and the clean dataset drawn from d ass = {(s1, y1), .
.
.
, (sn , yn )}.
the ambition ofthis work is to ﬁnd the best estimated parametersθ of the real mapping f : x → y, (x, y) ∈ dbased on the noisy data s∗.
we design three(1) recognizingsteps for achieving this goal:n from s∗ using negativethe set of noisy data s∗i ) | y∗n = {(si, y∗training, where s∗(cid:54)= yi}.
i(2) reﬁning s∗ by noise-ﬁltering and re-labeling,e.g., s∗ref ined = (s∗ \ s∗n) ∪ s∗n,relabeled, wherei ) ∈ s∗s∗n,relabeled = {(si, yi) | (si, y∗(3)iteratively perform (1) and (2) so the reﬁned datasets∗.
ref ined approaches the real dataset s..n}..3.1 negative training on distant data.
in order to perform robust training on the noisydistant data, we propose the use of negativetraining (nt), which trains based on the conceptthat “the input sentence does not belong to thiscomplementary label”.
we ﬁnd that nt not only.
2here, we randomly choose one of the multiple bag labels.
for injective relation classiﬁcation.
see details in sec.4.2..6203relation classifierbag levelinstance levelfilterconfidenceconfidencere-label(3) iterationrelation classifierinitialized(1) negative traininginstances with relation labels a, b, cfiltered instancesinstances re-labeled to label a, b, c (2) noise filtering & re-labeling(a) positive training.
(b) negative training.
(c) after sent.
(d) pt after sent.
figure 3: data distribution when training with pt and sent.
(a) during pt, the conﬁdence of the clean and noisydata increase simultaneously; (b) during nt, the conﬁdence of the noisy data is much lower than that of the cleandata; (c) after training with the sent method, the clean and noisy data are further separated; (d) pt after senthelps improve the convergence of the clean data..provides less noisy information, but also separatesthe noisy and clean data during training..3.1.1 positive trainingpositive training (pt) trains the model towardspredicting the given label, based on the conceptthat “the input sentence belongs to this label”.
here, given any input s with a label y∗ ∈ r ={1, 2, .
.
.
, c}, y ∈ {0, 1}c is the c-dimensionone-hot vector of y∗.
we denote p = f (s) as theprobability vector of a sentence given by a relationclassiﬁer f (·).
with the cross entropy loss function,the loss deﬁned in typical positive training is:.
lp t (f, y∗) = −.
yk log pk.
(1).
c(cid:88).
k=1.
where pk denotes the probability of the kth label.
optimizing on eq.1 meets the requirement of pl,as the probability of the given label approaches 1with the loss decreasing..3.1.2 negative trainingin negative training (nt), for each input s with alabel y∗ ∈ r, we generate a complementary labely∗ by randomly sampling from the label spaceexcept y∗, e.g., y∗ ∈ r\{y∗}.
with the crossentropy loss function, we deﬁne the loss in negativetraining as:.
ln t (f, y∗) = −.
yk log(1 − pk).
(2).
c(cid:88).
k=1.
different from pt, eq.2 aims to reduce theprobability value of the complementary label, aspk → 0 with the loss decreasing..to further illustrate the effect of nt, we trainthe classiﬁer with pt and nt respectively ona constructed tacred dataset with 30% noise.
(details shown in sec.4.1).
a histogram3 of thetraining data after pt and nt is shown in figs.
3(a),(b), which reveals that, when training withpt, the conﬁdence of clean data and noisy dataincrease with no difference, resulting in the modelto overﬁt noisy training data.
on the contrary, whentraining with nt, the conﬁdence of noisy data ismuch lower than that of clean data.
this resultconﬁrms that the model trained with nt suffersless from overﬁtting noisy data with less noisyinformation provided.
moreover, as the conﬁdencevalue of clean data and noisy data separate fromeach other, we are able to ﬁlter noisy data with acertain threshold.
fig.4 shows the details of thedata-ﬁltering effect.
after the ﬁrst iteration of nt,a modest threshold contributes to 97% precisionnoise-ﬁltering with about 50% recall, which furtherveriﬁes the effectiveness of nt on noisy datatraining..3.2 noise filtering and re-labeling.
in section 3.1, we have illustrated the effectivenessof nt on training with noisy data, as well as thecapability to recognize noisy instances.
while ﬁl-tering noisy data is important for training on distantdata, these ﬁltered data contain useful informationthat can boost performance if properly re-labeled.
in this section, we describe the proposed noise-ﬁltering and label-recovering strategy for reﬁningdistant data based on nt..3.2.1 filtering noisy dataas discussed before, it is intuitive to constructa ﬁltering strategy based on a certain thresholdafter nt.
however, in distant re, the long-tailproblem cannot be neglected.
during training, the.
3when drawing the histogram, we omitted the largeamount of “na”-class data (80% of the training data) fora clearer representation of the positive-class data..62040.00.20.40.60.81.0probability on given label020004000600080001000012000instance numberafter positive trainingnoisy dataclean data0.00.20.40.60.8probability on given label0200040006000800010000after negative trainingnoisy dataclean data0.00.20.40.60.81.0probability on given label02000400060008000100001200014000after sent trainingnoisy dataclean data0.00.20.40.60.81.0probability on given label0250050007500100001250015000pt after sentnoisy dataclean datadegree of convergence is disparate among differentclasses.
simply setting a uniform threshold mightharm the data distribution with instances of long-tail relations largely ﬁltered out.
therefore, weleverage a dynamic threshold for ﬁltering noisydata.
suppose the probability of class c of theith instance is pic ∈ (0, phc is themaximum probability value in class c. based onempirical experience, we assume the probabilityvalues follow a distribution where the noisy dataare largely distributed in low-value areas and theclean data are generally distributed in middle- orhigh-value areas.
therefore, the ﬁltering thresholdof class c is set to:.
c ), where ph.
t hc = t h · ph.
c , ph.
c =.
nmaxi=1.
{pi.
c}.
(3).
where t h is a global threshold.
in this way,the noise-ﬁltering threshold not only relies onthe degree of convergence in each class, but alsodynamically changes during the training phase,thus making it more suitable for noise-ﬁltering onlong-tail data..3.2.2 re-labeling useful dataafter noise-ﬁltering, the noisy instances are re-garded as unlabeled data, which also contain usefulinformation for training.
here, we design a simplestrategy for re-labeling these unlabeled data.
giventhe set of ﬁltered data du = {s1, .
.
.
, sm}, weuse the classiﬁer trained in this iteration to predictthe probability vectors {p1, .
.
.
, pm}.
then, were-label these instances by:.
ˆyi = arg max.
{pi.
k}, if max.
{pi.
k} > t hrelabel.
k.k.(4)k is the probability of the ith instance in.
where piclass k, and t hrelabel is the re-label threshold..3.3.iterative training algorithm.
although effective, simply performing a pipeline ofnt, noise-ﬁltering and re-labeling fail to take fulladvantage of each part, thus the model performancecan be further boosted through iterative training..as shown in fig.2, for each iteration, weﬁrst train the classiﬁer on the noisy data usingnt: for each instance, we randomly samplek complementary labels and calculate the losson these labels with eq.(2).
after m -epochsnegative training, the noise-ﬁltering and re-labelingprocesses are carried out for updating the trainingdata.
next, we perform a new iteration of training.
on the newly-reﬁned data.
here, we re-initializethe classiﬁer in every iteration for two reasons:first, re-initialization ensures that in each iteration,the new classiﬁer is trained on a dataset withhigher quality.
second, re-initialization introducesrandomness,thus contributing to more robustdata-ﬁltering.
finally, we stop the iteration afterobserving the best result on the dev set.
we thenperform a round of noise-ﬁltering and re-labelingwith the best model in the last iteration to obtainthe ﬁnal reﬁned data..fig.3(c) shows the data distribution after certainiterations of sent.
as seen, the noise and cleandata are separated by a large margin.
mostnoisy data are successfully ﬁltered out, withan acceptable number of clean data mistaken.
however, we can see that the model trained withnt still lacks convergence (with low-conﬁdencepredictions).
therefore, we train the classiﬁeron the iteratively-reﬁned data with pt for betterconvergence.
as shown in fig.3(d), the modelpredictions on most of the clean data are in highconﬁdence after pt training..4 experiments.
the experiments in this work are divided intotwo parts, respectively conducted on two datasets:the nyt-10 dataset (riedel et al., 2010) and thetacred dataset (zhang et al., 2017)..the ﬁrst part is the effectiveness study onsentence-level evaluation for distant re.
differentfrom bag-level evaluation, a sentence-level evalua-tion compute precision (prec.
), recall (rec.)
andf1 metric directly on all of the individual instancesin the dataset.
in this part, we adopt the nyt-10 data set for sentence-level training, followingthe setting of jia et al.
(2019), who publishes amanually labeled sentence-level test set.
4 besides,they also publish a test set for evaluating noise-ﬁltering ability.
details of the adopted dataset areshown in table 1..we construct the second part of experiments(sec.4.4) to better understand sent’s behaviors.
since no labeled training data are available in thedistant supervision setting, we construct a noisydataset with 30% noise from a labeled dataset,tacred (zhang et al., 2017) 5. we regard thisconstructed dataset as noisy-tacred.
the reason.
4https://github.com/paddlepaddle/research/tree/master/.
nlp/acl2019-arnor.
5https://github.com/yuhaozhang/tacred-relation.
6205datasets#label num..train.
dev.
test.
#instances#positive#noise#instances#positive#instances#positive.
nyt-1024371461110518unknown23793372164323.noisy-tacred41681242657520586226315436155093325.table 1: statistics of datasets6.
“positive” meanspositive instances that are not labeled as “na”.
notethat the positive instances of noisy-tacred includefalse-positive noise and the noise number in nyt-10 isunknown due to the inaccurate annotations..we choose this dataset is that 80% instances in thetraining data are “no relation”.
this “na” rate issimilar to the nyt data which contains 70% “na”relation type, thus analysis on this dataset is morecredible..then, each noisy label.
when constructing noisy-tacred, the noisyinstances are uniformly selected with 30% noiseratio.
is created bysampling a label from a complementary class witha weight of class frequency (in order to maintainthe data distribution).
note that the original datasetconsists of 80% “no relation” data, which means80% of the noisy instances are “false-positive”instances, corresponding to the large amount of“false-positive” noise in nyt-10.
details of thenoisy-tacred are also shown in table 1..4.1 baselines.
we compare our sent method with several strongbaselines in distant re.
these compared methodscan be categorized as: bag-level denoising methods,sentence-level denoising methods, sentence-levelnon-denoising methods..pcnn+selatt (lin et al., 2016): a bag-levelre model which leverages an attention mechanismto reduce noise effect..pcnn+ra bag att (ye and ling, 2019)short for pcnn+att ra+bag att, a bag-levelmodel containing both intra-bag and inter-bagattentions to alleviate noise..cnn+rl1 (qin et al., 2018): a rl-basedbag-level method.
different from cnn+rl2,they redistribute the ﬁltered data into the negativeexamples..cnn+rl2 (feng et al., 2018): a sentence-levelre model.
it jointly train a instance selector and a.
6statistics of nyt-10 are quoted from (jia et al., 2019)..cnn classiﬁer using reinforcement learning (rl).
arnor (jia et al., 2019): a sentence-level remodel which selects conﬁdent instances based onthe attention score on the selected patterns.
it is thestate-of-the-art method in sentence level..cnn (zeng et al., 2014), pcnn (zeng et al.,2015) and bilstm (zhang et al., 2015) are typicalarchitectures used in re..bilstm+att (zhang et al., 2017) leveragesan attention mechanism based on bilstm tocapture useful information..bilstm+bert (devlin et al., 2019): basedon bilstm, it utilizes the pre-trained bertrepresentations as word embedding..4.2.implementation details.
as sent is a model-agnostic framework, weimplement the classiﬁcation model with two typicalarchitectures: bilstm and bilstm+bert.
sincebilstm is also the base model of arnor, we cancompare these two methods more fairly.
duringsent training, we use the 50-dimension glovevectors as word embedding.
while for pt aftersent, we randomly initialize the 50-dimensionword embedding as the same in arnor.
in bothtraining phases, we use 50-dimension randomly-initialized position and entity type embedding.
wetrain a single-layer bilstm with hidden size 256using the adam optimizer at a learning rate of5e-4.
when implemented with bilstm+bert,the setting is the same as those with bilstmexcept that we use a 768-dimension ﬁxed bertrepresentation as word embedding (we use the“bert-base-uncased” pre-trained model).
we tunethe hyperparameters on the development set viaa grid search.
speciﬁcally, when training on thenyt dataset, we train the model for 10 epochsin each iteration, with the global data-ﬁlteringthreshold t h = 0.25, the re-labeling thresholdt hrelabel = 0.7 and negative samples numberk = 10. when training on the noisy-tacred,we train for 50 epochs in each iteration, witht h = 0.15, t hrelabel = 0.85 and k = 50..to deal with the multi-label problem, weutilize a simple method by randomly selecting oneof the bag labels for each sentence.
such randomselection turns the multi-label noise into the wrong-label noise, which is easier to handle.
according tosurdeanu et al.
(2012), there are 31% wrong-labelnoise and 7.5% multi-label noise in nyt-10, andincorrect selection may result in 4% extra wrong-.
6206method.
cnn(zeng et al., 2014)pcnn(zeng et al., 2015)bilstm(zhang et al., 2015)bilstm+att(zhang et al., 2017)bert(devlin et al., 2019)bilstm+bert(devlin et al., 2019).
pcnn+selatt(lin et al., 2016)pcnn+ra bag att(ye and ling, 2019)cnn+rl1 (qin et al., 2018)cnn+rl2 (feng et al., 2018)arnor(jia et al., 2019).
prec..38.3236.0936.7137.5934.7836.09.
46.0149.8437.7140.0062.45.devrec..65.2263.6666.4664.9165.1773.17.
30.4346.9052.6659.1758.51.f1.
48.2846.0747.2947.6145.3548.34.
36.6448.3343.9547.7360.36.prec..35.7536.0635.5234.9336.1933.23.
45.4156.7639.4140.2365.23.testrec..64.5464.8667.4165.1870.4472.70.
30.0350.6061.6163.7856.79.f1.
46.0146.3546.5345.4847.8145.61.
36.1553.5048.0749.3460.90.sent (bilstm)sent (bilstm+bert).
66.71±0.30 57.27±0.30 61.63±0.29 71.22±0.58 59.75±0.62 64.99±0.3469.94±0.51 63.11±0.61 66.35±0.11 76.34±0.56 63.66±0.17 69.42±0.13.
table 2: main results on sentence-level evaluation.
compared baselines include normal re model (the ﬁrst part ofthe table) and models for distant re (the second part of the table).
we ran the model three times to get the averageresults..label noise, which can be ﬁltered out through ntidentically with wrong-label instances..4.3 sentence-level evaluation.
table 2 shows the results of sent and otherbaselines on sentence-level evaluation, where theresults of sent are obtained by pt after sent.
we can observe that: 1) bag-level methods failto perform well on sentence-level evaluation,indicating that it is difﬁcult for these bag-levelapproaches to beneﬁt downstream tasks withexact sentence labels.
this result is consistent2)with the results in feng et al.
(2018).
when performing sentence-level training on thenoisy distant data, all baseline models showpoor results, including the preeminent pre-trainedlanguage model bert.
these results indicatethe negative impact of directly using bag-levellabels for sentence-level training regardless ofnoise.
3) the proposed sent method achievesa signiﬁcant improvement over previous sentence-level de-noising methods.
when implemented withbilstm, the model obtains a 4.09% higher f1score than arnor.
moreover, when implementedwith bilstm+bert, the f1 score is furtherimproved by 8.52%.
4) the sent method achievesmuch higher precision than the previous de-noisingmethods when maintaining comparable or higherrecall, indicating the effectiveness of the noise-ﬁltering and re-labeling approaches..noise reduction.
prec.
rec..f1.
cnn+rl2arnorsent (bilstm)sent (bilstm+bert).
40.5876.3780.0084.33.
96.3168.1388.4685.67.
57.1072.0284.0284.99.table 3: the noise-ﬁltering effect evaluated on a noise-annotated test set of nyt-10..4.3.1 noise-filtering effect on distant data.
in order to prove the effectiveness of sent in de-noising distant data, we conduct a noise-ﬁlteringexperiment following arnor.
we use a test setpublished by arnor, which consists of 200randomly selected sentences with an “is noise”annotation.
we perform a noise-ﬁltering process asdescribed in sec.3.2.1, and calculate the de-noiseaccuracy.
as seen in table 3, the sent methodachieves remarkable improvement over arnorin f1 score by 12%.
while improving in precision,sent achieves 20% gain over arnor in recall.
as arnor initializes the training data with a smallpart of frequent patterns, these patterns might limitthe model from generalizing to various correctdata.
different from arnor, sent leveragesnegative training to automatically learn the correctpatterns, showing better ability in diversity andgeneralization..6207method.
prec.
rec.
f1.
clean bilstm+att.
67.7 63.2 65.4.data bilstm.
61.4 61.7 61.5.noisydata.
bilstm+att.
32.8 43.8 37.5.bilstm.
37.8 45.5 41.3.sent (bilstm) 66.0 52.9 58.7.table 4: model performance on clean and noisy-tacred.
when trained on noisy data,the perfor-mance of base models degrades dramatically whilesent achieves comparable results with the modelstrained on clean data..(with 200 sampled instances), validating the de-noising ability of sent on different datasets.
2)as the training iteration progressed, the precision ofnoise-ﬁltering decreases with the recall promoting.
more noise-ﬁltering contributes to a cleaner dataset,while it might bring more false-noise mistakes.
therefore, we stop the iteration when the modelreaches the best score on the development set.
3)as for label-recovering, sent can achieve about70% precision with about 25% recall.
here, thethreshold setting is also a trade-off that we prefer toadopt a modest value for more accurate re-labeling..(a) head relation..(b) long-tail relation..figure 5: data distribution of a head relation (per:title)and a long-tail relation (per:cause of death) during nt.
the dynamically designed thresholds beneﬁts ﬁltering..4.4.3 effects of dynamically filteringas described in sec.3.2, we design a dynamicﬁltering threshold for long-tail data.
the effectof this strategy is shown in fig.5.
as seen, thedegree of convergence of the long-tail relation“per:cause of death” is much lower than that of thehead relation.
simply setting a uniform thresholdwould harm the data distribution with instancesof “per:cause of death” largely ﬁltered.
whilewith a dynamically determined threshold, bothdata from the head and the long-tail relations areappropriately ﬁltered..4.5 ablation study.
to better illustrate the contribution of each com-ponent in sent, we conduct an ablation studyby removing the following components: ﬁnal pt,re-labeling, dynamic threshold, re-initialization,nt.
the test results are shown in table 6. wecan observe that: 1) removing the ﬁnal positivetraining affects little to the performance.
thisis because the model trained with nt alreadyreaches high accuracy and the purpose of ﬁnal ptis only to achieve more conﬁdential predictions.
2) removing the re-labeling process harms theperformance, as the ﬁltered instances are simplydiscarded regardless of the useful information for.
figure 4: data-reﬁning details on noisy-tacred..4.4 analysing sent on “labeled noise”.
in this section, we analyze the effectiveness of thedata-reﬁning process with a self-constructed noisydata set: noisy-tacred (details in table 1)..4.4.1 performance on noisy-tacred.
table 4 shows the results of training on tacredand noisy-tacred.
as seen, the baseline modeldegrades dramatically on the noisy data, withthe lstm dropping by 20.2%.
however, aftertraining with sent, the bilstm model canachieve comparable results with the model thattrained on the clean data.
note that the de-noisingmethod is quite helpful in promoting the precisionscore, yet the recall is still lower than that on cleandata..4.4.2 effects of data-reﬁning.
we also evaluate the noise-ﬁltering and label-recovering ability on the noisy-tacred trainingset, as shown in fig.4.
we can observe that: 1)sent achieves about 85% f1 score on the noisy-tacred data.
this result is consistent with thenoise-ﬁltering results obtained on the nyt dataset.
620812345678910iteration0.50.60.70.80.91.0de-noisedenoise_pdenoise_rdenoise_f10.00.20.40.60.81.0re-labelrelabel_prelabel_rrelabel_f10.00.20.40.60.8probability on given label02505007501000125015001750instance numberper:titleth_c=0.0922noisy dataclean data0.0000.0250.0500.0750.1000.1250.1500.175probability on given label010203040506070instance numberper:cause_of_deathth_c=0.0187noisy dataclean datasentencethe plan ﬁled on behalf of the state ’s democratic congressional delegation , for instance , wouldmake the 25th district , which zigzags 300 miles from southern austin to mexico , much shorterand austin-based , which would help the incumbent democrat , lloyd doggett .
it would draw the lines in a way that imperils an incumbent democrat , representative lloyddoggett of austin , and divides that most liberal of texas cities and surrounding travis countyamong three districts , all solidly republican .
”a leather-and-metal chair bore a shameless resemblance to a barcelona chair by ludwig miesvan der rohe -lrb- who lived in chicago -rrb- .
the works of architects like frank lloyd wright , louis h. sullivan , ludwig mies van der roheand helmut jahn deﬁne chicago in many ways .
mr. freed received a bachelor ’s degree in architecture in 1953 from the illinois institute of tech-nology in chicago , which was then under the direction of ludwig ludwig mies van der rohe .
it ’s really tough right now , ” said norman j. ornstein , a resident scholar at the conservativeamerican enterprise institute and a member of the pbs board .
”.
three of the sailors were assigned to seal delivery team 1 , pearl harbor , hawaii ..an obituary on wednesday about philip merrill , a maryland publisher , misstated a journalismpost he held as an undergraduate ..bag label.
sentence label.
reﬁned label.
place livedplace of birth.
place of death.
na.
na.
na.
place lived.
na.
place of birth.
place lived.
place of death.
place of death.
place of death.
na.
na.
na.
na.
na.
na.
company.
contains.
place lived.
table 5: examples showing the ability of sent to reﬁne the bag-level noisy data into correct data.
texts in redand blue denote the head and tail entity, respectively..components.
prec.
rec..f1.
sent (bilstm).
− final pt.
− re-labeling.
71.22.
72.48.
59.75.
64.99.
57.89.
64.37.
66.67.
55.11.
60.34.
− dynamic threshold.
58.46.
49.23.
53.45.
− re-initialization.
48.61.
65.02.
55.63.
− nt.
41.58.
70.28.
52.24.table 6: an ablation study on nyt-10..training.
3) without dynamic threshold, cleaninstances from the tail classes are incorrectly ﬁl-tered out, which severely degrades the performance.
4) re-initialization also contributes a lot to theperformance.
the model trained on the originalnoisy data inevitably ﬁts to the noisy distribution,while re-initialization helps wash out the overﬁttedparameters and eliminate the noise effects, thuscontributing to better training and noise-ﬁltering.
5)training with pt instead of nt causes a dramaticdecline in performance, especially on the precision,which veriﬁes the effectiveness of nt to preventthe model from overﬁtting noisy data..4.6 case study.
as discussed, sent is able to reﬁne the distantre dataset.
in fact, there exists much noise in thenyt data that is difﬁcult to tackle with bag-levelmethods.
in table 5, we show some examples.
(1)the ﬁrst two rows are the sentences in a multi-labelbag.
we randomly choose one of the bag labels foreach sentence, and the model is able to correct thebad choice (by correcting the second sentence with“place lived” and the ﬁrst sentence with “na”).
(2) the following three rows show a bag with.
label “place of death”, while this whole bag isactually a “na” bag incorrectly labeled positive.
(3) sent can also recognize the positive samplesin “na”.
as shown in the last three rows, eachsentence labeled as “na” is actually expressing apositive label.
in fact, such false-negative problemis frequently seen in the nyt data, which contains70% negative instances that were labeled “na”only because the entity pairs do not participatein a relation in the database.
we believe thecapacity to recognize these false-negative samplescan signiﬁcantly boost the performance..5 conclusion.
in this paper, we present sent, a novel sentence-level framework based on negative training (nt)for sentence-level training on distant re data.
ntnot only prevent the model from overﬁtting noisydata, but also separate the noisy data from thetraining data.
by iteratively performing noise-ﬁltering and re-labeling based on nt, senthelps re-ﬁne the noisy distant data and achievesremarkable performance.
experimental resultsverify the improvement of sent over previousmethods on sentence-level relation extraction andnoise-ﬁltering effect..acknowledgements.
the authors wish to thank the anonymous reviewersfor their helpful comments.
this work was partiallyfunded by china national key r&d program(no.
2018yfb1005100), national natural sciencefoundation of china (no.
61976056, 62076069),shanghai municipal science and technologymajor project (no.2021shzdzx0103)..6209references.
christoph alt, marc h¨ubner, and leonhard hennig.
2019. fine-tuning pre-trained transformer languagemodels to distantly supervised relation extraction.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages1388–1398, florence, italy.
association for compu-tational linguistics..eric arazo, diego ortego, paul albert, noel o’connor,and kevin mcguinness.
2019. unsupervised labelnoise modeling and loss correction.
in internationalconference on machine learning, pages 312–321.
pmlr..the 2019 conference of.
fan bai and alan ritter.
2019. structured minimallysupervised learning for neural relation extraction.
in proceedings ofthenorth american chapter ofthe association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages3057–3069, minneapolis, minnesota.
associationfor computational linguistics..iz beltagy, kyle lo, and waleed ammar.
2019.combining distant and direct supervision for neuralin proceedings of the 2019relation extraction.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1858–1867, minneapolis, minnesota.
association for computational linguistics..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
advances in neural informationprocessing systems, 26:2787–2795..junfan chen, richong zhang, yongyi mao, hongyuguo, and jie xu.
2019. uncover the ground-truth re-lations in distant supervision: a neural expectation-in proceedings of themaximization framework.
2019 conference on empirical methods in naturallanguage processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 326–336, hong kong,china.
association for computational linguistics..xiang deng and huan sun.
2019..leveraging 2-hop distant supervision from table entity pairsthefor relation extraction.
2019 conference on empirical methods in naturallanguage processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 410–420, hong kong,china.
association for computational linguistics..in proceedings of.
jacob devlin, ming-wei chang, kenton lee, andbert: pre-trainingkristina toutanova.
2019.transformers for languageof deep bidirectionalthe 2019understanding.
conference of the north american chapter of theassociation for computational linguistics: human.
in proceedings of.
language technologies, volume 1 (long and shortpapers), pages 4171–4186, minneapolis, minnesota.
association for computational linguistics..anthony fader, luke zettlemoyer, and oren etzioni.
2014. open question answering over curated andin proceedings of theextracted knowledge bases.
20th acm sigkdd international conference onknowledge discovery and data mining, pages 1156–1165..jun feng, minlie huang, li zhao, yang yang,and xiaoyan zhu.
2018. reinforcement learningfor relation classiﬁcation from noisy data.
inproceedings of the aaai conference on artiﬁcialintelligence..jacob goldberger and ehud ben-reuven.
2016. train-ing deep neural-networks using a noise adaptationlayer..xu han, zhiyuan liu, and maosong sun.
2018a.
denoising distant supervision for relation extractionarxivvia instance-level adversarialpreprint arxiv:1805.10959..training..xu han, zhiyuan liu, and maosong sun.
2018b.
neural knowledge acquisition via mutual attentionbetween knowledge graph and text.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 32..xu han, pengfei yu, zhiyuan liu, maosong sun, andpeng li.
2018c.
hierarchical relation extractionin proceed-with coarse-to-ﬁne grained attention.
ings of the 2018 conference on empirical methodsin natural language processing, pages 2236–2245..raphael hoffmann, congle zhang, xiao ling, lukezettlemoyer, and daniel s. weld.
2011. knowledge-information extrac-based weak supervision forin proceedingstion of overlapping relations.
ofthe associationfor computational linguistics: human languagetechnologies, pages 541–550, portland, oregon,usa.
association for computational linguistics..the 49th annual meeting of.
linmei hu, luhao zhang, chuan shi, liqiang nie,weili guan, and cheng yang.
2019a.
improvingdistantly-supervised relation extraction with jointthe 2019label embedding.
conference on empirical methodsin naturallanguage processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3821–3829, hong kong,china.
association for computational linguistics..in proceedings of.
wei hu, zhiyuan li, and dingli yu.
2019b.
simpleand effective regularization methods for training onnoisily labeled data with generalization guarantee.
in international conference on learning represen-tations..guoliang ji, kang liu, shizhu he, jun zhao, et al.
2017. distant supervision for relation extractionwith sentence-level attention and entity descriptions.
in aaai, volume 3060..6210wei jia, dai dai, xinyan xiao, and hua wu.
2019.arnor: attention regularization based noise reduc-tion for distant supervision relation classiﬁcation.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 1399–1408, florence, italy.
association for computationallinguistics..2017.learning with noise: enhance distantlysupervised relation extraction with dynamic transi-in proceedings of the 55th annualtion matrix.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 430–439,vancouver, canada.
association for computationallinguistics..lu jiang, zhengyuan zhou, thomas leung, li-jia li,and li fei-fei.
2018. mentornet: learning data-driven curriculum for very deep neural networksin proceedings of the 35thon corrupted labels.
international conference on machine learning,volume 80 of proceedings of machine learningresearch, pages 2304–2313.
pmlr..youngdong kim, junho yim, juseung yun, and junmokim.
2019. nlnl: negative learning for noisy labels.
in proceedings ofthe ieee/cvf internationalconference on computer vision, pages 101–110..kai lei, daoyuan chen, yaliang li, nan du, minyang, wei fan, and ying shen.
2018. cooperativedenoising for distantly supervised relation extrac-the 27th internationaltion.
conference on computational linguistics, pages426–436, santa fe, new mexico, usa.
associationfor computational linguistics..in proceedings of.
junnan li, richard socher, and steven ch hoi.
2019.dividemix: learning with noisy labels as semi-supervised learning.
in international conference onlearning representations..yang li, guodong long, tao shen, tianyi zhou,lina yao, huan huo, and jing jiang.
2020.self-attention enhanced selective gate with entity-aware embedding for distantly supervised relationextraction.
proceedings of the aaai conference onartiﬁcial intelligence, 34(05):8269–8276..yankai lin, zhiyuan liu, and maosong sun.
2017.neural relation extraction with multi-lingual atten-in proceedings of the 55th annual meetingtion.
of the association for computational linguistics(volume 1: long papers), pages 34–43, vancouver,canada.
association for computational linguistics..yankai lin, shiqi shen, zhiyuan liu, huanboluan, and maosong sun.
2016. neural relationextraction with selective attention over instances.
in proceedings of the 54th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 2124–2133..tianyu liu, kexiang wang, baobao chang, andzhifang sui.
2017. a soft-label method for noise-tolerant distantly supervised relation extraction.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages1790–1795, copenhagen, denmark.
association forcomputational linguistics..bingfeng luo, yansong feng, zheng wang, zhanxingzhu, songfang huang, rui yan, and dongyan zhao..yueming lyu and ivor w. tsang.
2020. curriculumloss: robust learning and generalization againstin international conference onlabel corruption.
learning representations..eran malach and shai shalev-shwartz.
2017. decou-pling” when to update” from” how to update”.
innips..mike mintz, steven bills, rion snow, and danjurafsky.
2009. distant supervision for relationin proceedings ofextraction without labeled data.
the joint conference of the 47th annual meeting ofthe acl and the 4th international joint conferenceon natural language processing of the afnlp,pages 1003–1011..pengda qin, weiran xu, and william yang wang.
2018. robust distant supervision relation extractionarxiv preprintvia deep reinforcement learning.
arxiv:1805.09927..jianfeng qu, wen hua, dantong ouyang, xiaofangzhou, and ximing li.
2019. a ﬁne-grained andnoise-aware method for neural relation extraction.
in proceedings ofthe 28th acm internationalconference on information and knowledge manage-ment, pages 659–668..mengye ren, wenyuan zeng, bin yang, and raquelurtasun.
2018.learning to reweight examplesin proceedings of thefor robust deep learning.
35th international conference on machine learning,volume 80 of proceedings of machine learningresearch, pages 4334–4343.
pmlr..sebastian riedel, limin yao, and andrew mccallum.
2010. modeling relations and their mentionswithout labeled text.
in joint european conferenceon machine learning and knowledge discovery indatabases, pages 148–163.
springer..yuming shang.
2019. are noisy sentences useless for.
distant supervised relation extraction?.
learning for relation extraction..mihai surdeanu, julie tibshirani, ramesh nallapati,and christopher d. manning.
2012. multi-instancemulti-labelinproceedings ofthe 2012 joint conference onempirical methods in natural language processingand computational natural language learning,pages 455–465, jeju island, korea.
association forcomputational linguistics..shikhar vashishth, rishabh joshi, sai suman prayaga,chiranjib bhattacharyya, and partha talukdar.
2018.improving distantly-supervised neuralreside:inrelation extraction using side information..6211proceedings of the 2018 conference on empiricalmethods in natural language processing, pages1257–1266, brussels, belgium.
association forcomputational linguistics..patrick verga, david belanger, emma strubell,benjamin roth, and andrew mccallum.
2016.multilingual relation extraction using compositionalthe 2016universal schema.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 886–896, san diego,california.
association for computational linguis-tics..in proceedings of.
xiaozhi wang, xu han, yankai lin, zhiyuan liu,and maosong sun.
2018. adversarial multi-lingualin proceedings of theneural relation extraction.
27th international conference on computationallinguistics, pages 1156–1166, santa fe, newmexico, usa.
association for computational lin-guistics..zhen wang, jianwen zhang, jianlin feng, and zhengknowledge graph embedding byin proceedings ofintelligence,.
chen.
2014.translating on hyperplanes.
the aaai conference on artiﬁcialvolume 28..fei wu and daniel s. weld.
2010. open informationin proceedings of theextraction using wikipedia.
48th annual meeting of the association for com-putational linguistics, pages 118–127, uppsala,sweden.
association for computational linguistics..training for relation extraction..yi wu, david bamman, and stuart russell.
2017.adversarialinproceedings of the 2017 conference on empiricalmethods in natural language processing, pages1778–1783, copenhagen, denmark.
association forcomputational linguistics..xiaobo xia, tongliang liu, nannan wang, bo han,chen gong, gang niu, and masashi sugiyama.
are anchor points really indispensable in label-noiselearning?.
kun xu, siva reddy, yansong feng, songfang huang,and dongyan zhao.
2016. question answering onfreebase via relation extraction and textual evidence.
in proceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2326–2336, berlin, germany.
association for computational linguistics..xuchen yao and benjamin van durme.
2014..infor-mation extraction over structured data: questionin proceedings ofanswering with freebase.
the 52nd annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 956–966, baltimore, maryland.
associationfor computational linguistics..zhi-xiu ye and zhen-hua ling.
2019. distant supervi-sion relation extraction with intra-bag and inter-bagattentions.
arxiv preprint arxiv:1904.00143..xingrui yu, bo han, jiangchao yao, gang niu,ivor tsang, and masashi sugiyama.
2019. howdoes disagreement help generalization against labelin international conference oncorruption?
machine learning, pages 7164–7173.
pmlr..changsen yuan, heyan huang, chong feng, xiao liu,and xiaochi wei.
2019a.
distant supervision forrelation extraction with linear attenuation simulationin proceedingsand non-iid relevance embedding.
of the aaai conference on artiﬁcial intelligence,volume 33, pages 7418–7425..yujin yuan, liyuan liu, siliang tang, zhongfei zhang,yueting zhuang, shiliang pu, fei wu, and xiangcross-relation cross-bag attentionren.
2019b.
for distantly-supervised relation extraction.
inproceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 419–426..daojian zeng, kang liu, yubo chen, and jun zhao.
2015. distant supervision for relation extractionvia piecewise convolutional neural networks.
inproceedings of the 2015 conference on empiricallanguage processing, pagesmethods in natural1753–1762..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiﬁcation viaconvolutional deep neural network.
in proceedingsof coling 2014, the 25th international conferenceon computational linguistics: technical papers,pages 2335–2344, dublin,ireland.
dublin cityuniversity and association for computational lin-guistics..ningyu zhang, shumin deng, zhanlin sun, guanyingwang, xi chen, wei zhang, and huajun chen.
2019.long-tail relation extraction via knowledge graphembeddings and graph convolution networks.
arxivpreprint arxiv:1903.01306..shu zhang, dequan zheng, xinchen hu, and mingyang.
2015. bidirectional long short-term memorynetworks for relation classiﬁcation.
in proceedingsof the 29th paciﬁc asia conference on language, in-formation and computation, pages 73–78, shanghai,china..yuhao zhang, victor zhong, danqi chen, gabor an-geli, and christopher d. manning.
2017. position-aware attention and supervised data improve slotin proceedings of the 2017 conference onﬁlling.
empirical methods in natural language processing,pages 35–45, copenhagen, denmark.
associationfor computational linguistics..zhilu zhang and mert sabuncu.
2018. generalizedcross entropy loss for training deep neural networksin neuralwith noisy labels.
information processing systems, volume 31. curranassociates, inc..in advances.
shun zheng, xu han, yankai lin, peilin yu, lu chen,ling huang, zhiyuan liu, and wei xu.
2019..6212diag-nre: a neural pattern diagnosis frameworkfor distantly supervised neural relation extraction.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages1419–1429, florence, italy.
association for compu-tational linguistics..zhangdong zhu, jindian su, and yang zhou.
2019. im-proving distantly supervised relation classiﬁcationieee access,with attention and semantic weight.
7:91160–91168..6213