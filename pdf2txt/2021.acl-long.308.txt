veco: variable and flexible cross-lingual pre-training for languageunderstanding and generation.
fuli luoâˆ—, wei wangâˆ—, jiahao liu, yijia liu, bin bi, songfang huang, fei huang, luo sialibaba group{lfl259702,hebian.ww,glacier.ljh,yanshan.lyj}@alibaba-inc.com{b.bi,songfang.hsf,f.huang,luo.si}@alibaba-inc.com.
abstract.
existing work in multilingual pretraining hasdemonstrated the potential of cross-lingualtransferability by training a uniï¬ed trans-former encoder for multiple languages.
how-ever, much of this work only relies on theshared vocabulary and bilingual contexts toencourage the correlation across languages,which is loose and implicit for aligning thecontextual representations between languages.
in this paper, we plug a cross-attention mod-ule into the transformer encoder to explicitlybuild the interdependence between languages.
it can effectively avoid the degeneration ofpredicting masked words only conditioned onthe context in its own language.
more impor-tantly, when ï¬ne-tuning on downstream tasks,the cross-attention module can be plugged inor out on-demand, thus naturally beneï¬ting awider range of cross-lingual tasks, from lan-guage understanding to generation..as a result, the proposed cross-lingual modeldelivers new state-of-the-art results on vari-ous cross-lingual understanding tasks of thextreme benchmark, covering text classiï¬-cation, sequence labeling, question answer-ing, and sentence retrieval.
for cross-lingualgeneration tasks, it also outperforms all ex-isting cross-lingual models and state-of-the-art transformer variants on wmt14 english-to-german and english-to-french translationdatasets, with gains of up to 1âˆ¼2 bleu.
1.
1.introduction.
cross-lingual pre-trained models like mbert (de-vlin et al., 2019), xlm (lample and conneau,2019) and xlm-r (conneau et al., 2019) that tar-get providing contextualized representations for theinputs across languages, have shown large poten-.
*equal contribution.
1code and model are available at https://github..com/alibaba/alicemind/tree/main/veco.
(a) xlm (mlm + tlm).
(b) xlm-r (mlm).
figure 1: the attention scores of xlm and xlm-rwith the input of a pair of parallel sentences: take aseat and have a rest in english and its translated chi-nese sentence.
the darker line denotes a higher score.
we can found that there are only a few attention pat-terns across english and chinese subwords..tial on a variety of cross-lingual understanding andgeneration tasks..behind the great success, two major factors playthe role of aligning the contextual representationsbetween languages: 1) build the shared vocabularyacross languages through subword tokenization,which supports the simple extension of masked lan-guage modeling (mlm) from english corpus tomultilingual corpus; 2) capture the alignment inparallel data via concatenating two sentences asinput, called translation language modeling (tlm).
however, both of these two mechanisms rely onthe self-attention module (query=key/value) of thetransformer encoder to implicitly enhance the in-terdependence between languages, which may leadto few attention patterns across languages.
tak-ing figure 1 as an example, even though inputtinga pair of parallel sentences, both models only at-tend to the english context to build the represen-tation of english tokens, while ignoring the se-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3980â€“3994august1â€“6,2021.Â©2021associationforcomputationallinguistics39802021/2/1lightshot screenshotchrome-extension://mbniclmhobmnbdlbpiphghaielnnpgdp/screenshot.html?id=screenshot_0.00079184741005811081/1figure 2: a schematic comparison of cross-lingual pre-training tasks and their attention matrices.
when predictingthe masked words of different languages: a) mlm can only attend to the context in its own language; b) tlmimplicitly attend to a part of words across languages (as shown in figure 1).
however, c) the proposed ca-mlmcan: (1) not only attend to the context in its own language to predict words x2 and y3, (2) but also can ï¬rstly attendto its own context and then explicitly attend to all words across languages to predict words x3 and y2 via a plug-incross-attention module..mantically related chinese tokens.
that is, theself-attention module captures little communica-tion across languages, which is crucial for learninguniversal cross-lingual representations..based on the above observation, we propose toplug a cross-attention module (query!=key/value)into the transformer encoder and design a cross-attention mlm task to explicitly capture the inter-dependence between languages.
as illustrated infigure 2 (c), the cross-attention module takes therepresentation of x as query and y as key/value(purple lines) to build the representations of x inthe next layer, thus explicitly aligning the repre-sentations across languages (purple attention ma-trices).
it can effectively avoid the degeneration ofpredicting masked words only conditioned on thecontext in its own language.
moreover, what dis-tinguishes our work from pre-training an encoder-decoder model (liu et al., 2020b) is that we alsokeep the good nature (i.e., bidirectional contextualmodeling) of the original encoder by unpluggingthe cross-attention from the model to predicting themasked words (e.g., x2 and y3)..furthermore, when ï¬ne-tuning on various down-stream tasks, we can choose either plug-in or plug-out the cross-attention module on-demand, thusmaking it suitable for both cross-lingual languageunderstanding (nlu) and generation tasks (nlg).
for cross-lingual nlu tasks, if plugging the cross-attention module out, we can adopt the same ï¬ne-.
tuning methods as an encoder-only model likexlm.
however, we ï¬nd that plugging the cross-attention module in ï¬ne-tuning can better utilizethe bilingual context to boost the performance.
forcross-lingual nlg like machine translation (mt),the cross attention is already jointly pre-trainedwith the whole network.
therefore, the parame-ters of the decoder do not need to be re-adjustedsubstantially in the following tuning process, thusfundamentally solving the main drawback of utiliz-ing pre-trained encoders like xlm for initializingencoder-decoder models..we call our approach veco for â€œvariable andflexible cross-lingual pre-trainingâ€.
we validateveco on a variety of representative cross-lingualunderstanding and generation benchmarks.
regrad-ing cross-lingual understanding tasks, we conductexperiments on the xtreme benchmark consist-ing of 9 cross-lingual tasks, including text clas-siï¬cation, sequence labeling, question answering,and sentence retrieval.
veco ranks ï¬rst at thextreme leaderboard 2 at the submission deadline.
regrading cross-lingual generation tasks, we vali-date veco on the widely used wmt14 english-german and english-french machine translationbenchmarks.
veco obtains 44.5 and 31.7 bleuscores, consistently outperforming existing cross-lingual pre-training approaches and state-of-the-arttransformer variants by around 1âˆ¼2 bleu..2https://sites.research.google/xtreme.
3981b) translation language modeling (tlm)a) masked language modeling (mlm)ğ‘¥!âˆ’ğ‘¥#ğ‘¥$feed-forwardself-attentionğ‘¥"feed-forwardself-attentionâˆ’ğ‘¦!ğ‘¦$ğ‘¦#ğ‘¦"c) cross-attention mlm (ca-mlm)plug-inmoduleğ‘¥!âˆ’âˆ’âˆ’âˆ’ğ‘¦!ğ‘¥#ğ‘¥"ğ‘¦#ğ‘¦"ğ‘¥$ğ‘¦$feed-forwardself-attentioncross-attentionğ‘¥!âˆ’ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦$âˆ’ğ‘¦#ğ‘¥!âˆ’ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦$âˆ’ğ‘¦#ğ‘¥!âˆ’ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦$âˆ’ğ‘¦#ğ‘¥!âˆ’ğ‘¥"ğ‘¥#ğ‘¦!ğ‘¦$âˆ’ğ‘¦#ğ‘¥!âˆ’âˆ’ğ‘¥#ğ‘¦!âˆ’âˆ’ğ‘¦#ğ‘¥!âˆ’âˆ’ğ‘¥#ğ‘¦!âˆ’âˆ’ğ‘¦#ğ‘¥!âˆ’âˆ’ğ‘¦!ğ‘¥#ğ‘¦"ğ‘¥$ğ‘¦$feed-forwardğ‘¥"ğ‘¦#self-attention2 pre-training of veco.
2.1 overview of veco.
veco extends from a multi-layer transformer en-coder and plugs a cross-attention module in eachlayer.
given a pair of input (x, y) and its corruptedversion ( Ë†x, Ë†y) via randomly masking part of its to-kens, the model builds two types of contextualizedvector representation for each token:.
â€¢ one suit of contextual representations h, de-noted as green blocks and yellow blocks infigure 2 (c), are only build on self-attentionmodule (i.e., unpluging the cross-attentionmodule) in each layer..â€¢ another suit of contextual representations s,denoted as mixed color blocks in figure 2(c), are build on both the self-attention andcross-attention modules 3..the model is trained to predict the masked to-kens via two corresponding representations, condi-tioning on both its own context and paired context,respectively.
take predicting the masked wordsin sequence x as an example, the training objec-tive is the cross-entropy of the gold distributionand predicted distribution p (x| Ë†x) and p (x| Ë†y, Ë†x)computed via the above two suits of contextual rep-resentations.
thus, the training objective of cross-attention masked language modeling (ca-mlm)can be formulated as.
l(x, y) =âˆ’ logp (x| Ë†x; Î¸s) âˆ’ logp (x| Ë†y, Ë†x; Î¸s, Î¸c)âˆ’ logp (y| Ë†y; Î¸s) âˆ’ logp (y| Ë†x, Ë†y; Î¸s, Î¸c).
(1).
where Î¸s and Î¸c are the parameters of self-attentionand cross-attention modules..2.2 architecture.
the backbone network of veco is composed of astack of n transformer layers.
each layer has threemodules: a required self-attention module, a plug-and-play cross-attention module, and a requiredfeed-forward linear module.
both self-attentionand cross-attention modules are based on the multi-head attention (vaswani et al., 2017).
an attentionfunction can be described as mapping a query (q)and a set of key-value (k-v) pairs to an output..for the self-attention module, all the queries,keys and values are the same representations fromthe previous layer.
speciï¬cally, for the l-th trans-former layer, the output of a self-attention head aslis computed via:.
(2).
(3).
(4).
(5).
(6).
(7).
(8).
(9).
q = hlâˆ’1wqlk = hlâˆ’1wklv = hlâˆ’1wvl.as.
l = softmax(.
qktâˆšdk.
)v.where hlâˆ’1 are the previous layerâ€™s outputs,wql are the parameter matrices of self-attention modules..l , wk.
l , wv.
for the cross-attention module, the queries comefrom the previous layer, and the keys and valuescome from the last layerâ€™s representations of pairedinput.
speciï¬cally, for the l-th layer, the output ofa cross-attention head ac.
l is computed via:.
q = slâˆ’1uqlk = hluklv = hluvl.ac.
l = softmax(.
qktâˆšdk.
)v.where slâˆ’1 are the previous layerâ€™s outputs,uql , uvl are the parameter matrices of cross-attention modules..l , uk.
finally, the output hl of the last layer is usedto recover the masked tokens of x, conditioning onits own context..p (x| Ë†x) = softmax(f (hlp (y| Ë†y) = softmax(f (hl.
x ))y )).
(10).
(11).
where f is the feed-forward network that maps theoutput vectors into the dictionary.
hly arecomputed via eq 2âˆ¼5 when h0x and h0y are theword embeddings of x and y, respectively..x and hl.
meanwhile, sl, conditioning on the context ofthe paired sequence Ë†x and Ë†y, is used to predict themasked tokens of y..p (x| Ë†y, Ë†x) = softmax(f (slp (y| Ë†x, Ë†y) = softmax(f (sl.
x ))y )).
(12).
(13).
3for simplicity of illustration, we only show the mixed.
representations s of x3 and y2 in figure 2 (c)..x and sl.
where slthe corresponding word embeddings and hl..y are computed via eq 6âˆ¼9 with.
3982figure 3: the overview of veco.
during pre-training, a plug-and-play cross-attention module is jointly pre-trained along with the self-attention module.
when ï¬ne-tuning on natural language understanding (nlu) tasks,the cross-attention module can be either plug-in or plug-out on demand.
when ï¬ne-tuning on natural languagegeneration (nlg) tasks, veco can initialize an encoder-decoder module (the mainstream backbone model ofgeneration tasks) since all those necessary modules in the encoder and decoder are already pre-trained..note that when optimizing the objectives basedon eq 12 and eq 13, we apply a stop-gradientsoperation (chen and he, 2020) to hl (i.e., hlis treated as a constant in this term).
this opera-tion can largely speed up the training by avoidingthe backpropagation on a 2l-layer network.
more-over, it even stabilizes the training of deep post-layernorm transformer, which requires non-trivialefforts regarding carefully designing learning rateschedulers and cutting-edge optimizers (liu et al.,2020a; bachlechner et al., 2020)..3 fine-tuning veco for downstreamcross-lingual understanding andgeneration tasks.
as figure 3 illustrated, when ï¬ne-tuning on vari-ous downstream tasks, one advantage of veco isits ï¬‚exibility for initializing both the encoder-onlytransformer for understanding tasks and encoder-decoder transformer for generation tasks.
beyondit, we also explore a ï¬ne-tuning approach combinedwith the characteristics of veco ..3.1 veco for cross-lingual understanding.
due to the plug-and-play cross-attention module,we explore two ï¬ne-tuning approaches:.
â€¢ plug-out ï¬ne-tuning is to unplug the cross-attention module from the pre-trained model.
in other words, the architecture of the ï¬ne-tuned model is almost the same as mbert orxlm.
speciï¬cally, the contextual representa-tions from the last layer hlx is used to predictthe label of input x..the bilingual or automatically translated train-ing data y is available in the downstream task.
speciï¬cally, we concatenated the two repre-sentations [hlx ] to predict the label of x,y : sl[hl.
y ] to predict the label of y.
4..x : sl.
3.2 veco for cross-lingual generation.
for pre-trained encoders like xlm, it is not a triv-ial problem to incorporate them into the sequence-to-sequence architecture â€“ the mainstream back-bone model of generation tasks (zhu et al., 2020).
one of the drawbacks or challenges could be thatthe encoder-to-decoder attention is not pre-trained.
therefore, the parameters of the decoder need to bere-adjusted along with the encoder in the followingï¬ne-tuning process (ren et al., 2019)..however, under the framework of veco , thecross-attention is jointly pre-trained along withthe whole network, making it easy to providefull initialization for sequence-to-sequence models.
speciï¬cally, the self-attention module is used toinitialize both the corresponding modules in the en-coder and decoder for contextual modeling, whilethe cross-attention module is used to initialize theencoder-to-decoder attention.
itâ€™s okay whetheryou continue to tie the self-attention parameters dur-ing ï¬ne-tuning.
directly pre-training a sequence-to-sequence model like mbart (liu et al., 2020b)could be another solution for nlg tasks, but wefound mbart is not so effective in cross-lingualnlu tasks.
we refer the reader to the section 7 fordetailed experiments and analysis..â€¢ plug-in ï¬ne-tuning is to plug the cross-attention module into the ï¬ne-tuned model, if.
4plug-in ï¬ne-tuning is not suitable for the zero-shot setting(also called cross-lingual transfer) due to the lack of bilingualor translated pair (x, y).
3983veco fine-tuning: flexible for nlu and nlg tasksself-attentioncross-attentionfeed-forwardself-attentionfeed-forwardpre-training:train a plug-and-play cross-attention modulenlufine-tuning:nlgfine-tuningï¼šinitialize a encoder-decoder transformer 12plug-outfine-tuning self-attentionfeed-forwardplug-infine-tuning self-attentionfeed-forwardself-attentioncross-attentionfeed-forwardcross-attentionmodel.
architecture.
#parameters enc layers dec layers.
#languages.
#vocab.
training data.
mbert (devlin et al., 2019)xlm (lample and conneau, 2019)xlm-r (conneau et al., 2019)mrasp (lin et al., 2020)mmte (siddhant et al., 2020)mbart (liu et al., 2020b).
encoder-onlyencoder-onlyencoder-onlyencoder-decoderencoder-decoderencoder-decoder.
110m570m550m375m375m680m.
662m.
1224246612.
---6612.
1041001003210325.
50.
110k200k250k64k64k250k.
250k.
wikipediawikipediacommoncrawltranslationtranslationcommoncrawl.
veco.
flexible.
24*.
commoncrawl + translation.
table 1: comparison of large cross-lingual models.
* denotes veco uniï¬es the encoder and decoder..4 pre-training setup.
model conï¬guration we pre-train a 24-layermodel with 1024 embedding/hidden size and 4096feed-forward size.
we do not use language em-beddings to allow our model to better deal withdownstream tasks of unseen languages.
we adoptthe same 250k vocabulary that is also used byxlm-r (conneau et al., 2019).
table 1 shows theother details of baselines and veco ..pre-training data we collect monolingual andbilingual corpus covering 50 languages.
for mono-lingual training datasets, we reconstruct common-crawl corpus used in xlm-r (conneau et al.,2019).
we extract 1.36tb data in 50 languages,which contains 6.5g sentences and 0.4g docu-ments.
we up/down-sample the monolingual textlike xlm from each language with a smooth-ing parameter Î± = 0.5. for bilingual data, wecollect from the opus website 5 like previousworks (lample and conneau, 2019; chi et al.,2020b).
there are 6.4g parallel sentences, cov-ering 879 language pairs across 50 languages.
seemore statistics of training data in appendix a..optimization settings for each iteration, we al-ternately sample a batch of adjacent segments fromthe monolingual corpus and a batch of parallel sen-tences from bilingual datasets to conduct a pair ofmasked input ( Ë†x, Ë†y).
we adopt the translation lan-guage modeling (tlm) when the inputs are parallelbilingual sentences.
thus the overall training objec-tive is the sum of tlm and the proposed ca-mlmobjectives.
during training, the model parametersexcept for cross-attention are initialized by xlm-r.we ï¬rst freeze the parameters of xlm-r and onlyupdate the cross-attention parameters for faster con-vergence.
then, we jointly train the whole model.
we pre-train our model with mixed-precision train-ing using 64 nvidia telsa v100 32gb gpus.
ap-pendix a shows additional details..5http://opus.nlpl.eu/.
5 experiments on cross-lingual.
understanding tasks.
5.1 experimental setup.
downstream tasks we conduct cross-lingualnlu evaluations on xtreme (hu et al., 2020), arepresentative massively multilingual benchmarkthat consists of 9 understanding tasks over 40 lan-guages.
xtreme tasks can be classiï¬ed into fourdifferent categories: (1) sentence-pair classiï¬ca-tion: xnli (conneau et al., 2018), paws-x (yanget al., 2019); (2) structured prediction: pos (nivreet al., 2018), wikiann ner (pan et al., 2017);(3) question answering: xquad (artetxe et al.,2020), mlqa (lewis et al., 2020), tydiqa (clarket al., 2020);(4) sentence retrieval: bucc2018 (zweigenbaum et al., 2017), tatoeba (artetxeand schwenk, 2019).
tasks in the ï¬rst three cate-gories are provided: 1) golden training corpus inenglish, 2) translated training corpus in other lan-guages, and 3) dev/test set in all languages.
forsentence retrieval tasks, no training datasets areprovided.
we refer the reader to hu et al.
(2020)for additional details about the datasets..setting following.
fine-tuningpreviousworks (conneau et al., 2019; hu et al., 2020),we consider two typical ï¬ne-tuning settings:(1) cross-lingual transfer which ï¬ne-tunes thepre-trained model using english golden dataonly and directly performs inference on the testdata of different target languages; (2) translate-train-all ï¬ne-tunes a multilingual model on theconcatenation of all data (golden training corpusin english and translated training corpus in otherlanguages).
note that for two sequence-labelingtasks (pos, ner), the position of token labels inthe translated text generally differs from that inthe source text.
following filter (fang et al.,2020), we use the model trained only on theenglish training dataset as a teacher, to label thetranslated text.
to have a fair comparison with thestrong baseline xlm-r (conneau et al., 2019).
3984datasets#languagesmetrics.
xnli15acc.
paws-x pos ner.
xquad mlqa.
7acc.
33f1.
40f1.
11f1/em.
7f1/em.
tydiqa bucc tatoeba5f1.
9f1/em.
33acc.
cross-lingual transfer: fine-tune model on english training set and test on all languagesmmteâ€ 58.1/43.8mbertâ€ 59.7/43.0xlmâ€ 43.6/29.1xlm-râ€ 65.1/45.067.6/49.1vecoout.
60.3/41.461.4/44.248.5/32.671.6/53.271.7/53.2.
64.4/46.264.5/49.459.8/44.376.6/60.877.3/61.8.
58.362.261.265.465.7.
67.465.469.179.279.9.
81.381.980.986.488.7.
73.570.370.172.675.1.
59.856.756.866.085.0.
37.938.732.657.375.1.translate-train-all: fine-tune model on english training data and translated data of the target languagexlm-râ€¡xlm-râˆ—filtervecooutvecoin.
80.2/65.980.0/65.882.4/68.079.9/66.383.9/70.9.
72.8/54.373.0/54.376.2/57.773.1/54.977.5/59.3.
66.5/47.774.5/58.368.3/50.975.0/58.979.4/63.7.
-65.467.765.771.0.
90.490.291.491.192.8.
82.682.883.983.084.3.
-72.676.275.179.8.
-80.284.589.392.6.
-75.284.586.991.1.avg..59.559.655.568.173.1.
-74.477.077.281.0.table 2: xtreme results on each dataset (as of acl submission deadline).
averaged results on the four cate-gories can be found at leaderboard: https://sites.research.google/xtreme.
â€œâ€ â€ and â€œâ€¡â€ indicates resultsfrom hu et al.
(2020) and fang et al.
(2020), respectively.
â€œ*â€ indicates the results obtained by our implementation.
the detailed results for each language are in appendix d..under the translate-train-all setting, we also showthe results of xlm-r using the same ï¬ne-tuninghyperparameters as veco ..bilingual instance as the input of xlm-r. it furtherdemonstrates the effectiveness of veco and itsspecialized ï¬ne-tuning method..5.2 experimental results.
the detailed test results of nine tasks on thextreme benchmark are shown in table 2.itdemonstrates that the proposed veco outperformsprevious cross-lingual models on all datasets.
com-pared to xlm-r, it averagely scores 5.0 and 6.6points higher under the cross-lingual transfer andtranslation-train-all settings, respectively..in the cross-lingual transfer setting, veco deliv-ers a large improvement compared to xlm-r, espe-cially on zero-shot sentence retrieval tasks (bucc,tatoeba).
this phenomenon reï¬‚ects that our modelcan better build the interdependence between lan-guages.
thus it can better mine parallel sentencesin a multilingual corpus..under the translation-train-all setting, it canbe observed that veco with plug-in ï¬ne-tuning(vecoin) is better than plug-out ï¬ne-tuning(vecoout).
we conclude the reasons as two-fold.
on the input side, the plug-out ï¬ne-tuning individ-ually takes multilingual instances as input, whilethe plug-in ï¬ne-tuning considers the bilingual in-stances 6 at each run.
on the model side, theplug-in ï¬ne-tuning can encourage correspondenceacross language via the cross-attention module.
note that the plug-in ï¬ne-tuning method also out-performs filter (fang et al., 2020), an enhancedcross-lingual ï¬ne-tuning method that also takes the.
6english instance with its translated one..we conclude the reasons for the above perfor-mance improvement as two-fold: 1) the introduc-tion of bilingual data during pre-training, which isa direct way to enhance the cross-lingual abilityof the model; 2) stronger ability to enhance theinterdependence and fusion among languages viathe proposed ca-mlm pre-training tasks.
to ana-lyze which plays a leading role, we conduct a setof more fair experiments in section 7..6 experiments on cross-lingual.
generation tasks.
6.1 experimental setup.
datasets we choose the machine translation(mt) task, a typical cross-lingual generation sce-nario.
in order to illustrate the generality of ourapproach and have a fair comparison with themost recent state-of-the-art transformer work (liuet al., 2020a), we choose two most widely useddatasets: wmt14 englishâ†’german (en-de) andenglishâ†’french (en-fr) translation.
wmt14 en-de is a medium-resource dataset that provides4.5m pairs for training and validation.
we adoptstandard newstest2014 as the test set.
wmt14en-fr is a high-resource dataset that contains36m pairs of parallel sentences.
we use new-stest2012+newstest2013 for validation and new-stest2016 for test.
we measure case-insensitive to-kenized bleu with multi-bleu.perl and de-.
3985model.
wmt14 en-fr.
wmt14 en-de.
bleu sacrebleu bleu sacrebleu.
randomly initializebaselineliu et al.
(2020a).
42.943.8.randomly initialize + more bilingual data*baseline*.
-.
-.
cross-lingual model initializembartmraspxlm-rveco.
43.244.343.844.5.
40.441.8.
41.041.741.242.0.
28.730.1.
30.6.
30.030.330.931.7.
27.829.5.
29.5.
29.1-29.930.6.table 3: (left) results on machine translation.
(right) learning curves of different initialization methods..tokenized sacrebleu 7 to avoid the inï¬‚uence ofdifferent tokenization and normalization betweenmodels (post, 2018)..fine-tuning setting we ï¬ne-tune our model us-ing fairseq 8 toolkit and adopt comparable train-ing settings with baselines.
we run wmt 14 en-de and en-fr mt experiments on 16 and 32 v100gpus, respectively.
the batch size is 64k for en-de and 256k for en-fr.
the total training updatesare set to 100k.
the learning rate is 1e-4/2e-4, withlinear warm-up over the ï¬rst 16k steps and lineardecay.
we average the last 10 checkpoints and usebeam search with a beam size of 5..baselines we consider two types of transformerbaselines: randomly initialized and cross-lingualmodels initialized.
for random initialization, wereproduce a transformer baseline that adopts thesame architecture and ï¬ne-tuning hyperparame-ters as veco but with random initialization.
be-sides, we compare to the state-of-the-art deeptransformer (liu et al., 2020a).
for cross-lingualencoder-decoder models, we include mbart (liuet al., 2020b) and mrasp (lin et al., 2020), whichshow impressive results on mt.
note that sincewe tied the self-attention weights of each encoderlayer with each decoder layer, the whole parame-ters of mbart and veco are comparable.
wealso conduct the wmt experiments for xlm-r,following the totally same ï¬ne-tuning settings asveco , but leaving the encoder-to-decoder atten-tion un-initialized..7hash: bleu+case.mixed+lang.en-{de,fr}+numrefs.1+.
smooth.exp+test.wmt14/full+tok.13a+version.1.4.9.
8https://github.com/pytorch/fairseq.
6.2 experimental results.
table 3 (left) shows the results on the machinetranslation.
we can observe that veco can largelyoutperform the randomly initialized same-sizedtransformer baseline by 2.3 bleu points.
more-over, it even beats the (randomly initialized) state-of-the-art deep-transformer (liu et al., 2020a),which is three times deep as veco .
among thecross-lingual models, veco can consistently out-perform the best models, averaged on two datasets,by 0.8 bleu points..table 3 (right) displays the bleu scores ofsame-sized models during training.
we ï¬nd thatveco initialized model can get a surprising morethan 28 sacrebleu score just after 10 epochs,which is better than the ï¬nal score of the ran-domly initialized model at 35 epochs.
it reveals thatveco can provide a fairly good initialization forthe machine translation model, which can convergequickly and further boost the results..one might suspect that the main reason for theperformance improvement is leveraging parallelcorpus during pre-training.
to ï¬gure it out, weconduct a more comparable experiment.
we ï¬rsttrain an out-of-domain transformer model usingthe whole en-de parallel data (âˆ¼ 68m) used inveco pre-training, and then continue to train themodel on the in-domain wmt14 en-de trainingdataset.
results are shown in table 3 (left) markedwith *.
under this set of a totally fair comparison,veco still maintains a lead of 1.1 bleu score.
this directly conï¬rms that the improvement in mtis not only due to the use of bilingual data.
moreimportantly, ca-mlm ensures better use of bilin-gual and large-scale unlabeled multilingual corpus..3986101520253035epochs252627282930sacrebleuveco init.xlm-r init.random init.
method.
#layers.
wmt14 en-debleu sacrebleu.
randomly initialize.
veco initialize.
36.first-3last-3first-6last-6full-24.
28.528.6.
30.831.231.131.531.7.
27.627.7.
29.830.330.130.530.6.table 4: results of utilizing veco to initialize deepencoder and shallow decoder (3/6-layer) transformers..data models.
tasks.
xnli.
iwslt.
mono..bili..xlmmbartveco.
xlmmbartveco.
mlmmlmca-mlm.
59.857.360.6.mlm+tlmmlm+tlm.
64.560.8ca-mlm +tlm 67.7.
33.732.934.0.
33.934.536.0.table 5: ablation study of small-sized models on xnliand iwslt14 de-en translation dataset..6.3 potential of initializing shallow decoder.
online translation applications usually have a re-striction of inference time.
the most direct wayis to reduce the decoder layers since previous mtworks (liu et al., 2020a) have shown that deeper en-coders are more worthwhile than deeper decoders.
based on this, we also explore the potential of theveco to initialize deep encoder and shallow de-coder transformers, which is a blank in the cross-lingual pre-training works..table 4 contrasts two ways of initializing a trans-former with n decoder layers (n < 24) via select-ing: (1) the ï¬rst n layers; (2) the last n layers froma 24-layer pre-trained veco model.
we considern = {3, 6} to conduct experiments.
we ï¬nd that se-lecting the last n layers exhibits better performancethan selecting the ï¬rst n layers.
it reveals thatthe last several layers play a more important rolein making predictions over the whole vocabulary.
moreover, we can ï¬nd that there is 0.2âˆ¼0.3 bleugain when increasing the decoder layers from 3 to 6.however, we observe that only marginal improve-ment can be gained when further increasing thedecoder layers from 6 to 24, which is also in linewith the ï¬ndings in liu et al.
(2020a).
regardlessof the initialization method, the veco initializedmodel can gain consistent 1âˆ¼2 bleu improve-ment over the randomly initialized model..7 analysis and ablation study.
we perform an ablation study to investigate wherethe improvement in cross-lingual nlu and nlgtasks mainly comes from.
speciï¬cally, there arethree main aspects we have studied:.
1. how much performance improvement comesfrom the parallel translation corpus used inpre-training?.
task, especially compared to the mlm andtlm pre-training tasks?.
3. how about pre-training a sequence-to-sequence model like mbart for nlu andnlg tasks?.
to ï¬gure out these questions, we train xlm,mbart and veco model from scratch using thesame datasets and parameter settings (see appendixa for more details).
all of them is pre-trained viamlm and tlm tasks.
note that the mlm task gen-erally refers to predict the masked words of sourcelanguage, while the tlm task generally refers topredict the words of the target language.
speciï¬-cally for mbart that is under the framework ofencoder-decoder, the input of encoder is masked se-quence Ë†x, and the target of decoder is the maskedwords of source input x (for mlm task), or theparallel sentence y (for tlm task)..table 5 shows the results of two representa-tive datasets of cross-lingual nlu and nlg.
wecan observe that, when using monolingual corpusonly, veco can outperform xlm by 0.8 pointson the xnli dataset and 0.3 bleu scores onthe iwslt14 de-en translation dataset.
it sug-gests that the ca-mlm can still beneï¬t fromadjacent sentences in monolingual corpus 9, tobe equipped with a stronger ability of contextualmodeling.
moreover, when pre-training both onthe monolingual and bilingual corpus, veco caneven achieve a larger improvement compared toxlm, with 3.2 and 2.1 points improvement on twodatasets, respectively.
it reveals that ca-mlm ob-jective of veco can better utilize the bilingualcorpus, compared to only optimized by tlm andmlm of xlm..moreover, we ï¬nd that pre-training a sequence-to-sequence model like mbart (liu et al., 2020b).
9as noted in section 4, we take two adjacent sentences in.
2. how effective of the ca-mlm pre-training.
the monolingual corpus as (x, y)..3987performs worst on nlu tasks like xnli 10, al-most 6 points worse than veco and near 2 pointsworse than xlm.
one possible explanation couldbe that the unidirectional language modeling in thedecoder might be sub-optimal for nlu tasks.
andeven on the machine translation task, mbart stillperforms worse than veco when pre-training onthe same bilingual datasets.
we conclude that it isbecause that veco can do better in the contextualmodeling of source input x via a explicit maskedlanguage modeling objective in eq 10 applied tox2 in figure 2 (c)..8 related work.
mbert (devlin et al., 2019) is a key step towardsbuilding a uniï¬ed contextual language represen-tation over multiple languages.
it simply sharesall languagesâ€™ vocabulary and trains a bidirec-tional transformer encoder, achieving promisingresults in various cross-lingual nlu tasks.
therehave been several extensions that follow the sameencoder-only backbone as mbert.
the main dif-ference is the introduction of more training cor-pus (e.g., bilingual data) and pre-training tasks.
xlm (lample and conneau, 2019) utilizes bothmonolingual and bilingual corpus to perform themasked language modeling.
xlm-r (conneauet al., 2019) extends to be built on roberta (liuet al., 2019) using larger monolingual training data.
other works (huang et al., 2019; yang et al., 2020;chi et al., 2020b) propose new pre-training tasks toutilize the bilingual data better.
however, thereare two main drawbacks of these works.
first,they mainly rely on the self-attention module inthe transformer encoder to implicitly build the in-terdependence between languages, leading to fewattention patterns across languages due to the â€œlazyâ€network.
second, even though they show impres-sive performance improvement on cross-lingualunderstanding tasks like xnli, only marginal im-provement has been gained on cross-lingual gener-ation tasks like machine translation, especially onhigh-resource languages..a feasible solution for cross-language gener-ation is to pre-train a denoising auto-encoderit extendslike mbart (liu et al., 2020b).
bart (lewis et al., 2019)to the multilin-gual setting, demonstrating signiï¬cant gains inlow/medium-resource machine translation, but.
10we follow bart (lewis et al., 2019) by utilizing the ï¬nal.
representation from the decoder for classiï¬cation tasks..with a decrease in high resource languages.
unlikembart, chi et al.
(2020a) ï¬rst trains an encodervia mlm and then frozen the encoder to train thedecoder only via two generative tasks.
a similarapproach is also proposed in liang et al.
(2020) andlin et al.
(2020), with the main difference in thejoint training of encoder-decoder with code-switchtricks.
however, all these cross-lingual models em-phasize training a dedicated model for nlg.
thusthey may hurt the nlu capabilities of the model.
the ablation study in section 7 also validates that itis sub-optimal to train an encoder-encoder networkfor nlu tasks..this paper endeavors to build a uniï¬ed cross-lingual model for nlu and nlg tasks via a plug-and-play cross-attention module.
more importantly,the cross-attention module plays a role in the ex-plicit alignment of encoded representations of dif-ferent languages, thus largely contributing to build-ing a uniï¬ed cross-lingual model..9 conclusion.
we present veco, a variable and ï¬‚exible cross-lingual pre-training model, targets at explicitly cap-turing the interdependence between languages viaa plug-and-play cross-attention module.
based onthe ï¬‚exible characteristics, veco can initializeboth nlu preferred encoder-only and nlg spe-cialized encoder-decoder transformer.
moreover,we also introduce a plug-in ï¬ne-tuning approachto encourage the fusion between languages, com-bining the feature of veco and cross-languagedownstream tasks..taken together, veco achieves consistent im-provements on various language understanding andgeneration tasks, broadening the way of thinkingabout pre-trained backbone architecture and ï¬ne-tuning methods under the cross-lingual scenario..references.
mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, acl 2020, online, july 5-10,2020, pages 4623â€“4637.
association for computa-tional linguistics..mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
trans.
assoc.
comput.
linguistics, 7:597â€“610..3988thomas bachlechner, bodhisattwa prasad majumder,huanru henry mao, garrison w cottrell, and ju-rezero is all you need:lian mcauley.
2020.arxiv preprintfast convergence at large depth.
arxiv:2003.04887..xinlei chen and kaiming he.
2020..simple siamese representation learning.
abs/2011.10566..exploringcorr,.
zewen chi, li dong, furu wei, wenhui wang, xian-ling mao, and heyan huang.
2020a.
cross-lingualnatural language generation via pre-training.
in pro-ceedings of the aaai conference on artiï¬cial intel-ligence..zewen chi, li dong, furu wei, nan yang, sak-sham singhal, wenhui wang, xia song, xian-lingmao, heyan huang, and ming zhou.
2020b.
in-foxlm: an information-theoretic framework forcross-lingual language model pre-training.
arxivpreprint arxiv:2007.07834..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answeringin transac-in typologically diverse languages.
tions of the association of computational linguis-tics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmÂ´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale.
arxivpreprint arxiv:1911.02116..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of emnlp 2018, pages 2475â€“2485..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl-hlt..yuwei fang, shuohang wang, zhe gan, siqi sun,and jingjing liu.
2020. filter: an enhanced fu-sion method for cross-lingual language understand-ing.
arxiv preprint arxiv:2009.05166..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual general-ization.
arxiv preprint arxiv:2003.11080..haoyang huang, yaobo liang, nan duan, ming gong,linjun shou, daxin jiang, and ming zhou.
2019.unicoder: a universal language encoder by pre-arxivtraining with multiple cross-lingual tasks.
preprint arxiv:1909.00964..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
arxiv preprint arxiv:1808.06226..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
arxiv preprintarxiv:1901.07291..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov,and luke zettlemoyer.
2019.bart: denoising sequence-to-sequencepre-training for natural language generation, trans-arxiv preprintlation,arxiv:1910.13461..and comprehension..patrick lewis, barlas oË˜guz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
in proceedings of acl 2020..yaobo liang, nan duan, yeyun gong, ning wu, fen-fei guo, weizhen qi, ming gong, linjun shou,daxin jiang, guihong cao, et al.
2020. xglue:a new benchmark dataset for cross-lingual pre-arxivtraining, understanding and generation.
preprint arxiv:2004.01401..zehui lin, xiao pan, mingxuan wang, xipeng qiu,jiangtao feng, hao zhou, and lei li.
2020. pre-training multilingual neural machine translation byin proceedingsleveraging alignment information.
of the 2020 conference on empirical methods innatural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 2649â€“2663.
as-sociation for computational linguistics..xiaodong liu, kevin duh, liyuan liu, and jianfenggao.
2020a.
very deep transformers for neural ma-chine translation.
arxiv preprint arxiv:2008.07772..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020b.
multilingual denoisingpre-training for neural machine translation.
arxivpreprint arxiv:2001.08210..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..joakim nivre, mitchell abrams,.
Ë‡zeljko agiÂ´c, larsahrenberg, lene antonsen, maria jesus aranzabe,gashaw arutie, masayuki asahara, luma ateyah,mohammed attia, et al.
2018. universal dependen-cies 2.2..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of acl 2017, pages 1946â€“1958..3989matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation..shuo ren, yu wu, shujie liu, ming zhou, andshuai ma.
2019. explicit cross-lingual pre-trainingvolumefor unsupervised machine translation.
abs/1909.00180..aditya siddhant, melvin johnson, henry tsai, naveenari, jason riesa, ankur bapna, orhan firat, andkarthik raman.
2020. evaluating the cross-lingualeffectiveness of massively multilingual neural ma-chine translation.
in the thirty-fourth aaai con-ference on artiï¬cial intelligence, aaai 2020, pages8854â€“8861..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, Å‚ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems..guillaume wenzek, marie-anne lachaux, alexis con-neau, vishrav chaudhary, francisco guzman, ar-mand joulin, and edouard grave.
2019. ccnet: ex-tracting high quality monolingual datasets from webcrawl data.
arxiv preprint arxiv:1911.00359..jian yang, shuming ma, dongdong zhang, shuangzhiwu, zhoujun li, and ming zhou.
2020. alternatinglanguage modeling for cross-lingual pre-training.
inproceedings of the aaai conference on artiï¬cial in-telligence..yinfei yang, yuan zhang, chris tar, and jasonpaws-x: a cross-lingual ad-baldridge.
2019.versarial dataset for paraphrase identiï¬cation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 3685â€“3690. association for computational linguistics..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
incorporating bert into neural machine2020.in 8th international conference ontranslation.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..pierre zweigenbaum, serge sharoff, and reinhardrapp.
2017. overview of the second bucc sharedtask: spotting parallel sentences in comparable cor-pora.
in proceedings of the 10th workshop on build-ing and using comparable corpora, bucc@acl2017, vancouver, canada, august 3, 2017, pages 60â€“67. association for computational linguistics..et al., 2019).
there are 1.36tb monolingual datain 50 languages before up/down-sampling.
table 6reports the language codes and statistics of pre-training data.
we collect bilingual corpus in 50 lan-guages from the opus website11, including mul-tiun, unpc, bombay, eu-bookshop, opensubti-tles2018, tanzil, globalvoices, paracrawl, multi-paracrawl, dgt, tilde, europarl, wikipedia, ecb,ted2013, news-commentary, ubuntu, books,un, infopankki-v1, euconst, and bianet.
in to-tal, there are 1tb bilingual training data beforepre-processing, covering 879 language pairs.
ta-ble 7 lists the statistics for each language pair.
wethen apply subword tokenization directly on rawtext data using sentence piece model (kudo andrichardson, 2018) without any additional prepro-cessing..we use the whole corpus to train veco anda subset (âˆ¼ 1/4) that contains 33 languages totrain small-sized xlm, mbart and veco .
thefull set of pre-training hyperparameters for small-sized and large-sized veco (default) are listed intable 8..b more details about illustrated.
attention.
the models illustrated with attention patterns infigure 1 of main paper (not appendix), are thebase-sized xlm 12 and xlm-r 13. we show theattention scores averaged on all heads in the middlelayer..c fine-tuning details on xterme.
we select the model with the best average resultover all the languages on the dev sets, by searchingthe learning rate over [5e-6,8e-6,1e-5,2e-5,3e-5]for the cross-lingual transfer setting and [5e-6,6e-6,7e-6,8e-6,9e-6] for translate-train-all setting,training epoch over [3,5,10], and batch size over[16,32,64]..d detailed results on xtreme.
the detailed results of each xtreme task underthe cross-lingual transfer and translate-train-all set-tings on all languages are listed in the followingtables..a pre-training details.
for monolingual data, following xlm-r (con-neau et al., 2019), we build a clean commoncrawlcorpus using an open-source tool ccnet (wenzek.
11http://opus.nlpl.eu/12https://huggingface.co/.
xlm-mlm-tlm-xnli15-1024.
13https://huggingface.co/.
xlm-roberta-base.
3990language.
#document(m).
#sentence(m).
size(gb).
afarbgbncsdeeleneseteufrfaï¬fygugdhehihuiditjajvkakkkoltlvmsmlmymrplptnenlrurosiswtatetrthtlviuryozh.
0.0232.8230.9190.7503.98021.4101.740130.08717.5690.3470.34215.8192.5061.5300.0270.0390.0090.7550.5361.8163.4179.33627.9670.0020.1410.06111.6090.5520.2810.3340.1620.0450.0596.6428.6230.0806.51335.8871.9440.1320.0570.8760.28818.5476.2780.16612.1830.4600.000227.067.
382.735.
0.52242.65914.7439.21755.754310.94224.3342,215.534267.7645.2525.216267.88843.57023.7900.5370.5190.12612.3387.30329.96260.908133.006588.9260.1381.7561.545227.3967.9964.1593.7622.6150.8930.70893.760128.1070.82985.997580.29131.9292.9270.94520.3764.995291.081117.8265.611234.0717.5090.003497.408.
0.10711.7865.2174.2649.66866.3339.737479.09958.7740.8770.61358.02313.8313.9400.0540.2280.0203.0733.7626.42111.52830.85471.7850.0300.7660.44827.8371.4800.7980.4551.0250.3060.36519.08225.6120.42916.648203.1057.0560.9020.1796.4221.72140.32127.9410.67937.9192.0030.000587.005.total.
6,475.444.
1,360.526.table 6: the statistics of monolingual pre-training corpus..3991pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
pair.
#sent(k).
af-araf-bgaf-bnaf-csaf-deaf-elaf-enaf-esaf-etaf-faaf-ï¬af-fraf-fyaf-heaf-hiaf-huaf-idaf-itaf-jaaf-ltaf-lvaf-mlaf-msaf-nlaf-plaf-ptaf-roaf-ruaf-siaf-taaf-thaf-traf-viar-bgar-bnar-csar-dear-elar-enar-esar-etar-euar-faar-ï¬ar-frar-hear-hiar-huar-idar-itar-jaar-kaar-kkar-koar-ltar-lvar-mlar-msar-myar-near-nlar-plar-ptar-roar-ruar-siar-swar-taar-tear-thar-tlar-trar-urar-viar-yoar-zhbg-bnbg-csbg-debg-elbg-enbg-esbg-etbg-eubg-fabg-ï¬bg-frbg-hebg-hibg-hubg-idbg-itbg-jabg-kabg-kkbg-kobg-ltbg-lvbg-mlbg-ms.12.3418.191.1917.9319.2829.8344.7034.316.343.0710.2518.5636.9414.531.1516.324.5615.011.980.651.082.181.3122.611096.8922.6832.1915.410.981.132.0824.223.3023090.32378.2824147.2512733.6522486.6060392.5557561.299738.71578.305679.8517169.9050632.5220577.1696.2623770.386989.5620070.271847.98161.651.281262.601177.67433.66348.331555.330.180.4121273.7824819.8320379.5626187.1545992.72483.9616.5237.1519.332959.967.5826683.62126.332875.000.0128120.22310.1234502.4619852.8132130.8647247.0439728.5515188.54605.104927.5325191.0130185.9822887.4071.3834293.447047.2127649.851658.40193.273.401056.965604.114748.15283.771506.56.bg-mybg-nebg-nlbg-plbg-ptbg-robg-rubg-sibg-swbg-tabg-tebg-thbg-tlbg-trbg-urbg-vibg-zhbn-csbn-debn-elbn-enbn-esbn-etbn-eubn-fabn-ï¬bn-frbn-hebn-hibn-hubn-idbn-itbn-jabn-kabn-kobn-ltbn-lvbn-mlbn-msbn-mybn-nebn-nlbn-ptbn-robn-rubn-sibn-swbn-tabn-thbn-tlbn-trbn-urbn-vibn-zhcs-decs-elcs-encs-escs-etcs-eucs-facs-ï¬cs-frcs-hecs-hics-hucs-idcs-itcs-jacs-kacs-kkcs-kocs-ltcs-lvcs-mlcs-mscs-mycs-necs-nlcs-ptcs-rocs-rucs-sics-swcs-tacs-tecs-thcs-tlcs-trcs-urcs-vics-zhde-elde-ende-esde-etde-eude-fade-ï¬de-fr.
0.080.0130757.5033043.0330058.5438925.5217423.43460.5010.8027.1417.142733.846.6931179.3571.602855.13746.27340.51346.51340.94752.08480.35252.6842.42391.89279.35373.13302.6238.68321.36360.65301.31142.198.6893.9296.2441.2193.14203.840.780.78331.34333.59337.94392.1547.4923.9115.67129.602.05441.74108.74219.5785.2424049.8435372.2854470.4744962.4217819.46686.535417.4828031.4734876.0224503.2986.8639272.927310.2733935.961806.97163.351.261199.627694.126745.84319.931592.170.080.0734427.0732469.0139226.3119703.43454.2617.3432.8118.722858.537.4432797.28122.873040.14894.8730170.6483872.4741634.8015186.40534.933948.1425753.0644392.06.de-hede-hide-hude-idde-itde-jade-kade-kkde-kode-ltde-lvde-mlde-msde-myde-nede-nlde-ptde-rode-rude-side-swde-tade-tede-thde-tlde-trde-urde-vide-zhel-enel-esel-etel-euel-fael-ï¬el-frel-heel-hiel-huel-idel-itel-jael-kael-kkel-koel-ltel-lvel-mlel-msel-myel-neel-nlel-ptel-roel-ruel-siel-swel-tael-teel-thel-tlel-trel-urel-viel-yoel-zhen-esen-eten-euen-faen-ï¬en-fren-fyen-gden-heen-hien-huen-iden-iten-jaen-kaen-kken-koen-lten-lven-mlen-msen-myen-neen-nlen-plen-pten-roen-ruen-sien-swen-taen-teen-then-tl.
12751.69106.1124409.404786.8935936.621472.72123.123.72776.899134.998532.06294.161228.820.680.2834909.4932610.1024261.8210904.25324.8645.6142.3212.811695.5312.9117579.53218.892284.70587.9655078.4646876.2116463.57673.935137.5228885.6538560.8422042.8562.2634559.757098.2534337.631740.08167.392.331130.947400.426549.40302.851547.630.551.0437188.7835491.5437986.2617052.36466.444.8520.4418.102505.7110.1331048.8824.362966.140.11649.81156560.0022284.30805.787462.5242783.36161519.91126.1947.0230028.281844.3855233.879677.3376257.212177.89199.983.711493.9510992.899883.08573.952050.832.432.8965918.5459729.7761861.3660415.4665105.13601.16171.65125.9627.223375.0716.03.en-tren-uren-vien-yoen-zhes-etes-eues-faes-ï¬es-fres-hees-hies-hues-ides-ites-jaes-kaes-kkes-koes-ltes-lves-mles-mses-myes-nees-nles-ptes-roes-rues-sies-swes-taes-tees-thes-tles-tres-ures-vies-yoes-zhet-euet-faet-ï¬et-fret-fyet-heet-hiet-huet-idet-itet-jaet-kaet-kket-koet-ltet-lvet-mlet-mset-nlet-plet-ptet-roet-ruet-siet-swet-taet-teet-thet-tlet-tret-uret-viet-zheu-faeu-ï¬eu-freu-heeu-hieu-hueu-ideu-iteu-jaeu-kaeu-koeu-lteu-lveu-mleu-mseu-nleu-pteu-roeu-rueu-sieu-taeu-teeu-theu-tleu-treu-ureu-vi.
46584.82781.603563.390.1328952.0218090.74793.595696.7034222.0796233.2127060.4985.3543947.788015.6949423.511929.41181.192.481229.507702.996703.10339.711731.362.502.8746908.7947542.2648229.6055569.05512.2241.3331.1921.762976.4913.5539805.0279.443215.160.1228688.60406.333085.4115969.0815697.5951.639814.4943.9816819.434282.2314462.111176.51110.021.14492.797431.176728.85179.991135.8416560.6319633.0816768.4515880.626630.25331.220.0114.3414.441746.503.0911408.8219.522048.37405.30245.78581.61636.16566.719.98663.68307.85568.66139.149.4272.17108.1236.8142.72129.20619.88641.30715.99435.1234.563.350.7380.752.60722.772.01201.28.eu-zhfa-ï¬fa-frfa-hefa-hifa-hufa-idfa-itfa-jafa-kafa-kkfa-kofa-ltfa-lvfa-mlfa-msfa-myfa-nefa-nlfa-ptfa-rofa-rufa-sifa-swfa-tafa-tefa-thfa-tlfa-trfa-urfa-vifa-zhï¬-frï¬-heï¬-hiï¬-huï¬-idï¬-itï¬-jaï¬-kaï¬-kkï¬-koï¬-ltï¬-lvï¬-mlï¬-msï¬-nlï¬-plï¬-ptï¬-roï¬-ruï¬-siï¬-swï¬-taï¬-teï¬-thï¬-tlï¬-trï¬-urï¬-viï¬-zhfr-hefr-hifr-hufr-idfr-itfr-jafr-kafr-kkfr-kofr-ltfr-lvfr-mlfr-msfr-myfr-nefr-nlfr-ptfr-rofr-rufr-sifr-swfr-tafr-tefr-thfr-tlfr-trfr-urfr-vifr-yofr-zhfy-esfy-hefy-itfy-jafy-plfy-ptfy-rufy-swfy-tr.
19.764485.624507.064944.80186.235201.513220.004243.561072.1496.321.01627.97615.78228.40308.491072.220.060.015010.644998.095714.734205.20292.7869.5183.3010.111201.047.026217.24568.001514.04372.1028973.8117820.4955.6027350.305806.3626756.851599.82148.423.41859.317507.006732.38232.481276.9630693.7229451.8729269.5027988.1312403.26391.990.0220.0817.132288.655.9122551.9919.432517.08630.1221218.8868.3137027.576235.2941162.371608.52139.631.34991.609440.348569.67278.471423.081.471.4547363.7042850.1337249.8054231.81393.4829.3224.0311.932325.2213.1829245.9173.992752.320.1228008.7749.1244.0647.8837.6149.3795.8145.830.3745.40.fy-vigd-esgd-itgd-plgd-ptgd-rugd-trhe-hihe-huhe-idhe-ithe-jahe-kahe-kkhe-kohe-lthe-lvhe-mlhe-mshe-myhe-nlhe-plhe-pthe-rohe-ruhe-sihe-swhe-tahe-tehe-thhe-tlhe-trhe-urhe-vihe-zhhi-huhi-idhi-ithi-jahi-kahi-kohi-lthi-lvhi-mlhi-mshi-myhi-nehi-nlhi-plhi-pthi-rohi-ruhi-sihi-swhi-tahi-tehi-thhi-tlhi-trhi-urhi-vihi-zhhu-idhu-ithu-jahu-kahu-kkhu-kohu-lthu-lvhu-mlhu-mshu-myhu-nlhu-plhu-pthu-rohu-ruhu-sihu-swhu-tahu-tehu-thhu-tlhu-trhu-urhu-vihu-zhid-itid-jaid-kaid-kkid-koid-ltid-lvid-mlid-msid-myid-neid-nl.
34.9521.6213.2612.2918.9010.3914.1257.8523959.876362.2919908.661683.29149.062.381094.721220.91461.81250.071455.610.0522186.6124962.2321226.3626370.1514873.77435.870.0623.9918.652666.006.5825179.3220.572813.73563.2460.0585.8560.1246.140.8033.6623.6712.6130.2840.380.010.0492.46681.0862.4482.89142.5311.4112.5241.0023.1837.530.51176.39101.1032.9925.577253.8133513.061767.63165.842.581168.667623.586776.32279.131581.430.0633904.3439869.1431715.1938807.6119172.99460.990.6820.6317.572867.2310.7932494.9023.322974.61730.705831.161271.3185.071.03605.78855.43342.36230.671614.630.110.076493.33.id-ptid-roid-ruid-siid-swid-taid-teid-thid-tlid-trid-urid-viid-zhit-jait-kait-kkit-koit-ltit-lvit-mlit-msit-myit-neit-nlit-plit-ptit-roit-ruit-siit-swit-tait-teit-thit-tlit-trit-urit-viit-yoit-zhja-kaja-kkja-koja-ltja-lvja-mlja-msja-nlja-plja-ptja-roja-ruja-sija-swja-taja-teja-thja-tlja-trja-urja-vija-zhka-koka-ltka-lvka-mlka-mska-nlka-ptka-roka-ruka-sika-thka-tlka-trka-urka-vika-zhkk-ltkk-lvkk-mskk-nlkk-plkk-ptkk-rokk-rukk-thkk-trkk-viko-ltko-lvko-mlko-msko-myko-neko-nlko-plko-ptko-roko-ruko-si.
ko-sw6825.29ko-ta7944.59ko-te5039.44ko-th366.00ko-tl30.56ko-tr35.37ko-ur13.30ko-vi1562.94ko-zh7.80lt-lv8017.99lt-ml172.71lt-ms2081.70lt-nl356.46lt-pl1613.05lt-pt106.70lt-ro2.54lt-ru1125.97lt-si7359.92lt-sw6607.27lt-ta235.96lt-te1269.97lt-th0.36lt-tl1.02lt-tr37644.29lt-ur35037.31lt-vi35301.98lt-zh32153.38lv-ml17669.12lv-ms366.97lv-nl15.77lv-pl17.39lv-pt9.93lv-ro2447.55lv-ru13.30lv-si25770.29lv-sw69.89lv-ta2542.41lv-te0.10lv-th473.74lv-tr35.37lv-ur1.21lv-vi308.30281.74lv-zh99.97 ml-ms79.78 ml-nl489.33 ml-pt1716.42 ml-ro3295.60 ml-ru1756.87 ml-si1843.14 ml-sw1491.65 ml-ta162.96 ml-th6.24 ml-tl18.92 ml-tr5.68 ml-ur632.26 ml-vi10.06 ml-zh1896.56 ms-nl61.41 ms-pt679.31 ms-ro104.37 ms-ru17.13 ms-si30.49 ms-sw10.71 ms-ta6.56 ms-te31.86 ms-th155.10 ms-tl165.00 ms-tr182.79 ms-ur104.82 ms-vi7.96 ms-zh43.37 my-nl1.27 my-pt178.79 my-ro1.98 my-ru53.58 my-sw6.52 my-tr0.83 my-ur1.13 my-zhne-nl1.12ne-pt1.85ne-ro77.88ne-ru3.35ne-sw2.35ne-tr2.22ne-ur0.93ne-zh2.59nl-pt1.18nl-ro148.54nl-ru57.10nl-si42.92nl-sw291.25nl-ta0.12nl-te0.01nl-th1120.75nl-tl2722.47nl-tr1119.49nl-ur1242.76nl-vi959.46nl-zh58.66.pl-espl-ptpl-rupl-swpl-tlpl-trpl-urpl-vipt-ropt-rupt-sipt-swpt-tapt-tept-thpt-tlpt-trpt-urpt-vipt-yopt-zhro-ruro-siro-swro-taro-tero-thro-tlro-trro-urro-viro-zhru-siru-swru-taru-teru-thru-tlru-trru-urru-viru-yoru-zhsi-tasi-tesi-thsi-tlsi-trsi-ursi-visi-zhsw-tasw-thsw-trsw-ursw-yosw-zhta-teta-thta-trta-urta-vita-zhte-thte-trte-vith-tlth-trth-urth-vith-zhtl-trtl-vitr-urtr-vitr-zhur-viur-zhvi-zh.
6.7413.740.93230.841.211246.5857.21345.7956.436546.7666.40393.897497.189965.367663.845786.22950.02106.530.0213.049.71263.891.361377.404.47486.8440.6523.32163.286622.819460.936672.144833.77435.7334.420.014.104.01108.92515.301.08209.4014.71101.75268.10280.62325.97310.5928.0112.4715.9081.033.30439.25100.52124.3034.771409.071523.571732.681210.56204.068.9915.244.70413.177.261754.2268.94851.6985.860.100.100.030.810.150.030.020.130.090.380.041.300.050.030.060.0137775.7336051.6016582.78410.9231.3839.2116.072548.148.1828822.22171.712748.28866.75 total.
table 7: the statistics of bilingual (parallel) pre-training corpus..46863.4772437.9319170.231424.021039.3732470.18391.993790.7133802.9514698.48450.4013.0626.3719.322561.0910.3527428.7973.572963.830.05846.4419568.56504.2410.7233.5024.442874.738.6136549.6173.553207.73947.91340.1184.7761.5010.802194.9113.4319317.60417.232289.720.1028138.596.331.85109.383.02492.124.95210.1514.286.246.2491.9550.290.0319.3121.1614.1577.7649.8912.6513.020.9618.849.347.283054.0758.65672.82133.4514.515.86473.083178.031029.2112.5299.78148.22.
6,421,152.04.
3992pre-training hyperparameters.
large.
small.
number of layershidden sizeffn inner hidden sizeattention headsattention head sizeembedding sizemask percent (monolingual/ bilingual)learning rate decaywarmup stepslearning rateadam (cid:15)adam Î²1adam Î²2attention dropoutdropoutweight decaymax sequence length (monolingual/bilingual)batch size (monolingual/bilingual)train stepstotal parameters.
24102440961664102415%/25%linear12k2e-41e-60.90.980.10.10.01512/1281024/4096240k662m.
67683072126476815%/25%linear12k3e-41e-60.90.9990.10.10.01512/1281024/4096240k247m.
table 8: the pre-training hyperparameters..model.
en.
ar.
bg.
de.
el.
es.
fr.
hi.
ru.
sw.th.
tr.
ur.
vi.
zh.
avg..cross-lingual transfer77.2xlm-r79.2vecoout.
88.788.2.translate-train-allxlm-rvecooutvecoin.
88.688.989.3.
82.282.483.7.
83.083.1.
82.582.9.
80.881.2.
83.784.2.
82.282.8.
75.676.2.
79.180.3.
71.274.3.
77.477.0.
78.078.4.
71.771.3.
79.380.4.
78.279.1.
79.279.9.
85.286.087.0.
84.584.785.9.
84.585.385.8.
85.786.287.3.
84.285.886.7.
80.880.181.8.
81.883.083.6.
77.077.279.9.
80.280.982.5.
82.182.884.3.
77.775.377.7.
82.683.184.4.
82.783.084.0.
82.683.084.3.table 9: xnli accuracy scores for each language..model.
en.
de.
es.
fr.
ja.
ko.
zh.
avg..model.
de.
fr.
ru.
zh.
avg..cross-lingual transferxlm-rvecoout.
94.796.2.
89.791.3.translate-train-allvecooutvecoin.
96.496.5.
93.094.4.
90.191.4.
90.492.0.
78.781.8.
79.082.9.
82.385.1.
86.488.7.
93.094.3.
93.594.0.
87.289.0.
86.890.3.
87.991.0.
91.192.8.table 10: paws-x accuracy scores..cross-lingual transfer66.5xlm-r84.6vecoout.
67.589.6.translate-train-allvecooutvecoin.
93.095.4.
88.791.9.
73.587.4.
56.778.5.
66.085.0.
89.993.1.
85.789.9.
89.392.6.table 11: bucc f1 results..model.
af.
ar.
bg.
de.
el.
en.
es.
et.
eu.
fa.
ï¬.fr.
he.
hi.
hu.
id.
it.
cross-lingual transfer67.5xlm-r67.4vecoout.
89.888.3.translate-train-allvecoin.
92.5.cross-lingual transfer78.1xlm-r79.3vecoout.
15.931.4.translate-train-allvecoin.
45.1.
88.187.4.
88.588.5.
86.386.7.
96.195.9.
88.389.0.
86.587.8.
72.575.1.
70.670.9.
85.886.2.
87.288.9.
68.367.5.
76.476.2.
82.682.9.
72.472.9.
89.489.9.
73.7.
93.4.
91.8.
90.4.
95.2.
91.3.
90.6.
79.1.
79.8.
89.5.
91.4.
79.1.
80.6.
88.4.
74.8.
91.8.ja.
kk.
ko.
mr.nl.
pt.
ru.
ta.
te.
th.
tl.
tr.
ur.
vi.
yo.
zh.
avg..53.953.1.
80.884.3.
89.589.8.
87.688.3.
89.590.2.
65.264.3.
86.685.8.
47.248.0.
92.293.7.
76.377.2.
70.369.2.
56.858.1.
24.626.2.
25.739.4.
73.875.1.
78.0.
63.7.
84.5.
92.7.
90.1.
92.6.
72.6.
88.5.
55.2.
88.8.
76.8.
75.0.
70.5.
24.3.
63.0.
79.8.table 12: pos results (accuracy) for each language..3993cross-lingual transfer78.9xlm-r77.5vecoout.
84.783.8.translate-train-allvecoin.
80.7.cross-lingual transfer56.2xlm-r51.2vecoout.
71.667.1.translate-train-allvecoin.
77.0.cross-lingual transfer86.5 / 75.7xlm-r87.6 / 76.5vecoout.
translate-train-allvecooutvecoin.
88.3/77.990.2/79.5.
model.
en.
af.
ar.
bg.
bn.
de.
el.
es.
et.
eu.
fa.
ï¬.fr.
he.
hi.
hu.
id.
it.
ja.
jv.
53.048.2.
81.483.9.
78.877.2.
78.879.4.
79.579.3.
79.675.4.
79.180.4.
60.968.3.
61.968.2.
79.280.6.
80.580.1.
56.855.0.
73.071.0.
79.880.9.
53.052.9.
81.381.7.
23.219.4.
62.563.2.
82.5.
66.4.
84.1.
78.4.
82.2.
82.4.
79.7.
84.7.
78.2.
68.8.
84.9.
79.1.
69.7.
76.6.
85.1.
77.3.
83.8.
21.3.
70.3.ka.
kk.
ko.
ml.
mr.ms.my.
nl.
pt.
ru.
sw.ta.
te.
th.
tl.
tr.
ur.
vi.
yo.
zh.
60.059.9.
67.863.4.
68.165.0.
57.170.0.
54.356.1.
84.083.4.
81.983.1.
69.171.3.
70.570.5.
59.560.5.
55.856.2.
1.31.4.
73.271.3.
76.180.4.
56.469.3.
79.476.0.
33.637.4.
33.129.1.
67.2.
71.0.
73.3.
74.1.
71.8.
63.8.
85.5.
80.8.
72.8.
77.0.
69.1.
67.5.
2.6.
74.0.
85.2.
71.5.
76.4.
32.8.
31.0.model.
en.
ar.
de.
el.
es.
hi.
ru.
th.
tr.
vi.
zh.
avg..table 13: ner results (f1) for each language..68.6 / 49.073.6 / 56.1.
80.4 / 63.479.8 / 62.2.
79.8 / 61.779.6 / 61.6.
82.0 / 63.981.2 / 61.6.
76.7 / 59.774.7 / 57.6.
80.1 / 64.378.7 / 62.1.
74.2 / 62.872.8 / 60.6.
75.9 / 59.375.1 / 58.3.
79.1 / 59.079.0 / 59.8.
59.3 / 50.069.2 / 59.2.
76.6 / 60.877.3 / 61.8.
76.9/61.181.8/66.4.
80.5/64.685.4/69.8.
81.5/64.185.3/69.0.
84.2/66.887.2/70.8.
78.8/62.583.7/67.9.
80.2/66.185.6/71.6.
77.0/70.480.0/74.7.
77.8/62.282.4/68.6.
82.5/63.785.8/68.3.
71.6/69.474.9/73.1.
79.9/66.383.9/70.9.
table 14: xquad results (f1 / em) for each language..model.
en.
ar.
de.
es.
hi.
vi.
zh.
avg..cross-lingual transfer83.5 / 70.6xlm-r83.6 / 70.5vecoout.
translate-train-allvecooutvecoin.
84.1/71.387.5/75.5.
66.6 / 47.165.0 / 44.6.
70.1 / 54.969.8 / 54.6.
74.1 / 56.673.8 / 55.6.
70.6 / 53.169.1 / 51.4.
74.0 / 52.973.1 / 51.8.
62.1 / 37.067.3 / 43.6.
71.6 / 53.271.7 / 53.2.
67.8/47.172.3/52.1.
70.7/55.875.7/61.1.
74.6/56.678.8/61.6.
71.1/53.476.6/58.6.
74.8/54.479.3/59.1.
68.8/45.872.1/46.8.
73.1/54.977.5/59.3.
table 15: mlqa results (f1 / em) for each language..model.
en.
ar.
bn.
ï¬.id.
ko.
ru.
sw.te.
avg..67.6 / 40.473.1 / 52.8.
64.0 / 47.858.9 / 42.5.
70.5 / 53.270.9 / 55.1.
77.4 / 61.977.2 / 60.0.
31.9 / 10.954.2 / 39.9.
67.0 / 42.166.1 / 37.6.
66.1 / 48.165.8 / 45.7.
70.1 / 43.670.6 / 50.7.
65.1 / 45.067.6 / 49.1.
77.0/57.580.1/60.9.
72.2/56.680.8/68.1.
76.6/59.381.6/65.5.
80.0/64.484.3/69.7.
63.4/52.265.4/50.4.
72.8/50.577.8/55.8.
79.4/67.183.7/74.1.
76.0/58.081.0/63.4.
75.0/58.979.4/63.7.
table 16: tydiqa-golp results (f1 / em) for each language..model.
af.
ar.
bg.
bn.
de.
el.
es.
et.
eu.
fa.
ï¬.fr.
he.
hi.
hu.
id.
it.
ja.
71.686.7.
4357.7.
88.897.5.
61.881.5.
75.794.8.
52.289.7.
35.862.9.
70.582.1.
71.687.9.
73.788.8.
66.474.7.
72.280.7.
65.487.6.
7789.6.
68.389.2.
60.683.2.
91.391.5.
78.184.2.
98.598.9.
89.591.5.
97.497.9.
94.896.4.
79.885.8.
93.195.3.
95.495.9.
93.795.6.
85.889.6.
94.297.0.
93.895.1.
93.094.2.
92.294.1.
92.894.0.jv.
ka.
kk.
ko.
ml.
mr.nl.
pt.
ru.
sw.ta.
te.
th.
tl.
tr.
ur.
vi.
zh.
48.553.9.
61.475.3.
65.480.1.
56.864.2.
80.894.4.
82.292.8.
74.188.6.
20.337.4.
26.461.9.
35.965.8.
29.484.5.
36.752.5.
65.789.3.
24.364.3.
74.785.8.
68.382.7.
74.183.7.
88.791.2.
94.897.1.
82.587.9.
95.997.6.
94.696.1.
92.293.8.
69.782.6.
82.488.9.
91.095.3.
94.795.1.
73.079.8.
95.297.6.
63.891.4.
95.197.2.
93.995.2.table 17: tatoeba results (accuracy) for each language.
cross-lingual transfer71.5 / 56.8xlm-r71.3 / 58.2vecoout.
translate-train-allvecooutvecoin.
77.2/64.879.4/65.2.
cross-lingual transfer47.5xlm-r70.9vecoout.
58.248.2.translate-train-allvecooutvecoin.
80.988.5.
85.188.7.cross-lingual transfer52.1xlm-r58.5vecoout.
14.117.6.translate-train-allvecooutvecoin.
35.149.3.
83.086.6.
3994