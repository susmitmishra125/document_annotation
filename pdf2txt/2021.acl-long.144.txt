causal analysis of syntactic agreement mechanismsin neural language models.
matthew finlayson∗harvard universitycambridge, mamattbnfin@gmail.com.
aaron mueller∗johns hopkins universitybaltimore, mdamueller@jhu.edu.
sebastian gehrmanngoogle researchnew york, nygehrmann@google.com.
stuart shieberharvard universitycambridge, mashieber@seas.harvard.edu.
tal linzen†new york universitynew york, nylinzen@nyu.edu.
yonatan belinkov‡technion – iithaifa, israelbelinkov@technion.ac.il.
abstract.
targeted syntactic evaluations have demon-strated the ability of language models to per-form subject-verb agreement given difﬁcultcontexts.
to elucidate the mechanisms bywhich the models accomplish this behavior,this study applies causal mediation analysis topre-trained neural language models.
we inves-tigate the magnitude of models’ preferencesfor grammatical inﬂections, as well as whetherneurons process subject-verb agreement simi-larly across sentences with different syntacticstructures.
we uncover similarities and differ-ences across architectures and model sizes—notably, that larger models do not necessar-ily learn stronger preferences.
we also ob-serve two distinct mechanisms for producingsubject-verb agreement depending on the syn-tactic structure of the input sentence.
finally,we ﬁnd that language models rely on similarsets of neurons when given sentences with sim-ilar syntactic structure..1.introduction.
targeted syntactic evaluations have shown that neu-ral language models (lms) are able to predict thecorrect token from a set of grammatically mini-mally different continuations with high accuracy,even in difﬁcult contexts (linzen et al., 2016; gu-lordava et al., 2018), for constructions such assubject-verb agreement (van schijndel et al., 2019),ﬁller-gap dependencies (wilcox et al., 2018), andreﬂexive anaphora (marvin and linzen, 2018)..as an illustration of the targeted syntactic eval-uation paradigm, consider the following example,which demonstrates subject-verb agreement acrossan agreement attractor.
here, a model using a linear.
∗equal contribution.
† work done while visiting google research.
‡ supported by the viterbi fellowship in the center for.
computer engineering at the technion..analysis (i.e., inﬂecting based on the most recentnoun) would choose the ungrammatical inﬂection,while a model using a hierarchical analysis wouldchoose the grammatical inﬂection:.
(1) the key to the cabinets is/*are next to the coins..while we have a reasonable understanding of thegenerally correct behavior of lms in such con-texts, the mechanisms that underlie models’ sensi-tivity to syntactic agreement are still not well under-stood.
recent work has performed causal analysesof syntactic agreement units in lstm (hochre-iter and schmidhuber, 1997)-based lms (lakretzet al., 2019; lu et al., 2020) or causal analyses oflstm hidden representations’ impact on syntacticagreement (giulianelli et al., 2018), but the agree-ment mechanisms of transformer-based lms havenot been as extensively investigated.
transformer-based lms’ syntactic generalization abilities aresuperior to those of lstms (hu et al., 2020), whichmakes transformer-based models enticing candi-dates for further analysis..we apply the behavioral-structural method ofcausal mediation analysis (pearl, 2001) to investi-gate syntactic agreement in transformers, follow-ing the approach used by vig et al.
(2020a) for in-terpreting gender bias in pre-trained english lms.
this method allows us to implicate speciﬁc modelcomponents in the observed behavior of a model.
ifwe view a neural lm as a causal graph proceedingfrom inputs to outputs, we can view each modelcomponent (e.g., a neuron) as a mediator.
we mea-sure the contribution of a mediator to the observedoutput behavior by performing controlled interven-tions on input sentences and observing how theychange the probabilities of continuation pairs.
wefocus primarily on gpt-2 (radford et al., 2019), al-though we also analyze transformerxl (dai et al.,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1828–1843august1–6,2021.©2021associationforcomputationallinguistics18282019) and xlnet (yang et al., 2019)..we ﬁnd that both gpt-2 and transformer-xluse two distinct mechanisms to accomplish subject-verb agreement, one of which is active only whenthe subject and verb are adjacent.
conversely,xlnet uses one uniﬁed mechanism across syn-tactic structures.
even though larger models as-sign a higher probability to the correct inﬂectionmore often, this does not necessarily translate to alarger margin between the probability of the cor-rect and incorrect options.
additionally, in largermodels, agreement mechanisms are similar to thosein smaller models, but are more distributed acrosslayers.
finally, we ﬁnd that the most importantneurons for agreement are shared across differentstructures to various extents, and that the degree ofneuron overlap matches well with human intuitionsof syntactic similarity between structures..2 related work.
2.1 targeted syntactic evaluation.
many recent studies have treated neural lms andcontextualized word prediction models—primarilylstm lms (sundermeyer et al., 2012), gpt-2 (radford et al., 2019), and bert (devlin et al.,2019)—as psycholinguistic subjects to be studiedbehaviorally (linzen et al., 2016; gulordava et al.,2018; goldberg, 2019).
some have studied whethermodels prefer grammatical completions in subject-verb agreement contexts (marvin and linzen, 2018;van schijndel et al., 2019; goldberg, 2019; muelleret al., 2020; lakretz et al., 2021; futrell et al.,2019), as well as in ﬁller-gap dependencies (wilcoxet al., 2018, 2019).
these are based on the approachof linzen et al.
(2016), where a model’s ability tosyntactically generalize is measured by its ability tochoose the correct inﬂection in difﬁcult structuralcontexts instantiated by tokens that the model hasnot seen together during training.
in other words,this approach tests whether the model assigns thecorrect inﬂection a higher probability than an in-correct inﬂection given the same context.
thisapproach investigates the output behavior of themodel, but does not inform one of how the modeldoes this or which components are responsible forthe observed behavior..2.2 probing.
a separate line of analysis work has investigatedrepresentations associated with syntactic depen-dencies by deﬁning a family of functions (probes).
that map from model representations to some phe-nomenon that those representations are expected toencode.
for instance, several studies have mappedlm representations to either independent syntac-tic dependencies (belinkov, 2018; liu et al., 2019;tenney et al., 2019b) or full dependency parses(hewitt and manning, 2019; chi et al., 2020) as aproxy for discovering latent syntactic knowledgewithin the model.
most related, giulianelli et al.
(2018) use probes to investigate how lstms han-dle agreement..probing is more difﬁcult to interpret than behav-ioral approaches because the addition of a trainedclassiﬁer introduces confounds (hewitt and liang,2019): most notably, whether the probe maps frommodel representations to the desired output, orlearns the task itself.
probes also only give cor-relational evidence, rather than causal evidence(belinkov and glass, 2019).
see belinkov (2021)for a review of the shortcomings of probes..2.3 causal mediation analysis.
causal inference methods study the change in aresponse variable following an intervention; forexample, how do health outcomes change after apatient stops consuming nicotine products?
causalmediation analysis (robins and greenland, 1992;pearl, 2001; robins, 2003) focuses on the role of amediator in explaining the effect of a treatment onoutcomes.
for example, if a patient stops using to-bacco, are health outcomes mediated by the initialmethod of nicotine delivery (e.g., smoking tobaccovs. patches vs. nicotine gum)?.
this approach lends itself well to interpretingnlp models, as we can view a deep neural net-work as a graphical model from input to outputvia mediators, where mediators can be individualcomponents (e.g., neurons).
for lms, the inter-vention is a change to the input sentence, and theoutcome is a function of the probabilities of a setof continuations..this approach for interpreting nlp models wasintroduced by vig et al.
(2020a), who implicatespeciﬁc neurons and attention heads in mediatinggender bias in various pre-trained lms.
whileone ideally expects equal preferences for male andfemale completions given gender-ambiguous con-texts (for example, given the prompt u “the nursesaid that”, we want p(she|u) ≈ p(he|u)), this isnot the case for subject-verb agreement, where weexpect very strong preferences for grammatically.
1829simple agreement:the athlete confuses/*confuse.
within object relative clause:the friend (that) the lawyers *likes/like.
across one distractor:the kids gently *admires/admire.
across two distractors:the father openly and deliberately avoids/*avoid.
across prepositional phrase:the mother behind the cars approves/*approve.
across object relative clause:the farmer (that) the parents loveconfuses/*confuse.
figure 1: syntactic structures used in this study.
un-grammatical forms are marked with asterisks.
targetsubjects and their agreeing verb inﬂections are shownin blue, while attractors and their agreeing inﬂectionsare shown in red..correct completions over incorrect completions..3 experimental setup.
3.1 data.
first, we deﬁne prompts u. these prompts are aset of left contexts (beginnings of sentences), gen-erated from a vocabulary and a set of templatesdeveloped by lakretz et al.
(2019).
we expand thevocabulary with additional tokens, and add relativeclause (rc) templates.
we opt to synthetically gen-erate prompts rather than sample from a corpus tocontrol for the potential confound of token collo-cations in the training set.
we use prompts fromsix syntactic structures; an example of each may befound in figure 1. for each structure, we randomlysample 300 prompts from all possible noun-verbcombinations.
our dataset, code, and random seedsare available on github.1.
in the ‘simple agreement’ and ‘within rc’ con-structions, there is no separation between the targetsubject and verb.
the ‘across one distractor’ and‘across two distractors’ structures test the effect ofplacing one or two adverbs between the subjectand verb.
finally, the ‘across pp’ and ‘across rc’structures test the effect of adding a noun (and verbin the latter structure) between the main subject.
size.
layers embedding size heads.
distilsmallmediumlargexl.
612243648.
768768102412801600.
1212162025.table 1: gpt-2 sizes used in this study.
“embeddingsize” and “heads” refer to the number of neurons andattention heads per layer, respectively..and the main verb.
in the ‘across rc’ and ‘withinrc’ structures, we measure effects both with andwithout the complementizer that.2.
in each of these constructions, we deﬁne a cor-rect and an incorrect continuation.
here, we focuson the third-person singular/plural distinction..3.2 models.
we focus primarily on gpt-2 (radford et al., 2019),an autoregressive transformer-based (vaswaniet al., 2017) english lm.
we use several gpt-2sizes, including distilgpt-2 (sanh et al., 2020), avery small distilled version.
table 1 gives modeldetails for the different sizes of gpt-2..to investigate how differences in training acrosstransformer-based architectures manifest them-selves in syntactic agreement mechanisms, we alsoinvestigate transformer-xl (dai et al., 2019) andxlnet (yang et al., 2019).
transformer-xl is anautoregressive english lm whose training objec-tive is similar conceptually to gpt-2’s; however,it has a much longer effective context.
xlnet isan english lm which proceeds through variousword order permutations of the input tokens duringtraining, and which uses a distinct attention mask-ing mechanism as well; during testing, it proceedsautoregressively through the input similar to theother two models..4 total effect: how strongly do models.
prefer correct forms?.
we use the relative probabilities of the correct andincorrect tokens as a measure of the preferenceof a model (parameterized by θ) for the correctinﬂection of a verb v ∈ v given prompt u ∈ u withnumber feature sg:.
y(usg, v) =.
pθ(vpl | usg)pθ(vsg | usg).
(1).
1https://github.com/mattf1n/lm-intervention.
2a comparison of total and indirect effects when includingor excluding the complementizer may be found in appendix c..1830where y < 1 indicates a preference for the correctinﬂection, and y > 1 indicates a preference for theincorrect inﬂection.3.
to obtain counterfactual inputs, we now deﬁne aclass of interventions x that modify the prompts inu in a systematic way.
as we are concerned withthe ability of models to choose correct inﬂectionsdespite the presence of distractors and attractors,we deﬁne the intervention swap-number, which re-places the target subject with the same lexeme ofthe opposite number inﬂection (e.g., change “au-thor” to “authors” or vice versa).
we also deﬁnethe null intervention, which leaves u as-is (as invig et al.
2020a)..now we deﬁne yx(u, v), which is the value of yunder intervention x on prompt u. because the in-tervention swap-number entails swapping the sub-ject for a noun of the opposite number, we nowexpect y > 1 in equation 1 if the model prefersthe grammatically correct form, since the verb thatwas originally the correct inﬂection is now incor-rect and vice versa.
note that under this deﬁnition,yswap-number(usg, v) = 1/ynull(upl, v)..the total effect (te) for the interventionswap-number (illustrated in figure 2) is the rel-ative change between the probability ratio y underthe swap-number intervention and the ratio underthe null intervention:.
te(swap-number, null; y, u, v) =yswap-number(usg, v) − ynull(usg, v)ynull(usg, v)yswap-number(usg, v)/ynull(usg, v) − 1 =1/(ynull(usg, v) · ynull(upl, v)) − 1.
=.
(2).
we interpret this quantity as the overall prefer-ence of a model for the correct inﬂection of v incontext u. observe that this deﬁnition remains thesame when sg and pl are swapped in equation 2,therefore we do not specify whether u is plural orsingular in te(swap-number, null; y, u, v)..we are interested in the average total effect.
across prompts and verbs:.
te(swap-number, null; y) =.
eu,v.
(cid:20) yswap-number(u, v)ynull(u, v).
(cid:21).
− 1.
(3).
we calculate the average total effect for eachsyntactic construction for different sizes of gpt-2.
3we arbitrarily choose to start with sg; we can swap sg andpl in eq.
1 without loss of generality since we do not directlyobserve y. this is clariﬁed after eq.
2..figure 2: total effects are measured by performing anintervention on the prompt (here, changing the gram-matical number of the main subject), and measuringthe relative change in the response variable (the ratio ofprobabilities of the originally incorrect verb form overthe originally correct verb form)..and consider other models later on.
as a control,we also calculate total effects for models with ran-dom weights.
unlike in linzen et al.
(2016), wedo not measure accuracies by checking whetherone probability is higher than another.
rather, thetotal effect quantiﬁes the margin between the prob-abilities of correct and incorrect continuations withsome intervention..because larger models tend to exhibit correctsubject-verb agreement more often than smallerones (hu et al., 2020; van schijndel et al., 2019),we hypothesize that larger models will generallyhave larger tes for the same structure (i.e., wepredict that higher accuracy is indicative of largermargin between probabilities)..4.1 results.
figure 3 presents total effects by structure for var-ious sizes of gpt-2.
for models with randomweights, tes are always near-zero, and as suchare not shown in the ﬁgure..in ‘simple agreement’ and ‘within rc’, wherethere is no separation of subject and verb, tes varybetween 1,000 and 5,000, depending on model size.
this is far higher than the tes below 250 reportedfor gender bias in vig et al.
(2020a), which is to beexpected: gpt-2’s training objective explicitly opti-mizes for predicting (ideally grammatically correct)tokens given a context.
unlike vig et al.
(2020a),we do not observe larger tes for larger models..adverbial distractors increase total effects.
tes are even higher for structures where distractorsare present, with distilgpt-2 and gpt-2 small at-taining the highest tes in such contexts.
this issurprising, as one might expect subject-verb agree-.
1831figure 3: total effects for each structure by model size for gpt-2.
adverbial distractors increase total effects,while attractor phrases decrease them..ment accuracy to decline as the distance betweenthe subject and the verb increases.
we suspect thatadverbs are acting as cues that a verb will soonappear, thus increasing the probability of both thecorrect and incorrect verb, but increasing that ofthe correct verb more (for similar ﬁndings in hu-man sentence processing, see (vasishth and lewis,2006)).
additional analysis supports this hypothe-sis; see appendix b..attractors decrease total effects.
when pps orrcs separate the subject and verb, tes decrease.
the number of the attractor does not signiﬁcantlychange tes across pps, but does have a more no-table effect across rcs: gpt-2 is more certain ofits choices across singular rcs than across pluralrcs, as evidenced by higher tes for the former.
notably, gpt-2 medium tends to achieve the high-est tes in attractor structures, except in the ‘acrossplural rc’ structure..5 grammaticality margin: is agreementeasier for singular or plural subjects?.
total effect measures the effect of swapping thenumber of the subject, but does not distinguish thecase where the original subject (before swapping)was singular from the case where it was plural.
toinvestigate the effect of the original subject num-ber on the model’s preference for the correct (orincorrect) inﬂection, we deﬁne the metric grammat-icality margin (referred to hereafter as grammati-cality) as the reciprocal of y given prompt u with a.speciﬁc number feature sg or pl:.
g(usg, v) = 1/y(usg, v)g(upl, v) = 1/y(upl, v).
(4).
recalling the deﬁnition of y, this measure is theprobability ratio between the model correctly andincorrectly resolving subject-verb agreement.
wedeﬁne g as the reciprocal of y so that when themodel has a high preference for the correct inﬂec-tion over the incorrect inﬂection, g is large..differences in grammaticality values for pluraland singular subjects can indicate systematic biasestoward a certain grammatical number.
we expectthis quantity to be lower if there is an attractor ofa different number from the subject, whereas weexpect it to increase if the attractor is of the samenumber as the subject..5.1 results.
figure 4 presents grammaticality values separatelyfor singular and plural subjects, as well as singu-lar and plural attractors when applicable.
whilewe expect higher grammaticality values when thesubject number matches the attractor number, weinstead observe that plural subjects always havehigher grammaticality values regardless of thestructure or attractor number.
in other words,it is always easier for gpt-2 to form agreementdependencies between verbs and plural subjectsthan singular subjects.
this may be due to pluralverbs being encoded as “defaults” in gpt-2, as wasfound for lstm lms in jumelet et al.
(2019).
this.
1832simple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcstructure05000100001500020000250003000035000total effectsingular ppplural ppsingular rcplural rc050010001500distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xlfigure 4: grammaticality for each structure for gpt-2medium.
the subject number (indicated by bar color)refers to the grammatical number of the subject withwhich the target verb agrees; the number in the struc-ture name refers to the grammatical number of the at-tractor (in structures where attractors are present)..would make intuitive sense, because singular thirdperson verbs are marked in english present-tense.
attractors that separate subjects and verbsdecrease grammaticality, regardless of plural-ity.
the same is not true of distractors: placingadverbs between the subject and verb tends to havelittle effect, even though the ‘across two distractors’structure places the same token distance betweensubject and verb as ‘across a pp’.
this means thatdistance between subject and verb is less importantthan the type of the structure separating them..as expected, when holding the subject numberconstant (i.e., looking only at blue bars or only atorange bars in figure 4), grammaticality values arehigher when the attractor has the same number asthe subject..attractors.
that precede.
subjects havenumber-dependent impacts on grammaticality.
in the ‘within singular rc’ structure, grammati-cality is only slightly reduced for both singularand plural subjects compared to the ‘simpleagreement’ structure.
however, ‘within pluralrc’ has a polarizing effect: grammaticality isgreatly reduced for singular subjects, but greatlyincreased for plural subjects.
this is the onlyattractor structure with higher grammaticality thanthe simple case..6 natural indirect effect: whichcomponents mediate syntacticagreement?.
the natural indirect effect (nie), illustrated in fig-ure 5, is the relative change in the ratio y when theprompt u is not changed, but a model component.
figure 5: indirect effects are measured by setting anindividual neuron to the value it would have taken hadthe intervention occurred, then measuring the relativechange in the response variable..z (e.g., a neuron) is set to the value it would havetaken if the intervention had occurred..nie(swap-number, null; y, z) =(cid:21)(cid:20) ynull,zswap-number(u,v)(u, v)ynull(u, v).
− 1.eu,v.
(5).
this allows us to evaluate the contribution of spe-ciﬁc parts or regions of a model to the syntacticpreferences we observe.
more speciﬁcally, we canmeasure to what extent the total effect of swap-ping the subject on inﬂection preferences can beattributed to speciﬁc neurons..here, we independently analyze the individ-ual neuron nies for gpt-2, transformer-xl, andxlnet (future work could also investigate inter-vening on sets of neurons simultaneously).
wealso attempt to analyze attention heads for gpt-2, though we ﬁnd that they do not present con-sistent interpretable results with the swap-numberintervention (see appendix a).
this is consistentwith the ﬁndings of htut et al.
(2019) who do notﬁnd a straightforward connection between attentionweights and the model’s syntactic behavior..based on the ﬁndings of prior probing work ondependency parsing (hewitt and manning, 2019),we hypothesize that nies will peak in the upper-middle layers for all models.
because xlnetis exposed to all word order permutations of itsinput sentences during training, we hypothesizethat it will display similar indirect effect resultsacross syntactic structures.
conversely, gpt-2 andtransformer-xl always process input left-to-right,so we expect that for these two models, differingsyntactic structures will yield unique indirect effectresults..1833simple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcstructure0100200300grammaticalitysingular subjectplural subjectfigure 6: natural indirect effects of the top 5% of neurons in each layer of various gpt-2 sizes.
each ﬁgurefocuses on a single structure and compares across gpt-2 sizes..upper-middle layers.
this is in line with the prob-ing results of hewitt and manning (2019) and ten-ney et al.
(2019a), who ﬁnd that the highest amountof syntactic information is encoded in the upper-middle layers.
in the ﬁnal layers of the model, theeffect decreases sharply, reaching 0 in the upper-most layers.
the peak nie is lower here than forstructures where there is no separation, perhapsindicating that syntactic agreement information islocalized in fewer neurons when separation occurs.
even a single token between subject and verbbrings about this second indirect effect contour,indicating that distance is a less important fac-tor than the presence of any separation in invok-ing this second syntactic agreement mechanism.
the distinct indirect effect contours for the adja-cent and non-adjacent cases may indicate distinctsubject-verb agreement mechanisms for short- andlong-distance agreement, consistent with similarﬁndings for lstms (lakretz et al., 2019)..as a control, we repeated the experiment forgpt-2 with randomized weights.
we ﬁnd that forall structures, when weights are randomized, indi-rect effects peak at layer 0—albeit at values perhapstoo small to be meaningful—and then remain closeto 0 in higher layers.
this indicates that the vastmajority of the indirect effect observed for trainedmodels is an outcome of learning from the trainingdata rather than of the architecture..for each structure, the maximum nie per layerpeaks in.
is always lower for larger models..figure 7: natural indirect effects of the top 5% of neu-rons in each layer of gpt-2 medium..6.1 results.
for each model and structure, we select the 5% ofneurons with the highest nie in each layer; fig-ure 6 compares nies across model sizes, and fig-ure 7 compares nies across structures for gpt-2medium.4 we observe two distinct layer-wise con-tour patterns.
in structures where the target verbdirectly follows the subject (‘simple agreement’and ‘within rc’, the top 3 plots in figure 6), niescontinually increase in higher layers..conversely, for structures with subject-verb sep-aration (‘across one/two distractor(s)’, ‘across pp’,and ‘across rc’, the bottom 3 ﬁgures in figure 6),nies peak at layer 0 and (more notably) in the.
4we also produced ﬁgures using all neurons.
when doingso, the contour of the graph across layers did not change,but the magnitudes were lower since we average over moreneurons..183401020304050layer0.000.010.020.030.040.050.06indirect effectsimplegpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl01020304050layer0.000.020.040.060.08indirect effectwithin singular rcgpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl01020304050layer0.000.020.040.06indirect effectwithin plural rcgpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl01020304050layer0.0000.0020.0040.0060.008indirect effect1 distractorgpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl01020304050layer0.0000.0020.0040.0060.0080.0100.012indirect effectsingular ppgpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl01020304050layer0.0000.0020.0040.0060.0080.0100.012indirect effectplural rcgpt-2 small (random)distilgpt-2gpt-2 smallgpt-2 mediumgpt-2 largegpt-2 xl05101520layer0.0000.0020.0040.0060.0080.0100.0120.014indirect effectsimplewithin singular rcwithin plural rc1 distractor2 distractorssingular ppplural ppsingular rcplural rcthose seen in gpt-2.
in xlnet, we do not observethe same dichotomous behavior between subject-verb adjacent and subject-verb non-adjacent struc-tures; rather, the overall contours are all similar.
all of the indirect effects approach 0 in the ﬁnallayer.
this resembles the contours from gpt-2and transformer-xl for structures where subjectand verb are not adjacent.
we conjecture that thispattern arises because xlnet observes many wordorder permutations of the same inputs during train-ing; this acts as a form of regularization that pre-vents it from evolving bifurcating mechanisms forlocal and non-local dependencies..while sinha et al.
(2021) found that natural wordorder during pre-training matters little for down-stream performance on tasks in benchmarks likeglue (wang et al., 2018), they also found that ran-domizing word order greatly reduced model prefer-ences for correct inﬂections in syntactic evaluationstimuli.
this ﬁnding—coupled with the distinctword-order-dependent agreement mechanisms thatwe discover—suggests that models do make useof word order information, rather than just higher-order word collocation statistics..6.1.2 neuron overlap across structuresthe layer-wise nie contours in section 6.1 showthe nie of the top neurons in each layer, but do notshow which neurons make it into the top 5%.
to in-vestigate whether the same neurons are implicatedin subject-verb agreement across structures, we se-lect the top 5% of neurons per layer by nie andcalculate the proportion of these high-nie neuronsthat overlap between each pair of structures..does the extent of neuron sharing across struc-tures correlate with human intuitions of syntacticsimilarity?
to address this question, we computehypothesized syntactic similarities between struc-tures based on the following linguistic features:distance between subject and verb; presence of ad-verbial distractors, a relative clause, prepositionalphrase, and/or a noun attractor; and the numberof the noun attractor when present.
appendix d.1provides additional details on the calculation ofground-truth similarity..to quantify the similarity of the hypothesis ma-trix and a neuron overlap matrix, we calculate the (cid:96)1norm5 of the element-wise difference between thelower-left triangle of both matrices, as the matricesare symmetric.
we exclude the diagonal..5using the (cid:96)2 norm does not change which layer in each.
figure 8: natural indirect effects of the top 5% of neu-rons in each layer of transformer-xl..figure 9: natural indirect effects of the top 5% of neu-rons in each layer of xlnet..nies are also more distributed across layers forlarger models.
this suggests that structuralknowledge is concentrated in fewer neuronswith stronger inﬂectional preferences in smallermodels, and is more distributed across neuronsin larger models.
nonetheless, the overall contourof nies is similar across model sizes for a givenstructure, indicating that mechanisms of agree-ment are similar across model sizes..6.1.1 comparing gpt-2 to other.
architectures.
also investigate.
wethe neuron nies oftransformer-xl (figure 8) and xlnet (fig-ure 9) to observe whether syntax is represented ina similar manner across models (for total effectsacross architectures, see appendix e)..local and non-local agreement diverges ina similar way in gpt-2 and transformer-xl.
the layer-wise contour is similar for ‘simple agree-ment’ and ‘within rc’ across the two architectures,and differs signiﬁcantly from the cases where sub-ject and verb are separated, which is again similaracross architectures.
this supports our hypothesisthat gpt-2 and transformer-xl encode syntax ina similar manner..indirect effects in xlnet are different to.
model has the lowest difference norm..1835024681012141618layer0.000.010.020.030.040.050.060.070.08indirect effectsimplewithin singular rcwithin plural rc1 distractor2 distractorssingular ppplural ppsingular rcplural rc024681012layer0.00000.00250.00500.00750.01000.01250.01500.0175indirect effectsimplewithin singular rcwithin plural rc2 distractors1 distractorsingular ppplural ppsingular rcplural rcfigure 10: hypothesized syntactic similarity across structures (left), as well as the overlap of the top 5% of neuronsper-structure by indirect effect for gpt-2 (center-left), transformer-xl (center-right), and xlnet (right); the layerdisplayed is the one that shows the highest similarity to the hypothesized (ground-truth) matrix..for each model, we present neuron overlaps forthe layer with the lowest difference norm to thehypothesis (figure 10; for an analysis of layer-by-layer overlap change for gpt-2, see appendix d.2).
the lowest difference norms are 443 (gpt-2),510 (transformer-xl), and 486 (xlnet).
gpt-2medium’s overlap across structures at layer 21(of 24) is visually similar to the hypothesis, in-dicating that this layer in gpt-2 shares neuronsfor subject-verb agreement across structures ina way that aligns with human intuitions aboutsyntactic similarity.
interestingly, it learns to dothis without receiving explicit syntactic supervisionduring training..layer 15 (of 18) of transformer-xl displayssimilar trends to gpt-2, though the extent of over-lap is higher across structures in general here.
there is more signiﬁcant overlap between the ad-verbial distractor structures and the structures thatcontain attractors.
‘simple agreement’ also hasmore overlap with structures containing attractorsthan ‘within rc’, which is contrary to our hypoth-esis matrix.
we also note that ‘across singular rc’has more overlap with ‘across pp’ than ‘across plu-ral rc’ (and vice versa for ‘across plural rc’), in-dicating that the number of the attractor is moresalient to transformer-xl than the structure ofthe phrase containing the attractor..layer 8 (of 12) of xlnet gives rise to a nois-ier similarity matrix.
there is slightly more over-lap between structures across noun attractors, butthe extent of overlap is smaller compared to othermodels.
this suggests that more of the neuronsare specialized to processing speciﬁc structures.
however, the indirect effect ﬁndings for xlnetsuggest a more uniﬁed mechanism for syntacticagreement across all structures; if this were the.
case, we would expect neuron overlap to be high,and for the extent of overlap to be similar across allstructures, rather than being higher between moresimilar structures.
we observe the latter, but notthe former.
regardless, both observations furthersupport our hypothesis that xlnet uses differentmechanisms to resolve number agreement than theother two architectures..7 conclusions.
this study applied causal mediation analysis to dis-cover and interpret the mechanisms behind syntac-tic agreement in pre-trained neural language mod-els.
our results reveal the location and importanceof various neurons within various models, and pro-vide insights into the inner workings of these lms.
for future work, we suggest intervening ongroups of neurons and attention heads to see howthese components work together, and extending theanalysis to phenomena such as ﬁller-gap depen-dencies and negative polarity items.
further workshould also explore the impact of speciﬁc verbs onsyntactic agreement mechanisms (newman et al.,2021).
lastly, we suggest examining exampleswhere the model makes incorrect predictions to de-termine how models misuse the mechanisms fromsection 6.1..acknowledgements.
y.b.
was supported in part by the israel sci-ence foundation (grant no.
448/20) and byan azrieli foundation early career faculty fellow-ship.
a.m. was supported by a national sciencefoundation graduate research fellowship (grantno.
1746891)..1836siopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rcsiopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rchypothesized syptactic sioinarities020406080100hypothesized syptactic sioinaritysimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcgpt-2 medium layer 21020406080100top 5% neuron overlapsiopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rcsiopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rctrapsforoer-xl layer 15020406080100top 5% peurop overnapsiopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rcsiopne1 distractor2 distractorssipgunar pppnuran ppsipgunar rcpnuran rcwithip sipgunar rcwithip pnuran rcxlnet layer 8020406080100top 5% peurop overnapimpact statement.
in this paper, we apply causal mediation analysisin order to study the subject-verb agreement mech-anisms in language models.
while the focus ofthis work is on the analysis itself, our insights mayinﬂuence the training strategies for new models.
speciﬁcally, our ﬁndings on the relationship be-tween model size and syntactic agreement and thecomparison of different model architectures mayhelp researchers decide which model to use.
indoing so, others may try to extrapolate our ﬁnd-ings, which are limited to the domain of speciﬁcsyntactic structures and subject-verb agreement inenglish language models, to other tasks and lan-guages for which we cannot make these claims.
the focus on english of this study additionally fur-thers the discrepancy compared to other languageswhich continue to be studied much less..moreover, we do not study mitigation mecha-nisms for our ﬁndings and thus do not know theconsequences of modifying the training proceduresof language models beyond the three examples westudied.
one concrete example for a case where ourﬁndings could have wider impact regards our ﬁnd-ing that models have higher grammaticality for plu-ral subjects.
others may ﬁnd that this is undesiredbehavior and thus try to augment their training datato increase the number of subjects in singular form,which could have unanticipated consequences onmodel performance and mechanisms..beyond the concrete ﬁndings in this paper, thereare also broader considerations in the populariza-tion of causal mediation analysis.
speciﬁcally, aspointed out by vig et al.
(2020a), it is a challeng-ing problem to extend the effect measures beyondbinary cases.
while subject-verb agreement is bynature a binary problem, there are many others thatbeneﬁt from a more nuanced view, speciﬁcally intopics related to fairness and bias.
thus, by pop-ularizing an approach that is easier to apply in abinary case, we may have the unintended effectof complicating analyses conducted by others whowant to follow our approach.
as an active miti-gation, we direct readers to the extended versionof vig et al.
(2020b), which discusses effect mea-sures beyond the binary case..references.
yonatan belinkov.
2018. on internal language rep-resentations in deep learning: an analysis of ma-.
chine translation and speech recognition.
ph.d.thesis, massachusetts institute of technology..yonatan belinkov.
2021. probing classiﬁers: promises,arxiv preprint,.
shortcomings, and alternatives.
abs/2102.12452..yonatan belinkov and james glass.
2019. analysismethods in neural language processing: a survey.
transactions of the association for computationallinguistics, 7:49–72..ethan a. chi, john hewitt, and christopher d. man-ning.
2020.finding universal grammatical rela-tions in multilingual bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5564–5577, online.
as-sociation for computational linguistics..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..richard futrell, ethan wilcox, takashi morita, pengqian, miguel ballesteros, and roger levy.
2019.neural language models as psycholinguistic sub-jects: representations of syntactic state.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 32–42, minneapolis,minnesota.
association for computational linguis-tics..mario giulianelli, jack harding, florian mohnert,dieuwke hupkes, and willem zuidema.
2018. un-der the hood: using diagnostic classiﬁers to in-vestigate and improve how language models trackagreement information.
in proceedings of the 2018emnlp workshop blackboxnlp: analyzing and in-terpreting neural networks for nlp, pages 240–248..yoav goldberg.
2019. assessing bert’s syntactic.
abilities.
arxiv preprint 1901.05287..kristina gulordava, piotr bojanowski, edouard grave,tal linzen, and marco baroni.
2018. colorlessingreen recurrent networks dream hierarchically.
proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,.
1837volume 1 (long papers), pages 1195–1205, neworleans, louisiana.
association for computationallinguistics..syntax-sensitive dependencies.
transactions of theassociation for computational linguistics), 4:521–535..john hewitt and percy liang.
2019. designing andinterpreting probes with control tasks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2733–2743, hongkong, china.
association for computational lin-guistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word represen-in proceedings of the 2019 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4129–4138..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..phu mon htut, jason phang, shikha bordia, andsamuel r. bowman.
2019. do attention heads inbert track syntactic dependencies?
arxiv preprint,abs/1911.12246..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020. a systematic assessmentof syntactic generalization in neural language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 1725–1744, online.
association for compu-tational linguistics..jaap jumelet, willem zuidema, and dieuwke hupkes.
2019. analysing neural language models: con-textual decomposition reveals default reasoning innumber and gender assignment.
in proceedings ofthe 23rd conference on computational natural lan-guage learning (conll), pages 1–11, hong kong,china.
association for computational linguistics..yair lakretz, dieuwke hupkes, alessandra vergallito,marco marelli, marco baroni, and stanislas de-haene.
2021. mechanisms for handling nested de-pendencies in neural-network language models andhumans.
cognition, page 104699..yair lakretz, german kruszewski, theo desbordes,dieuwke hupkes, stanislas dehaene, and marco ba-roni.
2019. the emergence of number and syn-in proceed-tax units in lstm language models.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 11–20, minneapolis,minnesota.
association for computational linguis-tics..tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learn.
nelson f. liu, matt gardner, yonatan belinkov,matthew e. peters, and noah a. smith.
2019. lin-guistic knowledge and transferability of contextualrepresentations.
in proceedings of the 2019 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1073–1094, minneapolis, minnesota.
association for computational linguistics..kaiji lu, piotr mardziel, klas leino, matt fredrikson,and anupam datta.
2020. inﬂuence paths for char-acterizing subject-verb number agreement in lstmin proceedings of the 58th an-language models.
nual meeting of the association for computationallinguistics, pages 4748–4757, online.
associationfor computational linguistics..rebecca marvin and tal linzen.
2018. targeted syn-in proceed-tactic evaluation of language models.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202,brussels, belgium.
association for computationallinguistics..aaron mueller, garrett nicolai, panayiota petrou-zeniou, natalia talmina, and tal linzen.
2020.cross-linguistic syntactic evaluation of word predic-in proceedings of the 58th annualtion models.
meeting of the association for computational lin-guistics, pages 5523–5539, online.
association forcomputational linguistics..benjamin newman, kai-siang ang, julia gong, andjohn hewitt.
2021. reﬁning targeted syntactic eval-in proceedings of theuation of language models.
2021 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 3710–3723, on-line.
association for computational linguistics..judea pearl.
2001. direct and indirect effects.
in pro-ceedings of the seventeenth conference on uncer-tainty in artiﬁcial intelligence, pages 411–420..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog..james m. robins.
2003. semantics of causal dagmodels and the identiﬁcation of direct and indirecteffects.
oxford statistical science series, pages 70–82..james m. robins and sander greenland.
1992. identi-ﬁability and exchangeability for direct and indirecteffects.
epidemiology, pages 143–155..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2020. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint, abs/1910.01108..1838boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..ethan wilcox, roger levy, takashi morita, andrichard futrell.
2018. what do rnn languageinmodels learn about ﬁller–gap dependencies?
proceedings of the 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 211–221, brussels, belgium.
association for computational linguistics..ethan wilcox, peng qian, richard futrell, miguelballesteros, and roger levy.
2019. structural super-vision improves learning of non-local grammaticalin proceedings of the 2019 confer-dependencies.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3302–3312, minneapolis, minnesota.
association for computational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, volume 32. curranassociates, inc..marten van schijndel, aaron mueller, and tal linzen.
2019. quantity doesn’t buy quality syntax within proceedings of theneural language models.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5835–5841, hong kong,china.
association for computational linguistics..koustuv sinha, robin jia, dieuwke hupkes, joellepineau, adina williams, and douwe kiela.
2021.masked language modeling and the distributionalhypothesis: order word matters pre-training for lit-tle.
arxiv preprint, abs/2104.06644..martin sundermeyer, ralf schl¨uter, and hermann ney.
2012. lstm neural networks for language model-ing.
in thirteenth annual conference of the interna-tional speech communication association..ian tenney, dipanjan das, and ellie pavlick.
2019a.
bert rediscovers the classical nlp pipeline.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r thomas mccoy, najoung kim,benjamin van durme, sam bowman, dipanjan das,and ellie pavlick.
2019b.
what do you learn fromcontext?
probing for sentence structure in contextu-in international con-alized word representations.
ference on learning representations..shravan vasishth and richard l lewis.
2006.argument-head distance and processing complex-ity: explaining both locality and antilocality effects.
language, pages 767–794..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..jesse vig, sebastian gehrmann, yonatan belinkov,sharon qian, daniel nevo, yaron singer, and stu-art shieber.
2020a.
investigating gender bias in lan-guage models using causal mediation analysis.
inadvances in neural information processing systems,volume 33, pages 12388–12401.
curran associates,inc..jesse vig, sebastian gehrmann, yonatan belinkov,sharon qian, daniel nevo, yaron singer, and stu-art m. shieber.
2020b.
causal mediation analysisfor interpreting neural nlp: the case of gender bias.
arxiv preprint, abs/2004.12265..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
the 2018 emnlp workshop black-ceedings of.
1839a attention head indirect effects.
here, we present mean indirect effects acrossprompts for a sample of structures of each attentionhead in gpt-2 small (which has 12 layers) underboth the swap-number intervention (figure 11) andzero intervention (figure 12; deﬁned below)..for the swap-number intervention, we do not ob-serve any consistent trends across structures, exceptthat attention heads in upper-middle layers seemto account for most of the positive and negativenies.
head 10-9 (layer 10, head 9) has negativeindirect effects for most structures where there is aseparation between subject and verb, except ‘acrossplural rc’.
however, we do not observe strong in-direct effects for head 10-9 when subject and verbare adjacent.
head 11-11 has the most consistentlypositive indirect effects across structures, thoughits magnitude is typically low..indirect effects are largely positive for ‘withinplural rc’, but otherwise, indirect effects are fairlyevenly split between positive and negative.
thesum of indirect effects across heads for most struc-tures is close to 0, with many sums being a low-magnitude negative number.
this indicates thatthese attention indirect effects may simply be noise.
because attention heads seem robust to swap-ping the number of the subject, we also deﬁne thezero intervention.
here, we do not change u, butset the attention head’s value equal to 0 and ob-serve how this changes the effect; this has an in-terpretation as the controlled indirect effect frompearl (2001).
here, trends are more consistentacross structures, attractor numbers, and types ofdistractors.
head 0-10 is always strongly impli-cated; since this is in the bottom layer, this suggeststhat attention’s contribution to syntax is based onlexical (perhaps collocational) information and notstructural information.
this would align with htutet al.
(2019), who found that attention tends tocapture lexical grammatical features but not inter-word structural information.
qualitative analysisreveals that head 0-10 and head 2-8 always focus onthe 2nd and 5th words in the prompt, respectively.
thus, attention’s contribution to subject-verb agree-ment in lower layers may largely be based on whereimportant tokens appear in the input, rather thanany abstract structural information that would becomposed in the upper layers..however, for all structures except where we haveadverbial distractors, we see consistent positive in-direct effects in the uppermost layers as well.
no.
single attention head is strongly implicated, but thelayer effect is consistently positive and sometimesnears the magnitude of that in the lower layers.
this indicates that more abstract structural infor-mation may be present, but that this information isalso quite distributed across attention heads in theuppermost layers.
future work should investigateother interventions to better understand attention’srole in syntactic agreement..b adverbs increase the probability ofcorrect verbs more than incorrectverbs.
here, we show that separating the subject and verbwith adverbs tends to increase the probability ofthe verb (figure 13).
regardless of whether thesubject and verb agree, adding adverbs does alwaysincrease the probability of verbs.
note the logscale: we observe visually similar increases in logprobabilities after adding 1 or 2 adverb distractors,but visually similar differences at higher points inthe graph are actually much larger increases.
thissupports our hypothesis that adverbs increase theprobablity of all verbs, but increase the probabilityof the correct inﬂection probabilities more than thatof the incorrect one..c the (non-)impact of complementizers.
here, we investigate the effect of including or ex-cluding the complementizer that for the ‘across rc’and ‘within rc’ structures, observing both tes(figure 14) and neuron indirect effects (figure 15).
while we expect lower tes when the complemen-tizer is absent, we observe only minor reductions intotal effects in ‘across rc’; this holds across modelsizes.
for ‘within rc’, however, trends are size-dependent.
distilgpt-2, gpt-2 small, and gpt-2large appear mostly robust to the presence or ab-sence of the complementizer, though gpt-2 smalldoes have lower total effects in the ‘across pluralrc’ structure when that is absent.
meanwhile,gpt-2 medium more strongly prefers correct in-ﬂections when that is absent.
it is not immediatelyclear why this is the case, because deleting thecomplementizer introduces more ambiguity..there does not appear to be any signiﬁcant dif-ference in magnitude or contour of the indirecteffects across layers when including or excludingthe complementizer.
thus, while excluding thecomplementizer can make subject-verb agreementslightly more difﬁcult for lms (marvin and linzen,.
1840figure 11: attention indirect effects for gpt-2 small under the swap-number intervention..figure 12: attention indirect effects for gpt-2 small under the zero intervention..18410246810head0246810layersimple agreement0.0250.000         layer effect0.0160.0080.0000.0080.0160246810head0246810layerdistractor (1)0.0250.000         layer effect0.0120.0060.0000.0060.0120246810head0246810layerpp (singular)0.000.02         layer effect0.0120.0060.0000.0060.0120246810head0246810layeracross relative clause (singular)0.050.00         layer effect0.0450.0300.0150.0000.0150246810head0246810layersimple agreement0.000.25         layer effect0.000.150.300.450246810head0246810layerdistractor01         layer effect0.000.150.300.450246810head0246810layeracross relative clause (plural)0.00.5         layer effect0.000.150.300.450246810head0246810layerwithin relative clause (plural)0.00.5         layer effect0.000.150.300.45figure 13: the distribution of target verb probabili-ties for ‘simple agreement’, ‘across one distractor’, and‘across two distractors’.
note that the y-axis uses a logscale..2018), it does not appear to change the mechanismsthrough which subject-verb agreement happens inthe model..d additional neuron overlap details.
d.1 hypothesizing syntactic similarity.
to generate the hypothesis similarity matrix be-tween structures, we choose a set of features givenin table 2 that capture important syntactic infor-mation.
most of the features are binary; however,we also include a ternary feature and a numericalfeature.
the ternary feature, “attractor number”,can take on values sg, pl, and 0 when there is noattractor..to compute the similarity of two structures, weﬁrst sum the differences for each feature.
for thebinary and ternary features, the difference is 0 ifthe features have the same value, otherwise 1. forthe numerical feature, we take the absolute value ofthe difference between distances, scaled to a valuebetween 0 and 2. we scale the similarity to reducethe impact of the numerical feature on the totalsimilarity.6 finally, we take the maximum possibledifference across all pairs of structures, and sub-.
6we initially scaled this to be within the range [0, 1] likethe other features, but this caused “within relative clause” tohave high similarity to “across pp” and “across a relativeclause”.
thus, we increase its impact for more human-likehypotheses..figure 14: total effects for ‘across relative clause’ and‘within relative clause’ structures, with and without thecomplementizer that..figure 15: indirect effects for ‘across relative clause’and ‘within relative clause’ structures, with (solid lines)and without (dashed lines) the complementizer that..tract each pairwise distance from the maximum toobtain similarity scores.
we normalize the similari-ties to the range [0, 100] by dividing similarities bythe maximum possible similarity score; this is tomake them more comparable to the neuron overlapmatrices..d.2 neuron overlap across layers.
here, we present neuron overlaps across all lay-ers of distilgpt-2, the smallest model we analyze(figure 16).
we ﬁrst note that the overall extent ofneuron overlap across structures tends to increaseup to the upper-middle layers, before sharply de-creasing in the highest layer to near-zero values.
we ﬁnd that this trend holds for all other sizes ofgpt-2, as well as transformer-xl; generally, over-laps continue to increase until the upper-middle lay-ers, decreases slightly in the second-highest layer,and decreases sharply to zero in the highest layer..1842singular rcsingular rc (no that)plural rcplural rc (no that)within singular rcwithin singular rc (no that)within plural rcwithin plural rc (no that)structure010002000300040005000600070008000total effectdistilgpt-2gpt-2 smallgpt-2 mediumgpt-2 large05101520layer0.0000.0020.0040.0060.0080.0100.0120.014indirect effectwithin singular rcwithin singular rc (no that)within plural rcwithin plural rc (no that)singular rcsingular rc (no that)plural rcplural rc (no that)figure 16: overlap between structures of the top 5% of neurons in each of layer of distilgpt-2 by indirect effect..feature.
subject and verb separatedtokens between subject, verbhas adverbial distractor(s)has noun attractorattractor numberhas relative clausehas prepositional phrase.
type.
binarynumericalbinarybinaryternarybinarybinary.
table 2: features (and their types) used in calculatinghypothesized syntactic similarity..layer-by-layer difference ((cid:96)1) norms are presentedin table 3..layer no.
diff.
norm.
layer no.
diff.
norm.
0123.
677652565583.
456.
6275101301.table 3: difference (cid:96)1 norms between the hypothesismatrix and each layer of distilgpt-2..e total effects across architectures.
figure 17 presents total effects for all structuresacross architectures.
the magnitude of total effectis generally similar for xlnet and gpt-2 (exceptwhen dealing with relative clauses), whereas totaleffects are much smaller for transformer-xl.
it.
figure 17: total effects across structures by architec-ture (ordered by increasing number of layers/increasingparameterization)..seems that parametrization and model depth do notcorrelate well with total effects..perhaps the effects for transformer-xl aresmaller due to the longer effective contexts it has,which could make it prone to assigning smallerprobabilities to a larger set of tokens than gpt-2it iswhile still behaviorally performing well.
harder to explain the similarity between gpt-2 andxlnet, given the great differences between themin training and the divergence in their behavior asrevealed by the indirect effect results in §6.1.1..1843simple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 0020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 1020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 2020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 3020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 4020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 5020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcdistilgpt-2 layer 6020406080100top 5% neuron overlapsimple1 distractor2 distractorssingular ppplural ppsingular rcplural rcwithin singular rcwithin plural rcstructure0200040006000800010000total effectxlnettransformer-xlgpt-2 medium