which linguist invented the lightbulb?
presupposition veriﬁcation for question-answering.
najoung kim†,∗, ellie pavlickφ,δ, burcu karagol ayanδ, deepak ramachandranδ,∗†johns hopkins university φbrown university δgoogle researchn.kim@jhu.edu {epavlick,burcuka,ramachandrand}@google.com.
abstract.
b. unanswerable q: who is the current.
many question-answering (qa) datasetscontain unanswerable questions, buttheirin qa systems remains primi-treatmenttive.
our analysis of the natural ques-tions (kwiatkowski et al., 2019) dataset re-veals that a substantial portion of unanswer-able questions (∼21%) can be explained basedon the presence of unveriﬁable presupposi-tions.
through a user preference study, wedemonstrate that the oracle behavior of ourproposed system—which provides responsesbased on presupposition failure—is preferredover the oracle behavior of existing qa sys-tems.
then, we present a novel frameworkfor implementing such a system in three steps:presupposition generation, presupposition ver-iﬁcation, and explanation generation, report-ing progress on each.
finally, we show that asimple modiﬁcation of adding presuppositionsand their veriﬁability to the input of a com-petitive end-to-end qa system yields modestgains in qa performance and unanswerabil-ity detection, demonstrating the promise of ourapproach..1.introduction.
many question-answering (qa) datasets includ-ing natural questions (nq) (kwiatkowski et al.,2019) and squad 2.0 (rajpurkar et al., 2018) con-tain questions that are unanswerable.
while unan-swerable questions constitute a large part of exist-ing qa datasets (e.g., 51% of nq, 36% of squad2.0), their treatment remains primitive.
that is,(closed-book) qa systems label these questions asunanswerable without detailing why, as in (1):.
(1).
a. answerable q: who is the current.
monarch of the uk?
system: elizabeth ii..∗corresponding authors, †work done at google.
monarch of france?
system: unanswerable..unanswerability in qa arises due to a multitude ofreasons including retrieval failure and malformedquestions (kwiatkowski et al., 2019).
we focuson a subset of unanswerable questions—namely,questions containing failed presuppositions (back-ground assumptions that need to be satisﬁed)..questions containing failed presuppositions donot receive satisfactory treatment in current qa.
under a setup that allows for unanswerable as ananswer (as in several closed-book qa systems; fig-ure 1, left), the best case scenario is that the systemcorrectly identiﬁes that a question is unanswerableand gives a generic, unsatisfactory response as in(1-b).
under a setup that does not allow for unan-swerable (e.g., open-domain qa), a system’s at-tempt to answer these questions results in an inaccu-rate accommodation of false presuppositions.
forexample, google answers the question which lin-guist invented the lightbulb?
with thomas edison,and bing answers the question when did mariecurie discover uranium?
with 1896 (retrieved jan2021).
these answers are clearly inappropriate,because answering these questions with any nameor year endorses the false presuppositions some lin-guist invented the lightbulb and marie curie discov-ered uranium.
failures of this kind are extremelynoticeable and have recently been highlighted bysocial media (munroe, 2020), showing an outsizedimportance regardless of their effect on benchmarkmetrics..we propose a system that takes presuppositionsinto consideration through the following steps (fig-ure 1, right):.
1. presupposition generation: which linguistinvented the lightbulb?
→ some linguist in-vented the lightbulb..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3932–3945august1–6,2021.©2021associationforcomputationallinguistics3932figure 1: a comparison of existing closed-book qa pipelines (left) and the proposed qa pipeline in this work(right).
the gray part of the pipeline is only manually applied in this work to conduct headroom analysis..2. presupposition veriﬁcation: some linguistinvented the lightbulb.
→ not veriﬁable.
3. explanation generation: (some linguist in-vented the lightbulb, not veriﬁable) → thisquestion is unanswerable because there is in-sufﬁcient evidence that any linguist inventedthe lightbulb..our contribution can be summarized as follows:.
a.of.
subset.
unanswer-identify• weablefailedquestions—questions withpresuppositions—that are not handled well byexisting qa systems, and quantify their rolein naturally occurring questions through ananalysis of the nq dataset (s2, s3)..• we outline how a better qa system couldhandle questions with failed presuppositions,and validate that the oracle behavior of thisproposed system is more satisfactory to usersthan the oracle behavior of existing systemsthrough a user preference study (s4)..• we propose a novel framework for handlingpresuppositions in qa, breaking down theproblem into three parts (see steps above), andevaluate progress on each (s5).
we then inte-grate these steps end-to-end into a competitiveqa model and achieve modest gains (s6)..it is assumed that i, the speaker, do in fact owna hedgehog.
if i do not own one (hence the pre-supposition fails), uttering this sentence would beinappropriate.
questions may also be inappropriatein the same way when they contain failed presuppo-sitions, as in the question which linguist inventedthe lightbulb?..
presuppositions are often associated with spe-ciﬁc words or syntactic constructions (‘triggers’).
we compiled an initial list of presupposition trig-gers based on levinson (1983: 181–184) andvan der sandt (1992),1 and selected the followingtriggers based on their frequency in nq (» means‘presupposes’):.
• question words (what, where, who...): whodid jane talk to?
» jane talked to someone.
• deﬁnite article (the): i saw the cat » thereexists some contextually salient, unique cat.
• factive verbs (discover, ﬁnd out, prove...): i.found out that emma lied.
» emma lied..• possessive ’s: she likes fred’s sister.
» fred.
has a sister..• temporal adjuncts (when, during, while...): iwas walking when the murderer escaped fromprison.
» the murderer escaped from prison.
• counterfactuals (if + past): i would have beenhappier if i had a dog.
» i don’t have a dog..2 presuppositions.
presuppositions are implicit assumptions of utter-ances that interlocutors take for granted.
for exam-ple, if i uttered the sentence i love my hedgehog,.
our work focuses on presuppositions of ques-tions.
we assume presuppositions project from.
1we note that it is a simplifying view to treat all triggers.
under the banner of presupposition; see karttunen (2016)..3933qa modelknowledge sourceunanswerableqa modelq: which linguist invented the lightbulb?knowledge sourceunanswerablepresupposition generation (s5.1)p: some linguist invented the lightbulbq: which linguist invented the lightbulb?verification (s5.2)explanation generatorexplanation generation (s5.3)end-to-end qa (s6)user study (s4)because ...cause of unanswerability.
%.
example q.comment.
unveriﬁable presupposition.
30% what is the stock symbol for mars candy.
presupposition ‘stock symbol for mars candy exists’ fails.
reference resolution failureretrieval failuresubjectivitycommonsensical.
9% what kind of vw jetta do i have6% when did the salvation army come to australia3% what is the perfect height for a model3% where does how to make an american quilt take place document contains no evidence that the movie took place.
the system does not know who ‘i’ ispage retrieved was safe schools coalition australiarequires subjective judgment.
somewhere, but it is commonsensical that it did.
actually answerablenot a question/malformed question.
8% when do other cultures celebrate the new year3% where do you go my lovely full version.
the question was actually answerable given the documentnot an actual question.
table 1: example causes of unanswerability in nq.
% denotes the percentage of questions that both annotatorsagreed to be in the respective cause categories..wh-questions—that is, presuppositions (other thanthe presupposition introduced by the interrogativeform) remain constant under wh-questions as theydo under negation (e.g., i don’t like my sister hasthe same possessive presupposition as i like my sis-ter).
however, the projection problem is complex;for instance, when embedded under other operators,presuppositions can be overtly denied (levinson1983: 194).
see also schlenker (2008), abrusán(2011), schwarz and simonenko (2018), theiler(2020), i.a., for discussions regarding projectionpatterns under wh-questions.
we adopt the view ofstrawson (1950) that deﬁnite descriptions presup-pose both existence and (contextual) uniqueness,but this view is under debate.
see coppock andbeaver (2012), for instance, for an analysis of thethat does not presuppose existence and presupposesa weaker version of uniqueness.
furthermore, wecurrently do not distinguish predicative and argu-mental deﬁnites..presuppositions and unanswerability.
ques-tions containing failed presuppositions are oftentreated as unanswerable in qa datasets.
an ex-ample is the question what is the stock symbol formars candy?
from nq.
this question is not answer-able with any description of a stock symbol (thatis, an answer to the what question), because marsis not a publicly traded company and thus does nothave a stock symbol.
a better response would beto point out the presupposition failure, as in thereis no stock symbol for mars candy.
however, state-ments about negative factuality are rarely explicitlystated, possibly due to reporting bias (gordon andvan durme, 2013).
therefore, under an extractiveqa setup as in nq where the answers are spansfrom an answer source (e.g., a wikipedia article), itis likely that such questions will be unanswerable.
our proposal is based on the observation thatthe denial of a failed presupposition (¬p) can beused to explain the unanswerability of questions.
(q) containing failed presuppositions (p), as in (2)..(2).
q: who is the current monarch of france?
p: there is a current monarch of france.
¬p: there is no such thing as a currentmonarch of france..an answer that refers to the presupposition, such as¬p, would be more informative compared to bothunanswerable (1-b) and an extractive answer fromdocuments that are topically relevant but do notmention the false presupposition..3 analysis of unanswerable questions.
first, to quantify the role of presupposition failurein qa, two of the authors analyzed 100 randomlyselected unanswerable wh-questions in the nq de-velopment set.2 the annotators labeled each ques-tion as presupposition failure or not presuppositionfailure, depending on whether its unanswerabilitycould be explained by the presence of an unveriﬁ-able presupposition with respect to the associateddocument.
if the unanswerability could not beexplained in terms of presupposition failure, theannotators provided a reasoning.
the cohen’s κfor inter-annotator agreement was 0.586..we found that 30% of the analyzed questionscould be explained by the presence of an unver-iﬁable presupposition in the question, consider-ing only the cases where both annotators werein agreement (see table 1).3 after adjudicatingthe reasoning about unanswerability for the non-presupposition failure cases, another 21% fell intocases where presupposition failure could be par-tially informative (see table 1 and appendix afor details).
the unveriﬁable presuppositions were.
2the nq development set provides 5 answer annotationsper question—we only looked at questions with 5/5 null an-swers here..3wh-questions constitute ∼69% of the nq developmentset, so we expect the actual portion of questions with presup-position failiure-based explanation to be ∼21%..3934question: where can i buy a japanese dwarf ﬂying squirrel.
simple unanswerable.
this question is unanswerable..presupposition failure-based.
this question is unanswerable because we could not verify that you canbuy a japanese dwarf flying squirrel anywhere..extractive explanation.
dpr rewrite.
this question is unanswerable because it grows to a length of 20 cm (8 in)and has a membrane connecting its wrists and ankles which enables it toglide from tree to tree..after it was returned for the second time, the original owner, referring toit as “the prodigal gnome", said she had decided to keep it and wouldnot sell it on ebay again..table 2: systems (answer types) compared in the user preference study and examples..figure 2: results of the user preference study.
chart labels denote the two systems being compared (s1 vs. s2)..triggered by question words (19/30), the deﬁnitearticle the (10/30), and a factive verb (1/30)..type of the failed presupposition.
see sec-tion 5.3 for more details..4 user study with oracle explanation.
our hypothesis is that statements explicitly refer-ring to failed presuppositions can better4 speak tothe unanswerability of corresponding questions.
totest our hypothesis, we conducted a side-by-sidecomparison of the oracle output of our proposedsystem and the oracle output of existing (closed-book) qa systems for unanswerable questions.
weincluded two additional systems for comparison;the four system outputs compared are describedbelow (see table 2 for examples):.
• simple unanswerable: a simple assertionthat the question is unanswerable (i.e., thisquestion is unanswerable).
this is the ora-cle behavior of closed-book qa systems thatallow unanswerable as an answer..• presupposition failure-based explanation:a denial of the presupposition that is unveri-ﬁable from the answer source.
this takes theform of either this question is unanswerablebecause we could not verify that... or ...be-cause it is unclear that... depending on the.
4we deﬁne better as user preference in this study, but otherdimensions could also be considered such as trustworthiness..• extractive explanation: a random sentencefrom a wikipedia article that is topically re-lated to the question, preﬁxed by this questionis unanswerable because.... this system is in-troduced as a control to ensure that length biasis not in play in the main comparison (e.g.,users may a priori prefer longer, topically-related answers over short answers).
thatis, since our system, presupposition failure-based explanation, yields strictly longer an-swers than simple unanswerable, we want toensure that our system is not preferred merelydue to length rather than answer quality..• open-domain rewrite: a rewrite of the non-oracle output taken from the demo5 of densepassage retrieval (dpr; karpukhin et al.
2020), a competitive open-domain qa sys-tem.
this system is introduced to test whetherpresupposition failure can be easily addressedby expanding the answer source, since a singlewikipedia article was used to determine pre-supposition failure.
if presupposition failureis a problem particular only to closed-booksystems, a competitive open-domain systemwould sufﬁce to address this issue.
while theoutputs compared are not oracle, this system.
5http://qa.cs.washington.edu:2020/.
393518%15%0%65%s1: extractives2: no explanationsystem 1 is bettersystem 2 is betterboth are goodboth are bad39%8%2%49%s1: karpukhin (2020)s2: no explanation37%10%3%49%s1: karpukhin (2020)s2: extractive74%0%2%23%s1: presup.s2: no explanation57%9%7%24%s1: presup.s2: extractive41%26%6%24%s1: presup.s2: karpukhin (2020)question (input).
template.
presupposition (output).
which philosopher advocated the idea of return to naturewhen was it discovered that the sun rotateswhen is the year of the cat in chinese zodiacwhen is the year of the cat in chinese zodiacwhat do the colors on ecuador’s ﬂag mean.
some ______ exists__ is contextually unique__ has __.
some philosopher advocated the idea of return to naturethe sun rotates‘year of the cat in chinese zodiac’ exists‘year of the cat in chinese zodiac’ is contextually unique‘ecuador’ has ‘ﬂag’.
table 3: example input-output pairs of our presupposition generator.
text in italics denotes the part taken from theoriginal question, and the plain text is the part from the generation template.
all questions are taken from nq..has an advantage of being able to refer to allof wikipedia.
the raw output was rewrittento be well-formed, so that it was not unfairlydisadvantaged (see appendix b.2)..study.
we conducted a side-by-side study with100 unanswerable questions.
these questions wereunanswerable questions due to presupposition fail-ure, as judged independently and with high conﬁ-dence by two authors.6 we presented an exhaustivebinary comparison of four different types of an-swers for each question (six binary comparisonsper question).
we recruited ﬁve participants on aninternal crowdsourcing platform at google, whowere presented with all binary comparisons forall questions.
all comparisons were presented inrandom order, and the sides that the comparisonsappeared in were chosen at random.
for each com-parison, the raters were provided with an unanswer-able question, and were asked to choose the systemthat yielded the answer they preferred (either sys-tem 1 or 2).
they were also given the options bothanswers are good/bad.
see appendix b.1 for addi-tional details about the task setup..results.
figure 2 shows the user preferences forthe six binary comparisons, where blue and graydenote preferences for the two systems compared.
we ﬁnd that presupposition-based answers are pre-ferred against all three answer types with whichthey were compared, and prominently so whencompared to the oracle behavior of existing closed-book qa systems (4th chart, presup.
vs. no ex-planation).
this supports our hypothesis that pre-supposition failure-based answers would be moresatisfactory to the users, and suggests that buildinga qa system that approaches the oracle behaviorof our proposed system is a worthwhile pursuit..5 model components.
given that presupposition failure accounts for asubstantial proportion of unanswerable questions(section 3) and our proposed form of explanationsis useful (section 4), how can we build a qa sys-tem that offers such explanations?
we decomposethis task into three smaller sub-tasks: presuppo-sition generation, presupposition veriﬁcation, andexplanation generation.
then, we present progresstowards each subproblem using nq.7 we use atemplatic approach for the ﬁrst and last steps.
thesecond step involves veriﬁcation of the generatedpresuppositions of the question against an answersource, for which we test four different strategies:zero-shot transfer from natural language infer-ence (nli), an nli model ﬁnetuned on veriﬁcation,zero-shot transfer from fact veriﬁcation, and a rule-based/nli hybrid model.
since we used nq, ourmodels assume a closed-book setup with a singledocument as the source of veriﬁcation..5.1 step 1: presupposition generation.
linguistic triggers.
using the linguistic triggersdiscussed in section 2, we implemented a rule-based generator to templatically generate presuppo-sitions from questions.
see table 3 for examples,and appendix c for a full list..generation.
the generator takes as input a con-stituency parse tree of a question string from theberkeley parser (petrov et al., 2006) and appliestrigger-speciﬁc transformations to generate the pre-supposition string (e.g., taking the sentential com-plement of a factive verb).
if there are multipletriggers in a single question, all presuppositionscorresponding to the triggers are generated.
thus,a single question may have multiple presupposi-tions.
see table 3 for examples of input questionsand output presuppositions..6hence, this set did not necessarily overlap with the ran-domly selected unanswerable questions from section 3; wewanted to speciﬁcally ﬁnd a set of questions that were repre-sentative of the phenomena we address in this work..7code and data will be available at.
https://github.com/google-research/google-research/presup-qa.
3936how good is our generation?
we analyzed 53questions and 162 generated presuppositions toestimate the quality of our generated presupposi-tions.
this set of questions contained at least 10instances of presuppositions pertaining to each cat-egory.
one of the authors manually validated thegenerated presuppositions.
according to this anal-ysis, 82.7% (134/162) presuppositions were validpresuppositions of the question.
the remainingcases fell into two broad categories of error: un-grammatical (11%, 18/162) or grammatical but notpresupposed by the question (6.2%, 10/162).
thelatter category of errors is a limitation of our rule-based generator that does not take semantics intoaccount, and suggests an avenue by which futurework can yield improvements.
for instance, weuniformly apply the template ‘a’ has ‘b’8 for pre-suppositions triggered by ’s.
while this templateworks well for cases such as elsa’s sister » ‘elsa’has ‘sister’, it generates invalid presuppositionssuch as bachelor’s degree » #‘bachelor’ has ‘de-gree’.
finally, the projection problem is anotherlimitation.
for example, who does pip believe isestella’s mother has an embedded possessive undera nonfactive verb believe, but our generator wouldnevertheless generate ‘estella’ has ‘mother’..5.2 step 2: presupposition veriﬁcation.
the next step is to verify whether presuppositionsof a given question is veriﬁable from the answersource.
the presuppositions were ﬁrst generated us-ing the generator described in section 5.1, and thenmanually repaired to create a veriﬁcation datasetwith gold presuppositions.
this was to ensure thatveriﬁcation performance is estimated without apropagation of error from the previous step.
gen-erator outputs that were not presupposed by thequestions were excluded..to obtain the veriﬁcation labels, two of the au-thors annotated 462 presuppositions on their binaryveriﬁability (veriﬁable/not veriﬁable) based on thewikipedia page linked to each question (the linkswere provided in nq).
a presupposition was la-beled veriﬁable if the page contained any statementthat either asserted or implied the content of thepresupposition.
the cohen’s κ for inter-annotatoragreement was 0.658. the annotators reconciledthe disagreements based on a post-annotation dis-.
8we used a template that puts possessor and possesseenps in quotes instead of using different templates dependingon posessor/possessee plurality (e.g., a __ has a __/a __ has__/__ have a __/__ have __)..cussion to ﬁnalize the labels to be used in the exper-iments.
we divided the annotated presuppositionsinto development (n = 234) and test (n = 228)sets.9 we describe below four different strategieswe tested..zero-shot nli.
nli is a classiﬁcation task inwhich a model is given a premise-hypothesis pairand asked to infer whether the hypothesis is en-tailed by the premise.
we formulate presuppositionveriﬁcation as nli by treating the document as thepremise and the presupposition to verify as the hy-pothesis.
since wikipedia articles are often largerthan the maximum premise length that nli modelscan handle, we split the article into sentences andcreated n premise-hypothesis pairs for an articlewith n sentences.
then, we aggregated these pre-dictions and labeled the hypothesis (the presuppo-sition) as veriﬁable if there are at least k sentencesfrom the document that supported the presuppo-sition.
if we had a perfect veriﬁer, k = 1 wouldsufﬁce to perform veriﬁcation.
we used k = 1 forour experiments, but k could be treated as a hyper-parameter.
we used albert-xxlarge (lan et al.,2020) ﬁnetuned on mnli (williams et al., 2018)and qnli (wang et al., 2019) as our nli model..finer-tuned nli.
existing nli datasets such asqnli contain a broad distribution of entailmentpairs.
we adapted the model further to the distri-bution of entailment pairs that are speciﬁc to ourgenerated presuppositions (e.g., hypothesis: npis contextually unique) through additional ﬁnetun-ing (i.e., ﬁner-tuning).
through crowdsourcing onan internal platform, we collected entailment la-bels for 15,929 (presupposition, sentence) pairs,generated from 1000 questions in nq and 5 sen-tences sampled randomly from the correspondingwikipedia pages.
we continued training the modelﬁne-tuned on qnli on this additional dataset toyield a ﬁner-tuned nli model.
finally, we aggre-gated per-sentence labels as before to get veriﬁabil-ity labels for (presupposition, document) pairs..zero-shot fever.
fever is a fact veriﬁcationtask proposed by thorne et al.
(2018).
we for-mulate presupposition veriﬁcation as a fact veri-ﬁcation task by treating the wikipedia article asthe evidence source and the presupposition as theclaim.
while typical fever systems have a docu-.
9the dev/test set sizes did not exactly match because wekept presuppositions of same question within the same split,and each question had varying numbers of presuppositions..3937macro f1 acc..0.44.
0.500.550.540.58.
0.580.590.60.
0.78.
0.510.730.660.76.
0.710.770.79.model.
majority class.
zero-shot nli (albert mnli + wiki sentences)zero-shot nli (albert qnli + wiki sentences)zero-shot fever (kgat + wiki sentences)finer-tuned nli (albert qnli + wiki sentences).
rule-based/nli hybrid (albert qnli + wiki presuppositions)rule-based/nli hybrid (albert qnli + wiki sentences + wiki presuppositions)finer-tuned, rule-based/nli hybrid (albert qnli + wiki sentences + wiki presuppositions).
table 4: performance of veriﬁcation models tested.
models marked with ‘wiki sentence’ use sentences fromwikipedia articles as premises, and ‘wiki presuppositions’, generated presuppositions from wikipedia sentences..ment retrieval component, we bypass this step anddirectly perform evidence retrieval on the articlelinked to the question.
we used the graph neuralnetwork-based model of liu et al.
(2020) (kgat)that achieves competitive performance on fever.
a key difference between kgat and nli mod-els is that kgat can consider pieces of evidencejointly, whereas with nli, the pieces of evidenceare veriﬁed independently and aggregated at theend.
for presuppositions that require multihop rea-soning, kgat may succeed in cases where aggre-gated nli fails—e.g., for uniqueness.
that is, ifthere is no sentence in the document that bears thesame uniqueness presupposition, one would needto reason over all sentences in the document..rule-based/nli hybrid.
we consider a rule-based approach where we apply the same genera-tion method described in section 5 to the wikipediadocuments to extract the presuppositions of theevidence sentences.
the intended effect is to ex-tract content that is directly relevant to the taskat hand—that is, we are making the presupposi-tions of the documents explicit so that they canbe more easily compared to presuppositions beingveriﬁed.
however, a naïve string match betweenpresuppositions of the document and the questionswould not work, due to stylistic differences (e.g.,deﬁnite descriptions in wikipedia pages tend tohave more modiﬁers).
hence, we adopted a hybridapproach where the zero-shot qnli model wasused to verify (document presupposition, questionpresupposition) pairs..results.
our results (table 4) suggest that pre-supposition veriﬁcation is challenging to existingmodels, partly due to class imbalance.
only themodel that combines ﬁner-tuning and rule-baseddocument presuppositions make modest improve-.
ment over the majority class baseline (78% →79%).
nevertheless, gains in f1 were substan-tial for all models (44% → 60% in best model),showing that these strategies do impact veriﬁability,albeit with headroom for improvement.
qnli pro-vided the most effective zero-shot transfer, possiblybecause of domain match between our task and theqnli dataset—they are both based on wikipedia.
the fever model was unable to take advantageof multihop reasoning to improve over (q)nli,whereas using document presuppositions (rule-based/nli hybrid) led to gains over nli alone..5.3 step 3: explanation generation.
we used a template-based approach to explanationgeneration: we prepended the templates this ques-tion is unanswerable because we could not verifythat... or ...because it is unclear that... to the unver-iﬁable presupposition (3).
note that we worded thetemplate in terms of unveriﬁability of the presuppo-sition, rather than asserting that it is false.
under aclosed-book setup like nq, the only ground truthavailable to the model is a single document, whichleaves a possibility that the presupposition is veri-ﬁable outside of the document (except in the rareoccasion that it is refuted by the document).
there-fore, we believe that unveriﬁability, rather thanfailure, is a phrasing that reduces false negatives..(3).
q: when does back to the future part 4 comeoutunveriﬁable presupposition:there issome point in time that back to the futurepart 4 comes outsimple preﬁxing: this question is unan-swerable because we could not verify thatthere is some point in time that back to thefuture part 4 comes out..3938model.
average f1 long answer f1 short answer f1 unans.
acc unans.
f1.
etc (our replication)+ presuppositions (ﬂat)+ veriﬁcation labels (ﬂat)+ presups + labels (ﬂat)+ presups + labels (structured).
0.6450.6410.6450.6430.649.
0.7420.7350.7420.7440.743.
0.5480.5470.5470.5440.555.
0.6950.7020.6870.7020.703.
0.6940.7000.6840.7000.700.table 5: performance on nq development set with etc and etc augmented with presupposition information.
wecompare our augmentation results against our own replication of ainslie et al.
(2020) (ﬁrst row)..for the user study (section 4), we used a manual,more ﬂuent rewrite of the explanation generatedby simple preﬁxing.
in future work, ﬂuency is adimension that can be improved over templatic gen-eration.
for example, for (3), a ﬂuent model couldgenerate the response: this question is unanswer-able because we could not verify that back to thefuture part 4 will ever come out..6 end-to-end qa integration.
while the 3-step pipeline is designed to generateexplanations for unanswerability, the generated pre-suppositions and their veriﬁability can also provideuseful guidance even for a standard extractive qasystem.
they may prove useful both to unanswer-able and answerable questions, for instance by indi-cating which tokens of a document a model shouldattend to.
we test several approaches to augment-ing the input of a competitive extractive qa systemwith presuppositions and veriﬁcation labels..model and augmentation.
we used extendedtransformer construction (etc) (ainslie et al.,2020), a model that achieves competitive perfor-mance on nq, as our base model.
we adoptedthe conﬁguration that yielded the best reported nqperformance among etc-base models.10 we ex-periment with two approaches to encoding the pre-supposition information.
first, in the ﬂat model,we simply augment the input question representa-tion (token ids of the question) by concatenatingthe token ids of the generated presuppositions andthe veriﬁcation labels (0 or 1) from the albertqnli model.
second, in the structured model (fig-ure 4), we take advantage of the global input layerof etc that is used to encode the discourse unitsof large documents like paragraphs.
global tokensattend (via self-attention) to all tokens of their in-.
ternal text, but for other text in the document, theyonly attend to the corresponding global tokens.
weadd one global token for each presupposition, andallow the presupposition tokens to only attend toeach other and the global token.
the value of theglobal token is set to the veriﬁcation label (0 or 1)..metrics.
we evaluated our models on two sets ofmetrics: nq performance (long answer, short an-swer, and average f1) and unanswerability clas-siﬁcation (accuracy and f1).11 we included thelatter because our initial hypothesis was that sen-sitivity to presuppositions of questions would leadto better handling of unanswerable questions.
theetc nq model has a built-in answer type classiﬁ-cation step which is a 5-way classiﬁcation between{unanswerable, long answer, short answer, yes,no}.
we mapped the classiﬁer outputs to binaryanswerability labels by treating the predicted labelas unanswerable only if its logit was greater thanthe sum of all other options..results and discussion table 5 shows that aug-mentations that use only the presuppositions oronly the veriﬁcation labels do not lead to gains innq performance over the baseline, but the presup-positions do lead to gains on unanswerability clas-siﬁcation.
when both presuppositions and theirveriﬁability are provided, we see minor gains inaverage f1 and unanswerability classiﬁcation.12for unanswerability classiﬁcation, the improvedaccuracy is different from the baseline at the 86%(ﬂat) and 89% (structured) conﬁdence level usingmcnemar’s test.
the main bottleneck of our modelis the quality of the veriﬁcation labels used for aug-mentation (table 4)—noisy labels limit the capac-ity of the qa model to attend to the augmentations.
while the gain on unanswerability classiﬁca-tion is modest, an error analysis suggests that.
10the reported results in ainslie et al.
(2020) are obtainedusing a custom modiﬁcation to the inference procedure thatwe do not incorporate into our pipeline, since we are only in-terested in the relative gains from presupposition veriﬁcation..11here, we treated ≥ 4 null answers as unanswerable,.
following the deﬁnition in kwiatkowski et al.
(2019)..12to contextualize our results, a recently published nqmodel (ainslie et al., 2020) achieved a gain of around ∼2%..3939the added presuppositions modulate the predictionchange in our best-performing model (structured)from the baseline etc model.
looking at the caseswhere changes in model prediction (i.e., unanswer-able (u) ↔ answerable (a)) lead to correct an-swers, we observe an asymmetry in the two possi-ble directions of change.
the number of correct a→ u cases account for 11.9% of the total numberof unanswerable questions, whereas correct u →a cases account for 6.7% of answerable questions.
this asymmetry aligns with the expectation that thepresupposition-augmented model should achievegains through cases where unveriﬁed presupposi-tions render the question unanswerable.
for exam-ple, given the question who played david brent’sgirlfriend in the ofﬁce that contains a false presup-position david brent has a girlfriend, the struc-tured model changed its prediction to unanswer-able from the base model’s incorrect answer juliadavis (an actress, not david brent’s girlfriend ac-cording to the document: .
.
.
arrange a meetingwith the second woman (voiced by julia davis)).
on the other hand, such an asymmetry is not ob-served in cases where changes in model predictionresults in incorrect answers: incorrect a → u andu → a account for 9.1% and 9.2%, respectively.
more examples are shown in appendix f..7 related work.
while presuppositions are an active topic of re-search in theoretical and experimental linguistics(beaver, 1997; simons, 2013; schwarz, 2016, i.a.,),comparatively less attention has been given to pre-suppositions in nlp (but see clausen and manning(2009) and tremper and frank (2011)).
more re-cently, cianﬂone et al.
(2018) discuss automaticallydetecting presuppositions, focusing on adverbialtriggers (e.g., too, also...), which we excluded dueto their infrequency in nq.
jeretic et al.
(2020)investigate whether inferences triggered by presup-positions and implicatures are captured well bynli models, ﬁnding mixed results..regarding unanswerable questions, their impor-tance in qa (and therefore their inclusion in bench-marks) has been argued by works such as clark andgardner (2018) and zhu et al.
(2019).
the analysisportion of our work is similar in motivation to unan-swerability analyses in yatskar (2019) and asai andchoi (2020)—to better understand the causes ofunanswerability in qa.
hu et al.
(2019); zhanget al.
(2020); back et al.
(2020) consider answer-.
ability detection as a core motivation of their mod-eling approaches and propose components such asindependent no-answer losses, answer veriﬁcation,and answerability scores for answer spans..our work is most similar to geva et al.
(2021) inproposing to consider implicit assumptions of ques-tions.
furthermore, our work is complementary toqa explanation efforts like lamm et al.
(2020) thatonly consider answerable questions..finally, abstractive qa systems (e.g., fan et al.
2019) were not considered in this work, but their ap-plication to presupposition-based explanation gen-eration could be an avenue for future work..8 conclusion.
through an nq dataset analysis and a user prefer-ence study, we demonstrated that a signiﬁcant por-tion of unanswerable questions can be answeredmore effectively by calling out unveriﬁable pre-suppositions.
to build models that provide suchan answer, we proposed a novel framework thatdecomposes the task into subtasks that can be con-nected to existing problems in nlp: presuppositionidentiﬁcation (parsing and text generation), presup-position veriﬁcation (textual inference and fact ver-iﬁcation), and explanation generation (text genera-tion).
we observed that presupposition veriﬁcation,especially, is a challenging problem.
a combina-tion of a competitive nli model, ﬁner-tuning andrule-based hybrid inference gave substantial gainsover the baseline, but was still short of a fully satis-factory solution.
as a by-product, we showed thatveriﬁed presuppositions can modestly improve theperformance of an end-to-end qa model..in the future, we plan to build on this work byproposing qa systems that are more robust andcooperative.
for instance, different types of presup-position failures could be addressed by more ﬂuidanswer strategies—e.g., violation of uniquenesspresuppositions may be better handled by provid-ing all possible answers, rather than stating that theuniqueness presupposition was violated..acknowledgments.
we thank tom kwiatkowski, mike collins,tania rojas-esponda, eunsol choi, annie louis,michael tseng, kyle rawlins, tania bedrax-weiss,and elahe rahimtoroghi for helpful discussionsabout this project.
we also thank lora aroyo forhelp with user study design, and manzil zaheer forpointers about replicating the etc experiments..3940references.
márta abrusán.
2011. presuppositional and negativeislands: a semantic account.
natural language se-mantics, 19(3):257–321..joshua ainslie, santiago ontanon, chris alberti, va-clav cvicek, zachary fisher, philip pham, anirudhravula, sumit sanghai, qifan wang, and li yang.
2020. etc: encoding long and structured inputsin proceedings of the 2020 con-in transformers.
ference on empirical methods in natural languageprocessing (emnlp), pages 268–284, online.
asso-ciation for computational linguistics..akari asai and eunsol choi.
2020. challenges in in-formation seeking qa: unanswerable questions andparagraph retrieval.
arxiv:2010.11915..seohyun back, sai chetan chinthakindi, akhil kedia,haejun lee, and jaegul choo.
2020. neurquri:neural question requirement inspector for answer-ability prediction in machine reading comprehen-sion.
in international conference on learning rep-resentations..david beaver.
1997. presupposition.
in handbook oflogic and language, pages 939–1008.
elsevier..andre cianﬂone, yulan feng, jad kabbara, and jackiechi kit cheung.
2018. let’s do it “again”: a ﬁrstcomputational approach to detecting adverbial pre-supposition triggers.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 2747–2755, melbourne, australia.
association for com-putational linguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 845–855, melbourne,australia.
association for computational linguis-tics..david clausen and christopher d. manning.
2009. pre-supposed content and entailments in natural lan-guage inference.
in proceedings of the 2009 work-shop on applied textual inference (textinfer), pages70–73, suntec, singapore.
association for computa-tional linguistics..elizabeth coppock and david beaver.
2012. weakuniqueness: the only difference between deﬁnitesand indeﬁnites.
in semantics and linguistic theory,volume 22, pages 527–544..angela fan, yacine jernite, ethan perez, david grang-ier, jason weston, and michael auli.
2019. eli5:in proceedings oflong form question answering.
the 57th annual meeting of the association for com-putational linguistics, pages 3558–3567, florence,italy.
association for computational linguistics..mor geva, daniel khashabi, elad segal, tushar khot,dan roth, and jonathan berant.
2021. did aristotleuse a laptop?
a question answering benchmark withimplicit reasoning strategies.
arxiv:2101.02235..jonathan gordon and benjamin van durme.
2013. re-porting bias and knowledge acquisition.
in proceed-ings of the 2013 workshop on automated knowledgebase construction, akbc ’13, page 25–30, newyork, ny, usa.
association for computing machin-ery..minghao hu, furu wei, yuxing peng, zhen huang,nan yang, and dongsheng li.
2019. read+ verify:machine reading comprehension with unanswerablequestions.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 33, pages 6529–6537..paloma jeretic, alex warstadt, suvrat bhooshan, andadina williams.
2020. are natural language infer-ence models imppressive?
learning implicatureand presupposition.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8690–8705, online.
associationfor computational linguistics..vladimir karpukhin, barlas oguz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6769–6781, online.
association for computational lin-guistics..lauri karttunen.
2016. presupposition: what wentin semantics and linguistic theory, vol-.
wrong?
ume 26, pages 705–731..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:452–466..matthew lamm, jennimaria palomaki, chris alberti,daniel andor, eunsol choi, livio baldini soares,and michael collins.
2020. qed: a frameworkand dataset for explanations in question answering.
arxiv:2009.06354..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin interna-learning of language representations.
tional conference on learning representations..stephen c. levinson.
1983. pragmatics, cambridgetextbooks in linguistics, pages 181–184, 194. cam-bridge university press..3941zhenghao liu, chenyan xiong, maosong sun, andzhiyuan liu.
2020. fine-grained fact veriﬁcationwith kernel graph attention network.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7342–7351, on-line.
association for computational linguistics..randall munroe.
2020. learning new things from.
google.
https://twitter.com/xkcd/status/1333529967079120896. accessed: 2021-02-01..slav petrov, leon barrett, romain thibaux, and danklein.
2006. learning accurate, compact, and inter-pretable tree annotation.
in proceedings of the 21stinternational conference on computational linguis-tics and 44th annual meeting of the association forcomputational linguistics, pages 433–440, sydney,australia.
association for computational linguis-tics..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..rob a. van der sandt.
1992. presupposition projec-tion as anaphora resolution.
journal of semantics,9(4):333–377..philippe schlenker.
2008. presupposition projection:theoretical linguistics,.
explanatory strategies.
34(3):287–316..bernhard schwarz and alexandra simonenko.
2018.decomposing universal projection in questions.
insinn und bedeutung 22, volume 22, pages 361–374..florian schwarz.
2016. experimental work in presup-position and presupposition projection.
annual re-view of linguistics, 2(1):273–292..mandy simons.
2013. presupposing.
pragmatics of.
speech actions, pages 143–172..peter f. strawson.
1950..on referring.
mind,.
59(235):320–344..nadine theiler.
2020. an epistemic bridge for presup-position projection in questions.
in semantics andlinguistic theory, volume 30, pages 252–272..james.
andreas vlachos,.
and arpit mittal..thorne,christos2018.christodoulopoulos,fever: a large-scale dataset for fact extractionin proceedings of the 2018and veriﬁcation.
conference ofthe north american chapter ofthe association for computational linguistics:human language technologies, volume 1 (longpapers), pages 809–819, new orleans, louisiana.
association for computational linguistics..galina tremper and anette frank.
2011. extendingﬁne-grained semantic relation classiﬁcation to pre-supposition relations between verbs.
bochumer lin-guistische arbeitsberichte..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis plat-in inter-form for natural language understanding.
national conference on learning representations..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..mark yatskar.
2019. a qualitative comparison ofcoqa, squad 2.0 and quac.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 2318–2323, minneapolis,minnesota.
association for computational linguis-tics..zhuosheng zhang, junjie yang, and hai zhao.
2020.retrospective reader for machine reading compre-hension.
arxiv:2001.09694..haichao zhu, li dong, furu wei, wenhui wang, bingqin, and ting liu.
2019. learning to ask unan-swerable questions for machine reading comprehen-in proceedings of the 57th annual meetingsion.
of the association for computational linguistics,pages 4238–4248, florence, italy.
association forcomputational linguistics..a additional causes of unanswerable.
questions.
listed below are cases of unanswerable questionsfor which presupposition failure may be partiallyuseful:.
• document retrieval failure: the retrieveddocument is unrelated to the question, so thepresuppositions of the questions are unlikelyto be veriﬁable from the document..• failure of commonsensical presupposi-tions: the document does not directly sup-port the presupposition but the presuppositionis commonsensical..• presuppositions involving subjective judg-ments: veriﬁcation of the presupposition re-quires subjective judgment, such as the exis-tence of the best song..3942figure 3: the user interface for the user preference study..• reference resolution failure: the questioncontains an unresolved reference such as apro-form (i, here...) or a temporal expression(next year...).
therefore the presuppositionsalso fail due to unresolved reference..b user study.
b.1 task design.
figure 3 shows the user interface (ui) for the study.
the raters were given a guideline that instructedthem to select the answer that they preferred, imag-ining a situation in which they have entered thegiven question to two different qa systems.
toavoid biasing the participants towards any answertype, we used a completely unrelated, nonsensicalexample (q: are potatoes fruit?
system 1: yes,because they are not vegetables.
system 2: yes,because they are not tomatoes.)
in our guidelinedocument..b.2 dpr rewrites.
the dpr answers we used in the user study wererewrites of the original outputs.
dpr by defaultreturns a paragraph-length wikipedia passage thatcontains the short answer to the question.
from thisdefault output, we manually extracted the sentence-level context that fully contains the short answer,and repaired the context into a full sentence if theextracted context was a sentence fragment.
thiswas to ensure that all answers compared in thestudy were well-formed sentences, so that userpreference was determined by the content of thesentences rather than their well-formedness..c presupposition generation templates.
d data collection.
the user study (section 4) and data collection of en-tailment pairs from presuppositions and wikipediasentences (section 5) have been performed bycrowdsourcing internally at google.
details ofthe user study is in appendix b. entailment judge-ments were elicited from 3 raters for each pair, andmajority vote was used to assign a label.
becauseof class imbalance, all positive labels were kept inthe data and negative examples were down-sampledto 5 per document..e modeling details.
e.1 zero-shot nli.
mnli and qnli were trained following instruc-tions for ﬁne-tuning on top of albert-xxlargeathttps://github.com/google-research/albert/blob/master/albert_glue_fine_tuning_tutorial.ipynb with the default settingsand parameters..e.2 kgat.
we used the off-the-shelf model from https://github.com/thunlp/kernelgat (bert-base)..e.3 etc models.
for all etc-based models, we used the same modelparameter settings as ainslie et al.
(2020) usedfor nq, only adjusting the maximum global in-put length to 300 for the ﬂat models to accommo-date the larger set of tokens from presuppositions.
model selection was done by choosing hyperparam-eter conﬁgurations yielding maximum average f1.
weight lifting was done from bert-base insteadof roberta to keep the augmentation experimentssimple.
all models had 109m parameters..see table 6 for a full list of presupposition triggersand templates used for presupposition generation..all model training was done using the adamoptimizer with hyperparameter sweeps of learning.
3943question (input).
template.
presupposition (output).
who sings it’s a hard knock lifewhich philosopher advocated the idea of return to naturewhere do harry potter’s aunt and uncle livewhat did the treaty of paris do for the uswhen was the jury system abolished in indiahow did orchestra change in the romantic periodhow did orchestra change in the romantic periodwhy did jean valjean take care of cosettewhy did jean valjean take care of cosettewhen is the year of the cat in chinese zodiacwhen is the year of the cat in chinese zodiacwhat do the colors on ecuador’s ﬂag meanwhen was it discovered that the sun rotateshow old was macbeth when he died in the playwho would have been president if the south won the civil war.
there is someone that __some __there is some place that __there is something that __there is some point in time that ____there is some way that ____there is some reason that ____ exists__ is contextually unique__ has ______it is not true that __.
there is someone that sings it’s a hard knock lifesome philosopher advocated the idea of return to naturethere is some place that harry potter’s aunt and uncle livethere is something that the treaty of paris did for the usthere is some point in time that the jury system was abolished in indiaorchestra changed in the romantic periodthere is some way that orchestra changed in the romantic periodjean valjean took care of cosettethere is some reason that jean valjean took care of cosette‘year of the cat in chinese zodiac’ exists‘year of the cat in chinese zodiac’ is contextually unique‘ecuador’ has ‘ﬂag’the sun rotateshe died in the playit is not true that the south won the civil war.
table 6: example input-output pairs of our presupposition generator.
text in italics denotes the part taken from theoriginal question, and the plain text is the part from the generation template.
all questions are taken from nq..figure 4: the structured augmentation to the etcmodel.
qk are question tokens, pk are presuppositiontokens, sl are sentence tokens, pv are veriﬁcation la-bels, qid is the (constant) global question token andsid is the (constant) global sentence token..rates in {3×10−5, 5×10−5} and number of epochsin {3, 5} (i.e., 4 settings).
in cases of overﬁtting,an earlier checkpoint of the run with optimal vali-dation performance was picked.
all training wasdone on servers utilizing a tensor processing unit3.0 architecture.
average runtime of model trainingwith this architecture was 8 hours..figure 4 illustrates the structure augmented etcmodel that separates question and presuppositiontokens that we discussed in section 6..f etc prediction change examples.
we present selected examples of model predictionsfrom section 6 that illustrate the difference in be-havior of the baseline etc model and the struc-tured, presupposition-augmented model:.
1.
[correct answerable → unanswerable].
nq question: who played david brent’s girl-friend in the ofﬁcerelevant presupposition: david brent has agirlfriendwikipedia article: the ofﬁce christmasspecialsgold label: unanswerablebaseline label: answerablestructured model label: unanswerable.
explanation: the baseline model incorrectlypredicts arrange a meeting with the secondwoman (voiced by julia davis) as a long an-swer and julia davis as a short answer, in-ferring that the second woman met by davidbrent was his girlfriend.
the structured modelcorrectly ﬂips the prediction to unanswerable,possibly making use of the unveriﬁable pre-supposition david brent has a girlfriend..2.
[correct unanswerable → answerable].
nq question: when did cricket go to 6 balloversrelevant presupposition: cricket went to 6balls per over at some pointwikipedia article: over (cricket)gold label: answerablebaseline label: unanswerablestructured model label: answerableexplanation: the baseline model was likelyconfused because the long answer candidateonly mentions test cricket, but support forthe presupposition came from the sentencealthough six was the usual number of balls,it was not always the case, leading the struc-tured model to choose the correct long answercandidate..3.
[incorrect answerable → unanswerable].
there is some.
nq question: what is loihi and where doesit originate fromrelevant presupposition:place that it originates fromwikipedia article: l¯oihi seamountgold label: answerablebaseline label: answerablestructured model label: unanswerableexplanation: the baseline model ﬁnds thecorrect answer (hawaii hotspot) but the struc-.
3944tured model incorrectly changes the predic-tion.
this is likely due to veriﬁcation error—although the presupposition there is someplace that it originates from is veriﬁable, itwas incorrectly labeled as unveriﬁable.
pos-sibly, the the unresolved it contributed to thisveriﬁcation error, since our veriﬁer currentlydoes not take the question itself into consider-ation..3945