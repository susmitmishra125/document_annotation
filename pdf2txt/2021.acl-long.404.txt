a cognitive regularizer for language modeling.
jason wei.
clara meister.
ryan cotterell.
,.
google ai language.
jasonwei@google.com.
eth zürich.
university of cambridgeclara.meister@inf.ethz.ch.
ryan.cotterell@inf.ethz.ch.
abstract.
the uniform information density (uid) hy-pothesis, which posits that speakers behavingoptimally tend to distribute information uni-formly across a linguistic signal, has gainedtraction in psycholinguistics as an explanationfor certain syntactic, morphological, andprosodic choices.
in this work, we explorewhether the uid hypothesis can be opera-tionalized as an inductive bias for statisticallanguage modeling.
speciﬁcally, we augmentthe canonical mle objective for training lan-guage models with a regularizer that encodesuid.
in experiments on ten languages span-ning ﬁve language families, we ﬁnd that usinguid regularization consistently improvesperplexity in language models, having a largereffect when training data is limited.
moreover,via an analysis of generated sequences, weﬁnd that uid-regularized language modelshave other desirable properties, e.g., they gen-erate text that is more lexically diverse.
ourresults not only suggest that uid is a reason-able inductive bias for language modeling, butalso provide an alternative validation of theuid hypothesis using modern-day nlp tools..(a).
(b).
1.introduction.
language has been hypothesized to follow certaininformation-theoretic constraints.
one of the mostfamous of these constraints is the uniform infor-mation density (uid) hypothesis (fenk and fenk,1980; jaeger, 2010), which states that, subject tothe rules of the grammar, speakers aim to distributeinformation density across a linguistic signal asuniformly as possible.
that is, speakers behav-ing optimally should structure their utterances suchthat the differences between the peaks and troughsin information are minimized..in the psycholinguistics literature, the uid hy-pothesis has been used to explain a variety of lin-guistic phenomena ranging from how we shortenthe phonetic duration of more-predictable linguistic.
figure 1: graphical illustration of two examples regard-ing uid.
in (a), many speakers will prefer the versionwith the relativizer that (dotted blue line).
the uidhypothesis posits that this is because, without the rela-tivizer, the ﬁrst word of the relative clause, we, has highinformation density; and so including the relativizerdistributes the per-word information density more uni-formly.
in (b), the relativizer that is often omitted be-cause, at the onset of the relative clause, the informa-tion density of i is lower and therefore the distributionof information density is already relatively uniform.
il-lustration based on jaeger (2010)..units (aylett and turk, 2004) to when we decide touse optional syntactic relativizers (levy and jaeger,2007), among other phenomena (bell et al., 2003;frank and jaeger, 2008).
these studies often uselanguage models to estimate the information den-sity of linguistic units, taking observations of lowvariation of information density in well-formed ut-terances as evidence for the uid hypothesis..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5191–5202august1–6,2021.©2021associationforcomputationallinguistics5191in this paper, we propose a new experimentalparadigm that uses modern-day nlp models to testthe uid hypothesis.
whereas prior work has usedlanguage modeling as a tool for observing uid,1we explore the converse—can uid be used as atool to train better language models?
speciﬁcally,if the uid hypothesis is true, then we should beable to operationalize uid as a regularizer to helptrain language models.
moreover, observing lowerperplexity in language models trained with thisregularization would imply that the concept of uidis a good inductive bias for language modeling,thereby providing a new type of evidence for theuid hypothesis at scale..in experiments, we indeed ﬁnd such evidence:across a variety of languages and dataset sizes,uid regularization consistently improves perfor-mance, having a larger effect when training datais limited.
moreover, we observe that—in compar-ison with their unregularized counterparts—uid-regularized language models are (1) higher entropywhile achieving the same (or better) test set perplex-ity and (2) generate text that is longer and morelexically diverse.
our work is the ﬁrst to explorethe interaction between uid and training modern-day neural language models, and our ﬁndings—thata cognitively motivated objective can improve lan-guage model performance—open up new avenuesfor testing other psycholinguistic hypotheses in asimilar framework..2 preliminaries: language modeling.
the task of language modeling aims to estimate amodel of the probability of observing any givenstring in (a subset of) natural language.
for-mally, a language model p is an (unconditional)probability distribution over sequences of wordsw = (cid:104)w1, w2, .
.
.
(cid:105), where w consists of tokensfrom some vocabulary and begins and ends withspecial tokens bos and eos, respectively..today’s language models are typically param-eterized by neural networks (e.g., transformers(vaswani et al., 2017)),that follow a local-normalization scheme.
speciﬁcally, the model pro-vides a conditional distribution over the vocabularyat each time step; we can then compute the proba-.
1on its own, the term ‘uid’ is formally an attribute of alinguistic signal.
we also use it throughout this work to referto the concept that uid is a desirable property..bility of an entire sequence w as:.
pθ(w) =.
pθ(wt | w<t).
(1).
|w|(cid:89).
t=1.
where θ are the parameters of the model and weuse w<t to represent the ﬁrst t − 1 tokens of w.parameters are estimated by optimizing over someobjective l(θ).
the standard objective for lan-guage modeling is the negative log-likelihood of adataset w under the model:.
l(θ) = −.
log pθ(w).
(2).
(cid:88).
w∈w.
subsequently, we drop explicit dependence on θwhen it is obvious from context..to assess the goodness of ﬁt of a model p, wetypically evaluate its perplexity on some held-outdataset wtest, where perplexity (ppl) is deﬁned as.
ppl(p) = exp.
−.
log p(w).
(3).
(cid:32).
(cid:88).
1|w|.
w∈wtest.
(cid:33).
note that under this deﬁnition of perplexity, ourevaluation metric is slightly different than the train-ing objective; the former computes an averageover each sequence while the later treats all tokensequally, regardless of the length of the sequence inwhich they are present..3 uniform information density.
communication via natural language is a compli-cated and nuanced process that takes place undera host of cognitive and environmental constraints.
as a result, speakers have to make (perhaps subcon-scious) choices to best navigate this communicativedance.
a rational speaker would use these choicesto optimize the communicative properties of theirutterances.
one such locus of optimization is out-lined by the uniform information density (uid)hypothesis..3.1 the uid hypothesis.
at its core, the uid hypothesis aims to explaincertain phenomena in human language processingusing an information-theoretic approach: we canview language as a transfer of information, whichis transmitted with a certain density through a com-munication channel.
the uid hypothesis positsthat speakers that behave optimally will structure.
5192their utterances to avoid peaks and troughs in thisinformation density (aylett and turk, 2004; levyand jaeger, 2007; jaeger, 2010).
more formallystated: “within the bounds deﬁned by grammar,speakers prefer utterances that distribute informa-tion uniformly across the signal (information den-sity).
where speakers have a choice between sev-eral variants to encode their message, they preferthe variant with more-uniform information density(ceteris paribus)” (jaeger, 2010)..3.2 example: uid in syntactic reduction.
to better understand the uid hypothesis, considerthe concrete example of syntactic reduction (that-mentioning) from jaeger (2010), which we showgraphically in figure 1 and also describe below..ex.
a. my boss conﬁrmed [that] we are crazy..ex.
b. my boss thinks [that] i am crazy..in both these sentences, the use of the relativizerthat is syntactically optional—at the onset of a rel-ative clause (rc), speakers can, but do not haveto, include the relativizer.
many speakers, how-ever, would argue that the sentence ﬂows betterwith the relativizer included in example a and therelativizer omitted in example b..the uid hypothesis provides a potential expla-nation for this phenomenon.
when a rc is usedwithout a relativizer, the ﬁrst word of the rc con-veys two pieces of information: both the onset ofthe rc, as well as part of the rc’s internal con-tents.
in example a, many speakers would ﬁndthat the information density of the ﬁrst word in therc, we, is high, and so adding in the relative clausedistributes the information over two words, makingit easier to parse.
in example b, the informationdensity of the ﬁrst word in the rc, i, is lower rel-atively, and so we do not need to (or it is not asbeneﬁcial to) include the relativizer..3.3 measuring uid.
now that we better understand what the uid hy-pothesis attempts to explain, how might we opera-tionalize uid and ﬁnd quantitative evidence of thepressure for it in language?
first, to quantify theamount of information conveyed by a word, we turnto the most basic information-theoretic deﬁnition:the information conveyed by a word w in context isits shannon information content (shannon, 1948),also called surprisal.
ideally, this surprisal wouldbe measured using the “true” distribution over hu-man language.
because we do not have access to.
such a distribution, we often estimate it using a sta-tistical language model.
that is, given a statisticallanguage model p, which estimates the probabilityof a word given its context, the surprisal u(wt) ofword wt is deﬁned as the following:.
u(wt) = − log p(wt | w<t).
(4).
this setup provides a natural approach to exploringhow uid might manifest—if the uid hypothesisis true, then we should observe that variation insurprisal, as estimated by a language model, isminimized in natural language..using this approach, prior work has accumulatedevidence for uid across various levels of linguisticrepresentation (pluymaekers et al., 2005; bell et al.,2009, inter alia).
as some of the earliest exam-ples, aylett and turk (2004) showed that linguisticunits that had high surprisal according to a tri-gramlanguage model were uttered with longer syllabledurations, and levy and jaeger (2007) found thatfor rcs in which the ﬁrst word had higher surprisal,relativizers were more likely to be used in the rcduring actual speech.
further examples are givenin our related work section (§7)..4 uid-regularized language modeling.
while prior work has shown evidence that uid canhelp explain many of the choices we make whengenerating language, to the best of our knowledge,operationalizations of uid have not been explic-itly employed as part of the training objective inmodern-day nlp models.
this raises the simplequestion that is central to our paper:.
can uid serve as an inductivebias for training statistical lan-guage models?.
in an effort to answer this question, we presenta scheme for incorporating operationalizations ofuid into the language model training objective.
formally, we augment the canonical maximum like-lihood estimation objective2 in eq.
(2) with uid.
2note that the maximum likelihood estimation objectiveminimizes (over w ∈ w) − log p(wt | w<t), i.e., surprisal.
although such an objective may indirectly minimize peaksand dips in surprisal across a sequence simply by pushingthem towards 0, it does not explicitly include any sequencelevel penalty for even surprisal distribution..5193operationalizations as regularizers r. under thisnew objective, we minimize.
5 experimental setup.
lr(θ) = l(θ) + β · r(θ).
(5).
where β > 0 is the strength coefﬁcient of the regu-larizer.
we consider two natural operationalizationsof uid—inspired by collins (2014)—as regulariz-ers for training language models:.
variance regularizer.
uid concerns the distri-bution of information in language production, andso a natural measure of this behavior is the varianceof surprisals.
thus, we ﬁrst consider a regularizerthat penalizes high variance among the surprisalsof words in a given sequence:.
r(θ) =.
(u(wt) − µ)2.
(6).
1|w|.
|w|(cid:88).
t=1.
(cid:80)|w|.
where µ = 1t=1 u(wt).
note that here, and in|w|our subsequent regularizers, we estimate u(·) viaeq.
(4) using our model pθ..local consistency.
next, we consider a localconsistency regularizer that encourages the sur-prisals of adjacent words to have similar magnitude:.
r(θ) =.
1|w|−1.
|w|−1(cid:88).
(cid:16).
t=1.
u(wt) − u(wt+1).
(7).
(cid:17)2.this regularizer is also a reasonable operational-ization of uid—if every surprisal is similar to itsneighbor, then the density of information in thesequence will be close to uniform..though we focus on these two regularizers, otheroperationalizations of uid certainly exist.
for ex-ample, a similar variant of the above regularizers isthe max regularizer (meister et al., 2020a), whichpenalizes the highest surprisal in a sentence.3 fur-thermore, uid may also be deﬁned in terms ofparse steps (hale, 2001) or structural integrations(gibson, 2000), as well as in spoken language inthe form of ﬁller words like uh and um or wordrepetition during challenging lexical retrieval.
weconsider these operationalizations (as well as thebroader discussion of how to operationalize uid)as future work..3we also tried this operationalization in preliminary exper-iments, but results were not as strong as the variance or localconsistency regularizers..to empirically evaluate uid regularization, wetrain various language models with the uid-regularized objective (eq.
(5)) using the followingexperimental setup..datasets.
we employ datasets from multiple lan-guages and of varying sizes.
we use the europarlcorpus (koehn, 2005)—a multi-lingual dataset ofdiscussions from the european parliament that hasbeen commonly used for language modeling (cot-terell et al., 2018; mielke et al., 2019)—since itis roughly semantically controlled in that all utter-ances are presumably about the same topics.
weuse europarl v7 download from the acl 2014smt workshop4 and perform a 80–10–10 train-dev-test split on all ﬁve languages—czech, en-glish, french, german, and spanish—which yields46.7, 42.2, 47.2, 51.3, and 12.4 million trainingtokens for each language respectively..moreover, we experiment on languages fromseveral language families; the ﬁve languages ineuroparl that we consider are all indo-european,and so we look to wiki-40b (guo et al., 2020),which contains wikipedia dumps of a wide rangeof languages.
we choose a set of diverse languageswith training set sizes relatively similar to that ofeuroparl: finnish (a uralic language; 59.3m train-ing tokens), indonesian (an austronesian language;45.7m training tokens), and turkish (a turkic lan-guage; 38.1m training tokens).
to explore per-formance on lower-resource languages, we addi-tionally experiment with swahili5 (a niger-congolanguage; 6.3m training tokens) and tagalog (anaustronesian language; 4.2m training tokens).
forall languages, we performed tokenization using themosestokenizer.6 train, dev, and test set splits areshown in table 5 in the appendix..model framework and architecture.
for ourexperiments, we use the fairseq library (ottet al., 2019), a standard sequence modeling toolkitin pytorch.
as our model, we use fairseq’s de-fault transformer (with six decoder layers and eight.
4http://statmt.org/wmt14/.
translation-task.html.
5since there are no niger-congo languages in wiki-40b,we perform a 80-10-10 split on swahili wikidumps (seehttps://github.com/google-research/bert/blob/master/multilingual.md).
6https://pypi.org/project/.
mosestokenizer/.
5194attention heads), which achieves competitive7 lan-guage modeling performance (although the purposeof our paper is not to achieve or compare with thestate of the art).
for all experiments, we followedthe data-preprocessing scripts and recommendedhyperparameters provided in fairseq’s languagemodeling module; more detailed information canbe found on the github page.8.
uid regularizers.
for uid regularization, weexperiment with the variance (eq.
(6)) and localconsistency regularizers (eq.
(7)).
we found in pre-liminary experiments that effective regularizationstrengths were often near β = 0.01, and so weperformed a grid search over values within an or-der of magnitude around β = 0.01: β ∈ {0.006,0.008, 0.01, 0.02, 0.03, 0.04, 0.05}.
we choosethe model with the lowest dev loss to evaluate onthe test set..6 results.
in this section, we report results for models trainedunder the uid-regularized objective.
we ﬁndthat uid regularization consistently improvesperplexity for models trained on various languages(§6.1) and dataset sizes (§6.2).
additionally,we examine properties of text generated byuid-regularized models (§6.3) and analyze therelationship between our operationalization of uidand perplexity (§6.4)..6.1 languages.
table 1 shows the results of uid-regularized lan-guage models trained on various languages fromeuroparl and wiki-40b, and includes statisticalsigniﬁcance of changes in perplexity, as comparedwith baselines, computed using permutation tests9(efron and tibshirani, 1994).
for all languages,uid regularization signiﬁcantly improves perplex-ity for at least one of the two regularizers.
further-.
language (# train tokens).
perplexity.
czech (12.4m)baseline (no uid)+ uid: variance+ uid: local consistency.
english (46.7m)baseline (no uid)+ uid: variance+ uid: local consistency.
finnish (59.3m)baseline (no uid)+ uid: variance+ uid: local consistency.
french (51.3m)baseline (no uid)+ uid: variance+ uid: local consistency.
german (42.3m)baseline (no uid)+ uid: variance+ uid: local consistency.
indonesian (45.7m)baseline (no uid)+ uid: variance+ uid: local consistency.
spanish (47.2m)baseline (no uid)+ uid: variance+ uid: local consistency.
swahili (6.3m)baseline (no uid)+ uid: variance+ uid: local consistency.
tagalog (4.2m)baseline (no uid)+ uid: variance+ uid: local consistency.
turkish (38.1m)baseline (no uid)+ uid: variance+ uid: local consistency.
47.4747.24 (↓0.5%)47.08 (↓0.8%)†.
21.3421.08 (↓1.2%)†21.19 (↓0.7%)†.
51.5851.30 (↓0.5%)†51.49 (↓0.2%).
17.0817.02 (↓0.4%)†17.03 (↓0.3%)†.
26.6226.50 (↓0.4%)†26.45 (↓0.6%)†.
53.9653.66 (↓0.6%)†53.70 (↓0.5%).
22.5422.37 (↓0.8%)†22.44 (↓0.4%)†.
40.4539.79 (↓1.6%)†39.44 (↓2.5%)†.
80.4878.40 (↓2.5%)†78.12 (↓2.9%)†.
66.1365.70 (↓0.7%)†66.06 (↓0.1%).
table 1: uid regularizers improve perplexity for mul-tiple languages.
† indicates statistical signiﬁcance com-pared with the baseline (p < 0.05)..7on wikitext-103, the largest dataset we train on (103million tokens), we achieve a competitive perplexity of 29.89(c.f.
merity et al.
(2018)).
for smaller datasets, we tried asmaller transformer architecture of four decoder layers andfour attention heads, but it did not perform better than the sixdecoder layer and eight attention heads version, suggestingthat this architecture was not too large for the datasets we usein this paper (even the tagalog dataset we use is larger thanthe commonly used penn treebank and wikitext-2)..8https://github.com/pytorch/fairseq/.
tree/master/examples/language_model.
9http://www2.stat.duke.edu/~ar182/rr/.
examples-gallery/permutationtest.html.
more, uid regularization (under the best perform-ing β) never leads to worse perplexity.
these re-sults suggest that incorporating uid operational-izations into a model’s training objective leads toa better model of language, substantiating uniforminformation density as a valid inductive bias.
more-over, the improvement for many languages corrob-orates the expectation that uid should, due to itsinformation theoretic nature, hold across languages(jaeger and tily, 2011)..5195wmt’06 europarl wt-103.
# training tokens.
16.0m.
47.0m 103.2m.
49.70baseline (no uid)48.25†+ uid: variance+ uid: local consistency 48.79.
21.3421.08†21.19.
29.8929.5829.73.table 2: uid regularizers improve perplexity on lan-guage models trained on english datasets of vary-ing size.
improvements tend to be larger on smallerdatasets.
† indicates statistical signiﬁcance comparedwith the baseline (p < 0.05)..6.2 dataset size.
notably, we observe the largest improvements(1.6–2.9%) in perplexity in table 1 for the low-est resource languages, tagalog and swahili (with4.2 and 6.3 million training tokens respectively).
conversely, improvement was most marginal (0.2–0.5%) on the highest-resource languages, frenchand finnish (51.3 and 59.3 million training tokensrespectively).
to remove language as a confound-ing factor from this observation, we perform a con-trolled analysis of the effects of uid regularizationas a function of dataset size..we focus on english; in addition to the result onenglish europarl 2014 from table 1, which con-tains 47.0 million training tokens, we experimentwith the smaller monolingual english dataset fromthe 2006 naacl workshop on statistical machinetranslation (wmt’06),10 which has 17.0m tokensin its training set, as well as the larger wikitext-103benchmark (merity et al., 2017), which contains103 million tokens in its training set..table 2 shows the perplexities for models withand without uid regulariztion for these threedatasets.
as suggested by earlier results, improve-ments were strongest for the wmt’06 dataset, withan improvement of 1.4 perplexity points for thevariance regularizer and 0.9 ppl points for localconsistency.
for the larger europarl and wt-103datasets, on the other hand, improvement was moremodest, ranging from 0.1 to 0.3 perplexity points.
as further conﬁrmation that uid regularizationhas a greater impact on smaller datasets, we per-form an ablation study that roughly controls forlanguage content by training models on the subsetsof the same dataset.
for this ablation, we take sub-sets of 2, 4, 8, 12, 16, 24, and 32 million sentencesfrom the 47 million sentences in english europarl,.
80.
60.
40.
20.
0.
2.5.
1.5.
0.5.
2.
1.
0.enilesab.ytixelprep.ni.tnemevorpm.i.ytixelprep.uid: varianceuid: local consistency.
8.
16.
22447training tokens (millions).
32.figure 2: improvement in perplexity for uid regular-ized models trained on subsets of varying size sampledfrom the europarl english dataset (full dataset size 47.0million tokens).
uid regularization helped more whentraining data was more limited..and observe how much the uid regularizers im-prove perplexity for each training dataset size.
asshown in figure 2, the results tell the same story astable 2—uid regularization improves perplexitymore for smaller datasets..these results are consistent with the expectationthat models trained on smaller datasets are morelikely to overﬁt and could therefore beneﬁt morefrom regularization (melis et al., 2018).
as it ispossible that the models trained on smaller datasetscould beneﬁt from any kind of regularization,we experiment with label smoothing (szegedyet al., 2016), another regularization techniquethat similarly augments the training objectivewith a penalty.
table 4 shows these results formodels trained on wmt’06 and europarl withlabel smoothing—our experiments indicate that,across the board, label smoothing leads to worseperplexity compared with baseline models.11we take this result as further evidence that theimprovement from uid regularization stems fromthe uid hypothesis as a valid inductive bias, ratherthan simply a need for any kind of regularizationwhen training on smaller datasets..10we downloaded the given train-dev-test splits from.
https://www.statmt.org/wmt06/..11this negative result for applying label smoothing to lan-guage modeling is consistent with prior empirical ﬁndings(müller et al., 2019; gao et al., 2020; meister et al., 2020b)..5196sequence modelentropy.
length.
% unique n-gramsn = 3.n = 4.n = 2.baseline (no uid)+ uid: variance+ uid: local consistency.
22.924.023.3.
69.679.473.9.
37.740.739.1.
73.577.875.7.
90.993.392.1.table 3: text generated by uid-regularized language models is longer (higher average sequence length), higherentropy (computed via monte-carlo estimation), and more lexically diverse (a higher ratio of unique n-grams)..wmt’06.
europarl.
# training tokens.
16.0m.
47.0m.
baseline+ label smoothing, α = 0.01+ label smoothing, α = 0.05+ label smoothing, α = 0.1.
35.7536.1555.5690.57.
23.2226.2640.7968.26.table 4: label smoothing, another form of regulariza-tion that similarly augments the cross-entropy objectivewith a penalty, does not improve perplexity.
(resultsshown on dev set)..6.3 evaluating generated text.
unconditional models of language have been ob-served to produce generic text that can be short,bland, or repetitive (fan et al., 2018; kulikovet al., 2019; holtzman et al., 2020), and so in thissubsection we investigate how uid regularizationmight affect these characteristics in generated text.
for these experiments, we consider the baselinemodel, the variance-regularized model, and the lo-cal consistency-regularized model trained on en-glish europarl.
to obtain text samples, we generatesamples by sequentially sampling tokens accordingto the model’s predicted distribution until the end-of-sequence (eos) token is sampled, i.e., ancestralsampling.
note that for language model p, thissampling scheme is equivalent to directly samplingy ∼ p. we obtain 10,000 samples for each modeland report statistics in table 3..we analyze each set of generated sentences forseveral metrics.
first, we compute the averagelength of generated sentences.
next, we evaluatethe lexical diversity of generated texts by comput-ing the percent of unique n-grams for n ∈ {2, 3, 4}.
finally, sampling from a model also gives us ameans for estimating the language model’s entropy:.
h(p) = −.
p(y) log p(y).
(cid:88).
y∈supp(p)= −ey∼p (log p(y)).
(8).
(9).
vocabulary v. as this is exponentially large in |v|,directly computing h(p) is intractable.
we can useits equivalence to eq.
(9), however, to estimate h(p)with a simple monte-carlo estimator:.
ˆh(p) = −.
log p(y(k)).
(10).
1k.k(cid:88).
k=1.
where we sample y(k) ∼ p for k = 1, .
.
.
, k..table 3 shows results from uid-regularizedmodels compared with the baseline.
the modelstrained with the variance and local consistency reg-ularizers exhibit a preference for longer sequencelength and higher lexical diversity.
additionally,the entropy estimates of these models are notablyhigher, which, following the principle of maximumentropy (jaynes, 1957),12 can be seen as an addi-tional advantage of uid-regularized models overtheir unregularized counterparts..6.4 uid behavior.
to take a closer look at how uid regularizationaffects language models, we examine the relation-ship between minimizing perplexity and uid be-havior, where we quantify uid behavior as thevariance of models’ surprisals.
we consider mod-els trained on the english europarl dataset with thevariance regularizer at strengths β ∈ {0.01, 0.03,0.05, 0.07, 0.09} and our baseline (which is equiv-alent to β = 0), for further comparison, we alsotrain a model with β = −0.01 to observe the ef-fects of penalizing uid behavior.
we report resultson the europarl test set in figure 3..we observe that the model trained with a uidpenalty (negative β) indeed exhibits worse perplex-ity and uid behavior (variance of surprisals) on thetest set.
and as we might expect, models trainedwith higher β exhibit uid behavior more strongly,as our quantiﬁcation is part of their training objec-tive.
overall, from β = 0.01 to β = 0.05, both.
in the case of language models, supp(p) is the setof all strings that can be generated from the model’s.
12the principle of maximum entropy states that the proba-bility distribution that best represents the current knowledgestate is the one with the largest entropy..5197roivahebdu.i.)
slasirprus.foecnairav(.
18.5.
18.
17.5.
17.
16.5.
16.
15.5.
15.β = −0.01.
β = 0 (baseline).
β = 0.01.β = 0.03.β = 0.05.β = 0.07.β = 0.09.
21.
21.2.
21.8.
22.
21.421.6perplexity.
figure 3: a trade-off between perplexity (x-axis) andvariance of surprisals (a measure of uid behavior; y-axis).
the black pentagon indicates the β that yieldedthe best perplexity (β = 0.03)..perplexity and uid behavior are positively corre-lated with β, but when we optimize too much foruid (β ≥ 0.07), there is a trade-off in which modelperplexity begins to increase..we also observe an intriguing phenomenon infigure 3. models that achieve similar perplexitycan have substantially different uid behavior val-ues on the test set.
speciﬁcally, the β = 0 andβ = 0.07 models, which have almost the sameperplexity, have variance of surprisals of 17.8 and15.8—a difference of more than ten percent!
if suchmodels with similar perplexity can have varyingdeﬁnitions of what constitutes good uid behav-ior, then prior work, which has drawn conclusionson uid based on surprisals computed by a singlemodel (aylett and turk, 2004; levy and jaeger,2007; jain et al., 2018), may need revisiting.
asthis direction is outside the scope of the presentpaper, we leave it as future work..7 discussion and related work.
we discussed how operationalizing uid for lan-guage modeling leads to better models in a widevariety of settings.
these results both provide anew form of evidence for the uid hypothesis andbuild on prior work exploring uid in modern-daynlp models..guage model, is minimized in natural language.
inaddition to early studies that used this approach toﬁnd evidence for uid in syntactic reduction (levyand jaeger, 2007), morphosyntactic contractions(frank and jaeger, 2008), and prosodic structure(aylett and turk, 2004), the same line of reasoninghas been used by more recent work exploring avariety of other linguistic properties.
these studieshave found that word duration can be predicted bysyntactic surprisal (demberg et al., 2012; moore-cantwell, 2013), construction probability (kuper-man and bresnan, 2012), informativity (seyfarth,2014), and contextual predictability (jurafsky et al.,2001; bell et al., 2003; gahl and garnsey, 2004).
they have also observed that word length is re-ﬂected by conceptual complexity (lewis and frank,2016); word order choice can be predicted by pro-cessing cost (bloem, 2016; sikos et al., 2017);phonological patterns can be shaped by word pre-dictability (hall et al., 2018); and uid computedat the sequence level predicts human preferencesfor syntactic alternatives of the same sentence..whereas the above prior work has used languagemodeling as a tool for measuring uid, our paperhas explored the exact converse—we have askedwhether uid, operationalized as a regularizer, canbe used as a tool for training better language mod-els.
we argue that if the uid hypothesis holdsas a general principle, then we should be able toexploit it as a training criterion that improves lan-guage modeling.
and accordingly, our results showthat—across a variety of languages and datasetsizes—regularization for uid did indeed improveperplexity, which we view as an alternative kind ofevidence for the uid hypothesis at scale..notably, figure 3 at ﬁrst could appear to contra-dict the uid hypothesis, since models with betteruid behavior did not always achieve better perplex-ity.
we do not consider this as evidence againstthe uid hypothesis, however.
rather, we positthat when β is too large, we may be optimizingfor uid to the point of tending towards unnatu-ral language—a perfectly uniform dispersion ofinformation across an utterance may come at thecost of strange lexical choices.
in this light, such atrade-off should be somewhat expected..evidence for the uid hypothesis.
our work ex-tends the body of psycholinguistic research on uni-form information density, which has largely corrob-orated the uid hypothesis by providing evidencethat variation in surprisal, as estimated by a lan-.
uid in modern nlp.
in addition to the tradi-tional line of psycholinguistic work, there havealso been more-recent studies on uid in the con-text of modern nlp, although this work is rela-tively sparse.
rubino et al.
(2016) leverage infor-.
5198mation density encoded as surprisal at the word,part of speech, and syntax levels to help build astate-of-the-art model for mixed-domain transla-tionese detection.
jain et al.
(2018) incorporateuid measures across sentences into models de-signed to detect natural versus manipulated text.
perhaps the work that is most related to ours, meis-ter et al.
(2020a), leverages uid to explain whybeam search is an effective decoding algorithmand uses operationalizations of uid during beamsearch to alleviate problems with decoding poorlycalibrated machine translation models.
whereasmeister et al.
(2020a) focuses on decoding, ourwork shows the ﬁrst evidence that uid can be op-erationalized to aid training..8 conclusions.
in closing, we have proposed encoding uniforminformation density as a regularizer for training lan-guage models—a novel manner of incorporatingan established psycholinguistic theory into modernstatistical language modeling.
in experiments ona range of languages and dataset sizes, uid reg-ularization consistently improves perplexity overbaselines.
our results suggest that uid is a validinductive bias for improving the canonical maxi-mum likelihood objective in language modeling,providing a new, alternative type of evidence thatsupports the uid hypothesis at scale.
our workopens the door to future research directions suchas using similar techniques to validate other psy-cholinguistic phenomena, applying uid regulariza-tion in conditional language generation tasks, andexploring how uid regularized models perform indownstream nlp applications..ethical concerns.
language models have various ethical, environmen-tal, and ﬁnancial concerns.
we cannot do justiceto them here, but do see bender et al.
(2021) for apointer.
we do not foresee any additional ethicalconcerns with the contributions made in our workbeyond those discussed in bender et al.
(2021)..acknowledgements.
we thank roger levy for feedback in the middlestages of our work and tiago pimentel, david re-itter, tal linzen, and slav petrov for feedback onthe manuscript..references.
matthew aylett and alice turk.
2004. the smoothsignal redundancy hypothesis: a functional ex-planation forrelationships between redundancy,prosodic prominence, and duration in spontaneousspeech.
language and speech, 47(1):31–56.
pmid:15298329..alan bell, jason m. brenier, michelle gregory, cyn-thia girand, and dan jurafsky.
2009. predictabilityeffects on durations of content and function wordsin conversational english.
journal of memory andlanguage, 60(1):92–111..alan bell, daniel jurafsky, eric fosler-lussier, cyn-thia girand, michelle gregory, and daniel gildea.
2003. effects of disﬂuencies, predictability, and ut-terance position on word form variation in englishconversation.
the journal of the acoustical societyof america, 113(2):1001–1024..emily m. bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big?
in proceedings of the 2021 acm confer-ence on fairness, accountability, and transparency,facct ’21, page 610–623, new york, ny, usa.
as-sociation for computing machinery..jelke bloem.
2016. testing the processing hypoth-esis of word order variation using a probabilisticin proceedings of the workshoplanguage model.
on computational linguistics for linguistic com-plexity (cl4lc), pages 174–185, osaka, japan.
thecoling 2016 organizing committee..michael xavier collins.
2014..information densityand dependency length as complementary cogni-tive models.
journal of psycholinguistic research,43(5):651–681..ryan cotterell, sabrina j. mielke, jason eisner, andbrian roark.
2018. are all languages equally hardin proceedings of the 2018to language-model?
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 536–541, new orleans, louisiana.
associa-tion for computational linguistics..vera demberg, asad sayeed, philip gorinski, andnikolaos engonopoulos.
2012. syntactic surprisalaffects spoken word duration in conversational con-texts.
in proceedings of the 2012 joint conferenceon empirical methods in natural language process-ing and computational natural language learning,pages 356–367, jeju island, korea.
association forcomputational linguistics..bradley efron and robert j. tibshirani.
1994. an in-.
troduction to the bootstrap.
crc press..angela fan, mike lewis, and yann dauphin.
2018. hi-in proceedingserarchical neural story generation.
of the 56th annual meeting of the association for.
5199computational linguistics (volume 1: long papers),pages 889–898, melbourne, australia.
associationfor computational linguistics..august fenk and gertraud fenk.
1980. konstanzim kurzzeitgedächtnis-konstanz im sprachlichen in-formationsﬂuß.
zeitschrift für experimentelle undangewandte psychologie, 27:400–414..austin f. frank and t. florian jaeger.
2008. speakingrationally: uniform information density as an opti-in proceed-mal strategy for language production.
ings of the annual meeting of the cognitive sciencesociety, volume 30..susanne gahl and susan m. garnsey.
2004. knowl-edge of grammar, knowledge of usage: syntacticprobabilities affect pronunciation variation.
lan-guage, pages 748–775..yingbo gao, weiyue wang, christian herold, zijianyang, and hermann ney.
2020. towards a bet-ter understanding of label smoothing in neural ma-in proceedings of the 1st con-chine translation.
ference of the asia-paciﬁc chapter of the associa-tion for computational linguistics and the 10th in-ternational joint conference on natural languageprocessing, pages 212–223, suzhou, china.
associ-ation for computational linguistics..edward gibson.
2000. the dependency locality the-ory: a distance-based theory of linguistic complex-image, language, brain: papers from the ﬁrstity.
mind articulation project symposium, 2000:95–126..mandy guo, zihang dai, denny vrandeˇci´c, and ramial-rfou.
2020. wiki-40b: multilingual languagein proceedings of the 12th lan-model dataset.
guage resources and evaluation conference, pages2440–2452, marseille, france.
european languageresources association..john hale.
2001. a probabilistic earley parser as a psy-cholinguistic model.
in second meeting of the northamerican chapter of the association for computa-tional linguistics..kathleen currie hall, elizabeth hume, t. florianjaeger, and andrew wedel.
2018. the role of pre-dictability in shaping phonological patterns.
lin-guistics vanguard, 4(s2)..ari holtzman, jan buys, maxwell forbes, and yejinchoi.
2020. the curious case of neural text degen-eration.
in proceedings of the international confer-ence on learning representations..t. florian jaeger.
2010..redundancy and reduc-tion: speakers manage syntactic information den-sity.
cognitive psychology, 61(1)..t. florian jaeger and harry tily.
2011. on language‘utility’: processing complexity and communicativeefﬁciency.
wiley interdisciplinary reviews: cogni-tive science, 2..ayush jain, vishal singh, sidharth ranjan, rajakrish-nan rajkumar, and sumeet agarwal.
2018. uniforminformation density effects on syntactic choice inhindi.
in proceedings of the workshop on linguis-tic complexity and natural language processing,pages 38–48, santa fe, new-mexico.
associationfor computational linguistics..edwin t. jaynes.
1957. information theory and statis-.
tical mechanics.
physical review, 106(4):620..daniel jurafsky, alan bell, michelle gregory, andwilliam d. raymond.
2001. probabilistic relationsbetween words: evidence from reduction in lexi-cal production.
typological studies in language,45:229–254..philipp koehn.
2005. europarl: a parallel corpus forstatistical machine translation.
in mt summit, pages79–86..ilia kulikov, alexander miller, kyunghyun cho, andjason weston.
2019. importance of search and eval-uation strategies in neural dialogue modeling.
inproceedings of the 12th international conference onnatural language generation, pages 76–87, tokyo,japan.
association for computational linguistics..victor kuperman and joan bresnan.
2012. the effectsof construction probability on word durations duringspontaneous incremental sentence production.
jour-nal of memory and language, 66(4):588–611..roger p. levy and t. f. jaeger.
2007. speakers op-timize information density through syntactic reduc-tion.
in advances in neural information processingsystems..molly l. lewis and michael c. frank.
2016. thelength of words reﬂects their conceptual complexity.
cognition, 153:182–195..clara meister, ryan cotterell, and tim vieira.
2020a.
if beam search is the answer, what was the question?
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2173–2185, online.
association for computa-tional linguistics..clara meister, elizabeth salesky, and ryan cotterell.
generalized entropy regularization or:2020b.
inthere’s nothing special about label smoothing.
proceedings of the 58th annual meeting of the asso-ciation for computational linguistics, online.
asso-ciation for computational linguistics..gábor melis, chris dyer, and phil blunsom.
2018. onthe state of the art of evaluation in neural languagemodels.
in proceedings of the international confer-ence on learning representations..stephen merity, nitish shirish keskar, and richardsocher.
2018. an analysis of neural language mod-eling at multiple scales.
corr, abs/1803.08240..5200ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems..stephen merity, caiming xiong, james bradbury, andrichard socher.
2017. pointer sentinel mixture mod-els.
in proceedings of the international conferenceon learning representations..sabrina j. mielke, ryan cotterell, kyle gorman, brianroark, and jason eisner.
2019. what kind of lan-guage is hard to language-model?
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4975–4989, florence,italy.
association for computational linguistics..claire moore-cantwell.
2013. syntactic predictabilityinﬂuences duration.
in proceedings of meetings onacoustics.
acoustical society of america..rafael müller, simon kornblith, and geoffrey e. hin-ton.
2019. when does label smoothing help?
in ad-vances in neural information processing systems..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..mark pluymaekers, mirjam ernestus, and r. haraldbaayen.
2005. lexical frequency and acoustic re-duction in spoken dutch.
the journal of the acous-tical society of america, 118(4):2561–2569..raphael rubino, ekaterina lapshinova-koltunski, andjosef van genabith.
2016. information density andquality estimation features as translationese indica-in pro-tors for human translation classiﬁcation.
ceedings of the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages960–970, san diego, california.
association forcomputational linguistics..scott seyfarth.
2014. word informativity inﬂuencesacoustic duration: effects of contextual predictabil-ity on lexical representation.
cognition, 133(1):140–155..claude e. shannon.
1948. a mathematical theory ofcommunication.
the bell system technical journal,27(3):379–423..les sikos, clayton greenberg, heiner drenhaus, andmatthew w. crocker.
2017. information density ofencodings: the role of syntactic variation in compre-hension.
in proceedings of the 39th annual meetingof the cognitive science society..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computervision and pattern recognition, pages 2818–2826..5201a appendix.
datasets.
table 5 shows the train, dev, and test set splits for the language modeling datasets we use..language.
family.
source.
split.
size sentences tokens sentences tokens sentences tokens.
vocab.
train.
dev.
test.
english.
indo-european europarlwmt’06wt-103indo-european europarlindo-european europarlindo-european europarlindo-european europarluralic.
80–10–1080–10–10provided80–10–10czech80–10–10french80–10–10german80–10–10spanishfinnishwiki-40b providedindonesian austronesian wiki-40b providedaustronesian wiki-40b providedtagalogturkicturkishwiki-40b providedniger-congo wikipedia 80–10–10swahili.
64k62k268k64k64k64k64k128k128k128k128k128k.
1.6m 46.7m751k17.0m1.8m 103.2m517k12.4m1.6m 51.3m1.5m 42.3m1.6m 47.2m59.3m256k45.7m156k4.2m26k38.1m143k6.3m406k.
201k2.0k3.8k65k201k192k197k14.1k8.7k1.5k7.8k51k.
5.8m61k217k1.6m6.4m5.4m6.0m3.9m3.1m270k2.5m800k.
201k3.1k4.4k65k201k192k197k14.0k8.6k1.4k7.7k51k.
5.8m90k246k1.6m6.3m5.2m5.9m3.2m2.5m220k1.9m803k.
table 5: train, dev, and test splits, as well as vocab size, for the language modeling datasets that we use in this paper.
if train-dev-test splits were provided, then we used them.
otherwise, we performed a 80–10–10 train-dev-test split.
we found a vocab size of 64k to cover more than 98% of the training set for the indo-european languages, and avocab size of 62k allowed us to cover 100% in the training set of english wmt’06.
for the remaining languages,which had larger vocabularies, we followed wiki-40b (guo et al., 2020) and increased the vocab size to 128k..hyperparameters.
table 6 shows the optimized β hyperparameter from a grid-search over β ∈ {0.006,0.008, 0.01, 0.02, 0.03, 0.04, 0.05} for both regularizers on all datasets we use.
notably, the best β forvariance ranged from 1×10−2 to 5×10−2, and the best β for local consistency ranged from 6×10−3 to2×10−2.
for use on a new dataset, we recommend starting with 1×10−2, which we found almost alwaysimproved perplexity for both regularizers (on these datasets, at least)..uid regularizer.
variance.
best β.dev loss.
local consistencybest β.dev loss.
language.
source.
english.
europarl (full dataset)europarl (2m subset)europarl (4m subset)europarl (8m subset)europarl (12m subset)europarl (16m subset)europarl (24m subset)europarl (32m subset)wmt’06wt-103europarlczecheuroparlfrencheuroparlgermaneuroparlspanishfinnishwiki-40bindonesian wiki-40bwiki-40btagalogwiki-40bturkishwikipediaswahili.
2×10−22×10−22×10−22×10−22×10−25×10−24×10−21×10−23×10−21×10−23×10−21×10−22×10−23×10−21×10−23×10−24×10−23×10−22×10−2.
4.5196.4975.9405.5005.2365.0844.8414.7474.9744.9335.3884.1614.7824.5395.8115.8086.3196.1195.555.
8×10−31×10−21×10−28×10−38×10−32×10−22×10−21×10−21×10−28×10−31×10−26×10−38×10−31×10−26×10−38×10−38×10−38×10−36×10−3.
4.5296.4975.9485.5115.2305.0894.8434.7424.9914.9395.3914.1624.7794.5505.8195.8096.3196.1215.546.table 6: best β hyperparameters and dev losses for all experiments..5202