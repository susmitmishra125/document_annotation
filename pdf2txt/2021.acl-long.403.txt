learning event graph knowledge for abductive reasoning.
li du, xiao ding∗, ting liu, and bing qinresearch center for social computing and information retrievalharbin institute of technology, china{ldu, xding, tliu, qinb}@ir.hit.edu.cn.
abstract.
abductive reasoning aims atinferring themost plausible explanation for observed events,which would play critical roles in various nlpapplications, such as reading comprehensionand question answering.
to facilitate this task,a narrative text based abductive reasoning taskαnli is proposed, together with explorationsabout building reasoning framework using pre-trained language models.
however, abundantevent commonsense knowledge is not well ex-ploited for this task.
to ﬁll this gap, we pro-pose a variational autoencoder based modelege-roberta, which employs a latent variableto capture the necessary commonsense knowl-edge from event graph for guiding the abduc-tive reasoning task.
experimental results showthat through learning the external event graphknowledge, our approach outperforms the base-line methods on the αnli task..1.introduction.
abductive reasoning aims at seeking for the best ex-planations for incomplete observations (bhagavat-ula et al., 2019).
for example, given observationsforgot to close window when leaving home and theroom was in a mess, human beings can generatea reasonable hypothesis for explaining the obser-vations, such as a thief entered the room based oncommonsense knowledge in their mind.
however,due to the lack of commonsense knowledge andeffective reasoning mechanism, this is still a chal-lenging problem for today’s cognitive intelligentsystems (charniak and shimony, 1990; oh et al.,2013; kruengkrai et al., 2017)..most previous works focus on conducting ab-ductive reasoning based on formal logic (eshghiet al., 1988; levesque, 1989; ng et al., 1990; paul,1993).
however, the rigidity of formal logic lim-its the application of abductive reasoning in nlp.
∗corresponding author.
figure 1: (a) an example of abductive reasoning.
(b)additional commonsense knowledge (such as event i1and i2) is necessary for inferring the correct hypothe-sis.
such knowledge could be described using an eventgraph.
(c) a latent variable z is employed to learn thecommonsense knowledge from event graph..tasks, as it is hard to express the complex semanticsof natural language in a formal logic system.
tofacilitate this, bhagavatula et al.
(2019) proposeda natural language based abductive reasoning taskαnli.
as shown in figure 1 (a), given two ob-served events o1 and o2, the αnli task requiresthe prediction model to choose a more reasonableexplanation from two candidate hypothesis eventsh1 and h2.
both observed events and hypothe-sis events are daily-life events, and are describedin natural language.
together with the αnli task,bhagavatula et al.
(2019) also explored conductingsuch reasoning using pretrained language modelssuch as bert (devlin et al., 2019) and roberta(liu et al., 2019)..however, despite pretrained language modelscould capture rich linguistic knowledge beneﬁt forunderstanding the semantics of events, additionalcommonsense knowledge is still necessary for theabductive reasoning.
for example, as illustratedin figure 1 (b), given observations o1 and o2, tochoose the more likely explanation h1 : a thief.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5181–5190august1–6,2021.©2021associationforcomputationallinguistics5181entered the room and exclude h2 : a breeze blewin the window, prediction model should have thecommonsense knowledge that it is hardly possiblefor a breeze to mess up the room, whereas a thiefmay enter the room from the open window (i1),then rummage through the room (i2) and lead toa mess.
these intermediary events (i1 and i2) canserve as necessary commonsense knowledge forunderstanding the relationship between observedevents and hypothesis events..we notice that the observed events, hypothesisevents, intermediary events and their relationshipscould be described using an event graph, whichcan be constructed based on an auxiliary dataset.
the challenge is how to learn such commonsenseknowledge from the constructed event graph..to address this issue, we propose an event graphenhanced roberta (ege-roberta) model, anda two-stage training procedure.
speciﬁcally, asshown in figure 1 (c), on the basis of the robertaframework, we additionally introduce a latent vari-able z to model the information about the intermedi-ary events.
in the pretraining stage, ege-robertais trained upon an event-graph-based pseudo in-stance set to capture the commonsense knowledgeusing the latent variable z. in the ﬁnetuning stage,model adapts the commonsense knowledge cap-tured by z to conduct the abductive reasoning..experimental results show that ege-robertacould effectively learn the commonsense knowl-edge from a well-designed event graph, and im-prove the model performance on the αnli taskcompared to the baseline methods.
the code isreleased at https://github.com/sjcfr/ege-roberta..2 background.
2.1 problem formalization.
as shown in figure 1 (a), αnli can be deﬁned asa multiple-choice task.
given two observed eventso1 and o2 happened in a sequential order, oneneeds to choose a more reasonable hypothesis eventfrom two candidates h1 and h2 for explainingthe observations.
therefore, we formalize the ab-ductive reasoning task as a conditional distribu-tion p(y |o1, hi, o2), where hi ∈ {h1, h2}, andy ∈ [0, 1] is a relatedness score measuring thereasonableness of hi..in the αnli dataset, hi is set to be an expla-nation event happens intermediate to o1 and o2(bhagavatula et al., 2019).
hence, o1, o2 and hiform an event temporal sequence o1, hi, o2.
for.
brevity, we denote the event sequence as x =(o1, hi, o2).
therefore, taking the event order intoconsideration, we further characterize the abduc-tive reasoning task as p(y |x)..2.2 event graph.
formally, an event graph could be denoted as g ={v, r}, where v is the node set, and r is the edgeset.
each node vi ∈ v corresponds to an event,while rij ∈ r is a directed edge vi → vj alongwith a weight wij, which denotes the probabilitythat vj is the subsequent event of vi..given observed events and a certain hypoth-esis event, from the event graph we could ac-quire additional commonsense knowledge about:(1) the intermediary events, (2) the relation-ships between events.
as figure 1 (b) shows,the observed events, hypothesis event and inter-mediary events compose another event sequence(o1, i1, hi, i2, o2).
for clarity, we deﬁne such eventsequence as posterior event sequence x (cid:48), wherex (cid:48) = (o1, i1, hi, i2, o2).
the relationship betweenevents within x (cid:48) could be described by an adja-cency matrix a ∈ r5×5, with each element initial-ized using the edge weights of the event graph:.
ajk =.
(cid:26) wjk,0,.if vj → vk ∈ r,others..(1).
the matrix a could describe the adjacency rela-.
tionship between arbitrary two events in x (cid:48)..3 ege-roberta as a conditionalvariational autoencoder basedreasoning framework.
in this paper, rather than directly predicts the re-latedness score y based on the event sequence x,we propose to predict y based on both x and ad-ditional commonsense knowledge (i.e.
posteriorevent sequence x (cid:48) and adjacency matrix a).
tothis end, we introduce a latent variable z to learnsuch knowledge from an event graph through a twostage training procedure.
to effectively capture theevent graph knowledge through z and conduct theabductive reasoning task based on z, we frame theege-roberta model as a conditional variationalautoencoder (cvae) (sohn et al., 2015)..speciﬁcally, with regard to the latent variablez, ege-roberta characterizes the conditional dis-tribution p (y |x) using three neural networks:a prior network pθ(z|x), a recognition networkqφ(z|x (cid:48), a) and a neural likelihood pθ(y |x, z),.
5182figure 2: illustration of the pretraining, ﬁnetuning andprediction process of ege-roberta.
the grey color incircle denotes the availability of corresponding informa-tion.
for example, in the pretraining stage conducted onthe pseudo instance set, x, y and additional common-sense knowledge x (cid:48) and a are available.
while in theﬁnetuning stage on αnli, x (cid:48) and a are absent..where θ and φ denote the parameters of networks.
moreover, instead of directly maximize p (y |x),following cvae (sohn et al., 2015), ege-robertaproposes to maximize the evidence lower bound(elbo) of p (y |x) :.
lelbo(θ, φ) =eqφ(z|x(cid:48),a)log(pθ(y |x, z)).
− kl(qφ(z|x (cid:48), a)||pθ(z|x))≤ logp(y |x).
(2).
note that, in the recognition network, the latentvariable z is directly conditioned on x (cid:48) and a,where x (cid:48) = {o1, i1, hi, i2, o2} is the posterior eventsequence, a is an adjacency matrix describing therelationship between events within x (cid:48).
this en-ables z to capture the event graph knowledge fromx (cid:48) and a. through minimizing the kl term ofelbo, we can teach the prior network pθ(z|x) tolearn the event graph knowledge from the recog-nition network as much as possible.
then in theneural likelihood pθ(y |x, z) the relatedness scorey could be predicted based on x and z, whichcaptures the event graph knowledge..however, the event graph knowledge is absentin the αnli dataset.
to learn such knowledge, wedesign the following two-stage training procedure:pre-training stage: learning event graphknowledge from a pseudo instance set in thisstage, ege-roberta is pretrained on a prebuiltevent-graph-based pseudo instance set, which con-tains rich information about the intermediary eventsand the events relationships.
as shown in fig-ure 2 (a), the latent variable z is directly condi-tioned on x (cid:48) and a. therefore, z could be em-ployed to learn the event graph knowledge..finetuning stage: adapt event graphknowledge to the abductive reasoning taskas figure 2 (b) shows, at the ﬁnetuning stage,ege-roberta is trained on the αnli dataset.
figure 3: architecture of ege-roberta..without the additional information x (cid:48) and a. inthis stage model learns to adapt the captured eventgraph knowledge to the abductive reasoning task.
then as figure 2 (c) shows, after the two-stagetraining process, ege-roberta could predict therelatedness score y based on the latent variable z..4 architecture of ege-roberta.
we introduce the speciﬁc implementation ofege-roberta.
as illustrated in figure 3, ege-roberta introduces four modules in addition tothe roberta framework: (1) an aggregator provid-ing representation for any event within x and x (cid:48);(2) an attention-based prior network for modelingpθ(z|x); (3) a graph neural network based recog-nition network for modeling qφ(z|x (cid:48), a); (4) amerger to merge the latent variable z into robertaframe for downstream abductive reasoning task..4.1 event representation aggregator.
the event representation aggregator provides dis-tributed representation for events in both the eventsequence x and the posterior event sequence x (cid:48).
to this end, the aggregator employs attention mech-anism to aggregate token representations of theevent sequence from hidden states of roberta..1,. .
.
,x1.
1,. .
.
,x3.
given an event sequence x composed of to-kens [[cls], (x1l1 ),.
.
.
,(x3l3 )] (where [cls]is the special classiﬁcation token (devlin et al.,2019), and xjk is the kth token within the jth event),the m th transformer layer of roberta encodesthese tokens into contextualized distributed repre-sentations h (m ) = [h[cls], (h1l3 )],where hjk ∈ r1×d is the distributed representationof the kth token within the jth event.
then for the.
l1 ),.
.
.
,(h3.
1,. .
.
,h1.
1,. .
.
,h3.
5183(cid:80) hjlj.
jth event, the distributed representation is initial-ized as qj = 1.
multi-head attention mech-ljanism (multiattn) (vaswani et al., 2017) is em-ployed to softly select information from h (m ) andget the representation of each event:.
ej = multiattn(qj, h (m ))..(3).
for brevity, we denote the vector representa-tion of all events in x using a matrix ex , whereex = {e1, e2, e3} ∈ r3×d.
note that, through theembedding layer of roberta, position informa-tion has been injected into the token representations.
therefore, ex derived from token representationscarries event order information.
in addition, sinceex is obtained from the hidden states of roberta,rich linguistic knowledge within roberta couldbe utilized to enhance the comprehension of eventsemantics.
by the same way, the representation ofevents within x (cid:48) could be calculated, which wedenote as ex (cid:48)..4.2 recognition networkthe recognition network models qφ(z|x (cid:48), a)based on ex (cid:48) and a, where ex (cid:48) is the represen-tations of events within x (cid:48).
following traditionalvae, qφ(z|x (cid:48), a) is assumed to be a multivariategaussian distribution:.
qφ(z|x (cid:48), a) ∼ n (µ(cid:48)(x (cid:48), a), d),.
(4).
where d denotes the identity matrix..to obtain µ(x (cid:48), a), we ﬁrst combine ex (cid:48) andadjacency matrix a using a gnn (kipf et al.,2016):.
e(u )(cid:48).
= σ(aex(cid:48) w (u))..(5).
where σ(·) is the sigmoid function; w (u) ∈ rd×d is aweight matrix and e(u )(cid:48)are relational informationupdated event representations..then a multi-head self-attention operation is per-formed to promote the fusion of event semanticinformation and relational information:, e(u )(cid:48).
= multiattn(e(u )(cid:48).
e(u )(cid:48).
(6).
)..finally, to estimate µ(x (cid:48), a), we aggregate infor-using a readout function g(·):.
mation within e(u )(cid:48).
µ(cid:48) = g(e(u )(cid:48).
)..(7).
following zhou et al.
(2019) and zhong et al.
(2019), we set g(·) to be a mean-pooling operation.
hence, by estimating µ(cid:48) based on the relationalinformation updated event representation e(u )(cid:48),event graph knowledge about x (cid:48) and a is involvedinto the latent variable z..4.3 prior network.
the prior network models pθ(z|x) based on ex ,where ex is the representation matrix of events inx. the same as the recognition network, pθ(z|x)also follows multivariate normal distribution, whilethe parameters are different:.
pθ(z|x) ∼ n (µ(x), d),where d denotes the identity matrix..(8).
to obtain µ(x), different from the recognitionnetwork, the prior network starts from updatingex using a multi-head self-attention:.
e(u ) = multiattn(ex , ex ).
then an additional multi-head self-attention op-eration is performed to get deeper representations:.
(9).
e(u ) = multiattn(e(u ), e(u ))..(10)finally, µ(x) is estimated through aggregating.
information from e(u ):.
µ = g(e(u )),.
(11).
where g(·) is a mean-pooling operation..4.4 merger.
the merger module merges the latent variable z aswell as updated (deep) representation of events intothe n th transformer layer of roberta frame forpredicting the relatedness score.
to this end, weemploy multi-head attention mechanism to softlyselect relevant information from z and e(u ), andthen update the hidden state of the n th transformerlayer of roberta..speciﬁcally, in the pretraining stage:h (n )∗.
= multiattn(h (n ), [µ(cid:48); e(u )]),(12)where h (n ) is the hidden states of the n th trans-former layer of roberta, and h (n )∗is the eventgraph information updated hidden states..while in the ﬁnetuning and prediction stage:h (n )∗.
= multiattn(h (n ), [µ; e(u )])..(13).
note that, given x, pθ(µ|x) achieves its max-imum when z = µ. hence, making predictionsbased on µ could be regarded as ﬁnding the bestexplanation based on the most likely common-sense situation.
through integrating latent vari-able z, h (n )∗ contains the event graph knowledge.
by taking h (n )∗as the input of the subsequent(n + 1)th transformer layers of roberta for pre-dicting the relatedness score, the abductive reason-ing task is conducted based on the additional eventgraph knowledge..51844.5 optimizing.
the αnli task requires model to choose a morelikely hypothesis event from two candidates.
how-ever, in the pre-training stage, the negative exam-ples are absent in the pseudo instances.
to addressthis issue, following the method of liu et al.
(2019),in the pre-training stage ege-roberta is trainedto predict the masked tokens in the event sequencex rather than the relatedness score.
in addition, inorder to balance the masked token prediction losswith the kl term, we introduce an additional hy-perparameter λ. hence, the objective function inthe pretraining stage is deﬁned as follows:.
lelbo(θ, φ) =eq(z|x(cid:48),a)loglm lm (x, z; θ).
− λkl(qφ(z|x (cid:48), a)||pθ(z|x)),.
(14).
where loglmlm (x, z; θ) is the masked token predic-tion loss.
intuitively, through minimizing the klterm, we aim to transmit the event graph knowledgefrom the recognition network to the prior network.
in the ﬁnetuning stage, ege-roberta is trainedto adapt the learned event graph knowledge to theabductive reasoning task.
without the recognitonnetwork, we formulate the objective function as:.
l(θ) = pθ(y |z, x) = pθ(y |z, x)pθ(z|x)..(15).
4.6 training details.
we implement two different sizes of ege-robertamodel (i.e.
ege-roberta-base and ege-roberta-large) based on roberta-base framework androberta-large framework, respectively.
for theege-roberta-base model, in the aggregator, theprior network, the recognition network and themerger, the dimension of the attention mechanism dis set as 768, and all multi-head attention layers con-tain 12 heads.
while for the ege-roberta-largemodel, d is equal to 1024 and all multi-head atten-tion layers contain 16 heads.
in the ege-roberta-base model, token representations are aggregatedfrom the 7th transformer layer of roberta, andthe latent variable is merged to the 10th transformerlayer of roberta.
while for the ege-roberta-large model, the aggregator and merger layer areset as the 14th and 20th layer, respectively.
thebalance coefﬁcient λ equals 0.01. more details areprovided in the appendix..5 experiments.
5.1 αnli dataset.
the αnli dataset (bhagavatula et al., 2019) con-sists of 169,654, 1,532 and 4,056 (cid:104)o1, o2, h1, h2(cid:105).
(posterior) event sequenceobserved event 1 (o1)intermediary event 1 (i1)hypothesis event (h1)intermediary event 2 (i2)observed event 2 (o2)a pseudo instance={x, x’, a}, where.
story1(cid:13) i was doing exercise in gym.
2(cid:13) i felt very hot.
3(cid:13) i got up to turn on the fan.
4(cid:13) the fan began to cool down my room.
5(cid:13) i felt much more comfortable..x = (o1, h1, o2); x (cid:48) = (o1, i1, h1, i2, o2)a is initialized from the event graph..table 1: an example for illustrating the construction ofpseudo instances used for pretraining ege-roberta..quadruples in training, development and test set,respectively.
the observation events are collectedfrom a short story corpus rocstory (mostafazadehet al., 2016), while all of hypothesis events areindependently generated through crowdsourcing..5.2 construction of event graph.
the event graph serves as an external knowledgebase to provide information about the relation-ship between observation events and intermediaryevents.
to this end, we build the event graph basedon an auxiliary dataset, which are composed oftwo short story corpora independent to αnli, i.e.,vist (huang et al., 2016), and timetravel (qinet al., 2019).
both vist and timetravel are com-posed of ﬁve-sentences short stories.
totally thereare 121,326 stories in the auxiliary dataset..to construct the event graph, we deﬁne eachsentence in the auxiliary dataset as a node in theevent graph.
to get the edge weight wij betweentwo nodes vi and vj (i.e., the probability thatvj is the subsequent event of vi), we ﬁnetune aroberta-large model through a next sentence pre-diction task.
speciﬁcally, we deﬁne adjacent sen-tence pairs in the story text (for example, [1st, 2nd]sentence, [4th, 5th] sentence of a story) as posi-tive instances, deﬁne nonadjacent sentence pairsor sentences pairs in reverse order (such as [1st,3rd] sentence, [5th, 4th] sentence of a story) asnegative instances.
after that we sample 300,000positive and 300,000 negative instances from theauxiliary dataset.
then given an event pair (vi, vj),the ﬁnetuned roberta-large model would be ableto predict the probability that vj is the subsequentevent of vi..event graph based pseudo instance set forpretraining ege-roberta to effectively utilizethe event graph knowledge, we induce a set ofpseudo instances for pretraining the ege-robertamodel.
speciﬁcally, given a ﬁve-sentence-storywithin the auxiliary dataset, as table 1 shows, wedeﬁne the 1st and 5th sentence of the story as two.
5185observed events, the 3rd sentence as the hypothesisevent, the 2nd and 4th sentence as intermediaryevents, respectively.
in this way, the posterior eventsequence x (cid:48) and the event sequence x of a pseudoinstance could be obtained.
in addition, given x (cid:48),we initialize the elements of the adjacency matrix ausing the edge weights of the event graph, and scalea so that its row sums equal to 1. after the aboveoperations, each pseudo instance is composed ofan event sequence x, a posterior event sequencex (cid:48) which contains intermediary event information,and an adjacency matrix a which describes rela-tionships between events within x (cid:48)..5.3 baselines.
we compare ege-roberta with:• svm uses features about length, overlap and sen-timent to predict the more likely hypothesis event.
• infersent (conneau et al., 2017) represents sen-tences using a bi-lstm, and predicts the related-ness score using mlp.
• gpt (radford et al., 2018) is a multilayer-transformer based unidirectional pretrained lan-guage model.
• bert (devlin et al., 2019) is a multilayer-transformer based bi-directional pretrained lan-guage model.
• roberta (liu et al., 2019) refers robustly opti-mized bert.
• ege-robertau(npretrained) refers to the ege-roberta model without the pretraining stage.
• ege-robertaλ=0 refers to setting the balancecoefﬁcient to 0 in the pretraining stage.
note thatall pretrained-language-model-based baselines (i.e.,gpt, bert and roberta) are ﬁnetuned on theαnli dataset as the method of bhagavatula et al.
(2019) to adapt to the abductive reasoning task..in addition, we also list two concurrent works:(i) l2r (zhu et al., 2020) learns to rank the can-didate hypotheses with a novel scoring function.
(ii) roberta-gpt-mhka (paul et al., 2020) en-hances pretrained language model with social andcausal commonsense knowledge for αnli task..5.4 quantitative analysis.
we list the prediction accuracy (%) in table 2, andobserve that:.
(1) compared with svm and infersent, pre-trained language model based methods: gpt,bert, roberta and ege-roberta show sig-niﬁcant better performances in abductive reason-ing task.
this is because through the pre-training.
methodssvminfersent (conneau et al., 2017)gpt (radford et al., 2018)bert-base (devlin et al., 2019)roberta-base (liu et al., 2019)bert-large (devlin et al., 2019)roberta-large (liu et al., 2019)concurrent methodsl2r (zhu et al., 2020)roberta-gpt-mhka (paul et al., 2020)this workege-roberta-largeuege-roberta-largeλ=0ege-roberta-baseege-roberta-largehuman performance.
accu.
(%)50.650.863.163.371.568.983.9.
86.887.1.
83.884.275.987.591.4.table 2: accuracy on the test set of αnli..stage language models could capture rich linguis-tic knowledge that is helpful for understanding thesemantics of events..(2) comparison between ege-roberta-largeuwith ege-roberta-large shows thatthe pre-training process can increase the accuracy of ab-ductive reasoning.
in addition, comparison be-tween ege-roberta-largeλ=0 with ege-roberta-large indicates that in the pre-training process, ege-roberta could capture the event graph knowledgethrough the latent variable to enhance the abduc-tive reasoning.
furthermore, the relative close per-formance between ege-roberta-largeu and ege-roberta-largeλ=0 suggest that the main improve-ments of the performance is brought by the eventgraph knowledge..(3) compared to roberta, ege-robertaachieves higher prediction accuracy for both thebase and large sized model.
this result conﬁrmsour motivation that learning event graph knowledgecould be helpful for the abductive reasoning task.
(4) according to bhagavatula et al.
(2019), hu-man performance on the test set of αnli is 91.4%.
while the roberta-large model has achieved anaccuracy of 83.9%.
therefore, further improve-ments over roberta-large could be challenging.
through learning the event graph knowledge, ourproposed method ege-roberta further improvesthe relative accuracy..(5) our approach has comparable performancewith the sota concurrent work, which combinesroberta with gpt, and incorporates social andcausal commonsense into model.
the combinationof both methods would further increase the modelperformance..5186figure 4: accuracy of ege-roberta-base pretrainedwith different balance coefﬁcient λ..modelege-roberta-base-w/ ˜a-w/ ˜i1 and ˜i2.
accuracy (%)77.975.576.0.table 3: prediction accuracy of the ege-roberta-base model pretrained with randomly initialized adja-cency matrix ˜a / randomly sampled intermediary events{ ˜i1, ˜i2}..5.5 ablation study.
all studies are conducted on the development setof the αnli using the ege-roberta-base model.
inﬂuence of the balance coefﬁcient in the pre-training stage, the balance coefﬁcient λ controls thetrade off between event graph knowledge learningand abductive reasoning.
to investigate the speciﬁcinﬂuence of the balance coefﬁcient, we comparethe performance of ege-roberta model pretrainedwith different λ. as shown in figure 4, the predic-tion accuracy continues to increase as λ increasesfrom 0 to 0.01. this is because adequate eventgraph knowledge can offer guidance for the abduc-tive reasoning task.
while when λ exceeds 0.05,the accuracy start to decrease, as the over-emphasisof event graph knowledge learning would in turnundermine the model performance..inﬂuence of.
the external commonsenseknowledge we study the speciﬁc effect of theevent relational information and the intermediaryevent information by controlling the generation ofpseudo instances.
in speciﬁc, we eliminate the in-ﬂuence of the adjacency matrix a by replacing awith a randomly initialized matrix ˜a.
similarly, theinﬂuence of the intermediary events i1 and i2 iseliminated through substituting them by two ran-domly sampled events ˜i1 and ˜i2.
as table 3 shows,both the replacement of a and {i1, i2} lead to ob-vious decrease of model performance.
this demon-strates that ege-roberta can use both two kinds ofevent graph knowledge for enhancing the abductivereasoning task..5.6 sensitivity analysis.
to ﬁnd out if the improvement of ege-robertais brought by a certain dataset, and the speciﬁc.
datasetaccuracy#pseudo instancesaccuracy.
-w/o timetravel76.6.
40,00074.3.
60,00075.4.
-w/o vist75.7.
80,00076.2.
100,00077.0.table 4: sensitivity analysis about the source and num-ber of pseudo instances on the dev set of αnli..modelroberta.
ege-roberta.
posterior event sequence—-.
x (cid:48) = {o1, i1, hi, o2}x (cid:48) = {o1, hi, i1, o2}x (cid:48) = {o1, i1, i2, hi, o2}x (cid:48) = {o1, hi, i1, i2, o2}x (cid:48) = {o1, i1, hi, i2, o2}.
accu.
73.277.176.376.675.877.9.table 6: prediction accuracy (%) of the ege-roberta-base model pretrained with different forms of posteriorevent sequence..relationship between the model performance withthe number of pseudo instances, we conduct fol-lowing experiments: (1) excluding a certain datasetwhen inducing pseudo instances; (2) pretrainingthe ege-roberta-base model with different num-ber of pseudo instances.
the corresponding resultson the dev set of αnli is shown in table 4..we can ﬁnd that, the elimination of both datasetleads to decrease of model performances.
this sug-gests that the ege-roberta model could capturerelevant event graph knowledge from both dataset.
while the prediction accuracy continues to increasealong with the number of pseudo instances usedfor pretraining the ege-roberta model.
this isbecause the accumulation of commonsense knowl-edge is helpful for the abductive reasoning task.
in addition, it also indicates that the model perfor-mance could be further improved if the auxiliarydataset is even more enlarged..5.7 case study.
table 5 provides an example of model predic-tion results.
given two observed events o1 “hatesfall” and o2 “didn’t have to experience fall inguam”, the hypothesis event h1 “moved to guam”is more likely to explain the two motivations ofobserved events.
however, h1 implicitly relies ona precondition that in guam, fall could be eluded.
correspondingly, in the auxiliary dataset, thereis information supporting the hypothesis eventh1 that there is no fall in guam.
in this case,ege-roberta chooses the hypothesis event h1,whereas roberta chooses the wrong hypothesisevent h2.
this indicates that ege-roberta couldlearn the event graph knowledge in the pretrainingprocess for improving the reasoning performance..5187observed eventso1: i hated fall.
o2: i became happier becausei didn’t have to experiencefall in guam..hypothesis events.
model.
h1: i moved to guam.
(.
ege-roberta.
√.
).
h2:i took a vacation.
during the fall.
(×).
roberta.
commonsense knowledge from eg.
i1: it reminded me of death.
h: i couldn’t stand fall so i decided to move.
i2: i moved to guam.
where there was no fall season..table 5: example of abductive reasoning result made by roberta and ege-roberta, respectively..6 discussion.
in this paper, to involve the event graph knowl-edge, we formalize the posterior event sequence asx (cid:48) = {o1, i1, hi, i2, o2}.
while our approachalso allows other forms of posterior event se-quences, such as x (cid:48) = {o1, hi, i1, o2}, x (cid:48) ={o1, i1, hi, o2}, or x (cid:48) = {o1, i1, i2, hi, o2},etc.
we also pretrained ege-roberta on pseudo-instance sets derived by these manners.
the re-sults are shown in table 6. we ﬁnd that whateverforms of posterior event sequences involved inege-roberta, our approach can achieve consis-tently better performance than the baseline method.
this conﬁrms that our approach is sufﬁcientlygeneralizable to deal with various forms of exter-nal event-sequence knowledge.
furthermore, ege-roberta can also be equipped with more typesof event graph knowledge, such as backgroundknowledge by: formalizing the posterior eventsequence as x (cid:48) = {b1, .
.
.
, bm, e1, .
.
.
, en},where {b1, .
.
.
, bm} is a set of background eventsfor a given prior event sequence {e1, .
.
.
, en}.
this demonstrates the potential of ege-roberta inlearning different kinds of event graph knowledgefor different event inference tasks..7 related work.
7.1 abductive reasoning.
most previous studies focus on formal logic basedabductive reasoning (eshghi et al., 1988; levesque,1989; konolige, 1990; paul, 1993).
to infer themost reasonable hypothesis, the abductive reason-ing process could be divided into two steps: (1)proposing reasonable hypotheses; (2) ﬁnding thebest explanation from the hypotheses (levesque,1989; konolige, 1990; paul, 1993)..however, the rigidity of formal logic limits itsapplication in nlp domain.
to facilitate this, bha-gavatula et al.
(2019) proposed a text based ab-ductive reasoning task αnli.
to solve the this task,zhu et al.
(2020) formalize αnli as a rank learningtask, and propose a novel ranking function.
whilepaul et al.
(2020) enhances the reasoning modelwith social commonsense and causal commonsense.
knowledge.
compared to their works, for enhanc-ing the abductive reasoning process, we proposeto incorporate event graph knowledge by a cvaebased model ege-roberta.
in addition, we arguethat our approach can be easily extended to otherevent inference tasks..7.2 event graph based natural language.
inference.
understanding events and their relationships arecrucial for various natural language inference (nli)tasks (kruengkrai et al., 2017).
hence, a numberof previous studies explore conducting nli tasksbased on event graphs..for example, to predict the subsequent event fora given event context, li et al.
(2018) build an eventevolutionary graph (eeg), and make predictionusing a scaled graph neural network.
while wuet al.
(2019) predict the propagation of news eventthrough combining an historical event propagationgraph with temporal point process.
in addition tothe event prediction related tasks, liu et al.
(2017)propose to enhance the news recommendation byincorporating additional event graph information.
liu et al.
(2016) detect the textual contradiction byusing event graphs as additional evidence..in this paper, we employ event graph knowledgefor guiding the abductive reasoning.
to this end, wepropose a variational autoencoder based frameworkege-roberta, which employs a latent variablez to implicitly capture the necessary event graphknowledge and enhance the pretrained languagemodel roberta..8 conclusion.
in this paper, we propose a variational autoen-coder based framework ege-roberta with a two-stage training procedure for the abductive reason-ing task.
in the pretraining stage, ege-robertais able to learn commonsense knowledge from anevent graph through the latent variable, then inthe following stage the learned event graph knowl-edge can be adapted to the abductive reasoning task.
experimental results show improvement over thebaselines on the αnli task..51889 acknowledgments.
we thank the anonymous reviewers for their con-structive comments, and gratefully acknowledgethe support of the national key research and de-velopment program of china (2020aaa0106501),and the national natural science foundation ofchina (61976073)..references.
chandra bhagavatula, ronan le bras, chaitanyamalaviya, keisuke sakaguchi, ari holtzman, han-nah rashkin, doug downey, scott wen-tau yih, andyejin choi.
2019. abductive commonsense reason-ing.
arxiv preprint arxiv:1908.05739..eugene charniak and solomon eyal shimony.
1990.probabilistic semantics for cost based abduction.
brown university, department of computer science..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in natu-ral language processing, pages 670–680..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the 2019 naccl, pages 4171–4186..k eshghi, , and ra kowalski.
1988. abduction throughdeduction.
logic programming section technicalreport, department of computing, imperial college,london..ting-hao huang, francis ferraro, nasrin mostafazadeh,ishan misra, aishwarya agrawal, jacob devlin, rossgirshick, xiaodong he, pushmeet kohli, dhruv ba-tra, et al.
2016. visual storytelling.
in proceedingsof the 2016 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 1233–1239..thomas n. kipf, thomas n. welling, max, thomas n.welling, max, and max welling.
2016.semi-supervised classiﬁcation with graph convolutionalnetworks..kurt konolige.
1990. closure+ minimization implies.
abduction.
in proceedings of pricai90..canasai kruengkrai, kentaro torisawa, chikarahashimoto, julien kloetzer, jong-hoon oh, andmasahiro tanaka.
2017.improving event causal-ity recognition with multiple background knowledgesources using multi-column convolutional neural net-works.
in thirty-first aaai conference on artiﬁcialintelligence..hector j levesque.
1989. a knowledge-level account ofabduction.
in proceedings of the 11th internationaljoint conference on artiﬁcial intelligence-volume 2,pages 1061–1067..zhongyang li, xiao ding, , and ting , liu.
2018. con-structing narrative event evolutionary graph for scriptevent prediction.
in proceedings of the 27th inter-national joint conference on artiﬁcial intelligence,pages 4201–4207..maofu liu, limin wang, liqiang nie, jianhua dai, anddonghong ji.
2016. event graph based contradictionrecognition from big data collection.
neurocomput-ing, 181:64–75..shenghao liu, bang wang, and minghua xu.
2017.event recommendation based on graph random walk-ing and history preference reranking.
in proceedingsof the 40th international acm sigir conference onresearch and development in information retrieval,pages 861–864..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a corpusand cloze evaluation for deeper understanding ofcommonsense stories.
in naacl, pages 839–849..hwee tou ng, , , and raymond j mooney.
1990. therole of coherence in constructing and evaluating ab-ductive explanations.
in working notes, aaai springsymposium on automated abduction, stanford, cali-fornia..jong-hoon oh, kentaro torisawa, chikara hashimoto,motoki sano, stijn de saeger, and kiyonori ohtake.
2013. why-question answering using intra-and inter-sentential causal relations.
in proceedings of the 51stannual meeting of the association for computationallinguistics (volume 1: long papers), pages 1733–1743..debjit paul, anette frank, and and .
2020. socialcommonsense reasoning with multi-head knowledgeattention.
arxiv preprint arxiv:2010.05587..gabriele paul.
1993. approaches to abductive rea-soning: an overview.
artiﬁcial intelligence review,7(2):109–152..lianhui qin, antoine bosselut, ari holtzman, chandrabhagavatula, elizabeth clark, and yejin choi.
2019.counterfactual story reasoning and generation.
arxivpreprint arxiv:1909.04076..alec radford, karthik narasimhan, tim salimans,and ilya sutskever.
2018.improving languageunderstanding by generative pre-training.
url.
5189https://s3-us-west-2.
com/openai-assets/researchcovers/languageunsupervised/languageunderstanding paper.
pdf..amazonaws..kihyuk sohn, honglak lee, xinchen yan, et al.
2015.learning structured output representation using deepconditional generative models.
in nips, pages 3483–3491..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..weichang wu, huanxi liu, xiaohu zhang, yu liu,and hongyuan zha.
2019. modeling event propaga-tion via graph biased temporal point process.
arxivpreprint arxiv:1908.01623..wanjun zhong, jingjing xu, duyu tang, zenan xu,nan duan, ming zhou, jiahai wang, and jian yin.
2019. reasoning over semantic-level graph for factchecking.
arxiv preprint arxiv:1909.03745..jie zhou, xu han, cheng yang, zhiyuan liu,lifeng wang, changcheng li, and maosong sun.
2019. gear: graph-based evidence aggregatingand reasoning for fact veriﬁcation.
arxiv preprintarxiv:1908.01843..yunchang zhu, liang pang, yanyan lan, and xueqicheng.
2020. l2r2: leveraging ranking for abductivereasoning.
in proceedings of the 43rd internationalacm sigir conference on research and develop-ment in information retrieval, pages 1961–1964..5190