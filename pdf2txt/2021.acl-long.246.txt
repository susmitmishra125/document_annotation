fast and accurate neural machine translation with translation memory.
qiuxiang he1 guoping huang2 qu cui3 li li1∗ lemao liu21 southwest university 2 tencent ai lab 3 nanjing university.
hqxiang@email.swu.edu.cn.
cuiq@smail.nju.edu.cn.
{donkeyhuang,redmondliu}@tencent.com.
lily@swu.edu.cn.
abstract.
itis generally believed that a translationmemory (tm) should be beneﬁcial for ma-chine translation tasks.
unfortunately, existingwisdom demonstrates the superiority of tm-based neural machine translation (nmt) onlyon the tm-specialized translation tasks ratherthan general tasks, with a non-negligible com-in this paper, we pro-putational overhead.
pose a fast and accurate approach to tm-basednmt within the transformer framework: themodel architecture is simple and employs a sin-gle bilingual sentence as its tm, leading to ef-ﬁcient training and inference; and its parame-ters are effectively optimized through a noveltraining criterion.
extensive experiments onsix tm-specialized tasks show that the pro-posed approach substantially surpasses severalstrong baselines that use multiple tms,interms of bleu and running time.
in partic-ular, the proposed approach also advances thestrong baselines on two general tasks (wmtnews zh→en and en→de)..1.introduction.
a translation memory (tm) is originally collectedfrom the translation history of professional trans-lators, and provides the most similar source-targetsentence pairs for the source sentence to be trans-lated (garcia, 2009; koehn and senellart, 2010b;utiyama et al., 2011; robinson, 2012; huang et al.,2021).
a tm generally provides valuable transla-tion information particularly for those input sen-tences preferably matching the source sentencesin the tm, and many efforts have been devoted tointegrating a tm into statistical machine transla-tion (simard and isabelle, 2009; koehn and senel-lart, 2010a; ma et al., 2011; wang et al., 2013; liuet al., 2019)..recently there are increasing interests in im-proving neural machine translation (nmt) with a.
∗corresponding author..tm (li et al., 2016; farajian et al., 2017; gu et al.,2018; xia et al., 2019; bulte and tezcan, 2019; xuet al., 2020).
many notable approaches have beenproposed to augment an nmt model by using atm.
for example, zhang et al.
(2018) and he et al.
(2019) extract scored n-grams from a tm and thenreward each partial translation once it matches anextracted n-gram during beam search.
gu et al.
(2018) and xia et al.
(2019) use an auxiliary net-work to encode a tm and then integrate it into thenmt architecture.
bulte and tezcan (2019) and xuet al.
(2020) employ data augmentation to train annmt model whose training instances are bilingualsentences augmented by their tms.
despite theirimprovements on the tm-specialized translationtasks (aka jrc-acquis corpora) where a tm is verysimilar to test sentences, they consume consider-able computational overheads in either training ortesting, and particularly it is unclear whether theycan deliver gains over standard nmt on generaltasks where a tm is not very similar to test sen-tences.
indeed, both zhang et al.
(2018) and xuet al.
(2020) reported their failures on wmt newstranslation tasks..in this paper, we present a fast and accurate ap-proach for tm-based nmt which can be appliedto general translation tasks besides tm-specializedtasks.
we ﬁrst design a light-weight tm-basednmt model for efﬁciency: its tm includes a sin-gle bilingual sentence and we explore variant waysto encode the tm.
also, the designed model out-performs strong tm-based baselines.
second, wedeeply analyze its translation performance and ob-serve an issue of robustness: it decreases signiﬁ-cantly for those input sentences which are not verysimilar to their tms, although it obtains substan-tial improvements for other inputs.
to address thisissue, we propose a novel training criterion for op-timizing the parameters of our model inspired bymultiple-task learning (van dyk and meng, 2001;.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3170–3180august1–6,2021.©2021associationforcomputationallinguistics3170ben-david and borbely, 2008; qiu et al., 2013).
the ﬁrstthe loss function includes two terms:term is induced by the bilingual corpus with a tmwhereas the second term is induced by the bilingualcorpus without any tm.
in this way, the tm-basednmt model gains better performance and is robustto translate any input sentences no matter they aresimilar to their tm or not.
additionally, this makesit possible that a single uniﬁed model can handleboth translation situations (with or without a tm),which is practical for online services..to validate the effectiveness of the proposed ap-proach, we conduct extensive experiments on eighttranslation tasks including both tm-specializedtasks and general tasks (wmt).
our experimentsjustify that the proposed approach is better thanseveral strong tm-based baselines in speed, and itfurther delivers substantial gains (up to 4.7 bluepoints) over those baselines on tm-specializedtasks, leading to up to 8.5 bleu points over stan-dard transformer-based nmt.
in particular, it alsooutperforms strong baselines on two general trans-lation tasks, i.e., with a gain of 0.7 bleu pointson wmt14 en→de task and 1.0 bleu point onwmt17 zh→en task..this paper makes the following contributions:.
• it points out a critical issue about robustnesswhen training tm-based nmt models andprovides an elegant method to address thisissue..• it proposes a simple tm-based nmt modelthat outperforms strong tm-based baselinesin terms of both translation quality and speed..• it veriﬁes that a well-designed tm-basedtranslation model is able to advance strongmt baselines on general translation taskswhere a tm is not very similar to input sourcesentences..2 preliminary on nmt.
suppose x = {x1, ..., xn} is a source sentence andy = {y1, ..., ym} is the corresponding target sen-tence.
from the probabilistic perspective, nmtmodels the conditional probability of the target sen-tence y given the source sentence x. formally, for agiven x, nmt aims to generate the output y accord-ing to the conditional probability p (y|x) deﬁnedby neural networks:.
where y<i = {y1, .
.
.
, yi−1} denotes a preﬁx of y,and each factor p (yi|x, y<i) is deﬁned as follows:.
p (yi|x, y<i) = softmax.
(cid:16).
φ(hd,li.
(cid:17)).
(2).
i.where hd,lindicates the ith hidden unit at lthlayer in the decoding phrase under the encoder-decoder framework (bahdanau et al., 2016), and φis a linear network that projects hidden units ontovectors with dimension of the target vocabulary..i.recently, self-attention networks have attractedmany interests due to their ﬂexibility in parallelcomputation and modeling hd,l.
the state-of-the-art nmt model is transformer (vaswani et al.,2017), which uses stacked self-attention and fullyconnected layers for its encoder and decoder.
self-attention relies on an attention mechanism to com-pute a representation of a sequence.
in trans-former, there are three kinds of attention mech-anisms, including encoder multi-head attention, de-coder masked multi-head attention and encoder-decoder multi-head attention.
attention with hheads can be calculated by the equations:.
mh-att(q, u) =.
att(q, φj(u), ψj(u)).
(cid:20).
(cid:21)h.,.
j=1.
(3).
att(q, u, v) = softmax.
(cid:18) qu(cid:62)√d.(cid:19).
v.where q is a query vector and u is a two-dimensional matrix, [uj]hj=1 denotes concatenationof all vectors uj, φj and ψj stand for two linearprojections from one matrix to another matrix, re-spectively.
the 1√is the scaling factor, and d is theddimension of q. and we refer enthusiastic readersto vaswani et al.
(2017) for detailed deﬁnitions..3 model architecture.
in this section, in order to preferably bridge tmand nmt, we propose the architecture of tm-basednmt within the transformer.
to make our pro-posed model fast in running time and powerful inquality, at ﬁrst, we present a conﬁguration of tmto make the proposed model efﬁcient.
then we ex-plore three different methods to encode the tm intoa sequence of vectors in a coarse-to-ﬁne manner.
finally, we propose the architecture that decodes atarget word given an input source sentence and itstm representation..p (y|x) =.
p (yi|x, y<i).
(1).
following previous works (gu et al., 2018; zhanget al., 2018; xia et al., 2019), for each source.
m(cid:89).
i=1.
3.1 tm conﬁguration.
3171figure 1: the architecture of the proposed three methods.
1. the part in the dashed box is an example of ourmethods.
the source and target languages are german and english respectively.
x is the source sentence and y isthe corresponding target sentence.
2. the part outside of the dashed box shows the whole model architecture.
thecore component is the example layer which consists of multi-head attention and cross attention mechanisms.
forsimplicity, we omit an add and layer normalization in other sub-layers.
l is the number of operation layers..sentence x we employ apache lucene (bialeckiet al., 2020) to retrieve top-100 similar bilingualsentences from the training data.
then we adopt thefollowing similarity to re-rank the retrieved bilin-gual sentences and maintain top-k (k < 100)bilingual sentences as the tm for x:.
sim(x, xtm) = 1 −.
dist(x, xtm)max(|x|, |xtm|).
(4).
where dist denotes the edit-distance, and xtm isa retrieved source sentence from the training dataand its reference is ytm..previous studies show that the best translationquality is achieved when the size k of the tm islarger than 1. for example, the optimized k is setto be 5 in gu et al.
(2018) and xia et al.
(2019), andit is even set to be 100 in zhang et al.
(2018).
un-fortunately, such a large k signiﬁcantly decreasesthe translation speed because the computationalcomplexity is linear in the size of k. to make ourinference as efﬁcient as possible, we set k = 1 andemploy the most similar bilingual sentence denotedby (cid:104)xtm, ytm(cid:105) as the tm for x.1.
3.2 encoding tm.
in this subsection, we will describe how to encodethe tm (cid:104)xtm, ytm(cid:105) into a sequence of vectors m..1we also did some experiments on k = 2 and k = 4 inour proposed model, but we did not observe signiﬁcant gains..three variant methods for encoding a tm are illus-trated in the right part of figure 1..method 1: sentence (tf-s) given (cid:104)xtm, ytm(cid:105)for x, the ﬁrst method utilizes word embeddingand position embedding of ytm to represent m asfollows:.
m = etm = [ew(y1· · · , ew(yj (cid:48).
tm) + ep(y1tm) + ep(yj (cid:48).
tm),tm)].
(5).
where ew and ep are word embedding and positionembedding respectively, j (cid:48) is the length of ytm andthe symbol + denotes a simple addition operator..method 2: sentence with score (tf-ss) theﬁrst method is agnostic to the similarity score.
in-tuitively, if a tm (cid:104)xtm, ytm(cid:105) is with high similarity,ytm may be more helpful to predict a good trans-lation.
so, the second method takes the similarityscore into account and it deﬁnes m as follows:.
m = stm × etm(6)where stm = sim(x, xtm) is the similarity scoreand the symbol × denotes the scalar-multiplication..method 3: sentence with alignment (tf-sa)as shown in figure 1, xtm consists of the matchedparts (in orange color) and the unmatched parts (indark color) to x. since each word in the tm isnot of the same importance to the source sentencex, we should pay more attention to the words that.
3172atmmh att.
(        )feed forwardfeed forwardmh att.
(           )linear + softmaxl xx: der direktor führt den haushaltsplan des eti aus .y: the director shall implementthe budget of the eit .target embedding mh att.
(             )add & normexample layerxtm: der direktor führt den haushaltsplan der agentur aus .y    : the director shall implement the agency  ’  s budget  .tf-satf-ssword + position embeddingatm =  alignment ( xtm ,  y     )the director shallimplement stmatmstm  =  similarity ( x , xtm )✖source embeddingnote:   ✖    scalar-multiplication                              ()10···011aligntmy<ix,y<iy<iytm,xx,,y<i1y tmetmetmetm etm tf-s✖ stm stmtm mh att.
(             )an operator between a vector and a matrixare in the matched parts.
so, we further obtainthe word alignment between xtm and ytm throughfast-align toolkit (dyer et al., 2013).2 supposeatm is the word alignment between xtm and ytm:ajtm = 1 denotes yj is aligned to some xi other-wise ajtm = 0, where xi is also in x .
therefore,the third method deﬁnes m as follows:(cid:1).
m = atm ◦ (cid:0)stm × etm(7)where the symbol ◦ denotes an operator between avector and a matrix such that(cid:40).
stm × ejtmejtm.
if ajif aj.
tm = 0tm = 1.
(8).
mj =.
3.3 tm augmented nmtsuppose the encoded tm (cid:104)xtm, ytm(cid:105) is denotedby m, a sequence of vectors.
we aim to build amodel p (yi | x, y<i, m) for the source sentence x,given the m and preﬁx translation y<i at time stepi, leading to the entire translation model:p (y | x, xtm, ytm; θ) =.
p (yi | x, y<i, m).
(cid:89).
(9).
iwhere θ denotes the parameter of our proposedmodel.3.
example layer the model architecture ofp (yi | x, y<i, m) is illustrated at the left part offigure 1, where its architecture is generally sim-ilar to standard transformer and the core compo-nent is the example layer.
speciﬁcally, the exam-ple layer includes two multi-head attention oper-ators: the left multi-head attention (i.e.
mh-att(y<i, y<i)) is the same as transformer, and it is de-ﬁned on the preﬁx translation y<i; the right multi-head attention (i.e.
mh-att (y<i, ytm)) attempts tocapture information from the tm, and its query isfrom y<i while key and value are from the represen-tation of tm m. after the two parallel attention op-erators, two resulting sequences are passed to add& norm operator and a new sequence is obtainedas the query for the next multi-head attention (i.e.
mh-att (y<i, x)).
the following sub-layer is thesame as transformer and p (yi | x, y<i, m) can beobtained similar to the deﬁnition of standard nmtp (yi | x, y<i) as presented in section 2. we skipthose formal equations to rewrite p (yi | x, y<i, m)due to space limitation..2although some advanced word alignment toolkits (douand neubig, 2021; chen et al., 2021; jalili sabet et al., 2020)may lead to better performance, we still employ fast-align tobe in line with previous work for fair comparison (zhang et al.,2018; xia et al., 2019)..3in the rest of this paper, we may drop θ in the model for.
easier notations..in summary the entire model architecture is il-lustrated in figure 1: the dashed box in the rightpart shows the memory encoder, and the left partshows how the memory representation is used inthe nmt model similar to the transformer.
in ourmodel architecture, the encoder block contains twosub-layers and the decoder block contains threesub-layers.
the core sub-layer in the decoder blockis our proposed example layer, which consists ofmulti-head attention and cross attention.
by intro-ducing the memory encoder and example layer,the parameters in our model are increased only by8.96% compared to the standard nmt baseline..4 training.
is d.training.
thetm, yi.
=corpussupposetm(cid:105) | i ∈ [1, n ]}, where (cid:104)xi, yi(cid:105){(cid:104)xi, yi, xiis a bilingual sentence, and (cid:104)xitm(cid:105) is therelated tm which consists of a single bilingualsentence.
our goal is to learn the parameter θ ofthe tm-based nmt model p (y | x, xtm, ytm; θ)deﬁned in eq.
(9) using d..tm, yi.
the common wisdom is to optimize the pa-rameter under the maximum likelihood estimation(mle), i.e.
standard training.
formally, it mini-mizes the following criterion:.
n(cid:88).
−.
i.log p (yi | xi, xi.
tm, yi.
tm; θ)..robustness issue unfortunately,the modeltrained with mle suffers from an issue about ro-bustness even if its overall performance is muchbetter than standard transformer and outperformstm-based baselines on the es→en task.
accord-ing to our experiments (see table 4 later), our pro-posed model performs worse than the transformerfor those sentences which do not have a similar tm.
as a result, it would be dangerous to use the modelfor online services because users may provide aninput sentence whose tm is not similar to itself..the possible reason for the above issue is ex-plained as follows.
on the average case, the refer-ence y is strongly correlated to its tm target ytmin the training corpus d. for example, the averagesimilarity score is about 0.58 for es→en transla-tion task, according to our statistics.
because ofthe powerful ﬁtting ability of neural networks, themodel parameters will be guided to heavily dependon the given tm target ytm during training.
inthis way, if an input source sentence x has a highsimilarity with its given tm, the model will output.
3173high-quality results, as we also observed in table 5.on the contrary, once an input sentence is providedwith a low similar tm (cid:104)xtm, ytm(cid:105) (for instance,the similarity between 0 and 0.3, as shown in ta-ble 4), the translation quality of its output rapidlydecreases..training criterion in order to avoid the tmover-ﬁtting, we propose a simple yet elegantmethod, inspired by data augmentation (van dykand meng, 2001; li et al., 2019; zhong et al.,2020) and multiple-task learning (ben-david andborbely, 2008; qiu et al., 2013; liu et al., 2016).
speciﬁcally, we ﬁrst construct another corpusd0 = {(cid:104)xi, yi, null, null(cid:105) | i ∈ [1, n ]} fromd = {(cid:104)xi, yi, xiin theconstructed corpus, (cid:104)null, null(cid:105) plays a role of atm, but both source and target sides of the tmare empty sentences.4 then we train the modelp (y | x, xtm, ytm; θ) using both d and d0, i.e.
joint training, which is similar to multiple-tasklearning.
formally, we minimize the followingjoint loss function:n(cid:88).
tm(cid:105) | i ∈ [1, n ]}..tm, yi.
(cid:16).
log p (yi | xi, xi.
tm, yi.
tm; θ).
(cid:96)(d, d0; θ) = −.
i.
(cid:17)+ λ × log p (yi | xi, null, null; θ)(10)where 0 < λ is a coefﬁcient to trade off bothloss terms.
intuitively, the ﬁrst term induced byd guides the model to use the information from atm for prediction, and thereby it will generate ac-curate translations for those input source sentenceswhose tm is with high similarity.
on the otherhand, the second term induced by d0 teaches themodel to output good translations without informa-tion from a tm.
additionally, this makes it possiblethat a single uniﬁed model can handle both trans-lation scenarios (with or without a tm), which ispractical for online services..note that the proposed method is slightly dif-ferent from standard data augmentation (sennrichet al., 2016a; fadaee et al., 2017; fadaee and monz,2018; wang et al., 2018) and multiple-task learn-ing (dong et al., 2015; kiperwasser and balles-teros, 2018; wang et al., 2020) in nmt research.
these data augmentation techniques automaticallygenerate pseudo data based on the original train-ing data and then train a model using both originaland generated data.
however, the dataset d0 is.
4in the experiments, we implement null as the sentence.
including a single word, i.e.
“(cid:104)eos(cid:105)”..algorithm 1: joint training algorithm.
input: mini-batch size b, maximal iterationm , a learning rate schema η and twocorpus: d = {(cid:104)xi, yi, xii ∈ [1, n ]} and d0 ={(cid:104)xi, yi, null, null(cid:105) | i ∈ [1, n ]}.
tm, yi.
tm(cid:105) |.
output: the parameter θ..1 for 1 ≤ t ≤ m do.
2.
3.
4.
5.sample a mini-batch b with size of b/2from dsample a mini-batch b0 with size of b/2from d0calculate gradient ∆ = ∇θ(cid:96)(b, b0; θ)as deﬁned in eq.
(10)update parameter: θ = θ − ηt∆.
directly taken from the original d in our scenario.
also, multiple-task learning in their works typicallyinvolves different models that share some partialparameters rather than all parameters.
in contrast,both terms in our joint loss correspond to the sametask, i.e.
translation prediction given a source sen-tence and its tm; and both models are exactly thesame..the detailed joint training algorithm is presentedin algorithm 1. it follows the standard gradientdescent method for optimization.
note that in line2 and 3, it samples two mini-batches which donot share the same bilingual sentences to promotediversity, i.e., d and d0 are independently andrandomly sampled.
in our experiments, we employadam (kingma and ba, 2014) with default settingsas the learning rate schema..5 experiments.
in this section, we validate the effectiveness ofthe proposed approach: robustness for handlingboth translation situations (with or without a tm),running efﬁciency compared with the previous tm-based nmt models, translation quality on bothtm-specialized tasks and general mt tasks.
weuse the case-insensitive bleu score as the auto-matic metric (papineni et al., 2002) for the transla-tion quality evaluation..5.1 setup.
tm-specialized tasks we evaluate our proposedmodels with the jrc-acquis corpora, which in-clude three language pairs and lead to six trans-lation tasks in total: english↔german (en↔de),.
3174tm-specialized tasks general wmt tasksfr↔enes↔ende↔en en→de zh→en.
similarity.
sents percents baseline std train joint train(#).
tf-sa.
tf-sa.
(%).
tf.
train/sent(#) 740467 673856 693011 4558262 20605452.dev/sent(#).
2649.
2511.
2440.
3000.
2002.test/sent(#).
2650.
2585.
2461 3003/3004.
2001.en/word(#)29.44 32.68other/word(#) 33.35 35.58.
34.0034.22.
28.9429.90.
25.4623.03.table 1: statistics of the datasets.
the last two linesare average sentence lengths in english and other lan-guages..tm-specialized tasks general wmt tasksen→de zh→en.
all.
layersdropoutembeddingbatch sizesource vocabtarget vocab.
60.151235002000020000.
60.151225003200032000.
60.151240967500063000.table 2: training settings.
batch size refers to the to-ken number for each batch.
embedding refers to thenumber of word embedding dimensions.
for a fair com-parison, the source vocabulary size is 40000 in baselinefm+ on es→en task..english↔spanish (en↔es) and english↔french(en↔fr).
to compare with previous work, weadopt the same splitting of training/dev/test andpre-processing as gu et al.
(2018), zhang et al.
(2018), and xia et al.
(2019)..tasks the proposed models.
generalareevaluated on the widely-used general wmttasks: wmt14 english-to-german (en→de) andwmt17 chinese-to-english (zh→en) tasks.
forthe en→de task, we use newstest2013 as thedevelopment set, as well as employ newstest2014and newstest2017 as the test sets.
for the zh→entask, we employ newsdev2017 and newstest2017as the development and test set respectively..table 1 summarizes the data statistics for bothtm-specialized and general tasks.
in addition, weemploy byte pair encoding (bpe) (sennrich et al.,2016b) on all the tasks mentioned before..bleu.
devtest.
tf.
63.3562.79.tf-s.65.0065.52.tf-ss.
tf-sa.
67.0467.04.
67.2367.26.table 3: performance of our models under the standardtraining criterion.
bleu is reported on es→en task.
best results are highlighted..[0, 0.1)[0.1, 0.2)[0.2, 0.3)[0.3, 0.4)[0.4, 0.5)[0.5, 0.6)[0.6, 0.7)[0.7, 0.8)[0.8, 0.9)[0.9, 1).
2138462305272206203188377432.
0.085.3417.8711.8010.527.977.857.2714.5916.71.
[0, 0.3)[0.3, 1).
23.296021983 76.71.
[0, 1).
2585.
100.
36.9138.5347.8854.0262.2965.9471.8877.2079.9381.95.
45.3670.97.
62.79.
64.0537.7047.0754.7564.0171.3279.6385.9690.7194.60.
44.4578.22.
67.26.
74.4839.5249.0956.1966.1872.4880.0886.4591.3194.68.
46.4179.06.
68.49.table 4: translation accuracy in terms of bleu on thees→en task (test set only) for the divided subsets ac-cording to the similarity of tm..baseline systems we compare our proposedmodel with the strong baselines as follows:.
• tf (vaswani et al., 2017): it is the standard.
transformer..• tf-p (zhang et al., 2018):.
is re-implemented on top of transformer by our-selves..it.
• tf-g (xia et al., 2019) and tf-seq (guet al., 2018): tf-seq is a mimic implemen-tation over transformer by xia et al.
(2019).
we report the results from xia et al.
(2019)since they were also implemented over trans-former as comparison..• fm+ (xu et al., 2020): since xu et al.
(2020)adopt a different split on jrc corpus, the re-sults are not comparable to ours.
for a faircomparison, we re-implement a strong modelfm+ as a baseline which makes use of thesame metric to retrieve a tm as ours and isbetter than the method in bulte and tezcan(2019)..our modelsin the case of the three methodsproposed in this paper, tf-s, tf-ss and tf-sarefer to the method encoding tm by the sentence,sentence with score, and sentence with alignment,respectively.
we optimize their parameters throughboth standard training and joint training.
for jointtraining, the hyperparameter λ is set to be 1 for alltranslation tasks..system conﬁguration for a fair comparison, weemploy the same settings to train all baselines andour models, and the learning rate for all modelsis adam with the default hyper-parameters.
the.
3175details of the settings are shown in table 2..5.2 results and analysis on es→en task.
es→en.
standard training and robustness issue weﬁrst evaluate the proposed models under the stan-dard training criterion.
table 3 shows the compari-son among different tm encoding methods for ourmodels.
from this table, we can see that our mod-els achieve substantial improvements over trans-former (tf) which does not use any tm, even ifour models are simple and only utilize a singlebilingual sentence in the tm.
tf-sa performs bet-ter than tf-s and tf-ss thanks to the ﬁne-grainedalignment information encoded in the tm.
also,tf-sa outperforms all tm-based baselines by atleast 1.0 bleu point, compared with table 6..in addition, we exploit the inﬂuence of our mod-els on the similarity of a tm.
we thereby divide thetest dataset into ten subsets according to the similar-ity score and report the results in table 4. we ﬁndthat the gains of our models over the tf baselineare mainly from those sentences whose tms arewith relatively high similarity.
to our surprise, ourmodels perform worse than tf on the subset withrelatively low similarity except the subset with thelowest similarity.5 this result demonstrates thatour models with standard training are not robust tosimilarity scores, as deeply explained in the previ-ous section..joint training luckily the robustness issue canbe ﬁxed well by joint training, as depicted in theright part of table 4. we can see that our model isbetter than the baseline tf on the subset of [0, 0.3),and it substantially outperforms tf on the subsetof [0.3, 1).
with the help of joint training, tf-sadelivers gains of 1.2 bleu points over standardtraining, and gains of 5.7 bleu points over thestrong tf baseline on the entire test set..therefore, in the rest of the experiments, weemploy joint training to set up all of our modelsbecause it is robust to the low similarity of tms..without tm or with ref as tm the situationwithout any tm and the situation with referenceas a tm are more extreme cases of the robustnessissue.
as reported in table 5, if a perfect tm is.
5we further check these two exceptional sentences andﬁnd that they are very short in length.
in particular, their wordalignment results from the fast-align toolkit are very good,which may be beneﬁcial to our proposed model.
this mightbe the reason why our proposed model advances the baselinetransformer..bleu.
tf.
tf-s tf-ss tf-sa.
without tm 62.79ref as tmwith tm.
--.
without tm 24.12ref as tmwith tm.
--.
62.7288.6667.99.
24.1394.9024.22.
62.8393.1968.40.
24.2699.4325.12.
63.1592.3868.49.
24.1398.8125.03.zh→en.
table 5: bleu comparison on es→en and zh→entasks.
“ref as tm”, “with tm” and “without tm” re-spectively denote our models are provided a referenceas a tm, a retrieved tm, not provided a tm duringinference..bleu.
tf.
tf-p tf-seq fm+ tf-g tf-sa.
devtest.
63.35 65.5962.79 65.22.
64.8165.16.
66.44 66.3765.90 66.21.
68.6868.49.table 6: bleu comparison with baselines on es→entask..provided to our models, they can yield excellenttranslation results.
besides, the proposed methodsare not inferior to the standard transformer whenno tm is provided.
as a result, the proposed modelmakes it possible that a single uniﬁed model canhandle both translation situations (with or withouta tm), which is practical for online services..noisy tm to validate whether the model workswell with noisy tms, we also conduct a quick ex-periment by adding noises to tm for the test set byrandomly replacing words in the target side of tmwith incorrect words.
after replacing one and twowords, the proposed tf-sa achieves 68.17 bleupoints and 67.94 bleu points, respectively.
bothresults are slightly worse than the noise-free tf-sa(68.49) but still better than the best tm baseline(66.21).
note that both results are obtained with-out retraining tf-sa model with noisy tm.
thisfact demonstrates our model is even robust to noisytms and thus it is useful for the online tm..comparison with baselines table 6 illustratesthe results between the proposed model tf-saand the baselines.
it is clearly shown that tf-sasurpasses all tm-based baselines with a substantialmargin.
in details, tf-sa outperforms tf-p andtf-seq by about 3.2 bleu points, fm+ by about2.6 bleu points, and the strong baseline tf-g byabout 2.2 bleu points..running time since all tm-based models em-ploy the same retrieval metric and their retrieval.
3176time(s).
tf.
tf-p.tf-seq tf-g fm+.
tf-s.tf-ss.
tf-sa.
traintest.
37270.30.
-0.71.
178411.91.
70740.55.
77200.33.
43500.39.
43610.40.
45180.41.table 7: running time comparison on es→en task.
training time reports the time in seconds for training oneepoch on average, and testing time reports the time in seconds for translating one sentence on average..bleu.
tf.
tf-p tf-g tf-s tf-ss tf-sa.
fr→en66.25 69.69 70.87 72.00en→fr66.49 69.08 69.59 70.38es→en62.79 65.22 66.21 67.99en→es60.11 61.94 62.76 66.52de→en 58.50 61.49 61.72 65.58en→de 53.15 57.01 56.88 61.71.
72.5571.0368.4066.6164.8660.87.
72.3571.1168.4966.9465.5661.35.bleu.
tftf-p.tf-stf-sstf-sa.
wmt en→denews14.
news13.
news17.
wmt zh→entest17dev17.
26.1826.26.
26.5627.0226.66.
27.9327.79.
28.1328.2228.66.
26.8226.70.
26.6127.1927.48.
22.5222.65.
22.8823.8523.65.
24.1224.17.
24.2225.1225.03.table 8: translation accuracy in terms of bleu on thetm-specialized tasks..table 9: translation accuracy in terms of bleu on thegeneral wmt tasks..time is exactly the same, we only report the run-ning time of all tm-based nmt models excludingretrieval time in table 7. as reported in this table,our proposed model further saves signiﬁcant run-ning time over tf-seq and tf-g for both trainingand testing, besides achieving better translation per-formance.
in addition, although it requires slightoverhead in training, its testing is more efﬁcientthan tf-p; and our training is faster than fm+..5.3 overall translation quality.
5.3.1 on the tm-specialized datasets.
the experimental results of all the systems on thesix translation tasks of tm-specialized datasets arereported in table 8. several observations can bemade from the results.
first, the baseline tf-pand tf-g achieve substantial gains over the strongbaseline tf, outperforming by [1.1, 4.1] bleupoints.
this result is in line with the ﬁnding inzhang et al.
(2018) and xia et al.
(2019).
second,on the basis of that, compared with the strongestbaseline tf-g, our proposed tf-s, tf-ss and tf-sa can obtain further gains up to 4.9 bleu points,at least 1.2 bleu points..5.3.2 on the general wmt datasets.
it is important to mention that all previous tm-based approaches failed in getting notable improve-ments on the general wmt datasets.
since xiaet al.
(2019) did not conduct experiments on thewmt datasets and their implementation is not re-leased, we compare our models with two baselines:tf and tf-p. our experimental results on the gen-eral wmt datasets are reported in table 9. as we.
can see, the method tf-p is only comparable to thebaseline nmt, which is in line with the observa-tion in zhang et al.
(2018).
in contrast, our mod-els perform well on these tasks.
our best modelgains about 0.7 bleu points on the en→de and1.0 bleu point on the zh→en task, over bothbaselines on average.
the experimental resultsdemonstrate that a tm based translation model canadvance strong mt baselines on general translationtasks where a tm is not very similar to input sourcesentences.
what’s more, as shown in table 5, ourmodels can get excellent translation results while aperfect tm is provided..in a summary, based on the above extensive ex-perimental results, our proposed models substan-tially surpass several baselines on tm-specializedtasks and general tasks, in terms of bleu and run-ning time..6 related work.
in the statistical machine translation (smt) dia-gram, koehn and senellart (2010a) extract bilin-gual segments from a tm which matches thesource sentence to be translated, and employ aheuristic score to decide whether the extracted seg-ments should be used as decoding constraints ornot, then hardly constrain smt to decode for thoseunmatched parts of the source sentence.
ma et al.
(2011) design a ﬁne-grained classiﬁer, rather thanthe heuristic score, to predict the score for mak-ing more reliable decisions.
simard and isabelle(2009), wang et al.
(2013) and wang et al.
(2014)add the extracted bilingual segments to the transla-tion table of smt, and then bias the decoder in a.
3177soft constraint manner when decoding the sourcesentence with the augmented translation table.
liuet al.
(2012) use the retrieved bilingual sentencesto update the parameters for the log-linear modelbased smt..in recent years, many efforts are made on neuralmachine translation (nmt) associated with a tm.
li et al.
(2016) and farajian et al.
(2017) make fulluse of the retrieved tm sentence pairs to ﬁne-tunethe pre-trained nmt model on-the-ﬂy.
the mostobvious drawback of ﬁne-tuning is that the delay istoo long for testing sentences.
to avoid the onlinetuning process, zhang et al.
(2018) and he et al.
(2019) dynamically integrate translation pieces,based on n-grams extracted from the matched seg-ments in the tm target, into the beam search stage.
the second type of approach is efﬁcient but heavilydepends on the global hyper-parameter λ, which issensitive to the development set, leading to inferiorperformance..recently, there are notable approaches for thesake of further excavation on tm-based nmt.
bulte and tezcan (2019) and xu et al.
(2020) pro-pose data augmentation approaches by augmentinginput sentences with a tm which do not modifythe nmt model architecture.
gu et al.
(2018) andxia et al.
(2019) employ an auxiliary network toencode tms and integrate it into the nmt archi-tecture.
our model architecture is simpler than guet al.
(2018) and xia et al.
(2019) and we encode asingle tm target sentence and utilize simple atten-tion mechanisms on the tm.
and the architectureis more efﬁcient and leads to a faster translationspeed compared with gu et al.
(2018) and xia et al.
(2019).
in particular, we propose a novel trainingcriterion to make the tm-based nmt model morerobust in different translation situations (with orwithout a tm).
in parallel with our work, cai et al.
(2021) extend the translation memory from thebilingual setting to the monolingual setting througha cross-lingual retrieval technique, and khandel-wal et al.
(2021) report signiﬁcant improvementsin quality on general translation tasks as ours, buttheir inference speed is two orders of magnitudeslower than transformer because they perform con-textual word retrieval whose search space is muchlarger than that of sentence retrieval..7 conclusion.
this paper presents a simple tm-based nmtmodel that employs a single bilingual sentence as.
its tm and thus is fast in training and inference.
al-though the presented model with the standard train-ing outperforms strong tm-based baselines, it suf-fers from a robustness issue: its performance highlydepends on the similarity of a tm.
to address thisissue, we propose a novel training criterion inspiredby multiple-task learning and data augmentation.
experiments on tm-specialized tasks demonstrateits superiority over strong baselines in terms ofrunning time and bleu.
also, it is shown thata tm-based nmt model can advance the strongtransformer on general translation tasks like wmt..acknowledgments.
this work is supported by nsfc (grant no.
61877051).
we thank jiatao gu and mengzhouxia for providing their preprocessed datasets.
wealso thank the anonymous reviewers for providingvaluable suggestions and feedbacks..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2016. neural machine translation by jointlylearning to align and translate.
arxiv preprintarxiv:1409.0473..shai ben-david and reba schuller borbely.
2008.a notion of task relatedness yielding provablemultiple-task learning guarantees.
mach.
learn.,73(3):273–287..andrzej bialecki, robert muir, and grant ingersoll.
2020. apache lucene 4. in proceedings of the si-gir 2012 workshop on open source informationretrieval, osir@sigir 2012, pages 17–24..bram bulte and arda tezcan.
2019. neural fuzzy re-pair: integrating fuzzy matches into neural machinetranslation.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 1800–1809, florence, italy..deng cai, yan wang, huayang li, wai lam, andlemao liu.
2021. neural machine translation withmonolingual translation memory.
in proceedings ofthe 59th annual meeting of the association for com-putational linguistics and the 11th internationaljoint conference on natural language processing..chi chen, maosong sun, and yang liu.
2021. mask-align: self-supervised neural word alignment.
inthethe 59th annual meeting ofproceedings ofassociation for computational linguistics and the11th international joint conference on natural lan-guage processing..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-in proceedings of thetiple language translation..317853rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 1: long papers), pages 1723–1732..zi-yi dou and graham neubig.
2021. word alignmentby ﬁne-tuning embeddings on parallel corpora.
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 2112–2128..chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameteriza-tion of ibm model 2. in proceedings of the confer-ence of the north american chapter of the associ-ation of computational linguistics (naacl 2013),pages 644–648..david a van dyk and xiao-li meng.
2001. the art ofdata augmentation.
journal of computational andgraphical statistics, 10(1):1–50..marzieh fadaee, arianna bisazza, and christof monz.
2017. data augmentation for low-resource neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 2: short papers), pages 567–573..marzieh fadaee and christof monz.
2018. back-translation sampling by targeting difﬁcult words inin proceedings of theneural machine translation.
2018 conference on empirical methods in naturallanguage processing, pages 436–446..m. amin farajian, marco turchi, matteo negri, andmarcello federico.
2017. multi-domain neural ma-chine translation through unsupervised adaptation.
in proceedings of the second conference on ma-chine translation, pages 127–137..ignacio garcia.
2009. beyond translation memory:computers and the professional translator.
the jour-nal of specialised translation, 12(12):199–214..jiatao gu, yong wang, kyunghyun cho, and vic-search engine guided non-tor o.k.
li.
2018.parametric neural machine translation.
in proceed-ings of the 32nd aaai conference on artiﬁcial intel-ligence (aaai 2018), pages 5133–5140..masoud jalili sabet, philipp dufter, franc¸ois yvon,and hinrich sch¨utze.
2020. simalign: high qual-ity word alignments without parallel training data us-ing static and contextualized embeddings.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 1627–1643..urvashi khandelwal, angela fan, dan jurafsky, lukezettlemoyer, and mike lewis.
2021. nearest neigh-bor machine translation.
in proceedings of the 2021international conference on learning representa-tions..diederik p. kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..eliyahu kiperwasser and miguel ballesteros.
2018.scheduled multi-task learning: from syntax to trans-lation.
transactions of the association for computa-tional linguistics, 6:225–240..philipp koehn and jeaf senellart.
2010a.
convergenceof translation memory and statistical machine trans-in proceedings of amta workshop on mtlation.
research and the translation industry, pages 21–31..philipp koehn and jean senellart.
2010b.
convergenceof translation memory and statistical machine trans-in proceedings of amta workshop on mtlation.
research and the translation industry, pages 21–31..guanlin li, lemao liu, guoping huang, conghui zhu,and tiejun zhao.
2019. understanding data aug-mentation in neural machine translation: two per-spectives towards generalization.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5693–5699..xiaoqing li, jiajun zhang, and chengqing zong.
2016.one sentence one model for neural machine transla-tion.
arxiv preprint arxiv:1609.06490..lemao liu, hailong cao, taro watanabe, tiejun zhao,mo yu, and conghui zhu.
2012. locally trainingthe log-linear model for smt.
in proceedings of the2012 joint conference on empirical methods in nat-ural language processing and computational natu-ral language learning, pages 402–411..qiuxiang he, guoping huang, lemao liu, and li li.
2019. word position aware translation memory forin proceedings of theneural machine translation.
8th ccf international conference on natural lan-guage processing and chinese computing, pages367–379..lemao liu, masao utiyama, andrew finch, and ei-ichiro sumita.
2016. neural machine translationwith supervised attention.
in proceedings of col-ing 2016, the 26th international conference oncomputational linguistics: technical papers, pages3093–3102..guoping huang, lemao liu, xing wang, longyuewang, huayang li, zhaopeng tu, chengyan huang,and shuming shi.
2021.transmart: a practi-cal interactive machine translation system.
arxivpreprint arxiv:2105.13072..yang liu, kun wang, chengqing zong, and keh-yihsu.
2019. a uniﬁed framework and models for in-tegrating translation memory into phrase-based sta-tistical machine translation.
comput.
speech lang.,54:176–206..3179yanjun ma, yifan he, andy way, and josef van gen-abith.
2011. consistent translation using discrimi-native learning: a translation memory-inspired ap-in proceedings of the 49th annual meet-proach.
ing of the association for computational linguistics,pages 1239–1248..xinyi wang, hieu pham, zihang dai, and grahamneubig.
2018. switchout: an efﬁcient data aug-mentation algorithm for neural machine translation.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages856–861..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics acl 2002, pages 311–318..yiren wang, chengxiang zhai, and hany hassan.
2020. multi-task learning for multilingual neuralin proceedings of the 2020machine translation.
conference on empirical methods in natural lan-guage processing (emnlp), pages 1022–1034..mengzhou xia, guoping huang, lemao liu, andshuming shi.
2019. graph based translation mem-ory for neural machine translation.
in proceedingsof the 33rd aaai conference on artiﬁcial intelli-gence (aaai 2019), pages 7297–7304..jitao xu, josep crego, and jean senellart.
2020. boost-ing neural machine translation with similar trans-in proceedings of the 58th annual meet-lations.
ing of the association for computational linguistics,pages 1580–1590..jingyi zhang, masao utiyama, eiichro sumita, gra-ham neubig, and satoshi nakamura.
2018. guid-ing neural machine translation with retrieved trans-in proceedings of the 16th annuallation pieces.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies (naacl-hlt 2018), pages1325–1335..zhun zhong, liang zheng, guoliang kang, shaozi li,and yi yang.
2020. random erasing data augmen-in the thirty-fourth aaai conference ontation.
artiﬁcial intelligence, aaai, pages 13001–13008..xipeng qiu, jiayi zhao, and xuanjing huang.
2013.joint chinese word segmentation and pos taggingon heterogeneous annotated corpora with multiplein proceedings of the 2013 confer-task learning.
ence on empirical methods in natural languageprocessing, emnlp 2013, pages 658–668..douglas robinson.
2012. becoming a translator: anintroduction to the theory and practice of transla-tion.
routledge..rico sennrich, barry haddow, and alexandra birch.
2016a.
improving neural machine translation mod-in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (acl 2016), pages 1715–1725..michel simard and pierre isabelle.
2009. phrase-basedmachine translation in a computer-assisted transla-tion environment.
in proceedings of the twelfth ma-chine translation summit (mt summit xii), pages120–127..masao utiyama, graham neubig, takashi onishi, andeiichiro sumita.
2011. searching translation mem-ories for paraphrases.
in machine translation sum-mit, volume 13, pages 325–331..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008..kun wang, chengqing zong, and keh-yih su.
2013.integrating translation memory into phrase-basedin proceed-machine translation during decoding.
ings of the 51st annual meeting of the associationfor computational linguistics, pages 11–21..kun wang, chengqing zong, and keh-yih su.
2014.dynamically integrating cross-domain translationmemory into phrase-based machine translation dur-in proceedings of coling 2014,ing decoding.
the 25th international conference on computationallinguistics: technical papers, pages 398–408..3180