rewriter-evaluator architecture for neural machine translation.
yangming li♠ and kaisheng yao♥♠tencent ai lab♥ant groupnewmanli@tencent.com,kaisheng.yao@antgroup.com.
abstract.
a few approaches have been developed to im-prove neural machine translation (nmt) mod-els with multiple passes of decoding.
how-ever, their performance gains are limited be-cause of lacking proper policies to terminatethe multi-pass process.
to address this issue,we introduce a novel architecture of rewriter-evaluator.
translating a source sentence in-volves multiple rewriting passes.
in every pass,a rewriter generates a new translation to im-prove the past translation.
termination of thismulti-pass process is determined by a scoreof translation quality estimated by an evalua-tor.
we also propose prioritized gradient de-scent (pgd) to jointly and efﬁciently train therewriter and the evaluator.
extensive experi-ments on three machine translation tasks showthat our architecture notably improves the per-formances of nmt models and signiﬁcantlyoutperforms prior methods.
an oracle exper-iment reveals that it can largely reduce perfor-mance gaps to the oracle policy.
experimentsconﬁrm that the evaluator trained with pgd ismore accurate than prior methods in determin-ing proper numbers of rewriting..1.introduction.
encoder-decoder architecture (sutskever et al.,2014) has been widely used in natural languagegeneration, especially neural machine translation(nmt) (bahdanau et al., 2015; gehring et al., 2017;vaswani et al., 2017; zhang et al., 2019; kitaevet al., 2020).
given a source sentence, an en-coder ﬁrstly converts it into hidden representations,which are then conditioned by a decoder to producea target sentence.
in analogy to the developmentof statistical machine translation (smt) (och andney, 2002; shen et al., 2004; zhang and gildea,2008), some recent methods in nmt attempt to im-prove the encoder-decoder architecture with multi-pass decoding (xia et al., 2017; zhang et al., 2018;.
geng et al., 2018; niehues et al., 2016).
in thesemodels, more than one translation is generated fora source sentence.
except for the ﬁrst translation,each of the later translations is conditioned on theprevious one.
while these methods have achievedpromising results, they lack a proper terminationpoqlicy for this multi-turn process.
for instance,xia et al.
(2017); zhang et al.
(2018) adopt a ﬁxednumber of decoding passes, which is inﬂexibleand can be sub-optimal.
geng et al.
(2018) utilizereinforcement learning (rl) (sutton et al., 2000)to automatically decide the number of decodingpasses.
however, rl is known to be unstable dueto the high variance in gradient estimation (boyanand moore, 1995)..to address this problem, we introduce a novelarchitecture, rewriter-evaluator.
this architecturecontains a rewriter and an evaluator.
the trans-lation process involves multiple passes.
given asource sentence, at every turn, the rewriter gener-ates a new target sequence to improve the transla-tion from the prior pass, and the evaluator measuresthe translation quality to determine whether to endthe iterative rewriting process.
hence, the transla-tion process is continued until a certain condition ismet, such as no signiﬁcant improvement in the mea-sured translation quality.
in implementations, therewriter is a conditional language model (sutskeveret al., 2014) and the evaluator is a text matchingmodel (wang et al., 2017)..we also propose prioritized gradient descent(pgd) that facilitates training the rewriter and theevaluator both jointly and efﬁciently.
pgd uses apriority queue to store previous translation cases.
the queue stores translations with descending orderof their scores, computed from the evaluator.
thecapacity of the queue is limited to be a few times ofbatch size.
due to its limited size, the queue popsthose translations with high scores and only keepsthe translations with lower scores.
the samples in.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5701–5710august1–6,2021.©2021associationforcomputationallinguistics5701figure 1: general architecture of rewriter-evaluator..the queue are combined together with new casesfrom the training data to train the rewriter..rewriter-evaluator has been applied to improvetwo mainstream nmt models, rnnsearch (bah-danau et al., 2015) and transformer (vaswaniet al., 2017).
we have conducted extensive experi-ments on three translation tasks, nist chinese-to-english, wmt’18 chinese-to-english, andwmt’14 english-to-german.
the results showthat our architecture notably improves the perfor-mance of nmt models and signiﬁcantly outper-forms related approaches.
we conduct oracle ex-periment to understand the source of improvements.
the oracle can pick the best translation from all therewrites.
results indicate that the evaluator helpsour models achieve the performances close to theoracle, outperforming the methods of ﬁxing thenumber of rewriting turns.
compared against aver-aged performances using a ﬁxed number of rewrit-ing iterations, performance gaps to the oracle canbe reduced by 80.7% in the case of rnnsearch and75.8% in the case of transformer.
quantitatively,we ﬁnd the evaluator trained with pgd is signif-icantly more accurate in determining the optimalnumber of rewriting turns.
for example, whereasthe method in geng et al.
(2018) has 50.2% ac-curacy in wmt’14, the evaluator achieves 72.5%accuracy on transformer..2 rewriter-evaluator.
rewriter-evaluator consists of iterative processesinvolving a rewriting process ψ and an evaluationprocess φ. the process of translating an n-lengthsource sentence x = [x1, x2, · · · , xn] is an appli-cation of the above processes.
assume we are atthe k-th iteration (k ≥ 1).
the rewriter ψ gener-.
1., z(k−1)2.
1 , z(k).
, · · · , z(k−1)lk−1.
2 , · · · , z(k)ates a target sequence z(k) = [z(k)]lkgiven the source sentence x and the past trans-lation z(k−1) = [z(k−1)] fromthe (k − 1)-th turn.
lk and lk−1 are the sentencelengths.
the evaluator φ estimates the translationquality score q(k) of the new translation z(k), whichis used for determining whether to end the multi-turn process.
formally, the k-th pass of a transla-tion process is deﬁned as.
(cid:40)z(k) = ψ(x, z(k−1))q(k) = φ(x, z(k)).
..(1).
initially, z(0) and q(0) are respectively set as anempty string and −∞..the above procedure is repeatedly carried out un-til not much improvement in the estimated qualityscore can be achieved, i.e.,.
q(k) + (cid:15) < q(k−1), (cid:15) > 0,.
(2).
where (cid:15) is a small value tuned on the developmentset.
alternatively, the procedure is terminated ifa certain number of iterations k > 0 is reached.
in the former case, we adopt z(k−1) as the ﬁnaltranslation.
in the latter case, the last translationz(k) is accepted..2.1 architecture.
a general architecture of rewriter-evaluator us-ing encoder-decoder is illustrated in fig.
1. therewriter ψ consists of a source encoder f se, atarget decoder f t e, and a decoder gdec.
theevaluator φ shares encoders with the rewriter andcontains an estimator gest ..assume it is at the k-th pass.
firstly, the sourceencoder f se casts the source sentence x into word.
5702target encoder𝑓!
"source encoder𝑓#"decoder𝑔$"%estimator𝑔"#!target encoder𝑓!
"source encoder𝑓#"rewriter𝜓evaluator𝜙target sentence𝒛(’())source sentence𝒙target sentence𝒛(’)quality score𝑞(’())algorithm 1: prioritized gradient descent (pgd).
input: rewriter ψ, evaluator φ, training set t , batch size b, and expected iteration number e.output: well-trained rewriter ψ and well-trained evaluator φ..1 initialize an empty priority queue a with the capacity c ← b × e.2 while models are not converged do.
pop b cases with high quality scores from priority queue a and discard them.
randomly sample a b-sized batch of training cases s from t .
for (x, y) ∈ s do.
push the quadruple (x, y, [“sos”, “eos”], −∞) into queue a..initialize an empty priority queue d of limited size c.initialize an empty list f to collect samples for training.
for (x, y, z(k−1), r(k−1)) ∈ a do.
obtain translation z(k) and quality score q(k), respectively, using eq.
(5) and eq.
(6).
push sample (x, y, z(k), q(k)) into list f .
compute quality rate r(k) using eq.
(9).
push quadruple (x, y, z(k), r(k)) into queue d..optimize rewriter ψ with the samples in list f to reduce loss in eq.
(7).
optimize evaluator φ with the samples in list f to reduce loss in eq.
(8).
update priority queue a: a ← d..3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.representations hi, 1 ≤ i ≤ n:.
2.2 training criteria.
h = [h1; h2; · · · ; hn] = f se(x),.
(3).
where operation [; ] is row-wise vector concatena-tion.
similarly, the translation z(k−1) from the pre-vious turn k − 1 is encoded as.
p(k−1) = [p(k−1).
1.; p(k−1)2.; · · · ; p(k−1)lk−1.]
..(4).
= f t e(z(k−1)).
then, the decoder gdec of the rewriter ψ producesa new translation z(k) as.
z(k) = gdec(h, p(k−1))..(5).
ultimately, the evaluator φ scores the new transla-tion z(k) with the estimator gest :.
we represent the ground truth target sentence asa (m + 1)-length sequence y = [y0, y1, · · · , ym].
the rewriter ψ is trained via teacher forcing.
weuse oi to denote the probability of the i-th targetword, which is the prediction of feeding its priorwords [y0, y1, · · · , yi−1] into the decoder gdec.
the training loss for the rewriter is.
j ψ =.
(cid:88).
1≤i≤m.
− log(oi[yi])..(7).
where y0 = “[sos]” and ym = “[eos]”, markingthe ends of a target sentence..for the evaluator φ, we incur a hinge loss be-tween the translation score of the ground truth yand that of the current translation z(k) as.
(cid:40)p(k) = f t e(z(k)).
q(k) = gest (h, p(k)).
..(6).
j φ = max(0, 1 − q∗ + q(k)).
..(8).
(cid:40) q∗ = φ(x, y).
the implementation can be applied to a vari-ety of architectures.
the encoders, f se and f t e,can be any sequence model, such as cnn (kim,2014).
the decoder gdec is compatible with anylanguage model (e.g., transformer).
the estimatorgest is a text matching model, e.g., esim (chenet al., 2017).
in sec.
4, we apply this implementa-tion to improve generic nmt models..at training time, translation z(k) is generated viagreedy search, instead of beam search, to reducetraining time..3 prioritized gradient descent.
we present prioritized gradient descent (pgd) totrain the proposed architecture.
instead of the ran-dom sampling used in stochastic gradient descent.
5703uator φ. hence, the evaluator φ is jointly trainedwith the rewriter to learn discerning the quality oftranslations from the rewriter ψ, in order to helpthe rewriter reduce loss in eq.
(7)..pgd uses a large queue (b ×e) to aggregate thepast translations and newly sampled cases.
com-putationally, this is more efﬁcient than explicit btimes of rewriting to obtain samples.
this requiresextra memory space in exchange for lowing train-ing time.
in sec.
5.7, we will show that the addi-tional increase of training time by pgd is less than20%, which is tolerable..following sec.
2.1, we use rewriter-evaluator toimprove rnnsearch and transformer..rnnsearch w/ rewriter-evaluator.
the im-proved rnnsearch is illustrated in fig.
2. thetwo encoders (i.e., f se and f t e) and the decodergdec are gru (chung et al., 2014).
we omit com-putation details of these modules and follow theirsettings in bahdanau et al.
(2015).
note that, atevery decoding step, the hidden state of decoderis attended to not only hi, 1 ≤ i ≤ n but alsop(k−1)j., 1 ≤ j ≤ lk−1..we apply co-attention mechanism (parikh et al.,2016) to model the estimator f est .
firstly, wecapture the semantic alignment between the sourcesentence x and the translation z(k−1) as.
.
αi,j = ht(cid:88).
(cid:101)hi =.
(cid:101)p(k−1).
j.
=.
j.i wp(k−1).
j.exp(αi,j)j(cid:48) exp(αi,j(cid:48)).
(cid:80).
p(k−1)j.
(cid:88).
i.exp(αi,j)i(cid:48) exp(αi(cid:48),j).
(cid:80).
hi.
..(10).
then, we use average pooling to extract featuresand compute the quality score:.
q(k−1) = vt (cid:16) (cid:80).
i (cid:101)hin.⊕.
(cid:80).
j (cid:101)p(k−1)jlk−1.
(cid:17).
,.
(11).
where ⊕ is column-wise vector concatenation..transformer w/ rewriter-evaluator.
thetransformer (vaswani et al., 2017) is modiﬁed toan architecture in fig.
3. the input to the encodercontains a source sentence x, a special symbol“align”, and the past translation z(k−1):.
x(cid:48) = x (cid:12) [“align”] (cid:12) z(k−1),.
(12).
figure 2: rnnsearch with rewriter-evaluator..4 applications.
(sgd) (bottou and bousquet, 2008), pgd uses apriority queue to store previous training cases thatreceive low scores from the evaluator.
randomlysampled training cases together with those fromthe priority queue are used for training..details of pgd are illustrated in algorithm 1.initially, we set a priority queue a (1-st line) witha limited size c = b × e. b is the batch size.
e,the expected number of rewriting iterations, is setas k2 .
the queue a is ordered with a quality rate indescending order, where the top one correspondsto the highest rate.
the quality rate of a certainsample (x, y, z(k)) is computed as.
r(k) = (1 − ρ) ∗ bleu(z(k), y) + ρ ∗ q(k), (9).
where the weight ρ is controlled by an anneal-jj+1 with j being the current train-ing scheduleing epoch and bleu (papineni et al., 2002).
therate r(k) is dominated by bleu in the ﬁrst fewepochs, and is later dominated by the evaluationscore q(k) with an increasing number of epochs.
this design is to mitigate the cold start problemwhen training an evaluator φ. at every trainingepoch, pgd ﬁrstly discards a certain number ofprevious training samples with high quality rates(3-rd line) from queue a. it then replaces themwith newly sampled samples s (4-th to 6-th lines).
every sample (x, y, z(k−1), r(k−1)) in queue a isthen rewritten into a new translation z(k) by therewriter.
these are scored by the evaluator φ (10-thlines).
these new samples are used to respectivelytrain the rewriter ψ and the evaluator φ (14-th to15-th lines) with eq.
(7) and eq.
(8)..pgd keeps low-quality translations in the queuea for multi-pass rewriting until they are poppedout from queue a with high scores from the eval-.
5704rnnencoder(source)rnnencoder(target)𝑥!𝑥"𝑧!($%!)𝑧"($%!)𝒉!𝒉"𝒑!($%!)𝒑"($%!)co-attentionmechanismrnndecoder𝑞($%!)𝑧’($)𝑧"($)𝑧!
($)5.1 experimental setup.
for nist zh→en, the training set contains 1.25msentence pairs extracted from ldc corpora, includ-ing ldc2002e18, ldc2003e07, ldc2003e14,a portion of ldc2004t07, ldc2004t08, andldc2005t06.
we adopt nist 2002 (mt02) as thevalidation set.
we use nist 2003 (mt03), nist2004 (mt04), nist 2005 (mt05), and nist 2006(mt06) for tests.
for wmt’18 zh→en1, we use18.4m preprocessed data, with byte pair encoding(bpe) tokenization (sennrich et al., 2016).
we usenewstest2017 for validation and newstest2018 fortest.
for wmt’14 en→de2, following the samesetting as in vaswani et al.
(2017), we use 4.5mpreprocessed data that is tokenized via bpe with32k merge operations and a shared vocabulary forenglish and german.
we use newstest2013 fordevelopment and newstest2014 for test..we train all the models with 150k steps fornist zh→en, 300k steps for wmt’18 zh→en,and 300k steps for wmt’14 en→de.
we selectthe model that performs the best on validationsand report their performances on test sets.
us-ing multi-bleu.perl3, we measure case-insensitivebleu scores and case-sensitive ones for nistzh→en and wmt’14 en→de, respectively.
forwmt’18 zh→en, we use the case-sensitive bleuscores calculated by mteval-v13a.pl4.
the improve-ments of the proposed models over the baselinesare statistically signiﬁcant with a reject probabilitysmaller than 0.05 (koehn, 2004)..for rnnsearch, the dimensions of word embed-dings and hidden layers are both 600. encoder has3 layers and decoder has 2 layers.
dropout rate isset to 0.2. for transformer, we follow the settingof transformer-base in vaswani et al.
(2017).
bothmodels use beam size of 4 and the maximum num-ber of training tokens at every step is 4096. weuse adam (kingma and ba, 2014) for optimiza-tion.
in all the experiments, the proposed modelsrun on nvidia tesla v100 gpus.
for rewriter-evaluator, the maximum number of rewriting iter-ations k is 6 and termination threshold (cid:15) is 0.05.hyper-parameters are obtained by grid search, ex-cept for the transformer backbone..1http://www.statmt.org/wmt18/translation-task.html.
2http://www.statmt.org/wmt14/translation-task.html.
3https://github.com/moses-smt/mosesdecoder/blob/.
master/scripts/generic/multi-bleu.perl..4https://github.com/moses-smt/mosesdecoder/blob/.
master/scripts/generic/mteval-v13a.pl..figure 3: transformer with rewriter-evaluator..where operation (cid:12) denotes the concatenation oftwo sequences..the following mask matrix is applied to every.
layer in the encoder:.
.
.
0t1×n1.
1n×n11×n0lk−1×n 0t.
1×lk−1.
0n×lk−111×lk−11lk−1×lk−1.
.
 ..(13).
in this way, the words in x can’t attend to thosein z(k−1) and vice versa.
“align” can attend tothe words both in x and z(k−1).
this design is toavoid cross-sentence attention in encoder layers.
in earlier studies, we ﬁnd it slightly improves theperformances of models..we denote the representation for “align” inthe ﬁnal encoder layer as halign .
the estimatorf est obtains the quality score as.
q(k−1) = vt halign ,.
(14).
in which v is a learnable vector..5 experiments.
we have conducted extensive experiments onthree machine translation tasks: nist chinese-to-english (zh→en), wmt’18 chinese-to-english,and wmt’14 english-to-german (en→de).
theresults show that rewriter-evaluator signiﬁcantlyimproves the performances of nmt models andnotably outperforms prior post-editing methods.
oracle experiment veriﬁes the effectiveness of theevaluator.
termination accuracy analysis shows ourevaluator is much more accurate than prior meth-ods in determining the optimal number of rewritingturns.
we also perform ablation studies to explorethe effects of some components..
5705transformerencodertransformerdecoderdotproduct𝑥!𝑥"align𝑧!($%!)𝑧"($%!)𝒉!𝒉"𝒉’()*+𝒑!($%!)𝒑"($%!)𝑧!($)𝑧"($)𝑞($%!
)𝑧,($)method.
deliberation networks (xia et al., 2017)abd-nmt (zhang et al., 2018)adaptive multi-pass decoder (geng et al., 2018).
our work.
rnnsearchw/ rewriter-evaluatortransformerw/ rewriter-evaluator.
nist zh→en.
40.5641.2041.4340.42.avg.
mt03 mt04 mt05 mt0638.3137.6737.2037.8238.7138.0737.5938.0139.0538.5437.8638.3937.2037.6736.2936.7540.01 43.25 39.97 39.83 40.7746.7547.2247.6147.88 48.71 48.56 47.92 48.27.
46.58.
47.93.table 1: experiment results of the proposed models and all the baselines on nist zh→en..methodadaptive multi-pass decoder (geng et al., 2018).
wmt’14 en→de wmt’18 zh→en.
our work.
rnnsearchw/ rewriter-evaluatortransformerw/ rewriter-evaluator.
26.5525.7927.8627.5328.91.
22.3921.4723.7123.6525.08.table 2: experiment results on wmt’14 en→de and wmt’18 zh→en..5.2 results on nist chinese-to-english.
5.3 results on wmt tasks.
we adopt the following related baselines: 1) delib-eration networks (xia et al., 2017) adopts a sec-ond decoder to polish the raw sequence producedby the ﬁrst-pass decoder; 2) abd-nmt (zhanget al., 2018) uses a backward decoder to generate atranslation and a forward decoder to reﬁne it withattention mechanism; 3) adaptive multi-pass de-coder (geng et al., 2018) utilizes rl to model theiterative rewriting process..table 1 shows the results of the proposed mod-els and the baselines on nist.
baseline bleuscores are from geng et al.
(2018).
there are threeobservations.
firstly, rewriter-evaluator signif-icantly improves the translation quality of nmtmodels.
the averaged bleu score of rnnsearchis raised by 3.1% and that of transformer is in-creased by 1.05%.
secondly, the proposed archi-tecture notably outperforms prior multi-pass de-coding methods.
the performance of rnnsearchw/ rewriter-evaluator surpasses those of deliber-ation network by 2.46%, abd-nmt by 2.06%,and adaptive multi-pass decoder by 1.72%.
be-cause all of these systems use the same backboneof rnn-based nmt models, these results validatethat rewriter-evaluator is superior to other alter-native methods.
lastly, the proposed architecturecan improve transformer backbone by 1.05% onaverage, and the improvements are consistently ob-served on tasks from mt03 to mt06..to further conﬁrm the effectiveness of the pro-posed architecture, we make additional compar-isons on wmt’14 en→de and wmt’18 zh→en.
the results are demonstrated in table 2. becausethe above methods don’t have results on the twodatasets, we re-implement adaptive multi-pass de-coding for comparisons..these results are consistent with the observa-tions in sec.
5.2. we can see that the new architec-ture can improve bleu scores on both rnnsearchand transformer backbones.
for example, the im-provements on rnnsearch backbone are 2.13%on wmt’14 and 2.24% on wmt’18.
on trans-former backbone, scores are raised by 1.38% onwmt’14 and 1.43% on wmt’18 .
furthermore,rnnsearch w/ rewriter-evaluator outperformsadaptive multi-pass decoder by 1.31% and 1.32%,respectively, on the two tasks.
interestingly, the pro-posed architecture on rnnsearch backbone evensurpasses transformer on these two datasets.
forexample, the bleu score on wmt’14 increasesfrom 27.53% to 27.86%..5.4 oracle experiment.
we conduct oracle experiments on the test set ofwmt’14 en→de to understand potential improve-ments of our architecture.
an oracle selects the iter-ation that the corresponding rewrite has the highestbleu score.
its bleu scores are shown on the.
5706figure 4: the oracle experiment conducted on wmt’14 en→de..methodadaptive multi-pass decoderrnnsearch w/ rewriter-evaluatortransformer w/ rewriter-evaluator.
58.2775.2373.66.
30.6271.5872.46.
50.1860.5358.91.nist zh→en wmt’14 en→de wmt’18 zh→en.
table 3: pat scores of different methods on nist, wmt’14, and wmt’18..red dashed lines in fig.
4. the numbers on thegreen vertical bars are the bleu scores of adopt-ing a ﬁxed number of rewriting iterations.
theiraveraged number is shown on the dashed blue line.
bleu score from using our evaluator is shown onthe solid dark-blue line..results show that the evaluator, with 27.86%bleu score and 28.91 bleu score, is much betterthan the strategies of using a ﬁxed number of rewrit-ing turns.
the gaps between oracle and the aver-aged performance by rnnsearch and transformerwith ﬁxed iterations are 1.92% and 1.90%.
usingthe evaluator, these gaps are reduced relatively by80.7% for rnnsearch and 75.8% for transformer,respectively, down to 0.37% and 0.46%.
theseresults show that the evaluator is able to learn anappropriate termination policy, approximating theperformances of oracle policy..5.5 termination accuracy analysis.
we deﬁne a metric, percentage of accurate termina-tions (pat), to measure how precise a terminationpolicy can be.
pat is computed as.
1|u |.
(cid:88).
(x,y)∈u.
δ(wq(x, y) = wb(x, y)),.
(15).
param.
sharing k nist wmt’14 wmt’18.
(cid:55)(cid:51)(cid:51)(cid:51)(cid:51).
6 42.252 41.834 42.376 42.798 42.83.
26.1725.6426.2126.4326.37.
23.8823.2623.9824.1124.09.table 4: ablation studies conducted on the validationsets of nist, wmt’14, and wmt’18..maxk bleu(z(k), y).
the translations z(k), 1 ≤k ≤ k and their scores q(k), 1 ≤ k ≤ k areobtained using eq.
5 and eq.
6..for fair comparisons, the maximum number ofrewritings is set to 6 for both rewriter-evaluatorand adaptive multi-pass decoder (geng et al.,2018).
results in table 3 show that pat scoresfrom rewriter-evaluator are much higher thanthose of adaptive multi-pass decoder.
for in-stance, rnnsearch w/ rewriter-evaluator sur-passes adaptive multi-pass decoder by 40.96%on wmt’14 and 10.35% on wmt’18..5.6 ablation studies.
table 4 shows the results of ablation studies onnist, wmt’14, and wmt’18..where δ is the indicator function that outputs 1 ifits argument is true and 0 otherwise.
for each pair(x, y) in the test set u , wq(x, y) is the turn in-dex k with the highest quality score maxk q(k) andwb(x, y) is the one with the highest bleu score.
parameter sharing.
the encoders from eq.
(3)and eq.
(4) are shared between the rewriter andthe evaluator.
we ﬁnd this improves the perfor-mances of the proposed models.
for example, onnist, sharing encoders increases our bleu score.
5707123456rewriting turns (rnnsearch w/ rewriter-evaluator)25.025.526.026.527.027.528.028.529.0bleu score25.6126.5826.9126.0326.5826.21oracle28.23evaluator27.86avg.26.31123456rewriting turns (transformer w/ rewriter-evaluator)26.026.527.027.528.028.529.029.530.027.2327.0127.5428.0327.7427.29oracle29.37evaluator28.91avg.27.47method.
rnnsearchw/ rewriter-evaluatortransformerw/ rewriter-evaluator.
test.
wmt’14 en→detraining7h56m 11m26s9h17m 39m50s5h23m 14m11s6h36m 52m02s.
table 5: running time comparisons on wmt’14..from 42.25% to 42.79% with the same maximumiteration number of k..maximum number of iterations.
increasingthe maximum number of turns k generally im-proves the bleu scores.
for instance, on nist,k = 8 outperforms k = 2 by 1.0%, k = 4 by0.46%, and k = 6 by 0.04%.
however, describedin sec.
5.7, large k (e.g., 8) can increase inferencetime cost.
moreover, additional gains in perfor-mance from k = 8 is small.
we therefore setk = 6 by default..5.7 running time comparisons.
while achieving improved translation quality, themodels are trained with multiple passes of trans-lation.
therefore, a natural question is on the in-crease of training time and test time.
we reportresults on 4 gpus with the maximum rewritingturns k = 6 and the beam size set to 8. results onwmt’14 are listed in table 5..it shows that rewriter-evaluator increases thetest time by approximately 4 times, because ofmultiple passes of decoding.
however, trainingtime is only relatively increased by 15% and 18%,respectively on rnnsearch and transformer, dueto the large priority queue used in pgd to storeprevious translation cases..6 related work.
multi-pass decoding has been well studied in sta-tistical machine translation (brown et al., 1993;koehn et al., 2003, 2007; och and ney, 2004; chi-ang, 2005; dyer et al., 2013).
och (2003); ochand ney (2002) propose training models with mini-mum error rate criterion on lattices from ﬁrst-passdecoder.
marie and max (2015) introduce an itera-tive method to reﬁne search space generated fromsimple feature with additional information frommore complex feature.
shen et al.
(2004) investi-gate reranking of hypothesis using neural modelstrained with discriminative criterion.
neubig et al..(2015) propose to reconﬁrm effectiveness of rerank-ing.
chen et al.
(2008) present a regeneration ofsearch space from techniques such as n-gram ex-pansion.
these approaches are however appliedto shallow models such as log-linear models (ochand ney, 2002)..our work is closely related to recent efforts inmulti-pass decoding on nmt.
in these recent works(xia et al., 2017; zhang et al., 2018; geng et al.,2018), the models generate multiple target sen-tences for a source sentence and, except for theﬁrst one, each of them is based on the sentence gen-erated in the previous turn.
for example, xia et al.
(2017) propose deliberation networks that usesa second decoder to polish the raw sequence pro-duced by the ﬁrst-pass decoder.
while these meth-ods have achieved promising results, they lack aproper termination policy for the multi-pass transla-tion process.
zhang et al.
(2018) adopt a predeﬁnednumber of decoding passes, which is not ﬂexible.
geng et al.
(2018) incorporate post-editing mecha-nism into nmt model via rl.
however, rl can beunstable for training because of the high variancein gradient estimation.
the lack of a proper termi-nation policy results in premature terminations orover-translated sentences, which can largely limitthe performance gains of these methods..7 conclusion.
this paper has introduced a novel architecture,rewriter-evaluator, that achieves a proper termi-nation policy for multi-pass decoding in nmt.
atevery translation pass, given the source sentenceand its past translation, a rewriter generates a newtranslation, aiming at making further performanceimprovements over the past translations.
an evalu-ator estimates the translation quality to determinewhether to complete this iterative rewriting pro-cess.
we also propose pgd that facilitates train-ing the rewriter and the evaluator both jointly andefﬁciently.
we have applied rewriter-evaluatorto improve mainstream nmt models.
extensiveexperiments have been conducted on three transla-tion tasks, nist zh→en, wmt’18 zh→en, andwmt’14 en→de, showing that our architecturenotably improves the results of nmt models andsigniﬁcantly outperforms other related methods.
an oracle experiment and a termination accuracyanalysis show that the performance gains can beattributed to the improvements in completing therewriting process at proper iterations..5708acknowledgments.
this work was done when the ﬁrst author did intern-ship at ant group.
we thank anonymous reviewersfor their valuable suggestions..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in international con-ference on learning representation..l´eon bottou and olivier bousquet.
2008. the trade-offs of large scale learning.
in advances in neuralinformation processing systems, volume 20, pages161–168.
curran associates, inc..justin a boyan and andrew w moore.
1995. gener-alization in reinforcement learning: safely approx-in advances in neuralimating the value function.
information processing systems, pages 369–376..peter f. brown, stephen a. della pietra, vincent j.della pietra, and robert l. mercer.
1993. the math-ematics of statistical machine translation: parameterestimation.
computational linguistics, 19(2):263–311..boxing chen, min zhang, aiti aw, and haizhou li.
2008. regenerating hypotheses for statistical ma-in proceedings of the 22nd in-chine translation.
ternational conference on computational linguis-tics (coling 2008), pages 105–112, manchester, uk.
coling 2008 organizing committee..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstm forin proceedings of thenatural language inference.
55th annual meeting of the association for compu-tational linguistics, pages 1657–1668, vancouver,canada.
association for computational linguistics..david chiang.
2005. a hierarchical phrase-basedin pro-model for statistical machine translation.
ceedings of the 43rd annual meeting of the as-sociation for computational linguistics (acl’05),pages 263–270, ann arbor, michigan.
associationfor computational linguistics..kyunghyun cho, bart van merri¨enboer, dzmitry bah-danau, and yoshua bengio.
2014. on the propertiesof neural machine translation: encoder–decoder ap-proaches.
in proceedings of ssst-8, eighth work-shop on syntax, semantics and structure in statisti-cal translation, pages 103–111, doha, qatar.
asso-ciation for computational linguistics..junyoung chung, caglar gulcehre, kyunghyun cho,and yoshua bengio.
2014. empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arxiv preprint arxiv:1412.3555..chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameter-in proceedings of theization of ibm model 2.
2013 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 644–648, at-lanta, georgia.
association for computational lin-guistics..jonas gehring, michael auli, david grangier, denisyarats, and yann n. dauphin.
2017. convolutionalin proceedingssequence to sequence learning.
of the 34th international conference on machinelearning, volume 70 of proceedings of machinelearning research, pages 1243–1252, internationalconvention centre, sydney, australia.
pmlr..xinwei geng, xiaocheng feng, bing qin, and tingliu.
2018. adaptive multi-pass decoder for neuralin proceedings of the 2018machine translation.
conference on empirical methods in natural lan-guage processing, pages 523–532..yoon kim.
2014..convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in inter-national conference on learning representations..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..philipp koehn, franz j. och, and daniel marcu.
2003.statistical phrase-based translation.
in proceedingsof the 2003 human language technology confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 127–133..benjamin marie and aur´elien max.
2015. multi-passdecoding with complex feature guidance for statis-in proceedings of thetical machine translation..5709hlt-naacl 2004, pages 177–184, boston, mas-sachusetts, usa.
association for computationallinguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..richard s sutton, david a mcallester, satinder psingh, and yishay mansour.
2000. policy gradientmethods for reinforcement learning with function ap-proximation.
in advances in neural information pro-cessing systems, pages 1057–1063..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..zhiguo wang, wael hamza, and radu florian.
2017.bilateral multi-perspective matching for natural lan-guage sentences.
in proceedings of the twenty-sixthinternational joint conference on artiﬁcial intelli-gence, ijcai-17, pages 4144–4150..yingce xia, fei tian, lijun wu, jianxin lin, tao qin,nenghai yu, and tie-yan liu.
2017. deliberationnetworks: sequence generation beyond one-pass de-coding.
in advances in neural information process-ing systems, pages 1784–1794..hao zhang and daniel gildea.
2008. efﬁcient multi-pass decoding for synchronous context free gram-mars.
in proceedings of acl-08: hlt, pages 209–217, columbus, ohio.
association for computa-tional linguistics..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019. bridging the gap between trainingand inference for neural machine translation.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4334–4343, florence, italy.
association for computationallinguistics..xiangwen zhang, jinsong su, yue qin, yang liu, ron-grong ji, and hongji wang.
2018. asynchronousbidirectional decoding for neural machine transla-in thirty-second aaai conference on artiﬁ-tion.
cial intelligence..53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 2: short papers), pages 554–559, beijing,china.
association for computational linguistics..graham neubig, makoto morishita, and satoshi naka-mura.
2015. neural reranking improves subjectivequality of machine translation: naist at wat2015.
in proceedings ofthe 2nd workshop on asiantranslation (wat2015), pages 35–41, kyoto, japan.
workshop on asian translation..jan niehues, eunah cho, thanh-le ha, and alexwaibel.
2016. pre-translation for neural machinetranslation.
arxiv preprint arxiv:1610.05243..franz josef och.
2003. minimum error rate training instatistical machine translation.
in proceedings of the41st annual meeting of the association for compu-tational linguistics, pages 160–167, sapporo, japan.
association for computational linguistics..franz josef och and hermann ney.
2002. discrimina-tive training and maximum entropy models for sta-in proceedings of thetistical machine translation.
40th annual meeting of the association for com-putational linguistics, pages 295–302, philadelphia,pennsylvania, usa.
association for computationallinguistics..franz josef och and hermann ney.
2004. the align-ment template approach to statistical machine trans-lation.
computational linguistics, 30:417–449..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ankur parikh, oscar t¨ackstr¨om, dipanjan das, andjakob uszkoreit.
2016. a decomposable attentionin proceed-model for natural language inference.
ings of the 2016 conference on empirical methodsin natural language processing, pages 2249–2255,austin, texas.
association for computational lin-guistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..libin shen, anoop sarkar, and franz josef och.
2004.discriminative reranking for machine translation.
in proceedings of the human language technol-ogy conference of the north american chapterof the association for computational linguistics:.
5710