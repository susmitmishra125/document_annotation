accelerating bert inference for sequence labeling via early-exit.
xiaonan li, yunfan shao, tianxiang sun, hang yan, xipeng qiu∗, xuanjing huangshanghai key laboratory of intelligent information processing, fudan universityschool of computer science, fudan university{lixn20, yfshao19, txsun19, hyan19, xpqiu, xjhuang}@fudan.edu.cn.
abstract.
both performance and efﬁciency are crucialfactors for sequence labeling tasks in manyreal-world scenarios.
although the pre-trainedmodels (ptms) have signiﬁcantly improvedthe performance of various sequence labelingtasks, their computational cost is expensive.
to alleviate this problem, we extend the re-cent successful early-exit mechanism to accel-erate the inference of ptms for sequence label-ing tasks.
however, existing early-exit mech-anisms are speciﬁcally designed for sequence-level tasks, rather than sequence labeling.
inthis paper, we ﬁrst propose sentee: a sim-ple extension of sentence-level early-exitfor sequence labeling tasks.
to further reducecomputational cost, we also propose tokee:a token-level early-exit mechanism that al-lows partial tokens to exit early at differentlayers.
considering the local dependency in-herent in sequence labeling, we employed awindow-based criterion to decide for a tokenwhether or not to exit.
the token-level early-exit brings the gap between training and infer-ence, so we introduce an extra self-samplingﬁne-tuning stage to alleviate it.
the extensiveexperiments on three popular sequence label-ing tasks show that our approach can save up to66%∼75% inference cost with minimal perfor-mance degradation.
compared with compet-itive compressed models such as distilbert,our approach can achieve better performanceunder the same speed-up ratios of 2×, 3×, and4×.1.
chinese word segmentation and semantic role la-beling.
these tasks are usually fundamental andhighly time-demanding, therefore, apart from per-formance, their inference efﬁciency is also veryimportant..the past few years have witnessed the prevailingof pre-trained models (ptms) (qiu et al., 2020)on various sequence labeling tasks (nguyen et al.,2020; ke et al., 2020; tian et al., 2020; menggeet al., 2020).
despite their signiﬁcant improve-ments on sequence labeling, they are notorious forenormous computational cost and slow inferencespeed, which hinders their utility in real-time sce-narios or mobile-device scenarios..recently, early-exit mechanism (liu et al., 2020;xin et al., 2020; schwartz et al., 2020; zhou et al.,2020) has been introduced to accelerate inferencefor large-scale ptms.
in their methods, each layerof the ptm is coupled with a classiﬁer to pre-dict the label for a given instance.
at inferencestage, if the prediction is conﬁdent 2 enough atan earlier time, it is allowed to exit without pass-ing through the entire model.
figure 1(a) gives anillustration of early-exit mechanism for text classiﬁ-cation.
however, most existing early-exit methodsare targeted at sequence-level prediction, such astext classiﬁcation, in which the prediction and itsconﬁdence score are calculated over a sequence.
therefore, these methods cannot be directly appliedto sequence labeling tasks, where the prediction istoken-level and the conﬁdence score is required foreach token..in this paper, we aim to extend the early-exitmechanism to sequence labeling tasks.
first, weproposed the sentence-level early-exit (sen-tee), which is a simple extension of existing early-exit methods.
sentee allows a sequence of to-kens to exit together once the maximum uncertainty.
2in this paper, conﬁdent prediction indicates that the un-.
1.introduction.
sequence labeling plays an important role in natu-ral language processing (nlp).
many nlp taskscan be converted to sequence labeling tasks, such asnamed entity recognition, part-of-speech tagging,.
∗corresponding author.
1our implementation is publicly available at https://.
github.com/leesureman/sequence-labeling-early-exit..certainty of it is low..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages189–199august1–6,2021.©2021associationforcomputationallinguistics189lem, we introduce an additional ﬁne-tuning stagethat samples the token’s halting layer based on itsuncertainty and copies its representation to upperlayers during training.
we conduct extensive ex-periments on three sequence labeling tasks: ner,pos tagging, and cws.
experimental results showthat our approach can save up to 66% ∼75% infer-ence cost with minimal performance degradation.
compared with competitive compressed modelssuch as distilbert, our approach can achieve bet-ter performance under speed-up ratio of 2×, 3×,and 4×..2 bert for sequence labeling.
recently, ptms (qiu et al., 2020) have become themainstream backbone model for various sequencelabeling tasks.
the typical framework consists of abackbone encoder and a task-speciﬁc decoder..encoderin this paper, we use bert (devlinet al., 2019) as our backbone encoder .
the ar-chitecture of bert consists of multiple stackedtransformer layers (vaswani et al., 2017)..given a sequence of tokens x1, · · · , xn , the hid-den state of l-th transformer layer is denoted byh(l) = [h(l)n ], and h(0) is the bert in-put embedding..1 , · · · , h(l).
decoder usually, we can predict the label foreach token according to the hidden state of the toplayer.
the probability of labels is predicted by.
p = f (wh(l)) ∈ rn ×c,.
(1).
where n is the sequence length, c is the numberof labels, l is the number of bert layers, w is alearnable matrix, and f (·) is a simple softmax clas-siﬁer or conditional random ﬁeld (crf) (laffertyet al., 2001).
since we focus on inference accelera-tion and ptm performs well enough on sequencelabeling without crf (devlin et al., 2019), we donot consider using such a recurrent structure..3 early-exit for sequence labeling.
the inference speed and computational costs ofptms are crucial bottlenecks to hinder their appli-cation in many real-world scenarios.
in many tasks,the representations at an earlier layer of ptmsare usually adequate to make a correct prediction.
therefore, early-exit mechanisms (liu et al., 2020;xin et al., 2020; schwartz et al., 2020; zhou et al.,2020) are proposed to dynamically stop inference.
(a) early-exit for text classiﬁcation.
(b) sentence-level early-exit for sequence labeling.
(c) token-level early-exit for sequence labeling.
represents simply copying,.
figure 1: early-exit for text classiﬁcation and se-represents the self-attentionquence labeling, whereconnection,represents the conﬁdent and uncertain prediction.
thedarker the hidden state block, the deeper layer it is from.
due to space limit, the window-based uncertainty is notreﬂected..and.
of the tokens is below a threshold.
despite its ef-fectiveness, we ﬁnd it redundant for most tokensto update the representation at each layer.
thus,we proposed a token-level early-exit (tokee)that allows part of tokens that get conﬁdent predic-tions to exit earlier.
figure 1(b) and 1(c) illustrateour proposed sentee and tokee.
consideringthe local dependency inherent in sequence labelingtasks, we decide whether a token could exit basedon the uncertainty of a window of its context in-stead of itself.
for tokens that are already exited,we do not update their representation but just copyit to the upper layers.
however, this will introducea train-inference discrepancy.
to tackle this prob-.
190...itisnotgooditisnotgooditisnotgoodlayer(i+1)layer(i+2)layers(>i+2)confident?yes😎confident?no😕early-exit predictionconfident?no😕layer(i)negativenegativepositiveconfident?(pooling)confident?(pooling)no😕confident?
(pooling)yes😎early-exit predictiondellfoundeddellin1984dellfoundeddellin1984dellfoundeddellin1984no😕b-org😕b-org 😕o 😎b-time 😎o 😎b-org😎b-per 😕o 😎b-time 😎o 😎b-org😎b-per 😎o 😎b-time 😎o 😎...layer(i+1)layer(i+2)layers(>i+2)layer(i)dellfoundeddellin1984dellfoundeddellin1984dellfoundeddellin1984b-org😕b-org 😕o 😎b-time 😎o 😎b-org😎b-per 😎o 😎b-time 😎o 😎b-org😎b-per 😕o 😎b-time 😎o 😎...layer(i+1)layer(i+2)layers(>i+2)layer(i)yes😎early-exit prediction😎😕on the backbone model and make prediction withintermediate representation..however, these existing early-exit mechanismsare built on sentence-level prediction and unsuit-able for token-level prediction in sequence label-ing tasks.
in this section, we propose two early-exist mechanisms to accelerate the inference forsequence labeling tasks..3.1 token-level off-ramps.
to extend early-exit to sequence labeling, we cou-ple each layer of the ptm with token-level s thatcan be simply implemented as a linear classiﬁer.
once the off-ramps are trained with the golden la-bels, the instance has a chance to be predicted andexit at an earlier time instead of passing throughthe entire model..given a sequence of tokens x = x1, · · · , xn ,we can make predictions by the injected off-rampsat each layer.
for an off-ramp at l-th layer, the labeldistribution of all tokens is predicted by.
p(l) = f (l)(x; θ).
= softmax(wh(l)),.
(2).
(3).
where w is a learnable matrix, f (l) is the token-1 , · · · , p(l)level off-ramp at l-th layer, p(l) = [p(l)n ],p(l)n ∈ rc, indicates the predicted label distribu-tion at the l-th off-ramp for each token..uncertainty of the off-ramp with the predic-tion for each token at hand, we can calculate theuncertainty for each token as follows,.
u(l)n =.
−p(l).
n · log p(l)nlog c.,.
(4).
where p(l)the n-th token..n is the label probability distribution for.
3.2 early-exit strategies.
in the following sections, we will introduce twoearly-exit mechanisms for sequence labeling, atsentence-level and token-level..3.2.1 sentee: sentence-level early-exit.
sentence-level early-exit (sentee) is a simpleextension for sequential labeling tasks based onexisting early-exit approaches.
sentee allows asequence of tokens to exit together if their uncer-tainty is low enough.
therefore, sentee is toaggregate the uncertainty for each token to obtain.
an overall uncertainty for the whole sequence.
herewe perform a straight-forward but effective method,i.e., conduct max-pooling3 over uncertainties of allthe tokens,.
u(l) = max{u(l).
1 , · · · , u(l).
n },.
(5).
where u(l) represents the uncertainty for the wholeif u(l) < δ where δ is a pre-deﬁnedsentence.
threshold, we let the sentence exit at layer l. theintuition is that only when the model is conﬁdentof its prediction for the most difﬁcult token, thewhole sequence could exit..3.2.2 tokee: token-level early-exitdespite the effectiveness of sentee (see table 1),we ﬁnd it redundant for most simple tokens to befed into the deep layers.
the simple tokens thathave been correctly predicted in the shallow layercan not exit (under sentee) because the uncer-tainty of a small number of difﬁcult tokens is stillabove the threshold.
thus, to further accelerate theinference for sequence labeling tasks, we proposea token-level early-exit (tokee) method that al-lows simple tokens with conﬁdent predictions toexit early..window-based uncertainty note that a preva-lent problem in sequence labeling tasks is the localdependency (or label dependency).
that is, the la-bel of a token heavily depends on the tokens aroundit.
to that end, the calculation of the uncertainty fora given token should not only be based on itself butalso its context.
motivated by this, we proposed awindow-based uncertainty criterion to decide for atoken whether or not to exit at the current layer.
inparticular, the uncertainty for the token xn at l-thlayer is deﬁned as.
n = max{u(l)u(cid:48)(l).
n−k, · · · , u(l).
n+k},.
(6).
where k is a pre-deﬁned window size.
then we useu(cid:48)(l)n to decide whether the n th token can exit atlayer l, instead of u(l)n .
note that window-based un-certainty is equivalent to sentence-level uncertaintywhen k equals to the sentence length..halt-and-copy for tokens that have exited, theirrepresentation would not be updated in the upperlayers, i.e., the hidden states of exited tokens are.
3we also tried average-pooling, but it brings drastic per-formance drop.
we ﬁnd that the average uncertainty over thesequence is often overwhelmed by lots of easy tokens and thiscauses many wrong exits of difﬁcult tokens..191directly copied to the upper layers.4 such a halt-and-copy mechanism is rather intuitive in two-fold:.
• halt..if the uncertainty of a token is verysmall, there are also few chances that its pre-diction will be changed in the following layers.
so it is redundant to keep updating its repre-sentation..• copy..if the representation of a token canbe classiﬁed into a label with a high degreeof conﬁdence, then its representation alreadycontains the label information.
so we can di-rectly copy its representation into the upperlayers to help predict the labels of other to-kens..these exited tokens will not attend to other to-kens at upper layers but can still be attended byother tokens thus part of the layer-speciﬁc queryprojections in upper layers can be omitted.
by this,the computational complexity in self-attention is re-duced from o(n 2d) to o(n m d), where m (cid:28) nis the number of tokens that have not exited.
be-sides, the computational complexity of the point-wise ffn can also be reduced from o(n d2) too(m d2)..the halt-and-copy mechanism is also similar tomulti-pass sequence labeling paradigm, in whichthe tokens are labeled their in order of difﬁculty(easiest ﬁrst).
however, the copy mechanism re-sults in a train-inference discrepancy.
that is, alayer never processed the representation from itsnon-adjacent previous layers during training.
to al-leviate the discrepancy, we further proposed an ad-ditional ﬁne-tuning stage, which will be discussedin section 3.3.2..3.3 model training.
in this section, we describe the training process ofour proposed early-exit mechanisms..3.3.1 fine-tuning for sentee.
for sentence-level early-exit, we follow prior early-exit work for text classiﬁcation to jointly train theadded off-ramps.
for each off-ramp, the loss func-tion is as follows,.
ll =.
h.yn, f (l)(x; θ)n.(7).
(cid:17).
,.
(cid:16).
n(cid:88).
n=1.
4for english sequence labeling, we use the ﬁrst-poolingto get the representation of the word.
if a word exits, we willhalt-and-copy its all wordpieces..where h is the cross-entropy loss function, n isthe sequence length.
the total loss function foreach sample is a weighted sum of the losses for allthe off-ramps,.
ltotal =.
(cid:80)l.l=1 wlll(cid:80)ll=1 wl.
,.
(8).
where wl is the weight for the l-th off-ramp and lis the number of backbone layers.
following (zhouet al., 2020), we simply set wl = l. in this way,the deeper an off-ramp is, the weight of its loss isbigger, thus each off-ramp can be trained jointly ina relatively balanced way..3.3.2 fine-tuning for tokee.
since we equip halt-and-copy in tokee, the com-mon joint training off-ramps are not enough.
be-cause the model never conducts halt-and-copy intraining but does in inference.
in this stage, weaim to train the model to use the hidden state fromdifferent previous layers but not only the previousadjacent layer, just like in inference..random sampling a direct way is to uniformlysample halting layers of tokens.
however, haltinglayers at the inference are not random but dependson the difﬁculty of each token in the sequence.
sorandom sampling halting layers also causes the gapbetween training and inference..self-sampling instead, we use the ﬁne-tunedmodel itself to sample the halting layers.
for everysample in each training epoch, we will randomlysample a window size and threshold for it, andthen we can conduct tokee on the trained model,under the window size and threshold, without halt-and-copy.
thus we get the exiting layer of each to-ken, and we use it to re-forward the sample, by halt-ing and copying each token in the correspondinglayer.
in this way, the exiting layer of a token cancorrespond to its difﬁculty.
the deeper a token’sexiting layer is, the more difﬁcult it is.
becausewe sample the exiting layer using the model itself,we think the gap between training and inferencecan be further shrunk.
to avoid over-ﬁtting duringfurther training, we prevent the training loss fromfurther reducing, similar with the ﬂooding mecha-nism used by ishida et al.
(2020).
we also employthe sandwich rule to stabilize this training stage(yu and huang, 2019).
we compare self-samplingwith random sampling in section 4.4.4..192conll2003.
twitter.
ontonotes 4.0.weibo.
clue ner.
f1(∆).
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
1.00×.
79.18.
1.00×.
66.15.
1.00×.
79.11.
1.00×.
task.
dataset/model.
bert.
lstm-crf.
distilbert-6lsenteetokee.
distilbert-4lsenteetokee.
distilbert-3lsenteetokee.
task.
dataset/model.
bertlstm-crf.
distilbert-6lsenteetokee.
distilbert-4lsenteetokee.
distilbert-3lsenteetokee.
x2∼.
x3∼.
x4∼.
x2∼.
x3∼.
x4∼.
91.20.
-0.29.
-1.42+0.01-0.09.
-1.57-0.75-0.32.
-2.78-6.51-1.44.acc.
91.51-0.92.
-0.17-0.49-0.13.
-0.93-1.98-0.67.
-1.04-2.96-2.17.
1.00×.
-.
2.00×2.07×2.02×.
3.00×3.02×3.03×.
4.00×4.00×3.94×.
1.00×-.
2.00×2.01×1.96×.
3.00×3.03×2.95×.
4.00×3.98×3.89×.
77.97.
-12.65.
-1.79-0.82-0.20.
-3.37-5.54-1.32.
-5.08-11.50-3.96.
96.20-2.48.
-0.27-0.19-0.07.
-0.74-1.63-0.30.
-1.34-2.37-1.29.
-.
2.00×2.03×2.00×.
3.00×3.03×2.98×.
4.00×4.14×3.86×.
1.00×-.
2.00×1.95×2.04×.
3.00×3.12×3.01×.
4.00×3.91×3.98×.
ner.
-7.37.
-3.76-1.70-0.20.
-8.79-8.90-1.48.
-12.77-14.40-3.81.
95.04-6.30.
-0.19-0.35-0.17.
-1.21-2.47-1.04.
-4.16-4.78-4.03.
-.
2.00×1.94×2.00×.
3.00×2.95×3.02×.
4.00×3.78×3.81×.
1.00×-.
2.00×1.87×1.93×.
3.00×2.95×2.89×.
4.00×3.96×3.89×.
-9.40.
-1.95-0.23-0.15.
-5.92-2.51-0.70.
-9.05-8.31-2.04.
98.48-0.79.
-0.12-0.02-0.05.
-0.19-0.11-0.06.
-0.21-0.83-0.14.
-.
2.00×2.11×2.10×.
3.00×2.87×3.04×.
4.00×3.86×3.92×.
1.00×-.
2.00×1.99×2.02×.
3.00×3.03×3.01×.
4.00×4.02×3.85×.
-7.75.
-2.05-0.22-0.24.
-5.67-6.17-0.92.
-8.22-14.95-3.16.
98.04-3.03.
-0.15-0.11-0.07.
-0.43-0.47-0.20.
-1.01-1.23-0.53.
-.
2.00×2.04×1.99×.
3.00×2.98×2.91×.
4.00×3.91×4.19×.
1.00×-.
2.00×2.13×2.10×.
3.00×2.99×3.01×.
4.00×3.98×3.97×.
pos tagging.
cws.
ark twitter.
ctb5 pos.
ud pos.
ctb5 seg.
ud seg.
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
f1(∆).
speedup.
table 1: comparison on bert model.
the performance of lstm-crf is from previous paper (tian et al., 2020;mengge et al., 2020; yan et al., 2019; gui et al., 2018), and others are implemented by ourselves..task.
dataset/model.
ner.
pos.
cws.
ontonotes4.0f1(∆)/speedup.
ctb5 posf1(∆)/speedup.
ud segf1(∆)/speedup.
roberta-base.
roberta.
79.32 / 1.00×.
96.29 / 1.00×.
97.91 / 1.00×.
roberta 6lsenteetokee.
roberta 4lsenteetokee.
-4.03 / 2.00×-0.55 / 1.98×-0.19 / 2.01×.
-11.97 / 3.00×-5.61 / 3.03×-0.58 / 3.05×.
-0.11 / 2.00×-0.08 / 1.85×-0.06 / 1.98×.
-1.59 / 3.00×-1.02 / 2.80×-0.51 / 3.01×.
-0.28 / 2.00×-0.08 / 2.01×-0.02 / 1.97×.
-0.76 / 3.00×-0.51 / 2.99×-0.12 / 3.00×.
albert-base.
albert.
76.24 / 1.00×.
95.73 / 1.00×.
97.00 / 1.00×.
albert 6lsenteetokee.
albert 4lsenteetokee.
-5.78 / 2.00×-1.25 / 2.01×-0.43 / 1.97×.
-12.93 / 3.00×-6.90 / 2.83×-0.87 / 2.87×.
-0.42 / 2.00×-0.23 / 1.97×-0.08 / 2.00×.
-2.15 / 3.00×-1.47 / 3.11×-0.58 / 2.89×.
-0.20 / 2.00×-0.02 / 1.98×-0.03 / 2.01×.
-1.38 / 3.00×-0.41 / 2.97×-0.06 / 2.98×.
table 2: result on roberta and albert..4 experiment.
4.1 computational cost measure.
we use average ﬂoating-point operations (flops)as the measure of computational cost, whichdenotes how many ﬂoating-point operations themodel performs for a single sample.
the flops is.
universal enough since it is not involved with themodel running environment (cpu, gpu or tpu)and it can measure the theoretical running timeof the model.
in general, the lower the model’sflops is, the faster the model’s inference is..4.2 experimental setup.
4.2.1 dataset.
to verify the effectiveness of our methods, weconduct experiments on ten english and chi-nese datasets of sequence labeling, covering ner:conll2003 (tjong kim sang and de meulder,2003), twitter ner (zhang et al., 2018), ontonotes4.0 (chinese) (weischedel et al., 2011), weibo(peng and dredze, 2015; he and sun, 2017) andclue ner (xu et al., 2020), pos: ark twitter(gimpel et al., 2011; owoputi et al., 2013), ctb5pos (xue et al., 2005) and ud pos (nivre et al.,2016), cws: ctb5 seg (xue et al., 2005) andud seg (nivre et al., 2016).
besides the standardbenchmark dataset like conll2003 and ontonotes4.0, we also choose some datasets closer to real-world application to verify the actual utility of ourmethods, such as twitter ner and weibo in socialmedia domain.
we use the same dataset prepro-.
193cessing and split as in previous work (huang et al.,2015; mengge et al., 2020; jia et al., 2020; tianet al., 2020; nguyen et al., 2020)..4.2.2 baseline.
we compare our methods with three baselines:.
• bilstm-crf (huang et al., 2015; ma andhovy, 2016) the most widely used model insequence labeling tasks before the pre-trainedlanguage model prevails in nlp..• bert the powerful stacked transformer en-coder model, pre-trained on large-scale cor-pus, which we use as the backbone of ourmethods..• distilbert the most well-known distillationmethod of bert.
huggingface released 6 lay-ers distilbert for english (sanh et al., 2019).
for comparison, we distill {3, 4} and {3, 4,6} layers distilbert for english and chineseusing the same method..4.2.3 hyper-parameters.
for all datasets, we use batch size=10.
we performgrid search over learning rate in {5e-6,1e-5,2e-5}.
we choose learning rate and the model based onthe development set.
we use the adamw optimizer(loshchilov and hutter, 2019).
the warmup step,weight decay is set to 0.05, 0.01, respectively..4.3 main results.
for english datasets, we use the ‘bert-base-cased’ released by google (devlin et al., 2019)as backbone.
for chinese datasets, we use ‘bert-wwm’ released by (cui et al., 2019).
the distil-bert is distilled from the backbone bert..to fairly compare our methods with baselines,we turn the speedup ratio of our methods to beconsistent with the corresponding static baseline.
we report the average performance over 5 timesunder different random seeds.
the overall resultsare shown in table 1, where the speedup is basedon the backbone.
we can see both sentee andtokee brings little performance drop and outper-forms distilbert in speedup ratio of 2, which hasachieved similar effect like existing early-exit fortext classiﬁcation.
under higher speedup, 3× and4×, sentee shows its weakness but tokee canstill keep a certain performance.
and under 2∼4×speedup ratio, tokee has a lower performancedrop than distilbert.
what’s more, for datasetswhere bert can show its power than lstm-crf,.
e.g., chinese ner, tokee (4×) on bert canstill outperform lstm-crf signiﬁcantly.
thisindicates the potential utility of it in complicatedreal-world scenario..to explore the ﬁne-grained performance changeunder different speedup ratio, we visualize thespeedup-performance trade-off curve on 6 datasets,in figure2.
we observe that,.
• before the speedup ratio rises to a certain turn-ing point, there is almost no drop on perfor-mance.
after that, the performance will dropgradually.
this shows our methods keep thesuperiority of existing early-exit methods (xinet al., 2020)..• as the speedup rises, tokee will encounterthe speedup turning point later than sentee.
after both methods reach the turning point,sentee’s performance degradation is moredrastic than tokee.
these both indicate thehigher speedup ceiling of tokee..• on some datasets, such as conll2003, weobserve a little performance improvement un-der low speedup ratio, we attribute this to thepotential regularization brought by early-exit,such as alleviating overthinking (kaya et al.,2019)..to verify the versatility of our method over dif-ferent ptms, we also conduct experiments on twowell-known bert variants, roberta (liu et al.,2019)5 and albert (lan et al., 2020)6, as shownin table 2. we can see that sentee and to-kee also signiﬁcantly outperform static backboneinternal layer on three representative datasets ofcorresponding tasks.
for roberta and albert,we also observe the tokee can have a better per-formance than sentee under high speedup ratio..4.4 analysis.
in this section, we conduct a set of detailed analysison our methods..4.4.1 the effect of window sizewe show the performance change under differentk in figure 3, keeping the speedup ratio consis-tent.
we observe that: (1) when k is 0, in otherwords, not using window-based uncertainty buttoken-independent uncertainty, the performanceis the almost lowest across different speedup ra-tio, because it does not consider local dependency.
5https://github.com/ymcui/chinese-bert-wwm.
6https://github.com/brightmart/albert zh..194conll2003.
ontonotes 4.0.
100.
96.72.
70.99 70.19.
62.72.senteetokee.
senteetokee.
1.
1.5.
2.
2.5speed up.
3.
3.5.
4.
1.
1.5.
3.
3.5.
4.
2.
2.5speed up.
ctb5 pos.
ud pos.
senteetokee.
senteetokee.
1.
1.5.
2.
2.5speed up.
3.
3.5.
4.
1.
1.5.
2.
2.5speed up.
3.
3.5.
4.ctb5 seg.
ud seg.
1f∆.
0.
−5.
−10.
−15.
1f∆.
0.
−1.
−2.
−3.
−4.
−5.
0.
−0.5.
1f∆.
−1.
0.
−2.
−4.
−6.
1f∆.
1f∆.
0.
−0.5.
−1.
−1.5.
−2.
−2.5.
0.
−0.2.
−0.4.
1f∆.
−0.6.
−0.8.
cca8.pmar.-ff.o.cca4.pmar.-ff.o.
80.
60.
40.
20.
100.
80.
60.
40.
95.
95.
94.
94.
94.
94.
98.
97.
96.
95.cca8.pmar.-ff.o.cca4.pmar.-ff.o.
50.
46.15.
40.
30.59.
26.3.
22.3.
0 ∼ 0 .
1.
0 .
1 ∼ 0 .
2.
0 .
2 ∼ 0 .
3.
0 .
3 ∼ 0 .
4.
0 .
4 ∼ 0 .
5.
0 .
5 ∼ 0 .
6.
0 .
6 ∼ 0 .
7.
0 .
7 ∼ 0 .
8.
0 .
8 ∼ 0 .
9.
0 .
9 ∼ 1.
98.43 97.79.
93.54.
86.69.
80.8.
70.15.
61.51.
57.49.
52.66.
36.84.
0 .
7 ∼ 0 .
8.
0 .
8 ∼ 0 .
9.
0 .
9 ∼ 1.
0 ∼ 0 .
1.
0 .
1 ∼ 0 .
2.
0 .
2 ∼ 0 .
3.
0 .
4 ∼ 0 .
5.
0 .
3 ∼ 0 .
40 .
6 ∼ 0 .
7(a) uncertainty.
0 .
5 ∼ 0 .
6.
94.64.
94.7.
94.56.
94.43.
94.78 94.79.
94.8.
94.8.
94.81.
93.83.
94.69.
0.
1.
2.
3.
4.
5.
6.
7.
8.
9.
96.96.
96.72.
97.15 97.28 97.42.
97.6.
97.64 97.63 97.67.
0.
1.
2.
3.
4.
5.
6.
7.
8.
9.
(b) k.senteetokee.
senteetokee.
1.
1.5.
2.
3.
2.5speed up.
3.5.
4.
1.
1.5.
2.
2.5speed up.
3.
3.5.
4.figure 4: the relation of off-ramps’ accuracy betweenuncertainty and window size.
two off-ramps of shal-low and deep layers are analyzed on conll2003..figure 2: the performance-speedup trade-off curve onsix datasets of sentee and tokee..at all.
this shows the necessity of the window-based uncertainty.
(2) when k is relatively large, itwill bring signiﬁcant performance drop under highspeedup ratio (3× and 4×), like sentee.
(3) it isnecessary to choose an appropriate k under highspeedup ratio, where the effect of different k has ahigh variance..4.4.2 accuracy v.s.
uncertainty.
liu et al.
(2020) veriﬁed ‘the lower the uncertainty,the higher the accuracy’ on text classiﬁcation.
here,we’d like to verify our window-based uncertainty.
1f∆.
0.
−2.
−4.
−6.
−8.
speedup ∼ 2×speedup ∼ 3×speedup ∼ 4×.
on sequence labeling.
in detail, we verify the entirewindow-based uncertainty and its speciﬁc hyper-parameter, k, on conll2003, shown in figure 4.for the uncertainty, we intercept the 4 th and 8 thoff-ramps and calculate their accuracy in each un-certainty interval, when k=2.
the result shown infigure 4(a) indicates that ‘the lower the window-based uncertainty, the higher the accuracy’, similaras in text classiﬁcation.
for k, we set a certainthreshold = 0.3, and calculate accuracy of tokenswhose window-based uncertainty is small than thethreshold under different k, shown in figure 4(b).
the result shows that, as k increases: (1) the ac-curacy of screened tokens is higher.
this showsthat the wider of a token’s low-uncertainty neigh-borhood, the more accurate the token’s predictionis.
this also veriﬁes the validity of window-baseduncertainty strategy.
(2) the accuracy improve-ment slows down.
this shows the low relevance ofdistant tokens’ uncertainty and explains why largek performs not well under high speedup ratio: itdoes not help improving more accurate exiting butslowing down exiting..0.
2.
4.
8.
10.
12.
6k.4.4.3.inﬂuence of sequence length.
figure 3: the performance change over different win-dow size under the same speedup ratio..transformer-based ptms, e.g.
bert, face achallenge in processing long text, due to theo(n 2d) computational complexity brought by self-.
195oitarpudeeps.43.532.521.51.
0.
−2.
−4.
−6.
1f∆.
k = 2k = +∞.
10.
30.
50sentence length.
70.
> 70.figure 5: inﬂuence of sentence length..self-samplingrandom samplingnone.
1.
1.5.
2.
2.5speed up.
3.
3.5.
4.figure 6: comparison between different training way..attention.
since the tokee reduces the layer-wisecomputational complexity from o(n 2d + n d2)to o(n m d + m d2) and sentee does not, we’dlike to explore their effect over different sentencelength.
we compare the highest speedup ratio oftokee and sentee when performance drop < 1on ontonotes 4.0, shown in figure 5. we observethat tokee has a stable computational cost savingas the sentence length increases, but sentee’sspeedup ratio will gradually reduce.
for this, wegive an intuitive explanation.
in general, a longersentence has more tokens, it is more difﬁcult for themodel to give them all conﬁdent prediction at thesame layer.
this comparison reveals the potentialof tokee on accelerating long text inference..4.4.4 effects of self-sampling fine-tuning.
to verify the effect of self-sampling ﬁne-tuningin section 3.3.2, we compare it with random sam-pling and no extra ﬁne-tuning on conll2003.
theperformance-speedup trade-off curve of tokee isshown in figure 6, which shows self-sampling isalways better than random sampling for tokee.
as speedup ratio rises, this trend is more signiﬁcant.
this shows the self-sampling can help more in re-ducing the gap of training and inference.
as forno extra ﬁne-tuning, it will deteriorate drasticallyat high speedup ratio.
but it can roughly keep acertain capability at low speedup ratio, which weattribute to the residual-connection of ptm andsimilar results were reported by veit et al.
(2016)..figure 7: tokee layer distribution in a sentence..4.4.5 layer distribution of early-exit.
in tokee, by halt-and-copy mechanism, each to-ken goes through a different number of ptm layersaccording to the difﬁculty.
we show the averagedistribution of a sentence’s tokens exiting layersunder different speedup ratio on conll2003, infigure 7. we also draw the average exiting layernumber of sentee under the same speedup ratio.
we observe that as speedup ratio rises, more tokenswill exit at the earlier layer but a bit of tokens canstill go through the deeper layer even when 4×,meanwhile, the sentee’s average exiting layernumber reduces to 2.5, where the ptm’s encodingpower is severely cut down.
this gives an intuitiveexplanation of why tokee is more effective thansentee under high speedup ratio: although bothsentee and tokee can dynamically adjust com-putational cost on the sample-level, tokee canadjust do it in a more ﬁne-grained way..5 related work.
ptms are powerful but have high computationalcost.
to accelerate them, many attempts have beenmade.
a kind of methods is to reduce its size, suchas distillation (sanh et al., 2019; jiao et al., 2020),structural pruning (michel et al., 2019; fan et al.,2020) and quantization (shen et al., 2020)..another kind of methods is early-exit, whichdynamically adjusts the encoding layer number ofdifferent samples (liu et al., 2020; xin et al., 2020;schwartz et al., 2020; zhou et al., 2020; li et al.,2020).
while they introduced early-exit mecha-.
196123456789101112# layer0204060exited tokens (%)speedup ~ 2×sent-ee( f1 = -0.11)token-ee( f1 = -0.02)123456789101112# layer0204060exited tokens (%)speedup ~ 3×sent-ee( f1 = -2.28)token-ee( f1 = -0.54)123456789101112# layer0204060exited tokens (%)speedup ~ 4×sent-ee( f1 = -8.20)token-ee( f1 = -3.12)nism in simple classiﬁcation tasks, our methodsare proposed for the more complicated scenario:sequence labeling, where it has not only one predic-tion probability and it’s necessary to consider thedependency of token exitings.
elbayad et al.
(2020)proposed depth-adaptive transformer to accel-erate machine translation.
however, their early-exit mechanism is designed for auto-regressive se-quence generation, in which the exit of tokens mustbe in left-to-right order.
therefore, it is unsuit-able for language understanding tasks.
differentfrom their method, our early-exit mechanism canconsider the exit of all tokens simultaneously..6 conclusion and future work.
in this work, we propose two early-exit mecha-nisms for sequence labeling: sentee and to-kee.
the former is a simple extension of sequence-level early-exit while the latter is specially designedfor sequence labeling, which can conduct more ﬁne-grained computational cost allocation.
we equiptokee with window-based uncertainty and self-sampling ﬁnetuning to make it more robust andfaster.
the detailed analysis veriﬁes their effective-ness.
sentee and tokee can achieve 2× and3∼4× speedup with minimal performance drop..for future work, we wish to explore: (1) leverag-ing the exited token’s label information to help theexiting of remained tokens; (2) introducing crf orother global decoding methods into early-exit forsequence labeling..acknowledgments.
we thank anonymous reviewers for their detailedreviews and great suggestions.
this work was sup-ported by the national key research and develop-ment program of china (no.
2020aaa0106700),national natural science foundation of china (no.
62022027) and major scientiﬁc research projectof zhejiang lab (no.
2019kd0ad01)..references.
yiming cui, wanxiang che, ting liu, bing qin,ziqing yang, shijin wang, and guoping hu.
2019.pre-training with whole word masking for chinesebert.
arxiv preprint arxiv:1906.08101..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the association.
for computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..maha elbayad, jiatao gu, edouard grave, and michaelin 8thauli.
2020. depth-adaptive transformer.
international conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..angela fan, edouard grave, and armand joulin.
2020.reducing transformer depth on demand with struc-in 8th international conference ontured dropout.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..kevin gimpel, nathan schneider, brendan o’connor,dipanjan das, daniel mills,jacob eisenstein,michael heilman, dani yogatama, jeffrey flanigan,and noah a. smith.
2011. part-of-speech taggingfor twitter: annotation, features, and experiments.
in the 49th annual meeting of the association forcomputational linguistics: human language tech-nologies, proceedings of the conference, 19-24 june,2011, portland, oregon, usa - short papers, pages42–47.
the association for computer linguistics..tao gui, qi zhang, jingjing gong, minlong peng,di liang, keyu ding, and xuanjing huang.
2018.transferring from formal newswire domain with hy-pernet for twitter pos tagging.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 2540–2549, brus-sels, belgium.
association for computational lin-guistics..hangfeng he and xu sun.
2017. f-score driven maxmargin neural network for named entity recognitionin chinese social media.
in proceedings of the 15thconference of the european chapter of the associa-tion for computational linguistics, eacl 2017, va-lencia, spain, april 3-7, 2017, volume 2: short pa-pers, pages 713–718.
association for computationallinguistics..zhiheng huang, wei xu, and kai yu.
2015. bidi-rectional lstm-crf models for sequence tagging.
corr, abs/1508.01991..takashi ishida, ikko yamane, tomoya sakai, gangniu, and masashi sugiyama.
2020. do we needzero training loss after achieving zero training error?
in proceedings of the 37th international conferenceon machine learning, volume 119 of proceedingsof machine learning research, pages 4604–4614.
pmlr..chen jia, yuefeng shi, qinrong yang, and yue zhang.
2020. entity enhanced bert pre-training for chi-nese ner.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 6384–6396, online.
associa-tion for computational linguistics..197xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-in proceedings of the 2020guage understanding.
conference on empirical methods in natural lan-guage processing: findings, emnlp 2020, onlineevent, 16-20 november 2020, pages 4163–4174.
as-sociation for computational linguistics..yigitcan kaya, sanghyun hong, and tudor dumitras.
2019. shallow-deep networks: understanding andin proceedingsmitigating network overthinking.
of the 36th international conference on machinelearning, volume 97 of proceedings of machinelearning research, pages 3301–3310.
pmlr..zhen ke, liang shi, erli meng, bin wang, xipeng qiu,and xuanjing huang.
2020. uniﬁed multi-criteriachinese word segmentation with bert..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning, icml’01, pages 282–289, san francisco, ca, usa.
mor-gan kaufmann publishers inc..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learningin international con-of language representations.
ference on learning representations..lei li, yankai lin, shuhuai ren, deli chen, xu-ancheng ren, peng li, jie zhou, and xu sun.
2020.accelerating pre-trained language models via cali-brated cascade.
corr, abs/2012.14682..weijie liu, peng zhou, zhiruo wang, zhe zhao,haotang deng, and qi ju.
2020. fastbert: a self-distilling bert with adaptive inference time.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6035–6044, online.
association for computational lin-guistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..xue mengge, bowen yu, zhenyu zhang, tingwenliu, yue zhang, and bin wang.
2020. coarse-to-fine pre-training for named entity recognition.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 6345–6354, online.
association for computa-tional linguistics..paul michel, omer levy, and graham neubig.
2019.in ad-are sixteen heads really better than one?
vances in neural information processing systems32: annual conference on neural information pro-cessing systems 2019, neurips 2019, december 8-14, 2019, vancouver, bc, canada, pages 14014–14024..dat quoc nguyen, thanh vu, and anh tuan nguyen.
2020. bertweet: a pre-trained language modelin proceedings of the 2020for english tweets.
conference on empirical methods in natural lan-guage processing: system demonstrations, pages 9–14, online.
association for computational linguis-tics..joakim nivre, marie-catherine de marneffe, filipginter, yoav goldberg, jan hajic, christopher d.manning, ryan t. mcdonald, slav petrov, sampopyysalo, natalia silveira, reut tsarfaty, and danielzeman.
2016. universal dependencies v1: a mul-in proceedings oftilingual treebank collection.
the tenth international conference on languageresources and evaluation lrec 2016, portoroˇz,slovenia, may 23-28, 2016. european language re-sources association (elra)..olutobi owoputi, brendan o’connor, chris dyer,kevin gimpel, nathan schneider, and noah a.smith.
2013.improved part-of-speech taggingfor online conversational text with word clusters.
in human language technologies: conference ofthe north american chapter of the association ofcomputational linguistics, proceedings, june 9-14,2013, westin peachtree plaza hotel, atlanta, geor-gia, usa, pages 380–390.
the association for com-putational linguistics..nanyun peng and mark dredze.
2015. named entityrecognition for chinese social media with jointlyin proceedings of the 2015trained embeddings.
conference on empirical methods in natural lan-guage processing, pages 548–554, lisbon, portugal.
association for computational linguistics..xipeng qiu, tianxiang sun, yige xu, yunfan shao,ning dai, and xuanjing huang.
2020. pre-trainedmodels for naturallanguage processing: a sur-science china technological sciences,vey.
63(10):1872–1897..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled ver-sion of bert: smaller, faster, cheaper and lighter.
inneurips emc2 workshop..roy.
schwartz,.
swabhaswayamdipta, jesse dodge, and noah a. smith..stanovsky,.
gabriel.
198zhang, zhengliang yang, kyle richardson, andzhenzhong lan.
2020. clue: a chinese languagein proceed-understanding evaluation benchmark.
ings of the 28th international conference on com-putational linguistics, pages 4762–4772, barcelona,spain (online).
international committee on compu-tational linguistics..naiwen xue, fei xia, fu-dong chiou, and marthapalmer.
2005. the penn chinese treebank: phrasestructure annotation of a large corpus.
nat.
lang.
eng., 11(2):207–238..hang yan, bocao deng, xiaonan li, and xipeng qiu.
2019. tener: adapting transformer encoder fornamed entity recognition.
corr, abs/1911.04474..jiahui yu and thomas s. huang.
2019. universallyslimmable networks and improved training tech-in 2019 ieee/cvf international confer-niques.
ence on computer vision, iccv 2019, seoul, ko-rea (south), october 27 - november 2, 2019, pages1803–1811.
ieee..qi zhang, jinlan fu, xiaoyu liu, and xuanjing huang.
2018. adaptive co-attention network for named en-tity recognition in tweets..wangchunshu zhou, canwen xu, tao ge, julian j.mcauley, ke xu, and furu wei.
2020. bert losespatience: fast and robust inference with early exit.
in advances in neural information processing sys-tems 33: annual conference on neural informationprocessing systems 2020, neurips 2020, december6-12, 2020, virtual..2020. the right tool for the job: matching modelin proceedings of theand instance complexities.
58th annual meeting of the association for com-putational linguistics, pages 6640–6651, online.
association for computational linguistics..sheng shen, zhen dong, jiayu ye, linjian ma, zheweiyao, amir gholami, michael w. mahoney, and kurtkeutzer.
2020. q-bert: hessian based ultra low pre-cision quantization of bert.
proceedings of the aaaiconference on artiﬁcial intelligence, 34(05):8815–8821..yuanhe tian, yan song, xiang ao, fei xia, xiao-jun quan, tong zhang, and yonggang wang.
2020.joint chinese word segmentation and part-of-speechtagging via two-way attentions of auto-analyzedknowledge.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 8286–8296, online.
association for computa-tional linguistics..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..andreas veit, michael j. wilber, and serge j. be-longie.
2016. residual networks behave like ensem-bles of relatively shallow networks.
in advances inneural information processing systems 29: annualconference on neural information processing sys-tems 2016, december 5-10, 2016, barcelona, spain,pages 550–558..ralph weischedel, sameer pradhan, lance ramshaw,martha palmer, nianwen xue, mitchell marcus,ann taylor, craig greenberg, eduard hovy, robertontonotes release 4.0.belvin, et al.
2011.ldc2011t03, philadelphia, penn.
: linguistic dataconsortium..ji xin, raphael tang, jaejun lee, yaoliang yu, andjimmy lin.
2020. deebert: dynamic early exitingin proceedingsfor accelerating bert inference.
of the 58th annual meeting of the association forcomputational linguistics, pages 2246–2251, on-line.
association for computational linguistics..liang xu, hai hu, xuanwei zhang, lu li, chenjiecao, yudong li, yechen xu, kai sun, dian yu,cong yu, yin tian, qianqian dong, weitang liu,bo shi, yiming cui, junyi li, jun zeng, rongzhaowang, weijian xie, yanting li, yina patterson,zuoyu tian, yiwen zhang, he zhou, shaoweihualiu, zhe zhao, qipeng zhao, cong yue, xinrui.
199