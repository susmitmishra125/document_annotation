mmgcn: multimodal fusion via deep graph convolution network foremotion recognition in conversation.
jingwen hu, yuchen liu, jinming zhao, qin jin∗school of information, renmin university of china{hujingwen benja, liuyuchen alfred, zhaojinming, qjin}@ruc.edu.cn.
abstract.
emotion recognition in conversation (erc)is a crucial component in affective dialoguesystems, which helps the system understandusers’ emotions and generate empathetic re-sponses.
however, most works focus on mod-eling speaker and contextual information pri-marily on the textual modality or simply lever-information through fea-aging multimodalture concatenation.
in order to explore amore effective way of utilizing both multi-modal and long-distance contextual informa-tion, we propose a new model based on mul-timodal fused graph convolutional network,mmgcn, in this work.
mmgcn can not onlymake use of multimodal dependencies effec-tively, but also leverage speaker informationto model inter-speaker and intra-speaker de-pendency.
we evaluate our proposed modelon two public benchmark datasets, iemocapand meld, and the results prove the effec-tiveness of mmgcn, which outperforms othersota methods by a signiﬁcant margin underthe multimodal conversation setting..figure 1: illustration of an example conversation in theiemocap dataset.
1.introduction.
emotion is an important part of human daily com-munication.
emotion recognition in conversation(erc) aims to automatically identify and track theemotional status of speakers during a dialogue.
ithas attracted increasing attention from researchersin the ﬁeld of natural language processing and mul-timodal processing.
erc has a wide range of po-tential applications such as assisting conversationanalysis for legal trials and e-health services etc.
itis also a key component for building natural human-computer interactions that can produce emotionalresponses in a dialogue..the fast growing availability of conversationaldata on social media is one of the factors that boost.
∗corresponding author.
the research focus on emotion recognition in con-versation.
different from traditional emotion recog-nition on isolated utterances, emotion recognitionin conversation requires context modeling of indi-vidual utterances.
the context can be attributedto the preceding utterances, temporality in con-versation turns, or speaker related information etc.
different models have been proposed to capturethe contextual information in previous works, in-cluding the lstm-based model (poria et al., 2017),the conversational memory network (cmn) model(hazarika et al., 2018b), interactive conversationalmemory network (icon) model (hazarika et al.,2018a), and dialoguernn model (majumder et al.,2019) etc.
in the example conversation as shownin figure 1, the two speakers are chatting in thecontext of the male speaker being admitted to usc.
in this chatting scene, they change topics a few.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5666–5675august1–6,2021.©2021associationforcomputationallinguistics5666oh, yeah?
have you gotten letters yet?
um, where?
hang out?u.s.c..oh my gosh, that's so cool.but big packet, big packet is nice....so you're going to be right here in los angeles; that is so cool.i'm looking forward to it.so can we hang out?hey, yes, you know that's -that's what i'm coming for.oh, yay.
thank you.
no but that's an awesome school.
...well, you know i'm leaningtowards like communicationai.okay.
they have a lot of good schools, right?yeah, i mean it's just a really good school.i'm looking forward to getting there.
i mean, god, the campus is cool.
times, such as the female speaker inviting the malespeaker out to play and so on.
but they keep com-ing back to the topic of usc, and then both ofthem express an excitement emotional status.
itshows that long-distance contextual information isof great help to the prediction of speakers’ emo-tions.
however, previous models can not effec-tively capture both speaker and long-distance di-alogue contextual information simultaneously inmulti-speaker conversation scenarios.
ghosal etal.
(ghosal et al., 2019), therefore, ﬁrst propose thedialoguegcn model which applies graph convo-lutional network (gcn) to capture long-distancecontextual information in a conversation.
dia-loguegcn takes each utterance as a node and con-nects any nodes that are in the same window withina conversation.
it can well model both the dialoguecontext and speaker information which leads tothe state-of-the-art erc performance.
however,like most previous models, dialoggcn only fo-cuses on the textual modality of the conversation,ignoring effective combination of other modalitiessuch as visual and acoustic modalities.
works thatconsider multimodal contextual information oftenconduct the simple feature concatenation type ofmultimodal fusion..in order to effectively explore the multimodalinformation and at the same time capture long-distance contextual information, we propose a newmultimodal fused graph convolutional network(mmgcn) model in this work.
mmgcn con-structs the fully connected graph in each modal-ity, and builds edge connections between nodescorresponding to the same utterance across dif-ferent modalities, so that contextual informationacross different modalities can interact.
in addition,the speaker information is injected into mmgcnvia speaker embedding.
furthermore, differentfrom dialoguegcn, which is a non-spectral do-main gcn and its many optimized matrices oc-cupy too much computing resource, we encodethe multimodal graph using spectral domain gcnand extend the gcn from a single layer to deeplayers.
to verify the effectiveness of the proposedmodel, we carry out experiments on two benchmarkmultimodal conversation datasets, iemocap andmeld.
mmgcn signiﬁcantly outperforms othermodels on both datasets..the rest of the paper is organized as follows:section 2 discusses some related works; section 3introduces the proposed mmgcn model in details;.
section 4 and 5 present the experiment setups ontwo public benchmark datasets and the analysisof experiment results and ablation study; finally,section 6 draws some conclusions..2 related work.
2.1 emotion recognition in conversation.
with the fast development of social media, muchmore interaction data become available, includingseveral open-sourced conversation datasets suchas iemocap(busso et al., 2008), avec(schulleret al., 2012), meld(poria et al., 2018), etc.
erchas attracted much research attention recently..many previous works focus on modeling con-textual information due to its importance in erc.
poria et al.
(poria et al., 2017) leverage a lstm-based model to capture interaction history context.
hazarika et al.
(hazarika et al., 2018b,a) ﬁrst payattention to the importance of speaker informationand exploit different memory networks to modeldifferent speakers.
dialoguernn (majumder et al.,2019) leverage distinct grus to capture speakers’contextual information.
dialoguegcn (ghosalet al., 2019) construct the graph considering bothspeaker and conversation sequential informationand achieve the state-of-the-art performance..2.2 multimodal fusion.
most recent studies on erc focus primarily on thetextual modality.
(poria et al., 2017; hazarika et al.,2018b,a) leverage multimodal information throughconcatenating features from three modalities with-out modeling the interaction between modalities.
(chen et al., 2017) conduct multimodal fusion atthe word-level for emotion recognition of isolatedutterances.
(sahay et al., 2018) consider contextualinformation and use relations in the emotion labelsacross utterances to predict the emotion (zadehet al., 2018) propose mfn to fuse information ofmulti-views, which aligns features from differentmodalities well.
however, mfn neglects to modelspeaker information, which is signiﬁcant to erc aswell.
the state-of-the-art dialoguegcn model onlyconsiders the textual modality.
in order to explorea more effective way of fusing multiple modalitiesand at the same time capturing contextual conver-sation information, we propose mmgcn whichconstructs a graph based on all three muoldalities..5667figure 2: framework illustration of the mmgcn based emotion recognition in conversation, which consists ofthree key components: modality encoder, multimodal graph convolutional network, emotion classiﬁer..2.3 graph convolutional network.
graph convolutional networks have been widelyused in the past few years for their ability to copewith non-euclidean data.
mainstream gcn meth-ods can be divided into spectral domain methodsand non-spectral domain methods (veliˇckovi´c et al.,2017).
spectral domain gcn methods (zhanget al., 2019) are based on laplace spectral decom-position theory.
they can only deal with undi-rected graphs.
non-spectral domain gcn meth-ods (veliˇckovi´c et al., 2017; schlichtkrull et al.,2018; li et al., 2015) can be applied to both di-rected and undirected graphs, but consuming largercomputing resource.
recently, researchers haveproposed methods to make spectral domain gcndeeper without over-smoothing (li et al., 2019;chen et al., 2020).
in order to further improvemmgcn on erc, we encode the multimodalgraph using spectral domain gcn with deep layers..3 method.
a dialogue can be deﬁned as a sequence of utter-ances {u1, u2, ..., un }, where n is the number ofutterances.
each utterance involves three sourcesof utterance-aligned data corresponding to threemodalities, including acoustic (a), visual (v) andtextual (t) modalities, which can be represented asfollows:.
ui = {ua.
i , uv.
i , uti}.
(1).
where uai denote the raw feature represen-tation of ui from the acoustic, visual and textual.
i , uv.
i , ut.
modality, respectively.
the emotion recognition inconversation task aims to predict the emotional sta-tus label for each utterance ui in the conversationbased on the available information from all threemodalities.
figure 2 illustrates the overall frame-work of our proposed emotion recognition in con-versation system, which consists of three key mod-ules: modality encoder, multimodal fused graphconvolutional network (mmgcn), and emotionclassiﬁer..3.1 modality encoder.
as we mentioned above, the dialog context infor-mation is important for predicting the emotion labelof each utterance.
therefore, it is beneﬁcial to en-code the contextual information into the utterancefeature representation.
we generate the context-aware utterance feature encoding for each modalitythrough the corresponding modality encoder.
to bespeciﬁc, we apply a bidirectional long short termmemory (lstm) network to encode the sequentialtextual context information for the textual modality.
for the acoustic and visual modalities, we apply afully connected network.
the context-aware fea-ture encoding for each utterance can be formulatedas follows:.
i, ht.
i−1),.
←−−−−lstm(ut.
i, ht.
i+1)].
(2).
hti = [hai = w ai = w vhvi , uv.
−−−−→lstm(uti + bae uaie uvi + bvii , ut.
where uai are the context-independent rawfeature representation of utterance i from the acous-tic, visual and textual modalities, respectively.
the.
5668acoustic nodesintra-modal edgestextual nodesvisual nodesinter-modal edgesone-hotspeakerembeddinglayerspeaker embedding+fcpredicted labels…ෝ𝑦𝑖ℎ𝑖𝑎ℎ𝑖𝑡ℎ𝑖𝑣𝑔𝑖𝑎𝑔𝑖𝑡𝑔𝑖𝑣emotion classifiermodality encoderfcℎ1𝑎𝑢1𝑎𝑢2𝑎𝑢3𝑎ℎ2𝑎ℎ3𝑎acousticlstmlstmlstmℎ1𝑡ℎ2𝑡ℎ3𝑡𝑢1𝑡𝑢2𝑡𝑢3𝑡textualfcℎ1𝑣ℎ2𝑣ℎ3𝑣𝑢1𝑣𝑢2𝑣𝑢3𝑣visualℎ1′𝑎ℎ2′𝑎ℎ3′𝑎ℎ1′𝑡ℎ2′𝑡ℎ3′𝑡ℎ1′𝑣ℎ2′𝑣ℎ3′𝑣𝑔1𝑎𝑔2𝑎𝑔3𝑎𝑔1𝑡𝑔2𝑡𝑔3𝑡𝑔1𝑣𝑔2𝑣𝑔3𝑣multimodal fused graph convolutional network （mmgcn）gcn×𝒌layers++concatenationmodality encoder outputs the context-aware rawfeature encoding ha.
i accordingly..i , and ht.
i , hv.
3.2 multimodal fused gcn (mmgcn).
in order to capture the utterance-level contextualdependencies across multiple modalities, we pro-pose a multimodal fused graph convolutional net-work (mmgcn).
we construct a spectral domaingraph convolutional network to encode the multi-modal contextual information inspired by (li et al.,2019; chen et al., 2020).
we also stack more lay-ers to construct a deep gcn.
furthermore, we addlearned speaker-embeddings to encode the speaker-level contextual information..3.2.1 speaker embedding.
as mentioned above, speaker information is im-portant for erc.
in order to encode the speakeridentity information, we add speaker embeddingsto the features before constructing the graph.
as-suming there are m parties in a dialogue, then thesize of the speaker embedding is m .
we show atwo-speaker conversation case in figure 2. theoriginal speaker identity can be denoted with aone-hot vector si and the speaker embedding si iscalculated as follows:.
si = wssi + bsi.
(3).
the speaker embedding can then be leveraged toattach speaker information in the graph construc-tion..3.2.2 graph construction.
i , vti.i , vvi , which represent.
a dialogue with n utterances can be represented asan undirected graph g = (v, e), where v (|v| =3n ) denotes utterance nodes in three modalitiesand e ⊂ v × v is a set of relationships containingcontext, speaker and modality dependency.
weconstruct the graph as follows:nodes: each utterance is represented by threenodes vainitialized within a graph,i ,h(cid:48)vh(cid:48)ai ,h(cid:48)li , si],i , si],[hti, si] respectively, corresponding to the threemodalities.
thus, given a dialogue with n utter-ances, we construct a graph with 3n nodes.
edges: we assume that each utterance has certainconnection to other utterances in the same dialogue.
therefore, any two nodes in the same modality inthe same dialogue are connected in the graph.
fur-thermore, each node is connected with the nodeswhich correspond to the same utterance but from.
[ha.
[hv.
i will be con-.
i in the graph..different modalities.
for example, vanected with vvi and vtedge weighting: we assume that if two nodeshave higher similarity, the information interactionbetween them is also more important, and the edgeweight between them should be higher.
in orderto capture the similarities between node represen-tations, following (skianis et al., 2018), we usethe angular similarity to represent the edge weightbetween two nodes..there are two types of edges in the graph: 1)edges connecting nodes from the same modal-ity, and 2) edges connecting nodes from differentmodalities.
to differentiate them, we use differ-ent edge weighting strategies.
for the ﬁrst type ofedges, the edge weight is computed as:.
aij = 1 −.
arccos(sim(ni, nj))π.
(4).
where ni and nj denote the feature representationsof the i-th and j-th node in the graph.
for thesecond type of edges, the edge weight is computedas:.
).
(5).
aij = γ(1 −.
arccos(sim(ni, nj))πwhere γ is a hyper parameter.
graph learning: inspired by (chen et al., 2020),we build a deep graph convolutional network basedon the undirected graph formed following the aboveconstruction steps to further encode the contextualdependencies.
to be speciﬁc, given the undirectedgraph g = (v, e), let ˜p be the renormalized graphlaplacian matrix (kipf and welling, 2016) of g:.
˜p = ˜d−1/2 ˜a ˜d−1/2.
= (d + i)−1/2(a + i)(d + i)−1/2.
(6).
where a denotes the adjacency matrix, d denotesthe diagonal degree matrix of graph g, and i de-notes identity matrix.
the iteration of gcn fromdifferent layers can be formulated as:.
h(l+1) = σ(((1−α) ˜ph(l) +αh(0))((1−β(l))i +β(l)w (l))) (7).
where α and β(l) are two hyper parameters, σ de-notes the activation function and w (l) is a learnableweight matrix.
to ensure the decay of the weightmatrix adaptively increases when stacking morelayers, we set β(l) = log( ηl + 1), where η is also ahyper parameter.
a residual connection to the ﬁrstlayer h(0) is added to the representation ˜ph(l) andan identity mapping i is added to the weight matrixw (l).
with such residual connection, we can makemmgcn deeper to further improve performance..5669(8).
(9).
(10).
(11).
3.3 emotion classiﬁer.
as described in sec.
3.2.2, we initialize nodes withthe combination of utterance feature and speakerembedding, h(cid:48)i..(cid:48).
h.i = [h.(cid:48)ai , h.(cid:48)vi , h.(cid:48)ti ]..i , gv.
i and gt.
let gai be the features of different modal-ities encoded by the gcn.
the features correspond-ing to the same utterance are concatenated:.
we then can concatenate gi and hi to generate theﬁnal feature representation for each utterance:.
gi = [ga.i , gv.
i , gti ]..(cid:48).
ei = [h.i, gi],.
ei is then fed into a mlp with fully connected lay-ers to predict the emotion label ˆyi for the utterance:.
li = relu (wlei + bl)pi = sof tmax(wsmaxli + bsmax)(pi[k])ˆyi = arg min.
k.3.4 training objectives.
we use categorical cross-entropy along with l2-regularization as the loss function during training:.
l = −.
1s=1 c(s).
(cid:80)n.n(cid:88).
c(i)(cid:88).
i=1.
j=1.
logpi,j[yi,j] + λ (cid:107)θ(cid:107)2.
(12).
where n is the number of dialogues, c(i) is thenumber of utterances in dialogue i, pi,j is theprobability distribution of predicted emotion labelsof utterance j in dialogue i, yi,j is the expectedclass label of utterance j in dialogue i, λ is thel2-regularization weight, and θ is the set of alltrainable parameters.
we use stochastic gradientdescent based adam (kingma and ba, 2014) opti-mizer to train our network.
hyper parameters areoptimized using grid search..4 experiment setups.
4.1 dataset.
dataset.
iemocapmeld.
dialogues.
utterances.
train+val1201153.test31280.train+val581011098.test16232610.table 1: data distribution of iemocap and meld.
8:2 ratio.
table 1 shows the distribution of trainand test samples for both datasets..iemocap: the dataset contains 12 hours ofvideos of two-way conversations from ten uniquespeakers, where only the ﬁrst eight speakers fromsession one to four are used in the training set.
eachvideo contains a single dyadic dialogue, segmentedinto utterances.
there are in total 7433 utterancesand 151 dialogues.
each utterance in the dialogueis annotated with an emotion label from six classes,including happy, sad, neutral, angry, excited andfrustrated..meld: multi-modal emotion lines dataset(meld) is a multi-modal and multi-speaker con-versation dataset.
compared to the emotion linesdataset (chen et al., 2018), meld has threemodality-aligned conversation data with higherquality.
there are in total 13708 utterances, 1433conversations and 304 different speakers.
speciﬁ-cally, different from dyadic conversation datasetssuch as iemocap, meld has three or more speak-ers in a conversation.
each utterance in the di-alogue is annotated with an emotion label fromseven classes, including anger, disgust, fear, joy,neutral, sadness and surprise..4.2 utterance-level raw feature extraction.
the textual raw features are extracted usingtextcnn following (hazarika et al., 2018a).
theacoustic raw features are extracted using the opens-mile toolkit with is10 conﬁguration (schuller et al.,2011).
the visual facial expression features areextracted using a densenet (huang et al., 2015)pre-traind on the facial expression recognitionplus (fer+) corpus (barsoum et al., 2016)..we evaluate our proposed mmgcn model on twobenchmark datasets, iemocap(busso et al., 2008)and meld(poria et al., 2018).
both are multi-modal datasets with aligned acoustical, visual andtextual information of each utterance in a conversa-tion.
followed (ghosal et al., 2019), we partitionboth datasets into train and test sets with roughly.
4.3.implementation details.
the hyperparameters are set as follows: the num-ber of gcn layers are both 4 for iemocap andmeld.
the dropout is 0.4. the learning rateis 0.0003. the l2 regularization parameter is0.00003. α, η and γ are set as 0.1, 0.5 and 0.7respectively.
considering the class-imbalance in.
5670iemocap.
meld.
bc-lstmcmnicon.
happy34.4330.3829.91dialoguernn 39.1647.1dialoguegcn42.34mmgcn.
sad60.8762.4164.5781.6980.8878.67.neutral angry excited frustrated average(w) average(w)58.9251.8160.6952.3960.8157.3860.2759.7761.2158.7162.3261.73.
57.9560.2563.4272.9170.9774.33.
56.7359.8363.0467.3666.0869.00.
54.9556.1358.5464.5865.0466.22.
56.80--57.1158.2358.65.table 2: erc performance (f1-score) of different approaches on both iemocap and meld datasets under themultimodal setting, which means the input includes all the acoustic, visual, and textual modalities; bold fontdenotes the best performance.
average(w) means weighted average.
(the result of cmn and icon are deﬁcientfor suiting two-way conversations only).
meld, we use focal loss when training mmgcnon meld.
in addition, we add layer normalizationafter the speaker embedding..4.4 evaluation metrics and signiﬁcance test.
following previous works (hazarika et al., 2018a;majumder et al., 2019; ghosal et al., 2019), weuse weighted average f1-score as the evaluationmetric.
paired t-test is performed to test the signiﬁ-cance of performance improvement with a defaultsigniﬁcance level of 0.05..4.5 compared baselines.
in order to verify the effectiveness of our model,we implement and compare the following modelson emotion recognition in conversation.
bc-lstm (poria et al., 2017): it encodes con-textual information through bi-directional lstm(hochreiter and schmidhuber, 1997) network.
thecontext-aware features are then used for emotionclassiﬁcation.
bc-lstm ignores speaker informa-tion as it doesn’t attach any speaker-related infor-mation to their model.
cmn (hazarika et al., 2018b):it leveragesspeaker-dependent grus to model utterance con-text combining dialogue history information.
theutterance features with contextual information aresubject to two distinct memory networks for bothspeakers.
due to the ﬁxed number of memorynetwork blocks, cmn can only serve in dyadicconversation scenarios.
icon (hazarika et al., 2018a): it extends cmnto model distinct speakers respectively.
samewith cmn, two speaker-dependent grus are lever-aged.
besides, a global gru is used to track thechange of emotion status in the entire conversationand multi-layer memory networks are leveraged tomodel the global emotion status.
though icon.
improves the result of erc, it still cannot adapt toa multi-speaker scenario.
dialoguernn (majumder et al., 2019): it mod-els speakers and sequential information in dia-logues through three different grus, which in-clude global gru, speaker gru and emotiongru.
speciﬁcally, global gru models context in-formation, while speaker dependent gru modelsthe status of the certain speaker.
the two modulesupdate interactively.
emotion gru detects emo-tion of utterances in conversation.
furthermore, inthe multimodal setting, the concatenation of acous-tical, visual, and textual features is used when thespeaker talks, but only use visual features other-wise.
however, dialoguernn doesn’t improvemuch in multimodal settings.
dialoguegcn (ghosal et al., 2019): it appliesgcn to erc, in which the generated features canintegrate rich information.
speciﬁcally, utterance-level features encoded by bi-lstm are used to initial-ize the nodes of the graph, edges are constructedwithin a certain window.
utterances in the samedialogue but with long distance can be connecteddirectly.
relation gcn(schlichtkrull et al., 2018)and gnn(morris et al., 2019), which are both non-spectral domain gcn models, are leveraged to en-code the graph.
however, dialoguegcn only fo-cuses on the textual modality.
in order to comparewith our mmgcn under the multimodal setting,we extend dialoguegcn by simply concatenatingfeatures of three modalities..5 results and discussions.
we compare our proposed mmgcn with all thebaseline models presented in section 4.5 on iemo-cap and meld datasets under the multimodal set-ting.
in order to compare the results under the sameexperiment settings, we reimplement the models in.
5671(a) early fusion.
(b) late fusion.
(c) fusion through gated attention.
figure 3: illustration of the three types of multi-modal fusion methods.
the following experiments..5.1 comparison with other models.
table 2 shows the performance comparison ofmmgcn with other models on the two bench-mark datasets under the multimodal setting.
di-alougegcn was the best performing model whenusing only the textual modality.
under the multi-modal setting, dialoguegcn which is fed withthe concatenation of acoustic, visual and tex-tual features achieves some slight improvementover the single textual modality.
our proposedmmgcn improves the f1-score performance overdialoguegcn under the multimodal setting by ab-solute 1.18% on iemocap and 0.42% on meldon average, and the improvement is signiﬁcant withp-value < 0.05..5.2 mmgcn under various modality setting.
table 3 shows the performance comparison ofmmgcn under different multimodal settings onboth benchmark datasets.
from table 3 we cansee that the best single modality performance isachieved on the textual modality and the worst ison the visual modality, which is consistent withpreviously reported ﬁndings.
adding acoustic andvisual modalities can bring additional performanceimprovement over the textual modality..5.3 comparison with other fusion methods.
to verify the effectiveness of mmgcn in multi-modal fusion, we compare it with other multimodalfusion methods, including early fusion, late fusion,fusion through gated attention and other represen-tative fusion methods such as mfn(zadeh et al.,2018) and mult(tsai et al., 2019).
the ﬁrst threefusion methods are illustrated in figure 3. as for.
modalityavtatvtavt.
iemocap meld42.6333.2757.7258.0257.9258.65.
54.6633.8662.3565.7062.8966.22.table 3: erc performance of mmgcn under differentmultimodal settings, which means the input containsdifferent combination of the three modalities.
early fusion, multimodal features are concatenatedand fed into gcn directly.
as for late fusion, fea-tures of different modalities are fed into differentgcns respectively and concatenated afterwards.
as for fusion through gated attention, features arefed into different gcns the same way as in late fu-sion, and then to a gated attention module.
speciﬁ-cally, the gated attention module can be formulatedas follows:.
i.
).
rmji = tanh(wmj · hmj)rmki = tanh(wmk · hmkz = σ(wz · hmjr(mj ,mk)iei = [r(a,v).
i= z ∗ rmj, r(a,t)i.)
i + (1 − z) ∗ rmk, r(v,t)i.
].
i.i.i.
(13).
(14).
(15).
(16).
(17).
i.i.and hmk.
where mj and mk could be any modality among{a, v, t}, hmjrepresent the feature en-coded by the corresponding modality encoder, eirepresents the ﬁnal feature representation for the ithutterance.
considering mfn and mult are lever-aged to fuse multimodal information sequentially,they are used to replace the modality encoder.
thefused multimodal features are fed to the gcn mod-ule subsequently..table 4 shows that mmgcn with the graph-based multimodal fusion outperforms all other com-.
5672avtemotionclassifieremotionclassifierconcatenateavtavtemotionclassifiergated attentionfigure 4: visualization of the heatmap of the adjacent matrix for the 20th utterance in a conversation with threemodalities.
’m’ and ’f’ refer to the male and female speakers respectively.
deepgcnearly f usiondeepgcnlate f usiondeepgcngated attentiondeepgcnm f ndeepgcnm ultm m gcn.
iemocap meld57.9458.2658.1858.2157.9358.65.
64.4664.6264.4562.7762.3766.22.mmgcnw/ spkr embeddingw/o spkr embedding.
iemocap meld58.6558.38.
66.2265.76.table 6: ablation study of the speaker embedding im-pact on erc performance.
table 4: erc performance comparison of mmgcnand other multimodal fusion methods.
5.5.impact of speaker embedding.
layers12481632.iemocap meld58.4058.3858.6558.5458.3858.42.
66.1266.1766.2266.1066.0666.10.table 5: erc performance comparison of mmgcnwith different number of layers.
pared multimodal fusion methods..5.4 mmgcn with different layers.
we investigate the impact of the number of layers inmmgcn on the erc performance in table 5. theexperiment results show that a different numberof layers does affect the erc recognition perfor-mance.
speciﬁcally, mmgcn achieves the bestperformance with 4 layers on both iemocap andmeld..speaker embedding can differentiate input featuresfrom different speakers.
previous works have re-ported that speaker information can help improveemotion recognition performance.
we conduct theablation study to verify the contribution of speakerembedding in mmgcn as shown in table 6. as ex-pected, dropping speaker embedding in mmgcnleads to performance degradation, which is signiﬁ-cant by t-test with p<0.05..5.6 case study.
fig 4 depicts a scene in which a man and a womanquarrel with each other over a female friend ofthe man who came to meet with him across 700miles.
they are frustrated or angry in most cases.
at the beginning of the conversation, their emotionstates are both neutral.
over time, they becomeemotional.
they are both angry at the end of theconversation.
the heatmaps of the adjacent matrixfor the 20th utterance in the conversation from thethree modalities demonstrate that different fromsimple sequential models, mmgcn pays attentionnot only to the close context, but also relate to thecontext in long-distance.
for example, as shown.
5673acousticvisualtextualwhat’s that suppose to mean?
look at you, you’re shaking.12345678910111213141516171819202122020thutteranceuh, don’t look at me like that.what the hell is the matter with you?angfruangmmffffffffmmmmmmmmmmmmmin the textual heatmap, mmgcn can successfullyaggregate information from the most relevant ut-terances, even from long-distance utterances, forexample the 3rd utterance..6 conclusion.
in this paper, we propose an multimodal fusedgraph convolutional network (mmgcn) for multi-modal emotion recognition in conversation (erc).
mmgcn provides a more effective way of utiliz-ing both multimodal and long-distance contextualinformation.
it constructs a graph that capturesnot only intra-speaker context dependency but alsointer-modality dependency.
with the residual con-nection, mmgcn can have deep layers to furtherimprove recognition performance.
we carry outexperiments on two public benchmark datasets,iemocap and meld, and the experiment resultsprove the effectiveness of mmgcn, which outper-forms other state-of-the-art methods by a signif-icant margin under the multimodal conversationsetting..7 acknowledgement.
this work wassupported by the nationalkey r&d program of china under grant no.
2020aaa0108600, national natural sciencefoundation of china (no.
62072462), na-tional natural science foundation of china (no.
61772535), and beijing natural science founda-tion (no.
4192028)..references.
emad barsoum, cha zhang, cristian canton ferrer,and zhengyou zhang.
2016. training deep net-works for facial expression recognition with crowd-sourced label distribution.
new york, ny, usa.
as-sociation for computing machinery..carlos busso, murtaza bulut, chi-chun lee, abekazemzadeh, emily mower, samuel kim, jean-nette n chang, sungbok lee, and shrikanth siemocap: interactive emotionalnarayanan.
2008.language re-dyadic motion capture database.
sources and evaluation, 42(4):335..ming chen, zhewei wei, zengfeng huang, bolin ding,and yaliang li.
2020. simple and deep graph con-volutional networks.
in international conference onmachine learning, pages 1725–1735.
pmlr..minghai chen, sen wang, paul pu liang, tadas bal-truˇsaitis, amir zadeh, and louis-philippe morency..2017. multimodal sentiment analysis with word-level fusion and reinforcement learning.
in proceed-ings of the 19th acm international conference onmultimodal interaction, pages 163–171..sheng-yeh chen, chao-chun hsu, chuan-chun kuo,lun-wei ku, et al.
2018. emotionlines: an emotioncorpus of multi-party conversations.
arxiv preprintarxiv:1802.08379..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander gelbukh.
2019.dialoguegcn: a graph convolutional neural networkarxivfor emotion recognition in conversation.
preprint arxiv:1908.11540..devamanyu hazarika, soujanya poria, rada mihal-cea, erik cambria, and roger zimmermann.
2018a.
interactive conversational memory networkicon:in proceedingsfor multimodal emotion detection.
of the 2018 conference on empirical methods innatural language processing, pages 2594–2604..devamanyu hazarika, soujanya poria, amir zadeh,erik cambria, louis-philippe morency, and rogerzimmermann.
2018b.
conversational memory net-work for emotion recognition in dyadic dialoguevideos.
in proceedings of naacl-hlt, pages 2122–2132..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..gao huang, zhuang liu, laurens van der maaten, andkilian q. weinberger.
2015. densely connected con-volutional networks..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..thomas n kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
arxiv preprint arxiv:1609.02907..guohao li, matthias muller, ali thabet, and bernardghanem.
2019. deepgcns: can gcns go as deepas cnns?
in proceedings of the ieee internationalconference on computer vision, pages 9267–9276..yujia li, daniel tarlow, marc brockschmidt, andrichard zemel.
2015. gated graph sequence neuralnetworks.
arxiv preprint arxiv:1511.05493..navonil majumder, soujanya poria, devamanyu haz-arika, rada mihalcea, alexander gelbukh, and erikcambria.
2019. dialoguernn: an attentive rnn foremotion detection in conversations.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 6818–6825..christopher morris, martin ritzert, matthias fey,william l hamilton, jan eric lenssen, gaurav rat-tan, and martin grohe.
2019. weisfeiler and lemango neural: higher-order graph neural networks.
in.
5674dong zhang, liangqing wu, changlong sun,shoushan li, qiaoming zhu, and guodong zhou.
2019. modeling both context-and speaker-sensitivedependence for emotion detection in multi-speakerconversations.
in ijcai, pages 5415–5421..proceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 4602–4609..soujanya poria, erik cambria, devamanyu hazarika,navonil majumder, amir zadeh, and louis-philippemorency.
2017. context-dependent sentiment anal-ysis in user-generated videos.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages873–883..soujanya poria, devamanyu hazarika, navonil ma-jumder, gautam naik, erik cambria, and rada mi-halcea.
2018. meld: a multimodal multi-partydataset for emotion recognition in conversations.
arxiv preprint arxiv:1810.02508..saurav sahay, shachi h kumar, rui xia, jonathanhuang, and lama nachman.
2018. multimodal re-lational tensor network for sentiment and emotionclassiﬁcation.
arxiv preprint arxiv:1806.02923..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607.
springer..bj¨orn schuller, anton batliner, stefan steidl, and dinoseppi.
2011. recognising realistic emotions and af-fect in speech: state of the art and lessons learntfrom the ﬁrst challenge.
speech communication,53(9-10):1062–1087..bj¨orn schuller, michel valster, florian eyben, roddycowie, and maja pantic.
2012. avec 2012: the con-tinuous audio/visual emotion challenge.
in proceed-ings of the 14th acm international conference onmultimodal interaction, pages 449–456..konstantinos skianis, fragkiskos malliaros,.
andmichalis vazirgiannis.
2018. fusing document, col-lection and label graph-based representations withword embeddings for text classiﬁcation.
in proceed-ings of the twelfth workshop on graph-based meth-ods for natural language processing (textgraphs-12), pages 49–58..yao-hung hubert tsai, shaojie bai, paul pu liang,j zico kolter, louis-philippe morency, and rus-lan salakhutdinov.
2019. multimodal transformerfor unaligned multimodal language sequences.
inproceedings of the conference.
association for com-putational linguistics.
meeting, volume 2019, page6558. nih public access..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro lio, and yoshua bengio.
2017. graph attention networks.
arxiv preprintarxiv:1710.10903..amir zadeh, paul pu liang, navonil mazumder,soujanya poria, erik cambria, and louis-philippemorency.
2018. memory fusion network for multi-in proceedings of theview sequential learning.
aaai conference on artiﬁcial intelligence, vol-ume 32..5675