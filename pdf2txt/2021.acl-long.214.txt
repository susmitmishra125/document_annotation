evaluating evaluation measuresfor ordinal classiﬁcation and ordinal quantiﬁcation.
tetsuya sakaiwaseda university / shinjuku-ku okubo 3-4-1, tokyo 169-8555, japantetsuyasakai@acm.org.
abstract.
ordinal classiﬁcation (oc) is an importantclassiﬁcation task where the classes are or-dinal.
for example, an oc task for senti-ment analysis could have the following classes:highly positive, positive, neutral, negative,highly negative.
clearly, evaluation measuresfor an oc task should penalise misclassiﬁca-tions by considering the ordinal nature of theclasses (e.g., highly positive misclassiﬁed aspositive vs. misclassifed as highly negative).
ordinal quantiﬁcation (oq) is a related taskwhere the gold data is a distribution over or-dinal classes, and the system is required to es-timate this distribution.
evaluation measuresfor an oq task should also take the ordinal na-ture of the classes into account.
however, forboth oc and oq, there are only a small num-ber of known evaluation measures that meetthis basic requirement.
in the present study,we utilise data from the semeval and ntcircommunities to clarify the properties of nineevaluation measures in the context of oc tasks,and six measures in the context of oq tasks..1.introduction.
in nlp and many other experiment-oriented re-search disciplines, researchers rely heavily on eval-uation measures.
whenever we observe an im-provement in the score of our favourite measure,we either assume or hope that this implies that wehave managed to moved our system a little towardswhat we ultimately want to achieve.
hence it is ofutmost importance to examine whether evaluationmeasures are measuring what we want to measure,and to understand their properties..this paper concerns evaluation measures for or-dinal classiﬁcation (oc) and ordinal quantiﬁca-tion (oq) tasks.
in an oc task, the classes areordinal, not nominal.
for example, task 4 (senti-ment analysis in twitter) subtask c in semeval-2016/2017 is deﬁned as: given a set of tweets about.
a particular topic, estimate the sentiment conveyedby each tweet towards the topic on a ﬁve-point scale(highly negative, negative, neutral, positive, highlypositive) (nakov et al., 2016; rosenthal et al.,2017).
on the other hand, an oq task involvesa gold distribution of labels over ordinal classesand the system’s estimated distribution.
for exam-ple, task 4 subtask e of the semeval-2016/2017workshops is deﬁned as: given a set of tweets abouta particular topic, estimate the distribution of thetweets across the ﬁve ordinal classes already men-tioned above (nakov et al., 2016; rosenthal et al.,2017).
the dialogue breakdown detection chal-lenge (higashinaka et al., 2017) and the dialoguequality subtasks of the ntcir-14 short text con-versation (zeng et al., 2019) and the ntcir-15dialogue evaluation (zeng et al., 2020) tasks arealso oq tasks.
1.clearly, evaluation measures for oc and oqtasks should take the ordinal nature of the classesinto account.
for example, in oc, when a highlypositive item is misclassiﬁed as highly negative,that should be penalised more heavily than when itis misclassiﬁed as positive.
surprisingly, however,there are only a small number of known evaluationmeasures that meet this requirement.
in the presentstudy, we use data from the semeval and ntcircommunities to clarify the properties of nine eval-uation measures in the context of oc tasks, andsix measures in the context of oq tasks.
some ofthese measures satisfy the aforementioned basicrequirement for ordinal classes; others do not..1in terms of data structure, we observe that the relationshipbetween oc and oq are similar to that between paired dataand two-sample data in statistical signiﬁcance testing.
in oc,we examine which item is classiﬁed by the system into whichclass, and build a confusion matrix of gold and estimatedclasses.
in contrast, in oq, we compare the system’s distribu-tion of items with the gold distribution, but we do not concernourselves with which item in one distribution corresponds towhich item in the other..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2759–2769august1–6,2021.©2021associationforcomputationallinguistics2759section 2 discusses prior art.
section 3 providesformal deﬁnitions of the measures we examine,as this is of utmost importance for reproducibility.
section 4 describes the data we use to evaluate themeasures.
sections 5 and 6 report on the resultson the oc and oq measures, respectively.
finally,section 7 concludes this paper..2 prior art.
2.1 evaluating ordinal classiﬁcation.
as we have mentioned in section 1, task 4 sub-task c of the semeval-2016/2017 workshops is anoc task with ﬁve ordinal classes (nakov et al.,2016; rosenthal et al., 2017).
while semevalalso features other oc tasks with fewer classes(e.g., task 4 subtask a from the same years, withthree classes), we use the subtask c data as havingmore classes should enable us to see more clearlythe difference between measures that consider or-dinal classes and those that do not.2 note thatif there are only two classes, oc is reduced tonominal classiﬁcation.
subtask c used two evalu-ation measures that consider the ordinal nature ofthe classes: macroaveraged mean absolute error(maem ) and the standard mean absolute error(maeµ) (baccianella et al., 2009)..at acl 2020, amig´o et al.
(2020) proposeda measure speciﬁcally designed for oc, calledcloseness evaluation measure (cemord ), anddiscussed its axiomatic properties.
their meta-evaluation experiments primarily focused on com-paring it with other measures in terms of how eachmeasure agrees simultaneously with all of pre-selected “gold” measures.
however, while theirresults showed that cemord is similar to all ofthese gold measures, the outcome may differ if wechoose a different set of gold measures.
indeed, inthe context of evaluating information retrieval eval-uation measures, sakai and zeng (2019) demon-strated that a similar meta-evaluation approachcalled unanimity (amig´o et al., 2018) dependsheavily on the choice of gold measures.
moreover,while amig´o et al.
(2020) reported that cemordalso performs well in terms of consistency of sys-tem rankings across different data (which they referto as “robustness”), experimental details were notprovided in their paper.
hence, to complement theirwork, the present study conducts extensive and re-.
2semeval-2018 task 1 (affect in tweets) featured an octask with four classes (mohammad et al., 2018).
however, therun submission ﬁles of this task are not publicly available..producible experiments for oc measures.
our ocmeta-evaluation experiments cover nine measures,including maem , maeµ, and cemord ..2.2 evaluating ordinal quantiﬁcation.
as we have mentioned in section 1, task 4 sub-task e of the semeval-2016/2017 workshops isan oq task with ﬁve ordinal classes (nakov et al.,2016; rosenthal et al., 2017).3 subtask e usedearth mover’s distance (emd), remarking thatthis is “currently the only known measure for ordi-nal quantiﬁcation” (nakov et al., 2016; rosenthalet al., 2017).
subsequently, however, sakai (2018a)proposed a new suite of oq measures based onorder-aware divergence (od),4 and comparedthem with normalised match distance (nmd), anormalised version of emd.
sakai utilised datafrom the third dialogue breakdown detectionchallenge (dbdc3) (higashinaka et al., 2017),which features three ordinal classes, and showedthat his root symmetric normalised od (rsnod)measure behaves similarly to nmd.
however, hisexperiments relied on the run submission ﬁles fromhis own team, as he did not have access to the en-tire set of dbdc3 submission ﬁles.
on the otherhand, the organisers of dbdc3 (tsunomori et al.,2020) compared rsnod, nmd, and the ofﬁcialmeasures of dbdc (namely, mean squared er-ror and jensen-shannon divergence, which ignorethe ordinal nature of the classes) using all the runsubmission ﬁles from dbdc3.
they reported thatrsnod was the overall winner in terms of systemranking consistency and discriminative power, i.e.,the ability of a measure to obtain many statisticalsigniﬁcant differences (sakai, 2006, 2007, 2014).
in addition to the aforementioned two subtask edata sets from semeval, the present study utilisesthree data sets from the dialogue quality (dq) sub-tasks of the recent ntcir-15 dialogue evaluation(dialeval-1) task (zeng et al., 2020).
each dqsubtask is deﬁned as: given a helpdesk-customerdialogue, estimate the probability distribution overthe ﬁve-point likert-scale dialogue quality ratings(see section 4).
our oq meta-evaluation exper-iments cover six measures, including nmd andrsnod..3the valence ordinal classiﬁcation subtask of semeval-2018 task 1 (affect in tweets) is also an oq task, with sevenclasses (mohammad et al., 2018).
however, the submissionﬁles of this task are not publicly available..4see also sakai (2017) for an earlier discussion on od..27603 evaluation measure deﬁnitions.
3.1 classiﬁcation measures.
in the oc tasks of semeval-2016/2017, a set of top-ics was given to the participating systems, whereeach topic is associated with n tweets.
(n variesacross topics.)
given a set c of ordinal classes rep-resented by consecutive integers, each oc systemyields a |c| × |c| confusion matrix for each topic.
from this, we can calculate evaluation measuresdescribed below.
finally, the systems are evaluatedin terms of mean scores over the topic set..let cij denote the number of items (e.g., tweets)whose true class is j, classiﬁed by the system intoi (i, j ∈ c) so that n = (cid:80)(cid:80)i cij.
let c•j =j(cid:80)j cij, and c+ = {j ∈ c | c•j >0}.
that is, c+ is the set of gold classes that arenot empty.
we compute mae’s as follows..i cij, ci• = (cid:80).
mae m =.
1|c+|.
(cid:80).
(cid:88).
j∈c+.
i∈c |i − j|cijc•j.
,.
(1).
(cid:80).
(cid:80).
j∈c.
mae µ =.
i∈c |i − j|cijn...(2).
unlike the original formulation of maem by bac-cianella et al.
(2009), ours explicitly handles caseswhere there are empty gold classes (i.e., j s.t.
c•j = 0).
empty gold classes actually do existin the semeval data used in our experiments..it is clear from the weights used above (|i − j|)that maes assume equidistance, although this isnot guaranteed for ordinal classes.
hence amig´oet al.
(2020) propose the following alternative:.
cem ord =.
(cid:80).
(cid:80).
j∈c(cid:80).
i∈c prox ijcij.
j∈c prox jjc•j.
,.
(3).
where prox ij = − log2(max{0.5, kij}/n ), and.
(cid:40).
kij =.
c•i/2 + (cid:80)jc•i/2 + (cid:80)i−1.
l=i+1 c•ll=j c•l.
(i ≤ j)(i > j).
..(4).
our formulation of prox ij with a max operatorensures that it is a ﬁnite value even if kij = 0..we also consider weighted κ (cohen, 1968).
weﬁrst compute the expected agreements when thesystem and gold labels are independent: eij =ci•c•j/n .
weighted κ is then deﬁned as:.
κ = 1 −.
(cid:80).
(cid:80).
j∈c.
j∈c.
(cid:80).
(cid:80).
i∈c wijciji∈c wijeij.
,.
(5).
where wij is a predeﬁned weight for penalisingmisclassiﬁcation.
in the present study, we followthe approach of maes (eqs.
1-2) and considerlinear weighted κ: wij = |i − j|.
however, itshould be noted here that κ is not useful if theoc task involves baseline systems such as the onesincluded in the aforementioned semeval tasks: thatis, a system that always returns class 1, a systemthat always returns class 2, and so on.
it is easy tomathematically prove that κ returns a zero for alltopics for all such baseline systems..we also consider applying krippendorff’sα (krippendorff, 2018) to oc tasks.
the α is ameasure of data label reliability, and can handleany types of classes by plugging in an appropriatedistance function.
instead of the |c| × |c| confu-sion matrix, the α requires a |c| × n class-by-itemmatrix that contains label counts ni(u), which rep-resents the number of labels which say that item ubelongs to class i. for an oc task, ni(u) = 2 ifboth the gold and system labels for u is i; ni(u) = 1if either the gold or system label (but not both) foru is i; ni(u) = 0 if neither label says u belongs toi. thus, this matrix ignores which labels are fromthe gold data and which are from the system..for comparing two complete sets of labels (onefrom the gold data and the other from the sys-tem), the deﬁnition of krippendorff’s α is rela-tively simple.
let ni = (cid:80)u ni(u); this is the to-tal number of labels that class i received fromthe two sets of labels.
the observed coincidencefor classes i and j (i, j ∈ c, i (cid:54)= j) is given byoij = (cid:80)u ni(u)nj(u), while the expected coinci-dence is given by eij = ninj/(2n − 1).
the α isdeﬁned as:.
α = 1 −.
(cid:80).
(cid:80)i(cid:80).
(cid:80)i.j>i oijδ2ijj>i eijδ2ij ,.
where, for ordinal data,.
δ2ij = (.
nk −.
ni + nj2.
)2 ,.
j(cid:88).
k=i.
(6).
(7).
and for interval data, δ2ij = |i − j|2 (krippendorff,2018).
we shall refer to these two versions of α asα-ord and α-int, respectively.
unlike κ, the α’scan evaluate the aforementioned baseline systemswithout any problems..the three measures deﬁned below ignore theordinal nature of the classes.
that is, they areaxiomatically incorrect as oc evaluation measures..2761first, let us consider two different deﬁnitions of“macro f1” found in the literature (opitz and burst,2019): to avoid confusion, we give them differentnames in this paper.
for each j ∈ c+, let precj =cjj/cj• if cj• > 0, and precj = 0 if cj• = 0 (i.e.,the system never chooses class j).
let recj =cjj/c•j.
also, for any positive values p and r, letf 1(p, r) = 2pr/(p + r) if p + r > 0, and letf 1(p, r) = 0 if p = r = 0. then:.
f 1m =.
1|c+|.
(cid:88).
j∈c+.
f 1(precj, recj) ..(8).
now, let precm = (cid:80)recm = (cid:80).
j∈c+ recj/|c+|, and.
j∈c+ precj/|c+|,.
hmpr = f 1(precm , recm ) ..(9).
hmpr stands for harmonic mean of macroaver-aged precision and macroaveraged recall.
opitzand burst (2019) recommend what we call f1 mover what we call hmpr.
again, note that our for-mulations use c+ to clarify that empty gold classesare ignored..finally, we also consider accuracy:5.accuracy =.
(10).
(cid:80).
j∈c cjjn...from eqs.
2 and 10, it is clear that maeµ and ac-curacy ignore class imbalance (baccianella et al.,2009), unlike the other measures..3.2 quantiﬁcation measures.
in an oq task, a comparison of an estimated dis-tribution and the gold distribution over |c| ordi-nal classes yields one effectiveness score, as de-scribed below.
the systems are then evaluatedby mean scores over the test instances, e.g., top-ics (nakov et al., 2016; rosenthal et al., 2017) ordialogues (zeng et al., 2019, 2020).
let pi de-note the estimated probability for class i, so that(cid:80)i denote the trueprobability.
we also denote the entire probabilitydistributions by p and p∗, respectively.
i = (cid:80).
k≤i pk, and cp∗k. nor-malised match distance (nmd) used in the nt-cir dialogue quality subtasks (zeng et al., 2019,2020) is given by (sakai, 2018a):.
i∈c pi = 1. similarly, let p∗.
let cpi = (cid:80).
k≤i p∗.
nmd(p, p∗) =.
(cid:80).
i∈c |cpi − cp∗i ||c| − 1.
..(11).
5since the system and the gold data have the same totalnumber of items to classify (i.e., n ), accuracy is the same asmicroaveraged f1/recall/precision..this is simply a normalised version of emd used inthe oq tasks of semeval (see section 2.2) (nakovet al., 2016; rosenthal et al., 2017)..we also consider two measures that can handleoq tasks from sakai (2018a).
first, a distance-weighted sum of squares for class i is deﬁned as:.
dw i =.
|i − j|(pj − p∗.
j )2 ..(12).
(cid:88).
j∈c.
note that the above assumes equidistance.
letc∗ = {i ∈ c|p∗i > 0}.
that is, c∗ is the setof classes with a positive gold probability.
order-aware divergence is deﬁned as:.
od(p (cid:107) p∗) =.
dw i ,.
(13).
1|c∗|.
(cid:88).
i∈c∗.
with its symmetric version sod(p, p∗) =(od(p (cid:107) p∗) + od(p∗ (cid:107) p))/2.
root (symmetric)normalised order-aware divergence is deﬁned as:(cid:115).
rnod(p (cid:107) p∗) =.
rsnod(p, p∗) =.
od(p (cid:107) p∗)|c| − 1.
(cid:115).
sod(p, p∗)|c| − 1.,.
..(14).
(15).
the other three measures deﬁned below ignorethe ordinal nature of the classes (sakai, 2018a);they are axiomatically incorrect as oq measures.
normalised variational distance (nvd) is essen-tially the mean absolute error (mae):.
nvd(p, p∗) =.
|pi − p∗.
i | ..(16).
12.
(cid:88).
i∈c.
root normalised sum of squares (rnss) is essen-tially the root mean squared error (rmse):(cid:114) (cid:80).
i∈c(pi − p∗2.i )2.
..(17).
rnss (p, p∗) =.
the advantages of rmse over mae is discussedin chai and draxler (2014)..the kullback-leibler divergence (kld) for sys-tem and gold probability distributions over classesis given by:.
kld(p (cid:107) p∗) =.
(cid:88).
pi log2.
..(18).
pip∗i.i∈c s.t.
pi>0.
as this is undeﬁned if p∗i = 0, we use the more con-venient jensen-shannon divergence (jsd) instead,which is symmetric (lin, 1991):.
jsd(p, p∗) =.
kld(p (cid:107) pm ) + kld(p∗ (cid:107) pm )2.,(19).
where pm.
i = (pi + p∗.
i )/2..2762task/subtask.
short namein this papersem16t4csem17t4csem16t4esem17t4edq-{a, e, s} ntcir-15 (2020) dialeval-1 dq.
evaluationvenuesemeval-2016semeval-2017semeval-2016semeval-2017.
task 4 subtask ctask 4 subtask ctask 4 subtask etask 4 subtask e.tasktypeococoqoqoq.
language.
eeeec+e.
#ordinalclasses55555.test datasample size100125100125300.
#runs used.
1220121422 (13+9).
table 1: task data used in our oc and oq meta-evaluation experiments (c: chinese, e: english)..(i) sem16t4cα-ordα-inthmprf1mmaemκcemordaccuracy.
α-int hmpr0.8181.0000.818-------------(ii) sem17t4c α-int hmpr0.8210.811------.
α-ordα-inthmprf1mmaemκcemordaccuracy.
0.989-------.
f1m maem0.8180.8790.8180.8790.8790.8180.818---------f1m maem0.7890.8210.8000.8110.9260.8950.863---------.
κ cemord0.6060.8790.6060.8790.6060.9390.7270.8790.7270.8790.667-----κ cemord0.6950.6840.7890.7890.8420.747--.
0.7580.7680.8320.7680.842---.
accuracy maeµ−0.030 −0.152−0.030 −0.152−0.030 −0.1520.091 −0.0300.091 −0.0300.030 −0.0910.2420.364-0.879accuracy maeµ0.3370.3260.4320.4530.4840.3680.6000.863.
0.4530.4420.5260.5260.5790.4630.716-.
table 2: system ranking similarity in terms of kendall’s τ for each oc task.
correlation strengths are visualisedin colour (τ ≥ 0.8, 0.6 ≤ τ < 0.8, and τ < 0.6) to clarify the trends..4 task data.
table 1 provides an overview of the semeval andntcir task data that we leveraged for our oc andoq meta-evaluation experiments.
from semeval-2016/2017 task 4 (sentiment analysis in twit-ter) (nakov et al., 2016; rosenthal et al., 2017), wechose subtask c as our oc tasks, and subtask e asour oq tasks for the reason given in section 2.1.6moreover, for the oq meta-evaluation experiments,we also utilise the dq (dialogue quality) subtaskdata from ntcir-15 dialeval-1 (zeng et al., 2020).
as these subtasks require participating systems toestimate three different dialogue quality score dis-tributions, namely, a-score (task accomplishment),e-score (dialogue effectiveness), and s-score (cus-tomer satisfaction), we shall refer to the subtasksas dq-a, dq-e, and dq-s hereafter.
we utiliseboth chinese and english dq runs for our oqmeta-evaluation (22 runs in total), as the ntcirtask evaluates all runs using gold distributions thatare based on the chinese portion of the paralleldialogue corpus (zeng et al., 2020).
as the threentcir data sets are larger than the two semevaldata sets both in terms of sample size and the num-.
6we do not use the arabic data from 2017 as only two runswere submitted to subtasks c and e (rosenthal et al., 2017)..ber of systems, we shall focus on the oq meta-evaluation results with the ntcir data; the resultswith sem16t4e and sem17t4e can be found inthe appendix..5 meta-evaluation with ordinal.
classiﬁcation tasks.
5.1 system ranking similarity.
table 2 shows, for each oc task, the kendall’s τrank correlation values (sakai, 2014) between twosystem rankings for every pair of measures.
wecan observe that: (a) the α’s, the two “macro f1”measures (f1m and hmpr), maem and κ pro-duce similar rankings; (b) maeµ and accuracy(i.e., the two measures that ignore class imbalance)produce similar rankings, which are drastically dif-ferent from those of group a; and (c) cemordproduces a ranking that is substantially differentfrom the above two groups, although the ranking iscloser to those of group a. the huge gap betweengroups a and b strongly suggests that maeµ andaccuracy are not useful even as secondary mea-sures for evaluating oc systems..it should be noted that the semeval 2016/2017task 4 subtask c actually reported maeµ scoresin addition to the primary maem scores, and the.
2763measure.
mean τ.measure.
mean τ.sem16t4c.
(b) 10 vs. 10ve2 = 0.00730.
(a) full split (50 vs. 50)ve2 = 0.002110.976(cid:93)0.962(cid:91)0.935♣0.929♣0.904♦0.901♦0.884‡0.8060.799.κα-ordα-intmaeµhmprmaemf1mcemordaccuracy.
(c) full split (62 vs. 63)ve2 = 0.001140.910♠0.908♠0.907♠0.901♣0.871‡0.869‡0.866‡0.850†0.818.α-ordf1mα-inthmprcemordmaeµκmaemaccuracy.
sem17t4c.
(d) 10 vs. 10ve2 = 0.00503.α-ordκα-inthmprmaemf1mmaeµaccuracycemord.
hmprα-intf1mκα-ordcemordmaeµmaemaccuracy.
0.872♠0.868♠0.863♠0.799♣0.780♥0.758‡0.753‡0.625†0.595.
0.768♠0.761♣0.760♣0.751♥0.742♥0.729♦0.700†0.697†0.663.table 3: system ranking consistency for the oc tasks.
(cid:93)/(cid:91)/♠/♣/♥/♦/ ‡ /† means “statistically signiﬁcantlyoutperforms the worst 8/7/6/5/4/3/2/1 measure(s),” re-spectively.
ve2 is the residual variance computed fromeach 1000 × 9 trial-by-measure matrix of τ scores,which can be used for computing effect sizes.
for ex-ample, from part (a), the effect size for the differencebetween α-ord and cemord can be computed as(0.962 − 0.806)/0.00211 = 3.40..√.
system rankings according to these two measureswere completely different even in the ofﬁcial re-sults.
for example, in the 2016 results (table 12in nakov et al.
(2016)), while the baseline run thatalways returns neutral is ranked at 10 among the 12runs according to maem , the same run is rankedat the top according to maeµ.
similarly, in the2017 results (table 10 in rosenthal et al.
(2017)), arun ranked at 10 (tied with another run) among the20 runs according to maem is ranked at the topaccording to maeµ.
our results shown in table 2generalise these known discrepancies between therankings..5.2 system ranking consistency.
for each measure, we evaluate its system rankingconsistency (or “robustness” (amig´o et al., 2020))across two topic sets as follows (sakai, 2021):(1) randomly split the topic set in half, producetwo system rankings based on the mean scores overeach topic subset, and compute a kendall’s τ scorefor the two rankings; (2) repeat the above 1,000times and compute the mean τ ; (3) conduct a ran-.
domised paired tukey hsd test at α = 0.05 with5,000 trials on the mean τ scores to discuss statisti-cal signiﬁcance.7.
table 3 (a) and (c) show the consistency resultswith the oc tasks.
for example, part (a) shows thatwhen the 100 topics of sem16t4c were randomlysplit in half 1,000 times, κ statistically signiﬁcantlyoutperformed all other measures, as indicated bya “(cid:93).” table 3 (b) and (d) show variants of theseexperiments where only 10 topics are used in eachtopic subset, to discuss the robustness of measuresto small sample sizes.
if we take the averages of (a)and (c), the top three measures are the two α’s andκ, while the worst two measures are cemord andaccuracy; we obtain the same result if we take theaverages of (b) and (d).
thus, although amig´o et al.
(2020) reported that cemord performed well interms of “robustness,” this is not conﬁrmed in ourexperiments..recall that κ has a practical inconvenience: itcannot distinguish between baseline runs that al-ways return the same class.
while semeval16t4ccontains one such run (which always returns neu-tral), semeval17t4c contains as many as ﬁve suchruns (each always returning one of the ﬁve ordinalclasses).
this is probably why κ performs well intable 3(a) and (b) but not in (c) and (d)..5.3 discriminative power.
in the information retrieval research community,discriminative power (sakai, 2006, 2007, 2014) is awidely-used method for comparing evaluation mea-sures (e.g., anelli et al.
(2019); ashkan and metzler(2019); chuklin et al.
(2013); clarke et al.
(2020);golbus et al.
(2013); lu et al.
(2016); kanoulas andaslam (2009); leelanupab et al.
(2012); robertsonet al.
(2010); valcarce et al.
(2020)).
given a setof systems, a p-value for the difference in meansis obtained for every system pair (preferrably witha multiple comparison procedure (sakai, 2018b));highly discriminative measures are those than canobtain many small p-values.
while highly discrim-inative measures are not necessarily correct, wedo want measures to be sufﬁciently discriminativeso that we can draw some useful conclusions fromexperiments.
again, we use randomised paired.
7the tukey hsd (honestly signiﬁcant differences) testis a multiple comparison procedure: that is, it is like the t-test, but can compare the means of more than two systemswhile ensuring that the familywise type i error rate is α. therandomised version of this test is free from assumptions suchas normality and random sampling from a population (sakai,2018b)..2764(i) dq-arnssrsnodrnodnvdjsd(ii) dq-ernssrsnodrnodnvdjsd.
rsnod rnod nvd0.9390.8610.939--.
0.9130.870---.
0.835----.
rsnod rnod nvd0.9130.9480.957--.
0.9220.957---.
0.931----.
(iii) dq-s rsnod rnod nvd0.9570.8870.983--.
rnssrsnodrnodnvdjsd.
0.9740.887---.
0.861----.
jsd0.9050.8270.9390.931-jsd0.9130.9480.9910.948-jsd0.9220.8530.9480.965-.
nmd0.6360.7660.7230.6800.714nmd0.6880.7580.7490.7580.758nmd0.5580.6620.5840.5840.619.table 5:system ranking similarity in terms ofkendall’s τ for each oq task (ntcir).
correlationstrengths are visualised in colour (τ ≥ 0.9, 0.8 ≤ τ <0.9, and τ < 0.8) to clarify the trends..volve multiple baseline runs that always return thesame class.
such runs are unrealistic, so this lim-itation may not be a major problem.
on the otherhand, if the tasks do involve such baseline runs (asin semeval), we recommend α-ord as the pri-mary measure.
in either case, it would be goodto use both κ and α-ord to examine oc systemsfrom multiple angles.
according to our consis-tency and discriminative power experiments, usingα-int instead of α-ord (i.e., assuming equidis-tance) does not seem beneﬁcial for oc tasks..6 meta-evaluation with ordinal.
quantiﬁcation tasks.
6.1 system ranking similarity.
table 5 shows, for each oq task from ntcir, thekendall’s τ between two system rankings for ev-ery pair of measures.
it is clear from the “nmd”column that nmd is an outlier among the six mea-sures.
in other words, among the only axiomati-cally correct measures for oq tasks, rnod andrsnod are the ones that produce rankings that aresimilar to those produced by well-known measuressuch as jsd and nvd (i.e., normalised mae; seeeq.
16).
also, in table 5(i) and (iii), it can beobserved that the ranking by rsnod lies some-where between that by nmd (let us call it “groupx”) and those by the other measures (“group y”).
however, this is not true in table 5(ii), nor withour semeval results (see appendix table 8)..figure 1: discriminative power with randomised tukeyhsd tests (b = 5, 000 trials) for each oc task..(ii).
(i)(cid:88)(cid:88) (cid:88).
(iii)(cid:88)(cid:88).
κα-ordα-int.
cemord (cid:88) (cid:88)(cid:88) (cid:88)maem(cid:88)f1m(cid:88)hmpr(cid:88) (cid:88)maeµ(cid:88)accuracy.
(cid:88)(cid:88)(cid:88)(cid:88).
(iv)aa.a/baaabb.
(v)goodgood.
poorfairfairfairfairpoor.
(vi)goodfair.
poorpoorfairfairpoorpoor.
table 4: summary of the properties of oc measures.
(i) handles ordinal classes; (ii) handles systems that al-ways return the same class; (iii) handles class imbal-ance; (iv) system ranking similarity; (v) system rank-ing consistency; (vi) discriminative power..tukey hsd tests with 5,000 trials for obtaining thep-values..figure 1 shows the discriminative power curvesfor the oc tasks.
curves that are closer to theorigin (i.e., those with small p-values for manysystem pairs) are considered good.
we can ob-serve that (i) cemord , accuracy, maem , andmaeµ are the least discriminative measures in bothtasks.
(ii) among the other measures that performbetter, κ performs consistently well.
again, thefact that κ distinguishes itself from others in thesemeval16t4c results probably reﬂects the factthat the data set contains only one run that alwaysreturns the same class, which cannot be handledproperly by κ..5.4 recommendations for oc tasks.
table 4 summarises the properties of the nine mea-sures we examined in the context of oc tasks.
col-umn (iv) shows that, for example, the group ameasures produce similar rankings.
based on thistable, we recommend (linear weighted) κ as theprimary measure for oc tasks if the tasks do not in-.
276500.050.10.150.20.250.30.350.40.450.516111621263136414651566166mae-mmae-μcem-ordκα-ordα-intf1-mhmpraccuracy(i) sem16t4csample size: 100#systems: 12(66 system pairs)p-valuesystem pairs00.050.10.150.20.250.30.350.40.450.55161718191101111121131141151161171181mae-mmae-μcem-ordκα-ordα-intf1-mhmpraccuracy(ii) sem17t4csample size: 125#systems: 20(190 system pairs)system pairsp-valuedq-a.
(a) full split (150 vs. 150)ve2 = 0.00130.
0.909♣rnod0.885‡rnss0.882‡nvd0.879‡jsdrsnod 0.820†0.717nmd.
dq-e(c) full split (150 vs. 150)ve2 = 0.000519.
0.865♣nmd0.842♥jsd0.835♦rnss0.819‡nvd0.813rnodrsnod 0.811.dq-s(e) full split (150 vs. 150)ve2 = 0.00105.
0.906♥rnss0.901♦jsd0.897♦rnod0.870‡nvdrsnod 0.861†0.745nmd.
(b) 10 vs. 10ve2 = 0.00871.
0.558♣jsd0.507♦rnod0.497♦nvd0.456‡rnss0.424†nmdrsnod 0.404.
(d) 10 vs. 10ve2 = 0.00403.
0.624♣jsd0.610♥rnod0.594‡rnss0.592‡nvdrsnod 0.563†0.502nmd.
(f) 10 vs. 10ve2 = 0.00656.
0.580♣jsd0.547♥rnod0.523‡nvd0.514‡rnssrsnod 0.448†0.421nmd.
table 6: system ranking consistency for the oq tasks(ntcir).
♣/♥/♦/ ‡ /† means “statistically signiﬁ-cantly outperforms the worst 5/4/3/2/1 measure(s),” re-spectively.
ve2 is the residual variance computed fromeach 1000 × 6 trial-by-measure matrix of τ scores,which can be used for computing effect sizes.
for ex-ample, part (a), the effect size for the difference be-tween rnod and nmd can be computed as (0.909 −0.717)/0.00130 = 5.33 (i.e., over ﬁve standard devi-ations apart)..√.
6.2 system ranking consistency.
table 6 shows the system ranking consistency re-sults with the oq tasks from ntcir.
these exper-iments were conducted as described in section 5.2.if we take the averages of (a), (c), and (e) (i.e., ex-periments where the 300 dialogues are split in half),the worst measure is nmd, followed by rsnod.
moreover, the results are the same if we take the av-erages of (b), (d), and (f) (i.e., experiments wheretwo disjoint sets of 10 dialogues are used), weobtain the same result.
hence, among the axiomat-ically correct measures for oq tasks, rnod ap-pears to be the best in terms of system rankingconsistency, and that introducing symmetry (com-pare eqs.
14 and 15) may not be a good idea froma statistical stability point of view.
note that, forcomparing a system distribution with a gold distri-bution, symmetry is not a requirement..figure 2: discriminative power with randomised tukeyhsd tests (b = 5, 000 trials) for each oq task fromntcir..6.3 discriminative power.
figure 2 shows the discriminative power curves forthe oq tasks from ntcir.
we can observe that:(i) nmd performs extremely poorly in (i) and (iii),which is consistent with the full-split consistencyresults in table 6(a) and (e); (ii) rnod outper-forms rsnod in (i) and (iii).
although rsnodappears to perform well in (ii), if we consider the5% signiﬁcance level (i.e., 0.05 on the y-axis), thenumber of statistically signiﬁcantly different pairs(out of 231) is 117 for rnod, 116 for rsnod,nmd, and nvd, and 115 for rnss and jsd.
thatis, rnod performs well in (ii) also.
these resultsalso suggest that introducing symmetry to rnod(i.e., using rsnod instead) is not beneﬁcial..6.4 recommendations for oq tasks.
table 7 summarises the properties of the six mea-sures we examined in the context of oq tasks.
col-umn (iii) indicates that nmd is an outlier in termsof system ranking.
based on this table, we recom-mend rnod as the primary measure of oq tasks,as evaluating oq systems do not require the mea-sures to be symmetric.
as a secondary measure,we recommend nmd (i.e., a form of earth mover’sdistance) to examine the oq systems from a differ-ent angle, although its statistical stability (in termsof system ranking consistency and discriminativepower) seems relatively unpredictable.
althoughthe ntcir dialogue quality subtasks (zeng et al.,.
276600.050.10.150.20.250.30.350.40.450.55161718191101111121131141151161171181191201211221231nmdrnodrsnodnvdrnssjsd(i) dq-asample size: 300#systems: 22(231 system pairs)p-valuesystem pairs00.050.10.150.20.250.30.350.40.450.55161718191101111121131141151161171181191201211221231nmdrnodrsnodnvdrnssjsd(ii) dq-esample size: 300#systems: 22(231 system pairs)p-valuesystem pairs00.050.10.150.20.250.30.350.40.450.55161718191101111121131141151161171181191201211221231nmdrnodrsnodnvdrnssjsd(iii) dq-ssample size: 300#systems: 22(231 system pairs)p-valuesystem pairs(i)(ii)(cid:88) (cid:88)nmdrsnod (cid:88) (cid:88)rnod (cid:88)nvdrnssjsd.
(cid:88)(cid:88)(cid:88).
(iii)xyyyyy.
(iv)poorpoorgoodgoodgoodgood.
(v)poorfairfairfairfairfair.
table 7: summary of the properties of oq measures.
(i) handles ordinal classes; (ii) symmetric; (iii) systemranking similarity; (iv) system ranking consistency;(v) discriminative power..2019, 2020) have used nmd and rsnod as theofﬁcial measures, it may be beneﬁcial for them toreplace rsnod with rnod..(eq.
12) for rnod (and other equidistance-basedmeasures) may be replaced with a different weight-ing scheme (e.g., something similar to the prox ijweights of cem ord ) if need be..our ﬁnal and general remark is that it is of ut-most importance for researchers to understand theproperties of evaluation measures and ensure thatthey are appropriate for a given task.
our futurework includes evaluating and understanding evalu-ation measures for tasks other than oc and oq..acknowledgement.
this work was partially supported by jsps kak-enhi grant number 17h01830..7 conclusions.
references.
we conducted extensive evaluations of nine mea-sures in the context of oc tasks and six measures inthe context of oq tasks, using data from semevaland ntcir.
as we have discussed in sections 5.4and 6.4, our recommendations are as follows..oc tasks use (linear weighted) κ as the primarymeasure if the task does not involve multi-ple runs that always return the same class(e.g., one that always returns class 1, anotherthat always returns class 2, etc.).
otherwise,use α-ord (i.e., krippendorff’s α for ordi-nal classes) as the primary measure.
in eithercase, use both measures..oq tasks use rnod as the primary measure, and.
nmd as a secondary measure..all of our evaluation measure score matricesare available from https://waseda.box.com/acl2021packocoq, to help researchers reproduceour work..among the above recommended measures, re-call that linear weighted κ and rnod assumeequidistance (i.e., they rely on wij = |i − j|),while α-ord and nmd do not.
hence, if re-searchers want to avoid relying on the equidis-tance assumption (i.e., satisfy the ordinal invari-ance property (amig´o et al., 2020)), α-ord canbe used for oc tasks and nmd can be used for oqtasks.
however, we do not see relying on equidis-tance as a practical problem.
for example, notethat the linear weighted κ is just an instance ofthe weighted κ family: if necessary, the weightwij can be set for each pair of classes i and j ac-cording to practical needs.
similarly, wij = |i − j|.
enrique amig´o, julio gonzalo, stefano mizzaro, andjorge carrillo de albornoz.
2020. an effectivenessmetric for ordinal classiﬁcation: formal propertiesin proceedings of acland experimental results.
2020..enrique amig´o, damiano spina, and jorge carrillode albornoz.
2018. an axiomatic analysis of diver-sity evaluation metrics: introducting the rank-biasedutility metric.
in proceedings of acm sigir 2018,pages 625–634..vito walter anelli, tommaso di noia, eugenio di sci-ascio, claudio pomo, and azzurra ragone.
2019.on the discriminative power of hyper-parameters inin pro-cross-validation and how to choose them.
ceedings of acm recsys 2019, pages 447–451..azin ashkan and donald metzler.
2019. revisiting on-line personal search metrics with the user in mind.
in proceedings acm sigir 2019, pages 625–634..stefano baccianella, andrea esuli, and fabrizio sebas-tiani.
2009. evaluation measures for ordinal regres-sion.
in proceedings of isda 2009, pages 283–287..t. chai and r.r.
draxler.
2014. root mean square er-ror (rmse) or mean absolute error (mae)?
– argu-ments against avoiding rmse in the literature.
geo-scientiﬁc model development, 7:1247–1250..aleksandr chuklin, pavel serdyuov, and maartende rijke.
2013. click model-based information re-trieval metrics.
in proceedings of acm sigir 2013,pages 493–502..charles l.a. clarke, alexandra vtyurina, and mark d.smucker.
2020. ofﬂine evaluation without gain.
inproceedings of ictir 2020, pages 185–192..jacob cohen.
1968. weighted kappa: nominal scaleagreement provision for scaled disagreement or par-tial credit.
psychological bulletin, 70(4):213–220..2767tetsuya sakai.
2014. metrics, statistics, tests..inpromise winter school 2013: bridging betweeninformation retrieval and databases (lncs 8173),pages 116–163.
springer..tetsuya sakai.
2017. towards automatic evaluation ofmulti-turn dialogues: a task design that leveragesinherently subjective annotations.
in proceedings ofevia 2017, pages 24–30..tetsuya sakai.
2018a.
comparing two binned proba-bility distributions for information access evaluation.
in proceedings of acm sigir 2018, pages 1073–1076..tetsuya sakai.
2018b.
laboratory experiments in in-formation retrieval: sample sizes, effect sizes, andstatistical power.
springer..tetsuya sakai.
2021. on the instability of diminishingreturn ir measures.
in proceedings of ecir 2021part i (lncs 12656), pages 572–586..tetsuya sakai and zhaohao zeng.
2019. which diver-in proceed-.
sity evaluation measures are “good”?
ings of acm sigir 2019, pages 595–604..yuiko tsunomori, ryuichiro higashinaka, tetsurotakahashi, and michimasa inaba.
2020. selectionof evaluation metrics for dialogue breakdown detec-tion in dialogue breakdown detection challenge 3 (injapanese).
transactions of the japanese society forartiﬁcial intelligence, 35(1)..daniel valcarce, alejandro, bellog´ın, javier parapar,and pablo castells.
2020. assessing ranking metricsinformation retrievalin top-n recommendation.
journal, 23:411–448..zhaohao zeng, sosuke kato, and tetsuya sakai.
2019.overview of the ntcir-14 short text conversationtask: dialogue quality and nugget detection sub-tasks.
in proceedings of ntcir-14, pages 289–315..zhaohao zeng, sosuke kato, tetsuya sakai, and inhokang.
2020. overview of the ntcir-15 dialoguein proceedings ofevaluation task (dialeval-1).
ntcir-15, pages 13–34..peter b. golbus, javed a. aslam, and carles l.a.clarke.
2013. increasing evaluation sensitivity to di-versity.
information retrieval, 16:530–555..ryuichiro higashinaka, kotaro funakoshi, michimasainaba, yuiko tsunomori, tetsuro takahashi, andnobuhiro kaji.
2017. overview of dialogue break-down detection challenge 3. in proceedings of dia-log system technology challenge 6 (dstc6) work-shop..evangelos kanoulas and javed a. aslam.
2009. empir-ical justiﬁcation of the gain and discountfunction forndcg.
in proceedings of acm cikm 2009, pages611–620..klaus krippendorff.
2018. content analysis: an intro-duction to its methodology (fourth edition).
sagepublications..teerapong leelanupab, guido zuccon, and joemon m.jose.
2012. a comprehensive analysis of parametersettings for novelty-biased cumulative gain.
in pro-ceedings of acm cikm 2012, pages 1950–1954..jianhua lin.
1991. divergence measures based on theieee transactions on informa-.
shannon entropy.
tion theory, 37(1):145–151..xiaolu lu, alistair moffat, and j. shane culpep-per.
2016. the effect of pooling and evaluationdepth on ir metrics.
information retrieval journal,19(4):416–445..saif m. mohammad, felipe bravo-marquez, mo-hammad salameh, and svetlana kiritchenko.
2018.semeval-2018 task 1: affect in tweets.
in proceed-ings of international workshop on semantic evalua-tion (semeval-2018), new orleans, la, usa..preslav nakov, alan ritter, sara rosenthal, veselinstoyanov, and fabrizio sebastiani.
2016. semeval-2016 task 4: sentiment analysis in twitter.
in pro-ceedings of the 10th international workshop on se-mantic evaluation, semeval ’16, san diego, cali-fornia.
association for computational linguistics..juri opitz and sebastian burst.
2019. macro f1 and.
macro f1..stephen e. robertson, evangelos kanoulas, and em-ine yilmaz.
2010. extending average precision toin proceedings ofgraded relevance judgements.
acm sigir 2010, pages 603–610..sara rosenthal, noura farra, and preslav nakov.
2017.semeval-2017 task 4: sentiment analysis in twitter.
in proceedings of the 11th international workshopon semantic evaluation, semeval ’17, vancouver,canada.
association for computational linguistics..tetsuya sakai.
2006. evaluating evaluation metricsbased on the bootstrap.
in proceedings of acm si-gir 2006, pages 525–532..tetsuya sakai.
2007. alternatives to bpref.
in proceed-.
ings of acm sigir 2007, pages 71–78..2768appendix.
for completeness, this appendix reports on theoq experiments based on semeval16t4e and se-meval17t4e, which we omitted in the main bodyof the paper.
however, we view the oq resultsbased on the three ntcir data sets as more reli-able than these additional results, as the semevalscore matrices are much smaller than those fromntcir (see table 1)..table 8 shows the system ranking similarity re-sults with semeval16t4e and semeval17t4e; thistable complements table 5 in the paper..table 9 shows the system ranking consistencyresults with semeval16t4e and semeval17t4e;this table complements table 6 in the paper..figure 3 shows the discriminative power curvesfor semeval16t4e and semeval17t4e; this ﬁgurecomplements figure 2 in the paper..figure 3: discriminative power with randomised tukeyhsd tests (b = 5, 000 trials) for each oq task (se-meval)..(i) sem16t4ernssrsnodrnodnvdjsd(ii) seml17t4ernssrsnodrnodnvdjsd.
rsnod0.848----rsnod0.912----.
rnod0.9090.939---rnod0.9121.000---.
nvd0.8180.9090.909--nvd0.8900.9780.978--.
jsd0.7880.8790.8790.970-jsd0.8680.9560.9560.978-.
nmd0.8180.8480.9090.9390.909nmd0.7800.8680.8680.8900.912.table 8:system ranking similarity in terms ofkendall’s τ for each oq task (semeval).
correlationstrengths are visualised in colour (τ ≥ 0.9, 0.8 ≤ τ <0.9, and τ < 0.8) to clarify the trends..measure mean τ measure mean τ(i) sem16t4e.
full split (50 vs. 50)ve2 = 0.001750.934♣jsd0.847♥rnod0.831♦nvd0.815‡rnss0.788†nmdrsnod 0.767.full split (62 vs. 63)ve2 = 0.001070.905♣nmd0.878♥nvd0.867♦jsdrsnod 0.859‡0.826†rnod0.765rnss.
10 vs. 10ve2 = 0.00368.
0.771♣jsd0.708♦rnod0.705♦nvd0.690‡rnss0.674nmdrsnod 0.673.
10 vs. 10ve2 = 0.00342.
0.705♣nmd0.672♥jsd0.601♦nvd0.588†rnodrsnod 0.583†0.557rnss.
(ii) sem17t4e.
table 9: system ranking consistency for the oq tasks(semeval).
♣/♥/♦/ ‡ /† 5 4 3 2 1 means “statisti-cally signiﬁcantly outperforms the worst 5/4/3/2/1 mea-sure(s),” respectively.
ve2 is the residual variance com-puted from each 1000 × 6 split-by-measure matrix ofτ scores, which can be used for computing effect sizes.
for example, from (i) left, the effect size for the dif-ference between jsd and rnod can be computed as0.00175 = 2.08 (i.e., about two stan-(0.934−0.847)/dard deviations apart)..√.
276900.050.10.150.20.250.30.350.40.450.514710131619222528313437404346495255586164nmdrnodrsnodnvdrnssjsdp-valuesystem pairs(i) sem16t4esample size: 100#systems: 12(66 system pairs)00.050.10.150.20.250.30.350.40.450.5161116212631364146515661667176818691nmdrnodrsnodnvdrnssjsdp-valuesystem pairs(ii) sem17t4esample size: 125#systems: 14(91 system pairs)