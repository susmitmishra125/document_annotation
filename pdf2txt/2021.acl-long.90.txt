when do you need billions of words of pretraining data?.
yian zhang,∗,1 alex warstadt,∗,2 haau-sing li,3 and samuel r. bowman1,2,31dept.
of computer science, 2dept.
of linguistics, 3center for data sciencenew york university{yian.zhang, warstadt, xl3119, bowman}@nyu.edu.
abstract.
nlp is currently dominated by language mod-els like roberta which are pretrained onbillions of words.
but what exact knowl-edge or skills do transformer lms learn fromlarge-scale pretraining that they cannot learnfrom less data?
to explore this question,we adopt ﬁve styles of evaluation: classiﬁerprobing, information-theoretic probing, unsu-pervised relative acceptability judgments, un-supervised language model knowledge prob-ing, and ﬁne-tuning on nlu tasks.
we thendraw learning curves that track the growth ofthese different measures of model ability withrespect to pretraining data volume using theminibertas, a group of roberta modelspretrained on 1m, 10m, 100m and 1b words.
we ﬁnd that these lms require only about10m to 100m words to learn to reliably encodemost syntactic and semantic features we test.
they need a much larger quantity of data inorder to acquire enough commonsense knowl-edge and other skills required to master typi-cal downstream nlu tasks.
the results sug-gest that, while the ability to encode linguis-tic features is almost certainly necessary forlanguage understanding, it is likely that other,unidentiﬁed, forms of knowledge are the ma-jor drivers of recent improvements in languageunderstanding among large pretrained models..1.introduction.
pretrained language models (lms) like bert androberta have become ubiquitous in nlp.
newmodels require massive datasets of tens or evenhundreds of billions of words (brown et al., 2020)to improve on existing models on language un-derstanding benchmarks like glue (wang et al.,2018).
much recent work has used probing meth-ods to evaluate what these models do and do not.
*equal contribution.
figure 1: overall learning curves for the ﬁve evaluationmethods.
for each method, we compute overall perfor-mance for each roberta model tested as the macro av-erage over sub-task’s performance after normalization.
we ﬁt an exponential curve which we scale to havean initial value of 0 and an asymptote at 1. classiﬁerand mdl probing mainly test models’ encoding of lin-guistic features; blimp tests model’s understanding oflinguistic phenomena; lama tests factual knowledge;superglue is a suite of conventional nlu tasks..learn (belinkov and glass, 2019; tenney et al.,2019b; rogers et al., 2020; ettinger, 2020).
sincemost of these works only focus on models pre-trained on a ﬁxed data volume (usually billionsof words), many interesting questions regardingthe effect of the amount of pretraining data remainunanswered: what have data-rich models learnedthat makes them so effective on downstream tasks?
how much pretraining data is required for lms tolearn different grammatical features and linguisticphenomena?
which of these skills do we expect toimprove when we scale pretraining past 30 billionwords?
which aspects of grammar can be learnedfrom data volumes on par with the input to humanlearners, around 10m to 100m words (hart andrisley)?.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1112–1125august1–6,2021.©2021associationforcomputationallinguistics1112none1m10m100m1b30b0.00.20.40.60.81.0classifier probing(edge probing)mdl reflected(edge probing)blimplamasupergluerelative performancepretraining dataset sizewith these questions in mind, we evaluate andprobe the minibertas (warstadt et al., 2020b), agroup of roberta models pretrained on 1m, 10m,100m, and 1b words, and robertabase (liu et al.,2019) pretrained on about 30b words, using ﬁvemethods: first we use standard classiﬁer probingon the edge probing suite of nlp tasks (tenneyet al., 2019b) to measure the quality of the syn-tactic and semantic features that can be extractedby a downstream classiﬁer with each level of pre-training.
second, we apply minimum descriptionlength (mdl) probing (voita and titov, 2020) tothe edge probing suite, with the goal of quantify-ing the accessibility of these features.
third, wetest the models’ knowledge of various syntacticphenomena using unsupervised acceptability judg-ments on the blimp suite (warstadt et al., 2020a).
fourth, we probe the models’ world knowledge andcommonsense knowledge using unsupervised lan-guage model knowledge probing with the lamasuite (petroni et al., 2019).
finally, we ﬁne-tune themodels on ﬁve tasks from superglue (wang et al.,2019) to measure their ability to solve conventionalnlu tasks..for each evaluation method, we ﬁt an exponen-tial learning curve to the results as a function of theamount of pretraining data, shown in figure 1. wehave two main ﬁndings: first, the results of classi-ﬁer probing, mdl probing, and unsupervised rel-ative acceptability judgement (blimp) show thatthe linguistic knowledge of models pretrained on100m words and 30b words is similar, as is thedescription length of linguistic features.
second,roberta requires billions of words of pretrainingdata to effectively acquire factual knowledge andto make substantial improvements in performanceon dowstream nlu tasks.
from these results, weconclude that there are skills critical to solvingdownstream nlu tasks that lms can only acquirewith billions of words of pretraining data.
futurework will likely need to look beyond core linguis-tic knowledge if we are to better understand andadvance the abilities of large language models..2 methods.
we probe the minibertas, a set of 12 robertamodels pretrained from scratch by warstadt et al.
(2020b) on 1m, 10m, 100m, and 1b words, thepublicly available robertabase (liu et al., 2019),.
which is pretrained on about 30b words,1 and 3robertabase models with randomly initializedparameters..descriptions of the ﬁve evaluation methods ap-pear in the subsequent sections.2 in each exper-iment, we test all 16 models on each task in-volved.
to show the overall trend of improvement,we use non-linear least squares to ﬁt an exponen-tial learning curve to the results.3 we upsamplerobertabase results in regression in order to havean equal number of results for each data quantity.
we use a four-parameter exponential learning curveused to capture diminishing improvement in perfor-mance as a function of the number of practice trials(heathcote et al., 2000; leibowitz et al., 2010):.
e(pn) = p∞ − (p∞ − p0) · e−α·nβ.
where e(pn) is the expected performance after ntrials,4 p0 and p∞ and are the initial and asymp-totic performance, and α and β are coefﬁcients totranslate and dilate the curve in the log domain..we plot the results in a ﬁgure for each task,where the y-axis is the score and the x-axis is theamount of pretraining data.5 for some plots, weuse min-max normalization to adjust the resultsinto the range of [0, 1], where 0 and 1 are the in-ferred values of p0 and p∞, respectively.6.
3 classiﬁer probing.
we use the widely-adopted probing approach ofettinger et al.
(2016), adi et al.
(2017), and others—which we call classiﬁer probing—to test the extentto which linguistic features like part-of-speech andcoreference are encoded in the frozen model repre-sentations.
we adopt the ten probing tasks in the.
1the minibertas’ training data is randomly sampled fromwikipedia and smashwords in a ratio of 3:1. these twodatasets are what devlin et al.
(2019) use to pretrain bertand represent a subset of the data used to pretrain roberta.
robertabase’s training data also includes of news and webdata in addition to wikipedia and smashwords.
warstadt et al.
ran pretraining 25 times with varying hyperparameter valuesand model sizes for the 1m-, 10m-, and 100m-word settings,and 10 times for the 1b-word setting.
all the models werepretrained with early stopping on validation set perplexity.
for each dataset size, they released the three models with thelowest validation set perplexity, yielding 12 models in total..2code:.
https://github.com/nyu-mll/.
pretraining-learning-curves.
3we use scipy’s curve fit implementation.
4in our case, a trial is one word of pretraining.
5we plot the no-pretraining random baseline with an x-.
value of 1..6the unnormalized results are included in the appendix..1113figure 2: classiﬁer probing results for each task in the edge probing suite.
results are adjusted with min-maxnormalization for readability (see the appendix for a non-normalized version).
in each subplot we also plot theoverall edge-probing performance, which we calculate for each miniberta as its average f1 score on the 10 edge-probing tasks (after normalization).
for context, we also plot bertlarge performance for each task as reportedby tenney et al.
(2019a)..we see two advantages to this method: first, thedownstream classiﬁer setting and f1 evaluationmetric make these experiments easier to interpretin the context of earlier results than results fromrelatively novel probing metrics like minimum de-scription length.
second, we focus on relative dif-ferences between models rather than absolute per-formance, and include a randomly initialized base-line model in the comparison.
when the model rep-resentations are random, the probe’s performancereﬂects the probe’s own ability to solve the targettask.
therefore, any improvements over this base-line value are due to the representation rather thanthe probe itself..task formulation and training following ten-ney et al., we use attention pooling to generaterepresentation(s) of the token span(s) involved inthe task and train an mlp that predicts whether agiven label correctly describes the input span(s).
we adopt the “mix” representation approach de-scribed in the paper.
to train the probes, we use thesame hyperparameters used in tenney et al.
andtune the batch size and learning rate.8.
results we plot results in figure 2. from thesingle-task curves we conclude that most of the.
8we.
randomly sample 5 pairs{8, 16, 32, 64} × {5e−5, 1e−4, 5e−4}..from the.
range.
figure 3: edge probing results for each group of tasksadjusted using min-max normalization.
syntactic tasksare part-of-speech, dependencies, and constituents.
the commonsense task is winograd coref.
semantictasks are all remaining tasks..edge probing suite (tenney et al., 2019b).7.classiﬁer probing has recently come underscrutiny.
hewitt and liang (2019) and voita andtitov (2020) caution that the results depend on thecomplexity of the probe, and so do not preciselyreveal the quality of the representations.
however,.
7task data sources: part-of-speech, constituents, entities,srl, and ontonotes coref.
from weischedel et al.
(2013), de-pendencies from silveira et al.
(2014), sem.
proto role 1 fromteichert et al.
(2017), sem.
proto role 2 from rudinger et al.
(2018), relations (semeval) from hendrickx et al.
(2010),and winograd coref.
from rahman and ng (2012); whiteet al.
(2017)..11140.00.51.0part-of-speechdependenciesconstituentsrelations (semeval)srlnone1m10m100m1b30b0.00.51.0sem.
proto role 1none1m10m100m1b30bsem.
proto role 2none1m10m100m1b30bontonotes coref.none1m10m100m1b30bentitiesnone1m10m100m1b30bwinograd coref.performance (normalized)overalllearning curvetasklearning curveoverall resultstask resultsbert-largetask performancenone1m10m100m1b30b0.20.00.20.40.60.81.0syntacticlearning curvesemanticlearning curvewinograd learning curvesyntactic resultssemantic resultswinograd resultsperformace (normalized)figure 4: mdl results for each edge probing task.
we do not plot a exponential curve for the winograd coref.
re-sults because we could not ﬁnd an adequate ﬁt..feature learning occurs with <100m words of pre-training data.
based on the best-ﬁt curve, we canestimate that 90% of the attainable improvementsin overall performance are achieved with <20mwords.
most plots show broadly similar learn-ing curves, which rise sharply with less than 1mwords of pretraining data, reach the point of fastestgrowth (in the log domain) around 1m words, andare nearly saturated with 100m words.
the mostnotable exception to this pattern is the winogradtask, which only rises signiﬁcantly between 1b and30b words of pretraining data.9 as the winogradtask is designed to test commonsense knowledgeand reasoning, the results suggest that these fea-tures require more data to encode than syntacticand semantic ones, with the caveat that the datasetis smaller than the other edge probing tasks, andresults on winograd tasks are highly sensitive tofactors such as task formulation (liu et al., 2020).
we observe some general differences betweendifferent types of tasks.
figure 3 shows the ag-gregated learning curves of syntactic, semantic,and commonsense tasks.
the syntactic learningcurve rises slightly earlier than the semantic oneand 90% of the improvements in syntactic learningcan be made with about 10m words, while the se-mantic curve still rises slightly after 100m.
thisis not surprising, as semantic computation is gen-erally thought to depend on syntactic representa-.
9these results are also noisier, similar to what tenney et al..(2019b) ﬁnd..tions (heim and kratzer, 1998).
the commonsenselearning curve (for winograd coref.
only) rises farlater, and is projected to continue to rise long aftersyntactic and semantic features stop improving..4 minimum description length probing.
in this experiment, we study the minibertas withmdl probing (voita and titov, 2020), with thegoal of revealing not only the total amount of fea-ture information extracted by the probe, but alsothe effort taken by the probe to extract the fea-tures.
mdl measures the minimum number of bitsneeded to transmit the labels for a given task giventhat both the sender and the receiver have access tothe pretrained model’s encoding of the data..a well-trained decoder model can help extractlabels from the representations and thus reduce thenumber of bits needed to transmit the labels.
sincethe model itself will also need to be transmitted,the total description length is a sum of two terms:the data codelength is the number of bits neededto transmit the labels assuming the receiver has thetrained decoder model, i.e.
the cross-entropy loss ofthe decoder.
the model codelength is the numberof bits needed to transmit the decoder parameters.
we follow voita and titov’s online code esti-mation of mdl, where the decoder is implicitlytransmitted.
as in section 3, we train decodersusing the same hyperparameter settings and task.
1115080016002400part-of-speech0100200300dependencies0100020003000constituents0102030relations (semeval)0300600900srl0204060sem.
proto role 10102030sem.
proto role 2none1m10m100m1b30b04080120ontonotes coref.none1m10m100m1b30b050100150entitiesnone1m10m100m1b30b0.01.53.04.5winograd coref.none1m10m100m1b30b01mdl (normalized)overallminimum description length (kbits)model codelengthdata codelengthoverall codelengthlearning curveoverall codelengthresultsdeﬁnitions as tenney et al.
(2019b).10.results we plot the online code results in figure4. the overall codelength shows a similar trend toedge probing: most of the reduction in feature code-length is achieved with fewer than 100m words.
mdl for syntactic features decreases even sooner.
results for winograd are idiosyncratic, probablydue to the failure of the probes to learn the task..the changes in model codelength and data code-length are shown on the bar plots in figure 4. wecompute the data codelength following voita andtitov (2020) using the training set loss of a clas-siﬁer trained on the entire training set, and themodel codelength is the total codelength minusthe data codelength.
the monotonically decreas-ing data codelength simply reﬂects the fact that themore data rich roberta models have smaller loss.
when it comes to the model codelength, however,we generally observe the global minimum for therandomly initialized models (i.e., at “none”).
thisis expected, and intuitively reﬂects the fact that adecoder trained on random representations wouldprovide little information about the labels, and soit would be optimal to transmit a very simple de-coder.
on many tasks, the model codelength startsto decrease when the pretraining data volume ex-ceeds a certain amount.
however, this trend is notconsistent across tasks and the effect is relativelysmall..5 unsupervised grammaticality.
judgement.
we use the blimp benchmark (warstadt et al.,2020a) to test models’ knowledge of individualgrammatical phenomena in english.
blimp is achallenge set of 67 tasks, each containing 1000minimal pairs of sentences that highlight a particu-lar morphological, syntactic, or semantic phenom-ena.
minimal pairs in blimp consist of two sen-tences that differ only by a single edit, but contrastin grammatical acceptability.
a language modelclassiﬁes a minimal pair correctly if it assigns ahigher probability to the acceptable sentence.
sinceroberta is a masked language model (mlm), wemeasure pseudo log-likelihood (wang and cho,2019) to score sentences (salazar et al., 2020)..results we plot learning curves for blimp infigure 5. warstadt et al.
organize the 67 tasks in.
blimp into 12 categories based on the phenom-ena tested and for each category we plot the aver-age accuracy for the tasks in the category.
we donot normalize results in this plot.
for the no-databaseline, we plot chance accuracy of 50% ratherthan making empirical measurements from randomroberta models..we ﬁnd the greatest improvement in overallblimp performance between 1m and 100m wordsof pretraining data.
with 100m words, sensitivityto contrasts in acceptability overall is within 9 accu-racy points of humans, and improves only 6 pointswith additional data.
this shows that substantialknowledge of many grammatical phenomena canbe acquired from 100m words of raw text..we also observe signiﬁcant variation in howmuch data is needed to learn different phenomena.
we see the steepest learning curves on agreementphenomena, with nearly all improvements occur-ring between 1m and 10m words.
for phenom-ena involving wh-dependencies, i.e.
ﬁller-gap de-pendencies and island effects, we observe shallowand delayed learning curves with 90% of possibleimprovements occurring between 1m and 100mwords.
the relative difﬁculty of wh-dependenciescan probably be ascribed to the long-distance na-ture and lower frequency of those phenomena.
wealso observe that the phenomena tested in the quan-tiﬁers category are never effectively learned, evenby robertabase.
these phenomena include sub-tle semantic contrasts—for example nobody ate{more than, *at least} two cookies—which mayinvolve difﬁcult-to-learn pragmatic knowledge (co-hen and krifka, 2014)..6 unsupervised language model.
knowledge probe.
lama is a test suite introduced by petroni et al.
to test lms’ factual knowledge.
it contains over50,000 cloze statements converted from subject-relation-object triples or question-answer pairs ex-tracted from four datasets: googlere,11 tre-x (el-sahar et al., 2018), conceptnet (speer and havasi,2012), and squad (rajpurkar et al., 2016).
thegoogle-re and t-rex tasks are each divided intothree sub-tasks..results we plot the results on lama in figure6. the fastest growing point of most curves appearsafter 100m words.
this relatively large quantity of.
10unlike us, voita and titov redeﬁne the edge probing tasks.
11source: https://code.google.com/archive/.
as standard multi-class classiﬁcation tasks..p/relation-extraction-corpus/..1116figure 5: blimp results by category.
blimp has 67 constituent datasets covering 12 linguistic phenomena.
foreach task the objective is to predict the more grammatically acceptable sentence of a minimal pair in an unsuper-vised setting.
for context, we also plot human accuracy numbers from warstadt et al.
(2020a) and robertalargeperformance from salazar et al.
(2020)..data may be needed for the model to be exposed torelevant factual knowledge.
the learning curves formany lama tasks do not show clear signs of satu-ration in the range of 0 to 30b words, suggestingfurther improvements are likely with much largerdata quantities.
among lama tasks, concept-net most directly tests commonsense knowledge.
the steep slope of the conceptnet curve between100m and 30b words of pretraining data and thelarge precision jump (> 0.05) from 1b to 30bshow that increasing the pretraining data to over1b words signiﬁcantly improve the lm’s common-sense knowledge, which explains the shape of thewinograd coref.
learning curve in section 3..7 fine-tuning on nlu tasks.
superglue is a benchmark suite of eightclassiﬁcation-based language-understanding tasks(wang et al., 2019).
we test each miniberta onﬁve superglue tasks on which we expect to seesigniﬁcant variation at these scales.12 the hyperpa-.
12task data sources: cb from de marneffe et al.
(2019),boolq from clark et al.
(2019), copa from roemmele et al.
(2011), wic from pilehvar and camacho-collados (2019);miller (1995); schuler (2005), and rte from dagan et al..rameter search range used for each task is describedin the appendix..results we plot the results on the selected su-perglue tasks in figure 7. improvements in su-perglue performance require a relatively largevolume of pretraining data.
for most tasks, thepoint of fastest improvement in our interpolatedcurve occurs with more than 1b words.
none of thetasks (with the possible exception of commitment-bank) show any signiﬁcant sign of saturation at30b words.
this suggests that some key nlu skillsare not learnt with fewer than billions of words, andthat models are likely to continue improving sub-stantially on these tasks given 10 to 100 times morepretraining data..8 discussion.
figure 1 plots the overall learning curves for theseﬁve methods together.
the most striking resultis that good nlu task performance requires farmore data than achieving good representationsfor linguistic features.
classiﬁer probing, mdl.
(2006); bar haim et al.
(2006); giampiccolo et al.
(2007);bentivogli et al.
(2009)..11176080100anaphor agreementargument structurebindingcontrol/raisingdet.-noun agreement6080100ellipsisfiller-gap dep.none1m10m100m1b30birregular formsnone1m10m100m1b30bisland effectsnone1m10m100m1b30bnpi licensingnone1m10m100m1b30b6080100quantifiersnone1m10m100m1b30bsubj.-verb agreementaccuracyoveralllearning curvephenomenonlearning curveoverall resultsphenomenon resultsroberta-largetask performancehuman task agreementfigure 6: lama results.
the metric for all tasks is mean precision at 1, i.e.
the proportion of examples where themodel assigns the highest probability to the ground truth token.
for context, we also plot robertalarge results..figure 7: superglue results.
the metric for boolq, copa, wic, rte is accuracy, and for cb it is the averageof accuracy and f1 score.
results are adjusted with min-max normalization for readability (see the appendix for anon-normalized version).
for context, we plot robertalarge performance reported at https://github.com/pytorch/fairseq/tree/master/examples/roberta..probing, and acceptability judgment performanceall improve rapidly between 1m and 10m wordsand show little improvement beyond 100m words,while performance on the nlu tasks in super-glue appears to improve most rapidly with over1b words and will likely continue improving atlarger data scales.
while the linguistic features wetest are undoubtedly needed to robustly solve mostnlu tasks, a model that can extract and encode alarge proportion of these features may still performpoorly on superglue.
what drives improvementsin nlu task performance at larger data scales re-mains an open question..factual knowledge may play a large role in ex-plaining superglue performance.
this hypoth-esis is backed up by results from the winogradedge-probing task (figure 2) and the lama tasks(figure 6), which suggest that most of the im-.
provements in the model’s world and commonsenseknowledge are made with over 100m words.
how-ever, the lama learning curve shows signs ofslowing between 1b and 30b words, the super-glue curve does not..another possible explanation is that linguisticfeatures encoded by a model may not be easily ac-cessible during ﬁne-turning.
warstadt et al.
(2020b)found that roberta can learn to reliably extractmany linguistic features with little pretraining data,but requires billions of words of pretraining databefore it uses those features preferentially whengeneralizing..in light of warstadt et al.’s ﬁndings, we hadinitially hypothesized that feature accessibility asmeasured by mdl might show a shallower or laterlearning curve than standard classiﬁer probing.13.
13warstadt et al.’s experiments are quite different to ours..11180.000.050.100.15google-re:date of birth0.000.050.100.15google-re:place of birth0.000.050.100.15google-re:place of death0.000.050.100.15google-re:total0.000.080.16conceptnetnone1m10m100m1b30b0.00.20.40.6t-rex: 1-1none1m10m100m1b30b0.00.10.20.3t-rex: n-1none1m10m100m1b30b0.000.060.120.18t-rex: n-mnone1m10m100m1b30b0.000.080.160.24t-rex: totalnone1m10m100m1b30b0.000.060.120.18squadmean precision @ 1 overalllearning curvetasklearning curveoverall resultstask resultsroberta-largetask performancenone1m10m100m1b30b0.00.51.0cbnone1m10m100m1b30bboolqnone1m10m100m1b30bcopanone1m10m100m1b30bwicnone1m10m100m1b30brteperformance(normalized)overalllearning curvetasklearning curveoverall resultstask resultsroberta-largetask performanceour ﬁndings do not support this hypothesis: fig-ure 1 shows no substantial difference between theclassiﬁer probing mdl probing curves..however, we do not totally rule out the possi-bility that linguistic feature accessibility continuesto improve with massive pretraining sets.
thereare potential modiﬁcations to voita and titov’sapproach that could more faithfully estimate fea-ture accessibility.
first, although roberta is actu-ally ﬁne-tuned in most applications, we and voitaand titov measure mdl taking the outputs of thefrozen roberta model as input to a trainable mlpdecoder.
it may be more relevant to measure mdlby ﬁne-tuning the entire model (lovering et al.,2021).
second, mdl actually estimates the infor-mation content of a particular dataset, rather thanthe feature itself.
whitney et al.
(2020) propose analternative to mdl that measures feature complex-ity in a way that does not depend on the size of thedataset..9 related work.
probing neural network representations has beenan active area of research in recent years (belinkovand glass, 2019; rogers et al., 2020).
with theadvent of large pretrained transformers like bert(devlin et al., 2019), numerous papers have usedclassiﬁer probing methods to attempt to locatelinguistic features in learned representations withstriking positive results (tenney et al., 2019b; he-witt and manning, 2019).
however, another threadhas found problems with many probing methods:classiﬁer probes can learn too much from train-ing data (hewitt and liang, 2019) and can fail todistinguish features that are extractable from fea-tures that are actually used when generalizing ondownstream tasks (voita and titov, 2020; pimentelet al., 2020; elazar et al., 2020).
moreover, dif-ferent probing methods often yield contradictoryresults (warstadt et al., 2019)..there have also been a few earlier studies inves-tigating the relationship between pretraining datavolume and linguistic knowledge in language mod-els.
studies of unsupervised acceptability judg-ments ﬁnd fairly consistent evidence of rapid im-provements in linguistic knowledge up to about10m words of pretraining data, after which im-provements slow down for most phenomena.
van.
they measure roberta’s preference for linguistic featuresover surface features during ﬁne-tuning on ambiguous classiﬁ-cation tasks..schijndel et al.
(2019) ﬁnd large improvements inknowledge of subject-verb agreement and reﬂexivebinding up to 10m words, and little improvementbetween 10m and 80m words.
hu et al.
(2020)ﬁnd that gpt-2 trained on 42m words performsroughly as well on a syntax benchmark as a similarmodel trained on 100 times that amount.
otherstudies have investigated how one model’s linguis-tic knowledge changes during the training process,as a function of the number of updates (saphra andlopez, 2019; chiang et al., 2020)..raffel et al.
(2020) also investigate how per-formance on superglue (and other downstreamtasks) improves with pretraining dataset size be-tween about 8m and 34b tokens.
in contrast to ourﬁndings, they ﬁnd that models with around 500mtokens of pretraining data can perform similarlyon downstream tasks to models with 34b words.
however, there are many differences in our set-tings that may lead to this divergence.
for example,they pretrain for a ﬁxed number of iterations (total-ing 34b token updates), whereas the minibertaswe use were pretrained with early stopping.
theyalso use preﬁx prompts in their task formulations,and adopt an encoder-decoder architecture and thustheir model has roughly twice the number of pa-rameters of the largest model we evaluate..there is also some recent work that investigatesthe effect of pretraining data size of other lan-guages.
micheli et al.
(2020) pretrain bert-basedlanguage models on 10mb, 100mb, 500mb, 1gb,2gb, and 4gb of french text and test them on aquestion answering task.
they ﬁnd that the frenchmlm pretrained on 100mb of raw text has sim-ilar performance to the ones pretrained on largerdatasets on the task, and that corpus-speciﬁc self-supervised learning does not make a signiﬁcant dif-ference.
martin et al.
(2020) also show that frenchmlms can already learn a lot from small-scalepretraining..concurrent work (liu et al., 2021) probesroberta models pretrained on different numbersof iterations using a set of probing tasks similar toours.
they ﬁnd that linguistic abilities are acquiredfastest, world and commonsense knowledge learn-ing takes more iterations, and reasoning abilitiesare never stably acquired.
both studies show thatlinguistic knowledge is easier to learn than factualknowledge..111910 conclusion.
we track several aspects of roberta’s ability aspretraining data increases.
we ﬁnd that ability insyntax and semantics largely saturates after only10m to 100m words of pretraining data—on parwith the data available to human learners—whilelearning factual knowledge requires much moredata.
we also ﬁnd that scaling pretraining data sizepast billions of words signiﬁcantly improves thenlu performance, though we cannot fully explainwhat abilities drive this improvement.
answeringthis question could be a stepping stone to moredata-efﬁcient models..acknowledgments.
this material is based upon work supported bythe national science foundation under grant no.
1850208. any opinions, ﬁndings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily re-ﬂect the views of the national science foundation.
we would like to thank udit arora, jason phang,clara vania, and ml2 for feedback on an earlierdraft.
thanks also to kyunghyun cho, tal linzen,grusha prasad, and emin orhan for suggestionsregarding the exponential learning curve, and toelena voita, ian tenney, and haokun liu for thediscussion about the implementation of the probingmethods..ethical considerations.
there are several ethical reasons to study lms withlimited pretraining data.
training massive lmslike roberta from scratch comes with non-trivialenvironmental costs (strubell et al., 2019), and theyare expensive to train, limiting contributions to pre-training research from scientists in lower-resourcecontexts.
by evaluating lms with limited pretrain-ing, we demonstrate that smaller lms match mas-sive ones in performance in many respects.
we alsoidentify a clear gap in our knowledge regardingwhy extensive pretraining is effective.
answeringthis question could lead to more efﬁcient pretrain-ing and ultimately reduce environmental costs andmake nlp more accessible.
on the other hand,there is a danger that our work, by projecting sub-stantial gains in model performance by increasingpretraining size, could legitimize and encouragethe trend of ever growing datasets..massive lms also replicate social biases presentin training data (nangia et al., 2020).
by establish-.
ing benchmarks for smaller lms and highlightingtheir efﬁcacy for certain purposes, we hope to spurfuture work that takes advantage of smaller pretrain-ing datasets to carefully curate the data distribution,as advocated by bender et al.
(2021), in order tobuild lms that do less to reproduce harmful biasesand are more inclusive of minority dialects..references.
yossi adi, einat kermany, yonatan belinkov, oferlavi, and yoav goldberg.
2017. fine-grained anal-ysis of sentence embeddings using auxiliary pre-in proceedings of iclr conferencediction tasks.
track.
toulon, france..roy bar haim, ido dagan, bill dolan, lisa ferro,danilo giampiccolo, bernardo magnini, and idanszpektor.
2006. the second pascal recognisingtextual entailment challenge.
in proceedings of thesecond pascal challenges workshop on recognis-ing textual entailment..yonatan belinkov and james r. glass.
2019. analysismethods in neural language processing: a survey.
transactions of the association for computationallinguistics, 7:49–72..emily m bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language modelsbe too big.
proceedings of facct..luisa bentivogli, ido dagan, hoa trang dang, danilogiampiccolo, and bernardo magnini.
2009. theﬁfth pascal recognizing textual entailment chal-lenge.
in textual analysis conference (tac)..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in advances in neural information processingsystems..cheng-han chiang, sung-feng huang, and hung-yilee.
2020. pretrained language model embryology:the birth of albert.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 6813–6828, on-line.
association for computational linguistics..christopher clark, kenton lee, ming-wei chang,tom kwiatkowski, michael collins, and kristinatoutanova.
2019. boolq: exploring the surprising.
1120in proceed-difﬁculty of natural yes/no questions.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2924–2936..betty hart and todd r. risley.
american parent-ing of language-learning children: persisting differ-ences in family-child interactions observed in natu-ral home environments.
developmental psychology,28(6):1096..ariel cohen and manfred krifka.
2014. superlativequantiﬁers and meta-speech acts.
linguistics andphilosophy, 37(1):41–90..ido dagan, oren glickman, and bernardo magnini.
the pascal recognising textual entail-2006.in machine learning challenges.
ment challenge.
evaluating predictive uncertainty, visual objectclassiﬁcation, and recognising textual entailment.
springer..marie-catherine de marneffe, mandy simons, and ju-dith tonhauser.
2019. the commitmentbank: inves-tigating projection in naturally occurring discourse.
in proceedings of sinn und bedeutung, volume 23,pages 107–124..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..yanai elazar, shauli ravfogel, alon jacovi, and yoavgoldberg.
2020. when bert forgets how to pos:amnesic probing of linguistic properties and mlmpredictions.
arxiv preprint 2006.00995..hady elsahar, pavlos vougiouklis, arslen remaci,christophe gravier,jonathon hare, frederiquelaforest, and elena simperl.
2018. t-rex: a largescale alignment of natural language with knowledgebase triples.
in proceedings of the eleventh interna-tional conference on language resources and eval-uation (lrec-2018)..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..allyson ettinger, ahmed elgohary, and philip resnik.
2016. probing for semantic evidence of compositionby means of simple classiﬁcation tasks.
in proceed-ings of the 1st workshop on evaluating vector-spacerepresentations for nlp, pages 134–139..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recogniz-ing textual entailment challenge.
in proceedings ofthe acl-pascal workshop on textual entailmentand paraphrasing.
association for computationallinguistics..andrew heathcote, scott brown, and douglas jk me-whort.
2000. the power law repealed: the case foran exponential law of practice.
psychonomic bul-letin & review, 7(2):185–207..irene heim and angelika kratzer.
1998. semantics in.
generative grammar.
blackwell oxford..iris hendrickx, su nam kim, zornitsa kozareva,preslav nakov, diarmuid ´o s´eaghdha, sebastianpad´o, marco pennacchiotti, lorenza romano, andstan szpakowicz.
2010.semeval-2010 task 8:multi-way classiﬁcation of semantic relations be-in proceedings of thetween pairs of nominals.
5th international workshop on semantic evalua-tion, pages 33–38, uppsala, sweden.
associationfor computational linguistics..john hewitt and percy liang.
2019. designing and in-terpreting probes with control tasks.
in conferenceon empirical methods in natural language process-ing.
association for computational linguistics..john hewitt and christopher d manning.
2019. astructural probe for ﬁnding syntax in word represen-in proceedings of the 2019 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4129–4138..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020. a systematic assessmentof syntactic generalization in neural language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 1725–1744, online.
association for compu-tational linguistics..nathaniel leibowitz, barak baum, giora enden, andamir karniel.
2010. the exponential learning equa-tion as a function of successful trials results in sig-moid performance.
journal of mathematical psy-chology, 54(3):338–340..haokun liu, william huang, dhara mungra, andsamuel r. bowman.
2020. precise task formaliza-tion matters in winograd schema evaluations.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8275–8280, online.
association for computa-tional linguistics..leo z. liu, yizhong wang, jungo kasai, hannaneh ha-jishirzi, and noah a. smith.
2021. probing acrosstime: what does roberta know and when?
corr,abs/2104.07885..1121yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..charles lovering, rohan jha, tal linzen, and el-lie pavlick.
2021. predicting inductive biases ofﬁne-tuned models.
in international conference onlearning representations..louis martin, benjamin muller, pedro javier or-tiz su´arez, yoann dupont, laurent romary, ´ericde la clergerie, djam´e seddah, and benoˆıt sagot.
2020. camembert: a tasty french language model.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages7203–7219, online.
association for computationallinguistics..vincent micheli, martin d’hoffschmidt, and franc¸oisfleuret.
2020. on the importance of pre-trainingdata volume for compact language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7853–7858, online.
association for computa-tional linguistics..george a miller.
1995. wordnet: a lexical database.
for english.
communications of the acm..nikita nangia, clara vania, rasika bhalerao, andsamuel r. bowman.
2020. crows-pairs: a chal-lenge dataset for measuring social biases in maskedlanguage models.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1953–1967, online.
as-sociation for computational linguistics..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..mohammad taher pilehvar and jose camacho-collados.
2019. wic: the word-in-context datasetfor evaluating context-sensitive meaning representa-tions.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt).
association for computational lin-guistics..tiago pimentel, josef valvoda, rowan hall maudslay,ran zmigrod, adina williams, and ryan cotterell.
2020.information-theoretic probing for linguisticstructure.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4609–4622, online.
association for computa-tional linguistics..yada pruksachatkun,.
jason phang, haokun liu,phu mon htut, xiaoyi zhang, richard yuanzhepang, clara vania, katharina kann, and samuel r.bowman.
2020. intermediate-task transfer learningwith pretrained language models: when and whyin proceedings of the 58th annualdoes it work?
meeting of the association for computational lin-guistics, pages 5231–5247, online.
association forcomputational linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..altaf rahman and vincent ng.
2012. resolving com-plex cases of deﬁnite pronouns: the winogradschema challenge.
in proceedings of the 2012 jointconference on empirical methods in natural lan-guage processing and computational natural lan-guage learning, pages 777–789, jeju island, korea.
association for computational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392..melissa roemmele, cosmin adrian bejan, and an-drew s. gordon.
2011. choice of plausible alterna-tives: an evaluation of commonsense causal reason-ing.
in 2011 aaai spring symposium series..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
in findings of emnlp..rachel rudinger, adam teichert, ryan culkin, shengzhang, and benjamin van durme.
2018. neural-in pro-davidsonian semantic proto-role labeling.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 944–955, brussels, belgium.
association for computa-tional linguistics..julian salazar, davis liang, toan q. nguyen, and ka-trin kirchhoff.
2020. masked language model scor-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 2699–2712, online.
association for compu-tational linguistics..naomi saphra and adam lopez.
2019. understand-ing learning dynamics of language models within proceedings of the 2019 conferencesvcca.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 3257–3267, minneapolis, minnesota.
associ-ation for computational linguistics..1122marten van schijndel, aaron mueller, and tal linzen.
2019. quantity doesn’t buy quality syntax within proceedings of theneural language models.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5831–5837, hong kong,china.
association for computational linguistics..karin kipper schuler.
2005..verbnet: a broad-coverage, comprehensive verb lexicon.
ph.d. the-sis, university of pennsylvania..natalia silveira, timothy dozat, marie-catherinede marneffe, samuel bowman, miriam connor,john bauer, and chris manning.
2014. a gold stan-dard dependency corpus for english.
in proceedingsof the ninth international conference on languageresources and evaluation (lrec’14), pages 2897–2904, reykjavik, iceland.
european language re-sources association (elra)..robert speer and catherine havasi.
2012. represent-ing general relational knowledge in conceptnet 5. inlrec, pages 3679–3686..emma strubell, ananya ganesh, and andrew mccal-lum.
2019. energy and policy considerations forin proceedings of the 57thdeep learning in nlp.
annual meeting of the association for computa-tional linguistics, pages 3645–3650, florence, italy.
association for computational linguistics..adam teichert, adam poliak, benjamin van durme,and matthew gormley.
2017. semantic proto-rolein aaai conference on artiﬁcial intelli-labeling.
gence..ian tenney, dipanjan das, and ellie pavlick.
2019a.
inbert rediscovers the classical nlp pipeline.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r. thomas mccoy, najoung kim,benjamin van durme, samuel r bowman, dipan-jan das, et al.
2019b.
what do you learn from con-text?
probing for sentence structure in contextual-ized word representations.
in proceedings of iclr..elena voita and ivan titov.
2020..information-theoretic probing with minimum description length.
in proceedings of the 2020 conference on empiri-cal methods in natural language processing, puntacana, dominican republic.
association for compu-tational linguistics..alex wang and kyunghyun cho.
2019. bert hasa mouth, and it must speak: bert as a markovin proceedings ofrandom ﬁeld language model.
the workshop on methods for optimizing and eval-uating neural language generation, pages 30–36,minneapolis, minnesota.
association for computa-tional linguistics..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r. bowman.
2019. superglue:a stickier benchmark for general-purpose languageunderstanding systems.
in 33rd conference on neu-ral information processing systems..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355..alex warstadt, yu cao, ioana grosu, wei peng, ha-gen blix, yining nie, anna alsop, shikha bordia,haokun liu, alicia parrish, sheng-fu wang, jasonphang, anhad mohananey, phu mon htut, palomajeretiˇc, and samuel r. bowman.
2019. investigatingbert’s knowledge of language: five analysis meth-ods with npis.
in proceedings of emnlp-ijcnlp,pages 2870–2880..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020a.
blimp: the benchmark of lin-transactionsguistic minimal pairs for english.
of the association for computational linguistics,8:377–392..alex warstadt, yian zhang, xiaocheng li, haokunliu, and samuel r. bowman.
2020b.
learningwhich features matter: roberta acquires a prefer-inence for linguistic generalizations (eventually).
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 217–235, online.
association for computa-tional linguistics..ralph weischedel, martha palmer, marcus mitchell,eduard hovy, sameer pradhan, lance ramshaw, ni-anwen xue, ann taylor, jeff kaufman, michellefranchini, mohammed el-bachouti, robert belvin,and ann houston.
2013. ontonotes release 5.0ldc2013t19.
linguistic data consortium..aaron steven white, pushpendre rastogi, kevin duh,inference is ev-and benjamin van durme.
2017.erything: recasting semantic resources into a uni-in proceedings of theﬁed evaluation framework.
eighth international joint conference on naturallanguage processing (volume 1: long papers),pages 996–1005..william f whitney, min jae song, david brand-fonbrener, jaan altosaar, and kyunghyun cho.
2020. evaluating representations by the complex-ity of learning low-loss predictors.
arxiv preprintarxiv:2009.07368..1123a appendices.
task.
batch size.
learning rate.
validation interval max epochs.
boolq.
cb.
{2,4,8}.
{2,4,8}.
{1e-6, 5e-6, 1e-5}.
{1e-5, 5e-5, 1e-4}.
copa {16,32,64}.
{1e-6, 5e-6, 1e-5}.
rte.
wic.
{2,4,8}.
{5e-6, 1e-5, 5e-5}.
{16,32,64}.
{1e-5, 5e-5, 1e-4}.
2400.
60.
100.
1000.
1000.table 1: hyperparameter search ranges for the superglue tasks.
our search ranges are largely based on thoseused in pruksachatkun et al.
(2020)..eldom.human.
5-gramlstmtxlgpt-2bertbaserobertabase1b-11b-21b-3100m-1100m-2100m-310m-110m-210m-31m-11m-21m-3.
allrevo.
88.6.
60.568.968.780.184.285.482.381.082.076.379.779.172.072.671.458.558.558.7.
97.5.
47.991.794.199.697.097.397.797.598.693.997.295.888.091.191.467.966.068.4.rgaa.na.rtsg.ra.gnidnib.s.iarl.rtc.rgad-n.sispille.pagrellif.ralugerri.
90.0.
71.973.269.578.380.083.580.779.179.374.679.176.970.370.171.160.460.060.3.
87.3.
64.473.574.780.182.377.877.378.378.572.775.476.074.071.671.458.557.857.5.
83.9.
68.567.071.580.579.681.980.779.477.277.079.675.470.370.766.459.458.859.1.
92.2.
70.085.483.093.397.697.095.896.095.393.294.595.690.091.690.559.561.161.3.
85.0.
36.967.677.286.689.491.491.692.291.289.991.693.783.786.085.354.655.755.1.
86.9.
58.172.564.979.083.190.183.182.183.174.378.876.866.867.365.861.661.561.2.
97.0.
79.589.178.284.196.596.292.594.894.889.992.793.989.684.391.378.178.677.7.sreifitnauq.
86.6.
53.564.569.371.371.269.868.761.770.561.664.760.962.958.662.364.865.567.2.ipn.88.1.
45.551.755.278.984.781.079.981.282.676.677.280.271.375.669.154.255.056.6.rgas-v90.9.
60.380.176.089.092.491.989.489.689.578.187.586.974.577.081.152.554.252.9.table 2: blimp results.
5-gram, lstm, txl, gpt-2 scores come from warstadt et al.
(2020a).
bertbase scorescome from salazar et al.
(2020)..10.
40.
40.
40.
10.dnalsi.
84.9.
53.742.945.863.173.680.769.763.466.560.663.062.551.553.646.850.848.748.5.
1124figure 8: our absolute edge probing dev set results (not normalized) compared to bertlarge test set results fromtenney et al.
(2019b)..figure 9: our absolute superglue results (not normalized) compared to robertalarge results from liu et al.
(2019)..1125050100part-of-speechdependenciesconstituentsrelations (semeval)srlnone1m10m100m1b30b050100sem.
proto role 1none1m10m100m1b30bsem.
proto role 2none1m10m100m1b30bontonotes coref.none1m10m100m1b30bentitiesnone1m10m100m1b30bwinograd coref.performance (accuracy or f1)overalllearning curvetasklearning curveoverall resultstask resultsbert-largetask performancenone1m10m100m1b30b0.00.51.0cbnone1m10m100m1b30bboolqnone1m10m100m1b30bcopanone1m10m100m1b30bwicnone1m10m100m1b30brteperformance(accuracy/f1)overalllearning curvetasklearning curveoverall resultstask resultsroberta-largetask performance