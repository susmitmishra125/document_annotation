measuring fine-grained domain relevance of terms:a hierarchical core-fringe approach.
jie huang1,3 kevin chen-chuan chang1,3.
jinjun xiong2,3 wen-mei hwu1,3.
1university of illinois at urbana-champaign, usa2ibm thomas j. watson research center, usa3ibm-illinois center for cognitive computing systems research (c3sr), usa{jeffhj, kcchang, w-hwu}@illinois.edujinjun@us.ibm.com.
abstract.
we propose to measure ﬁne-grained domainrelevance– the degree that a term is relevantto a broad (e.g., computer science) or narrow(e.g., deep learning) domain.
such measure-ment is crucial for many downstream tasks innatural language processing.
to handle long-tail terms, we build a core-anchored semanticgraph, which uses core terms with rich descrip-tion information to bridge the vast remainingfringe terms semantically.
to support a ﬁne-grained domain without relying on a matchingcorpus for supervision, we develop hierarchi-cal core-fringe learning, which learns core andfringe terms jointly in a semi-supervised man-ner contextualized in the hierarchy of the do-main.
to reduce expensive human efforts, weemploy automatic annotation and hierarchi-cal positive-unlabeled learning.
our approachapplies to big or small domains, covers heador tail terms, and requires little human effort.
extensive experiments demonstrate that ourmethods outperform strong baselines and evensurpass professional human performance.1.
1.introduction.
with countless terms in human languages, no onecan know all terms, especially those belonging toa technical domain.
even for domain experts, itis quite challenging to identify all terms in the do-mains they are specialized in.
however, recogniz-ing and understanding domain-relevant terms is thebasis to master domain knowledge.
and having asense of domains that terms are relevant to is aninitial and crucial step for term understanding..in this paper, as our problem, we propose tomeasure ﬁne-grained domain relevance, which isdeﬁned as the degree that a term is relevant to a.
1the code and data, along with several.
term listswith domain relevance scores produced by our meth-ods are available at https://github.com/jeffhj/domain-relevance..given domain, and the given domain can be broador narrow– an important property of terms that hasnot been carefully studied before.
e.g., deep learn-ing is a term relevant to the domains of computerscience and, more speciﬁcally, machine learning,but not so much to others like database or compiler.
thus, it has a high domain relevance for the formerdomains but a low one for the latter.
from anotherperspective, we propose to decouple extraction andevaluation in automatic term extraction that aims toextract domain-speciﬁc terms from texts (amjadianet al., 2018; h¨atty et al., 2020).
this decouplingsetting is novel and useful because it is not limitedto broad domains where a domain-speciﬁc corpusis available, and also does not require terms mustappear in the corpus..a good command of domain relevance of termswill facilitate many downstream applications.
e.g.,to build a domain taxonomy or ontology, a crucialstep is to acquire relevant terms (al-aswadi et al.,2019; shang et al., 2020).
also, it can provide or ﬁl-ter necessary candidate terms for domain-focusedinnatural language tasks (huang et al., 2020).
addition, for text classiﬁcation and recommenda-tion, the domain relevance of a document can bemeasured by that of its terms..we aim to measure ﬁne-grained domain rele-vance as a semantic property of any term in humanlanguages.
therefore, to be practical, the proposedmodel for domain relevance measuring must meetthe following requirements: 1) covering almostall terms in human languages; 2) applying to a widerange of broad and narrow domains; and 3) relyingon little or no human annotation..however, among countless terms, only some ofthem are popular ones organized and associatedwith rich information on the web, e.g., wikipediapages, which we can leverage to characterize thedomain relevance of such “head terms.” in contrast,there are numerous “long-tail terms”– those not as.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3641–3651august1–6,2021.©2021associationforcomputationallinguistics3641frequently used– which lack descriptive informa-tion.
as challenge 1, how to measure the domainrelevance for such long-tail terms?.
on the other hand, among possible domains ofinterest, only those broad ones (e.g., physics, com-puter science) naturally have domain-speciﬁc cor-pora.
many existing works (velardi et al., 2001;amjadian et al., 2018; h¨atty et al., 2020) have re-lied on such domain-speciﬁc corpora to identifydomain-speciﬁc terms by contrasting their distribu-tions to general ones.
in contrast, those ﬁne-graineddomains (e.g., quantum mechanics, deep learning)–which can be any topics of interest– do not usuallyhave a matching corpus.
as challenge 2, how toachieve good performance for a ﬁne-grained do-main without assuming a domain-speciﬁc corpus?
finally, automatic learning usually requires largeamounts of training data.
since there are countlessterms and plentiful domains, human annotation isvery time-consuming and laborious.
as challenge3, how to reduce expensive human efforts when ap-plying machine learning methods to our problem?.
as our solutions, we propose a hierarchical core-fringe domain relevance learning approach that ad-dresses these challenges.
first, to deal with long-tail terms, we design the core-anchored semanticgraph, which includes core terms which have richdescription and fringe terms without that informa-tion.
based on this graph, we can bridge the do-main relevance through term relevance and includeany term in evaluation.
second, to leverage thegraph and support ﬁne-grained domains withoutrelying on domain-speciﬁc corpora, we propose hi-erarchical core-fringe learning, which learns thedomain relevance of core and fringe terms jointlyin a semi-supervised manner contextualized in thehierarchy of the domain.
third, to reduce humaneffort, we employ automatic annotation and hier-archical positive-unlabeled learning, which allowto train our model with little even no human effort..overall, our framework consists of two pro-cesses: 1) the ofﬂine construction process, wherea domain relevance measuring model is trained bytaking a large set of seed terms and their featuresas input; 2) the online query process, where thetrained model can return the domain relevance ofquery terms by including them in the core-anchoredsemantic graph.
our approach applies to a widerange of domains and can handle any query, whilenearly no human effort is required.
to validate theeffectiveness of our proposed methods, we conduct.
extensive experiments on various domains withdifferent settings.
results show our methods sig-niﬁcantly outperform well-designed baselines andeven surpass human performance by professionals..2 related work.
the problem of domain relevance of terms is re-lated to automatic term extraction, which aims toextract domain-speciﬁc terms from texts automati-cally.
compared to our task, automatic term extrac-tion, where extraction and evaluation are combined,possesses a limited application and has a relativelylarge dependence on corpora and human annota-tion, so it is limited to several broad domains andmay only cover a small number of terms.
existingapproaches for automatic term extraction can beroughly divided into three categories: linguistic,statistical, and machine learning methods.
linguis-tic methods apply human-designed rules to identifytechnical/legal terms in a target corpus (handleret al., 2016; ha and hyland, 2017).
statisticalmethods use statistical information, e.g., frequencyof terms, to identify terms from a corpus (frantziet al., 2000; nakagawa and mori, 2002; velardiet al., 2001; drouin, 2003; meijer et al., 2014).
machine learning methods learn a classiﬁer, e.g.,logistic regression classiﬁer, with manually labeleddata (conrado et al., 2013; fedorenko et al., 2014;h¨atty et al., 2017).
there also exists some work onautomatic term extraction with wikipedia (vivaldiet al., 2012; wu et al., 2012).
however, terms stud-ied there are restricted to terms associated with awikipedia page..recently, inspired by distributed representationsof words (mikolov et al., 2013a), methods basedon deep learning are proposed and achieve state-of-the-art performance.
amjadian et al.
(2016, 2018)design supervised learning methods by taking theconcatenation of domain-speciﬁc and general wordembeddings as input.
h¨atty et al.
(2020) propose amulti-channel neural network model that leveragesdomain-speciﬁc and general word embeddings..the techniques behind our hierarchical core-fringe learning methods are related to research ongraph neural networks (gnns) (kipf and welling,2017; hamilton et al., 2017); hierarchical text clas-siﬁcation (vens et al., 2008; wehrmann et al., 2018;zhou et al., 2020); and positive-unlabeled learning(liu et al., 2003; elkan and noto, 2008; bekkerand davis, 2020)..3642in this ﬁgure, machine learning is a core term associated with afigure 1: the overview of the framework.
wikipedia page, few-shot learning is a fringe term included in the ofﬂine core-anchored semantic graph, andquantum chemistry is a fringe term included in the online process.
best viewed in color..3 methodology.
we study the fine-grained domain relevance ofterms, which is deﬁned as follows:.
deﬁnition 1 (fine-grained domain relevance)the ﬁne-grained domain relevance of a term is thedegree that the term is relevant to a given domain,and the given domain can be broad or narrow..the domain relevance of terms depends on manyfactors.
in general, a term with higher semantic rel-evance, broader meaning scope, and better usagepossesses a higher domain relevance regarding thetarget domain.
to measure the ﬁne-grained domainrelevance of terms, we propose a hierarchical core-fringe approach, which includes an ofﬂine trainingprocess and can handle any query term in evalua-tion.
the overview of the framework is illustratedin figure 1..3.1 core-anchored semantic graph.
there exist countless terms in human languages;thus it is impractical to include all terms in a systeminitially.
to build the ofﬂine system, we need to pro-vide seed terms, which can come from knowledgebases or be extracted from broad, large corpora byexisting term/phrase extraction methods (handleret al., 2016; shang et al., 2018)..in addition to providing seed terms, we shouldalso give some knowledge to machines so that theycan differentiate whether a term is domain-relevantor not.
to this end, we can leverage the descrip-tion information of terms.
for instance, wikipedia.
contains a large number of terms (the surface formof page titles), where each term is associated witha wikipedia article page.
with this page informa-tion, humans can easily judge whether a term isdomain-relevant or not.
in section 3.3, we willshow the labeling can even be done completelyautomatically..however, considering the countless terms, thenumber of terms that are well-organized and associ-ated with rich description is small.
how to measurethe ﬁne-grained domain relevance of terms with-out rich information is quite challenging for bothmachines and humans..fortunately, terms are not isolated, while com-plex relations exist between them.
if a term isrelevant to a domain, it must also be relevant tosome domain-relevant terms and vice versa.
this isto say, we can bridge the domain relevance of termsthrough term relevance.
summarizing the obser-vations, we divide terms into two categories: coreterms, which are terms associated with rich descrip-tion information, e.g., wikipedia article pages, andfringe terms, which are terms without that informa-tion.
we assume, for each term, there exist somerelevant core terms that share similar domains.
ifwe can ﬁnd the most relevant core terms for a giventerm, its domain relevance can be evaluated withthe help of those terms.
to this end, we can utilizethe rich information of core terms for ranking..taking wikipedia as an example, each core termis associated with an article page, so they can.
3643few-shot learningquantum chemistry⋯0.8770.001⋯corefringequery termsdomain relevancecflhicflmachine learningdeep learningfew-shot learningquantum mechanics ⋯seed termsmodel traininggraph constructionofflineonlinebe returned as the ranking results (result term)for a given term (query term).
considering thedata resources, we use the built-in elasticsearchbased wikipedia search engine2 (gormley andtong, 2015).
more speciﬁcally, we set the max-imum number of links as k (5 as default).
for aquery term v, i.e., any seed term, we ﬁrst achievethe top 2k wikipedia pages with exact match.
foreach result term u in the core, we create a link fromu to v. if the number of links is smaller than k,we do this process again without exact match andbuild additional links.
finally, we construct a termgraph, named core-anchored semantic graph,where nodes are terms and edges are links betweenterms..in addition, for terms that are not provided ini-tially, we can also handle them as fringe terms andconnect them to core terms in evaluation.
in thisway, we can include any term in the graph..3.2 hierarchical core-fringe learning.
in this section, we aim to design learning methodsto learn the ﬁne-grained domain relevance of coreand fringe terms jointly.
in addition to using theterm graph, we can achieve features of both coreand fringe terms based on their linguistic and statis-tical properties (terryn et al., 2019; conrado et al.,2013) or distributed representations (mikolov et al.,2013b; yu and dredze, 2015).
we assume the la-bels, i.e., domain-relevant or not, of core terms areavailable, which can be achieved by an automaticannotation mechanism introduced in section 3.3.as stated above, if a term is highly relevant toa given domain, it must also be highly relevant tosome other terms with a high domain relevanceand vice versa.
therefore, to measure the domainrelevance of a term, in addition to using its own fea-tures, we aggregate its neighbors’ features.
specif-ically, we propagate the features of terms via theterm graph and use the label information of coreterms for supervision.
in this way, core and fringeterms help each other, and the domain relevanceis learned jointly.
the propagation process can beachieved by graph convolutions (hammond et al.,2011).
we ﬁrst apply the vanilla graph convolu-tional networks (gcns) (kipf and welling, 2017)in our framework.
the graph convolution operation(gcnconv) at the l-th layer is formulated as the.
following aggregation and update process:.
h(l+1)i.
= φ.
(cid:16) (cid:88).
w (l).
c h(l).
j + b(l)c.(cid:17).
,.
(1).
1cij.
j∈ni∪{i}.
where ni is the neighbor set of node i. cij is thenormalization constant.
h(l)j ∈ rd(l)×1 is the hid-den state of node j at the l-th layer, with d(l) beingthe number of units; h(0)j = xj, which is the fea-ture vector of node j. w (l)is thetrainable weight matrix at the l-th layer, and b(l)iscthe bias vector.
φ(·) is the nonlinearity activationfunction, e.g., relu(·) = max(0, ·)..c ∈ rd(l+1)×d(l).
since core terms are labeled as domain-relevantor not, we can use the labels to calculate the loss:.
l = −.
(cid:88).
i∈vcore.
(yi log zi + (1 − yi) log(1 − zi)),.
(2)where yi is the label of node i regarding the targetdomain, and zi = σ(hoi being the outputof the last gcnconv layer for node i and σ(·)being the sigmoid function.
the weights of themodel are trained by minimizing the loss.
therelative domain relevance is obtained as s = z..i ), with ho.
combining with the overall framework, we getthe ﬁrst domain relevance measuring model, cfl,i.e., core-fringe domain relevance learning..cfl is useful to measure the domain relevancefor broad domains such as computer science.
fordomains with relatively narrow scopes, e.g., ma-chine learning, we can also leverage the label in-formation of domains at the higher level of thehierarchy, e.g., cs → ai → ml, which is basedon the idea that a domain-relevant term regardingthe target domain should also be relevant to theparent domain.
inspired by related work on hierar-chical multi-label classiﬁcation (vens et al., 2008;wehrmann et al., 2018), we introduce a hierarchi-cal learning method considering both global andlocal information..we ﬁrst apply lc gcnconv layers according toeq.
(1) and get the output of the last gcnconvlayer, which is h(lc).
in order not to confuse, weomit the subscript that identiﬁes the node number.
for each domain in the hierarchy, we introduce ahierarchical global activation ap.
the activation atthe (l + 1)-th level of the hierarchy is given as.
i.a(l+1)p.= φ(w (l).
p [a(l).
p ; h(lc)] + b(l).
p ),.
(3).
2https://en.wikipedia.org/w/index.php?.
search.
where [·; ·] indicates the concatenation of two vec-p h(lc) + b(0)tors; a(1)p ).
the global in-.
p = φ(w (0).
3644formation is produced after a fully connected layer:.
zp = σ(w (lp).
p a(lp).
p + b(lp).
p.),.
(4).
where lp is the total number of hierarchical levels.
to achieve the local information for each levelof the hierarchy, the model ﬁrst generates the localhidden state a(l).
q by a fully connected layer:.
q = φ(w (l)a(l).
t a(l).
p + b(l)t )..(5).
the local information at the l-th level of the hierar-chy is then produced as.
q = σ(w (l)z(l).
q a(l).
q + b(l).
q )..(6).
in our core-fringe framework, all the core termsare labeled at each level of the hierarchy.
therefore,the loss of hierarchical learning is computed as.
lh = (cid:15)(zp, y(lp)) +.
(cid:15)(z(l).
q , y(l)),.
(7).
lp(cid:88).
l=1.
where y(l) denotes the labels regarding the domainat the l-th level of the hierarchy and (cid:15)(z, y) is thebinary cross-entropy loss described in eq.
(2).
intesting, the relative domain relevance s is calcu-lated as.
s = α · zp + (1 − α) · (z(1).
q ◦ z(2).
q , ..., z(lp).
q.
), (8).
where ◦ denotes element-wise multiplication.
αis a hyperparameter to balance the global and lo-cal information (0.5 as default).
combining withour general framework, we refer to this model ashicfl, i.e., hierarchical cfl..online query process.
if seed terms are providedby extracting from broad, large corpora relevant tothe target domain, most terms of interest will be al-ready included in the ofﬂine process.
in evaluation,for terms that are not provided initially, our modeltreats them as fringe terms.
speciﬁcally, when re-ceiving such a term, the model connects it to coreterms by the method described in section 3.1. withits features (e.g., compositional term embeddings)or only its neighbors’ features (when features can-not be generated directly), the trained model canreturn the domain relevance of any query..3.3 automatic annotation and hierarchical.
positive-unlabeled learning.
time-consuming and laborious because the num-ber of core terms is very large regarding a widerange of domains.
fortunately, in addition to build-ing the term graph, we can also leverage the richinformation of core terms for automatic annotation.
in the core-anchored semantic graph constructedwith wikipedia, each core term is associated witha wikipedia page, and each page is assigned oneor more categories.
all the categories form a hier-archy, furthermore providing a category tree.
fora given domain, we can ﬁrst traverse from a rootcategory and collect some gold subcategories.
forinstance, for computer science, we treat category:subﬁelds of computer science3 as the root categoryand take categories at the ﬁrst three levels of it asgold subcategories.
then we collect categories foreach core term and examine whether the term itselfor one of the categories is a gold subcategory.
ifso, we label the term as positive.
otherwise, welabel it as negative.
we can also combine gold sub-categories from some existing domain taxonomiesand extract the categories of core terms from thetext description, which usually contains useful textpatterns like “x is a subﬁeld of y”..hierarchical positive-unlabeled learning.
ac-cording to the above methods, we can learn the ﬁne-grained domain relevance of terms for any domainas long as we can collect enough gold subcategoriesfor that domain.
however, for domains at the lowlevel of the hierarchy, e.g., deep learning, a cate-gory tree might not be available in wikipedia.
todeal with this issue, we apply our learning methodsin a positive-unlabeled (pu) setting (bekker anddavis, 2020), where only a small number of terms,e.g., 10, are labeled as positive, and all the otherterms are unlabeled.
we use this setting based onthe following consideration: if a user is interestedin a speciﬁc domain, it is quite easy for her to givesome important terms relevant to that domain..beneﬁting from our hierarchical core-fringelearning approach, we can still obtain labels fordomains at the high level of the hierarchy with theautomatic annotation mechanism.
therefore, allthe negative examples of the last labeled hierarchycan be used as reliable negatives for the target do-main.
for instance, if the target domain is deeplearning, which is in the cs → ai → ml → dlhierarchy, we consider all the non-ml terms asthe reliable negatives for dl.
taking the positively.
automatic annotation.
for the ﬁne-grained do-main relevance problem, human annotation is very.
3https://en.wikipedia.org/wiki/.
category:subfields_of_computer_science.
3645labeled examples and the reliable negatives for su-pervision, we can learn the domain relevance ofterms by our proposed hicfl model contextual-ized in the hierarchy of the domain..4 experiments.
in this section, we evaluate our model from differ-ent perspectives.
1) we compare with baselinesby treating some labeled terms as queries.
2) wecompare with human professionals by letting hu-mans and machines judge which term in a querypair is more relevant to a target domain.
3) weconduct intuitive case studies by ranking termsaccording to their domain relevance..4.1 experimental setup.
datasets and preprocessing.
to build the sys-tem, for ofﬂine processing, we extract seed termsfrom the arxiv dataset (version 6)4. as an ex-ample, for computer science or its sub-domains,we collect the abstracts in computer science ac-cording to the arxiv category taxonomy5, andapply phrasemachine to extract terms (handleret al., 2016) with lemmatization and several ﬁl-tering rules: frequency > 10; length ≤ 6; onlycontain letters, numbers, and hyphen; not a stop-word or a single letter..we select three broad domains, including com-puter science (cs), physics (phy), and mathemat-ics (math); and three narrow sub-domains of them,including machine learning (ml), quantum me-chanics (qm), and abstract algebra (aa), with thehierarchies cs → ai → ml, phy → mechanics →qm, and math → algebra → aa.
each broad do-main and its sub-domains share seed terms becausethey share a corpus.
to achieve gold subcategoriesfor automatic annotation (section 3.3), we collectsubcategories at the ﬁrst three levels of a root cate-gory (e.g., category: subﬁelds of physics) for broaddomains (e.g., physics); or the ﬁrst two levels fornarrow domains, e.g., category: machine learningfor machine learning.
table 1 reports the total sizesand the ratios that are core terms..baselines.
since our task on ﬁne-grained domainrelevance is new, there is no existing baseline formodel comparison.
we adapt the following mod-els on relevant tasks in our setting with additionalinputs (e.g., domain-speciﬁc corpora):.
4https://www.kaggle.com/.
cornell-university/arxiv.
5https://arxiv.org/category_taxonomy.
domain.
#terms.
core ratio.
cs ml 113,038phy qm 416,431math aa 103,984.
27.7%12.1%26.4%.
table 1: the statistics of the data..• relative domain frequency (rdf): sincedomain-relevant terms usually occur more in adomain-speciﬁc corpus, we apply a statisticalmethod using freqs(w)/freqg(w) to measure thedomain relevance of term w, where freqs(·) andfreqg(·) denote the frequency of occurrence inthe domain-speciﬁc/general corpora respectively.
• logistic regression (lr): logistic regressionis a standard supervised learning method.
weuse core terms with labels (domain-relevant ornot) as training data, where features are termembeddings trained by a general corpus..• multilayer perceptron (mlp): mlp is a stan-dard neural neural-based model.
we train mlpusing embeddings trained with a domain-speciﬁccorpus or a general corpus as term features, re-spectively.
we also concatenate the two embed-dings as features (amjadian et al., 2016, 2018).
• multi-channel (mc): multi-channel (h¨attyet al., 2020) is the state-of-the-art model for au-tomatic term extraction, which is based on amulti-channel neural network that takes domain-speciﬁc and general corpora as input..training.
for all supervised learning methods, weapply automatic annotation in section 3.3, i.e., weautomatically label all the core terms for modeltraining.
in the pu setting, we remove labels ontarget domains.
only 20 (10 in the case studies)domain-relevant core terms are randomly selectedas the positives, with the remaining terms unla-beled.
in training, all the negative examples at theprevious level of the hierarchy are used as reliablenegatives..implementation details.
though our proposedmethods are independent of corpora, some base-lines (e.g., mc) require term embeddings trainedfrom general/domain-speciﬁc corpora.
for easyand fair comparison, we adopt the following ap-proach to generate term features.
we consider eachterm as a single token, and apply word2vec cbow(mikolov et al., 2013a) with negative sampling,where dimensionality is 100, window size is 5, andnumber of negative samples is 5. the training cor-.
3646computer science.
physics.
mathematics.
roc-auc.
pr-auc.
roc-auc.
pr-auc.
roc-auc.
pr-auc.
rdfsglrgsmlpmlp gmlpmccfl gcflc.0.7140.802±0.0000.819±0.0030.863±0.001sg 0.867±0.001sg 0.868±0.0020.885±0.0010.883±0.001.
0.4170.535±0.0000.594±0.0030.674±0.0020.667±0.0020.664±0.0060.712±0.0020.708±0.002.
0.7360.822±0.0000.853±0.0010.874±0.0010.875±0.0010.877±0.0030.905±0.0000.901±0.000.
0.4960.670±0.0000.739±0.0040.761±0.0030.765±0.0020.768±0.0040.812±0.0020.800±0.001.
0.6940.854±0.0000.868±0.0000.904±0.0010.904±0.0010.903±0.0010.918±0.0010.919±0.001.
0.5790.769±0.0000.803±0.0010.846±0.0020.843±0.0030.843±0.0020.870±0.0020.879±0.002.
s and g indicate the corpus used.
s: domain-speciﬁc corpus, g: general corpus, sg: both.
c means the pre-trained compositional glove embeddings are used..table 2: results for broad domains..pus can be a general one (the entire arxiv corpus,denoted as g), or a domain-speciﬁc one (the sub-corpus in the branch of the corresponding domain,denoted as s).
we also apply compositional gloveembeddings (pennington et al., 2014) (element-wise addition of the pre-trained 100d word embed-dings, denoted as c) as non-corpus-speciﬁc fea-tures of terms for reference..for all the neural network-based models, we useadam (kingma and ba, 2015) with learning rate of0.01 for optimization, and adopt a ﬁxed hidden di-mensionality of 256 and a ﬁxed dropout ratio of 0.5.for the learning part of cfl and hicfl, we applytwo gcnconv layers and use the symmetric graphfor training.
to avoid overﬁtting, we adopt batchnormalization (ioffe and szegedy, 2015) right aftereach layer (except for the output layer) and be-fore activation and apply dropout (hinton et al.,2012) after the activation.
we also try to add reg-ularizations for mlp and mc with full-batch ormini-batch training, and select the best architecture.
to construct the core-anchored semantic graph, weset k as 5. all experiments are run on an nvidiaquadro rtx 5000 with 16gb of memory underthe pytorch framework.
the training of cfl forthe cs domain can ﬁnish in 1 minute..we report the mean and standard deviation ofthe test results corresponding to the best validationresults with 5 different random seeds..4.2 comparison to baselines.
to compare with baselines, we separate a portion ofcore terms as queries for evaluation.
speciﬁcally,for each domain, we use 80% labeled terms fortraining, 10% for validation, and 10% for testing.
(with automatic annotation).
terms in the valida-tion and testing sets are treated as fringe terms.
bydoing this, the evaluation can represent the generalperformance for all fringe terms to some extent.
and the model comparison is fair since the richinformation of terms for evaluation is not used intraining.
we also create a test set with careful hu-man annotation on machine learning to supportour overall evaluation, which contains 2000 terms,with half for evaluation and half for testing..as evaluation metrics, we calculate both roc-auc and pr-auc with automatic or manuallycreated labels.
roc-auc is the area under the re-ceiver operating characteristic curve, and pr-aucis the area under the precision-recall curve.
if amodel achieves higher values, most of the domain-relevant terms are ranked higher, which means themodel has a better measurement on the domainrelevance of terms..table 2 and table 3 show the results for threebroad/narrow domains respectively.
we observeour proposed cfl and hicfl outperform all thebaselines, and the standard deviations are low.
compared to mlp, cfl achieves much better per-formance beneﬁting from the core-anchored seman-tic graph and feature aggregation, which demon-strates the domain relevance can be bridged viaterm relevance.
compared to cfl, hicfl worksbetter owing to hierarchical learning..in the pu setting– the situation when automaticannotation is not applied to the target domain, al-though only 20 positives are given, hicfl stillachieves satisfactory performance and signiﬁcantlyoutperforms all the baselines (table 4)..the pr-auc scores on the manually created test.
3647machine learning.
roc-auc.
pr-auc.
quantum mechanicspr-auc.
roc-auc.
abstract algebra.
roc-auc.
pr-auc.
lrmlpmlpmlpmccfl.
g0.917±0.000s0.902±0.0010.932±0.001gsg 0.928±0.001sg 0.928±0.0020.950±0.002ghicfl g0.965±0.003s and g indicate the corpus used.
s: domain-speciﬁc corpus, g: general corpus, sg: both..0.421±0.0000.545±0.0040.587±0.0140.574±0.0070.590±0.0030.678±0.0030.691±0.003.
0.879±0.0000.903±0.0010.922±0.0010.923±0.0000.924±0.0010.950±0.0000.957±0.001.
0.872±0.0000.910±0.0000.923±0.0000.925±0.0010.924±0.0010.938±0.0010.942±0.002.
0.346±0.0000.453±0.0090.562±0.0100.574±0.0110.554±0.0070.627±0.0130.645±0.014.
0.525±0.0000.641±0.0070.658±0.0060.673±0.0040.685±0.0050.751±0.0090.769±0.006.
table 3: results for narrow domains..machine learning.
roc-auc.
pr-auc.
quantum mechanicspr-auc.
roc-auc.
abstract algebra.
roc-auc.
pr-auc.
lrmlpmlpmlpmccfl.
0.860±0.000g0.804±0.003sg0.836±0.005sg 0.844±0.003sg 0.852±0.0060.918±0.001ghicfl g0.940±0.008.
0.206±0.0000.144±0.0030.234±0.0160.230±0.0150.251±0.0190.441±0.0090.508±0.026.
0.788±0.0000.767±0.0090.813±0.0060.796±0.0080.795±0.0140.897±0.0020.897±0.004.
0.280±0.0000.260±0.0050.295±0.0110.291±0.0110.303±0.0170.408±0.0040.421±0.014.
0.833±0.0000.804±0.0060.842±0.0030.839±0.0060.861±0.0040.887±0.0020.915±0.002.
0.429±0.0000.421±0.0100.467±0.0110.463±0.0130.547±0.0060.563±0.0180.648±0.009.
table 4: results for narrow domains (pu learning)..pr-auc.
pr-auc (pu).
lrmlpmlpmlpmccfl.
g 0.509±0.0000.550±0.017sg 0.586±0.016sg 0.590±0.005sg 0.603±0.016g 0.703±0.017hicfl g 0.755±0.011.
0.449±0.0000.113±0.0100.299±0.0270.217±0.0130.281±0.0120.525±0.0130.581±0.036.
table 5: results (pr-auc) for machine learning withmanual labeling..set without and with the pu setting are reportedin table 5. we observe that the results are gener-ally consistent with results reported in table 3 andtable 4, which indicates the evaluation with coreterms can work just as well..4.3 comparison to human performance.
in this section, we aim to compare our model withhuman professionals in measuring the ﬁne-graineddomain relevance of terms.
because it is difﬁ-cult for humans to assign a score representing do-.
ml-aihuman0.698±0.087hicfl 0.854±0.017.
ml-cs0.846±0.0740.932±0.007.
ai-cs0.716±0.1150.768±0.023.
table 6: accuracies of domain relevance comparison..main relevance directly, we generate term pairs asqueries and let humans judge which one in a pairis more relevant to machine learning.
speciﬁcally,we create 100 ml-ai, ml-cs, and ai-cs pairsrespectively.
taking ml-ai as an example, eachquery pair consists of an ml term and an ai term,and the judgment is considered right if the ml termis selected..the human annotation is conducted by ﬁve se-nior students majoring in computer science anddoing research related to terminology.
becausethere is no clear boundary between ml, ai, andcs, it is possible that a cs term is more relevantto machine learning than an ai term.
however, theoverall trend is that the higher the accuracy, thebetter the performance.
from table 6, we observethat hicfl far outperforms human performance..3648the depth of the background color indicates the domain relevance.
the darker the color, the higher the domain relevance (annotated by the authors);* indicates the term is a core term, otherwise it is a fringe term..1-10supervised learning*convolutional neural network*machine learning*deep learning*semi-supervised learning*q-learning*reinforcement learning*unsupervised learning*recurrent neural network*generative adversarial network*.
1001-1010regularization strategy.
101-110adversarial machine learning*temporal-difference learning* weakly-supervised approachrestricted boltzmann machinebackpropagation through time*svmsword2vec*rbmshierarchical clustering*stochastic gradient descent*svm*.
non-convex learningsample-efﬁcient learningcnn-rnn modeldeep bayesianclassiﬁcation scoreclassiﬁcation algorithm*.
learned embedding.
10001-10010method for detectiongait parameterstochastic method.
numerical experimentsecond-order methodlandmark datasetgeneral object detectioncold-start recommendationsimilarity of image.
100001-100010tumor regionmutual trustinherent problemhealthcare system*two-phase*posetrackhalf*mfcsborda count*diverse way.
node classiﬁcation problem recommendation diversity.
table 7: ranking results for machine learning with hicfl..given positives (10): deep learning, neural network, deep neural network, deep reinforcement learning, multilayer perceptron, convolutional neural network, recurrent neuralnetwork, long short-term memory, backpropagation, activation function..1-10convolutional neural network*recurrent neural network*artiﬁcial neural network*feedforward neural network*deep learning*neural network*generative adversarial network*multilayer perceptron*long short-term memory*neural architecture search*.
101-110discriminative lossdropout regularizationsemantic segmentation*mask-rcnnprobabilistic neural network*pretrained networkdiscriminator modelsequence-to-sequence learningautoencodersconditional variational autoencoder.
1001-1010multi-task deep learningself-supervisionstate-of-the-art deep learning algorithmgenerative probabilistic modeltranslation modelprobabilistic segmentationhandwritten digit classiﬁcationdeep learning classiﬁcationmulti-task reinforcement learningskip-gram*.
10001-10010low light imageface datasetestimation networkmethod on benchmark datasetsdistributed constraintgradient informationmodel on a varietymodel constraintautomatic detectionfeature redundancy.
100001-100010law enforcement agency*case of channelrelease*ahonen*electoral controlrunge*many studymean value*efﬁcient beampvt*.
table 8: ranking results for deep learning with hicfl (pu learning)..although we have reduced the difﬁculty, the taskis still very challenging for human professionals..4.4 case studies.
we interpret our results by ranking terms accord-ing to their domain relevance regarding machinelearning or deep learning, with hierarchy cs →ai → ml → dl.
for cs-ml, we label terms withautomatic annotation.
for dl, we create 10 dlterms manually as the positives for pu learning..table 7 and table 8 show the ranking results(1-10 represents terms ranked 1st to 10th).
weobserve the performance is satisfactory.
for ml,important concepts such as supervised learning, un-supervised learning, and deep learning are rankedvery high.
also, terms ranked before 1010th areall good domain-relevant terms.
for dl, althoughonly 10 positives are provided, the ranking resultsare quite impressive.
e.g., unlabeled positive termslike artiﬁcial neural network, generative adversarialnetwork, and neural architecture search are rankedvery high.
besides, terms ranked 101st to 110th areall highly relevant to dl, and terms ranked 1001stto 1010th are related to ml..5 conclusion.
we introduce and study the ﬁne-grained domainrelevance of terms– an important property of termsthat has not been carefully studied before.
we.
propose a hierarchical core-fringe domain rele-vance learning approach, which can cover almostall terms in human languages and various domains,while requires little or even no human annotation.
we believe this work will inspire an automatedsolution for knowledge management and help awide range of downstream applications in naturallanguage processing.
it is also interesting to inte-grate our methods to more challenging tasks, forexample, to characterize more complex propertiesof terms even understand terms..acknowledgments.
we thank the anonymous reviewers for their valu-able comments and suggestions.
this material isbased upon work supported by the national sci-ence foundation iis 16-19302 and iis 16-33755,zhejiang university zju research 083650, ibm-illinois center for cognitive computing systemsresearch (c3sr) - a research collaboration as partof the ibm cognitive horizon network, grantsfrom ebay and microsoft azure, uiuc ovcrccil planning grant 434s34, uiuc csbs smallgrant 434c8u, and uiuc new frontiers initiative.
any opinions, ﬁndings, and conclusions or recom-mendations expressed in this publication are thoseof the author(s) and do not necessarily reﬂect theviews of the funding agencies..3649references.
fatima n al-aswadi, huah yong chan,.
andkeng hoon gan.
2019. automatic ontology con-struction from text: a review from shallow to deeplearning trend.
artiﬁcial intelligence review, pages1–28..ehsan amjadian, diana inkpen, t sima paribakht, andfarahnaz faez.
2018. distributed speciﬁcity for au-tomatic terminology extraction.
terminology.
inter-national journal of theoretical and applied issuesin specialized communication, 24(1):23–40..ehsan amjadian, diana inkpen, tahereh paribakht,and farahnaz faez.
2016. local-global vectors toin pro-improve unigram terminology extraction.
ceedings of the 5th international workshop on com-putational terminology, pages 2–11..jessa bekker and jesse davis.
2020. learning frompositive and unlabeled data: a survey.
mach.
learn.,109(4):719–760..merley conrado, thiago pardo, and solange oliveirarezende.
2013. a machine learning approach to au-tomatic term extraction using a rich feature set.
inproceedings of the 2013 naacl hlt student re-search workshop, pages 16–23..patrick drouin.
2003..term extraction using non-technical corpora as a point of leverage.
terminol-ogy, 9(1):99–115..charles elkan and keith noto.
2008. learning classi-ﬁers from only positive and unlabeled data.
in pro-ceedings of the 14th acm sigkdd internationalconference on knowledge discovery and data min-ing, pages 213–220..denis fedorenko, n astrakhantsev, and d turdakov.
2014. automatic recognition of domain-speciﬁcterms: an experimental evaluation.
proceedings ofthe institute for system programming, 26(4):55–72..katerina frantzi, sophia ananiadou, and hideki mima.
2000. automatic recognition of multi-word terms:.
the c-value/nc-value method.
international journalon digital libraries, 3(2):115–130..david k hammond, pierre vandergheynst, and r´emigribonval.
2011. wavelets on graphs via spec-tral graph theory.
applied and computational har-monic analysis, 30(2):129–150..abram handler, matthew denny, hanna wallach, andbrendan o’connor.
2016. bag of what?
simplenoun phrase extraction for text analysis.
in proceed-ings of the first workshop on nlp and computa-tional social science, pages 114–124..anna h¨atty, michael dorna, and sabine schulteim walde.
2017. evaluating the reliability and in-teraction of recursively used feature classes for ter-minology extraction.
in proceedings of the studentresearch workshop at the 15th conference of the eu-ropean chapter of the association for computationallinguistics, pages 113–121..anna h¨atty, dominik schlechtweg, michael dorna,and sabine schulte im walde.
2020. predicting de-grees of technicality in automatic terminology ex-in proceedings of the 58th annual meet-traction.
ing of the association for computational linguistics,pages 2883–2889..geoffrey e hinton, nitish srivastava, alex krizhevsky,ilya sutskever, and ruslan r salakhutdinov.
2012.improving neural networks by preventing co-arxiv preprintadaptation of feature detectors.
arxiv:1207.0580..jie huang, zilong wang, kevin chen-chuan chang,wen-mei hwu, and jinjun xiong.
2020. exploringin proceedings of thesemantic capacity of terms.
2020 conference on empirical methods in naturallanguage processing..sergey ioffe and christian szegedy.
2015. batch nor-malization: accelerating deep network training byin internationalreducing internal covariate shift.
conference on machine learning, pages 448–456..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
proceedings ofthe 3rd international conference on learning rep-resentations..clinton gormley and zachary tong.
2015. elastic-search: the deﬁnitive guide: a distributed real-timesearch and analytics engine.
” o’reilly media,inc.”..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalin proceedings of international confer-networks.
ence on learning representations..althea ying ho ha and ken hyland.
2017. what istechnicality?
a technicality analysis model for eapvocabulary.
journal of english for academic pur-poses, 28:35–49..william l hamilton, rex ying, and jure leskovec.
inductive representation learning on large2017.in proceedings of the 31st internationalgraphs.
conference on neural information processing sys-tems, pages 1025–1035..bing liu, yang dai, xiaoli li, wee sun lee, andphilip s yu.
2003. building text classiﬁers usingpositive and unlabeled examples.
in third ieee in-ternational conference on data mining, pages 179–186. ieee..kevin meijer, flavius frasincar, and frederik hogen-boom.
2014. a semantic approach for extracting do-main taxonomies from text.
decision support sys-tems, 62:78–93..3650tomas mikolov, kai chen, greg corrado, and jef-frey dean.
2013a.
efﬁcient estimation of wordarxiv preprintrepresentations in vector space.
arxiv:1301.3781..mo yu and mark dredze.
2015. learning compositionmodels for phrase embeddings.
transactions of theassociation for computational linguistics, 3:227–242..jie zhou, chunping ma, dingkun long, guangwei xu,ning ding, haoyu zhang, pengjun xie, and gong-shen liu.
2020. hierarchy-aware global model forhierarchical text classiﬁcation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1106–1117..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013b.
distributed representa-tions of words and phrases and their compositional-in advances in neural information processingity.
systems, pages 3111–3119..hiroshi nakagawa and tatsunori mori.
2002. a simplebut powerful automatic term extraction method.
incoling-02: computerm 2002: second interna-tional workshop on computational terminology..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..chao shang, sarthak dash, md faisal mahbubchowdhury, nandana mihindukulasooriya, and al-ﬁo gliozzo.
2020. taxonomy construction of un-seen domains via graph-based cross-domain knowl-in proceedings of the 58th annualedge transfer.
meeting of the association for computational lin-guistics, pages 2198–2208..jingbo shang, jialu liu, meng jiang, xiang ren,clare r voss, and jiawei han.
2018. automatedieeephrase mining from massive text corpora.
transactions on knowledge and data engineering,30(10):1825–1837..ayla rigouts terryn, v´eronique hoste, and els lefever.
2019.in no uncertain terms: a dataset for mono-lingual and multilingual automatic term extractionfrom comparable corpora.
language resources andevaluation, pages 1–34..paola velardi, michele missikoff, and roberto basili.
2001. identiﬁcation of relevant terms to support theconstruction of domain ontologies.
in proceedingsof the acl 2001 workshop on human languagetechnology and knowledge management..celine vens, jan struyf, leander schietgat, saˇsodˇzeroski, and hendrik blockeel.
2008. decisiontrees for hierarchical multi-label classiﬁcation.
ma-chine learning, 73(2):185..jorge vivaldi, luis adri´an cabrera-diego, gerardosierra, and mar´ıa pozzi.
2012. using wikipedia tovalidate the terminology found in a corpus of basictextbooks.
in lrec, pages 3820–3827..jonatas wehrmann, ricardo cerri, and rodrigo bar-ros.
2018. hierarchical multi-label classiﬁcationnetworks.
in international conference on machinelearning, pages 5075–5084..wenjuan wu, tao liu, he hu, and xiaoyong du.
2012.extracting domain-relevant term using wikipediabased on random walk model.
in 2012 seventh chi-nagrid annual conference, pages 68–75.
ieee..3651