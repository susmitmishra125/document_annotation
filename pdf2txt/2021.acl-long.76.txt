a targeted assessment of incremental processing in neural languagemodels and humans.
ethan gotlieb wilcoxharvard universitydepartment of linguisticswilcoxeg@g.harvard.edu.
pranali vanimitbrain and cognitive sciencepvani@mit.edu.
roger p. levymitbrain and cognitive sciencerplevy@mit.edu.
abstract.
we present a targeted, scaled-up comparisonof incremental processing in humans and neu-ral language models by collecting by-word re-action time data for sixteen different syntac-tic test suites across a range of structural phe-nomena.
human reaction time data comesfrom a novel online experimental paradigmcalled the interpolated maze task.
we com-pare human reaction times to by-word proba-bilities for four contemporary language mod-els, with different architectures and trained ona range of data set sizes.
we ﬁnd that acrossmany phenomena, both humans and languagemodels show increased processing difﬁcultyin ungrammatical sentence regions with hu-man and model ‘accuracy’ scores (`a la mar-vin and linzen (2018)) about equal.
how-ever, although language model outputs matchhumans in direction, we show that modelssystematically under-predict the difference inmagnitude of incremental processing difﬁcultybetween grammatical and ungrammatical sen-tences.
speciﬁcally, when models encountersyntactic violations they fail to accurately pre-dict the longer reaction times observed in thehuman data.
these results call into questionwhether contemporary language models areapproaching human-like performance for sen-sitivity to syntactic violations..1.introduction.
a substantial body of work has investigated con-temporary language models (lms) by assessingwhether their behavior is consistent with the rulesof syntax (hu et al., 2020; marvin and linzen,2018; warstadt et al., 2020).1 among otherstructures, these studies have investigated agree-ment (linzen et al., 2016; gulordava et al., 2018).
1data.
and codeat.
foundhttps://github.com/wilcoxeg/.
this paper.
can be.
for.
onlinetargeted-assessment-imaze.
long distance dependencies (wilcox et al., 2018),pronominal and particle licensing (jumelet andhupkes, 2018; futrell et al., 2019), and expecta-tions for phrase-level constituents (futrell et al.,2018).
many of the studies which report aggre-gate behavior across a broad number of phenom-ena focus on accuracy scores, or the proportionof time lms or human subjects in an online ex-periment prefer a grammatical variant in match-ing grammatical / ungrammatical sentence pairs.
while these investigations provide much insight,they collapse a crucial dimension of comparison,namely the difference in magnitude between thegrammatical and ungrammatical conditions.
aslong as the direction of their predictions are thesame, an lm which ﬁnds grammatical conditionsonly marginally worse than their correspondingungrammatical counterpart will receive the samescore as a model that displays large differencesbetween the two conditions..at the same time, a related line of work hasinvestigated the quantitative relationship betweenincremental predictions of language models andhuman reaction times (hale, 2001; levy, 2008).
smith and levy (2013) found that this relationshipis log-linear across multiple orders of magnitudefor 3-gram models, and recent investigations haveshown that this holds for contemporary neural net-work models as well (wilcox et al., 2020; good-kind and bicknell, 2018).
so far, this work haslargely focused on the aggregate relationship, in-stead of isolating individual phenomena in targetedtesting environments..we combine these two approaches with a tar-geted assessment of incremental processing in neu-ral language models and humans.
we collect in-cremental processing data on a series of sixteentest suites, adapted from hu et al.
(2020), eachof which targets a different syntactic phenomenon.
for lm incremental processing data, we collect.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages939–952august1–6,2021.©2021associationforcomputationallinguistics939test suite name.
wh-cleft structures.
filler-gap dependency, subject gapfiller-gap dependency, object gapfiller-gap dependency, pp gap.
tag.
cleft.
fgd-subjfgd-objfgd-pp.
example.
what she did/spied was see the giraffe/the giraffe.
i know who/thati know who/that my mother senti know who/that my mother sent the present tolast weekend../my mother sent the present to taylor.
/the present to taylor../taylor.
main verb/reduced rc gardenpath.
mvrr.
the ship ∅/that was sunk/steered in the storm carried treasure..npi licensing, any, subj rc modiﬁernpi licensing, any, obj rc modiﬁernpi licensing, ever, subj rc modiﬁernpi licensing, ever, obj rc modiﬁer.
no/the senator that no/the journalist likes has gotten any votes.
npl-any-srcno/the senator that likes no/the journalists has gotten any votes.
npl-any-orcno/the senator that no/the journalist likes has ever won.
npl-ever-srcnpl-ever-orc no/the senator that likes no/the journalists has ever won..subject-verb number agr., subj rc modiﬁersubject-verb number agr., obj rc modiﬁersubject-verb number agr., pp modiﬁer.
svna-srcsvna-orcsvna-pp.
the lawyer/lawyers that helped the mayor is/are organized.
the lawyer/lawyers that the mayor hired is/are very organized.
the lawyer/lawyers next to the mayor is/are very organized..reﬂexive anaphora, masc., subj rc modiﬁer rna-m-src.
reﬂexive anaphora, masc., obj rc modiﬁer.
rna-m-orc.
reﬂexive anaphora, fem., subj rc modiﬁer.
rna-f-src.
reﬂexive anaphora, fem., obj rc modiﬁer.
rna-f-orc.
that.
dukes/duke.
the dukes/duke that hunted the rabbits saw himself/themselvesin the mirror.
thethehimself/themselves in the mirror.
theherself/themselves in the mirror.
theherself/themselves in the mirror..queens/queen.
queens/queen.
distrust.
knights.
distrust.
knights.
hunted.
rabbits.
that.
that.
saw.
saw.
saw.
the.
the.
table 1: the sixteen test suites evaluated in this paper.
sentence regions which are manipulated to form the fourconditions in each test suite are indicated with bold.
critical regions are underlined..by-word probabilities for four contemporary neu-ral network architectures.
for human incrementalprocessing data, we use by-word reaction times(rts).
we collect these by deploying a novel on-line measurement paradigm called the interpolatedmaze, which is based on the maze task (forsteret al., 2009).
in the maze task, participants mustread a sentence incrementally by selecting the cor-rect word from two possible continuations, one ofwhich is ungrammatical.
the time it takes partici-pants to select the correct choice has been shownto effectively capture incremental processing costand can be deployed at scale (boyce et al., 2020)..we deploy three analysis techniques to investi-gate how well models capture the human incremen-tal processing data.
first, we compute accuracymetrics (for lms) and consistency scores (for hu-mans) for each of our test suites, which correspondto the proportion of the time behavior is consis-tent with the relevant grammatical rules.
we ﬁndthat, for this analysis, humans and machine perfor-mance is about equal.
next, we compare the ob-served reaction-time slowdown between grammati-cal/ungrammatical conditions within a test suite tothe slowdown predicted by each of our models.
forthis analysis we use the methodology developed byvan schijndel and linzen (2018), who use a ms/bit(milliseconds of reaction time per bit of surprisal).
conversion metric derived from a ﬁtted regressionmodel to convert between the outputs of lms andslowdowns in human reaction times.
we ﬁnd thatmodels systematically under-predict the observedhuman data.
in our third analysis, we train a linearregression models to predict reaction times fromprobabilities in non-critical sentence regions, andshow that these models are relatively poor at pre-dicting reaction times in critical sentence regions.
that is, in areas of the sentence where human reac-tion time is inﬂuenced by grammatical violations,lm probabilities routinely under-predict humanprocessing difﬁculty as measured by reaction time.
taken together, these results indicate that contem-porary neural network languages models are sys-tematically less sensitive to grammatical violationscompared to humans..2 methods.
we collect incremental processing data on a seriesof test suites, each of which targets an individualsyntactic phenomenon.
composition of the testsuites is described in section 2.1. methods usedto collect incremental processing data are outlinedin section 2.2, for human reaction times.
section2.3 describes the models tested.
linear regressionmodels used to predict reaction times from modeloutputs will be referred to as ‘linear fits’ to avoid.
940confusion with language models..2.1 syntactic test suites.
we use sixteen test suites for syntactic generaliza-tion, adapted from hu et al.
(2020).
test suitesconsist of 20-25 items.
each item appears in fourconditions, two grammatical and two ungrammat-ical.2 table 1 gives the name of each test suite,an example, as well as a tag, which we will useto refer to that suite in ﬁgures.
when test suiteshave modiﬁers they always included distractors ofthe opposite grammatical category.
for examplesingular reﬂexive anaphora sentences with subjectrelative clause modiﬁers would have a plural nounin the relative clause (e.g.
the bishop who likes thekings saw *themselves/himself in the mirror.).
following the logic from hu et al.
(2020), eachtest suite comes with two or more criteria, whichspeciﬁes an inequality that should hold in a partic-ular critical region if model behavior follows therules of the relevant grammatical construction.
ac-curacy scores for each test suite are generated bycomputing the proportion of the time the inequalityholds within the critical region, across items in atest suite.
in hu et al., test suites include criteriathat correspond to 2-way contrasts between gram-matical/ungrammatical conditions as well as 2x2interactions between four conditions.
we only lookat the 2-way contrasts, here..the incremental processing measure we de-rive from a language model to determine its ac-curacy according to a suite’s inequality predic-tions is surprisal.
surprisal is the inverse logprobability of a word given its context: s(xi) =− log2 p(xi|x1...xi−1), measured in bits.
in thispaper, we novelly extend the usage of these in-equalities to determine a human consistency scorefor each test suite, by checking the mean reactiontimes for the various conditions of each item inthe suite against the suite’s criteria.
for natural-istic corpus materials, the effect of surprisal onhuman reaction times has been shown to be linear(smith and levy, 2013; goodkind and bicknell,2018; wilcox et al., 2020), motivating this usage ofsyntactic generalization criteria on human readingpatterns.
we use the same criteria as described inappendix b of hu et al.
(2020)..to walk through a single test suite in detail, (1).
2for the mvrr test suites, the ‘ungrammatical’ condi-tions are plausibly licensed by the grammar, but are unlikely.
following convention in linguistics, ungrammatical sentenceswill be marked with a *..gives an example of all four conditions of the mainverb / reduced relative clause suite, with criticalregions underlined.
(1) a. the artist drawn a portrait was impressed with the.
work.
[un-reduced, unambiguous].
b. the artist that was drawn a portrait was impressed.
with the work.
[reduced, unambiguous].
c. the artist painted a portrait was impressed with the.
work.
[un-reduced, unambiguous].
d. the artist that was painted a portrait was impressed.
with the work.
[reduced, ambiguous].
the logic of the test suite relies on the fact thatstrings like painted are ambiguous between activepast-tense main verbs and passive participles thatintroduce a reduced relative clause.
on the otherhand, verbs like drawn unambiguously introducea reduced relative clause.
if subjects believe thatthe ambiguous form of the verb introduces a mainverb, they should ﬁnd the critical-region verb wasimpressed surprising.
that is, relative to the [re-duced, ambiguous] conditions, not reducingthe verb or not using an ambiguous verb shouldmake the critical region less surprising (1 and 2below).
furthermore, the effect of not reducing therelative clause should be smaller for unambiguousverbs than for ambiguous ones (3)..if we denote for convenience sx(wi) as the sur-prisal of word wi in the context of version x of atest suite item, then the following list outlines thesethree predictions as inequalities, which we used todetermine accuracy scores on our test suites..1. sd(was impressed) < sc(was impressed)2. sd(was impressed) < sb(was impressed)3.
(sd(was impressed) - sc(was impressed)) < (sb(was.
impressed) - sa(was impressed)).
to foreshadow our results, the mvrr panels offigures 3 and in appendix a show that all threeof these criteria are met for most items both byall models and by human average reaction times.
unlike our other test suites, these predictions donot correspond to contrasts between sentences thatvary based on their grammaticality, but rather onpredictive processing that prefers the main-verbanalysis for locally ambiguous strings..2.2 the interpolated maze task.
human reaction time data was collected via a novelimplementation of the maze task (forster et al.,2009) which we call the interpolated maze.
in amaze task participants read through a sentence; ateach index they are presented with two possiblecontinuations, one word is a plausible next-word.
941the.
x-x-x.
beside beaver.
slapped pretty.
its.
of.
the.
x-x-x.
bliﬀor.
beaver.
slapped sulped.
its.
eet.
the.
x-x-x.
bliﬀor.
beaver.
slapped pretty.
its.
of.
time.
ago.
tail.
time.
twul.
tail.
time.
twul.
tail.
grammatical maze (g-maze).
lexical maze (l-maze).
interpolated maze (i-maze).
figure 1: the maze task: participants read the sentence word-by-word.
at each index they must select the rightcontinuation.
for this study, we introduce the interpolated maze, which is a blend of g-maze and l-maze..in the sentence and the other word is a distrac-tor.
participants must select the correct continua-tion by pressing a key on their keyboard.
figure1 shows a cartoon of this process for three vari-ants of the maze task.
in the g(rammatical)-mazeversion, the distractor word is a word of english,only it does not constitute a grammatical continua-tion.
in the l(exical)-maze variant, the word is anon-english nonce word.
if participants select thewrong continuation, the trial ends and they beginreading the next sentence.
the time it takes partici-pants to select the correct word by pressing a keyhas been shown to be a robust measure of incremen-tal processing difﬁculty, with slowdowns occurringon target words instead of in subsequent spilloverregions as is the case with other online processingmeasures such as self-paced reading (boyce et al.,2020)..of these two variants, g-maze has been shownto produce higher sensitivity results than l-maze(boyce et al., 2020), however because each indexmust present one possible continuation, it cannotbe used be used for items that have ungrammaticalconditions.
at the critical choice point, both thedistractor and the continuation would be ungram-matical and participants would not know whichcontinuation to select.
to solve this problem wedeploy a novel variant of the maze task called inter-polated maze, or i-maze.
in i-maze, we interweaveg-maze and l-maze choices, with l-maze distrac-tors in critical regions where one of the conditionsis ungrammatical.
participants are instructed tochoose english words over nonce-words, thus mak-ing the ‘right’ choice in these regions unambiguous.
in order not to clump l-maze distractors only incritical regions, we randomly sample ∼25% of allother words and render them as l-maze choices..for a full comparison of i-maze, g-maze and l-maze see vani et al.
(2021).
g-maze distractorswere generated with the scripts provided in boyceet al.
(2020), which uses a neural-network basedlanguage model to automatically generate high sur-prisal distractor words.
nonce words were gener-ated with wuggy (keuleers and brysbaert, 2010).
experiments were hosted on ibex farm (drum-mond, 2013), with participants recruited on ama-zon m-turk.
reaction time data for each item wascollected from thirty separate participants..2.3 models tested.
jrnn is the ‘big lstm+cnn inputs’ from joze-fowicz et al.
(2016).
it was trained on the onebillion word benchmark (chelba et al., 2013) withtwo hidden layers of 8196 units each and cnncharacter embeddings as input.
grnn is the best-performing model described inthe supplementary materials of gulordava et al.
(2018).
it was trained on 90 million tokens ofenglish wikipedia with two hidden layers of 650hidden units.
gpt-2 is the model presented in radford et al.
(2019), and was trained on 40gb of internet text.
we use the version of gpt-2 available through thelanguage modeling zoo distribution3rnng (dyer et al., 2016) jointly models a sen-tence as well as its syntactic parse.
the modelexplicitly represents parse trees and composes par-tially built phrase structures.
models are supervisedwith penn-treebank style parses during training.
we use the average of the three rnng-bllip-lgmodels from hu et al.
(2020)..3https://cpllab.github.io/lm-zoo/index..html#welcome-to-lm-zoo.
942figure 2: comparison between human consistency scores and model accuracy scores.
averages are taken acrossall predictions within a test suite, error bars are 95% binomial conﬁdence intervals.
scores are similar betweenhumans and models.
2.4 addressing two possible confounds.
before we turn to our results, we will brieﬂy ad-dress two possible confounds with our methods:first, while it may be the case that the relation-ship between surprisal and reaction time is linearin most sentence areas, this linearity may breakdown in high surprisal regions regardless of theunderlying grammaticality of the sentence.
thus,any potential badness of our linear ﬁts in criticalregions is an epiphenomenon of the fact that theywere trained in regions where the linearity holdsand tested in regions where it does not.
whilethere is some evidence that the linear relationshipbetween surprisal may ﬂatten off in high surprisalregions for self-paced reading (see, e.g.
figure 1 inwilcox et al.
(2020)), data collected for maze taskfor both grnn and a large transformer modelshows that the linear relationship holds even invery high surprisal regions, even exceeding 20 bits(boyce and levy, 2020) (see, especially figure 3).
the second confound has to do with the inter-polated maze task.
it may be the case that switch-ing between tasks incurs a cognitive load, thus un-grammatical sentence regions might be read moreslowly, but only because they are always associatedwith a switch from grammatical to lexical distrac-tors.
this could be worrisome, however we ﬁndthat reaction times in non-critical regions for l-.
maze decisions are actually slightly faster thang-maze decisions (p < 0.001 by a t-test).
fur-thermore, all of our reported contrasts are betweenl-maze items, so this is controlled for in our anal-yses..3 results.
3.1 test suite accuracy.
in this section we discuss test suite accuracy scores,which are computed using the predictions asso-ciated with each test suite.
for models, successon a prediction means that the model found mate-rial in a speciﬁed critical region more probable inthe grammatical condition than the ungrammaticalcondition.
for humans, a corresponding metric,consistency scores, report the proportion of timesthe critical region material was read more quicklyin the grammatical condition than in the ungram-matical condition.
scores are calculated across thetotal number of items in a test suite.
because mul-tiple subjects provided reaction time data for eachitem, we ﬁrst average item-level data across allparticipants before calculating consistency scores.
the accuracy/consistency scores for each of ourtest suites can be seen in figure 2. in this ﬁgureeach facet represents the results from a single testsuite, which aggregates across two or more predic-.
943rna-m-srcsvna-orcsvna-ppsvna-srcnpl-ever-srcrna-f-orcrna-f-srcrna-m-orcmvrrnpl-any-orcnpl-any-srcnpl-ever-orccleftfgd-objfgd-ppfgd-sbjhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnn0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00modelscoremodelhumangpt2rnngjrnngrnnaccuracy/consistency scores human rts vs. model surprisalsfigure 3: comparison between human and (predicted) model reaction-time slowdows between grammatical andungrammatical conditions.
averages are taken across all predictions within a test suite, error bars are 95% conﬁ-dence intervals.
models systematically under-predict the observed slowdown..tions.
a full breakdown of test suite by predictioncan be seen in appendix b. chance, which is 50%accuracy, is marked with a dashed blue line..humans perform above chance on 13/16 testsuites.
human rts are at or below chance for3/4 of the reﬂexive anaphora agreement tests andthe subject-verb number agreement with an ob-ject relative clause modiﬁer.
for the reﬂexiveanaphora tests, the low scores are driven by poorperformance when the noun that must be matchedis singular, such as in the lawyer who the judgesfear hurt herself/*themselves.
notably, human re-action times for negative polarity items and fornumber agreement on verbs and reﬂexive pronounsare known to be susceptible to facilitatory inter-ference effects from intervening attractors of thesort that are used in our test suites (vasishth et al.,2008; j¨ager et al., 2020).
in general, human consis-tency scores in this study are below that reportedin marvin and linzen (2018), who use an ofﬂineforced-choice paradigm, in which participants mustjudge which of two sentences sounds more natural.
nevertheless, for the vast majority of test suites,humans show robust sensitivity to the grammati-cal effects being tested, and failure is due to spe-ciﬁc biases, such as the singular reﬂexive behaviordiscussed above, not general insensitivity to themanipulations..model correlationgrnnjrnngpt2rnng.
0.450.680.710.65.p-value0.07< 0.01< 0.01< 0.01.table 2: correlations between model accuracy scoresand human consistency scores across test suites..racy scores.
the relatively strong correlation scoresindicate that the strength of signal for a syntacticgeneralization in model surprisal differentials ispredictive of the signal-to-noise ratio for the gener-alization in human reaction times..3.2 slowdown between conditions.
in this section we turn to the size of the contrast be-tween grammatical and ungrammatical conditions.
for humans, this contrast indicates a slowdown,where critical regions of ungrammatical sentencesare read more difﬁcultly than their correspondinggrammatical variants.
for lms, this contrast indi-cates a surprisal difference, where ungrammaticalconditions are more surprising than their grammat-ical counterparts.
do differences in surprisal accu-rately predict the slowdowns observed in humanreaction time data?.
table 2 shows the cross-suite correlations be-tween human consistency scores and model accu-.
to derive a predicted reaction-time slowdownfrom the model surprisals, we followed the method-.
944gpt2rnngjrnngrnnhumancleftfgd-objfgd-ppfgd-sbjmvrrnpl-any-orcnpl-any-srcnpl-ever-orcnpl-ever-srcrna-f-orcrna-f-srcrna-m-orcrna-m-srcsvna-orcsvna-ppsvna-src02004000200400020040002004000200400test suiteslowdown in millisecondspredicted vs. observed slowdown between conditionsresidual error.
histogram of residual values.
rorr.e.i. laudser etuosba naem.l.210.
190.
180.
170.
●.
● ●.
200.
●.
●.
●.
ytisned.i.noger. lacitir.c.i.noger. lacitir.c−non.0.0020.0010.000.
0.0020.0010.000.
0.0020.0010.000.
0.0020.0010.000.
●.
●.
●.
●.
gpt2 grnn jrnn rnngmodel.
−400.
−200.
400.
600.
200.
0residual.
gpt2.grnn.jrnn.rnng.figure 4: residuals for reaction times in critical regions from a linear ﬁt trained to predict reaction times fromsurprisal values in non-critical sentence regions.
the left facet shows mean absolute residual error and the centershows a histogram of the raw values, with larger residuals in critical regions.
the right facet shows a breakdown bycondition for the filler–gap dependency tests (grnn model), with larger residual values in the ungrammaticalconditions.
for this plot, labels indicate condition name, with a reference provided in appendix a. error bars are95% conﬁdence intervals..modelgrnnjrnngpt2rnng.
surprisal estimate8.8ms/bit0.5ms/bit12.0ms/bit19.0ms/bit.
p-value< 0.001< 0.05< 0.001< 0.001.table 3: surprisal estimates from linear fits.
ology outlined in van schijndel and linzen (2018).
this approach draws on the fact that the relation-ship between surprisal and human reaction time islinear across multiple orders of magnitude (smithand levy, 2013; wilcox et al., 2020), including formaze data (boyce and levy, 2020).
for each lm,we trained a linear ﬁt that predicts reaction timefrom surprisal value at the word-level.
the modelis ﬁt on rts from all l-maze distractor trials, criti-cal and non-critical region alike, and includes wordfrequency and word length as additional predic-tors, with random slopes for each item and eachparticipant.
the linear model’s surprisal estimate,therefore, is the slowdown in processing time pre-dicted for each bit of surprisal.
we treat this num-ber as a scalar and multiply it by the differencein surprisal between conditions to derive the totalpredicted slowdown due to syntactic violation fromthe language models.
for all of our ﬁts, we founda signiﬁcant effect for all of our predictors.
theestimates for each model’s surprisal term are givenin table 3..the results from this analysis can be seen infigure 3, with the various test suites on the x-axisand observed or predicted slowdowns on the y-axis.
as with accuracy scores, we average across predic-.
tions within each test suite.
humans demonstratepositive slowdowns in 11/16 test suites, with re-ﬂexive anaphora again proving the exception tothe general trend.
as is evident from the heightof the bars, models systematically under-predictthe slowdown observed in the human data.
mod-els’ predictions are outside of the 95% conﬁdenceintervals for the humans slowdowns in 7/16 testsuites for gpt2, 8/16 for rnng, 9/16 for grnnand 12/16 for jrnn.
the mean predicted differ-ence between models and humans across all testsuites is 95ms (gpt2), 107ms (rnng), 117ms(grnn) and 126ms (jrnn).
these data indicatethat models are less sensitive to the contrast be-tween grammatical and ungrammatical conditionsthan are humans, at least in this controlled testingenvironment..3.3 residuals.
in this section, we discuss a follow-up analysis con-ducted to validate the conclusion that models areunder-predicting reaction times in critical regions.
to do this, we train linear ﬁts on data from the non-critical regions, and get their residuals on data fromthese regions as well the critical regions.
the linearﬁts are exactly the same as the ones described in theprevious section, except instead of being trained onboth critical and non-critical l-maze trials, they aretrained on non-critical l-maze trials alone.
if theconclusion from the last section is correct, then weshould see larger residuals for the critical-regiondata then for the non-critical region data..the results from this analysis can be seen in theright and center facets of figure 4. the left facet.
9457685111012915141613grammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsgrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditionsungrammatical  conditions0200400600800fgd-objfgd-ppfgd-sbjtestmean residualcritical region residuals (grnn)shows the mean absolute value of the residuals foreach of our lms, both for the critical and non-critical region.
the center facet shows a histogramof the same data.
from both plots it is clear that thecritical region residuals are greater than the resid-uals computed for words in other regions of thesentence.
from the histograms, we can see that thecritical region residuals are systematically higheron average than the non-critical region residuals.
this indicates that the models under-predict the rtvalues in the critical regions..the difference between residuals provides addi-tional evidence that models under-predict reactiontimes in critical regions compared to words in otherparts of the sentence.
however, it does not showthat models under predict reaction times speciﬁ-cally for ungrammatical sentences.
to investigatethis, we break down average residual by condition,within each of our sixteen test suites.
the full re-sults for this breakdown can be seen in appendix b,with the results for the filler–gap dependency testsfor the grnn model in the right facet of figure4.4 across all tests, we ﬁnd that ungrammaticalconditions show much higher residual error.
themean absolute value of the residual error is 163msin grammatical conditions, but in ungrammaticalconditions it is 244ms.
the values of the twoconditions are signiﬁcantly different (p < 0.001by a t-test).
generally, residuals are largest forcleft, filler–gap dependency and mvrr suites,and smaller for suites that involve npi licensing,anaphora agreement and subject-verb numberagreement.
human reaction-times are known to besusceptible to interference effects from distractorsfor these syntactic phenomena (j¨ager et al., 2020),which may explain why residuals are smaller forthese suites.
taken together this analysis demon-strates that model surprisal values speciﬁcally un-der predict human reaction times in ungrammaticalcritical regions, suggesting that they are less sensi-tive to syntactic violations than are humans..4 discussion.
our experiments have tackled the question ofwhether syntactic difﬁculty can be reduced to by-word probabilities by providing a comparison oflanguage model and human behavior that is bothincremental and targeted.
our methods build on.
4with the mvrr test suite, no conditions are technicallyungrammatical, however we treat the reduced ambiguous con-dition as ungrammatical for the purposes of this analysis..those presented in van schijndel and linzen (2018)and van schijndel and linzen (2020), but differfrom theirs in a number of key respects, whichwe review brieﬂy below to highlight to novel as-pects of our own investigation.
first, all of our testsuites target grammatical/ungrammatical contrasts(except for the mvrr gardenpath test), whereasvan schijndel and linzen test locally ambiguoussentence regions that (may) require re-analysis forproper processing.
second, we assess a broad rangeof grammatical violations across sixteen test suitesthat target seven distinct structures.
third, we de-ploy a novel measurement of processing time (inter-polated maze), instead of self-paced reading.
weﬁt our own linear models from the i-maze data, anduse a ms/bit scalar term derived from lexical distrac-tor items.
finally, we provide a novel analysis thatcompares the residuals of linear ﬁts between crit-ical and non-critical regions, and we break downthese residuals based on the grammaticality of thecondition..4.1 model comparison.
while none of our models is able to capture hu-manlike sensitivity in ungrammatical critical re-gions, we do see some variation between them,with rnng and gpt-2 in particular showing themost humanlike results.
to compare model per-formance for accuracy scores (i.e.
the results pre-sented in section 3.1), we ﬁt pairwise logistic re-gression models, with the model class as the solepredictor, and random slopes for nested item/testsuite combinations and predictions (this becausepredictions are shared across test suites of the sametype).
we ﬁnd that gpt-2 performs signiﬁcantlybeter than both jrnn and grnn (p < 0.01)and the contrast between rnng and grnn ap-proaches signiﬁcance (p = 0.07) none of the otherpairwise comparisons are signiﬁcant..to compare model performance at predictinghuman slowdown in critical regions, we look atthe difference in residual errors between the mod-els from section 3.3 in the critical regions.
weﬁt liner regression models with the residual aspredictor variable, nested item/test suite combina-tions, and condition as random slopes.
we ﬁnda signiﬁcant contrast between gpt-2 and jrnn(p < 0.05), with gpt-2 performing better, anda near-signiﬁcant contrast between rnng andjrnn (p = 0.053).
overall, these results sup-port the conclusion that gpt-2 and rnng have.
946figure 5: the effect of an additional ms/bit scalar term on model performance from tests in section 3.2. resultsindicate that both the rnng and grnn models could reach near human-like performance (within the humanconﬁdence intervals 90% of the time) when the scalar term is around 10..a mild advantage over the other models.
this isespecially interesting for the rnng model, giventhat it was trained on orders of magnitude less datathan gpt-2..4.2 single stage models.
for the last decade, a “single-stage” theory of incre-mental processing (levy, 2008), in which word sur-prisal in a left-to-right language model (with a largeor unlimited beam for models that explicitly repre-sent multiple incremental parses) is the sole deter-minant of the processing difﬁculty that arises due tothe relationship between a word and the context itappears in, has been a prominent candidate theoryfor both experimental (staub, 2011) and compu-tational (frank and bod, 2011) psycholinguisticinvestigations.
although such a “single-stage” cancapture the qualitative difﬁculty patterns inducedby garden-pathing and other grammar-based ex-pectation violations (hale, 2001; levy, 2013), wenow see that it quantitatively under-predicts thedifﬁculty induced when grammatical expectationviolations are involved, as measured by self-pacedreading (van schijndel and linzen, 2020) and re-sponse times in the maze task (here)..but just how bleak is the outlook for single-stagemodels?
to investigate this, we re-analyze the re-sults from section 3.2 with theoretical model per-formance that includes an additional scalar termthat corresponds with an increase in the slope forsurprisal relative to that obtained from the ﬁt toreaction times.
the results in figure 5. here, they-axis shows the proportion of tests for which themodels are within the conﬁdence intervals of hu-.
man results, and the x-axis shows this scalar term.
we ﬁnd that models achieve 90% accuracy levelswhen the scalar term is 4 for gpt2, 11 for rnngand 23 for grnn.
what this means is that if eitherthe ms/bit scalar term, or the surprisal in ungram-matical conditions were (slightly under) an order ofmagnitude greater, then the models’ performancewould match humans..while we agree with the assessment from vanschijndel and linzen (2020) that these results posea challenge for contemporary implemented mod-els, we do not necessarily believe that they cannotbe overcome within the framework of single-stagemodels, especially ones that are mediated by sym-bolic representations like the rnng.
multiple op-tions exist that could magnify surprisal values inlocally ambiguous or ungrammatical regions, suchas a reduced beam size (roark, 2001) or particleﬁlters (levy et al., 2009).
taken together, theserecent results highlight a key question for futureresearch—what additional modeling mechanismswill be needed to accurately predict not only quali-tative but also quantitative patterns of human difﬁ-culty in language processing..acknowledgements.
rpl gratefully acknowledges nsf grant bcs-1551866, a google faculty research award, andfunding from the mit–ibm ai lab..9470.40.60.81.00255075100ms/bit effect term scalarproportion of tests  within 95% cis of humansmodelrnnggrnngpt2jrnntheoretical model performance (task from section 3.2)ethical considerations.
data were collected under an institutional re-view board (irb) approved protocol for onlinehuman subject experimentation.
participants werecompensated $2.00 for their participation in i-maze experiments.
experiments took ∼15 min-utes, which meant participants were being compen-sated ∼$8.00/hour.
we chose this rate because it isslightly above federal minimum wage, which wetake to be a fair baseline for compensation.
allinformation associated with experimental partici-pants was anonymized prior to analysis..references.
veronica boyce, richard futrell, and roger p levy.
2020. maze made easy: better and easier measure-ment of incremental processing difﬁculty.
journalof memory and language, 111:104082..veronica boyce and roger levy.
2020. a-maze ofnatural stories: texts are comprehensible using themaze task.
proceedings of the architectures andmechanisms for languages processing conference..ciprian chelba, tomas mikolov, mike schuster, qi ge,thorsten brants, phillipp koehn, and tony robin-son.
2013. one billion word benchmark for measur-ing progress in statistical language modeling.
arxivpreprint arxiv:1312.3005..alex drummond.
2013..ibex farm.
online server:.
http://spellout.
net/ibexfarm..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies..kenneth i forster, christine guerrera, and lisa elliot.
2009. the maze task: measuring forced incrementalsentence processing time.
behavior research meth-ods, 41(1):163–171..stefan l. frank and rens bod.
2011..insensitivityof the human sentence-processing system to hierar-chical structure.
psychological science, 22(6):829–834..richard futrell, ethan wilcox, takashi morita, androger levy.
2018. rnns as psycholinguistic sub-jects: syntactic state and grammatical dependency.
arxiv preprint arxiv:1809.01329..chapter of the association for computational lin-guistics: human language technologies..adam goodkind and klinton bicknell.
2018. predic-tive power of word surprisal for reading times is ain pro-linear function of language model quality.
ceedings of the 8th workshop on cognitive modelingand computational linguistics (cmcl 2018), pages10–18..kristina gulordava, piotr bojanowski, edouard grave,tal linzen, and marco baroni.
2018. colorlessgreen recurrent networks dream hierarchically.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies..john hale.
2001. a probabilistic earley parser as a psy-cholinguistic model.
in proceedings of the secondmeeting of the north american chapter of the asso-ciation for computational linguistics on languagetechnologies, pages 1–8.
association for computa-tional linguistics..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger p levy.
2020. a systematic assessmentof syntactic generalization in neural language mod-els.
arxiv preprint arxiv:2005.03692..lena a j¨ager, daniela mertzen, julie a van dyke,interference patternsand shravan vasishth.
2020.in subject-verb agreement and reﬂexives revisited:a large-sample study.
journal of memory and lan-guage, 111:104063..rafal jozefowicz, oriol vinyals, mike schuster, noamshazeer, and yonghui wu.
2016. exploring the lim-its of language modeling.
arxiv, 1602.02410..jaap jumelet and dieuwke hupkes.
2018. do languagemodels understand anything?
on the ability of lstmsto understand negative polarity items.
arxiv preprintarxiv:1808.10627..emmanuel keuleers and marc brysbaert.
2010.wuggy: a multilingual pseudoword generator.
be-havior research methods, 42(3):627–633..roger levy.
2008. expectation-based syntactic com-.
prehension.
cognition, 106(3):1126–1177..roger levy.
2013. memory and surprisal.
in hu-man sentence comprehension.
in roger p. g. vangompel, editor, sentence processing, pages 78–114.
hove: psychology press..roger p levy, florencia reali, and thomas l grifﬁths.
2009. modeling the effects of memory on humanonline sentence processing with particle ﬁlters.
inadvances in neural information processing systems,pages 937–944..richard futrell, ethan wilcox, takashi morita, pengqian, miguel ballesteros, and roger levy.
2019.neural language models as psycholinguistic sub-jects: representations of syntactic state.
in proceed-ings of the 2019 conference of the north american.
tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learnsyntax-sensitive dependencies.
transactions of theassociation for computational linguistics, 4:521–535..948rebecca marvin and tal linzen.
2018. targeted syn-in proceed-tactic evaluation of language models.
ings of the 2018 conference on empirical methodsin natural language processing..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..brian roark.
2001. probabilistic top-down parsingand language modeling.
computational linguistics,27(2):249–276..marten van schijndel and tal linzen.
2020. single-stage prediction models do not explain the magni-tude of syntactic disambiguation difﬁculty..nathaniel j smith and roger levy.
2013. the effect ofword predictability on reading time is logarithmic.
cognition, 128(3):302–319..adrian staub.
2011. word recognition and syntactic at-tachment in reading: evidence for a staged architec-ture.
journal of experimental psychology: general,140(3):407..marten van schijndel and tal linzen.
2018. model-ing garden path effects without explicit hierarchicalsyntax.
in cogsci..pranali vani, ethan gotlieb wilcox, and roger levy.
2021. using the interpolated maze task to assessincremental processing in english relative clauses.
proceedings of the annual meeting of the cognitivescience society..shravan vasishth, sven br¨ussow, richard l lewis,and heiner drenhaus.
2008. processing polarity:how the ungrammatical intrudes on the grammati-cal.
cognitive science, 32(4):685–712..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel rbowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..ethan wilcox, roger levy, takashi morita, andrichard futrell.
2018. what do rnn languagemodels learn about ﬁller-gap dependencies?
inproceedings of the 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp..ethan gotlieb wilcox, jon gauthier, jennifer hu,peng qian, and roger levy.
2020. on the predic-tive power of neural language models for humanreal-time comprehension behavior.
arxiv preprintarxiv:2006.01912..a consistency/accuracy scores by.
prediction.
figure 6 gives accuracy scores for humans andlm models, broken down by individual predic-tions.
predictions are taken from (hu et al., 2020),outlined in their appendix b. prediction names cor-respond to the licensed element of the sentence,so sing match prediction for reﬂexive anaphoralicensing corresponds to the contrast where him-self or herself is grammatical (as opposed to them-selves).
accuracy/consistency scores are similarbetween humans and models for cleft structures,ﬁller–gap dependencies (except for subject tests,which we discuss below), mvrr gardenpath andsubject verb number agreement suites.
in therest of this appendix, we focus in on structures thatshow different accuracy/consistency score patternsfor humans and models..for ﬁller–gap dependency tests, the human datadiffers from the model data when there is a gapin the subject position (fgd-sbj test).
in thiscase, both achieve relatively high scores for thewh prediction (yellow bars), but lower scoresﬁlled-gap prediction (i know *who/that mymother...).
(it should be noted that this con-trast is not one strictly of grammaticality in thecritical region, as the sentence could be felicitouslycompleted by a gap in the object position.)
this be-havior is in perfect alignment with the large amountof data demonstrating that english speakers takelonger processing object gaps over subject gaps,and suggests that such expectations are weaker inour neural models..and anaphor.
turning to npi.
licensing,we see a consistent pattern of differencebetween humans and models.
for the npitests, models perform much worse than hu-the swap intervener predictions (nomans atsenator that the lawyer liked ...the senator that noever/any vs.ever/any), whereaslawyer liked ...human participants performed about as well onthese tests as on the others.
for reﬂexive anaphoralicensing, human performance is worse for thesingular predictions, regardless of the genderof the pronoun, indicating a plural bias acrossthe board.
for models, this is true only for thefeminine pronoun (herself ), and the difference inaccuracy is much greater than the human differencein consistency scores.
when the masculine versionof the pronoun is used, models show similar.
949figure 6: test suite accuracy / consistency scores bro-ken down by individual predictions..scores for both the singular and plural predictions.
this pattern is consistent with a plural bias inhumans, but a bias against speciﬁcally the feminine(singular) form of the pronoun in models..b linear fit residuals by condition.
table 4 gives a breakdown of all test suite condi-tions, with an example and a tag used for labelingfor the left panel of figure 4 in the main text andfor the ﬁgures in this appendix.
ungrammaticalconditions are marked with a star.
figure 7 showsthe residuals from our linear ﬁts for each condi-tion/test suite pair.
see the ﬁgure caption for moredetail..950rna-m-srcsvna-orcsvna-ppsvna-srcnpl-ever-srcrna-f-orcrna-f-srcrna-m-orcmvrrnpl-any-orcnpl-any-srcnpl-ever-orccleftfgd-objfgd-ppfgd-sbjhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnnhumangpt2rnngjrnngrnn0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00modelaccuracypredictionambig_predictionfilledgap_predictionneg_intervener_predictionnp_match_predictionplural_match_predictionpos_intervener_predictionreduced_predictionsing_match_predictionswap_intervener_predictionvp_match_predictionwh_predictionaccuracy scores human rts vs. model surprisalsfigure 7: residuals for predicted reaction times in critical regions, from a linear ﬁt trained to predict reactiontimes from surprisal values in non-critical regions.
labels indicate condition name, with a reference provided inappendix a. error bars are 95% conﬁdence intervals.
across the majority of test suites, ungrammatical conditionsshow larger residuals, indicating that they are predicted less well by lm surprisal values..95131423142314231422019181720191817201918172019181734333635343336353433363534333635504952515049525150495251504952517685768576857685222124232221242322212423222124233837403938374039383740393837403954535655545356555453565554535655111012911101291110129111012926252827262528272625282726252827424144434241444342414443424144435857605958576059585760595857605915141613151416131514161315141613302932313029323130293231302932314645484746454847464548474645484762616463626164636261646362616463rna-m-srcsvna-orcsvna-ppsvna-srcnpl-ever-srcrna-f-orcrna-f-srcrna-m-orcmvrrnpl-any-orcnpl-any-srcnpl-ever-orccleftfgd-objfgd-ppfgd-sbjgpt2grnnjrnnrnnggpt2grnnjrnnrnnggpt2grnnjrnnrnnggpt2grnnjrnnrnng0200400600800020040060080002004006008000200400600800mean residualcondition is grammaticalaafalsetrueresiduals for reading times in critical regioncondition label.
test suite name condition name.
example.
1234.
5678910111213141516.
17181920.
21222324252627282930313233343536.
37383940414243444546474849505152.
535455565758596061626364.reduced-ambigreduced-unambigunreduced-ambigunreduced-unambig.
the ship sunk the the storm carried treasure.
the ship steered in the storm carried treasure.
the ship that was sunk in the storm carried treasure.
the ship that was steered in the storm carried treasure..cleftcleftcleftcleft.
fgd-objfgd-objfgd-objfgd-objfgd-ppfgd-ppfgd-ppfgd-ppfgd-sbjfgd-sbjfgd-sbjfgd-sbj.
mvrrmvrrmvrrmvrr.
npl-any-orcnpl-any-orcnpl-any-orcnpl-any-orcnpl-any-srcnpl-any-srcnpl-any-srcnpl-any-srcnpl-ever-orcnpl-ever-orcnpl-ever-orcnpl-ever-orcnpl-ever-srcnpl-ever-srcnpl-ever-srcnpl-ever-src.
rna-f-orcrna-f-orcrna-f-orcrna-f-orcrna-f-srcrna-f-srcrna-f-srcrna-f-srcrna-m-orcrna-m-orcrna-m-orcrna-m-orcrna-m-srcrna-m-srcrna-m-srcrna-m-src.
svna-orcsvna-orcsvna-orcsvna-orcsvna-ppsvna-ppsvna-ppsvna-ppsvna-srcsvna-srcsvna-srcsvna-src.
np-matchnp-mismatchvp-matchvp-mismatch.
that-gapthat-nogapwhat-gapwhat-nogapthat-gapthat-nogapwhat-gapwhat-nogapthat-gapthat-nogapwhat-gapwhat-nogap.
neg-negneg-pospos-negpos-posneg-negneg-pospos-negpos-posneg-negneg-pospos-negpos-posneg-negneg-pospos-negpos-pos.
match-pluralmatch-singmismatch-pluralmismatch-singmatch-pluralmatch-singmismatch-pluralmismatch-singmatch-pluralmatch-singmismatch-pluralmismatch-singmatch-pluralmatch-singmismatch-pluralmismatch-sing.
match-pluralmatch-singmismatch-pluralmismatch-singmatch-pluralmatch-singmismatch-pluralmismatch-singmatch-pluralmatch-singmismatch-pluralmismatch-sing.
what she spied was the giraffe*what she spied was see the giraffewhat she did was see the giraffe*what she did was see the giraffe.
*i know that my mother sent — to taylor yesterday.
i know that my mother sent the present to taylor yesterday.
i know what my mother sent — to taylor yesterday.
*i know what my mother sent the present to taylor yesterday.
*i know that my mother sent the present to – yesterday.
i know that my mother sent the present to taylor yesterday.
i know who my mother sent the present to — yesterday.
*i know who my mother sent the present to taylor yesterday.
*i know that — sent the present to taylor yesterday.
i know that my mother sent the present to taylor yesterday.
i know who — sent the present to taylor yesterday.
*i know who my mother sent the present to taylor yesterday..no senator that no journalist likes has gotten any votes.
no senator that the journalist likes has gotten any votes.
*the senator that no journalist likes has gotten any votes.
*the senator that the journalist likes has gotten any votes.
no senator that likes no journalists has gotten any votes.
no senator that likes the journalists has gotten any votes.
*the senator that likes no journalists has gotten any votes.
*the senator that likes the journalist has gotten any votes.
no senator that no journalist likes has ever won.
no senator that the journalist likes has ever won.
*the senator that no journalist likes has ever won.
*the senator that the journalist likes has ever won.
no senator that likes no journalists has ever won.
no senator that likes the journalists has ever won.
*the senator that likes no journalists has ever won.
*the senator that likes the journalist has ever won..the queens who the dukes mistrust saw themselves in the mirror.
the queen who the duke mistrusts saw herself in the mirror.
*the queens who the dukes mistrust saw herself in the mirror.
*the queen who the dukes mistrust saw themselves in the mirror.
the queens who hunted the rabbit saw themselves in the mirror.
the queen who hunted the rabbits saw herself in the mirror.
*the queens who hunted the rabbit saw herself in the mirror.
*the queen who hunted the rabbits saw themselves in the mirror.
the dukes who the dukes mistrust saw themselves in the mirror.
the duke who the duke mistrusts saw himself in the mirror.
*the dukes who the dukes mistrust saw himself in the mirror.
*the duke who the dukes mistrust saw themselves in the mirror.
the dukes who hunted the rabbit saw themselves in the mirror.
the duke who hunted the rabbits saw himself in the mirror.
*the dukes who hunted the rabbit saw himself in the mirror.
*the duke who hunted the rabbits saw themselves in the mirror..the lawyers that helped the mayor are organized.
the lawyer that helped the mayors is organized.
*the lawyers that helped the mayor is organized.
*the lawyer that helped the mayors are organized.
the lawyers that the mayor helped are organized.
the lawyer that the mayors helped is organized.
*the lawyers that the mayor helped is organized.
*the lawyer that the mayors helped are organized.
the lawyers next to the mayor are organized.
the lawyer next to the mayors is organized.
*the lawyers next to the mayor is organized.
*the lawyer next to the mayors is organized..table 4: conditions for each of the test suites assessed in this paper, with a tag (used for labeling in figure 7) andan example.
ungrammatical sentences are marked with a star (∗).
952