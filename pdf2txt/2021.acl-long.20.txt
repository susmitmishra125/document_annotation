reﬁning sample embeddings with relation prototypes toenhance continual relation extraction.
li cui1, deqing yang1∗, jiaxin yu1, chengwei hu1,jiayang cheng1, jingjie yi1 and yanghua xiao2,3∗1school of data science, fudan university2shanghai key laboratory of data science, school of computer science, fudan university3fudan-aishu cognitive intelligence joint research center, shanghai, chinafd2014cl@gmail.com{yangdeqing, jiaxinyu20, cwhu20, chengjy17, jjyi20, shawyh}@fudan.edu.cn.
abstract.
continual learning has gained increasing at-tention in recent years, thanks to its biologi-cal interpretation and efﬁciency in many real-world applications.
as a typical task of con-tinual learning, continual relation extraction(cre) aims to extract relations between enti-ties from texts, where the samples of differ-ent relations are delivered into the model con-tinuously.
some previous works have provedthat storing typical samples of old relations inmemory can help the model keep a stable un-derstanding of old relations and avoid forget-ting them.
however, most methods heavilydepend on the memory size in that they sim-ply replay these memorized samples in subse-quent tasks.
to fully utilize memorized sam-ples, in this paper, we employ relation proto-type to extract useful information of each re-lation.
speciﬁcally, the prototype embeddingfor a speciﬁc relation is computed based onmemorized samples of this relation, which iscollected by k-means algorithm.
the proto-types of all observed relations at current learn-ing stage are used to re-initialize a memorynetwork to reﬁne subsequent sample embed-dings, which ensures the model’s stable under-standing on all observed relations when learn-ing a new task.
compared with previous cremodels, our model utilizes the memory infor-mation sufﬁciently and efﬁciently, resulting inenhanced cre performance.
our experimentsshow that the proposed model outperformsthe state-of-the-art cre models and has greatadvantage in avoiding catastrophic forgetting.
the code and datasets have been released onhttps://github.com/fd2014cl/rp-cre..1.introduction.
as one of the most important tasks in informationextraction (ie), relation extraction (re) has been.
∗corresponding author.
widely applied in many downstream tasks, suchas knowledge base construction and completion(riedel et al., 2013).
the goal of re is to recognizea relation predeﬁned in knowledge graphs (kgs)for an entity pair in texts.
for example, given theentity pair [christopher nolan, interstellar] in thesentence “interstellar is an epic science ﬁction ﬁlmdirected by christopher nolan”, the relation the-director-of should be recognized by an re model.
conventional re models (zeng et al., 2014;zhou et al., 2016; zhang et al., 2018a) always as-sume a ﬁxed pre-deﬁned set of relations and per-form once-and-for-all training on a ﬁxed dataset.
therefore, these models can not well handle thelearning of new relations, which often emerge inmany realistic applications given the continuousand iterative nature of our world (hadsell et al.,2020).
to adapt to such a situation, the paradigmof continual relation extraction (cre) is proposed(wang et al., 2019; han et al., 2020; wu et al.,2021).
compared with conventional re, cre fo-cuses more on helping a model keep a stable under-standing of old relations while learning emergingrelations, which in fact could be precisely modeledby continual learning..continual learning (or lifelong learning) systemsare deﬁned as adaptive algorithms capable of learn-ing from a continuous stream of information (parisiet al., 2019), where the information is progressivelyavailable over time and the number of learningtasks is not pre-deﬁned.
continual learning remainsa long-standing challenge for machine learning anddeep learning (hassabis et al., 2017; thrun andmitchell, 1995), as its main obstacle is the tendencyof models to forget existing knowledge when learn-ing from new observations (french, 1999), whichis called as catastrophic forgetting.
recent workstry to address the problem of catastrophic forget-ting in three ways, including consolidation-basedmethods (kirkpatrick et al., 2017), dynamic archi-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages232–243august1–6,2021.©2021associationforcomputationallinguistics232tecture (chen et al., 2015; fernando et al., 2017)and memory-based methods (lopez-paz and ran-zato, 2017; aljundi et al., 2018; chaudhry et al.,2018), in which memory-based methods have beenproven promising in nlp tasks..in recent years, some memory-based cre mod-els have made signiﬁcant progress in overcom-ing catastrophic forgetting while learning new re-lations, such as ea-emr (wang et al., 2019),mllre (obamuyide and vlachos, 2019), cml(wu et al., 2021) and emar (han et al., 2020).
despite of their effectiveness, there are some chal-lenges remaining in current cre.
one noticeablechallenge is how to restore the sample embeddingspace disrupted by the learning of new tasks, giventhat re models’ performance is very sensitive tothe quality of sample embeddings.
another chal-lenge is that most existing cre models have notfully exploited memorized samples.
in order toenhance re performance and overcome the over-ﬁtting problem caused by high replay frequency,the samples memorized in these models usuallyhave the same magnitude as the original trainingsamples (wu et al., 2021), which is unrealistic inreal-world tasks..inspired by prototypical networks (snell et al.,2017) for few-shot classiﬁcation, we employ rela-tion prototypes to represent different relations inthis paper, which help the model understand differ-ent relations well.
furthermore, these prototypesare used to reﬁne sample embeddings in cre.
thisprocess is named as prototypical reﬁning in thispaper.
speciﬁcally, the prototype for a speciﬁc re-lation is the average embedding of typical sampleslabeled with this relation, which are collected by k-means and memorized by our model for future use.
the prototypical reﬁning can help our model re-cover from the disruption of embedding space andavoid catastrophic forgetting during learning newrelations, thus enhance our model’s cre perfor-mance.
another advantage of prototypical reﬁningis the efﬁcient utilization of memorized samples, re-sulting in our model’s less dependence on memorysize..our contributions in this paper are summarized.
as follows:.
(1) we propose a novel cre model whichachieves enhanced performance through reﬁningsample embeddings with relation prototypes and iseffective in avoiding catastrophic forgetting..(2) the paradigm we proposed for reﬁning sam-.
ple embeddings takes full advantage of the typ-ical samples stored in memory, and reduces themodel’s dependence on memory size (number ofmemorized samples)..(3) our extensive experiments upon two rebenchmark datasets justify our model’s remarkablesuperiority over the state-of-the-art cre modelsand less dependence on memory size..2 related works.
conventional studies in relation extraction (re)mainly focus on designing and utilizing variousdeep neural networks to discover the relations be-tween entities given contexts, including: (1) con-volutional neural networks (cnns) (zeng et al.,2014, 2015; nguyen and grishman, 2015; lin et al.,2016; ji et al., 2017) can effectively extract localtextual features.
(2) recurrent neural networks(rnns) (zhang and wang, 2015; xu et al., 2015;zhou et al., 2016; zhang et al., 2018a) are particu-larly capable of learning long-distance relation pat-terns.
(3) graph neural networks (gnns) (zhanget al., 2018b; fu et al., 2019; zhu et al., 2019)build word/entity graphs for cross-sentence reason-ing.
recently, pre-trained language models (de-vlin et al., 2019) have also been extensively usedin re tasks (wu and he, 2019; wei et al., 2020;baldini soares et al., 2019), and have achievedstate-of-the-arts performance..however, most of these models can only extracta ﬁxed set of pre-deﬁned relations.
hence, con-tinual relation learning, i.e., cre, has been pro-posed to overcome this problem.
existing contin-ual learning methods can be divided into three cat-egories: (1) regularization methods (kirkpatricket al., 2017; zenke et al., 2017; liu et al., 2018)alleviate catastrophic forgetting by imposing con-straints on updating the neural weights important toprevious tasks.
(2) dynamic architecture methods(chen et al., 2015; fernando et al., 2017) changearchitectural properties in response to new informa-tion by dynamically accommodating novel neuralresources.
(3) memory-based methods (lopez-pazand ranzato, 2017; aljundi et al., 2018; chaudhryet al., 2018) remember a few examples in previ-ous tasks and continually replay the memory withemerging new tasks.
for cre, the memory-basedmethods have been proven most promising (wanget al., 2019; han et al., 2020).
in addition, in or-der to accurately represent relations with limitedsamples, the idea of prototypical networks is intro-.
233duced into re(gao et al., 2019; ding et al., 2021).
there are also many memory networks proposedto remember information of long periods, such aslstm (hochreiter and schmidhuber, 1997) andmemory-augmented neural networks (graves et al.,2016; santoro et al., 2016).
besides, a new memorymodule (santoro et al., 2018) has demonstratedits success in relational reasoning, which employsmulti-head attention to allow memory interaction..algorithm 1: training procedure for tkinput: dk, rk, ˜rk−1, ˜mk−1output: ˜rk, ˜mk.
1 p k ← ∅;2 for each r ∈ ˜rk−1 do.
get mr from ˜mk−1;h r ← ∅;for each (xi, ti, r) ∈ mr do.
3 methodology.
in this section, we introduce our cre model indetails.
at ﬁrst, we formalize the problem of creand the memory module used in our model..3.1 task formalization.
in general, a single relation extraction (re) taskis to identify (classify) the relation between twoentities expressed in a sentence.
formally, theobjective of cre is to accomplish a sequenceof k re tasks {t1, t2, .
.
.
, tk}, where the k-th task tk has its own training set dk and re-lation set rk.
suppose dk contains n trainingsamples {(x1, t1, y1), .
.
.
, (xn , tn , yn )} where in-stance (xi, ti, yi), 1 ≤ i ≤ n indicates that therelation of entity pair ti in sentence xi is yi ∈ rk.
in fact, each task tk is an independent multi-classiﬁcation task to identify various relations inrk.
a cre model should perform well on extract-ing the relations in all k tasks after being trainedwith the samples of these tasks.
in other words, themodel should be capable of identifying the relationof a given entity pair into ˜rk, where ˜rk = ∪ki=1riis the relation set already observed till the k-th task.
inspired by current cre models (wu and he,2019; han et al., 2020), we adopt an episodic mem-ory module to store typical samples of relations thatthe model has learned in former tasks.
the memorymodule for relation r is represented as a memorizedsample set mr = {(x1, t1, r), .
.
.
, (xo, to, r)},where each sample is labeled with r and o isthe memory size (sample number).
therefore,the episodic memory for the observed relationsin t1 ∼ tk is ˜mk = ∪r∈ ˜rk.
mr..3.2 model learning pipeline.
the learning procedure of our model for a currenttask tk is shown in algorithm 1. the procedurecontains four major steps:.
prototype generation (line 2 ∼ 13): we ﬁrstobtain the prototype pr of each old relation r in.
3.
4.
5.
6.
7.
8.
9.
10.
11.
22.
23.
24.
25.
26.
27.
28.
29.
34.
//get xi’s embedding hi through e;hi ← e(xi, ti);h r ← h r ∪ hi;.
end//compute r’s prototype as the averageof h r’s embeddings;pr ← avg(h r);p k ← p k ∪ pr;.
1213 end14 ˜rk ← ˜rk−1 ∪ rk;15 ˜mk ← ˜mk−1;16 for i = 1 to epochs1 do.
update e and c according to l1 on dk;.
1718 end19 for each r ∈ rk doh r ← ∅;20for each (xi, ti, yi) ∈ dk do.
21.if yi = r then.
hi ← e(xi, ti);h r ← h r ∪ hi;.
end.
endgenerate mr by k-means on h r;˜mk ← ˜mk ∪ mr;pr ← avg(h r);p k ← p k ∪ pr;.
3031 end32 feed p k into m;33 for i = 1 to epochs2 do.
update e, m and c according to l2 on˜mk with the prototypical reﬁningconducted by m;.
35 end.
˜rk−1 by averaging the embeddings of memorizedsamples in mr with sample encoder e (section3.3).
these prototypes constitute a prototype setp k, which is used to memorize model’s embed-ding space before training on tk.
note that theencoder e is continuously changing with tasks, theprototypes of relations need to be regenerated at.
234figure 1: the structure of sample encoder e..the beginning of each task..initial training (line 16 ∼ 18): the parametersin sample encoder e and relation classiﬁer c aretuned with the training samples in dk (section 3.4).
sample selection (line 19 ∼ 31): for each re-lation r in rk, which is unobserved in the formertasks, we retrieve all samples labeled with r fromdk.
then we use k-means algorithm to clusterthese samples.
in each cluster, we take the sampleclosest to the centroid as the memorized typicalsample of r, to constitute mr (section 3.5).
then,we generate r’s prototype pr based on mr to ex-pand the prototype set p k..prototypical reﬁning (line 32 ∼ 35): to re-cover the disruption of sample embedding space,which is caused by training on tk, we use rela-tion prototype set p k to reﬁne sample embeddings.
speciﬁcally, p k is used to initialize our attention-based memory network m (section 3.6).
the sam-ples in ˜mk are encoded into embeddings by e, andthen reﬁned by m before being fed to c, to computethe loss function and update model parameters..in general, the parameter update of our modelfor tk includes two stages: (1) initial training ondk, where samples are encoded by encoder e. (2)prototypical reﬁning on ˜mk, where sample embed-dings are generated by encoder e and then reﬁnedby memory network m..next, we introduce this procedure in detail..3.3 sample encoder.
the structure of this sample encoder is displayed infigure 1, which is used to obtain the embedding ofeach sample.
in our model, the encoder e is builtupon bert (devlin et al., 2019; wolf et al., 2020),given its excellent performance on text encoding asa representative pre-trained language model.
in ad-dition, entity information has been proven effective.
in sample encoding for re tasks (wu and he, 2019;baldini soares et al., 2019).
thus, we highlight theexistence of entities in the sentence to augment e,through adding special tokens to mark the start andend position of entities.
speciﬁcally, we use [e11],[e12], [e21] and [e22] to denote the start and endposition of head and tail entity, respectively..next, a sample’s hidden representation is theconcatenation of token embeddings of [e11] and[e21], which has been proven effective in previousworks (baldini soares et al., 2019).
by feeding thisconcatenation into a fully connected layer alongwith layer normalization, a sample’s ﬁnal embed-ding h is generated as follows.
h = ln.
(cid:18)w (cid:0)concat[h11, h21](cid:1) + b.
(cid:19),.
(1).
where h11, h21 ∈ rh (h is the dimension of berthidden representation) are the hidden representa-tions of [e11] and [e21], w ∈ rd×2h (d is sampleembedding dimension) and b ∈ rd are trainableparameters, and ln (·) is the operation of layernormalization..3.4.initial training for new task.
according to the general assumption of cre, allrelations in rk are unobserved in former taskst1 ∼ tk−1.
we ﬁrst introduce the model’s ini-tial training on a simple multi-classiﬁcation task.
speciﬁcally, classiﬁer c in our model is a linearsoftmax classiﬁer.
for training set dk, the lossfunction is deﬁned as.
l1(θ) =.
−logp (yi|xi, ti),.
(2).
|dk|(cid:88).
i=1.
where p (yi|xi, ti) is calculated by classiﬁer cbased on sample (xi, ti, yi)’s embedding outputby sample encoder e..235bert[cls][e11][e12]w1w2…wp…[e21][e22]wq……wn…entity1entity2concat.fullyconnected layer+layernormalizationencoder outputencoder inputh11h21hfigure 2: attention-based memory network m. (a) the overall work ﬂow of memory network, where input is thesample embedding generated by e. (b) basic structure of attention heads in attention module..3.5 selecting typical samples to memorize.
relations.
for each relation r in rk, we select several typicalsamples into mr after the initial training with dk.
as the budget of memory is relatively smaller, it isimportant to select informative and diverse samplesto represent r. inspired by (han et al., 2020), we ap-ply k-means algorithm upon the embeddings of r’ssamples, which are generated by sample encoder e.suppose the number of clusters is o, which is alsothe number of typical samples that we will storeto represent r. then, in each cluster we choosethe sample closest to the centroid to represent thecluster and add it into the memory.
such operationensures that the samples stored in the memory arediverse enough and representative for the relation..3.6 reﬁning sample embeddings with.
relation prototypes.
we propose this module to reﬁne the sample em-beddings..after the initial training for the new task tk, oldrelations’ embedding space is likely to be disruptedbecause the model is tuned towards ﬁtting tk’slearning objective (section 3.4).
instead of just re-playing memorized samples for recovery, which isa common practice in continual learning, we reﬁnesample embeddings based on relation prototypes.
before applying our prototypical reﬁning, weﬁrst obtain the prototype embedding pr for eachold relation r in ˜rk−1 to constitute the prototypeset p k. this step (prototype generation) is con-ducted before the initial training for tk (initialtraining) to memorize the former state of ourmodel.
then, we construct an attention-based mem-.
ory network m based on p k for prototypical reﬁn-ing, as shown in figure 2. this network’s input isthe sample embedding generated by e, and its out-put is fed into c for relation classiﬁcation.
basedon prototypical reﬁning conducted by memory net-work m, our model’s embedding space is restored.
given a sample (x, t, y), its embedding h ∈ rdis generated by e and will be fed to memory net-work m. we also denote the head number of ourmemory network as n and the hidden dimensionof each head as d1.
the output of the i-th attentionhead is hi ∈ rd1, which is computed as.
hi = at n (qi, ki, v i)(cid:19)(cid:18) qikti√d1.
= sof tmax.
v i,.
(3).
(cid:19),.
(4).
(5).
(cid:48).
where qi ∈ rd1 is the linear transformation ofinput h, and ki, v i ∈ rl×d1 (l is the current sizeof ˜rk) is the linear transformation of p k. then,we concatenate each head’s output into the outputof multi-head attention layer as.
(cid:18).
˜h = ln.
w1.
(cid:0)concat[h1, h2, .
.
.
, hn ](cid:1) + h.where w 1 ∈ rd×n d1 is a trainable matrix..at last, the ﬁnal output of m is a residual output.
computed as.
(cid:18).
(cid:48).
˜h.
= ln.
w 2˜h + ˜h.
(cid:19),.
where w 2 ∈ rd×d is also a trainable matrix.
˜his the reﬁned embedding of (x, t, y), which incor-porating the information of prototypes p k throughequation 3 and is fed to the classiﬁer c..236mlp++attentioninputmemoryoutputresidualconnectionsquerykeyvaluememoryinputwqwkwvsoftmax(qkt)vrefinedinput(a)(b)we take ˜mk as the training set in this stage and.
the loss function is.
l2(θ) =.
−logp (yi|xi, ti),.
(6).
| ˜mk|(cid:88).
i=1.
˜mk, andwhere (xi, ti, yi) is a sample inp (yi|xi, ti) is calculated by c based on its em-bedding, which is ﬁrst generated by e and reﬁnedby m..based on the typicality and diversity of mem-orized samples (samples that can well representmost samples in this relation), training on ˜mk canrestore the disrupted embedding space of our modelwith a relatively small computational cost, whichallows our model to regain a stable understandingof old relations..3.7 prediction.
in order to maintain the consistency of training andprediction, our model uses the embeddings reﬁnedby m for prediction after training on a new task..4 experiments.
4.1 datasets.
our experiments were conducted upon the follow-ing two widely used datasets.
the training-test-validation split ratio is 3:1:1..fewrel(han et al., 2018) it is an re benchmarkdataset originally proposed for few-shot learning,which is annotated by crowd workers and contains100 relations and 70,000 samples in total.
in ourexperiments, we used the version of 80 relationsthat has been used (as the training and valid set) forcre..tacred (zhang et al., 2017) it is a large-scalere dataset with 42 relations (including no relation)and 106,264 samples built over newswire and webdocuments.
based on the open relation assumptionof cre, we removed no relation in our experi-ments.
at the same time, in order to limit the sam-ple imbalance of tacred, we limited the numberof training samples of each relation to 320 and thenumber of test samples of each relation to 40..4.2 compared models.
we introduce the following state-of-the-art crebaselines to be compared with our model in ourexperiments..ea-emr (wang et al., 2019) maintains a mem-ory to alleviate the problem of catastrophic forget-ting..emar (han et al., 2020) introduces memoryactivation and reconsolidation for continual relationlearning..cml (wu et al., 2021) proposes a curriculum-meta learning method to tackle the order-sensitivityand catastrophic forgetting in cre..as we adopt pre-trained language model for sam-ple encoding, we replace the encoder (bi-lstm)in emar with bert for a fair comparison.
thisemar’s variant is denoted as emar+bert.
be-sides, we denote our cre model with relation pro-totypes as rp-cre in result display.
since ourmodel only uses the information of memorizedsamples in attention-based memory network, wefurther proposed a variant of our model denoted asrp-cre+memory activation, by adding a mem-ory activation (han et al., 2020) step before atten-tion operation, to verify whether more memoryreplay is needed..4.3 experimental settings.
in previous cre experiments (wang et al., 2019;han et al., 2020), relations are ﬁrst divided into10 clusters to simulate 10 tasks.
however, thereare two drawbacks of this setting: (1) recogniz-ing all relations before training is unrealistic andcontrary to the setting of lifelong learning.
(2) therelations in one cluster generally have more seman-tic relevance.
therefore, we adopted a completelyrandom sampling strategy on relation-level in ourexperiments, which is more diverse and realistic.
in addition, the task order of all models is exactlythe same..in the context of continual learning, we pay moreattention to the variation trend of models’ perfor-mance while learning new tasks.
therefore, aftertraining for each new task, we will evaluate theclassiﬁcation accuracy of the models on the testset, which is composed of the test samples of allobserved relations..given that most recent cre models are evalu-ated by distinguishing true relation labels from asmall number of sampled negative labels (wanget al., 2019), which is too simple and rigid forrealistic applications.
therefore, we take a rigor-ous multi-classiﬁcation task on all observed rela-tions as the evaluation of our model.
it is alsothe reason that the baselines’ performance is much.
237table 1: accuracy (%) on all observed relations (which will continue to accumulate over time) at the stage of learn-ing current task, indicating that our model (rp-cre) signiﬁcantly surpasses other models and has an advantage incomparison with emar+bert..modelea-emremarcmlemar+bertrp-cre+memory activationrp-cre (ours).
modelea-emremarcmlemar+bertrp-cre+memory activationrp-cre (ours).
t189.088.591.298.898.097.9.t147.573.657.296.697.197.6.fewrelt359.166.668.289.591.891.6.t454.263.858.285.786.889.2.tacredt429.942.339.378.682.182.4.t338.348.341.381.087.486.1.t269.073.274.889.191.492.7.t240.157.051.485.791.490.6.t547.855.853.783.687.688.4.t528.437.735.973.978.379.8.t646.154.350.484.886.986.8.t627.334.028.972.377.877.2.t743.152.947.879.383.785.1.t726.932.627.371.774.975.1.t840.750.944.480.081.984.1.t825.830.026.972.273.573.7.t938.648.843.177.180.182.2.t922.927.624.872.673.672.4.t1035.246.339.773.879.581.5.t1019.825.123.471.072.372.4.worse than their reported results in the original pa-pers.
the method of choosing hyper-parameterfor our model is manual tuning.
for reproducingour experiment results conveniently, our model’ssource code, detailed hyper-parameter conﬁgu-rations and processed samples are provided onhttps://github.com/fd2014cl/rp-cre..4.4 overall performance comparison.
the performance of our model and baselines areshown in table 1, where the reported scores are theaverage of 5 rounds of training.
hyper-parameterconﬁgurations of baselines are the same as thatreported in original papers.
result of each task isthe accuracy on test data of all observed relations..based on the results, we ﬁnd that:(1) our strict test and sampling strategy actu-ally increase the difﬁculties of cre, causing greatdifﬁculties to the compared cre models.
this phe-nomenon is especially obvious in tacred thathas class-imbalance, even if we have made somerestrictions to the number of samples for each rela-tion..(2) pre-trained language models, such as bert,can gain outstanding performance in cre.
takeemar for example, replacing bi-lstm in it withbert brings more than 50% of improvement forthe last task in fewrel (46.3% to 73.8%), and morethan 150% of improvement in tacred (25.1% to71.0%).
we think this is mainly due to bert’s.
capability of making rapid migration to new tasks.
the remarkable advantage of the bert-based mod-els in table 1 in tacred further justiﬁes bert’sinsensitivity to sample imbalance..(3) compared with emar+bert, our modelalso has great advantage, proving that our modelcan take full advantage of memorized samples andmaintain relatively stable performance in continuallearning..(4) adding memory activation to our models didnot signiﬁcantly improve performance, indicatingthat it is sufﬁcient to adopt relation prototypes incre..(5) note that all models have similar perfor-mance on the former tasks, but our model obtainsmore stable performance towards the emergenceof new tasks.
it implies our model’s advantage inlong-term memory, which will be proven in section4.5..the average time consumption (on the machinewith a single rtx3090) of training rp-cre is1h28min, emar is 37min and emar+bert is3h21min.
our model’s time consumption is mainlydue to the massive parameters of bert.
givenour model’s apparent performance improvementwith respect to emar, such time consumption isrelatively acceptable..238table 2: accuracy (%) on the test sets from every previous task at the stage of learning the last task (with the samesize of memory), indicating that our model has better performance on previous tasks..modelrp-cre (ours)emar+bert.
t182.875.2.t268.459.6.t389.077.6.t478.865.8.t475.765.9.t688.180.5.t777.358.9.t882.960.0.t992.387.6.t1090.898.0.figure 3: visualized process of alleviating the disruption of sample embedding space after learning a new task.
(a)recovery result of emar+bert.
(b) recovery result of rp-cre..4.5 long-term effectiveness of episodic.
memory.
to explore long-term effectiveness of episodicmemory in our model, we compared our modelwith emar+bert on fewrel, which is similar toour model in selecting memorized samples.
resultsare shown in table 2, where each score is the classi-ﬁcation accuracy for all relations on test set of eachformer task.
we conclude that after training on 10sequential tasks, our model performs better on theformer tasks.
it indicates that our model has a muchstable understanding of old relations in old tasks.
inboth models, memorized samples of old relationsare used to restore the model’s performance on oldrelations (memory reconsolidation in emar, pro-totypical reﬁning in our model).
in order to ﬁnd thereason of emar’s inferior performance on the for-mer tasks, we display the visualization the varyingof sample embedding space during model training..concretely, we used t-sne (van der maaten andhinton, 2008) for dimension reduction and chosememorized samples from relation participant forvisualization, which were fed into the two modelson the same task.
figure 3 shows the sample posi-tions in the embedding space, where the blue dots.
represent the memorized samples and the red dotrepresents the relation’s prototype (the centroid ofmemorized samples before learning the new task).
left two sub-ﬁgures display how sample embed-ding space is disrupted by the learning of new tasks.
right two sub-ﬁgures display how the model re-covers..from figure 3, we notice that although emar’ssample embeddings are getting closer to the formercentroid (relation prototype) after memory recon-solidation, they converge in fact.
comparatively,our model restores the embedding space while re-taining the diversity between samples.
in terms oftypicality and diversity of memorized samples, it isnot our purpose to encode all memorized samplesinto exactly the same point in the embedding space,since it may damage the diversity of these sam-ples and reduce the information provided by thesamples, during model’s recovery from disruptedcondition..this result is mainly due to that the loss functionselected in emar’s memory reconsolidation is tooradical, focusing only on reducing the absolute dis-tance between a memorized sample and the relationprototype.
therefore, our strategy of reﬁning sam-ple embeddings with relation prototypes (section.
239figure 4: comparison of model’s dependence on memory size, indicating that our model has a weaker dependenceon memory size.
the x-axis is the serial id of current task, and y-axis represents the model’s classiﬁcationaccuracy on test set from all observed relations at current stage..3.6) better preserves the diversity of memorizedsamples, as it takes into account various featuresof samples rather than the true labels.
it eventuallyretains more information of memorized samples..4.6 model dependence on memory size.
in most memory-based cre models, memory size(number of memorized samples) is a key factoraffecting model performance.
however, most ofprevious models do not fully utilize the informa-tion provided by memorized samples, resulting intheir dependence on memory size.
even worse, thememorized samples have the same magnitude asthe original samples.
in section 3.6, we have em-phasized the advantages of our model in retainingand full utilization of memory information.
weveriﬁed whether our model relies less on memorysize through comparison experiments, of which theresults are shown in figure 4..we chose emar+bert as the main competitor,in which the conﬁguration and task sequence re-mained unchanged.
the only variable we adjustedis memory size.
based on the results we concludethat, as memory size decreases, our model obtainsless decreased performance than emar+bert(performance degradation is inevitable).
eventhough emar showed a relatively stable per-formance in the ﬁrst two tasks, its performancedropped signiﬁcantly in the subsequent tasks.
thisis consistent with the long-term effectiveness ofmemory we have analyzed in section 4.5. thediversity of samples in emar would graduallydisappear, making it highly dependent on mem-ory size.
comparatively, our model’s dependence.
on memory size is weak because it preserves thediversity of samples..5 conclusionin this paper, we propose a novel cre modelobtaining enhanced performance through reﬁningsample embeddings.
in our model, the sampleembeddings are reﬁned by an attention-based mem-ory network fed with relation prototypes, that aregenerated from memorized samples.
the compari-son experiments show that our model signiﬁcantlyoutperforms current state-of-the-art cre models.
as most current cre models are memory-based,we further explore the long-term effectiveness ofepisodic memory.
the results show that our modelhas great advantages in maintaining diversity ofmemorized samples and performs well in avoid-ing catastrophic forgetting of old relations (tasks).
because of the efﬁciency in memory mechanism,our model depends less on memory size.
in futurework, we will explore whether the mechanism ofreﬁning sample embeddings with prototypes can beused in other classiﬁcation-based continual learn-ing tasks..acknowledgments.
we sincerely thank all anonymous reviewers forthis work is sup-their valuable comments.
ported by national key research and devel-opment project (no.2020aaa0109302), shang-hai science and technology innovation ac-tion plan (no.19511120400) and shanghai mu-nicipal science and technology major project(no.2021shzdzx0103)..240references.
rahaf aljundi, francesca babiloni, mohamed elho-seiny, marcus rohrbach, and tinne tuytelaars.
2018. memory aware synapses: learning what (not)in proceedings of the european confer-to forget.
ence on computer vision, pages 139–154..livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2895–2905..arslan chaudhry, marc’aurelio ranzato, marcusrohrbach, and mohamed elhoseiny.
2018. efﬁ-cient lifelong learning with a-gem.
arxiv preprintarxiv:1812.00420..tianqi chen, ian goodfellow, and jonathon shlens.
2015. net2net: accelerating learning via knowl-edge transfer.
arxiv preprint arxiv:1511.05641..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics, pages 4171–4186..ning ding, xiaobin wang, yao fu, guangwei xu, ruiwang, pengjun xie, ying shen, fei huang, hai-taozheng, and rui zhang.
2021. prototypical represen-tation learning for relation extraction.
arxiv preprintarxiv:2103.11647..chrisantha fernando, dylan banarse, charles blundell,yori zwols, david ha, andrei a rusu, alexanderpritzel, and daan wierstra.
2017. pathnet: evolu-tion channels gradient descent in super neural net-works.
arxiv preprint arxiv:1701.08734..robert m french.
1999. catastrophic forgetting in con-nectionist networks.
trends in cognitive sciences,3(4):128–135..tsu-jui fu, peng-hsuan li, and wei-yun ma.
2019.graphrel: modeling text as relational graphs forjoint entity and relation extraction.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1409–1418..tianyu gao, xu han, zhiyuan liu, and maosong sun.
2019. hybrid attention-based prototypical networksin pro-for noisy few-shot relation classiﬁcation.
ceedings of the aaai conference on artiﬁcial intel-ligence, volume 33..alex graves, greg wayne, malcolm reynolds,tim harley, ivo danihelka, agnieszka grabska-barwi´nska, sergio g´omez colmenarejo, edwardgrefenstette, tiago ramalho, john agapiou, et al.
hybrid computing using a neural net-2016.nature,work with dynamic external memory.
538(7626):471–476..raia hadsell, dushyant rao, andrei a rusu, and raz-van pascanu.
2020. embracing change: continuallearning in deep neural networks.
trends in cogni-tive sciences..xu han, yi dai, tianyu gao, yankai lin, zhiyuan liu,peng li, maosong sun, and jie zhou.
2020. contin-ual relation learning via episodic memory activationand reconsolidation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 6429–6440..xu han, hao zhu, pengfei yu, ziyun wang, yuanyao, zhiyuan liu, and maosong sun.
2018. fewrel:a large-scale supervised few-shot relation classiﬁca-tion dataset with state-of-the-art evaluation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4803–4809..demis hassabis, dharshan kumaran, christophersummerﬁeld,and matthew botvinick.
2017.neuroscience-inspired artiﬁcial intelligence.
neu-ron, 95(2):245–258..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..guoliang ji, kang liu, shizhu he, and jun zhao.
2017. distant supervision for relation extractionwith sentence-level attention and entity descriptions.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 31..james kirkpatrick, razvan pascanu, neil rabinowitz,joel veness, guillaume desjardins, andrei a rusu,kieran milan, john quan, tiago ramalho, ag-nieszka grabska-barwinska, et al.
2017. over-coming catastrophic forgetting in neural networks.
proceedings of the national academy of sciences,114(13):3521–3526..yankai lin, shiqi shen, zhiyuan liu, huanbo luan,and maosong sun.
2016. neural relation extractionwith selective attention over instances.
in proceed-ings of the 54th annual meeting of the associationfor computational linguistics, pages 2124–2133..xialei liu, marc masana, luis herranz, joost van deweijer, antonio m lopez, and andrew d bagdanov.
2018. rotate your networks: better weight con-solidation and less catastrophic forgetting.
in 201824th international conference on pattern recogni-tion, pages 2262–2268..david lopez-paz and marc’aurelio ranzato.
2017.gradient episodic memory for continual learning.
arxiv preprint arxiv:1706.08840..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(11)..241thien huu nguyen and ralph grishman.
2015. rela-tion extraction: perspective from convolutional neu-ral networks.
in proceedings of the 1st workshop onvector space modeling for natural language pro-cessing, pages 39–48..abiola obamuyide and andreas vlachos.
2019. meta-learning improves lifelong relation extraction.
inproceedings of the 4th workshop on representationlearning for nlp, pages 224–229..german i parisi, ronald kemker, jose l part, christo-pher kanan, and stefan wermter.
2019. continuallifelong learning with neural networks: a review.
neural networks, 113:54–71..sebastian riedel, limin yao, andrew mccallum, andbenjamin m. marlin.
2013. relation extraction withmatrix factorization and universal schemas.
in pro-ceedings of the 2013 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages74–84..adam santoro, sergey bartunov, matthew botvinick,daan wierstra, and timothy lillicrap.
2016. meta-learning with memory-augmented neural networks.
in international conference on machine learning,pages 1842–1850..adam santoro, ryan faulkner, david raposo, jackrae, mike chrzanowski, theophane weber, daanwierstra, oriol vinyals, razvan pascanu, and timo-thy lillicrap.
2018. relational recurrent neural net-works.
arxiv preprint arxiv:1806.01822..jake snell, kevin swersky, and richard zemel.
2017.prototypical networks for few-shot learning.
inproceedings of the 31st international conferenceon neural information processing systems, pages4080–4090..sebastian thrun and tom m mitchell.
1995. lifelongrobot learning.
robotics and autonomous systems,15(1-2):25–46..hong wang, wenhan xiong, mo yu, xiaoxiao guo,shiyu chang, and william yang wang.
2019. sen-tence embedding alignment for lifelong relation ex-traction.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 796–806..zhepei wei, jianlin su, yue wang, yuan tian, andyi chang.
2020. a novel cascade binary taggingin pro-framework for relational triple extraction.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1476–1488..thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pierriccistac, morgan funtowicz, et al.
2020. transform-ers: state-of-the-art natural language processing.
inproceedings of the 2020 conference on empirical.
methods in natural language processing: systemdemonstrations, pages 38–45..shanchan wu and yifan he.
2019. enriching pre-trained language model with entity information forin proceedings of the 28threlation classiﬁcation.
acm international conference on information andknowledge management, pages 2361–2364..tongtong wu, xuekai li, yuan-fang li, reza haf-fari, guilin qi, yujin zhu, and guoqiang xu.
2021. curriculum-meta learning for order-robustarxiv preprintcontinualarxiv:2101.01926..relation extraction..yan xu, lili mou, ge li, yunchuan chen, hao peng,and zhi jin.
2015. classifying relations via longshort term memory networks along shortest depen-dency paths.
in proceedings of the 2015 conferenceon empirical methods in natural language process-ing, pages 1785–1794..daojian zeng, kang liu, yubo chen, and jun zhao.
2015. distant supervision for relation extraction viain pro-piecewise convolutional neural networks.
ceedings of the 2015 conference on empirical meth-ods in natural language processing, pages 1753–1762..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiﬁcation via con-volutional deep neural network.
in proceedings ofcoling 2014, the 25th international conferenceon computational linguistics: technical papers,pages 2335–2344..friedemann zenke, ben poole, and surya ganguli.
2017. continual learning through synaptic intel-in international conference on machineligence.
learning, pages 3987–3995..dongxu zhang and dong wang.
2015. relation classi-ﬁcation via recurrent neural network.
arxiv preprintarxiv:1508.01006..runyan zhang, fanrong meng, yong zhou, and bingliu.
2018a.
relation classiﬁcation via recurrent neu-ral network with attention and tensor layers.
bigdata mining and analytics, 1(3):234–244..yuhao zhang, peng qi, and christopher d. manning.
2018b.
graph convolution over pruned dependencytrees improves relation extraction.
in proceedings ofthe 2018 conference on empirical methods in natu-ral language processing, pages 2205–2215..yuhao zhang, victor zhong, danqi chen, gabor an-geli, and christopher d. manning.
2017. position-aware attention and supervised data improve slotin proceedings of the 2017 conference onﬁlling.
empirical methods in natural language processing,pages 35–45..peng zhou, wei shi, jun tian, zhenyu qi, bingchen li,hongwei hao, and bo xu.
2016. attention-basedbidirectional long short-term memory networks for.
242in proceedings of the 54threlation classiﬁcation.
annual meeting of the association for computa-tional linguistics, pages 207–212..hao zhu, yankai lin, zhiyuan liu, jie fu, tat-sengchua, and maosong sun.
2019. graph neural net-works with generated parameters for relation extrac-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 1331–1339..243