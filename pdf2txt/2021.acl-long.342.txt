semantic representation for dialogue modeling.
xuefeng baiâ™ â™¥ , yulong chenâ™ â™¥ , linfeng songâ™£ , yue zhangâ™¥â™¦â™  zhejiang university, chinaâ™¥ school of engineering, westlake university, chinaâ™£ tencent ai lab, bellevue, wa, usaâ™¦ institute of advanced technology, westlake institute for advanced study, china.
abstract.
although neural models have achieved com-petitive results in dialogue systems, they haveshown limited ability in representing core se-mantics, such as ignoring important entities.
to this end, we exploit abstract meaningrepresentation (amr) to help dialogue mod-eling.
compared with the textual input, amrexplicitly provides core semantic knowledgeand reduces data sparsity.
we develop analgorithm to construct dialogue-level amrgraphs from sentence-level amrs and exploretwo ways to incorporate amrs into dialoguesystems.
experimental results on both dia-logue understanding and response generationtasks show the superiority of our model.
toour knowledge, we are the ï¬rst to leveragea formal semantic representation into neuraldialogue modeling..1.introduction.
dialogue systems have received increasing re-search attention (wen et al., 2015; serban et al.,2017; bao et al., 2020), with much recent workfocusing on social chats (ritter et al., 2011; liet al., 2017) and task-oriented dialogues (wen et al.,2017; dinan et al., 2019).
there are two salientsubtasks in dialogue modeling, namely dialogueunderstanding (choi et al., 2018; reddy et al.,2019; yu et al., 2020) and response generation (liet al., 2017; budzianowski et al., 2018).
the formerrefers to understanding of semantic and discoursedetails in a dialogue history, and the latter concernsmaking a ï¬‚uent, novel and coherent utterance..the current state-of-the-art methods employneural networks and end-to-end training (sutskeveret al., 2014; bahdanau et al., 2015) for dialoguemodeling.
for instance, sequence-to-sequencemodels have been used to encode a dialogue history,before directly synthesizing the next utterance(vinyals and le, 2015; wen et al., 2017; bao et al.,.
figure 1: a conversation from dailydialog.
someimportant contents are marked with squares..2020).
despite giving strong empirical results,neural models can suffer from spurious featureassociations in their neural semantic representation(poliak et al., 2018; kaushik et al., 2020), whichcan lead to weak robustness, inducing irrelevantdialogue states (xu and sarikaya, 2014; sharmaet al., 2019; rastogi et al., 2019) and generatingunfaithful or irrelevant text (maynez et al., 2020;niu and bansal, 2020).
as shown in figure 1,the baseline transformer model pays attentionto the word â€œlambâ€ but ignores its surroundingcontext, which has important contents (markedwith squares) that indicate its true meaning, therebygiving an irrelevant response that is related tofood.
intuitively, such issues can be alleviatedby having a structural representation of semanticinformation, which treats entities as nodes andbuilds structural relations between nodes, makingit easy to ï¬nd the most salient context.
explicitstructures are also more interpretable compared to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4430â€“4445august1â€“6,2021.Â©2021associationforcomputationallinguistics4430dialoguehistory:â€¦speaker-1:recently, iâ€™ve been obsessed with horror films.
speaker-2:oh,how can you be infatuated with horror films?
theyâ€™re so scary .
speaker-1:yeah, you are right i used to not watch horror films, but after seeing silence of the lamb with mike last month, i fell in love with them.
speaker-2:itâ€™s amazing.
but if i were you, i wouldn't have the courage to watch the first one.
speaker-1:but it's really exciting .ground-truth:maybe, but i would rather watch romance, science fiction, crime or even disaster movie instead of a horror pictureâ€¦transformer:great.iâ€™mlookingforwardtoit.ijustcanâ€™tkeepawayfromthefoodthatisaw.
neural representation and have been shown usefulfor information extraction (strubell et al., 2018;sun et al., 2019; li et al., 2020; bai et al., 2021;sachan et al., 2021), summarization (liu et al.,2015; hardy and vlachos, 2018; liao et al., 2018)and machine translation (marcheggiani et al., 2018;song et al., 2019a)..we explore amr (banarescu et al., 2013) asa semantic representation for dialogue historiesin order to better represent conversations.
asshown in the central block of figure 2, amr isone type of sentential semantic representations,which models a sentence using a rooted directedacyclic graph, highlighting its main concepts (e.g.
â€œmistakeâ€) and semantic relations (e.g., â€œarg0â€1),it canwhile abstracting away function words.
thus potentially offer core concepts and explicitstructures needed for aggregating the main contentin dialogue.
in addition, amr can also be usefulfor reducing the negative inï¬‚uence of variances insurface forms with the same meaning, which addsto data sparsity..existing work on amr parsing focuses on thesentence level.
however, as the left block offigure 2 shows, the semantic structure of a dialoguehistory can consist of rich cross-utterance co-reference links (marked with squares) and multiplespeaker interactions.
to this end, we propose analgorithm to automatically derive dialogue-levelamrs from utterance-level amrs, by addingcross-utterance links that indicate speakers, identi-cal mentions and co-reference links.
one exampleis shown in the right block of figure 2, where newlyadded edges are in color.
we consider two mainapproaches of making use of such dialogue-levelamr structures.
for the ï¬rst method, we mergean amr with tokens in its corresponding sentencevia amr-to-text alignments, before encoding theresulting structure using a graph transformer (zhuet al., 2019).
for the second method, we separatelyencode an amr and its corresponding sentence,before leveraging both representations via featurefusion (mangai et al., 2010) or dual attention (cal-ixto et al., 2017)..we verify the effectiveness of the proposedframework on a dialogue relation extractiontask (yu et al., 2020) and a response generationtask (li et al., 2017).
experimental results showthat the proposed framework outperforms previous.
1please refer to propbank (kingsbury and palmer, 2002;.
palmer et al., 2005) for more details..methods (vaswani et al., 2017; bao et al., 2020;yu et al., 2020), achieving the new state-of-the-artresults on both benchmarks.
deep analysis and hu-man evaluation suggest that semantic informationintroduced by amr can help our model to betterunderstand long dialogues and improve the coher-ence of dialogue generation.
one more advantageis that amr is helpful to enhance the robustnessand has a potential to improve the interpretabilityof neural models.
to our knowledge, this isthe ï¬rst attempt to leverage the amr semanticrepresentation into neural networks for dialogue un-derstanding and generation.
our code is availableat https://github.com/muyeby/amr-dialogue..2 constructing dialogue amrs.
figure 2 illustrates our method for constructing adialogue-level amr graph from multiple utterance-level amrs.
given a dialogue consisting multipleutterances, we adopt a pretrained amr parser (caiand lam, 2020) to obtain an amr graph foreach utterance.
for utterances containing multiplesentences, we parse them into multiple amrgraphs, and mark them belonging to the sameutterance.
we construct each dialogue amr graphby making connections between utterance amrs.
in particular, we take three strategies accordingto speaker,identical concept and co-referenceinformation..speaker we add a dummy node and connect it toall root nodes of utterance amrs.
we add speakertags (e.g., speaker1 and speaker2) to the edgesto distinguish different speakers.
the dummy nodeensures that all utterance amrs are connected sothat information can be exchanged during graphencoding.
besides, it serves as the global root nodeto represent the whole dialogue..identical concept there can be identical men-tions in different utterances (e.g.
â€œpossibleâ€ in theï¬rst and the forth utterances in figure 2), resultingin repeated concept nodes in utterance amrs.
weconnect nodes corresponding to the same non-pronoun concepts by edges labeled with same2.
this type of connection can further enhance cross-sentence information exchange..inter-sentence co-reference a major challengefor dialogues understanding is posed by pronouns,.
2compared with co-reference, identical concept relationscan connect different words which share the same meaninge.g.
(cid:104)could, might(cid:105) , (cid:104)fear, afraid(cid:105)..4431figure 2: dialogue amr graph construction process.
step 1: parse raw-text utterance into utterance amr graphs;step 2: connect utterance amr graphs into a dialogue amr graph..which are frequent in conversations (grosz et al.,1995; newman et al., 2008; quan et al., 2019).
weconduct co-reference resolution on dialogue textusing an off-to-shelf model3 in order to identifyconcept nodes in utterance amrs that refer tothe same entity.
for example, in figure 2, â€œiâ€in the ï¬rst utterance, and â€œsirâ€ in the secondutterance refer to the same entity, speakr1.
weadd edges labeled with coref between them,starting from later nodes to earlier nodes (later andearlier here refer to the temporal order of ongoingconversation), to indicate their relation4..3 baseline system.
we adopt a standard transformer (vaswani et al.,2017) for dialogue history encoding.
typically, atransformer encoder consists of l layers, takinga sequence of tokens (i.e., dialogue history) s ={w1, w2, ..., wn }, where wi is the i-th token andn is the sequence length, as input and producesvectorized word representations {hln }iteratively, l âˆˆ [1, ..., l].
overall, a transformerencoder can be written as:.
2, ..., hl.
1, hl.
h = seqencoder(emb(s)),.
(1).
1 , hl.
where h = {hl2 , ..., hln }, and emb denotesa function that maps a sequence of tokens intothe corresponding embeddings.
each transformerlayer consists of two sub-layers: a self-attentionsub-layer and a position-wise feed forward network.
the former calculates a set of attention scores:.
Î±ij = attn(hi, hj)..(2).
3https://github.com/huggingface/neuralcoref4for simplicity, we omit the coreference links between the.
second and third utterance for display..which are used to update the hidden state of wi:.
(cid:88)n.hli =.
j=1.
Î±ij(w v hlâˆ’1.
),.
j.
(3).
where w v is a parameter matrix..the position-wise feed-forward (ffn) layer.
consists of two linear transformations:.
ffn(h) = w2relu(w1h + b1) + b2,.
(4).
where w1, w2, b1, b2 are model parameters..3.1 dialogue understanding task.
we take the dialogue relation extraction task (yuet al., 2020) as an example.
given a dialoguehistory s and an argument (or entity) pair (a1, a2),the goal is to predict the corresponding relationtype r âˆˆ r between a1 and a2..we follow a previous dialogue relation extrac-tion model (chen et al., 2020) to feed the hiddenstates of a1 and a2 (denoted as ha1, ha2) into aclassiï¬er to obtain the probability of each relationtypes:.
prel = softmax(w3[ha1; ha2] + b3),.
(5).
where w3 and b3 are model parameters.
the k-thvalue of prel is the conditional probability of k-threlation in r..given a training instance (cid:104)s, a1, a2, r(cid:105), the local.
loss is:.
(cid:96) = âˆ’logp (r|s, a1, a2; Î¸),.
(6).
where Î¸ denotes the set of model parameters.
inpractice, we use bert (devlin et al., 2019) forcalculating ha1 and ha2, which can be regardedas pre-trained initialization of the transformerencoder..4432(cid:2)(cid:4) (cid:1)raw utterance texts(b) utterance amr graphs(c) dialogue amr graphamrparsinggraph merges1s2couldihavemybill,please?s2certainly,sir.s1iâ€™mafraidtherehasbeenamistake.whatcoulditbe?
:arg0possiblehaveinterrogative:modeii:possbill:arg1:arg1sayicertainsir:arg1:arg2fearimistake:arg1:arg1:arg0:arg0possible:arg1it:modeunknownpossiblepossible:arg0haveinterrogative:modeii:possbill:arg1sayi:arg0certainsir:arg1:arg2fearimistake:arg0:arg1dummy:speaker1:speaker2:speaker1:speaker2:coref:coref:arg1:arg1it:sameunknown:mode:coref(a).
(b).
(c).
figure 3: amr for dialogue modeling.
independently..(a) using amr to enrich text representation..(b,c) using amr.
3.2 dialogue response generation task.
given a dialogue history s, we use a standard auto-regressive transformer decoder (vaswani et al.,2017) to generate a response y = {y1, y2, ..., y|y|}.
at time step t, the previous output word ytâˆ’1 isï¬rstly transformed into a hidden state st by a self-attention layer as equations 2 and 3. then anencoder-decoder attention mechanism is applied toobtain a context vector from encoder output hiddenstates{hl.
2 , .
.
.
, hl.
1 , hl.
n }:.
Ë†Î±ti = attn(st, hl(cid:88)n.i ),Ë†Î±tihli ,.
ct =.
i=1.
(7).
the obtained context vector ct is then used tocalculate the output probability distribution for thenext word yt over the target vocabulary5:.
an amr is a directed acyclic graph g = (cid:104)v, e(cid:105),where v denotes a set of nodes (i.e.
amr concepts)and e (i.e.
amr relations) denotes a set of labelededges.
an edge can be further represented by atriple (cid:104)ni, rij, nj(cid:105), meaning that the edge is fromnode ni to nj with label rij..we consider two main ways of making useof dialogue-level amrs.
the ï¬rst method (fig-ure 3(a)) uses amr semantic relations to enricha textual representation of the dialogue history.
we project amr nodes onto the correspondingtokens, extending transformer by encoding se-mantic relations between words.
for the secondapproach, we separately encode an amr and itssentence, and use either feature fusion (figure 3(b))or dual attention (figure 3(c)) to incorporate theirembeddings..pvoc = softmax(w4ct + b4),.
(8).
4.1 graph encoding.
where w4, b4 are trainable model parameters.
thek-th value of pvoc is the conditional probability ofk-th word in vocabulary given a dialogue..given a dialogue history-response pair {s, y},.
the model minimizes a cross-entropy loss:.
(cid:96) = âˆ’.
logpvoc(yt|ytâˆ’1, ..., y1, s; Î¸),.
(9).
|y |(cid:88).
t=1.
where Î¸ denotes all model parameters..4 proposed model.
our model takes a dialogue history s and thecorresponding dialogue amr as input.
formally,.
5similar to the encoder, there is also multi-head attention,a position-wise feed-forward layer and residual connections,which we omit in the equations..we adopt a graph transformer (zhu et al., 2019) toencode an amr graph, which extends the standardtransformer (vaswani et al., 2017) for modelingstructural input.
a l-layer graph transformer takesa set of node embeddings {n1, n2, ..., nm } and aset of edge embeddings {rij|i âˆˆ [1, ..., m ], j âˆˆ[1, ..., m ]} as input6 and produces more abstract2, ..., hlnode features {hlm } iteratively, wherel âˆˆ [1, ..., l].
the key difference between agraph transformer and a standard transformer isthe graph attention layer.
compared with self-attention layer (equation 2), the graph attentionlayer explicitly considers graph edges when updat-ing node hidden states.
for example, give an edge(cid:104)ni, rij, nj(cid:105), the attention score Ë†Î±ij is calculated.
1, hl.
6if there is no relation between ni and nj, rij=â€œnoneâ€.
4433ğ‘¤ğ‘¤1ğ‘¤ğ‘¤2ğ‘¤ğ‘¤3ğ‘¤ğ‘¤4ğ‘¤ğ‘¤5transformergraph transformerğ‘’ğ‘’1ğ‘’ğ‘’3ğ‘’ğ‘’2projected amr edgestextâ„1Ì‚ğ‘ ğ‘ â„2Ì‚ğ‘ ğ‘ â„3Ì‚ğ‘ ğ‘ â„4Ì‚ğ‘ ğ‘ â„5Ì‚ğ‘ ğ‘ â„1ğ‘†ğ‘†â„2ğ‘†ğ‘†â„3ğ‘†ğ‘†â„4ğ‘†ğ‘†â„5ğ‘†ğ‘†ğ‘›ğ‘›1ğ‘›ğ‘›2ğ‘›ğ‘›3ğ‘›ğ‘›4ğ‘’ğ‘’3ğ‘’ğ‘’1ğ‘’ğ‘’2graph encodersequence encoderğ‘¤ğ‘¤1ğ‘¤ğ‘¤2ğ‘¤ğ‘¤3ğ‘¤ğ‘¤4ğ‘¤ğ‘¤5feature fusionï¿½â„1ï¿½â„2ï¿½â„3ï¿½â„4ï¿½â„5ğ‘›ğ‘›1ğ‘›ğ‘›2ğ‘›ğ‘›3ğ‘›ğ‘›4ğ‘’ğ‘’3ğ‘’ğ‘’1ğ‘’ğ‘’2graph encodersequence encoderğ‘¤ğ‘¤1ğ‘¤ğ‘¤2ğ‘¤ğ‘¤3ğ‘¤ğ‘¤4ğ‘¤ğ‘¤5dual attentionğ‘¦ğ‘¦ğ‘¡ğ‘¡ğ‘ ğ‘ ğ‘¡ğ‘¡ğ‘ğ‘ğ‘¡ğ‘¡ğ‘¦ğ‘¦ğ‘¡ğ‘¡+1ğ‘ ğ‘ ğ‘¡ğ‘¡+1â€¦â€¦as:.
Ë†Î±ij =.
exp(Ë†eij)m=1 exp (Ë†eim).
,.
(cid:80)m.(w qhlâˆ’1.
i.
)t (w khlâˆ’1âˆš.
j + w rrij).
,.
Ë†eij =.
d.(10).
where w r is a transformation matrix, rij is theembedding of relation rij, d is hidden state size,and {h0m } = {n1, n2, ..., nm }.
thehidden state of ni is then updated as:.
2, ..., h0.
1, h0.
hli =.
(cid:88)m.j=1.
Î±ij(w v hlâˆ’1.
j + w rrij),.
(11).
where w v is a parameter matrix.
overall, givenan input amr graph g = (cid:104)v, e(cid:105), the graphtransformer encoder can be written as.
h = graphencoder(emb(v), emb(e)),.
2 , ..., hlwhere h = {hlgraph encoder hidden states..1 , hl.
(12)m } denotes top-layer.
4.2 enriching text representation.
we ï¬rst use the jamr aligner (flanigan et al.,2014) to obtain a node-to-word alignment, thenadopt the alignment to project the amr edges ontotext with following rules:.
Ë†rij =.
ï£±ï£´ï£².
ï£´ï£³.
ri(cid:48)j(cid:48),self,none,.
if a(ni(cid:48)) = wi, a(nj(cid:48)) = wj,if i = j,.
otherwise,.
(13)(k âˆˆwhere a is a one-to-k alignmentin this way, we obtain a projected[0, .
.
.
, n ]).
graph g(cid:48) = (cid:104)v (cid:48), e (cid:48)(cid:105), where v (cid:48) represents the set ofinput words {w1, w2, ..., wn } and e (cid:48) denotes a setof word-to-word semantic relations..inspired by previous work on amr graphmodeling (guo et al., 2019; song et al., 2019b;sun et al., 2019), we adopt a hierarchical encoderthat stacks a sequence encoder and a graph encoder.
a sequence encoder (seqencoder) transforms adialogue history into a set of hidden states:.
h s = seqencoder(emb(s))..(14).
a graph encoder incorporates the projected.
relations features into h s:.
Ë†s = graphencoder(h s, emb(e (cid:48))),.
h.(15).
in addition, we add a residual connection be-tween graph adapter and sequence encoder to fuse.
word representations before and after reï¬nement(as shown in figure 3(b)):.
h f = layernorm(h s + h.Ë†s)..(16).
where layernorm denotes the layer normaliza-tion (ba et al., 2016).
we name the hierarchicalencoder as hier, which can be used for bothdialogue understanding and dialogue responsegeneration..4.3 leveraging both text and structure cues.
we consider integrating both text cues and amrstructure cues for dialogue understanding andresponse generation, using a dual-encoder network.
first, a sequence encoder is used to transform adialogue history s into a text memory (denotedas h s = {hsn }) using equation 1.second, the amr graph g is encoded into graphmemory (denoted as h g = {hgm }) bya graph transformer encoder using equation 12..2 , ..., hg.
2 , ..., hs.
1 , hg.
1 , hs.
for dialogue understanding (figure 3(b)) anddialogue response generation (figure 3(c)), slightlydifferent methods of feature integration are useddue to their different nature of outputs.
dialogue understanding.
similar to section 4.2,we ï¬rst use the jamr aligner to obtain a node-to-word alignment a. then we fuse the word andamr node representations as follows:.
Ë†hi =.
(cid:40)f (hsf (hs.
i , hgj ),i , hâˆ…),.
if âˆƒj, a(nj) = wi,.
otherwise,.
(17).
where hâˆ… is the vector representation of the dummynode (see figure 2), f is deï¬ned as:.
h = layernorm(h1 + h2)..(18).
the fused word representations are then fed into aclassiï¬er for relation prediction (equation 5).
dialogue response generation.
we replace thestandard encoder-decoder attention (equation 7)with a dual-attention mechanism (song et al.,2019a).
in particular, given a decoder hidden statest at time step t, the dual-attention mechanismcalculates a graph context vector cst and a textcontext vector cg.
t , simultaneously:.
Ë†Î±ti = attn(st, hsi ),Ë†Î±tj = attn(st, hgj ),(cid:88)n.Ë†Î±tihsi ,.
cst =.
cgt =.
i=1(cid:88)m.j=1.
Ë†Î±tjhgj ,.
(19).
4434data-v1.
data-v2.
model.
aggcnâ€ lsrâ€ dhgatâ€ bertberts.
bertchierdual.
dev.
test.
dev.
test.
f1(Î´).
f1c(Î´).
f1(Î´).
f1c(Î´).
f1(Î´).
f1c(Î´).
f1(Î´).
f1c(Î´).
46.6(-)44.5(-)57.7(-)60.6(1.2)63.0(1.5).
66.8(0.9)68.2(0.8)68.3(0.6).
40.5(-)-52.7(-)55.4(0.9)57.3(1.2).
60.9(1.0)62.2(0.7)62.2(0.2).
46.2(-)44.4(-)56.1(-)58.5(2.0)61.2(0.9).
66.1(1.1)67.0(0.9)67.3(0.4).
39.5 (-)-50.7(-)53.2(1.6)55.4(0.9).
60.2(0.8)61.3(0.6)61.4(0.2).
---59.4 (0.7)62.2(1.3).
66.2(0.9)68.0(0.6)68.2(0.5).
---54.7(0.8)57.0(1.0).
60.5(1.1)62.2(0.4)62.3(0.4).
---57.9(1.0)59.5(2.1).
65.1(0.8)66.7(0.3)67.1(0.4).
---53.1(0.7)54.2(1.4).
59.8(1.2)61.0(0.4)61.1(0.5).
table 1: performance on dialogre, where Î´ denotes the standard deviation computed from 5 runs, and â€  indicatesresults reported by chen et al.
(2020)..and the ï¬nal context vector Ë†ct is calculated as:.
5.2 main results.
ct = w c[cs.
t ; cg.
t ] + bc,.
(20).
where w c and bc are model parameters..we name the dual-encoder model as dual..5 dialogue understanding experiments.
we evaluate our model on dialogre (yu et al.,2020), which contains totally 1,788 dialogues,10,168 relational triples and 36 relation types intotal.
on average, a dialogue in dialogre contains4.5 relational triples and 12.9 turns.
we reportexperimental results on both original (v1) andupdated (v2) english version.7.
5.1 settings.
(2020).
the same input format and hyper-we adoptforparameter settings as yu et al.
in par-the proposed model and baselines.
ticular,the input sequence is constructed as[cls]d[sep]a1[sep]a2[sep], where d de-notes the dialogue, and a1 and a2 are the twoassociated arguments.
in the bert model of yuet al.
(2020), only the hidden state of the [cls]token is fed into a classiï¬er for prediction, whileour baseline (bertc) additionally takes the hiddenstates of a1 and a2.
all hyperparameters are se-lected by prediction accuracy on validation dataset(see table 6 for detailed hyperparameters).
metrics following previous work on dialogre,we report macro f1 score on relations in both thestandard (f1) and conversational settings (f1c; yuet al., 2020).
f1c is computed over the ï¬rst fewturns of a dialogue where two arguments are ï¬rstmentioned..table 1 shows the results of different systems ondialogre.
we compare the proposed model withtwo bert-based approches, bert and berts.
based on bert, berts (yu et al., 2020) high-lights speaker information by replacing speakerarguments with special tokens.
for complete-ness, we also include recent methods, such asaggcn (guo et al., 2019), lsr (nan et al., 2020)and dhgat (chen et al., 2020).
bertc and hier,dual represent our baseline and the proposedmodels, respectively..by incorporating speaker information, bertsgives the best performance among the previoussystem.
our bertc baseline outperforms bertsby a large margin, as bertc additionally considersargument representations for classiï¬cation.
hiersigniï¬cantly (p < 0.01)8 outperforms bertc in allsettings, with 1.4 points of improvement in termsof f1 score on average.
a similar trend is observedunder f1c.
this shows that semantic information inamr is beneï¬cial to dialogue relation extraction,since amr highlights core entities and semanticrelations between them.
dual obtains slightlybetter results than hier, which shows effect ofseparately encoding a semantic structure..finally, the standard deviation values of bothdual and hier are lower than the baselines.
this indicates that our approaches are more robustregarding model initialization..5.3.impact of argument distance.
we split the dialogues of the dialogre (v2) devsetinto ï¬ve groups by the utterance-based distancebetween two arguments.
as shown in figure 4,dual gives better results than bertc except when.
7https://dataset.org/dialogre/.
8we use pair-wised t-test..4435figure 4: the performance of bertc (baseline) anddual (ours) regarding argument distances..the argument distance is less than 5. in particular,dual surpasses bertc by a large margin whenthe arguments distance is greater than 20. thecomparison indicates that amr can help a model tobetter handle long-term dependencies by improvingthe entity recall.
in addition to utterance distance,we also consider word distance and observe asimilar trend (as shown in appendix 7)..5.4 case study.
figure 5 shows a conversation between a managerand an employee who might have taken a leave.
the baseline model incorrectly predicts that therelation between two interlocutors is parent andchild.
it might be inï¬‚uenced by the last sentencein the conversation, assuming that it is a dialoguebetween family members.
however, the proposedmodel successful predicts the interlocutorsâ€™ re-lation, suggesting it can extract global semanticinformation in the dialogue from a comprehensiveperspective..6 response generation experiments.
we conduct experiments on the dailydialog bench-mark (li et al., 2017), which contains 13,119 dailymulti-turn conversations.
on average, the numberof turns for each dialogue is 7.9, and each utterancehas 14.6 tokens..figure 5: case study for dialogue relation extraction..model.
bleu-1/2/3/4.
distinct-1/2.
seq2seqâ€ ivaemiplato w/o lâ€ (cid:91)platoâ€ (cid:91).
transformerhierdual.
33.6/26.8/-/-30.9/24.9/-/-40.5/32.2/-/-39.7/31.1/-/-.
38.3/31.7/29.1/27.841.3/35.4/33.2/32.140.8/35.0/32.7/31.5.
3.0/12.82.9/25.04.6/24.65.3/29.1.
5.8/30.56.5/32.36.6/33.0.
table 2: performance on dailydialog.
results markedwith â€  are from bao et al.
(2020).
models marked with(cid:91) requires external corpus for pretraining..the target response while the latter assesses thegeneration diversity, which is deï¬ned as the numberof distinct uni- or bi-grams divided by the totalamount of generated words.
in addition, we alsoconduct human evaluation.
following bao et al.
(2020), we ask annotators who study linguistics toevaluate model outputs from four aspects, whichare ï¬‚uency, coherence, informativeness and overallperformance.
the scores are in a scale of {0, 1, 2}.
the higher, the better..6.1 settings.
6.2 automatic evaluation results.
we take transformer as a baseline.
our hyperpa-rameters are selected by word prediction accuracyon validation dataset.
the detailed hyperparame-ters are given in appendix (see table 6).
metric we set the decoding beam size as 5and adopt bleu-1/2/3/4 (papineni et al., 2002)and distinct-1/2 (li et al., 2016) as automaticevaluation metrics.
the former measures the n-gram overlap between generated response and.
table 2 reports the performances of the previousstate-of-the-art methods and proposed models onthe dailydialog testset.
for the previous methods,plato and plato w/o l are both transformermodels pre-trained on large-scale conversationaldata (8.3 million samples) and ï¬netuned on dai-lydialog.
for completeness, we also report othersystems including seq2seq (vinyals and le, 2015)and ivaemi (fang et al., 2019)..4436<5[5,10)[10, 15)[15, 20]>20argument distance (# utterances)6466687072f1baselineoursdialogue:speaker-1:a new place for a new ross.
i'm gonnahave you and all the guys from work over once it's y'know, furnished.speaker-2:i must say it's nice to see you back on your feet.speaker-1:well i am that.
and that whole rage thing is definitely behind me.speaker-2:i wonder if its time for you to rejoin our team at the museum?speaker-1:oh donald that-that would be great.
i am totally ready to come back to work.
iâ€¦what?
no!
wh-what are you doing?!!
get off my sister!!!!!!!!!!!!
!ground-truth: per:boss(s1, s2)baseline: per:parent(s1, s2)ours: per:boss(s1, s2)model.
fluency coherence.
inf..overall.
setting.
dialogre (v2) dailydialog.
transformerhierdual.
1.761.861.88.
0.861.041.04.
1.401.481.52.
0.660.820.84.table 3: human evaluation results on dailydialog.
inf.
stands for informativeness..dialog-amr(dual).
-speaker-ident.
concept-corefutter-amrtext.
68.267.568.067.867.466.2.
38.2/5.937.7/5.737.9/5.837.4/5.636.9/5.635.4/5.5.
table 4: ablation study on the development sets of bothdialogre (v2) and dailydialog..among the previous systems, plato andplato w/o l report the best performances.
ourtransformer baseline is highly competitive in termsof bleu and distinct scores.
compared with thetransformer baseline, both dual and hier showbetter numbers regarding bleu and distinct, andthe gains of both models are signiï¬cant (p < 0.01).
this indicates that semantic information in amrgraphs is useful for dialogue response generation.
in particular, the gains come from better recallof the important entities and their relations in adialogue history, which can leads to generating amore detailed response..6.3 human evaluation results.
we conduct human evaluation on randomly se-lected 50 dialogues and corresponding generatedresponses of the baseline and our models.
asshown in table 3, the transformer baseline givesthe lowest scores, while dual sees the highestscores from all aspects.
our main advantage is onthe coherence, meaning that amrs are effective onrecalling important concepts and relations.
as theresult, it makes it easier for our models to generatecoherent replies.
examples are shown in figure 8in appendix.
comparatively, all systems achievehigh scores regarding fluency, suggesting that thisaspect is not the current bottleneck for responsegeneration..7 analysis.
this section contains analysis concerning the ef-fects of graph features, dialogue length and modelrobustness.
we use dual model for experimentssince it gives slightly better results than hier..7.1 ablation on amr graph.
table 4 shows the results of our best performingmodels on the two datasets regarding different con-ï¬gurations on the dialogue amr graphs.
we reportthe average f1 score for dialogre and the bleu-1/distinct-1 score for dailydialog.
first, usingutterance-level amr improves the text baselineby 1.2 points and 1.5 points with regard to f1 and.
figure 6: devset performance against dialogue lengths..bleu-1 scores, respectively.
this indicates thatthe semantic knowledge in formal amr is helpfulfor dialogue modeling..second, our manually added relations (in sec-tion 2) also leads to improvements, ranging from0.5 to 1.0 in bleu-1 score.
the speaker relation isthe most important for dialogue relation extraction,a possible reason is that dialogre dataset mainlyfocus on person entities.
also, co-reference rela-tions help the most in dialogue response generation.
the identical concept relations give least improve-ments among three relations.
finally, combiningall relations to build a dialog-amr graph achievesbest performance on both datasets..7.2.impact of dialogue length.
we group the devset of dialogre (v2) and daily-dialog into ï¬ve groups according to the numberof utterances in a dialogue.
figure 6 summarizesthe performance of the baseline and the proposedmodel on dialogue understanding (du) and re-sponse generation (rg) tasks.
in dialogue under-standing, our model gives slightly better f1 scoresthan the baseline when a dialogue has smaller than12 utterance.
the performance improvement ismore signiï¬cant when modeling a long dialogue.
this conï¬rms our motivation that amr can helpto understand long dialogues.
in dialogue responsegeneration, our model consistently outperformsthe transformer baseline by a large margin on.
4437<4[4,8)[8,12)[12, 16]>16dialogue length (# utterances)3040506070baseline(du)ours(du)baseline(rg)ours(rg)model.
original.
paraphrased.
baselineours.
100100.
94.5098.50.table 5: f1 on original and paraphrased testsets..dialogues of different lengths, still with moreimprovements on larger dialogues.
overall, theseresults are consistent with table 1 and 2, showingthat amr can provide useful semantic informationand alleviate the issue of long-range dependency..7.3 robustness against input.
recent studies show that neural network-baseddialog models lack robustness (shalyminov andlee, 2018; einolghozati et al., 2019).
we select 100instances from the testset of dialogre (v2) whereboth baseline and our model gives true prediction,before paraphrasing the source dialogues manually(see appendix b.3 for paraphrasing guidelines.)..
results on the paraphrased dataset are givenin table 5. the performance of baseline modeldrop from 100 to 94.5 on paraphrased dataset.
bycontrast, the result of our model reaches 98.5, 4points higher than baseline.
this conï¬rms ourassumption that amr can reduce data sparsity, thusimprove the robustness of neural models..8 related work.
semantic parsing for dialogue some previouswork builds domain-speciï¬ed semantic schemafor task-oriented dialogues.
for example, in thepegasus (zue et al., 1994) system, a sentenceis ï¬rst transformed into a semantic frame and thenused for travel planing.
wirsching et al.
(2012)use semantic features to help a dialogue systemperform certain database operations.
gupta et al.
(2018) represent task-oriented conversations as se-mantic trees where intents and slots are tree nodes.
they solve intent classiï¬cation and slot-ï¬lling taskvia semantic parsing.
cheng et al.
(2020) designa rooted semantic graph that integrates domains,verbs, operators and slots in order to performdialogue state tracking.
all these structures aredesigned for speciï¬ed task only.
in contrast, weinvestigate a general semantic representation forthe modeling of everyday conversations..constructing amrs beyond sentence levelthere are a few attempts to construct amrsbeyond the sentence level.
liu et al.
(2015) con-struct document-level amrs by merging identical.
concepts of sentence-level amrs for abstractivesummerization, and liao et al.
(2018) furtherextend this approach to multi-document summer-ization.
oâ€™gorman et al.
(2018) manually annotateco-reference information across sentence amrs.
we focus on creating conversation-level amrs tofacilitate information exchange more effectivelyfor dialogue modeling..bonial et al.
(2020) adapt amrs on dialogues byenriching the standard amr schema with dialogueacts, tense and aspect, and they construct a datasetconsisting of 340 dialogue amrs.
however, theypropose theoretical changes in the schema forannotating amrs, while we explore empiricalsolutions that leverage existing amrs of thestandard schema on dialogues..amr parsing and encoding our work is alsorelated to amr parsing (flanigan et al., 2014;konstas et al., 2017a; lyu and titov, 2018; guo andlu, 2018; zhang et al., 2019; cai and lam, 2020)and amr encoding (konstas et al., 2017b; songet al., 2018; zhu et al., 2019; song et al., 2020;zhao et al., 2020; bai et al., 2020).
the former taskmakes it possible to use automatically-generatedamrs for downstream applications, while the latterhelps to effectively exploit structural informationin amrs.
in this work, we investigate amrs fordialogue representation and combine amrs withtext for dialogue modeling..9 conclusion.
we investigated the feasibility of using amrsfor dialogue modeling, describing an algorithmto construct dialogue-level amrs automaticallyand exploiting two ways to incorporate amrsinto neural dialogue systems.
experiments ontwo benchmarks show advantages of using amrsemantic representations model on both dialogueunderstanding and dialogue response generation..acknowledgments.
yue zhang is the corresponding author.
we wouldlike to thank the anonymous reviewers for theirinsightful comments and jinhao jiang for his helpfor data preparation.
this work has been supportedby tencent ai lab rhino-bird focused researchprogram.
it also receives support from the westlakeuniversity and bright dream joint institute forintelligent robotics, and a research grant fromrxhui inc..4438references.
lei jimmy ba, jamie ryan kiros, and geoffrey e.corr,.
layer normalization..hinton.
2016.abs/1607.06450..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..xuefeng bai, pengbo liu, and yue zhang.
2021. inves-tigating typed syntactic dependencies for targetedsentiment classiï¬cation using graph attention neu-ieee/acm transactions on audio,ral network.
speech, and language processing, 29:503â€“514..xuefeng bai, linfeng song, and yue zhang.
2020.online back-parsing for amr-to-text generation.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 1206â€“1219, online.
association for computa-tional linguistics..laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifï¬tt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse..siqi bao, huang he, fan wang, hua wu, and haifengwang.
2020. plato: pre-trained dialogue genera-tion model with discrete latent variable.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 85â€“96, online.
association for computational linguistics..claire bonial, lucia donatelli, mitchell abrams,stephanie m. lukin, stephen tratz, matthew marge,ron artstein, david traum, and clare voss.
2020.dialogue-amr: abstract meaning representationfor dialogue.
in proceedings of the 12th languageresources and evaluation conference, pages 684â€“695, marseille, france.
european language re-sources association..paweÅ‚ budzianowski, tsung-hsien wen, bo-hsiangtseng, iËœnigo casanueva, stefan ultes, osman ra-madan, and milica gaË‡siÂ´c.
2018. multiwoz - alarge-scale multi-domain wizard-of-oz dataset forin proceedingstask-oriented dialogue modelling.
of the 2018 conference on empirical methods innatural language processing, pages 5016â€“5026,brussels, belgium.
association for computationallinguistics..deng cai and wai lam.
2020. amr parsing viagraph-sequence iterative inference.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1290â€“1301, on-line.
association for computational linguistics..iacer calixto, qun liu, and nick campbell.
2017.doubly-attentive decoder for multi-modal neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1913â€“1924, vancouver, canada.
association for computa-tional linguistics..hui chen, pengfei hong, wei han, navonil majumder,and soujanya poria.
2020. dialogue relation ex-traction with document-level heterogeneous graphattention networks.
corr, abs/2009.05092..jianpeng.
cheng,.
devang agrawal,.
hÂ´ectormartÂ´Ä±nez alonso, shruti bhargava, joris driesen,federico flego, dain kaplan, dimitri kartsaklis,lin li, dhivya piraviperumal, jason d. williams,hong yu, diarmuid Â´o sÂ´eaghdha, and andersjohannsen.
2020. conversational semantic parsingin proceedings of thefor dialog state tracking.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8107â€“8117,online.
association for computational linguistics..eunsol choi, he he, mohit iyyer, mark yatskar,wen-tau yih, yejin choi, percy liang, and lukezettlemoyer.
2018. quac: question answering incontext.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 2174â€“2184, brussels, belgium.
associationfor computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171â€“4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2019. wizardof wikipedia: knowledge-powered conversationalagents.
in 7th international conference on learningrepresentations, iclr 2019, new orleans, la, usa,may 6-9, 2019. openreview.net..arash einolghozati, sonal gupta, mrinal mohit, andimproving robustness of task.
rushin shah.
2019.oriented dialog systems.
corr, abs/1911.05153..le fang, chunyuan li, jianfeng gao, wen dong, andchangyou chen.
2019. implicit deep latent variablein proceedings of themodels for text generation.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3946â€“3956, hong kong,china.
association for computational linguistics..jeffrey flanigan, sam thomson, jaime carbonell,chris dyer, and noah a. smith.
2014. a discrim-inative graph-based parser for the abstract meaning.
4439in proceedings of the 52nd an-representation.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1426â€“1436, baltimore, maryland.
association for compu-tational linguistics..barbara j. grosz, aravind k. joshi, and scott wein-stein.
1995. centering: a framework for modelingthe local coherence of discourse.
computationallinguistics, 21(2):203â€“225..zhijiang guo and wei lu.
2018. better transition-based amr parsing with a reï¬ned search space.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 1712â€“1722, brussels, belgium.
associationfor computational linguistics..zhijiang guo, yan zhang, and wei lu.
2019. attentionguided graph convolutional networks for relationthe 57th annualextraction.
meeting of the association for computational lin-guistics, pages 241â€“251, florence, italy.
associationfor computational linguistics..in proceedings of.
sonal gupta, rushin shah, mrinal mohit, anuj ku-mar, and mike lewis.
2018. semantic parsing fortask oriented dialog using hierarchical representa-in proceedings of the 2018 conference ontions.
empirical methods in natural language processing,pages 2787â€“2792, brussels, belgium.
associationfor computational linguistics..hardy hardy and andreas vlachos.
2018. guidedneural language generation for abstractive summa-rization using abstract meaning representation.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages768â€“773, brussels, belgium.
association for com-putational linguistics..divyansh kaushik,.
eduard h. hovy,2020..andlearningzachary chase lipton.
that makes a differencetheinwith8thlearningrepresentations, iclr 2020, addis ababa, ethiopia,april 26-30, 2020. openreview.net..differencecounterfactually-augmentedinternational conference.
data.
on.
paul r. kingsbury and martha palmer.
2002. fromtreebank to propbank.
in proceedings of the thirdinternational conference on language resourcesand evaluation, lrec 2002, may 29-31, 2002, laspalmas, canary islands, spain.
european languageresources association..ioannis konstas, srinivasan iyer, mark yatskar, yejinchoi, and luke zettlemoyer.
2017a.
neural amr:sequence-to-sequence models for parsing and gener-in proceedings of the 55th annual meetingation.
of the association for computational linguistics,acl 2017, vancouver, canada, july 30 - august 4,volume 1: long papers, pages 146â€“157.
associationfor computational linguistics..ioannis konstas, srinivasan iyer, mark yatskar, yejinchoi, and luke zettlemoyer.
2017b.
neural amr:sequence-to-sequence models for parsing and gener-ation.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 146â€“157, vancouver,canada.
association for computational linguistics..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110â€“119, san diego, california.
associationfor computational linguistics..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-ally labelled multi-turn dialogue dataset.
in proceed-ings of the eighth international joint conferenceon natural language processing (volume 1: longpapers), pages 986â€“995, taipei, taiwan.
asian fed-eration of natural language processing..zhongli li, qingyu zhou, chao li, ke xu, and yunboimproving bert with syntax-aware.
cao.
2020.local attention.
corr, abs/2012.15150..kexin liao, logan lebanoff, and fei liu.
2018. ab-stract meaning representation for multi-documentin proceedings of the 27th inter-summarization.
national conference on computational linguistics,pages 1178â€“1190, santa fe, new mexico, usa.
association for computational linguistics..fei liu, jeffrey flanigan, sam thomson, normansadeh, and noah a. smith.
2015. toward abstrac-tive summarization using semantic representations.
in proceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1077â€“1086, denver, colorado.
associationfor computational linguistics..chunchuan lyu and ivan titov.
2018. amr parsingin pro-as graph prediction with latent alignment.
ceedings of the 56th annual meeting of the asso-ciation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume1: long papers, pages 397â€“407.
association forcomputational linguistics..utthara gosa mangai, suranjana samanta, sukhendudas, and pinaki roy chowdhury.
2010. a sur-vey of decision fusion and feature fusion strategiesiete technical review,for pattern classiï¬cation.
27(4):293â€“307..diego marcheggiani, jasmijn bastings, and ivan titov.
exploiting semantics in neural machine2018.translation with graph convolutional networks.
inproceedings of the 2018 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 2 (short papers), pages 486â€“492, new.
4440orleans, louisiana.
association for computationallinguistics..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factu-ality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1906â€“1919, on-line.
association for computational linguistics..guoshun nan, zhijiang guo, ivan sekulic, and wei lu.
2020. reasoning with latent structure reï¬nement fordocument-level relation extraction.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1546â€“1557, on-line.
association for computational linguistics..matthew l newman, carla j groom, lori d handel-man, and james w pennebaker.
2008. gender dif-ferences in language use: an analysis of 14,000 textsamples.
discourse processes: a multidisciplinaryjournal, 45(3):211â€“236..tong niu and mohit bansal.
2020..avgout: asimple output-probability measure to eliminate dullresponses.
in proceedings of the aaai conferenceon artiï¬cial intelligence, pages 8560â€“8567..tim oâ€™gorman, michael regan, kira grifï¬tt, ulfhermjakob, kevin knight, and martha palmer.
2018.amr beyond the sentence: the multi-sentence amrin proceedings of the 27th internationalcorpus.
conference on computational linguistics, pages3693â€“3702, santa fe, new mexico, usa.
associ-ation for computational linguistics..martha palmer, paul r. kingsbury, and daniel gildea.
2005. the proposition bank: an annotated corpus ofsemantic roles.
comput.
linguistics, 31(1):71â€“106..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automaticevaluation of machine translation.
in proceedings ofthe 40th annual meeting of the association for com-putational linguistics, pages 311â€“318, philadelphia,pennsylvania, usa.
association for computationallinguistics..adam poliak, jason naradowsky, aparajita haldar,rachel rudinger, and benjamin van durme.
2018.hypothesis only baselines in natural language in-in proceedings of the seventh joint con-ference.
ference on lexical and computational semantics,pages 180â€“191, new orleans, louisiana.
associa-tion for computational linguistics..jun quan, deyi xiong, bonnie webber, and changjianhu.
2019. gecor: an end-to-end generative el-lipsis and co-reference resolution model for task-oriented dialogue.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 4547â€“4557, hong kong, china.
association for computational linguistics..pushpendre rastogi, arpit gupta, tongfei chen, andmathias lambert.
2019. scaling multi-domain di-alogue state tracking via query reformulation.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 2 (industry papers), pages 97â€“105, min-neapolis, minnesota.
association for computationallinguistics..siva reddy, danqi chen, and christopher d. manning.
2019. coqa: a conversational question answeringchallenge.
transactions of the association for com-putational linguistics, 7:249â€“266..alan ritter, colin cherry, and william b. dolan.
2011.data-driven response generation in social media.
inproceedings of the 2011 conference on empiricalmethods in natural language processing, pages583â€“593, edinburgh, scotland, uk.
association forcomputational linguistics..devendra singh sachan, yuhao zhang, peng qi, andwilliam l. hamilton.
2021. do syntax trees helpinpre-trained transformers extract information?
proceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, eacl 2021, online, april19 - 23, 2021, pages 2647â€“2661.
association forcomputational linguistics..iulian vlad serban, alessandro sordoni, ryan lowe,laurent charlin, joelle pineau, aaron c. courville,and yoshua bengio.
2017. a hierarchical latentvariable encoder-decoder model for generating di-in proceedings of the thirty-first aaaialogues.
conference on artiï¬cial intelligence, february 4-9,2017, san francisco, california, usa, pages 3295â€“3301. aaai press..igor shalyminov and sungjin lee.
2018. improving ro-bustness of neural dialog systems in a data-efï¬cientway with turn dropout.
in the thirty-second annualconference on neural information processing sys-tems (nips) 2018, workshop on conversational ai:â€œtodayâ€™s practice and tomorrowâ€™s potential..sanuj sharma, prafulla kumar choubey, and ruihonghuang.
2019. improving dialogue state tracking byin proceedings ofdiscerning the relevant context.
the 2019 conference of the north american chapterof the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 576â€“581, minneapolis,minnesota.
association for computational linguis-tics..linfeng song, daniel gildea, yue zhang, zhiguowang, and jinsong su.
2019a.
semantic neuralmachine translation using amr.
transactions of theassociation for computational linguistics, 7:19â€“31..linfeng song, ante wang, jinsong su, yue zhang,kun xu, yubin ge, and dong yu.
2020. structuralinformation preserving for graph-to-text generation..4441in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages7987â€“7998, online.
association for computationallinguistics..linfeng song, yue zhang, daniel gildea, mo yu,zhiguo wang, and jinsong su.
2019b.
leveragingdependency forest for neural medical relation extrac-in proceedings of the 2019 conference ontion.
empirical methods in natural language process-ing and the 9th international joint conference onnatural language processing (emnlp-ijcnlp),pages 208â€“218, hong kong, china.
association forcomputational linguistics..linfeng song, yue zhang, zhiguo wang, and danielgildea.
2018. a graph-to-sequence model for amr-in proceedings of the 56th an-to-text generation.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1616â€“1626, melbourne, australia.
association for compu-tational linguistics..patrick verga, daniel andor,emma strubell,and andrew mccallum.
2018.david weiss,linguistically-informed self-attention for semanticthe 2018role labeling.
conference on empirical methodsin naturallanguage processing, brussels, belgium, october31 - november 4,pages 5027â€“5038.
association for computational linguistics..in proceedings of.
2018,.kai sun, richong zhang, samuel mensah, yongyimao, and xudong liu.
2019. aspect-level senti-ment analysis via convolution over dependency tree.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5679â€“5688, hong kong, china.
association for computa-tional linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104â€“3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998â€“6008..oriol vinyals and quoc v. le.
2015. a neural conver-.
sational model.
corr, abs/1506.05869..tsung-hsien wen, milica gaË‡siÂ´c, nikola mrkË‡siÂ´c, pei-hao su, david vandyke, and steve young.
2015.semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
inproceedings of the 2015 conference on empirical.
methods in natural language processing, pages1711â€“1721, lisbon, portugal.
association for com-putational linguistics..tsung-hsien wen, david vandyke, nikola mrkË‡siÂ´c,milica gaË‡siÂ´c, lina m. rojas-barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguein proceedings of the 15th conferencesystem.
of the european chapter of the association forcomputational linguistics: volume 1, long papers,pages 438â€“449, valencia, spain.
association forcomputational linguistics..gÂ¨unther wirsching, markus huber, christian kÂ¨olbl,robert lorenz, and ronald rÂ¨omer.
2012. semanticdialogue modeling.
in anna esposito, antoni-etta m. esposito, alessandro vinciarelli, rÂ¨udigerhoffmann, and vincent c. mÂ¨uller, editors, cogni-tive behavioural systems: cost 2102 internationaltraining school, dresden, germany, february 21-26, 2011, volume 7403..p. xu and r. sarikaya.
2014. contextual domainclassiï¬cation in spoken language understanding sys-tems using recurrent neural network.
in 2014 ieeeinternational conference on acoustics, speech andsignal processing (icassp), pages 136â€“140..dian yu, kai sun, claire cardie, and dong yu.
2020.dialogue-based relation extraction.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 4927â€“4940, on-line.
association for computational linguistics..sheng zhang, xutai ma, kevin duh, and benjaminvan durme.
2019. amr parsing as sequence-to-graph transduction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 80â€“94, florence, italy.
associa-tion for computational linguistics..yanbin zhao, lu chen, zhi chen, ruisheng cao,su zhu, and kai yu.
2020. line graph enhancedamr-to-text generation with mix-order graph at-in proceedings of the 58th an-tention networks.
nual meeting of the association for computationallinguistics, pages 732â€“741, online.
association forcomputational linguistics..jie zhu, junhui li, muhua zhu, longhua qian, minzhang, and guodong zhou.
2019. modeling graphstructure in transformer for better amr-to-text gen-eration.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5459â€“5468, hong kong, china.
association forcomputational linguistics..victor zue, stephanie seneff,.
joseph polifroni,michael phillips, christine pao, david goddeau,james glass, and eric brill.
1994.pegasus:a spoken language interface for on-line air travel.
4442in human language technology: pro-planning.
ceedings of a workshop held at plainsboro, newjersey, march 8-11, 1994..4443b.2 case study for dialogue response.
generation.
figure 8 represents a conversation between ahotel service and a guest who wants to book aroom, along with its ground-truth response andmodel-generated responses.
we can observe thattransformerâ€™s output is general and not consistentwith dialogue history.
while proposed modelsâ€™outputs can capture the core information â€œroomâ€from the history, and are more relevant to the topic.
besides, the output given by proposed model issemantically similar to the ground-truth output, butusing novel words to response, indicating that themodel not only captures the simple dependencybetween input and output sentences, but also learnsdeep semantic information of the dialogue history..b.3 paraphrasing guidelines.
we ask annotators to paraphrase the dialoguesfollowing 3 guidelines:.
â€¢ do not change the original meaning.
â€¢ paraphrase the sentence by using different.
lexicon and syntax structures..â€¢ paraphrase the dialogue as much as they can.
we also ask a judge to evaluate whether theparaphrased dialogue (sentences) convey the samemeaning of the original ones..figure 7: performance against argument word distance..a model parameters.
table 6 lists all model hyperparameters used forexperiments.
in particular, we share the wordvocabulary of encoder and decoder for responsegeneration.
we implement our baselines and pro-posed model based on pytorch.
the preprocesseddata and source code will be released at https://github.com/muyeby/amr-dialogue..b more experimental results.
b.1.
impact of argument distance.
in addition to utterance distance used in figure 4,we also consider word-based distance as a metricto measure argument distance.
figure 7 shows f1scores of baseline and our model on 5 groups of testinstances.
it can be seen that our model gives betterresults than baseline system among all distanceslonger than 30. in particular, our model surpassbaseline by 8 points when argument distance islonger than 120..figure 8: case study for dialogue response generation..4444<30[30,60)[60,90)[90,120]>120argument distance (# tokens)6264666870f1baselineoursdialoguehistory:â€¦speaker-1:we have new room rates, sir.
will that be acceptable to you?
speaker-2:well , it depends on the price, of course.
what is it?
speaker-2:it's $ 308 a night.
speaker-1:i have no problem with that.
speaker-2:great!
would you prefer smoking or nonsmoking?
speaker-1:definitely nonsmoking.
i can't handle that smell.ground-truth: now, is a queen-size bed okay?transformer: iâ€™m sorry, sir.
iâ€™ll be ï¬ne.ours: thatâ€™ll be nonsmoking.
now, do you prefer a single queen-size bed?
sequence encoder.
graph encoder(hier).
graph encoder(dual).
others.
setting.
dialogre.
dailydialog.
dropoutencoder layersattention headsembedding sizehidden layer sizeword vocabulary sizefeed-forward layer sizenumber of parameters.
dropoutencoder layersattention headshidden layer sizerelation embedding sizefeed-forward layer sizenumber of parameters.
dropoutencoder layersattention headshidden layer sizerelation embedding sizeconcept vocabulary sizefeed-forward layer sizenumber of parameters.
0.1121276876831k3072110m.
0.1285126410244m.
0.138512645.2k102411m.
0.14851251216k102438m.
0.1285126410244m.
0.1485126410k102420m.
optimizerbatch sizelearning ratetraining epochdecoder layerstraining devicetraining time.
adam483e-530-tesla v100120min.
adam201e-42004tesla v10048h.
table 6: hyperparameters of our models on dialogre and dailydialog..4445