keep it simple: unsupervised simpliﬁcation of multi-paragraph text.
philippe labanuc berkeley.
tobias schnabelmicrosoft.
paul n. bennettmicrosoft.
marti a. hearstuc berkeley∗.
abstract.
this work presents keep it simple (kis), anew approach to unsupervised text simpliﬁca-tion which learns to balance a reward acrossthree properties: ﬂuency, salience and simplic-ity.
we train the model with a novel algorithmto optimize the reward (k-scst), in whichthe model proposes several candidate simpli-ﬁcations, computes each candidate’s reward,and encourages candidates that outperform themean reward.
finally, we propose a realis-tic text comprehension task as an evaluationmethod for text simpliﬁcation.
when tested onthe english news domain, the kis model out-performs strong supervised baselines by morethan 4 sari points, and can help people com-plete a comprehension task an average of 18%faster while retaining accuracy, when com-pared to the original text..1.introduction.
the main objective of text simpliﬁcation is to makea complex text accessible to a wide audience byincreasing its readability.
in contrast with text sum-marization – in which key content is selected toremain in the summary and other content is elided– in text simpliﬁcation, ideally all relevant contentis preserved..we propose that text simpliﬁcation algorithmsneed to balance three properties: (1) ﬂuency: thesimpliﬁed text should use well-formed english sen-tences, (2) salience: the simpliﬁed text should relaythe same information as the original, and (3) sim-plicity: the simpliﬁed text should be syntacticallyand lexically simpler than the original..figure 1 provides intuition for the necessity ofeach of the three properties.
it shows the origi-nal text and the output of the full proposed modelcompared to three reduced versions:.
∗ author.
emails:.
{phillab,hearst}@berkeley.edu,.
{tobias.schnabel,paul.n.bennett}@microsoft.com.
figure 1: motivating example for the kis method,based on a cbs article (lewis, 2021).
we optimize athree-component reward: ﬂuency, salience and simplic-ity.
we show model outputs when trained with all threecomponents, and with a missing component..without fluency, the generator has no incen-tive to generate full sentences, and learns it canboost the simplicity score by generating shortphrases with excessive punctuation..without salience, the generator does not gainby covering facts in the original text, and can im-prove the simplicity score by learning to removefacts (e.g., not mentioning planet mars by name).
without simplicity, the generator is not guidedto favor syntactically and lexically simpler re-writes.
in figure 1, model no simplicity is in factmore complex than the original according to read-ability measures..as we show in the related work section (sec-tion 2), there are no high-quality, large datasetspublicly released for text simpliﬁcation.
in thiswork, we build on recent progress of reinforcementlearning (rl)-based training of text generators: we.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6365–6378august1–6,2021.©2021associationforcomputationallinguistics6365original: nasa's curiosity rover just celebrated a majormilestone — 3,000 days on the surface of mars.
to mark theoccasion, the space agency has released a stunning newpanorama of the red planet, captured by the rover.model full: nasa's curiosity rover has now passed 3,000days of travel on the surface of mars.
to mark the milestone,the space agency released a huge panorama of mars, asseen by the rover.model no fluency: nasa's curiosity rover.
celebrated.
amajor milestone — 3,000 days on.
the of.. to mark.
thespace agency has.
a stunning new panorama.. red planet.captured by.
the rover.
howevermodel no salience: nasa's curiosity rover just celebrated amajor milestone.
the space agency has released a stunningnew panoramic of the red planet, captured by the team.
itwas by the rover's panoramic camera.model no simplicity: nasa's curiosity rover has celebrateda major milestone, 3,000 days on the ground of mars.
tomark the occasion, the space agency has unveiled a stunningnew panoramic view of the red planet, captured by the rover.
formulate a reference-free reward for text simpliﬁ-cation and directly optimize it, circumventing theneed for aligned data..our main contribution is the keep it simple(kis) procedure, a novel unsupervised method fortext simpliﬁcation.
applied to the english news do-main, kis outperforms several supervised modelson common simpliﬁcation metrics such as sari(xu et al., 2016) and the flesch-kincaid gradelevel (kincaid et al., 1975)..a second contribution is a new algorithm for rl-based training of text generators, k-scst, whichis an extension of self-critical sequence training(rennie et al., 2017).
for each input, we generatek sampled outputs (vs. 2 in scst), and use themean population reward as a baseline.
we show insection 4 that in our domain, k-scst outperformsmodels trained with scst..a third contribution is a novel evaluation methodfor text simpliﬁcation.
based on the assumptionthat simpliﬁed text should enable faster readingwith better understanding, we propose a realistictext comprehension task.
we show that peoplereading texts simpliﬁed by kis are able to completecomprehension tasks faster than comparison texts.
another departure from previous work is that wework with paragraphs as units of text.
most workin text simpliﬁcation is done at the sentence level,despite work such as zhong et al.
(2020) showingthat common simpliﬁcation phenomena occur atthe level of the paragraph, (e.g., the deletion, inser-tion or re-ordering of full sentences).
speciﬁcally,we train our models to simplify full paragraphs,and evaluate our models in a human evaluation onshort documents (i.e., 3-4 paragraphs)..through rigorous empirical evaluation, wedemonstrate the strong performance of our ap-proach; automated results show that this unsuper-vised approach is able to outperform strong su-pervised models by 4 sari points or more.
wepublicly released the code and model checkpoints1..2 related work.
simpliﬁcation datasets.
early datasets were ﬁrstbased on simple wikipedia2: wikismall (zhuet al., 2010), later expanded into wikilarge (zhangand lapata, 2017).
xu et al.
(2015) show there arequality concerns with simple wikipedia datasets,.
1https://github.com/tingofurro/keep_.
it_simple.
and propose newsela3 as a replacement.
newselais a project led by educators re-writing news ar-ticles targeting different school grade levels.
weview newsela as the gold-standard for our work,and use the public newsela release of 1,911 groupsof articles to design and evaluate our work.
us-ing a coarse paragraph alignment algorithm, weextract 40,000 paired simple/complex paragraphstargeting a separation of 4 grade levels.
we callthis dataset the paired newsela dataset, which weuse for analysis and baseline training..seq2seq for simpliﬁcation.
text simpliﬁca-tion is most commonly framed as a sequence-to-sequence (seq2seq) task, leveraging model archi-tectures of other seq2seq tasks, such as natural ma-chine translation (zhu et al., 2010; wubben et al.,2012).
martin et al.
(2020) introduce access, aﬁnetuned transformer model that achieves state-of-the-art performance on wikilarge.
accesscan customize simpliﬁcations on parameters suchas compression rate and paraphrase amount.
wedirectly compare our approach to access..data availability remains one of the main lim-itations to seq2seq-based text simpliﬁcation.
weside-step this issue entirely by working with unsu-pervised data, only requiring a small dataset withcoarse-level alignments for calibration..lexical simpliﬁcation focuses on the substi-tution of single words or phrases with simplerequivalents, with diverse approaches using lexicaldatabases such as wordnet (thomas and anderson,2012), to using contextualized word vectors (qianget al., 2020).
these methods tend to be limited, asthey do not consider syntactic complexity, and haveno direct way of modeling deletions and insertions.
we incorporate a lexical score (lscore) as one ofthe rewards in our simplicity component..text-edit for simpliﬁcation.
recent work(dong et al., 2019; stahlberg and kumar, 2020)has modeled text simpliﬁcation as a text-edit task,learning sequences of word-edits that transform theinput into the output.
text editing offers explain-ability, at the cost of added model complexity.
weﬁnd that without explicitly representing edits, thekis model easily learns to copy (using attentionheads) and deviate from the original text.
outputscan be post-processed into edits, if desired..unsupervised simpliﬁcation has mostly beenlimited to lexical simpliﬁcation.
recently suryaet al.
(2019) (unsup nts) proposed a system that.
2https://simple.wikipedia.org/.
3https://newsela.com/.
6366can perform both lexical and syntactic simpliﬁca-tion, with a joint encoder, and two decoders (simpleand complex).
we directly compare our unsuper-vised approach to unsup nts..rl for simpliﬁcation.
prior work (zhang andlapata, 2017; guo et al., 2018) used reinforce-ment learning (rl)-based simpliﬁcation.
how-ever, in both cases, components of the reward ortraining procedure involved reference simpliﬁca-tions, requiring an aligned dataset.
by designinga reference-free reward, we are able to train ourmodel with rl without supervision..evaluation of simpliﬁcation.
this usually fallsinto two categories: automatic ofﬂine evaluation,and human evaluation.
automatic evaluations usu-ally involve using n-gram overlap calculations suchas bleu (papineni et al., 2002) and sari (xuet al., 2016)).
sari was shown to correlate betterwith human judgements of simplicity than bleu,and it has since become a standard (zhang and lap-ata, 2017; surya et al., 2019; martin et al., 2020).
inour experiments, we report both sari and bleu..human evaluation is typically done in an intrin-sic way – e.g., by directly rating factors like ﬂuency,simplicity and relevance of model outputs (suryaet al., 2019; wubben et al., 2012).
in this work,we propose an extrinsic, task-based protocol.
inour comprehension study, we directly measure howmuch simpliﬁed texts can help a human reader an-swer questions more efﬁciently.
the closest to ourevaluation design is that of angrosh et al.
(2014)with the important difference that we require par-ticipants to resubmit after erroneous answers.
inpilot studies, we found this step to be crucial forhigh-quality responses..3 kis components.
in kis, we approach unsupervised simpliﬁcation asa (non-differentiable) reward maximization prob-lem.
as shown in figure 2, there are four compo-nents to the reward: simplicity, ﬂuency, salienceand guardrails which are jointly optimized.
thisis essential to avoid trivial solutions that only con-sider subsets.
we therefore use the product of allcomponents as the total reward, because the prod-uct is sensitive to the sharp decrease of a singlecomponent.
for example, the triggering of a singleguardrail leads to the zeroing of the total reward.
each component is normalized to the [0, 1] range..figure 2: keep it simple is an unsupervised trainingprocedure for text simpliﬁcation.
the text generator(gpt-2) produces candidate simpliﬁcations, scored ac-cording to ﬂuency, simplicity, salience.
guardrails en-force the model does not learn high-scoring shortcuts..def s_score(original,simple):.
fstart = fkgl(original)tgt = target_delta(fstart)fendd = fend-fstartreturn clip(1-((d-tgt)/tgt),0,1).
= fkgl(simple).
def target_delta(fstart):.
# line-fitted from analysisif fstart < 4.0:.
return 0.1.if fstart < 12:.
return 0.5*fstart-1.9.
return 0.8*fstart-5.6.
figure 3: sscore algorithm.
fkgl computes theflesch-kincaid grade level..3.1 simplicity.
the simplicity score should establish whether thegenerator’s output uses simpler language than theoriginal text.
we follow prior work (ferr´es et al.,2016) and organize our score into a syntactic scoresscore, and a lexical score lscore.
syntactic sim-pliﬁcation focuses on reducing the complexity of asentence, for example by reducing the number ofwords in a clause, or reducing distant dependencies.
in lexical simpliﬁcation, the objective is to replacecomplex phrases with simpler synonyms.
to pro-duce a single simplicity score, we take the productof sscore and lscore (both in [0, 1])..3.1.1 syntactic simplicity: sscorewe measure syntactic complexity via the flesch-kincaid grade level (fkgl) as it is easy to computeand maps to a grade-level which also correspondsto the scale used by newsela.
other readability met-rics such as dale-chall formula (dale and chall,1948), or the gunning-fog index (gunning, 1969)could be used, and future work could examine theeffect of choosing one readability metric over the.
6367     generator           salienceoriginaltext       simplicityscoreoptimization       fluency           guardrailssimplifiedtextbecause word frequency follows a zipf powerlaw, we use speer et al.
(2018)’s log normaliza-tion, adjusting the frequency on a [0, 8] range, withwords at 0 being non-existent in the corpus, and 8for most common words.
as an example, the wordvigorous has a frequency of 3.54, while its morecommon synonym strong obtains 5.23..we compute the average zipf frequency of theset of inserted words (z(w2 − w1)), and the setof deleted words (z(w1 − w2)).
the difference.
∆z(w1, w2) = z(w2 − w1) − z(w1 − w2).
(1).
should be positive.
analysis of the paired newselacorpus reveals that 91% of pairs have a positive∆z(w1, w2), with a median value of 0.4. we usethis median as the target zipf shift in the lscore,and use a ramp shape similar to the sscore, clippedbetween 0 and 1 (denoted as [·]+):.
lscore(w1, w2) =.
1 − |∆z(w1,w2)−0.4|.
0.4.
(2).
(cid:34).
(cid:35)+.
3.2 fluency.
we use two sub-components for the ﬂuency com-ponent: a pre-trained language-model, and a dis-criminator trained dynamically with the generator..3.2.1 language-model fluency.
language models assign a probability to a sequenceof words.
this probability is often used to measureﬂuency of generated text (kann et al., 2018; salazaret al., 2020).
the kis ﬂuency score is based ona language model in a way similar way to labanet al.
(2020).
the language model is used to ob-tain a likelihood of the original paragraph (lm (p))and of the generated output lm (q).
we use av-erage log-likelihood, for numerical stability.
thelanguage model ﬂuency score is then:.
lmscore(p, q) =.
1 −.
(cid:104).
lm (p) − lm (q)λ.
(cid:105)+.
(3).
λ is a tunable hyper-parameter.
if the lm (q) islower than lm (p) by λ or more, lmscore(p, q) =0. if lm (q) is above or equal to lm (p), thenlmscore(p, q) = 1, and otherwise, it is a linearinterpolation..we set λ = 1.3 as it is the value for whichthe paired newsela dataset achieves an averagelmscore of 0.9..figure 4: analysis (kernel density estimate plot)of change in flesch-kincaid grade level in thepaired newsela dataset.
most simple paragraphs havelower fkgl than the original paragraphs (positive∆f kgl).
when the original paragraph’s fkgl ishigher (x-axis), the change in fkgl tends to be larger(y-axis).
we ﬁt a linear approximation, which we useto compute the sscore..other.
another viable option is the lexile score(smith et al., 2016), however, because its imple-mentation is not publicly released, we cannot use itduring training and we report it only for evaluation(done manually on the lexile hub4)..figure 3 shows the sscore algorithm.
we com-pute the original paragraph’s fkgl (fstart),used to compute a target fkgl (tgt).
the scoreis a linear ramp measuring how close the achievedfkgl (fend) is to the target, clipped to [0, 1]..in the initial design, the target drop was a con-stant: 4 grade levels, independent of fstart.
however, analysis on the paired newsela corpusrevealed that the target fkgl should depend onthe initial fkgl.
this makes sense intuitively: analready syntactically simple paragraph should notrequire further simpliﬁcation, while more complexparagraphs require more simpliﬁcation.
figure 4shows the positive correlation between the originalparagraph’s fkgl and the drop of fkgl in thesimpliﬁed text.
we ﬁt a piece-wise linear functionto calculate the target fkgl drop from the initialparagraph..3.1.2 lexical simplicity: lscorelexical simplicity focuses on whether words inthe input paragraph (w1) are more complex thanones in the output paragraph (w2).
we rely on theobservation that word frequency and difﬁculty arecorrelated (breland, 1996), and use word frequencyin a large corpus of text (brysbaert and new, 2009)to determine simplicity..4https://hub.lexile.com.
63685.07.510.012.515.017.5fkgl of original paragraph50510fkgl in newsela rewritelinear approximation3.2.2 discriminator fluencythe lmscore is static and deterministic, which canbe limiting, as the generator can learn during train-ing how to adapt and exploit ﬂaws in the language-model (e.g., learning to alter capitalization)..inspired from the generative adversarial net-work (gan) framework (goodfellow et al., 2014),we create a dynamic discriminator, trained in con-junction with the generator, dynamically adaptingthe ﬂuency score during training..speciﬁcally, we use a roberta model (liuet al., 2019) as the basis for the discriminator, a clas-siﬁer with two labels: 1 for authentic paragraphs,and 0 for generator outputs..as the generator produces outputs, they are as-signed a label of 0 and added to a training buffer,while the original paragraphs are assigned a labelof 1 and added to the training buffer as well..once the training buffer reaches a size of 2,000samples, the discriminator is trained, using 90% ofthe training buffer.
we train the discriminator for5 epochs (details of training are in appendix a.1).
at the end of each epoch, we checkpoint the dis-criminator model.
we compare the 5 checkpointsin terms of f-1 performance on the remaining 10%of the training buffer, and keep the best checkpointas the new discriminator..the discriminator’s probability that a paragraph.
(q) is authentic is the discriminator score:.
dscore(q) = pdisc(y = 1|x = q).
(4).
as with gans, there is an equilibrium betweenthe generator attempting to maximize the proba-bility of generating real outputs (“fooling” the dis-criminator), and the discriminator succeeding atdistinguishing generated and authentic texts..3.3 salience.
for the salience component, we use the coveragemodel introduced in the summary loop (labanet al., 2020) for the domain of text summarization,and adapt it to the simpliﬁcation domain..the coverage model is a transformer-basedmodel trained to look at generated text and answerﬁll-in-the-blank questions about the original text.
the score is based on model accuracy at ﬁlling inthe blanks: the more is ﬁlled in, the more relevantthe generated content is, and the higher the score.
a key element of the coverage model is its mask-ing procedure, which decides which words to mask.
in the summary loop, a limited number of extracted.
keywords (up to 15 words) are masked.
by contrast,for simpliﬁcation, we mask all non-stop words,amounting to a masking rate of about 40%..this change reﬂects a difference in expectationbetween summarization and simpliﬁcation: in sum-marization, only key components are expected tobe recovered from a summary, whereas in simpli-ﬁcation most of the original paragraph should berecoverable.
coverage ranges in [0, 1], and refer-ence simpliﬁcations in the paired newsela corpusobtain an average score of 0.76, conﬁrming thatmanual simpliﬁcation can achieve high coverage..3.4 guardrails.
we use guardrails as simple pattern-based scores toavoid common pathological generation problemsthat we observed.
unlike the main components,guardrails are binary, giving a score of 1 (pass) un-less they trigger (score of 0).
we use two guardrails:brevity and inaccuracy..3.4.1 brevity guardrail.
the brevity guardrail ensures the length of gen-erated paragraph (l2) falls in a range around theoriginal paragraph’s length (l1).
we compute acompression ratio: c = l2/l1.
if cmin ≤ c ≤cmax, the guardrail passes, otherwise it triggers..we set [cmin, cmax] = [0.6, 1.5], because thesevalues ensure the guardrail is not triggered on 98%of the paired newsela dataset; this can be adapteddepending on the application..3.4.2.inaccuracy guardrail.
modern text generation models are known to hallu-cinate facts (huang et al., 2020), which has led thecommunity to create models to detect and correcthallucinations (cao et al., 2020; zhang et al., 2020;wang et al., 2020)..we propose a light-weight inaccuracy detectoras a guardrail.
we use a named entity recognition(ner) model (honnibal et al., 2020) to extractentities present in the original paragraph (e1) andthe model’s output (e2).
we trigger the guardrailif an entity present in e2 is not in e1..even though human writers can successfully in-troduce new entities without creating inaccuracies(e.g., replacing the city la paz with the country bo-livia), we ﬁnd that text generators predominantlyintroduce inaccuracies with novel entities.
thissimple heuristic can eventually be replaced onceinaccuracy detection technology matures..6369by this sampled population: ¯rs = (rs1 + ... +rsk)/k, which we use as the baseline, instead ofˆr.
the loss l becomes:n(cid:88).
k(cid:88).
log p(wsji.
|wsj.
1 ...wsj.
i−1, p ).
( ¯rs−rsj).
l =.
j=1.
i=0.
(6)we use a gpt2-medium for the generator, ini-tialized with the released pre-trained checkpoint.
experimental details such as data and optimizerused are provided in appendix a.1..in figure 5, we show results of a direct compar-ison of scst (k = 2) with k-scst varying k in{4, 6, 8}, while keeping other components of thetraining ﬁxed.
because of the variance involved inrl training, we recorded six independent trainingruns for each setting (for a total of 24 runs), andplot the average reward across runs of a setting, aswell as the standard error of the mean (sem)..we observe that increasing k leads to higheraverage reward, and less variation in the reward.
in our setting, k-scst boosts performance andstabilizes training.
we use k = 8 in all ﬁnal models,as increasing k further is impractical due to gpumemory limitations..we believe k-scst’s advantage stems from twofactors: ﬁrst, obtaining a better estimate of thedistribution of rewards by sampling more outputs,second, by using the mean reward as the baseline,saving on computation of a separate baseline gener-ation.
we believe k-scst can also improve learn-ing in other text generation applications and planto pursue this in future work..5 experiments.
we present results experimentally validating thekis procedure for text simpliﬁcation.
we give re-sults based on automatic metrics, on a novel humancomprehension task, and from an ablation study..5.1 models compared.
we compare the kis model to three strong super-vised models, and an unsupervised approach..access from (martin et al., 2020), is a state-of-the-art transformer model trained on wikilarge(300,000 pairs of complex/simple sentences).
thismodel uses default parameters (nbchar =0.95,levsim=0.75)..access90 is identical to access, with dif-ferent parameters (nbchar =0.90, levsim=0.75),reducing target compression from 95% to 90%,matching the average compression rate in newsela..figure 5: training kis models comparing scstwith k-scst.
we try 4, 6 and 8 as values for k. in-creasing k improves performance and stability..4 kis training.
rennie et al.
(2017) introduced self-critical se-quence training (scst) as an effective algorithmfor reward-based training of text generators, suc-cessfully applying it to image captioning.
the efﬁ-cacy of scst was later conﬁrmed on other text gen-eration tasks such as question generation (zhangand bansal, 2019), and summarization (celikyil-maz et al., 2018; laban et al., 2020).
in scst, aprobabilistic model is used to generate two distinctcandidates: cs, a candidate constructed by sam-pling the word distribution at each step, and ˆc, bytaking the argmax of the word distribution at eachstep.
each candidate is scored, obtaining rewardsof rs and ˆr, respectively, and the loss is:.
l = ( ˆr − rs).
log p(ws.
i |ws.
1 ...ws.
i−1, p ) (5).
n(cid:88).
i=0.
where p(wsi |...) represents the probability of thei-th word conditioned on previously generated sam-pled sequence according to the model, p is the inputparagraph, and n the number of words in the gen-erated sequence.
intuitively, minimizing this lossincreases the likelihood of the sampled sequence ifrs > ˆr, and decreases it otherwise, both increas-ing the expected total reward..one limitation in scst occurs when the twosequences achieve comparable rewards (rs (cid:39) ˆr):the loss nears zero, and the model has little to learn,wasting a training sample.
in our experiments withscst, this can occur with 30% of samples..we propose an extension of scst, which wecall k-scst.
we generate k sampled candidates(k > 2), compute the rewards of each candidaters1, ..., rsk, as well as the mean reward achieved.
637024681012hours of training105104103102total score8-scst6-scst4-scstscstmodel.
sari bleu %fkgl %lexile comp.
cov..newselafinetune baselineaccess defaultaccess 90unsup ntskis model.
-.470.666.674.677.709.
-.719.649.644.535.526.
8768869348100.
795263645772.
.918.903.958.921.753.852.
.754.894.805.789.618.640.table 1: automatic results on newsela test-set.
sariand bleu are reference-based metrics.
%fkgl and%lexile are percentages of model outputs lowering thegrade level.
comp.
is the average compression ratio (#words), and cov.
the output’s average coverage score..finetune baseline is a gpt2-medium modelﬁnetuned on the paired newsela dataset.
largepre-trained models often perform competitively inlow-resource environments, making this a strongpoint of comparison..unsup nts from (surya et al., 2019) is an unsu-pervised approach based on successively encodingand denoising text using a gru architecture..training details for the kis model and finetune.
baseline are in appendix a.1..5.2 automatic results.
we put aside 500 samples from the paired newseladataset as a test set to compare models on auto-matic metrics.
we compare models on sari andbleu, report the percentage when readability mea-sures see an improvement in readability: %fkgl,and %lexile and compute the average compres-sion rate (comp.
), and coverage (cov.).
results aresummarized in table 1..the kis model achieves the highest sari scoreby a margin of 0.04, even though it is an unsuper-vised approach..finetune baseline achieves the highest bleuand salience scores, but lowest sari score.
weinterpret this as showing the model takes the leastrisk: high salience, with little simpliﬁcation..we observe that all models are able to increasereadability in terms of fkgl and lexile comparedto original paragraphs.
we note that for almost allmodels, the percentage is lower for the lexile mea-sure than for fkgl, showing that an improvementin lexile score is more difﬁcult to achieve thanfkgl.
the kis model achieves an increase in lex-ile readability 72% of the time, the closest ﬁgureto 79% of the newsela human-written reference..we note that the perfect performance of kis on%fkgl could be explained by the fact that fkglis a part of a component being optimized (sscore),however lexile was not..in terms of compression, the kis model com-presses the second most, most likely hurting itscoverage.
adjusting the brevity guardrail couldencourage the model to compress less.
access90has the compression rate closest to newsela refer-ences, but this only leads to a modest improvementin sari when compared to access..overall, the newsela references achieve thebest percentage of lexile readability improvement,while outperforming the kis model at coverage:there is still a gap between human-written simpliﬁ-cations and model-generated ones..5.3 human comprehension study.
we propose a human comprehension study to evalu-ate the usefulness of simpliﬁcation results.
simpli-ﬁed text should be easier to read than the originaltext, while retaining accuracy and understanding.
we design a task to evaluate how well both manualand automated simpliﬁcations achieve this objec-tive.
the main idea is to show readers a text andask them to answer multiple-choice questions, eval-uating the texts based on time and retries needed toselect the correct answer..5.3.1 study design.
five different versions of each document weregenerated as stimuli: the original document, thenewsela reference, and versions from the threebest-performing methods from the last section:kis, finetune baseline, and access.
we did notinclude unsup nts in our analysis, because of itslow performance on %fkgl and %lexile metrics.
associated with each document are ﬁve manuallygenerated multiple-choice questions, each with oneor more correct answers and one to four distractors.
the original and the newsela texts were checkedmanually by experimenters to ensure that all allowfor questions to be answered correctly.
crowd-workers were shown four documents in succession,in a between-participants design.
order of docu-ment and stimuli type were randomized.
figure 6shows two stimuli of a document (original and kis)along with the comprehension questions.
(the en-tire set of ﬁve stimuli can be found in figure a2 inthe appendix.).
after several rounds of pilot testing, we arrived.
at the following design choices:.
document theme.
we chose recent news arti-cles involving complex themes (e.g., trajectory oficeberg) as the source of documents.
for news ar-ticles, recency seems to engage participants, and.
6371figure 6: example task (from a washington post article (kelati, 2020)) for the comprehension study.
shownare two of ﬁve stimuli: original document (left), and kis model output (right).
participants read a text and answeredcomprehension questions (bottom).
average completion time was 160 seconds (original) and 136 seconds (kismodel output)..technical terms increase the impact of simpliﬁca-tion..section length.
we chose document length of3-4 paragraphs (or 200 words), and ﬁve compre-hension questions.
document length should notbe too short (makes some questions trivial), or toolong (adds a retrieval component to the task)..selection of questions.
questions were gener-ated via a gpt2 question generation model ﬁne-tuned on the newsqa dataset (trischler et al.,2017).
we select questions answerable by boththe original and newsela references, attempting tohave both factoid (answer is entity) and reasoningquestions..re-submission until correct.
when submittinganswers, participants received feedback on whichwere incorrect, and were required to re-submit un-til all answers were correct.
this aligns the ob-jective of the participant (i.e., ﬁnishing the taskrapidly), with the task’s objective (i.e., measuringparticipant’s efﬁciency at understanding).
this alsogives a way to discourage participants from “brute-forcing” the task, re-submitting many combinationsuntil one works..we note that some components of the study suchas the choice of document themes and the selectionof comprehension questions are elements that cre-ate variability in the results.
we release the modelsused in the study, as well all generated texts thatwere evaluated to enable follow-up research and toaid reproducibility..model.
time (sec).
# subs.
comp.
caspeed.
(cid:91) original(cid:92) newsela∀ access∃ finetune baseline∇ kis model.
174.0163.3188.5161.0 ∀142.6 (cid:91) (cid:92) ∀ 4.10 ∀.
4.235.106.694.70.
1.01.080.960.970.87.
1.001.150.881.041.06.table 2: results of the human comprehensionstudy.
we measure average completion time (time),number of submissions (#subs.
), compression ra-tio (comp.)
and a compression-accounted speed-up(caspeed).
each text version is assigned a symbolused to indicate statistical signiﬁcance (p < 0.05)..5.3.2 study results.
we ran the study on mechanical turk, acceptingcrowd-workers with 1700+ completed tasks, andan acceptance rate of 97%+.
the study was activefor two weeks in december 2020, and remuneratedparticipants completing all four sections at a rate of$10/hour.
(appendix a.2 shows crowd-worker in-structions and the document/version distributions.)
when removing “brute-forced” submissions (10+re-submissions), we are left with 244 submissions,used for result analysis reported in table 2, (a moredetailed results table is included in appendix a.4.)
we measure two outcomes: question comple-tion time (in seconds), and number of submissionsto correctness.
we performed a kruskal-wallistest (kruskal and wallis, 1952) with a dunn post-hoc test (dunn, 1964) for statistical signiﬁcancebetween pairs of conditions..in line with study objectives, simpliﬁed texts.
6372original [lexile grade 11] each summer, libraries in st. louis,missouri, host many types of free camps — yoga, chess and even aharry potter “sorting hat camp.” in 2020, camp dreams seemed far-fetched given the global coronavirus pandemic.
that didn’t stop st.louis libraries, though.instead of canceling, they brought camp into kids’ homes.
so childrenwho signed up for ukulele camp got a beginner’s guidebook,instructional dvd and an actual ukulele in the mail.
it was all free.
inaddition, camp sessions still occurred.
advisers met with kids usingvirtual formats.joe monahan, manager of youth services for the st. louis librarysystem, says that of the 70 camps originally scheduled, 54 were heldvirtually.paula langsam, a youth services manager at the soon-to-reopenmartin luther king junior memorial library in washington, d.c., says,“in a way, our work has changed a lot.
we didn’t used to do videos alot.”kis model [lexile grade 9] in the summer months, st. louishas many free classes for kids, including yoga, chess and a harrypotter “sorting hat camp.” in 2020, camp dreams again seemedfar-fetched given the crisis.
that didn’t stop st. louis libraries,though.they brought camp in.
so kids who signed up for ukulele camp gota beginner’s guidebook, a lesson dvd and a real ukulele in themailbox.
it was all free.
in addition, camp sessions continued.advisers tried out a virtual format.joe monahan, the manager of youth services for the st. louislibrary system, says that of the 70 camps originally scheduled, 54were held mostly.paula langsam, a youth services manager at the martin lutherking junior library, says, “in a way, our work changed a lot.
wedidn’t do videos a lot.”who manages the st louis library kids programs?joe monahan, paula langsam, st. louis camp leaderswere any camps in st. louis cancelled?yes, nohow many camps were scheduled, how many were run?54 and 70, 70 and 54, 70 and 0, 54 and 0how did the ukulele camp meet?in the park, virtually, did not meetwhat camps did the libraries host?yoga, chess, pottery, ukulelehelp participants complete the task faster than read-ing original texts, with three of the four simpliﬁedversions leading to improvements in completiontimes.
participants were fastest with kis simpli-ﬁcations (18% faster).
the kis model led to astatistically signiﬁcant speed-up compared to theoriginals, newsela references, and access sim-pliﬁcations.
access simpliﬁcations surprisinglyled to a non-signiﬁcant slow-down, which we at-tribute to a potential loss in ﬂuency that might haveconfused participants..one important factor we consider is that shorterpassages (i.e., smaller compression) might lead to aspeed-up regardless of simplicity.
we conﬁrm thisby ﬁnding a small positive correlation between pas-sage length and completion time of 0.09. we com-pute a compression-adjusted speed-up (caspeed)ratio by: (1) computing the passage length of eachsimpliﬁed version, (2) linearly extrapolating the ex-pected completion time for this passage length fororiginal paragraphs, and (3) computing the ratio ofthe extrapolation to the observed completion time.
if caspeed > 1, participants were faster than ex-pected for the passage length.
newsela referenceparagraphs achieve the best caspeed, followed bythe kis model.
this suggests that good simpliﬁca-tion can involve making texts longer..5.4 ablation study.
we train three ablated models, each missing a re-ward component to gain understanding in the valueof each component of the kis procedure..figure 1 gives a qualitative perspective on eachablation.
without ﬂuency, the generator learns togenerate incomplete sentences, without salience, itomits important information, and without simplic-ity, it can sometimes “complexify”..we computed complete automatic results for theablated models, and ﬁnd that each ablation leads toa decrease on an evaluation metric, conﬁrming thatall three components are necessary to generate high-quality simpliﬁcations (details in appendix a.5)..inclusion of supervised signal.
in this work,we establish that text simpliﬁcation can be ap-proached in an unsupervised manner.
in futurework, keep it simple could be used as a pre-training strategy, or used jointly with supervisedtraining..reproducibility of human evaluation.
eventhough we release the models, stimuli and compre-hension questions used in the human evaluation,some elements of the procedure introduce random-ness.
participating crowd-workers differ in literacylevel which may have an effect on their perfor-mance at the task (alonzo et al., 2021)..new settings, domains and languages.
welimited our experiments to the simpliﬁcation of en-glish news articles following prior work, but planto pursue other languages in the future.
similarly,because keep it simple does not require labeleddata, it can be applied to new settings (e.g., rewrit-ing to inverse the effects of simpliﬁcation), or tonew domains (e.g., legal texts)..7 conclusion.
we have shown that text simpliﬁcation can be ap-proached in an unsupervised manner via kis.
byoptimizing a reward comprised of simplicity, ﬂu-ency and salience components, kis is able to out-perform strong supervised models on automaticmetrics (+0.04 in sari).
we propose a humancomprehension task to evaluate the usefulness ofsimpliﬁcation and show that simpliﬁcations tend tolead to a measurable speed-up in task completion,with kis texts producing the best speed-up of 18%on average.
these are ﬁrst steps for unsupervisedtext simpliﬁcation, and we suggest that future workshould focus on adapting the methodology to newdomains (i.e., legal), non-english languages, andreﬁning optimized rewards to take factuality intoaccount..6 limitations and future work.
acknowledgments.
improved accuracy scoring.
the currentguardrail for inaccuracy is rudimentary; trainedmodels still generate non-factual simpliﬁcations.
recent work in fact-checking for the summariza-tion domain (kryscinski et al., 2020; li et al., 2018)could be adapted to the simpliﬁcation domain toimprove this..we would like to thank katie stasaski, dongyeopkang, and the acl reviewers for their helpful com-ments, as well as newsela for providing a versionof their simpliﬁed news corpus.
this work wassupported by a microsoft bair commons grant aswell as a microsoft azure sponsorship..6373references.
oliver alonzo, jessica trussell, becca dingman, andmatt huenerfauth.
2021. comparison of methodsfor evaluating complexity of simpliﬁed texts amongdeaf and hard-of-hearing adults at different literacylevels.
in proceedings of the 2021 chi conferenceon human factors in computing systems, pages 1–12..mandya angrosh, tadashi nomoto, and advaith sid-dharthan.
2014. lexico-syntactic text simpliﬁcationand compression with typed dependencies.
in pro-ceedings of coling 2014, the 25th internationalconference on computational linguistics: tech-nical papers, pages 1996–2006, dublin, ireland.
dublin city university and association for compu-tational linguistics..h. breland.
1996. word frequency and word difﬁculty:a comparison of counts in four corpora.
psycholog-ical science, 7:96 – 99..m. brysbaert and b. new.
2009. moving beyondkucera and francis: a critical evaluation of currentword frequency norms and the introduction of a newand improved word frequency measure for americanenglish.
behavior research methods, 41:977–990..meng cao, yue dong, jiapeng wu, and jackie chi kitcheung.
2020. factual error correction for abstrac-in proceedings of thetive summarization models.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 6251–6258..asli celikyilmaz, antoine bosselut, xiaodong he, andyejin choi.
2018. deep communicating agents forin proceedings of theabstractive summarization.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 1662–1675..edgar dale and jeanne s chall.
1948. a formula forpredicting readability: instructions.
educational re-search bulletin, pages 37–54..yue dong, zichao li, mehdi rezagholizadeh, andjackie chi kit cheung.
2019. editnts: an neuralprogrammer-interpreter model for sentence simpliﬁ-in proceedings ofcation through explicit editing.
the 57th annual meeting of the association for com-putational linguistics, pages 3393–3402..olive jean dunn.
1964. multiple comparisons using.
rank sums.
technometrics, 6(3):241–252..daniel ferr´es, montserrat marimon, horacio saggion,et al.
2016. yats: yet another text simpliﬁer.
ininternational conference on applications of naturallanguage to information systems, pages 335–342.
springer..ian goodfellow, jean pouget-abadie, mehdi mirza,bing xu, david warde-farley, sherjil ozair, aaron.
courville, and yoshua bengio.
2014. generative ad-in advances in neural informationversarial nets.
processing systems, volume 27..robert gunning.
1969. the fog index after twentyyears.
journal of business communication, 6(2):3–13..han guo, ramakanth pasunuru, and mohit bansal.
2018. dynamic multi-level multi-task learning forsentence simpliﬁcation.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 462–476..matthew honnibal,.
ines montani, soﬁe van lan-spacy:and adriane boyd.
2020.deghem,industrial-strength natural language processing inpython.
doi.org/10.5281/zenodo.1212303..dandan huang, leyang cui, sen yang, guangshengbao, kun wang, jun xie, and yue zhang.
2020.what have we achieved on text summarization?
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 446–469..katharina kann, sascha rothe, and katja filippova.
sentence-level ﬂuency evaluation: refer-2018.in proceedings ofences help, but can be spared!
the 22nd conference on computational natural lan-guage learning, pages 313–323..haben kelati.
2020. librarians ﬁnd creative ways toserve kids when buildings are closed for browsing.
the washington post..j. peter kincaid, robert p. fishburne jr., richard l.rogers, and brad s. chissom.
1975. derivation ofnew readability formulas (automated readability in-dex, fog count and ﬂesch reading ease formula) fornavy enlisted personnel.
technical report, navaltechnical training command millington tn re-search branch..william h kruskal and w allen wallis.
1952. use ofranks in one-criterion variance analysis.
journal ofthe american statistical association, 47(260):583–621..wojciech kryscinski, bryan mccann, caiming xiong,and richard socher.
2020. evaluating the factualconsistency of abstractive text summarization.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9332–9346..philippe laban, andrew hsi, john canny, and marti a.hearst.
2020. the summary loop: learning to writein pro-abstractive summaries without examples.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5135–5150. association for computational linguistics..sophie lewis.
2021. nasa curiosity rover celebrates3,000th day on mars with stunning panorama ofplanet.
cbs news..6374haoran li, junnan zhu, jiajun zhang, and chengqingzong.
2018. ensure the correctness of the summary:incorporate entailment knowledge into abstractivesentence summarization.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 1430–1441..adam trischler, tong wang, xingdi yuan, justin har-ris, alessandro sordoni, philip bachman, and ka-heer suleman.
2017. newsqa: a machine compre-in proceedings of the 2nd work-hension dataset.
shop on representation learning for nlp, pages191–200..y. liu, myle ott, naman goyal, jingfei du, mandarjoshi, danqi chen, omer levy, m. lewis, lukezettlemoyer, and veselin stoyanov.
2019. roberta:a robustly optimized bert pretraining approach.
arxiv, abs/1907.11692..alex wang, kyunghyun cho, and mike lewis.
2020.asking and answering questions to evaluate the fac-in proceedings oftual consistency of summaries.
the 58th annual meeting of the association for com-putational linguistics, pages 5008–5020..louis martin, ´eric villemonte de la clergerie, benoˆıtsagot, and antoine bordes.
2020. controllable sen-in proceedings of the 12thtence simpliﬁcation.
language resources and evaluation conference,pages 4689–4698..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..jipeng qiang, yun li, yi zhu, yunhao yuan, and xin-dong wu.
2020. lexical simpliﬁcation with pre-trained encoders.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 34, pages8649–8656..steven j rennie, etienne marcheret, youssef mroueh,jerret ross, and vaibhava goel.
2017. self-criticalsequence training for image captioning.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 7008–7024..julian salazar, davis liang, toan q nguyen, and ka-trin kirchhoff.
2020. masked language model scor-in proceedings of the 58th annual meetinging.
of the association for computational linguistics,pages 2699–2712..malbert smith, j. turner, eleanor e. sanford-moore,and heather h. koons.
2016. the lexile frameworkfor reading: an introduction to what it is and how touse it..robyn speer, joshua chin, andrew lin, sara jewett,and lance nathan.
2018. luminosoinsight / word-freq: v2.2.
doi.org/10.5281/zenodo.1443582..felix stahlberg and shankar kumar.
2020. seq2edits:sequence transduction using span-level edit opera-in proceedings of the 2020 conference ontions.
empirical methods in natural language processing(emnlp), pages 5147–5159..sai surya, abhijit mishra, anirban laha, parag jain,and karthik sankaranarayanan.
2019. unsupervisedin proceedings of theneural text simpliﬁcation.
57th annual meeting of the association for compu-tational linguistics, pages 2058–2068..s rebecca thomas and sven anderson.
2012.wordnet-based lexical simpliﬁcation of a document.
in konvens, pages 80–88..sander wubben, antal van den bosch, and emiel krah-mer.
2012. sentence simpliﬁcation by monolingualmachine translation.
in proceedings of the 50th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1015–1024..w. xu, chris callison-burch, and courtney napoles.
2015. problems in current text simpliﬁcation re-search: new data can help.
transactions of the asso-ciation for computational linguistics, 3:283–297..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics, 4:401–415..shiyue zhang and mohit bansal.
2019. address-ing semantic drift in question generation for semi-in proceedings ofsupervised question answering.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp)..xingxing zhang and mirella lapata.
2017. sentencesimpliﬁcation with deep reinforcement learning.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages584–594..yuhao zhang, derek merck, emily tsai, christopher dmanning, and curtis langlotz.
2020. optimizingthe factual correctness of a summary: a study ofin proceedings ofsummarizing radiology reports.
the 58th annual meeting of the association for com-putational linguistics, pages 5108–5120..yang zhong, chao jiang, wei xu, and junyi jessy li.
2020. discourse level factors for sentence deletionin proceedings of the aaaiin text simpliﬁcation.
conference on artiﬁcial intelligence, volume 34,pages 9709–9716..zhemin zhu, delphine bernhard, and iryna gurevych.
2010. a monolingual tree-based translation modelin proceedings of thefor sentence simpliﬁcation.
23rd international conference on computationallinguistics (coling 2010), pages 1353–1361..6375ethical considerations.
we present a method for text simpliﬁcation and ver-ify its performance on text from the news domainin the english language.
even though we expectthe method to be adaptable to other domains andlanguages, we have not veriﬁed this assumptionexperimentally and limit our claims to the englishnews domain..when comparing to prior work (e.g., accessmodel), we obtained implementations directly fromthe authors (through github repositories) and pro-duced results following the recommended setting,with an objective to present prior work as a strongcomparison point..for the human evaluation, we paid the annota-tors above the minimum wage, and did not collectany personal identiﬁable information.
we selectedtopics to avoid sensitive or political subjects andhad our protocols reviewed by the university’s irbcommittee (protocol id: 2018-07-11230).
we re-lied on a third party (amazon mechanical turk) toremunerate the crowd-workers..a appendices.
a.1 training details.
we detail the model architecture size, data, opti-mizer of the models we train in the paper.
allmodels were trained using pytorch and hugging-face’s transformers library5.
we use the apex6library to enable half-precision training..the kis procedure was trained on a singlegpu, either an nvidia v-100 (16gb memory) ora quadro rtx 8000 (48 gb memory).
we ran atotal of around 200 experiments, with an averagerun-time of one week..because the procedure is unsupervised,.
themodel was trained using a large unreleased cor-pus of news articles, containing 7 million newsarticles in english..kis model is initialized with a gpt2-mediummodel.
we used the adam optimizer, with a learn-ing rate of 10−6, a batch-size of 1, using k-scstwith k = 8..finetune baseline is initialized with a gpt2-medium model.
we train using using standardteacher forcing on the 40,000 samples in the pairednewsela dataset, reserving 2,000 samples for val-idation.
we use the adam optimizer, and use the.
5https://github.com/huggingface/transformers6https://github.com/nvidia/apex.
validation set to choose a learning rate of 10−5,and a batch-size of 8, and run for 3 epochs beforeseeing a plateau in the validation loss..discriminator model.
is initialized with aroberta-base, and retrained every time the train-ing buffer reaches 2,000 samples.
the discrim-inator is reset to the original roberta-base eachtime the training buffer is full.
we use a standardcross-entropy loss, the adam optimizer with alearning rate of 10−5 and a batch size of 8. eachtime we retrain, we run for 5 epochs, and check-point one model after each epoch.
the checkpointthat achieves the highest performance on a valida-tion set becomes the new discriminator for the nextround..a.2 human evaluation instructions.
figure a1 shows the instructions given to crowd-worker participants for the manual evaluation..• the entire hit should take no more than 15minutes:(1) you will answer a pre-questionnaire.
(2) read 4 short news stories and answercomprehension questions about each.
• if you believe the answer is not in thedocument, you can select the option “answernot in document”.
• there is no time limit for each individualdocument or question.
• you can leave at any point but will notcomplete the hit.
• you can complete this task at most once.
• if you have a question/problem, contact usat email..instructions given to participants of thefigure a1:comprehension evaluation.
participants were recruitedon amazon mechanical turk (mturk), on which jobsare named “hit”..a.3 full example of generated texts.
figure a2 is a complement to figure 6, with theﬁve stimuli that were shown for the covid librariesdocument..a.4 detailed of human evaluation results.
table a1 details the timing and number of par-ticipants for each combination of document andstimuli..6376figure a2: complement to figure 6. example task for the comprehension study.
participants were assignedto one of ﬁve settings: original, newsela, kis, finetune baseline, and access.
participants were instructed toanswer the ﬁve comprehension questions..simpliﬁcation model.
document idmarvel showcovid librariessustainable foodiceberg collisionversion aggregate.
original newsela sup.
base.
access kis140 (11)152 (12)182 (10)167 (14)181 (13)163 (13)139 (12)208 (14)161 (46)174 (53).
209 (14)190 (13)242 (13)104 (12)188 (52).
209 (11)180 (12)144 (10)116 (11)163 (44).
126 (13)171 (12)154 (12)119 (12)143 (49).
table a1: average time taken and number of participants in each of the document/stimuli combinations.
also shown are aggregates (mean time taken and total number of participants)..6377original [lexile grade 11] each summer, libraries in st. louis, missouri, host many types of free camps — yoga, chess and even aharry potter “sorting hat camp.” in 2020, camp dreams seemed far-fetched given the global coronavirus pandemic.
that didn’t stopst.
louis libraries, though.instead of canceling, they brought camp into kids’ homes.
so children who signed up for ukulele camp got a beginner’s guidebook,instructional dvd and an actual ukulele in the mail.
it was all free.
in addition, camp sessions still occurred.
advisers met with kidsusing virtual formats.joe monahan, manager of youth services for the st. louis library system, says that of the 70 camps originally scheduled, 54 were heldvirtually.paula langsam, a youth services manager at the soon-to-reopen martin luther king junior memorial library in washington, d.c.,says, “in a way, our work has changed a lot.
we didn’t used to do videos a lot.”who manages the st louis library kids programs?joe monahan, paula langsam, st. louis camp leaderswere any camps in st. louis cancelled?yes, nohow many camps were scheduled, how many were run?54 and 70, 70 and 54, 70 and 0, 54 and 0how did the ukulele camp meet?in the park, virtually, did not meetwhat camps did the libraries host?yoga, chess, pottery, ukulelekis model [lexile grade 9] in the summer months, st. louis has many free classes for kids, including yoga, chess and a harrypotter “sorting hat camp.” in 2020, camp dreams again seemed far-fetched given the crisis.
that didn’t stop st. louis libraries, though.they brought camp in.
so kids who signed up for ukulele camp got a beginner’s guidebook, a lesson dvd and a real ukulele in themailbox.
it was all free.
in addition, camp sessions continued.
advisers tried out a virtual format.joe monahan, the manager of youth services for the st. louis library system, says that of the 70 camps originally scheduled, 54 wereheld mostly.paula langsam, a youth services manager at the martin luther king junior library, says, “in a way, our work changed a lot.
we didn’tdo videos a lot.”newsela [lexile grade 7] in st. louis, missouri, libraries hold many types of free camps in the summer.
they have yoga, chess andeven a harry potter camp.
in 2020, camp ideas seemed unlikely due to the spread of covid-19.
that did not stop st. louis libraries,though.
they did not cancel their programs.
they brought camp into kids' homes.children who signed up for ukulele camp got a beginner's book, instructional video and a ukulele in the mail.
it was all free.
in addition,camp sessions still went on.
camp leaders met with kids over video calls.joe monahan runs the kids programs for the st. louis libraries.
he said 70 camps were scheduled at first.
some were dropped but 54were still held online.paula langsam works at the martin luther king junior memorial library in washington, d.c. it reopened on september 24. she said,"our work has changed a lot.
we didn't used to do videos a lot.
"finetune baseline [lexile grade 9] each summer, st. louis libraries host many types of free camps — yoga, chess and even aharry potter "sorting hat camp."
in 2020, camp dreams seemed far-fetched given the global cough .
however , the future lookedbright for libraries in st. louis.instead of canceling, they brought camp into kids' homes.
this is called ukulele camp.
kids signed up for it in the mail.
they got abeginner's guidebook, video and an actual ukulele to play.
it was all free.
advisers met with kids using virtual formats.joe monahan, manager of youth services for the st. louis library system, says that 70 camps were originally planned.
of those, 54were held mostly.paula langsam is a youth services manager who works at the martin luther king junior memorial library in washington, d.c. shesays, "in a way, our work has changed a lot.
we didn't used to do videos a lot.
"access [lexile grade 11] each summer, libraries in st. louis, missouri, has many different types of free camps that are yoga, chessand even a harry potter gang sorting hat camp.
in 2020, camp dreams seemed far-fetched that there was the global coronaviruspandemic.
that did not stop st. louis libraries, though.instead of being canceled, they brought camp into children's homes.
so children who signed up for ukulele camp got a guidebook.they also had an actual ukulelele in the mail.
it was all free.
in addition, camp meetings still happened.
advisers met with new childrenusing virtual formats.joe monahan, also known as joe monahan, has youth services for the st. louis library system says that of the 70 camps first started,54 were held.paula langsam, also known as paula langsam, is a youth services manager at the soon-to-reopen martin luther king junior library inwashington, d. we did not use to do many videos a lot.
modelkis fullkis no fluencykis no saliencekis no simplicity.
sari bleu %fkgl %lexile comp.
cov.
0.6360.7090.9010.7180.7010.6950.8090.672.
0.5260.6110.5910.617.
0.851.021.010.92.
1009910051.
72956523.table a2: automatic results of the three ablation models.
sari and bleu are reference-based metrics.
%fkgl and % lexile are the percentage of simpliﬁed paragraphs with a lower fkgl and lexile score than theoriginal paragraph.
comp.
is the average compression ratio (# of words), and cov.
is the average coverage scoreof the simpliﬁcations..a.5 detail of ablation study results.
table a2 details the metric results of the three ab-lated models, an extension to table 1. an exampleoutput of each ablated model, illustrating the limi-tation when a score component is missing, is givenin figure 1..one surprising element is that the model trainedwithout ﬂuency achieves higher scores on almostall metrics, compared to the full model.
this sur-prising fact is due to the fact that without ﬂuency,the model does not learn to generate full sentences(see the example in figure 1).
instead, the modellearns to concatenate high-scoring phrases together,which can boost automatic metrics artiﬁcially.
infact, the strong performance of a model generatingincomplete sentences reveals a limitation of currentautomatic metrics, such as bleu and sari..6378