kace: generating knowledge-aware contrastive explanations fornatural language inferenceqianglong chen1,2∗, feng ji3†, xiangji zeng1, feng-lin li2,ji zhang2, haiqing chen2, yin zhang1‡1college of computer science and technology, zhejiang university, china2damo academy, alibaba group, china, 3tencent, china{chenqianglong,zengxiangji,zhangyin98}@zju.edu.cn{fenglin.lfl,zj122146,haiqing.chenhq}@alibaba-inc.com{neilji}@tencent.com.
abstract.
in order to better understand the reason be-hind model behaviors (i.e., making predic-tions), most recent work has exploited gener-ative models to provide complementary expla-nations.
however, existing approaches in nat-ural language processing (nlp) mainly focuson “why a” rather than contrastive “why anot b”, which is shown to be able to betterdistinguish confusing candidates and improvemodel performance in other research ﬁelds.
inthis paper, we focus on generating contrastiveexplanations with counterfactual examples innli and propose a novel knowledge-awaregeneration framework (kace).
speciﬁcally,we ﬁrst identify rationales (i.e., key phrases)from input sentences, and use them as keyperturbations for generating counterfactual ex-amples.
after obtaining qualiﬁed counterfac-tual examples, we take them along with orig-inal examples and external knowledge as in-put, and employ a knowledge-aware genera-tive pre-trained language model to generatecontrastive explanations.
experimental resultsshow that contrastive explanations are bene-ﬁcial to clarify the difference between pre-dicted answer and other answer options.
more-over, we train an bert-large based nli modelenhanced with contrastive explanations andachieve an accuracy of 91.9% on snli, gain-ing an improvement of 5.7% against etpa(“explain-then-predict-attention”) and 0.6%against nile (“why a”)..1.introduction.
in recent years, pre-trained language models (de-vlin et al., 2019; liu et al., 2019; yang et al., 2019)have been widely adopted in many tasks of natu-ral language processing (talmor et al., 2019; choiet al., 2018; bowman et al., 2015).
however, due to.
∗ work is done during internship at alibaba group.
† the work is mainly conducted while being at alibaba.
group..‡ corresponding author: yin zhang.
the lack of textual explanations, most downstreammodels become more complicated and difﬁcult tounderstand.
end users, especially those workingin critical domains such as healthcare or online ed-ucation, become more skeptical and reluctant toadopt or trust them, although these models havebeen proved to improve the decision-making per-formance.
therefore, providing faithful textualexplanations has become a promising way to over-come the black-box property of neural networks,which has attracted the attention of academia andindustrial communities..recently, the majority of existing methods (xuet al., 2020; cheng et al., 2020; karimi et al., 2020;ramamurthy et al., 2020; atanasova et al., 2020;kumar and talukdar, 2020) in natural languageprocessing try to explain the predictions of neu-ral models in a model-intrinsic or model-agnostic(also known as post-hoc) way.
while post-hocmodels (chen et al., 2020b; karimi et al., 2020;kumar and talukdar, 2020) provide explanationsafter making predictions without affecting the over-all accuracy, most of them neglect the rationales ininputs and provide textual explanations just in theform of “why a”.
however, we argue that con-trastive explanations in the form of “why a notb” could provide more informative and importantclues that are easier to understand and persuadeend-users.
moreover, we believe that contrastiveexplanations could beneﬁt downstream tasks (e.g.,nli), since such kind of explanations contain morehelpful information (e.g.
relations between ratio-nales) that can be used to improve model perfor-mance..to further enhance the explainability and per-formance of nli, we propose a novel textual con-trastive explanation generation framework in thispaper, which is post-hoc and considers rationales,counterfactual examples, and external knowledge.
speciﬁcally, we ﬁrst identify rationales (i.e., keyphrases) from a premise-hypothesis (p-h) pair with.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2516–2527august1–6,2021.©2021associationforcomputationallinguistics2516figure 1: the overall workﬂow of contrastive explanation generation, which contains rationale identiﬁcation, coun-terfactual example generation (as described in figure 2) and selection, and knowledge-aware contrastive explana-tion generation.
in our “why a not b” paradigm, we will generate explanations for a and each other class b(i.e., we will generate “why not neutral” and “why not entailment” in this example).
the counterfactualexample selection aims to select one most qualiﬁed for any other class b..label a, and then use them as the key perturbationsfor transforming and generating candidate counter-factual examples.
then we further select one mostqualiﬁed counterfactual example for any other labelb. note that the acquisition of a qualiﬁed counter-factual example of class b is essential to generatea meaningful explanation for “why not b”, oth-erwise the resultant contrastive explanation will begroundless or useless.
after that, we take the se-lected examples along with the original p-h pairand related external knowledge as input, and ﬁnallyemploy a knowledge-aware pre-trained languagemodel to generate contrastive explanation, whichwill specify why the prediction label is a ratherthan b, and clarify the confusions for end-users.
moreover, we train an nli model enhanced withcontrastive explanations and achieve the new state-of-art performance on snli..the contributions of this paper are as follows:.
• we introduce a novel knowledge-aware con-trastive explanation generation framework(kace) for natural language inference tasks..• we consider the rationales in inputs and re-gard them as important perturbations for gen-erating counterfactual examples rather thanjust discarding them like previous post-hocwork (hendricks et al., 2018; cheng et al.,2020)..• we integrate external knowledge with gener-ative pre-trained language model rather thanonly taking original inputs (kumar and taluk-.
dar, 2020; rajani et al., 2019) for contrastiveexplanation generation..• experimental results show that knowledge-aware contrastive explanations are able to clar-ify the difference between predicted class andthe others, which help to clarify the confu-sion of end-users and further improve modelperformance than “why a” explanations1..2 task deﬁnition and overall workﬂow.
here, we deﬁne the task of contrastive explana-tion generation for nli.
given a trained neuralnetwork model f with input x and predicted classa, the problem of generating contrastive explana-tions (ce) to an input x is to specify why x belongsto category/class a rather than b, deﬁned as:.
r = rationales(x, a)x(cid:48) = reversal(x, b, r)ce = generator(x(cid:48), x, a).
(1).
(2).
(3).
in equation 1, we ﬁrst identify a set of ratio-nales in given inputs, as described in section 3.1,and in equation 2 we generate counterfactual ex-amples with reversal mechanism as presented insection 3.2. in equation 3, we take the selectedcounterfactual example along with original exam-ple and external knowledge as input, and employ aknowledge-aware generator to produce contrastiveexplanation as detailed in section 3.3..1our code will be released as soon as possible at.
https://github.com/ai4nlp/kace.
2517rationaleidentificationcounterfactualexamplegenerationcounterfactualexampleselectioncontrastiveexplanationgeneration premise: a woman and a young child are makingsculptures out of clay.hypothesis: a man and woman painting on canvas.label: contradictionpremise: young child, making sculptures, clayhypothesis: man, painting, canvaswhy contradiction not neutralcontrastive explanation: why contradiction: making sculpturesout of clay is different from paiting oncanvas.
why not neutral:making sculpturesout of clay is a type of artconceptnet/dictionaryconceptsknowledge& rationaledefinitionretrievalpremise: a woman and a young child are makingsculptures out of clay.hypothesis:a child and a woman make something.label:neutralrefer to figure 2figure 2: counterfactual example generation for snli and imdb..3 approach.
3.1 rationale identiﬁcation.
considering that rationales are important featuresof an instance, it is essential to regard rationalesas key perturbations for counterfactual examplegeneration.
in this paper, we formulate rationaleidentiﬁcation as a token-level sequence labellingtask where 1 indicates a rationale token and 0 indi-cates a background token..being similar with (thorne et al., 2019), we ﬁrstconstruct the input sequence for a premise p anda hypothesis h as sp=(cid:104)s(cid:105) label (cid:104)s(cid:105) p remise (cid:104)s(cid:105)and sh=(cid:104)s(cid:105) hypothesis (cid:104)s(cid:105), where (cid:104)s(cid:105) is a spe-cial token that separates the components.
let yrepresent the relation between sp and sh wherey ∈ {entailment, contradiction, neutral}.
for eachinstance, we need to identify a subset r of zero ormore tokens as rationales from both premise andhypothesis sentences.
both premise and hypothe-sis are encoded with roberta (liu et al., 2019),yielding hidden representation hp=[· · · , hpj , · · · ]and hh=[· · · , hh.
i , · · · ] respectively.
as rationalizer is proposed by (zhao and vy-diswaran, 2021), we follow this work for ratio-nale identiﬁcation using cross attention to embedthe hypothesis (premise) into premise (hypothesis),which is deﬁned as:.
aij =.
exp((hhm=0 exp((hh.
i )t t anh(w t.1 hpj ))1 hpi )t t anh(w t.(cid:80)lp.
m)).
(4).
ˆhhi = [hh.
i , p ooling(hp),.
(cid:88).
aijhpj ].
k.(5).
where aij denotes the attention score of jth tokenin premise to the ith token in the hypothesis, lpdenotes the length of the premise sentence and w1is a trainable parameter matrix.
the representationof ith token in the hypothesis, denoted as ˆhhi , iscreated by concatenating its original state represen-tation, max-pooling representation over hp, and thecorresponding sum of attention representation fromhp.
at last, we use a softmax layer with a lineartransformation to model the probability of the ithtoken in sh being a rationale token..3.2 counterfactual example generation.
as we have introduced above, counterfactual ex-amples of other classes are of key importance togenerate contrastive explanations.
in this part, wedescribe how to generate counterfactual examples.
given a trained neural network model f , theproblem of generating counterfactual examplefor an instance x is to ﬁnd a set of examplesc1, c2, ..., ck that lead to a desired prediction y(cid:48).
the counterfactual examples are explainable andcontrastive when they appropriately consider prox-imity, diversity and validity..here, we deﬁne a three-part loss function to se-.
lect qualiﬁed counterfactual example:.
l = lvalid + λ1ldist + λ2ldiv.
(6).
2518long, fascinating, soulful.
never have ibeen so sad to see ending credits roll.negativepremisehypothesissnli labela woman and a young childare making sculptures out of clay.
a man and womanpainting on canvas.contradictiona child and womanmaking somethinga woman and a young childare making sculptures out of clay.
neutralunchangedreplacedchangedcounterfactual example generation  for snli and imdb long, boring, blasphemous.
never havei been so glad to see ending credits roll.postivechangedimdb labelreplaced tokenrationalefigure 3: details of the approach.
the framework consists of: a) rationale identiﬁcation, b) counterfactual examplegeneration and selection, c) knowledge-aware contrastive explanation generation.
given the original input p-hpairs and the annotated label a, an other label b, the approach identify the rationales, generate counterfactualexamples and produce contrastive explanation based on them..where λ1 and λ2 are hyperparameters for balanc-ing ldist and ldiv.
for generating counterfactualexample, the validity term, which ensures the gen-erated counterfactual examples have desired pre-diction target, is deﬁned as:.
lvalid =.
loss(f (ci), y(cid:48)).
(7).
k(cid:88).
i=1.
meanwhile, the generated examples should beproximal to the original instance as described in(cheng et al., 2020), which means only a smallchange needs to be made.
we do not expect a bigchange that transforms a large portion of the origi-nal, in which way there will be no difference withmerely presenting an example of counter classesand the corresponding explanation will be uninfor-mative or useless.
that is, we expect that resultantexamples are able to preserve the main content ofinput while changing domain-related parts..ldist =.
dist(ci, x).
(8).
k(cid:88).
i=1.
in this paper, we choose a weighted heterogeneousmanhattan-overlay metric (wilson and martinez,1997) to calculate the distance as follows:.
dist(c, x) =.
dt(ct, xt).
(9).
(cid:88).
t.where t indicates a rationale..to achieve diversity, we want generated exam-ples to be different from each other.
speciﬁcally,.
we calculate the pairwise distance of a set of coun-terfactual examples and minimize:.
ldiv = −.
dist(ci, cj).
(10).
1k.k(cid:88).
k(cid:88).
i=1.
j=i.
after deﬁning the loss function, we use a reversalmechanism to produce counterfactual examples.
in the reversal mechanism, we use hypernym andhyponym of tokens in wordnet2 for perturbation.
for example, as shown in figure 2, the orig-inal premise and hypothesis are “a woman anda young child are making sculptures out of clay”and “a man and a woman painting on canvas”, andthe label is “contradiction”.
we ﬁnd from word-net the hypernyms of “making sculptures out ofclay” and “painting on canvas” as “doing art” and“making something” respectively.
we replace themwith their hypernyms to obtain counterfactual ex-amples, and use the model f trained on the originalp-h training dataset to predict the resultant exam-ples (equation 7), and keep those belong to neutralor entailment.
after the validity justiﬁcation, weperform further selection by following equation 8and equation 10, and choose the samples with thesmallest loss for neutral and entailment for lattercontrastive explanation generation..3.3 contrastive explanation generation.
after obtaining qualiﬁed counterfactual examples,some work (cheng et al., 2020; wachter et al.,.
2https://wordnet.princeton.edu/.
2519h tokensp tokenscross attentionhphhrationales tokenoriginal input p-h pairspre-trained lm(roberta / bert)reversal mechanismloss evaluationcounterfactual examplecandidatelabel y and the other label y'knowledge extraction from concepetnetwhy a generator  (gpt-2)contrastive explanation (why a not b)why not b generator(gpt-2)rationaleidentificationcounterfactualexample generationknowledge-awarecontrastive explanation generator(a)(b)(c)2017; verma et al., 2020) provides them as counter-factual explanation directly.
however, since coun-terfactual examples do not provide explanationsexplicitly, it could be difﬁcult for users to under-stand.
hence, in this part, we focus on generatingcontrastive explanation via knowledge-aware gen-erative language model, which explain “why anot b” rather than merely “why a”..while traditional approach generate explana-tion with shap3 or lime4, recent work has ex-ploited to use pre-trained generative language mod-els (radford et al., 2019; lewis et al., 2020; raffelin this paper, we use knowledge-et al., 2020).
aware pre-trained language model to generate con-trastive explanation..knowledge extraction given selected counter-factual examples and identiﬁed rationales, we ex-tract relevant knowledge to enhance the generativelanguage model.
we acquire structured knowledgeand rationale deﬁnitions from conceptnet5 and dic-tionary source6 separately.
for conceptnet, we ex-tract knowledge with breadth-first-search (bfs)algorithm as described in (ji et al., 2020).
fordictionary, we extract the deﬁnition of rationalesby following (chen et al., 2020a).
after extrac-tion, we concatenate these knowledge for trainingknowledge-aware explanation generator..knowledge-aware explanation generatorfor contrastive explanation generation, we dividethe “why a not b” problem into two simplequestion: 1) why the label of the input belong to a,2) why the label of the input not belong to b..in previous study, (kumar and talukdar, 2020)proposed a label-speciﬁc explanation generator,which ﬁne-tuned gpt2 independently for each la-bel.
however, the generator can only produce ex-planations for “why a”.
for the other part of con-trastive explanation, we collect some contrastiveexplanations annotated by human and use them toﬁne-tune a “why not b” generator..taking a premise-hypothesis pair x along withthe qualiﬁed counterfactual example x(cid:48) and ex-tracted knowledge ke as input, which is in theform of (cid:104)s(cid:105) label (cid:104)s(cid:105) x (cid:104)s(cid:105) x(cid:48) (cid:104)s(cid:105) ke (cid:104)s(cid:105), our ﬁne-tuned language model generates explanations thatsupport the corresponding label in a “why a not.
3https://github.com/slundberg/shap4https://github.com/marcotcr/lime5https://github.com/commonsense/conceptnet5/6https://dictionary.cambridge.org/.
b” way.
with these explanations, end-users can ob-serve and understand the difference between origi-nal input and counterfactual example explicitly..4 experiments.
4.1 datasets.
4.1.1 natural language inference.
snli & e-snli the snli dataset(bowmanet al., 2015) is a balanced collection of p-h anno-tated pairs with labels from {entailment, neutral,contradiction}, which consists of about 550k, 10kand 10k examples for train, development, and testset, respectively 7.
(camburu et al., 2018) extendthe snli dataset to e-snli 8 with natural languageexplanations of the ground truth labels.
annotatorswere asked to highlight words in the premise andhypothesis pairs which could explain the labels andwrite a natural language explanation using the high-lighted words.
in this paper, we use the highlightedwords for rationale identiﬁcation and use the natu-ral language explanation to ﬁne-tune the languagemodel based “why a” generator..imdb the imdb dataset (maas et al., 2011) isa movie reviews dataset for sentiment classiﬁcation.
it contains 25,000 training data and 25,000 test datawith movie reviews labeled as positive or negative.
in this paper, we use imdb as a out-of-domaindataset to evaluate if counterfactual examples canimprove the robustness of our model..4.2 evaluation.
we are committed to generate contrastive expla-nations which can distinguish the predicted labeland others at semantic level, hence, bleu (pa-pineni et al., 2002) score is not a proper way tomeasure the quality of explanations.
that is, itcan be better conﬁrmed by manual evaluation.
inthis work, we use manual evaluation and casestudy for contrastive explanations quality evalu-ation.
meanwhile, we use accuracy to measurethe effectiveness of generated contrastive explana-tions on improving model performance in termsof data augmentation (organized in the form of(cid:104)s(cid:105) ce (cid:104)s(cid:105) p remise (cid:104)s(cid:105) hypothesis (cid:104)s(cid:105))..7https://nlp.stanford.edu/projects/snli/snli 1.0.zip8https://github.com/oanamariacamburu/e-snli.
2520table 1: different types of explanations, including token-level explanation, e-snli explanation and contrastiveexplanation.
the explanation of e-snli explains why the label of a given pair is contradiction, while the contrastiveexplanation speciﬁes why the label is contradiction and not neutral or entailment..premise-hypothesis pair.
label.
token-level explanation(rationales).
knowledge from conceptnet.
counterfactual examples.
contrastive explanation.
nile:post-hoc(why contradiction?)
lirex-base(why contradiction?).
e-snli explanation.
(golden annotated ).
a woman and a young child are making sculpturesout of clay.
(p)a man and woman painting on canvas.
(h)contradictionresults of our approachyoung child, making sculptures, clay, manpainting, canvassculpture is a type of art.
canvas is used for art / painting onclay is used for making sculpture.
a woman and a young child are making sculpturesout of clay.
(p)a child and a woman make something.
(perturbed h)making sculptures out of clay different from paintingon canvas,.
explanation of other methods.
women are not men..a young child is not a man..a young child is not a man.
making sculptures out of clay is a different type of artand medium than painting on canvas..(why contradiction not neutral?)
making sculptures out of clay is a type of art..4.3 baselines.
4.3.1 pre-trained language modelroberta & bert for sequence labelling dur-ing rationale identiﬁcation, we use roberta-largeand bert-large, which have 24 layers, 16 attentionheads and a hidden size of 1024 (355m parametersfor roberta-large, 340m parameters for bert-large).
for downstream classiﬁcations tasks, a clas-siﬁcation layer is added over the hidden state of theﬁrst [cls] token at the last layer..gpt-2 for natural language explanation genera-tion, we use the gpt-2 architecture (radford et al.,2019).
in particular, we use the gpt2-mediummodel that has 24 layers, 16 attention heads and ahidden size of 1024 (345m parameters).
we ﬁne-tuned gpt-2 model with label-speciﬁc examplesthat are integrated with contrastive examples andexternal knowledge from conceptnet..4.3.2 nli baselinesesim & sembert & ca-mtl esim (chenet al., 2017) proposes a enhanced sequential infer-.
ence model that considers recursive architecturesin both local inference modeling and inferencecomposition, and incorporates syntactic parsinginformation.
(zhang et al., 2020) incorporateexplicit contextual semantics from pre-trainedsemantic role labeling and introduce an improvedlanguage representation model, semantics-awarebert (sembert), which is capable of explicitlyabsorbing contextual semantics with a bertbackbone.
ca-mtl (pilault et al., 2021) is a noveltransformer based architecture that consists of anew conditional attention mechanism as well asa set of task conditioned modules that facilitateweight sharing, and achieves the new state-of-artperformance on snli..4.4 nli with explanation baselines.
etpa (camburu et al., 2018) propose explain-then-predict-attention (etpa) that generates anexplanation and then predicts the label with onlythe generated explanation..2521nile:post-hoc(kumar and talukdar, 2020)propose natural language inference over label-speciﬁc explanations (nile).
a premise and hy-pothesis pair is input to label-speciﬁc a candidateexplanation generator that generates natural lan-guage explanations supporting the correspondinglabel.
the generated explanations are then fed intoan explanation processor, which predicts labels us-ing evidence presented in these explanations..lirex-base(zhao and vydiswaran, 2021) pro-pose lirex-base that incorporates both a rationaleenabled explanation generator and an instance se-lector to select only relevant, plausible natural lan-guage explanations (nles) to augment nli modelsand evaluate on the standardized snli..4.5 experiment setting.
for rationale identiﬁcation, we use roberta-baseto extract hidden representations and set the learn-ing rate to 2e-5, dropout to 0.02, batch size to 8and number of epochs to 10. meanwhile, we useadamw (loshchilov and hutter, 2018) as the op-timizer and adopt cross-entropy loss as the lossfunction.
in the counterfactual example generationpart, we build a hypernym and hyponym table, anduse hypernym and hyponym of tokens in word-net for perturbation.
in the contrastive explanationgeneration part, we use gpt-2 as the generativelanguage model for training “why a” generatorand “why not b” generator.
for generator, weset the learning rate to 5e-5, adam epsilon to 1e-8,length for generation to 100..4.6 results and analysis.
table 2: human evaluation of contrastive and baselineexplanations on 100 snli test samples.
average scoreof two annotators (%)..modelnile:post-hoclirex-basecontrastive exp.
explanations quality81.588.590.5.explanation generation for snliin table 1,we present the inputs of our model, the results ofour approach that include token-level explanation(rationales), counterfactual example and generatedcontrastive explanation, compared with manuallyannotated explanation and generated “why a” ex-planations by nile:post-hoc and lirex-base..compared with “why a” explanations that aresimple and lack essential information, the con-trastive explanation contains more informationsuch as “making sculptures out of clay is a type ofart” and “making sculptures is different from paint-ing on canvas”.
as shown in table 1, we providenot only the contrastive explanation but also theidentiﬁed rationales and reversed counterfactualexample for reference..to quantitatively assess contrastive explanations,we compared our method with lirex-base andnile:post-hoc in terms of explanation qualitythrough human evaluation on 100 snli test sam-ples.
the explanation quality refers to whether anexplanation provides enough essential informationfor a predicted label.
as shown in table 2, con-trastive explanations produced by our method havea better quality by obtaining over 2.0% and 9.0%than lirex-base and nile:post-hoc ..table 3: the accuracy (%) of our method comparedwith roberta-large and bert-large on snli..modeltraditional baselineesimbert-largesembert-largebert-wwmsembert-wwmca-mtlwhy a exp generatoretpanile: post-hoclirex-basewhy a not b exp generatorbert-large+contrastive exproberta-large+contrastive exphuman exp performancebert-large + human exproberta-large + human exp.
dev test.
88.491.392.092.192.292.4.
87.091.992.2.
88.691.191.691.691.992.1.
86.291.591.6.
91.592.2.
91.992.1.
91.692.7.
92.292.6.explanation enhanced nliin table 3, we re-port the experimental results of our method andother baselines include bert, sembert (zhanget al., 2020), ca-mtl (pilault et al., 2021),nile:post-hoc (kumar and talukdar, 2020) andlirex-base (zhao and vydiswaran, 2021) onsnli.
with contrastive explanations, we are ableto improve the performance of both bert-largeand roberta-large.
compared with nile:post-hoc (kumar and talukdar, 2020), the same scale.
2522bert-large model with contrastive explanationsbrings a gain of 0.4% on test, which indicatesthe knowledge-aware contrastive generator are bet-ter than the generator of nile.
compared withlirex-base that uses roberta-large (zhao andvydiswaran, 2021), the bert-large model androberta-large with contrastive explanations bringa gain of 0.3% and 1.0% separately, which suggestscontrastive explanations are better than rationale en-abled explanation.
in general, contrastive explana-tions can achieve new state-of-art performance andget it closer to human annotation (a gain of 1.1%on bert-large).
we believe that contrastive ex-planations contain more helpful information (e.g.,relations between rationales, differences betweenoriginal and counterfactual examples) that can beused to improve model performance..ablation study we perform ablation studieswith bert-large on the snli dataset to evaluatethe impacts of different components employed inour method, and report the results in table 4. weisolated rationales, counterfactual examples andexternal knowledge, separately.
the model with-out rationales means we generate contrastive ex-planations with counterfactual examples generatedthrough randomly replacing tokens and extractedexternal knowledge.
the model without counterfac-tual examples means we extracted knowledge withgiven rationales and generate contrastive explana-tion with them.
the model without external knowl-edge means we generate contrastive explanationonly with rationales and counterfactual examples.
the model without contrastive explanation actuallyis the bert-large baseline in snli.
we can ob-serve that each component is helpful.
especially,if we remove external knowledge and contrastiveexplanations, we can see a clear decrease of 0.6%and 0.8%, respectively.
it indicates that externalknowledge and contrastive explanation generationare the most essential components, while rationalesand counterfactual examples affect the performanceless.
on one hand, the ablation study results show,external knowledge and rationales affect more thancounterfactual examples on explanation generation.
on the other hand, the results suggest that eachcomponent contributes positively, and indicate theimportance of knowledge aware contrastive expla-nations, as we highlighted in the title..out of domain counterfactual exampleinthis part, we use the generated counterfactual ex-.
table 4: the accuracy (%) of ablation studies on snli..modelour modelw/o rationalesw/o counterfactual examplew/o external knowledgew/o contrastive exp.
dev test91.991.591.591.091.691.491.391.291.191.3.amples of imdb for out of domain evaluation.
asshown in table 5, we train bert-base on two differ-ent training sets: the original training set traino,and the union of original training examples and gen-erated counterfactual examples traino∪c, andevaluate it with two separated dev sets: the originaldev set devo and the generated counterfactual ex-ample dev set devc.
experimental results shownthat bert-base model enhanced with counterfac-tual examples achieves 88.5% and 95.1%, bringinga gain of 11.0% on devc while a slight decreaseof 1.7% on devo.
it indicates that counterfactualexamples can help to improve the robustness ofmodel for more diversiﬁed data distribution..with imdb evaluation, we demonstrate thatcounterfactual examples can not only help to gen-erate contrastive explanation, but also contribute todata augmentation.
in the experiments on snli, weevaluated the effectiveness of counterfactual exam-ple in contrastive explanation generation.
in imdbexperiments, we further verify the effectivenessof counter-factual examples for data augmentationwith only rationales identiﬁcation and heuristic re-versal mechanism..table 5: the accuracy of bert-base on imdb, beingtrained with traino and traino∪c, evaluated ondevo and devc..modelbert-base (traino)bert-base (traino∪c).
devo devc86.190.295.188.5.
5 related work.
5.1 counterfactual example generation.
counterfactual example aims to ﬁnd a minimalchange in data that “ﬂips” the model’s predictionand is used for explanation.
(wachter et al., 2017)ﬁrst propose the concept of unconditional coun-terfactual explanations and a framework to gener-ate counterfactual explanations.
(hendricks et al.,.
25232018) ﬁrst consider the evidence that is discrimina-tive for one class but not present in another class,and learn a model to generate counterfactual expla-nations for why a model predicts class a instead ofb. in this paper, we focus on counterfactual exam-ple generation providing contrastive example fornatural language inference..5.2 post-hoc explanation generation.
for post-hoc explainable nlp system, we can di-vide explanations into three types: feature-based,example-based and concept-based..for feature-based explanation, (ribeiro et al.,2016) propose lime and (guidotti et al., 2018)extend lime by ﬁtting a decision tree classiﬁer toapproximate the non-linear model.
however, thereis no guarantee that they are faithful to the originalmodel.
for example-based explanation, (kim et al.,2016) select both prototypes and criticisms fromthe original data points.
(wachter et al., 2017) pro-pose counterfactual explanations providing alterna-tive perturbations.
for concept-based explanation,(ghorbani et al., 2019) explains model decisionsthrough concepts that are more understandable tohuman than individual features or characters.
inthis paper, we integrate counterfactual example andconcepts for contrastive explanation generation..we extract concepts knowledge from conceptnetand dictionary to train knowledge-aware explana-tion generators.
we show that contrastive explana-tions that specify why a model makes prediction arather than b can provide more faithful informationthan other “why a” explanations.
moreover, con-trastive explanations can be used for data augmenta-tion to improve the performance and robustness ofexisting model.
the exploration of contrastive ex-planation in other nlp tasks (i.e.
question answer-ing) and better evaluation metrics for explanationwill be performed in the future..acknowledgments.
we thank the anonymous reviewers for their help-ful comments on this paper.
this work is sup-ported by national key r&d program of china(no.
2018aaa0101900), the nsfc projects (no.
62072399, no.
u19b2042, no.
61402403), chi-nese knowledge center for engineering sciencesand technology, moe engineering research cen-ter of digital library, alibaba research intern pro-gram of alibaba group, alibaba-zhejiang univer-sity joint institute of frontier technologies, andthe fundamental research funds for the centraluniversities..5.3 natural language inference.
references.
for natural language inference, (bowman et al.,2015) propose snli which contains samples ofpremise and hypothesis pairs with human anno-tations.
in order to provide interpretable and ro-bust explanations for model decisions, (camburuet al., 2018) extend the snli dataset with naturallanguage explanations of the ground truth labels,named e-snli.
for explanation generation in nli,(kumar and talukdar, 2020) propose nile, whichutilizes label-speciﬁc generators to produce labelsalong with explanation.
however, (zhao and vy-diswaran, 2021) ﬁnd nile do not take into accountthe variability inherent in human explanation, andpropose lirex which incorporates a rationale en-abled explanation generator.
in this paper, we con-sider generating contrastive explanations in nli..6 conclusion.
in this paper, we focus on knowledge-aware con-trastive explanation generation for nli.
we gen-erate counterfactual examples by changing iden-tiﬁed rationales of given instances.
afterwards,.
pepa atanasova, jakob grue simonsen, christina li-oma, and isabelle augenstein.
2020. generatingin proceedings of thefact checking explanations.
58th annual meeting of the association for compu-tational linguistics, pages 7352–7364, online.
as-sociation for computational linguistics..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..oana-maria camburu, tim rockt¨aschel, thomaslukasiewicz, and phil blunsom.
2018. e-snli: nat-ural language inference with natural language expla-nations.
in advances in neural information process-ing systems, volume 31. curran associates, inc..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstmin proceedings offor natural language inference.
the 55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1657–1668, vancouver, canada.
associationfor computational linguistics..2524qianglong chen, feng ji, haiqing chen, and yinimproving commonsense ques-zhang.
2020a.
tion answering by graph-based iterative retrievalin proceedingsover multiple knowledge sources.
of the 28th international conference on compu-tational linguistics, pages 2583–2594, barcelona,spain (online).
international committee on compu-tational linguistics..zhongxia chen, xiting wang, xing xie, mehulparsana, akshay soni, xiang ao, and enhong chen.
2020b.
towards explainable conversational recom-in proceedings of the twenty-ninthmendation.
international joint conference on artiﬁcial intel-ligence, ijcai-20, pages 2994–3000.
internationaljoint conferences on artiﬁcial intelligence organi-zation.
main track..furui cheng, yao ming, and huamin qu.
2020. dece:decision explorer with counterfactual explanationsfor machine learning models.
ieee transactions onvisualization and computer graphics..eunsol choi, he he, mohit iyyer, mark yatskar, wen-tau yih, yejin choi, percy liang, and luke zettle-moyer.
2018. quac: question answering in con-in proceedings of the 2018 conference ontext.
empirical methods in natural language processing,pages 2174–2184, brussels, belgium.
associationfor computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..amirata ghorbani, james wexler, james y zou, andbeen kim.
2019. towards automatic concept-basedin advances in neural informationexplanations.
processing systems, pages 9273–9282..riccardo guidotti, anna monreale, salvatore ruggieri,dino pedreschi, franco turini, and fosca giannotti.
2018. local rule-based explanations of black boxdecision systems.
arxiv preprint arxiv:1805.10820..lisa anne hendricks, ronghang hu, trevor darrell,and zeynep akata.
2018. generating counterfactualexplanations with natural language.
in icml work-shop on human interpretability in machine learn-ing, pages 95–98..haozhe ji, pei ke, shaohan huang, furu wei, xiaoyanzhu, and minlie huang.
2020. language generationwith multi-hop reasoning on commonsense knowl-edge graph.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 725–736, online.
associationfor computational linguistics..amir-hossein karimi, gilles barthe, borja balle, andisabel valera.
2020. model-agnostic counterfactualin inter-explanations for consequential decisions.
national conference on artiﬁcial intelligence andstatistics, pages 895–905..been kim, rajiv khanna, and oluwasanmi o koyejo.
2016. examples are not enough, learn to criticize!
criticism for interpretability.
in advances in neuralinformation processing systems, volume 29. curranassociates, inc..sawan kumar and partha talukdar.
2020. nile : natu-ral language inference with faithful natural languagein proceedings of the 58th annualexplanations.
meeting of the association for computational lin-guistics, pages 8730–8742, online.
association forcomputational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2018. fixing weight.
decay regularization in adam.
iclr..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..jonathan pilault, amine el hattami, and christopherpal.
2021. conditionally adaptive multi-task learn-improving transfer learning in {nlp} usinging:fewer parameters & less data.
in international con-ference on learning representations..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..2525colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, volume 32. curranassociates, inc..zhuosheng zhang, yuwei wu, hai zhao, zuchao li,shuailiang zhang, xi zhou, and xiang zhou.
2020.semantics-aware bert for language understanding.
in proceedings of the aaai conference on artiﬁcialintelligence, 05, pages 9628–9635..xinyan zhao and vg vydiswaran.
2021. lirex: aug-menting language inference with relevant explana-tion.
proceedings of the aaai conference on artiﬁ-cial intelligence..nazneen fatema rajani, bryan mccann, caimingxiong, and richard socher.
2019. explain yourself!
leveraging language models for commonsense rea-in proceedings of the 57th annual meet-soning.
ing of the association for computational linguis-tics, pages 4932–4942, florence, italy.
associationfor computational linguistics..karthikeyan natesan ramamurthy, bhanukiran vinza-muri, yunfeng zhang, and amit dhurandhar.
2020.arxivmodel agnostic multilevel explanations.
preprint arxiv:2003.06005..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
” why should i trust you?” explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery and data mining,pages 1135–1144..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsenseknowledge.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4149–4158, minneapolis, minnesota.
associ-ation for computational linguistics..james.
andreas vlachos,.
and arpit mittal..thorne,christos2019.christodoulopoulos,generating token-level explanations for naturalin proceedings of the 2019language inference.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 963–969, minneapolis, minnesota.
association for computational linguistics..sahil verma, john dickerson, and keegan hines.
2020.counterfactual explanations for machine learning: areview.
arxiv preprint arxiv:2010.10596..sandra wachter, brent mittelstadt, and chris russell.
2017. counterfactual explanations without openingthe black box: automated decisions and the gdpr.
harv.
jl & tech., 31:841..d randall wilson and tony r martinez.
1997..im-proved heterogeneous distance functions.
journalof artiﬁcial intelligence research, 6:1–34..shuyuan xu, yunqi li, shuchang liu, zuohui fu, andyongfeng zhang.
2020. learning post-hoc causalexplanations for recommendation.
arxiv preprintarxiv:2006.16977..2526to contradiction, the top 10 removed words are“white, man, wearing, shirt,black, blue, two, stand-ing,woman, red”, the top 10 inserted words are“woman, man, there, playing,two, wearing, one,men, girl,no”..the demand for contrastive explanation a“contrastive explanation” explains not only whysome event a occurred, but why a occurred asopposed to some alternative event b. some philoso-phers argue that agents could only be morally re-sponsible for their choices if those choices havecontrastive explanations, since they would other-wise be “luck infested”.
moreover, if the answerpredicted by a well-trained model is a but confus-ing with b, it is natural for end-users to ask “whythe answer is a rather than b”.
a similar scenario ispossible to occur when a child is going to recognizecharacters or learn other language skills.
therefore,contrastive explanation generation is essential incritical domains..a appendices.
reported experimental results here, we re-port some other experimental results for reproduc-tion.
we use 2 rtx-6000 gpus for generator train-ing.
for each epoch, it takes 3 hours to ﬁne-tunethe contrastive generator.
as we set 4 epochs foreach “why a” generator and “why not b” gen-erator, it takes 12 hours for each approach.
thereare 355m parameters in roberta-large, 340m pa-rameters in bert-large and 345m parameters ingpt2-medium.
and our code is based on pytorch..the difference between counterfactual exam-ple and contrastive explanation in this paper,we generate contrastive explanations with qualiﬁedcounterfactual examples.
as counterfactual exam-ples provide example-based explanations, the con-trastive explanations provide concept-based expla-nations and explain “why a not b”.
meanwhile,for end-user, contrastive explanations are easierto understand than counterfactual example, whichcan integrate external knowledge from knowledgebases..common replaced words here, we showsome common replaced words in reversal mech-anism..for entailment to neutral, the top 10 removedwords are “man, wearing, white, blue,black, shirt,one, young, people, woman”, the top 10 insertedwords are “people, there, playing, man, person,wearing, outside, two, old, near”.
for entailmentto contradiction, the top 10 removed words are“man, wearing, white, blue,black, two, shirt, one,young,people”, the top 10 inserted words are “peo-ple, man, woman, playing,no, inside, person, two,wearing, women”..for contradiction to neutral, the top 10 removedwords are “wearing, blue, black, man,white, two,red, sitting, young, standing”, the top 10 insertedwords are “people, playing, man, woman, two,wearing, near, tall, men, old”.
for contradiction toentailment, the top 10 removed words are “wear-ing, blue, black, man,white, two, red, shirt, young,one”, the top 10 inserted words are “people, there,man, two, wearing,playing, people, men, woman,outside”..for neutral to entailment, the top 10 removedwords are “white, wearing, shirt, black,blue, man,two, standing,young, red”, the top 10 insertedwords are “playing, wearing, man, two, there,woman, people, men, near, person”.
for neutral.
2527