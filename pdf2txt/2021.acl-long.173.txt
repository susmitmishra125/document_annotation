data augmentation for text generation without any augmented data.
wei bi∗ huayang li∗ jiacheng huangtencent ai lab, shenzhen, china{victoriabi,alanili,eziohuang}@tencent.com.
abstract.
data augmentation is an effective way to im-prove the performance of many neural text gen-eration models.
however, current data aug-mentation methods need to deﬁne or chooseproper data mapping functions that map theoriginal samples into the augmented samples.
in this work, we derive an objective to for-mulate the problem of data augmentation ontext generation tasks without any use of aug-mented data constructed by speciﬁc mappingfunctions.
our proposed objective can be efﬁ-ciently optimized and applied to popular lossfunctions on text generation tasks with a con-vergence rate guarantee.
experiments on ﬁvedatasets of two text generation tasks show thatour approach can approximate or even surpasspopular data augmentation methods..1.introduction.
end-to-end neural models are generally trained in adata-driven paradigm.
many researchers have pro-posed powerful network structures to ﬁt trainingit has also become ubiquitous to in-data well.
crease the training data amount to improve modelperformance.
data augmentation is an effectivetechnique to create additional samples in both vi-sion and text classiﬁcation tasks (perez and wang,2017; shorten and khoshgoftaar, 2019; wei andzou, 2019), which perturb samples without chang-ing their labels.
for text generation tasks, therecan be more types of data perturbation to constructaugmented samples, including corrupting the in-put text (xie et al., 2017), the output text (norouziet al., 2016; kurata et al., 2016), or both (zhanget al., 2020).
as such, classiﬁcation tasks can be re-garded as special cases of generation tasks in termsof incorporating data augmentation techniques, andthis work mainly discusses text generation tasks..∗equal contribution..the focus of previous work on text data augmen-tation has been to design proper augmentation tech-niques to create augmented samples.
some aug-mentation methods have been proposed for generaltext tasks.
for example, different general replace-ment operations have been explored to edit wordsin a text sample, ranging from simple look-up ta-bles (zhang et al., 2015) to pretrained masked lan-guage models (kobayashi, 2018; wu et al., 2019).
sennrich et al.
(2016) propose to augment textsequences by back-translation.
for some gener-ation tasks such as dialogue generation, generalaugmentation methods may not yield stable im-provements and it requires to carefully incorporatethe task property to design useful augmented sam-ples (zhang et al., 2020).
all these methods needto explicitly construct augmented samples, and thedata mapping functions from the original samplesto the augmented samples are mostly deﬁned apri-ori.
this motivates us to raise a question, whetherwe can skip the step to deﬁne or choose properaugmented data mapping functions to accomplisheffective data augmentation..to answer this question, we aim to formulate theproblem of data augmentation for general text gen-eration models without any use of augmented datamapping functions.
we start from a conventionaldata augmentation objective, which is a weightedcombination of loss functions associated with theoriginal and augmented samples.
we show thatthe loss parts of the augmented samples can bere-parameterized by variables not dependent onthe augmented data mapping functions, if a simpleeuclidean loss function between the sentence rep-resentations is applied.
based on this observation,we propose to directly deﬁne a distribution on there-parameterized variables.
then we optimize theexpectation of the augmented loss parts over thisdistribution to approximate the original augmentedloss parts computed with various augmented data.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2223–2237august1–6,2021.©2021associationforcomputationallinguistics2223mapping functions.
we make different assump-tions on the variable distributions and ﬁnd that ourproposed objective can be computed and optimizedefﬁciently by simple gradient weighting.
if stochas-tic gradient descent (sgd) is used, our objectiveis guaranteed with the convergence rate o(1/t ).
our objective can be coupled with popular lossfunctions on text generation tasks, including theword mover’s distance (kusner et al., 2015) andthe cross-entropy loss..√.
our approach, which utilizes the proposed objec-tive and optimizes it by sgd, has two advantages.
first, it provides a uniﬁed formulation of variousdata perturbation types in general text generationmodels, which sheds a light on understanding theworking mechanism of data augmentation.
sec-ond, the optimization of our approach is simpleand efﬁcient.
without introducing any new sampleduring training, we can avoid additional calculationefforts on augmented samples, often with the totalsize much larger than the original data size.
hence,our approach maintains high training efﬁciency..extensive experiments are conducted to vali-date the effectiveness of our approach.
we mainlyuse the lstm-based network structure (bahdanauet al., 2015; luong et al., 2015b) and perform ex-periments on two text generation tasks - neural ma-chine translation and single-turn conversational re-sponse generation.
results on ﬁve datasets demon-strate that the proposed approach can approximateor even surpass popular data augmentation meth-ods such as masked language model (devlin et al.,2019) and back-translation (sennrich et al., 2016)..2 related work.
data augmentation has shown promising improve-ments on neural models for different text genera-tion tasks such as language modeling (xie et al.,2017), machine translation (sennrich et al., 2016)and dialogue generation (niu and bansal, 2019;cai et al., 2020).
existing text data augmentationmethods can be mainly categorized into word-levelaugmentation and sentence-level augmentation..word-level augmentation methods perturb wordswithin the original sentence.
common operationsinclude word insertion and deletion (wei and zou,2019), synonym replacement (zhang et al., 2015),and embedding mix-up (guo et al., 2019).
maskedlanguage models can be used by masking somepercentages of tokens at random, and predictingthe masked words based on its context (wu et al.,.
2019; cai et al., 2020)..sentence-level data augmentation is not limitedto edit only a few words in the original sentence,but to generate a complete sentence.
for example,back-translation is originally proposed to translatemonolingual target language data into source lan-guage to augment training pairs in machine transla-tion (sennrich et al., 2016).
it is later extended toparaphrase sentences in any text dataset, in whichtwo translation models are applied: one translationmodel from the source language to target languageand another from the target to the source.
gan-based and vae-based models have also achievedimpressive results to create entire sentences to aug-ment the training data (hu et al., 2017; chenget al., 2019).
for dialogue generation, retrievedsentences can be good supplement of the originalcorpus (zhang et al., 2020)..both word-level and sentence-level augmenta-tion methods need to deﬁne their augmented datamapping functions (i.e.
operations to edit words ormodels to generate sentences) apriori.
some workstrain policies to sample a set of word-level oper-ations (niu and bansal, 2019), but the operationcandidates are still pre-deﬁned.
a few works learnto construct augmented samples and optimize thenetwork jointly (hu et al., 2019; cai et al., 2020).
different from previous work, our goal is not topropose or learn novel augmented data mappingfunctions.
instead, we investigate whether the ef-fectiveness of data augmentation can be achievedwhile we do not bother to use any speciﬁc aug-mented data mapping function..besides data augmentation, data weighting isanother useful way to improve model learning.
itassigns a weight to each sample to adapt its impor-tance during training.
the sample weights are oftencarefully deﬁned (freund and schapire, 1997; ben-gio et al., 2009) or learnt by another network (jianget al., 2018; shu et al., 2019).
data augmentationis often combined with data weighting together toweight the original and augmented samples..3 background.
we are given original samples d = {(x, y)} withx, y both as text sequences.
without loss of gener-ality, a deep generation model is to learn a mappingfunction fx,y by a deep neural network that outputsy given x. as mentioned in the introduction, textgeneration tasks mainly have three types of aug-mented data:.
2224• one (or several) perturbed input text ˆx by one (orseveral) augmented data mapping function φˆx;• one (or several) perturbed output text ˆy by one(or several) augmented data mapping functions φˆy;• one (or several) perturbed paired text ( ˆx, ˆy) bycorresponding augmented data mapping functions.
proper augmented data mapping functions are of-ten supposed to generate perturbed sequences orsequence pairs that are close to the original one.
they are assumed to be given apriori in optimizingthe generation model for now..let (cid:96)(fx,y(x), y) denote the loss function to beminimized for each sample.
we ﬁrst use aug-mented data in the input domain as an exampleto present the problem formulation and introduceour approach, then later discuss other types of aug-mented data.
data augmentation methods generallyapply an augmented loss per sample with its aug-mented samples:.
(cid:88).
ˆx:φˆx∈f.
(cid:96)aug = (cid:96)(fx,y(x), y) +.
wˆx(cid:96)(fx,y( ˆx), y).
(1)where wˆx is the importance weight associated witheach augmented sample, φˆx is the augmented datamapping function that constructs ˆx, and f is thefunction space containing all feasible augmenteddata mapping functions..4 our approach.
in this section, we aim to formulate the problem ofdata augmentation for general text generation mod-els without any use of augmented data mappingfunctions.
we introduce our approach by assumingthat the loss function (cid:96) is the most simple euclideandistance, i.e..(cid:96)(u, v) = (cid:107)u − v(cid:107)2.
(2).
where u and v are the sentence representations oftwo sentences, i.e.
the target sequence and the pre-dicted sequence.
other conventional loss functionsin text generation will be discussed in section 5..we ﬁrst rewrite each loss part of an augmenteddata point in (1) from a polar coordinate systemin sec 4.1. in this way, we can regard the totalaugmented loss part with multiple augmented datamapping functions as sampling different points inthe polar coordinate system.
this inspires us thatwe can skip to deﬁne any augmented data mappingfunction, but only design a joint distribution of the.
perturbation radius and perturbation angle in thepolar coordinate system.
in sec 4.2, we show twoprobability distribution substantiations, and ﬁndthat our approach can be optimized efﬁciently bysimply re-weighting the gradients.
in sec 4.3, wediscuss the extension of our approach for otheraugmented data mapping function types..4.1 proposed objective.
by treating fx,y(x), fx,y( ˆx) and y as three verticesin the euclidean space, we can form a triangle (il-lustrated in fig.
1a) with the three vertices andthe loss between them as edges.
for a given aug-mented data mapping function φˆx and a sample(x, y), we can rewrite (cid:96)(fx,y( ˆx), y) using the po-lar coordinate system with fx,y(x) as the pole and(fx,y(x), y) as the polar axis:.
(cid:96)2(fx,y( ˆx), y) =.
(cid:96)2(fx,y(x), y) + (cid:96)2(fx,y(x), fx,y( ˆx))−2(cid:96)(fx,y(x), fx,y(ˆx))(cid:96)(fx,y(x), y) cos θ.
(3).
where θ is the radian of fx,y( ˆx).
we canobserve that,the rewritten augmented sampleloss part depends on the original sample loss(cid:96)(fx,y(x), y) as well as the radius r and radianθ of fx,y( ˆx).
here r is the data perturbation dis-tance (cid:96)(fx,y(x), fx,y(ˆx)).
therefore, we can mapeach augmented data mapping function φˆx ∈ finto (r, θ) ∈ p , where p is a joint distribution of(r, θ) 1. a weighted summation of the augmentedloss parts from different augmented data mappingfunctions can be seen as an empirical estimationof the expectation of the rewritten loss by sam-pling different (r, θ)’s from their joint distributionp , though the corresponding ground truth p is notobserved..this inspires us how to avoid to speciﬁcally de-sign or choose several augmented data mappingfunctions and their weights used in (1).
we candirectly design the distribution p of (r, θ) and op-timize the expectation of the rewritten loss (i.e.
the right hand side in (3)) under this distribution.
hence, we propose to optimize the following ob-jective to mimic the effect of data augmentation:.
1it is worth pointing out that even if the three vertices (i.e.,fx,y( ˆx), y, and fx,y(x) ) lie in high dimensional spaces, wecan always use the distribution of (r, θ) cover all possibletriangles formed by them.
and our derivation will not lose itsgeneralization in high dimensional spaces, since we does notmake use of the vertices but only edges of the triangles..2225(a) with a perturbed input ˆx.
(b) with a perturbed output ˆy.
(c) with a perturbed paired text ( ˆx, ˆy).
figure 1: illustration of the polar coordinate systems for three kinds of data perturbation.
rays in the ﬁgures arethe polar axes.
our approach expresses edges in dots by their corresponding polar coordinates..(cid:96)our = (cid:96)(fx,y(x), y)+e(r,θ)∈p [φ((cid:96)(fx,y(x), y))](4)where φ(e; r, θ) is a function of an edge e in theloss function space given (r, θ):.
φ(e; r, θ) =.
e2 + r2 − 2er cos θ..(5).
(cid:112).
4.2 optimization.
we design speciﬁc distributions of (r, θ) used inthe proposed objective (4) and their optimization.
we assume the two variables are independent:.
p(r, θ) = p(r)p(θ)..(6).
in the following corollary, we ﬁrst show the resultby assuming that both r and θ follow uniform dis-tributions.
recall that proper data mapping func-tions augment samples close to the original one.
an ideal case is thus to perturb samples with theiroutput representations uniformly surrounding thatof the original sample.
the uniform distributionwith a small perturbation radius upper bound r cansimulate this ideal case.
corollary 1. we are given the perturbation dis-tance upper bound r and assume that.
r ∼ u(0, r), θ ∼ u(0, π)..(7).
e(r,θ)∈p [φ((cid:96)(fx,y(x), y))] is upper bounded by12 (cid:96)(fx,y(x), y) + c1 · (cid:96)2(fx,y(x), y) + c2(r),where c1 is a constant and c2(r) is another con-stant dependent on r.proof is in the appendix.
with the above result,we can optimize the objective in (4) by minimizingthe derived upper bound.
we calculate its gradient:.
∂(cid:96)our∂θ.
=.
32.
·.
∂(cid:96)(θ)∂θ.
+ 2c1 · (cid:96)(θ).
(8).
∂(cid:96)(θ)∂θ.
where θ contains all neural model parameters.
itcan be observed that the major difference of theabove gradient compared with the original one ofthe objective in (1) lies in the second part of (8),which weights the original gradient by the lossvalue.
this means that the performance improve-ment brought by data augmentation under our for-mulation can be equivalently accomplished by spe-cialized data weighting.
indeed, many data weight-ing methods (lin et al., 2017) favors hard examplesby reducing the gradient contribution from easyexamples and increasing the importance of hardexamples (example with large loss value in our ap-proach), which signiﬁcantly boost the performance.
this in turn shows that simple uniform distributionsassumed here should be reasonable and effective.
instead of uniform distribution, we can assume auniform distribution on θ but an exponential distri-bution on r such that a small perturbation distanceis preferred with a higher probability.
corollary 2. we are given the expected value ofthe perturbation distance as r and assume that.
r ∼ exp(.
), θ ∼ u(0, π)..(9).
1r.2.e(r,θ)∈p [φ((cid:96)(fx,y(x), y))] is upper bounded byc1(r) · (cid:96)(fx,y(x), y) + c1(r)· (cid:96)2(fx,y(x), y) +c2(r), where c1(r) and c2(r) are constants de-pendent on r.proof is in the appendix.
the above corollaryshows that even if different distributions are as-sumed, we can still use gradient weighting to opti-mize the proposed objective, where c1(r) can beset as a hyper-parameter..if the loss is lipschitz smooth, of which eu-clidean distance is the case, we can prove the con-vergence of our approach with the convergence rate.
2226<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="kkyfhekjaftdmfjayt65h6sssaw=">aaab6hicbvbns8naej3ur1q/qh69lbbbu0le0wpri8cw7ae0owy2k3btzhn2n0ij/qvepcji1z/kzx/jts1bwx8mpn6bywzekaiujet+o4w19y3nrej2awd3b/+gfhju0ngqgdzzlglvcahgwsu2dtcco4lcgguc28h4bua3n1bphsshm0nqj+hq8pazaqzuup1yxa26c5bv4uwkajnq/fjxbxcznejpmkbadz03mx5glefm4ltuszumli3peluwshqh9rp5ovnyzpubcwnlsxoyv39pzdtsehiftjoizqsxvzn4n9dntxjjz1wmquhjfovcvbatk9nxzmavmimmllcmul2vsbfvlbmbtcmg4c2/vepaf1xvquo2liu12zyoipzakzydb9dqg3uoqxmyidzdk7w5j86l8+58lfoltj5zdh/gfp4a3nom+g==</latexit>r<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="zs9ztll99ylw3glurc/gnaob/ge=">aaacahicbvdlssnafj3uv62vqasxbgaluefkiooui25cvrapaeoytcbt0mkkzeykiwtjr7hxoyhbp8odf+okzukrb4y5nhmv997jxyxkzvlfrmvpewv1rbpe29jc2t4xd/e6mkoejh0csuj0psqjo5x0ffwm9gnbuogx0vmmn4xfeybc0ojfqzqmtohgnayui6ul1zwi3gx6muanorcxx6ah/rjpfukadatpzqd/erskdvci7zqfqz/csui4wgxjobctwdkzeopirvlamjekrnicrmsgkuchku42oychx1rxyraj/bicm/vnr4zcweymk0okxnlrk8t/vegigisnozxofof4pihigfqrlnkaphuek5zqgrcgeleix0ggrhrmnr2cvxjyx9i9a9oxtevuvn66luoogknwbbrabpegbw5bg3qabjl4ai/g1xg0no03431ewjhknn3wc8bhnyz1lsm=</latexit>fx,y(x)<latexit sha1_base64="nl4jdq6enuseolyccjxcwiqpevq=">aaacbnicbvdlssnafj3uv62vqesrbotqquoiii6lblxwsa9oqphmju3qyyozitsernz4k25ckolwb3dn3zhps9dwa8mczrmxe+9xy0afnixvrbk0vlk6vl2vbwxube/ou3tdesuckw6owmt7lhke0zb0jjwm9gnouoay0nphn4xfeybc0ci8l2lm7aanq+ptjksshp3qd7ljazo3rbgsmevgzbnpol5skucnjl43msyucjgyjamdem1h/7k8cccbcsvmsiibactszhcxfdos16xekbjhmrqsgaihcoiws+kzotxwigf9iksxsjhvf3dkkbdfcqoyqhik5r1c/m8bjnk/sjmaxokkiz4n8hmgzqsltkbhocgspyogzknafeir4ghllvxnhwdon7xiumdn86jp3j3xw9dlhfvwai5aa5jgertalwiddsdgetydv/cmpwkv2rv2msutagxppvgd7fmhj2azka==</latexit>fx,y(ˆx)<latexit sha1_base64="qujckutdwivqmkoodpzpn4d505a=">aaab/xicbvdns8mwhe3n15xf9epmjtget6mvry9dlx4nudlyy0jtdatlk5kkqi3ff8wlb0w8+n94878x3xrqzqchj/d+p/lygorrpr3n26otla+srtxxgxubw9s79u5et4luytlfggnzd5aijhls1vqz0k8kqxhayh0wus79+wcifrx8tmcj8wm04jsiggkjde0db4x07gwchsqlzzvnrtg0m07lmqiuercitvchm7s/vfdgnczcy4augrhoov0csu0xi0xdsxvjej6gerkyylfmlj9p0xfw2cghjiq0h2s4vx9v5chwztyzgsm9vvnekf7ndvidxfo55umqccezh6kuqs1gwqumqsrys8wqhcu1wseei4mwnou1tanu/jcxse+05z63nnuzzvuqqqmodseroaeuuabtcam6oasweatp4bw8wu/wi/vufcxga1a1sw/+wpr8abmilgw=</latexit>ˆy<latexit sha1_base64="2bjshp6fcqw1tt2kwnfokdu82ou=">aaab9xicbvdlsgmxfl1tx7w+qi7dbivgqsyiosuig5cv7apaswqymty0kwxjrild/8onc0xc+i/u/bsz7sy09udi4zx7yckjes60cd1vp7syura+ud6sbg3v7o5v9w/awqak0barxkpugdxltncwyybtbqiojgnoo8h4jvc7j1rpjsw9mstuj/fqsigrbkz00a8kd/uktlc2mq6qnbfuzocwiveqghrodqpf/vcsnkbcei617nluyvwmk8mip9nkp9u0wwsmh7rnqcax1x42sz1fj1yjussvpckgmfp7i8oxzqpzyribkv70cve/r5ea6mrpmehsqwwzpxslhbmj8gpqybqlhk8swuqxmxwrevaygftuxzbglx55mbtp6t5f3b07rzwuizrkcathcaoexeidbqejlscg4ble4c15cl6cd+djplpyip1d+apn8wdwi5mo</latexit>y√.
t ), if sgd is used.
the proof is providedo(1/in the appendix, which is extended from resultsin reddi et al.
(2016).
theorem 1. suppose (cid:96)our is in the class of ﬁnite-sum lipschitz smooth functions, has δ-boundedgradients, and the weight of the loss gradientis clipped to be bounded by [w1, w2].
let thelearning rate of sgd αt = c/t where c =(cid:113) 2((cid:96)our(θ0)−(cid:96)our(θ∗))lσ2w1w2.
where l is the lipschitzconstant and θ∗ is an optimal solution.
then theiterates of sgd of our approach with (cid:96)our satisfy:.
√.
e[||∇(cid:96)our(θt)||2] ≤.
min0≤t≤t −1(cid:115).
2((cid:96)our(θ0) − (cid:96)our(θ∗))lw1t w2.
σ.
(10).
4.3 other types of augmented data.
we now discuss how our approach can be applied toother types of augmented data.
for augmented dataon the output domain, the objective in (1) becomes:.
(cid:96)aug = (cid:96)(fx,y(x), y) +.
wˆy(cid:96)(fx,y(x), ˆy)..(cid:88).
φˆy∈f.
(11)the augmented loss part can be rewritten usingthe polar coordinate system with y as the pole and(y, fx,y(x)) as the polar axis, illustrated in fig.
1b:.
(cid:96)2(fx,y(x), ˆy) = (cid:96)2(y, fx,y(x)) + (cid:96)2(y, ˆy).
−2(cid:96)(y, fx,y(x))(cid:96)(y, ˆy) cos θ.
(12).
similarly, the augmented data mapping functionφˆy can be re-parameterized into a function of theradius r = (cid:96)(y, ˆy) (still the perturbation distance)and the radian of ˆy.
the objective turns out to bethe same as (4)..for data perturbation on both the input and out-.
put space, we have:.
(cid:96)aug = (cid:96)(fx,y(x), y) +.
wˆx,ˆy(cid:96)(fx,y( ˆx), ˆy)..(cid:88).
φˆx,ˆy∈f.
(13)illustrated in fig.
1c, we ﬁrst make use of the trian-gle inequality that:.
(cid:96)(fx,y( ˆx), ˆy) ≤.
((cid:96)(fx,y( ˆx), y) + (cid:96)(y, ˆy)).
+.
((cid:96)(fx,y( ˆx), fx,y(x)) + (cid:96)(fx,y(x), ˆy))..12.
12.using (3) and (12), the objective is rewritten as:.
(cid:96)our = (cid:96)(fx,y(x), y).
+e(r,θ)∈p [r + φ((cid:96)(fx,y(x), y))]..(15).
note that e(r,θ)∈p [r] is a scalar which is not depen-dent on any learning parameter.
thus optimizingthe above objective is equivalent to optimizing (4).
from the above analysis, we can see that our pro-posed objective in (4) can be applied to handle allthree kinds of augmented data mapping functionsin text generation models..5 loss function.
in theory, our approach can be applied to any lips-chitz smooth loss function that holds the equation(3).
in this section, we show another valid lossfunction in our approach – the word mover’s dis-tance (wmd) (kusner et al., 2015; zhao et al.,2019), which is previously used in various textgeneration tasks.
next, we discuss the cross en-tropy loss, in which the proposed objective is notan upper-bound of the data augmentation objective.
however, our approach can still converge with thesame convergence rate and experimental results inthe next section validate the effectiveness of ourapproach with the cross-entropy loss..5.1 word mover’s distance.
transport dis-wmd, also named the optimaltance (chen et al., 2018a), leverages optimal trans-port to ﬁnd an optimal matching of similar wordsbetween two sequences, providing a way to mea-sure their semantic similarity:.
(cid:96)w m d(u, v) = minti,j.
ti,jdi,j.
(16).
(cid:88).
i,j.
s.t..ti,j = pu,i ∀i.
m(cid:88).
j=1.
n(cid:88).
i=1.
ti,j = pv,j ∀j.
where pu,i/pv,j is the probability distribution of thesentence, i.e.
(cid:80)i pu,i = 1 and (cid:80)j pv,j = 1. di,jis the cost for mis-predicting ui to vj, where thesquared euclidean distance di,j = (cid:107)ui − vj(cid:107)2 isused and ui/vj is the word embedding vector.
notethat the euclidean distance in (2) is a special caseof wmd by replacing the 1-gram used in wmd.
(14).
2227√.
to n-gram with n larger than the sentence’s length.
wmd is the squared l2 wasserstein distance.
wetake its squared root, i.e.
(cid:96)w d =(cid:96)w m d, whichholds an upper bound as the right hand side in (3).
also, (cid:96)w d is lipschitz smooth.
theorem 2. for the l2 wasserstein distancew2(·, ·) on the wasserstein space w 2(rn) andany x, y, z ∈ w 2(rn), we have.
w2(y, z)2 ≤ w2(x, y)2 + w2(z, x)2−2 · w2(x, y) · w2(z, x) · cos θ..(17).
here θ is the angel between the γxy and γzx, γxyis the geodesic (shortest path) connecting x, y inw 2(rn), and γzx is the geodesic connecting z, xin w 2(rn).
theorem 3. u and v are given as ﬁxed.
assum-ing that uθ is lipschitz continuous with respect tothe parameters θ. then (cid:96)w d(uθ, v) is lipschitzcontinuous with respect to the parameters θ..roughly speaking, according to sturm et al.
(2006)[proposition 2.10], the sectional curvature ofwasserstein space w 2(rn) is non-negative.
hence,every geodesic triangle in w 2(rn) is fatter thanthe one with same sides length in r2.
as a conse-quence, an inequality like cosine law is satisﬁed onw 2(rn), i.e., theorem 2 holds.
a formal proofof the above two theorems is provided in the ap-pendix.
thus, all our derivations in section.
4 hold.
the exact computation of (cid:96)w d is expensive dur-ing training.
in our experiments, we resort to theinexact proximal point method for optimal trans-port algorithm to compute it (chen et al., 2018a)..5.2 cross-entropy loss.
although wmd is effective for various sequencegeneration tasks, the most conventional loss func-tion adopted in existing generation models is thecross-entropy loss.
it measures the word differenceat each word yi of the output sequence y:.
(cid:96)ce(yi, pi) = yti log(pi)|y|(cid:88).
(cid:96)ce(y, p) =.
(cid:96)ce(yi, pi).
(19).
i=1.
(18).
where yi is the target one-hot vector with the cor-rect dimension as 1 and 0 elsewhere, and pi is thepredicted probability output by a softmax layer.
weadopt the maximum likelihood estimation as thetraining paradigm by assuming truth for precedingwords in predicting pi..the cross-entropy loss is also lipschitz smooth,and thus we can guarantee its convergence fromtheorem 1. unfortunately, it does not satisfy theequation in (3), and thus minimizing our objectivein (4) does not necessarily approximate the dataaugmentation objective in (1).
in our experiments,we also try the cross-entropy loss, and results showthat our objective is effective to improve the modelperformance compared with the base model.
thisis not surprising since our approach is optimizedby gradient weighting and thus at least it is a usefuldata weighting method..6 experiments.
the proposed approach provides a new paradigmand understanding of data augmentation for textgeneration.
to evaluate that our approach canmimic the effect of data augmentation, we con-duct experiments on two text generation tasks –neural machine translation and conversational re-sponse generation.
we compare our approach withtwo most popular data augmentation methods (onetoken-level and one sentence-level augmentationmethod) that can be applied on various text genera-tion tasks:• masked language model (mlm): we use a pre-trained bert (devlin et al., 2019; wolf et al.,2020) and randomly choose 15% of the words foreach sentence.
bert takes in these masked wordsto predict these masked positions with new words.
we augment one sample from each original train-ing sample.
thus the data size increases to twiceof the original one.
note that we only augment theenglish side of translation datasets.
• back-translation (bt): for neural machine trans-lation, we employ a ﬁxed target-to-source transla-tion model trained on the original dataset.
for con-versational response generation, we perturb boththe input and output text of the original sample pairusing two pretrained translation model: an english-to-german model and its backward counterpart,which are obtained using the wmt14 corpus with4.5m sentence pairs2.
we again augment one sam-ple from each original training sample..we set the same weight w of all augmented lossparts used in (cid:96)aug as a hyper-parameter, and tuneit on the development set of each dataset.
sinceeuclidean distance is a special case of wmd as dis-.
2datasets used in this work can be found at https:.
//nlp.stanford.edu/projects/nmt/,http://coai.cs.tsinghua.edu.cn/hml/dataset/#commonsense.
2228de⇒en en⇒de vi⇒en en⇒vi fr⇒en en⇒fr.
model27.98ce28.70ce+mlm29.35ce+bt29.16ce+ourswd28.53wd+mlm 28.8028.56wd+bt28.91wd+ours.
22.8523.2324.0923.2622.9522.9823.1023.42.
24.2224.4025.0024.7424.0324.3324.5124.26.
27.0926.2027.4127.1226.6926.8826.7426.73.
40.4940.0340.8740.4639.7139.5739.7740.46.
40.8640.7942.6440.9440.4840.6140.6041.07.it⇒en en⇒it26.8529.7026.9029.3527.9430.4427.1129.7927.0829.7429.9826.5927.3329.5627.1529.86.table 1: bleu scores on various translation datasets.
ce: cross-entropy loss; wd: l2 wasserstein distance.
thebest results are in bold, and the second-best results are in underline..cussed in sec 5.1, we show results of all methodswith the use of the cross-entropy loss and wd.
wemainly use the fairseq (ott et al., 2019) seq2seqimplementation as our model.
both encoder anddecoder are one-layer lstm.
the word embeddingdimension is 256. attention (luong et al., 2015b)is used with a dropout rate of 0.1. all parametersare randomly initialized based on the uniform dis-tribution [−0.1, +0.1].
we use sgd to optimizeour models, and the learning rate is started with 1.0.after 8 epochs, we start to halve the learning rateafter each epoch.
all experiments are run on a sin-gle nvidia v100 gpu.
code for our experimentsare available once our work is accepted..6.1 neural machine translation.
we use translation benchmarks iwslt14 en–de,en–fr, en–it, and iwslt15 en–vi in our experi-ments.
the datasets of iwslt14 are pre-processedwith the script in fairseq 3. for iwslt14 datasets,we use tst2011 as validation set and tst2012 as testset.
the iwslt15 dataset is the same as that usedin luong et al.
(2015a), and the validation and testsets are tst2012 and tst2013, respectively..table 1 shows the bleu scores on their test sets.
for both cross-entropy loss and l2 wasserstein dis-tance, all data augmentation methods (mlm, btand ours) perform better than the correspond-ing base models in most cases.
the improvementmargins are different across the various datasets.
the reason may be that the datasets are in differentscales and the alignment difﬁculty between dif-ferent languages can also vary.
the performanceof mlm is not stable from our results, which islargely due to that masked tokens are possible to.
3https://github.com/pytorch/fairseq/.
blob/master/examples/translation/prepare-iwslt14.sh.
figure 2: bleu scores by models updated with thesame number of samples..be ﬁlled in with different semantic ones and thusthe semantics of the sentence changes.
therefore,the augmented data are not aligned indeed, andthe translation model learning can be distracted.
note that we also evaluate our method using thetransformer model and get some similar ﬁndings.
experimental results of the transformer model arepresented in the appendix..compared to bt and mlm, our approach thatmimics the effect of data augmentation withoutactually constructing augmented samples, showsencouraging results.
note that our proposed objec-tive may not have a theoretical guarantee on thecross-entropy loss.
yet, it still manages to improvethe base model except for fr⇒en, and surpassesmlm on all datasets.
with the use of l2 wasser-stein distance, our approach even outperforms btand achieves the best performance on half test sets.
this validates the beneﬁts of not using any spe-ciﬁc data augmentation mapping function in dataaugmentation as in our proposed objective..2229024681012141618202224number of samples (× training data size)051015202530bleuende+btende+oursdeen+btdeen+oursmodelppl bleu bleu-1 bleu-2 dist1 dist20.855ce7.220.868ce+mlm 6.820.8517.38ce+bt0.8647.10ce+ours0.863wd7.100.881wd+mlm 7.096.92wd+bt0.8530.8557.01wd+ours0.897-human.
16.3516.6517.0416.4115.0915.7515.9716.56-.
0.8890.9170.8920.8940.8720.9130.8810.8930.947.
1.381.311.331.441.331.251.291.39-.
0.750.760.680.850.870.570.810.84-.
flu3.5713.5523.5573.6323.6443.5753.5793.6294.235.rel3.3143.1843.2493.3703.3543.1883.2793.4474.086.table 2: automatic and human evaluation results on reddit.
human: the gold reference of the query.
the bestresults are in bold, and the second-best results are in underline..range of x-axis.
both bt and our approach demon-strate their robustness under different settings oftheir hyper-parameters..6.2 conversational response generation.
we use the english single-round reddit conversa-tion dataset (zhou et al., 2018).
following previ-ous work on data augmentation for dialogue sys-tem (cai et al., 2020; zhang et al., 2020), we sim-ulate a low data regime so that data augmentationis expected to be more effective.
thus, we selectdata pairs with the length of both the query andresponse less than 20, and randomly split theminto 200k for training, 2k for validation and 5kfor testing.
automatic evaluation for each methodis performed on all test data.
we report perplex-ity, bleu and bleu-k (k=1,2) to measure theresponse coherence; distinct-k (k=1,2) (li et al.,2016) to measure the response diversity.
we alsohire ﬁve annotators from a commercial annotationcompany for manual evaluation on 200 pairs ran-domly sampled from the test set.
results of allmethods are shufﬂed for annotation fairness.
eachannotator rates each response on a 5-point scale(1: not acceptable; 3: acceptable; 5: excellent; 2and 4: used in unsure case) from two perspectives:fluency and relevance..results are summarized in table 2. on auto-matic metrics, bt only shows marginal improve-ments on a few metrics, which can not exhibit itsstrength as in translation tasks.
mlm effectivelyincreases the response diversity (dist1&2).
thisis due to nature of the conversation data that con-versation pair often remains coherent even if thesemantics of the query or response has been slightly.
figure 3: bleu scores by models trained with differ-ent hyper-parameters.
values in the x-axis are re-scaledin order to visualize them in the same range..we provide further analysis on the performanceof our approach versus bt.
in fig.
2, we comparetesting bleu scores obtained by models updatedwith the same number of samples.
since we con-struct one augmented sample from each originaltraining sample, the total number of samples usedin bt is twice as much as that of our approach.
we can see that our approach achieves compatibleperformance with bt, while only requires half ofthe training data.
this shows that our approach,without involving additional calculations on extrasamples, can effectively save the computationalexpense.
fig.
3 shows the sensitivity of perfor-mance under different hyper-parameters.
for ourapproach, we vary across different c1(r)’s; forbt, we vary the sample weight w of the augmentedsamples.
we re-scale c1(r) by 10−4 and w by10−1, in order to visualize them within the same.
223012345678910hyper-parameter value051015202530bleuende+btende+oursdeen+btdeen+ourschanged.
thus, mlm can increase data diversity,which is appreciated in training response genera-tion models.
in terms of human evaluation, btand mlm can barely improve the base model.
asfor our approach, it achieves the best or secondbest results on most metrics for both loss functions,demonstrating more robust performance than btand mlm.
this is consistent with our statementin the introduction that we often need to designproper augmented data mapping functions care-fully for a target generation task, which requiresnon-trivial work.
as such, it is meaningful to avoidthe use of speciﬁc data augmentation techniquesand ﬁnd a uniﬁed formulation of data augmenta-tion for general generation tasks.
from our results,the proposed objective demonstrates its power toachieve the effect of data augmentation across dif-ferent generation tasks..7 conclusions and future work.
we have proposed an objective of formulating dataaugmentation without any use of any augmenteddata mapping function.
we show its optimizationand provide the corresponding convergence rate.
both the l2 wasserstein distance and the cross-entropy loss are discussed with their use in ourobjective and their corresponding theoretical guar-antees.
different from previous data augmenta-tion works that need to add manipulated data intothe training process, our gradient based approachprovides a potential way to obtain performanceimprovements, which may come from augmenteddata, without incurring the computational expense.
experiments on both neural machine translationand conversational response generation validatethe effectiveness of our objective compared to ex-isting popular data augmentation methods: maskedlanguage models and back-translation..we believe this work provides a new understand-ing of data augmentation.
our approach can also beuseful to a wide range of tasks including text clas-siﬁcation tasks, which can be seen as special casesof text generation tasks, and cross-modality gener-ation tasks such as image captioning, in which wecan skip the step to use various image augmentationtechniques..we would like to point out that some parts ofour approach can be improved in the future, whichmay lead to a better performance and generaliza-tion.
firstly, current distributions we choose in there-parameterized loss are relatively simple.
some.
points under current continuous distributions maynot correspond to valid text sequences in the orig-inal text space, due to the discreteness of naturallanguages.
a possible way is that we change toleverage more informative distributions, such as in-cluding prior distributions computed from severalaugmented samples.
secondly, our method is de-rived under the framework of sgd and it is possibleto extend it to the adam framework (kingma andba, 2014; chen et al., 2018b; reddi et al., 2019).
we also leave the more general version of our workin the future..references.
dzmitry bahdanau, kyung hyun cho, and yoshuabengio.
2015. neural machine translation by jointlyin proceedings oflearning to align and translate.
the international conference on learning represen-tations (iclr)..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inproceedings of the international conference on ma-chine learning (icml), pages 41–48..hengyi cai, hongshen chen, yonghao song, chengzhang, xiaofang zhao, and dawei yin.
2020. datamanipulation: towards effective instance learningfor neural dialogue generation via learning to aug-in proceedings of the annualment and reweight.
meeting of the association for computational lin-guistics (acl), pages 6334–6343..liqun chen, yizhe zhang, ruiyi zhang, chenyangtao, zhe gan, haichao zhang, bai li, dinghanshen, changyou chen, and lawrence carin.
2018a.
improving sequence-to-sequence learning via opti-in proceedings of the internationalmal transport.
conference on learning representations (iclr)..xiangyi chen, sijia liu, ruoyu sun, and mingyi hong.
2018b.
on the convergence of a class of adam-type algorithms for non-convex optimization.
arxivpreprint arxiv:1808.02941..yong cheng, lu jiang, and wolfgang macherey.
2019.robust neural machine translation with doubly ad-versarial inputs.
in proceedings of the annual meet-ing of the association for computational linguistics(acl), pages 4324–4333..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 4171–4186..yoav freund and robert e schapire.
1997. a decision-theoretic generalization of on-line learning and an.
2231application to boosting.
journal of computer andsystem sciences, 55(1):119–139..hongyu guo, yongyi mao, and richong zhang.
2019.augmenting data with mixup for sentence clas-arxiv preprintsiﬁcation: an empirical study.
arxiv:1905.08941..zhiting hu, bowen tan, russ r salakhutdinov, tom mmitchell, and eric p xing.
2019. learning data ma-nipulation for augmentation and weighting.
in ad-vances in neural information processing systems(neurips), pages 15764–15775..zhiting hu, zichao yang, xiaodan liang, ruslansalakhutdinov, and eric p xing.
2017. towardin proceedings ofcontrolled generation of text.
the international conference on machine learning(icml), pages 1587–1596..lu jiang, zhengyuan zhou, thomas leung, li-jia li,and li fei-fei.
2018. mentornet: learning data-driven curriculum for very deep neural networksin proceedings of the inter-on corrupted labels.
national conference on machine learning (icml),pages 2304–2313..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic rela-tions.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt), pages 452–457..minh-thang luong, christopher d manning, et al.
2015a.
stanford neural machine translation systemsfor spoken language domains.
in proceedings of theinternational workshop on spoken language trans-lation (iwslt), pages 76–79..minh-thang luong, hieu pham, and christopher dmanning.
2015b.
effective approaches to attention-in proceedingsbased neural machine translation.
of the conference on empirical methods in naturallanguage processing (emnlp), pages 1412–1421..tong niu and mohit bansal.
2019. automaticallylearning data augmentation policies for dialoguetasks.
in proceedings of the conference on empir-ical methods in natural language processing andthe international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1317–1323..mohammad norouzi, samy bengio, navdeep jaitly,mike schuster, yonghui wu, dale schuurmans, et al.
2016. reward augmented maximum likelihood forneural structured prediction.
in advances in neuralinformation processing systems (neurips), pages1723–1731..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies (naacl-hlt)..luis perez and jason wang.
2017. the effectiveness ofdata augmentation in image classiﬁcation using deeplearning.
arxiv preprint arxiv:1712.04621..gakuto kurata, bing xiang, and bowen zhou.
2016.labeled data generation with encoder-decoder lstmfor semantic slot ﬁlling.
in proceddings of the con-ference of the international speech communicationassociation (interspeech), pages 725–729..sashank j reddi, ahmed hefny, suvrit sra, barnabaspoczos, and alex smola.
2016. stochastic variancereduction for nonconvex optimization.
in proceed-ings of the international conference on machinelearning (icml), pages 314–323..matt kusner, yu sun, nicholas kolkin, and kilianweinberger.
2015. from word embeddings to doc-in proceedings of internationalument distances.
conference on machine learning (icml), pages957–966..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting objec-tive function for neural conversation models.
in pro-ceedings of the conference of the north americanchapter of the association for computational lin-guistics: human language technologies (naacl-hlt), pages 110–119..sashank j reddi, satyen kale, and sanjiv kumar.
2019.on the convergence of adam and beyond.
arxivpreprint arxiv:1904.09237..rico sennrich, barry haddow, and alexandra birch.
2016. improving neural machine translation modelsin proceedings of the an-with monolingual data.
nual meeting of the association for computationallinguistics (acl), pages 86–96..connor shorten and taghi m khoshgoftaar.
2019. asurvey on image data augmentation for deep learn-ing.
journal of big data, 6(1):60..tsung-yi lin, priya goyal, ross girshick, kaiminghe, and piotr doll´ar.
2017. focal loss for densein proceedings of the ieee in-object detection.
ternational conference on computer vision (iccv),pages 2980–2988..jun shu, qi xie, lixuan yi, qian zhao, sanpingzhou, zongben xu, and deyu meng.
2019. meta-weight-net: learning an explicit mapping for sam-ple weighting.
in advances in neural informationprocessing systems (neurips), pages 1919–1930..2232karl-theodor sturm et al.
2006. on the geome-try of metric measure spaces.
acta mathematica,196(1):65–131..in proceedings of the inter-with graph attention.
national joint conferences on artiﬁcial intelligence(ijcai), pages 4623–4629..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in the neural informationyou need.
processing systems (neurips), pages 6000–6010..jason wei and kai zou.
2019. eda: easy data aug-mentation techniques for boosting performance onin proceedings of thetext classiﬁcation tasks.
conference on empirical methods in natural lan-guage processing and the international joint con-ference on natural language processing (emnlp-ijcnlp), pages 6383–6389..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the conference on empir-ical methods in natural language processing: sys-tem demonstrations (emnlp), pages 38–45..xing wu, shangwen lv, liangjun zang, jizhonghan, and songlin hu.
2019. conditional bert con-in proceedings of the in-textual augmentation.
ternational conference on computational science(iccs), pages 84–95..ziang xie, sida i wang, jiwei li, daniel l´evy, aimingnie, dan jurafsky, and andrew y ng.
2017. datanoising as smoothing in neural network languagein proceedings of the international con-models.
ference on learning representations (iclr)..rongsheng zhang, yinhe zheng, jianzhi shao, xiaoximao, yadong xi, and minlie huang.
2020. dia-logue distillation: open-domain dialogue augmenta-tion using unpaired data.
in proceedings of the con-ference on empirical methods in natural languageprocessing (emnlp), pages 3449–3460..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
advances in neural information process-ing systems (neurips), 28:649–657..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the conference on empirical methods in naturallanguage processing (emnlp), pages 563–578..hao zhou, tom young, minlie huang, haizhou zhao,jingfang xu, and xiaoyan zhu.
2018.com-monsense knowledge aware conversation generation.
2233l2 + r2 − 2lr cos θdθ +.
l2 + r2 − 2lr cos θdθ)dr.(cid:90) π.
(cid:112).
θ=π/2.
a proof of corollary 1.
(cid:112).
e(r,θ)∈p [(cid:90) π(cid:90) r.l2 + r2 − 2lr cos θ].
l2 + r2 − 2lr cos θdrdθ,.
1r.·.
·.
1π(cid:90) π/2.
(cid:112).
(cid:112).
θ=0.
θ=011πr.·.
(.
=.
=.
≤.
=.
≤.
=.
r=0(cid:90) r.r=0(cid:90) r.r=0121212.
11r2r4r4.
(cid:112).
(.
l2 + r2 + l + r)dr.l +.
+.
l2 + r2dr.
(cid:90) r.(cid:112).
12r12r.
r=0(cid:90) r.r=0.
1 + l2 + r22.dr.l +.
+.
l + l2c1 + c2(r)..where l = (cid:96)(fx,y(x), y), c1 = 1.
4 , c2(r) = r2.
12 + r.4 + 14 ..b proof of corollary 2.
(20).
(21).
(22).
(23).
(cid:90) ∞.
(cid:90) π.
1r.exp(−.
rr.).
1π.
(cid:112)(.
l2 + r2 − 2lr cos θdrdθ.
(cid:112).
l2 + r2 + l + r)dr.θ=0.
r=0(cid:90) r.).
(.
).
=.
≤.
r=0(cid:90) r.r exp(−.
r exp(−.
rrrr.1212r32r32= lc1(r) + l2 c1(r).
r=0r22r22.
(1 − e−1)l +.
(1 − e−1)l +.
=.
≤.
(l + r)dr +.
r exp(−.
).
(.
l2 + r2)dr.(cid:90) r.r=0.
(cid:90) r.r=0(cid:90) r.r=0.
(1 − 2e−1) +.
r exp(−.
(1 − 2e−1) +.
r exp(−.
(cid:112).
rr.12rrrr.l2 + r2)dr.).
).
(.
(cid:112).
121 + l2 + r24.dr.+ c2(r).
2.where c1(r) = (1 − e−1) r2.
2 , and c2(r) = r3.
2 + 3r2.
4 − ( r3.
2 + r4.
2 )e−1..c proof of theorem 1.we study the nonconvex ﬁnite-sum problems of the form.
l(θ) :=.
minθ.
(cid:96)our(θ, xi, yi),.
1n.n(cid:88).
i=1.
where both l and (cid:96)our may be nonconvex.
for ease of notation, we use (cid:96) to denote (cid:96)our in the followingof the proof.
we denote the class of such ﬁnite-sum lipschitz smooth functions by fn.
we optimizefunctions in fn with the gradient in eq.
8 by sgd.
for l ∈ fn, sgd takes an index i ∈ [n] and a samplein the training set, and returns the pair ((cid:96)i(θ), ∇(cid:96)i(θ)).
deﬁnition 1. we say l : rd → r is l-smooth if there is a constant l such that.
||∇(cid:96)(θ(cid:48)) − ∇(cid:96)(θ)|| ≤ l||θ(cid:48) − θ||, ∀θ(cid:48), θ ∈ rd..(24).
2234deﬁnition 2. a point θ is called (cid:15)-accurate if ||∇(cid:96)(θ)||2 ≤ (cid:15).
a stochastic iterative algorithm is said toachieve (cid:15)-accuracy in t iterations if e[||∇(cid:96)(θt)||2] ≤ (cid:15), where the expectation is over the stochasticity ofthe algorithm.
deﬁnition 3. we say (cid:96) ∈ fn has σ-bounded gradients if ||∇(cid:96)i(θ)|| ≤ σ for all i ∈ [n] and θ ∈ rd..let αt denote the learning rate at iteration t, and wit be the gradient weight assigned to sample i by our.
approach.
by sgd, we have.
θt+1 = θt − αtwit∇(cid:96)it(θt), i ∈ [n]..(25).
deﬁnition 4. we say the positive gradient weight w in our approach is bounded if there exist constantsw1 and w2 such that w1 ≤ wi ≤ w2 for all i ∈ [n]..proof of theorem1.
according to the lipschitz continuity of ∇(cid:96), the iterates of our approach satisfy thefollowing bound:.
e[(cid:96)(θt+1)] ≤ e[(cid:96)(θt) + (cid:104)∇(cid:96)(θt), θt+1 − θt(cid:105) +.
||θt+1 − θt||2]..(26).
after substituting (25) into (26), we have:.
e[(cid:96)(θt+1)] ≤ e[(cid:96)(θt)] − αtwte[||∇(cid:96)(θt)||2] +.
e[||∇(cid:96)it(θt)||2].
≤ e[(cid:96)(θt)] − αtwte[||∇(cid:96)(θt)||2] +.
σ2..the ﬁrst inequality follows from the unbiasedness of the stochastic gradient eit[∇(cid:96)it(θt)] = ∇(cid:96)(θt).
the second inequality uses the assumption on gradient boundedness in deﬁnition 3. re-arranging (27)we obtain.
l2.t w2lα2t2t w2lα2t2.e[||∇(cid:96)(θt)||2] ≤.
e[(cid:96)(θt) − (cid:96)(θt+1)] +.
1αtwt.
lαtwt2.σ2..summing (28) from t = 0 to t − 1 and using that αt is a ﬁxed α, we obtain.
e[||∇(cid:96)(θt)||2] ≤.
e[||∇(cid:96)(θt)||2].
mint.t −1(cid:88).
t=0t −1(cid:88).
1t.1t.t=01t αw21t αw21√t.≤.
≤.
≤.
≤.
1αwt.
e[(cid:96)(θt) − (cid:96)(θt+1)] +.
1t.t −1(cid:88).
t=0.
lαwt2.σ2.
(cid:0)(cid:96)(θ0 − (cid:96)(θt )(cid:1) +.
(cid:0)(cid:96)(θ0 − (cid:96)(θ∗)(cid:1) +.
lαw12lαw12.σ2.
σ2.
(cid:18) 1cw2.
((cid:96)(θ0) − (cid:96)(θ∗)) +.
(cid:19).
σ2.
..lcw12.
(27).
(28).
(29).
the ﬁrst step holds because the minimum is less than the average.
the second step is obtained from (28).
the third step follows from the assumption on gradient weight boundedness in deﬁnition 4. the fourthstep is obtained from the fact that (cid:96)(θ∗) ≤ (cid:96)(θt ).
the ﬁnal inequality follows upon using α = c/t .
by setting c =.
in the above inequality, we get the desired result..√.
(cid:113) 2((cid:96)(θ0)−(cid:96)(θ∗))lσ2w1w2.
2235d proof of (cid:96)w d.we begin with some concepts in mathematics.
let (x, | · , · |) be a complete metric space.
deﬁnition 5. a rectiﬁable curve γ(t) : i ⊂ r+ → x connecting two points p, q is called a geodesic ifits length is equal to |p, q| and it has unit speed.
here, we say that γ(t) : i → x has unit speed, if for anys, t ∈ i, s < t, we have, the length of the restriction.
γ : [s, t] → x.is t − s. a metric space x is called a geodesic space if, for every pair of points p, q ∈ x, there existssome geodesic connecting them.
deﬁnition 6. we say that, a geodesic space (x, |· , ·|) has non-negative curvature in the sense of alexan-drov, if it satisﬁes the following property:.
• for any p ∈ x, and for any unit speed geodesics γ(s) : i → x and σ(t) : j → x with.
γ(0) = σ(0) := p, the comparison angle.
(cid:101)∠γ(s)pσ(t) := arccos.
(cid:18) t2 + s2 − |γ(s), σ(t)|22 · s · t.(cid:19).
is non-increasing with respect to each of the variables t and s..the angle between γ and σ at p is deﬁned by.
lims,t→0+.
arccos.
(cid:18) t2 + s2 − |γ(s), σ(t)|22 · s · t.(cid:19).
∈ [0, π]..in other words, every geodesic triangle in x is fatter than the one with sides length in r2 (figure 4)..figure 4: geodesic space with non-negative curvature.
according to sturm et al.
(2006)[proposition 2.10], the wasserstein space w 2(rn) has non-negative.
curvature in the sense of alexandrov.
precisely,lemma 1. sturm et al.
(2006)[proposition 2.10] let n ≥ 1. the wasserstein space w 2(rn) equippedwith the l2 wasserstein distance w2(·, ·) has non-negative curvature in the sense of alexandrov.
proof of theorem 2. let x = w 2(rn) and |· , ·| be the l2 wasserstein distance.
for any x, y, z ∈ x,we denote by γxy (γzx) the geodesic connecting x and y (resp.
z and x).
by the above lemma, x hasnon-negative curvature in the sense of alexandrov, hence according to deﬁnition 6, one can deﬁne theangle between γxy and γzx at x, denoted by θ, and we have.
θ ≥ (cid:101)∠yxz := arccos.
(cid:18) |x, y|2 + |z, x|2 − |y, z|22 · |x, y| · |z, x|.
(cid:19).
,.
cos θ ≤.
|x, y|2 + |z, x|2 − |y, z|22 · |x, y| · |z, x|.
..|y, z|2 ≤ |x, y|2 + |z, x|2 − 2|x, y| · |z, x| · cos θ..which implies.
equivalently,.
hence, we complete the proof..2236proof of theorem 3. we derive from the deﬁnition of (cid:96)w d and the triangle inequality for the l2 wasser-stein distance that for any θ, θ(cid:48),.
(cid:107)(cid:96)w d(uθ, v) − (cid:96)w d(uθ(cid:48), v)(cid:107) ≤ (cid:96)w d(uθ(cid:48), uθ).
1/2w m d(uθ(cid:48), uθ)= (cid:96)1/2.
≤.
.
ti,jdi,j.
.
(cid:88).
i,j.
where ti,j satisﬁes.
(cid:88).
j.ti,j = puθ,i ∀i,.
ti,j = puθ(cid:48) ,j ∀j..(cid:88).
i.take ti,j = δij · puθ,i.
according to the assumption that uθ is lipschitz continuous with respect to theparameters θ, we have.
di,i = (cid:107)uθ,i − uθ(cid:48),i(cid:107)2 ≤ l · (cid:107)θ(cid:48) − θ(cid:107)2.for some constant l > 0. hence, we get that.
.
1/2.
(cid:32).
(cid:88).
ti,jdi,j.
.
≤.
.
.
(cid:88).
i,j.
ti,i · l · (cid:107)θ(cid:48) − θ(cid:107)2.
(cid:33)1/2.
i.i.
(cid:32).
(cid:88).
=.
ti,i.
(cid:33)1/2.
· l1/2 · (cid:107)θ(cid:48) − θ(cid:107).
= l1/2 · (cid:107)θ(cid:48) − θ(cid:107)..finally, we got.
(cid:107)(cid:96)w d(uθ, v) − (cid:96)w d(uθ(cid:48), v)(cid:107) ≤ l1/2 · (cid:107)θ(cid:48) − θ(cid:107)..hence, we complete the proof..e experimental results of transformer.
we also evaluate our method using the transformer architecture on two translation tasks.
to preventthe model from over-ﬁtting, we use a transformer model with a 2-layer encoder and a 2-layer decoder.
other hyper-parameters are almost the same as in vaswani et al.
(2017), except for the optimizer.
in ourexperiment, we use sgd to train the model, instead of adam (vaswani et al., 2017), since our approachis derived under sgd.
results are shown in table 3, which are consistent with the observations fromthe lstm model.
we hope that our approach and theoretical analysis can be extended to the adamframework (kingma and ba, 2014; chen et al., 2018b; reddi et al., 2019) in the future..model29.18ce29.20ce+mlm30.01ce+bt29.25ce+ourswd28.60wd+mlm 29.0228.92wd+bt29.51wd+ours.
de⇒en en⇒de vi⇒en en⇒vi26.0225.9727.6226.8426.4326.1326.3826.66.
25.0425.6825.7725.4924.7925.0824.8825.11.
24.3624.4025.4524.6224.3824.4924.8224.96.table 3: bleu scores on two translation datasets using the transformer model.
ce: cross-entropy loss; wd: l2wasserstein distance.
the best results are in bold, and the second-best results are in underline..2237