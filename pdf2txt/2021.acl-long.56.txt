improving dialog systems for negotiation with personality modeling.
runzhe yang∗princeton universityrunzhey@princeton.edu.
jingxiao chen∗shanghai jiao tong universitytimemachine@sjtu.edu.cn.
karthik narasimhanprinceton universitykarthikn@princeton.edu.
abstract.
in this paper, we explore the ability to modeland infer personality types of opponents, pre-dict their responses, and use this informationto adapt a dialog agent’s high-level strategy innegotiation tasks.
inspired by the idea of in-corporating a theory of mind (tom) into ma-chines, we introduce a probabilistic formula-tion to encapsulate the opponent’s personal-ity type during both learning and inference.
we test our approach on the craigslistbar-gain dataset (he et al., 2018) and show thatour method using tom inference achieves a20% higher dialog agreement rate compared tobaselines on a mixed population of opponents.
we also ﬁnd that our model displays diversenegotiation behavior with different types of op-ponents.1.
1.introduction.
developing dialog systems for negotiation is chal-lenging since the task requires a combination ofgood communication skills and strategic reasoningcapabilities (traum et al., 2008; young et al., 2013;keizer et al., 2017).
while recent neural models(wen et al., 2017; dhingra et al., 2017; zhou et al.,2019; he et al., 2018) have shown that useful dia-logue strategies can be learned from ofﬂine corpora,they do not explicitly model the mental state ofother agents, which can make it challenging to gen-erate tailored strategies and utterances for differenttypes of opponents..in this paper, we introduce a new frameworkfor generating strategic dialog inspired by theidea of theory of mind (tom) from cognitive sci-ence (premack and woodruff, 1978; bruner, 1981;wimmer and perner, 1983).
when negotiating withothers, humans innately infer the intention of the.
∗authors contributed equally.
1code and data available at https://github.com/.
princeton-nlp/negotiationtom.
other party, and guess how their own utteranceswould affect the opponent’s mental state.
to em-ulate this capability in machines, we train a ﬁrst-order tom model to predict an opponent’s responsegiven the current state and the agent’s own possibleutterances.
this ﬁrst-order tom model can then beincorporated into dialog agents to enable one-steplookaheads during inference..in order to predict future responses, we modelthe opponent’s personality type as a intermediatevariable (z), which can be predicted using the di-alogue history.
we use this predicted personality,along with the previous state and utterance to cal-culate the likelihood of the opponent’s next statefor all possible actions that our agent can take inthe current state.
this allows us to compute anexpected value of return for each action, whichis subsequently used to produce a policy for ouragent.
we propose two variants of our tom-baseddialog agent – an explicit version that outputs theopponent type as an intermediate prediction, and animplicit version that models the opponent type asa latent variable.
both models can be instantiatedas end-to-end neural networks and can be trainedusing reinforcement learning..our approach differs from existing opponentmodeling work (lee et al., 2018; hadjinikolis et al.,2013; oren and norman, 2009; rienstra et al.,2013; he and boyd-graber, 2016) in three aspects:1) it provides strategic beneﬁt during inferencewhich leads to more successful negotiations, 2)it can ﬂexibly adjust the degree of dependence ontom predictions by changing a temperature param-eter, and 3) it utilizes text utterances to infer typesof opponents, thereby capturing side information(e.g., emotion) that is useful yet absent from stan-dard dialog state transitions..we perform experiments on a modiﬁed versionof the craigslistbargain negotiation task (heet al., 2018), where the agent is matched with dif-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages681–693august1–6,2021.©2021associationforcomputationallinguistics681ferent opponents from diverse populations (e.g.,cooperative, competitive, and aggressive negotia-tors), without being provided information abouttheir identity.
empirically, our method outperformsseveral baselines on the task by completing moredeals and achieving higher utility.
for instance,our model achieves about 20% higher dialog agree-ment rate and utility than a baseline dialog managertrained with reinforcement learning.
our analysisreveals that the agent demonstrates diverse negotia-tion behavior and adapts well to different types ofopponents..2 related work.
speaker-follower models and rational speechacts.
our work is related to recent papers usingthe rational speech acts (rsa) model for naturallanguage (goodman and stuhlm¨uller, 2013; mon-roe and potts, 2015; goodman and frank, 2016;shen et al., 2019).
rsa has also been appliedto language grounding (andreas and klein, 2016)and vision-language navigation (fried et al., 2018).
our ﬁrst-order theory of mind modeling is differ-ent since we learn how the speaker’s intent andutterance affect the opponent’s reaction, insteadof assuming the optimality of the listener in thespeaker’s mind.
recent rsa model (white et al.,2020) considers speakers and listeners in resource-constrained settings, while we do not enforce con-straints on opponents..our approach with explicit characteristic model-ing is also similar to the tomnet (rabinowitz et al.,2018), which uses a multi-agent reinforcementlearning setting to learn identity embeddings ofpopulations from past trajectories, and predict themental state of an agent using the current trajectory.
however, our ﬁrst-order tom models for negotia-tion also take utterances into account, which makesimproving upon a base rl policy non-trivial..theory of mind in dialog systems.
theory ofmind for modeling user personality types and pre-dicting responses has been studied in the contextof building user simulators (georgila et al., 2006;rieser and lemon, 2006) for training rl-baseddialog systems, and to make dialog systems ex-plainable (chandrasekaran et al., 2017).
recentwork on dialog policy learning has employed the-ory of mind with a focus on speciﬁc domains.
therecursive mental model (rmm) (roman et al.,2020) was proposed for navigation settings, wherequestions and answers are generated between a.navigating agent and a guiding agent.
another ap-proach – answerer in questioner’s mind (aqm)(lee et al., 2018) – tackled an answer guessinggame with information-theoretic methods.
in thesedomains, the opponents are assumed to be coopera-tive, while our method is applicable for interactingwith both cooperative and competitive opponents.
recently, jang et al.
(2020) employed bayesian-optimal monte-carlo planning for end-to-end di-alog generation at the utterance level.
however,their method only models the latent goal of theopponent instead of potential responses like we do..opponent modeling in rl.
apart from dialogsystems, opponent modeling has been explored inother multi-agent reinforcement learning settings(wen et al., 2019; von der osten et al., 2017; heand boyd-graber, 2016; hadjinikolis et al., 2013;rienstra et al., 2013).
our approach differs fromthese works by: 1) providing strategic beneﬁt dur-ing real-time inference, 2) adjusting the degree ofdependence on the tom predictions through a tem-perature parameter, and 3) utilizing text utterancesin the dialog to infer types of opponents, therebycapturing side information that is useful yet absentfrom standard state transitions..3 framework.
task.
we consider a task-oriented dialog settingwhere there are two agents, a buyer and a seller.
the buyer’s goal is to purchase the listed item withminimum cost, and the seller’s goal is to sell theitem at a price as high as possible.
the item de-scription is public for both agents, while the targetprices are private for both buyer and seller.
twoagents negotiate in alternating turns until they con-clude with an agreement or disagreement..mdp formulation.
we formulate the negoti-ation process between two agents as a multi-agent markov decision process (mamdp),(cid:104)n , s, a, p, r, π, n(cid:105).
n = {−1, 1} is the setindicating two agents (buyer=-1 / seller = 1).
ais the action space consisting of dialog acts.
forexample, a valid dialog act ait ∈ a can encode theintent (inform, propose, counter, etc.)
and pricethat the agent i tries to express in the t-th round.
two agents act alternatively, i.e., if at the round tonly the agent i moves, then at the round t + 1 onlythe agent −i moves..s is the state space consisting of the negotiationstatus.
we deﬁne s0 ∈ s as the initial status of thedialog, which contains the information about items.
682figure 1: our theory of mind (tom) framework of negotiation systems.
the interaction between a buyer and aseller can be divided into three levels: the utterance level, dialog act level, and state level.
the parser extracts anintent and key information (e.g., price) from an input utterance as a dialog act.
both intents and key information,along with the context (e.g.
description about the item), contribute to the state of dialog.
the traditional rl-baseddialog manager decides a dialog act based on the current state.
and the generator converts the abstract dialogact back to a natural language utterance, also based on the previous state.
the ﬁrst-order tom model explicitlypredicts the response of the opponent and the state transition, which supports more strategic negotiation..1, a−i.
t−1, a−i.
2 , .
.
.
, ai.
to be negotiated (e.g., initial price, description).
wealso deﬁne st = (s0, ait ).
inthis way, the only randomness of the environmentcomes from the opponents policy (st−1 → a−it ),i.e., st−1 → st is stochastic, while (st−1, a−it ) →st is deterministic.
note that the state st is onlypartially observable in reality, since one can onlyinfer the true intent from the corresponding utter-ance.
we provide a summary of all the symbolsused in table 1..3.1 negotiation systems.
as illustrated in figure 1, our negotiation systemencapsulates three important modules followingtraditional goal-oriented dialog systems (younget al., 2013):.
• a parser that converts the opponent’s ut-terance u−it−1 to dialog act a−it−1 (e.g., “areyou interested in this gopro” → con-ﬁrm(price=none)).
since the dialog actsin our system do not intend to capture thecomplete semantics of a sentence, a simplerule-based parser is effective;.
• a manager that decides the responding dia-t according to the current dialog statet−1).
our tom model is.
log act aist−1 = (s0, .
.
.
, a−iapplied to this component of the system;.
• a generator that produces natural languageresponse uit based on the current dialog actait and the dialog state st−1, or equivalentlyst (e.g.,the previous dialog state + pro-pose(price=$230) → “how does $230 forthe gopro sound?”).
it can be either deter-ministic to reduce computational cost or prob-abilistic to encourage diversity in language..following (he et al., 2018), the parser andthe generator modules are obtained by rule-basedmethod or supervised learning in advance, andﬁxed when training the dialog manager using su-pervised learning (sl) or ﬁne turning using rein-forcement learning (rl).
the sl dialog manageremploys a neural network to model state transi-tions p (st|st−1) (or equivalently, π(ait|st−1)) ofthe training corpus by minimizing the cross en-tropy loss.
the rl dialog manager further ﬁnetunes the sl model by maximizing a composedreward function with reinforcement learning.
thelearned dialog policy π(ait|st−1) can be further im-proved by enforcing some hand-craft rules..there are two main problems with the sl orrl manager.
first, the policy learned by an rl-based dialog manager produces reactive responses(tamar et al., 2016) , which are usually inadequatein a long term planning problem requiring more.
683•hero4 black camera •standard housing 131', •rechargeable battery, •flat adhesive mount, •3-way pivot armgopro hero4 black  + battery bacpacprice:  $265buyer: yes, i am  interested.target: $243buyer: how does $230  for it sound?atstpropose  (price = $230)counter  (price = $255)generatorparserst+1st-1at+1agree (price = $230)p=0.01…p=0.15…st+1at+1s0a2afﬁrm (price = none)a1conﬁrm (price = none)generatortomseller: are you interested in this gopro?s1s2managertom…………utterance levelstate levelseller’s actsbuyer’s actsseller: it is in very good condition.s3a3inform (price = none)parsersymbol.
deﬁnition.
n = {−1, 1}.
s0 ∈ s.ait ∈ a.st ∈ s.p(st|st−1).
πi(ai.
t|st−1).
r = {r−i, ri}.
n.uit.z−i.
1, a−i.
identities of the two players(buyer = -1 / seller = 1)initial state of the dialog (e.g.,list price, description).
dialog act of agent i. if −i, itdenotes the opponent.
state at the end of round t, st :=(s0, aitransition probability, associ-ated with agent policiesprobability of the agent i choos-ing ait given the previous dialogstate.
reward functions for agents iand −i.
maximum length of dialogs..2 , .
.
.
, ai.
t−1, a−it ).
utterance of agent i. if −i, itdenotes the opponent.
type the opponent.
annotationsare available in the corpus..table 1: notation for our mamdp formulation of task-oriented dialog for negotiation..strategic thinking, such as negotiation.
second,it does not take the effect of the agent’s generatedutterances on opponents’ reactions into account.
toaddress these problems, we propose an approachto incorporate the theory of mind (tom) (premackand woodruff, 1978) into the inference process.
this enables one-step looking ahead to considerthe effect of the agent’s utterances and generatemore thoughtful strategies..4 first-order theory of mind for dialog.
the goal of the ﬁrst-order theory of mind is topredict how a dialog act and an utterance gener-ated by us would affect the reaction of the op-ponent.
as illustrated in figure 1, suppose thatour current dialog state is st−1, which consists ofthe history of past dialog acts and the initial in-formation, as well as the current utterance u−it−1from the opponent.
the tom model simulatesthe situations where we take dialog act ait (e.g.,propose(price=$230)) and utter the sentence uit(“how does $ 230 for it sound”), and estimatesthe probability distribution of the opponents re-sponse ait+1.
by combining actions and states bydeﬁnition, our ﬁrst-order tom model estimates thetransition probability t (st+1|u−i.
t−1, st, ui.
t)..in practice, the opponent may have different lan-guage preferences (e.g., using more aggressive or.
t−1, st, ui.
mild words when countering) and strategies (e.g.,tend to insist on their target price or agree to a com-promise).
the ﬁrst-order tom can either implicitlycapture these personalities by learning the tran-sition t (st+1|u−it), or explicitly infer thetype of the opponent’s personalities z−i ﬁrst, fromthe past interaction and the opponent’s utterance,i.e., learning an identiﬁer z−it−1),and then learns the transition based on that infor-mation, i.e., t (st+1|z−it−1, st, uit), to make accurateprediction about opponents reaction..t−1 = f (st−1, u−i.
4.1 first-order tom policies with explicit.
personality modeling.
we introduce a policy with an explicit ﬁrst-ordertom model t (st+1|z−i, st, uit), where the oppo-nent’s personality z−i can be estimated from partialdialog.
during training, the ground truth of the typeof opponents personalities, z, is given.
thereforewe can train an identiﬁer z−it−1)with extra supervision to predict the opponentstype every round.
during the inference process,the probability of taking action ait, i.e., a policyπtom(ai.
t−1 = f (st−1, u−i.
t−1), is proportional to.
t|st−1, z−i.
.
.
1β.exp.
(cid:88).
uit.g(ui(cid:124).
t|st, z−i(cid:123)(cid:122)generator.
t−1)(cid:125).
(cid:88).
st+1.
t (st+1|z−it−1, st, uit)(cid:123)(cid:122)(cid:125)(cid:124)1st-order tom.
v (st+1)(cid:124) (cid:123)(cid:122) (cid:125)value fn...
.
,.
t|st, z−i.
where the exponent can be interpreted as the ex-pected best return over opponent’s next moves, af-ter taking action ait at state st−1 (compressed asst).
in the above expression, t (st+1|z−it−1, st, uit)is the explicit ﬁrst-order tom model, which canbe trained by supervised learning from the corpus;g(uit−1) is the generator which renders ut-terance conditioned on the current state and thepersonality of the opponent; v (st+1), is the valuefunction estimated by the rl-based dialog man-ager, which gives the best future return estimationsupposing the current state is st+1.
it approximatesv (st+1, z−it−1) when it is nearly optimal.
β is thetemperature parameter.
since πtom is normal-ized as a boltzmann distribution, when tempera-ture β → ∞, πtom is a uniform distribution overthe next states; when β → 0, πtom is nearly deter-ministic assigning most probability mass to the stwith the largest expected value after one-step tomlooking ahead..6844.2 first-order tom policies with implicit.
personality modeling.
we also introduce ﬁrst-order tom policy with im-plicit personality modeling, where we do not havea module explicitly which explicitly predicts theopponent identity z. instead, we combine the iden-tiﬁer and tom model in the explicit version, todirectly learn t (st+1|u−it) without extrat|st−1, u−it−1) issupervision.
proportional to.
t−1, st, uiin this case, πtom(ai.
.
.
1β.exp.
(cid:88).
uit.g(uit|st)(cid:124) (cid:123)(cid:122) (cid:125)generator.
(cid:88).
st+1.
t (st+1|u−it−1, st, uit)(cid:124)(cid:125)(cid:123)(cid:122)1st-order tom.
v (st+1)(cid:124) (cid:123)(cid:122) (cid:125)value fn...
.
,.
t−1, st, ui.
where t (st+1|u−it) is called the implicitﬁrst-order tom model, and the rest of compo-nents are similar to the explicit version..we call πtom a ﬁrst-order tom policy, becauseit utilizes the ﬁrst-order transition of the opponent,and estimates the expected outcome of performinga certain action which leads to state st. the person-alities of the opponent are implicitly inferred fromthe previous utterance u−it−1 and the history st. inpractice, the summation (expectation) is approxi-mated by monte carlo sampling..t−1, st, ui.
implicit vs explicit model.
we expect both ex-plicit and implicit tom models to provide sev-eral unique beneﬁts.
first, co-training the iden-tiﬁer f (st−1, u−it−1) and the explicit ﬁrst-ordertom model t (st+1|z−i, st, uit) is expected to havebetter sample efﬁciency than the implicit tommodel t (st+1|u−it) since it utilizes theprior knowledge that personality identity affectsstate transition, and is trained with more supervi-sion.
besides, with the personality z−i, the gen-erator and the value functions can also adapt todifferent populations of opponents.
however, theannotations for opponent types are not available forall corpora, therefore the implicit model would bea more general approach..4.3 combining the rl policy as a prior.
after learning the above two tom models from thecorpus, we leverage the pre-trained rl policy as aprior with the 1st-order tom policy to perform theinference.
the ﬁnal policy is given by.
π(ai.
t|st−1, z−i.
t−1) ∝ πrl(ai.
t|st−1) · πtom(ai.
t|st−1, z−i.
t−1),.
where πrl is a policy obtained in a previous rltraining process (see section 5)..from a bayesian point of view, πrl can be seenas a prior p(ait|st−1), and the πtom is analog to thelikelihood p(best return|ait, st−1) by its deﬁnition(not strictly true since it has to be summed up toone) which modiﬁes the probability assignment inπrl, i.e., the posterior p(ait|best return, st−1).
thisgives the probability that the current agent shouldmove to st in order to reach the highest return inthe end.
πtom modiﬁes the probability assignmentin πrl, when β → ∞ in πtom, it is equivalent to theoriginal rl policy πrl..5 dialog managers.
we compare three hybrid dialog managers com-bining neural networks and rules to control the ﬂowof dialog:.
(1) the sl+rule manager employs a lstm-based network to learn the transitions fromst−1 to st from corpus.
rules ensure that onlydeals meeting 70% target are acceptable..(2) the rl manager uses an actor-critic method(mnih et al., 2016), which contains a policynetwork with the same neural network archi-tecture as the sl manager, and a value net-work predicts the future returns given states..(3) the tom manager uses the ﬁrst-order tomto learnpolicy as described in section 4.the best response policy πtom(ait−1)which is aware of the opponent’s personalitiesand mental state..t|st−1, u−i.
an extra lstm model is used to encode u−1t−1 inboth explicit and implicit tom models, and learnt−1 = lstm(u−1the personality z−it−1, st−1) in ex-plicit tom models which encodes a distribution.
note that for all three managers, we applied rea-sonable hand-crafted rules to prevent unreasonablepolicies.
speciﬁcally, the agent will never offera price below its bottom line and will reject theopponent’s offer if it is worse than its bottom line..training and fine tuning.
we ﬁrst train the su-pervised learning (sl) manager to minimize a lossfunction for the dialog act predictions.
lsl = ceintent + α · mseprice,.
which is a linear combination of the cross entropyloss between the predicted intent and the groundtruth intent, and the mean squared error betweenthe predicted price and the ground truth price.
the.
685dialog act.
deﬁnition.
example.
greetinquire.
inform.
propose(price=).
counter(price=).
say hello or chat randomly.
ask any question about product, year, price,usage, etc.
provide information about the product, year,usage, etc.
initiate a price or a price range for the prod-uct.
propose a new price or a new price range (canbe the same)..counter-noprice.
want to propose a new price but do speciﬁ-cally mention a new price..conﬁrm.
aﬃrm.
deny.
agree(price=)disagree(price=).
ask question about with information to beconﬁrmed.
give an afﬁrmative response to a con-ﬁrm/propose.
giveﬁrm/propose.
make a deal.
cannot make a deal..a negative.
response.
to a.con-.
oﬀer(price=)acceptrejectquit.
ﬁnal offer with price, no utteranceﬁnal acceptance, no utteranceﬁnal rejection, no utteranceleave the negotiation, no utterance.
“hello i am interested in buying.”“how long have you had it?
does it come with any ofthe accessories?”“it has the capability to offer great support for thoseover 6 foot.”“the list price is 600 but am willing to negotiate.”.
“i’m sorry, i ﬁnd homeopathy and any other pseudo-science to be profoundly upsetting as well.
i’d bewilling to go as high as $5100.”“i’m sorry, that’s far too low.
since you know ec’sreputation for quality, you know it’s worth more thanthat.”“will the chair work for someone who is under 6 feettall?”“yes absolutely the interface is quick and the phone isup to date as far the updates go.”“no, i would expect that you would pick it up.”.
“fair enough.
$30 it is!”“i cannot take $65 for something that is worth almosttwice that.
sorry, but no deal.”offer($65)acceptrejectquit.
table 2: our redesigned dialog acts based on the craigslistbargin dataset, where propose, counter, agree,disagree are four intents must be followed by a price slot, oﬀer, accept, reject, and quit are four terminal dialogacts with no corresponding natural language..reinforcement learning (rl) manager is then ﬁnedtuned from the sl manager to maximize a rewardfunction described in section 6, with the actor-critic methods (mnih et al., 2016).
the actor net-work is initialized as the sl manager’s lstm-based network, and the critic network is partiallyinitialized with the same network, followed by amlp to predict the value..for the tom manager, we reuse v (st+1) froma well trained rl manager’s critic network, andﬁx it during inference.
the implicit ﬁrst-ordertom model t (st+1|u−it−1, st, uit) is directly trainedvia supervised learning to minimize the same losslsl.
for the explicit ﬁrst-order tom model,t (st+1|z−it), we ﬁrst train a lstm-basedidentiﬁer z−it−1), which receivesground truth opponent personality z−i from the cor-pus during training.
t (st+1|z−it) is learnedwith the input from the well-trained identiﬁer..t−1 = f (st−1, u−i.
t−1, st, ui.
t−1, st, ui.
to obtain the 1st-order tom policy for the in-ference, we approximate the sum (expectation) inπtom by monte carlo sampling with the generator,and discretize the price in a normalized price range.
in practice, we found quantizing the price rangewith 100 units is a good balance between time com-sumption and the quality of approximation..6 experimental setup.
we test our tom negotiation framework on thecraigslistbargain (he et al., 2018), whichcontains 6682 human-human dialogs between abuyer and a seller alternately bargaining for theprice of an item on craigslist..ontology.
we redesign the ontology of thecraigslistbargain dataset to support a morediverse dialog act than the original coarse dialogacts (he et al., 2018), which can reﬂect more waysof mental state change in a negotiation.
we usedthe microsoft language understanding intelligentservice (luis) to relabel the dataset , and mergedsome similar label types, such as insist and vague-price into counter-noprice, and intro and greatinto greet.
all ﬁfteen dialog acts after our mod-iﬁcations are in table 2. there are four intentspropose, counter, agree, disagree that must befollowed by a price slot, and four terminal actsaccept, reject, and quit.
when an agent takes anoﬀer action, the other agent has to respond withaccept or reject.
note that the function of this di-alog act is not to capture the full semantic meaningof one utterance, but to serve as a logical skeletonfor the dialog..686reward function design.
we set the reward rifor the agent i to be a linear function of the ﬁnalprice, such that the buyer achieves maximal rewardof 1 at its target price, the seller achieves maximalreward of 1 at the listing price, and both agentsreceive zero rewards at the midpoint of the listingprice and the target price.
when there is no deal,both agents receives equivalent penalty..diverse opponent populations.
all our negotia-tion experiments are conducted against variationsof the sl+rule manager as the opponent.
for thevariations, we create 7 different opponent popu-lations (id=0∼6) by injecting different rules forchanging prices and rendering utterances.
pricechanging rules are functions of the number of sen-tences in the conversation history, which model theagreeability and the ﬂexibility of a person.
whenrendering utterances, we use a template-based lan-guage generator as in (he et al., 2018), and insertpopulation-speciﬁc tokens in utterances by sam-pling according to different opponent types..the cooperative population (id=5) will gradu-ally compromise and move its price from the mid-point.
the utterances of this population also con-tain more polite and mild words indicating its ne-gotiable position.
the most aggressive population(id=0) will insist its price until the end, and uttersmore stubborn words.
the competitive population(id=6) compromises from target price slower thanthe cooperative.
the other populations will followprice changing curves in between these two ex-tremes, and also have different language properties.
the population types are accessible during trainingas ground truth values of zi to provide supervision(see appendix a for details)..models.
the dialog managers we compare aredescribed in section 5. for the utterance parser,we use microsoft language understanding intel-ligent service (luis) (williams et al., 2015) with10 annotated training examples for each dialog act.
for the generator, we use a retrieval-based modelsimilar to he et al., 2018 which samples an utter-ance from the top 10 matched templates..evaluation metrics.
we evaluate generated di-alogs across four aspects:.
1. agreement rate (ag), which is the percent-.
age of dialogs that reach agreements..2. objective utility (ut), which is given by.
(cid:26)(pdeal − p −i.
target)/∆p, deal;.
uti =.
0,.no deal.
target − p −i.
where pdeal is the ﬁnal deal price, the andtotal price range ∆p = p itarget,target, and p −iwhere p itarget are the extreme tar-get prices of the two agents.
note that thisis different from the subjective utility of eachagent based on only its own price range, whichmay result in utilities > 1 or < 0 more often..3. deal fairness (fa), which is only for com-pleted deals, as fai = 1 − 2 ∗ |uti − 0.5|..4. dialog length (len), which is the average.
turns of sample dialogs..7 results.
improvement of dialog policy.
we evaluatesl+rule, rl, and our tom model on a mixedpopulation for 4352 dialogs, which contains about630 dialogs for each population.
as shown in table3, our explicit tom model consistently achievesthe highest agreement rate (ag), with 56%, 4%,and 20% improvements compared to vanilla rlagainst cooperative, competitive, and mixed pop-ulations, respectively.
though deal agreement ishard for competitive opponents, our explicit tommodel achieves more than 30% improvement onthe deal utility when interacting with this popula-tion.
on the mixed population, the reward (re)for sl+rule agent is low, as it is not directly opti-mized for better reward.
rl agent improves the rea lot compared with the sl+rule baseline.
how-ever, both tom agents achieve better reward evenwhen compared with rl agent, which shows theadvantage of strategic planning.
besides, unlikethe sl+rule only pursues high utility when there isa deal, but ends with every low ag, our tom mod-els best balance both the agreement rate and agentutility of each dialog, and outperforms sl+ruleand rl for all populations..implicit vs. explicit models.
we found that theimplicit tom model can also achieve better agand ut than the baselines for all populations.
butthe overall performance is slightly worse than theexplicit tom model.
this can be explained by thefact that the explicit model has more informationabout the population type during training.
onemay worry about the potential error cascade issuethe explicit tom models, as we see in figure 2,.
687method.
cooperative opponents (id=5).
competitive opponents (id=6).
mixed population (id=0∼6).
ag ↑ ut ↑.
fa ↑ len ↓ ag ↑ ut ↑.
fa ↑ len ↓ ag ↑ ut ↑.
fa ↑ len ↓ re ↑.
sl+rulerltom (implicit)tom (explicit).
0.0060.570.760.88.
0.0060.570.720.78.
0.000.000.000.03.
10.5115.4813.1411.34.
0.0050.420.340.44.
0.0050.180.200.24.
0.000.320.450.55.
10.6416.1014.2612.10.
0.0090.470.480.56.
0.0080.380.440.47.
0.000.000.000.10.
10.59 −0.480.0015.7913.870.150.1611.74.table 3: agreement rate (ag), agent utility (ut), deal fairness (fa), dialog length (len), and reward (re) for dialogmanagers (sl, rl, implicit and explicit tom, with the best β = 0.05) playing against cooperative, competitive,and mixed populations.
small negative fairness scores are truncated as zeros in the table..item: phone, listing price: $50a: tom(buyer), bottom: $50, top $38b: cooperative(seller), bottom: $35, top: $50.
item: furniture, listing price: $50a: tom(buyer), bottom: $50, top: $38b: competitive(seller), bottom: $35, top: $50.
a: hello are you selling the blackberry?
b: sure thing!
what price are you thinking?
a: absolutely, but the price is a little higher than iwould likeb: my phone is in excellent condition and priced to sellquickly.
a: would you be willing to take $38.
b: yea i can do $44 for it.
you come pick it up from methough.
in person.
thanksa: i think so, too!
sounds great!
b: offer($43) a: accept.
a: hi.
how are you?
b: maybe.
what do you have in mind?
a: how about $38 dollarsb: look, this is an expensive chair if you are willing to buyit somewhere else.
i-m only selling it cause i’m movingout of the country.otherwise i’d keep it but i don’t wantto just give it away, you know?
how about $49?
a: ok. i can do $46 if you split the shipping in halfb: if you come pick it up today, that sounds fair.
a: sounds fair to me, thank youb: offer($48) a: accept.
table 4: example dialogs generated by tom (explicit) against cooperative (left) / competitive (right) opponents..the top 1 accuracy of the identiﬁer in the explicitmodel is only 69%, though it is signiﬁcantly abovethe chance.
our experiment show that even withan imperfect identiﬁer, the explicit model can stilloutperform an implicit model, which is directlyoptimized for better performance..population-aware strategies.
as table 3 shows,the tom model can provide more deal fairness(fa, normalized price difference to the midpoint)to competitive opponents, since they rarely com-promise, meanwhile reaching higher ag and ut.
when opponents are cooperative and easy to nego-tiate with, our tom model can achieve much betteragent utility by taking advantage of losing some di-alog fairness.
this implies our tom model is ableto utilize different characteristics of the opponentsin the strategy generation..we provide some sample dialogs from the ex-plicit tom model in table 4. when the seller iscompetitive, the buyer can adaptively raise its priceand exchange for additional beneﬁts, e.g., “ok.
ican do $46 if you split the shipping in half” , tomake the deal happen.
we note that sometimes theoffer prices slightly deviate from the agreed princein negotiation but the tom agent still accepts.
thismay be because the deﬂects of sl-based opponentsis predictable to the tom agent..figure 2: top 1 and top 3 accuracy of the characteristicidentiﬁer (z) during interaction with opponents..effectiveness of the opponent identiﬁer.
fig-ure 2 shows the identiﬁer can capture the opponentidentities well during interaction.
the accuracyof the identiﬁer increases as the dialog progresses.
the top 1 accuracy after 6 opponent’s turns is above69%, and the top 3 accuracy is above 84%, wherethe chance is only 14.2%.
the average top 1 ac-curacy is 43.8% for all turns in 5000 dialogs ofdifferent lengths.
we also ﬁnd the explicit tommodels can better prevent overﬁtting than implicitmodels.
more details are in appendix b..visualization of population embeddings.
infigure 3, we show the pca visualization of the.
688# opponent’s turnsavg.top 1top 3randomtroduced a probabilistic formulation for ﬁrst-ordertom and introduce two ways to incorporate it into adialog agent, by 1) explicitly and 2) implicitly mod-eling the personality of the opponent.
we tested ourapproach on a modiﬁed version of the craigslist-bargain dataset (he et al., 2018) with diverseopponents.
our experiments show that our methodusing tom inference achieves about 20% higherdialog agreement rate and utility compared to base-lines on a mixed population of opponents.
whennegotiating with the cooperative opponents, the im-provement of agreement rate is 54%.
some direc-tions for future work include developing efﬁcientschemes to approximate the value computation forfuture states, exploring higher orders of tom, aswell as a tighter integration of tom into utterancegeneration and processing..ethical considerations.
our dataset is modiﬁed from the open-sourcedcraigslistbargain dataset (he et al., 2018),which consists of negotiation dialogs between sell-ers and buyers on items from the craigslist website.
the initial dataset was collected using crowd work-ers on amazon mechanical turk (amt) playingthe role of buyers and sellers.
we redesigned theontology to support more diverse dialog acts thanthe original coarse dialog acts.
we manually la-beled 10 examples for each intent, and used themicrosoft language understanding intelligent ser-vice to relabel the whole dataset.
we create sevendifferent populations by injecting different rulesabout changing prices and rendering utterances..our paper involves an nlp application that cannegotiate with people to reach agreement on deals.
it is still at an early exploration stage so we do notexpect it will currently cause any negative socialimpact such as massive job loss.
if a mature versionof such a system is deployed in the future, it maylead to less fair deals between the ai system andhumans, as the system is optimized to ﬁnd the beststrategy that maximizes its own utility.
but overall,we believe it will encourage market efﬁciency..acknowledgements.
we thank robert hawkins, jens tuyls, vishvak mu-rahari, howard chen and members of the princetonnlp group for helpful discussions and feedback.
this research was supported by an amazon re-search award..figure 3: pca visualization of latent variables in theexplicit (upper) and implicit (lower) tom models.
col-ors indicate different opponent populations..normalized latent variables in both explicit andimplicit tom models.
the latent variables areextracted from one layer before the output of theidentiﬁer or its equivalence in the implicit model.
the explicit tom model learns embeddings encod-ing different opponent populations, as the majorvariances of variable are captured by the differ-ence of opponent populations.
however, withoutextra supervision, the extraction of the populationidentity is difﬁcult in the implicit tom model.
fur-ther analysis shows that the variances of the latentvariables in the implicit tom model are mainly ex-plained by intent types.
we include more detailedanalysis and t-sne visualization in appendix b..8 conclusion.
in this work, we proposed a novel framework to in-tegrate the concept of theory of mind (tom) intogenerating task-oriented dialogs.
our approachprovides the ability to model and infer personalitytypes of opponents, predict changes in their mentalstate, and use this information to adapt the agent’shigh-level strategy in negotiation tasks.
we in-.
689references.
jacob andreas and dan klein.
2016. reasoning aboutpragmatics with neural listeners and speakers.
inproceedings of the 2016 conference on empiricalmethods in natural language processing, pages1173–1182..jerome s bruner.
1981. intention in the structure of ac-tion and interaction.
advances in infancy research..arjun chandrasekaran, deshraj yadav, prithvijit chat-topadhyay, viraj prabhu, and devi parikh.
2017. ittakes two to tango: towards theory of ai’s mind.
corr, abs/1704.00717..bhuwan dhingra, lihong li, xiujun li, jianfeng gao,yun-nung chen, faisal ahmed, and li deng.
2017.towards end-to-end reinforcement learning of dia-in proceed-logue agents for information access.
ings of the 55th annual meeting of the associationfor computational linguistics, acl 2017, vancou-ver, canada, july 30 - august 4, volume 1: longpapers, pages 484–495..daniel fried, ronghang hu, volkan cirik, annarohrbach, jacob andreas, louis-philippe morency,taylor berg-kirkpatrick, kate saenko, dan klein,and trevor darrell.
2018. speaker-follower mod-els for vision-and-language navigation.
in advancesin neural information processing systems, pages3314–3325..kallirroi georgila,.
james henderson, and oliverlemon.
2006. user simulation for spoken dia-logue systems: learning and evaluation.
in inter-speech 2006 - icslp, ninth international confer-ence on spoken language processing, pittsburgh,pa, usa, september 17-21, 2006. isca..noah d goodman and michael c frank.
2016. prag-matic language interpretation as probabilistic infer-ence.
trends in cognitive sciences, 20(11):818–829..noah d goodman and andreas stuhlm¨uller.
2013.knowledge and implicature: modeling language un-derstanding as social cognition.
topics in cognitivescience, 5(1):173–184..christos hadjinikolis, yiannis siantos, sanjay modgil,elizabeth black, and peter mcburney.
2013. oppo-nent modelling in persuasion dialogues.
in twenty-third international joint conference on artiﬁcial in-telligence..he he and jordan l. boyd-graber.
2016. opponentin pro-modeling in deep reinforcement learning.
ceedings of the 33nd international conference onmachine learning, icml 2016, new york city, ny,usa, june 19-24, 2016, pages 1804–1813..he he, derek chen, anusha balakrishnan, and percyliang.
2018. decoupling strategy and generation inin proceedings of the 2018negotiation dialogues.
conference on empirical methods in natural lan-guage processing, brussels, belgium, october 31 -november 4, 2018, pages 2333–2343..youngsoo jang, jongmin lee, and kee-eung kim.
2020. bayes-adaptive monte-carlo planning andlearning for goal-oriented dialogues.
in the thirty-fourth aaai conference on artiﬁcial intelligence,aaai 2020, the thirty-second innovative appli-cations of artiﬁcial intelligence conference, iaai2020, the tenth aaai symposium on educationaladvances in artiﬁcial intelligence, eaai 2020, newyork, ny, usa, february 7-12, 2020, pages 7994–8001..simon keizer, markus guhe, heriberto cuay´ahuitl,ioannis efstathiou, klaus-peter engelbrecht, mi-hai sorin dobre, alex lascarides, and oliverlemon.
2017. evaluating persuasion strategies anddeep reinforcement learning methods for negotiationin proceedings of the 15th con-dialogue agents.
ference of the european chapter of the associationfor computational linguistics, eacl 2017, valen-cia, spain, april 3-7, 2017, volume 2: short papers,pages 480–484..sang-woo lee, yu-jung heo, and byoung-tak zhang.
2018. answerer in questioner’s mind: informationtheoretic approach to goal-oriented visual dialog.
inadvances in neural information processing systems,pages 2579–2589..volodymyr mnih, adri`a puigdom`enech badia, mehdimirza, alex graves, timothy p. lillicrap, timharley, david silver, and koray kavukcuoglu.
2016. asynchronous methods for deep reinforce-in proceedings of the 33nd inter-ment learning.
national conference on machine learning, icml2016, new york city, ny, usa, june 19-24, 2016,pages 1928–1937..will monroe and christopher potts.
2015. learningin the rational speech acts model.
arxiv preprintarxiv:1510.06807..nir oren and timothy j norman.
2009. arguing us-ing opponent models.
in international workshop onargumentation in multi-agent systems, pages 160–174. springer..friedrich burkhard von der osten, michael kirley, andtim miller.
2017. the minds of many: opponentin proceedings ofmodeling in a stochastic game.
the twenty-sixth international joint conference onartiﬁcial intelligence, ijcai 2017, melbourne, aus-tralia, august 19-25, 2017, pages 3845–3851..david premack and guy woodruff.
1978. does thechimpanzee have a theory of mind?
behavioral andbrain sciences, 1(4):515–526..neil c. rabinowitz, frank perbet, h. francis song,chiyuan zhang, s. m. ali eslami, and matthewbotvinick.
2018. machine theory of mind.
in pro-ceedings of the 35th international conference onmachine learning, icml 2018, stockholmsm¨assan,stockholm, sweden, july 10-15, 2018, pages 4215–4224..690tjitze rienstra, matthias thimm, and nir oren.
2013.opponent models with uncertainty for strategic argu-mentation.
in twenty-third international joint con-ference on artiﬁcial intelligence..heinz wimmer and josef perner.
1983. beliefs aboutbeliefs: representation and constraining function ofwrong beliefs in young children’s understanding ofdeception.
cognition, 13(1):103–128..steve j. young, milica gasic, blaise thomson, and ja-son d. williams.
2013. pomdp-based statistical spo-ken dialog systems: a review.
proceedings of theieee, 101(5):1160–1179..yiheng zhou, he he, alan w. black, and yuliatsvetkov.
2019. a dynamic strategy coach for ef-fective negotiation.
in proceedings of the 20th an-nual sigdial meeting on discourse and dialogue,sigdial 2019, stockholm, sweden, september 11-13,2019, pages 367–378..verena rieser and oliver lemon.
2006. cluster-baseduser simulations for learning dialogue strategies.
ininterspeech 2006 - icslp, ninth internationalconference on spoken language processing, pitts-burgh, pa, usa, september 17-21, 2006. isca..homero roman roman, yonatan bisk, jesse thoma-son, asli c¸ elikyilmaz, and jianfeng gao.
2020.rmm: a recursive mental model for dialog naviga-tion.
corr, abs/2005.00728..sheng shen, daniel fried, jacob andreas, and danklein.
2019. pragmatically informative text gener-in proceedings of the 2019 conference ofation.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4060–4067..aviv tamar, sergey levine, pieter abbeel, yi wu, andgarrett thomas.
2016. value iteration networks.
in advances in neural information processing sys-tems 29: annual conference on neural informa-tion processing systems 2016, december 5-10, 2016,barcelona, spain, pages 2146–2154..david traum, stacy c marsella, jonathan gratch, jinalee, and arno hartholt.
2008. multi-party, multi-issue, multi-strategy negotiation for multi-modal vir-tual agents.
in international workshop on intelligentvirtual agents, pages 117–130.
springer..tsung-hsien wen, david vandyke, nikola mrksic,milica gasic, lina maria rojas-barahona, pei-haosu, stefan ultes, and steve j. young.
2017. anetwork-based end-to-end trainable task-oriented di-alogue system.
in proceedings of the 15th confer-ence of the european chapter of the associationfor computational linguistics, eacl 2017, valen-cia, spain, april 3-7, 2017, volume 1: long papers,pages 438–449..ying wen, yaodong yang, rui luo, jun wang, andwei pan.
2019. probabilistic recursive reasoningfor multi-agent reinforcement learning.
in 7th inter-national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019..julia white, jesse mu, and noah d. goodman.
2020.learning to refer informatively by amortizing prag-matic reasoning.
corr, abs/2006.00418..jason d. williams, eslam kamal, mokhtar ashour,hani amr, jessica miller, and geoffrey zweig.
2015.fast and easy language understanding for dialog sys-tems with microsoft language understanding intelli-gent service (luis).
in proceedings of the sigdial2015 conference, the 16th annual meeting of thespecial interest group on discourse and dialogue,2-4 september 2015, prague, czech republic, pages159–161..691appendix.
a experimental setup.
to test our proposed framework in a realis-tic persuasive negotiation setting, we use thecraigslistbargain dataset (he et al., 2018),which contains 6682 human-human dialogs be-tween a buyer and a seller alternately bargaining forthe price of an item on craigslist.
the listed priceand a description is presented to both agents, and aprivate price is assigned to the buyer as the target.
we set the reward ri to be a linear function of theﬁnal price, such that the buyer achieves maximalreward of 1 at its target price, the seller achievesmaximal reward of 1 at the listing price, and bothagents receive zero rewards at the midpoint of thelisting price and the target price.
when there isno deal, both agents receives equivalent penalty of-0.5..ontology we redesign the ontology of thecraigslistbargain dataset to support a morediverse dialog act than the original coarse dialogacts (he et al., 2018), which can reﬂect more waysof mental state change in a negotiation.
a dialogact consists of intent and a set of arguments.
in ourexperiments, we only focus on the price as it is themost important goal of this task.
all ﬁfteen dialogacts are listed in table 2. there are four intentspropose, counter, agree, disagree that must befollowed by a price slot, and accept, reject, andquit are four terminal dialog acts with no utterance.
when an agent takes an oﬀer, the other agent hasto respond with accept or reject.
note that thefunction of this dialog act is not to capture the fullsemantic meaning of one utterance, but to serve asa logic skeleton of the dialog..system design parser: we use microsoft lan-guage understanding intelligent service (luis)(williams et al., 2015) with 10 starting trainingexamples for each dialog act in our experiment.
generator: we use a retrieval-based model simi-lar to he et al., 2018 which samples an utterancefrom the top 10 matched templates.
we comparedthree hybrid dialog managers combining neuralnets and rules to control the ﬂow of the dialog.
(1) the sl manager employs a neural network tolearn the transitions from st−1 to st from dataset.
we use a sequence model with two-layer lstm.
with 300 hidden units for both the encoder andthe decoder.
(2) the rl manager uses an actor-critic method (mnih et al., 2016), which containsa policy network with the same neural network ar-chitecture as the sl manager, and a value networkpredicts the cumulative reward given input states.
the rl manager also learns πi(st|st−1) but withthe goal of maximizing the total reward.
(3) thetom manager uses the ﬁrst-order tom policy asdescribed in 4 to learn the best responding policyπtom(st|st−1, u−it−1) with the awareness of the oppo-nent’s characteristics and mental state change.
anextra lstm model is used to learn the character-istic identity z−it−1 = f (st−1, u−it−1) in the explicittom model.
for all three managers, we improvethe learned policy by enforcing hand-craft rules.
for example, the agent should never offer price be-low its bottom line and reject the opponent’s offerif it is worse than its bottom line..populations of opponents when playingagainst with a sl manager, we create 7 differentpopulations of opponents by injecting rules forchanging the price and rendering utterance.
pricechanging rules are functions of the number ofsentences in the conversation history, which modelthe agreeability and the ﬂexibility of a person.
theagreeability of a person is reﬂected in the rangeof relative prices (utility) at which a deal couldbe made.
for example, a competitive opponenthas a higher lower bound on the price, while acooperative opponent has a lower initial price.
theﬂexibility of a person is reﬂected in the slope andconvexity of the price-changing rules.
the pricechanging function for the most aggressive andstubborn opponent has a zero slope, encouragingthem to insist on their initial price until the endof the dialog.
the more determined a seller is,the more concave the price changing functionbecomes..when rendering utterances, we use a template-based language generator as in (he et al., 2018),and insert population-speciﬁc tokens in utterancesby sampling according to different opponent types.
for example, in the utterances from a competitiveopponent, words like “afraid” or “unfortunately”appear more often, while words like “great” or “ok”will appear more frequently in the utterances from acooperative opponent.
utterances of different pop-ulations should follow different distributions, andthese sets of tokens are designed for this purpose.
we vary the price range, slope, and convexity to.
692obtain the different behaviors for the seven differ-ent opponent types.
the mildest population willgradually compromise and lower its price (or raiseits price if it is buyer).
the utterances of this pop-ulation also contain more polite and mild wordsindicating its negotiable position.
and the mostaggressive population will insist its price until theend, and utters more stubborn words.
the otherﬁve populations will follow different price chang-ing curves in between these two extremes, and alsohave different language properties.
all of thesepopulations will deal at a certain price range, whichdepends on latest proposal price and current dialoglength in different ways..training and fine tuning we train the sl man-ager on 5000 dialogs for 20 epochs and choose themodel with the lowest validation loss.
the rlmanager is ﬁne-tuned from a well-trained sl agentby playing against itself.
we choose the modelwith the highest reward..t−1, st, ui.
for the tom manager,.
the value functionv (st+1) is borrowed from a well trained rl man-ager, and ﬁxed during inference.
the implicit ﬁrst-order tom model t (st+1|u−it) is trainedin a similar way as the sl manager.
to obtain theexplicit ﬁrst-order tom model t (st+1|z−it−1, st, uit),we co-trained it with a lstm-based identiﬁert−1 = f (st−1, u−iz−it−1) for 2,000 episodes.
eachrun on training each manager was performed usinga single nvidia gtx 2080 ti gpu with 16gbram in approximate 2 hours.
all the managerswere trained using adam with learning rate 0.001.for the tom manager, hyperparameter β was ran-domly searched in range of 0.05, 0.1, 1, 10, and thesetting with the best results was β = 0.05..b additional experimental results.
b.1 comparison of implicit and explicit.
tom models.
we compared the implicit and the explicit tommodels as described in appendix section 4 in themain article.
here additional figure 4 shows thevalidation mean squared error between the pre-dicted price and the ground truth price of the oppo-nent in the next turn (if exists) over 3,584 dialogs.
two models can both be trained well to perform thisone-step prediction, while the explicit model withan identiﬁer has slightly better sample efﬁciencyand better prevents overﬁtting.
this supports ourhypothesise that the prior of different types of op-ponents is important..b.2 visualization of latent variables.
figure 5: t-sne and pca visualization of latent vari-ables in the explicit tom model (left) and the implicittom model (right).
colors indicate different intentstaken by the the agent..figure 6: the same plot as figure 5. colors indicatedifferent intents taken by the the opponent..figure 4: the validation price mse loss curves of im-plicit and explicit ﬁrst-order tom models.
the explicitmodel with an identiﬁer has slightly better sample efﬁ-ciency and better prevents overﬁtting..figure 7: the same plot as figure 5. colors indicatenumbers of rounds of interaction..693tom  w/ identiﬁer  (explicit)tom     price mse.
x 1000# episode