repsum: unsupervised dialogue summarization based on replacementstrategy.
xiyan fu1, yating zhang2, tianyi wang2xiaozhong liu3, changlong sun2 and zhenglu yang11 nankai university, tianjin, china2 alibaba group, hangzhou, china3 indiana university, bloomington, usafuxiyan@mail.nankai.edu.cn, {ranran.zyt,will.wty}@alibaba-inc.comliu237@indiana.edu, changlong.scl@taobao.com, yangzl@nankai.edu.cn.
abstract.
in the ﬁeld of dialogue summarization, due tothe lack of training data, it is often difﬁcult forsupervised summary generation methods tolearn vital information from dialogue context.
several works on unsupervised summarizationfor document by leveraging semantic informa-tion solely or auto-encoder strategy (i.e., sen-tence compression), they however cannot beadapted to the dialogue scene due to the lim-ited words in utterances and huge gap betweenthe dialogue and its summary.
in this study,we propose a novel unsupervised strategy toaddress this challenge, which roots from thehypothetical foundation that a superior sum-mary approximates a replacement of the orig-inal dialogue, and they are roughly equivalentfor auxiliary (self-supervised) tasks, e.g., dia-logue generation.
the proposed strategy rep-sum is applied to generate both extractive andabstractive summary with the guidance of thefollowed nth utterance generation and classi-ﬁcation tasks.
extensive experiments on vari-ous datasets demonstrate the superiority of theproposed model compared with other unsuper-vised methods..figure 1: a summary is generated from the input di-alogue ﬁrstly, and then the original dialogue and itscorresponding summary are exploited for nth utteranceprediction, respectively.
j is the ground truth, and j indifferent colors are the decoded utterances based on theoriginal dialogue and the generated summary respec-tively.
the difference between decoded j is employedfor optimization of summary generation.
the motiva-tion is that a superior summary approximates a replace-ment of the original dialogue, and they are roughlyequivalent for auxiliary tasks..1.introduction.
dialogue summarization distills key informationfrom a dialogue context and synopsizes it into aconcise summary.
as a novel topic of critical im-portance, it offers powerful potentials for a numberof scenarios, e.g, the court debate in civil trial, thecustomer service calls arisen from agent(s) and cus-tomer, the business meeting engaged with multi-members.
it also assists users in quick access andconsumes the essential content in the dialogue..major attempts on dialogue summarization aretemplate-based (wang and cardie, 2013; oya et al.,2014) in the primitive stage by extracting key in-formation and ﬁlling it into the learned templates.
however, these template-based techniques limit the.
scope of their applications and cannot be adaptedto a wider range of conversational data since theirinput structure is predeﬁned and the learned tem-plates are domain-speciﬁc.
later, various works ex-plore the assistance from labeled auxiliary informa-tion for summary generation, by leveraging eitherdialogue act (goo and chen, 2018), or key pointsequence (liu et al., 2019).
the former predictsthe dialogue act label of each utterance as explicitinteractive signals, while the latter attempts to learnthe logic of the summary via key point sequence.
recently, ganesh and dingliwal (2019) convertsthe dialogue into a document by aptly capturingdiscourse relations which proves to be effectiveunder the scenario of document summarization..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6042–6051august1–6,2021.©2021associationforcomputationallinguistics6042while prior deep content generation methodsrely on large amounts of annotated data, they arerarely available for dialogue summarization due tothe prohibitive costs of labeled data.
a straightfor-ward way to alleviate the dependency of the anno-tated data is to apply the existing unsupervisedmethods designed for document summarization(rossiello et al., 2017; zheng and lapata, 2019;baziotis et al., 2019; chu and liu, 2019) to thedialogue scene.
however, we argue that these meth-ods accompany weakness either in extractive or inabstractive dialogue summarization.
in terms ofextractive methods, they mainly rely on semanticinformation without any supervision signals.
as aresult, they are ragged in effects due to the limitedwords in dialogue utterances.
as for abstractiveapproaches, they are commonly designed with anauto-encoder (ae) where the latent variable de-codes to a summary which attempts to reconstructthe original input representation.
hence, they areconstrained to the small gap between the input textand the target summary (e.g., sentence compres-sion) while failing to reconstruct long input text(e.g., dialogue)..in this paper, we propose an innovative unsu-pervised strategy, dubbed repsum, which can beapplied to both extractive and abstractive summa-rization.
the key intuition is derived from the eval-uation methods of extrinsic summarization (mani,2001), which testiﬁes the impact of summariza-tion based on how it affects the completion ofsome other tasks, such as information retrieval, rel-evance assessment, reading comprehension, etc.
we claim that a superior summary can offer asemantic replacement of the original dialogue,which provides equivalent information for com-pleting auxiliary tasks, e.g., dialogue generation,as shown in ﬁgure 1. speciﬁcally, we proposetwo auxiliary tasks which are nth utterance genera-tion and nth utterance selection from k candidatesbased on the previous contents.
both the dialogueand the summary aim to achieve decent perfor-mances on the speciﬁc task respectively.
besides,we introduce kl divergence to curtail the differ-ence between results based on the dialogue andthe summary.
this strategy provides the summa-rization with essential self-supervised signals viaauxiliary tasks.
furthermore, it decouples the train-ing from the reconstruction of ae, which enablesto support longer text or dialogue to be effectivelysummarized..our main contributions are as follows:.
• we propose repsum, an unsupervised (orself-supervised) strategy for dialogue summa-rization, which roots from the hypothesis thata superior summary approximates a replace-ment of the original dialogue for completingother tasks.
it leverages several intrinsic self-supervised signals..• based on the repsum strategy, we proposethe corresponding model and employ it to bothextractive and abstractive summarization..• the extensive experiments with multiple dia-logue datasets demonstrate the superiority ofthe proposed model over several unsupervisedapproaches..2 related work.
dialogue summarization extracts signiﬁcant in-formation from dialogues.
most of the initial worksadopted extractive-based methods.
for instance,bui et al.
(2009) produced multiple short frag-ments from utterances and then selected the parseof the summary by svm combined with semantic-similarity features.
later, (oya et al., 2014; wangand cardie, 2013) induced abstractive generationtemplates for constructing candidate summary sen-tences.
moreover, to beneﬁt from the existing tech-nologies for document summarization, ganesh anddingliwal (2019) converted the conversation intoa text document through discourse relations andlexical information and then created summariesvia pointer-generator (see et al., 2017).
however,given that dialogues are different from documentsin terms of interactive patterns, most researchersexplored to summarize the dialogue by leveragingauxiliary information hidden in the utterances.
forexample, goo and chen (2018) proposed to uti-lize dialogue act as an auxiliary supervised signaland design a sentence-gated mechanism for model-ing the relationships between dialogue acts and thesummary.
in addition, liu et al.
(2019) predictedthe keypoint sequence ﬁrst and then use it to guidethe summary prediction..in contrast to the supervision works, we focuson the unsupervised dialogue summarization con-sidering the high cost and limitation of the labeleddata in the dialogue scene.
additionally, our pro-posed strategy is applicable to both extractive andabstractive models without using any outer infor-.
6043mation (e.g., template, dialogue acts, and keypoint)but leveraging its intrinsic self-supervised nature.
unsupervised summarization historically, un-supervised summarization focused on extractingutterances directly.
for example, lead choosesthe ﬁrst several utterances and textrank (mihal-cea and tarau, 2004) ranks utterances by runninga graph-based algorithm, where each node rep-resents an utterance and the weight between anytwo nodes is calculated by the semantic similarity.
later, rossiello et al.
(2017) proposed a centroid-based method for text summarization that exploitsthe compositional capabilities of word embeddings.
zheng and lapata (2019) improved it by buildinggraphs with directed edges considering the relativepositions of any two sentences which contributesto their respective centrality.
in recent works, thetask of unsupervised summarization is framed asa self-supervised auto-encoder problem, namelysentence compression.
miao and blunsom (2016);baziotis et al.
(2019); chu and liu (2019) appliedthe auto-encoder framework, where the expectedabstract is set to the latent variables from which theinput sentence is reconstructed.
f´evry and phang(2018) added noise to extend sentences and traineda denoising auto-encoder to recover the input text.
braˇzinskas et al.
(2020) introduced a hierarchicalvariational auto-encoder to associate the individualreviews with stochastic latent codes for opinionsummarization.
recently, another line of worksfocused on edit-distance-based approaches.
westet al.
(2019) summarized by applying the informa-tion bottleneck principle to the objective of condi-tional language modeling.
in addition, zhou andrush (2019); schumann et al.
(2020) summarizedby hill climbing with word-level extraction, whichsearches the text for a high-scoring summary bydiscrete optimization..compared to these works, to the best of ourknowledge, our model is one of the pioneers at-tempting unsupervised dialogue summarization.
toimprove the effectiveness, we devise a generalizedstrategy repsum that incentivizes the summaryto complete the auxiliary tasks as the original dia-logue does, thus providing self-training signals andin turn enabling long texts to be summarized..3 repsum model.
3.1 mechanism.
repsum roots from the hypothetical foundationthat a superior summary approximates a replace-.
figure 2: the overall ﬂow chart of the proposed model.
the middle square is the unsupervised dialogue summa-rization generation process.
further, both the dialogueand the corresponding summary are employed on aux-iliary tasks (i.e., nth utterance generation and classiﬁ-cation).
the innovation lies to a superior summary isthe replacement of the original dialogue..ment of the original dialogue, and they areroughly equivalent for completing auxiliary (self-supervised) tasks.
figure 2 shows the ﬂow chart ofthe introduced replacement strategy.
speciﬁcally,the summary generation module aims at generatinga summary from the original dialogue.
during thisgeneration process, two auxiliary tasks, nth utter-ance generation and nth utterance classiﬁcation,are constructed to transform unsupervised dialoguesummarization task into self-supervised mode bylearning through auxiliary tasks.
furthermore, weapply repsum to extractive and abstractive sum-marization, experiments verify its effectiveness inan empirical point of view..3.2 auxiliary tasks.
as introduced above, we leverage two auxiliarytasks to act as self-supervised signals to assist thegeneration process of a superior summary.
giventhat the summary is the replacement of the origi-nal dialogue, the input dialogue and the generatedsummary are expected to achieve similar resultson these tasks respectively.
hence, we add thekullback–leibler (kl) divergence to curtail thedifferences between the results of each auxiliarytask based on the input dialogue and the generatedsummary.
the details are denoted as follows:.
task1: generation (tg) aims at generating thenth utterance.
we employ the commonly usedencoder-decoder structure.
the whole dialogue is.
6044concatenated and encoded (as a document) by thebi-directional lstm(hochreiter and schmidhuber,1997) for the sake of fair comparison with otherbaselines.
the representation of each word is theconcatenation of the forward and backward lstmstates, i.e., hi = [hf wd, hbwd].
as for the decoder,iwe employ a uni-directional lstm with attentionmechanism (luong et al., 2015).
concretely, theattention distribution at and the following contextvector ct are formulated as:.
i.ati = σ(hiwast),.
ct =.
aihi.
(1).
n(cid:88).
i=1.
where wa is the learnable parameter and σ is thesoftmax function.
the context vector and the cur-rent decoder state st are employed for predictingthe probability distribution of the output word overall the vocabulary words:.
p(yt) = σ(wp(φ(wk[yt−1; st; ct] + bk)) + bp).
(2)where wp, wk, bp and bk are learnable parameters.
σ is the softmax function and φ is the tanh function.
we choose the negative logliklihood as the lossfunction, and the loss of the utterance generationbased on the dialogue via the path of encdia →decdia (see figure 2) is denoted as:.
lt gdia = −.
log p(lt|l<t; encdia).
(3).
q(cid:88).
t=1.
where l = {l1, l2, ..., lq} is the generated utterance.
similarly, the utterance generation based on thegenerated summary lt gsum is calculated via theprocess of encsum → decsum in figure 2. to guar-antee the similar performance of the results basedon the original dialogue and the generated sum-mary, we also add kl divergence to curtail thedifference between the probability distribution ofprediction at each timestep:.
q(cid:88).
t=1.
lt gkl =.
kl(p(lt|l<t; encdia)||p(lt|l<t; encsum)).
(4)hence, the loss for the nth utterance generation.
task is denoted as:.
lt g = α0lt gdia + α1lt gsum + α2lt gkl.
(5).
where α0, α1, and α2 are the weight for each loss.
task2: classiﬁcation (tc) is designed to selectthe correct nth utterance from the k candidate.
utterances.
similar to the dialogue encoding in thetask tg, we choose the bi-lstm as the encoder.
the dialogue representation hd is the average of thehidden state of each word.
besides, each candidateis also encoded by the bi-lstm and projected to adense vector by logit layer f , and then concatenatedto hd, formulated as [f (uci); hd].
the probabilityof each utterance belonging to the correct answer iscalculated by a logistic layer.
furthermore, we usecross-entropy for training via the process of encdia→ classif ierdia (see figure 2).
the loss based onthe dialogue is formulated as:.
lt cdia = −.
znlogˆzn.
(6).
k(cid:88).
n=1.
similarily, lt csum based on the generated sum-mary through encsum → classif iersum is calcu-lated.
we also use the kl divergence to measurethe difference between the results from the dialogueand generated summary:.
lt ckl = kl(p(ucdia)||p(ucsum)).
(7).
where p(ucdia) and p(ucsum) is the probability dis-tribution on k candidates.
all in all, the loss of thenth utterance selection task is formulated as:.
lt c = α3lt cdia + α4lt csum + α5lt ckl.
(8).
where α3, α4, and α5 are the weight for each loss.
parameters α0 to α5 are used for normalization..3.3 unsupervised summarization.
the repsum is employed to both the extractive andabstractive summarization:extractive summarization we consider the ex-tractive summarization as a sentence binary classi-ﬁcation task as (nallapati et al., 2017) does, whichmeans r utterances in a dialogue with label oneare extracted to be an extractive summary.
specif-ically, we use encext (encdia in the figure 2) ap-plied by the bi-lstm to encode utterances in di-alogue, and they are represented as hidden statesh1, h2, ..., hn−1.
then, the representation of thedialogue is the average pooling of the concatenatedhidden states of the entire utterances, denoted as:.
d = φ(wd.
[hf wdi.; hbwdi.]
+ bd).
(9).
1n − 1.n−1(cid:88).
i=1.
where wd and bd are learnable parameters, and φis the tanh function.
for utterances classiﬁcation,.
6045each utterance is concatenated with the dialoguerepresentation d. and a logistic layer predicts theprobability belonging to the generated summary, asshown below:.
p(ui = 1) = ψ(whhi + hiwhdd + bh).
(10).
where wh, whd and bh are learnable parameters,and ψ is the sigmoid function.
later, we choosethe top probability r utterances as the extractivesummary.
after obtaining the initial generated sum-mary, the unsupervised extractive summarizationcan be guided under the repsum strategy.
speciﬁ-cally, the extractive-based summary is optimized bythe auxiliary tasks for the sake of effective resultsand similar performance of the dialogue.
hence,the training loss for extractive summarization in-cluding nth utterance generation and classiﬁcationis denoted as:.
lext = lext.
t g + lextt c.(11).
abstractive summarization the abstractive sum-marization process follows the conventionalencoder-decoder structure.
for each time step, theword prediction probability is calculated via eq.
2.to generate the abstractive summary used for theauxiliary tasks, we sample each word from the prob-ability (cid:101)yt ∼ sof tmax(p(yt)) and encode them asenca sum (encsum in the figure 2).
however, itis a non-differentiable process, which can not betrained directly..hence, we use the straight-through (st) gum-ble estimator introduced in (bengio et al., 2013)to solve this problem.
during the forward trainingpass and test process, we use the reparametriza-tion trick as a variance approximation of sam-pling from the original probability (maddison et al.,2014).
speciﬁcally, sampling word is transformedto take the argmax from a new probability, (cid:101)y isdiscretized using argmax and sampling as:.
test set sizeavg.
utterance numavg.
utterance lengthavg.
summary length.
ami132219.89293.26161.33.justice152537.5416.11159.50.table 1: statistics of datasets.
where |v | is the vocabulary size and the τ ∈ (0, ∞)is the temperature parameter.
samples from gum-ble softmax distributions are identical to samplesfrom a categorical distribution as τ → 0. the inputfor the encoder enca sum is denoted as:.
eabsyt =.
e(wi)p(yit).
(14).
|v |(cid:88).
i=1.
where e(wi) is the word embedding of the words.
after the acquisition of the abstractive summary,we also employ the repsum strategy for training.
due to the difﬁculty of the generation, we sup-ply two more other auxiliary losses.
firstly, theexperiments indicate that the model is difﬁcult toconverge due to the lack of any guidance for thedecoder (see w/o fake-sum in table 5), we em-ploy the extractive summary as a fake summaryfor teacher forcing training.
hence, the fake sum-mary generation loss lf s is calculated followingthe eq.
1, eq.
2 and eq.
3. moreover, given thatabstractive summary is limited to readability andﬂuency, we pre-train a language model with dia-logue utterances to solve this problem.
we aimto generate ﬂuent summaries by adding languagemodeling loss, which approaches the output predic-tion to language output:.
llm = kl(p(yt)||plm(yt)).
(15).
hence, the training loss for the unsupervised ab-stractive dialogue summarization is denoted as:.
labs = labs.
t g + labs.
t c + α6lf s + α7llm (16).
parameters α6 and α7 are normalization weight..(cid:101)yt = argmax(log(p(yt)) + g),ξ ∼ u (0, 1).
g = −log(−log(ξ)),.
(12).
4 experimental setup.
4.1 dataset.
where g is the gumble distribution and u is theuniform distribution.
as for computing the gradientin the backward pass, we use a continuous anddifferentiable approximation to argmax:.
p(yi.
t) =.
exp((log(p(yij=1 exp((log(p(yj.
(cid:80)|v |.
t)) + gi)/τ ).
t )) + gj)/τ ).
(13).
we evaluate repsum on a meeting dataset in en-glish ami and a multi-party court debate datasetin chinese justice.
the statistics are presented indetails (see tabel 1).
ami.
the ami1 meeting corpus (carletta et al.,2005) consists of 100 hours of meeting recordings.
1http://groups.inf.ed.ac.uk/ami/corpus/overview.shtml.
6046type.
model.
extractive.
abstractive.
oraclelead3textrank (mihalcea and tarau, 2004)centroid (rossiello et al., 2017)pacsum(zheng and lapata, 2019)repsum-ext (ours)2g shuf f´evry and phang (2018)meansum(chu and liu, 2019)seq3(baziotis et al., 2019)repsum-abs (ours).
r-124.579.1511.2714.0816.1518.7714.0816.0917.0618.88.amir-24.441.780.842.092.232.242.092.302.232.38.r-l15.035.367.198.199.1410.808.1811.1411.8515.62.r-137.2817.6920.7222.3123.3625.8820.1921.2522.4724.23.justicer-221.053.336.516.537.038.214.155.543.886.37.r-l32.7811.5213.5613.6614.6615.9712.0813.4414.6715.14.table 2: comparison of our mechanism employed in extractive and abstractive summarization with other base-line models.
all the results are evaluated by the rouge on the ami nad justice dataset (pairwise t-test at 5%signiﬁcance level)..in english.
it includes high-quality and manuallyproduced transcription, dialogue acts, topic seg-mentation, extractive and abstractive summaries,etc.
in this work, we use the recording transcriptsas the original input and the provided abstractivesummary as the expected summary to be generated.
justice.
the court debate records consist of 30,000dispute cases.
in the court trial scenario, there aremultiple roles (i.e., judge, plaintiff, defendant).
inthe whole debate dialogue, the plaintiff and thedefendant debate on controversy focus leading bythe judge.
after the trial, the judge summarizes thefacts recognized through the trial.
thus we use thecourt debate transcript as the original input and thefact description as the expected summary..4.2 parameter settingsin our experiments2, we optimize the proposedmodel using adam optimizer (kingma and ba,2014) with the learning rate of 3e-4.
we train on asingle teslap100 gpu with a batch size of 16. thevocabulary size is 30,000 and embedding dimen-sion for each word is 200. the hidden size is 200for both encoder and decoder.
for gumble softmax,we set the temperature τ to 0.5. in the auxiliarytask c2, we denote k as 4, which means we selectthe other 3 similar utterances.
they are chosenfrom all the utterances in the dataset randomly.
forextractive summarization, we pick out the top 3utterances by their probability.
we set the α0 to α7equals 0.5, 0.5, 5, 1, 1, 2, 1, 0.006 respectively tobalance the scale of each module..4.3 baselines.
we ﬁrstly report the performance of the oracleas an upper bound, which uses a greedy algo-.
2the code can be found in https://github.com/xiyan524/.
repsum.
rithm to extract several utterances to maximize therouge compared with the ground truth.
lead3extracts the ﬁrst three utterances as the summary..as for the extractive-based methods, we com-pare with classical textrank (mihalcea and tarau,2004) which converts the dialogue to a weighted-graph where each node represents an utterance andthe edge weight expresses the semantic similaritybetween any two utterances.
centroid (rossielloet al., 2017) proposes a centroid-based method fortext summarization that exploits the compositionalcapabilities of word embeddings.
pacsum (zhengand lapata, 2019) improves the textrank by build-ing graphs with directed edges considering the rel-ative positions of any two sentences contributingto their respective centrality..with regard to the abstractive-based methods,we compare with several auto-encoder based ap-proaches.
2g shuf (f´evry and phang, 2018) addsnoise to extend sentences and trains a denoisingauto-encoder to recover the original input text.
seq3 (baziotis et al., 2019) constructs a compres-sor to generate summary and a reconstructor toregenerate input sentence via two chained encoder-decoder pairs.
meansum (chu and liu, 2019) em-ploys the mean of the representations of the inputto decode a reasonable summary..5 experimental results.
5.1 quantitative analysis.
table 2 shows the experimental results based onthe ami and the justice datasets.
rouge 3 score(lin, 2004) is used for evaluation..for extractive summarization, we found the up-per bound oracle is quite low in dialogue sum-marization (see the ﬁrst row in table 2) compared.
3https://github.com/pltrdy/ﬁles2rouge.
6047model.
textrankcentroidpacsumrepsum-ext2g shufmeansumseq3repsum-abs.
ami.
justice.
relevanceκavg0.510.570.830.880.771.021.170.790.780.560.840.890.811.111.230.82.fluencyκ0.810.800.760.810.760.680.690.72.avg1.551.641.671.690.780.891.031.22.relevanceκavg0.680.690.711.150.661.131.210.630.630.710.610.830.591.091.170.68.fluencyκ0.760.810.790.760.810.670.720.69.avg1.341.421.511.540.811.021.181.20.table 3: human evaluation.
we report the averagescore (avg) and the κ value in relevance and ﬂuency..type.
ext..abs..task.
r-118.77tg+tc-w/o tc18.36-w/o tg 16.8918.88tg+tc-w/o tc18.60-w/o tg 16.13.amir-22.242.182.112.381.941.72.r-l10.809.949.2615.6210.5510.05.justicer-28.217.896.336.376.515.20.r-l15.9715.5114.2415.1414.2913.50.r-125.8825.4522.8024.2323.6322.75.table 4: ablation study for the auxiliary tasks in re-placement mechanism on the ami and justice dataset.
ext.
represent extractive and abstractivebased summarization respectively..and abs..with the document summarization where r-1 scoreusually approaches to 50 as reported in (liu andlapata, 2019).
it indicates that the dialogue sum-marization is much more challenging.
additionallyami dataset is more appropriate for abstractivesummarization since its oracle scores are muchlower than those for justice dataset.
the score oflead3 estimates the information distribution overdialogues.
furthermore, our proposed repsum-extis compared with other four state-of-the-art mod-els with signiﬁcant improvement in rouge score.
table 2 demonstrates that the repsum strategy iseffective for extractive summarization..for abstractive summarization, we mainly com-pare repsum-abs with ae-based methods.
weemploy the same encoder and decoder settingsfor baselines for a fair comparison.
in terms ofrouge value, our model outperforms all the base-lines, especially in r-l score.
we consider that theauxiliary tasks training mechanism helps to preventthe focus on single-word reconstruction, but aimsto remain signiﬁcant continuous information..5.2 human evaluation.
in order to ensure the rationality/correctness of thegenerated summary, we also conducted a humanevaluation.
the annotators are required to estimatethe quality of the generated summaries with respectto the relevance indicating the connection betweenthe dialogue and the summary and ﬂuency repre-senting the readability.
the scores are divided intothree levels: +2, +1, 0, in which a higher scorestands for excellent.
we report the average scoreand coefﬁcient κ which indicates the consistencyof evaluation by different annotators.
speciﬁcally,we choose 100 examples for each dataset and sixannotators are required to evaluate all the testedmethods.
the annotators are experienced graduatestudents who have taken the annotation trainingbefore the experiment.
results shown in table 3indicate that our proposed strategy is superior to.
mothodsressum-abs-w/o dia-task-w/o sum-task-w/o kl-w/o lm-w/o fake-sum.
r-118.8816.5514.3416.7717.87-.
r-22.381.111.312.200.70-.
r-l15.6213.499.7814.7913.37-.
table 5: ablation study of each component based onabstractive summarization on the ami dataset..all the baselines.
furthermore, compared to theabstractive-based methods, extractive-based meth-ods perform better on ﬂuency.
we consider that thedifference is due to sentence integrity..5.3 ablation study.
to evaluate the effectiveness of the proposed rep-sum strategy, we conduct two ablation studies.
weﬁrst measure the inﬂuence of each auxiliary task(see table 4).
further, we verify the contributionof each module, shown in table 5..table 4 indicates that combining the two auxil-iary tasks achieves the best performance on bothextractive and abstractive methods.
the declineof performance is observed once we remove eithertask, especially the generation task.
we assumethat the classiﬁcation task is considerably straight-forward, which may not require afﬂuent semanticinformation.
however, it serves as an auxiliarysection with complicated generation tasks..furthermore, we remove each component to in-vestigate the module effectiveness in repsum-abs.
the result is shown in table 5. it indicates thatall the components make a positive contribution.
to be speciﬁc, fake summary (-w/o fake-sum) isthe critical point, which contributes to the modelconvergence.
besides, if we remove tasks based onthe generated summary (-w/o sum-task), the perfor-mance declines signiﬁcantly.
it proves the assump-tion that a superior summary is supposed to conductthe auxiliary tasks as original dialogue does.
eitherremoving tasks based on the dialogue (-w/o dia-.
6048fake summaryrandomextractive-based.
r-115.4518.88.r-22.392.38.r-l10.0715.62.table 6: effectiveness of potential fake summarychoices for abstractive summarization on the ami..t.34567.r-110.3819.8718.6318.6518.77.amir-20.882.201.851.941.84.r-l7.1311.0510.7310.6110.72.r-115.7122.8022.1522.6722.51.justicer-23.036.335.526.065.87.r-l9.9214.2413.6313.9813.87.table 7: effectiveness of candidate numbers in the aux-iliary task classiﬁcation.
it is based on the extractivesummarization of the ami and justice dataset..figure 3: effectiveness of nth utterance selection inthe auxiliary task generation.
it is based on the justicedataset..task) or adding kl divergence (-w/o kl) to controlsimilar effectiveness between dialogue and gen-erated summary, tends to harm the performance.
moreover, we notice that the pre-trained languagemodel (-w/o lm) beneﬁts the bi-gram by noticingthe signiﬁcant decrease in r-2.
the extractive-based method is ignored since its components arethe same as the abstractive-based approach..5.4 discussion.
fake summary extensive experiments show thatabstractive summarization is difﬁcult to convergewithout word-level guidance.
hence, we proposeto construct a fake summary to solve this problem.
in this section, we conduct two experiments fordifferent fake summary construction.
we ﬁrst at-tempt to select t utterances randomly.
further, wechoose an extractive summary.
table 6 shows thatthe random selection result is inferior to extrac-tive summary guidance.
given the consideration ofhigh accuracy, we choose the extractive summaryas guidance in this work.
however, we assumethat random selection can be also employed forefﬁciency consideration if necessary.
candidates number in tc to further explore theeffectiveness of the auxiliary task classiﬁcation(tc) for unsupervised dialogue summarization, weconduct experiments by varying the candidate’snumber k. such number inﬂuences the perfor-mance of the extractive summarization on bothami and justice datasets.
we set the number vary-ing from 3 to 7. the performance of our model withthe variation of the number k is shown in table 7.it indicates that the r-1 approaches a stable valuewith slight ﬂuctuation when we increase the k con-tinuously.
besides, there exists a drastic increase.
in r-1 when k is augmented from 2 to 3. hence,given the trade-off between the efﬁciency and thegeneration quality, we choose 4 as the number ofcandidates for all the experiments.
utterance choice in tg the selection of nth utter-ance for generation in the dialogue is crucial for themodel effectiveness.
meaningless utterances suchas ”hmmm”, ”the meeting is over” in meeting, and”please sign the transcript after checking” in courtdebates may be useless.
at the same time, none ofthe contextual information is integrant.
hence, weconduct experiments to testify the effectiveness ofthree different utterance selection strategies: ran-dom selects the nth utterance randomly.
the utter-ances before nth are regarded as the input.
if theremained dialogue utterances are less than 5, theexample is discarded.
last chooses the last utter-ance of each dialogue for prediction.
moreover,sec splits the dialogue into several sections andthen picks the last utterance of each section.
“sec”is segmented based on the rule which requires eachsection to contain at least 8 utterances with at least5 words and 3 signiﬁcant utterances whose tf-idfvalue is superior to the threshold..figure 3 shows the result conducted on justicedataset4.
it proves that meaningful utterance ben-eﬁts the performance.
speciﬁcally, last leads tothe worst result on both r-1 and r-l due to theuniversal utterance at the end of a dialogue.
weconsider that random prevents semantic informa-tion deﬁciency through selecting crucial utterancesoccasionally compared with sec which achievesthe best performance..4the performance on ami dataset shows a similar pattern.
we only show the visualized result on the justice dataset dueto the paper length limitation..60496 conclusion.
this work investigates the problem of unsuperviseddialogue summarization.
we propose a novel un-supervised strategy repsum, which roots from thehypothetical foundation that a superior summaryapproximates a replacement of the original dia-logue, and they are roughly equivalent for com-pleting auxiliary tasks.
repsum is employed onboth extractive and abstractive-based models via aself-supervision from two auxiliary tasks.
compre-hensive experiments on various datasets show theeffectiveness of the proposed mechanism comparedto the other unsupervised baselines..7 acknowledgments.
we sincerely thank wei liu, yu duan, and jiezhou for the helpful discussions.
this research wassupported by the national key research and de-velopment program of china (2018yfc0830200;2018yfc0830206; 2020yfc0832505).
references.
christos baziotis,.
ion androutsopoulos,.
ioanniskonstas, and alexandros potamianos.
2019. seqˆ3:differentiablesequence-to-sequence-to-sequenceautoencoder for unsupervised abstractive sentencein proceedings of the 2019 con-compression.
ference oftheassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 673–681, minneapolis, minnesota.
association for computational linguistics..the north american chapter of.
yoshua bengio, nicholas l´eonard,.
and aaroncourville.
2013. estimating or propagating gradi-ents through stochastic neurons for conditional com-putation.
arxiv preprint arxiv:1308.3432..arthur braˇzinskas, mirella lapata, and ivan titov.
unsupervised opinion summarization as2020.in proceedings of thecopycat-review generation.
58th annual meeting of the association for compu-tational linguistics, pages 5151–5169, online.
as-sociation for computational linguistics..trung h bui, matthew frampton, john dowding, andstanley peters.
2009. extracting decisions frommulti-party dialogue using directed graphical mod-in proceedings of theels and semantic similarity.
sigdial 2009 conference, pages 235–243..jean carletta, simone ashby, sebastien bourban, mikeflynn, mael guillemot, thomas hain, jaroslavkadlec, vasilis karaiskos, wessel kraaij, melissakronenthal, et al.
2005. the ami meeting corpus:a pre-announcement.
in international workshop onmachine learning for multimodal interaction, pages28–39.
springer..eric chu and peter liu.
2019. meansum: a neuralmodel for unsupervised multi-document abstractivesummarization.
in international conference on ma-chine learning, pages 1223–1232..thibault f´evry and jason phang.
2018. unsuper-vised sentence compression using denoising auto-encoders.
in proceedings of the 22nd conference oncomputational natural language learning, pages413–422, brussels, belgium.
association for com-putational linguistics..prakhar ganesh and saket dingliwal.
2019. abstrac-tive summarization of spoken and written conversa-tion.
arxiv preprint arxiv:1902.01615..chih-wen goo and yun-nung chen.
2018. abstrac-tive dialogue summarization with sentence-gatedmodeling optimized by dialogue acts.
in ieee spo-ken language technology workshop (slt), pages735–742..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
in internationalconference on learning representations..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
text summarizationbranches out, 8..chunyi liu, peng wang, jiang xu, zang li, andjieping ye.
2019. automatic dialogue summarygeneration for customer service.
in proceedings ofthe 25th acm sigkdd international conference onknowledge discovery & data mining, pages 1957–1965..yang liu and mirella lapata.
2019. text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..chris j maddison, daniel tarlow, and tom minka.
2014. a* sampling.
in conference and workshopon neural information processing systems, pages3086–3094..inderjeet mani.
2001. summarization evaluation: an.
overview..6050joint conference on natural language processing(emnlp-ijcnlp), pages 3752–3761, hong kong,china.
association for computational linguistics..hao zheng and mirella lapata.
2019. sentence cen-trality revisited for unsupervised summarization.
intheproceedings ofassociation for computational linguistics, pages6236–6247, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
jiawei zhou and alexander rush.
2019. simple un-supervised summarization by contextual matching.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages5101–5106, florence, italy.
association for compu-tational linguistics..yishu miao and phil blunsom.
2016. language as alatent variable: discrete generative models for sen-tence compression.
in proceedings of the 2016 con-ference on empirical methods in natural languageprocessing, pages 319–328, austin, texas.
associa-tion for computational linguistics..rada mihalcea and paul tarau.
2004..textrank:bringing order into text.
in proceedings of the 2004conference on empirical methods in natural lan-guage processing, pages 404–411, barcelona, spain.
association for computational linguistics..ramesh nallapati, feifei zhai, and bowen zhou.
2017.summarunner: a recurrent neural network based se-quence model for extractive summarization of docu-ments.
in proceedings of the aaai conference onartiﬁcial intelligence, pages 3075–3081..tatsuro oya, yashar mehdad, giuseppe carenini, andraymond ng.
2014. a template-based abstractivemeeting summarization: leveraging summary andsource text relationships.
in proceedings of the 8thinternational natural language generation confer-ence (inlg), pages 45–53, philadelphia, pennsylva-nia, u.s.a. association for computational linguis-tics..gaetano rossiello, pierpaolo basile, and giovannisemeraro.
2017. centroid-based text summariza-tion through compositionality of word embeddings.
in proceedings of the multiling 2017 workshopon summarization and summary evaluation acrosssource types and genres, pages 12–21, valencia,spain.
association for computational linguistics..raphael schumann, lili mou, yao lu, olga vech-tomova, and katja markert.
2020. discrete opti-mization for unsupervised sentence summarizationin proceedings of thewith word-level extraction.
58th annual meeting of the association for compu-tational linguistics, pages 5032–5042, online.
as-sociation for computational linguistics..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..lu wang and claire cardie.
2013..domain-independent abstract generation for focused meetingsummarization.
in proceedings of the 51st annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 1395–1405,soﬁa, bulgaria.
association for computational lin-guistics..peter west, ari holtzman, jan buys, and yejinchoi.
2019. bottlesum: unsupervised and self-supervised sentence summarization using the infor-mation bottleneck principle.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th international.
6051