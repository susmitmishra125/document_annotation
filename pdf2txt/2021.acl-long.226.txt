lightweight cross-lingual sentence representation learning.
zhuoyuan mao♠chenhui chu♠ martin jaggi♣♠kyoto university, japan.
prakhar gupta♣.
sadao kurohashi♠.
♣epfl, switzerland.
zhuoyuanmao, chu, kuro}.
{.
prakhar.gupta, martin.jaggi}.
{.
@epfl.ch.
@nlp.ist.i.kyoto-u.ac.jp.
abstract.
for.
improvement.
large-scale modelslearning ﬁxed-dimensional cross-lingual sentence represen-tations like laser (artetxe and schwenk,2019b) lead to signiﬁcantinperformance on downstream tasks.
however,further increases and modiﬁcations based onsuch large-scale models are usually impracti-cal due to memory limitations.
in this work,we introduce a lightweight dual-transformerarchitecture with just 2 layers for generatingmemory-efﬁcient cross-lingual sentence repre-sentations.
we explore different training tasksand observe that current cross-lingual trainingtasks leave a lot to be desired for this shallowarchitecture.
to ameliorate this, we proposea novel cross-lingual language model, whichcombines the existing single-word maskedlanguage model with the newly proposedcross-lingual token-level reconstruction task.
the training task bywe further augmentthe introduction of two computationally-litesentence-level contrastive learning tasks to en-hance the alignment of cross-lingual sentencerepresentation space, which compensates forthe learning bottleneck of the lightweighttransformer for generative tasks.
our compar-isons with competing models on cross-lingualsentence retrieval and multilingual documentclassiﬁcation conﬁrm the effectiveness of thenewly proposed training tasks for a shallowmodel.
1.
1.introduction.
sentence.
cross-lingualrepresentation mod-els (schwenk and douze, 2017; espa˜na-bonetet al., 2017; yu et al., 2018; devlin et al., 2019;chidambaram et al., 2019; artetxe and schwenk,2019b; kim et al., 2019; sabet et al., 2019;conneau and lample, 2019; feng et al., 2020; li.
1https://github.com/mao-ku/lightweight-crosslingual-sent2vec.
and mak, 2020) learn language-agnostic represen-tations facilitating tasks like cross-lingual sentenceretrieval(xsr) and cross-lingual knowledgetransfer on downstream tasks without the need fortraining a new monolingual representation modelfrom scratch.
thus, such models beneﬁt froman increased amount of data during training andlead to improved performances for low-resourcelanguages..the above-mentioned models can be categorizedinto two classes.
on one hand, global ﬁne-tuningmethods like mbert (devlin et al., 2019) andxlm (conneau and lample, 2019) require beingﬁne-tuned globally which results in a signiﬁcantoverhead of its own.
on the other hand, ﬁxed-dimensional methods like laser (artetxe andschwenk, 2019b) ﬁx the sentence representationsduring the pre-training phase, and subsequently theﬁne-tuning for speciﬁc downstream tasks withoutback-propagating to the pre-trained model will beextremely computationally-lite.
lightweight mod-els have been sufﬁciently explored for the formergroup by either shrinking the model (lan et al.,2020) or training a student model (sanh et al., 2019;jiao et al., 2020; reimers and gurevych, 2020; sunet al., 2020).
however, the lightweight modelsfor the latter group have not been explored before,which may have a more promising future for de-ploying task-speciﬁc ﬁne-tuning onto edge devices.
in this work, we propose a variety of train-ing tasks for a lightweight cross-lingual sentencemodel while retaining the robustness.
to im-prove the computational efﬁciency, we utilize alightweight dual-transformer architecture with just2 layers, signiﬁcantly decreasing the memory con-sumption and accelerating the training to furtherimprove the efﬁciency.
our model uses signiﬁ-cantly less number of parameters compared to bothglobal ﬁne-tuning methods like mbert, and ﬁxed-dimensional representation methods like laser,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2902–2913august1–6,2021.©2021associationforcomputationallinguistics2902method.
architecture.
dh.
df c.attn h enc.
dec. params..mbert (devlin et al., 2019)laser (artetxe and schwenk, 2019b) bi-lstmt-laser (li and mak, 2020).
transformer.
transformer.
512.
7682×1,024.
3,072n/a4,096.ours.
transformer.
512.
1,024.
12n/a16.
8.
12 n/a51.
56.
2 n/a.
110m154m246m.
30m.
table 1: model sizes of related work and ours.
our work mainly focuses on the comparison with previous ﬁxed-dimensional methods like laser, t-laser, etc.
dh , dfc, attnh , enc., dec., params.
denote dimension of thehidden state, dimension of the feed-forward hidden state, number of the attention heads, number of the encoderlayers, number of the decoder layers, and number of the parameters respectively..and t-laser (li and mak, 2020) (see table 1)..given a ﬁxed training-set and model architec-ture, the robustness of the sentence representa-tion is dependent on the training task.
it is muchmore difﬁcult for a lightweight model to learn ro-bust representations merely with existing genera-tive tasks (see section 2 and section 4.5), whichin ordercould be attributed to its smaller size.
to ameliorate this problem, we redesign a cross-lingual language model by combining the single-word masked language model (smlm) with cross-lingual token-level reconstruction (xtr).
further-more, we introduce two contrastive learning meth-ods as auxiliary tasks to compensate for the learn-ing bottleneck of lightweight transformer for gen-erative tasks.
following the state-of-the-art ﬁxed-dimensional model laser, we proceed to learncross-lingual sentence representations from par-allel sentences, where we employ 2-layer dual-transformer encoders to shrink the model architec-ture.
by introducing the above-stated training tasks,we establish a computationally-lite framework fortraining cross-lingual sentence models..we evaluate the learned sentence representationson cross-lingual tasks including multilingual doc-ument classiﬁcation (mldoc) (schwenk and li,2018) and xsr.
our results conﬁrm the ability ofour lightweight model to yield robust sentence rep-resentations.
we also do a systematic study on theperformance of our model in an ablative manner.
the contributions of this work can be summarizedas follows:.
• we implement ﬁxed-dimensional cross-lingual sentence representation learning in alightweight model, achieving improved train-ing efﬁciency and competitive performance ofthe learned sentence representations..• our proposed novel generative and contrastivetasks allow cross-lingual sentence representa-.
tion efﬁciently trainable by the lightweightmodel.
the contribution from each task isempirically analyzed..2 related work.
a majority of training tasks for learning ﬁxed-dimensional cross-lingual sentence representationscan be ascribed to one of the following 2 categories:generative or contrastive.
in this section, we revisitthe previous work in these 2 categories, which iscrucial for designing a cross-lingual representationmodel.
generative tasks.
generative tasks measure agenerative probability between predicted tokensand real tokens by training a language model.
bert-style mlm (devlin et al., 2019) masks andpredicts contextualized tokens within a given sen-tence.
for the cross-lingual scenario, cross-lingualsupervision is implemented by shared cognates andjoint training (devlin et al., 2019), concatenatingsource sentences in multiple languages (conneauand lample, 2019; conneau et al., 2020a) or ex-plicitly predicting the translated token (ren et al.,2019).
the [cls] embedding or pooled embed-ding of all the tokens is introduced as the classi-ﬁer embedding, which can be used as sentenceembedding for sentence-level tasks (reimers andgurevych, 2019).
sequence to sequence meth-ods (schwenk and douze, 2017; espa˜na-bonetet al., 2017; artetxe and schwenk, 2019b; li andmak, 2020) autoregressively reconstruct the trans-lation of the source sentence.
the intermediatestate between the encoder and the decoder are ex-tracted as sentence representations.
particularly,the cross-lingual sentence representation qualityof laser (artetxe and schwenk, 2019b) beneﬁtsfrom a massively multilingual machine translationtask covering 93 languages.
in our work, we revisitthe bert-style training tasks and introduce a novel.
2903generative loss enhanced by kl-divergence basedtoken distribution prediction.
our proposed gener-ative task performs effectively for the lightweightdual-transformer framework while other generativetasks should be implemented via a large-capacitymodel.
contrastive tasks.
contrastive tasks measure(contrast) the similarities of sample pairs in therepresentation space.
negative sampling, whichis a typical feature of the contrastive methods isﬁrst introduced in the work of word representa-tion learning (mikolov et al., 2013).
subsequently,contrastive tasks gradually emerged in many nlptasks in various ways: negative sampling in knowl-edge graph embedding learning (bordes et al.,2013; wang et al., 2014), next sentence predic-tion in bert (devlin et al., 2019), token-leveldiscrimination in electra (clark et al., 2020),sentence-level discrimination in declutr (giorgiet al., 2020), and hierarchical contrastive learningin hictl (wei et al., 2020).
for the cross-lingualsentence representation training, typical ones in-clude using correct and wrong translation pairsintroduced by guo et al.
(2018); yang et al.
(2019);chidambaram et al.
(2019); feng et al.
(2020) orutilizing similarities between sentence pairs by in-troducing a regularization term (yu et al., 2018).
as another advantage, contrastive methods haveproven to be more efﬁcient than generative meth-ods (clark et al., 2020).
inspired by previous work,for our lightweight model, we propose a robustsentence-level contrastive task by leveraging simi-larity relationships arising from translation pairs..3 methodology.
we perform cross-lingual sentence representationlearning by a lightweight dual-transformer frame-work.
concerning the training tasks, we proposea novel cross-lingual language model, which com-bines smlm and xtr.
moreover, we introducetwo sentence-level self-supervised learning tasks(sentence alignment and sentence similarity losses)to leverage robust parallel level supervision to bet-ter conduct the cross-lingual sentence representa-tion space alignment..3.1 architecture.
we employ the dual transformer sharing parameterswithout any decoder as the basic unit to encodeparallel sentences respectively, to avoid the lossin efﬁciency caused by the presence of a decoder..unlike xlm (conneau and lample, 2019), weutilize a dual model architecture rather than a singletransformer to encode sentence pairs, because itcan force the encoder to capture more cross-lingualcharacteristics (reimers and gurevych, 2019; fenget al., 2020).
moreover, we decrease the number oflayers and embedding dimension to accelerate thetraining phase, as shown in table 1..the architecture of the proposed method is illus-trated in figure 1 (left).
we build sentence represen-tations on the top of 2-layer transformer (vaswaniet al., 2017) encoders by a mean-pooling opera-tion from the ﬁnal states of all the positions withina sentence.
pre-trained sentence representationsfor downstream tasks are denoted by u and v,which are used to compute the loss for the sentence-level contrastive task.
moreover, we add a fully-connected layer before computing the loss of thecross-lingual language model inspired by chenet al.
(2020).
this linear layer can enhance ourlightweight model by a nontrivial margin, becausethe hidden state for computing loss for the genera-tive task is far different from the sentence presen-tation we aim to train.
two transformer encodersand linear layers share parameters, which has beenproved effective and necessary for cross-lingualrepresentation learning (conneau et al., 2020b)..3.2 generative task.
smlm.
smlm is proposed by sabet et al.
(2019),which is a variant of the standard mlm in bert(devlin et al., 2019).
smlm can enforce the mono-lingual performance, because the prediction of anumber of masked tokens in mlm is too compli-cated for the shallow transformer encoder to learn.2inspired by this, we implement smlm by a dualtransformer architecture.
the transformer encoderfor language l1 predicts a masked token in a sen-tence in l1 as the monolingual loss.
the language l2encoder sharing all the parameters with l1 encoderpredicts the same masked token by the correspond-ing sentence (translation in l2) as the cross-lingualloss, as shown in figure 1 (top right).
speciﬁcally,for a parallel corpus c and language l1 and l2, theloss of smlm computed from l1 encoder el1 and.
2a detailed comparison between smlm and mlm underour lightweight model setting is conducted (see section 4.5)..2904figure 1: architecture of the proposed model (left), proposed uniﬁed generative task (top right), and pro-posed sentence-level contrastive task (bottom right).
in the left sub-ﬁgure, [m] denotes the masked tokenintroduced by smlm.
hidden states u and v are 512-dimensional sentence representations for the sentence-levelcontrastive task and for downstream tasks.
in the top right sub-ﬁgure, smlm is inspired by sabet et al.
(2019);xtr and ugt are our proposed methods.
q1 and q2 respectively denote 2 distributions at the top of left sub-ﬁgure,the token distributions that we introduce as labels for the model to learn.
in the bottom right sub-ﬁgure, n de-notes the size of a mini-batch.
indicates the sentencerepresentation of the i-th l1 sentence in the mini-batch and same for j in (cid:50)..and (cid:50) represent language l1 and l2, respectively.
i in.
(cid:13).
(cid:13).
l2 encoder el2 is formulated as:.
sm lm =.
l.(cid:88).
(cid:8).
log (cid:0)p (wt.
sl\{wt}; θ)(cid:1)|.
−.
s∈cl,l(cid:48)∈{l1,l2}l(cid:54)=l(cid:48).
log (p (wt.
−.
sl(cid:48); θ)) (cid:9)|(1).
where wt is the word to be predicted, sl1\{wt} isa sentence in which wt is masked, s = (sl1, sl2)denotes a parallel sentence pair, θ represents theparameters to be trained in el1 and el2, and theclassiﬁcation probability p is computed by soft-max on the top of the embedding layer.
xtr.
inspired by laser, we also use a recon-struction loss.
however, introducing a decoder toimplement the translation loss like laser will in-crease the computational overhead associated withour model, which contradicts with our objective todesign a computationally-lite model architecture.
to implement the reconstruction loss with justthe encoder, we propose a xtr loss by which wejointly enforce the encoder to reconstruct the worddistribution of corresponding target sentence asshown by q in figure 1 (top right).
speciﬁcally, weutilize the following kl-divergence based formu-lation as the training loss:.
xm lm = (cid:80)l.s∈cl,l(cid:48)∈{l1,l2}l(cid:54)=l(cid:48).
(cid:8).
− d.(cid:0)p (hsl; θ).
kl.
q (cid:0)wsl(cid:48).
(cid:1)(cid:1).
(cid:107).
(cid:0)p (cid:0)hsl(cid:48) ; θ(cid:1).
kl.
−d.
q (wsl)(cid:1) (cid:9)(2).
(cid:107).
d.wherekl denotes kl-divergence based loss,p (hsl; θ) represents the hidden state on the topof encoder el as shown in figure 1 (left) under theinput sl, and wsl indicates the set that contains allthe tokens in sl.
we utilize discrete uniform distri-bution for the tokens in target language to deﬁne qfor wsl.
speciﬁcally, q (wsl) is deﬁned as:.
q (wi) =.
.
.
nwisl(cid:48)(cid:107).
,.
(cid:107)0,.wi.
∈wi /∈.
sl(cid:48).
sl(cid:48).
(3).
sl(cid:48).
where nwi indicates the number of words wi inindicates the length of sl(cid:48).3sentence sl(cid:48) anduniﬁed generative task (ugt).
finally, weunify smlm (eq.
(1)) and xtr (eq.
(2)) byredeﬁning the label distribution q (wsl) for kl-divergence based loss.
as shown in figure 1 (top.
(cid:107).
(cid:107).
3we set all the nwi to be 1 in the current implementa-tion.
word frequency will be taken into consideration for thegenerative task in future work..2905joint vocabulary2-layertransformer encodermean-poollinearsoftmaxi [m] a student .𝐷!"𝑞(𝒘!!
"∖”$%”)2-layertransformer encodermean-poollinearsoftmaxje suis un étudiant .𝐷!
"true labels for generative task𝑞(𝒘!#$)share weightsshare weights~sentence-levelcontrastive task𝒖𝒗embedding layerembedding layer3a mini-batch12n12n............ℒ$&’():11~22~….12~12...13~13…3~ℒ*’%:..23~23𝑛𝑛(𝑛−1)language 1language 2.....right), the model is forced to learn under the su-pervision of a biased cross-lingual probability dis-tribution of tokens.
it is formulated the same aseq.
(3) if the token wt is masked from sl(cid:48), else ifwt is masked within sl:.
q (wi) =.
(4).
.
2.,.
nwisl(cid:48)(cid:107)1/2,.
(cid:107).
0,.wi.
sl(cid:48).
∈wi = wtothers.
3.3 sentence-level contrastive task.
meanwhile, as shown in figure 1 (bottom right),we introduce two auxiliary similarity-based train-ing tasks to strengthen sentence-level supervision.
we construct these two assisting tasks on the basisof mean pooled sentence representations, aimingto capture sentence similarity information acrosslanguages..inspired by guo et al.
(2018); yang et al.
(2019);feng et al.
(2020), we propose a sentence alignmentloss.
the sentence alignment loss aims to force thetransformer model to recognize the sentence pair,where one sentence is the translation of the other.
one positive and other negative samples contributeto the gradient update in a single batch, which pro-vides contrastive training patterns for the modeltraining.
for contrastively discriminating positiveand negative samples, we use (batchsize2negative samples.4 this indicates all the sentenceswithin a batch except the positive one will be nega-tive samples..1).
−.
×.
more precisely, assuming the mean pooled sen-tence representations of sl1 and sl2 are u(sl1)and v(sl2).
assume that bi is a speciﬁc batchof several paired sentences, uij and vij respec-tively indicate the representation of j-th sentences(j) = (s) in language l1 and l2 withinbatch bi.
note that the masked token wt is omit-ted in the following equations.
the above-proposedin-batch sentence alignment loss to align sentencepairs is deﬁned as:.
(j)l2.
(j)l1.
, s.align =.
l.−.
(cid:88).
(cid:88).
i.j.
(log.
(cid:80).
ijvik).
ijvij).
exp (u(cid:62)k exp (u(cid:62)exp (u(cid:62)k exp (u(cid:62).
ijvij).
ikvij).
).
(5).
+ log.
(cid:80).
4for each language, there are batchsize − 1 negativesamples.
note that this contrastive task is different from thosein yang et al.
(2019) and feng et al.
(2020), where they utilizecosine similarity while we directly use the inner product toaccelerate the model..where s(k), s(j).
bi..∈.
we further introduce a sentence similarity lossto better align similarities for all the sentencepairs throughout a batch.
by constructing thesesimilarity-based sentence-level contrastive tasks,we hope that it can force the sentence representa-tions to be competent for sentence-level alignmentdownstream tasks.
speciﬁcally, in-batch sentencesimilarity loss,.
sim is formulated as:.
l.sim =.
l.−.
(cid:88).
(cid:88).
log cos.i.j.
(cid:40).
π2.
(.
(cid:80).
uij2).
exp (u(cid:62)ij1k exp (u(cid:62)ij1.
vij2).
exp (v(cid:62)ij1k exp (v(cid:62)ij1.
vik).
(cid:80).
−.
uik)(cid:41)).
(6).
where s(k), s(j).
bi.5.
∈.
l.in summary, eq.
(5) optimizes a loss for thecontrastive task by discriminating correct transla-tion from others for a given sentence, as shown infigure 1 (align in bottom right).
eq.
(6) alignsthe cross similarities between every sentence pairswithin a batch, as shown in figure 1 (sim in bot-tom right).
the similarity score matrix generatedby the inner product between sentence pairs in abatch will be trained to be a symmetrical matrixwith diagonal elements approximate to 1 after thesoftmax operation..l.3.4 weighted loss for generative and.
contrastive tasks.
we jointly minimize the loss of the generative taskand two auxiliary contrastive tasks with the weightcombination of (1, 2, 2):6.
(ω0, ω1, ω2) =.
xm lm + 2.align + 2.l.l.l.sim.
l.(7).
l.wherexm lm denotes the loss of eq.
(2) and thelabel distribution for kl-divergence based loss isthe uniﬁed reconstruction distribution formulatedby eq.
(4).
sim represent the losses inalign andeq.
(5) and eq.
(6), respectively..l.l.5with regard to eq.
6, log cos is employed for implement-ing a regression loss because we focused on the hidden statesafter softmax that indicate the probabilities.
we will con-sider using mse loss on the states before softmax in futureexploration..6we assign a bigger weight for contrastive tasks accord-ing to the task discrepency between the generative task andcontrastive tasks introduced by sentence pair similarities..29064 experiments.
we evaluate our cross-lingual sentence representa-tion models by cross-lingual document classiﬁca-tion and bitext mining for these 2 main downstreamtasks belong to 2 groups: unrelated and related tothe training task.
for the former, we select ml-doc (schwenk and li, 2018) to evaluate the clas-siﬁer transfer ability of the cross-lingual model,while for the latter we conduct sentence retrievalon another parallel dataset europarl7 to evaluatethe performance of our models..4.1 conﬁguration details.
language pair.
en-fr.
en-de.
en-es.
en-it.
rawfiltered.
51.3m 36.9m 39.0m 22.1m37.8m 29.6m 32.8m 17.3m.
table 2: training data overview.
number of raw andﬁltered parallel sentences from paracrawl v5.0..we build our pytorch implementation on topof huggingface’s transformers library (wolfet al., 2020).
training data is composed of theparacrawl8 (ba˜n´on et al., 2020) v5.0 datasets foreach language pair.
we experiment on english–french, english–german, english–spanish andenglish–italian.
we ﬁlter the parallel corpus foreach language pair by removing sentences thatcover tokens out of 2 languages.
raw and ﬁlterednumber of the parallel sentences for each pair areshown in table 2.
10,000 sentences are selected forvalidation on each language pair.
we tokenize sen-tences by sentencepiece9 (kudo, 2018) and builda shared vocabulary with the size of 50k for eachlanguage pair..for each encoder, we use the transformer ar-chitecture with 2 hidden layers, 8 attention heads,hidden size of 512 and ﬁlter size of 1,024, and theparameters of two encoders are shared with eachother.
the sentence representations generated are512 dimensional.
for the training phase, it mini-mizes the weighted losses for our proposed cross-lingual language model jointly with 2 auxiliarytasks.
we train 12 epochs for each language pair(30 epochs for english-italian because of nearlyhalf number of parallel sentences) with the adam.
7https://www.statmt.org/europarl/8http://opus.nlpl.eu/paracrawl-v5.php9https://github.com/google/.
sentencepiece.
optimizer, learning rate of 0.001 with warm-upstrategy for 3 epochs (6 epochs for english-italian)and dropout-probability of 0.1 on a single titanx pascal gpu with the batch size of 128 pairedsentences.
training loss for each language pair canconverge within 10 gpu (12gb)days, which isfar more efﬁcient than most cross-lingual sentencerepresentation learning methods.10.
×.
4.2 baselines.
for evaluation on the mldoc benchmark,we use the state-of-the-art ﬁxed-dimensionalword representation methods multicca+cnnmethod (schwenk and li, 2018) and bi-sent2vec (sanh et al., 2019), the representativeﬁxed-dimensional sentence representation meth-ods (yu et al., 2018), laser (artetxe andschwenk, 2019b), and t-laser (li and mak,2020) as baselines.
in addition, as referenceonly, we present the results of the global ﬁne-tuning methods, mbert (devlin et al., 2019)and the state-of-the-art bert-based variant, multi-fit (eisenschlos et al., 2019)..for the xsr task, bilingual ﬁxed-dimensionalmethods, bi-vec (luong et al., 2015) & bi-sent2vec(sabet et al., 2019), and multilin-gual ﬁxed-dimensional methods, transgram (coul-mance et al., 2015) & laser (artetxe andschwenk, 2019b) are used as baselines..note that t-laser and laser are trained on223m parallel sentences on 93 languages, whichuses signiﬁcantly more training data than ours..we also show the results by comparingwith (reimers and gurevych, 2020) in appendix a,which is a recent work using global ﬁne-tuningmethods to generate multilingual sentence repre-sentations..4.3 mldoc: zero-shot cross-lingual.
document classiﬁcation.
the mldoc task, which consists of news docu-ments given in 8 different languages, is a bench-mark to evaluate cross-lingual sentence represen-tations.
we conduct our evaluations in a zero-shot scenario: we train and validate a new linear.
10note that it is impractical to compare the efﬁciency withlaser, which is trained by 80 v100 gpu×days due todifferent training data settings.
however, it is obvious thatour lightweight model is signiﬁcantly more efﬁcient thanthe 5-layer lstm-based encoder-decoder model structure oflaser, because of the parallel computing nature of the trans-former encoder (vaswani et al., 2017) of our model withoutany decoder..2907method.
ﬁxed-dimensional word representation methodsmulticca + cnn (schwenk and li, 2018)bi-sent2vec (sabet et al., 2019)ﬁxed-dimensional sentence representation methodsyu et al.
(2018)laser (artetxe and schwenk, 2019b)t-laser (li and mak, 2020).
oursreference: global ﬁne-tuning style methodsmbert (devlin et al., 2019)multifit (eisenschlos et al., 2019).
en-fr.
en-de.
en-es.
en-it.
→ ← → ← → ← → ←.
72.481.6.
64.882.2.
81.286.5.
56.079.2.
72.574.0.
74.071.5.
69.475.0.
53.772.6.
80.878.070.7.
81.080.178.2.
80.286.386.8.
77.180.879.0.
74.179.371.4.
74.169.674.5.
70.870.268.7.
74.874.276.0.
85.1.
82.4.
88.8.
80.8.
80.8.
79.2.
74.3.
79.9.
81.4.
83.089.4.
--.
82.491.6.
--.
75.079.1.
--.
68.376.0.
--.
--.
avg..68.077.8.
76.677.375.7.table 3: mldoc benchmark results (zero-shot scenario).
we compare our models primarily with ﬁxed-dimensional models in which bi-sent2vec and laser are state-of-the-art bag-of-words based and contextualsentence representation models, respectively.
we also compare with global ﬁne-tuning style methods here forreference.
each result is the mean value of 5 runs..method.
bilingual representation methodsbi-vec (luong et al., 2015)bi-sent2vec (sabet et al., 2019).
oursmultilingual representation methodstransgram (coulmance et al., 2015)laser (artetxe and schwenk, 2019b).
en-fr.
en-de.
en-es.
en-it.
avg..→ ← → ← → ← → ←.
81.687.4.
83.487.8.
71.684.0.
68.184.2.
81.689.6.
83.489.7.
74.287.6.
72.487.9.
77.087.3.
90.2.
90.8.
86.3.
86.9.
90.7.
91.2.
86.9.
87.6.
88.8.
80.495.3.
81.694.7.
72.794.6.
69.194.3.
83.894.5.
82.794.1.
77.995.6.
77.295.6.
78.294.8.table 4: cross-lingual sentence retrieval results.
we report p@1 scores of 2,000 source queries when searchingamong 200k sentences in the target language.
here global ﬁne-tuning style methods are not considered, becausethey require training data to be ﬁne-tuned.
best performances among bilingual representation methods are in bold..classiﬁer on the top of the pre-trained sentencerepresentations in the source language, and thenevaluate the classiﬁer on the test set for the targetlanguage.
we implement the evaluation by face-book’s mldoc library.11 as shown in table 3,our lightweight transformer model obtains the bestresults for most language pairs compared with pre-vious ﬁxed-dimensional word and sentence repre-sentation learning methods.
our methods yieldonly slightly worse performance even when com-pared with the state-of-the-art global ﬁne-tuningstyle method, multifit (eisenschlos et al., 2019),on this task.
this is because the entire model willbe updated in the ﬁne-tuning phase, which indi-cates more parameters will be task-speciﬁc afterﬁne-tuning.
for ﬁxed-dimensional methods, just an.
11https://github.com/facebookresearch/.
mldoc.
additional dense layer will be trained, which leadsto their higher efﬁciency..4.4 xsr: cross-lingual sentence retrieval.
we also conduct an evaluation to gauge the qual-ity of our cross-lingual sentence representationson the bitext mining task, which is identical tosome components of the training task.
speciﬁcally,given 2,000 sentences in the source language, weconduct the corresponding sentence retrieval from200k sentences in the target language.
p@1 scoresof our lightweight models and previous bilingualrepresentation methods calculated by artetxe andschwenk (2019a) are reported.
as shown in ta-ble 4, we observe that our lightweight models out-perform the bilingual pooling-based representationlearning methods by a signiﬁcant margin, whichreﬂects the basic ability of the contextualized rep-.
2908m t.en.
fr.
mldocenen.
es.
en.
fr.
xsren.
fr.
→81.785.184.284.283.0.
→79.482.481.982.080.8.es.
→75.580.881.281.179.8.en.
→74.979.278.178.778.3.fr.
→89.490.290.991.491.2.en.
→90.090.891.591.592.0.es.
→86.490.791.191.591.7.es.
en.
→87.791.292.092.291.9.
7,13511,60716,80421,92328,024.
1924293444.n.12346.table 5: training efﬁciencies with different numbers of layers.
n denotes number of layers within the trans-former encoder; m and t indicate memory overhead (mb) and training time (min), respectively.
memory overheadchanges for different languages and here we report the numbers on english–french.
training time is measuredevery 10,000 training steps.
the results are reported by using a single v100 gpu card with the batch size of 128sentences.
2-layer is the default setting for our lightweight model..tasks.
mlmsmlmxtrmlmxtr⊕ugt (smlm.
xtr).
⊕.
mldocenen.
xsren.
en.
fr.
es.
en.
fr.
fr.
→78.575.084.282.285.1.
→77.678.781.278.282.4.es.
→74.675.379.978.480.8.en.
→75.974.077.676.779.2.fr.
→19.685.089.584.189.8.en.
→25.485.390.885.090.6.es.
→11.286.490.387.689.4.es.
en.
→28.587.189.588.989.6.table 6: effectiveness of different generative tasks.
ugt indicates “smlmtraining task combining smlm and xtr.
mlmxtr..xtr”, which indicates the⊕xtr denotes the uniﬁed training task combining mlm and.
⊕.
resentations generated by our lightweight models.
however, our lightweight models underperformlaser, which can be attributed to our lightweightcapacities and bilingual settings.
note that laseruses signiﬁcantly larger multilingual training data(see section 4.2)..4.5 analyses.
we perform ablation experiments to conﬁrm theefﬁciency and the effectiveness of each trainingtask for our models.
analyses for other hyper-parameter conﬁgurations of batch size, sentencerepresentation dimension, and training corpus sizeare presented in appendix a.relation among number of layers, efﬁciency,and performances.
we report the efﬁciency statis-tics and performances of our proposed methodstrained by different layer number settings.
asshown in table 5, we observe a linear increase ofmemory occupation and training time per 10,000training steps by increasing the number of trans-former encoder layers.
speciﬁcally, a 6-layer trans-former encoder occupies nearly 2.5 times memoryand costs 1.8 times training time compared to our.
×.
≈.
2.5.
2-layer model.
therefore, given the same memoryoccupation (by adjusting the batch size), theoreti-cally our lightweight model can be implemented1.8) faster than the 6-layerover 4 times (model.
concerning the respective performances onmldoc and xsr, we see that lightweight modelwith 2 transformer layers obtains the peak perfor-mance on mldoc, and the performances decreasewhen we add more layers.
this indicates that the2-layer transformer encoder is an ideal structure forour proposed training tasks on the document clas-siﬁcation task.
on the other hand, performanceson xsr keep increasing gradually with more lay-ers, where the 1-layer model can even yield decentperformance on this task..our proposed training tasks perform well fromthe 2-layer model, while 6 layers are required forstandard mlm and 5 lstm layers are required forlaser.
this is why we use 2-layer as the basicunit for our model..effectiveness of different generative tasks.
wereport the results with different generative tasks intable 6. we observe that xtr outperforms othergenerative tasks by a signiﬁcant margin on both.
2909mldocenen.
tasks.
ugt+ align+ align + sim.
en.
fr.
es.
en.
fr.
fr.
→85.184.182.3.
→82.481.980.3.es.
→80.878.777.6.en.
→79.277.976.2.fr.
→89.889.990.2.en.
→90.690.490.8.es.
→89.489.890.7.es.
en.
→89.690.991.2.xsren.
table 7: effectiveness of the contrastive tasks.
ugt indicates the training without any sentence-level contrastivetasks..mldoc and xsr downstream tasks.
xtr yieldsfurther improvements when uniﬁed with smlm,which is introduced as the generative task in ourmodel.
this demonstrates the necessity of a well-designed generative task for the lightweight dual-transformer architecture.
effectiveness of the contrastive tasks.
in ta-ble 7, we study the contribution of the sentence-level contrastive tasks.
we observe that a higherperformance on mldoc is yielded by the vanillamodel while more sentence-level contrastive tasksimprove the performance on xsr.
this can beattributed to the similar nature between the super-vision provided by sentence-level contrastive tasksand xsr process.
in other words, contrastive-styletasks have a detrimental effect on the documentclassiﬁcation downstream task.
in future work, wewill explore how to train a balanced sentence rep-resentation model with contrastive tasks..5 conclusion.
in this paper, we presented a lightweight dual-transformer based cross-lingual sentence represen-tation learning method.
for the ﬁxed 2-layer dual-transformer framework, we explored several gener-ative and contrastive tasks to ensure the sentencerepresentation quality and facilitate the improve-in spite of thement of the training efﬁciency.
lightweight model capacity, we reported substan-tial improvements on mldoc compared to ﬁxed-dimensional representation methods and we ob-tained comparable results on xsr.
in the future,we plan to verify whether our proposed methodscan be combined with knowledge distillation..acknowledgements.
we would like to thank all the reviewers for theirvaluable comments and suggestions to improve thispaper.
this work was partially supported by grant-in-aid for young scientists #19k20343, jsps..references.
mikel artetxe and holger schwenk.
2019a.
margin-based parallel corpus mining with multilingual sen-in proceedings of the 57th an-tence embeddings.
nual meeting of the association for computationallinguistics, pages 3197–3203, florence, italy.
asso-ciation for computational linguistics..mikel artetxe and holger schwenk.
2019b.
mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transac-tions of the association for computational linguis-tics, 7:597–610..marta ba˜n´on, pinzhen chen, barry haddow, ken-neth heaﬁeld, hieu hoang, miquel espl`a-gomis,mikel l. forcada, amir kamran, faheem kirefu,philipp koehn, sergio ortiz rojas, leopoldopla sempere, gema ram´ırez-s´anchez, elsasarr´ıas, marek strelec, brian thompson, williamwaites, dion wiggins, and jaume zaragoza.
2020.paracrawl: web-scale acquisition of parallel cor-in proceedings of the 58th annual meetingpora.
ofthe association for computational linguis-tics, pages 4555–4567, online.
association forcomputational linguistics..antoine bordes, nicolas usunier, alberto garc´ıa-dur´an,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 2787–2795..ting chen, simon kornblith, mohammad norouzi,and geoffrey e. hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning, icml 2020, 13-18 july 2020,virtual event, pages 1597–1607..muthu chidambaram, yinfei yang, daniel cer, steveyuan, yunhsuan sung, brian strope, and raykurzweil.
2019. learning cross-lingual sentencerepresentations via a multi-task dual-encoder model.
in proceedings of the 4th workshop on represen-tation learning for nlp (repl4nlp-2019), pages250–259, florence, italy.
association for computa-tional linguistics..2910kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020a.
unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, 8-14 december 2019,vancouver, bc, canada, pages 7057–7067..alexis conneau, shijie wu, haoran li, luke zettle-moyer, and veselin stoyanov.
2020b.
emergingcross-lingual structure in pretrained language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 6022–6034, online.
association for compu-tational linguistics..jocelyn coulmance, jean-marc marty, guillaume wen-zek, and amine benhalloum.
2015. trans-gram,in proceed-fast cross-lingual word-embeddings.
ings of the 2015 conference on empirical meth-ods in natural language processing, pages 1109–1113, lisbon, portugal.
association for computa-tional linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..julian eisenschlos, sebastian ruder, piotr czapla,marcin kadras, sylvain gugger,and jeremyhoward.
2019. multifit: efﬁcient multi-lingual lan-in proceedings of theguage model ﬁne-tuning.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5702–5707, hong kong,china.
association for computational linguistics..fangxiaoyu feng, yinfei yang, daniel cer, naveenlanguage-corr,.
arivazhagan, and wei wang.
2020.agnostic bert sentence embedding.
abs/2007.01852..john m. giorgi, osvald nitski, gary d. bader, andbo wang.
2020. declutr: deep contrastive learn-ing for unsupervised textual representations.
corr,abs/2006.03659..mandy guo, qinlan shen, yinfei yang, hemingge, daniel cer, gustavo hernandez abrego, keithstevens, noah constant, yun-hsuan sung, brianstrope, and ray kurzweil.
2018. effective parallelcorpus mining using bilingual sentence embeddings.
in proceedings of the third conference on machinetranslation: research papers, pages 165–176, brus-sels, belgium.
association for computational lin-guistics..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in findings of the associationfor computational linguistics: emnlp 2020, pages4163–4174, online.
association for computationallinguistics..yunsu kim, hendrik rosendahl, nick rossenbach,jan rosendahl, shahram khadivi, and hermannney.
2019. learning bilingual sentence embed-dings via autoencoding and computing similaritieswith a multilayer perceptron.
in proceedings of the4th workshop on representation learning for nlp(repl4nlp-2019), pages 61–71, florence, italy.
as-sociation for computational linguistics..taku kudo.
2018. subword regularization: improvingneural network translation models with multiple sub-word candidates.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 66–75, mel-bourne, australia.
association for computationallinguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020..wei li and brian mak.
2020. transformer basedmultilingual document embedding model.
corr,abs/2008.08567..cristina espa˜na-bonet,.
´ad´am csaba varga, albertobarr´on-cede˜no, and josef van genabith.
2017. anempirical analysis of nmt-derived interlingual em-beddings and their use in parallel sentence identiﬁca-tion.
ieee j. sel.
top.
signal process., 11(8):1340–1350..thang luong, hieu pham, and christopher d. man-ning.
2015. bilingual word representations withmonolingual quality in mind.
in proceedings of the1st workshop on vector space modeling for naturallanguage processing, pages 151–159, denver, col-orado.
association for computational linguistics..2911tomas mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013. distributed rep-resentations of words and phrases and their com-in advances in neural informationpositionality.
processing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 3111–3119..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..nils reimers and iryna gurevych.
2020. makingmonolingual sentence embeddings multilingual us-in proceedings of theing knowledge distillation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 4512–4525,online.
association for computational linguistics..shuo ren, yu wu, shujie liu, ming zhou, and shuaima.
2019. explicit cross-lingual pre-training forin proceedingsunsupervised machine translation.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 770–779, hongkong, china.
association for computational lin-guistics..ali sabet, prakhar gupta, jean-baptiste cordonnier,robert west, and martin jaggi.
2019. robust cross-lingual embeddings from parallel sentences.
corr,abs/1912.12481..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
corr,abs/1910.01108..pages 2158–2170, online.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..zhen wang, jianwen zhang, jianlin feng, and zhengchen.
2014. knowledge graph embedding by trans-lating on hyperplanes.
in proceedings of the twenty-eighth aaai conference on artiﬁcial intelligence,july 27 -31, 2014, qu´ebec city, qu´ebec, canada,pages 1112–1119..xiangpeng wei, yue hu, rongxiang weng, luxi xing,heng yu, and weihua luo.
2020. on learninguniversal representations across languages.
corr,abs/2007.15960..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..yinfei yang, gustavo hern´andez ´abrego, steve yuan,mandy guo, qinlan shen, daniel cer, yun-hsuanim-sung, brian strope, and ray kurzweil.
2019.proving multilingual sentence embedding using bi-directional dual encoder with additive margin soft-max.
in proceedings of the twenty-eighth interna-tional joint conference on artiﬁcial intelligence, ij-cai 2019, macao, china, august 10-16, 2019, pages5370–5378.
ijcai.org..holger schwenk and matthijs douze.
2017. learn-ing joint multilingual sentence representations within proceedings of theneural machine translation.
2nd workshop on representation learning for nlp,pages 157–167, vancouver, canada.
association forcomputational linguistics..katherine yu, haoran li, and barlas oguz.
2018.multilingual seq2seq training with similarity lossin pro-for cross-lingual document classiﬁcation.
ceedings of the third workshop on representationlearning for nlp, pages 175–179, melbourne, aus-tralia.
association for computational linguistics..holger schwenk and xian li.
2018. a corpus for mul-tilingual document classiﬁcation in eight languages.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020. mobilebert:a compact task-agnostic bert for resource-limitedin proceedings of the 58th annual meet-devices.
ing of the association for computational linguistics,.
a appendices.
comparisons with reimers and gurevych (2020).
reimers and gurevych (2020) use the knowledgedistillation to train multilingual sentence embed-dings where pre-trained encoders are utilized toinitialize the teacher and student model, which isa kind of the global ﬁne-tuning style methods ﬁne-tuned by parallel sentences.
as the results on ml-doc and xsr shown in table 8, their multilingual.
2912method.
mldocreimers and gurevych (2020)oursxsrreimers and gurevych (2020)ours.
en-fr.
en-de.
en-es.
en-it.
avg..→ ← → ← → ← → ←.
68.085.1.
78.582.4.
77.688.8.
79.280.8.
72.780.8.
72.279.2.
68.574.3.
74.279.9.
73.981.4.
93.090.2.
92.390.8.
89.986.3.
89.286.9.
93.990.7.
92.991.2.
91.786.9.
91.487.6.
91.888.8.table 8: comparisons with reimers and gurevych (2020) on mldoc and xsr..batch size.
mldocfrfr.
en.
xsrfrfr.
en.
→82.984.182.9.en.
→82.681.981.1.
→89.690.290.2.en.
→90.390.890.7.
64128256.table 9: effect of the batch size..corpus size.
mldocfrfr.
en.
xsrfrfr.
en.
→82.582.583.085.1.en.
→80.780.381.582.4.
→90.890.590.290.2.en.
→90.591.291.090.8.
12.5%25%50%100%.
table 10: impact of the corpus size..representations yield good performance on bitextmining but perform poorly on classiﬁcation tasks.
this demonstrates the importance of exploring task-agnostic multilingual sentence representations likelaser and ours.
batch size.
we investigate the effect of the batchsize for contrastive tasks, where different batchsizes indicate the discrepancy of the negative sam-ple numbers.
as shown in table 9, larger batchharms the lightweight model based sentence repre-sentation learning and 128 is reported as the bestbatch size setting for our lightweight model.
fur-thermore, batch size of 128 allows the training to beassigned on 12gb gpu card while a larger batchsize requires more gpu memory.
corpus size.
we show the impact of the size of theparallel corpus on english-french in table 10. formldoc, we observe higher accuracy on larger cor-pus while for xsr, a small fraction of the large cor-pus sufﬁces to yield effective results.
this indicates.
figure 2: performance of different representation di-mensions on mldoc (.m) and xsr (.x).
arrows de-notes direction of zero-shot setting..that more parallel data improves the performanceon mldoc.
sentence representation dimension.
in fig-ure 2, we presentthe effect of the sentencerepresentation dimension.
512-dimensional sen-tence representations signiﬁcantly outperform 256-dimensional ones in our lightweight model.
more-over, representation size of 512 yields better per-formance without increasing the training time..2913en-fr.men-de.men-es.men-it.men-fr.xen-de.xen-es.xen-it.xlanguagepair72.575.077.580.082.585.087.590.0accuracy256→512→256←512←