to pos tag or not to pos tag: the impact of pos tags onmorphological learning in low-resource settings.
sarah moeller ling liu mans huldenuniversity of colorado boulderfirst.last@colorado.edu.
abstract.
part-of-speech (pos) tags routinely appear asfeatures in morphological tasks.
pos taggersare often one of the ﬁrst nlp tools devel-oped for low-resource languages.
however, asnlp expands to new languages it cannot as-sume that pos tags will be available to train apos tagger.
this paper empirically examinesthe impact of pos tags on two morphologicaltasks with the transformer architecture.
eachtask is run twice, once with and once with-out pos tags, on otherwise identical data fromten well-described languages and ﬁve under-documented languages.
we ﬁnd that the pres-ence or absence of pos tags does not havea signiﬁcant bearing on the performance ofin joint segmentation and gloss-either task.
ing, the largest average difference is an .09 im-provement in f1-scores by removing pos tags.
in reinﬂection, the greatest average differenceis 1.2% in accuracy for published data and 5%for unpublished data.
these results are indi-cators that nlp and documentary linguisticsmay beneﬁt each other even when a pos tagset does not yet exist for a language..1.introduction.
parts of speech (pos), also known as word classesor lexical categories, communicate informationabout a word, its morphological structure and in-ﬂectional paradigm, and its potential grammaticalrole in a clause.
pos tagging is a well-studiedproblem in nlp.
it is one of the ﬁrst tasks under-taken for a new data set and a pos tagger is of-ten one of the ﬁrst nlp resources built for low-resource languages (yarowsky and ngai, 2001;cox, 2010; de pauw, 2012; baldridge and gar-rette, 2013; duong, 2017; anastasopoulos, 2019;millour and fort, 2019; eskander et al., 2020b).
although this priority on early pos tagging maybe simply due to the relative ease of building a postagger, it seems to reﬂect an assumption that pos.
figure 1: average f1-scores on joint segmentation andglossing on interlinear glossed texts from ﬁeldwork inﬁve languages found that pos-tags have little and irreg-ular impact..tags simplify or improve other nlp tasks (krauwer,2003).
as far as we are aware, this assumption hasnot been methodically tested..this paper examines the impact of pos tagson morphological learning, an important area forlow-resource languages, many of which are moremorphologically complex than english, mandarin,or other large-resource languages.
morphologi-cal learning can help reduce the out-of-vocabularyproblem in morphologically complex languages,especially in low-resource settings.
morphologicallearning also holds high priority in documentaryand descriptive linguistics as a necessary founda-tion for further descriptive work.
we focus on tworelated tasks that involve morphological learning:joint morpheme segmentation/glossing and mor-phological reinﬂection.
joint segmentation andglossing segments a word into its component mor-phemes and glosses the segments.
reinﬂection gen-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages966–978august1–6,2021.©2021associationforcomputationallinguistics966the impact of pos tags on computational mor-phology may hold implications for linguistic theoryas well.
the nature of lexical categories (rauh,2010), the criteria for identifying them (croft,2000), and even their very reality as a universalproperty of language (gil, 2005) are not entirelysettled among linguists.
if the morphological struc-ture of unseen words can be analyzed and gener-ated without reference to lexical categories, thenperhaps such categories should not be consideredan inherent property of the lexicon (rauh et al.,2016)..this paper describes experiments that were runon corpora differing only in the presence or absenceof pos tags.
the results, which are generalizedin figures 1 and 2, indicate that pos tags do nothave signiﬁcant impact on computational morpho-logical learning.
section 2 presents related workin lexical categories, pos-tagging, segmentationand glossing, and (re)inﬂection.
sections 3 and 4describe the corpora and the nlp architecture used.
the segmentation and glossing task and results arepresented in section 5. the reinﬂection task andresults are presented in section 6. implications ofboth experiments are discussed in section 7..2 related work.
work on pos tagging has led to the developmentof several related resources in nlp and linguis-tics including numerous methods for automatic tag-ging (e.g.
kupiec (1992); toutanova and johnson(2008)) as well as tag sets.
the most popular tagset for english was developed by the penn tree-bank project (taylor et al., 2003).
a universalpos tag set was proposed by petrov et al.
(2012)and has been widely adopted.
it closely followstraditional linguistic conventions for common lexi-cal categories as can be seen by comparing to theleipzig glossing rules (institute, 2008) which alsohas recommended tags for less common categories.
many nlp models have been applied to seg-mentation and glossing of low-resource languagesbut they often tackle just one of the two tasks,e.g.
segmentation only (ruokolainen et al., 2014;wang et al., 2016; kann et al., 2018; mager et al.,2020; sorokin, 2019; eskander et al., 2020a).
au-tomatic morpheme segmentation was introducedby harris (1970) and much earlier segmentationresearch implemented unsupervised learning (gold-smith, 2001; creutz and lagus, 2002; poon et al.,2009).
published linguistic descriptive data is used.
figure 2: during reinﬂection generation of four in-terlinear ﬁeld corpora and four “cleaned” versions ofthose corpora the presence or absence of pos tags doesnot make a signiﬁcant or consistent difference in accu-racy of inﬂected forms..erates unseen inﬂected word forms from morpho-logical features based on a language’s inﬂectionalpatterns.
since lexical categories (pos) are iden-tiﬁed partly by morphological structure, it seemsreasonable to assume the reverse – that knowing aword’s part of speech makes it easier for a model toanalyze its morphological structure.
for example,knowing that a word is a noun in english makesit extremely unlikely that a ﬁnal substring (e)ncould be a participial afﬁx (e.g.
“oven” - noun;cf.
“driven” - verb).
on the other hand, pos tagsmay be providing redundant information when, forexample, an afﬁx that marks a morphosyntacticfeature is identical across all categories where thatfeature appears (e.g.
the russian morpheme /-i/‘pl’ is identical for for plural nouns and plural verbagreement).
however, these hypotheses must betested before claiming either one..the impact of (not) having pos tags has perhapsnot been examined closely in part because it seemssafe to assume that pos tags or a pos tagger willbe available.
however, as nlp expands its reach tonew languages, pos tags may not be readily avail-able.
in fact, the lexical categories present in thelanguage may not even be described yet when databecomes available.
in documentary and descriptivelinguistics, the description and tagging of lexicalcategories takes a relatively low priority comparedto its place in nlp (cf.
bird and chiang (2012)’sworkﬂow).
yet interlinear glossed texts (igt) areoften the largest available annotated resource for alow-resource language—and sometimes the onlyavailable resource..967as training data usually after some preprocessing.
glossing-only experiments make the assumptionthat data is already segmented into morphemes.
forexample, mcmillan-major (2020) trained a condi-tional random ﬁeld (crf) model to produce a glossline for several high-resource languages and threelow-resource languages.
the low-resource lan-guage data came from interlinearized data that waspolished for publication.
mcmillan-major (2020)and some other experiments such as samardzicet al.
(2015) use information from lines of interlin-earized texts such as translation and pos tags..computational approaches to morphological in-ﬂection or reinﬂection have been developed by dur-rett and denero (2013); nicolai et al.
(2015); liuand mao (2016); cotterell et al.
(2017); kann andsch¨utze (2016); aharoni and goldberg (2017), etc.
some of the work was developed as part of thesigmorphon shared tasks.1 our work partlyreplicates the conll-sigmorphon reinﬂectionshared tasks (cotterell et al., 2016, 2017, 2018a).
sequence-to-sequence neural network models havebeen very successful at handling the morphological(re)inﬂection task, even in low-resource conditionswith model improvement designed to tackle the sit-uation (kann et al., 2017; silfverberg et al., 2017;sharma et al., 2018; makarov and clematide, 2018;anastasopoulos and neubig, 2019; wu and cot-terell, 2019; liu, 2021).
the transformer (vaswaniet al., 2017a) is the model architecture which pro-duces the current state-of-the-art performance onthis task (vylomova et al., 2020; wu et al., 2020;liu and hulden, 2020b,a).
therefore, we use thetransformer for all the experiments in this paper.
this paper is an expansion of a section inmoeller et al.
(2020).
the experimental setup andsigmorphon languages are the same as thatwork, but it does not look at what happens whenpos tags are available in the ﬁeld data.
we ex-panded the re-inﬂection task to ﬁeld corpora.
wealso ran the sigmorphon experiments 5 timesinstead of one time.
the addition of the segmen-tation and glossing was inspired by moeller andhulden (2021)..3 data.
we use published data in ten languages and unpub-lished data in ﬁve low-resource languages.
thepublished and unpublished data is used for the mor-.
languageadyghearabicbasquefinnishgermanpersianrussianspanishswahiliturkish.
posn, adjn, v, adjvn, v, adjn, adjvn, v, adjn, vn, v, adjn, v, adj.
table 1: sigmorphon languages and the lexical cat-egories found in the data..phological reinﬂection but only the unpublisheddata for segmentation and glossing..3.1 sigmorphon data.
for the morphological reinﬂection task we usedatasets that were released forthe conll-sigmorphon 2018 shared task 1 (cotterell et al.,2018b).
we selected 10 languages that belong todifferent families and are typologically diverse withregards to morphology.
the languages and the in-ﬂected lexical categories available for the sharedtask are listed in table 1. the language family andmorphological typology for each language is avail-able on the unimorph ofﬁcial website.2 only thelisted lexical categories were pos-tagged..3.2.interlinear glossed texts.
the manually-annotated interlinear glossed texts(igt) were created in documentary and descrip-tive projects for ﬁve low-resource and under-documented languages.
the corpora represent arange of documentary ﬁeld projects rather than arange of language typology, although they do rep-resent three different language families on fourcontinents.
it is difﬁcult to ﬁnd corpora of under-documented languages with (enough) pos tags toconduct our pos experiments precisely becauseof the low priority of pos-tagging in documen-tary and descriptive linguistics.
we were unableto use half of the ﬁeld corpora available to us forthis reason.
however, because we are interested inleveraging nlp for ﬁeldwork, we felt it is impor-tant to work with the noisy ﬁeld data, rather thanuse (often morphologically simpler) high-resource.
1https://sigmorphon.github.io/.
sharedtasks/.
2https://unimorph.github.io.
968language tokens4.5kalas101klamkang14klezgi12kmanipurinat¨ugu16.5k.
pos-tagged86%384546%46,55796%13,63617%206766%10,994.inﬂected623n/a8433,2601,954.table 2: the approximate total number token counts in the ﬁeld data does not include multiple-word-expressions(when parsed as such) and ignores personal nouns and digits.
the amount of segmented, glossed, and pos-taggedtokens is shown as a number and a percentage of the corpus.
of those the inﬂected words were usable for thereinﬂection..languages with reduced data size.3.
the corpora were compiled during projects thateach had their own priorities and workﬂow andthis resulted in the differing amounts of annotationshown in table 2.4 only the tokens that were seg-mented, glossed, and pos-tagged could be used.
the pos tags were provided by the annotators.
forthe reinﬂection task, the data was further limited toinﬂected forms.
the collection of inﬂected formswas automatically extracted and grouped based onthe gloss of the root morpheme (noisy version).
we happened to have cleaned versions for the re-inﬂection task and include those for the sake ofcompleteness.
the cleaned versions were createdfrom the noisy versions that had been checked bylanguage experts.5.
it is worth noting that the lamkang (used onlyfor the segmentation and glossing study), manipuri,and nat¨ugu corpora are the result of many yearsof work and these extended projects eventually ledto signiﬁcant pos tagging.
two other large andcompletely segmented/glossed corpora could notbe included because the lexical categories had notbeen tagged.
the lezgi project used pos tags atan early stage because the research was focusedon verb tenses (donet, 2014).
all pos tags in thesmaller alas corpus, and many in the lezgi corpus,were added speciﬁcally for our research..3we investigated the online database of interlinear text(odin) since the aggregation project at university ofwashington has projected pos tags from english, but as yet,we have not found a corpus of comparative size to the smallestﬁeld corpus.
perhaps because we focused on ﬁnding morepolysynthetic languages in order to balance the diversity ofmorphological types and because preprocessing the odinformat is time-consuming..4rights holders gave informed consent to use the data forthis research and links are provide to the corpora that arepublicly available..5inﬂection data available at: https://github.com/.
linguistliu/igt.
alas[btz] (alas-kluet, batak alas, batak alas-kluet) is an austronesian language spoken by200,000 people on the indonesian island of suma-tra (eberhard et al., 2020).
its morphology fea-tures reduplication, inﬁxation, and circumﬁxation.
the pos set in the corpus is: adj, adv, aux,cardnum, clf, conj, cop, dem, distrnum,existmrkr, interj, n, nprop, ordnum, prep,pro, prt, quant, refl, relpro, v, vd, vi,vt.6.
lamkang [lmk] is a northern kuki-chin lan-guage of the tibeto-burman family with an es-timated 4 to 10 thousand speakers primarily inmanipur, india but also in burma (thounaojamand chelliah, 2007).
its morphology tends towardagglutination with many stem-stem patterns to sig-nal syntactic categories.
the corpus is accessiblethrough the computational resources for southasian languages (corsal) digital archive at theuniversity of north texas.7 the pos tag set is:adn, advl, dem, conn, coordconn, cop, in-terj, n, npr, num, ordnum, postp, pron, ptc,quant, subo, unk, v, vc, vi, vt..lezgi[lez] (lezgian) is a highly agglutinativelanguage belonging to the lezgic branch of thenakh-daghestanian (northeast caucasian) family.
it is spoken by over 400,000 speakers in russiaand azerbaijan (eberhard et al., 2020).
it featuresoverwhelmingly sufﬁxing agglutinative morphol-ogy.
the pos tag set is: adj, adv, cardnum,conn, coordconn, dem, det, indfpro, in-terj, interrog, msd, multipnum, n, nprop,num, ordnum, pers, poss, post, prep, pro,proform, prt, ptcp, recp, subordconn, v,.
6all pos were used for the segmentation and glossingtask.
tags in boldface indicate pos that are inﬂected and weretherefore used in the reinﬂection task..7https://digital.library.unt.edu/.
explore/collections/saalt/.
969verbprt, vf, vnf, voc..manipuri[mni] (meitei, meetei) is a tibeto-burman language spoken by nearly two millionpeople, primarily in the state of manipur, and isone of india’s ofﬁcial languages.
it nonetheless hasbeen classiﬁed as vulnerable to extinction (mose-ley, 2010).
it is a tonal language with weakly suf-ﬁxing, agglutinative morphology (chelliah, 1997).
the corpus is at corsal.8 the pos set is: adv,interj, n, proform, unk, v..nat ¨ugu [ntu] belongs to the reefs-santa cruzgroup in the austronesian family and is spokenby about 4,000 people in the temotu provinceof the solomon islands.
it has mainly aggluti-native morphology with complex verb structures(næss and boerger, 2008).
the corpus is storedat sil language & culture archives.9 the postags set is: a-d-p2, adj, adv, clause, conj,dem, det, gen, gerund, interrog, intj, n,n.(kx.cl), ncomp, neg, nom1, np, np(comp),nprop, num, ord, particle, pclf, per-spro, phrase, pn, posspro, prep, pro,rprn, subr, unk, v, vi, vp, vt, z-gerund..4 models.
for simple comparisons, we chose a single neu-ral model architecture for both tasks.
the taskswere trained with the transformer (vaswani et al.,2017b), the current state-of-the-art neural modelarchitecture for morphological tasks (vylomovaet al., 2020; liu and hulden, 2020b).
we used theimplementation of the transformer model in thefairseq toolkit (ott et al., 2019)10 with character-level transduction (wu et al., 2020) for morphologylearning in low-resource settings.
following (wuet al., 2020), we employ n = 4 layers for the en-coder and the decoder, each with 4 self-attentionheads.
the embedding size for the encoder anddecoder is 256, and the hidden layer size is 1024.we use a dropout rate of 0.3 for encoding and beamsearch with a width of 5 at decoding time.
theadam algorithm (kingma and ba, 2014) (β1 = 0.9,β2 = 0.98) is used to optimize the cross entropyloss with label smoothing (szegedy et al., 2016) of0.1. all models have been trained on an nvidia.
gp102 [titan xp] gpu for 10k maximum up-dates with a batch size of 400..5 pos for segmentation and glossing.
the ﬁrst study asks whether pos tags makes asigniﬁcant impact on automated morpheme seg-menting and glossing.
the experiment tests andcompares two models on data that is identical ex-cept for the presence/lack of pos tags..we chose morpheme segmentation and glossingbecause it is a high-priority and early step in docu-menting and describing new languages.
segment-ing words into morphemes and glossing (strictlytranslating) them is usually the ﬁrst task undertakenafter new data has been transcribed.
therefore, itis important to study how to provide and improveautomated assistance for ﬁeld linguists.
automaticsystems could greatly beneﬁt the analysis of en-dangered languages and combat the “annotationbottleneck” caused by current manual methods (si-mons and lewis, 2013; holton et al., 2017; seifartet al., 2018)..although adding pos tagging as a high-prioritytask would add to that bottleneck, if the tags have asigniﬁcant and positive impact on automating seg-mentation and glossing, then linguists may receivelong-term beneﬁts from the addition to their work-ﬂow.
therefore, we explore the impact of postags at very low-resource settings and the impactof pos tags when a new ﬁeld project takes timeto tag some, but not all, tokens.
this is also whywe chose noisy ﬁeld corpora, rather than published,polished corpora which are not like the data thatlinguists typically work with.
we are interested inhow pos tags inﬂuence segmentation and glossingin the earliest work with a new language..5.1 experimental setup.
three transformer models were trained.
the en-glish example in (1) shows the input and output ofmodels 1, 2, and 3. model 1, shown in (1a), hasno pos tags.
models 2 and 3 have a pos tags,as shown in (1b).
model 2 has pos tags on everyword but model 3 includes pos tags only for somewords, simulating projects unable to complete pos-tagging..8https://digital.library.unt.edu/.
explore/collections/mdr.
9https://www.sil.org/resources/search/.
a..b..language/ntu.
latest/.
10https://fairseq.readthedocs.io/en/.
(1).
input 1:.
t.a.x.e.s.input 2/3 :.
t.a.x.e.s n.c. output:.
tax#levy.
-es#pl.
970languagealaslamkanglezgimanipurinat¨ugu.
1% 3% 6.5% 10% 20% 30% 40% 100%-.09.00-.01.05.02.03.00-.01.00.01.
.02.08-.01.00.03.
.05.08.03.00.02.
.03.07.04.00.03.
.02.07.02.01.02.
.04.08.03.00.04.
.05.08.03.01.03.table 3: the difference in f1 scores with/out pos tags when training segmentation and glossing on increasingamounts of annotated data, as percentages of total available training data.
negative scores indicates that addingpos tags improve results..languagealaslamkanglezgimanipurinat¨ugu.
0% 1% 3% 6.5% 10% 20% 30% 40% 100%.6968.6517.6902.8645.8332.8573.7317.7505.7501.8921.8889.8903.9006.8855.8995.
.6647.8527.7480.8896.8999.
.6627.8482.7498.8874.8932.
.6708.8524.7471.8897.8965.
.6415.8195.7542.8877.8782.
.6546.8298.7529.8882.8864.
.6448.8074.7564.8885.8748.table 4: f1 scores on segmenting and glossing when trained on all data with increasing percentages of pos tags..all three models are trained on all the availabletraining data.
models 1 and 2 are also trained ondifferent proportions of training data in order tosimulate very small corpora.
these proportions oftraining data start at 1% and are gradually increasedto 40% of available training data..even when pos tags are included in interlinearﬁeld data, it is rarely completed as table 2 clearlyindicates.in order to simulate this reality model 3was trained on all the available training data but theproportion of inputs with pos tags was graduallyand randomly increased..the training/development/test split is 8/1/1.
allmodels are trained and evaluated on a 10-fold cross-validation.
the folds were trained twice, once withand once without pos tags; no other changes weremade to the data.
all folds were evaluated on a sin-gle, consistent held-out test set.
since we wantedto simulate a realistic ﬁeld situation where the sys-tem is segmenting and glossing newly transcribedbut unannotated text, the test inputs do not includepos tags..5.2 segmentation and glossing results.
pos tags have no consistent positive or negativeeffect on automated segmentation and glossing inlow-resource settings.
the overall impact of postags is not signiﬁcant.
table 3 shows the differ-ences when f1-scores without pos tags are sub-tracted from the f1-scores with pos tags, withvarious amounts of training data.
the largest dif-ference is just under .1 points..a few interesting observations can be made thatshould be explored with more languages.
manipurishows the smallest differences overall; it also hasthe fewest pos-tagged words and the smallest tagset.
the largest differences are seen in the alas andlamkang corpora.
alas also has a relatively smallamount of pos-tagged words, but it has quite alarge tag set.
as the size of the alas training dataincreases, the impact of pos tags becomes morepronounced, suggesting that perhaps a relativelylarge pos tag set may have a greater effect on re-sults in medium settings.
lamkang has the largestamount of pos-tagged words, but of those, a sig-niﬁcant number were tagged as unk.
it is not clearwhether the unk tag is limited to categories thathave not been fully analyzed or if it is a default tagthat covers a diverse set of words.
the differencemade by adding pos tags all but disappears whenall the lamkang data is trained, suggesting that asmaller data set is more impacted by a large tag setor inconsistent annotations..overall, increasing the number of pos tags inthe training data has minimal impact.
table 4shows the f1-scores when the amount of pos tagsin the data is gradually increased.
for example, at30%, one of three random training instances havea pos tag.
in most cases, having incomplete pos-tagged data hurts performance compared to havepos tags on all words or none at all.
the system ei-ther performs worse, or, in the case of lezgi, makesvery small improvement (.0063 points).
except forlezgi, as more pos tags are added, the system.
971tends to improve slightly but never matches thebest scores..6 pos for reinﬂection.
the second study asks whether pos tags make asigniﬁcant impact on learning inﬂectional patternsand generating unseen inﬂected forms.
we chosethe morphological re-inﬂection task because it iseasy to reproduce and to compare with the orig-inal sigmorphon shared task.
eliciting andanalyzing a language’s inﬂectional patterns is a rec-ommended next step after morpheme segmentationand glossing (bird and chiang, 2012).
the inﬂec-tional pattern of a lexeme or a lexical category isalso known as a morphological paradigm.
learningmorphological paradigms can be viewed in termsof ﬁlling in, or generating, the missing forms ofa paradigm table by generalizing over inﬂectionalpatterns (ackerman et al., 2009; ahlberg et al.,2014, 2015; liu and hulden, 2017; malouf, 2017;silfverberg et al., 2018; silfverberg and hulden,2018)..the experiments in this section partly replicatesthe conll-sigmorphon 2018 shared task 1 ofmorphological reinﬂection.
reinﬂection consistsof generating unknown inﬂected forms, given arelated inﬂected form f ((cid:96), (cid:126)tγ1) and a target mor-phological feature vector (cid:126)tγ2.
thus, it correspondsto learning the mapping f : σ∗ × t → σ∗.
thegoal is then to produce the inﬂected form f ((cid:96), (cid:126)tγ2).
an inﬂected form is generated when the model isgiven a related inﬂected form and the target mor-phological features (which are essentially glossesof afﬁxes) of the inﬂected form to be generated.
inprevious work, pos tags have been included bydefault as part of the morphological features.
thatis, they have been assumed to be helpful and to beavailable..6.1 experimental setup.
the models were trained on individual languagesin three different data sets.
the ﬁrst data set isthe published unimorph inﬂectional data in tenlanguages.
the second data set is inﬂected wordforms extracted from unpublished igt in four lan-guages; the third is the clean, or corrected, versionsof the second data set.
the unimorph data wasextracted from published data and is the “clean-est”.
its inﬂected forms and morphological fea-tures were double-checked and the forms providedwere selected to provide a balanced picture of.
the language’s morphological structure.
the in-ﬂected forms extracted from the igt contains onlyinﬂected forms attested in original texts whichare transcribed samples of natural oral speech.
the noisy version was automatically grouped intoparadigms based on the assumption that identi-cal glosses of root morphemes signiﬁed the samelemma, and therefore the same morphologicalparadigm.
the clean data was made by askinglanguage experts to examine the noisy data and re-group paradigms when root morphemes were incor-rectly glossed.
they also corrected typos and mor-phological features that were incorrectly glossed..for the unimorph data, the original sigmor-phon training/validation/test splits were kept.
theprepared medium setting of 1,000 training exam-ples was used.
this setting was chosen becauseof the three possible settings (100, 1k and 10k), itis the closest in size to number of inﬂected wordforms extracted from the four igt corpora, whichprovided between 600 and 3,000 training examples.
an 8/1/1 training/development/tests split was usedfor the igt data..6.2 reinﬂection results.
five reinﬂection models with random seeds weretrained on each data set.
all models were trainedtwice, once with and once without pos tags onthe input.
crosswise pairs were compared by sub-tracting the results with pos tags from the resultswithout pos, giving 25 accuracy scores per lan-guage.
figures 3 and 4 show the average and rangeof differences between the two..the range of differences shows that pos tags donot have a consistently positive or negative impact.
only two languages show a clear tendency to beimpact in one way.
in nat¨ugu, pos tags improveaccuracy while in adyghe, they decreases accuracy..the average difference in accuracy on any dataset is rarely more than 1 percentage point.
as thedata becomes less polished, the impact of postags increases slightly and the range of differencesgrows noticeably.
the largest average difference(∼5 percentage points) seen in the noisy data fromﬁeld igt.
this indicates that time invested in pol-ishing existing igt data may give a better returnthan time spent on pos-tagging.
for the sigmor-phon languages, the largest mean difference isbarely over 2 points and for the clean igt-extracteddata the largest mean difference is about 3 points..972figure 3: the difference in accuracy with/out pos on the reinﬂection task with sigmorphon languages.
neg-ative scores indicates that adding pos tags improves results.
the bar shows the mean of the differences and linesindicate the range of the mean plus or minus the standard deviation..figure 4: the difference in accuracy with/out pos on the reinﬂection task with cleaned and noisy ﬁeld data.
negative scores indicates that adding pos tags improve results.
the bar shows the mean of the differences andline indicates the range of the mean plus or minus the standard deviation..7 discussion.
the number of language we used is not large buta few general observations can be made.
for bothtasks the impact made by the presence or absenceof pos tags is minimal.
still, the best results with asmall corpus are achieved when either all or no to-kens are pos-tagged, at least for segmentation andglossing.
this suggests that having a completelytagged corpus is better than an incompletely taggedcorpus, so perhaps limited annotation time mightbe better spent on more segmentation and glossing.
the size or speciﬁcity of the tag set may make adifference in the impact of pos tags.
when com-paring the tag sets in the conll-sigmorphon2018 shared task data and the igt from ﬁeldwork,.
the difference in the number of lexical categoriesis signiﬁcant.
the conll-sigmorphon 2018shared task data sets have at most three: noun (n),verb (v), and adjective (adj).
the igt corporahave larger tag sets; for example, they may havetags for both ﬁnite verb form (vf) and non-ﬁniteforms (vnf).
the smallest igt tag set has six cate-gories (manipuri).
that is twice as many pos tagsas the sigmorphon languages, but still muchsmaller than the other corpora, which have over 20unique tags..however, the difference in results cannot be def-initely attributed to tag set size.
the igt tag setsare larger because the goal of descriptive work isto discover ﬁne-grained categories, whereas theunimorph data use more general categories which.
973adyghearabicbasquefinnishgermanpersianrussianspanishswahiliturkish4.0%3.0%2.0%1.0%0.0%1.0%2.0%3.0%4.0%accuracy (%): pos - noposbtzlezmnintubtz-noisylez-noisymni-noisyntu-noisy8.0%6.0%4.0%2.0%0.0%2.0%4.0%6.0%8.0%accuracy (%): pos - noposare common for language learning material or gen-eral dictionaries.
similar ﬁne-grained distinctionsappear in the penn treebank tag set and are pre-sumably useful for nlp tasks.
future work couldre-tag igt with more general categories to test howthe size and speciﬁcity of pos tags on small cor-pora impact these tasks.
this could be fruitful areaof research because it might help us predict theusefulness of another linguistic category: the cat-egory of morphemes.
morpheme-level categoriesare similar to pos tags but tagged for individualmorphemes.
interestingly, morpheme categoriesgenerally take higher priority than word-level tagsin documentary and descriptive linguists and aretherefore more often available in ﬁeld data..consistency of annotation may be signiﬁcant.
itis likely that the pos tags in the unimorph datawere added carefully and correctly, but the ﬁelddata were likely tagged as the lexical categorieswere being discovered and described.
the differ-ences in results between the two data sets may bedue to these factors, but the differences are nothuge.
so it seems possible that the effect of postags may be similar no matter how the pos tagsare added.
a different approach to pos-tagging,such as training with context might affect results.
this possibility points to many future useful exper-iments.
we believe there may be many unresolvedissues related to the way the pos tags were addedor which pos tags were used.
one auxiliary taskwould be to project pos tags from the target lan-guage of the translated sentences that are usuallyavailable in igt even before morpheme segmen-tation and glosses.
also, metrics for annotationquality could be devised so that its impact is betterunderstood.
linguists need to know as they startannotation how best to perform their earliest analy-sis and annotation so that they gain optimal beneﬁtfrom automated help later..finally, although a consistent impact by postags cannot be seen on morphological learningacross all corpora, some corpora did show a moreor less consistent impact from the presence orabsence of pos tags.
sometimes better resultswere achieved by removing pos tags, sometimesby adding them.
reinﬂection in adyghe and the“clean” version of lezgi data tend to improve whenpos tags are removed while persian, russian, andthe noisy version of nat¨ugu generally have more ac-curate results when pos tags are available.
in seg-mentation and glossing, alas and lamkang show in.
some settings nearly .1 points difference when postags are added and removed, respectively.
withthese trends, a more interesting question for thesecorpora becomes “when are pos tags helpful?”and this should be explored further..8 conclusion.
we conclude that the presence or absence of postags does not have a signiﬁcant impact on two mor-phological learning tasks: segmentation and gloss-ing, or reinﬂection.
no clear advantage is gainedor lost from pos-tagging on low-resource data.
insegmentation and glossing, the greatest averagedifference is a loss of .09 f1-score when a largepos tag set is added to a small ﬁeld corpus.
inreinﬂection, the overall tendency, though slight, isthat accuracy decreases when pos tags are added.
the greatest average difference is 1.2 percentagepoints of accuracy for published data, 2.2 pointsfor unpublished “clean” data, and 5 points for un-published noisy data..we hypothesize that pos tags do not have asigniﬁcant impact on these tasks because the infor-mation provided by pos tags is implicitly learned.
these are, of course, not the only two tasks wherepos tags could be leveraged for low-resource lan-guages so we cannot make a deﬁnitive statement re-garding the impact of pos tags in other nlp taskswith low-resource languages, particularly ones thatmore syntactic or semantic in nature.
further me-thodical research needs to be done in order to pro-duce a deﬁnitive analysis.
however, it does bringinto question whether the development of pos tag-gers and pos tagging should be prioritized less..future work should explore how other tasks areimpacted by pos tags.
the results might inﬂuenceworkﬂow priorities for documentary and descrip-tive linguists who want to receive beneﬁt from, orgive it to, nlp.
when a sophisticated pos tag setand pos taggers are available for a language, lever-aging pos tags is trivial.
however, as nlp expandsinto a broader range of languages, the usefulnessof pos tags may become an important question be-cause documentary and descriptive linguistics doesnot currently place a high priority on lexical cate-gories.
discovering a language’s lexical categoriesrequires a detailed understanding of the language’ssyntax—something linguists do not always possessin the early stages of describing a new language..974references.
farrell ackerman, james p blevins, and robert mal-ouf.
2009. parts and wholes: implicative patterns ininﬂectional paradigms.
analogy in grammar: formand acquisition, 54:81..roee aharoni and yoav goldberg.
2017. morpholog-ical inﬂection generation with hard monotonic at-tention.
proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), 1:2004–2015..malin ahlberg, markus forsberg, and mans hulden.
2014. semi-supervised learning of morphologicalparadigms and lexicons.
in proceedings of the 14thconference of the european chapter of the asso-ciation for computational linguistics, pages 569–578, gothenburg, sweden.
association for compu-tational linguistics..malin ahlberg, markus forsberg, and mans hulden.
2015. paradigm classiﬁcation in supervised learn-ing of morphology.
in proceedings of the 2015 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 1024–1029, denver, col-orado.
association for computational linguistics..antonios anastasopoulos.
2019. computational toolsfor endangered language documentation.
ph.d.thesis, university of notre dame..antonios anastasopoulos and graham neubig.
2019.pushing the limits of low-resource morphological in-ﬂection.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages984–996, hong kong, china.
association for com-putational linguistics..jason baldridge and dan garrette.
2013. learning apart-of-speech tagger from two hours of annota-tion.
in proceedings of the north american chap-ter of the association for computational linguistics:human language technologies (naacl-hlt-13)..steven bird and david chiang.
2012. machine trans-lation for language preservation.
in proceedings ofcoling 2012, pages 125–134, mumbai..shobhana lakshmi chelliah.
1997. a grammar of mei-thei, volume 17 of mouton grammar library.
degruyter mouton, berlin, boston.
publication title:a grammar of meithei..ryan cotterell, christo kirov, john sylak-glassman,g´eraldine walther, ekaterina vylomova, arya d.mccarthy, katharina kann, sabrina j. mielke, gar-rett nicolai, miikka silfverberg, david yarowsky,jason eisner, and mans hulden.
2018a.
theconll–sigmorphon 2018 shared task: uni-in proceedingsversal morphological reinﬂection.
of the conll–sigmorphon 2018 shared task:universal morphological reinﬂection, pages 1–27,.
brussels.
association for computational linguis-tics..ryan cotterell, christo kirov, john sylak-glassman,g´eraldine walther, ekaterina vylomova, arya d.mccarthy, katharina kann, sebastian mielke, gar-rett nicolai, miikka silfverberg, david yarowsky,jason eisner, and mans hulden.
2018b.
theconll–sigmorphon 2018 shared task: uni-versal morphological reinﬂection.
in proceedingsof the conll sigmorphon 2018 shared task:universal morphological reinﬂection, pages 1–27,brussels.
association for computational linguis-tics..ryan cotterell, christo kirov, john sylak-glassman,g´eraldine walther, ekaterina vylomova, patrickxia, manaal faruqui, sandra k¨ubler, davidyarowsky, jason eisner, and mans hulden.
2017.conll-sigmorphon 2017 shared task: uni-versal morphological reinﬂection in 52 languages.
in proceedings of the conll sigmorphon 2017shared task: universal morphological reinﬂection,vancouver.
association for computational linguis-tics..ryan cotterell, christo kirov, john sylak-glassman,david yarowsky, jason eisner, and mans hulden.
shared2016.in proceed-task—morphologicalings ofthe 14th sigmorphon workshop oncomputational research in phonetics, phonology,and morphology, pages 10–22..the sigmorphon 2016reinﬂection..christopher cox.
2010. probabilistic tagging of mi-nority language data: a case study using qtag.
ins.th.
gries, s. wulff, and m. davies, editors, cor-pus linguistic applications: current studies, new di-rections., pages 213–231.
rodopi, amsterdam:..mathias creutz and krista lagus.
2002. unsupervisedin proceedings of thediscovery of morphemes.
acl-02 workshop on morphological and phonolog-ical learning-volume 6, pages 21–30, philadelphia,pa. association for computational linguistics..william croft.
2000. parts of speech as language uni-inversals and as language-particular categories.
petra m. vogel and bernard comrie, editors, ap-proaches to the typology of word classes, vol-ume 23 of empirical approaches to language ty-pology [ealt], pages 65–102.
de gruyter mouton..guy de pauw.
2012. resource-light bantu part-in proceedings of the work-of-speech tagging.
shop on language technology for normalisation ofless-resourced languages (saltmil8/aflat2012),istanbul, turkey.
european language resources as-sociation (elra)..charles donet.
2014. the importance of verb saliencein the followability of lezgi oral narratives.
mas-ter’s thesis, graduate institute of applied linguis-tics, dallas, tx..975long duong.
2017. natural language processing forresource-poor languages.
ph.d. thesis, universityof melbourne, melbourne, australia..greg durrett and john denero.
2013..supervisedlearning of complete morphological paradigms.
inproceedings of naacl-hlt, atlanta, georgia.
as-sociation for computational linguistics..david m. eberhard, gary f. simons, and charles d.fennig, editors.
2020. ethnologue: languages ofthe world, twenty-third edition.
sil international,dallas, texas..ramy eskander, francesca callejas, elizabeth nichols,judith klavans, and smaranda muresan.
2020a.
morphagram, evaluation and framework for unsu-pervised morphological segmentation.
in proceed-ings of the 12th language resources and evaluationconference, pages 7112–7122, marseille, france.
european language resources association..ramy eskander, smaranda muresan, and michaelcollins.
2020b.
unsupervised cross-lingual part-of-speech tagging for truly low-resource scenar-ios.
in proceedings of the 2020 conference on em-pirical methods in natural language processing,pages 4820–4831.
association for computationallinguistic..david gil.
2005. word order without syntactic cate-gories: how riau indonesian does it.
in andrewcarnie, sheila ann dooley, and heidi harley, edi-tors, verb first: on the syntax of verb-initial lan-guages, volume 73 of linguistik aktuell/linguisticstoday, pages 243–264.
john benjamins publishing.
google-books-id: stieprsmdbic..john goldsmith.
2001. unsupervised learning of themorphology of a natural language.
computationallinguistics, 27(2):153–198..zellig s. harris.
1970. from phoneme to morpheme.
in papers in structural and transformational lin-guistics, pages 32–67.
springer netherlands, dor-drecht..gary holton, kavon hooshiar,.
and nicholasthieberger.
2017.developing collection man-agement tools to create more robust and reliablelinguistic data.
in 2nd workshop on computationalmethods for endangered languages..max planck institute.
2008. the leipzig glossingrules: conventions for interlinear morpheme-by-morpheme glosses..katharina kann,.
jesus manuel mager hois,ivan vladimir meza-ruiz, and hinrich sch¨utze.
2018. fortiﬁcation of neural morphological segmen-tation models for polysynthetic minimal-resourcelanguages.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages47–57, new orleans, louisiana.
association forcomputational linguistics..katharina kann and hinrich sch¨utze.
2016. single-model encoder-decoder with explicit morpholog-in proceed-ical representation for reinﬂection.
ings of the 54th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 555–560, berlin, germany.
associationfor computational linguistics..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..steven krauwer.
2003. the basic language resource kit(blark) as the ﬁrst milestone for the language re-sources roadmap.
in proceedings of specom 2003,pages 8–15..julian kupiec.
1992. robust part-of-speech tagging us-ing a hidden markov model.
computer speech &language, 6(3):225–242..ling liu.
2021. computational morphology witharxiv preprint.
neural network approaches.
arxiv:2105.09404..ling liu and mans hulden.
2017. evaluation of ﬁnitestate morphological analyzers based on paradigm ex-in proceedings of thetraction from wiktionary.
13th international conference on finite state meth-ods and natural language processing (fsmnlp2017), pages 69–74, ume˚a, sweden.
association forcomputational linguistics..ling liu and mans hulden.
2020a.
analogy mod-in proceedingsels for neural word inﬂection.
of the 28th international conference on compu-tational linguistics, pages 2861–2878, barcelona,spain (online).
international committee on compu-tational linguistics..ling liu and mans hulden.
2020b.
leveraging princi-pal parts for morphological inﬂection.
in proceed-ings of the 17th sigmorphon workshop on com-putational research in phonetics, phonology, andmorphology, pages 153–161, online.
associationfor computational linguistics..katharina kann, ryan cotterell, and hinrich sch¨utze.
2017. one-shot neural cross-lingual transferin proceedings of thefor paradigm completion.
55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1993–2003, vancouver, canada.
associationfor computational linguistics..ling liu and lingshuang jack mao.
2016. morpho-logical reinﬂection with conditional random ﬁeldsin proceedings of theand unsupervised features.
14th sigmorphon workshop on computationalresearch in phonetics, phonology, and morphol-ogy, pages 36–40, berlin, germany.
association forcomputational linguistics..976manuel mager, ¨ozlem c¸ etino˘glu, and katharina kann.
tackling the low-resource challenge for2020.canonical segmentation.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 5237–5250, on-line.
association for computational linguistics..peter makarov and simon clematide.
2018. uzhat conll–sigmorphon 2018 shared task onuniversal morphological reinﬂection.
in proceed-ings of the conll–sigmorphon 2018 sharedtask: universal morphological reinﬂection, pages69–75, brussels.
association for computational lin-guistics..robert malouf.
2017. abstractive morphological learn-ing with a recurrent neural network.
morphology,27(4):431–458..angelina mcmillan-major.
2020. automating glossgeneration in interlinear glossed text.
in proceed-ings of the society for computation in linguistics,volume 3..alice millour and kar¨en fort.
2019. unsuperviseddata augmentation for less-resourced languagesin proceedings ofwith no standardized spelling.
the international conference on recent advances innatural language processing (ranlp 2019), pages776–784, varna, bulgaria.
incoma ltd..sarah moeller and mans hulden.
2021. integrating au-tomated segmentation and glossing into documen-tary and descriptive linguistics.
in proceedings ofthe workshop on computational methods for endan-gered languages, volume 1, pages 86–95, honolulu,hi.
association for computational linguistics..sarah moeller, ling liu, changbing yang, katharinakann, and mans hulden.
2020. igt2p: from inter-linear glossed texts to paradigms.
in proceedingsof the 2020 conference on empirical methods innatural language processi, pages 5251–5262.
as-sociation for computational linguistics..christopher moseley, editor.
2010. atlas of the world’slanguages in danger, third edition.
unesco pub-lishing, paris..garrett nicolai, colin cherry, and grzegorz kon-inﬂection generation as discrimina-drak.
2015.in proceedings of thetive string transduction.
2015 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 922–931, den-ver, colorado.
association for computational lin-guistics..˚ashild næss.
and brenda h. boerger..2008.reefs–santa cruz as oceanic: evidence fromthe verb complex.
oceanic linguistics, 47(1):185–212. publisher: university of hawai’i press..toolkit for sequence modeling.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..slav petrov, dipanjan das, and ryan mcdonald.
2012.in proceed-a universal part-of-speech tagset.
ings of the eighth international conference on lan-guage resources and evaluation (lrec’12), pages2089–2096, istanbul, turkey.
european languageresources association (elra)..hoifung poon, colin cherry, and kristina toutanova.
2009. unsupervised morphological segmentationin proceedings of humanwith log-linear models.
language technologies: the 2009 annual confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 209–217.
association for computational linguistics..gisa rauh.
2010. syntactic categories: their identiﬁ-cation and description in linguistic theories.
ox-ford university press..gisa rauh, jens fleischhauer, anja latrouite, andrainer osswald.
2016. linguistic categories andthe syntax-semantics interface: evaluating com-in explorations of the syntax-peting approaches.
semantics interface, pages 15–55.
d¨usseldorf..teemu ruokolainen, oskar kohonen, sami virpioja,and mikko kurimo.
2014. painless semi-supervisedmorphological segmentation using conditional ran-dom ﬁelds.
in proceedings of the 14th conference ofthe european chapter of the association for compu-tational linguistics, volume 2: short papers, pages84–89, gothenburg, sweden.
association for com-putational linguistics..tanja samardzic, robert schikowski, and sabineautomatic interlinear glossing asstoll.
2015.in proceed-two-level sequence classiﬁcation.
the 9th sighum workshop on lan-ings ofguage technology for cultural heritage, social sci-ences, and humanities (latech), pages 68–72, bei-jing, china.
association for computational linguis-tics..frank seifart, nicholas evans, harald hammarstr¨om,languagelanguage,.
and stephen c. levinson.
2018.documentation twenty-ﬁve years on.
94(4):e324–e345..abhishek.
ganesh katrapati,.
andsharma,dipti misra sharma.
2018.iit(bhu)–iiithat conll–sigmorphon 2018 shared task onuniversal morphological reinﬂection.
in proceed-ings of the conll–sigmorphon 2018 sharedtask: universal morphological reinﬂection, pages105–111, brussels.
association for computationallinguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019..miikka silfverberg and mans hulden.
2018. anencoder-decoder approach to the paradigm cell ﬁll-ing problem.
in proceedings of the 2018 conference.
977ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017b.
attention isin proceedings of the 31st interna-all you need.
tional conference on neural information processingsystems, pages 6000–6010, long beach, california,usa.
curran associates inc..ekaterina vylomova,.
jennifer white,.
eliza-beth salesky, sabrina j. mielke, shijie wu,edoardo maria ponti, rowan hall maudslay, ranzmigrod, josef valvoda, svetlana toldova, francistyers, elena klyachko,ilya yegorov, nataliakrizhanovsky, paula czarnowska, irene nikkarinen,andrew krizhanovsky, tiago pimentel, lucastorroba hennigen, christo kirov, garrett nicolai,adina williams, antonios anastasopoulos, hilariacruz, eleanor chodroff, ryan cotterell, miikkasilfverberg, and mans hulden.
2020. sigmor-phon 2020 shared task 0: typologically diversein proceedings of themorphological inﬂection.
17th sigmorphon workshop on computationalresearch in phonetics, phonology, and morphology,pages 1–39, online.
association for computationallinguistics..linlin wang, zhu cao, yu xia, and gerard de melo.
2016. morphological segmentation with windowlstm neural networks.
proceedings of the aaaiconference on artiﬁcial intelligence, 30(1)..shijie wu and ryan cotterell.
2019..exact hardmonotonic attention for character-level transduc-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 1530–1537, florence, italy.
association forcomputational linguistics..shijie wu, ryan cotterell, and mans hulden.
2020.applying the transformer to character-level trans-duction.
arxiv:2005.10213 [cs]..david yarowsky and grace ngai.
2001. inducing mul-tilingual pos taggers and np bracketers via ro-bust projection across aligned corpora.
in secondmeeting of the north american chapter of the asso-ciation for computational linguistics..on empirical methods in natural language process-ing, pages 2883–2889, brussels, belgium.
associa-tion for computational linguistics..miikka silfverberg, ling liu, and mans hulden.
2018.a computational model for the linguistic notion ofmorphological paradigm.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 1615–1626, santa fe, new mexico, usa.
association for computational linguistics..miikka silfverberg, adam wiemerslage, ling liu, andlingshuang jack mao.
2017. data augmentation forin proceedings of themorphological reinﬂection.
conll sigmorphon 2017 shared task: univer-sal morphological reinﬂection, pages 90–99, van-couver.
association for computational linguistics..gary f. simons and m. paul lewis.
2013. the world’slanguages in crisis: a 20-year update.
in elena mi-has, bernard perley, gabriel rei-doval, and kath-leen wheatley, editors, responses to language en-dangerment: in honor of mickey noonan.
new direc-tions in language documentation and language revi-talization, number 142 in studies in language com-panion series, pages 3–20.
john benjamins, amster-dam..alexey sorokin.
2019. convolutional neural networksfor low-resource morpheme segmentation: baselinein proceedings of the 16thor state-of-the-art?
workshop on computational research in phonetics,phonology, and morphology, pages 154–159, flo-rence, italy.
association for computational linguis-tics..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..ann taylor, mitchell marcus, and beatrice santorini.
2003. the penn treebank: an overview.
inanne abeill´e, editor, treebanks: building and us-ing parsed corpora, text, speech and languagetechnology, pages 5–22.
springer netherlands, dor-drecht..harimohon thounaojam and shobhana l. chelliah.
2007. the lamkang language: grammatical sketch,texts and lexicon.
linguistics of the tibeto-burmanarea, 30(1):1–189..kristina toutanova and mark johnson.
2008..abayesian lda-based model for semi-supervisedin proceedings of nips.
part-of-speech tagging.
mit press..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017a.
attention is allyou need.
arxiv preprint arxiv:1706.03762..978