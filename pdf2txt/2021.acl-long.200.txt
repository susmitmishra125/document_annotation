beyond sentence-level end-to-end speech translation: context helps.
biao zhang1.
ivan titov1,2 barry haddow1 rico sennrich3,1.
1school of informatics, university of edinburgh2illc, university of amsterdam3department of computational linguistics, university of zurichb.zhang@ed.ac.uk, {ititov,bhaddow}@inf.ed.ac.uk, sennrich@cl.uzh.ch.
abstract.
document-level contextualinformation hasshown beneﬁts to text-based machine transla-tion, but whether and how context helps end-to-end (e2e) speech translation (st) is stillunder-studied.
we ﬁll this gap through exten-sive experiments using a simple concatenation-based context-aware st model, paired withadaptive feature selection on speech encodingsfor computational efﬁciency.
we investigateseveral decoding approaches, and introduce in-model ensemble decoding which jointly per-forms document- and sentence-level transla-tion using the same model.
our results on themust-c benchmark with transformer demon-strate the effectiveness of context to e2e st.compared to sentence-level st, context-awarest obtains better translation quality (+0.18-2.61 bleu), improves pronoun and homo-phone translation, shows better robustness to(artiﬁcial) audio segmentation errors, and re-duces latency and ﬂicker to deliver higher qual-ity for simultaneous translation.1.
1.introduction.
document-level context often offers extra informa-tive clues that could improve the understanding ofindividual sentences.
such clues have been proveneffective for textual machine translation (mt), par-ticularly in handling translation errors speciﬁc todiscourse phenomena, such as inaccurate corefer-ence of pronouns (guillou, 2016) and mistransla-tion of ambiguous words (rios et al., 2017).
be-sides, ensuring consistency in translation is virtu-ally impossible without document-level context aswell (voita et al., 2019).
analogous to mt, speechtranslation (st) also suffers from these translationissues, and super-sentential context could in factbe more valuable to st because 1) homophones.
figure 1: overview of the concatenation-based context-aware st. yn denotes the n-th target sentence in a document;xn denotes the speech encodings extracted from the n-th audiosegment.
we use dashed gray box to indicate the concatenationoperation.
“<s>”: sentence separator symbol..and acoustic noise bring additional ambiguity tost, and 2) a common use case in st is simulta-neous translation, where the system has to outputtranslations of sentence fragments, and may haveto predict future input to account for word orderdifferences between the source and target language(grissom ii et al., 2014).
both for ambiguity fromthe acoustic signal, and operating on small sentencefragments, we hypothesize that access to extra con-text2 will be beneﬁcial..although recent studies on st have achievedpromising results with end-to-end (e2e) mod-els (anastasopoulos and chiang, 2018; di gangiet al., 2019; zhang et al., 2020a; wang et al., 2020;dong et al., 2020), nevertheless, they mainly focuson sentence-level translation.
one practical chal-lenge when scaling up sentence-level e2e st to thedocument-level is the encoding of very long audiosegments, which can easily hit the computationalbottleneck, especially with transformers (vaswaniet al., 2017).
so far, the research question ofwhether and how contextual information beneﬁtse2e st has received little attention..in this paper, we answer this question through ex-tensive experiments by exploring a concatenation-.
1source code is available at https://github.com/.
2by default, we use context to denote both source- and.
bzhanggo/zero..target-side information from previous sentences..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2566–2578august1–6,2021.©2021associationforcomputationallinguistics2566based context-aware st model.
figure 1 illus-trates our model, where neighboring source (tar-get) sequences are chained together into one se-quence for joint translation.
this paradigm onlyrequires data-level manipulation, thus allowing usto reuse any existing sentence-level e2e st models.
despite its simplicity, this approach successfullyleverages contextual information to improve textualmt (tiedemann and scherrer, 2017; bawden et al.,2018; lopes et al., 2020), and here we adapt it to st.as for the computational bottleneck, we shortenthe speech encoding sequence via adaptive featureselection (zhang et al., 2020b,a, afs), which onlyretains a small subset of encodings (∼16%) foreach audio segment..we investigate several decoding methods, in-cluding chunk-based decoding and sliding-windowbased decoding.
we also study an extension ofthe latter with the constraint of target preﬁx, wherethe preﬁx denotes the translation of previous con-text speeches.
we ﬁnd that using these methodssometimes results in misaligned translations, par-ticularly when using the constraint.
this issuemanifests itself in mismatching sentence bound-aries and producing over- and/or under-translation,which greatly hurts sentence-based evaluation met-rics.
to avoid such misalignments, we introduce in-model ensemble decoding (imed) to regularize thedocument-level translation with its sentence-levelcounterpart.
note that we use the same context-aware st model here for both types of translation –that’s why we call it in-model ensemble..we adopt transformer (vaswani et al., 2017) forexperiments with the must-c dataset (di gangiet al., 2019).
we study the impact of context ontranslation in different settings.
our results demon-strate the effectiveness of contextual modeling.
ourmain ﬁndings are summarized below:.
• incorporating context improves overall trans-lation quality (+0.18-2.61 bleu) and beneﬁtspronoun translation across different languagepairs, resonating with previous ﬁndings in tex-tual mt (miculicich et al., 2018; huo et al.,2020).
in addition, context also improves thetranslation of homophones..• st models with contexts suffer less from (ar-.
tiﬁcial) audio segmentation errors..• contextual modeling improves translationquality and reduces latency and ﬂicker forsimultaneous translation under re-translationstrategy (arivazhagan et al., 2020a)..2 related work.
our work is inspired by pioneer studies on context-aware textual mt.
context beyond the current sen-tence carries information whose importance fortranslation cohesion and coherence has long beenposited (hardmeier et al., 2012; xiong and zhang,2013).
with the rapid development of neural mtand also available document-level textual datasets,research in this direction gained great popular-ity.
recent efforts often focus on either advancedcontextual neural architecture development (tiede-mann and scherrer, 2017; kuang et al., 2018; mi-culicich et al., 2018; zhang et al., 2018, 2020c;kang et al., 2020; chen et al., 2020; ma et al.,2020a; zheng et al., 2020) and/or improved analy-sis and evaluation targeted at speciﬁc discoursephenomena (bawden et al., 2018; l¨aubli et al.,2018; guillou et al., 2018; voita et al., 2019; kimet al., 2019; cai and xiong, 2020).
we follow thisresearch line, and adapt the concatenation-basedcontextual model (tiedemann and scherrer, 2017;bawden et al., 2018; lopes et al., 2020) to st. ourmain interest lies in exploring the impact of contexton st. developing dedicated contextual models forst is beyond the scope of this study, which weleave to future work..context-aware st extends the sentence-level sttowards streaming st which allows models to ac-cess unlimited previous audio inputs.
instead ofimproving contextual modeling, many studies onstreaming st aim at developing better sentence/-word segmentation policies to avoid segmenta-tion errors that greatly hurt translation (matusovet al., 2007; rangarajan sridhar et al., 2013; iranzo-s´anchez et al., 2020; zhang and zhang, 2020; ari-vazhagan et al., 2020b).
very recently, ma et al.
(2020b) proposed a memory augmented trans-former encoder for streaming st, where the previ-ous audio features are summarized into a growingcontinuous memory to improve the model’s contextawareness.
despite its success, this method ignoresthe target-side context, which turns out to have sig-niﬁcant positive impact on st in our experiments.
our study still relies on oracle sentence segmen-tation of the audio.
the most related work to oursis (gaido et al., 2020), which also investigated con-textualized translation and showed that context-aware st is less sensitive to audio segmentationerrors.
while they exclusively focus on the robust-ness to segmentation errors, our study investigatesthe beneﬁts of context-aware e2e st more broadly..2567(a) cbd.
(b) swbd.
(c) swbd-cons.
(d) imed.
figure 2: illustration of different decoding methods: chunk-based decoding (cbd, 2a), sliding-window based decoding without(swbd, 2b) and with (swbd-cons, 2c) the target preﬁx constraint and the proposed in-model ensemble decoding (imed, 2d).
the dashed blue box denotes model generation; the solid gray box (2c, 2d) indicates the target preﬁx constraint; sentences in thegray rectangle (2b) are discarded after generation.
the dashed arrow in imed stands for the sentence-level translation..3 context-aware st via concatenation.
we extend the sentence-level st with document-level context, by modeling up to c previoussource/target segments/sentences for translation.
formally, given a pre-segmented audio (source doc-ument) a = (cid:0)a1, .
.
.
, an (cid:1) as well as its pairedtarget document y = (cid:0)y1, .
.
.
, yn (cid:1), the model istrained to maximize the following likelihood:.
log p (y|a) =.
log p (cid:0)yn|xn, cn.
y , cnx.
(cid:1) ,.
(1).
n(cid:88).
n=1.
x and cn.
i=1 and {yn−i}c.where xn = afs (an), i.e.
the speech encod-ings extracted via afs (zhang et al., 2020a).
anand yn denote the n-th audio segment and targetsentence, respectively.
n is the number of seg-ments/sentences in the document.
cny standfor the source and target context, respectively, i.e.
{xn−i}ci=1.
adaptive feature selection audio segment isoften converted into frame-based features for neu-ral modeling.
different from text, each segmentmight contain hundreds or even thousands of suchfeatures, making contextual modeling computation-ally difﬁcult.
zhang et al.
(2020a) found that mostspeech encodings emitted by a transformer-basedaudio encoder carry little information for transla-tion, and their deletion even improves translationquality.
we follow zhang et al.
(2020a) and per-form afs to only extract those informative encod-ings (∼16%) optimized via sentence-level speechrecognition with l0drop (zhang et al., 2020b).
this greatly shortens the speech encoding sequence,thus enabling broader context exploration..x /cn.
the previous context (cny ) (tiedemann andscherrer, 2017; bawden et al., 2018) as shown infigure 1. after obtaining the afs-based encodings(xn) for each audio segment, we concatenate thoseencodings of neighboring segments to form thesource input.
the same is applied to the target-sidesentences, except for a separator symbol “<s>”inserted in-between sentences to distinguishsentence boundaries.3 such modeling enables us touse arbitrary encoder-decoder models for context-aware st, such as the transformer (vaswani et al.,2017) used in this paper.
despite no dedicatedhierarchical modeling (miculicich et al., 2018), thisparadigm still allows for intra- and inter-sentenceattention during encoding and decoding, whichexplicitly utilizes context for translation and hasbeen proven successful (lopes et al., 2020)..4.inference.
concatenation-based contextual modeling allowsfor different inference strategies with possibletrade-offs between simplicity/efﬁciency and accu-racy.
we investigate the following inference strate-gies (see figure 2):.
chunk-based decoding (cbd) cbd splits allinto non-audio segments in one documentoverlapping chunks, with each chunk concatenat-ing c + 1 segments, as shown in figure 2a.
cbddirectly translates each chunk, and then recoverssentence-level translation via the separator symbol“<s>”.
cbd is the most efﬁcient inference strat-egy, only encoding/decoding each sentence once,but it might suffer from misaligned translation,.
concatenation-based contextual modelingwe adopt the concatenation method to incorporate.
3note that we did not add similar boundary information toaudio segments, because afs implicitly captures these signalsthrough independent segment encoding..2568producing more or fewer sentences than the inputsegments.
we simply drop the extra generated sen-tences and replace the missing ones with “<unk>”when computing sentence-based evaluation metrics.
also, cbd introduces an independence assumptionbetween chunks..sliding window-based decoding(swbd)swbd avoids such inter-chunk independenceby sequentially translating each audio segment(xn), together with its corresponding previoussource context (cnx ).
we distinguish two variantsof swbd.
the ﬁrst variant, swbd, translatesthe concatenated segments and regards the lastgenerated sentence as the translation of the currentsegment while discarding all other generationsintroduce(figure 2b).
note thatinconsistencies between the output produced at atime step, and the one used as target context infuture time steps.
by contrast, the second variant,swbd-cons, leverages the previously generated(up to c) sentences as a decoding constraint, basedon which the model only needs to generate onesentence (figure 2c)..this might.
in-model ensemble decoding (imed) we ob-serve that swbd still suffers from misalignedtranslation, where the translation of the current seg-ment might contain information from previous seg-ments.
we introduce imed to alleviate this issue asshown in figure 2d.
imed extends swbd-consby interpolating the document-level prediction (pd)with the sentence-level prediction (ps) as follows:.
λps.
θ (yn.
t |yn.
<t, xn) + (1 − λ)pd.
θ (yn.
t |c) ,.
(2).
x , cn.
y , xn, yn.
where c = {cn<t}, λ is a hyperparam-eter, ynt denotes the t-th target word in sentenceyn, and both predictions are based on the samemodel θ. intuitively, the sentence-level translationacts as a regularizer, avoiding the over- or under-translation.
note imed with λ = 0 corresponds toswbd-cons..5 experiments.
5.1 setup.
we use the must-c dataset (di gangi et al., 2019)for experiments, which was collected from englishted talks and covers translations from englishto 8 different languages, including german (de),spanish (es), french (fr), italian (it), dutch (nl),portuguese (pt), romanian (ro) and russian (ru).
must-c offers a standard training, development.
and test set split for each language pair, with eachdataset consisting of english audio, english tran-scriptions and their translations.
each training setcontains transcribed speeches of ∼452 hours with∼252k utterances on average.
we report results ontst-common, whose size ranges from 2502 (es)to 2641 (de) utterances.
we perform our majorstudy on must-c en-de..to construct acoustic features, for each audiosegment, we extract 40-channel log-mel ﬁlterbanksusing overlapping windows of 25 ms and stepsize of 10 ms. we enrich these features withtheir ﬁrst and second-order derivatives, followedby mean subtraction and variance normalization.
following zhang et al.
(2020a), we perform non-overlapping feature stacking to combine the fea-tures of three consecutive frames.
all the texts aretokenized and truecased (koehn et al., 2007), without-of-vocabulary words handled by bpe segmen-tation (sennrich et al., 2016), using 16k mergingoperations..model settings and evaluation our context-aware st follows transformer base (vaswani et al.,2017): 6 layers, 8 attention heads, and hidden/feed-forward size 512/2048.
we use adam (β1 =0.9, β2 = 0.98) (kingma and ba, 2015) for pa-rameter updates with label smoothing of 0.1. weuse the same learning rate schedule as vaswani et al.
(2017) and set the warmup step to 4k.
we applydropout to attention weights and residual connec-tions with a rate of 0.2 and 0.5, respectively.
by de-fault, we set c = 2 and λ = 0.5. following (zhanget al., 2020a), we apply afs((cid:15) = −0.1, β = 2/3)to both temporal and feature dimensions for fea-ture selection, which prunes out ∼84% speech en-codings.
we initialize our context-aware st withthe sentence-level baseline, i.e.
st+afs, and thenﬁnetune the model for 20k steps based on the con-catenation method with a batch size of around 40ksubwords.4 we adopt beam search for decoding,with a beam size of 4 and length penalty of 0.6. weaverage the last 5 checkpoints for evaluation..we measure general translation quality with tok-enized case-sensitive bleu (papineni et al., 2002)and also report the detokenized one via sacre-bleu (post, 2018)5 for cross-paper comparison.
we calculate bleu based on sentences unless oth-.
4our experiments show that such initialization eases thelearning of long inputs and improves the convergence ofcontext-aware st..5signature: bleu+c.mixed+#.1+s.exp+tok.13a+v.1.3.6.
2569id model.
bleu.
apt.
baseline (st+afs).
22.38 (27.40).
60.77.ours + cbdours + swbdours + swbd-consours + imed.
22.72 (27.95)22.70 (28.02)22.11 (27.98)22.86 (28.03).
1 + 20k-step ﬁnetuning5 + λ = 1.0.
22.02 (27.00)22.42 (27.62).
1 + lp = 1.03 + lp = 1.05 + lp = 1.03 w/o cny5 w/o cny.
3 w/o baseline initial.
5 w/o baseline initial..22.71 (27.77)22.97 (28.29)22.94 (28.11).
21.12 (26.17)20.72 (25.43).
21.75 (27.15)21.97 (27.20).
62.3162.8360.9462.56.
61.5861.96.
61.8963.5162.76.
59.5158.18.
62.2962.08.
1.
2345.
67.
8910.
1112.
1314.table 1: case-sensitive tokenized bleu and apt for dif-ferent models and settings on must-c en-de test set.
num-bers in bracket denote document-based bleu.
lp: the lengthpenalty for beam search decoding.
“w/o cny ”: models thatare trained without target-side context.
best results are high-lighted in bold.
note c = 2, λ = 0.5 and lp = 0.6 by default..erwise speciﬁed.
we use apt (miculicich werlenand popescu-belis, 2017), the accuracy of pronountranslation, as an approximate proxy for document-level evaluation.
word alignment required by aptis automatically extracted via fast align (dyer et al.,2013) with the strategy “grow-diag-ﬁnal-and”..5.2 results on must-c en-de.
does context improve translation?
yes, but thedecoding method matters for context-aware st. ta-ble 1 summarizes the results.
our model withimed outperforms baseline by +0.48 bleu (sig-niﬁcant at p < 0.05)6 and +1.79 apt (1→5),clearly showing the beneﬁts from contextual model-ing.
although swbd-cons yields worse sentence-based bleu (-0.27, 1→4), it still beats baseline indocument-based bleu (+0.58) and pronoun trans-lation (+0.17 apt).
the reason behind this inferiorbleu partially lies in misaligned translation (seetable 8 in appendix for example).
we observe thatswbd-cons sometimes segments its output in away that is misaligned to the reference segmenta-tion.
this also hurts cbd, where cbd producesmismatched sentences for around 1.8% cases.
thisis only a problem if we rely on the sentence-levelalignment for bleu, but not when we measuredocument-based bleu (in brackets), where trans-lations in one document are concatenated into asequence for bleu calculation.
overall, swbd.
6we perform signiﬁcance test using bootstrap-hypothesis-.
difference-signiﬁcance.pl in moses (koehn et al., 2007)..and imed are more stable and perform the best,and swbd surpasses baseline by 2.06 apt (1→3).
we will proceed with using imed and swbd formore reliable results with apt and later analysis.
since we ﬁnetune our model based on the pre-trained baseline, directly comparing with baselinemight be unfair.
to offset its inﬂuence, we continueto train baseline for the same 20k steps, followingthe settings in section 5.1. results show that thisextra training (1→6) slightly deteriorates bleu(-0.36) and only explains part of the improvementin apt (+0.81).
therefore, the gain brought byswbd and imed does not come from longer train-ing.
however, we do observe that initializing fromthe sentence-level baseline beneﬁts context-awarest, compared to directly training context-aware stfrom the afs model (13→3, 14→4)..apart from faster convergence and higher qual-ity, another beneﬁt of this ﬁnetuning is that thetrained context-aware st still carries the abilityto translate individual sentences.
table 1 showsthat using context-aware st for sentence-leveltranslation (1→7) yields similar bleu to base-line (+0.04) but surprisingly much better pronountranslation (+1.19), although it still underperformsswbd and imed.
the fact that we can performsentence-level st using the same context-aware stmodel indicates that it can be useful for ensembling,as conﬁrmed by the effectiveness of imed..upon closer inspection, we ﬁnd that context-aware st prefers to produce longer translationsthan baseline.
to control for the effects of out-put length on bleu differences, we experimentwith larger length penalty (lp: 0.6→1.0) to beamsearch.
results in table 1 show that biasing the de-coding greatly improves sentence-level st (1→8),achieving performance on par with context-awarest (when lp is 0.6) in terms of bleu with simi-lar translation lengths but still falling short of pro-noun translation (-0.94 apt, 8→3).
in addition, weobserve that context-aware st also beneﬁts fromdecoding with larger length penalty, beating allsentence-level st models (3→9, 5→10).
particu-larly, swbd with lp of 1.0 delivers the best bleuof 22.97 and apt of 63.51 (3→9).
note we adoptlp of 0.6 for the following experiments..does target-side context matter for context-aware st?
yes, it matters a lot.
by default, weutilize both source- and target-side context for con-textual modeling.
removing the target-side part(also at training), as shown in table 1 (11, 12), sub-.
2570model.
bleu.
apt.
swbdswbd + random cnx.imedimed + random cnximed + random cnyy & cnimed + random cny.
22.7022.31.
22.8621.8321.9921.76.
62.8361.16.
62.5659.9560.0159.67.table 2: case-sensitive tokenized bleu and apt forcontext-aware st with random source/target context on must-c en-de test set.
we report average performance over threeruns with different random seeds.
c = 2, λ = 0.5. incorrectcontext hurts our model..stantially weakens translation quality, even leadingto worse performance than baseline.
apart fromoffering direct target-side translation clues, we ar-gue that the target-side context also enforces thecontext-aware st to utilize the source-side contextfor translation, thus beneﬁting its training.
thisobservation echoes with several previous studieson textual translation (bawden et al., 2018; huoet al., 2020; lopes et al., 2020)..does the model learn to utilize context?
yes.
we answer this question by studying the impactof incorrect context on our model.
we replace thecorrect source context with some random audiosegments from the same document, and randomlyselect the target context from previous translationsduring decoding.
intuitively, the performance ofour model should be intact if it ignores the con-text.
note that we trained our model with correctcontexts but test it with random contexts here..results in table 2 show that the randomizedcontext, either source- or target-side, hurts the per-formance of our model in both bleu and apt,similar to the ﬁndings in (voita et al., 2018), andthe translation of pronouns suffers more (> -1.6apt).
compared to swbd, the incorrect contexthas more negative impact on imed, resulting inworse performance than baseline (table 1), al-though imed also uses sentence-level translation.
we ascribe this to the target preﬁx constraint inimed which makes translation errors at early de-coding much easier to propagate.
we observe thatthe incorrect target context acts similarly to itssource counterpart under imed, albeit its selectionscope is much smaller (only limited to the trans-lated segments), and combining both contexts leadsto a slight but consistent performance degradation.
these results demonstrate that our model indeedlearns to use contextual information for translation..figure 3: case-sensitive tokenized bleu (top) and apt(bottom) as a function of context size c on must-c en-detest set..figure 4: case-sensitive tokenized bleu (left y-axis) andapt (right y-axis) on must-c en-de test set when varying λfor imed.
solid and dashed curves are for bleu and apt,respectively.
c = 2..how much context sentences should we use?
although adding extra context provides more in-formation, it makes learning harder: neural modelsoften struggle with long sequences.
figure 3 showsthe impact of context size on translation.
we ﬁndthat our models do not beneﬁt from context sizebeyond 2 previous segments.
figure 3 also showsthat the overall trend of the impact of c on bleuand apt is similar for different decoding meth-ods.
increasing c to 1 delivers the best apt, whilecontext-aware st achieves its best bleu at c = 2.we use c = 2 for the following experiments..impact of λ on imed.
imed heavily relies onthe hyperparameter λ (eq.
2) to control its prefer-ence between sentence-level and document-leveldecoding.
figure 4 shows its impact on translation.
257122.022.222.422.622.8bleu01234c61.061.562.062.563.0aptswbdimedλ=0.50.00.10.20.30.40.50.60.70.80.91.0λ22.222.422.622.8bleu61.061.562.062.5aptbleuaptmodel.
baseline (st+afs).
ours + swbdours + imedours + imed λ = 1.0.acchp.
48.93.
49.9049.6648.77.table 3: translation accuracy of homophones (acchp) onmust-c en-de test set.
c = 2, λ = 0.5..quality, which clearly reveals a trade-off.
the per-formance of imed (bleu and apt) reaches itspeak at λ = 0.4, and decreases when λ becomeseither smaller or larger.
the optimal value of λ forimed might vary greatly across different languagepairs.
it also shows some difference across evalu-ation sets (see figure 7 in appendix).
in the fol-lowing experiments, we will apply equal weighting(λ = 0.5), a common choice for model ensemblesand not substantially worse than the optimum onthis dataset..impact of context on homophone translation.
homophones (words that sound the same but holddifferent meanings, such as “i” vs. “eye” and“would” vs. “wood”) and other acoustically sim-ilar words increase the learning difﬁculty of stmodels compared to textual mt.
to allow for au-tomatic quantitative evaluation, we extract wordsfrom the must-c test set transcriptions whichshare the same phonemes with montreal forcedaligner (mcauliffe et al., 2017).
we collect all ho-mophones and evaluate their translation accuracy(acchp) in the same way as apt..table 3 shows that context-aware st outper-forms baseline by > 0.73 acchp, where swbdperforms slightly better than imed.
after remov-ing the document-level decoding, imed (λ = 1.0)performance drops greatly, even underperformingbaseline.
while we see some improvements tohomophone translations, they are in the same rela-tive range as general improvements from context.
anecdotal examples from manual inspection (seetable 7 in appendix) indicate that context may attimes help disambiguate acoustically similar forms,but that (near-)homophones still remain a salientsource of translation errors..context improves the robustness of st modelsto audio segmentation errors.
in must-c, theaudio is already well-segmented, with each seg-ment corresponding to a short transcript.
neverthe-less, natural audio, streaming speeches in particular,has no such segment boundaries, and how to parti-.
model.
random gold.
baseline (st+afs).
ours + swbdours + imed.
20.40.
21.8322.03.
27.40.
28.0228.03.table 4: document-level case-sensitive tokenized bleu fordifferent models on must-c en-de test set with erroneousaudio segmentation.
we report average bleu over three runs;each run uses a different random seed to simulate segmentationerrors.
c = 2, λ = 0.5. random/gold: document-basedbleu when the random/gold segments are used..tion audio itself is an active research area (rangara-jan sridhar et al., 2013; zhang and zhang, 2020).
since st models are often trained with gold seg-ments, they inevitably suffer from segmentationerrors at inference when the gold ones are unavail-able..the bottleneck mainly comes from the incom-pleteness of each segment, which, we argue, con-textual information could alleviate.
we simulatesegmentation errors by randomly re-segmenting theaudio in must-c en-de test set based on the givensegment number.
especially, given an audio withn gold segments, we randomly re-segment it inton disjoint pieces, where each piece usually has dif-ferent boundaries against its gold counterpart.7 weevaluate different st models with document-basedbleu..table 4 summarizes the results.
segmentationnoise deteriorates translation quality for all st mod-els to a large degree (> -6 bleu).
compared tosentence-level st, context-aware st is less sen-sitive to those errors.
in particular, our modelwith imed yields a document-based bleu of22.03, substantially outperforming baseline (by1.63 bleu).
our results also conﬁrm the ﬁndingsof gaido et al.
(2020)..context beneﬁts simultaneous translation.
si-multaneous translation requires that we start de-coding before receiving the whole audio input tominimize latency; operating on such short units in-creases ambiguity, and the model may be forced topredict future input to account for word order differ-ences, which we hypothesize is easier with accessto super-sentential context.
we focus on segment-.
7note we intentionally keep the same segment number, n ,in the simulated noisy segmentation, because this offers usa fair setup to analyze the impact of segmentation errors onthe ﬁnal translation when compared to the gold segmentation.
this avoids the potential inﬂuence resulting from mismatchedsegment number.
we leave the study of the model’s robustnessto genuine segmentation noises to future work..2572metric.
model.
de.
es.
fr.
it.
nl.
pt.
ro.
ru.
bleu↑.
sacrebleu↑.
apt↑.
acchp↑.
baseline (st+afs)ours + swbdours + imed.
baseline (st+afs)ours + swbdours + imed.
baseline (st+afs)ours + swbdours + imed.
baseline (st+afs)ours + swbdours + imed.
22.3822.7022.86.
22.422.722.9.
60.7762.8362.56.
48.9349.9049.66.
27.0427.1227.50.
26.927.027.3.
32.8733.0133.60.
43.8543.7344.66.
33.4334.2334.28.
31.632.432.5.
63.6764.5864.66.
56.9657.3057.76.
23.3523.4623.53.
23.023.023.1.
34.7435.2035.20.
41.0840.0440.62.
25.0525.8426.12.
24.925.726.0.
61.0061.6961.75.
50.7351.4852.07.
26.5526.6327.37.
26.326.427.1.
34.7935.5636.50.
43.6444.0345.42.
21.8723.7024.48.
21.022.823.6.
38.2840.3040.92.
47.0747.6648.49.
14.9215.5315.95.
14.715.415.8.
40.6141.7442.32.
30.8032.6732.56.table 5: results on must-c for 8 language pairs.
we set c = 2, λ = 0.5. numbers in bold are the best results..model.
bleu↑ dal↓ ne↓.
baseline (st+afs).
ours + swbdours + swbd-consours + imed.
21.02.
21.8621.9822.55.
3.97.
3.823.753.91.
1.72.
1.951.591.64.table 6: simultaneous translation results (bleu, dal andne) for different models on must-c en-de test set.
c =2, λ = 0.5..level e2e simultaneous translation, and adopt there-translation method (niehues et al., 2016; ari-vazhagan et al., 2020b,a) where we translate thesource input segment from scratch after every 1second.
for training, we ﬁnetune each model forextra 20k steps with a 1:1 mix of full-segment andpreﬁx pairs, following arivazhagan et al.
(2020a).
we construct the preﬁx pairs by uniformly select-ing an audio preﬁx length and then proportionallydeciding the target preﬁx length based on the sen-tence length.
note that the context inputs in ourmodel are still full segments/sentences.
we adopttokenized bleu, differentiable average lagging(dal), and normalized erasure (ne) to evaluatethe translation quality, latency and stability, respec-tively, following arivazhagan et al.
(2020a).
notedal and ne are measured based on words..results in table 6 show that context-aware stimproves translation quality (> +0.84 bleu) andreduces translation latency (> -0.06 dal) regard-less of the decoding method.
it also enhances trans-lation stability when the target preﬁx constraintis applied (> -0.08 ne, swbd-cons & imed).
swbd performs worse in ne, because it allowschanges in the translation of context which in-creases instability.
overall, context provides extrainformation to the translation model, before the.
figure 5: dal (left y-axis) and ne (right y-axis) as a func-tion of λ for imed on must-c en-de test set in simultaneoustranslation setting.
solid and dashed curves are for dal andne, respectively.
c = 2. λ → 0.0: document-level decoding;λ → 1.0: sentence-level decoding..e2e st models see the whole input, which beneﬁtssimultaneous translation..figure 5 further illustrates how context impactssimultaneous translation.
with the increase ofsentence-level decoding (λ → 1.0), imed pro-duces higher dal and ne, i.e.
worse quality.
weascribe the reduction of latency and stability in ourmodel to the inclusion of contextual information..5.3 results on other language pairs.
table 5 summarizes the results for all 8 transla-tion pairs covered by must-c. overall, our modelobtains improvements over most metrics and lan-guage pairs, despite their different language charac-teristics.
out of 8 languages, our model performsrelatively worse on es and it with smaller bleugains and even negative results in acchp.
by con-trast, our model yields the largest improvement onro.
in particular, our model with imed achieves adetokenized bleu of 23.6 on en-ro, surpassingthe state-of-the-art result 22.2 (zhao et al., 2020)reported so far..25730.00.10.20.30.40.50.60.70.80.91.0λ3.83.94.04.1dal1.601.651.701.751.801.85nedalne6 conclusion and future work.
our experiments conﬁrm the effectiveness ofcontext-aware modeling for end-to-end speechtranslation.
with concatenation-based contextualmodeling and appropriate decoding method, weobserve positive impact of context on translation.
context-aware st improves general translationquality in bleu, and also helps pronoun and ho-mophone translation.
st models become less sen-sitive to (artiﬁcial) audio segmentation errors withcontext.
in addition, context also improves simulta-neous translation by reducing latency and erasure.
we observe overall positive results over differentlanguages and evaluation metrics on the must-ccorpus..in the future, we will investigate more dedicatedneural architectures to handle long-form speechinput.
while we relied on a dataset with sentencesegmentation in this work, we are interested in re-moving the reliance on segmentation at inferencetime to implement the full-ﬂedged streaming trans-lation scenario..acknowledgements.
we thank the reviewers for their insightful com-ments.
this project has received funding fromthe european union’s horizon 2020 research andinnovation programme under grant agreements825460 (elitr).
rico sennrich acknowledges sup-port of the swiss national science foundation(mutamur; no.
176727)..references.
antonios anastasopoulos and david chiang.
2018.tied multitask learning for neural speech translation.
in proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 82–91, new orleans,louisiana.
association for computational linguis-tics..naveen arivazhagan, colin cherry, wolfgangmacherey, and george foster.
2020a.
re-translationversus streaming for simultaneous translation.
inproceedings of the 17th international conferenceon spoken language translation, pages 220–227,online.
association for computational linguistics..naveen arivazhagan, colin cherry, isabelle te, wolf-gang macherey, pallavi baljekar, and george fos-ter.
2020b.
longform, simultaneous, spoken language translation..re-translation strategies for.
in icassp 2020-2020 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 7919–7923.
ieee..rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1304–1313, new orleans, louisiana.
association for computational linguistics..xinyi cai and deyi xiong.
2020. a test suite for eval-uating discourse phenomena in document-level neu-ral machine translation.
in proceedings of the sec-ond international workshop of discourse process-ing, pages 13–17, suzhou, china.
association forcomputational linguistics..junxuan chen, xiang li, jiarui zhang, chulun zhou,jianwei cui, bin wang, and jinsong su.
2020. mod-eling discourse structure for document-level neuralin proceedings of the firstmachine translation.
workshop on automatic simultaneous translation,pages 30–36, seattle, washington.
association forcomputational linguistics..mattia a. di gangi, roldano cattoni, luisa bentivogli,matteo negri, and marco turchi.
2019. must-c:a multilingual speech translation corpus.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 2012–2017,minneapolis, minnesota.
association for computa-tional linguistics..mattia a. di gangi, matteo negri, and marco turchi.
2019. adapting transformer to end-to-end spokenin proc.
interspeech 2019,language translation.
pages 1133–1137..qianqian dong, mingxuan wang, hao zhou, shuangxu, bo xu, and lei li.
2020. sdst: successive de-coding for speech-to-text translation.
arxiv preprintarxiv:2009.09737..chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameter-in proceedings of theization of ibm model 2.
2013 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 644–648, at-lanta, georgia.
association for computational lin-guistics..marco gaido, mattia a. di gangi, matteo negri,mauro cettolo, and marco turchi.
2020. con-textualized translation of automatically segmentedin proc.
interspeech 2020, pages 1471–speech.
1475..alvin grissom ii, he he, jordan boyd-graber, johnmorgan, and hal daum´e iii.
2014. don’t until.
2574the ﬁnal verb wait: reinforcement learning for si-multaneous machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1342–1352, doha, qatar.
association for computationallinguistics..liane guillou, christian hardmeier, ekaterinalapshinova-koltunski, and sharid lo´aiciga.
2018.a pronoun test suite evaluation of the english–german mt systems at wmt 2018. in proceedingsof the third conference on machine translation:shared task papers, pages 570–577, belgium, brus-sels.
association for computational linguistics..liane kirsten guillou.
2016..incorporating pronounfunction into statistical machine translation.
ph.d.thesis, university of edinburgh..christian hardmeier, joakim nivre, and j¨org tiede-mann.
2012. document-wide decoding for phrase-in proceed-based statistical machine translation.
ings of the 2012 joint conference on empiricalmethods in natural language processing and com-putational natural language learning, pages 1179–1190, jeju island, korea.
association for computa-tional linguistics..jingjing huo, christian herold, yingbo gao, leonarddahlmann, shahram khadivi, and hermann ney.
2020. diving deep into context-aware neural ma-chine translation.
in proceedings of the fifth confer-ence on machine translation, pages 604–616, on-line.
association for computational linguistics..javier iranzo-s´anchez, adri`a gim´enez pastor, joan al-bert silvestre-cerd`a, pau baquero-arnal,jorgecivera saiz, and alfons juan.
2020. direct segmen-intation models for streaming speech translation.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2599–2611, online.
association for computa-tional linguistics..xiaomian kang, yang zhao,.
jiajun zhang, andchengqing zong.
2020. dynamic context selectionfor document-level neural machine translation via re-in proceedings of the 2020inforcement learning.
conference on empirical methods in natural lan-guage processing (emnlp), pages 2242–2254, on-line.
association for computational linguistics..yunsu kim, duc thanh tran, and hermann ney.
2019.when and why is document-level context useful inneural machine translation?
in proceedings of thefourth workshop on discourse in machine trans-lation (discomt 2019), pages 24–34, hong kong,china.
association for computational linguistics..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,.
brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..shaohui kuang, deyi xiong, weihua luo, andguodong zhou.
2018. modeling coherence forneural machine translation with dynamic and topicin proceedings of the 27th internationalcaches.
conference on computational linguistics, pages596–606, santa fe, new mexico, usa.
associationfor computational linguistics..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
ain proceed-case for document-level evaluation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 4791–4796,brussels, belgium.
association for computationallinguistics..ant´onio lopes, m. amin farajian, rachel bawden,michael zhang, and andr´e f. t. martins.
2020.document-level neural mt: a systematic compari-son.
in proceedings of the 22nd annual conferenceof the european association for machine transla-tion, pages 225–234, lisboa, portugal.
european as-sociation for machine translation..shuming ma, dongdong zhang, and ming zhou.
2020a.
a simple and effective uniﬁed encoder fordocument-level machine translation.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 3505–3511, on-line.
association for computational linguistics..xutai ma, yongqiang wang, mohammad javaddousti, philipp koehn, and juan pino.
2020b.
streaming simultaneous speech translation witharxiv preprintaugmented memory transformer.
arxiv:2011.00033..evgeny matusov, dustin hillard, mathew magimai-doss, dilek hakkani-t¨ur, mari ostendorf, and her-mann ney.
2007. improving speech translation within eighth annualautomatic boundary prediction.
conference of the international speech communica-tion association..michael mcauliffe, michaela socolof, sarah mi-huc, michael wagner, and morgan sonderegger.
2017. montreal forced aligner: trainable text-in interspeech, vol-speech alignment using kaldi.
ume 2017, pages 498–502..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neu-ral machine translation with hierarchical attentionin proceedings of the 2018 conferencenetworks..2575on empirical methods in natural language process-ing, pages 2947–2954, brussels, belgium.
associa-tion for computational linguistics..lesly miculicich werlen and andrei popescu-belis.
2017. validation of an automatic metric for the ac-in proceed-curacy of pronoun translation (apt).
ings of the third workshop on discourse in machinetranslation, pages 17–25, copenhagen, denmark.
association for computational linguistics..jan niehues, thai son nguyen, eunah cho, thanh-leha, kevin kilgour, markus m¨uller, matthias sper-ber, sebastian st¨uker, and alex waibel.
2016. dy-namic transcription for low-latency speech transla-tion.
in interspeech 2016, pages 2513–2517..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, belgium, brussels.
association for computa-tional linguistics..vivek kumar rangarajan sridhar, john chen, srinivasbangalore, andrej ljolje, and rathinavelu chengal-varayan.
2013. segmentation strategies for stream-ing speech translation.
in proceedings of the 2013conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 230–238, atlanta,georgia.
association for computational linguistics..annette rios, laura mascarell, and rico sennrich.
2017. improving word sense disambiguation in neu-ral machine translation with sense embeddings.
inproceedings of the second conference on machinetranslation, pages 11–19, copenhagen, denmark.
association for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the third workshop on discourse in machinetranslation, pages 82–92, copenhagen, denmark.
association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allin i. guyon, u. v. luxburg, s. bengio,you need..h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..elena voita, rico sennrich, and ivan titov.
2019.when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1198–1212, flo-rence, italy.
association for computational linguis-tics..elena voita, pavel serdyukov, rico sennrich, and ivantitov.
2018. context-aware neural machine trans-in proceedingslation learns anaphora resolution.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1264–1274, melbourne, australia.
associa-tion for computational linguistics..chengyi wang, yu wu, shujie liu, ming zhou, andzhenglu yang.
2020. curriculum pre-training forend-to-end speech translation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 3728–3738, online.
as-sociation for computational linguistics..deyi xiong and min zhang.
2013. a topic-basedcoherence model for statistical machine translation.
in proceedings of the twenty-seventh aaai con-ference on artiﬁcial intelligence, aaai’13, page977–983.
aaai press..biao zhang, ivan titov, barry haddow, and rico sen-nrich.
2020a.
adaptive feature selection for end-to-end speech translation.
in findings of the associa-tion for computational linguistics: emnlp 2020,pages 2533–2544, online.
association for compu-tational linguistics..biao zhang, ivan titov, and rico sennrich.
2020b.
on sparsifying encoder outputs in sequence-to-sequence models.
arxiv preprint arxiv:2004.11854..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 533–542, brussels, bel-gium.
association for computational linguistics..pei zhang, boxing chen, niyu ge, and kai fan.
2020c.
long-short term masking transformer: a simplebut effective baseline for document-level neural ma-chine translation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1081–1087, online.
as-sociation for computational linguistics..ruiqing zhang and chuanqiang zhang.
2020. dy-namic sentence boundary detection for simultaneousin proceedings of the first workshoptranslation..2576on automatic simultaneous translation, pages 1–9,seattle, washington.
association for computationallinguistics..chengqi zhao, mingxuan wang, and lei li.
2020.arxiv.
neurst: neural speech translation toolkit.
preprint arxiv:2012.10018..zaixiang zheng, xiang yue, shujian huang, jiajunchen, and alexandra birch.
2020. towards mak-ing the most of context in neural machine translation.
in proceedings of the twenty-ninth internationaljoint conference on artiﬁcial intelligence, ijcai-20, pages 3983–3989.
international joint confer-ences on artiﬁcial intelligence organization.
maintrack..a impact of c and λ on dev set.
results in figure 6 and 7 show that the optimalvalue of c and λ also differs across evaluationsets.
overall, setting c = 2 and λ = 0.5 offers usdecent performance.
note again, we selected theseconﬁgurations for generality and simplicity ratherthan its being optimal..b case study on homophone.
translation.
c examples for misaligned translation.
figure 6: case-sensitive tokenized bleu (top) and apt(bottom) as a function of context size c on must-c en-dedev set..figure 7: case-sensitive tokenized bleu (left y-axis) andapt (right y-axis) on must-c en-de dev set when varyingλ for imed.
solid and dashed curves are for bleu and apt,respectively.
c = 2..257721.5021.7522.0022.2522.5022.75bleu01234c57.057.558.058.5aptswbdimedλ=0.50.00.10.20.30.40.50.60.70.80.91.0λ22.022.222.422.622.8bleu57.057.558.058.559.0aptbleuaptcontextsourcereference.
i remember my ﬁrst ﬁre.
i was the second volunteer on the scene, so there was a pretty good chance i was going to get in.
ich war der zweite freiwillige an der brandstelle, ich hatte also recht gute chancen hinein zu k¨onnen..baselineours + swbdours + swbd-consours + imed.
ich war der zweite freiwillige auf der cnu, also war ich ziemlich gut darin.
ich war der zweite freiwillige auf der cnn, also gab es eine ziemlich gute chance, dass ich sie bekommen w¨urde.
ich war der zweite freiwillige auf dem cnn, also gab es eine ziemlich gute chance, dass ich sie bekommen w¨urde.
ich war der zweite freiwillige auf dem cnn, also war ich ziemlich gut darin, dass ich ihn kriegen w¨urde..contextsourcereference.
the human genome project started in 1990, and it took 13 years.
it cost 2.7 billion dollars.
es kostete 2,7 milliarden dollar..es kostet 2,7 milliarden dollar.
(en: costs)baselinees kostete 2,7 milliarden dollar.
ours + swbdours + swbd-cons es kostete 2,7 milliarden dollar.
ours + imed.
es kostet 2,7 milliarden dollar.
(en: costs).
table 7: examples of translation errors due to confusion with near-homophones (bold) from the must-c en-de test set..source.
(1).
reference.
translation.
she asked the monk, ”why is it that her hand is so warm and the rest of her is so cold?” ”becauseyou have been holding it since this morning,” he said.
”you have not let it go.”sie fragte den m¨onch: ”wieso ist ihre hand so warm und der rest von ihr ist so kalt?” ”weil siesie seit heute morgen halten”, sagte er.
”sie haben sie nicht losgelassen.”sie fragte den monat: ”warum ist ihre hand so warm?” und der rest von ihr ist so kalt, weil ihrseit diesem morgen das h¨alt..(2).
if there is a sinew in our family, it runs through the women..sourcereference wenn es in unserer familie ein band gibt, dann verl¨auft es durch die frauen.
translation er sagte: ”sie haben es nicht geschafft, loszulassen.”.
table 8: example of misaligned translation for swbd-cons from the must-c en-de test set.
the translation for the secondsegment (2) actually aligns with the ﬁrst one (1), as highlighted in bold..2578