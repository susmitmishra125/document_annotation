a neural transition-based model for argumentation miningjianzhu bao1,2∗, chuang fan1,2∗, jipeng wu1,2, yixue dang3jiachen du1,2, ruifeng xu1,4†1harbin institute of technology (shenzhen), china2joint lab of china merchants securities and hitsz3china merchants securities co., ltd.4peng cheng laboratory, shenzhen, china{jianzhubao, fanchuanghit}@gmail.comwujipeng@stu.hit.edu.cn, dangyixue@cmschina.com.cnjacobvan199165@gmail.com, xuruifeng@hit.edu.cn.
abstract.
the goal of argumentation mining is to au-tomatically extract argumentation structuresfrom argumentative texts.
most existing meth-ods determine argumentative relations by ex-haustively enumerating all possible pairs of ar-gument components, which suffer from low ef-ﬁciency and class imbalance.
moreover, dueto the complex nature of argumentation, thereis, so far, no universal method that can addressboth tree and non-tree structured argumenta-tion.
towards these issues, we propose a neu-ral transition-based model for argumentationmining, which incrementally builds an argu-mentation graph by generating a sequence ofactions, avoiding inefﬁcient enumeration op-erations.
furthermore, our model can han-dle both tree and non-tree structured argumen-tation without introducing any structural con-straints.
experimental results show that ourmodel achieves the best performance on twopublic datasets of different structures..figure 1: an example of argumentation mining fromthe cdcp dataset (park and cardie, 2018).
policy,fact, and value represent the types of acs and reasonrefers to the types of ars.
note that, the cdcp datasetwe use is preprocessed by niculae et al.
(2017)..1.introduction.
argumentation mining (am) aims to identify the ar-gumentation structures in text, which has receivedwidespread attention in recent years (lawrence andreed, 2019).
it has been shown beneﬁcial in abroad range of ﬁelds, such as information retrieval(carstens and toni, 2015; stab et al., 2018), auto-mated essay scoring (wachsmuth et al., 2016; keet al., 2018), and legal decision support (palau andmoens, 2009; walker et al., 2018).
given a piece ofparagraph-level argumentative text, an am systemﬁrst detects argument components (acs), whichare segments of text with argumentative meaning,and then extracts the argumentative relations (ars)between acs to obtain an argumentation graph,where the nodes and edges represent acs and ars,.
∗equal contribution† corresponding author.
respectively.
an example of am is shown in fig-ure 1, where the text is segmented into ﬁve acs,and there are four ars.
in this instance, the typesof ac2 and ac3 are fact (non-experiential objec-tive proposition) and value (proposition containingvalue judgments), respectively.
in addition, thereis an ar from ac2 to ac3, i.e., “the check eitherbounced or it did not.” is the reason of “there’snot a lot of grey area here.”, for the latter is a valuejudgment based on the fact of the former..generally, am involves several subtasks, in-cluding 1) argument component segmentation(acs), which separates argumentative text fromnon-argumentative text; 2) argument componenttype classiﬁcation (actc), which determines thetypes of acs (e.g., policy, fact, value, etc.
); 3)argumentative relation identiﬁcation (ari), whichidentiﬁes ars between acs; 4) argumentative rela-tion type classiﬁcation (artc), which determines.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6354–6364august1–6,2021.©2021associationforcomputationallinguistics6354[eitherstateorfederallawshouldprovidesomepenaltyforpassingannsfor“closedaccount”checkthatincludesapresumptionofguilt.]ac1[thecheckeitherbouncedoritdidnot.]ac2[there’snotalotofgreyareahere.]ac3[thatsaid,eitherstateorfederallawshouldalsoallowthedrafterofthechecka“safeharbor”whereinthey···]ac4[michiganhassuchaprocedureanditseemsrelativelyequitable.
]ac5amac2factac3valueac1policyreasonargumentative textargumentation graphreasonreasonac4policyac5valuereasonthe types of ars (e.g., reason and evidence).
mostprevious works assume that subtask 1) acs hasbeen completed, that is, acs have been segmented,and focus on other subtasks (potash et al., 2017;kuribayashi et al., 2019; chakrabarty et al., 2019).
in this paper, we also make such an assumption,and perform actc and ari on this basis..among all the subtasks of am, ari is the mostchallenging because it requires understanding com-plex semantic interactions between acs.
mostprevious works exhaustively enumerate all possiblepairs of acs (i.e., all acs are matched to each otherby cartesian products) to determine the ars be-tween them (kuribayashi et al., 2019; morio et al.,2020).
however, these approaches are of low ef-ﬁciency and can cause class imbalance, since themajority of ac pairs have no relation.
besides, dueto different annotation schemes, there are mainlytwo kinds of structures of argumentation graphs,tree (stab and gurevych, 2014; peldszus, 2014) andnon-tree (park and cardie, 2018).
brieﬂy, in treestructures, each ac has at most one outgoing ar,but there is no such restriction in non-tree structures(figure 1).
however, studies on these two kindsof structures are usually conducted separately.
todate, there is no universal method that can addressboth tree and non-tree structured argumentationwithout any corpus-speciﬁc constraints..towards these issues, we present a neuraltransition-based model for am, which can clas-sify the types of acs and identify ars simultane-ously.
our model predicts a sequence of actions toincrementally construct a directed argumentationgraph, often with o(n) parsing complexity.
thisallows our model to avoid inefﬁcient enumerationoperations and reduce the number of potential acpairs that need evaluating, thus alleviating the classimbalance problem and achieving speedup.
also,our transition-based model does not introduce anycorpus-speciﬁc structural constraints, and thus canhandle both tree and non-tree structured argumenta-tion, yielding promising generalization ability.
fur-thermore, we enhance our transition-based modelwith pre-trained bert (devlin et al., 2019), anduse lstm (hochreiter and schmidhuber, 1997) torepresent the parser state of our model..extensive experiments on two public datasetswith different structures show that our transition-based model outperforms previous methods, andachieves state-of-the-art results.
further analysisreveals that our model is of low parsing complexity.
and has a strong structure adaptive ability.
to thebest of our knowledge, we are the ﬁrst to investigatetransition-based methods for am..2 related work.
in computational am, there are mainly two typesof approaches to model argumentation structures,that is, tree and non-tree..2.1 tree structured am.
most previous works assume that the argumenta-tion graphs can be viewed as tree or forest struc-tures, which makes the problem computationallyeasier because many tree-based structural con-straints can be applied..under the theory of van eemeren et al.
(2004),palau and moens (2009) modeled argumentationin the legal text as tree structures and used hand-crafted context-free grammar to identify thesestructures.
presented by stab and gurevych (2014,2017), the tree structured persuasive essay (pe)dataset has been utilized in a number of studiesin am.
following this dataset, persing and ng(2016) and stab and gurevych (2017) leveraged theinteger linear programming (ilp) framework tojointly predict ars and ac types, in which severalstructural constraints are deﬁned to ensure the treestructures.
the arg-microtext (mt) dataset, cre-ated by peldszus (2014), is another tree structureddataset.
studies on this dataset usually apply de-coding mechanisms based on tree structures, suchas minimum spanning trees (mst) (peldszus andstede, 2015) and ilp (afantenos et al., 2018)..regarding neural network-based methods, egeret al.
(2017) studied am as a dependency pars-ing and a sequence labeling problem with multipleneural networks.
potash et al.
(2017) introducedthe sequence-to-sequence based pointer networks(vinyals et al., 2015) to am, and used the outputof encoder and decoder to identify ac types andthe presence of ars, respectively.
kuribayashiet al.
(2019) proposed an argumentation structureparsing model based on span representation, whichused elmo (peters et al., 2018) to obtain represen-tations for acs..2.2 non-tree structured am.
those studies described in section 2.1 are all basedupon the assumption that the argumentation formstree structures.
however, this assumption is some-what idealistic since argumentation structures in.
6355real-life scenarios may not be such well-formed.
hence, some studies have focused on non-treestructured am, and these studies typically use theconsumer debt collection practices (cdcp) (parkand cardie, 2018) dataset.
regarding this dataset,niculae et al.
(2017) presented a structured learningapproach based on factor graphs, which can alsohandle the tree structured pe dataset.
however,the factor graph needs to be speciﬁcally designedaccording to the types of argumentation structures.
galassi et al.
(2018) adopted residual networksfor am on the cdcp dataset.
recently, morioet al.
(2020) proposed a model devoted to non-treestructured am, with a task-speciﬁc parameteriza-tion module to encode acs and a biafﬁne attentionmodule to capture ars..to the best of our knowledge, until now thereis no universal method that can address both treeand non-tree structured argumentation without anycorpus-speciﬁc design.
thus, in this work, weﬁll this gap by proposing a neural transition-basedmodel that can identify both tree and non-tree argu-mentation structures without introducing any priorstructural assumptions..2.3 transition-based methods.
transition-based methods are commonly used independency parsing (chen and manning, 2014;g´omez-rodr´ıguez et al., 2018), and has alsobeen successfully applied to other nlp tasks withpromising performance, such as discourse parsing(yu et al., 2018), information extraction (zhanget al., 2019), word segmentation (zhang et al.,2016) and mention recognition (wang et al., 2018)..3 task deﬁnition.
following previous works (potash et al., 2017;kuribayashi et al., 2019), we assume subtask 1)acs has been completed, i.e., the spans of acsare given.
then, we aim at jointly classifying actypes (actc) and determining the presence of ars(ari).
the reason why we do not jointly conductar type classiﬁcation (artc) is that performingartc along with actc and ari jointly will hurtthe overall performance.
more details on this issuewill be discussed in section 6.4..formally, we assume a piece of argumentationrelated paragraph p = (w1, w2, .
.
.
, wm) consist-ing of m tokens and a set x = (x1, x2, .
.
.
, xn)consisting of n ac spans are given.
each ac spanxi is a tuple containing the beginning token index.
figure 2: the architecture of our model.
ars are iden-tiﬁed by the action prediction.
the types of acs aredetermined by the ac type classiﬁer..bi and the ending token index ei of this ac, i.e.,xi = (bi, ei).
the goal is to classify the typesof acs and identify the ars, and ﬁnally obtain adirected argumentation graph with acs and arsrepresenting nodes and edges, respectively..4 our approach.
we present a neural transition-based model for am,which can jointly learn actc and ari.
our modelgenerates a sequence of actions in terms of theparser state to incrementally build an argumenta-tion graph.
we utilize bert and lstm to rep-resent our parser state, which contains a stack σto store processed acs, a buffer β to store unpro-cessed acs, a delay set d to record acs that needto be removed subsequently, and an action list αto record historical actions.
then, the learningproblem is framed as: given the parser state ofcurrent step t: (σt, βt, dt, αt), predict an actionto determine the parser state of the next step, andsimultaneously identify ars according to the pre-dicted action.
figure 2 shows the architecture ofour model.
in the following, we ﬁrst introduceour transition system, then describe the parser staterepresentation..6356(𝑏1,𝑒1)(𝑏1,𝑒1)𝒆𝑑with preconditionaction  prediction𝝈0𝝈1stack···𝜷0𝜷1···𝜶𝑡−1𝜶𝑡−2··················𝐡1𝐡2𝐡3𝐡4𝐡𝑚···𝑤1𝑤2𝑤3𝑤4𝑤𝑚bert𝐜1𝐜2𝐜𝑛···𝐂bufferactionlist(𝑏1,𝑒1)𝑋𝒓𝑡ac typeclassifieractionshdedderaradla.
change of state(σ0|σ1|σ, β0|β, d, r) ⇒ (β0|σ0|σ1|σ, β, d, r)(σ0|σ1|σ, β0|β, d, r) ⇒ (σ0|σ, β0|β, d − {σ1}, r)(σ0|σ1|σ, |β, d, r) ⇒ (σ0|σ, |β, d, r)(σ0|σ1|σ, |β, d, r) ⇒ (σ1|σ, |β, d, r ∪ {σ0 → σ1})(σ0|σ1|σ, β0|β, d, r) ⇒ (β0|σ0|σ1|σ, β, d ∪ {σ0}, r ∪ {σ0 → σ1})(σ0|σ1|σ, β0|β, d, r) ⇒ (σ0|σ, β0|β, d, r ∪ {σ0 ← σ1}).
preconditionσ1 /∈ d ∧ β (cid:54)= [ ]σ1 ∈ d ∧ β (cid:54)= [ ]β = [ ]β = [ ]β (cid:54)= [ ].
table 1: actions designed in our transition system.
r denotes the set of ars extracted so far.
for simplicity, weomit the superscript t and use the subscript i ∈ {0, 1, ...} to denote the element index in stack and buffer.
forexample, σ0|σ1|σ denotes the top two items in stack.
an action can be selected only if its precondition is satisﬁed..stackσ.bufferβ.delay-setd∅.
actionα.
[ ].
[1].
[2,1].
[3,2,1].
[3,1].
[4,3,1].
[4,1].
[5,4,1].
[4,1].
[4].
[1,2,3,4,5].
[2,3,4,5].
[3,4,5].
[4,5].
[4,5].
[5].
[5].
[ ].
[ ].
[ ].
∅.
∅.
{2}.
{2}.
{2}.
{2}.
{2}.
{2}.
{2, 3}.
arr.-.
-2 → 1.
3 ← 2.
3 → 1.
-5 → 4.
-.
-.
-.
sh.
shradlaraddedsh.
ra.
de.
-.
table 2: transition sequence for the text in figure 1.for simplicity, we use indices to denote acs..4.1 transition system.
our transition system contains six types of actions.
different actions will change the state in differentways, which are also summarized in table 1:• shift (sh): when βt is not empty and σ1 isnot in dt, pop β0 from βt and move it to the topof σt..• delete-delay (ded).
when βt is not emptyand σ1 is in dt, remove σ1 from σt and dt, andkeep βt unchanged..• delete (de).
when βt is empty, remove σ1.
from σt and keep βt and dt unchanged..• right-arc (ra).
when βt is empty, removeσ0 from σt and assign an ar from σ0 to σ1.
• right-arc-delay (rad).
when βt is notempty, pop β0 from βt and move it to the top ofσt.
then assign an ar from σ0 to σ1 and addσ0 into dt for delayed deletion.
this strategycan help extract more ars related to σ0..• left-arc (la).
remove σ1 from σt and assign.
an ar from σ1 to σ0.
table 2 illustrates the golden transition sequence.
of the text in figure 1. this example text containsﬁve acs and four ars.
at the initial state, allacs are in buffer.
then, a series of actions changethe parser state according to table 1, and extractars simultaneously.
this procedure stops whenmeeting the terminal state, that is, buffer is emptyand stack only contains one element..4.2 state representation.
we employ bert to obtain the representation ofeach ac and use lstm to encode the long-termdependencies of stack, buffer and action list..representation of acs.
we feed the input para-graph p = (w1, w2, .
.
.
, wm) into bert to getthe contextual representation matrix h ∈ rm×db,where db is the vector dimension of the last layerof bert.
in this way, paragraph p can be repre-sented as h = (h1, h2, .
.
.
, hm), where hi is thecontextual representation of the i-th token of p..then, we use the ac spans set x =(x1, x2, .
.
.
, xn) to produce a contextual represen-tation of each ac from h by mean pooling over therepresentations of words in each ac span.
specif-ically, for the i-th ac with span xi = (bi, ei), thecontextual representation of this ac could be ob-tained by:.
ui =.
1ei − bi + 1.ei(cid:88).
j=bi.
hj.
(1).
where ui ∈ rdb.
in addition, following previ-ous works (potash et al., 2017; kuribayashi et al.,2019), we also combine some extra features withui to represent acs, including the bag-of-words(bow) vector, position and paragraph type embed-ding of each ac1.
we denote these features of thei-th ac as φi.
then, the i-th ac is represented by.
1details of these features are described in appendix a..6357the concatenation of ui and φi:.
ci = [ui; φi].
(2).
hence, the acs in paragraph p can be representedas c = (c1, c2, .
.
.
, cn)..representation of parser state.
our transition-based model utilizes the parser state to predict asequence of actions.
at each step t, we denoteour parser state as (σt, βt, dt, αt).
σt and βt arestack and buffer, which store the representations ofprocessed and unprocessed acs, respectively.
dtis the delay set that records acs that need to beremoved from stack subsequently.
αt is the actionlist that stores the actions generated so far.
at thebeginning, all acs are in the buffer, i.e., the initialparser state is ([ ], [c1, c2, .
.
.
, cn], ∅, [ ]).
then, aseries of predicted actions will iteratively changethe parser state..speciﬁcally, at step t, we have σt =(σ0, σ1, .
.
.
), βt = (β0, β1, .
.
.
), where σi and βiindicate the representations of acs in the stack andthe buffer at the current state.
in addition, we alsohave αt = (.
.
.
, αt−2, αt−1) where αi denotesthe distributed representation of the i-th action ob-tained by a looking-up table ea.
in order to capturethe context information in the stack σt, we feed itinto a bidirectional lstm:.
through another looking-up table ed.
thus, theparser state representation rt can be obtained by:.
rt = [s0; s1; b0; at−1; ed].
(6).
where s0 and s1 denote the ﬁrst and second ele-ments of st, b0 is the ﬁrst element of the bt, andat−1 indicates the last action representation of at..4.3 action prediction.
to predict the current action at step t, we ﬁrst applya multi-layer perceptron (mlp) with relu acti-vation to squeeze the state representation rt to alower-dimensional vector zt, and then compute theaction probability by a softmax output layer:.
p(αt|zt) =.
zt = mlpa(rt)exp(w(cid:62)α(cid:48)∈a(s) exp(w(cid:62).
(cid:80).
α zt + bα).
α(cid:48)zt + bα(cid:48)).
(7).
(8).
where wα denotes a learnable parameter matrix,bα is the bias term, αt is the predicted action forstep t. a(s) represents the set of valid candidateactions that may be taken according to the precon-ditions.
for efﬁcient decoding, we greedily take thecandidate action with the highest probability.
withthe predicted action sequence, we could identifyars according to table 1. note that, the univocalsupervision over actions for one input paragraph isbuilt based on the gold labels of ars..st = [s0, s1, .
.
.].
= bilstms([σ0, σ1, .
.
.]).
4.4 training.
where st ∈ r|σt|×2dl is the output of lstm fromboth directions, |σt| is the size of stack, and dl isthe hidden size of lstm.
similarly, we can obtainthe contextual representation of βt by:.
we jointly train an ac type classiﬁer over the acrepresentations: p(yi|c) = sof tmax(mlpc(ci)),where yi is the predicted type for the i-th ac.
fi-nally, combining this task with action prediction,the training objective of our model can be obtained:.
bt = [b0, b1, .
.
.].
= bilstmb([β0, β1, .
.
.]).
j (θ) =.
logp(αt|zt) +.
logp(yi|c).
(3).
(4).
where bt ∈ r|βt|×2dl, |βt| is the size of buffer.
be-sides, in order to incorporate the historical actioninformation into our model, we apply a unidirec-tional lstm to process the action list:.
(cid:88).
t.+.
λ2.
||θ||2.
(cid:88).
i.
(9).
where λ is the coefﬁcient of l2-norm regulariza-tion, and θ denotes all the parameters in this model..at = [.
.
.
, at−2, at−1].
= lstma([.
.
.
, αt−2, αt−1]).
(5).
5 experimental setup.
5.1 dataset.
where at ∈ r|αt|×dl, |αt| is the size of action list.
furthermore, since the relative distance betweenthe pair (σ0, σ1) is a strong feature for determiningtheir relations, we represent it as an embedding ed.
we conduct experiments on two datasets: persua-sive essays (pe) (stab and gurevych, 2017) andconsumer debt collection practices (cdcp) (nic-ulae et al., 2017; park and cardie, 2018)..6358the pe dataset contains 402 essays (1,833 para-graphs), in which 80 essays (369 paragraphs) areheld out for testing.
there are three types of acsin this dataset: major-claim, claim, and premise.
also, each ac in pe dataset has at most one out-going ar.
that is, the argumentation graph of oneparagraph can be either directed trees or forests.
we extend each ac by including its argumentativemarker in the same manner as kuribayashi et al.
(2019)..the cdcp dataset consists of 731 paragraphs,and 150 of them are reserved for testing.
it providesﬁve types of acs (propositions): reference, fact,testimony, value, and policy.
unlike pe dataset,each ac in cdcp dataset can have two or moreoutgoing ars, thus forming non-tree structures..5.2.implementation details.
for pe dataset, we randomly choose 10% of thetraining set as the validation set, which is consis-tent with the work of kuribayashi et al.
(2019).
forcdcp dataset, we randomly choose 15% of thetraining set for validation.
following potash et al.
(2017), for actc, we employ f1 score for eachac type and their macro averaged score to measurethe performance.
similarly, for ari, we present f1scores for the presence/absence of links betweenacs and their macro averaged score.
all experi-ments are performed 5 times with different randomseeds, and the scores are averaged.
we ﬁnetune uncased bertbase.
2 in our model.
adamw optimizer (loshchilov and hutter, 2019)is adopted for parameter optimization, and the ini-tial learning rates for the bert layer and otherlayers are set to 1e-5 and 1e-3, respectively.
alllstms are 1 layer with the hidden size of 256,and the hidden size of mlp is 512. besides, thedropout rate (srivastava et al., 2014) is set to 0.5,and the batch size is set to 32. all parametersof our model are unﬁxed and can be learned dur-ing training.
we train the model 50 epochs withearly stopping strategy, and choose model parame-ters with the best performance (average of macrof1 scores of actc and ari) on the validationset.
our model is implemented in pytorch (paszkeet al., 2019) on a nvidia tesla v100 gpu..5.3 baselines.
in order to evaluate our proposed bert-transmodel, we compare it with several baselines..2https://github.com/huggingface/.
transformers.
for pe dataset, the following baselines are com-.
pared:joint-ilp (stab and gurevych, 2017) jointly op-timizes ac types and ars by integer linear pro-gramming (ilp).
st-svm-full is structured svm with full factorgraph, which performs best on pe dataset in thework of niculae et al.
(2017).
joint-pn (potash et al., 2017) applies pointer net-work with attention mechanism to am, which canjointly address both actc and ari.
span-lstm (kuribayashi et al., 2019) employslstm-minus-based span representation with pre-trained elmo embedding for am, which is thecurrent state-of-the-art method on pe dataset..for cdcp dataset, we compare our model with.
the following baselines:deep-res-lg (galassi et al., 2018) applies resid-ual network model with link-guided training proce-dure, to perform actc and ari.
st-svm-strict is structured svm with strict factorgraph, which performs best on cdcp dataset in thework of (niculae et al., 2017).
tsp-plba (morio et al., 2020) uses task-speciﬁcparameterization to encode acs and biafﬁne atten-tion to capture ars with elmo based features,which is the current state-of-the-art method oncdcp dataset..furthermore, in order to show the effectivenessof our proposed transition system, we implementedtwo additional baselines:span-lstm-trans incorporates the span repre-sentation method used in span-lstm and our tran-sition system on pe dataset.
for a fair comparison,features and elmo used to represent acs are con-sistent with that of span-lstm.
elmo-trans replaces bert in our proposedmodel with elmo on cdcp dataset for a fair com-parison with tsp-plba..6 results and analysis.
6.1 main results.
the overall performance of our proposed modeland the baselines are shown in table 3 and table 4.our model achieves the best performance on bothdatasets.
on pe dataset, our model outperforms thecurrent sota model span-lstm by at least 1.1%and 1.4% in macro f1 score over actc and ari,respectively.
on cdcp dataset, compared withtsp-plba, our model obtains at least 3.6% higher.
6359method.
joint-ilpst-svm-fulljoint-pnspan-lstmspan-lstm-transbert-trans (ours).
actc.
macro mc89.182.678.277.689.484.9-87.393.887.588.493.2.claim premise macro75.190.368.2-90.264.576.792.173.281.1--82.092.276.482.593.178.8.arirel58.560.160.8-69.870.6.no-rel91.8-92.5-94.194.3.table 3: comparison results with baselines on pe dataset (%).
the best scores are in bold..actc.
method.
deep-res-lgst-svm-stricttsp-plbaelmo-transbert-trans (ours).
macro value72.265.376.473.2-78.980.078.983.282.5.policy74.476.8-82.386.3.testimony72.971.5-80.684.9.fact40.341.3-51.558.3.ref66.7100.0-100.0100.0.macro---67.167.8.arirel29.326.734.035.637.3.no-rel---98.698.3.table 4: comparison results with baselines on cdcp dataset (%).
the best scores are in bold..method.
bert-trans (ours)w/o lstmw/o bufferw/o actionw/o distancew/o bow.
actc.
ari.
macro ∇ macro ∇88.4--2.087.9-1.888.1-1.687.8-0.788.1-1.985.9.
82.580.580.780.981.880.6.
--0.5-0.3-0.6-0.3-2.5.table 5: the results of ablation experiments on pedataset (%).
the best scores are in bold..macro f1 score over actc, and achieves about3.3% higher relation f1 over ari..we also show the results where our bert-basedac representation is replaced by the elmo-basedmethod, that is, span-lstm-trans on pe datasetand elmo-trans on cdcp dataset.
we foundthat, without employing pre-trained bert, span-lstm-trans and elmo-trans still outperformspan-lstm and tsp-plba over ari, respec-tively, which demonstrates the effectiveness of ourproposed transition system.
it can also be observedthat our bert-based ac representation methodcan further improve the model performance..some of the baselines improve overall perfor-mance by imposing structural constraints when pre-dicting or decoding.
for example, joint-pn onlypredicts one outgoing ar for each ac to partiallyenforce the predicted argumentation graphs as treestructures.
similarly, to ensure tree structures,span-lstm applies mst algorithm based on theprobabilities calculated by the model.
however,these two methods can only deal with tree struc-tured argumentation.
the method proposed by nic-.
ulae et al.
(2017), which is based on factor graph,can handle both tree and no-tree structured argu-mentative text (st-svm-full and st-svm-strict),but the factor graph need to be speciﬁcally designedfor datasets of different structures.
differently, ourproposed model can handle datasets of both treeand non-tree structures without introducing anycorpus-speciﬁc structural constraints and also out-performs all the structured baselines..6.2 ablation study.
we conduct ablation experiments on the pe datasetto further investigate the impacts of each compo-nent in bert-trans.
the results are shown in table5. it can be observed that applying lstm to en-code buffer, stack, and action list contributes about2.0% macro f1 score of ari, showing the neces-sity of capturing non-local dependencies in parserstate.
also, incorporating buffer into parser statecan improve the macro f1 score of ari by about1.8%, for buffer can provide crucial informationabout subsequent acs to be processed.
besides, themacro f1 score of ari drops heavily without ac-tion list (-1.6%), indicating that the historical actioninformation has a signiﬁcant impact on predictingthe next action.
without the distance informationbetween the top two acs of the stack, the macrof1 score of ari decreases by 0.7%.
the modelcomponents described above mainly affect ari bymodifying the parsing procedure, but have littleimpact on actc.
however, bow feature has a sig-niﬁcant inﬂuence on both two tasks, and removingit causes 2.5% and 1.9% decreases in macro f1score of actc and ari, respectively..63606.3 parsing complexity.
(a) pe dataset.
(b) cdcp dataset.
figure 3: the number of actions relative to the numberof acs of each paragraph..most previous models parse argumentationgraphs by exhaustively enumerating all possiblepairs of acs, that is, all acs are connected bycartesian products, which lead to o(n2) parsingcomplexity.
differently, our transition-based modelcan incrementally parse an argumentation graph bypredicting a sequence of actions, often with linearparsing complexity.
concretely, given a paragraphwith n acs, our system can parse it with o(n)actions..parsing complexity of our transition system canbe determined by the number of actions performedwith respect to the number of acs in a paragraph.
speciﬁcally, we measure the length of the actionsequence predicted by our model for every para-graph from the test sets of pe dataset and cdcpdataset and depict the relation between them andthe number of acs.
as shown in figure 3, the num-ber of predicted actions is linearly related to thenumber of acs in both two datasets, proving thatour system can construct an argumentation graphwith o(n) complexity.
in addition, we also com-pared our model with the current state-of-the-artmodel on pe dataset, i.e., span-lstm, in terms oftraining time, and our model is around two timesfaster..6.4.joint learning analysis.
following kuribayashi et al.
(2019), we also try toadd the task of ar type classiﬁcation (artc) toour model for joint learning on pe dataset.
how-ever, as shown in table 6, jointly learning artctogether with actc and ari degrades the over-all performance, while learning artc separatelyactually yields better performance.
such an obser-vation is consistent with the joint learning results.
method.
joint tasks.
bert-trans(ours).
span-lstm.
allactc+ariartcallactc+ariartc.
macro f1actc ari artc78.481.886.882.588.4-81.0--79.080.785.7-81.187.379.6--.
table 6: joint learning results on pe dataset (%).
allindicates joint learning of all three subtasks..(a) pe dataset..(b) cdcp dataset..figure 4: confusion matrices of structure type on testset.
vertical direction indicates predicted structure type,horizontal direction indicates gold structure type..of span-lstm in kuribayashi et al.
(2019).
thereason may be that the class labels are usually veryunbalanced for artc (around 1:10 in pe datasetand 1:25 in cdcp dataset), such that the high un-certainty can seriously affect the overall learning.
thus, we mainly focus on joint learning of actcand ari.
we also argue that learning artc in-dividually is better than jointly learning it withother subtasks.
besides, our model outperformsspan-lstm over actc and ari even when jointlearning all three subtasks..6.5 structure adaptive.
to validate the structure adaptive ability of ourmodel on both tree and non-tree structures, we an-alyze the structure type of the predicted argumen-tation graphs on the test set of both pe and cdcpdatasets in figure 4. it can be seen that for non-treestructured cdcp dataset, even though there arefew non-tree structured paragraphs in the test set ofcdcp (only 16%), our model is still able to identify29.2% of them.
this is an acceptable performanceconsidering the poor results of ari on the cdcpdataset due to the complex non-tree structures.
fortree structured pe dataset, our model predicts allthe paragraphs as tree structures, showing a strongstructure adaptive ability.
in contrast, most previ-ous models like joint-pn and span-lstm can onlypredict tree structures..6361number of acsnumber of actionsnumber of acsnumber of actions359000non-treetreetreenon-tree1201767non-treetreetreenon-tree7 conclusion.
in this paper, we propose a neural transition-basedmodel for argumentation mining, which can incre-mentally construct an argumentation graph by pre-dicting a sequence of actions.
our proposed modelcan handle both tree and non-tree structures, andoften with linear parsing complexity.
the experi-mental results on two public datasets demonstratethe effectiveness of our model.
one potential draw-back of our model is the greedy decoding for actionprediction.
for future work, we plan to optimizethe decoding process by using methods like beamsearch to further boost the performance..acknowledgments.
this work was partially supported by nationalnatural science foundation of china (61632011,61876053, 62006062), guangdong provincecovid-19 pandemic control research fund-ing (2020kzdzx1224), shenzhen foundationalresearch funding (jcyj20180507183527919,jcyj20180507183608379), and the joint lab ofchina merchants securities and hitsz..references.
stergos d. afantenos, andreas peldszus, and manfredstede.
2018. comparing decoding mechanisms forparsing argumentative structures.
argument com-put., 9(3):177–192..lucas carstens and francesca toni.
2015. towardsin proceed-relation based argumentation mining.
ings of the 2nd workshop on argumentation mining,argmining@hlt-naacl 2015, june 4, 2015, den-ver, colorado, usa, pages 29–34.
the associationfor computational linguistics..tuhin chakrabarty, christopher hidey, smarandamuresan, kathy mckeown, and alyssa hwang.
2019. ampersand: argument mining for per-in proceedings of thesuasive online discussions.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing,emnlp-ijcnlp 2019, hong kong, china, novem-ber 3-7, 2019, pages 2933–2943.
association forcomputational linguistics..danqi chen and christopher d. manning.
2014. afast and accurate dependency parser using neural net-in proceedings of the 2014 conference onworks.
empirical methods in natural language processing,emnlp 2014, october 25-29, 2014, doha, qatar, ameeting of sigdat, a special interest group of theacl, pages 740–750.
acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..steffen eger,.
and irynajohannes daxenberger,gurevych.
2017. neural end-to-end learning forin proceed-computational argumentation mining.
ings of the 55th annual meeting of the associationfor computational linguistics, acl 2017, vancou-ver, canada, july 30 - august 4, volume 1: long pa-pers, pages 11–22.
association for computationallinguistics..andrea galassi, marco lippi, and paolo torroni.
2018.argumentative link prediction using residual net-works and multi-objective learning.
in proceedingsof the 5th workshop on argument mining, argmin-ing@emnlp 2018, brussels, belgium, november1, 2018, pages 1–10.
association for computationallinguistics..carlos g´omez-rodr´ıguez, tianze shi, and lillian lee.
2018. global transition-based non-projective depen-in proceedings of the 56th annualdency parsing.
meeting of the association for computational lin-guistics, acl 2018, melbourne, australia, july 15-20, 2018, volume 1: long papers, pages 2664–2675.
association for computational linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural comput., 9(8):1735–1780..zixuan ke, winston carlile, nishant gurrapadi, andvincent ng.
2018. learning to give feedback: mod-eling attributes affecting argument persuasivenessin proceedings of the twenty-in student essays.
seventh international joint conference on artiﬁcialintelligence, ijcai 2018, july 13-19, 2018, stock-holm, sweden, pages 4130–4136.
ijcai.org..tatsuki kuribayashi, hiroki ouchi, naoya inoue, paulreisert, toshinori miyoshi, jun suzuki, and kentaroinui.
2019. an empirical study of span represen-in pro-tations in argumentation structure parsing.
ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 4691–4698.
association for computa-tional linguistics..john lawrence and chris reed.
2019. argument min-ing: a survey.
comput.
linguistics, 45(4):765–818..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..6362gaku morio, hiroaki ozaki, terufumi morishita, yutakoreeda, and kohsuke yanai.
2020. towards bet-ter non-tree argument mining: proposition-level bi-afﬁne parsing with task-speciﬁc parameterization.
in proceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 3259–3266.
associa-tion for computational linguistics..vlad niculae, joonsuk park, and claire cardie.
2017.argument mining with structured svms and rnns.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics, acl 2017,vancouver, canada, july 30 - august 4, volume 1:long papers, pages 985–995.
association for com-putational linguistics..raquel mochales palau and marie-francine moens.
2009. argumentation mining: the detection, classi-ﬁcation and structure of arguments in text.
in the12th international conference on artiﬁcial intelli-gence and law, proceedings of the conference, june8-12, 2009, barcelona, spain, pages 98–107.
acm..joonsuk park and claire cardie.
2018. a corpus oferulemaking user comments for measuring evalua-bility of arguments.
in proceedings of the eleventhinternational conference on language resourcesand evaluation, lrec 2018, miyazaki, japan, may7-12, 2018. european language resources associa-tion (elra)..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas k¨opf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,py-junjie bai, and soumith chintala.
2019.torch: an imperative style, high-performance deepin advances in neural informa-learning library.
tion processing systems 32: annual conferenceon neural information processing systems 2019,neurips 2019, december 8-14, 2019, vancouver,bc, canada, pages 8024–8035..andreas peldszus.
2014..towards segment-basedrecognition of argumentation structure in short texts.
in proceedings of the first workshop on argumentmining, hosted by the 52nd annual meeting of theassociation for computational linguistics, argmin-ing@acl 2014, june 26, 2014, baltimore, mary-land, usa, pages 88–97.
the association for com-puter linguistics..andreas peldszus and manfred stede.
2015. joint pre-diction in mst-style discourse parsing for argumenta-tion mining.
in proceedings of the 2015 conferenceon empirical methods in natural language process-ing, emnlp 2015, lisbon, portugal, september 17-21, 2015, pages 938–948.
the association for com-putational linguistics..isaac persing and vincent ng.
2016. end-to-end argu-mentation mining in student essays.
in naacl hlt.
2016, the 2016 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, san diegocalifornia, usa, june 12-17, 2016, pages 1384–1394. the association for computational linguis-tics..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2018, new or-leans, louisiana, usa, june 1-6, 2018, volume 1(long papers), pages 2227–2237.
association forcomputational linguistics..peter potash, alexey romanov, and anna rumshisky.
2017. here’s my point: joint pointer architecture forargument mining.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, emnlp 2017, copenhagen, denmark,september 9-11, 2017, pages 1364–1373.
associa-tion for computational linguistics..nitish srivastava, geoffrey e. hinton, alexkrizhevsky, ilya sutskever, and ruslan salakhutdi-nov. 2014. dropout: a simple way to prevent neuralj. mach.
learn.
res.,networks from overﬁtting.
15(1):1929–1958..christian stab and iryna gurevych.
2014. annotatingargument components and relations in persuasive es-in coling 2014, 25th international con-says.
ference on computational linguistics, proceedingsof the conference: technical papers, august 23-29,2014, dublin, ireland, pages 1501–1510.
acl..christian stab and iryna gurevych.
2017. parsing argu-mentation structures in persuasive essays.
comput.
linguistics, 43(3):619–659..christian stab, tristan miller, benjamin schiller,pranav rai, and iryna gurevych.
2018. cross-topicargument mining from heterogeneous sources.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, brussels,belgium, october 31 - november 4, 2018, pages3664–3674.
association for computational linguis-tics..frans van eemeren, rob grootendorst, and frans hvan eemeren.
2004. a systematic theory of argu-mentation: the pragma-dialectical approach.
cam-bridge university press..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems 28: annual con-ference on neural information processing systems2015, december 7-12, 2015, montreal, quebec,canada, pages 2692–2700..6363• paragraph type embedding of each ac: thetype (intro, body, conclusion) of the paragraphin which the ac is present, which is also rep-resented as an embedding vector through an-other looking-up table et..henning wachsmuth, khalid al khatib, and bennostein.
2016. using argument mining to assess theargumentation quality of essays.
in coling 2016,26th international conference on computationallinguistics, proceedings of the conference: techni-cal papers, december 11-16, 2016, osaka, japan,pages 1680–1691.
acl..vern r. walker, dina foerster, julia monica ponce,and matthew rosen.
2018. evidence types, credi-bility factors, and patterns or soft rules for weighingconﬂicting evidence: argument mining in the con-text of legal rules governing evidence assessment.
in proceedings of the 5th workshop on argumentmining, argmining@emnlp 2018, brussels, bel-gium, november 1, 2018, pages 68–78.
associationfor computational linguistics..bailin wang, wei lu, yu wang, and hongxia jin.
2018.a neural transition-based model for nested mentionrecognition.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, brussels, belgium, october 31 - november 4,2018, pages 1011–1017.
association for computa-tional linguistics..nan yu, meishan zhang, and guohong fu.
2018.transition-based neural rst parsing with implicitin proceedings of the 27th inter-syntax features.
national conference on computational linguistics,coling 2018, santa fe, new mexico, usa, august20-26, 2018, pages 559–570.
association for com-putational linguistics..junchi zhang, yanxia qin, yue zhang, mengchi liu,and donghong ji.
2019. extracting entities andevents as a single task using a transition-based neuralmodel.
in proceedings of the twenty-eighth interna-tional joint conference on artiﬁcial intelligence, ij-cai 2019, macao, china, august 10-16, 2019, pages5422–5428.
ijcai.org..meishan zhang, yue zhang, and guohong fu.
2016.transition-based neural word segmentation.
in pro-ceedings of the 54th annual meeting of the associ-ation for computational linguistics, acl 2016, au-gust 7-12, 2016, berlin, germany, volume 1: longpapers.
the association for computer linguistics..appendices.
a extra features.
following the work of potash et al.
(2017) andkuribayashi et al.
(2019), we further incorporatesome extra features to represent acs, including:.
• the bag-of-words (bow) vector: one-hot vec-.
tor, which is later fed into a mlp layer..• position embedding of each ac: the posi-tion of an ac in the paragraph, which is rep-resented as an embedding vector through alooking-up table ep..6364