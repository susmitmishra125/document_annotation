personalized transformer for explainable recommendation.
lei li1 yongfeng zhang2 li chen11hong kong baptist university, hong kong, china2rutgers university, new brunswick, usa1{csleili,lichen}@comp.hkbu.edu.hk2yongfeng.zhang@rutgers.edu.
abstract.
personalization of natural language generationplays a vital role in a large spectrum of tasks,such as explainable recommendation, reviewsummarization and dialog systems.
in thesetasks, user and item ids are important identi-ﬁers for personalization.
transformer, whichis demonstrated with strong language model-ing capability, however, is not personalizedand fails to make use of the user and itemids since the id tokens are not even in thesame semantic space as the words.
to addressthis problem, we present a personalized trans-former for explainable recommendation (pe-ter1), on which we design a simple and ef-fective learning objective that utilizes the idsto predict the words in the target explanation,so as to endow the ids with linguistic mean-ings and to achieve personalized transformer.
besides generating explanations, peter canalso make recommendations, which makes it auniﬁed model for the whole recommendation-explanation pipeline.
extensive experimentsshow that our small unpretrained model outper-forms ﬁne-tuned bert on the generation task,in terms of both effectiveness and efﬁciency,which highlights the importance and the niceutility of our design..1.introduction.
recent years have witnessed the successful appli-cation of natural language generation.
many of theapplications in fact require certain degree of per-sonalization, such as explainable recommendation(zhang et al., 2014; li et al., 2020c; zhang andchen, 2020), review generation (dong et al., 2017),review summarization (li et al., 2019), and conver-sational systems (zhang et al., 2018; chen et al.,2020).
in these tasks, user and item ids that distin-guish one user/item from the others are crucial to.
1https://github.com/lileipisces/peter.
personalization.
for example, in recommender sys-tems, different users may care about different itemfeatures (e.g., style vs. quality), and different itemsmay have different characteristics (e.g., fashionablevs. comfortable).
the goal of explainable recom-mendation (zhang and chen, 2020) is to provide anexplanation to a user for a recommended item, soas to justify how the recommendation might matchhis/her interests.
that is, given a pair of user id anditem id, the system needs to generate an explana-tion, such as “the style of the jacket is fashionable”(see the last column of table 4 for more examples).
transformer (vaswani et al., 2017), whosestrong language modeling ability has been demon-strated on a variety of tasks (radford et al., 2018;devlin et al., 2019; brown et al., 2020), however, isrelatively under-explored for personalized naturallanguage generation.
since ids and words are invery different semantic spaces, it would be prob-lematic to directly put them together for attentionlearning, because by doing so, the ids are treatedas words, but the ids appear far less frequentlythan the words.
for example, a paragraph of re-view (and thus hundreds of words) on e-commerceplatform only corresponds to a single pair of userid and item id.
as such, the ids may be regardedas out-of-vocabulary tokens, to which the modelis insensitive.
as shown in fig.
1(a), when gener-ating an explanation for a user-item pair, standardtransformer relies heavily on the special <bos>token instead of the user or the item.
this wouldresult in identical explanations over different user-item pairs (see usr score in table 2), deviatingfrom our personalization goal..to address this problem, we bridge ids andwords by designing an elegant task called contextprediction, which maps ids onto words to be gen-erated by the explanation task.
this in some wayresembles one’s drafting-polishing process, whereby predicting some words the context prediction.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4947–4957august1–6,2021.©2021associationforcomputationallinguistics4947(a) standard transformer model, where the user and theitem have no contribution to each generation step..(b) our peter model, where the user and item ids playsigniﬁcant roles in the generation steps..figure 1: attention visualization of two models when generating an explanation for the same user-item pair (seethe ﬁrst two columns).
they are both from the last attention layer, so the target sequences are offset by one positionfor better illustration.
the larger the attention weights, the lighter the cells..task does the job of drafting.
then, the explana-tion generation task polishes these words so as toform a readable sentence.
meanwhile, we demon-strate that conducting recommendation task on thesame model is also feasible, so we name it peter,which stands for personalized transformer for ex-plainable recommendation.
as we can see in fig.
1(b), when peter generates an explanation for thesame user-item pair, it can utilize the informationof both the user and the item, which illustrates theeffectiveness of our context prediction task..in addition, peter is ﬂexible to incorporateitem features that can help to guide its generation.
this can be very useful when, for instance, a userproactively asks the system to explain certain fea-ture(s) of a recommendation (li et al., 2020c), e.g.,price.
then, we would expect the model to gen-erate a targeted explanation, such as “great jacket,especially for the price”.
peter is a small un-pretrained transformer with only 2 layers, yet itoutperforms a ﬁne-tuned bert (ni et al., 2019)on most metrics by a large margin, and takes lesstime to train, as shown in our experiments.
thismanifests the superiority of our model..in summary, our key contributions are:.
• we propose peter that makes recommenda-tion and generates explanation simultaneouslybased on user and item ids for explainable rec-ommendation.
to the best of our knowledge,we are the ﬁrst to enable transformer withpersonalized natural language generation..• we evaluate the generated explanations onnot only text quality metrics (such as bleuand rouge), but also metrics that particu-larly focus on explainability from the angleof item features.
extensive experiments showthat our model can outperform state-of-the-artbaselines on large datasets..• our solution sheds light on a broader scopeof ﬁelds that also need personalization (e.g.,personalized conversational systems).
in ad-dition, it points out a way for transformer todeal with heterogeneous inputs, e.g., text andimages in multimodal artiﬁcial intelligence..2 related work.
explainable recommendation (zhang et al.,2014; zhang and chen, 2020) has been studiedfrom two major perspectives: human-computerinteraction and machine learning.
the former(gedikli et al., 2014; chen and wang, 2017; chenet al., 2019b) investigates how people perceive dif-ferent styles of explanations, while the latter pro-vides explanations by designing new explainablerecommendation algorithms, to which our work ismore related.
there exist various types of explana-tion styles, such as pre-deﬁned templates (zhanget al., 2014; li et al., 2020a), ranked sentences(chen et al., 2019d; li et al., 2021), image visu-alizations (chen et al., 2019c), knowledge graphpaths (ai et al., 2018; xian et al., 2019; fu et al.,2020; xian et al., 2020), reasoning rules (shi et al.,.
4948[user][item]<bos>thehotelislocatedintheheartofthecityandthecitycentreissource[user][item]thehotelislocatedintheheartofthecityandthecitycentreis<eos>target0.00.20.40.60.81.0[user][item]<bos>thepoolareaisniceandthegymisverywellequippedsource[rating][context]thepoolareaisniceandthegymisverywellequipped<eos>target0.00.20.40.60.81.02020; chen et al., 2021; zhu et al., 2021), etc.,among which, recently, generated natural languageexplanations (ni et al., 2019; li et al., 2020c) havereceived much attention, mainly owing to the ad-vancement of natural language generation technol-ogy and the availability of textual data on recom-mendation platforms such as e-commerce.
how-ever, previous works mostly rely on recurrent neu-ral networks (rnn), e.g., lstm (hochreiter andschmidhuber, 1997) and gru (cho et al., 2014),leaving the potentially more effective transformerunder-explored, which motivates this work..transformer (vaswani et al., 2017) was ﬁrstbrought to machine translation with the architec-ture of encoder-decoder.
later works (liu et al.,2018; devlin et al., 2019) show that it remainseffective, even when the encoder or the decoderis removed, reducing nearly half of the parame-ters.
under the paradigm of pre-training plus ﬁne-tuning, transformer’s effectiveness has been con-ﬁrmed on a wide range of tasks, including both nat-ural language understanding and generation (rad-ford et al., 2018; devlin et al., 2019; dong et al.,2019).
particularly, it is able to perform novel tasks,e.g., arithmetic, after scaling up both the modeland the training data (radford et al., 2019; brownet al., 2020).
however, it may not be friendly to re-searchers who do not possess large amounts of com-puting resources.
instead, our work explores smallunpretrained models, as they are computationallycheaper and more ﬂexible when being adapted tonew applications, e.g., personalized generation..personalized generation usually involves theids of users and items.
previous approaches typi-cally adopt multi-layer perceptron (mlp) to encodethe ids into a context vector, from which rnncan decode a word sequence.
this strategy can befound in many applications, such as review gener-ation (dong et al., 2017), tip generation (li et al.,2017) and explanation generation (li et al., 2020c).
however, it does not ﬁt transformer that relies en-tirely on self-attention.
probably because a propersolution to deal with heterogeneous inputs (i.e., idsand words) is yet to be found, previous works withtransformer for personalized generation replaceids with text segments, such as persona attributes(zheng et al., 2020), movie titles (zhou et al., 2020)and item features (ni et al., 2019), which are in thesame semantic space as the word sequence to begenerated.
in comparison, our solution is to designan effective task that can give the ids linguistic.
figure 2: our proposed model peter that containsthree tasks.
the input features are optional..meanings, thus connecting ids with words..3 problem formulation.
the goal of our explanation task is to generate anatural language sentence ˆeu,i for a pair of useru and item i to justify why i is recommended tou. meanwhile, our model peter can also makerecommendations by estimating a rating ˆru,i thatpredicts u’s preference towards i. at the testingstage, only user u and item i are used as inputs forproducing both explanation and recommendation.
when item features fu,i are available, our modelis ﬂexible to incorporate them by simply concate-nating them at the beginning of the explanation.
inthis case, the features are also needed in the testingstage.
in the following, we will discuss both cases..4 methodology.
in this section, we present the details of our modelpeter.
first, we show how to encode differenttypes of tokens in a sequence.
then, we brieﬂyreview transformer and introduce our revised at-tention masking matrix.
at last, we formulate thethree tasks, i.e., explanation generation, context pre-diction and recommendation, and integrate theminto a multi-task learning framework..4.1.input representation.
we ﬁrst introduce our way to encode heterogeneousinputs into vector representations.
as shown in fig.
2, the input to our model is a sequence, consisting.
4949transformer with 𝐿layerstransformer with 𝐿layerslinear layerƹ𝑟𝑢,𝑖ƹ𝑒1ƹ𝑒2<ෞ𝑒𝑜𝑠>𝑢𝑖<𝑏𝑜𝑠>ƹ𝑒2ƹ𝑒1𝑓2𝑓1𝐬𝐿,1𝐬𝐿,2𝐬𝐿,5𝐬𝐿,6𝐬𝐿,7𝐬𝐿,3𝐬𝐿,4useritemfeatures (opt.
)explanationratingpredictioncontext predictionexplanation generationmlpǁ𝑒1ǁ𝑒2<෦𝑒𝑜𝑠>4.2 transformer and attention masking.
to enable the three tasks, we show how to modifythe attention masking mechanism in transformer(vaswani et al., 2017).
transformer consists of lidentical layers, each of which is composed of twosub-layers: multi-head self-attention and position-wise feed-forward network.
the l-th layer encodesthe previous layer’s output sl−1 into sl, wherel ∈ [1, l].
in the multi-head self-attention sub-layer, the computation of each attention head isalso identical, and among the h heads of the l-thlayer, the h-th head al,h is computed as follows:.
al,h = softmax(.
+ m)vl,h.
ql,hk(cid:62)l,h√d.ql,h = sl−1wqvl,h = sl−1wvl,h(cid:40).
l,h, kl,h = sl−1wkl,h,.
(1).
m =.
0,allow to attend−∞, prevent from attending.
where sl−1 ∈ r|s|×d is the (l − 1)-th layer’s out-l,h ∈ rd× dput, wqh are projection ma-trices, d denotes the dimension of embeddings, andm ∈ r|s|×|s| is the attention masking matrix..l,h, wk.
l,h, wv.
each element in m controls whether a token inthe sequence can attend to another.
for example,in the unidirectional left-to-right language model(radford et al., 2018), the lower triangular part ofm is set to 0 and the remaining part −∞, so as toallow each token to attend to past tokens (includ-ing itself), but prevent it from attending to futuretokens.
we call it left-to-right masking.
as ourmodel is not limited to the left-to-right explanationgeneration task, we modify the masking mecha-nism to accommodate the other two tasks (i.e., con-text prediction and recommendation).
as shown infig.
3, the ﬁrst two tokens u and i in the sequencecan attend to each other, because both context pre-diction and recommendation tasks need them.
toecho our model, we name it peter masking..4.3 explanation and recommendation.
in the following, we perform the three tasks, af-ter obtaining the sequence’s ﬁnal representationsl = [sl,1, · · · , sl,|s|] from transformer.
thekey challenge lies in the personalization of expla-nation generation task, for which we design thecontext prediction task.
for both tasks, we apply alinear layer to the ﬁnal representation of each tokento map it into a |v|-sized vector.
as an example,.
figure 3: the attention masking used in our model thatwe call peter masking.
the orange box highlights itsdifference from the left-to-right masking..of user id u, item id i, features fu,i, and expla-nation eu,i.
the user and the item serve for thepurpose of personalization, i.e., aiming to makethe generated explanation reﬂect both the user’sinterests and the item’s attributes.
the features canguide the model to talk about certain topics.
for in-stance, a conversational recommender system mayexplain a recommendation’s specialty to the userwith the goal of knowing more about his/her pref-erence (chen et al., 2020).
since the features arenot always available, in our experiments we testboth cases (with and without them).
when they areavailable, the input sequence can be represented ass = [u, i, f1, · · · , f|fu,i|, e1, · · · , e|eu,i|], wheref1, · · · , f|fu,i| are the features and e1, · · · , e|eu,i|are the explanation’s word sequence.
|fu,i| denotesthe number of features and |eu,i| is the number ofwords in the explanation..clearly there are three types of tokens in thesequence s, i.e., users, items, and words (includ-ing features), for which we prepare three sets ofrandomly initialized token embeddings u, i andv respectively, besides the positional embeddingsp that encode the position of each token in thesequence.
notice that, we do not add users anditems to the vocabulary v, given that it costs moretime to predict a word out of the huge amount ofids (for example, millions of users and items ine-commerce).
after performing embedding look-up, we can obtain the sequence’s token represen-tation [u, i, f1, · · · , f|fu,i|, e1, · · · , e|eu,i|] and itspositional representation [p1, · · · , p|s|], where |s|is the length of the sequence.
the input repre-sentation of the sequence is the addition of thecorresponding token representation and positionalrepresentation, denoted as s0 = [s0,1, · · · , s0,|s|]..4950sourcetarget𝑢𝑖𝑓1𝑓2<𝑏𝑜𝑠>ƹ𝑒1ƹ𝑒2𝑢𝑖<𝑏𝑜𝑠>ƹ𝑒1ƹ𝑒2allow to attendprevent from attending𝑓1𝑓2after passing through this layer, sl,t becomes ct:.
ct = softmax(wvsl,t + bv).
(2).
where wv ∈ r|v|×d and bv ∈ r|v| are weight pa-rameters.
the vector ct represents the probabilitydistribution over the vocabulary v, from which aword e with probability ce.
t can be sampled..explanation generation: we adopt the neg-ative log-likelihood (nll) as the explanationtask’s loss function, and compute the mean of user-item pairs in the training set:.
le =.
1|t |.
(cid:88).
(u,i)∈t.
1|eu,i|.
|eu,i|(cid:88).
t=1.
− log cet.
2+|fu,i|+t.
(3)where t denotes the training set.
the probabilitycetis offset by 2 + |fu,i| positions because thetexplanation is placed at the end of the sequence,and |fu,i| = 0 when the features are unavailable.
at the testing stage, along with u, i, and fu,i (ifavailable), we feed the model a special begin-of-sequence token <bos>.
from its resulting proba-bility distribution c<bos>, the model can predict aword.
for simplicity, among the many decodingmethods, we opt for greedy decoding that samplesthe word with the largest probability.
then we canconcatenate this predicted word at the end of thesequence to form a new input sequence for gener-ating another word.
we do this repeatedly until themodel produces a special end-of-sequence token<eos>, or the generated explanation ˆeu,i reachesa pre-deﬁned length..context prediction: as discussed earlier, whenthere is only one task of explanation generation,transformer fails to make use of user id and itemid, resulting in identical sentences.
to address thisissue, we design this task to map the ids onto thewords in the explanation, so as to build a connec-tion between them.
since the ﬁrst two positions (uand i) of the sequence are allowed to attend to eachother, both of their ﬁnal representations absorb theinformation of the user and the item.
thus, wecan use either of them to perform this task.
here,we use the 2nd one for better illustration in fig.
2.again, we adopt nll as the loss function:.
lc =.
1|t |.
(cid:88).
(u,i)∈t.
1|eu,i|.
|eu,i|(cid:88).
t=1.
− log cet2.
(4).
where the difference from eq.
(3) is that all pre-dicted words are from the 2nd position, which iswhy they are not sequentially ordered (see fig.
2)..rating prediction: recommendation can beseen as a prediction problem (chen et al., 2021)where the goal is to predict a score ˆru,i based onthe ids of user u and item i. as both u and iin the sequence can attend to each other, their ﬁ-nal representations capture the interaction betweenthem.
next, we map the 1st representation sl,1 intoa scalar (because the 2nd one is used for contextprediction).
to this end, we employ multi-layer per-ceptron (mlp) with one hidden layer as follows:.
(5).
ˆru,i = wrσ(wrsl,1 + br) + brwhere wr ∈ rd×d, br ∈ rd, wr ∈ r1×d andbr ∈ r are weight parameters, and σ(·) is the sig-moid function.
therefore, it can be seen that it isfeasible to do both recommendation and explana-tion on transformer.
as recommendation is not thekey focus of this paper, we leave its improvementin the future work.
for this task, we use meansquare error (mse) as the loss function:.
lr =.
1|t |.
(cid:88).
(u,i)∈t.
(ru,i − ˆru,i)2.
(6).
where ru,i is the ground-truth rating..multi-task learning: at last, we integrate thethree tasks into a multi-task learning frameworkwhose objective function is deﬁned as:.
j = min.
(λele + λclc + λrlr).
(7).
θ.where θ denotes all the trainable parameters inthe model, and λe, λc and λr are regularizationweights that balance the learning of different tasks.
in this way, the model can be trained efﬁciently inan end-to-end manner..5 experimental setup.
5.1 datasets.
for experimentation, we adopt three publicly avail-able explainable recommendation datasets, andtheir data splits (li et al., 2020c).
during thesplitting process, each dataset is randomly dividedinto training, validation and testing sets with ra-tio 8:1:1 for 5 times, and the training set holds atleast one record for each user and each item.
thethree datasets are respectively from tripadvisor.
495127,147#users20,266#items1,293,247#records7,340#features47.64#records / user#records / item63.81#words / exp12.32* exp denotes explanation..yelp amazon7,5067,360441,7835,39958.8660.0214.14.tripadvisor9,7656,280320,0235,06932.7750.9613.01.table 1: statistics of the three datasets..(hotel), amazon (movies & tv) and yelp (restau-rant).
each record in the datasets is comprised of auser id, an item id, a rating, an explanation, and afeature.
the explanations are sentences extractedfrom user reviews.
each explanation contains atleast one item feature, e.g., bedroom, which ensuresthe explanation quality.
statistics of the datasetsare shown in table 1. we can see that yelp is muchlarger than the other two in terms of size, makingit closer to the real-world situation where there aremillions of users and items..5.2 evaluation metrics.
to evaluate the recommendation performance, weadopt two commonly used metrics: root meansquare error (rmse) and mean absolute error(mae).
as to explanation performance, we mea-sure the generated explanations from two main per-spectives: text quality and explainability.
for theformer, we adopt bleu (papineni et al., 2002) inmachine translation and rouge (lin, 2004) intext summarization, and report bleu-1 and bleu-4, and precision, recall and f1 of rouge-1 androuge-2.
though being widely used, blue androuge are not ﬂawless.
for example, it is dif-ﬁcult for them to detect the problem of identicalsentences generated by transformer.
these iden-tical sentences might not be used as explanations,because they are less likely to well explain the spe-cial property of different recommendations.
toquantitatively measure how severe the problem is,we adopt usr that computes the unique sentenceratio of generated sentences (li et al., 2020c)..text quality, however, is not equal to explain-bility.
in the case of explainable recommendation,users may value more an explanation that justi-ﬁes a recommendation’s advantages on certain fea-tures (li et al., 2020c; chen et al., 2019a).
to thisend, we adopt the other three metrics proposed by(li et al., 2020c): feature matching ratio (fmr),feature coverage ratio (fcr) and feature diver-sity (div).
fmr measures whether a generated.
explanation contains the feature in the ground-truth.
fcr is computed as the number of distinct featurescontained in all the generated explanations, dividedby the total number of features in the whole dataset.
div measures the intersection of features betweenany two generated explanations..for rmse, mae and div, the lower, the better,.
while it is opposite for the rest of metrics..5.3 compared methods.
we introduce baselines, ﬁrst for explanation andthen for recommendation.
for the former, we di-vide the baselines into two groups, depending onwhether the feature is used or not..the following models leverage only user anditem ids to generate explanations (without feature).
we denote our model without feature as peter..• transformer (vaswani et al., 2017) performsthe explanation generation task by treatinguser and item ids as words.
we also testedencoder-decoder transformer, where the en-coder encodes the ids for the decoder to de-code, but its results turned out to be the same,so we do not report it..• nrt (li et al., 2017) can predict a rating andgenerate a tip simultaneously based on userand item ids.
we take the explanations inthe datasets as tips.
moreover, we found thatthe model’s problem of generating identicalsentences (as reported in li et al., 2020c) iscaused by the l2 regularization in its originaldesign.
for fair comparison, we removed it..• att2seq (dong et al., 2017) is a review gener-ation approach and we take the explanationsas reviews.
this model has an attention mod-ule, but we found that it makes the generatedcontent unreadable in the task.
to be fair, weremoved it as well..when features are used, we denote our model aspeter+, and compare it with two recent models:.
• acmlm (ni et al., 2019) is a ﬁne-tunedbert (devlin et al., 2019), where an atten-tion layer is introduced to encode the featuresfrom both the user and the item.
by predict-ing masked tokens, this model can producediverse sentences..• nete (li et al., 2020c) is a tailored gru(cho et al., 2014) that incorporates a given.
4952explainability.
fmr↑ fcr↑ div↓ usr↑ b1↑.
text qualityr1-r↑.
r1-f↑.
r2-p↑.
r2-r↑ r2-f↑.
b4↑.
r1-p↑yelp19.180.4217.690.650.5818.730.73** 18.547.890.2433.982.69.
2.462.372.41.
0.010.06transformer 0.06nrt 0.070.120.110.13att2seq 0.070.12peter 0.08** 0.19** 1.54** 0.130.950.310.520.270.34.
0.951.48peter+ 0.86** 0.38** 1.08.acmlm 0.05nete 0.80.
7.3911.6610.2910.777.0119.3120.80** 3.43** 35.44** 26.12** 27.95** 10.65** 7.44** 7.94**.
12.5613.5513.2913.77** 2.02**6.8225.56.
0.921.221.141.38** 1.49**0.485.54.
10.2912.1111.2812.207.5422.51.
1.711.761.85.
1.091.331.31.
0.396.33.
0.448.93.
0.01transformer 0.100.003.26nrt 0.120.070.172.93att2seq 0.120.330.202.74peter 0.12** 0.211.75** 0.290.310.962.070.570.191.93peter+ 0.77** 0.31** 1.20** 0.46.acmlm 0.10nete 0.71.amazon19.680.5921.030.960.9520.791.17** 19.8111.650.2233.872.46.
9.7112.9312.5612.779.5218.7619.75** 3.06** 34.71** 23.99** 26.35** 9.04**.
14.1115.5615.3515.239.6924.81.
11.9413.5713.3113.8010.3921.43.
2.102.712.622.800.717.58.
1.552.051.99.
1.391.841.782.08** 2.20**0.814.776.23** 6.71**.
0.645.46.tripadvisor.
transformer 0.040.00nrt 0.060.090.15att2seq 0.06peter 0.07** 0.130.410.27peter+ 0.89** 0.35.acmlm 0.07nete 0.78.
10.000.004.270.080.174.322.95** 0.080.940.780.572.220.251.61.
12.790.7115.050.991.0315.2715.96** 1.11*0.023.4522.393.6624.32** 4.55** 37.48** 29.21** 30.49** 11.92** 8.98** 9.24**.
15.882.2215.402.292.4015.9216.48** 2.330.183.7210.2027.71.
16.5218.2218.9719.074.8635.68.
16.3814.3914.7216.093.8224.86.
2.342.012.092.090.167.66.
2.631.982.032.170.206.98.table 2: performance comparison of the generation methods in terms of explainability and text quality on threedatasets.
the methods are divided into two groups according to whether features are used or not.
b1 and b4 standfor bleu-1 and bleu-4.
r1-p, r1-r, r1-f, r2-p, r2-r and r2-f denote precision, recall and f1 of rouge-1and rouge-2.
bleu and rouge are percentage values (% symbol omitted for table clarity), while the othersare absolute values.
the best performing values are boldfaced, and the second best underlined.
** and * indicatethe statistical signiﬁcance over the second best baseline respectively for p < 0.01 and p < 0.05 via student’s t-test..feature into the decoding process to generatetemplate-like explanations.
it can also makerecommendations..timeacmlm 97.057.7peter+.
epochs325.time/epoch32.32.3.for recommendation, besides nrt and nete,.
we include another two traditional methods:.
table 3: efﬁciency comparison of two transformer-based models in terms of training minutes on the tri-padvisor dataset, tested on nvidia tesla p40..• pmf (mnih and salakhutdinov, 2007) isa standard probabilistic matrix factorizationmethod that characterizes users and items bylatent factors..• svd++ (koren, 2008) leverages a user’s in-teracted items to enhance the latent factors..5.4.implementation details.
we train each model on the training set, tune thehyper-parameters on the validation set, and reportthe performance on the testing set.
the results areaveraged on the 5 data splits.
we adopt the codes ofacmlm and nete, and implement all the othermethods.
for nrt, att2seq, nete and our pe-ter and peter+, we set the size of vocabularyto 20,000 by keeping the most frequent words.
wedo not apply this to transformer, otherwise users.
and items (regarded as words) may be ﬁltered out.
we set both the number of context words and thelength of explanations to 15, because the meanlength of explanations is approximately 13 (see ta-ble 1).
acmlm adopts sub-words, so we do notapply the above two steps to it.
we reuse the otherdefault settings of the baselines..for transformer, peter and peter+, we setthe embedding size d to 512 and the dimension offeed-forward network to 2,048, following (vaswaniet al., 2017), but the number of layers l and atten-tion heads h are both 2. for our models peterand peter+, we set the regularization weights λe,λc and λr to 1.0, 1.0 and 0.1, respectively.
weoptimize the model via stochastic gradient descent(robbins and monro, 1951), and apply gradientclipping (pascanu et al., 2013) with a threshold of.
4953ground-truthpeterpeter+ground-truthpeterpeter+.
top-15 context words.
<eos> the and a pool was with nice is very were to good in of<eos> the and a was pool with to nice good very were is of in.
<eos> the and a was were separate bathroom with shower large very had in is<eos> the and a was bathroom shower with large in separate were room very is.
explanationthe rooms are spacious and the bathroom has a large tubthe pool area is nice and the gym is very well equipped <eos>the rooms were clean and comfortable <eos>beautiful lobby and nice barthe bathroom was large and the shower was great <eos>the lobby was very nice and the rooms were very comfortable <eos>.
table 4: context words and explanations on two different cases as generated by our peter and peter+ on tri-padvisor dataset.
the boldfaced words in the ground-truth are the key features.
generated features are underlined..1.0. the batch size is set to 128, and the learningrate 1.0. at each epoch, we save the model if itachieves the lowest loss on the validation set, butwhen there is no improvement, we decrease thelearning rate by a factor of 0.25. when the latterhappens for 5 times, we stop training and load thesaved model for prediction..6 results and analysis.
6.1 quantitative analysis on explanations.
in table 2, we compare the performance of expla-nation generation methods in two groups.
we ﬁrstanalyze models that make use of item features (i.e.,acmlm, nete and peter+).
our peter+ con-sistently and signiﬁcantly outperforms acmlmand nete on the three datasets in terms of textquality (bleu and rouge).
this shows the ef-fectiveness of our model in generating high-qualitysentences.
notice that li et al.
(2020b) conducteda user survey and reported that nete’s explana-tions were perceived useful by most participants.
itsuggests that our model’s explanations with betterquality could also be very useful to real users..again, in terms of text quality, the performancegap between peter+ and acmlm (a ﬁne-tunedbert) is extremely large, because the latter’s gen-eration is achieved by predicting masked tokens,which is quite different from word-by-word gener-ation.
this may explain why acmlm producesdiverse sentences (high usr), which, however, isless meaningful when text quality cannot be guaran-teed.
furthermore, peter+ beats both acmlmand nete on the explainability metric fmr thatcares about whether a generated explanation men-tions the feature in the ground-truth.
this is quiteuseful in real-world applications when the systemis asked to explain a particular feature.
regardingthe other two explainability metrics fcr and div,peter+ is also very competitive.
acmlm gainsbetter performance on some cases, because at thetraining stage it is exposed to more features (fromboth the user and the item), which is unfair to bothpeter+ and nete..next, we discuss the results of the models that.
yelp.
r↓pmf1.091.01svd++1.01nrt1.01netepeter 1.01.m↓0.880.780.780.790.78.amazonr↓1.030.960.950.960.95.m↓0.810.720.700.730.71.tripadvisorm↓r↓0.700.870.610.800.790.610.600.790.630.81.table 5: recommendation performance comparison interms of rmse (r for short) and mae (denoted as m).
the best performing values are boldfaced..only leverage user and item ids for generation.
asit can be seen, transformer generates identical ex-planations on each dataset, resulting in nearly 0score on unique sentence ratio (usr).
owingto the context prediction task, our peter suc-cessfully addresses this issue, producing diverse(comparable usr) and high-quality (best bleu-4) sentences.
in particular, on the largest datasetyelp, it achieves the best performance on most ofthe metrics.
this again demonstrates the effective-ness of our model.
on amazon and tripadvisor,nrt and att2seq are very competitive, becausewe ﬁxed their generation issues (see section 5.3).
in addition, the two datasets are small and thus thetraining samples are limited, so our model may un-derﬁt, which is why it does not always reach thebest performance..besides explanation performance, we also inves-tigate the efﬁciency of different transformer-basedmodels.
on the same machine (nvidia tesla p40)and dataset (tripadvisor), we compare the train-ing minutes of acmlm and our peter+ in table3. compared with acmlm, our model takes lesstime to train (2.3 minutes per epoch), since it hasonly 2 layers and thus less parameters.
but becauseit is unpretrained and learned from scratch, it needsmore training epochs..6.2 qualitative case study on explanations.
in table 4, we present two examples generated bypeter and peter+ on the tripadvisor dataset.
we can see that peter generates distinct contextwords and explanations for different user-item pairs.
this conﬁrms that our proposed solution can in-deed endow the user and item ids with linguis-.
4954disable lcdisable lrleft-to-right maskingpeter.
explainabilityfcr0.03 ↓0.14 ↑0.15 ↑0.13.div5.75 ↓2.90 ↑2.68 ↑2.95.fmr0.06 ↓0.070.070.07.usr0.01 ↓0.10 ↑0.12 ↑0.08.recommendation.
text qualitybleu-1 bleu-4 rmse mae0.61 ↑15.37 ↓3.10 ↓16.16 ↑0.68 ↓15.73 ↓0.6315.96.
0.80 ↑3.23 ↓0.87 ↓0.81.
0.86 ↓1.15 ↑1.111.11.table 6: ablation study on the smallest dataset tripadvisor.
arrows ↑ and ↓ respectively denote the performanceincrease and decrease compared with peter..tic meanings, as well as achieving certain degreeof personalization for natural language generation.
among the commonly used context words, e.g.,the, there are some important features (underlined),according to which the model then generates an ex-planation that talks about them.
admittedly, thereis still much room for improvement of the contextprediction task, so as to more accurately predict thefeatures in the ground-truth (e.g., rooms vs. poolin the ﬁrst example).
one alternative is to leveragethe features to guide the model’s generation.
thisexplains why peter+ is able to generate an ex-planation that talks about rooms rather than pool,making it semantically closer to the ground-truth.
it thus demonstrates our model’s ﬂexibility in in-corporating these features..6.3 recommendation performance.
table 5 presents the performance comparisonof different recommendation methods.
on thelargest dataset yelp with approximately 1.3 millionrecords, our model peter performs as good as thethree competitive baselines (i.e., svd++, nrt andnete), which shows the rationale of our recom-mendation module.
since our model peter hasmore parameters to learn, it may underﬁt on smalldatasets.
this explains why it does not always per-form the best on tripadvisor and amazon.
whenmore training data are available to transformer,usually the performance will become better, as evi-denced by gpt-2 (radford et al., 2019) and gpt-3(brown et al., 2020).
thus, we can expect ourmodel to perform well in real-world applications,where the training data are bigger than the testingdatasets, e.g., billion-scale users in amazon..6.4 ablation study.
in table 6, we provide an ablation study conductedon the tripadvisor dataset.
after disabling thecontext prediction task lc by setting λc = 0, theperformances of both explainability and text qual-ity drop dramatically, and the unique sentence ratio(usr) is nearly approaching transformer’s (seetable 2).
it hence conﬁrms this task’s effectiveness..as lc is highly correlated with the recommenda-tion task lr via the user and item ids (see section4.3), the removal of lc leads to slight improve-ment on recommendation performance.
we canalso observe a reversed phenomenon when we dis-able lr.
when peter masking is replaced by theleft-to-right masking that prevents the model fromaccessing the item information, the recommenda-tion performance drops sharply.
overall, peterreaches an optimal situation, where its explainabil-ity, text quality and recommendation performanceare all reasonably good..7 conclusion.
we propose a simple and effective solution to ad-dress the personalized generation problem of trans-former, unleashing its language modeling powerto generate explanations for recommender systems.
extensive experiments show that the solution isboth effective and efﬁcient.
it opens up a new wayof exploiting transformer by designing good tasksinstead of scaling up model size.
there are variousapplications of personalized generation for whichtransformer is still less explored.
our next stepis to adopt our solution for personalized questionanswering systems and personalized conversationalagents.
we also plan to incorporate item imagesinto the model, so as to generate visual explanationsfor recommendations, since “a picture is worth athousand words”.
another meaningful extensionis to adapt the model to cross-lingual explanationgeneration, because international platforms, e.g.,amazon, may serve users who speak different lan-guages..acknowledgments.
this work was partially supported by hkbuircms/19-20/d05, rgc/hkbu12201620, andnsf iis-1910154 and iis-2007907.
any opin-ions, ﬁndings, conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reﬂect those of the sponsors..4955references.
qingyao ai, vahid azizi, xu chen, and yongfengzhang.
2018. learning heterogeneous knowledgebase embeddings for explainable recommendation.
algorithms, 11(9):137..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in 2019 annual conference of the north amer-ican chapter of the association for computationallinguistics..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
in advances in neural information process-ing systems..li dong, shaohan huang, furu wei, mirella lapata,ming zhou, and ke xu.
2017. learning to generateproduct reviews from attributes.
in proceedings ofthe 15th conference of the european chapter of theassociation for computational linguistics: volume1, long papers, pages 623–632..hanxiong chen, xu chen, shaoyun shi, and yongfengzhang.
2019a.
generate natural language explana-in proceedings of si-tions for recommendation.
gir’19 workshop on explainable recommendationand search.
acm..hanxiong chen, shaoyun shi, yunqi li, and yongfengzhang.
2021. neural collaborative reasoning.
inproceedings of the web conference 2021..li chen and feng wang.
2017. explaining recom-mendations based on feature sentiments in productin proceedings of the 22nd internationalreviews.
conference on intelligent user interfaces, pages 17–28..li chen, dongning yan, and feng wang.
2019b.
userevaluations on sentiment-based recommendation ex-planations.
acm transactions on interactive intelli-gent systems (tiis), 9(4):1–38..xu chen, hanxiong chen, hongteng xu, yongfengzhang, yixin cao, zheng qin, and hongyuan zha.
2019c.
personalized fashion recommendation withvisual explanations based on multimodal attentionnetwork: towards visually explainable recommen-in proceedings of the 42nd internationaldation.
acm sigir conference on research and develop-ment in information retrieval, pages 765–774..xu chen, yongfeng zhang, and zheng qin.
2019d.
dy-namic explainable recommendation based on neuralattentive models.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 33, pages53–60..zhongxia chen, xiting wang, xing xie, mehulparsana, akshay soni, xiang ao, and enhong chen.
2020. towards explainable conversational recom-in proceedings of the twenty-ninthmendation.
international joint conference on artiﬁcial intelli-gence..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder-decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1724–1734..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, pages 13063–13075..zuohui fu, yikun xian, ruoyuan gao, jieyu zhao,qiaoying huang, yingqiang ge, shuyuan xu, shi-jie geng, chirag shah, yongfeng zhang, et al.
2020. fairness-aware explainable recommendationover knowledge graphs.
in proceedings of the 43rdinternational acm sigir conference on researchand development in information retrieval..fatih gedikli, dietmar jannach, and mouzhi ge.
2014.how should i explain?
a comparison of differentin-explanation types for recommender systems.
ternational journal of human-computer studies,72(4):367–382..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..yehuda koren.
2008. factorization meets the neighbor-hood: a multifaceted collaborative ﬁltering model.
in proceedings of the 14th acm sigkdd interna-tional conference on knowledge discovery and datamining, pages 426–434..junjie li, haoran li, and chengqing zong.
2019. to-wards personalized review summarization via user-in proceedings of theaware sequence network.
aaai conference on artiﬁcial intelligence, vol-ume 33, pages 6690–6697..lei li, li chen, and ruihai dong.
2020a.
caesar:context-aware explanation based on supervised at-tention for service recommendations.
journal of in-telligent information systems, pages 1–24..lei li, li chen, and yongfeng zhang.
2020b.
to-wards controllable explanation generation for recom-in compan-mender systems via neural template.
ion proceedings of the web conference 2020, pages198–202..lei li, yongfeng zhang, and li chen.
2020c.
gen-erate neural template explanations for recommenda-tion.
in proceedings of the 29th acm internationalconference on information & knowledge manage-ment, pages 755–764..4956lei li, yongfeng zhang, and li chen.
2021. extra:explanation ranking datasets for explainable recom-mendation.
in proceedings of the 44th internationalacm sigir conference on research and develop-ment in information retrieval..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..piji li, zihao wang, zhaochun ren, lidong bing, andwai lam.
2017. neural rating regression with ab-stractive tips generation for recommendation.
inproceedings of the 40th international acm sigirconference on research and development in infor-mation retrieval, pages 345–354..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..peter j liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser, and noamshazeer.
2018. generating wikipedia by summariz-ing long sequences.
in the sixth international con-ference on learning representations..andriy mnih and russ r salakhutdinov.
2007. proba-bilistic matrix factorization.
in advances in neuralinformation processing systems, pages 1257–1264..jianmo ni, jiacheng li, and julian mcauley.
2019.justifying recommendations using distantly-labeledreviews and ﬁne-grained aspects.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 188–197..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..razvan pascanu, tomas mikolov, and yoshua bengio.
2013. on the difﬁculty of training recurrent neuralin international conference on machinenetworks.
learning, pages 1310–1318..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..herbert robbins and sutton monro.
1951. a stochasticapproximation method.
the annals of mathematicalstatistics, pages 400–407..shaoyun shi, hanxiong chen, weizhi ma, jiaxin mao,min zhang, and yongfeng zhang.
2020. neuralin proceedings of the 29th acmlogic reasoning.
international conference on information & knowl-edge management, pages 1365–1374..yikun xian, zuohui fu, s muthukrishnan, gerardde melo, and yongfeng zhang.
2019. reinforce-ment knowledge graph reasoning for explainablerecommendation.
in proceedings of the 42nd inter-national acm sigir conference on research anddevelopment in information retrieval, pages 285–294..yikun xian, zuohui fu, handong zhao, yingqiang ge,xu chen, qiaoying huang, shijie geng, zhou qin,gerard de melo, shan muthukrishnan, et al.
2020.cafe: coarse-to-ﬁne neural symbolic reasoning forexplainable recommendation.
in proceedings of the29th acm international conference on information& knowledge management, pages 1645–1654..yongfeng zhang and xu chen.
2020. explainablerecommendation: a survey and new perspectives.
foundations and trends r(cid:13) in information retrieval,14(1):1–101..yongfeng zhang, xu chen, qingyao ai, liu yang,and w bruce croft.
2018. towards conversationalsearch and recommendation: system ask, user re-in proceedings of the 27th acm interna-spond.
tional conference on information and knowledgemanagement, pages 177–186..yongfeng zhang, guokun lai, min zhang, yi zhang,yiqun liu, and shaoping ma.
2014. explicit fac-tor models for explainable recommendation basedon phrase-level sentiment analysis.
in proceedingsof the 37th international acm sigir conference onresearch & development in information retrieval,pages 83–92..yinhe zheng, rongsheng zhang, minlie huang, andxiaoxi mao.
2020. a pre-training based personal-ized dialogue generation model with persona-sparsedata.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, pages 9693–9700..kun zhou, wayne xin zhao, shuqing bian, yuan-hang zhou, ji-rong wen, and jingsong yu.
2020.improving conversational recommender systems viain pro-knowledge graph based semantic fusion.
ceedings of the 26th acm sigkdd internationalconference on knowledge discovery & data min-ing, pages 1006–1014..yaxin zhu, yikun xian, zuohui fu, gerard de melo,and yongfeng zhang.
2021. faithfully explainablerecommendation via neural logic reasoning.
in 2021annual conference of the north american chapterof the association for computational linguistics..4957