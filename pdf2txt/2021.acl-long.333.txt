convolutions and self-attention: re-interpreting relative positions inpre-trained language models.
tyler a. chang1,2, yifan xu1, weijian xu1, zhuowen tu11university of california san diego2halıcıo˘glu data science institute{tachang, yix081, wex041, ztu}@ucsd.edu.
abstract.
in this paper, we detail the relationship be-tween convolutions and self-attention in nat-ural language tasks.
we show that relativeposition embeddings in self-attention layersare equivalent to recently-proposed dynamiclightweight convolutions, and we considermultiple new ways of integrating convolutionsinto transformer self-attention.
speciﬁcally,we propose composite attention, which unitesprevious relative position embedding meth-ods under a convolutional framework.
weconduct experiments by training bert withcomposite attention, ﬁnding that convolutionsconsistently improve performance on multi-ple downstream tasks, replacing absolute posi-tion embeddings.
to inform future work, wepresent results comparing lightweight convo-lutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection pointsfor convolutions in self-attention layers..1.introduction.
in recent years, transformer-based language mod-els have brought dramatic improvements on a widerange of natural language tasks (brown et al.,2020; devlin et al., 2019).
the central innovationof transformer architectures is the self-attentionmechanism (vaswani et al., 2017), which hasgrown beyond nlp, extending into domains rang-ing from computer vision (dosovitskiy et al., 2021)and speech recognition (dong et al., 2018) to rein-forcement learning (parisotto et al., 2020; touvronet al., 2020)..in computer vision, self-attention and convolu-tions have been combined to achieve competitiveresults for image classiﬁcation (bello et al., 2019).
similarly, researchers in nlp have begun integrat-ing convolutions into self-attention for natural lan-guage tasks.
recent work has shown initial success.
adding convolutional modules to self-attention inpre-trained language models (jiang et al., 2020), oreven replacing self-attention entirely with dynamicconvolutions (wu et al., 2019).
these successesdefy theoretical proofs showing that multi-headedself-attention with relative position embeddings isstrictly more expressive than convolution (cordon-nier et al., 2020).
to identify why convolutionshave been successful in nlp, we seek to isolate thedifferences between self-attention and convolutionin the context of natural language..in this work, we formalize the relationshipbetween self-attention and convolution in trans-former encoders by generalizing relative positionembeddings, and we identify the beneﬁts of eachapproach for language model pre-training.
weshow that self-attention is a type of dynamiclightweight convolution, a data-dependent convo-lution that ties weights across input channels (wuet al., 2019).
notably, previous methods of en-coding relative positions (shaw et al., 2018; raf-fel et al., 2020) are direct implementations oflightweight convolutions.
under our framework,the beneﬁts of convolution come from an ability tocapture local position information in sentences..then, we propose composite attention, which ap-plies a lightweight convolution that combines previ-ous relative position embedding methods.
we ﬁndthat composite attention sufﬁciently captures theinformation provided by many other convolutions.
to validate our framework, we train bert modelsthat integrate self-attention with multiple convo-lution types, evaluating our models on the gluebenchmark (wang et al., 2018).
all of our con-volutional variants outperform the default model,demonstrating the effectiveness of convolutions inenhancing self-attention for natural language tasks.
our empirical results provide evidence for futureresearch integrating convolutions and self-attentionfor nlp..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4322–4333august1–6,2021.©2021associationforcomputationallinguistics4322figure 1: generating attention maps using standard self-attention (top) and ﬁxed lightweight convolution (bottom).
attention weights αij are analogous to convolution kernel weights βj−i..2 self-attention and lightweight.
2.2 lightweight convolutions.
convolutions.
first, we outline the relationship between self-attention and convolutions.
speciﬁcally, we showthat a self-attention operation can be viewed as adynamic lightweight convolution, a depthwise con-volution that ties weights along channels (wu et al.,2019).
we then isolate the differences betweenself-attention and lightweight convolutions, high-lighting the beneﬁts of each approach in languagemodels..2.1 self-attention.
in a transformer self-attention layer,inputsx1, ..., xn ∈ rd are projected to correspondingqueries, keys, and values by linear transformationsw q, w k, w v ∈ rd×dh for each attention head,projecting into the head dimension size dh.
outputvectors y1, ..., yn ∈ rd are linear combinations ofvalues, concatenating all attention heads.
valueweights (before softmaxing) are determined by:.
αij =.
(xiw q)(xjw k)t√dh.
..(1).
intuitively, αij represents the attention that token ipays to token j, incorporating the value xjw v intothe resulting vector yi.
from the attention scoresbetween various tokens i and j, an attention mapof αij is produced (see figure 1)..in contrast, a standard one-dimensional convolu-tion slides a kernel of weights along the input se-quence; each feature in each output representationyi is a weighted sum of all features (called “chan-nels”) in the surrounding xi.
to save parameters,it is common to consider depthwise convolutionswhere each channel c in yi is a weighted sum onlyof the features in channel c for the surrounding xi.
formally, each entry of yi can be written as:.
yi,c =.
(cid:88).
−k≤j−i≤k.
βj−i,c xj,c.
(2).
where k is the kernel size in each direction.
eachscalar βj−i,c represents the attention paid to rela-tive position j − i for channel c. to further simplifydepthwise convolutions for use in language models,wu et al.
(2019) propose lightweight convolutions,which tie weights βj−i,c along all channels c. asa result, the lightweight convolution contains only2k + 1 weights, one scalar βj−i for each relativeposition considered.
then, each yi is a linear com-bination of surrounding xi:.
(cid:88).
yi =.
βj−i xj.
−k≤j−i≤k.
(3).
importantly, we can then consider each βj−i as anattention weight analogous to self-attention, repre-senting the attention that token i pays to token j..4323octopususedtheusedcoconuttheashellasshieldtoken itoken joctopustheusedcoconuttheashellasshieldusedβj-iαijtoken itokenjattention vectorconvolutionkernelthe lightweight convolution produces an attentionmap of βj−i as visualized in figure 1..finally,.
furthering the similarity betweenlightweight convolutions and self-attention, wuet al.
(2019) propose dynamic lightweight convolu-tions, which dynamically compute relative weightsβj−i based on individual input tokens.
in otherwords, each row in figure 1 has relative weightsdetermined dynamically based on the input tokenxi for that row.
because attentions for relative posi-tions are no longer ﬁxed across rows, the attentionmap in figure 1 achieves similar ﬂexibility to stan-dard self-attention..2.3 self-attention vs. convolution.
we have shown that both self-attention andlightweight convolution compute linear combina-tions of token representations, but we now isolatethe differences between the two approaches.
per-haps most importantly, the two methods assignattention scores αij and βj−i in fundamentally dif-ferent ways..self-attention computes αij based on the dotproduct between query i and key j, ignoring therelative position between i and j. in this way, self-attention layers model interactions exclusively be-tween token representations.
if the tokens are arbi-trarily shufﬂed in a standard self-attention layer, theoutput for each token is unchanged.
all position in-formation is injected before the ﬁrst self-attentionlayer in the form of absolute position embeddings.
in contrast, dynamic lightweight convolutionsassign attention scores directly to relative positions.
this allows convolutions to directly integrate rela-tive position information without relying on abso-lute positions.
thus, convolutions could be betterat capturing local information in sentences.
how-ever, convolutions alone are limited in their abilityto model interactions between tokens because theylack the query-key mechanism central to standardself-attention.
in future sections, we consider meth-ods of integrating the two approaches..3.integrating lightweight convolutions.
previous work has sought to integrate local informa-tion into global self-attention.
this can be achievedby restricting the range of self-attention to nearbytokens, or by incorporating relative position infor-mation into attention maps (hofst¨atter et al., 2020;raganato et al., 2020; wei et al., 2021).
notably,shaw et al.
(2018) introduced relative position em-.
beddings, which inspired similar embeddings inmodels such as transformer-xl and xlnet (daiet al., 2019; yang et al., 2019).
in this section,we show that several previous methods of encod-ing relative positions are direct implementations oflightweight convolutions..3.1 relative embeddings as lightweight.
convolutions.
first, the simplest way to combine self-attentionwith lightweight convolution is to generate a stan-dard attention map, then add the attention map gen-erated by a lightweight convolution.
given a ﬁxedlightweight convolution, this results in attentionscores as follows:.
αij =.
(xiw q)(xjw k)t√dh.
+ βj−i.
(4).
this is exactly the relative position term used in t5(raffel et al., 2020) and tupe (ke et al., 2021)..we further consider a dynamic lightweight con-volution, where the βj−i weights are computedby passing the query through a linear feedforwardlayer w c ∈ rdh×(2k+1) (wu et al., 2019).1 be-cause w c is linear, each weight βj−i is equal tothe dot product between the query and the (j − i)column of w c. we then obtain attention scores:.
αij =.
(xiw q)(xjw k)t√dh.
+ (xiw q)(w c.j−i)t.if we scale the dynamic lightweight convolutionterm according to the head dimension size, we ob-tain precisely the relative embeddings proposed inshaw et al.
(2018):.
(xiw q)(xjw k + w c√.
j−i)t.αij =.
(5).
dh.
under this interpretation, shaw’s relative embed-dings are essentially identical to the dynamiclightweight convolutions used in wu et al.
(2019).
in both formulations, relative position weights arecomputed as dot products between the query anda learned relative position embedding.
previouswork has considered relative positions in languagemodels independently from convolutions, but ourderivations suggest that the underlying mechanismsmay be the same..1wu et al.
(2019) generate dynamic lightweight convolu-tions based on the entire query layer (dimension size d).
inour work, we generate convolutions based on queries for in-dividual attention heads (dimension size dh), to be consistentwith the relative embeddings in shaw et al.
(2018)..4324lightweight convolutiontype, bert-smallno convolutionno convolution + abs position∗fixed (raffel et al.
2020)dynamic (shaw et al.
2018)composite (equation 6; ours).
params cola mnli-m73.276.177.278.478.2.
13.41m 13.913.43m 30.813.42m 42.113.43m 39.113.43m 40.4.mnli-mm71.875.976.377.477.4.lightweight convolutiontype, bert-baseno convolution + abs position∗ 108.82m 50.3108.73m 50.0fixed (raffel et al.
2020)108.74m 50.9dynamic (shaw et al.
2018)108.74m 50.4composite (equation 6; ours).
params cola mnli-m82.081.581.681.6.mnli-mm81.280.580.580.8.mrpc qnli qqp rte sst sts glue.
77.980.483.883.885.0.
80.778.582.783.483.3.
74.574.475.977.577.7.
62.062.264.464.464.7.
81.985.187.187.387.8.
79.376.881.481.482.1.mrpc qnli qqp rte sst sts glue.
85.085.684.685.4.
84.686.085.385.1.
78.678.578.578.7.
68.968.969.569.7.
91.491.491.691.2.
84.984.984.885.7.
68.471.174.574.775.2.
78.578.678.678.7.table 1: glue test set performance for models with lightweight convolutions added to self-attention.
columnsindicate scores on individual glue tasks; the ﬁnal glue score is the average of individual task scores.
∗ denotesthe default bert model..3.2 composite attention and lightweight.
convolution experiments.
to validate lightweight convolutions in combina-tion with self-attention, we pre-trained and evalu-ated bert-small models (devlin et al., 2019; clarket al., 2020) that incorporated lightweight convolu-tions..pre-training to maximize similarity with de-vlin et al.
(2019), we pre-trained models on thebookcorpus (zhu et al., 2015) and wikitext-103datasets (merity et al., 2017) using masked lan-guage modeling.
small models were pre-trainedfor 125,000 steps, with batch size 128 and learn-ing rate 0.0003. full pre-training and ﬁne-tuningdetails are outlined in appendix a.1.2.
evaluation models were evaluated on the gluebenchmark, a suite of sentence classiﬁcation tasksincluding natural language inference (nli), gram-maticality judgments, sentiment classiﬁcation, andtextual similarity (wang et al., 2018).
for each task,we ran ten ﬁne-tuning runs and used the model withthe best score on the development set.
we reportscores on the glue test set.
development scoresand statistics for all experiments are reported inappendix a.2..models we trained two baseline models, a de-fault bert-small with standard absolute positionembeddings, and a bert-small with no positioninformation whatsoever.
then, we trained modelswith ﬁxed lightweight convolutions (equation 4;.
2code.
is.
at https://github.com/mlpc-ucsd/bert_convolutions, built upon thehuggingface transformers library (wolf et al., 2020)..available.
raffel et al.
2020), and dynamic lightweight convo-lutions that generated convolution weights based oneach query (i.e.
using relative embeddings, equa-tion 5; shaw et al.
2018)..finally, we propose composite attention, whichsimply adds dynamic lightweight convolutions toﬁxed lightweight convolutions, resulting in atten-tion scores αij as follows:.
(xiw q)(xj w k )t√dh(cid:123)(cid:122)self-attention.
(cid:124).
(cid:125).
+.
(xiw q)(w c√.
j−i)t.dh(cid:124)(cid:125)(cid:123)(cid:122)dynamic convolution(relative embeddings).
+ βj−i(cid:124)(cid:123)(cid:122)(cid:125)fixedconvolution.
(6)intuitively, composite attention has the ﬂexibilityof dynamic lightweight convolutions, while stillallowing models to incorporate relative positionsdirectly through ﬁxed lightweight convolutions.
al-ternatively, composite attention can be interpretedas adding a ﬁxed bias term to relative position em-beddings..all of our experiments used a convolution ker-nel size of 17, or eight positions in each direction,a mid-range value that has been found to workwell for both relative positions and convolution inlanguage models (huang et al., 2020; jiang et al.,2020; shaw et al., 2018).
as in shaw et al.
(2018),relative embeddings w cj−i shared weights acrossheads.
unless stated otherwise, models used noabsolute position embeddings..for completeness, we also considered dynamiclightweight convolutions based on the key (as op-posed to the query).
in contrast to query-based.
4325lightweight convolutions, key-based convolutionsallow each token to dictate which relative posi-tions should pay attention to it, rather than dic-tating which relative positions it should pay at-tention to.
referring to the visualization in fig-ure 1, key-based dynamic convolutions correspondto columns instead of rows.
these key-based dy-namic lightweight convolutions are the same asthe relative embeddings proposed in huang et al.
(2020), but they are now formulated as dynamiclightweight convolutions..3.3 lightweight convolution results.
glue test set results are presented in table 1..consistently.
convolutions.
im-lightweightproved performance.
notably, even the ﬁxedlightweight convolution was sufﬁcient to replaceabsolute position embeddings, outperforming thedefault bert-small model.
this indicates thateven na¨ıve sampling from nearby tokens can bebeneﬁcial to language model performance..dynamic convolutions provided further im-provements.
when the lightweight convolutionswere generated dynamically based on token queries,the models outperformed the default model byeven larger margins.
this improvement over ﬁxedlightweight convolutions suggests that different to-kens ﬁnd it useful to generate different lightweightconvolutions, paying attention to different relativepositions in a sentence..composite attention performed the best.
combining ﬁxed lightweight convolutions with dy-namic lightweight convolutions proved an effectivestrategy for encoding relative positions.
althoughcomposite attention is simply a combination ofshaw et al.
(2018) and raffel et al.
(2020)’s relativeposition embeddings, it validates convolution asa viable method of encoding relative positions inself-attention..key-based dynamic convolutions provided noadditional beneﬁt.
when we generated an ad-ditional lightweight convolution based on keys, themodel performed worse than composite attentionalone (glue 74.0 compared to 75.2).
this resultclariﬁes the ﬁndings of huang et al.
(2020), whoreported only small improvements from query andkey-based relative position embeddings for a subsetof the glue tasks..figure 2: learned convolution kernel weights βj−i forthe ﬁxed lightweight convolution (equation 4)..grammaticality judgments were particularlysensitive to position information.
on the colalinguistic acceptability;task (the corpus ofwarstadt et al.
2019), there was a dramatic per-formance drop when absolute position embed-dings were removed.
however, when any type oflightweight convolution was added, performanceimproved even over the baseline established by ab-solute positions.
the pronounced effects of localposition information on the cola task support theintuitive hypothesis that local dependencies are par-ticularly important for grammaticality judgments.
this result also suggests that convolutions couldbe beneﬁcial to more local tasks (e.g.
token-leveltasks) along with sentence classiﬁcation tasks..3.4.interpreting lightweight convolutions.
to better understand how lightweight convolu-tions improve language models, we visualized thelearned lightweight convolution kernel weights infigure 2. qualitatively, the kernels exhibited spe-ciﬁc types of patterns:.
• paying particular attention to the previous or.
next token..• paying graded attention either to past or futuretokens, dictated by how far the target token isfrom the present token..these observations support the assumption thatnearby tokens are relevant to the interpretation ofthe current token.
they also align with the ﬁndings.
4326of voita et al.
(2019), who identiﬁed “positional”attention heads that focus primarily on the next orprevious token.
from this perspective, lightweightconvolutions allow language models to explicitlyrepresent nearby tokens’ positions..interestingly, we also found that some kernelspaid fairly uniform attention to all tokens, evendecreasing attention to nearby and adjacent tokens.
it is likely that these attention heads focused onmore global information, relying on the query-keyattention mechanism rather than the convolution..3.5 bert-base models.
to thoroughly assess the impact of composite at-tention on pre-trained language models, we trainedfull-sized bert models for 1m steps each, repli-cating our bert-small experiments.
pre-trainingdetails are outlined in appendix a.1..results are presented in table 1. differences be-tween models decreased substantially for full sizedmodels, and the relative performances of differentapproaches varied across tasks.
our results suggestthat relative position information is more usefulfor smaller or more data-limited models; extendingthe beneﬁts of convolutions robustly from smallmodels to larger models is an important directionfor future research.
that said, even in the largermodels, composite attention slightly outperformedthe other position embedding methods in overallglue score.
our results demonstrate that convo-lutions can perform at least on par with absoluteposition embeddings even in larger models..4 non-lightweight convolutions.
the previous section found that lightweight convo-lutions consistently improved pre-trained languagemodel performance.
next, we investigated whetherthe additional ﬂexibility of non-lightweight convo-lutions could provide additional beneﬁts.
speciﬁ-cally, we considered convolutions that were ﬁxedbut non-lightweight.
in other words, convolutionweights were ﬁxed regardless of the input query,but weights were not tied across channels, equiv-alent to a standard depthwise convolution.
weonly considered ﬁxed depthwise convolutions be-cause under existing frameworks, dynamic depth-wise convolutions would introduce large numbersof parameters..to implement depthwise convolutions, we addeda convolution term identical to the ﬁxed lightweightconvolution in equation 4, except that βj−i was.
figure 3: learned convolution kernel weights βj−i,c(equation 7) for the depthwise convolution in the deep-est attention layer.
channels correspond to the 256 fea-tures in each token representation.
channels are sortedsuch that kernels differentiating the previous and nexttoken are grouped together..learned separately for each feature channel:3.αij,c =.
(xiw q)(xjw k)t√dh.
+ βj−i,c.
(7).
this is equivalent to adding a depthwise convo-lution of the token values to the standard self-attention output..4.1 non-lightweight convolution experiments.
we ran experiments using the same setup as thelightweight convolution experiments in section3.2. to compare the effects of dynamic lightweightconvolutions (e.g.
composite attention) and non-lightweight (depthwise) convolutions, we trainedmodels using each possible combination of the twoconvolutions.
results are presented in table 2..depthwise convolutions were less effective thanlightweight convolutions.
as with lightweightconvolutions, the depthwise convolutions effec-tively replaced absolute position embeddings, out-performing the default model.
however, ﬁxeddepthwise convolutions performed worse than ﬁxedlightweight convolutions on the majority of tasks.
this indicates that ﬂexibility across channels is notcritical to the success of convolutions in languagemodels..3for computational efﬁciency, we applied the softmaxto the attention scores prior to adding the convolution termβj−i,c, to avoid computing softmax scores separately for eachindividual channel.
softmax is not commonly applied in depth-wise convolutions..4327convolutions.
mrpc qnli qqp rte sst sts glue.
no convolution + abs position∗composite (equation 6)fixed depthwisefixed depthwise + composite.
params cola mnli-m76.178.277.677.4.
13.43m 30.813.43m 40.413.47m 36.913.48m 38.0.mnli-mm75.977.476.176.3.
80.485.080.682.8.
78.583.381.983.7.
74.477.776.477.7.
62.264.764.565.3.
85.187.887.587.3.
76.882.179.782.3.
71.175.273.574.5.table 2: glue test set performance for bert-small models with added depthwise convolutions and compositeattention.
∗ denotes the default bert-small model..no composite attention.
+composite attention.
query/keylinearconvolutionlinearconvolution.
valuelinearlinearconvolutionconvolution.
params glue13.43m ∗71.171.913.53m73.413.47m72.013.58m.
query/keylinearconvolutionlinearconvolution.
valuelinearlinearconvolutionconvolution.
params glue75.213.43m74.513.54m73.913.48m74.013.59m.
table 3: bert-small performance on the glue test set when replacing queries, keys, and values with depthwise-separable convolutions for half of the attention heads.
∗ denotes the use of absolute position embeddings in thedefault bert-small model..composite attention already provided the nec-essary ﬂexibility.
composite attention outper-formed the ﬁxed depthwise convolutions; evenwhen composite attention was combined withdepthwise convolutions, there was no overall im-provement over composite attention alone.
thissuggests that in the context of language, dynamiclightweight convolutions efﬁciently encode any lo-cal position information provided by depthwiseconvolutions..depthwise convolutions differentiated previousand next tokens.
in previous sections, we foundthat lightweight convolution kernels often pay at-tention speciﬁcally to adjacent tokens.
as can beseen in figure 3, this result was even more pro-nounced in depthwise convolutions, with individ-ual channels focusing on the previous or next token.
interestingly, other channels speciﬁcally directedattention away from adjacent tokens.
this indicatesthat the relevant information about next and previ-ous tokens can be compressed into a subset of thefeature channels, freeing other channels to considermore distant or position-independent information..5 convolutional queries, keys, and values.
improvements over the non-convolutional base-lines indicate that convolutions are beneﬁcial to lan-guage model pre-training, serving as replacementsfor absolute position embeddings.
our previousexperiments applied different types of convolutionsto self-attention values.
to take this result one step.
further, we replaced the linear query, key, and valueprojections themselves with convolutional layers.
intuitively, applying convolutions before self-attention induces even more mixing of token rep-resentations.
if convolutions are built into everyquery, key, and value, then it becomes impossiblefor a token i to pay attention to a single token jwithout also incorporating information about to-kens surrounding token j..5.1 convolutional q, k, v experiments.
as in sections 3.2 and 4.1, we ran experiments onbert-small.
we replaced the query, key and valueprojections with depthwise-separable convolutionsin half of the self-attention heads.4 this alignswith previous work in which only half of the outputdimensions for each token were generated usingconvolutions (jiang et al., 2020).
indeed, our initialexplorations found that it was more effective toreplace the linear projections in only half, not all,the attention heads..then, we considered whether convolutions fromprevious experiments provided additional beneﬁtsover convolutional queries, keys, and values.
totest this, we trained bert-small models with com-posite attention (equation 6), adding convolutionalqueries, keys, and values..4depthwise-separable convolutions are a common wayto save convolution parameters.
a depthwise convolution isapplied ﬁrst, applying an independent convolution for eachchannel.
then, a pointwise convolution (i.e.
a feedforwardlayer) mixes the channels to produce the ﬁnal output..43285.2 convolutional q, k, v results.
results are presented in table 3. similar to our pre-vious convolution experiments, all convolutionalreplacements successfully outperformed the defaultmodel.
these results strongly support the conclu-sion that convolutions are a viable method of en-coding positional information for language tasks.
however, all convolutional replacements forqueries, keys, and values slightly decreased theperformance of models using composite attention.
convolutional values in particular were effectivein models without composite attention, but theyslightly decreased performance in models that al-ready incorporated such lightweight convolutions.
we conclude that although convolutions can beneﬁtmodels by adding local position information, thereis a limit to how much local mixing should be done.
it is sufﬁcient to apply convolutions to token valueson top of self-attention; additional convolutionallayers applied before the self-attention map enforceunnecessary mixing of token representations..6 discussion.
our results demonstrate that convolutions provideconsistent beneﬁts to pre-trained language models.
our proposed composite attention mechanism com-bines previous relative position embedding meth-ods, showing that convolutions can effectively com-pensate for the lack of local position informationin transformer models..6.1 related work.
our work unites and builds upon previous workusing convolutions and relative positions in trans-formers.
we adopted the relative embeddingsfrom shaw et al.
(2018) and huang et al.
(2020),showing that these embeddings are equivalent tothe dynamic lightweight convolutions in wu et al.
(2019).
combining these dynamic lightweightconvolutions with ﬁxed lightweight convolutions(equivalent to the relative position terms in raffelet al.
2020), we studied relative embeddings underthe framework of convolution integrated with self-attention.
as far as we are aware, our work is theﬁrst to holistically compare relative positions, con-volutions, and self-attention in language models..building upon dynamic lightweight convolu-tions, recent work has incorporated both depthwise-separable and dynamic lightweight convolutions inpre-trained language models.
jiang et al.
(2020)proposed convbert, which adds a convolutional.
module alongside the standard self-attention mech-anism in bert.
convbert’s convolutional mod-ule consists of a depthwise-separable convolutioncombining with a query to generate a dynamiclightweight convolution.
under our integratedframework, this is analogous to the model whichuses depthwise-separable convolutions for queriesand keys, using composite attention as a query-based dynamic lightweight convolution (see table3).
to make this comparison concrete, we traineda convbert-small model using the same setup asour experiments.
indeed, the analogous model un-der our framework outperformed convbert-small(glue score 74.5 compared to 70.3).
details forthe convbert comparison can be found in ap-pendix a.3..finally, recent work has proved theoretical rela-tionships between self-attention and convolution.
cordonnier et al.
(2020) showed that given enoughself-attention heads, self-attention weights can ex-press any convolution; in fact, they showed thatself-attention layers often learn such convolutionalstructures when trained on vision tasks.
how-ever, this theoretical equivalence does not explainconvolution-based improvements for transformersin language tasks.
to clarify the relationship be-tween self-attention and convolution in language,our work characterizes self-attention as a type ofdynamic lightweight convolution.
by establishinga per-parameter equivalence between relative po-sition embeddings and wu’s dynamic lightweightconvolutions, we provide a concrete foundationwhere self-attention and convolution are used to-gether in practice..7 conclusion.
in this work, we formalized the relationship be-tween self-attention and convolution.
we proposedcomposite attention, which combines self-attentionwith lightweight convolution, uniting previous ap-proaches to relative positions.
our formulation andempirical results demonstrate that convolutions canimprove self-attention by providing local positioninformation in sentences, capable of replacing ab-solute position embeddings entirely..our ﬁndings provide a solid foundation fromwhich to study convolutions and self-attention inlanguage tasks.
the spatially-oriented nature ofconvolutional neural networks translates directlyinto positional information in language.
as visionand language researchers strive towards common.
4329deep learning architectures, it is important to rec-ognize how architectures for vision tasks can beadapted to linguistic domains..acknowledgments.
this work is funded by nsf iis-1717431.
zhuowen tu is also funded under the qualcommfaculty award.
tyler chang is partially supportedby the ucsd hdsi graduate fellowship..references.
irwan bello, barret zoph, ashish vaswani, jonathonshlens, and quoc le.
2019. attention augmentedin international confer-convolutional networks.
ence on computer vision..tom brown, benjamin mann, nick ryder, melaniesubbiah,jared d kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei.
2020. language models are few-shot learners.
inproceedings of the 34th conference on neural infor-mation processing systems..kevin clark, minh-thang luong, quoc le, andchristopher manning.
2020.electra: pre-training text encoders as discriminators rather thangenerators.
in proceedings of the international con-ference on learning representations..jean-baptiste cordonnier, andreas loukas, and mar-tin jaggi.
2020. on the relationship between self-attention and convolutional layers.
in proceedingsof the international conference on learning repre-sentations..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..linhao dong, shuang xu, and bo xu.
2018. speech-transformer: a no-recurrence sequence-to-sequencemodel for speech recognition.
in ieee internationalconference on acoustics, speech and signal pro-cessing, pages 5884–5888..alexey dosovitskiy,.
lucas beyer, alexanderkolesnikov, dirk weissenborn, xiaohua zhai,thomas unterthiner, mostafa dehghani, matthiasminderer, georg heigold, sylvain gelly, jakobuszkoreit, and neil houlsby.
2021. an image isworth 16x16 words: transformers for image recog-nition at scale.
in proceedings of the internationalconference on learning representations..sebastian hofst¨atter, hamed zamani, bhaskar mitra,nick craswell, and allan hanbury.
2020. localself-attention over long text for efﬁcient documentretrieval.
in proceedings of the 43rd internationalacm sigir conference on research and develop-ment in information retrieval, new york, ny, usa.
association for computing machinery..zhiheng huang, davis liang, peng xu, and bing xi-improve transformer models with bet-ang.
2020.ter relative position embeddings.
in findings of theassociation for computational linguistics: emnlp2020, pages 3327–3335, online.
association forcomputational linguistics..zihang jiang, weihao yu, daquan zhou, yunpengchen, jiashi feng, and shuicheng yan.
2020. con-vbert: improving bert with span-based dynamicconvolution.
in proceedings of the 34th conferenceon neural information processing systems..guolin ke, di he, and tie-yan liu.
2021. rethinkingpositional encoding in language pre-training.
in pro-ceedings of the international conference on learn-ing representations..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..stephen merity, caiming xiong, james bradbury, andrichard socher.
2017. pointer sentinel mixture mod-els.
in proceedings of the fifth international confer-ence on learning representations..emilio parisotto, francis song, jack rae, razvan pas-canu, caglar gulcehre, siddhant jayakumar, maxjaderberg, rapha¨el lopez kaufman, aidan clark,seb noury, matthew botvinick, nicolas heess, andraia hadsell.
2020. stabilizing transformers for re-inforcement learning.
in proceedings of the interna-tional conference on machine learning..jason phang, thibault f´evry, and samuel bowman.
2018.sentence encoders on stilts: supple-mentary training on intermediate labeled-data tasks.
arxiv preprint arxiv:1811.01088..4330colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..alessandro raganato, yves scherrer, and j¨org tiede-mann.
2020. fixed encoder self-attention patternsin find-in transformer-based machine translation.
ings of the association for computational linguis-tics: emnlp 2020, pages 556–568, online.
associ-ation for computational linguistics..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..hugo touvron, matthieu cord, matthijs douze, fran-cisco massa, alexandre sablayrolles, and herv´ej´egou.
2020. training data-efﬁcient image trans-arxivformers and distillation through attention.
preprint arxiv:2012.12877..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st conference onneural information processing systems..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..wei wei, zanbo wang, xianling mao, guangyou zhou,pan zhou, and sheng jiang.
2021. position-awareself-attention based neural sequence labeling.
pat-tern recognition, 110..clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..felix wu, angela fan, alexei baevski, yann dauphin,and michael auli.
2019. pay less attention withlightweight and dynamic convolutions.
in proceed-ings of the seventh international conference onlearning representations..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ salakhutdinov, and quoc le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, volume 32..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in 2015 ieee international con-ference on computer vision, pages 19–27..hyperparameterlayershidden sizeintermediate hidden sizeattention headsattention head sizeembedding sizevocab sizemax sequence lengthmask proportionlearning rate decaywarmup stepslearning rateadam (cid:15)adam β1adam β2attention dropoutdropoutweight decaybatch sizetrain steps.
1280.15.
122561024464128.small base127683072126476830004 300041280.15linear linear10000 100001e-41e-60.90.9990.10.10.012561m.
3e-41e-60.90.9990.10.10.01128125k.
table 4: pre-training hyperparameters..a appendix.
a.1 pre-training and ﬁne-tuning details.
thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,.
bert models (devlin et al.
2019; clark et al.
2020) were pre-trained on the bookcorpus (zhuet al., 2015) and wikitext-103 datasets (merity.
4331training distributions (wang et al., 2018)..a.3 detailed convbert comparison.
convbert adds a convolutional module alongsidethe standard self-attention mechanism in bert(jiang et al., 2020).
convbert uses half the num-ber of standard self-attention heads, using convolu-tional modules for the other half.
in each convolu-tional module, a depthwise-separable convolutionis multiplied pointwise with the query in the cor-responding self-attention head.
this convolutionalquery is fed into a linear layer to generate a dy-namic lightweight convolution..under our framework, the analogous model re-places half of the queries and keys with depthwise-separable convolutions and uses composite atten-tion (a query-based dynamic lightweight convolu-tion; see table 3 in the full paper).
in both models(convbert and our own), half of the attentionheads use a convolutional query.
additionally, inboth models, the convolutional query is used togenerate a dynamic lightweight convolution..however, in our model, the dynamic lightweightconvolution (in this case, composite attention) isused for all attention heads, not just the convolu-tional heads.
furthermore, our convolutional headsstill use a self-attention mechanism along with thedynamic lightweight convolutions, by generatingconvolutional keys.
in this way, our model addsconvolutions to convbert’s self-attention heads,and adds self-attention to convbert’s convolu-tional heads..then, we investigated whether the separate self-attention and convolutional modules in convbertprovide any beneﬁt over our integrated convolu-tion and self-attention.
we trained a convbert-small model using the same pre-training setup asour bert-small experiments, comparing perfor-mance to the analogous model under our frame-work.
results are shown in table 7.indeed,integrated convolutions and self-attention outper-formed convbert-small, using only 3% more pa-rameters..hyperparameterlearning rate decaywarmup stepslearning rate.
adam (cid:15)adam β1adam β2attention dropoutdropoutweight decaybatch size.
train steps.
valuelinear10% of total1e-4 for qnli or base-size3e-4 otherwise1e-60.90.9990.10.10128 for mnli/qqp32 otherwise10 epochs for rte/sts4 epochs for mnli/qqp3 epochs otherwise.
table 5: fine-tuning hyperparameters.
we used inter-mediate task training for rte, sts, and mrpc, initial-izing from a checkpoint ﬁne-tuned on the mnli task(clark et al.
2020; phang et al.
2018)..et al., 2017) using masked language modeling.
pre-training examples were formatted as sentence pairswithout the next sentence prediction objective.
intotal, our dataset consisted of 31m unique sentencepairs.5 sentences were tokenized by training an un-cased sentencepiece tokenizer (kudo and richard-son, 2018), and input and output token embeddingswere tied during pre-training.
models were evalu-ated on the glue benchmark (wang et al., 2018).
including ten ﬁne-tuning runs for each glue task,each bert-small model took about 24 hours totrain on two titan xp gpus.
each bert-basemodel took about 16 days to train on 8 gpus.
pre-training hyperparameters are listed in table 4, andﬁne-tuning hyperparameters are listed in table 5.hyperparameters are based on those used in clarket al.
(2020) and devlin et al.
(2019)..a.2 glue development results.
results for each model on the glue developmentset are reported in table 6. we report averagesover ten ﬁne-tuning runs for each task, includingstandard errors of the mean.
each overall gluescore was computed as the average of individualtask scores; we computed glue score averagesand standard errors over ten glue scores, cor-responding to the ten ﬁne-tuning runs.
we notethat development scores were generally higher thantest scores due to differences between the test and.
5because bert-small models were only trained for125,000 steps with batch size 128, small models were trainedon 16m sentence pairs..4332paramscola7.0 ± 2.413.41m13.43m 33.5 ± 0.413.42m 38.3 ± 0.813.43m 38.4 ± 0.713.43m 40.9 ± 0.713.44m 40.0 ± 0.613.47m 38.0 ± 0.613.48m 40.4 ± 0.713.53m 33.4 ± 0.413.47m 34.7 ± 0.913.58m 31.9 ± 0.713.54m 39.3 ± 0.813.48m 37.9 ± 0.713.59m 38.2 ± 1.013.09m 33.3 ± 1.5.convolution type, bert-smallno convolutionno convolution + abs position∗fixed lightweight (raffel et al.
2020)dynamic lightweight (shaw et al.
2018)composite (equation 6)composite + key-based dynamicfixed depthwisecomposite + ﬁxed depthwiseconvolutional qkconvolutional valueconvolutional qkvcomposite + convolutional qkcomposite + convolutional valuecomposite + convolutional qkvconvbertconvolution type, bert-baseno convolution + abs position∗108.82m 57.6 ± 0.6108.73m 58.9 ± 0.5fixed lightweight (raffel et al.
2020)dynamic lightweight (shaw et al.
2018) 108.74m 58.4 ± 0.5108.74m 58.5 ± 0.5composite (equation 6).
mnli-m mnli-mm73.0 ± 0.176.1 ± 0.177.2 ± 0.177.6 ± 0.178.0 ± 0.177.7 ± 0.176.8 ± 0.177.4 ± 0.176.4 ± 0.176.6 ± 0.176.3 ± 0.177.2 ± 0.178.1 ± 0.177.3 ± 0.176.8 ± 0.1.
73.0 ± 0.175.8 ± 0.177.2 ± 0.177.9 ± 0.177.9 ± 0.177.9 ± 0.176.9 ± 0.077.2 ± 0.176.3 ± 0.176.2 ± 0.076.3 ± 0.177.4 ± 0.177.8 ± 0.177.4 ± 0.176.7 ± 0.1.mrpc80.9 ± 0.483.3 ± 0.484.0 ± 0.585.6 ± 0.586.2 ± 0.386.3 ± 0.382.8 ± 0.585.0 ± 0.383.3 ± 0.283.4 ± 0.483.7 ± 0.485.4 ± 0.385.6 ± 0.485.3 ± 0.483.9 ± 0.5.qnli80.1 ± 0.278.2 ± 0.382.1 ± 0.182.8 ± 0.183.0 ± 0.183.3 ± 0.181.9 ± 0.183.3 ± 0.181.3 ± 0.282.4 ± 0.180.4 ± 0.281.9 ± 0.183.6 ± 0.182.8 ± 0.177.1 ± 0.8.
82.0 ± 0.181.9 ± 0.181.8 ± 0.181.9 ± 0.1.
81.9 ± 0.181.6 ± 0.181.8 ± 0.181.6 ± 0.1.
88.4 ± 0.287.7 ± 0.386.7 ± 0.486.0 ± 1.2.
84.7 ± 0.386.2 ± 0.185.6 ± 0.285.0 ± 0.3.convolution type, bert-smallno convolutionno convolution + abs position∗fixed lightweight (raffel et al.
2020)dynamic lightweight (shaw et al.
2018)composite (equation 6)composite + key-based dynamicfixed depthwisecomposite + ﬁxed depthwiseconvolutional qkconvolutional valueconvolutional qkvcomposite + convolutional qkcomposite + convolutional valuecomposite + convolutional qkvconvbertconvolution type, bert-baseno convolution + abs position∗fixed lightweight (raffel et al.
2020)dynamic lightweight (shaw et al.
2018)composite (equation 6).
qqp84.4 ± 0.184.9 ± 0.086.2 ± 0.087.2 ± 0.087.3 ± 0.087.4 ± 0.086.1 ± 0.187.3 ± 0.085.1 ± 0.186.6 ± 0.084.6 ± 0.286.7 ± 0.087.5 ± 0.087.0 ± 0.085.1 ± 0.1.rte61.0 ± 0.564.4 ± 0.564.7 ± 0.965.1 ± 0.966.1 ± 0.766.3 ± 0.464.2 ± 0.763.5 ± 0.863.0 ± 1.065.2 ± 0.766.1 ± 0.964.0 ± 0.965.1 ± 0.564.9 ± 0.864.6 ± 0.5.sst80.9 ± 0.985.0 ± 0.286.9 ± 0.286.8 ± 0.286.9 ± 0.186.5 ± 0.387.2 ± 0.287.1 ± 0.286.1 ± 0.287.2 ± 0.386.4 ± 0.187.5 ± 0.287.5 ± 0.186.9 ± 0.186.3 ± 0.3.sts83.7 ± 0.182.4 ± 0.185.2 ± 0.185.6 ± 0.185.9 ± 0.186.1 ± 0.284.4 ± 0.186.1 ± 0.184.5 ± 0.185.0 ± 0.184.4 ± 0.185.7 ± 0.186.4 ± 0.185.9 ± 0.184.0 ± 0.2.
88.7 ± 0.088.8 ± 0.088.7 ± 0.088.7 ± 0.0.
69.9 ± 0.570.9 ± 0.770.6 ± 0.671.0 ± 0.7.
90.4 ± 0.190.8 ± 0.191.1 ± 0.190.5 ± 0.1.
88.4 ± 0.188.1 ± 0.187.7 ± 0.388.4 ± 0.1.glue69.3 ± 0.373.7 ± 0.175.7 ± 0.276.3 ± 0.176.9 ± 0.176.8 ± 0.175.4 ± 0.176.4 ± 0.174.4 ± 0.175.2 ± 0.174.4 ± 0.176.1 ± 0.176.6 ± 0.176.2 ± 0.274.2 ± 0.3.
81.0 ± 0.281.3 ± 0.281.1 ± 0.281.2 ± 0.2.table 6: glue development set scores for each model described in the main paper, reporting averages and standarderrors of the mean over ten ﬁne-tuning runs for each task.
∗ denotes the default bert model..model, bert-small.
convbertintegrated convolutions andself-attention (ours).
params cola mnli-m75.477.5.
13.1m 25.513.5m 37.9.mnli-mm73.976.6.mrpc qnli qqp rte sst sts glue.
79.783.7.
76.083.1.
74.776.6.
64.365.3.
85.688.7.
77.981.1.
70.374.5.table 7: comparison between convbert-small and the analogous model under our framework, reporting gluetest set results..4333