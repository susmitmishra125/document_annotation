mlbinet: a cross-sentence collective event detection network.
dongfang lou1,2 ∗, zhilin liao 1,2 ∗, shumin deng1,2, ningyu zhang1,2 †, huajun chen1,2 †1 zhejiang university & azft joint lab for knowledge engine2 hangzhou innovation center, zhejiang universityloudongfang2015@163.com, zhilinliao@yeah.net{231sm,zhangningyu,huajunsir}@zju.edu.cn.
abstract.
we consider the problem of collectively de-tecting multiple events, particularly in cross-sentence settings.
the key to dealing with theproblem is to encode semantic information andmodel event inter-dependency at a document-level.
in this paper, we reformulate it asa seq2seq task and propose a multi-layerbidirectional network (mlbinet) to capturethe document-level association of events andsemantic information simultaneously.
speciﬁ-cally, a bidirectional decoder is ﬁrstly devisedto model event inter-dependency within a sen-tence when decoding the event tag vector se-quence.
secondly, an information aggregationmodule is employed to aggregate sentence-level semantic and event tag information.
fi-nally, we stack multiple bidirectional decodersand feed cross-sentence information, forminga multi-layer bidirectional tagging architectureto iteratively propagate information across sen-tences.
we show that our approach providessigniﬁcant improvement in performance com-pared to the current state-of-the-art results1..1.introduction.
event detection (ed) is a crucial sub-task of eventextraction, which aims to identify and classifyevent triggers.
for instance, the document shown intable 1, which contains six sentences {s1, .
.
.
, s6},the ed system is required to identify four events:an injure event triggered by “injuries”, two attackevents triggered by “ﬁring” and “ﬁght”, and a dieevent triggered by “death”..detecting event triggers from natural languagetext is a challenge task because of the followingproblems: a).
sentence-level contextual repre-sentation and document-level information ag-gregation (chen et al., 2018; zhao et al., 2018;∗ equal contribution and shared co-ﬁrst authorship.
† corresponding author.
1the code is available in https://github.com/.
zjunlp/doced..s1: what a brave young womans2: did you hear about the injuries[injure] she sustaineds3: did you hear about the ﬁring[attack] she dids4: she was going to ﬁght[attack] to the death[die]s5: she was captured but she was one tough cookies6: god bless here.
table 1: an example document in ace 2005 corpuswith cross-sentence semantic enhancement and eventinter-dependency.
speciﬁcally, semantic informationof s2 provides latent information to enhance s3, andattack event in s4 also contributes to s3..shen et al., 2020).
in ace 2005 corpus, the argu-ments of a single event instance may be scattered inmultiple sentences (zheng et al., 2019; ebner et al.,2019), which indicates that document-level infor-mation aggregation is critical for ed task.
what’smore, a word in different contexts would expressdifferent meanings and trigger different events.
forexample, in table 1, “ﬁring” in s3 means the ac-tion of ﬁring guns (attack event) or forcing some-body to leave their job (end position event).
tospecify its event type, cross-sentence informationshould be considered.
b).
intra-sentence andinter-sentence event inter-dependency model-ing (liao and grishman, 2010; chen et al., 2018;liu et al., 2018).
for s4 in table 1, an attack eventis triggered by “ﬁght”, and a die event is triggeredby “death”.
this kind of event co-occurrence iscommon in ace 2005 corpus, we investigated thedataset and found that about 44.4% of the triggersappeared in this way.
the cross-sentence eventco-occurrence shown in s4 and s3 is also very com-mon.
therefore, modeling the sentence-level anddocument-level event inter-dependency is crucialfor jointly detecting multiple events..to address those issues, previous approaches(chen et al., 2015; nguyen et al., 2016; liu et al.,2018; yan et al., 2019; liu et al., 2019; zhang et al.,2019) mainly focused on sentence-level event de-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4829–4839august1–6,2021.©2021associationforcomputationallinguistics4829tection, neglecting the document-level event inter-dependency and semantic information.
some stud-ies (chen et al., 2018; zhao et al., 2018) tried tointegrate semantic information across sentencesvia the attention mechanism.
for the document-level event inter-dependency modeling, liao andgrishman (2010) extended the features with eventtypes to capture dependencies between differentevents in a document.
although great progress hasbeen made in ed task due to recent advances indeep learning, there is still no uniﬁed frameworkto model the document-level semantic informationand event inter-dependency..we try to analyze the ace 2005 data to re-understand the challenges encountered in ed task.
firstly, we ﬁnd that event detection is essentiallya special seq2seq task, in which the source se-quence is a given document or sentence, and theevent tag sequence is target of task.
seq2seq taskscan be effectively modeled via the rnn-basedencoder-decoder framework, in which the encodercaptures rich semantic information, while the de-coder generates a sequence of target symbols withinter-dependency been captured.
this separate en-coder and decoder framework can correspondinglydeal with the semantic aggregation and event inter-dependency modeling challenges in ed task.
sec-ondly, for the propagation of cross-sentence in-formation, we ﬁnd that the relevant informationis mainly stored in several neighboring sentences,while little is stored in distant sentences.
for ex-ample, as shown in table 1, it seems that s2 and s4contribute more to s3 than s1 and s5..in this paper, we propose a novel multi-layerbidirectional network (mlbinet) for ed task.
a bidirectional decoder layer is ﬁrstly devisedto decode the event tag vector corresponding toeach token with forward and backward event inter-dependency been captured.
then, the event-relatedinformation in the sentence is summarized througha sentence information aggregation module.
fi-nally, the multiple bidirectional tagging layersstacking mechanism is proposed to propagate cross-sentence information between adjacent sentences,and capture long-range information as the increas-ing of layers.
we conducted experimental studieson ace 2005 corpus to demonstrate its beneﬁts incross-sentence joint event detection.
our contribu-tions are summarized as follows:.
• we propose a novel bidirectional decodermodel to explicitly capture bidirectional event.
inter-dependency within a sentence, alleviat-ing long-range forgetting problem of tradi-tional tagging structure;.
• we propose a model called mlbinet to prop-agate semantic and event inter-dependencyinformation across sentences and detect mul-tiple events collectively;.
• we achieve the best performance (f1 value)on ace 2005 corpus, surpassing the state-of-the-art by 1.9 points..2 approach.
generally, event detection on ace 2005 corpusis treated as a classiﬁcation problem, which isto determine whether it forms a part of an eventtrigger.
speciﬁcally, for a given document d ={s1, .
.
.
, sn}, where si = {wi,1, .
.
.
, wi,ni} de-notes the i-th sentence containing ni tokens.
weare required to predict the triggered event type se-quence yi = {yi,1, .
.
.
, yi,ni} based on contextualinformation of d. without ambiguity, we omit thesubscript i..for a given sentence, the event tags correspond-ing to tokens are associated, which is important forcollectively detecting multiple events (chen et al.,2018; liu et al., 2018).
the way tokens are clas-siﬁed independently will miss the association.
inorder to capture the event inter-dependency, thesequential information of event tag should be re-tained.
intuitively, the ed task can be regardedas event tag sequence generation problem, whichis essentially a seq2seq task.
speciﬁcally, thesource sequence is a given document or sentence,and the event tag sequence to be generated is thetarget sequence.
for instance, for sentence “didyou hear about the injuries she sustained”, thedecoder model is required to generate a tag se-quence [o, o, o, o, o, b injure, o, o], where “o”denotes that the corresponding token is not part ofevent trigger and “b injure” indicates an injureevent is triggered..we introduce the rnn-based encoder-decoderframework for ed task, considering that it is an efﬁ-cient solution for seq2seq tasks.
and we propose amulti-layer bidirectional network called mlbinetshown in figure 1 to deal with the challenges indetecting multiple events collectively.
the modelframework consists of four components: the se-mantic encoder, the bidirectional decoder, the in-formation aggregation module and stacking of mul-.
4830figure 1: the architecture of our multi-layer bidirectional network (mlbinet).
the red arrow represents the inputi−1 ; ik−1of semantic representation xt, the green arrow represents the input of adjacent sentences information [ik−1i+1 ]integrated in the previous layer, and the blue arrow represents the input of forward event tag vector..tiple bidirectional tagging layers.
we ﬁrstly intro-duce the encoder-decoder framework and discussits compatibility with the ed task..the attention mechanism to dynamically changingcontext vector ct in the decoding process, where ctcan be uniformly expressed as.
2.1 encoder–decoder.
the rnn-based encoder-decoder framework (choet al., 2014; sutskever et al., 2014; bahdanau et al.,2015; luong et al., 2015; gu et al., 2016) consistsof two components: a) an encoder which convertsthe source sentence into a ﬁxed length vector c andb) a decoder is to unfold the context vector c intothe target sentence.
as is formalized in (gu et al.,2016), the source sentence si is converted into aﬁxed length vector c by the encoder rnn,.
ht = f (ht−1, wt), c = φ({h1, .
.
.
, hni}).
where f is the rnn function, {ht} are the rnnstates, wt is the t-th token of source sentence, cis the so-called context vector, and φ summarizesthe hidden states, e.g.
choosing the last state hni.
and the decoder rnn translates c into the targetsentence according to:.
st = f (yt−1, st−1, c)p(yt|y<t, si) = g(yt−1, st, c).
(1).
where st is the state at time t, yt is the predictedsymbol at time t, g is a classiﬁer over the vocabu-lary, and y<t denotes the history {y1, .
.
.
, yt−1}..studies (bahdanau et al., 2015; luong et al.,2015) have shown that summarizing the entiresource sentence into a ﬁxed length vector will limitthe performance of the decoder.
they introduced.
ct =.
αtτ hτ.
(2).
ni(cid:88).
τ =1.
where αtτ is the contribution weight of τ -th sourcetoken’s state to context vector at time t, hτ denotesthe representation of τ -th token..we introduce the encoder-decoder frameworkto model ed task, mainly considering the follow-ing advantages: a) the separate encoder moduleis ﬂexible in fusing sentence-level and document-level semantic information and b) the rnn decodermodel (1) can capture sequential event tag depen-dency as the predicted tag vectors before t will beused as input for predicting t-th symbol..the encoder-decoder framework for ed task isslightly different from the general seq2seq task asfollows: a) for ed task, the length of event tagsequence (target sequence) is known because itselements correspond one-to-one with tokens in thesource sequence.
however, the length of target se-quence in the general seq2seq task is unknown.
b)the vocabulary of decoder for ed task is a collec-tion of event types, instead of words..2.2 semantic encoder.
in this module, we encode the sentence-level con-textual information for each token with bidirec-tional lstm (bilstm) and self-attention mech-anism.
firstly, each token is transformed into.
4831comprehensive representation by concatenating itsword embedding and ner type embedding.
theword embedding matrix is pretrained by skip-grammodel (mikolov et al., 2013), and the ner typeembedding matrix is randomly initialized and up-dated in the training process.
for a given token wt,its embedded vector is denoted as et..we apply the bilstm (zaremba and sutskever,2014) model for sentence-level semantic encod-ing, which can effectively capture sequentialand contextual information for each token.
thebilstm architecture is composed of a forward−→h t =i.e.,lstm and a backward lstm,←−←−−−−−−−−→lstm(h t+1, et).
af-lstm(ter encoding, the contextual representation of eachtoken is ht = [.
−→h t−1, et),.
←−h t =.
←−h t]..−→h t;.
attention mechanism between tokens withina sentence has been proven to further integratelong-range contextual semantic information.
foreach token wt, its contextual representation is theweighted average of the semantic information ofall tokens in the sentence.
we apply the attentionmechanism proposed by (luong et al., 2015) withthe weights derived by.
forward decoderin addition to the semanticcontext vector ct = xt, the event information pre-viously involved can help determine the event typetriggered by t-th token.
this kind of associationcan be captured by the forward decoder model:.
→s t = ffw(.
→s t−1, xt).
→y t−1,→s t + by).
→y t = ˜f (wy.
(4).
→s t} are the stateswhere ffw is the forward rnn, {→of forward rnn, {y t} are the forward event tagvectors.
compared with general decoder (1), theclassiﬁer g(·) over vocabulary is replaced with atransformation ˜f (·) (identity function, tanh, sig-moid, etc.)
to obtain the event tag vector..backward decoder considering the associatedevents may also be mentioned later, we devise abackward decoder to capture this kind of depen-dency as follows:.
←s t = fbw(.
←s t+1, xt).
←y t+1,←s t + by).
←y t = ˜f (wy.
(5).
αt,j =.
exp(zt,j)m=1 exp(zt,m).
(cid:80)ni.
zt,m = tanh(h(cid:62).
t wsahm + bsa).
(3).
where fbw is the backward rnn, {states of backward rnn, {event tag vectors..←s t} are the←y t} are the backward.
and the contextual representation of wt is ha(cid:80)ni.
t =j=1 αt,jhj.
by concatenating its lexical embed-ding and contextual representation, we get the ﬁnalcomprehensive semantic representation of wt asxt = [ha.
t ; et]..2.3 bidirectional decoder.
the decoder layer for ed task is to generate a se-quence of event tags corresponding to tokens.
asis noted, the tag sequence (target sequence) ele-ments and tokens (source sequence) are in one-to-one correspondence.
therefore, the context vec-tor c shown in (1) and (2) can be personalizeddirectly by ct = xt, which is equivalent to atten-tion with degenerate weights.
that is, αtt = 1 andαtτ = 0, ∀τ (cid:54)= t..in traditional seq2seq tasks, the target sequencelength is unknown during the inference process,so only the forward decoder is feasible.
however,for the ed task, the length of the target sequenceis known when given source sequence.
thus, wedevise a bidirectional decoder to model event inter-dependency within a sentence..→y t andbidirectional decoder by concatenating←←y t] withy t, we get the event tag vector yt = [bidirectional event inter-dependency been captured.
the semantic and event-related entity informationis also carried by yt as xt is an indirect input..→y t;.
an alternative method modeling the sentence-level event inter-dependency called hierarchicaltagging layer is proposed by (chen et al., 2018).
the bidirectional decoder is quite different fromthe hierarchical tagging layer as follows:.
• the bidirectional decoder models event inter-dependency immediately by combining a for-ward and a backward decoder.
the hierar-chical tagging layer utilizes two forward de-coders and the tag attention mechanism tocapture bidirectional event inter-dependency..• in the bidirectional decoder, the ed task isformalized as a special seq2seq task, whichcan simplify the event inter-dependency mod-eling problem and cross-sentence informationpropagation problem discussed below..4832the bidirectional rnn decoder unfolds theevent tag vector corresponding to each token, andcaptures the bidirectional event inter-dependencywithin the sentence.
to propagate informationacross sentences, we need to ﬁrstly aggregate use-ful information of each sentence..2.4.information aggregation.
for current sentence si, the information we areconcerned about can be summarized as record-ing which entities and tokens trigger which events.
thus, to summarize the information, we devise an-other lstm layer (information aggregation mod-ule shown in figure 1) with the event tag vector ytas input.
the information at t-th token is computedby.
˜it =.
−−−−→lstm(˜it−1, yt).
(6).
we choose the last state ˜ini as the summary infor-mation, which is ii = ˜ini..the sentence-level information aggregation mod-ule bridges the information across sentences, asthe well-formalized information can be easily inte-grated into the decoding process of other sentences,enhancing the event-related signal..2.5 multi-layer bidirectional network.
in this module, we introduce a multiple bidirec-tional tagging layers stacking mechanism to ag-gregate information of adjacent sentences into thebidirectional decoder, and propagate informationacross sentences.
the information ({yt}, ii) ob-tained by the bidirectional decoder layer and infor-mation aggregation module has captured the eventrelevant information within a sentence.
however,the cross-sentence information has not yet inter-acted.
for a given sentence, as we can see in table1, its relevant information is mainly stored in sev-eral neighboring sentences, while distant sentencesare rarely relevant.
thus, we propose to transmitthe summarized sentence information ii amongadjacent sentences..for the decoder framework shown in (4) and(5), the cross-sentence information can be inte-grated by extending the input with ii−1 and ii+1.
further, we introduce a multiple bidirectional tag-ging layers stacking mechanism shown in fig-ure 1 to iteratively aggregate information of ad-jacent sentences.
the overall framework is namedmulti-layer bidirectional network (mlbinet).
as shown in figure 1, a bidirectional tagging layer.
is composed of a bidirectional decoder and an in-formation aggregation module.
for sentence si, theoutputs of k-th layer can be computed by.
i−1 , ik−1i+1 )i−1 , ik−1i+1 ).
→y.
→s t−1, xt, ik−1←s t+1, xt, ik−1.
k→s t = ffw(t−1,k←←ys t = fbw(t+1,k→→t = ˜f (wys t + by)yk←t = ˜f (wys t + by)kk→ykt ]t ;y.t = [.
←y.
←y.
(7).
where ik−1i−1 is the sentence information of si−1 ag-gregated in (k-1)-th layer, and {ykt } are event tagvectors obtained in k-th layer.
the equation sug-gests that for each token of source sentence si,the input of cross-sentence information is identi-cal [ik−1it is reasonable as their cross-sentence information available is the same for eachtoken of current sentence..i−1 , ik−1i+1 ]..the iteration process shown in equation (7) isactually an evolutionary diffusion of the cross-sentence semantic and event information in the doc-ument.
speciﬁcally, in the ﬁrst tagging layer, infor-mation of current sentence is effectively modeledby the bidirectional decoder and information ag-gregation module.
in the second layer, informationof adjacent sentences is propagated to current sen-tence by plugging in i1i+1 to the decoder.
in general, in the k-th (k ≥ 3) layer, since si−1has captured the information of sentence si−k+1in the (k-1)-th layer, then si can obtain informa-tion in si−k+1 by acquiring the information in si−1.
thus, as the number of decoder layers increases,the model will capture information from distant sen-tences.
for k-layer bidirectional tagging model,the sentence information with the longest distanceof k-1 can be captured..i−1 and i1.
k=1 αk−1yk.
we deﬁne the ﬁnal event tag vector of wt asthe weighted sum of {ykt }k in different layers, i.e.,t = (cid:80)kydt , where α ∈ (0, 1] is a weightdecay parameter.
it means that cross-sentence infor-mation can supplement to the current sentence, andthe contribution gradually decreases as the distanceincreases when α < 1..we note that the parameters of bidirectional de-coder and information aggregation module at dif-ferent layers can be shared, because they encodeand propagate the same structured information.
inthis paper, we set the parameters of different layersto be the same..48332.6 loss function.
in order to train the networks, we minimize thenegative log-likelihood loss function j(θ),.
j(θ) = −.
(cid:88).
(cid:88).
(cid:88).
d∈d.
s∈d.
wt∈s.
log p(oyt.
t |d; θ).
(8).
where d denotes training documents set.
the tagprobability for token wt is computed by.
methodsdmcnnhbtngmajmeedmbert-bootmoganedss-vq-vaemlbinet (1-layer)mlbinet (2-layer)mlbinet (3-layer).
p75.677.976.377.979.575.774.174.274.7.r63.669.171.372.572.377.878.583.783.0.f169.173.373.775.175.776.776.278.678.6.p(oj.
t |d; θ) = exp(oj.
t )/.
exp(omt ).
(9).
table 2: performance comparison of different methodson the test set with gold-standard entities..ot = woyd.
t + bom(cid:88).
m=1.
where m is the number of event classes, p(ojt |d; θ)is the probability that assigning event type j totoken wt in document d when parameter is θ..3.2 baselines.
3 experiments.
3.1 dataset and settings.
we performed extensive experimental studies onthe ace 2005 corpus to demonstrate the effective-ness of our method on ed task.
it deﬁnes 33 typesof events and an extra none type for the non-trigger tokens.
we formalize it as a task to generatea sequence of 67-class event tag (with bio taggingschema).
the data splitting for training, validationand testing follows (ji and grishman, 2008; chenet al., 2015; liu et al., 2018; chen et al., 2018;huang and ji, 2020), where the training set con-tains 529 documents, the validation set contains30 documents and the remaining 40 documents areused as testing set..we evaluated the performance of three multi-layer settings with 1-, 2- and 3-layer mlbinet,respectively.
we use the adam (kingma and ba,2017) for optimization.
in all three settings, we cutevery 8 consecutive sentences into a new documentand padding when needed.
each sentence is trun-cated or padded to make it 50 in length.
we set thedimension of word embedding as 100, the dimen-sion of golden ner type and subtype embeddingas 20. we set the dropout rate as 0.5 and penaltycoefﬁcient as 2 ∗ 10−5 to avoid overﬁtting.
thehidden size of semantic encoder layer and decoderlayer is set to 100 and 200, respectively.
the sizeof forward and backward event tag vectors is set to100. and we set the batch size as 64, the learningrate as 5 ∗ 10−4 with decay rate 0.99, the weightdecay parameter α as 1.0. the results we reportare the average of 10 trials..for comparison, we investigated the performanceof the following state-of-the-art methods: 1) dm-cnn (chen et al., 2015), which extracts multi-ple events from one sentence with dynamic multi-pooling cnn; 2) hbtngma (chen et al., 2018),which models sentence event inter-dependency viaa hierarchical tagging model; 3) jmee (liu et al.,2018), which models the sentence-level event inter-dependency via a graph model of the sentence syn-tactic parsing graph; 4) dmbert-boot (wanget al., 2019), which augments the training datawith external unlabeled data by adversarial mech-anism; 5) moganed (yan et al., 2019), whichuses graph convolution network with aggregativeattention to explicitly model and aggregate multi-order syntactic representations; 6) ss-vq-vae(huang and ji, 2020), which learns to induct newevent type by a semi-supervised vector quantizedvariational autoencoder framework, and ﬁne-tuneswith the pre-trained bert-large model..3.3 overall performance.
table 2 presents the overall performance compari-son between different methods with gold-standardentities.
as shown, under 2-layer and 3-layer set-tings, our proposed model mlbinet achieves bet-ter performance, surpassing the current state-of-the-art by 1.9 points.
more speciﬁcally, our modelsachieve higher recalls by at least 0.7, 5.9 and 5.2points, respectively..the powerful encoder of bert pre-trainedmodel (devlin et al., 2018) has been proven to im-prove the performance of downstream nlp tasks.
the 2-layer mlbinet outperforms bert-boot(bert-base) and ss-vq-vae (bert-large) by3.5 and 1.9 points, respectively.
it proves the im-.
4834methodsdmcnnhbtngmajmeemlbinet (1-layer)mlbinet (2-layer)mlbinet (3-layer).
1/174.378.475.277.980.680.3.
1/n50.959.572.775.177.177.4.all69.173.373.776.278.678.6.methodsbackwardforwardbidirectional.
1-layer72.272.876.2.
2-layer75.076.078.6.
3-layer75.576.578.6.table 4: the performance of our proposed method withdifferent multi-layer settings or decoder methods..table 3: system performance on single event sen-tences (1/1) and multiple event sentences (1/n).
1/1means one sentence that has one event; otherwise, 1/nis used.
“all” means all test data are included..portance of event inter-dependency modeling andcross-sentence information integration for ed task.
when only information of current sentence isavailable, the 1-layer mlbinet outperforms hbt-ngma by 2.9 points.
it proves that the hierarchicaltagging mechanism adopted by hbtngma is notas effective as the bidirectional decoding mecha-nism we proposed.
intuitively, the bidirectional de-coder models event inter-dependency explicitly bya forward decoder and a backward decoder, whichis more efﬁcient than hierarchies..3.4 effect on extracting multiple events.
the existing event inter-dependency modelingmethods (chen et al., 2015, 2018; liu et al., 2018)aim to extract multiple events jointly within a sen-tence.
to demonstrate that sentence-level eventinter-dependency modeling beneﬁts from cross-sentence information propagation, we evaluatedthe performance of our model in single event ex-traction (1/1) and multiple events joint extraction(1/n).
1/1 means one sentence that has one event;otherwise, 1/n is used..the experimental results are presented in ta-ble 3. as shown, we can verify the importanceof cross-sentence information propagation mech-anism and bidirectional decoder in sentence-levelmultiple events joint extraction based on the fol-lowing results: a) when only the current sentenceinformation is available, the 1-layer mlbinet out-performs existing methods at least by 2.4 points in1/n case, which proves the effectiveness of bidirec-tional decoder we proposed; b) for ours 2-layer and3-layer models, their performance in both 1/1 and1/n cases surpasses the current methods by a largemargin, which proves the importance of propagat-ing information across sentences for single eventand multiple events extraction.
we conclude that it.
methodsbaseline (1-layer)average (2-layer)concat (2-layer)lstm (2-layer).
p74.174.575.074.2.r78.582.582.683.7.f176.278.378.678.6.table 5: the performance of mlbinet with differentkinds of information aggregation mechanisms..is the propagating information across sentences andbidirectional decoder which make cross-sentencejoint event detection successful..3.5 analysis of decoder layer.
table 4 presents the performance of the modelin three decoder mechanisms: forward, backwardand bidirectional decoder, as well as three multi-layer settings.
we can reach the following con-clusions: a) under three decoder mechanisms, theperformance of the proposed model will be signiﬁ-cantly improved as the number of decoder layersincreases; b) the bidirectional decoder dominatesboth forward decoder and backward decoder, andforward decoder dominates backward decoder; c)the information propagation across sentences willenhance event relevant signal regardless of the de-coder mechanism applied.
among the three de-coder models, the bidirectional decoder performsbest because of its ability in capturing bidirectionalevent inter-dependency, which proves both the for-ward and backward decoders are critical for eventinter-dependency modeling..3.6 analysis of aggregation model.
in information aggregation module, we introducea lstm shown in (6) to aggregate sentence infor-mation, and then propagate to other sentences viathe bidirectional decoder.
we compare other ag-gregation methods: a) concat means the sentenceinformation is aggregated by simply concatenatingthe ﬁrst and last event tag vector of the sentence,and b) average means the sentence information isaggregated by averaging the event tag vectors oftokens in the sentence.
the experimental results.
4835are presented in table 5..compared with the baseline 1-layer model, otherthree 2-layer settings equipped with information ag-gregation and cross-sentence propagation performsbetter.
it proves that sentence information aggrega-tion module can integrate some useful informationand propagate it to other sentences through the de-coder.
on the other hand, the performance of lstmand concat are comparable and stronger than aver-age.
considering that the input of the informationaggregation module is the event tag vector obtainedby the bidirectional decoder, which has capturedthe sequential event information.
therefore, it isnot surprising that lstm does not have that greatadvantage over concat and average..4 related work.
event detection is a well-studied task with researcheffort in the last decade.
the existing methods(chen et al., 2015; nguyen and grishman, 2015;liu et al., 2017; nguyen and grishman, 2018; denget al., 2020; tong et al., 2020; lai et al., 2020; liuet al., 2020; li et al., 2020; cui et al., 2020; denget al., 2021; shen et al., 2021) mainly focus onsentence-level event trigger extraction, neglectingthe document information.
or the document-levelsemantic and event inter-dependency informationare modeled separately..for the problem of event inter-dependency mod-eling, some methods were proposed to jointly ex-tract triggers within a sentence.
among them, chenet al.
(2015) used dynamic multi-pooling cnn topreserve information of multiple events; nguyenet al.
(2016) utilized the bidirectional recurrent neu-ral networks to extract events; liu et al.
(2018)introduced syntactic shortcut arcs to enhance in-formation ﬂow and used graph neural networks tomodel graph information; chen et al.
(2018) pro-posed a hierarchical tagging lstm layer and tag-ging attention mechanism to model the event inter-dependency within a sentence.
considering thatadjacent sentences also store some relevant eventinformation, which would enhance the event sig-nals of other sentences.
these methods would missthe event inter-dependency information across sen-tences.
for document-level event inter-dependencymodeling, lin et al.
(2020) proposed to incorporateglobal features to capture the cross-subtask andcross-instance interactions..the deep learning methods on document-levelsemantic information aggregation are primarily.
based on multi-level attention mechanism.
chenet al.
(2018) integrated document information by in-troducing a multi-level attention.
zhao et al.
(2018)used trigger and sentence supervised attention to ag-gregate information and enhance the sentence-levelevent detection.
zheng et al.
(2019) utilized thememory network to store document level contex-tual information and entities.
some feature-baseddocument level information aggregation methodswere proposed by (ji and grishman, 2008; liaoand grishman, 2010; hong et al., 2011; huang andriloff, 2012; reichart and barzilay, 2012; lu androth, 2012).
and zhang et al.
(2020) proposed toaggregate the document-level information by latenttopic modeling.
the attention-based document-level information aggregation mechanisms treat allsentences in the document equally, which may in-troduce some noises from distant sentences.
andthe feature-based methods require extensive humanengineering, which also greatly affects the portabil-ity of the model..5 conclusions.
this paper presents a novel multi-layer bidirec-tional network (mlbinet) to propagate document-level semantic and event inter-dependency infor-mation for event detection task.
to the best of ourknowledge, this is the ﬁrst work to unify them inone model.
firstly, a bidirectional decoder is pro-posed to explicitly model the sentence-level eventinter-dependency, and event relevant informationwithin a sentence is aggregated by an informationaggregation module.
then the multiple bidirec-tional tagging layers stacking mechanism is devisedto iteratively propagate semantic and event-relatedinformation across sentence.
we conducted exten-sive experiments on the widely-used ace 2005corpus, the results demonstrate the effectiveness ofour model, as well as all modules we proposed..in the future, we will extend the model to theevent argument extraction task and other informa-tion extraction tasks, where the document-levelsemantic aggregation and object inter-dependencyare critical.
for example, the recently concerneddocument-level relation extraction (quirk andpoon, 2017; yao et al., 2019), which requires read-ing multiple sentences in a document to extractentities and infer their relations by synthesizing allinformation of the document.
for other sequencelabeling tasks, such as the named entity recogni-tion, we can also utilize the proposed architecture.
4836to model the entity label dependency..acknowledgments.
we wantto express gratitude to the anony-mous reviewers for their hard work and kindcomments.
this work is funded by ns-fcu19b2027/91846204, national key r&d pro-gram of china (funding no.sq2018yfc000004)..references.
dzmitry bahdanau, kyunghyun cho, and yoshuaneural machine translation bycorr,.
bengio.
2015.jointly learning to align and translate.
abs/1409.0473..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing, pages 167–176..yubo chen, hang yang, kang liu, jun zhao, and yan-tao jia.
2018. collective event detection via a hier-archical and bias tagging networks with gated multi-in proceedings of thelevel attention mechanisms.
2018 conference on empirical methods in naturallanguage processing, pages 1267–1276..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder-decoderfor statistical machine translation.
proceedings ofthe 2014 conference on empirical methods in natu-ral language processing (emnlp)..shiyao cui, bowen yu, tingwen liu, zhenyu zhang,xuebin wang, and jinqiao shi.
2020.edge-enhanced graph convolution networks for event de-tection with syntactic relation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing: findings, pages 2329–2339..shumin deng, ningyu zhang, jiaojian kang, yichizhang, wei zhang, and huajun chen.
2020. meta-learning with dynamic-memory-based prototypicalin proceed-network for few-shot event detection.
ings of the 13th international conference on websearch and data mining, pages 151–159..shumin deng, ningyu zhang, luoqiu li, hui chen,huaixiao tou, mosha chen, fei huang, and huajunchen.
2021. ontoed: low-resource event detectionwith ontology embedding.
in acl.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deep.
bidirectional transformers for language understand-ing.
proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long and short papers)..seth ebner, patrick xia, ryan culkin, kyle rawlins,and benjamin van durme.
2019. multi-sentence ar-gument linking.
arxiv preprint arxiv:1911.03766..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1631–1640..yu hong, jianfeng zhang, bin ma, jianmin yao,guodong zhou, and qiaoming zhu.
2011. usingcross-entity inference to improve event extraction.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 1127–1136..lifu huang and heng ji.
2020..semi-supervisednew event type induction and event detection.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 718–724..ruihong huang and ellen riloff.
2012. modeling tex-tual cohesion for event extraction.
proceedings ofthe 26th conference on artiﬁcial intelligence..heng ji and ralph grishman.
2008. reﬁning event ex-traction through cross-document inference.
in pro-ceedings of acl-08: hlt, pages 254–262..diederik p. kingma and jimmy ba.
2017. adam: a.method for stochastic optimization..viet dac lai, tuan ngo nguyen, and thien huunguyen.
2020. event detection: gate diversity andsyntactic importance scoresfor graph convolutionneural networks.
arxiv preprint arxiv:2010.14123..fayuan li, weihua peng, yuguang chen, quan wang,lu pan, yajuan lyu, and yong zhu.
2020. eventextraction as multi-turn question answering.
inproceedings of the 2020 conference on empiricalmethods in natural language processing: findings,pages 829–838..shasha liao and ralph grishman.
2010. using doc-ument level cross-event inference to improve eventextraction.
in proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 789–797..ying lin, heng ji, fei huang, and lingfei wu.
2020.a joint neural model for information extraction withglobal features.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 7999–8009..4837jian liu, yubo chen, and kang liu.
2019. exploit-ing the ground-truth: an adversarial imitation basedknowledge distillation approach for event detection.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 6754–6761..jian liu, yubo chen, kang liu, wei bi, and xiaojiangliu.
2020. event extraction as machine reading com-prehension.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 1641–1651..shulin liu, yubo chen, kang liu, and jun zhao.
2017.exploiting argument information to improve eventdetection via supervised attention mechanisms.
inmeeting of the association for computational lin-guistics, pages 1789–1798..xiao liu, zhunchen luo, and heyan huang.
2018.jointly multiple events extraction via attention-based graph information aggregation.
proceedingsof the 2018 conference on empirical methods innatural language processing..wei lu and dan roth.
2012. automatic event extrac-in pro-tion with structured preference modeling.
ceedings of the 50th annual meeting of the associa-tion for computational linguistics, pages 835–844..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, pages 1412–1421..tomas mikolov, kai chen, gregory s. corrado, andjeffrey dean.
2013. efﬁcient estimation of word rep-international confer-resentations in vector space.
ence on learning representations, abs/1301.3781..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentin proceedings of the 2016 con-neural networks.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 300–309..thien huu nguyen and ralph grishman.
2015. eventdetection and domain adaptation with convolutionalneural networks.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing, pages 365–371..thien huu nguyen and ralph grishman.
2018. graphconvolutional networks with argument-aware pool-ing for event detection.
in proceedings of the thirty-second aaai conference on artiﬁcial intelligence,pages 5900–5907..chris quirk and hoifung poon.
2017. distant super-vision for relation extraction beyond the sentenceboundary.
proceedings of the 15th conference ofthe european chapter of the association for com-putational linguistics..roi reichart and regina barzilay.
2012. multi eventextraction guided by global constraints.
in proceed-ings of the 2012 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, pages 70–79..shirong shen, guilin qi, zhen li, sheng bi, andlusheng wang.
2020. hierarchical chinese legalevent extraction via pedal attention mechanism.
inproceedings of the 28th international conference oncomputational linguistics, pages 100–113..shirong shen, tongtong wu, guilin qi, yuan-fang li,gholamreza haffari, and sheng bi.
2021. adap-tive knowledge-enhanced bayesian meta-learningin findings of acl.
for few-shot event detection.
association for computational linguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems, pages 3104–3112..meihan tong, bin xu, shuai wang, yixin cao, leihou, juanzi li, and jun xie.
2020. improving eventdetection via open-domain trigger knowledge.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5887–5897..xiaozhi wang, xu han, zhiyuan liu, maosong sun,and peng li.
2019. adversarial training for weaklyin proceedings of thesupervised event detection.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 998–1008..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5770–5774..yuan yao, deming ye, peng li, xu han, yankai lin,zhenghao liu, zhiyuan liu, lixin huang, jie zhou,and maosong sun.
2019. docred: a large-scaledocument-level relation extraction dataset.
proceed-ings of the 57th annual meeting of the associationfor computational linguistics..wojciech zaremba and ilya sutskever.
2014. learning.
to execute.
arxiv preprint arxiv:1410.4615..junchi zhang, mengchi liu, and yue zhang.
2020.topic-informed neural approach for biomedicalevent extraction.
artiﬁcial intelligence in medicine,103:101783..tongtao zhang, heng ji, and avirup sil.
2019. jointentity and event extraction with generative adversar-ial imitation learning.
data intelligence, 1(2):99–120..4838yue zhao, xiaolong jin, yuanzhuo wang, and xueqicheng.
2018. document embedding enhanced eventdetection with hierarchical and supervised attention.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics, pages 414–419..shun zheng, wei cao, wei xu, and jiang bian.
2019.doc2edag: an end-to-end document-level frame-work for chinese ﬁnancial event extraction.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 337–346..4839