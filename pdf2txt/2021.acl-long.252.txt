joint models for answer veriﬁcation in question answering systems.
zeyu zhang∗ thuy vu alessandro moschittischool of information, the university of arizona, tucson, az, usaamazon alexa ai, manhattan beach, ca, usazeyuzhang@email.arizona.edu, {thuyvu, amosch}@amazon.com.
abstract.
this paper studies joint models for selectingcorrect answer sentences among the top k pro-vided by answer sentence selection (as2) mod-ules, which are core components of retrieval-based question answering (qa) systems.
ourwork shows that a critical step to effectively ex-ploiting an answer set regards modeling the in-terrelated information between pair of answers.
for this purpose, we build a three-way multi-classiﬁer, which decides if an answer supports,refutes, or is neutral with respect to anotherone.
more speciﬁcally, our neural architectureintegrates a state-of-the-art as2 module withthe multi-classiﬁer, and a joint layer connectingall components.
we tested our models on wik-iqa, trec-qa, and a real-world dataset.
theresults show that our models obtain the newstate of the art in as2..1.introduction.
automated question answering (qa) research hasreceived a renewed attention thanks to the diffusionof virtual assistants.
among the different types ofmethods to implement qa systems, we focus onanswer sentence selection (as2) research, orig-inated from trec-qa track (voorhees and tice,1999), as it proposes efﬁcient models that are moresuitable for a production setting, e.g., they are moreefﬁcient than those developed in machine reading(mr) work (chen et al., 2017)..garg et al.
(2020) proposed the tanda ap-proach based on pre-trained transformer models,obtaining impressive improvement over the stateof the art for as2, measured on the two most useddatasets, wikiqa (yang et al., 2015) and trec-qa (wang et al., 2007).
however, tanda wasapplied only to pointwise rerankers (pr), e.g., sim-ple binary classiﬁers.
bonadiman and moschitti.
∗work done while the author was an intern at amazon.
alexa.
claim:ev1:.
ev2:.
ev3:.
joe walsh was inducted in 2001.as a member of the eagles, walsh was inducted into therock and roll hall of fame in 1998, and into the vocalgroup hall of fame in 2001.joseph fidler walsh (born november 20, 1947) isan american singer songwriter,composer, multi-instrumentalist and record producer.
walsh was awarded with the vocal group hall of famein 2001..table 1: a claim veriﬁcation example from fever..(2020) tried to improve this model by jointly mod-eling all answer candidates with listwise methods,e.g., (bian et al., 2017).
unfortunately, mergingthe embeddings from all candidates with standardapproaches, e.g., cnn or lstm, did not improveover tanda..a more structured approach to building jointmodels over sentences can instead be observed infact veriﬁcation systems, e.g., the methods de-veloped in the fever challenge (thorne et al.,2018a).
such systems take a claim, e.g., joe walshwas inducted in 2001, as input (see tab.
1), andverify if it is valid, using related sentences calledevidences (typically retrieved by a search engine).
for example, ev1, as a member of the eagles,walsh was inducted into the rock and roll hall offame in 1998, and into the vocal group hall offame in 2001, and ev3, walsh was awarded withthe vocal group hall of fame in 2001, support theveracity of the claim.
in contrast, ev2 is neutralas it describes who joe walsh is but does not con-tribute to establish the induction.
we conjecturethat supporting evidence for answer correctness inas2 task can be modeled with a similar rationale.
in this paper, we design joint models for as2based on the assumption that, given q and a tar-get answer candidate t, the other answer candi-dates, (c1, ..ck) can provide positive, negative, orneutral support to decide the correctness of t. ourﬁrst approach exploits fact checking research: weadapted a state-of-the-art fever system, kgat(liu et al., 2020), for as2.
we deﬁned a claim as.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3252–3262august1–6,2021.©2021associationforcomputationallinguistics3252a pair constituted of the question and one targetanswer, while considering all the other answers asevidences.
we re-trained and rebuilt all its embed-dings for the as2 task..our second method, answer support-basedreranker (asr), is completely new, it is based onthe representation of the pair, (q, t), generated bystate-of-the-art as2 models, concatenated with therepresentation of all the pairs (t, ci).
the latter sum-marizes the contribution of each ci to t using a max-pooling operation.
ci can be unrelated to (q, t) sincethe candidates are automatically retrieved, thus itmay introduce just noise.
to mitigate this problem,we use an answer support classiﬁer (asc) to learnthe relatedness between t and ci by classifying theirembedding, which we obtain by applying a trans-former network to their concatenated text.
asctunes the (t, ci) embedding parameters accordingto the evidence that ci provides to t. our answersupport-based reranker (asr) signiﬁcantly im-proves the state of the art, and is also simpler thanour approach based on kgat..our third method is an extension of asr.
itshould be noted that, although asr exploits theinformation from the k candidates, it still producesa score for a target t without knowing the scoresproduced for the other target answers.
thus, wejointly model the representation obtained for eachtarget in a multi-asr (masr) architecture, whichcan then carry out a complete global reasoning overall target answers..we experimented with our models over threedatasets, wikiqa, trec-qa and wqa, wherethe latter is an internal dataset built on anonymizedcustomer questions.
the results show that:.
• asr improves the best current model for as2,i.e., tanda by ∼3%, corresponding to an er-ror reduction of 10% in accuracy, on both wik-iqa and trec-qa..• we also obtain a relative improvement of ∼3%over tanda on wqa, conﬁrming that asris a general solution to design accurate qa sys-tems..• most interestingly, masr improves asr byadditional 2%, conﬁrming the beneﬁt of jointmodeling..finally, it is interesting to mention that masr im-provement is also due to the use of fever data for.
pre-ﬁne-tuning asc, suggesting that the fact veriﬁ-cation inference and the answer support inferenceare similar..2 problem deﬁnition and related workwe consider retrieval-based qa systems, which aremainly constituted by (i) a search engine, retrievingdocuments related to the questions; and (ii) an as2model, which reranks passages/sentences extractedfrom the documents.
the top sentence is typicallyused as ﬁnal answer for the users..2.1 answer sentence selection (as2).
the task of reranking answer-sentence candidatesprovided by a retrieval engine can be modeled witha classiﬁer scoring the candidates.
let q be an ele-ment of the question set, q, and a = {c1, .
.
.
, cn}be a set of candidates for q, a reranker can be de-ﬁned as r : q × π(a) → π(a), where π(a) isthe set of all permutations of a. previous worktargeting ranking problems in the text domain hasclassiﬁed reranking functions into three buckets:pointwise, pairwise, and listwise methods.
pointwise reranking: this approach learnsp(q, ci), which is the probability of ci correctlyanswering q, using a standard binary classiﬁcationsetting.
the ﬁnal rank is simply obtained sorting ci,based on p(q, ci).
previous work estimates p(q, ci)with neural models (severyn and moschitti, 2015),also using attention mechanisms, e.g., compare-aggregate (yoon et al., 2019), inter-weighted align-ment networks (shen et al., 2017), and pre-trainedtransformer models, which are the state of the art.
garg et al.
(2020) proposed tanda, which isthe current most accurate model on wikiqa andtrec-qa.
pairwise reranking: the method considers bi-nary classiﬁers of the form χ(q, ci, cj) for deter-mining the partial rank between ci and cj, then thescoring function p(q, ci) is obtained by summingup all the contributions with respect to the targetcandidate t = ci, e.g., p(q, ci) = (cid:80)j χ(q, ci, cj).
there has been a large body of work precedingtransformer models, e.g., (laskar et al., 2020; tay-yar madabushi et al., 2018; rao et al., 2016).
how-ever, these methods are largely outperformed bythe pointwise tanda model.
listwise reranking: this approach, e.g., (bianet al., 2017; cao et al., 2007; ai et al., 2018), aimsat learning p(q, π), π ∈ π(a), using the informa-tion on the entire set of candidates.
the loss func-tion for training such networks is constituted by the.
3253contribution of all elements of its ranked items.
theclosest work to our research is by bonadiman andmoschitti (2020), who designed several joint mod-els.
these improved early neural networks basedon cnn and lstm for as2, but failed to improvethe state of the art using pre-trained transformermodels..2.2.joint models in question answering.
mr is a popular qa task that identiﬁes an answerstring in a paragraph or a text of limited size for aquestion.
its application to retrieval scenario hasalso been studied (chen et al., 2017; hu et al.,2019; kratzwald and feuerriegel, 2018).
however,the large volume of retrieved content makes theiruse not practical yet.
moreover, the joint model-ing aspect of mr regards sentences from the sameparagraphs.
jin et al.
(2020) use the relation be-tween candidates in multi-task learning approachfor as2.
however, they do not exploit transformermodels, thus their results are rather below the stateof the art..in contrast with the work above, our modelingis driven by an answer support strategy, where thepieces of information are taken from different docu-ments.
this makes our model even more unique; itallows us to design innovative joint models, whichare still not designed in any mr systems..2.3 fact veriﬁcation for question answering.
fact veriﬁcation has become a social need giventhe massive amount of information generated daily.
the problem is, therefore, becoming increasinglyimportant in nlp context (mihaylova et al., 2018).
in qa, answer veriﬁcation is directly relevant dueto its nature of content delivery (mihaylova et al.,2019).
the problem has been explored in mr set-ting (wang et al., 2018).
zhang et al.
(2020a) alsoproposed to fact check for product questions us-ing additional associated evidence sentences.
thelatter are retrieved based on similarity scores com-puted with both tf-idf and sentence-embeddingsfrom pre-trained bert models.
while the processis technically sound, the retrieval of evidence is anexpensive process, which is prohibitive to scale inproduction.
we instead address this problem byleveraging the top answer candidates..3 baseline models for as2.
in this section, we describe our baseline models,which are constituted by pointwise, pairwise, and.
listwise strategies..3.1 pointwise models.
1,...,tokq.
n and c =tokc.
one simple and effective method to build an an-swer selector is to use a pre-trained transformermodel, adding a simple classiﬁcation layer to it,and ﬁne-tuning the model on the as2 task.
specif-ically, q = tokq1,...,tokcmare encoded in the input of the transformer by de-limiting them using three tags: [cls], [sep] and[eos], inserted at the beginning, as separator, andat the end, respectively.
this input is encoded asthree embeddings based on tokens, segments andtheir positions, which are fed as input to severallayers (up to 24).
each of them contains sublayersfor multi-head attention, normalization and feedforward processing.
the result of this transforma-tion is an embedding, e, representing (q, c), whichmodels the dependencies between words and seg-ments of the two sentences..for the downstream task, e is fed (after applyinga non-linearity function) to a fully connected layerhaving weights: w and b. the output layer can beused to implement the task function.
for example, asoftmax can be used to model the probability of thequestion/candidate pair classiﬁcation, as: p(q, c) =sof tmax(w × tanh(e(q, c)) + b)..we can train this model with log cross-entropyloss: l = − (cid:80)l∈{0,1} yl × log(ˆyl) on pairs of texts,where yl is the correct and incorrect answer la-bel, ˆy1 = p(q, c), and ˆy0 = 1 − p(q, c).
train-ing the transformer from scratch requires a largeamount of labeled data, but it can be pre-trainedusing a masked language model, and the next sen-tence prediction tasks, for which labels can be au-tomatically generated.
several methods for pre-training transformer-based language models havebeen proposed, e.g., bert (devlin et al., 2018),roberta (liu et al., 2019), xlnet (yang et al.,2019), albert (lan et al., 2020)..3.2 our joint model baselines.
to better show the potential of our approach andthe complexity of the task, we designed three jointmodel baselines based on: (i) a multiclassiﬁer ap-proach (a listwise method), and (ii) a pairwise jointmodel operating over k + 1 candidates, and ouradaptation of kgat model (a pairwise method)..joint model multi-classiﬁer the ﬁrst baselineis also a transformer-based architecture: we con-catenate the question with the top k + 1 answer can-.
3254didates, i.e., (q[sep ]c1[sep ]c2 .
.
.
[sep ]ck+1),and provide this input to the same transformermodel used for pointwise reranking.
we use the ﬁ-nal hidden vector e corresponding to the ﬁrst inputtoken [cls] generated by the transformer, and aclassiﬁcation layer with weights w ∈ r(k+1)×|e|,and train the model using a standard cross-entropyclassiﬁcation loss: y × log(sof tmax(ew t )),where y is a one-hot vector representing labels forthe k + 1 candidates, i.e., |y| = k + 1. we usea transformer model ﬁne-tuned with the tanda-roberta-base or large models, i.e., robertamodels ﬁne-tuned on asnq (garg et al., 2020).
the scores for the candidate answers are calculatedas (cid:0)p(c1), .., p(ck+1)(cid:1) = sof tmax(ew t ).
then,we rerank ci according their probability..joint model pairwise our second baseline issimilar to the ﬁrst.
we concatenate the questionwith each ci to constitute the (q, ci) pairs, whichare input to the transformer, and we use the ﬁrstinput token [cls] as the representation of each(q, ci) pair.
then, we concatenate the embedding ofthe pair containing the target candidate, (q, t) withthe embedding of all the other candidates’ [cls].
(q, t) is always in the ﬁrst position.
we train themodel using a standard classiﬁcation loss.
at clas-siﬁcation time, we select one target candidate at atime, and set it in the ﬁrst position, followed by allthe others.
we classify all k + 1 candidates anduse their score for reranking them.
it should benoted that to qualify for a pairwise approach, jointmodel pairwise should use a ranking loss.
how-ever, we always use standard cross-entropy loss asit is more efﬁcient and the different is performanceis negligible..joint model with kgat liu et al.
(2020) pre-sented an interesting model, kernel graph atten-tion network (kgat), for fact veriﬁcation: givena claimed fact f , and a set of evidences ev ={ev1, ev2, .
.
.
, evm}, their model carries out jointreasoning over ev, e.g., aggregating informationto estimate the probability of f to be true or false,p(y|f, ev), where y ∈{true, false}..the approach is based on a fully connectedgraph, g, whose nodes are the ni = (f, evi) pairs,and p(y|f, ev) = p(y|f, evi, ev)p(evi|f, ev),where p(y|f, evi, ev) = p(y|ni, g) is the labelprobability in each node i conditioned on the wholegraph, and p(evi|f, ev) = p(ni|g) is the proba-bility of selecting the most informative evidence.
kgat uses an edge kernel to perform a hierarchi-.
cal attention mechanism, which propagates infor-mation between nodes and aggregate evidences..we built a kgat model for as2 as follows: wereplace (i) evi with the set of candidate answersci, and (ii) the claim f with the question and atarget answer pair, (q, t).
kgat constructs theevidence graph g by using each claim-evidencepair as a node, which, in our case, is ((q, t), ci),and connects all node pairs with edges, making ita fully-connected evidence graph.
this way, sen-tence and token attention operate over the triplets,(q, t, ci), establishing semantic links, which canhelp to support or undermine the correctness of t.the original kgat aggregates all the pieces ofinformation we built, based on their relevance, todetermine the probability of t. as we use as2data, the probability will be about the correctnessof t. more in detail, we initialize the node represen-tation using the contextual embeddings obtainedwith two tanda-roberta-base models 1: theﬁrst produces the embedding of (q, t), while thesecond outputs the embedding of (q, ci).
then, weapply a max-pooling operation on these two to getthe ﬁnal node representation.
the rest of the archi-tecture is identical to the original kgat.
finally,at test time, we select one ci at a time, as the targett, and compute its probability, which ranks ci..4.joint answer support models for as2.
we proposed the answer support reranker (asr),which uses an answer pair classiﬁer to provide evi-dence to a target answer t. given a question q, anda subset of its top-k+1 ranked answer candidates,a (reranked by an as2 model), we build a function,σ : q × c × ck → r such that σ(q, t, a \ {t})provides the probability of t to be correct, where cis the set of sentence-candidates.
we also designa multi-classiﬁer masr, which combines k asrmodels, one for each different target answer..4.1 answer support-based reranker (asr).
we developed asr architecture described in fig-ure 1c.
this consists of three main components:.
1. a pointwise reranker (pr), which provides theembedding of the input (q, t), described in fig-ure 1a.
this is essentially the state-of-the-artas2 model based on the tanda approach ap-plied to roberta pre-trained transformer..1https://github.com/alexa/wqa tanda.
3255(a) baseline reranker usingtransformers..(b) pairwise representationusing transformers..(c) answer support reranker.
(d) multi answer support reranker.
figure 1: multi-answer support reranker and its build-ing blocks..2. to reduce the noise that may be introduced byirrelevant ci, we use the answer support classi-ﬁer (asc), which classiﬁes each (t, ci) in oneof the following four classes:.
0 : t and ci are both correct,1 : t is correct while ci is not,2 : vice versa, and3 : both incorrect..this multi-classiﬁer, described in figure 1b, isbuilt on top a roberta transformer, which pro-duced a pairwise representation (pwr).
ascis trained end-to-end with the rest of the net-work in a multi-task learning fashion, using itsspeciﬁc cross-entropy loss, computed with thelabels above..3. the asr (see figure 1c) uses the joint represen-tation of (q, t) with (t, ci), i = 1, .., k, where t.improvment..and ci are the top-candidates reranked by pr.
the k representations are summarized by apply-ing a max-pooling operation, which will aggre-gate all the supporting or not supporting proper-ties of the candidates with respect to the targetanswer.
the concatenation of the pr embed-ding with the max-pooling embedding is givenas input to the ﬁnal classiﬁcation layer, whichscores t with respect to q, also using the infor-mation from the other candidates.
for trainingand testing, we select a t from the k + 1 candi-dates of q at a time, and compute its score.
thisway, we can rerank all the k + 1 candidates withtheir scores..implementation details: asr is a pr that alsoexploits the relation between t and a \ {t}.
we useroberta to generate the [cls] ∈ rd embeddingof (q, t) = et.
we denote with ˆej the [cls]output by another roberta transformer appliedto answer pairs, i.e., (t, cj).
then, we concatenateet to the max-pooling tensor from ˆe1, .., ˆek:.
v = [et : maxpool([ ˆe1, .., ˆek])],(1)where v ∈ r2d is the ﬁnal representation of thetarget answer t. then, we use a standard feed-forward network to implement a binary classiﬁca-tion layer: p(yi|q, t, ck) = sof tmax(v w t + b),where w ∈ r2×2d and b are parameters to trans-form the representation of the target answer t fromdimension 2d to dimension 2, which representscorrect or incorrect labels.
asc labels there can be different interpretationswhen attempting to deﬁne labels for answer pairs.
an alternative to the deﬁnition illustrated above isto use the following fever compatible encoding:.
0 : t is correct, while ci can be any value, as alsoan incorrect ci may provide important context(corresponding to fever support label);.
1 : t is incorrect, ci correct, since ci can provideevidence that t is not similar to a correct an-swer (corresponding to fever refutal label);and.
2 : both are incorrect, in this case, nothing canbe told (corresponding to fever neutral la-bel)..4.2 multi-answer support reranker (masr).
asr still selects answers with a pointwise ap-proach2.
this means that we can improve it by.
2again, using ranking loss did not provide a signiﬁcant.
3256dataset.
wikiqa.
#q873trec-qa 1,229wqa 5,000.train#a+1,0406,40342,962.
#a-7,63247,014163,289.
#q12165905.dev.
#a+1402058,179.
#a-.
99091228,096.
#q237681,000.test#a+2932488,256.
#a-2,0581,19430,123.data splittraindevtest.
supported80,0356,6666,666.refuted not enough info35,63929,7756,6666,6666,6666,666.table 2: as2 dataset statistics.
table 3: fever dataset statistics.
building a listwise model, to select the best answerfor each question, by utilizing the information fromall target answers.
in particular, the architecture ofmasr shown in figure 1d is made up of two parts:(i) a list of asr containing k + 1 asr blocks, inwhich each asr block provides the representationof a target answer t. (ii) a ﬁnal multiclassiﬁer anda softmax function, which scores each t from k + 1embedding concatenation and selects the one withhighest score.
for training and testing, we selectthe t from the k + 1 candidates of q based on asoftmax output at a time..implementation details: the goal of masr isto measure the relation between k + 1 target an-swers, t0, .., tk.
the representation of each targetanswer is the embedding v ∈ r2d from equa-tion 1 in asr.
then, we concatenate the hiddenvectors of k + 1 target answers to form a matrixv(q,k+1) ∈ r(k+1)×2d.
we use this matrix and aclassiﬁcation layer weights w ∈ r2d, and computea standard multi-class classiﬁcation loss:.
lm asr = y ∗ log(sof tmax(v(q,k+1)w t ), (2).
where y is a one-hot-vector, and |y| = |k + 1|..5 experiments.
in these experiments, we compare our models:kgat, asr and masr with pointwise models,which are the state of the art for as2.
we also com-pare them with our joint model baselines (pairwiseand listwise).
finally, we provide an error analysis..5.1 datasets.
we used two most popular as2 datasets, and onereal world application dataset we built to test thegenerality of our approach..wikiqa is a qa dataset (yang et al., 2015) con-taining a sample of questions and answer-sentencecandidates from bing query logs over wikipedia.
the answers are manually labeled.
we follow themost used setting: training with all the questionsthat have at least one correct answer, and validatingand testing with all the questions having at leastone correct and one incorrect answer..trec-qa is another popular qa benchmarkby wang et al.
(2007).
we use the same splitsof the original data, following the common settingof previous work, e.g., (garg et al., 2020)..wqa the web-based question answering is adataset built by alexa ai as part of the effort toimprove understanding and benchmarking in qasystems.
the creation process includes the follow-ing steps: (i) given a set of questions we collectedfrom the web, a search engine is used to retrieveup to 1,000 web pages from an index containinghundreds of millions pages.
(ii) from the set ofretrieved documents, all candidate sentences are ex-tracted and ranked using as2 models from (garget al., 2020).
finally, (iii) top candidates for eachquestion are manually assessed as correct or incor-rect by human judges.
this allowed us to obtaina richer variety of answers from multiple sourceswith a higher average number of answers..table 2 reports the corpus statistics of wikiqa,.
trec-qa, and wqa3..fever is a large-scale public corpus, proposedby thorne et al.
(2018a) for fact veriﬁcationtask, consisting of 185,455 annotated claims from5,416,537 documents from the wikipedia dump injune 2017. all claims are labelled as supported,refuted or not enough info by annotators.
table 3shows the statistics of the dataset, which remainsthe same as in (thorne et al., 2018b)..5.2 training and testing details.
metrics the performance of qa systems is typi-cally measured with accuracy in providing correctanswers, i.e., the percentage of correct responses.
this is also referred to precision-at-1 (p@1) in thecontext of reranking, while standard precision andrecall are not essential in our case as we assumethe system does not abstain from providing answers.
we also use mean average precision (map) andmean reciprocal recall (mrr) evaluated on thetest set, using the entire set of candidates for each.
3the public version of wqa will be released in theshort-term future.
please search for a publication with ti-tle wqa: a dataset for web-based question answering taskson arxiv.org..3257roberta base.
wikiqa.
trec-qa.
reranker by garg et al., 2020our rerankerjoint model multi-classiﬁer (k=5)joint model pairwise (k=3)kgat (k=2)asr (k=3)masr (k=3)masr-f (k=3)masr-fp (k=3).
p@1.
–0.8189†0.7819†0.8272†0.8436†0.84360.82300.82720.8436.map mrr0.88900.88600.85420.89270.89910.90140.88910.89180.8998.
0.90100.89830.86840.90450.91200.91230.90170.90310.9113.p@1.
–0.91180.89710.95590.94120.97060.92650.94120.9559.map mrr0.91400.90430.90520.91960.91550.92570.92000.92220.9191.
0.95200.94980.94240.97430.96450.98160.96320.97060.9743.p@1.wqamap.
mrr.
––.
––.
––-2.29†% -1.00% -1.23%2.67†% 0.39% 1.39%2.10% 0.39% 0.93%†2.86% 0.86% 1.39%3.82% 0.70% 1.67%2.67% 0.55% 1.47%4.96% 0.94% 2.43%.
table 4: results on wikiqa, trec-qa and wqa, using roberta base transformer.
† is used to indicate that thedifference in p@1 between asr and the other marked systems is statistically signiﬁcant at 95%..joint-multiclassifer.
joint-pair.
kgat.
asr.
).
%.
(.
tnemevorpm.i.
5.
4.
3.
2.
1.
0.
−1.
−2.
−3.
1.
2.
4.
5.
3k.figure 2: impact of k on the wqa dev.
set.
question (this varies according to the dataset), tohave a direct comparison with the state of the art..models we use the pre-trained roberta-base(12 layer) and roberta-large-mnli (24 layer)models, which were released as checkpoints foruse in downstream tasks4..reranker training we adopt adam optimizer(kingma and ba, 2014) with a learning rate of 2e-5for the transfer step on the asnq dataset (garget al., 2020), and a learning rate of 1e-6 for theadapt step on the target dataset.
we apply earlystopping on the development set of the target cor-pus for both ﬁne-tuning steps based on the highestmap score.
we set the max number of epochsequal to 3 and 9 for the adapt and transfer steps,respectively.
we set the maximum sequence lengthfor roberta to 128 tokens..kgat and asr training again, we use theadam optimizer with a learning rate of 2e-6 fortraining the asr model on the target dataset.
weutilize 1 tesla v100 gpu with 32gb memory anda train batch size of eight.
we set the maximum se-quence length for roberta base/large to 130 to-kens and the number of training epochs to 20. theother training conﬁgurations are the same of theoriginal kgat model from (liu et al., 2020).
weuse two transformer models for asr: a roberta.
4https://github.com/pytorch/fairseq.
base/large for pr, and one for asc.
we set themaximum sequence length for roberta to 128tokens and the number of epochs to 20..masr training we use the same conﬁgurationof the asr training, including the optimizer type,learning rate, the number of epochs, gpu type,maximum sequence length, etc.
additionally, wedesign two different models masr-f, using anasc classiﬁer targeting the fever labels, andmasr-fp, which initializes asc with the datafrom fever.
this is possible as the labels arecompatible..5.3 choosing the best k.the selection of the hyper-parameter k, i.e., thenumber of candidates to consider for supporting atarget answer is rather tricky.
indeed, the standardvalidation set is typically used for tuning pr.
thismeans that the candidates pr moves to the top k +1positions are optimistically accurate.
thus, whenselecting also the optimal k on the same validationset, there is high risk to overﬁt the model..we solved this problem by running a pr versionnot heavily optimized on the dev.
set, i.e., we ran-domly choose a checkpoint after the standard threeepochs of ﬁne-tuning of roberta transformer.
ad-ditionally, we tuned k only using the wqa dev.
set,which contains ∼ 36, 000 q/a pairs.
wikiqa andtrec-qa dev.
sets are too small to be used (121and 65 questions, respectively).
fig.
2 plots theimprovement of four different models, joint modelmulti-classiﬁer, joint model pairwise, kgat, andasr, when using different k values.
their bestresults are reached for 5, 3, 2, and 3, respectively.
we note that the most reliable curve shape (convex)is the one of asr and joint model pairwise..32585.4 comparative results.
table 4 reports the p@1, map and mrr of thererankers, and different answer supporting modelson wikiqa, trec-qa and wqa datasets.
aswqa is an internal dataset, we only report theimprovement over pr in the tables.
all models useroberta-base pre-trained checkpoint and startfrom the same set of k candidates reranked by pr(state-of-the-art model).
the table shows that:.
• pr replicates the map and mrr of the state-of-the-art reranker by garg et al.
(2020) onwikiqa..• joint model multi-classiﬁer performs lowerthan pr for all measures and all datasets.
thisis in line with the ﬁndings of bonadiman andmoschitti (2020), who also did not obtain im-provement when jointly used all the candidatesaltogether in a representation..• joint model pairwise differs from asr as it con-catenates the embeddings of the (q, ci), insteadof using max-pooling, and does not use any an-swer support classiﬁer (asc).
still, it exploitsthe idea of aggregating the information of allpairs (q, ci) with respect to a target answer t,which proves to be effective, as the model im-proves on pr over all measures and datasets..• our kgat version for as2 also improves prover all datasets and almost all measures, con-ﬁrming that the idea of using candidates as sup-port of the target answer is generally valid.
how-ever, it is not superior to joint model pairwise..• asr achieves the highest performance amongall models (but masr-fp on wqa), alldatasets, and all measures.
for example, it out-performs pr by almost 3 absolute percent pointsin p@1 on wikiqa, and by almost 6 points ontrec from 91.18% to 97.06%, which corre-sponds to an error reduction of 60%..• masr and masr-f do not achieve better per-formance than joint model pairwise on wikiqaand trec, although masr outperforms allbaselines and even asr on wqa.
this sug-gests that the signiﬁcantly higher number ofparameters of masr cannot be trained on smallcorpus, while wqa has a sufﬁcient number ofexamples..robertalarge.
garg et al.,our rerankerkgat (k=2)asr (k=3).
wikiqa.
trec-qa.
p@1–0.87240.86420.8971.map mrr0.92000.91510.90940.9280.
0.93300.92660.92180.9399.p@1.
–0.97060.95590.9706.map mrr0.94300.94810.94070.9488.
0.97400.98160.97430.9816.table 5: results on wikiqa and trec-qa, usingroberta large transformer..• masr-fp exploiting fever for the initial-ization of asc performs better than masrand masr-f on wikiqa and trec.
interest-ingly, it signiﬁcantly outperforms asr by 2%on wqa.
this conﬁrms the potential of themodel when enough training data is available..• we perform randomization test (yeh, 2000) toverify if the models signiﬁcantly differ in termsof prediction outcome.
we use 100,000 trialsfor each calculation.
the results conﬁrm thestatistically signiﬁcant difference between asrand all the baselines, with p < 0.05 for wikiqa,and between asr and all models (i.e., includingalso kgat) on wqa..5.5 ofﬁcial state of the art.
as the state of the art for as2 is obtained usingroberta large, we trained kgat and asr usingthis pre-trained language model.
table 5 also re-ports the comparison with pr, which is the ofﬁcialstate of the art.
again, our pr replicates the re-sults of garg et al.
(2020), obtaining slightly lowerperformance on wikiqa but higher on trec-qa.
kgat performs lower than pr on both datasets..asr establishes the new state of the art onwikiqa with an map of 92.80 vs. 92.00. the p@1also signiﬁcantly improves by 2%, i.e., achieving89.71, which is impressively high.
also, on trec-qa, asr outperforms all models, being on parwith pr regarding p@1. the latter is 97.06, whichcorresponds to mistaking the answers of only twoquestions.
we manually checked these and foundout that these were two annotation errors: asrachieves perfect accuracy while pr only mistakesone answer.
of course, this just provides evidencethat pr based on roberta-large solves the taskof selecting the best answers (i.e., measuring p@1on this dataset is not meaningful anymore)..5.6 model discussion.
table 6 reports the accuracy of asc inside to dif-ferent models.
in asr, it uses 4-way categories,while in masr-based models, it uses the three.
3259wikiqaacc f10.590.460.460.49.
0.000.000.000.37.trec-qaacc f10.560.450.640.65.
0.800.620.780.73.wqaacc f10.580.530.580.59.
0.640.610.680.69.asrmasrmasr-fmasr-fp.
table 6: the accuracy and f1 of category 0 for asc.
fever labels (see sec.
4.1).
acc is the overallaccuracy while f1 refers to the category 0. we notethat asc in masr-fp achieves the highest accu-racy with respect to the average over all datasets.
this happens since we pre-ﬁne-tuned it with thefever data..we analyzed examples for which asr is correctand pr is not.
tab.
7 shows that, given q and k = 3candidates, pr chooses c1, a suitable but wronganswer.
this probably happens since the answerbest matches the syntactic/semantic pattern of thequestion, which asks for a type of color, indeed, theanswer offers such type, primary colors.
pr doesnot rely on any background information that cansupport the set of colors in the answer.
in contrast,asr selects c2 as it can rely on the support of otheranswers.
its asc provides an average score forthe category 0 (both members are correct) of c2,i.e., 1i(cid:54)=2 asc(c2, ci) = 0.653, while for c1 thekaverage score is signiﬁcant lower, i.e., 0.522. thisprovides higher support for c2, which is used byasr to rerank the output of pr..(cid:80).
tab.
8 shows an interesting case where all thesentences contain the required information, i.e.,february.
however, pr and asr both choose an-swer c0, which is correct but not natural, as it pro-vides the requested information indirectly.
also, itcontains a lot of ancillary information.
in contrast,masr is able to rerank the best answer, c1, in thetop position..6 conclusion.
we have proposed new joint models for as2.
asrencodes the relation between the target answer andall the other candidates, using an additional trans-former model, and an answer support classiﬁer,while masr jointly models the asr representa-tions for all target answers.
we extensively testedkgat, asr, masr, and other joint model base-lines we designed..the results show that our models can outperformthe state of the art.
most interestingly, asr con-stantly outperforms all the models (but masr-fp),on all datasets, through all measures, and for bothbase and large transformers.
for example, asr.
q: what kind of colors are in the rainbow?
c1: red, yellow, and blue are called the primary colors.
c2:.
the order of the colors in the rainbow goes: red, orange,yellow, green, blue, indigo and violet.
the colors in all rainbows are present in the same order:red, orange, yellow, green, blue, indigo, and violet.
c4: a rainbow occurs when white light bends and separates.
c3:.
into red, orange, yellow, green blue, indigo and violet..table 7: a question with answer candidates ranked bypr; asr chose c2..c1:.
q: what’s the month of valentine’s day?
c0: celebrated on february 14 every year, saint valentine’sday or valentine’s day is the traditional day on whichlovers convey their love to each other by sending valen-tine’s cards, sometimes even anonymously.
february is historically chosen to be the month of loveand romance and the month to celebrate valentine’s day.
in order for today to be valentine’s day, it’s necessary thattoday is in the month of february.
every year, valentine’s day is celebrated on february 14in many countries around the world..c3:.
c2:.
table 8:a question with answer candidates{c0, c1, c2, c3} ranked by pr; asr reranks as{c0, c3, c2, c1}; and masr reranks as {c1, c3, c0, c2};c1 is the natural correct answer..achieves the best reported results, i.e., map val-ues of 92.80% and 94.88, on wikiqa and trec-qa, respectively.
masr improves asr by 2% onwqa, since this contains enough data to train theasr representations jointly..references.
qingyao ai, keping bi, jiafeng guo, and w. brucecroft.
2018. learning a deep listwise context modelfor ranking reﬁnement.
corr, abs/1804.05936..weijie bian, si li, zhao yang, guang chen, andzhiqing lin.
2017. a compare-aggregate modelwith dynamic-clip attention for answer selection.
incikm, pages 1987–1990.
acm..daniele bonadiman and alessandro moschitti.
2020.a study on efﬁciency, accuracy and documentstructure for answer sentence selection.
corr,abs/2003.02349..zhe cao, tao qin, tie-yan liu, ming-feng tsai, andhang li.
2007. learning to rank: from pairwiseapproach to listwise approach.
in proceedings of the24th international conference on machine learning,icml ’07, pages 129–136, new york, ny, usa.
acm..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-domain questions.
corr, abs/1704.00051..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of.
3260deep bidirectional transformers for language under-standing.
corr, abs/1810.04805..siddhant garg, thuy vu, and alessandro moschitti.
2020. tanda: transfer and adapt pre-trained trans-former models for answer sentence selection.
in thethirty-fourth aaai conference on artiﬁcial intelli-gence, aaai 2020, the thirty-second innovative ap-plications of artiﬁcial intelligence conference, iaai2020, the tenth aaai symposium on educationaladvances in artiﬁcial intelligence, eaai 2020, newyork, ny, usa, february 7-12, 2020, pages 7780–7788. aaai press..minghao hu, yuxing peng, zhen huang, and dong-sheng li.
2019. retrieve, read, rerank: towardsend-to-end multi-document reading comprehension.
corr, abs/1906.04618..zan-xia jin, bo-wen zhang, fang zhou, jingyan qin,and xu-cheng yin.
2020. ranking via partial order-ing for answer selection.
information sciences..diederik p. kingma and jimmy ba.
2014. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..bernhard kratzwald and stefan feuerriegel.
2018.adaptive document retrieval for deep question an-swering.
in emnlp’18, pages 576–581..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learningof language representations.
in iclr 2020..md tahmid rahman laskar, jimmy xiangji huang, andenamul hoque.
2020. contextualized embeddingsbased transformer encoder for sentence similaritymodeling in answer selection task.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 5505–5514, marseille, france.
euro-pean language resources association..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
corr, abs/1907.11692..zhenghao liu, chenyan xiong, maosong sun, andzhiyuan liu.
2020. fine-grained fact veriﬁcationwith kernel graph attention network.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7342–7351, on-line.
association for computational linguistics..tsvetomila mihaylova, georgi karadzhov, pepaatanasova, ramy baly, mitra mohtarami, andpreslav nakov.
2019. semeval-2019 task 8: factchecking in community question answering forums.
in proceedings of the 13th international workshopon semantic evaluation, pages 860–869, minneapo-lis, minnesota, usa.
association for computationallinguistics..tsvetomila mihaylova, preslav nakov, llu´ıs m`arquez,alberto barr´on-cede˜no, mitra mohtarami, georgikaradzhov, and james r. glass.
2018. fact checkingin community forums.
aaai-2018..jinfeng rao, hua he, and jimmy j. lin.
2016. noise-contrastive estimation for answer selection with deepneural networks.
in cikm, pages 1913–1916.
acm..aliaksei severyn and alessandro moschitti.
2015.learning to rank short text pairs with convolutionaldeep neural networks.
in sigir’15..gehui shen, yunlun yang, and zhi-hong deng.
2017.inter-weighted alignment network for sentence pairmodeling.
in emnlp’17, pages 1179–1189, copen-hagen, denmark..harish tayyar madabushi, mark lee, and john barnden.
2018. integrating question classiﬁcation and deepin col-learning for improved answer selection.
ing’18, pages 3283–3294..james.
andreas vlachos,.
christosthorne,christodoulopoulos,and arpit mittal.
2018a.
fever: a large-scale dataset for fact extractionin proceedings of the 2018and veriﬁcation.
conference ofthe north american chapter ofthe association for computational linguistics:human language technologies, volume 1 (longpapers), pages 809–819, new orleans, louisiana.
association for computational linguistics..james thorne, andreas vlachos, oana cocarascu,christos christodoulopoulos, and arpit mittal.
the fact extraction and veriﬁcation2018b.
the(fever) shared task.
first workshop on fact extraction and veriﬁcation(fever), pages 1–9, brussels, belgium.
associationfor computational linguistics..in proceedings of.
e. voorhees and d. tice.
1999. the trec-8 questionanswering track evaluation, pages 77–82.
depart-ment of commerce, national institute of standardsand technology..mengqiu wang, noah a. smith, and teruko mita-mura.
2007. what is the jeopardy model?
ain emnlp-quasi-synchronous grammar for qa.
conll’07, pages 22–32, prague, czech republic.
association for computational linguistics..yizhong wang, kai liu, jing liu, wei he, yajuan lyu,hua wu, sujian li, and haifeng wang.
2018. multi-passage machine reading comprehension with cross-passage answer veriﬁcation.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1918–1927, melbourne, australia.
association forcomputational linguistics..yi yang, wen-tau yih, and christopher meek.
2015.wikiqa: a challenge dataset for open-domain ques-in proceedings of the 2015 con-tion answering.
ference on empirical methods in natural languageprocessing, pages 2013–2018..3261zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining for lan-guage understanding.
corr, abs/1906.08237..alexander s. yeh.
2000. more accurate tests for thestatistical signiﬁcance of result differences.
corr,cs.cl/0008005..seunghyun yoon, franck dernoncourt, doo soon kim,trung bui, and kyomin jung.
2019. a compare-aggregate model with latent clustering for answerselection.
corr, abs/1905.12897..wenxuan zhang, yang deng, jing ma, and wai lam.
2020a.
answerfact: fact checking in product ques-in proceedings of the 2020 con-tion answering.
ference on empirical methods in natural languageprocessing (emnlp), pages 2407–2417, online.
as-sociation for computational linguistics..yingxue zhang, fandong meng, peng li, ping jian,and jie zhou.
2020b.
ms-ranker: accumulating evi-dence from potentially correct candidates for answerselection.
corr, abs/2010.04970..3262