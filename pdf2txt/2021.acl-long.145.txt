bird’s eye: probing for linguistic graph structureswith a simple information-theoretic approach.
yifan houdepartment of computer scienceeth z¨urichyifan.hou@inf.ethz.ch.
mrinmaya sachandepartment of computer scienceeth z¨urichmrinmaya.sachan@inf.ethz.ch.
abstract.
nlp has a rich history of representing ourprior understanding of language in the form ofgraphs.
recent work on analyzing contextual-ized text representations has focused on hand-designed probe models to understand how andto what extent do these representations en-code a particular linguistic phenomenon.
how-ever, due to the inter-dependence of variousphenomena and randomness of training probemodels, detecting how these representationsencode the rich information in these linguis-tic graphs remains a challenging problem.
inthis paper, we propose a new information-theoretic probe, bird’s eye, which is afairly simple probe method for detecting if andhow these representations encode the informa-tion in these linguistic graphs.
instead of us-ing classiﬁer performance, our probe takes aninformation-theoretic view of probing and es-timates the mutual information between thelinguistic graph embedded in a continuousspace and the contextualized word represen-tations.
furthermore, we also propose an ap-proach to use our probe to investigate local-ized linguistic information in the linguisticgraphs using perturbation analysis.
we callthis probing setup worm’s eye.
using theseprobes, we analyze bert models on theirability to encode a syntactic and a semanticgraph structure, and ﬁnd that these models en-code to some degree both syntactic as wellas semantic information; albeit syntactic infor-mation to a greater extent.
our implementa-tion is available in https://github.com/yifan-h/graph_probe-birds_eye..1.introduction.
graphs have served as a predominant represen-tation for various linguistic phenomena in natu-ral language (marcus et al., 1993; de marneffeet al., 2006; hockenmaier and steedman, 2007;hajic et al., 2012; abend and rappoport, 2013; ba-narescu et al., 2013; bos, 2013).
these graph based.
representations have served our intuition for repre-senting both language structure (chomsky, 1957)as well as meaning (koller et al., 2019)..with the growing popularity of pretrained lan-guage models that build contextualized text repre-sentations (reid et al., 2020; devlin et al., 2019,inter alia), various probing models have been in-troduced to understand if and how our linguis-tic intuitions are encoded in these representations.
these probes train supervised models to predictpieces of linguistic information such as pos (part-of-speech), morphology, syntactic and semanticrelations, and other local or long-range phenom-ena in language (belinkov et al., 2017; conneauet al., 2018; hewitt and manning, 2019; tenneyet al., 2019b; jawahar et al., 2019).
however, itis still an open question if these representationssomehow encode entire linguistic graph structuressuch as dependency and constituency parse trees orgraph structured meaning representations such asamr (abstract meaning representation), ucca(universal conceptual cognitive annotation), etc.
a popular recent work, the structural probe (he-witt and manning, 2019), has investigated howcontextualized representations encode syntax trees.
they tested if a linear transformation of the net-work’s word representation space can predict par-ticular features of the syntax tree, namely, the dis-tance between words and depth of words in the tree.
thus, the structural probe cannot by itself answerthe question if these representations encode entirelinguistic graph structures.
moreover, the struc-tural probe is only designed for tree structures andcannot be extended to general graphs..in this work, we introduce a new probing ap-proach, bird’s eye, which can be used to de-tect if contextualized text representations encodeentire linguistic graphs.
bird’s eye is a simpleinformation-theoretic probe (pimentel et al., 2020b)which ﬁrst encodes the linguistic graph into a con-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1844–1859august1–6,2021.©2021associationforcomputationallinguistics1844figure 1: methodology of bird’s eye: to probe pretrained language models, linguistic graphs are embedded ina continuous space and the mutual information between graph embeddings and word representations is calculated..tinuous representation using graph embedding ap-proaches (cai et al., 2018) and then, estimates themutual information between the linguistic graphrepresentation space and the contextualized wordrepresentation space.
an illustration of the probeapproach is given in figure 1. the informationtheoretic approach is more reliable than training aprobe and using accuracy for probing as it is de-batable if the classiﬁer-based probe is probing ortrying to solve the task (hewitt and liang, 2019; pi-mentel et al., 2020b).
we further extend bird’seye to probe for localized linguistic informationin the linguistic graphs such as pos or dependencyarc labels in dependency parses.
we call this probe,worm’s eye..in our experiments, we ﬁrst illustrate the relia-bility of our probe methods and show the random-ness of previous probe methods that use accuracy.
then, we use bird’s eye to detect syntacticand semantic structures in bert, showing thatmuch syntactic and some semantic structure areencoded in bert.
besides, we also use worm’seye to probe for speciﬁc linguistic information insyntactic trees and semantic graphs respectively tosee which kinds of localized linguistic informationis encoded in bert.
our probing results are con-sistent with previous probe methods (hewitt andmanning, 2019; reif et al., 2019; liu et al., 2019;tenney et al., 2019a,b; wu et al., 2021).
we alsodiscuss limitations of our probe and how futurework can build upon our foundation..2 bird’s eye probe.
in this section, we introduce our information-theoretic approach for probing linguistic graphstructures in word representations.
the mi esti-mate is used to understand how much of the infor-mation in the linguistic graph structure has beenlearnt by the pretrained models..let x = {x1, .
.
.
, xt } denote an input sentence(each xi is the contextual embedding of a tokenin the given vocabulary v) and g denote the cor-.
responding linguistic graph.
furthermore, let xdenote a random variable that takes values rangingover all possible token sequences in v. correspond-ingly, let g denote a random variable that rangesover all possible corresponding linguistic graphs.
we use i(x ; g) to denote the linguistic structureinformation that is included in the given word rep-resentations.
note that the mi value i(x ; g) isalways non-negative, and a large mi implies thatmore of the structure information is encoded in theword representations.
in order to make the mi com-putation easier, we additionally assume alignmentsbetween the nodes v in the graph g and the wordsin x. this alignment is one to one, for example,in dependency parsing (marcus et al., 1993) but analigner might be needed in some cases (banarescuet al., 2013)..there are three main challenges in estimatingmi in our setting.
first, the mi estimation of dis-crete graphs and continuous features has been anelusive problem (ross, 2014; kraskov et al., 2004;escolano et al., 2017), since there is no widely ac-cepted deﬁnition of mutual information in this set-ting.
second, the dimensionality of the contextual-ized word representations is very high.
traditionalmethods (moon et al., 1995; steuer et al., 2002;paninski, 2003) for mi estimation do not scale wellwith large sample size or dimension (gao et al.,2015).
getting accurate estimates of mutual infor-mation in the high dimension is not easy.
third,graphs across different linguistic formalisms couldhave different entropy values, and thus the mi valuei(x ; g) may be uncomparable across the differ-ent linguistic graph formalisms.
for example, ifsyntactic trees g and semantic graphs g(cid:48) have thesame mi value with x i.e.
i(x ; g) = i(x ; g(cid:48))while the entropy values are fairly different i.e.
i(x ; g) ≈ h(g) << h(g(cid:48)), it is not proper toconclude that x contains the same amount of in-formation from structures g and g(cid:48), since they cor-respond to different percentages of the amount ofuncertainty.
thus, the mi values must be inter-.
1845preted carefully..bird’s eye tackles the aforementioned difﬁ-culties by transforming the linguistic graphs intoa continuous space using a graph embedding ap-proach.
then the mi between graph embeddingsand word representations is estimated using a re-cently proposed method (belghazi et al., 2018)which performs well even in high dimensions.
fi-nally, we also estimate a lower and upper bound ofthe mi, which is used to interpret the mi value.
wedescribe various stages of bird’s eye below:.
2.1 graph embedding.
the provided linguistic graphs can typically be rep-resented as an adjacency matrix.
directly calculat-ing mi with the adjacency matrix is not useful dueto the sparsity and discreteness of the adjacency ma-trix representation.
thus, we transform the graphsinto a continuous space where each node is rep-resented by a continuous representation of samedimensionality..theoretically, if the graph embedding approachis perfect, we can use the invariant property ofmutual information (kraskov et al., 2004).
thisproperty states that under some fairly strong con-ditions, there exists an invertible function f (·) thatsatisﬁes g = f −1(f (g)), where the graph embed-dings are z = f (g).
thus, we can transform ginto graph embeddings z, and:.
i(x ; z) ≈ i(x ; g).
(1).
in this paper, we use deepwalk (perozziet al., 2014), which is based on the skip-grammodel (huang et al., 1993; mikolov et al., 2013)for graph embeddings1.
speciﬁcally, given a nodev ∈ v encoded as the one-hot vector 1v, the modeltries to predict its neighbor’s vector 1v(cid:48) wherev(cid:48) ∈ nv.
the graph g = {v, e} is ﬁrst sampledto generate a set of random walks.
then the graphneighborhood relationship is represented by the co-occurrence of nodes in the walk paths.
finally, forall the walks, word2vec (mikolov et al., 2013) withskip-gram is used to maximize the co-occurrencelikelihood2:.
let z = ⊕{zv|v ∈ v } denote the learnt graphembedding where zv is the embedding of node v.here ⊕ denotes the concatenation operation..in our experiments, we also explore to what ex-tent the original linguistic graphs can be restoredby the graph embeddings, which tests the extentto which eq.
1 holds and if we can use i(x ; z)instead of i(x ; g) to estimate mi.
more detailscan be found in appendix a.
2.2 mutual information estimation.
to estimate i(x ; z) in high dimensions, we maxi-mize the compression lemma lower bound (baner-jee, 2006) as mentioned in belghazi et al.
(2018).
speciﬁcally, for a pair of random variables xand z, the mutual information is equivalent tothe kullback-leibler (kl) divergence betweenthe joint distribution px z and the product of themarginal distributions px ⊗ pz :.
i(x ; z) = dkl(px z ||px ⊗ pz )..(3).
from the compression lemma lower bound (baner-jee, 2006), the kl divergence dkl(p||q) can bebounded as:.
dkl(p||q) ≥ supt ∈f.
ep[t ] − log(eq[et ]),.
(4).
where f can be any class of functions t : ω → rsatisfying certain integrability constraints.
thus, inthe inequality 4, the lower bound can be obtainedby ﬁnding a function in the set f:.
i(x ; z) ≥ supt ∈f.
epx z [t ] − log(epx ⊗pz [et ])..to get a tight estimate of i(x ; z), we need thelower bound to be as high as possible.
thus, the miestimation problem turns into an optimization prob-lem to maximize the compression lemma lowerbound.
to ensure that, similar to belghazi et al.
(2018), we let f = {tθ}θ∈θ be the set of functionsparametrized by a neural network, and optimize theneural network using stochastic gradient descent.
formally, the objective function is:.
l(θ) =.
(cid:89).
(cid:89).
p(1v(cid:48)|1v; θ)..(2).
(e.maxθ∈θ.
p(n)x z.
[tθ] − log(e.x ⊗p(n)p(n).
z.
[etθ ]))..(5).
v∈v.
v(cid:48)∈nv.
1note that the bird’s eye probe is a general probe.
other graph embedding approaches can also be selected forthe transformation under speciﬁc conditions..2note that in word2vec, the window size is a hyperparam-eter that need to be selected by users.
here, for simplicity, weset the window size as 1..x z , p(n).
x and p(n).
here, p(n)z are empirical joint andmarginal distributions over a sample of n (sentence,graph) pairs..we calculate graph embeddings for each sen-tence independently, and regard one sentence as.
1846a mini-batch to optimize the neural network itera-tively for mi estimation.
note that different fromexisting probe models, our objective of the neuralnetwork is to ﬁnd an optimal function in f andestimate mi, rather than use prediction accuracy.
besides, the neural network is very simple (mlp).
therefore, there is no need to split dataset into train-ing and test to test generalization in mi estimation3.
the negative of the training loss as eq.
5 can betaken as mi estimation directly (belghazi et al.,2018; cristiani et al., 2020).
in our experiments,we verify the effectiveness of the mi estimationmethod to prove that the probe is stable.
more tech-nical details of the mi estimation model and how itis trained are given in appendix b..2.3 control bounds.
next, we introduce two control bounds to interpretthe mi value, whose functions are similar to thecontrol task introduced by hewitt and liang (2019).
as mentioned, comparing mi alone across differenttypes of structures is not useful, since the entropyvalues of graph embeddings can also be different.
thus, we calculate an upper and a lower bound ofthe mi value based on the graph structures.
insteadof using the mi value alone, we interpret it by itsrelative value in terms of the two control bounds.
formally, for the mi between graph embeddingsand word representations, we have:.
i(r; z) ≤ i(x ; z) ≤ i(z; z)..(6).
the lower bound is the mi between a truly randomvariable r (i.e., independent of the graph) and thegraph embedding z. thus, i(r; z) = 0. theupper bound telescopes to the graph structure’sself-entropy4..using these two control bounds, we interpret the.
structure information by the relative mi value5:.
scales the mi value for graph embeddings withdifferent self-entropy values into the same range:m ig(g) ∈ [0, 1].
intuitively, mig captures whatpercentage of the structure information is encodedin word representations..since m ig(g) is scaled using i(r; z), it alsohelps reduce the error in mi estimation.
as men-tioned, we maximize compression lemma lowerbound 5 as the mi estimate.
however, there couldbe a gap between it and the ground-truth mi value.
based on the fact that the ground-truth i(r; z) =0, we can know that the gap i(r; z) − ˆi(r; z)is equal to − ˆi(r; z).
in mig (eq.
7), the gap isadded for both numerator and denominator, whichreduces the error brought by mi estimation6..2.4 worm’s eye probe for localized.
linguistic structure.
bird’s eye allows us to probe for entire lin-guisitic structures.
however, for us to have a com-plete understanding, we might also want to probefor some localized information in the linguisticgraphs.
for example, we may want to know ifbert knows about pos tags or certain dependencyrelations in the syntax parse.
we formulate probingfor localized linguistic information as probing fora subgraph of the linguistic graph and reuse ourbird’s eye probe for it.
we call this settingworm’s eye as we are now analyzing if theserepresentations capture local sub-structures..to probe localized linguistic information gs ={vs, es}, we use perturbation of the original struc-ture for analysis.
speciﬁcally, we add a perturba-tion to the original graph embedding z based onthe subgraph gs.
for all the nodes in vs or nodesconnected by edges in es, we add a noise on theircorresponding node representations in z. let z(cid:48)denote the corrupted graph embedding.
then, wedeﬁne the following:.
m ig(g) =.
ˆi(x ; z) − ˆi(r; z)ˆi(z; z) − ˆi(r; z).
,.
(7).
m il(gs) = 1 −.
ˆi(x ; z (cid:48)) − ˆi(r; z)ˆi(x ; z) − ˆi(r; z).
,.
(8).
the mi estimates ˆi(z; z) and ˆi(r; z) can be ob-tained in the same way as ˆi(x ; z) (using the miestimation method mentioned above).
mig (eq.
7).
3alternatively, the dataset can be divided evenly into train-.
ing and test for mi estimation..4note that for continuous random variables z, the numberof values that z can take is inﬁnite.
in this condition, i(z; z)tends to inﬁnity.
thus, we use a small noise (cid:15) and approximateit as i(z + (cid:15), z)..5the deﬁnition is similar to the uncertainty coefﬁcient..mil describes how much mi is contributed bythe local structure gs.
when the local structureis the whole graph, z (cid:48) is completely noisy andm il(gs) equals to 1, which means the entire mivalue i(x ; z) is contributed by the local struc-ture.
if the local structure is an empty set, we have.
6in our experiment, we show that the estimated valuessatisfy | ˆi(r; z)| < 10−3 × ˆi(z; z), which is small enoughto be ignored..1847z (cid:48) = z. then we can get m il(gs) = 0, repre-senting that the local structure does not contributeanything to the mi value..if we control the perturbation of different typesof local structures at the same level, we can com-pare how well they are captured by the word repre-sentations relative to each other using eq.
8. specif-ically, for relations with labels, e.g., types of de-pendency relations in syntax trees, we set the sameperturbation on the graph embeddings.
then, wetest and compare m il(gs) for different types ofrelations.
larger m il(gs) for a particular relationtype implies that more information about this rela-tion type is encoded in the word representations..3 probing for syntactic and semantic.
graph structures.
we use our bird’s eye probe to detect two lin-guistic structures in the pretrained models, namely,dependency syntax (marcus et al., 1993; de marn-effe et al., 2006) and a more semantic formalism,amr (banarescu et al., 2013)..we ﬁrst use our model to probe for stanforddependencies (de marneffe et al., 2006).
for asentence x with tokens {x1, x2, ..xt }, the syntaxtree deﬁnes a directed labelled tree where tokens xiare represented as nodes and relations among themas labeled edges.
we ignore the edge direction andlabels for simplicity in our work7.
future work canconsider incorporating edge direction and labels.
we embed the given syntax tree into a continuousspace as mentioned before.
then, we calculate themig (eq.
7) as described before to determine howmuch syntax information is captured in the givencontextualized representations..next, we test if contextualized representationscapture a semantic graph representation – the ab-stract meaning representation (amr) (banarescuet al., 2013).
different from syntactic trees, se-mantic graphs are not tree structured, and therecan be loops or reentrencies.
in the amr anno-tation, plurality, articles and tenses were droppedand thus, there is no one-to-one corresponding be-tween words in the sentence and nodes in the amrgraph.
thus, we use an off-the-shelf aligner (pour-damghani et al., 2014) and calculate mi betweenthe amr graph embedding and the representationsof those words that are aligned with a node in theamr graph.
for simplicity, edge directions and.
7the stanford dependency tree also contains one empty.
root node, which is also ignored.
labels are also ignored in this setting..4 experiments.
our experiments mainly comprise of two parts:1. veriﬁcation of the probe: the ﬁrst part is forveriﬁcation of the probing methodology and ensur-ing that the graph embeddings retain informationabout the linguistic graphs i.e eq.
1 holds.
we dothis by testing if the graph embeddings can be usedto restore the original graph.
2. probing for graph structures: the secondpart is about using the probe to detect syntactic andsemantic graph structures in bert.
importantly,we probe if pretrained bert captures entire graphstructures as well as speciﬁc relational informationin these linguistic graphs.
to contrast with previousaccuracy and training based probes, we also train agroup of simple mlp models with different numberof hidden layers and use accuracy for probing.
weshow that designing and training a model to probeentire or localized linguistic structures is not asreliable as our information-theoretic approach..we use gold annotations from the penn treebankand the amr bank for all our experiments.
forthe contextualized word representations, we selectpretrained bert models, speciﬁcally bert-base(uncased) and bert-large (uncased).
since bertgenerates word-piece embeddings, to align themwith gold word-level tokens, we represent eachtoken as the average of its word-piece embeddingsas in hewitt and manning (2019).
we also usetwo non-contextual word embeddings as baselines:glove embeddings (pennington et al., 2014) andelmo-0, character-level word embeddings withno contextual information generated by pretrainedelmo (reid et al., 2020)..4.1 evaluation of graph embeddings.
we ﬁrst evaluate how well the graph embeddingscan capture the linguistic graph structures by pre-dicting the original graphs with them.
we use sim-ple mlps of 6 different settings with varying num-ber of hidden layers.
more details can be found inappendix c. we use auc score as the metric toevaluate the graph prediction performance, whichis a common metric in link prediction that com-putes area under the roc curve (fawcett, 2006).
the results are presented in table 1. we cansee that for both syntax trees and semantic graphs,mlps can achieve good performance in restoringthe original graph using graph embeddings where.
1848figure 2: m ig scores with syntactic and semantic structures, respectively for word representations in bertmodels (bert-base with 12 layers and bert-large with 24 layers).
note that results at the input layer are alsoreported, where the bert hidden layer index is 0)..table 1: performance (auc score for link prediction)on restoring graphs with graph embeddings.
# of hidden layers.
syntax tree amr graph.
012345.
0.56200.63300.96370.97800.98060.9791.
0.56200.54940.88040.92630.91620.9192.the auc score is quite high.
thus, we can be conﬁ-dent that equation 1 holds, and we can calculate mibased on the graph embeddings.
future work canexplore better graph embedding approaches.
wealso evaluate our probe by adding noisy represen-tations to the graph embeddings to prove that it iscapable of teasing out different levels of dependen-cies.
details can be referred to in appendix d..4.2 probing entire structures.
we ﬁrst used the bird’s eye probe to detect ifentire linguistic structures are encoded in hiddenrepresentations of bert8.
we also include twonon-contextual word representations – glove andelmo-0 as baselines.
we report m ig as the re-sults of our probe on the two graph structures infigures 2(a) and 2(b)..the m ig estimations for syntactic structureprobing of both bert-base and bert-large arequite high, which implies that bert encodes muchsyntactic information.
however, for the semanticstructure, the m ig scores of bert models arelower, suggesting that bert does not encode thesemantic structures as well.
these two conclusionsare consistent with previous works (liu et al., 2019;.
8for all mi estimation experiments, we repeat the experi-.
ment 20 times and take the average to get stable results..tenney et al., 2019b; wu et al., 2021) which havefound that unlike syntax, semantics is not capturedwell by the pretrained models..we also observe an interesting trend when com-paring m ig across layers.
we ﬁnd that for syntax,m ig starts to decrease in the upper layers, espe-cially for the bert-large.
this is consistent withprevious works which report that bert modelssyntax more in the lower and middle layers (ten-ney et al., 2019a).
for semantic graphs, m ig issteady across all layers.
it means that semanticinformation is spread across the entire model.
theresults are consistent with existing work (rogerset al., 2020).
for the two non-contextual base-lines, glove and elmo-0, we can see that theirm ig scores are lower compared with contextual-ized representations, especially for syntax.
previ-ous work (hewitt and manning, 2019) has drawnsimilar conclusions.
while for the semantic graphs,the gap is not signiﬁcant..4.3 probing localized information.
in this section, we show how we can use theworm’s eye probe to understand if the contextu-alized representations capture localized linguisticinformation in the dependency parses such as posinformation or relational dependency information.
as described before, we design various perturba-tion experiments using our worm’s eye probe.
for probing pos information or a dependency re-lation type, we add noise to the graph embeddingsof the corresponding node(s).
after that, we cal-culate the m il ratio (eq.
8) to show how muchparticular linguistic information (pos or relationtype information) is contained in the word repre-sentations.
we repeat the experiment 20 times and.
18490510152025bert hidden layer index0.00.20.40.60.81.0miglower boundupper boundgloveelmo0bert-basebert-large0510152025bert hidden layer index0.00.20.40.60.81.0miglower boundupper boundgloveelmo0bert-basebert-largeuse boxplots to present all the results..figure 3: m il scores of probing 5 types of pos tags(localized syntactic structure) for word representationsin bert-base (output layer).
the local structure is de-cided by the pos tags attached on nodes..first we use worm’s eye to test for pos in-formation, which is tagged as node labels in thedependency tree.
we select 5 pos tags: in, nnp,dt, jj, and nns, which have high and roughlythe same frequencies in the penn treebank dataset.
complete statistics about the pos tag frequenciescan be found in appendix e. we ensure that theamount of perturbation of the graph embeddingsis the same for each type.
figure 3 presents the re-sults.
we ﬁnd that nnp achieves the highest m ilscore, while nns achieves the lowest.
this impliesthat bert encodes syntactic information for sin-gular proper nouns (nnp) and adjectives (jj) morethan plural nouns (nns)..figure 4: m il scores of probing 5 types of localizedsyntactic structure for word representations in bert-base (output layer).
the local structure is decided bythe dependency relation labels attached on edges..next, we probe 5 types of universal depen-dency relations in the penn treebank dataset (ptb).
these are prep, pobj, det, nn and nsubj.
these 5relations also roughly occur the same number oftimes in ptb.
complete statistics about the num-ber of occurences of these relation types can befound in appendix e. similarly, for each type of.
figure 5: auc scores of predicting syntactic trees byvarious word representations..figure 6: auc scores of predicting semantic graphs byvarious word representations..relation, we add same amount of perturbation tograph embeddings of nodes connected by the spe-ciﬁc relations.
figure 4 shows the results, wherensubj relations have the lowest m il score com-pared with other 4 types.
this means that bertencodes more syntactic structure for prepositionalmodiﬁers (prep), object of a preposition (pobj), andnoun compound modiﬁer (nn) than nominal subject(nsubj).
reif et al.
(2019) have drawn similar con-clusions while probing for dependency arc labels.
similar experiment for semantic structure can befound in appendix f..4.4 on accuracy-based probing.
in contrast to our information-theoretic approachto probing, we train a group of mlp models toprobe entire and local structures in bert-base.
weshow that these probe results mainly depend on themodel complexity rather than the structure itself.
probing entire graph structures.
a group ofmlps are trained to predict entire syntactic andsemantic structures with word representations.
fig-ure 5 and figure 6 show the results.
their trends aresimilar.
shallow mlps perform the worst and deepones perform much better.
previous work on struc-tural probing (hewitt and manning, 2019) arguesthat powerful models could parse the word repre-.
1850innnpdtjjnnscorrupted node type0.00.20.40.60.81.0milpreppobjdetnnnsubjcorrupted edge type0.20.40.60.81.0mil012345# of mlp hidden layers0.40.50.60.70.80.91.0auc scorerandom embeddingsgloveelmo0bert-basebert-large012345# of mlp hidden layers0.40.60.8auc scorerandom embeddingsgloveelmo0bert-basebert-largesentations, thus a simple model should be designed.
however, in table 1, we ﬁnd that linear model evencould not restore the graph by its embeddings.
ob-viously, its performance cannot indicate how muchstructure information is included in the graph em-beddings.
thus, there is no reasonable principle todecide the complexity of the probe model.
giventhis, designing and training a model is not suitableto probe entire structures.
a similar argument hasbeen placed by previous works (pimentel et al.,2020a,b; lovering et al., 2021)..table 2: performance (auc score) of predicting graphstructures with mlps of different complexity.
relation type.
linear.
1.
2.
3.
4.
5.preppobjdetnnnsubjarggeneralop.
0.69660.62240.70160.68770.67540.66860.66210.6500.
0.72320.66190.72220.72210.69380.66520.65740.6500.
0.98380.98740.99280.98840.98410.91850.91920.9098.
0.98580.98880.99380.98990.98590.92170.92230.9177.
0.98630.98910.99400.99040.98680.91890.92210.9130.
0.98660.98940.99430.99030.98690.92150.92210.9160.probing localized graph structures.
to provethat accuracies of probe models for localized struc-ture also mainly depend on the model’s complexityrather than the local structure, we train the groupof mlps to predict the entire syntactic structure byword representations, and calculate the auc scoresfor each type of relations in test set as probing re-sults.
table 2 shows the auc score of predictingspeciﬁc type of relations.
for syntactic structure,same 5 types of relations are selected, and for se-mantic graphs, we select 3 groups of relations toprobe: arg, general and op.
complete statistics ofamr bank are in appendix e. from the results, wecan ﬁnd that for mlp models with different numberof hidden layers, the ranks of auc scores of rela-tion prediction are quite different.
for both syntaxtrees and semantic graphs, there is no consistent in-terpretation of the results to conclude which typesof relations are encoded in bert.
we also run theexperiment in the perturbation settings, which canbe referred to appendix g..combining the results of probing with accuracyin figure 5, figure 6, and table 2, we can ﬁnd thatthe prediction decisions are not based purely onthe structure but rather on spurious heuristics.
thishas also been concluded and discussed in somerecent works (hewitt and liang, 2019; loveringet al., 2021).
thus, training models is not feasibleto probe structures.
for our probe methods, therandomness of models such as complexity is not anissue, since the one with highest estimation should.
be selected for tighter compression lemma bound 4as introduced by pimentel et al.
(2020b)..4.5 hyperparameter and efﬁciency.
information-theoretic approaches sacriﬁce simplic-ity and efﬁciency to achieve reliable probing resultscompared to accuracy-based probes.
even thoughour probes are quite simple, there are more hyperpa-rameters that need to be selected by users comparedto accuracy-based probes.
to help users implementour methods in their setting, we brieﬂy describesome guiding principles to help them select hyper-parameters, and point out several potential ways tomake our probing approach more efﬁcient..our probes are composed of two steps: (a) com-putation of the graph embedding, and (b) estima-tion of the mutual information.
the guiding prin-cipal in the graph embedding step is to retain asmuch linguistic graph information as possible.
inour experiments, we used default hyperparametersin deepwalk (perozzi et al., 2014) for simplicity.
details can be found in appendix a. however,users may use also use other graph embeddingapproaches that incorporate edge labels, etc.
toimprove our model.
as the mutual information es-timation procedure is estimating a lower bound tothe true mutual information, the guiding principlefor hyperparameter selection in this step shouldbe to let the mi estimation values be as large aspossible.
in particular, model size is worth noting.
deeper models can achieve a tighter lower bound.
however, these are less efﬁcient than shallow ones.
thus, the selection of mi estimator’s complexity isa tradeoff.
according to our empirical experience,a relatively good choice is to use a two-layer mlps.
more details can be found in appendix d. notethat it might also be harder to achieve convergencewith deeper models as training of mi estimators isnotoriously difﬁcult.
we leave a better explorationof this to future work..potential users might also resort to other solu-tions to make the probes more efﬁcient.
if thebottleneck is in the graph embedding step, somefast approaches (hamilton et al., 2017; tang et al.,2015) can be chosen instead.
if the mutual infor-mation estimation step is the bottlenneck, somesampling strategies can be used.
a simple wayis to sample a subset of the dataset, and optimizeeq.
5 based on that subset.
alternatively, potentialusers can use more sophisticated sampling strate-gies in training as in recht and r´e (2012).
these.
1851approaches achieve a much better convergence ratefor mi estimation..tors in high-dimensional space..5 related work.
syntax and semantics probing.
many existingworks probe language models directly or indirectlyshowing how much syntactic and semantic infor-mation is encoded in them.
belinkov et al.
(2017)tested nmt models and found that higher layersencode semantic information while lower layersperform better at pos tagging.
similarly, jawa-har et al.
(2019) tested various bert layers andfound that it encodes a rich hierarchy of linguisticinformation in the intermediate layers.
tenney et al.
(2019b); wu et al.
(2021) compared the syntacticand semantic information in bert and its variants,and found that more syntactic information is en-coded than semantic information.
conneau et al.
(2018) focused on probing various linguistic fea-tures with 10 different designed tasks.
hewitt andmanning (2019) designed a tree distance and depthprediction task to probe syntax tree structures..information theoretic probe.
with the pop-ularity of probe methods, limitations of previousmethods have also been found.
information theo-retic methods have been proposed as an alternative.
to avoid the randomness of performance broughtby the varying sizes of the probe models, pimentelet al.
(2020b) proposed an information-theoreticprobe with control functions, which used mutualinformation instead of model performance for prob-ing.
voita and titov (2020) restricted the probemodel size by minimum description length.
train-ing a model is recast as teaching it to effectivelytransmit the data.
lovering et al.
(2021) pointedout that if we train a model to probe, the decisionsare often not based on information itself, but ratheron spurious heuristics speciﬁc to the training set..mutual information estimation.
mutual in-formation estimation is a well-known difﬁcultproblem, especially when the feature vectors arein a high dimensional space (chow and huang,2005; peng et al., 2005).
there are many tra-ditional ways to estimate mi, such as the well-known histogram approach (steuer et al., 2002;paninski, 2003), density estimations using a ker-nel (moon et al., 1995), and nearest-neighbor dis-tance (kraskov et al., 2004).
belghazi et al.
(2018)was recently proposed as a way to estimate miusing neural networks, which showed marked im-provement over previous methods for feature vec-.
6 limitations and future work.
in this paper we propose a general information-theoretic probe method, which is capable of prob-ing for linguistic graph structures and avoids therandomness of training a model.
in the experi-ments, we use our probe method to show the extentto which syntax trees and semantic graphs are en-coded in pretrained bert models.
further, weperform a simple perturbation analysis to show thatwith small modiﬁcations, the probe can also beused to probe for speciﬁc linguistic sub-structures.
there are some limitations of our probe.
first, agraph embedding is used, and some structure infor-mation could be lost in this process.
we providesimple ways to test this.
second, training a miestimation model is difﬁcult.
future work can con-sider building on our framework by exploring bettergraph embedding and mi estimation techniques..broader impact and discussion of ethics.
in recent years, deep learning approaches have beenthe main models for state-of-the-art systems in nat-ural language processing.
however, understandingthe decision making in these systems has been hard,and has challenges when these systems are usedin human contexts.
probing helps us gain inter-pretability and hence is useful in deploying theseblack-box models.
our work introduces a simpleand general way for understanding how linguisticproperties represented as graph structures are en-coded in large pretrained language models whichare being applied to a wide range of structures innlp.
the methodology and probing results can behelpful to the development of future nlp models.
while our model is not tuned for any speciﬁcreal-world application domain, our methods couldbe used in sensitive contexts such as legal or health-care settings, and it is essential that any work us-ing our probe method undertake extensive quality-assurance and robustness testing before using it intheir setting.
the datasets used in our work do notcontain any sensitive information to the best of ourknowledge..acknowledgments.
we would like to thank reviewers for the construc-tive comments and providing suggestions for fu-ture work.
this work was funded by snf project#201009..1852references.
omri abend and ari rappoport.
2013. universal con-ceptual cognitive annotation (ucca).
in proceed-ings of the 51st annual meeting of the associationfor computational linguistics, acl 2013, 4-9 au-gust 2013, soﬁa, bulgaria, volume 1: long papers,pages 228–238.
the association for computer lin-guistics..laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifﬁtt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse, law-id@acl 2013, august 8-9, 2013,soﬁa, bulgaria, pages 178–186.
the association forcomputer linguistics..arindam banerjee.
2006. on bayesian bounds..inmachine learning, proceedings of the twenty-thirdinternational conference (icml 2006), pittsburgh,pennsylvania, usa, june 25-29, 2006, volume 148of acm international conference proceeding se-ries, pages 81–88.
acm..mohamed ishmael belghazi, aristide baratin, sai ra-jeswar, sherjil ozair, yoshua bengio, r. devonhjelm, and aaron c. courville.
2018. mutual in-formation neural estimation.
in proceedings of the35th international conference on machine learning,icml 2018, stockholmsm¨assan, stockholm, sweden,july 10-15, 2018, volume 80 of proceedings of ma-chine learning research, pages 530–539.
pmlr..yonatan belinkov, llu´ıs m`arquez, hassan sajjad,nadir durrani, fahim dalvi, and james r. glass.
2017. evaluating layers of representation in neuralmachine translation on part-of-speech and semantictagging tasks.
in proceedings of the eighth interna-tional joint conference on natural language pro-cessing, ijcnlp 2017, taipei, taiwan, november27 - december 1, 2017 - volume 1: long papers,pages 1–10.
asian federation of natural languageprocessing..johan bos.
2013. the groningen meaning bank..inproceedings of the joint symposium on semanticprocessing.
textual inference and structures in cor-pora, jssp 2013, trento, italy, november 20-22,2013, page 2. association for computational lin-guistics..hongyun cai, vincent w. zheng, and kevin chen-chuan chang.
2018. a comprehensive surveyof graph embedding: problems,techniques, andieee trans.
knowl.
data eng.,applications.
30(9):1616–1637..noam chomsky.
1957. logical structures in language.
american documentation (pre-1986), 8(4):284..tommy w. s. chow and di huang.
2005. estimatingoptimal feature subsets using efﬁcient estimation of.
high-dimensional mutual information.
ieee trans.
neural networks, 16(1):213–224..alexis conneau, germ´an kruszewski, guillaume lam-ple, lo¨ıc barrault, and marco baroni.
2018. whatyou can cram into a single \$&!#* vector: probinginsentence embeddings for linguistic properties.
proceedings of the 56th annual meeting of the as-sociation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume1: long papers, pages 2126–2136.
association forcomputational linguistics..valence cristiani, maxime lecomte, and philippe mau-rine.
2020. leakage assessment through neural esti-mation of the mutual information.
in applied cryp-tography and network security workshops - acns2020 satellite workshops, aiblock, aihws, aiots,cloud s&p, sci, secmt, and simla, rome, italy,october 19-22, 2020, proceedings, volume 12418 oflecture notes in computer science, pages 144–162.
springer..marie-catherine de marneffe, bill maccartney,christopher d manning, et al.
2006. generat-ing typed dependency parses from phrase structureparses.
in lrec, volume 6, pages 449–454..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..francisco escolano, edwin r. hancock, miguel angellozano, and manuel curado.
2017. the mutual in-formation between graphs.
pattern recognit.
lett.,87:12–19..tom fawcett.
2006. an introduction to roc analysis..pattern recognit.
lett., 27(8):861–874..shuyang gao, greg ver steeg, and aram galstyan.
2015. efﬁcient estimation of mutual informationin proceedingsfor strongly dependent variables.
of the eighteenth international conference on artiﬁ-cial intelligence and statistics, aistats 2015, sandiego, california, usa, may 9-12, 2015, volume 38of jmlr workshop and conference proceedings.
jmlr.org..jan hajic, eva hajicov´a, jarmila panevov´a, petr sgall,ondrej bojar, silvie cinkov´a, eva fuc´ıkov´a, mariemikulov´a, petr pajas, jan popelka, jir´ı semeck´y,jana sindlerov´a, jan step´anek, josef toman, zdenkauresov´a, and zdenek zabokrtsk´y.
2012. announc-ing prague czech-english dependency treebank 2.0.in proceedings of the eighth international confer-ence on language resources and evaluation, lrec2012, istanbul, turkey, may 23-25, 2012, pages.
18533153–3160.
european language resources associ-ation (elra)..william l. hamilton, zhitao ying, and jure leskovec.
2017.inductive representation learning on largegraphs.
in advances in neural information process-ing systems 30: annual conference on neural in-formation processing systems 2017, december 4-9,2017, long beach, ca, usa, pages 1024–1034..john hewitt and percy liang.
2019. designing andinterpreting probes with control tasks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 2733–2743.
associationfor computational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4129–4138.
association for computa-tional linguistics..julia hockenmaier and mark steedman.
2007. ccg-bank: a corpus of ccg derivations and dependencystructures extracted from the penn treebank.
com-put.
linguistics, 33(3):355–396..xuedong huang, fileno a. alleva, hsiao-wuen hon,mei-yuh hwang, kai-fu lee, and ronald rosen-feld.
1993. the sphinx-ii speech recognition sys-tem: an overview.
comput.
speech lang., 7(2):137–148..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structure oflanguage?
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 3651–3657.
associationfor computational linguistics..alexander koller, stephan oepen, and weiwei sun.
2019. graph-based meaning representations: de-sign and processing.
in proceedings of the 57th con-ference of the association for computational lin-guistics: tutorial abstracts, acl 2019, florence,italy, july 28, 2019, volume 4: tutorial abstracts,pages 6–11.
association for computational linguis-tics..alexander kraskov, harald st¨ogbauer, and peter grass-berger.
2004. estimating mutual information.
phys-ical review e, 69(6):066138..nelson f. liu, matt gardner, yonatan belinkov,matthew e. peters, and noah a. smith.
2019. lin-guistic knowledge and transferability of contextual.
representations.
in proceedings of the 2019 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2019, minneapo-lis, mn, usa, june 2-7, 2019, volume 1 (long andshort papers), pages 1073–1094.
association forcomputational linguistics..charles lovering, rohan jha, tal linzen, and elliepavlick.
2021. predicting inductive biases of pre-in international conference ontrained models.
learning representations..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
comput.
lin-guistics, 19(2):313–330..marie-catherine de marneffe, bill maccartney, andchristopher d. manning.
2006. generating typeddependency parses from phrase structure parses.
in proceedings of the fifth international confer-ence on language resources and evaluation, lrec2006, genoa, italy, may 22-28, 2006, pages 449–454. european language resources association(elra)..tom´as mikolov, ilya sutskever, kai chen, gregory s.corrado, and jeffrey dean.
2013. distributed rep-resentations of words and phrases and their com-in advances in neural informationpositionality.
processing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 3111–3119..young-il moon, balaji rajagopalan, and upmanulall.
1995. estimation of mutual information us-ing kernel density estimators.
physical review e,52(3):2318..liam paninski.
2003. estimation of entropy and mu-neural comput., 15(6):1191–.
information..tual1253..hanchuan peng, fuhui long, and chris h. q. ding.
2005. feature selection based on mutual informa-tion: criteria of max-dependency, max-relevance,ieee trans.
pattern anal.
and min-redundancy.
mach.
intell., 27(8):1226–1238..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing, emnlp 2014, october 25-29, 2014,doha, qatar, a meeting of sigdat, a special inter-est group of the acl, pages 1532–1543.
acl..bryan perozzi, rami al-rfou, and steven skiena.
2014. deepwalk: online learning of social represen-in the 20th acm sigkdd internationaltations.
conference on knowledge discovery and data min-ing, kdd ’14, new york, ny, usa - august 24 - 27,2014, pages 701–710.
acm..1854jian tang, meng qu, mingzhe wang, ming zhang, junyan, and qiaozhu mei.
2015. line: large-scale in-in proceedings offormation network embedding.
the 24th international conference on world wideweb, www 2015, florence, italy, may 18-22, 2015,pages 1067–1077.
acm..ian tenney, dipanjan das, and ellie pavlick.
2019a.
bert rediscovers the classical nlp pipeline.
inproceedings of the 57th conference of the asso-ciation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 4593–4601.
association forcomputational linguistics..ian tenney, patrick xia, berlin chen, alex wang,adam poliak, r. thomas mccoy, najoung kim,benjamin van durme, samuel r. bowman, dipan-jan das, and ellie pavlick.
2019b.
what do youlearn from context?
probing for sentence structurein contextualized word representations.
in 7th inter-national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..elena voita and ivan titov.
2020..information-theoretic probing with minimum description length.
in proceedings of the 2020 conference on empiricalmethods in natural language processing, emnlp2020, online, november 16-20, 2020, pages 183–196. association for computational linguistics..zhaofeng wu, hao peng, and noah a. smith.
2021.infusing ﬁnetuning with semantic dependencies.
trans.
assoc.
comput.
linguistics, 9:226–242..tiago pimentel, naomi saphra, adina williams, andryan cotterell.
2020a.
pareto probing: trading offaccuracy for complexity.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing, emnlp 2020, online, novem-ber 16-20, 2020, pages 3138–3153.
association forcomputational linguistics..tiago pimentel, josef valvoda, rowan hall maudslay,ran zmigrod, adina williams, and ryan cotterell.
2020b.
information-theoretic probing for linguisticstructure.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,acl 2020, online, july 5-10, 2020, pages 4609–4622. association for computational linguistics..nima pourdamghani, yang gao, ulf hermjakob, andkevin knight.
2014. aligning english strings withabstract meaning representation graphs.
in proceed-ings of the 2014 conference on empirical methodsin natural language processing, emnlp 2014, oc-tober 25-29, 2014, doha, qatar, a meeting of sig-dat, a special interest group of the acl, pages425–429.
acl..benjamin recht and christopher r´e.
2012..be-neath the valley of the noncommutative arithmetic-conjectures, case-geometric mean inequality:arxiv preprintstudies,and consequences.
arxiv:1202.4184..machel reid, edison marrese-taylor, and yutakamatsuo.
2020. vcdm: leveraging variational bi-encoding and deep contextualized word representa-tions for improved deﬁnition modeling.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 6331–6344.
as-sociation for computational linguistics..emily reif, ann yuan, martin wattenberg, fernanda b.vi´egas, andy coenen, adam pearce, and been kim.
2019. visualizing and measuring the geometry ofbert.
in advances in neural information process-ing systems 32: annual conference on neural infor-mation processing systems 2019, neurips 2019, de-cember 8-14, 2019, vancouver, bc, canada, pages8592–8600..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
trans.
assoc.
comput.
linguis-tics, 8:842–866..brian c ross.
2014. mutual information betweenplos one,.
discrete and continuous data sets.
9(2):e87357..ralph e. steuer, j¨urgen kurths, carsten o. daub,janko weise, and joachim selbig.
2002. the mutualinformation: detecting and evaluating dependenciesin proceedings of the euro-between variables.
pean conference on computational biology (eccb2002), october 6-9, 2002, saarbr¨ucken, germany,pages 231–240..1855a details of graph embedding.
in this section, we present the technique detailsof the graph embedding approach, as well as pa-rameters.
given a graph such as syntax tree andsemantic graph, we ﬁrst run random walk algorithmon it to sample walk paths.
the random walk strat-egy is simple, each time a neighbor of current nodeis selected from its neighbor set based on uniformdistribution.
for each node, the length of randomwalk path is 10. and the repeat time is 100. in gen-eral, each node has 100 different walk paths withlength 10. then we put those paths into word2vecmodel (mikolov et al., 2013), with window sizeequal to 2, since we only want graph embeddingsto capture the one-hop neighborhood relationships.
the hidden states, in other words graph embed-dings are vectors with 128 dimensions..b details of mi estimation.
in this section we present technique details aboutmi estimation, such as the neural network modeldesign and parameters.
there are two terms in theobjective function 5: one is about joint distribution.
e.p(n)x z.
[tθ].
and another is about marginal distribution.
log(e.x ⊗p(n)p(n).
z.
[etθ ])..for the joint distribution part, we concatenate thegraph embeddings z and word representations xﬁrst, and then put them into our designed neural net-work to compute a scalar.
then the average valueof the scalar is computed.
for the marginal part,we randomly shufﬂe the representations x .
afterrandom shufﬂe, there is no dependency between xand z anymore.
then, we put the concatenation ofthe shufﬂed representations and graph embeddingsinto the neural network to get another scalar.
wetake the exponential value of that scalar and takethe average value for the whole dataset..as aforementioned, the selection of model size isa tradeoff.
in our experiments, we design an mlpmodel with two layers for mi estimation.
the ﬁrstlayer is linear without nonlinear activation func-tion, to encode graph embeddings and word rep-resentations into same space, with 64 dimensions.
then we concatenate those two hidden states andput them through a nonlinear layer to get a scalar.
for example, we have one sentence with 10 words..assume we can get graph embeddings with size10 ∗ 128, and word representations of bert withsize 10 ∗ 768. then we use a linear function tomap those two vectors into hidden space, say with32 dimensions.
then we concatenate those twoembeddings as a 10 ∗ 64 matrix, and use one extralinear function with nonlinear activation functionsto map it as 10∗1 matrix.
then we can get the com-pression lemma lower bound as the mean value ofthe 10 ∗ 1 matrix, which is the mutual informationestimation that we want..the loss is deﬁned directly as the minus value ofobjective function.
with stochastic gradient decent,we can maximize the lower bound to get the esti-mation.
about the mini-batch, since the documentcontains many sentences, we select one sentenceas one min-batch to optimize the neural network.
the reason why we treat one sentence as oneminibatch is that we get word representations ofbert and graph embeddings in that way.
onesentence has a complete syntax tree structure, andgetting word representations with one sentence inbert can make attention computed within the sen-tence.
another reason is that using two sentencesas input may exceed the maximum bert inputsize: 512 tokens sometimes.
however, if we usemini-batch to estimate the mutual information, itbrings errors.
the reason is that if we want to es-timate the mutual information between x and z,the expectation should be all the data points thatwe know.
but here we use minibatch to calculatethe expectation for one batch only.
to alleviate thiserror, as introduced in belghazi et al.
(2018), weselect small learning rate to keep the error small..c details of mlp models.
this section introduces how to use mlp to dolink prediction task, as well as the details aboutmlps.
for one sentence, given its graph embed-dings, we simply use mlp to calculate a score forall node pairs, and then compare with the ground-truth graph with the predicted distribution vector.
auc score is computed based on the distributionvector and ground-truth vector.
note that since thegraph is very sparse, it makes the task very difﬁcult.
generally, the task can be regarded as a binary clas-siﬁcation task with an extremely unbalanced datadistribution..for the details, linear mlp simply predicts thegraph by the concatenation of two input vectorsto decide whether there is an edge between them..1856for mlps with hidden layers, the concatenatedvectors are through non-linear layers ﬁrst, then theﬁnal output layer is linear.
the dimension of allhidden states is 128. and the learning rate is 10−4..d reliability of mi estimation.
mi estimation for features in high-dimensionalspace is difﬁcult.
to prove that our estimated mivalues are quite accurate, we test the mi estimationmethod (belghazi et al., 2018) on sets of graphembeddings with different levels of dependencies.
to have that, we add noise on graph embeddingsas z (cid:48).
z and z (cid:48) can have different dependenciesbased on the added noise rate.
noise vectors aresampled from a standard gaussian independently.
we test the estimation for various levels of noise,from original graph embeddings z to the condi-tion that 100% signals are noise σ. for exam-ple, 40% means that for each graph embeddingz(cid:48) = 60% × z + 40% × σ. then, we calculateˆi(z (cid:48); z) to see whether the values is small withlarge noise added..e statistics of penn treebank and amr.
bank.
we provide the relation number and connectedword number of penn treebank dataset and amrbank in this section.
the word number for postags of penn treebank is also provided.
the statis-tics are for the whole dataset.
we only report de-.
table 4: penn treebank statistics of syntax relations.
statistic.
relation.
# of relations % of relations # of connected words.
punctpreppobjdetnnnsubjamodrootdobjaux# others.
121,395100,99798,58686,22881,38173,80266,38143,94843,05437,26726,1791.
11.609.6489.4188.2377.7747.0506.3414.1984.1133.56025.01.
193,648189,783197,164172,446143,248147,498125,11987,89685,99773,436509,641.tails of relations ranked top 10. however, there areother 35 types of relations, which are categorizedtogether in type # others.
for the pos tagging.
table 5: penn treebank statistics of pos tags.
statistic.
# of words % of words.
table 3: mi value with different level of noise.
tags.
mi percentage (%).
structure.
noise ratio (%).
syntactic tree.
semantic graph.
0102030405060708090100.
100.0091.8474.1062.2349.5338.6129.4622.0912.252.07-0.03.
100.0080.4063.2951.8832.7020.449.363.740.940.02-0.06.nninnnpdtjjnnscdrbvbdvb# others.
146,228108,434101,42790,15867,39665,86740,33734,33133,43029,001203,578.
15.8911.7811.029.7987.3247.1584.3843.7313.6333.15222.12.table 3 presents the exact values.
to make itmore readable, we report the mi percentage ofˆi(z (cid:48); z)/ ˆi(z; z).
from the results, we ﬁnd thatfor the two structures, results are not very sim-ilar.
but the general tendencies are consistent,where less dependencies caused by larger noisehave smaller mi values.
note that when the noiserate is 100%, ˆi(z (cid:48), z) degenerates to the lowerbound ˆi(r, z).
as mentioned before, the absolutevalue of it represents the gap between the estimatedmi and ground-truth mi, which is very small (lessthan 10−3 × ˆi(z; z)).
it also proves that our miestimations are reliable..statistics, we also only present tags with word num-ber ranked top 10. there are still 28 types of postags that are catergorized into one type # others..table 6: amr bank statistics of relations.
statistic.
relation.
# of relations % of relations.
arggeneralopquantitiesothersdate.
409,322208,28767,30713,0925,2161,114.
58.1129.579.5561.8590.74060.1582.
1857the amr graphs are different from syntax trees.
the relations of amr graphs can be classiﬁed into6 groups, and each group contains many types.
speciﬁcally, the group general includes: “accom-panier”, “age”, “beneﬁciary”, “concession”, “con-dition”, “consist”, “degree”, “destination”, “direc-tion”, “domain”, “duration”, “example”, “extent”,“frequency”, “instrument”, “location”, “manner”,“medium”, “mod”, “name”, “part”, “path”, “polar-ity”, “poss”, “purpose”, “source”, “subevent”, “sub-set”, “time”, “topic”, “value”, “ord”, and “range”.
and the quantities group includes “quant”, “scale”,and “unit”..for the description, arg represents frame argu-ments, following propbank conventions.
generalare composed of a set of general semantic relations.
op means the relations for lists.
similarly, quanti-ties are relations for quantities.
and date containsrelations for date-entities..f probe speciﬁc semantic relations.
g probing localized information with.
accuracy.
similar to our localized probing experiments, weadd perturbations on word representations.
speciﬁ-cally, we corrupt word representations equally andwith same number for each relation type.
then wetrain an mlp with 5 hidden layers to predict entirestructures with the corrupted word representations.
auc scores of all relation types are calculated.
as in the worm’s eye, we only report 5 typesof relations for syntactic and 3 types for seman-tic structures.
results are shown in figure 8(a)and figure 8(b).
we can ﬁnd that the probe ac-curacies are very unusual.
first, corrupt one typeof relations, the accuracies for other types of re-lations change signiﬁcantly.
besides, the mlp istrained with corrupted relations e.g., nsubj whilepredicts prep with worst auc score.
the resultsalso prove the point that prediction decisions arenot based purely on the structure but rather on spu-rious heuristics (lovering et al., 2021)..figure 7: m il scores of probing 5 types of localizedsemantic structure for word representations in bert-base (output layer).
the local structure is decided bythe labels attached on edges in amr graph..for semantic structure, we run the perturbationexperiment in a similar way.
different from syntaxtrees, the relations of amr graphs can be groupedinto 6 types.
and number distribution is not veryeven.
thus, we corrupt the graph embeddingswith 50% noise.
other settings are similar to thatof syntactic structures.
from the results, we canfound that bert encodes more structure informa-tion about arg and general relations.
while for theop relations, which represents the relations for lists,are not well encoded..1858arggeneralopcorrupted edge type0.40.60.81.0milfigure 8: probe speciﬁc syntactic and semantic relations.
1859preppobjdetnnnsubjcorrupted relation typensubjnndetpobjpreprelation type0.97510.97920.98290.97970.97540.98080.97990.98580.98310.98620.99180.99130.98820.99060.98940.98910.98730.98840.95520.98610.98010.98510.98390.97970.9598syntactic relationarggeneralopcorrupted relation typeopgeneralargrelation type0.92550.92330.91450.92680.92450.91540.91750.91930.9083semantic relation