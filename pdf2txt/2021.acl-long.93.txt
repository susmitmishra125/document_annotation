uncovering constraint-based behavior in neural models via targetedfine-tuning.
forrest davis and marten van schijndeldepartment of linguisticscornell university{fd252|mv443}@cornell.edu.
abstract.
a growing body of literature has focused ondetailing the linguistic knowledge embeddedin large, pretrained language models.
existingwork has shown that non-linguistic biases inmodels can drive model behavior away fromlinguistic generalizations.
we hypothesizedthat competing linguistic processes within alanguage, rather than just non-linguistic modelbiases, could obscure underlying linguisticknowledge.
we tested this claim by exploringa single phenomenon in four languages: en-glish, chinese, spanish, and italian.
whilehuman behavior has been found to be similaracross languages, we ﬁnd cross-linguistic vari-ation in model behavior.
we show that compet-ing processes in a language act as constraintson model behavior and demonstrate that tar-geted ﬁne-tuning can re-weight the learnedconstraints, uncovering otherwise dormant lin-guistic knowledge in models.
our results sug-gest that models need to learn both the linguis-tic constraints in a language and their relativeranking, with mismatches in either producingnon-human-like behavior..1.introduction.
ever larger pretrained language models continueto demonstrate success on a variety of nlp bench-marks (e.g., devlin et al., 2019; brown et al., 2020).
one common approach for understanding whythese models are successful is centered on infer-ring what linguistic knowledge such models ac-quire (e.g., linzen et al., 2016; hewitt and man-ning, 2019; hu et al., 2020; warstadt et al., 2020a).
linguistic knowledge alone, of course, does notfully account for model behavior; non-linguisticheuristics have also been shown to drive modelbehavior (e.g., sentence length; see mccoy et al.,2019; warstadt et al., 2020b).
nevertheless, whenlooking across a variety of experimental methods,.
models appear to acquire some grammatical knowl-edge (see warstadt et al., 2019)..however, investigations of linguistic knowledgein language models are limited by the overwhelm-ing prominence of work solely on english (thoughsee gulordava et al., 2018; ravfogel et al., 2018;mueller et al., 2020).
prior work has shown non-linguistic biases of neural language models mimicenglish-like linguistic structure, limiting the gener-alizability of claims founded on english data (e.g.,dyer et al., 2019; davis and van schijndel, 2020b).
in the present study, we show via cross-linguisticcomparison, that knowledge of competing linguis-tic constraints can obscure underlying linguisticknowledge..our investigation is centered on a single dis-course phenomena, implicit causality (ic) verbs,in four languages: english, chinese, spanish, anditalian.
when an ic verb occurs in a sentence,interpretations of pronouns are affected:.
(1).
a..b..lavender frightened kate because shewas so terrifying.
lavender admired kate because shewas so amazing..in (1), both lavender and kate agree in genderwith she, so both are possible antecedents.
how-ever, english speakers overwhelmingly interpretshe as referring to lavender in (1-a) and kate in(1-b).
verbs that have a subject preference (e.g.,frightened) are called subject-biased ic verbs, andverbs with an object preference (e.g., admired) arecalled object-biased ic verbs..ic has been a rich source of psycholinguisticinvestigation (e.g., garvey and caramazza, 1974;hartshorne, 2014; williams, 2020).
current ac-counts of ic ground the phenomenon within thelinguistic signal without the need for additionalpragmatic inferences by comprehenders (e.g., ro-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1159–1171august1–6,2021.©2021associationforcomputationallinguistics1159hde et al., 2011; hartshorne et al., 2013).
recentinvestigations of ic in neural language models con-ﬁrms that the ic bias of english is learnable, atleast to some degree, from text data alone (davisand van schijndel, 2020a; upadhye et al., 2020).
the ability of models trained on other languagesto acquire an ic bias, however, has not been ex-plored.
within the psycholinguistic literature, ichas been shown to be remarkably consistent cross-linguistically (see hartshorne et al., 2013; ngoand kaiser, 2020).
that is, ic verbs have beenattested in a variety of languages.
given the cross-linguistic consistency of ic, then, models trainedon other languages should also demonstrate an icbias.
however, using two popular model types,bert based (devlin et al., 2019) and robertabased (liu et al., 2019),1 we ﬁnd that models onlyacquired a human-like ic bias in english and chi-nese but not in spanish and italian..we relate this to a crucial difference in the pres-ence of a competing linguistic constraint affectingpronouns in the target languages.
namely, span-ish and italian have a well studied process calledpro drop, which allows for subjects to be ‘empty’(rizzi, 1986).
an english equivalent would be“(she) likes bert” where she can be elided.
whileic verbs increase the probability of a pronoun thatrefers to a particular antecedent, pro drop disprefersany overt pronoun in subject position (i.e.
the targetlocation in our study).
that is, both processes arein direct competition in our experiments.
as a re-sult, spanish and italian models are susceptible toovergeneralizing any learned pro-drop knowledge,favoring no pronouns rather than ic-conditionedpronoun generation..to exhibit an ic bias, models of spanish anditalian have two tasks:learn the relevant con-straints (i.e.
ic and pro drop) and the relative rank-ing of these constraints.
we ﬁnd that the modelslearn both constraints, but, critically, instantiate thewrong ranking, favoring pro drop to an ic bias.
using ﬁne-tuning to demote pro drop, we are ableto uncover otherwise dormant ic knowledge inspanish and italian.
thus, the apparent failure ofthe spanish and italian models to pattern like en-glish and chinese is not evidence on its own of amodel’s inability to acquire the requisite linguistic.
1these model types were chosen for ease of access toexisting models.
pretrained, large auto-regressive models arelargely restricted to english, and prior work suggests thatlstms are limited in their ability to acquire an ic bias inenglish (davis and van schijndel, 2020a)..knowledge, but is in fact evidence that models areunable to adjudicate between competing linguisticconstraints in a human-like way.
in english andchinese, the promotion of a pro-drop process viaﬁne-tuning has the opposing effect, diminishing anic bias in model behavior.
as such, our results in-dicate that non-human like behavior can be drivenby failure either to learn the underlying linguis-tic constraints or to learn the relevant constraintranking..2 related work.
this work is intimately related to the growing bodyof literature investigating linguistic knowledge inlarge, pretrained models.
largely, this literaturearticulates model knowledge via isolated linguis-tic phenomena, such as subject-verb agreement(e.g., linzen et al., 2016; mueller et al., 2020),negative polarity items (e.g., marvin and linzen,2018; warstadt et al., 2019), and discourse andpragmatic structure (including implicit causality;e.g., ettinger, 2020; schuster et al., 2020; jereticet al., 2020; upadhye et al., 2020).
our study dif-fers, largely, in framing model linguistic knowledgeas sets of competing constraints, which privilegesthe interaction between linguistic phenomena..prior work has noted competing generalizationsinﬂuencing model behavior via the distinctionof non-linguistic vs. linguistic biases (e.g., mc-coy et al., 2019; davis and van schijndel, 2020a;warstadt et al., 2020b).
the ﬁndings in warstadtet al.
(2020b), that linguistic knowledge is repre-sented within a model much earlier than attestationin model behavior, bears resemblance to our claims.
we ﬁnd that linguistic knowledge can, in fact, liedormant due to other linguistic processes in a lan-guage, not just due to non-linguistic preferences.
our ﬁndings suggest that some linguistic knowl-edge may never surface in model behavior, thoughfurther work is needed on this point..in the construction of our experiments, we wereinspired by synthetic language studies which probethe underlying linguistic capabilities of languagemodels (e.g., mccoy et al., 2018; ravfogel et al.,2019).
we made use of synthetically modiﬁed lan-guage data that accentuated, or weakened, evidencefor certain linguistic processes.
the goal of suchmodiﬁcation in our work is quite similar both towork which attempts to remove targeted linguisticknowledge in model representations (e.g., ravfogelet al., 2020; elazar et al., 2021) and to work which.
1160lang tokensmodelenbertenrobertachinese bertzhchinese roberta zhesbetoesrupertaititalian bertumbertoititgilberto.
3.3b30b5.4b5.4b3b3b2b0.6b11b.
table 1: summary of models investigated with lan-guage and approximate number of tokens in train-ing.
for roberta we use the approximation given inwarstadt et al.
(2020b)..investigates the representational space of modelsvia priming (prasad et al., 2019; misra et al., 2020).
in the present study, rather than identifying isolatedlinguistic knowledge or using priming to study rela-tions between underlying linguistic representations,we ask how linguistic representations interact todrive model behavior..3 models.
prior work on ic in neural language models hasbeen restricted to autoregressive models for ease ofcomparison to human results (e.g., upadhye et al.,2020).
in the present study, we focused on two pop-ular non-autoregressive language model variants,bert (devlin et al., 2019) and roberta (liuet al., 2019).
we used existing models available viahuggingface (wolf et al., 2020)..multilingual models have been claimed to per-form worse on targeted linguistics tasks than mono-lingual models (e.g., mueller et al., 2020).
weconﬁrmed this claim by evaluating mbert whichexhibited no ic bias in any language.2 thus, wefocus in the rest of this paper on monolingual mod-els (summarized in table 1).
for english, we usedthe bert base uncased model and the robertabase model.
for chinese, we evaluated bert androberta models from cui et al.
(2020).
for span-ish, we used beto (ca˜nete et al., 2020) and ru-perta (romero, 2020).
for italian, we evaluatedan uncased italian bert 3 as well as two robertabased models, umberto (parisi et al., 2020) andgilberto (ravasio and di perna, 2020)..2results are provided in appendix b3https://huggingface.co/dbmdz/bert-base-italian-uncased.
4 experimental stimuli and measures.
our list of target verbs was derived from existingpsycholinguistic studies of ic verbs.4 for english,we used the ic verbs from ferstl et al.
(2011)..each verb in the human experiment was codedfor ic bias based on continuations of sentence frag-ments (e.g., kate accused bill because ...).
forspanish, we used the ic verbs from goikoetxeaet al.
(2008), which followed a similar paradigm asferstl et al.
(2011) for english.
participants weregiven sentence fragments and asked to completethe sentence and circle their intended referent.
thestudy reported the percent of subject continuationsfor 100 verbs, from which we used the 61 verbswhich had a signiﬁcant ic bias (i.e.
excluding verbswith no signiﬁcant subject or object bias)..for italian, we used the 40 ic verbs reportedin mannetti and de grada (1991).
human partici-pants were given ambiguous completed sentenceswith no overt pronoun like “john feared michaelbecause of the kind of person (he) is” and wereasked to judge who the null pronoun referred to,with the average number of responses that gave thesubject as the antecedent reported.5 for chinese,we used 59 ic verbs reported in hartshorne et al.
(2013), which determined average subject bias perverb in a similar way as mannetti and de grada(1991) (i.e.
judgments of antecedent preferencesgiven ambiguous sentences, this time with overtpronouns).6.we generated stimuli using 14 pairs of stereotyp-ical male and female nouns (e.g., man vs. woman,husband vs. wife) in each language, rather thanrely on proper names as was done in the humanexperiments.
the models we investigated are bidi-rectional, so we used a neutral right context, wasthere, for english and spanish, where human ex-.
4all stimuli, as well as code for reproducing the re-sults of the paper are available at https://github.com/forrestdavis/implicitcausality .
for each lan-guage investigated, the stimuli were evaluated for grammati-cality by native speakers with academic training in linguistics.
5speciﬁcally, mannetti and de grada (1991) grouped theverbs into four categories and reported the average per cat-egory as well as individual verb results for the most biasedverbs and the negative/positive valency verbs.
additionally,ﬁgures showing average responses across various conditionswas reported for one of the categories.
from the combinationof this information, the average scores for all but two verbswere able to be determined.
the remaining two verbs wereassigned the reported average score of their stimuli group..6in hartshorne et al.
(2013), 60 verbs were reported, butafter consultation with a native speaker with academic train-ing in linguistics, one verb was excluded due to perceivedungrammaticality of the construction..1161figure 1: model scores for a) bert, b) roberta, c)chinese bert, and d) chinese roberta at the pro-noun grouped by antecedent; stimuli derived from fer-stl et al.
(2011) and hartshorne et al.
(2013).
figure 2: model scores for a) spanish bert (beto),b) italian bert, c) umberto, and d) gilberto at thepronoun grouped by antecedent; stimuli derived fromgoikoetxea et al.
(2008) and mannetti and de grada(1991).
periments provided no right context.7 for italianwe utilized the full sentences investigated in thehuman experiments.
the chinese human experi-ment also used full sentences, but relied on noncewords (i.e.
novel, constructed words like sliktopoz),so we chose instead to generate sentences like theenglish and spanish ones.
all stimuli had subjectsand objects that differed in gender, such that allnouns occurred in subject or object position (i.e.
the stimuli were fully balanced for gender):.
(2).
the man admired the woman because[mask] was there.8.
the mismatch in gender forced the choice of pro-noun to be unambiguous.
for each stimulus, wegathered the scores assigned to the third personsingular male and female pronouns (e.g., he andshe).9 our measures were grouped by antecedenttype (i.e.
the pronoun refers to the subject or theobject) and whether the verb was object-biased orsubject-biased.
for example, bert assigns to (2)a score of 0.01 for the subject antecedent (i.e.
he)and 0.97 for the object (i.e.
she), in line with theobject-bias of admire..5 models inconsistently capture implicit.
causality.
as exempliﬁed in (1), repeated below, ic verb biasmodulates the preference for pronouns..(3).
a..b..lavender frightened kate because shewas so terrifying.
lavender admired kate because shewas so amazing..an object-biased ic verb (e.g., admired) shouldincrease the likelihood of pronouns that refer to theobject, and a subject-biased ic verb (e.g., fright-ened) should increase the likelihood of referenceto the subject.
given that all the investigated stim-uli were disambiguated by gender, we categorizedour results by the antecedent of the pronoun andthe ic verb bias.
we ﬁrst turn to english and chi-nese, which showed an ic bias in line with existingwork on ic bias in autoregressive english models(e.g., upadhye et al., 2020; davis and van schijn-del, 2020a).
we then detail the results for spanishand italian, where only very limited, if any, ic biaswas observed..7using here, outside, or inside as the right context produces.
5.1 english and chinese.
qualitatively the same patterns..8the model-speciﬁc mask token was used.
additionally,all models were uncased, with the exception of roberta, solower cased stimuli were used..9in spoken chinese, the male and female pronouns arehomophonous.
they are, however, distinguished in writing..the results for english and chinese are given infigure 1 and detailed in appendix b. all modelsdemonstrated a greater preference for pronouns re-ferring to the object after an object-biased ic verb.
1162than after a subject-biased ic verb.10 additionally,they had greater preferences for pronouns refer-ring to the subject after a subject-biased ic verbthan after a object-biased ic verb.
that is, all mod-els showed the expected ic-bias effect.
generally,there was an overall greater preference for referringto the object, in line with a recency bias, with theexception of roberta, where subject-biased icverbs neutralized the recency effect..5.2 spanish and italian.
the results for spanish and italian are given in fig-ure 2 and detailed in appendix b. in stark contrastto the models of english and chinese, an ic biaswas either not demonstrated or was only weaklyattested.
for spanish, beto showed a greater pref-erence for pronouns referencing the object after anobject-biased ic verb than after a subject-biasedic verb.
there was no corresponding ic effect forpronouns referring to the subject, and ruperta (aroberta based model) had no ic effect at all..italian bert and gilberto (a roberta basedmodel) had no signiﬁcant effect of ic-verb on pro-nouns referring to the object.
there was a signif-icant, albeit very small, increased score for pro-nouns referring to the subject after a subject-biasedic verb in line with a weak subject-ic bias.
sim-ilarly, umberto (a roberta based model) hadsigniﬁcant, yet tiny ic effects, where object-biasedic verbs increased the score of pronouns refer-ring to objects compared to subject-biased ic verbs(conversely with pronouns referring to the subject).
any signiﬁcant effects in spanish and italianwere much smaller than their counterparts in en-glish (as is visually apparent between figure 1 andfigure 2), and each of the spanish and italian mod-els failed to demonstrate at least one of the iceffects..6 pro drop and implicit causality:.
competing constraints.
we were left with an apparent mismatch betweenmodels of english and chinese and models of span-ish and italian.
in the former, an ic verb bias mod-ulated pronoun preferences.
in the latter, the same.
10throughout the paper, statistical signiﬁcance was deter-mined by two-way t-tests evaluating the difference betweenpronouns referring to objects after subject-biased and object-biased ic verbs, and similarly for pronouns referring to thesubject.
the threshold for statistical signiﬁcance was p =0.0009, after adjusting for the 54 statistical tests conducted inthe paper..ic verb bias was comparably absent.
recall that,for humans, the psycholinguistic literature suggeststhat ic bias is, in fact, quite consistent across lan-guages (see hartshorne et al., 2013)..we found a possible reason for why the two setsof models behave so differently by carefully consid-ering the languages under investigation.
languagescan be thought of as systems of competing linguis-tic constraints (e.g., optimality theory; prince andsmolensky, 2004).
spanish and italian exhibit prodrop and typical grammatical sentences often lackovert pronouns in subject position, opting instead torely on rich agreement systems to disambiguate theintended subject at the verb (rizzi, 1986).
this con-straint competes with ic, which favors pronounsthat refer to either the subject or the object.
chinesealso allows for empty arguments (both subjectsand objects), typically called discourse pro-drop(huang, 1984).11 as the name suggests, however,this process is more discourse constrained than theprocess in spanish and italian.
for example, inchinese, the empty subject can only refer to thesubject of the preceding sentence (see liu, 2014).
as a means of comparison, in surveying three uni-versal dependencies datasets,12 8% of nsubj (ornsubj:pass) relations were pronouns for chinese,while only 2% and 3% were pronouns in spanishand italian respectively.
english lies on the oppo-site end of the continuum, requiring overt pronounsin the absence of other nominals (cf.
he likes nlpand *likes nlp)..therefore, it’s possible that the presence of com-peting constraints in spanish and italian obscuredthe underlying ic knowledge: one constraint pre-ferring pronouns which referred to the subject orobject and the other constraint penalizing overtpronouns in subject positions (i.e.
the target posi-tion masked in our experiments).
in the followingsections, we removed or otherwise demoted thedominance of each model’s pro-drop constraint forspanish and italian, and introduced or promoteda pro-drop like constraint in english and chinese.
we found that the degree of ic bias in model behav-ior could be controlled by the presence, or absence,of a competing pro-drop constraint..6.1 methodology.
we constructed two classes of dataset to ﬁne-tunethe models on.
the ﬁrst aimed to demote the pro-.
11other names common to the literature include topic drop,.
radical pro drop, and rampant pro drop..12chinese gsd, italian isdt, and spanish ancora..1163figure 3: after ﬁne-tuning on baseline data (i.e.
pro-drop sentences), model scores for a) spanish bert(beto), b) italian bert, c) umberto, and d)gilberto at the pronoun grouped by antecedent; stim-uli derived from goikoetxea et al.
(2008) and mannettiand de grada (1991).
figure 4: after ﬁne-tuning on sentences removing prodrop (i.e.
adding a subject pronoun), model scores fora) spanish bert (beto), b) italian bert, c) um-berto, and d) gilberto at the pronoun groupedby antecedent; stimuli derived from goikoetxea et al.
(2008) and mannetti and de grada (1991).
drop constraint in spanish and italian.
the secondaimed to inject a pro-drop constraint into englishand chinese.
for both we relied on universal de-pendencies datasets.
for spanish, we used the an-cora spanish newswire corpus (taul´e et al., 2008),for italian we used isdt (bosco et al., 2013) andvit (delmonte et al., 2007), for english we usedthe english web treebank (silveira et al., 2014),and for chinese, we used the traditional chineseuniversal dependencies treebank annotated bygoogle (gsd) and the chinese parallel universaldependencies (pud) corpus from the 2017 conllshared task (zeman et al., 2017)..for demoting pro drop, we found ﬁnite (i.e.
in-ﬂected) verbs that did not have a subject relationin the corpora.13 we then added a pronoun, match-ing the person and number information given onthe verb, alternating the gender.
for italian, thisamounted to a dataset of 3798 sentences with atotal of 4608 pronouns (2,284 he or she) added.
for parity with italian, we restricted spanish to adataset of the ﬁrst 4000 sentences, which had 5,559pronouns (3,573 he or she) added.
for the additionof a pro-drop constraint in english and chinese, wefound and removed pronouns that bore a subjectrelation to a verb.
this amounted to 935 modi-ﬁed sentences and 1083 removed pronouns (774he or she) in chinese and 4000 modiﬁed sentences.
and 5984 removed pronouns (2188 he or she) inenglish.14.
for each language, 500 unmodiﬁed sentenceswere used for validation, and unchanged versionsof all the sentences were kept and used to ﬁne-tunethe models as a baseline to ensure that there wasnothing about the data themselves that changed theic-bias of the models.
moreover, the ﬁne-tuningdata was ﬁltered to ensure that no verbs evaluated inour test data were included.
fine-tuning proceededusing huggingface’s api.
each model was ﬁne-tuned with a masked language modeling objectivefor 3 epochs with a learning rate of 5e-5, followingthe ﬁne-tuning details in (devlin et al., 2019).15.
6.2 demoting pro drop: spanish and italian.
as a baseline, we ﬁne-tuned the spanish and italianmodels on unmodiﬁed versions of all the data weused for demoting pro drop.
the baseline resultsare given in figure 3. we found the same qualita-tive effects detailed in section 5.2, conﬁrming thatthe data used for ﬁne-tuning on their own did notproduce model behavior in line with an ic bias..we turn now to our main experimental manipu-.
14a fuller breakdown of the ﬁne-tuning data is given inappendix a with the full training and evaluation data given onour github.
we restricted english to the ﬁrst 4000 sentencesfor parity with italian/spanish.
using the full set of sentencesresulted in qualitatively the same pattern.
we used the maxi-mum number of sentences we could take from chinese ud.
15we provide a colab script for reproducing all ﬁne-tuned.
13in particular, verbs that lacked any nsubj, nsubj:pass, expl,.
expl:impers, or expl:pass dependents.
models on our github..1164figure 5: after ﬁne-tuning on baseline data (i.e.
with-out removing subject pronouns), model scores for a)bert, b) roberta, c) chinese bert, and d) chineseroberta at the pronoun grouped by antecedent; stim-uli derived from ferstl et al.
(2011) and hartshorne et al.
(2013).
figure 6: after ﬁne-tuning on sentences with pro drop(i.e.
no subject pronouns), model scores for a) bert, b)roberta, c) chinese bert, and d) chinese robertaat the pronoun grouped by antecedent; stimuli derivedfrom ferstl et al.
(2011) and hartshorne et al.
(2013).
lation: ﬁne-tuning the spanish and italian modelson sentences that exhibit the opposite of a pro-dropeffect.
it is worth repeating that the ﬁne-tuningdata shared no verbs or sentence frames with ourtest data.
the results are given in figure 4. strik-ingly, an object-biased ic effect (pronouns refer-ring to the object were more likely after object-biased ic verbs than subject-biased ic verbs) wasobserved for italian bert and gilberto despiteno such effect being observed in the base mod-els.
moreover, both models showed a more thandoubled subject-biased ic verb effect.
umbertoalso showed increased ic effects, as compared tothe base models.
similarly for spanish, a subject-biased ic verb effect materialized for beto whenno corresponding effect was observed with the basemodel.
the object-biased ic verb effect remainedsimilar to what was reported in section 5.2. forruperta, which showed no ic knowledge in theinitial investigation, no ic knowledge surfaced af-ter ﬁne-tuning.
we conclude that ruperta has nounderlying knowledge of ic, though further workshould investigate this claim..taken together these results indicate that simplyﬁne-tuning on a small number of sentences can re-rank the linguistic constraints inﬂuencing modelbehavior and uncover other linguistic knowledge(in our case an underlying ic-bias).
that is, modelbehavior can hide linguistic knowledge not justbecause of non-linguistic heuristics, but also due.
to over-zealously learning one isolated aspect oflinguistic structure at the expense of another..6.3 promoting pro drop: english and.
chinese.
next, we ﬁne-tune a pro-drop constraint into mod-els of english and chinese.
recall that both mod-els showed an ic effect, for both object-biased andsubject-biased ic verbs.
moreover, both languageslack the pro-drop process found in spanish anditalian (though chinese allows null arguments)..as with spanish and italian, we ﬁne-tuned theenglish and chinese models on unmodiﬁed ver-sions of the training sentences as a baseline (i.e.
the sentences kept their pronouns) with the resultsgiven in figure 5. there was no qualitative dif-ference from the ic effects noted in section 5.1.that is, for both english and chinese, pronounsreferring to the object were more likely after object-biased ic verbs than after subject-biased ic verbs,and conversely pronouns referring to the subjectwere more likely after subject-biased than object-biased ic verbs..the results after ﬁne-tuning the models on datamimicking a spanish and italian like pro-drop pro-cess (i.e.
no pronouns in subject position) are givenin figure 6 and detailed in appendix b. despiteﬁne-tuning on only 0.0004% and 0.003% of thedata roberta and bert were trained on, re-spectively, the ic effects observed in section 5.1were severely diminished in english.
however,.
1165the subject-biased ic verb effect remained robustin both models.
for chinese bert, the subject-biased ic verb effect in the base model was lostand the object-biased ic verb effect was reduced.
the subject-biased ic verb effect was similarlyattenuated in chinese roberta.
however, theobject-biased ic verb effect remained..for both languages, exposure to relatively littlepro-drop data weakened the ic effect in behaviorand even removed it in the case of subject-biasedic verbs in chinese bert.
this result strengthensour claim that competition between learned linguis-tic constraints can obscure underlying linguisticknowledge in model behavior..7 discussion.
the present study investigated the ability ofroberta and bert models to demonstrate knowl-edge of implicit causality across four languages(recall the contrast between lavender frightenedkate and lavender admired kate in (1)).
contraryto humans, who show consistent subject and object-biased ic verb preferences across languages (seehartshorne et al., 2013), bert and roberta mod-els of spanish and italian failed to demonstrate thefull ic bias found in english and chinese bertand roberta models (with our english resultssupporting prior work on ic bias in neural mod-els and extending it to non-autoregressive models;upadhye et al., 2020; davis and van schijndel,2020a).
following standard behavioral probing(e.g., linzen et al., 2016), this mismatch could beinterpreted as evidence of differences in linguisticknowledge across languages.
that is, model be-havior in spanish and italian was inconsistent withpredictions from the psycholinguistic ic literature,suggesting that these models lack knowledge ofimplicit causality.
however, we found that to be anincorrect inference; the models did have underlyingknowledge of ic..other linguistic processes inﬂuence pronounsin spanish and italian, and we showed that com-petition between multiple distinct constraints af-fects model behavior.
one constraint (pro drop)decreases the probability of overt pronouns in sub-ject position, while the other (ic) increases theprobability of pronouns that refer to particular an-tecedents (subject-biased verbs like frightened fa-voring subjects and object-biased verbs like ad-mired favoring objects).
models of spanish anditalian, then, must learn not only these two con-.
straints, but also their ranking (i.e.
should the modelgenerate a pronoun as ic dictates, or generate nopronoun in line with pro drop).
by ﬁne-tuning themodels on data contrary to pro drop (i.e.
with overtpronouns in subject position), we uncovered other-wise hidden ic knowledge.
moreover, we foundthat ﬁne-tuning a pro-drop constraint into englishand chinese greatly diminished ic’s inﬂuence onmodel behavior (with as little as 0.0004% of a mod-els original training data)..taken together, we conclude that there are twoways of understanding mismatches between modellinguistic behavior and human linguistic behavior.
either a model fails to learn the necessary linguisticconstraint, or it succeeds in learning the constraintbut fails to learn the correct interaction with otherconstraints.
existing literature points to a num-ber of reasons a model may be unable to learn alinguistic representation, including the inability tolearn mappings between form and meaning and thelack of embodiment (e.g., bender and koller, 2020;bisk et al., 2020).
we suggest that researchersshould re-conceptualize linguistic inference on thepart of neural models as inference of constraintsand constraint ranking in order to better understandmodel behavior.
we believe such framing will openadditional connections with linguistic theory andpsycholinguistics.
minimally, we believe targetedﬁne-tuning for constraint re-ranking may providea general method both to understand what linguis-tic knowledge these models possess and to aid inmaking their linguistic behavior more human-like..8 conclusion and future work.
the present study provided evidence that modelbehavior can be meaningfully described, and un-derstood, with reference to competing constraints.
we believe that this is a potentially fruitful way ofreasoning about model linguistic knowledge.
pos-sible future directions include pairing our behav-ioral analyses with representational probing in or-der to more explicitly link model representationsand model behavior (e.g., ettinger et al., 2016; he-witt and liang, 2019) or exploring constraint com-petition in different models, like gpt-2 which hasreceived considerable attention for its apparent lin-guistic behavior (e.g., hu et al., 2020) and its abilityto predict neural responses (e.g., schrimpf et al.,2020)..1166acknowledgments.
we would like to thank members of the c.psydlab, the cornell nlp group, and the stanfordnlp group, who gave valuable feedback on earlierforms of this work.
thanks also to the anonymousreviewers whose comments improved the paper..references.
emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-standing in the age of data.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5185–5198, online.
as-sociation for computational linguistics..yonatan bisk, ari holtzman, jesse thomason, jacobandreas, yoshua bengio, joyce chai, mirella lap-ata, angeliki lazaridou, jonathan may, aleksandrnisnevich, nicolas pinto, and joseph turian.
2020.in proceedings ofexperience grounds language.
the 2020 conference on empirical methods in natu-ral language processing, pages 8718–8735, online.
association for computational linguistics..cristina bosco, simonetta montemagni, and mariasimi.
2013. converting italian treebanks: towardsin pro-an italian stanford dependency treebank.
ceedings of the 7th linguistic annotation workshopand interoperability with discourse, pages 61–69,soﬁa, bulgaria.
association for computational lin-guistics..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shotlearners.
arxiv:2005.14165 [cs]..jos´e ca˜nete, gabriel chaperon, rodrigo fuentes, jou-hui ho, hojin kang, and jorge p´erez.
2020. span-ish pre-trained bert model and evaluation data.
in pml4dc at iclr 2020..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language pro-cessing.
in findings of the association for compu-tational linguistics: emnlp 2020, pages 657–668,online.
association for computational linguistics..forrest davis and marten van schijndel.
2020a.
dis-course structure interacts with reference but not syn-in proceedings oftax in neural language models..the 24th conference on computational natural lan-guage learning, pages 396–407, online.
associa-tion for computational linguistics..forrest davis and marten van schijndel.
2020b.
re-current neural network language models alwayslearn english-like relative clause attachment.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1979–1990, online.
association for computational lin-guistics..rodolfo delmonte, antonella bristot, and sara tonelli.
2007. vit – venice italian treebank: syntacticin sixth internationaland quantitative features.
workshop on treebanks and linguistic theories, vol-ume 1, pages 43–54..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..chris dyer, g´abor melis, and phil blunsom.
2019. acritical analysis of biased parsers in unsupervisedparsing.
arxiv:1909.09428 [cs]..yanai elazar, shauli ravfogel, alon jacovi, and yoavgoldberg.
2021. amnesic probing: behavioral ex-planation with amnesic counterfactuals.
transac-tions of the association for computational linguis-tics, 9:160–175..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnosticsfor language models.
transactions of the associa-tion for computational linguistics, 8:34–48..allyson ettinger, ahmed elgohary, and philip resnik.
2016. probing for semantic evidence of compositionby means of simple classiﬁcation tasks.
in proceed-ings of the 1st workshop on evaluating vector-spacerepresentations for nlp, pages 134–139.
associa-tion for computational linguistics..evelyn c. ferstl, alan garnham, and christinamanouilidou.
2011.implicit causality bias in en-glish: a corpus of 300 verbs.
behavior researchmethods, 43(1):124–135..catherine garvey and alfonso caramazza.
1974. im-linguistic inquiry,.
plicit causality in verbs.
5(3):459–464..edurne goikoetxea, gema pascual, and joana acha.
2008. normative study of the implicit causality of100 interpersonal verbs in spanish.
behavior re-search methods, 40(3):760–772..1167kristina gulordava, piotr bojanowski, edouard grave,tal linzen, and marco baroni.
2018. colorlessgreen recurrent networks dream hierarchically.
in proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 1195–1205.
associ-ation for computational linguistics..joshua k. hartshorne.
2014. what is implicit causal-language, cognition and neuroscience,.
ity?
29(7):804–824..joshua k. hartshorne, yasutada sudo, and mikiuruwashi.
2013. are implicit causality pronoun res-olution biases consistent across languages and cul-tures?
experimental psychology, 60(3):179–196..john hewitt and percy liang.
2019. designing andinterpreting probes with control tasks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, pages 2733–2743.
association for compu-tational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for finding syntax in word rep-in proceedings of the 2019 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 4129–4138.
associationfor computational linguistics..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020. a systematic assessment ofsyntactic generalization in neural language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 1725–1744, online.
association for compu-tational linguistics..c.-t. james huang.
1984. on the distribution andreference of empty pronouns.
linguistic inquiry,15(4):531–574..paloma jeretic, alex warstadt, suvrat bhooshan, andadina williams.
2020. are natural language infer-ence models imppressive?
learning implicatureand presupposition.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8690–8705, online.
associationfor computational linguistics..tal linzen, emmanuel dupoux, and yoav gold-berg.
2016. assessing the ability of lstms totransac-learn syntax-sensitive dependencies.
tions of the association for computational linguis-tics, 4(0):521–535..chi-ming louis liu.
2014. a modular theory of rad-.
ical pro drop.
ph.d., harvard university..roberta: a robustly optimized bert pretrain-ing approach.
arxiv:1907.11692 [cs]..l. mannetti and e. de grada.
1991..interpersonalverbs: implicit causality of action verbs and contex-tual factors: implicit causality of action verbs.
euro-pean journal of social psychology, 21(5):429–443..rebecca marvin and tal linzen.
2018. targeted syn-tactic evaluation of language models.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202.
association for computational linguistics..r thomas mccoy, robert frank, and tal linzen.
2018.revisiting the poverty of the stimulus: hierarchicalgeneralization without a hierarchical bias in recur-rent neural networks.
proceedings of the 40th an-nual virtual meeting of the cognitive science soci-ety..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticin pro-heuristics in natural language inference.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 3428–3448, florence, italy.
association for computa-tional linguistics..kanishka misra, allyson ettinger, and julia rayz.
2020. exploring bert’s sensitivity to lexical cuesin findingsusing tests from semantic priming.
of the association for computational linguistics:emnlp 2020, pages 4625–4635, online.
associa-tion for computational linguistics..aaron mueller, garrett nicolai, panayiota petrou-zeniou, natalia talmina, and tal linzen.
2020.cross-linguistic syntactic evaluation of word pre-diction models.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5523–5539.
association for compu-tational linguistics..binh ngo and elsi kaiser.
2020. implicit causality: acomparison of english and vietnamese verbs.
uni-versity of pennsylvania working papers in linguis-tics, 26..loreto parisi, simone francia, and paolo magnani.
2020. umberto: an italian language modeltrained with whole word masking..grusha prasad, marten van schijndel, and tal linzen.
2019. using priming to uncover the organiza-tion of syntactic representations in neural lan-guage models.
in proceedings of the 23rd confer-ence on computational natural language learning(conll), pages 66–76.
association for computa-tional linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019..alan prince and paul smolensky.
2004. optimalitytheory: constraint interaction in generative gram-mar.
blackwell pub., malden, ma..1168giulio ravasio and leonardo di perna.
2020.gilberto: an italian pretrained language modelbased on roberta..shauli ravfogel, yanai elazar, hila gonen, michaeltwiton, and yoav goldberg.
2020. null it out:guarding protected attributes by iterative nullspaceprojection.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7237–7256, online.
association for computa-tional linguistics..shauli ravfogel, yoav goldberg, and tal linzen.
2019.studying the inductive biases of rnns with syn-thetic variations of natural languages.
in proceed-ings of the 2019 conference of the north ameri-can chapter of the association for computationallinguistics: human language technologies, pages3532–3542.
association for computational linguis-tics..shauli ravfogel, yoav goldberg, and francis tyers.
2018. can lstm learn to capture agreement?
in proceedings of the 2018the case of basque.
emnlp workshop blackboxnlp: analyzing and in-terpreting neural networks for nlp, pages 98–107.
association for computational linguistics..luigi rizzi.
1986. null objects in italian and the the-.
ory of pro.
linguistic inquiry, 17(3):501–557..h. rohde, r. levy, and a. kehler.
2011. anticipatingexplanations in relative clause processing.
cogni-tion, 118(3):339–358..manuel romero.
2020..ruperta: the spanish.
roberta..martin schrimpf,.
idan blank, greta tuckute, ca-rina kauf, eghbal a. hosseini, nancy kanwisher,joshua tenenbaum, and evelina fedorenko.
2020.artiﬁcial neural networks accurately predict lan-biorxiv, pageguage processing in the brain.
2020.06.26.174482..sebastian schuster, yuxing chen, and judith degen.
2020. harnessing the linguistic signal to predictscalar inferences.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5387–5403, online.
association forcomputational linguistics..natalia silveira, timothy dozat, marie-catherinede marneffe, samuel bowman, miriam connor,john bauer, and christopher d. manning.
2014. agold standard dependency corpus for english.
inproceedings of the ninth international conferenceon language resources and evaluation (lrec-2014)..mariona taul´e, m. ant`onia mart´ı, and marta recasens.
2008. ancora: multilevel annotated corpora forcatalan and spanish.
in proceedings of the sixth in-ternational conference on language resources andevaluation..shiva upadhye, leon bergen, and andrew kehler.
2020. predicting reference: what do languagemodels learn about discourse models?
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages977–982, online.
association for computationallinguistics..alex warstadt, yu cao, ioana grosu, wei peng, ha-gen blix, yining nie, anna alsop, shikha bordia,haokun liu, alicia parrish, sheng-fu wang, jasonphang, anhad mohananey, phu mon htut, palomajeretic, and samuel r. bowman.
2019.investigat-ing bert’s knowledge of language: five anal-in proceedings of theysis methods with npis.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing,pages 2877–2887, hong kong, china.
associationfor computational linguistics..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020a.
blimp: the benchmark of lin-transactionsguistic minimal pairs for english.
of the association for computational linguistics,8:377–392..alex warstadt, yian zhang, xiaocheng li, haokunliu, and samuel r. bowman.
2020b.
learningwhich features matter: roberta acquires a pref-erence for linguistic generalizations (eventually).
in proceedings of the 2020 conference on empiri-cal methods in natural language processing, pages217–235, online.
association for computationallinguistics..elyce dominique williams.
2020. language experi-ence predicts pronoun comprehension in implicitcausality sentences.
master’s, university of northcarolina at chapel hill..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..daniel zeman, martin popel, milan straka,.
janhajiˇc, joakim nivre, filip ginter, juhani luotolahti,sampo pyysalo, slav petrov, martin potthast, fran-cis tyers, elena badmaeva, memduh gokirmak,anna nedoluzhko, silvie cinkov´a, jan hajiˇc jr.,jaroslava hlav´aˇcov´a, v´aclava kettnerov´a, zdeˇnkaureˇsov´a, jenna kanerva, stina ojala, anna mis-sil¨a, christopher d. manning, sebastian schuster,.
1169table 4: breakdown of pronouns added for spanishﬁne-tuning data.
pronoun person and number were de-termined by annotations in ud data, with na beingpronouns unmarked for number.
there were a total of4000 sentences comprised of 5559 tokens in the train-ing set..sg519993574.pl na4177944.
---.
sg6543992284.pl na41794679.
---.
123.
123.table 5: breakdown of pronouns added for italian ﬁne-tuning data.
pronoun person and number were deter-mined by annotations in ud data, with na being pro-nouns unmarked for number.
there were a total of3798 sentences comprised of 4608 tokens in the train-ing set..b expanded results (including mbert).
the full details of the pairwise t-tests conductedfor the present study are given below (includingthe results for mbert).
the results for englishmodels are in table 6, for chinese models table7, for spanish models table 8, and italian modelstable 9..siva reddy, dima taji, nizar habash, herman le-ung, marie-catherine de marneffe, manuela san-guinetti, maria simi, hiroshi kanayama, valeriade paiva, kira droganova, h´ector mart´ınez alonso,c¸ a˘grı c¸ ¨oltekin, umut sulubacak, hans uszkor-eit, vivien macketanz, aljoscha burchardt, kimharris, katrin marheinecke, georg rehm, tolgakayadelen, mohammed attia, ali elkahky, zhuoranyu, emily pitler, saran lertpradit, michael mandl,jesse kirchner, hector fernandez alcalde, jana str-nadov´a, esha banerjee, ruli manurung, antoniostella, atsuko shimada, sookyoung kwak, gustavomendonc¸a, tatiana lando, rattima nitisaroj, andjosie li.
2017. conll 2017 shared task: multi-lingual parsing from raw text to universal depen-dencies.
in proceedings of the conll 2017 sharedtask: multilingual parsing from raw text to univer-sal dependencies, pages 1–19.
association for com-putational linguistics..a additional fine-tuning training.
information.
the full breakdown of pronouns added or removedin the ﬁne-tuning training data are detailed below.
english can be found in table 2, chinese can befound in table 3, spanish can be found in table 4,and italian can be found in table 5..sg1927-1548.pl617-640.na-1252-.
123.table 2: breakdown of pronouns removed for englishﬁne-tuning data.
pronoun person and number were de-termined by annotations in ud data, with na beingpronouns unmarked for number.
there were a total of4000 sentences comprised of 66929 tokens in the train-ing set..sg pl na6656-212-774164-.
123.table 3: breakdown of pronouns removed for chineseﬁne-tuning data.
pronoun person and number were de-termined by annotations in ud data, with na beingpronouns unmarked for number.
there were a total of935 sentences comprised of 108949 characters in thetraining set..1170o-o µ o-s µ cimodel0.520.72bert0.520.75bert base0.520.51bert pro0.410.57roberta0.45roberta base 0.580.230.35roberta pro0.590.58mbert.
[0.19,0.21][0.11,0.13][0.14,0.15][0.15,0.17][0.11,0.13][0.11,0.13][-0.003,-0.01].
p< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−160.001.s-o µ s-s µ ci0.260.130.150.060.110.040.430.310.370.310.190.160.280.29.
[0.12,0.13][0.08,0.09][0.06,0.07][0.11,0.13][0.07,0.08][0.03,0.04][-0.002,-0.01].
p< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−160.0002.table 6: results from pairwise t-tests for english across the investigated models.
o-o refers to object antecedentafter object-biased ic verb and o-s to object antecedent after subject-biased ic verb (similarly for subject an-tecedents s-o and s-s).
ci is 95% conﬁdence intervals (where positive is an ic effect).
bert base andbert pro refer to models ﬁne-tuned on baseline data and data with a pro-drop process respectively..o-o µ o-s µ cimodelbert0.390.41bert base0.470.530.23bert pro0.230.330.40roberta0.46roberta base 0.520.290.32roberta pro0.070.08mbert.
[0.003,0.05][0.03,0.08][-0.02,0.02][0.04,0.08][0.04,0.08][0.002,0.06][0.01,0.03].
p0.000032.2e−60.941.16e−98.4e−77e−62e−6.
s-o µ s-s µ ci0.220.110.250.120.110.040.120.060.110.050.060.030.060.08.
[0.09,0.12][0.11,0.14][0.05,0.07][0.04,0.06][0.05,0.07][0.02,0.04][-0.009,-0.002].
p< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−16< 2.2e−161.3e−5.
table 7: results from pairwise t-tests for chinese across the investigated models.
o-o refers to object antecedentafter object-biased ic verb and o-s to object antecedent after subject-biased ic verb (similarly for subject an-tecedents s-o and s-s).
ci is 95% conﬁdence intervals (where positive is an ic effect).
bert base andbert pro refer to models ﬁne-tuned on baseline data and data with a pro-drop process respectively..o-o µ o-s µ cimodel0.460.53bert0.300.37bert base0.670.73bert proroberta0.100.090.06roberta base 0.060.480.48roberta prombert0.110.12.
[0.04,0.09][0.05,0.08][0.05,0.07][-0.008,-0.01][-0.005,-0.002][-0.03,0.01][0.001,0.01].
p1.4e−88e−12< 2.2e−160.030.00020.420.02.s-o µ s-s µ ci0.050.050.030.030.130.160.060.060.040.040.300.290.020.02.
[0.0007,0.01][-0.004,0.007][0.01,0.03][0.0007,0.007][-0.0003,0.004][-0.006,0.02][-0.0002,-0.002].
p0.030.611.2e−70.020.090.240.03.table 8: results from pairwise t-tests for spanish across the investigated models.
o-o refers to object antecedentafter object-biased ic verb and o-s to object antecedent after subject-biased ic verb (similarly for subject an-tecedents s-o and s-s).
ci is 95% conﬁdence intervals (where positive is an ic effect).
bert base andbert pro refer to models ﬁne-tuned on baseline data and data with a pro-drop process respectively..o-o µ o-s µ cimodel0.190.21bert0.160.17bert base0.560.63bert pro0.050.06umberto0.09umberto base 0.120.580.67umberto pro0.250.26gilberto0.240.24gilberto base0.500.54gilberto pro0.140.13mbert.
[0.005,0.03][0.006,0.02][0.04,0.07][0.01,0.02][0.02,0.04][0.07,0.11][-0.006,0.02][-0.006,0.01][0.03,0.06][-0.004,-0.02].
p0.0040.0021e−134e−63e−95e−160.300.443e−70.0003.s-o µ s-s µ ci0.110.090.080.060.320.260.020.0090.020.010.280.190.220.200.180.160.450.400.130.12.
[0.01,0.03][0.01,0.02][0.05,0.07][0.004,0.01][0.01,0.02][0.07,0.11][0.01,0.03][0.01,0.03][0.04,0.07][0.003,0.02].
p1.3e−94e−6< 2.2e−162e−99e−12< 2.2e−160.00023e−73e−100.003.table 9: results from pairwise t-tests for italian across the investigated models.
o-o refers to object antecedentafter object-biased ic verb and o-s to object antecedent after subject-biased ic verb (similarly for subject an-tecedents s-o and s-s).
ci is 95% conﬁdence intervals (where positive is an ic effect).
bert base andbert pro refer to models ﬁne-tuned on baseline data and data with a pro-drop process respectively..1171