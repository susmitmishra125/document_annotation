text-free image-to-speech synthesis using learned segmental units.
wei-ning hsu1∗, david harwath2∗, tyler miller2∗, christopher song3, james glass11massachusetts institute of technology2university of texas at austin3johns hopkins universitywnhsu@csail.mit.edu, harwath@utexas.edu.
abstract.
in this paper we present the ﬁrst model fordirectly synthesizing ﬂuent, natural-soundingspoken audio captions for images that doesnot require natural language text as an inter-mediate representation or source of supervi-sion.
instead, we connect the image caption-ing module and the speech synthesis modulewith a set of discrete, sub-word speech unitsthat are discovered with a self-supervised vi-sual grounding task.
we conduct experimentson the flickr8k spoken caption dataset in addi-tion to a novel corpus of spoken audio captionscollected for the popular mscoco dataset,demonstrating that our generated captions alsocapture diverse visual semantics of the imagesthey describe.
we investigate several differentintermediate speech representations, and em-pirically ﬁnd that the representation must sat-isfy several important properties to serve asdrop-in replacements for text..1.introduction.
although there are over 7,000 languages spokenworldwide (lewis et al., 2016), only several dozenhave enough data available to support supervisedspeech recognition, and many languages do noteven employ a writing system (adda et al., 2016).
in contrast, most people learn to use spoken lan-guage long before they learn to read and write,suggesting that linguistic annotation is not a pre-requisite for speech processing systems.
this lineof reasoning motivates research that aims to dis-cover meaningful linguistic abstractions (phones,words, etc.)
directly from the speech signal, withthe intention that they could reduce the reliance ofspoken language systems on text transcripts..a rich body of work has recently emerged inves-tigating representation learning for speech usingvisual grounding objectives (synnaeve et al., 2014;harwath and glass, 2015; harwath et al., 2016;.
kamper et al., 2017; havard et al., 2019a; merkxet al., 2019; chrupała et al., 2017; alishahi et al.,2017; scharenborg et al., 2018; hsu and glass,2018a; kamper et al., 2018; surís et al., 2019; il-harco et al., 2019; eloff et al., 2019), as well ashow word-like and subword-like linguistic unitscan be made to emerge within these models (har-wath and glass, 2017; harwath et al., 2019; drexlerand glass, 2017; alishahi et al., 2017; harwathet al., 2019; harwath and glass, 2019; havard et al.,2019b; harwath et al., 2020).
so far, these effortshave predominantly focused on inference, wherethe goal is to learn a mapping from speech wave-forms to a semantic embedding space.
generationof speech conditioned on a point in a semanticspace has been less explored, and is what we focuson in this work.
we hypothesize that generativeapproaches offer interesting advantages over rely-ing solely on inference.
for example, prior workshave demonstrated the capability of recognizing vi-sually descriptive words, but have not been shownto learn non-visual words or grammar.
our experi-ments show that these aspects of spoken languageare learned to some degree by a visually-groundedgenerative model of speech..speciﬁcally, we introduce a model capable ofdirectly generating ﬂuent spoken audio captions ofimages without the need for natural language text,either as an intermediate representation or a formof supervision during training (figure 1).
tremen-dous progress has been made recently in naturallanguage image caption generation (kiros et al.,2014; mao et al., 2015; vinyals et al., 2015; karpa-thy and fei-fei, 2015; xu et al., 2015; rennie et al.,2017; dai and lin, 2017; lu et al., 2017; andersonet al., 2018; lu et al., 2018) and naturalistic text-to-speech synthesis (tts) (ping et al., 2017; taigmanet al., 2017; wang et al., 2017; shen et al., 2018;oord et al., 2016).
combining these models pro-vides a means for generating spoken image descrip-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5284–5300august1–6,2021.©2021associationforcomputationallinguistics5284figure 1: spoken image captions generated from the proposed model, with diversity in both linguistic content andacoustic properties, controlled through the i2u and the u2s models, respectively.
transcriptions are provided onlyfor illustration.
audio samples are available at https://wnhsu.github.io/image-to-speech-demo..tions, but existing approaches for training thesemodels are reliant on text during training.
instead,we leverage sub-word speech units discovered us-ing a self-supervised learning objective as a drop-inreplacement for the text.
we hypothesize that byusing such techniques, an even wider variety of tra-ditionally text-based nlp models could be appliedto speech data without the need for transcription orautomatic speech recognition (asr) systems.
be-cause all human languages utilize small, discretephonetic inventories (international phonetic asso-ciation, 1999), we posit that our framework shouldbe applicable for any language in the world.
in ourexperiments, we demonstrate that not just any setof discovered speech units can function in this role.
we ﬁnd the greatest success with units that are dis-crete, exhibit a low frame-rate, and highly robustto speaker and environmental variability.
the maincontributions of our paper are as follows:.
1. the ﬁrst methodology for ﬂuent image-to-speech synthesis that does not rely on text.
acritical aspect of our approach is factorizing themodel into an image-to-unit (i2u) module and aunit-to-speech (u2s) module, where the speechunits are discovered in a self-supervised fashion.
this approach enables disentanglement of linguis-tic variability and acoustic/speaker variability..2. extensive analysis on the properties re-quired for learned units to replace text.
whilethe idea may seem simple and straightforward, ob-taining proper units is not a trivial task.
in fact,most of the units experimented in this paper failto serve as drop-in replacements.
moreover, wedemonstrate that what are deemed good units varysigniﬁcantly for inference and generation..3..demonstrating insufﬁciency of beamsearch-based evaluation.
we show that evenwhen an i2u model fails to generate sensible cap-tion through beam search decoding, it can still pro-.
duce reasonable captions by sampling from the pos-terior, hinting that posterior mode-based evaluationcan only inspect limited aspects of a model..4. proposing a semantic diversity-aware met-ric.
we identify issues of an existing metric (vi-jayakumar et al., 2018) and propose m-spice forsampling-based evaluation to address the problems.
5. over 600,000 spoken audio captions forthe mscoco dataset.
we collect 742 hours ofspeech from 2,352 people tasked with reading eachcaption out loud.
this dataset will be made pub-licly available to support work at the intersectionof speech, language, and vision..2 related work.
image-to-text and image-to-speech caption-ing.
signiﬁcant progress towards generating re-alistic (text) captions that describe the content ofvisual images was made with the advent of deepneural networks (vinyals et al., 2015; karpathyand fei-fei, 2015; xu et al., 2015; anderson et al.,2018).
far less work has focused on generat-ing spoken audio captions from natural images.
training an image-to-speech system using separate(image, text) and (text, speech) datasets was ex-plored in (ma et al., 2019).
hasegawa-johnsonet al.
(2017) is the only prior work that has ex-plored image-to-speech synthesis without usingtext, but with limited results.
in that work, bleuscores were only computed in terms of unsuper-vised acoustic units, not an estimate of the actualwords produced by the synthesizer, which can beproblematic as discussed in section 4. the result-ing captions were not evaluated for ﬂuency, nat-uralness, or intelligibility, and the bleu scoresin terms of the unsupervised units were very low(0.014 on the mscoco test set) compared toours (0.274).
wang et al.
(2020b) is a concurrentwork that proposes a text-free end-to-end image-to-.
5285a person in a blue jacket is on a snowboard on a snow covered slopea snowboarder is snowboarding on the side of the mountaina snowboarder is snowboarding on the side of the mountainsame unit sequence, different speakersdifferent unit sequences, same speakerspeech model, which simpliﬁes the task by usingpairs of image and synthesized speech generatedfrom a single-speaker tts model to reduce theacoustic variation.
in contrast, by leveraging robustlearned units, our i2u module can be trained onreal speech with abundant variation, and the u2smodule serves as a vocoder that requires a smallamount of clean speech (transcripts not needed).
hence, our system imposes less data constraintsyet still outperforms wang et al.
(2020b)..voice conversion without text aims to convertthe speaker identity in a recording while preserv-ing the textual content (abe et al., 1990; stylianouet al., 1998; toda et al., 2007).
it has recently seenprogress using neural approaches (hsu et al., 2016,2017a,b; fang et al., 2018; chorowski et al., 2018;chou et al., 2018; lorenzo-trueba et al., 2018;serrà et al., 2019), but the most relevant work toour own is the zerospeech 2019 challenge (dunbaret al., 2019; tjandra et al., 2019; cho et al., 2019),which addresses unsupervised learning of discretespeech units that can replace text and be used asinput to tts models.
unlike image-to-speech syn-thesis, these tasks only infer phonetic units fromgiven audio recordings instead of generating ones.
speech pre-training and its applications.
in-terest in this area has recently surged.
various learn-ing objectives have been proposed, including auto-encoding with structured latent spaces (van denoord et al., 2017; eloff et al., 2019; chorowskiet al., 2019; hsu et al., 2017b; hsu and glass,2018b; khurana et al., 2019), predictive cod-ing (chung et al., 2019; wang et al., 2020a), con-trastive learning (oord et al., 2018; schneider et al.,2019), and more.
prior work addresses inferringlinguistic content such as phones from the learnedrepresentations (baevski et al., 2020; kharitonovet al., 2020; hsu et al., 2021).
in contrast, thiswork focuses on generating the learned represen-tation from a different modality, which evaluatesrepresentations from a different perspective..3 method.
3.1 framework overview.
a depiction of our modeling approach is shown infigure 2. caption generation for an image involvesa cascade of two components: given an input im-age i, we ﬁrst generate a linguistic unit sequenceu according to the i2u module p (u | i).
giventhe linguistic symbol sequence u , we generate aspeech waveform s according to the u2s module.
p (s | u ).
if the linguistic unit sequence u wereto take the form of natural language text, the modelwould be equivalent to the cascade of a conven-tional image captioning system followed by a ttsmodule.
note that we assume s ⊥ i | u becauseprosody variation is not dependent on the imagefor the datasets considered..the key idea in this paper is to instead deﬁne uto be a sequence of learned speech units that are asrobust and compact as possible like text, but discov-ered without text supervision.
we deﬁne inferencewith this s2u model as u = f (s), enabling usto “transcribe” any given speech audio waveforms into a sequence of units u .
the addition ofthis third component enables us to train p (u | i)from a dataset of images paired with spoken cap-tions {(i1, s1), .
.
.
, (in , sn )}.
the conditionalindependence assumption between s and i giventhe u enables us to choose any arbitrary speechdataset for training p (s | u ), therefore enablingthe speaker characteristics and other acoustic prop-erties to be independently controllable from thei2u system (wang et al., 2018; hsu et al., 2019;henter et al., 2018; akuzawa et al., 2018)..3.2 datasets.
table 1 summarizes the ﬁve datasets used for train-ing s2u, i2u, and u2s models.
note that wedeliberately choose different datasets for trainingeach module, which aims to examine the robust-ness of the units when transferring across domains,including shift in speaker demography, speakingstyle (scripted/spontaneous), and linguistic content(book/newspaper/image description).
among thethree datasets with image and speech pairs: places,flickr8k, mscoco, we chose the latter two fortraining i2u models, because they include ﬁve cap-tions per image, which is more suitable for captionmetrics such as spice (anderson et al., 2016);moreover, they are commonly used image caption-ing datasets with many text-based baselines in theliterature.
places only contains one spoken captionper image and has not been used for captioning..speciﬁcally, as part of this work we collect spo-kencoco, a spoken version of the mscoco cap-tioning dataset (lin et al., 2014) with 742 hoursfrom 2532 speakers, via amazon mechanical turkby displaying the text to a person and havingthem read it aloud.
additional details regardingthe dataset can be found in appendix section a.note that although there exists a speech version.
5286figure 2: diagram of our proposed framework.
the resdavenet-vq model was trained using a {2} → {2, 3}curriculum (in the notation given in harwath et al.
(2020))..data.
#utt.
#spk maj. spk.
description.
s2u placesaudio (harwath et al., 2016).
400k.
2683 american.
spontaneous image caption.
i2u.
u2s.
flickr8kaudio (harwath and glass, 2015)spokencoco (this work).
ljspeech (ito, 2017)vctk (veaux et al., 2017).
1832353.
40k605k.
13k40k.
american.
scripted image caption.
1 americanbritish.
109.read non-ﬁction booksread newspaper.
hr.
936.
46742.
2444.table 1:mscoco (lin et al., 2014), flickr8k (rashtchian et al., 2010), and places (zhou et al., 2014), are also used..speech dataset summary.
for training s2u and i2u models, their corresponding image datasets,.
of mscoco named speech-coco (havard et al.,2017), it is comprised of only synthesized speechusing a concatenative tts model in eight speak-ers’ voice.
disﬂuencies (e.g.
“uh”) are randomlyinserted in between words to imitate real speech.
compared to spokencoco, speech-coco offerslimited diversity and naturalness..3.3 learning robust linguistic units from.
visually-grounded speech.
we propose to build the s2u model uponresdavenet-vq, an audio-visual groundingmodel introduced in harwath et al.
(2020) thathas shown to learn discrete phone- and word-like units in the intermediate vector quantizing(vq) layers.
this model is trained to associatespeech with contextually relevant visual inputs us-ing a triplet loss (weinberger and saul, 2009),which can be interpreted as maximizing a mu-tual information lower bound between image andspeech (tschannen et al., 2020).
since visual se-mantics are described with words, which in turn arecomposed of phones, the representations learnedby resdavenet-vq are forced to be predictive ofwords and phones rather than speaker, noise, etc.
in contrast, many of the speech representationsare trained by reconstructing (chorowski et al.,2019; hsu et al., 2017b) or predicting unseenspeech signals (chung et al., 2019), which wouldinevitable capture factors unrelated to the linguistic.
content.
to demonstrate the advantage of repre-sentation learning with grounding, we will com-pare resdavenet-vq with a reconstruction basedmodel, wavenet-vq, trained on the placesaudiodataset.
we denote the units extracted from thismodel with wvq.
we use the implementation ofharwath et al.
(2020) for resdavenet-vq, andcho et al.
(2019) for wavenet-vq which achievesthe best zerospeech 2019 challenge performance..3.4 unit selection and run length encoding.
although the resdavenet-vq model has beenshown to be capable of learning both phone-likeand word-like units, the experiments in (harwathet al., 2020) show that only several hundred wordsare explicitly learned, which tend to be “visualwords.” conversely, the phone-like units learnedby the lower vq layers of the model were shownto cover all of the phones in american english (asthere are only several dozens).
for this reason, wechoose to use phone-like units learned by the lowervq layers to represent u ..nominally, the vq layers will output one-hotvectors at a uniform temporal rate, downsampledwith respect to the framerate of the acoustic inputdepending upon which vq layer is used.
givenan input computed with a 10ms frame shift, thetwo vq layers investigated in this paper (vq2and vq3) respectively output vectors every 20msand 40ms.
in general, the vq units are repeated.
52873 * (resblock + vq)263 32 32 32 32 208 208 5 5 5 476 570 395 16...263 32 208 5 476 570 395 16...speech-to-unit model (resdavenet-vq)pre-trained on (image,speech) pairst / 4 units(fixed unit rate)lossy run-length encodingn (≤ t / 4) units(variable unit rate)t framesresnet-1011l lstm + attention263 32 208 5 476 570 395 16...image-to-unit model (show, attend, and tell)14x14 feature map3l conv+ 1l blstmpre-net + lstm + attention + postnet...unit-to-speech model (tacotron 2)n feature vecsimagelearned unitsspeechfor several consecutive frames.
we can decreasethe average length of the symbol sequence u byemploying a lossy form of run-length encoding(rle) (see figure 2) which retains the sequenceof symbol identities but discards duration informa-tion.
each unit then represents a variable-lengthsegment.
this removes the burden of unit durationmodeling from the i2u model and shifts it onto theu2s model, which we will show to be crucial..3.5.image-to-unit and unit-to-speech.
both the i2u model and the u2s model arebased upon recurrent seq2seq with attention net-works (bahdanau et al., 2015).
speciﬁcally, weadopt show-attend-and-tell (sat) (xu et al.,2015) for the i2u model.
it has an image encoderpre-trained for classiﬁcation, which is languageagnostic and hence should work in any languagewithin our proposed framework.
the decoder onthe other hand is randomly initialized.
we train thesat model for two stages, where the encoder pa-rameters are only updated in the second stage.
wedistinguish the models from the two stages withsat and sat-ft (ﬁnetuned) respectively whenpresenting the results.
for the u2s model, weadopt tacotron2 (shen et al., 2018) and waveg-low (prenger et al., 2019) for unit-to-spectrogramand spectrogram-to-waveform generation, respec-tively.
in particular, a pre-trained waveglow isused without ﬁne-tuning..the i2u model is trained on (i, f (s)) pairs,which requires pairs of image and speech, whilethe u2s model is trained on (f (s), s) pairs, whichcan be obtained from arbitrary set of speech.
both models are trained with the maximum like-lihood objective (ei,u [log p (u | i)] for i2u andes,u [log p (s | u )] for u2s)..4 experiments.
we design experiments to address three questions:first, how can we measure the performanceof an image-to-speech system?
our system canfail to produce a good caption if the i2u model failsto encode linguistic/semantic information into theunit sequence, or if the u2s model fails to synthe-size an intelligible waveform given a unit sequence.
to better localize these failure modes, we evaluatethe full i2s system as well as the u2s system inisolation.
we evaluate the u2s system by using itas a vocoder to synthesize unit sequences inferredfrom real speech and soliciting human judgements.
in the form of mean opinion score (mos) andside-by-side (sxs) preference tests (table 2)..to evaluate the i2s system, we can use anymethod that measures the semantic informationcontained in the generated speech.
we considertwo sets of end-to-end metrics: word-based andretrieval-based, and one set of proxy unit-basedmetrics.
word-based metrics transcribe a gener-ated spoken caption into text (manually or with anasr system) and then measure word-based cap-tioning metrics against a set of reference captions,such as bleu-4 (papineni et al., 2002) (adjusted n-gram precision), meteor (denkowski and lavie,2014) (uni-gram f-score considering word-to-wordalignment), rouge (lin, 2004) (n-gram recall),cider (vedantam et al., 2015) (tf-idf weightedn-gram cosine similarity), and spice (andersonet al., 2016) (f-score of semantic propositions inscene graphs).
this enables comparison betweenimage-to-speech systems with a text “upperbound”,but is not applicable to unwritten languages..retrieval-based metrics include image-to-speechand speech-to-image retrieval (harwath et al.,2020), which require a separately trained cross-modal retrieval model for evaluation.
such metricsare text-free, but they cannot measure other aspectsof language generation such as syntactic correct-ness (partially captured by bleu-4) and scope ofthe learned vocabulary.
lastly, unit-based metricsare similar to text-based, but replace words withunits when computing n-gram statistics.
however,systems built on different units are not directly com-parable, and second, can be inﬂated if duration ismodeled using unit repetition..second, what properties must learned unitshave to be a drop-in replacement for text?
themost essential differences between text and speechare the amount of information encoded and the se-quence lengths.
beyond text, speech also encodesprosody, speaker, environment information and theduration for each phone, all of which are minimallycorrelated with the conditioned images.
we hypoth-esize that learned speech units should discard suchinformation in order to seamlessly connect the i2uand u2s modules.
to verify it, we pay particularattention to the variations of the learned units inframe rate (vq2/vq3), encoding of duration in-formation (rle or not), and robustness to domainshift (wvq/vq3).
units are run-length encodedby default.
table 2a shows the properties of theunits before run-length encoding..5288third, how should language generation mod-els be evaluated more generally?
we examineevaluation of the i2s model using beam search-based decoding as well as sampling-based decod-ing.
we ﬁnd that because evaluation metrics thatare reliant on beam search-based decoding onlyevaluate the mode of a model’s posterior, they donot reﬂect the ability of a model to generate diverselinguistic content.
furthermore, we show that it ispossible for a model’s posterior mode to be linguis-tically meaningless, and yet meaningful languagecan still be generated with sampling-based decod-ing.
towards this end, we introduce a novel multi-hypothesis evaluation metric (m-spice), whichuses sampling-based decoding (instead of beamsearch) to generate a set of captions.
we can thencompute the overall coverage of this caption setagainst a reference; see section 4.4 for details..4.1 evaluating the u2s model.
we construct a tacotron-2 model for each of thethree unit types on the ljspeech audio data bytranscribing each ljspeech utterances into an unitsequence, then train the u2s model from the rle-ed unit sequence and spectrogram pairs.
we eval-uate the naturalness of the speech produced byeach model on held-out data, both in-domain usingljspeech and out-of-domain (ood) using spoken-coco.1 amazon mechanical turk (amt) work-ers performed side-by-side preference tests (sxs)and naturalness evaluation based on mean opinionscores (mos) on a scale from 1 to 5 for each u2smodel, which we display in table 2. although vq2was preferred for in-domain synthesis on ljspeech,vq3 achieved the highest scores and least degrada-tion (-0.387) on the out-of-domain spokencoco,indicating that out of the three units vq3 has thestrongest robustness to domain shift..4.2.incorporating the i2u model.
we trained an sat model on spokencoco foreach of the three rle-ed units, as well as vq3units without rle.
we also compare to text charac-ters and words; the full hyperparameter and train-ing details for all models are provided in sectionb in the appendix, but in general we kept theseas constant as possible when comparing differentlinguistic representations..before connecting the u2s model, we noticedthat all rle speech unit models except the one.
1in-domainess is deﬁned with respect to the u2s training.
data (ljspeech) not the s2u training data (placesaudio)..unit.
abxerror.
framerate.
mos.
ljspeech.
spokencoco.
14.52% 40msvq3vq212.51% 20mswvq 24.87% 40ms.
3.723 ± 0.0393.932 ± 0.0363.658 ± 0.040.
3.336 ± 0.0442.961 ± 0.0452.896 ± 0.053.
(a) properties of the units and mos of the u2s models trainedon these units with 95% conﬁdence interval.
abx errors arecomputed on the zerospeech 2020 english test set..unit.
a.b.a.ljspeechsame.
b.spokencocosame.
b.a.vq3 vq223.9vq3 wvq 36.6.
31.537.1.
44.626.3.
40.458.3.
32.521.8.
27.519.9.
(b) sxs preference (%) of the u2s models..table 2: subjective evaluation of u2s models trainedon ljspeech and re-synthesize units inferred fromljspeech or spokencoco recordings..symbol.
image-to-unit output.
vq3.
vq2wvqvq3 \ rle.
decoded with beam search (beam size=5)263 32 208 5 336 100 717 803 256 803 815 144 120144 654 936 48 417 272 417 362 766 825 284 614...(71 791)*n (until reaching max decoder length)(181 232)*n (until reaching max decoder length)263 (32)*n (until reaching max decoder length).
vq3.
vq2.
wvq.
vq3 \ rle.
decoded with top-k sampling search (k=5).
263 208 467 717 288 426 986 72 44 341 151 801 102227 320 426 288 66 570 683 351 313 910 820...(71 791)*4 175 51 139 359 173 599 307 419 133 62185 165 315 883 175 191 71 791 71 48 511 765...(181 232)*5 181 225 124 232 181 232 225 232 181 225124 225 232 181 252 169 211 147 89 67 156...263 (32)*15 208 208 5 5 336 100 803 256 560 417 870870 870 968 910 250 543 820 587 909 909....table 3: exemplar output from sat models..trained on vq3 units failed during beam searchdecoding on the test images (wvq consistentlyfailed, while vq2 sometimes succeeded); ratherthan producing a diverse sequence of output units,the decoder would generally get stuck in a loopuntil the maximum decoding length was reached.
this also happened using vq3 units without rle,indicating that the decoder could not model unitduration.
example outputs are provided in table 3.we hypothesize that the reason the vq2 and wvqunits failed is due to their lack of invariance todomain shift, as evidenced by their decay in nat-uralness when used for ood synthesis as shownin table 2. this may cause the entropy of the unitdistribution conditioned on an image to be higheras each phoneme may be represented by multipleunits, and therefore the i2u model suffers fromthe same looping issues as the unconditional lan-guage model of text, as observed in (holtzmanet al., 2018; fan et al., 2018; holtzman et al., 2020;.
5289model.
u.b-4.
m.mscocor.b-4.
m.flickr8kr.xu et al.
(2015)lu et al.
(2017).
wordword.
0.2430.327.
0.2390.260.
-0.540.
-1.042.
0.213-.
0.203-.
--.
wang et al.
(2020b).
n/a.
-.
-.
-.
0.035.
0.113.
0.232.
0.080.c.-.
s.--.
-.
c.--.
s.--.
-.
sat.
sat-ft.wordcharvq3.
wordcharvq3.
0.3150.2890.186.
0.3390.3230.233.
0.2530.2390.186.
0.2650.2560.212.
0.5330.5120.446.
0.5510.5360.478.
0.9840.8790.584.
1.0621.0020.732.
0.1850.1720.127.
0.1960.1870.149.
0.2160.1900.116.
0.2250.1910.125.
0.2070.1900.141.
0.2150.1960.145.
0.4690.4410.390.
0.4830.4500.391.
0.5500.4760.232.
0.5840.5190.245.
0.1490.1360.091.
0.1550.1430.095.table 4: word-based caption evaluation using bleu-4, meteor, rouge, cider, and spice.
asr is usedto transcribe the spoken captions generated by the proposed vq3 model into text for evaluation.
the beam size∈ {1, 3, 5, 8, 10} was chosen for each model to maximize spice.
our word-based sat models outperform (xuet al., 2015) because we use a stronger image encoder (resnet-101)..symbol.
word-based.
unit-based.
retrieval-based.
b-4.
m.r.c.s.b-4.
m.r.c.image to speechr@5.r@10.r@1.speech to imager@5.r@10.r@1.vq3.
0.233.
0.212.
0.478.
0.732.
0.149.
0.261.
0.198.
0.334.
0.211.
0.240.
0.603.
0.766.
0.265.
0.611.
0.765.sat-ft model, decoded with beam search.
sat model, decoded with beam search.
vq3vq2wvqvq3 \ rle.
0.1860.0680.0100.000.
0.1860.1380.0690.002.
0.4460.3430.2860.001.
0.5840.2620.0090.000.
0.1270.0840.0110.001.
0.2740.1720.0200.163.
0.1960.1320.0480.168.
0.3280.1780.0810.218.
0.2150.0270.0000.000.
0.1570.090.0000.000.
0.4510.2890.0050.003.
0.6230.4260.0100.007.
0.1580.0930.0010.001.
0.4500.2830.0060.006.
0.6110.4200.0110.011.table 5: comparison of the three sets of metrics on different units and models trained on mscoco..kulikov et al., 2019; welleck et al., 2020)..to evaluate the full image-to-speech model, weﬁrst train an asr system on the re-synthesizedspokencoco captions using the vq3 tacotron-2model.
this enables us to estimate a word-leveltranscription of the spoken captions produced byour system.
in order to verify that the synthesizedcaptions are intelligible to humans and the asrsystem did not simply learn to recognize artifactsof the synthesized speech, we asked amt work-ers to transcribe into words a set of 500 captionsgenerated by our i2u→u2s system and also evalu-ated their naturalness.
three workers transcribedand three workers rated each caption, allowingus to compute an mos score (3.615±0.038), aword error rate (wer) between the 3 human tran-scriptions (9.40%), as well as an average wer be-tween the human and asr-produced transcriptions(13.97%).
this conﬁrms that our system producesreasonably natural speech and asr is sufﬁcientlyaccurate for transcribing synthesized speech..table 4 summarizes our results on mscocoand flickr8k using beam search.
we compare withthe literature for bottom-up text captioning (row1-2) and text-free end-to-end image-to-speech syn-thesis (row 3).
we train the decoder of an satmodel while keeping the image encoder ﬁxed (row.
4-6), in addition to ﬁne-tuning the encoder (row7-9).
despite having no access to text, the sat-ft speech captioning model trained on vq3 unitsachieves a bleu-4 score of .233 with beam searchdecoding on mscoco.
this is very close to the.243 achieved by the original sat word-based cap-tioning model.
figure 1 shows that the generatedcaptions are ﬂuent and reﬂect the implicit learningof some syntactic rules.
it is evident that the pro-posed model is capable of generating ﬂuent andmeaningful image captions..results comparing four unit representations onall three sets of metrics are shown in table 5. firstof all, by comparing word-based and unit-basedevaluations, we do note that the relative rankingamong vq3, vq2, and wvq is consistent acrossbleu-4, meteor, and rouge for sat models,however, vq3 \ rle achieves abnormally highscores on these metrics despite producing trivialcaptions for all images as shown in table 3. thisis because unit “32” has learned to represent non-speech frames such as silence, which frequentlyoccurs at both the beginning and end of utterances.
without rle, consecutive strings of “32” unitsare extremely common in both the candidate andreference captions, which inﬂates the scores of thismodel.
the exception here is the cider metric,.
5290figure 3: mscoco test spice scores of various units and decoding methods.
vq3\rle denotes vq3 unitswithout rle.
top-k sampling considers only the k-most probable units at each step..which incorporates tf-idf weighting that tends tode-emphasize these kinds of uninformative patterns.
nonetheless, when comparing sat and sat-ftwith vq3 units, cider does not rank them thesame as word-based metrics..regarding retrieval-based evaluation, despite thefact that the resdavenet model was only trainedon the original, human-spoken captions for themscoco images, it works very well for the fullysynthetic captions.
the speech and image retrievalscores for 1k human-spoken validation captionsare 0.867 and 0.828 r@10, respectively, whilethe sat-ft vq3 model achieves 0.766 and 0.765r@10. this indicates that this image-to-speechmodel is able to infer the salient semantic con-tent of an input image, generate a unit sequencethat captures that content, and generate speech thatis sufﬁciently natural sounding for the resdav-enet model to recover that semantic information.
several of the other image-to-speech models alsoachieve respectable retrieval performance, and theoverall ranking of the models mirrors that which wefound when using word-based evaluation metrics..4.3 from mode to distribution: evaluatingcaptions generated via sampling.
the results in the previous section only evaluatebeam search decoding with the i2u model, and donot fully reveal the posterior over captions for aninput image, or whether the unit representationsthat failed with beam search would work well withother methods.
to probe this, we evaluate the mod-els using sampling-based caption generation.
fig-ure 3 shows the spice scores on spokencocousing beam search and two sampling-based meth-ods.
vq3 still performs the best of all unit typeswith both beam search and sampled decoding.
vq2can sometimes generate captions with beam searchwhen the beam is kept small, but as the beam growsit begins to loop and the scores become very low..figure 4: vocabulary size learned by the proposed i2smodel (on mscoco).
figure 5: m-spice on mscoco.
black dashed linesshow the highest value for beam search when n=1..we see that all unit types can generate reasonablecaptions when decoding via sampling.
moreover,we discovered that 1) resdavenet-vq units con-sistently outperform the wavenet-vq units, sug-gesting that they better capture sub-word structure,and 2) vq3 \ rle achieves better scores than vq2when using a larger temperature or k for top-k..we estimated the vocabulary size of the sat-ftmodel with vq3 by counting the number of uniquerecognized words produced at least 3 times whencaptioning the spokencoco test images.
thesenumbers are shown for the model under the vari-ous decoding methods in figure 4. the number ofcaptions per image is denoted by n, where top can-didates are used for beam search and i.i.d.
samplesare drawn for sampling.
sampling-based decodingreveals a larger vocabulary size than beam search,and the number of words learned by our models(≥ 212) is far greater than the number of wordslearned by the resdavenet-vq model (approx..5291speaker gender.
region.
b4.
m.s.-.
p247p231p294p345p307.
u2s trained on ljspeech0.233.
-.
f.0.212.
0.149.u2s trained on vctk0.2340.2330.2360.2340.234.scottishenglishamericanamericancanadian.
mffmf.0.2110.2100.2120.2090.211.
0.1480.1460.1480.1440.148.table 6: results of disentangled voice control via syn-thesizing the same units with a single and a multispeaker u2s model.
units are decoded using beamsearch from the sat-ft vq3 mscoco model..279) in (harwath et al., 2020).
we hypothesizethat training a model to generate spoken captionsencourages it to learn many more words than onlybeing trained to retrieve images from captions.
wealso hypothesize that because beam search attemptsto ﬁnd the mode of the posterior over captions, ittends to produce a smaller set of words and doesnot reveal the breadth of the model distribution..4.4 new diversity-aware metric: m-spice.
the previous section showed that even when thespice scores were comparable, sampling-baseddecoding revealed a much larger model vocabularythan beam search, especially when multiple cap-tions are generated for each image.
this highlightsa limitation of spice in measuring the diversity.
formally speaking, spice computes an f-scorebetween two bags of semantic propositions t (s)and t (c) parsed from a set of references s = {si}iand a hypothesis c, where t (c) denotes a bag ofpropositions extracted from a scene graph parsedc, and we can compute that for multiple sentenceswith t (s) = ∪i(t (si))..(cid:80).
to extend spice for scoring multiple hypothe-ses c = {cj}jj=1, one can compute an averagespice: 1j f 1(t (s), t (cj)), or use the ora-jcle spice proposed in vijayakumar et al.
(2018):maxjf 1(t (s), t (cj)).
however, these metricsfail to capture the diversity among hypotheses.
consider two hypothesis set, c1 = {c12} andc2 = {c22}, where t (c11) ={(girl), (table), (girl, sit-at, table)}, t (c22) = {(girl),(girl, young)}, and t (s) = {(girl), (table), (girl,young), (girl, sit-at, table)}..1, c12) = t (c2.
1) = t (c1.
1, c2.
to address the deﬁciencies of the existing met-rics, we propose a new metric named multi-candidate spice (m-spice), which takes theunion of the candidate propositions and computes.
the f-score against the reference propositions:f 1(t (s), ∪jt (cj)).
m-spice assigns a higherscore if the set captures diverse and correct proposi-tions, and it is obvious that the score of c2 is higherthan c1 as desired.figure 5 shows the m-spicescores of our sat-ft model using vq3 units onspokencoco.
when evaluating over multiple cap-tions (n > 1), using the beam search hypothesesincreases the score less than sampling..4.5 disentangled voice control forimage-to-speech synthesis.
we examine to what extent the vq3 units areportable across different speakers by training a u2smodel on the vctk dataset that additionally takesa speaker id as input.
the resulting model is ableto generate speech with the voice of any vctkspeaker.
we evaluate the captions produced by thissystem on spokencoco for 5 speakers in table 6.to compute these scores we transcribe the cap-tions generated by each model into text using theasr system we describe in section 4.2, which wassolely trained on re-synthesized spokencoco cap-tions using the ljspeech u2s model.
the scoresin table 6 indicate not only that the i2u model canbe easily integrated with u2s models representinga diverse set of speakers, but also that the ljspeechasr system works very well on the speech synthe-sized from the vctk models..5 conclusion.
in this paper, we presented the ﬁrst model capa-ble of generating ﬂuent spoken captions of imageswithout relying on text, which almost matches theperformance of early text-based image captioningmodels.
our comprehensive experiments demon-strated that learned units need to be robust, of lowframerate, and encoding little or none duration in-formation to be a drop-in replacement for text.
wealso identiﬁed the caveats of mode-based evalua-tion and proposed a new metric to address seman-tic diversity.
as part of this work, a novel datasetof over 600k spoken captions for the mscocodataset is introduced, which we will make publiclyavailable to the research community..future work should investigate applying the pro-posed method to additional languages, devisingimproved speech unit representations, and jointlytraining the speech unit model with the i2s model.
this would offer the opportunity to explore newanalysis-by-synthesis training objectives..5292references.
masanobu abe, satoshi nakamura, kiyohiro shikano,and hisao kuwabara.
1990.voice conversionthrough vector quantization.
journal of the acous-tical society of japan, 11(2):71–76..gilles adda, sebastian stüker, martine adda-decker,odette ambouroue, laurent besacier, david bla-chon, hélene bonneau-maynard, pierre godard, fa-tima hamlaoui, dmitry idiatov, et al.
2016. break-ing the unwritten language barrier: the bulbproject.
procedia computer science, 81:8–14..kei akuzawa, yusuke iwasawa, and yutaka matsuo.
2018. expressive speech synthesis via modeling ex-pressions with variational autoencoder.
proc.
an-nual conference of international speech communi-cation association (interspeech)..afra alishahi, marie barking, and grzegorz chrupała.
2017. encoding of phonology in a recurrent neuralin proc.
acl confer-model of grounded speech.
ence on natural language learning (conll)..peter anderson, basura fernando, mark johnson, andstephen gould.
2016. spice: semantic proposi-tional image caption evaluation.
in proc.
ieee eu-ropean conference on computer vision (eccv)..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and visual question answering.
inproc.
ieee conference on computer vision and pat-tern recognition (cvpr)..alexei baevski, steffen schneider, and michael auli.
2020. vq-wav2vec: self-supervised learning of dis-crete speech representations.
in proc.
internationalconference on learning representations (iclr)..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin proc.
inter-learning to align and translate.
national conference on learning representations(iclr)..suhee cho, yeonjung hong, yookyunk shin, andyoungsun cho.
2019. vqvae with speaker adver-sarial training..jan chorowski, ron j. weiss, samy bengio, and aäronvan den oord.
2019. unsupervised speech represen-ieeetation learning using wavenet autoencoders.
transactions on audio, speech and language pro-cessing..jan chorowski, ron j weiss, rif a saurous, and samybengio.
2018. on using backpropagation for speechin proc.
texture generation and voice conversion.
international conference on acoustics, speech andsignal processing (icassp)..ju-chieh chou, cheng-chieh yeh, hung-yi lee, andlin-shan lee.
2018. multi-target voice conversion.
without parallel data by adversarially learning disen-tangled audio representations.
in proc.
annual con-ference of international speech communication as-sociation (interspeech)..grzegorz chrupała, lieke gelderloos, and afra al-ishahi.
2017. representations of language in amodel of visually grounded speech signal.
in proc.
annual meeting of the association for computa-tional linguistics (acl)..yu-an chung, wei-ning hsu, hao tang, and james r.glass.
2019. an unsupervised autoregressive modelfor speech representation learning.
in proc.
annualconference of international speech communicationassociation (interspeech)..bo dai and dahua lin.
2017. contrastive learning forimage captioning.
in proc.
neural information pro-cessing systems (neurips)..michael denkowski and alon lavie.
2014. meteor uni-versal: language speciﬁc translation evaluation forany target language.
in in proceedings of the ninthworkshop on statistical machine translation..jennifer drexler and james glass.
2017. analysis ofaudio-visual features for unsupervised speech recog-nition.
in proc.
grounded language understandingworkshop..ewan dunbar, robin algayres, julien karadayi, math-ieu bernard, juan benjumea, xuan-nga cao, luciemiskic, charlotte dugrain, lucas ondel, alan w.black, laurent besacier, sakriani sakti, and em-manuel dupoux.
2019. the zero resource speechin proc.
annualchallenge 2019: tts without t.conference of international speech communicationassociation (interspeech)..ryan eloff, herman engelbrecht, and herman kam-per.
2019. multimodal one-shot learning of speechin proc.
international conference onand images.
acoustics, speech and signal processing (icassp)..angela fan, mike lewis, and yann dauphin.
2018. hi-erarchical neural story generation.
in proc.
annualmeeting of the association for computational lin-guistics (acl)..fuming fang, junichi yamagishi, isao echizen, andjaime lorenzo-trueba.
2018. high-quality nonpar-allel voice conversion based on cycle-consistent ad-in proc.
international confer-versarial network.
ence on acoustics, speech and signal processing(icassp)..david harwath and james glass.
2015. deep mul-timodal semantic embeddings for speech and im-ages.
in proc.
ieee workshop on automatic speechrecognition and understanding (asru)..david harwath and james glass.
2017. learning word-like units from joint audio-visual analysis.
in proc.
annual meeting of the association for computa-tional linguistics (acl)..5293david harwath and james glass.
2019. towards vi-sually grounded sub-word speech unit discovery.
in proc.
international conference on acoustics,speech and signal processing (icassp)..david harwath, wei-ning hsu, and james glass.
2020.learning hierarchical discrete linguistic units fromin proc.
internationalvisually-grounded speech.
conference on learning representations (iclr)..david harwath, adrià recasens, dídac surís, galenchuang, antonio torralba, and james glass.
2019.jointly discovering visual objects and spoken wordsinternational journal offrom raw sensory input.
computer vision..david harwath, antonio torralba, and james r. glass.
2016. unsupervised learning of spoken languagein proc.
neural informationwith visual context.
processing systems (neurips)..mark hasegawa-johnson, alan black, lucas ondel,odette scharenborg, and francesco ciannella.
2017.image2speech: automatically generating audio de-in international conferencescriptions of images.
on natural language, signal and speech process-ing..william havard, laurent besacier, and olivier rosec.
2017. speech-coco: 600k visually grounded spokencaptions aligned to mscoco data set.
arxiv preprintarxiv:1707.08435..william havard, jean-pierre chevrot, and laurent be-sacier.
2019a.
models of visually grounded speechsignal pay attention to nouns: a bilingual experi-in proc.
interna-ment on english and japanese.
tional conference on acoustics, speech and signalprocessing (icassp)..william havard, jean-pierre chevrot, and laurent be-sacier.
2019b.
word recognition, competition, andactivation in a model of visually grounded speech.
in proc.
acl conference on natural languagelearning (conll)..gustav eje henter,.
jaime lorenzo-trueba, xinwang, and junichi yamagishi.
2018.deepencoder-decoder models for unsupervised learningarxiv preprintof controllable speech synthesis.
arxiv:1807.11470..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural textdegeneration.
in proc.
international conference onlearning representations (iclr)..ari holtzman, jan buys, maxwell forbes, antoinebosselut, david golub, and yejin choi.
2018.learning to write with cooperative discriminators.
in proc.
annual meeting of the association for com-putational linguistics (acl)..chin-cheng hsu, hsin-te hwang, yi-chiao wu,yu tsao, and hsin-min wang.
2016. voice con-version from non-parallel corpora using variational.
in asia-paciﬁc signal and infor-auto-encoder.
mation processing association annual summit andconference (apsipa)..wei-ning hsu and james glass.
2018a.
disentanglingby partitioning: a representation learning frame-work for multimodal sensory data.
arxiv preprintarxiv:1805.11264..wei-ning hsu and james glass.
2018b.
scalable fac-torized hierarchical variational autoencoder training.
in proc.
annual conference of international speechcommunication association (interspeech)..wei-ning hsu, yao-hung hubert tsai, benjaminbolte, ruslan salakhutdinov, and abdelrahman mo-hamed.
2021. hubert: how much can a bad teacherbeneﬁt asr pre-training?
in icassp.
ieee..wei-ning hsu, yu zhang, and james glass.
2017a.
learning latent representations for speech genera-in proc.
annual confer-tion and transformation.
ence of international speech communication asso-ciation (interspeech)..wei-ning hsu, yu zhang, and james glass.
2017b.
unsupervised learning of disentangled and in-terpretable representations from sequential data.
in proc.
neural information processing systems(neurips)..wei-ning hsu, yu zhang, ron weiss, heiga zen,yonghui wu, yuan cao, and yuxuan wang.
2019.hierarchical generative modeling for controllablespeech synthesis.
in proc.
international conferenceon learning representations (iclr)..gabriel ilharco, yuan zhang, and jason baldridge.
2019. large-scale representation learning from vi-in proc.
sually grounded untranscribed speech.
acl conference on natural language learning(conll)..international phonetic association.
1999. handbookof the international phonetic association: a guideto the use of the international phonetic alphabet.
cambridge university press..keith ito.
2017. the lj speech dataset.
https://.
keithito.com/lj-speech-dataset/..herman.
shane.
settle,.
kamper,.
gregoryshakhnarovich, and karen livescu.
2017.vi-sually grounded learning of keyword predictionin proc.
annual con-from untranscribed speech.
ference of international speech communicationassociation (interspeech)..herman kamper, gregory shakhnarovich, and karenlivescu.
2018. semantic speech retrieval with avisually grounded model of untranscribed speech.
ieee transactions on audio, speech and languageprocessing, pp:1–1..5294andrej karpathy and li fei-fei.
2015. deep visual-semantic alignments for generating image descrip-in proc.
ieee conference on computer vi-tions.
sion and pattern recognition (cvpr)..shuang ma, daniel mcduff, and yale song.
2019. un-paired image-to-speech synthesis with multimodalinformation bottleneck.
in proc.
ieee internationalconference on computer vision (iccv)..eugene kharitonov, morgane rivière, gabriel syn-pierre-emmanuel mazaré,naeve, lior wolf,matthijs douze, and emmanuel dupoux.
2020.data augmenting contrastive learning of speechrepresentations in the time domain.
arxiv preprintarxiv:2007.00991..sameer khurana, shaﬁq rayhan joty, ahmed ali, andjames glass.
2019. a factorial deep markov modelfor unsupervised disentangled representation learn-in proc.
international confer-ing from speech.
ence on acoustics, speech and signal processing(icassp)..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proc.
inter-national conference on learning representations(iclr)..ryan kiros, ruslan salakhutdinov, and rich zemel.
in2014. multimodal neural language models.
proc.
international conference on machine learn-ing (icml)..ilia kulikov, alexander miller, kyunghyun cho, andjason weston.
2019. importance of search and eval-uation strategies in neural dialogue modeling.
inproceedings of the 12th international conference onnatural language generation..m. paul lewis, gary f. simon, and charles d. fennig.
2016. ethnologue: languages of the world, nine-teenth edition.
sil international.
online version:http://www.ethnologue.com..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out..tsung-yi lin, michael maire, serge j. belongie, jameshays, pietro perona, deva ramanan, piotr dollár,and c. lawrence zitnick.
2014. microsoft coco:common objects in context.
arxiv, abs/1405.0312..jaime lorenzo-trueba, junichi yamagishi, tomokitoda, daisuke saito, fernando villavicencio, tomikinnunen, and zhenhua ling.
2018. the voice con-version challenge 2018: promoting development ofparallel and nonparallel methods.
arxiv preprintarxiv:1804.04262..junhua mao, wei xu, yi yang, jiang wang, zhihenghuang, and alan yuille.
2015. deep captioningwith multimodal recurrent neural networks (m-rnn).
in proc.
international conference on learning rep-resentations (iclr)..danny merkx, stefan l. frank, and mirjam ernestus.
2019. language learning using speech to imagein proc.
annual conference of interna-retrieval.
tional speech communication association (inter-speech)..aaron van den oord, oriol vinyals, and koraykavukcuoglu.
2017. neural discrete representationin proc.
neural information processinglearning.
systems (neurips)..aaron van den oord, sander dieleman, heiga zen,karen simonyan, oriol vinyals, alex graves,nal kalchbrenner, andrew senior, and koraykavukcuoglu.
2016. wavenet: a generative modelfor raw audio.
arxiv preprint arxiv:1609.03499..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics..wei ping, kainan peng, andrew gibiansky, ser-can o arik, ajay kannan, sharan narang, jonathanraiman, and john miller.
2017. deep voice 3:scaling text-to-speech with convolutional sequencelearning.
arxiv preprint arxiv:1710.07654..ryan prenger, rafael valle, and bryan catanzaro.
2019. waveglow: a ﬂow-based generative networkin proc.
international con-for speech synthesis.
ference on acoustics, speech and signal processing(icassp)..cyrus rashtchian, peter young, micah hodosh, andjulia hockenmaier.
2010. collecting image anno-tations using amazon’s mechanical turk.
in proc.
naacl conference on human language technolo-gies (naacl-hlt)..jiasen lu, caiming xiong, devi parikh, and richardsocher.
2017. knowing when to look: adaptive at-tention via a visual sentinel for image captioning.
inproc.
ieee conference on computer vision and pat-tern recognition (cvpr)..steven j rennie, etienne marcheret, youssef mroueh,jerret ross, and vaibhava goel.
2017. self-criticalin proc.
sequence training for image captioning.
ieee conference on computer vision and patternrecognition (cvpr)..jiasen lu, jianwei yang, dhruv batra, and devi parikh.
in proc.
ieee confer-2018. neural baby talk.
ence on computer vision and pattern recognition(cvpr)..odette scharenborg, laurent besacier, alan w. black,mark hasegawa-johnson, florian metze, grahamneubig, sebastian stüker, pierre godard, markusmüller, lucas ondel, shruti palaskar, philip arthur,.
5295francesco ciannella, mingxing du, elin larsen,danny merkx, rachid riad, liming wang, andemmanuel dupoux.
2018. linguistic unit discov-ery from multi-modal inputs in unwritten languages:summary of the "speaking rosetta" jsalt 2017in proc.
international conference onworkshop.
acoustics, speech and signal processing (icassp)..steffen schneider, alexei baevski, ronan collobert,and michael auli.
2019. wav2vec: unsupervisedpre-training for speech recognition.
in proc.
annualconference of international speech communicationassociation (interspeech)..joan serrà, santiago pascual, and carlos seguraperales.
2019. blow: a single-scale hypercondi-tioned ﬂow for non-parallel raw-audio voice conver-in proc.
neural information processing sys-sion.
tems (neurips)..jonathan shen, ruoming pang, ron j weiss, mikeschuster, navdeep jaitly, zongheng yang, zhifengchen, yu zhang, yuxuan wang, rj skerrv-ryan,et al.
2018. natural tts synthesis by conditioningin proc.
wavenet on mel spectrogram predictions.
international conference on acoustics, speech andsignal processing (icassp), pages 4779–4783.
ieee..yannis stylianou, olivier cappé, and eric moulines.
1998. continuous probabilistic transform for voiceconversion.
ieee transactions on speech and au-dio processing, 6(2):131–142..dídac surís, adrià recasens, david bau, david har-wath, james glass, and antonio torralba.
2019.learning words by drawing images.
in proc.
ieeeconference on computer vision and pattern recog-nition (cvpr)..gabriel synnaeve, maarten versteegh, and emmanueldupoux.
2014. learning words from images andspeech.
in proc.
neural information processing sys-tems (neurips)..yaniv taigman, lior wolf, adam polyak, and eliyanachmani.
2017. voiceloop: voice ﬁtting andsynthesis via a phonological loop.
arxiv preprintarxiv:1707.06588..andros tjandra, berrak sisman, mingyang zhang,sakriani sakti, haizhou li, and satoshi nakamura.
2019. vqvae unsupervised unit discovery andmulti-scale code2spec inverter for zerospeech chal-lenge 2019. arxiv preprint arxiv:1905.11449..tomoki toda, alan w black, and keiichi tokuda.
2007.voice conversion based on maximum-likelihood estimation of spectral parameter trajec-tory.
ieee transactions on audio, speech and lan-guage processing, 15(8):2222–2235..michael tschannen, josip djolonga, paul k. ruben-stein, sylvain gelly, and mario lucic.
2020. onmutual information maximization for representationin proc.
international conference onlearning.
learning representations (iclr)..christophe veaux, junichi yamagishi, and kirstencstr vctk corpus: en-macdonald.
2017.glish multi-speaker corpus for cstr voice cloningtoolkit..ramakrishna vedantam, c. lawrence zitnick, anddevi parikh.
2015. cider: consensus-based imagedescription evaluation.
proc.
ieee conference oncomputer vision and pattern recognition (cvpr)..ashwin k vijayakumar, michael cogswell, ram-prasaath r selvaraju, qing sun, stefan lee, davidcrandall, and dhruv batra.
2018. diverse beamsearch for improved description of complex scenes.
in proc.
aaai conference on artiﬁcial intelligence(aaai)..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in proc.
ieee conference oncomputer vision and pattern recognition (cvpr)..weiran wang, qingming tang, and karen livescu.
2020a.
unsupervised pre-training of bidirectionalspeech encoders via masked reconstruction.
in proc.
international conference on acoustics, speech andsignal processing (icassp)..xinsheng wang, siyuan feng,.
jihua zhu, markhasegawa-johnson, and odette scharenborg.
2020b.
show and speak: directly synthesize spoken descrip-tion of images.
arxiv preprint arxiv:2010.12267..yuxuan wang, r.j. skerry-ryan, daisy stanton,yonghui wu, ron weiss, navdeep jaitly, zonghengyang, ying xiao, zhifeng chen, samy bengio,quoc le, yannis agiomyrgiannakis, rob clark, andrif saurous.
2017. tacotron: towards end-to-endspeech synthesis.
in proc.
annual conference of in-ternational speech communication association (in-terspeech)..yuxuan wang, daisy stanton, yu zhang, rj skerry-ryan, eric battenberg, joel shor, ying xiao, feiren, ye jia, and rif a saurous.
2018. style tokens:unsupervised style modeling, control and transferarxiv preprintin end-to-end speech synthesis.
arxiv:1803.09017..kilian q. weinberger and lawrence k. saul.
2009.distance metric learning for large margin nearestneighbor classiﬁcation.
journal of machine learn-ing research (jmlr)..sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2020. neu-ral text generation with unlikelihood training.
inproc.
international conference on learning repre-sentations (iclr)..kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-tion.
in proc.
international conference on machinelearning (icml)..5296bolei zhou, agata lapedriza, jianxiong xiao, anto-nio torralba, and aude oliva.
2014. learning deepfeatures for scene recognition using places database.
in proc.
neural information processing systems(neurips)..a visually-grounded speech datasets.
table a1 displays details of the three visually-grounded speech datasets used in this paper.
whencomputing duration statistics, we exclude utter-ances longer than 15s for spokencoco andflickr8k audio, and 40s for places audio, becausewe found that those utterances resulted from in-correct operation of the data collection interface(e.g., workers forgot to stop recording).
when com-puting vocabulary sizes and word statistics, texttranscripts are normalized by lower-casing all thealphabets and removing characters that are neitheralphabets nor digits..for the spokencoco data collection on ama-zon mechanical turk, we displayed the text of amscoco caption to a user and asked them torecord themselves reading the caption out loud.
for quality control, we ran a speech recognitionsystem in the background and estimated the word-level transcription for each recording.
we com-puted the word error rate of the asr output againstthe text that the user was prompted to read, andonly accepted the caption if the word error ratewas under 30%.
in the case that the word errorrate was higher, the user was asked to re-recordtheir speech.
we paid the users $0.015 per cap-tion recorded, which in conjunction with the 20%overhead charged by amazon resulted in a totalcollection cost of $10,898.91..spokencoco.
flickr8kaudio.
placesaudio.
#utts#spks#imgs#utts-per-imgutt duration µutt duration σ#words/utt#words/sec.
durationvocab sizetype.
605495235312328754.12s1.31s10.452.41742hr19683scripted.
40000183800054.33s1.33s10.812.6346hr8718scripted.
400000268340000018.37s4.53s19.292.31936hr41217unscritped.
table a1:visually-grounded speech datasets used in the paper..statistics and properties of the three.
b detailed experimental setups.
in this section, we provide details about data pre-processing, model architecture, and training hy-perparameters for each module used in this paper.
the same setups are used for all unit types unlessotherwise stated..b.1.
image-to-unit model.
data images are reshaped to 256×256×3 ma-trices and are per-channel normalized with µ =[0.485, 0.456, 0.406] and σ =[0.229, 0.224, 0.225].
during training, unit sequences are truncated orpadded to the target length shown in table a2.
thetarget lengths are determined such that there areless than 10% sequences truncated while still al-lowing a reasonable batch size to be used.
unitsthat occurred less than ﬁve times are excluded.
se-quences are not truncated during evaluation.
wefollow the data splits used in (harwath et al., 2020)for places, and (karpathy and fei-fei, 2015) forflickr8k and spokencoco (the “karpathy split”)..word char vq3 vq2 wvq vq3 \ rle.
target lengthsequence truncated (%)batch size (sat)batch size (sat-ft).
181.128032.
701.746032.
1006.904020.
2009.3740-.
1107.8040-.
1606.3540-.
table a2: conﬁguration for each type of units used inthe image-to-unit model..an.
adopt.
open-source.
model were-implementation2 of show, attend, and tell (xuet al., 2015) (sat) with soft attention, whichreplaces the original cnn encoder with aresnet-101 pre-trained on imagenet for imageclassiﬁcation.
the last two layers of the resnet areremoved (a pooling and a fully-connected layer)such that the encoder produces a 14×14×2048feature map for each image..training adam (kingma and ba, 2015) witha learning rate of 10−4 is used for optimizingboth stages (sat and sat-ft).
the training objec-tive is maximum likelihood combined with a dou-bly stochastic attention regularization introducedin (xu et al., 2015) with a weight of 1. dropout isapplied to the input of decoder softmax layer witha probability of 0.5 during training.
gradients areclipped at 5 for each dimension.
the ﬁrst stage istrained for at most 30 epochs, and the best check-point from which is used to initialize the second.
2link to the sat implementation on github.
5297figure a1: utterance duration histograms for the three visually-grounded speech datasets..figure a2: m-spice f-score (same as figure 5) and recall on the spokencoco test set with different candidateproposal methods..stage trained for at most another 20 epochs.
mod-els are selected based on the unit bleu-4 scoreon the validation set.
using two nvidia titanx pascal gpus with data parallel training, eachepoch takes about 2.8 hours for vq3 units and 5.3hours for vq2 units..b.2 unit-to-speech model.
data rle-ed unit sequences are used as input forall systems (vq3 and vq3 \ rle systems sharethe same u2s model).
the native audio samplerates in ljspeech and vctk are 22050hz and48khz, respectively.
for consistency and compati-bility with the spectrogram-to-waveform model,we down-sample those in vctk to 22050hz.
following tacotron2, we compute a 80 dimen-sional mel spectrogram for each audio ﬁle witha 256-sample (11.6ms) frame hop, a 1024-sample(46.4ms) frame size, and a hann window function.
utterances longer than 8 seconds are discarded dur-ing training to accommodate for the gpu memoryconstraints.
we follow the data splits providedat https://github.com/nvidia/tacotron2 for.
ljspeech.
for the multi-speaker vctk dataset,we randomly sample 2.5% of the utterances fromeach speaker for validation..an re-implementation3 ofmodel we usetacotron2 (shen et al., 2018) for u2s models.
forsingle-speaker models trained on ljspeech, the ex-act same hyperparameters and model architectureare used as (shen et al., 2018).
for multi-speakermodels trained on vctk, we create an additionalspeaker embedding table of 256 dimensions for allspeakers and control the speaker identity throughthese speaker embeddings.
speaker embeddingsare injected at two places in the decoder: ﬁrstin concatenation with the original input to thedecoder lstm, and second in concatenation withthe output of the decoder lstm, right beforepredicting the stop token and the spectra of aframe.
a pre-trained4 waveglow (prenger et al.,2019) vocoder is used for all u2s models, whichdemonstrates the universality of vocoder models.
3https://github.com/nvidia/tacotron24https://github.com/nvidia/waveglow.
5298metric.
symbol.
sampling with temperature.
t = 1.0.t = 0.7.t = 0.4.t = 0.1.top-k sampling (t = 1.0)k = 3k = 5k = 10.top-k sampling (t = 0.7)k = 3k = 5k = 10.bleu-4.
meteor.
rouge-l.cider.
spice.
vq3vq2wvqvq3 \ rle.
vq3vq2wvqvq3 \ rle.
vq3vq2wvqvq3 \ rle.
vq3vq2wvqvq3 \ rle.
vq3vq2wvqvq3 \ rle.
0.0520.0390.0330.049.
0.1240.1150.0960.119.
0.3030.2930.2700.295.
0.1950.1430.0950.182.
0.0630.0520.0350.060.
0.0970.0580.0470.075.
0.1510.1340.1060.135.
0.3580.3300.2970.330.
0.3450.2310.1500.277.
0.0930.0740.0460.078.
0.1320.0680.0250.035.
0.1680.1460.0780.055.
0.4030.3510.2870.152.
0.4610.2720.0440.130.
0.1110.0860.0190.034.
0.1370.0660.0120.000.
0.1650.1400.0690.002.
0.4160.3450.2870.001.
0.4510.2670.0090.000.
0.1140.0870.0110.001.
0.0840.0590.0560.070.
0.1470.1340.1120.136.
0.3460.3250.3120.328.
0.3120.2200.1800.260.
0.0860.0730.0510.077.
0.1080.0680.0500.087.
0.1600.1420.1040.146.
0.3710.3450.3090.349.
0.3830.2600.1450.316.
0.1000.0820.0420.087.
0.1200.0690.0370.092.
0.1660.1470.0880.148.
0.3860.3510.2920.355.
0.4240.2770.0820.340.
0.1080.0850.0260.091.
0.1090.0640.0520.082.
0.1590.1400.1050.141.
0.3730.3400.3090.340.
0.3950.2510.1540.304.
0.1000.0790.0430.083.
0.1190.0700.0420.094.
0.1650.1440.0940.144.
0.3860.3480.2950.348.
0.4310.2700.1160.328.
0.1060.0840.0340.088.
0.1240.0710.0250.093.
0.1680.1470.0800.141.
0.3970.3550.2760.350.
0.4440.2780.0550.332.
0.1090.0870.0200.086.table a3: results of sat models trained on mscoco and decoded with various sampling methods..n.123510.
1.
551----.
beam searchbeam size=?
5.
8.
3.
479572693--.
447523620681-.
421502585625-.
10.
411474562617700.sampling (t: temperature; k: top-k).
(t, k) = (?, all).
1.0.
0.7.
0.4.
0.1.
(t, k) = (1.0, ?)
35.
10.
(t, k) = (0.7, ?)
35.
10.
14472100255032394311.
9781367164421112876.
689917107513051664.
5616968039381155.
10581522185523673176.
9081289151518612512.
7701025122215111954.
694907106912661618.
663867100312091552.
67085197311551437.table a4: the vocabulary size of the vq3 sat-ft model as estimated by various decoding approaches.
thenumbers in this table display the speciﬁc values of the curves depicted in figure 4..and how little acoustic properties of interest areaffected by them..training a batch size of 64 are used for all sys-tems.
adam (kingma and ba, 2015) with an initiallearning rate of 10−3 is used to minimize the meansquare error from spectrogram prediction and thebinary cross entropy from stop token predictioncombined.
l2 regularization for the parameterswith a weight of 10−6 is applied, and the l2 normof the gradients are clipped at 1. models are trainedfor 500 epochs on ljspeech and 250 epochs onvctk, and selected based on the validation loss.
empirically, each training epoch on ljspeech takesabout 12 minutes using two nvidia titan x pas-cal gpus for both vq2 and vq3 models..c full results of decoding via sampling.
table a3 presents the word-based evaluation re-sults of decoding via sampling for all 5 metrics,supplementing figure 3 in the main paper that onlypresents the spice results.
we see that rankingbetween symbols are generally consistent among.
all those metrics, except the ranking between wvqand vq3 \ rle when sampling with a temperatureof 0.4. this is a relatively low-score regime whenboth model are transiting from generating trivialcaption (t = 0.1) to non-trivial captions (t = 0.7)..d full results of learned vocabulary.
size.
in table a4, we display the numerical results de-picted graphically in figure 4..e more image-to-speech samples.
table a5 shows captions sampled from the vq3model trained on mscoco.
here, we note thatthe sampled captions exhibit diversity both theircontent and linguistic style.
we observe that thecaptioning model has learned to produce captionsthat correctly use quantiﬁers and conjugate verbs(“a couple of cows walking” vs. “a cow is stand-ing”).
the model also disentangles object identityfrom attributes such as color “red ﬁre hydrant” vs.“yellow ﬁre hydrant” vs. “green ﬁre hydrant”)..5299image.
generated spoken captions / transcripts (sat-ft, vq3, sampling (t, k) = (0.4, 3))trial 2.trial 3.trial 1.the airplane is parked onthe ﬁeld.
a plane is parked in thegrass near a white andwhite airplane.
a small airplane that isstanding in a ﬁeld.
a surfer riding a wave inthe water.
the man is riding the wavein the water.
a surfer is riding a waveon a wave.
the bus parked on the sideof the road.
a large red bus is stoppedin the road.
a bus is parked on the road.
a couple of cows walkingin a ﬁeld.
a couple of cows in agrassy ﬁeld.
a couple of cows walkingin a grassy ﬁeld.
a cow is standing in astore.
a brown cow walkingdown the side of a street.
a brown and white cowstanding in a line.
a red ﬁre hydrant is sittingon the side of the street.
a red ﬁre hydrant sittingon a sidewalk in aconcrete.
a red ﬁre hydrant sittingon the side of a road.
a yellow ﬁre hydrant inthe middle of the side of aroad.
a yellow ﬁre hydrant issitting in the park.
a yellow ﬁre hydrant in aline on the side of a street.
a ﬁre hydrant on asidewalk in the middle.
a green ﬁre hydrant on theside of the road.
a ﬁre hydrant with a curbon the side of the street.
table a5: samples.
more at https://wnhsu.github.io/image-to-speech-demo/2_vq3_sample_diversity_sat-ft_model.
5300