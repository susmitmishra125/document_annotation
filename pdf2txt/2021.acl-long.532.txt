privacy at scale: introducing the privaseer corpusof web privacy policies.
mukund srinath.
shomir wilson.
c. lee giles.
college of information sciences and technologypennsylvania state universityuniversity park, pa, usa{mukund, shomir, clg20}@psu.edu.
abstract.
organisations disclose their privacy practicesby posting privacy policies on their websites.
even though internet users often care abouttheir digital privacy, they usually do not readprivacy policies, since understanding them re-quires a signiﬁcant investment of time and ef-fort.
natural language processing has beenused to create experimental tools to interpretprivacy policies, but there has been a lack oflarge privacy policy corpora to facilitate thecreation of large-scale semi-supervised and un-supervised models to interpret and simplifyprivacy policies.
thus, we present the pri-vaseer corpus of 1,005,380 english languagewebsite privacy policies collected from theweb.
the number of unique websites repre-sented in privaseer is about ten times largerthan the next largest public collection of webprivacy policies, and it surpasses the aggre-gate of unique websites represented in all otherpublicly available privacy policy corpora com-bined.
we describe a corpus creation pipelinewith stages that include a web crawler, lan-guage detection, document classiﬁcation, du-plicate and near-duplicate removal, and con-tent extraction.
we employ an unsupervisedtopic modelling approach to investigate thecontents of policy documents in the corpusand discuss the distribution of topics in pri-vacy policies at web scale.
we further inves-tigate the relationship between privacy policydomain pageranks and text features of the pri-vacy policies.
finally, we use the corpus to pre-train privbert, a transformer-based privacypolicy language model, and obtain state of theart results on the data practice classiﬁcationand question answering tasks..1.introduction.
a privacy policy is a legal document that an or-ganisation uses to disclose how they collect, ana-lyze, share, and protect users’ personal informa-tion.
legal jurisdictions around the world require.
organisations to make their privacy policies readilyavailable to their users, and laws such as generaldata protection regulation (gdpr) and califor-nia consumer privacy act (ccpa) place speciﬁcexpectations upon privacy policies.
however, al-though many internet users have concerns abouttheir privacy (madden, 2017), most fail to under-stand privacy policies (meiselwitz, 2013).
studiesshow that privacy policies require a considerable in-vestment in time to read (obar and oeldorf-hirsch,2018) and estimate that it would require approxi-mately 200 hours to read all the privacy policiesthat an average person would come across everyyear (mcdonald and cranor, 2008)..natural language processing (nlp) provides anopportunity to automate the extraction of salientdetails from privacy policies, thereby reducing hu-man effort and enabling the creation of tools forinternet users to understand and control their on-line privacy.
existing research has achieved somesuccess using expert annotated corpora of a fewhundred or a few thousand privacy policies (wilsonet al., 2016; zimmeck et al., 2019; ramanath et al.,2014), but issues of accuracy, scalability and gener-alization remain.
more importantly, annotations inthe privacy policy domain are expensive.
privacypolicies are difﬁcult to understand and many taskssuch as privacy practice classiﬁcation (wilson et al.,2016), privacy question answering (ravichanderet al., 2019), vague sentence detection (lebanoffand liu, 2018), and detection of compliance issues(zimmeck et al., 2019) require skilled legal expertsto annotate the dataset.
in contrast, approaches in-volving large amounts of unlabeled privacy policiesremain relatively unexplored..modern robust.
language models,.
such astransformer-based architectures, beneﬁt from in-creasingly large training sets.
these models canbe used on downstream tasks (devlin et al., 2019)to improve performance.
results have shown that.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6829–6839august1–6,2021.©2021associationforcomputationallinguistics6829in-domain ﬁne tuning of such pre-trained languagemodels have produced a signiﬁcant boost in perfor-mance on many tasks (gururangan et al., 2020) ina variety of domains, suggesting a need for a largercollection of privacy policies to enable similar re-sults in the privacy domain..to satisfy the need for a much larger corpus ofprivacy policies, we introduce the privaseer cor-pus of 1,005,380 english language website privacypolicies.
the number of unique websites repre-sented in privaseer is about ten times larger thanthe next largest public collection of web privacypolicies (amos et al., 2020), and it surpasses the ag-gregate of unique websites represented in all otherpublicly available web privacy policy corpora com-bined.
we describe the corpus creation pipeline,with stages including a web crawler, language de-tection, document classiﬁcation, duplicate and near-duplication removal, and content extraction.
wethen analyse the lengths and top level distributionof the privacy policies in the corpus and use topicmodelling to explore the component topics.
sub-sequently, we pretrain privbert, a transformer-based language model, using the corpus and eval-uate it on data practice classiﬁcation and questionanswering tasks.
we release the corpus, a searchengine for the corpus (srinath et al., 2021), the doc-ument collection pipeline, and a language model tosupport further research in the privacy domain.1.
2 related work.
prior collections of privacy policy corpora haveled to progress in privacy research.
wilson et al.
(2016) released the opp-115 corpus, a dataset of115 privacy policies with manual annotations of23k ﬁne-grained data practices, and they createda baseline for classifying privacy policy text intoone of ten categories.
the corpus was used totrain models to extract opt-out choices from privacypolicies (sathyendra et al., 2016), to automaticallyidentify policies on websites and ﬁnd complianceissues (story et al., 2019), and to classify privacypractices and answer privacy related non-factoidquestions (harkous et al., 2018)..other corpora similar to opp-115 corpus haveenabled research on privacy practices.
the priva-cyqa corpus contains 1,750 questions and expert-annotated answers for the privacy question answer-ing task (ravichander et al., 2019).
similarly,.
lebanoff and liu (2018) constructed the ﬁrst cor-pus of human-annotated vague words and sentencesin privacy policies and studied automatic vague-ness detection.
sathyendra et al.
(2017) presenteda dataset and developed a model to automaticallyidentify and label opt-out choices offered in privacypolicies.
similarly, zimmeck et al.
(2019) releaseda set of over 400k urls to android app privacypolicy pages collected by crawling the google playstore.
amos et al.
(2020) collected privacy poli-cies from around 130,000 websites from over twodecades and analysed the evolution of the onlineprivacy landscape.
finally, nokhbeh zaeem andbarber (2021) collected a corpus of around 100kprivacy policies using the domains from dmoz, awebsite which maintained categories of websiteson the internet..prior work in privacy and human-computer in-teraction establishes the motivation for studyingthese documents.
although most internet users areconcerned about privacy (madden, 2017), rudolphet al.
(2018) reports that a signiﬁcant number donot make the effort to read privacy notices becausethey perceive them to be too time-consuming ortoo complicated (obar and oeldorf-hirsch, 2018).
responding to the opaqueness of these document,schaub et al.
(2015) introduced methods to easethe design of privacy notices and their integration,and kelley et al.
(2010) designed and tested a “pri-vacy nutrition label” approach to present privacyinformation visually.
suggestions to improve thepresentation of privacy information, have not beenadopted by many organisations.
apple has begundisplaying privacy labels in its app stores havingcollected the information from app developers;however, concise privacy information for websitesremains an open problem..3 document collection.
to build the privaseer corpus, we create a pipelineconcentrating on focused crawling chakrabartiet al.
(1999); diligenti et al.
(2000) of privacypolicy documents.
we used common crawl,2 de-scribed below, to gather seed urls to privacy poli-cies on the web.
we ﬁltered the common crawlurls to gather a set of possible links to web siteprivacy policies.
we then crawled the ﬁltered setto obtain candidate privacy policy documents.
thecomplete pipeline from the common crawl urldump to the gold standard privacy policy corpus is.
1all artifacts are available at https://privaseer..ist.psu.edu/..2https://commoncrawl.org/.
6830figure 1: corpus creation pipeline.
in figure 1..the common crawl foundation has been re-leasing large monthly internet web crawls alongwith their web graphs since 2008. monthly crawlarchives provide a “snapshot of the web” by includ-ing re-crawls of popular domains and crawls of newdomains.
we downloaded the url dump of themay 2019 archive.3 common crawl reports thatthe archive contains 2.65 billion web pages or 220tb of uncompressed content which were crawledbetween 19th and 27th of may, 2019. we applied aselection criteria on the downloaded url dump toﬁlter the urls of likely privacy policy pages..due to legal requirements, organizations typi-cally include a link to their privacy policy in thefooter of the website landing page commonly withthe names privacy policy, privacy notice, and dataprotection.
we selected those urls which hadthe word “privacy” or the words “data” and “pro-tection” from the common crawl url archive.
we were able to extract 3.9 million urls that ﬁtthis selection criterion.
informal experiments sug-gested that this selection of keywords was optimalfor retrieving the most privacy policies with as fewfalse positives as possible.
to ﬁnd the accuracy ofthis technique, we manually examined 115 englishlanguage website landing pages and their privacypolicy urls from the opp-115 corpus (wilsonet al., 2016) since it was built to cover the diversedistribution of privacy policies on the web, in termsof website popularity and sector of commerce.
wefound that out of 115 websites, 4 websites did nothave their privacy policy links either on the landingpage or one hop from the landing page and 5 otherwebsites did not satisfy our url selection criteria.
thus, our crawling technique would cover about.
3https://commoncrawl.s3.amazonaws.com/.
crawl-data/cc-main-2019-22/cc-index.
paths.gz.
92.17% ± 6.51% of english privacy policies on theweb with a 95% conﬁdence interval..we crawled the 3.9 million selected urls us-ing scrapy4 for about 48 hours between the 4thand 10th of august 2019, for a few hours eachday.
3.2 million urls were successfully crawled,henceforth referred to as candidate privacy policies,while 0.4 million led to error pages and 0.3 millionurls were discarded as duplicates..4 document filtering.
language detection.
we focused on privacy poli-cies written in the english language, to enable com-parisons with prior corpora of privacy policies.
toidentify the natural language of each candidate doc-ument, we used the open-source python packagelangid (lui and baldwin, 2012).
langid is a naivebayes-based classiﬁer pretrained on 97 differentlanguages, designed to achieve consistently highaccuracy over a wide range of languages, domains,and lengths of text.
the complete set of documentswas divided into 97 languages and an unknown lan-guage category.
we found that the vast majority ofdocuments were in english.
we set aside candidatedocuments that were not identiﬁed as english bylangid and were left with 2.1 million candidates.
content extraction.
manual inspection of theenglish language web pages showed that they in-cluded content other than the main text: often theyhad a header, a footer, a navigation menu, and ban-ners.
we refer to this extra content in a web page asboilerplate.
boilerplate draws away from the focusof the main content in a web page and thereforevarious techniques have been used to remove boil-erplate from web pages (gottron, 2007; weningeret al., 2016).
after manual comparison of a num-ber of content extraction tools, we used the open-source python package boilerpipe (kohlsch¨utter.
4https://scrapy.org/.
6831et al., 2010) due to its superior performance.
boil-erpipe effectively strips web pages of boilerplateusing shallow text features, structural features anddensity based features..document classiﬁcation.
some of the webpages in the english language candidate documentset may not have been privacy policies and insteadsimply satisﬁed our url selection criteria.
to sep-arate privacy policies from other web documentswe used a supervised machine learning approach.
two researchers in the team labeled 1,600 ran-domly selected candidate documents based on apreset scheme in consultation with a privacy expert.
while both the researchers had substantial priorexperience with privacy policies, the privacy expertwas consulted to eliminate uncertainty in the anno-tations of a few documents.
lack of agreement inthe annotations occurred for six documents, whichwere settled by discussion with the expert.
out of1,600 documents, 1,145 were privacy policies and455 were not privacy policies..we trained four supervised machine learningmodels using the manually labelled documentswith features extracted from the urls and thewords in the web page.
we trained three randomforest models and ﬁne-tuned a transformer basedpretrained language model, namely roberta (liuet al., 2019).
the three random forest models weretrained on three different sets of features: one usingthe features extracted from the url, one using thefeatures extracted from the document content, anda combined model using features from both..for the url model, the words in the url pathwere extracted and the tf-idf of each term wasrecorded to create the features (baykan et al., 2009).
as privacy policy urls tend to be shorter and havefewer path segments than typical urls, lengthand the number of path segments were added asfeatures.
since the classes were unbalanced, weover-sampled from the minority class using the syn-thetic minority over-sampling technique (smote)(chawla et al., 2002).
similarly, for the documentmodel, we used tf-idf features after tokenizing thedocument using a regex tokenizer and removingstop words.
the combined model was a combina-tion of the url and document features..to train the roberta model on the privacy pol-icy classiﬁcation task, we used the sequence clas-siﬁcation head of the pretrained language modelfrom huggingface (wolf et al., 2019).
we usedthe pretrained roberta tokenizer to tokenize text.
extracted from the documents.
since roberta ac-cepts a maximum of 512 tokens as input, only theﬁrst 512 tokens of text from the documents wereused for training while the rest was discarded.
asshown in the analysis section, the average length ofa privacy policy in terms of the number of wordsis 1,871. thus 512 tokens would take into accountabout a fourth of an average privacy policy..the 1,600 labelled documents were randomlydivided into 960 documents for training, 240 doc-uments for validation and 400 documents for test-ing.
using 5-fold cross-validation, we tuned thehyperparameters for the models separately with thevalidation set and then used the held-out test set toreport the test results.
due to its size, it was possi-ble for the held out test set to have a biased sam-ple.
thus we repeated the sampling and trainingprocesses with a 5-fold cross-validation approach.
table 1 shows performance of the models after theresults from test sets were averaged.
since thetransformer based model had the best results, weran it on all the the candidate privacy policies.
outof 2.1 million english candidate privacy polices,1.54 million were classiﬁed as privacy policies andthe rest were discarded..modelurl baseddocument basedcombinedroberta.
precision recall0.890.950.970.98.
0.880.930.940.97.f10.880.940.950.97.table 1: document classiﬁcation.
url cross veriﬁcation.
legal jurisdictionsaround the world require organisations to maketheir privacy policies readily available to their users.
as a result, most organisations include a link totheir privacy policy in the footer of their websitelanding page.
in order to focus privaseer corpuson privacy policies that users are intended to read,we cross-veriﬁed the urls of the privacy poli-cies in our corpus with those that we obtained bycrawling the homepages (landing page) of these do-mains.
between the 8th and 10th november 2019,we crawled the landing pages and pages one hopfrom the landing pages for all the domains of theurls in our corpus.
we then gathered the urlssatisfying our selection criteria and cross-veriﬁedthem with the urls in our existing corpus.
aftercross-verifying the urls, we were left with a setof 1.1 million web pages..6832duplicate and near-duplicate detection.
ex-amination of the corpus revealed that it containedmany duplicate and near-duplicate documents.
weremoved exact duplicates by hashing all the rawdocuments and discarding multiple copies of ex-act hashes.
through manual inspection, we foundthat a number of privacy policies from different do-mains had very similar wording, differing only bythe organisation or website name.
we reason thatthis similarity could be due to the use of privacypolicy templates or generators.
we also found abun-dant examples of near-duplicate privacy policieson the same website.
we reason that this similaritycould be due to the presence of archived versionsof privacy policies on the website.
since we aimedto collect a comprehensive corpus of contemporarypolicies, we only removed similar policies (near-duplicates) from same domain domains..to remove near-duplicates from within the samedomain we used simhashing (charikar, 2002).
simhashing is a hashing technique in which sim-ilar inputs produce similar hashes.
after creatingshingles (broder et al., 1997) of size three, wecreated 64 bit document simhashes and measureddocument similarity by calculating the hammingdistance (manku et al., 2007) between documentsimhashes of privacy policies within the same do-main.
we then obtained a list of all pairs of similardocuments based on a distance threshold (measuredbased on the number of differing bits) that was de-termined after manual examination of a numberof pairs of privacy policies.
we then ﬁltered theduplicates based on a greedy approach retainingpolicies that were longer in length.
the remainingdocuments comprised the corpus..5 corpus analysis.
the privaseer corpus consists of 1,005,380 pri-vacy policies from 995,475 different web domains.
privacy policies in this corpus have a mean wordlength of about 1,871 words and range between aminimum of 143 words and a maximum of 16,980words.
the corpus contains policies from over 800different top level domains (tlds).
.com, .org, and.net make up a major share of the corpus covering63%, 5% and 3% respectively.
country-level do-mains like .uk, .au, .ca and .du show the geographicvariety of the sources of the corpus covering 12%,4%, and 2% respectively.
the distribution of popu-lar tlds (.com, .org, .net) roughly matches internettld trends suggesting that the corpus contains a.random sample of internet web domains.
moreover,commoncrawl release statistics estimating the rep-resentativeness of monthly crawls which supportthe claim that monthly crawl archives and in turnthe privaseer corpus are a representative sampleof the web.
in addition to monthly crawl dumps,common crawl releases web graphs with pager-anks of the domains in a crawl.
the pagerankvalues were calculated from the web graph usingthe gauss-seidel algorithm (arasu et al., 2002).
pagerank values can be used as a substitute forpopularity where higher values suggest more popu-lar domains..readability.
readability of a text can be de-ﬁned as the ease of understanding or comprehen-sion due to the style of writing (klare et al., 1963).
along with length, readability plays a role in in-ternet users’ decisions to either read or ignore aprivacy policy (ermakova et al., 2015).
while priorstudies on readability have shown that privacy poli-cies are difﬁcult to understand for the average inter-net user, they were conducted using small samplesof policies and therefore may not be representativeof the larger internet (fabian et al., 2017).
whilethere are a variety of readability metrics, we calcu-lated the readability of the policies in the corpususing the flesh-kincaid grade level (fkg) metricfor comparison with prior literature and since it isthe the most widely used metric.
the fkg metricpresents the readability score as a u.s. grade level.
we obtained a mean fkg score of 14.87 and a stan-dard deviation of 4.8. this score can be interpretedas an average of 14.87 years of education in the u.s.(roughly two years of college education) is requiredto understand a privacy policy.
in contrast, fabianet al.
(2017) found that the mean fkg score is 13.6when they conducted an analysis of readability ofprivacy policies using 50k documents..topic modelling.
topic modelling is an unsu-pervised machine learning method that extractsthe most probable distribution of words into topicsthrough an iterative process (wallach, 2006).
weused topic modelling to explore the distributionof themes of text in our corpus.
topic modellingusing a large corpus such as privaseer helps inves-tigate the themes present in privacy policies at webscale and also enables the comparison of themesthat occur in the rapidly evolving online privacylandscape.
we used latent dirichlet allocation(lda), as our approach to topic modelling (bleiet al., 2003).
since lda works well when each in-.
6833put document deals with a single topic, we dividedeach privacy policy into its constituent paragraphs(sarne et al., 2019), tokenized the paragraphs usinga regex character matching tokenizer and lemma-tized the individual words using nltk’s wordnetlemmatizer.
we experimented with topics sizes of7, 8, 9, 10, 11, 13 and 15. we manually evaluatedthe topic clusters by inspecting the words that mostrepresented the topics.
we noted that the cohesive-ness of the topics decreased as the number of topicsincreased.
we chose a topic size of 9, since largertopic sizes produced markedly less coherent topics..for each topic, we identiﬁed a corresponding en-try from the opp-115 annotation scheme (wilsonet al., 2016), which was created by legal experts tolabel the contents of privacy policies.
while wil-son et al.
(2016) followed a bottom-up approachand identiﬁed different categories from analysis ofdata practices in privacy policies, we followed atop-down approach and applied topic modellingto the corpus in order to extract common themesfor paragraphs.
the categories identiﬁed in theopp-115 corpus can be found in table 2..we found that two lda topics contained vocab-ulary corresponding with the opp-115 categoryfirst party collection/use, one dealing with pur-pose and information type collected and the otherdealing with collection method.
two lda top-ics corresponded with the opp-115 category thirdparty sharing and collection, one detailing the ac-tion of collection, and one explaining its purposeand effects(advertising and analytics).
one of thelda topics exclusively comprised of vocabularyrelated to cookies which could be related to bothﬁrst party or third party data collection techniques.
the opp-115 categories privacy contact informa-tion, data security and policy change appeared asseparate topics while a topic corresponding to theopp-115 category international and speciﬁc audi-ences appeared to be primarily related to europeanaudiences and gdpr..it is likely that the divergence between opp-115categories and lda topics comes from a differ-ence in approaches: the opp-115 categories repre-sent themes that privacy experts expected to ﬁndin privacy policies, which diverge from the actualdistribution of themes in this text genre.
figure2 shows the percentage of privacy policies in thecorpus that contain each topic.
from the ﬁgure wesee that information regarding the type and purposeof data collected by ﬁrst and third party sources are.
figure 2: topic distribution.
the most common topics.
about 77% of policiescontain language regarding third parties.
this isconsistent with prior research on third party datacollection (libert, 2018).
in contrast, language re-garding advertising and analytics appears in only38% of policies in the corpus.
topics correspond-ing to data security, policy change and contact infor-mation also occur in a majority of privacy policies.
language corresponding to the gdpr and euro-pean audiences appears in 55% of policies.
a studyof the distribution of privacy policy topics on theweb is important since they inform us about real-world trends and the need for resource allocationto enforce of privacy regulations..figure 3 shows how the number of topics in pri-vacy policies vary with respect to the pagerankvalue.
the whiskers in the plot represent the 95%conﬁdence interval of the means of the numberof topics in the privacy policies in each pager-ank value bin.
the pagerank values were binnedwith a constant value of 0.25 such that each binhad at least 1k privacy policies.
the plot suggeststhat more popular domains (as given by pagerankvalue) tend to address a greater number of topicsin their privacy policies.
this behaviour is consis-tent with manual inspection and is likely due to alarger and more diverse user base as well as thegreater levels of regulatory scrutiny that accom-pany it in the case of more popular domains.
forexample, popular organisations tend to be multi-national thereby requiring to address privacy lawsfrom multiple jurisdictions such as gdpr from theeuropean union and ccpa from the united states.
we found a similar pattern between privacy policylength and pagerank value thereby further support-ing our claim that the more popular domain privacypolicies tend to address a greater number of topics.
in addition we found that readability and pager-.
6834ank follow a similar pattern where privacy policiesof more popular domains (as given by pagerankvalues) tend to be slightly more difﬁcult to read..figure 3: relationship between number of topics andprivacy policy domain pagerank value.
6 privbert.
in order to address the requirement of a lan-guage model for the privacy domain, we createdprivbert.
bert is a contextualized word repre-sentation model that is pretrained using bidirec-tional transformers (devlin et al., 2019).
it was pre-trained on the masked language modelling and thenext sentence prediction tasks and has been shownto achieve state of the art results in many nlp tasks.
roberta improved upon the results achieved bybert by making improvements to the trainingtechnique (liu et al., 2019).
we pretrain privbertstarting with the pretrained robertabase model(12 layers, 768 hidden size, 12 attention heads,110m parameters).
roberta was trained on cor-pora of books, news articles, wikipedia and socialmedia comments and works well as a general pur-pose language model.
privacy policies written inlegalese differ signiﬁcantly in language when com-pared to the corpora used to train bert and itsvariants, thereby prompting the need for a sepa-rate pretrained language model.
prior literaturehas shown that in-domain language models such asscibert (beltagy et al., 2019) and biobert (leeet al., 2020) perform signiﬁcantly better on tasksin their respective domains..we use the byte pair encoding tokenization tech-nique utilized in roberta and retain its casedvocabulary.
we did not create a new vocabularysince the two vocabularies are not signiﬁcantly dif-ferent and any out-of-vocabulary words can be rep-resented and tuned for the privacy domain usingthe byte pair encoding vocabulary of roberta..we preprocessed the privacy policy documents tocreate sequences of a maximum length of 512 to-kens.
inputs signiﬁcantly shorter than the maxi-mum length occasionally occurred since we did notcreate sequences that crossed document boundaries.
we trained privbert using dynamic masked lan-guage modelling (liu et al., 2019) for 50k stepswith a batch size of 512 using the gradient accu-mulation technique on two nvidia titan rtx for8 days with a peak learning rate of 8e-5.
otherhyperparameters were set similar to roberta..finetuning privbert.
we evaluated the perfor-mance of privbert on two tasks: (i) data practiceclassiﬁcation (ii) answer sentences selection..for the data practice classiﬁcation task, we lever-aged the opp-115 corpus introduced by wilsonet al.
(2016).
the opp-115 corpus contains man-ual annotations of 23k ﬁne-grained data practiceson 115 privacy policies annotated by legal experts.
to the best of our knowledge, this is the most de-tailed and widely used dataset of annotated privacypolicies in the research community.
the opp-115corpus contains paragraph-sized segments anno-tated according to one or more of the twelve coarse-grained categories of data practices.
we ﬁne-tunedprivbert on the opp-115 corpus to predict thecoarse-grained categories of data practices.
wedivided the corpus in the ratio 3:1:1 for training,validation and testing respectively.
since each seg-ment in the corpus could belong to more than onecategory and there are twelve categories in total,we treated the problem as a multi-class, multi-labelclassiﬁcation problem.
after manually tuning hy-perparameters, we trained the model with a dropoutof 0.15 and a learning rate of 2.5e-5..table 2 shows the results for the data practiceclassiﬁcation task comparing the performance be-tween roberta, privbert and polisis (harkouset al., 2018), a cnn based classiﬁcation model.
we report reproduced results for polisis since theoriginal paper takes into account both the pres-ence and absence of a label while calculating thescore for each label (nejad et al., 2020).
dueto the unbalanced nature of the dataset, we re-port the macro-average and micro-average scores.
privbert achieves state of the art results improv-ing not only on the macro-average f1 score ofroberta by about 4% but also improving on thef1 score for every category in the task..for the question answering task, we leveragedthe privacyqa corpus (ravichander et al., 2019)..6835label.
support.
first party collection and usethird party sharing and collectionuser choice/controlprivacy contact informationintroductory/genericpractice not covereddata securityuser access, edit and deletionpolicy changedo not trackinternational and speciﬁc audiencesdata retentionmacro averagesmicro averages.
polisisf10.820.820.720.840.730.130.750.700.881.00.820.400.710.78.robertar0.890.910.780.760.670.320.750.880.901.00.780.570.770.82.f10.910.880.800.790.760.390.820.790.841.00.810.670.790.84.p0.920.860.830.840.880.500.910.720.791.00.830.800.820.86.privbertr0.880.930.790.800.760.440.800.880.951.00.840.710.820.85.p0.950.880.870.820.790.650.940.800.871.00.890.830.860.88.f10.920.910.830.810.770.520.860.840.911.00.860.770.830.87.
2502037742782540242135614833833.table 2: test performance comparison of three models on the data practice classiﬁcation task (p:precision, r:recall).
privacyqa consists of 1,750 questions about thecontents of privacy policies from 35 privacy docu-ments.
while crowdworkers were asked to comeup with privacy related questions based on publicinformation about an application from the googleplay store, legal experts were recruited to identifyrelevant evidence within respective privacy poli-cies that answered the question asked by the crowd-workers.
the goal of the question answering taskis to identify a set sentences in the privacy pol-icy that has information relevant to the question.
ravichander et al.
(2019) divided the corpus into1,350 questions for training and validation and 400questions for testing where each question in thetest set is annotated by at least three experts.
weﬁne-tuned privbert on the training set as a bi-nary classiﬁcation task on each question-answersentence pair to identify if the sentence is evidencefor the question or not.
we trained the model witha dropout of 0.2 and a learning rate of 3e-6 withthe positive and negative classes weighted in theratio 8:1 during training.
we used sentence level f1as the evaluation metric as described by ravichan-der et al.
(2019), where precision and recall arecalculated by measuring the overlap between thepredicted sentences and gold standard sentences..table 3 shows the results for the answer sentenceselection task comparing the performance betweenbert and privbert.
results from bert are asreported by ravichander et al.
(2019).
privbertachieves state of the art results improving on theresults of bert by about 6%.
privbert therefore.
modelbertprivbert.
precision recall0.3480.424.
0.4420.483.f10.390.452.table 3: performance comparison on the answer sen-tence selection task.
has been shown to achieve state of the art resultsin two signiﬁcantly disparate tasks in the privacydomain suggesting that it can be used to improvethe performance on various real-world tasks andapplication in the privacy domain..7 conclusion.
we created the privaseer corpus which is the ﬁrstlarge scale corpus of contemporary website privacypolicies and consists of just over 1 million docu-ments.
we designed a novel pipeline to build thecorpus, which included web crawling, language de-tection, document classiﬁcation, duplicate removal,document cross veriﬁcation, content extraction, andnear duplicate removal..topic modelling showed the distribution ofthemes of privacy practices in policies, correspond-ing to the expectations of legal experts in someways, but differing in others.
the positive rela-tionship between pagerank of a domain and thenumber of topics covered in its policy indicates thatmore popular domains have a slightly greater cov-erage of these topics.
we hypothesize that this isbecause more popular domains tend to have a largerand more diverse user base prompting the privacy.
6836policies to address laws from various jurisdictions.
prior research on the readability based on smallcorpora of privacy policies had found that theywere generally hard to understand for the averageinternet user.
our large scale analysis using theflesch-kincaid readability metric was consistentwith prior ﬁndings.
we found that on average about14.87 years or roughly about two years of u.s. col-lege education was required to understand a privacypolicy..we pretrained privbert a language model forthe privacy domain based on roberta.
we evalu-ated privbert on the data practice classiﬁcationand the question answering tasks and achieved stateof the art results..we believe that the privaseer corpus will helpadvance research techniques to automate the ex-traction of salient details from privacy policies.
privbert will help improve results on varioustasks in the privacy domain and help build stableand reliable privacy preserving technology.
thisshould beneﬁt internet users, regulators, and re-searchers in many ways..acknowledgments.
this work was partly supported by a seed grantfrom the college of information sciences and tech-nology at the pennsylvania state university.
wealso acknowledge adam mcmillen for technicalsupport..references.
ryan amos, gunes acar, elena lucherini, mihirkshirsagar, arvind narayanan, and jonathan mayer.
2020. privacy policies over time: curation andanal-ysis of a million-document dataset.
arxiv preprintarxiv:2008.09159..arvind arasu, jasmine novak, andrew tomkins, andjohn tomlin.
2002. pagerank computation and thestructure of the web: experiments and algorithms.
in proceedings of the eleventh international worldwide web conference, poster track, pages 107–117..eda baykan, monika henzinger, ludmila marian, andingmar weber.
2009. purely url-based topic classi-in proceedings of the 18th internationalﬁcation.
conference on world wide web, pages 1109–1110..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3606–3611..david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..andrei z broder, steven c glassman, mark s man-asse, and geoffrey zweig.
1997. syntactic cluster-ing of the web.
computer networks and isdn sys-tems, 29(8-13):1157–1166..soumen chakrabarti, martin van den berg, and by-ron dom.
1999. focused crawling: a new approachto topic-speciﬁc web resource discovery.
computernetworks, 31(11-16):1623–1640..moses s charikar.
2002. similarity estimation tech-niques from rounding algorithms.
in proceedings ofthe thiry-fourth annual acm symposium on theoryof computing, pages 380–388.
acm..nitesh v chawla, kevin w bowyer, lawrence o hall,and w philip kegelmeyer.
2002. smote: syntheticminority over-sampling technique.
journal of artiﬁ-cial intelligence research, 16:321–357..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..michelangelo diligenti,.
stevelawrence, c lee giles, and marco gori.
2000.focused crawling using context graphs.
in vldb..frans coetzee,.
tatiana ermakova, benjamin fabian, and eleonorababina.
2015. readability of privacy policies ofhealthcare websites.
in proceesings of the 12th in-ternational conference on wirtschaftsinformatik..benjamin fabian, tatiana ermakova, and tino lentz.
2017. large-scale readability analysis of privacyin proceedings of the international con-policies.
ference on web intelligence, pages 18–25.
acm..thomas gottron.
2007. evaluating content extractionon html documents.
in proceedings of the 2nd inter-national conference on internet technologies andapplications (ita’07), pages 123–132.
citeseer..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
arxivpreprint arxiv:2004.10964..hamza harkous, kassem fawaz, r´emi lebret, florianschaub, kang g shin, and karl aberer.
2018. poli-sis: automated analysis and presentation of privacypolicies using deep learning.
in 27th {usenix} se-curity symposium ({usenix} security 18), pages531–548..6837patrick gage kelley, lucian cesca, joanna bresee, andlorrie faith cranor.
2010. standardizing privacy no-tices: an online study of the nutrition label approach.
in proceedings of the sigchi conference on hu-man factors in computing systems, pages 1573–1582. acm..george roger klare et al.
1963. measurement of read-.
ability..christian kohlsch¨utter, peter fankhauser, and wolf-gang nejdl.
2010. boilerplate detection using shal-low text features.
in proceedings of the third acminternational conference on web search and datamining, pages 441–450..logan lebanoff and fei liu.
2018. automatic detec-tion of vague words and sentences in privacy poli-in proceedings of the 2018 conference oncies.
empirical methods in natural language processing,pages 3508–3517..biobert:.
j lee, w yoon, s kim, d kim, ch so, and j kang.
2020.lan-guage representation model for biomedical text min-ing.
bioinformatics (oxford, england), 36(4):1234–1240..a pre-trained biomedical.
timothy libert.
2018. an automated approach to audit-ing disclosure of third-party data collection in web-in proceedings of the 2018site privacy policies.
world wide web conference, pages 207–216..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..marco lui and timothy baldwin.
2012..langid.
py:an off-the-shelf language identiﬁcation tool.
in pro-ceedings of the acl 2012 system demonstrations,pages 25–30.
association for computational lin-guistics..mary madden.
2017. privacy, security, and digital in-.
equality..gurmeet singh manku, arvind jain, and anishdas sarma.
2007. detecting near-duplicates forin proceedings of the 16th interna-web crawling.
tional conference on world wide web, pages 141–150. acm..aleecia m mcdonald and lorrie faith cranor.
2008.the cost of reading privacy policies.
isjlp, 4:543..gabriele meiselwitz.
2013. readability assessment ofpolicies and procedures of social networking sites.
in international conference on online communitiesand social computing, pages 67–75.
springer..systems security and privacy protection, pages 370–383. springer..razieh nokhbeh zaeem and k suzanne barber.
2021.a large publicly available corpus of website pri-vacy policies based on dmoz.
in proceedings of theeleventh acm conference on data and applicationsecurity and privacy, pages 143–148..jonathan a obar and anne oeldorf-hirsch.
2018. thebiggest lie on the internet: ignoring the privacy poli-cies and terms of service policies of social network-ing services.
information, communication & soci-ety, pages 1–20..rohan ramanath, fei liu, norman sadeh, and noah asmith.
2014. unsupervised alignment of privacypolicies using hidden markov models.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 605–610..abhilasha ravichander, alan w black, shomir wilson,thomas norton, and norman sadeh.
2019. ques-tion answering for privacy policies: combining com-putational and legal perspectives.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4949–4959..manuel rudolph, denis feth, and svenja polst.
2018.why users ignore privacy policies–a survey and in-tention model for explaining user privacy behavior.
in international conference on human-computerinteraction, pages 587–598.
springer..david sarne, jonathan schler, alon singer, ayelet sela,and ittai bar siman tov.
2019. unsupervised topicextraction from privacy policies.
in companion pro-ceedings of the 2019 world wide web conference,pages 563–568.
acm..kanthashree mysore sathyendra, florian schaub,shomir wilson, and norman sadeh.
2016. au-tomatic extraction of opt-out choices from privacypolicies.
in 2016 aaai fall symposium series..kanthashree mysore sathyendra, shomir wilson, flo-rian schaub, sebastian zimmeck, and normansadeh.
2017. identifying the provision of choices inprivacy policy text.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 2774–2779..florian schaub, rebecca balebako, adam l durity,and lorrie faith cranor.
2015. a design space forin eleventh symposiumeffective privacy notices.
on usable privacy and security ({soups} 2015),pages 1–17..najmeh mousavi nejad, pablo jabat, rostislavnedelchev, simon scerri, and damien graux.
2020.establishing a strong baseline for privacy policy clas-siﬁcation.
in ifip international conference on ict.
mukund srinath, soundarya sundareswara, and gilesc lee wilson, shomir.
2021. privaseer: a privacyin international conferencepolicy search engine.
on web engineering, pages 286–301.
springer..6838peter story, sebastian zimmeck, abhilasha ravichan-der, daniel smullen, ziqi wang, joel reidenberg,n cameron russell, and norman sadeh.
2019.natural language processing for mobile app pri-in aaai spring symposium onvacy compliance.
privacy-enhancing artiﬁcial intelligence and lan-guage technologies..hanna m wallach.
2006. topic modeling: beyond bag-of-words.
in proceedings of the 23rd internationalconference on machine learning, pages 977–984..tim weninger, rodrigo palacios, valter crescenzi,thomas gottron, and paolo merialdo.
2016. webcontent extraction: a metaanalysis of its past andthoughts on its future.
acm sigkdd explorationsnewsletter, 17(2):17–23..shomir wilson, florian schaub, aswarth abhilashdara, frederick liu, sushain cherivirala, pedro gio-vanni leon, mads schaarup andersen, sebas-tian zimmeck, kanthashree mysore sathyendra,n cameron russell, thomas b norton, eduardhovy, joel reidenberg, and norman sadeh.
2016.the creation and analysis of a website privacy pol-icy corpus.
in proceedings of the 54th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1330–1340..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2019.huggingface’s transformers: state-of-the-art naturallanguage processing.
arxiv, abs/1910.03771..sebastian zimmeck, peter story, daniel smullen, ab-hilasha ravichander, ziqi wang, joel reidenberg,n cameron russell, and norman sadeh.
2019.maps: scaling privacy compliance analysis to a mil-lion apps.
proceedings on privacy enhancing tech-nologies, 2019(3):66–86..6839