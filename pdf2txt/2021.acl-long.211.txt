towards quantiﬁable dialogue coherence evaluation.
zheng ye1, liucun lu1, lishan huang2, liang lin2,3, xiaodan liang1∗1shenzhen campus of sun yat-sen university, 2sun yat-sen university, 3dark matter ai inc.{yezh7,lulc,huanglsh6}@mail2.sysu.edu.cn,linliang@ieee.org, xdliang328@gmail.com.
abstract.
automatic dialogue coherence evaluation hasattracted increasing attention and is crucial fordeveloping promising dialogue systems.
how-ever, existing metrics have two major limita-tions: (a) they are mostly trained in a simpli-ﬁed two-level setting (coherent vs. incoherent),while humans give likert-type multi-level co-herence scores, dubbed as “quantiﬁable”; (b)their predicted coherence scores cannot alignwith the actual human rating standards due tothe absence of human guidance during train-ing.
to address these limitations, we proposequantiﬁable dialogue coherence evaluation(quantidce), a novel framework aiming totrain a quantiﬁable dialogue coherence metricthat can reﬂect the actual human rating stan-dards.
speciﬁcally, quantidce includes twotraining stages, multi-level ranking (mlr)pre-training and knowledge distillation (kd)ﬁne-tuning.
during mlr pre-training, a newmlr loss is proposed for enabling the modelto learn the coarse judgement of coherencedegrees.
then, during kd ﬁne-tuning, thepretrained model is further ﬁnetuned to learnthe actual human rating standards with onlyvery few human-annotated data.
to advo-cate the generalizability even with limited ﬁne-tuning data, a novel kd regularization is intro-duced to retain the knowledge learned at thepre-training stage.
experimental results showthat the model trained by quantidce presentsstronger correlations with human judgementsthan the other state-of-the-art metrics.
1.
1.introduction.
dialogue coherence, which requires a response tobe ﬂuent, consistent and context-related, is an es-sential property for developing promising dialogue.
∗corresponding author.
1the code and trained checkpoints are available at https:.
//github.com/james-yip/quantidce..figure 1: likert-type multi-level human rating vs. two-level automatic evaluation.
human rating always con-siders multiple coherence degrees, while most of theexisting automatic metrics only learn to distinguish thecoherence dialogues from the incoherent ones and giverelatively extreme coherence scores..systems (cervone et al., 2018).
however, it is stillchallenging to evaluate the coherence of a responsegenerated by a dialogue system.
although humanevaluation is always considered as the most accu-rate way to evaluate the coherence, it is expensiveand high-latency, which cannot meet the evaluationdemand of the frequent development of dialoguesystems.
therefore, automatic evaluation metricsare developed to serve as human proxies that canrapidly compute the dialogue coherence and returnrelatively accurate results..the current widely used metrics measure thelexical word-overlap between generated responsesand reference responses, such as bleu (papineniet al., 2002) and rouge (lin, 2004).
however,they have been demonstrated to be biased and cor-relate poorly with human judgements since no se-mantic information is considered (liu et al., 2016;novikova et al., 2017).
to overcome this issue,researchers turned to develop learnable metricsbased on neural networks that incorporate the se-mantic information, such as ruber (tao et al.,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2718–2729august1–6,2021.©2021associationforcomputationallinguistics271813254contextresponseu1:ilikeanimals,especiallydogs.
how about you?u2:haha,ilikecatsmore.i like cats more too and i work in a pet store.u3:doyourworkforapetstore?v1very coherent?mostly coherent?mostly incoherent?very incoherent?……01coherent?incoherent?……2018), bert-ruber (ghazarian et al., 2019) andgrade (huang et al., 2020).
however, these met-rics deviate from the actual human rating due to twolimitations.
first, they simplify the coherence eval-uation task in a two-level setting, i.e., coherent orincoherent, by maximizing the differences betweenthe positive coherent dialogues and the negative in-coherent ones obtained by some negative samplingstrategies.
in contrast, humans usually adopt likertscaling and give coherence scores from multiplelevels like 1 to 5, as shown in figure 1. second, toavoid relying on large-scale human-annotated data,they are mostly trained in a purely unsupervisedmanner and cannot align with the human ratingdue to the absence of introducing the actual humanrating standards during training..to address the above limitations, we propose anovel dialogue coherence metric training frame-work, named as quantiﬁable dialogue coherenceevaluation (quantidce).
this framework consistsof two training stages: multi-level ranking (mlr)pre-training and knowledge distillation (kd) ﬁne-tuning.
at the mlr pre-training stage, a new multi-level ranking (mlr) loss is proposed for learningthe coarse judgement of coherence degrees.
specif-ically, the mlr loss separates the context-responsepairs with different coherence levels and compactsthe pairs within the same level in one-dimensionalscore space.
as a result, the pretrained model isable to distinguish different coherence-level dia-logue responses for a given context and predictsmore accurate coherence scores.
at the kd ﬁne-tuning stage, the pretrained model is further ﬁne-tuned to learn the actual human rating standardswith only very few human-annotated coherencescores.
to mitigate overﬁtting into the scarce an-notated data during ﬁne-tuning, a novel knowledgedistillation regularization loss is introduced to re-tain the knowledge learned at the pre-training stage,where the pretrained model (teacher) provides thesoft targets for the model during ﬁne-tuning (stu-dent).
experimental results show that the metrictrained by our quantidce obviously outperformsthe other state-of-the-art metrics in terms of thepearson, spearman and kendall correlations withhuman judgements by around 5% points on aver-age.
to summarize our contributions:.
1) we propose quantidce, a novel quantiﬁabletraining framework for dialogue coherence eval-uation, which aims to align the automatic scoreswith the actual human rating standards via mlr.
pre-training and kd ﬁne-tuning.
to the best ofour knowledge, it is the ﬁrst attempt to considerthe quantiﬁable problem for dialogue coherenceevaluation..2) extensive experiments demonstrate the ef-fectiveness of our quantidce, which enables thetrained metric to have obviously stronger correla-tions with human judgements than the other state-of-the-art metrics..2 related work.
automatic coherence evaluation.
the widelyused automatic metrics, such as bleu (papineniet al., 2002), meteor (banerjee and lavie, 2005)and rouge (lin, 2004), use statistical rules tomeasure the degree of lexical word-overlap be-tween generated responses and reference responses.
however, these metrics have been demonstrated tocorrelate poorly with human judgments due to theabsence of semantic information (liu et al., 2016;novikova et al., 2017).
therefore, the subsequentmetrics are considered to incorporate the seman-tic information.
for instance, bertscore (zhanget al., 2020) turns to measure the soft semanticword-overlap rather than the hard lexical word-overlap like bleu.
moreover, learnable metricsencoding the semantic information have been at-tracting interests recently, which are trained in a su-pervised manner with large-scale human-annotateddata, such as adem (lowe et al., 2017), or trainedin an unsupervised manner with automatically con-structed data, such as ruber (tao et al., 2018)and bert-ruber (ghazarian et al., 2019).
fur-thermore, the recently proposed coherence met-ric, grade (huang et al., 2020), introduces thegraph information of dialogue topic transitionsand achieves the current state-of-the-art results.
note that these learnable metrics are trained in atwo-level training objective to separate the coher-ent dialogues from the incoherent ones, while ourquantidce models the task in a multi-level settingwhich is closer to the actual human rating..knowledge distillation.
knowledge distillation(kd) is a method that transfers the knowledge froma large trained teacher model to a smaller studentmodel by using the soft targets provided by theteacher (hinton et al., 2015).
in recent years, kdhas been applied to many speciﬁc tasks (sun et al.,2020; wei et al., 2019; kim and rush, 2016; sourtyet al., 2020).
unlike these previous works, we usekd to retain knowledge learned at the pre-training.
2719figure 2: the overall pipeline of our quantidce, consisting of two training stages which are marked by theblue and the black one-way arrows.
each input dialogue example contains one context with three-level candidateresponses and ﬁve responses for each level, shown as red, orange and green rectangles respectively.
the solidcircle represents the centroid score for each level of the ith dialogue.
at mlr pre-training stage, the context-response pairs are encoded with bert and transformed into the coherence scores through the mlp predictionnetwork, and then mlr loss is applied to optimize the network.
the dotted two-way arrows indicate that bothends should be separated, while the solid two-way arrows indicate that both ends should be compact.
and at thekd ﬁne-tuning stage, the student model is ﬁrst initialized with the teacher model and optimized by kd-mse loss..stage during ﬁne-tuning and do not compress themodel size of the student model..network and a multi-layer perceptron (mlp) as thepredictor network..3 quantidce framework.
in this section, we present quantidce, a two-stageframework for dialogue coherence metric learn-ing, consisting of multi-level ranking (mlr)pre-training and knowledge distillation (kd) ﬁne-tuning.
as illustrated in figure 2, given a met-ric model m (section 3.1), quantidce enablesm to learn multi-level representations for context-response pairs with different levels of coherencedegrees during the pre-training stage (section 3.2),and further to learn the rating standards of humanswith only a fraction of data during the ﬁne-tuningstage (section 3.3).
after these two training stages,the quantiﬁable gap between automatic metrics andhumans can be obviously reduced..3.1 model architecture.
in our quantidce framework, the metric model mis composed of: (1) an encoder network for encod-ing the input context-response pairs into featuresand (2) a predictor network for transforming the en-coded features into coherence scores.
speciﬁcally,we adopt bert (devlin et al., 2019) as the encoder.
given a context c = {c1, · · · , cm} and aresponse r = {r1, · · · , rn} where ci and riare tokens of the context and the responsethe c and r are concatenatedrespectively,as {[cls], c1, · · · , cm, [sep], r1, · · · , rn, [sep]},denoted as [c; r].
then the coherence score ˆs ofthe response r w.r.t.
the context c is predicted by:.
ˆs = m lp (bert ([c; r])),.
(1).
where m lp is a three-layer fully-connected net-work in which the activation functions of the threelayers are two exponential linear units (clevertet al., 2016) and a sigmoid function, respectively..3.2 mlr pre-training.
for learning the coarse judgement of coherencedegrees without the direct supervision of score an-notations, the model m is ﬁrst pretrained by mini-mizing a new multi-level ranking (mlr) loss on alarge-scale dialogue dataset.
concretely, the mlrloss is composed of a separation loss, a compact-ness loss and an ordering loss..formally, given a training dataset dpt =i=1 where ci is a dialogue context and.
{(ci, ri)}n1.
2720stage one: mlr pre-training𝓛𝑚𝑙𝑟bertmlpbertmlphuman ratingstage two: kd fine-tuning𝓁𝑘𝑑𝓁𝑚𝑠𝑒teacher modelkdstudent modellevel-1 centroid scorelevel-2 centroid scorelevel-3 centroid score＜＜level-1 scores𝒊𝒕𝒉dialogue example contextlevel-1level-2level-3responseseparationcompactness level-2 scoreslevel-3 scores𝓛kd_𝑚𝑠𝑒＜＜separation  losscompactness lossordering lossconcat………………concatconcati,k)}l.i,1, · · · , rj.
ri = {(rjj=1 is a response set withl coherence levels2 and k responses for each level,the model m is trained by minimizing the follow-ing mlr loss:.
lmlr =.
((cid:96)sep.i + (cid:96)com.
i + (cid:96)ord.
i.
),.
(2).
1n1.
n1(cid:88).
i=1.
i., (cid:96)comi., and (cid:96)ord.
where (cid:96)seprefer to the separationiloss, the compactness loss and the ordering loss ofthe ith example, respectively..the separation lossaims to separate the fea-tures of context-response pairs with different coher-ence levels by separating the coherence scores ofthe different pairs3.
moreover, to efﬁciently com-pute the loss, we ﬁrst compute the centroids ofthe context-response pairs belonging to the samecoherence level for the ith dialogue example, i.e.,ei = {eji ∈ r} whereˆsji,k is the coherence score of the context-responsepair (ci, rji,k), and the separation loss between thecentroids is then computed as follows:.
i,k|j ∈ [1, l], ej.
i = (cid:80)k.k=1 ˆsj.
(cid:96)sepi =.
l−1(cid:88).
l(cid:88).
j=1.
l=j+1.
max(0, w ∗ λ − d(ej.
i , el.
i)),.
(3).
where d(·) is the l1 distance, λ is the lower boundfor the distance between two centroids, and w =l − j is the distance weight used for amplifying thelower bound w.r.t.
the coherence-level gap..the compactness lossaims to compact the pairswithin the same level, which served as a regular-ization role to avoid the occurrence of outliers foreach coherence level.
speciﬁcally, the coherencescore ˆsji,k is forced to be closer to the correspondingcentroid ej.
i as follows:.
(cid:96)comi =.
max(0, d(ej.
i , ˆsj.
i,k) − µ),.
(4).
l(cid:88).
k(cid:88).
j=1.
k=1.
where µ is the upper bound for the distance be-tween the centroid of a certain coherence level andthe score within this level..2the coherence level is in ascending order, i.e., the re-sponse in a higher level is more coherent than the lower one.
3we also tried to directly restrict the features of different-level pairs to be separated, but the performance dropped com-pared with restricting the scores..the ordering lossis ﬁnally introduced to assurethat the rank order of the predicted scores satisﬁesthe pre-deﬁned order of coherence degrees, i.e.,ˆsji,k < ˆsj+1i,k , j ∈ [1, l−1], k ∈ [1, k].
it is criticalsince the separation loss only restricts the scoresof the pairs from different coherence levels to beseparated and this restriction is also satisﬁed whenthe scores of the highest level are lower than thescores of the lowest level.
similar to the separationloss, the ordering loss is also computed betweeneach two centroids as follows:.
(cid:96)ordi =.
l−1(cid:88).
l(cid:88).
j=1.
l=j+1.
max(0, el.
i − ej.
i )..(5).
3.3 kd fine-tuning.
the model m pretrained by the mlr loss is fur-ther trained at the kd ﬁne-tuning stage to directlylearn the actual human rating standards with only afraction of annotated data..formally, given a training dataset df t ={(ci, ri, si)}n2i=1 where ci, ri and si are the dia-logue context, the corresponding response and thehuman-annotated coherence score of ri w.r.t.
cirespectively, the previous ﬁne-tuning approach forthe scoring task usually optimizes the model mwith an mse loss between the predicted score ˆsiand the human score si:.
(6).
i = (si − ˆsi)2.
(cid:96)msehowever, by minimizing (cid:96)mse.
i.for each exam-ple, the model m will be easily over-ﬁtting onthe very few annotated data, and thus the modelgeneralizability will be dramatically reduced.
toovercome this issue, a novel knowledge distillation(kd) regularization is introduced for retaining theknowledge learned at the mlr pre-training stage.
concretely, the pretrained model m is treated asthe teacher model that provides the soft targets forthe student model ˆm which is entirely copied fromm .
and we adopt the distillation objectives oftinybert (jiao et al., 2020), including the distil-lations of the embedding layer, the transformerlayers and the prediction layer.
the kd loss is thenformulated as:.
(cid:96)kdi =.
||ot.
i − ˆot.
i||2.
2 +.
||at.
i − ˆat.
i||22,.
(7).
t +1(cid:88).
t=0.
t(cid:88).
t=1.
where || · ||2number of the transformer layers, ot.
2 indicates the squared l2 norm, t is thei are.
i and ˆot.
2721algorithm 1 training procedure of quantidce.
training datasets dpt and df t, metric.
input:model moutput: student model ˆm1: initialize m with bertbase2: for all (ci, ri) ∈ dpt dosi = m (ci, ri)3:compute the centroids ei for sicompute (cid:96)sepcompute (cid:96)comcompute lmlrupdate m to minimize lmlr.
and (cid:96)ordbetween ei and si.
for ei.
4:.
5:.
6:.
7:.
i.i.i.
8:9: end for10: initialize ˆm with m11: for all (ci, ri, si) ∈ df t do12:.
13:.
14:.
15:.
16:.
oi, ai = m (ci, ri)ˆsi, ˆoi, ˆai = ˆm (ci, ri)compute (cid:96)mseicompute (cid:96)kdcompute lkd mseupdate ˆm to minimize lkd mse.
between si and ˆsii between oi, ai and ˆoi, ˆai.
17:18: end for19: return student model ˆm.
the tth layer outputs of m and ˆm respectively, atiand ˆati are the attention matrices of the tth trans-former layer.
note that the layer 0 and the layert+1 refer to the embedding layer and the predictionlayer respectively..overall, the loss function for kd ﬁne-tuning,named as kd-mse loss, is the weighted sum of(cid:96)msei across the whole training dataset df t:i.and (cid:96)kd.
lkd mse =.
(α ∗ (cid:96)mse.
i + β ∗ (cid:96)kd.
i ),.
(8).
1n2.
n2(cid:88).
i=1.
where α and β are hyperparameters, and we empir-ically found that α = 1 and β = 5 performs well..the overall training procedure is summarized in.
algorithm 1..4 experiments.
4.1 experimental setup.
baseline metrics.
we compare the metric modeltrained by our quantidce with eight popu-lar automatic dialogue metrics, including threelexical word-overlap metrics: bleu (papineniet al., 2002), rouge (lin, 2004) and me-teor (banerjee and lavie, 2005), one seman-tic word-overlap metric, bertscore (zhang et al.,.
2020), and four learnable metrics: adem (loweet al., 2017), bert-ruber (ghazarian et al.,2019), bleurt (sellam et al., 2020) andgrade (huang et al., 2020)..evaluation.
our quantidce and the baselinesare evaluated by computing the correlations be-tween the model-predicted scores and the human-rated scores.
speciﬁcally, we adopt pearson, spear-man and kendall as the correlation measures and alarge-scale human judgement benchmark (huanget al., 2020) to provide the human-rated scores.
this benchmark contains 1,200 unique (context, re-sponse, human-rated score) triplets for metric eval-uation where the contexts were randomly selectedfrom the test set of three chit-chat datasets includ-ing dailydialog (li et al., 2017), convai2 (dinanet al., 2019) and empatheticdialogues (rashkinet al., 2019), and the responses were produced byboth the retrieval-based dialogue models and thegeneration-based ones to assure response diversity..training datasets.
we use two datasets, daily-dialog++4 and dailydialogeval5, to support thepre-training and ﬁne-tuning of quantidce, respec-tively.
the dailydialog++ dataset (sai et al., 2020)contains over 11k conversations, which augmentsthe original dailydialog dataset with multiple re-sponses of different quality levels including ﬁvegolden reference responses, ﬁve adversarial irrele-vant responses and ﬁve random selected responsesfor each context.
therefore, in this work, we setthe number of coherence levels l = 3 where thepairs containing the random responses, the adver-sarial responses and the reference responses respec-tively belong to the levels from 1 to 3. as to theﬁne-tuning data, we use the dailydialog humanjudgement dataset, denoted as dailydialogeval,which is a subset of the adopted evaluation bench-mark (huang et al., 2020), with 300 human ratingdata in total, and randomly split the data into train-ing (90%) and validation (10%) sets..implementation details.
we use bertbaseto initialize the encoder network, which is in linewith the current sota metric, grade.
for themlr pre-training, we pretrain our model for 5epochs with batch size 3 and learning rate 2e-5where the lower bound for the separation loss λ =0.3 and the upper bound for the compactness loss.
4https://github.com/iitmnlp/dialogue-evaluation-with-bert.
5https://github.com/li3cmz/grade.
2722metric.
pearson.
spearman kendall average.
loss.
pearson spearman kendall average.
bleurougemeteorbertscoreadembert-ruberbleurtgradequantidce.
convai2.
0.003 *0.1360.1450.2250.026 *0.2660.1520.4960.554empatheticdialogues.
0.1280.1400.1810.2250.037 *0.2660.1490.5030.554.
0.0880.0970.1230.1540.049 *0.1850.1030.3560.395.
-0.051 *bleu0.029 *rouge0.118meteor0.046 *bertscoreadem0.007 *bert-ruber -0.022 *0.203bleurt0.350grade0.412quantidce.
0.002 *-0.013 *0.055 *0.033 *0.009 *-0.040 *0.1920.3440.393.
0.005 *-0.010 *0.04 *0.021 *0.040 *-0.029 *0.130.2430.274.
0.0730.1240.150.2010.0370.2390.1350.4520.501.
-0.0150.0020.0710.0330.019-0.0300.1750.3120.360.table 1: correlations between automatic evaluationmetrics and human judgements on two datasets (con-vai2 and empatheticdialogues).
the star * indicatesresults with p-value > 0.05, which are not statistically signiﬁ-cant..µ = 0.1. for the kd ﬁne-tuning, we further ﬁne-tune the pretrained model for 20 epochs with batchsize 10 and learning rate 5e-6.
for all the training,bertadam is used as the optimizer with β1 = 0.9and β2 = 0.999. for the transformer-layer distilla-tion, we distill all the transformer layers since themodel architectures of the teacher and the studentare exactly the same..4.2 experimental results.
metric performance.
the correlation results ofquantidce and the other baseline metrics on thelarge-scale human judgement benchmark are pre-sented in table 1, including the convai2 and theempatheticdialogues datasets.6 for a fair com-parison, the learnable baseline metrics, adem,bert-ruber and grade, are trained on thetraining dataset we adopted, i.e., dailydialog++.7generally, quantidce achieves an absolute aver-aged correlation improvement by around 5% pointsover the current sota, grade.
besides, all theresults of quantidce are statistically signiﬁcantwith p-value <0.01..6the dailydialogeval dataset was not used for evalua-.
tion since we used it for ﬁne-tuning..7bleurt was not trained on dailydialog++ since thisdataset is not suitable for the bleurt pre-training strategy.
instead, we trained bleurt with the ﬁne-tuning data we used.
the training details of these baseline metrics are provided inappendix a..bcerankingsupconfatvanilla mlrmlr (ours).
bcerankingsupconfatvanilla mlrmlr (ours).
convai20.5050.5040.5230.5210.5360.554.
0.5050.5070.4950.5160.5220.554empatheticdialogues0.3540.3990.3320.3810.4030.412.
0.3530.3890.3150.3580.3870.393.
0.3610.3600.3670.3710.3790.395.
0.2430.2720.220.2450.2670.274.
0.4570.4570.4620.4690.4790.501.
0.3170.3530.2890.3280.3520.360.table 2: correlations between human judgements andthe metric models trained with different losses duringpre-training and the same kd-mse loss during ﬁne-tuning.
ranking represents the margin ranking loss..pre-training objective.
to verify the superior-ity of our pre-training objective, namely the mlrloss, we investigated the performance of several ex-isting loss functions for pre-training compared withours.
speciﬁcally, two categories of loss functionsused for metric training are adopted, including (a)the two-level setting and (b) the multi-level setting.
the binary cross entropy (bce) loss and the marginranking loss are adopted for the two-level setting,while another three loss functions are adopted forthe multi-level setting, including the supervisedcontrastive (supcon) loss (khosla et al., 2020), thefast-approximated triplet (fat) loss (yuan et al.,2019) and the vanilla mlr loss (lin et al., 2020) 8.as shown in table 2, the performance of our mlrloss is the best among all the pre-training objec-tives.
and we also found that the multi-level set-ting losses perform better than the two-level ones,especially on the convai2 dataset.
moreover, inorder to more intuitively analyze the performancesof these pre-training objectives, we also visualizethe encoded features and the predicted scores ofthe model m after being pretrained by the aboveloss functions on the dailydialog++ dataset with-out ﬁne-tuning.9 as shown in figure 3, (a) thebce loss cannot separate the level-1 scores fromthe level-2 ones and the corresponding features arealso mixed; (b) the fat loss, on the other hand,separates the features of different levels well, butdoes not consider the relative gaps where the dis-tances between the level-1 and level-3 features are.
8the details of these pre-training loss fucntions are pro-.
vided in appendix b..9the visualization results of the ranking loss, supcon loss.
and vanilla mlr loss are provided in appendix c..2723figure 3: visualizations of features (the scatter plots in the upper row) and scores (the violin plots in the lower row)on the dailydialog++ dataset.
the features and scores in each of the three columns are obtained from the metricmodel m only pretrained with the bce loss, the fat loss and our mlr loss, respectively..pearson spearman kendall averageconvai2 (best epoch).
loss.
msemse (ﬁx encoder)kd-mse (ours).
msemse (ﬁx encoder)kd-mse (ours).
msemse (ﬁx encoder)kd-mse (ours).
0.2720.4770.554.
0.2780.3840.412.
0.9340.3790.804.empatheticdialogues (best epoch).
dailydialogeval (last epoch).
0.3690.4770.554.
0.2760.3670.393.
0.9450.4020.832.
0.2550.3370.395.
0.1870.2530.274.
0.8670.2810.678.
0.2990.4300.501.
0.2470.3350.360.
0.9150.3540.771.table 3: correlations between human judgements andthe metric model m further trained with different ﬁne-tuning losses after mlr pre-training..not larger than those between level-1 and level-2;(c) in contrast, our mlr loss separates both thefeatures and the scores well and also considers therelative gaps between different levels..fine-tuning objective.
furthermore, we alsoveriﬁed the effectiveness of our kd-mse loss dur-ing ﬁne-tuning by comparing with other ﬁne-tuninglosses, including the pure mse loss without kdregularization as shown in equation 6 and the samemse loss except for freezing the encoder networkand only ﬁnetuning the predictor network i.e.
themlp, denoted as mse (ﬁx encoder).
as the resultsshown in table 3, compared with the other twolosses, the model ﬁnetuned by our kd-mse losshas the highest correlation results on both convai2and empatheticdialogues.
moreover, by compar-.
figure 4: score visualization on the dailydialog++dataset where the scores are predicted by our quan-tidce after kd ﬁne-tuning..ing the results of mse and kd-mse, we can ﬁndthat introducing kd regularization leads to obvi-ous averaged correlation improvements by 20.2%points on convai2 and 11.3% points on empa-theticdialogues, which veriﬁes the effectivenessof the kd loss.
besides, we also reported the last-epoch correlation results on the training dataset,dailydialogeval.
and the results of mse andmse (ﬁx encoder) indicate the phenomena of over-ﬁtting and under-ﬁtting into dailydialogeval re-spectively, which explain the reasons of their lowperformance on the two evaluation datasets.
incontrast, our kd-mse loss enables the model tolearn the actual human rating standards from thescarce annotated data and avoid overﬁtting it si-.
2724(c)mlr(ours)(a)bce(b)fat0.10.20.30.40.50.60.70.80.9score321levelmetricquantidcew/o mlr pre-training.
w/o separation lossw/o compactness lossw/o ordering loss.
w/o kd ﬁne-tuning.
pearson spearman kendall average0.5540.3730.3880.526-0.4940.531.
0.5010.3250.3640.489-0.4620.484.
0.5540.3570.4160.550-0.5220.540.
0.3950.2460.2890.390-0.3710.381.table 4: ablation studies on the convai2 dataset by re-moving one of the component in quantidce, includingthe mlr loss (w/o mlr pre-training), the kd+mseloss (w/o kd ﬁne-tuning), and three secondary lossesof the mlr loss..u1: i need to book a plane ticket to london.
u2: round-trip or one-way?
r: round trip or one way trip?
coherence score (human / quantidce / grade) : 2.10 / 2.85 / 4.52u1: yum.
you will ﬁnd me in the kitchen and if not i am ﬁshing.
u2: wow that’s pretty cool what else you do for fun?
r: probably ﬁsh it is great.
coherence score (human / quantidce / grade) : 2.50 / 3.94 / 4.27.table 5: two representative examples to show thestrength and weakness of our quantidce where u1and u2 are two utterances of the context and r is thecorresponding response..number of data for fine-tuning.
moreover,we also investigated how the scale of data for ﬁne-tuning effects the model performance by increas-ing the number of ﬁne-tuning data 5% each timefrom zero.
the trend of the model performance ispresented in figure 5. we observed that minimiz-ing our kd-mse loss made the correlation resultshave a gradually increasing trend after an initial de-crease.10 more speciﬁcally, the result achieved thestandard before ﬁne-tuning at around the 70% datascale and continued increasing until 100% with aﬁnal improvement by around 2% points.
for com-parison, the performance trends of mse and mse(ﬁx encoder) are also provided.
and the resultspresent overall decreasing trends of the model per-formance, which indicates that the model trainedby mse or mse (ﬁx encoder) cannot beneﬁt fromthe increasing of data scale, due to the severe over-ﬁtting or under-ﬁtting.
therefore, to effectivelyutilize the limited data, it is important to enablethe update of the entire network and add some con-straints to avoid over-ﬁtting, such as our proposedkd regularization..4.4 case study.
to illustrate the performance of quantidce, tworepresentative examples are shown in table 5 .
theﬁrst example shows the strength of quantidcewhere the coherence score given by ours is closerto the human rating score compared with the ex-tremely high score given by grade.
however,in the second example, both our quantidce andgrade deviate from the human score, possiblybecause the number of coherence levels we adoptedin this work (l = 3) is insufﬁcient as humans usu-ally consider more levels of dialogue coherence..10the initial decrease probably attributes to the randomnessof data sampling where the smaller the sampling ratio is, thehigher the probability that noisy samples dominate the sam-pled data will be.
and overﬁtting into the noisy samples leadsto the performance decrease..figure 5: the performance trends when changing thenumber of annotated data used for different ﬁne-tuningobjectives.
each point in the line chart indicates the av-eraged correlation of pearson, spearman and kendall..multaneously.
finally, in figure 4, we present thevisualization of the scores predicted by our quan-tidce after kd ﬁne-tuning.
compared with thescore distributions before ﬁne-tuning in figure 3(c),the ﬁnetuned score distributions of the level-1 andlevel-3 are wider and partly overlap with the level-2 distribution.
it is predictable as the judgementsof coherence are always subjective and humanstend to give vague and middle scores instead ofextremely high or low scores..4.3 ablation studies.
component analysis.
to verify the contribu-tions of the core components in our quantidce, wefurther conducted ablation studies on the convai2dataset.
as shown in table 4, both the mlr pre-training and kd ﬁne-tuning contribute to the betterperformance of quantidce.
besides, we also con-ducted ablations by removing one of the secondaryloss during mlr pre-training, including the sepa-ration loss, the compactness loss and the orderingloss.
the results show that the performance ben-eﬁts from all these losses in which the separationloss and the ordering loss are crucial for training ametric with strong and positive human correlations..27250102030405060708090100data scale (%)0.200.250.300.350.400.450.50correlationmsemse (fix encoder)kd-mse (ours)5 conclusion.
in this paper, we propose quantidce, a novel train-ing framework aiming to bridge the gap betweenthe training objective and the actual human ratingand train a quantiﬁable dialogue coherence met-ric.
in general, quantidce includes two trainingstages, mlr pre-training for learning the coarsehuman judgements of dialogue coherence degrees,and kd ﬁne-tuning for learning the actual humanrating standards.
experimental results show thatthe metric trained by quantidce presents strongcorrelations with human judgements.
for futurework, it is interesting to investigate a more efﬁcientway to obtain multi-level data and extend the multi-level setting into the general evaluation for naturallanguage generation..acknowledgments.
science.
we thank all anonymous reviewers for theirconstructive comments.
this work was supportedin part by national key r&d program of2020aaa0109700,china under grant no.
offoundationnational naturalchina(nsfc) under grant no.u19a2073and no.61976233, guangdong province ba-sic and applied basic research (regionaljoint fund-key) grant no.2019b1515120039,shenzhenprogramfundamental research(project no.
rcyx20200714114642083, no.
jcyj20190807154211365), zhijiang lab’s openfund (no.
2020aa3ab14) and csig youngfellow support fund..references.
satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, michigan.
association for computational lin-guistics..alessandra cervone, evgeny stepanov, and giuseppericcardi.
2018. coherence models for dialogue.
in proceedings of the 19th annual conference ofthe international speech communcation associa-tion, pages 1011–1015..djork-arn´e clevert, thomas unterthiner, and sepphochreiter.
2016. fast and accurate deep networklearning by exponential linear units (elus).
in inter-national conference on learning representations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, varvara logacheva, valentin malykh,alexander h. miller, kurt shuster, jack urbanek,douwe kiela, arthur szlam, iulian serban, ryanlowe, shrimai prabhumoye, alan w. black, alexan-der i. rudnicky, jason williams, joelle pineau,mikhail burtsev, and jason weston.
2019. the sec-ond conversational intelligence challenge (convai2).
computing research repository, arxiv:1902.00098.version 1..sarik ghazarian, johnny wei, aram galstyan, andnanyun peng.
2019. better automatic evaluation ofopen-domain dialogue systems with contextualizedin proceedings of the workshop onembeddings.
methods for optimizing and evaluating neural lan-guage generation, pages 82–89, minneapolis, min-nesota.
association for computational linguistics..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
computing research repository, arxiv:1503.02531.version 1..lishan huang, zheng ye, jinghui qin, liang lin, andxiaodan liang.
2020. grade: automatic graph-enhanced coherence metric for evaluating open-in proceedings of thedomain dialogue systems.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 9230–9240,online.
association for computational linguistics..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in findings of the associationfor computational linguistics: emnlp 2020, pages4163–4174, online.
association for computationallinguistics..prannay khosla, piotr teterwak, chen wang, aaronsarna, yonglong tian, phillip isola, aaronmaschinot, ce liu, and dilip krishnan.
2020. su-pervised contrastive learning.
in advances in neu-ral information processing systems 33: annual con-ference on neural information processing systems2020, neurips 2020, december 6-12, 2020, virtual..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 1317–1327, austin,texas.
association for computational linguistics..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-.
2726ally labelled multi-turn dialogue dataset.
in proceed-ings of the eighth international joint conference onnatural language processing (volume 1: long pa-pers), pages 986–995, taipei, taiwan.
asian federa-tion of natural language processing..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..zibo lin, deng cai, yan wang, xiaojiang liu, haitaozheng, and shuming shi.
2020. the world is notbinary: learning to rank with grayscale data for di-in proceedings of thealogue response selection.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 9220–9229,online.
association for computational linguistics..chia-wei liu, ryan lowe, iulian serban, mike nose-worthy, laurent charlin, and joelle pineau.
2016.how not to evaluate your dialogue system: anempirical study of unsupervised evaluation metricsfor dialogue response generation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2122–2132, austin,texas.
association for computational linguistics..ryan lowe, michael noseworthy, iulian vlad ser-ban, nicolas angelard-gontier, yoshua bengio, andjoelle pineau.
2017. towards an automatic tur-ing test: learning to evaluate dialogue responses.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1116–1126, vancouver,canada.
association for computational linguistics..jekaterina novikova, ondˇrej duˇsek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedingsnew evaluation metrics for nlg.
of the 2017 conference on empirical methods innatural language processing, pages 2241–2252,copenhagen, denmark.
association for computa-tional linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark andin proceedings of the 57th annual meet-dataset.
ing of the association for computational linguis-tics, pages 5370–5381, florence, italy.
associationfor computational linguistics..ananya b. sai, akash kumar mohankumar, sid-dhartha arora, and mitesh m. khapra.
2020.im-proving dialog evaluation with a multi-referenceadversarial dataset and large scale pretraining..computing research repository, arxiv:2009.11321.version 1..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..rapha¨el sourty, jose g. moreno, franc¸ois-paul ser-vant, and lynda tamine-lechani.
2020. knowledgebase embedding by cooperative knowledge distilla-tion.
in proceedings of the 28th international con-ference on computational linguistics, pages 5579–5590, barcelona, spain (online).
international com-mittee on computational linguistics..jingyuan sun, shaonan wang, jiajun zhang, andchengqing zong.
2020. distill and replay for con-tinual language learning.
in proceedings of the 28thinternational conference on computational linguis-tics, pages 3569–3579, barcelona, spain (online).
international committee on computational linguis-tics..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in aaai..hao-ran wei, shujian huang, ran wang, xin-yu dai,and jiajun chen.
2019. online distilling from check-in proceed-points for neural machine translation.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 1932–1941, min-neapolis, minnesota.
association for computationallinguistics..ye yuan, wuyang chen, yang yang, and zhangyanglossin defense ofwang.
2019.again: learning robust person re-identiﬁcation withfast approximated triplet loss and label distillation.
computing research repository, arxiv:1912.07863.version 2..the triplet.
tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..a training details of the learnable.
baseline metrics.
a) following sai et al.
(2020), we trained ademby ﬁrst initializing it with the ofﬁcial checkpointand further ﬁnetuning on dailydialog++ with atarget of 5 for level-3 pairs and 1 for level-1 pairs;b) bert-ruber and grade were both trainedon dailydialog++ where level-3 pairs as positive.
2727samples and both level-1 and level-2 pairs as neg-ative samples, except that the former use cross-entropy loss while the latter use ranking loss; c)bleurt was initialized with the ofﬁcial recom-mended checkpoint bleurt-base and ﬁnetunedon dailydilaogeval by following the ofﬁce guide-lines11..vanilla mlr loss.
the vanilla mlr loss (linet al., 2020) is the extension of the margin rankingloss to a multi-level version by repeatedly applyingthe original margin ranking loss between differentlevels, which can be directly applied to our evalua-tion task..c visualizations of the pre-training.
b details of the pre-training losses.
losses.
we have already compared the visualization resultsof the bce loss and the fat loss.
for a supple-ment, here we mainly introduce the visualizationsof the margin ranking loss, the supcon loss andthe vanilla mlr loss in detail..as we can see in figure 6, (a) the margin rank-ing loss cannot separate the level-1 scores fromthe level-2 ones and the corresponding features arealso mixed, which is similar to the bce loss; (b)the supcon loss, on the other hand, can distinguishthe features and scores of the three levels to someextent, and the scores of different levels are alsoseparated but do not follow the real rank order, i.e.,level-1 < level-2 < level-3; (c) the ﬁnal vanillamlr loss can separate the context-response pairswith different coherence level in feature space andthe predicted scores also follow the actual rankorder.
however, its score distributions are not com-pact enough for the level-1 and level-3..bce loss.
the binary cross entropy (bce) lossis adopted for the experiments of the two-levelsetting, where both the adversarial irrelevant re-sponses and random selected responses of the dai-lydialog++ dataset (sai et al., 2020) are treated asnegative samples and labeled as 0, while the goldenreference responses are treated as positive samplesand labeled as 1..margin ranking loss.
similarly, the marginranking loss simpliﬁes the evaluation task as atwo-level setting and maximizes the differencesbetween the positive coherent dialogues and thenegative incoherent ones.
as the name suggests,the focus of the margin ranking loss is ranking,which aims at ranking the scores of positive co-herent dialogues ahead of the negative incoherentones..supcon loss.
the supervised contrastive (sup-con) loss (khosla et al., 2020), which pulls the pos-itive anchors closer and pushes the negatives fartheraway in representation space, can be adopted forthe multi-level setting.
here, for our multi-levelsetting, we consider the dialogues of level-1, level-2, and level-3 as positive anchors successively, andthe remaining two levels as corresponding nega-tives..fat loss.
the fast-approximated triplet (fat)loss (yuan et al., 2019) replaces the traditionalpoint-to-point distances of the triplet loss withpoint-to-cluster distances, through an upper boundrelaxation of the triplet form, which is ﬁrst appliedfor the classiﬁcation task and obviously reducesthe computation cost.
to use fat loss in our eval-uation task, we consider the different coherencelevels as different classes and perform the fat lossto separate the context-response pairs with differentcoherence levels..11https://github.com/google-research/.
bleurt.
2728figure 6: visualizations of features (the scatter plots in the upper row) and scores (the violin plots in the lower row)on the dailydialog++ dataset.
the features and scores in each of the three columns are obtained from the metricmodel m only pretrained with the margin ranking loss, the supcon loss and the vanilla mlr loss, respectively..2729(c)vanillamlr(b)supcon(a)ranking