towards quantiï¬able dialogue coherence evaluation.
zheng ye1, liucun lu1, lishan huang2, liang lin2,3, xiaodan liang1âˆ—1shenzhen campus of sun yat-sen university, 2sun yat-sen university, 3dark matter ai inc.{yezh7,lulc,huanglsh6}@mail2.sysu.edu.cn,linliang@ieee.org, xdliang328@gmail.com.
abstract.
automatic dialogue coherence evaluation hasattracted increasing attention and is crucial fordeveloping promising dialogue systems.
how-ever, existing metrics have two major limita-tions: (a) they are mostly trained in a simpli-ï¬ed two-level setting (coherent vs. incoherent),while humans give likert-type multi-level co-herence scores, dubbed as â€œquantiï¬ableâ€; (b)their predicted coherence scores cannot alignwith the actual human rating standards due tothe absence of human guidance during train-ing.
to address these limitations, we proposequantiï¬able dialogue coherence evaluation(quantidce), a novel framework aiming totrain a quantiï¬able dialogue coherence metricthat can reï¬‚ect the actual human rating stan-dards.
speciï¬cally, quantidce includes twotraining stages, multi-level ranking (mlr)pre-training and knowledge distillation (kd)ï¬ne-tuning.
during mlr pre-training, a newmlr loss is proposed for enabling the modelto learn the coarse judgement of coherencedegrees.
then, during kd ï¬ne-tuning, thepretrained model is further ï¬netuned to learnthe actual human rating standards with onlyvery few human-annotated data.
to advo-cate the generalizability even with limited ï¬ne-tuning data, a novel kd regularization is intro-duced to retain the knowledge learned at thepre-training stage.
experimental results showthat the model trained by quantidce presentsstronger correlations with human judgementsthan the other state-of-the-art metrics.
1.
1.introduction.
dialogue coherence, which requires a response tobe ï¬‚uent, consistent and context-related, is an es-sential property for developing promising dialogue.
âˆ—corresponding author.
1the code and trained checkpoints are available at https:.
//github.com/james-yip/quantidce..figure 1: likert-type multi-level human rating vs. two-level automatic evaluation.
human rating always con-siders multiple coherence degrees, while most of theexisting automatic metrics only learn to distinguish thecoherence dialogues from the incoherent ones and giverelatively extreme coherence scores..systems (cervone et al., 2018).
however, it is stillchallenging to evaluate the coherence of a responsegenerated by a dialogue system.
although humanevaluation is always considered as the most accu-rate way to evaluate the coherence, it is expensiveand high-latency, which cannot meet the evaluationdemand of the frequent development of dialoguesystems.
therefore, automatic evaluation metricsare developed to serve as human proxies that canrapidly compute the dialogue coherence and returnrelatively accurate results..the current widely used metrics measure thelexical word-overlap between generated responsesand reference responses, such as bleu (papineniet al., 2002) and rouge (lin, 2004).
however,they have been demonstrated to be biased and cor-relate poorly with human judgements since no se-mantic information is considered (liu et al., 2016;novikova et al., 2017).
to overcome this issue,researchers turned to develop learnable metricsbased on neural networks that incorporate the se-mantic information, such as ruber (tao et al.,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2718â€“2729august1â€“6,2021.Â©2021associationforcomputationallinguistics271813254contextresponseu1:ilikeanimals,especiallydogs.
how about you?u2:haha,ilikecatsmore.i like cats more too and i work in a pet store.u3:doyourworkforapetstore?v1very coherent?mostly coherent?mostly incoherent?very incoherent?â€¦â€¦01coherent?incoherent?â€¦â€¦2018), bert-ruber (ghazarian et al., 2019) andgrade (huang et al., 2020).
however, these met-rics deviate from the actual human rating due to twolimitations.
first, they simplify the coherence eval-uation task in a two-level setting, i.e., coherent orincoherent, by maximizing the differences betweenthe positive coherent dialogues and the negative in-coherent ones obtained by some negative samplingstrategies.
in contrast, humans usually adopt likertscaling and give coherence scores from multiplelevels like 1 to 5, as shown in figure 1. second, toavoid relying on large-scale human-annotated data,they are mostly trained in a purely unsupervisedmanner and cannot align with the human ratingdue to the absence of introducing the actual humanrating standards during training..to address the above limitations, we propose anovel dialogue coherence metric training frame-work, named as quantiï¬able dialogue coherenceevaluation (quantidce).
this framework consistsof two training stages: multi-level ranking (mlr)pre-training and knowledge distillation (kd) ï¬ne-tuning.
at the mlr pre-training stage, a new multi-level ranking (mlr) loss is proposed for learningthe coarse judgement of coherence degrees.
specif-ically, the mlr loss separates the context-responsepairs with different coherence levels and compactsthe pairs within the same level in one-dimensionalscore space.
as a result, the pretrained model isable to distinguish different coherence-level dia-logue responses for a given context and predictsmore accurate coherence scores.
at the kd ï¬ne-tuning stage, the pretrained model is further ï¬ne-tuned to learn the actual human rating standardswith only very few human-annotated coherencescores.
to mitigate overï¬tting into the scarce an-notated data during ï¬ne-tuning, a novel knowledgedistillation regularization loss is introduced to re-tain the knowledge learned at the pre-training stage,where the pretrained model (teacher) provides thesoft targets for the model during ï¬ne-tuning (stu-dent).
experimental results show that the metrictrained by our quantidce obviously outperformsthe other state-of-the-art metrics in terms of thepearson, spearman and kendall correlations withhuman judgements by around 5% points on aver-age.
to summarize our contributions:.
1) we propose quantidce, a novel quantiï¬abletraining framework for dialogue coherence eval-uation, which aims to align the automatic scoreswith the actual human rating standards via mlr.
pre-training and kd ï¬ne-tuning.
to the best ofour knowledge, it is the ï¬rst attempt to considerthe quantiï¬able problem for dialogue coherenceevaluation..2) extensive experiments demonstrate the ef-fectiveness of our quantidce, which enables thetrained metric to have obviously stronger correla-tions with human judgements than the other state-of-the-art metrics..2 related work.
automatic coherence evaluation.
the widelyused automatic metrics, such as bleu (papineniet al., 2002), meteor (banerjee and lavie, 2005)and rouge (lin, 2004), use statistical rules tomeasure the degree of lexical word-overlap be-tween generated responses and reference responses.
however, these metrics have been demonstrated tocorrelate poorly with human judgments due to theabsence of semantic information (liu et al., 2016;novikova et al., 2017).
therefore, the subsequentmetrics are considered to incorporate the seman-tic information.
for instance, bertscore (zhanget al., 2020) turns to measure the soft semanticword-overlap rather than the hard lexical word-overlap like bleu.
moreover, learnable metricsencoding the semantic information have been at-tracting interests recently, which are trained in a su-pervised manner with large-scale human-annotateddata, such as adem (lowe et al., 2017), or trainedin an unsupervised manner with automatically con-structed data, such as ruber (tao et al., 2018)and bert-ruber (ghazarian et al., 2019).
fur-thermore, the recently proposed coherence met-ric, grade (huang et al., 2020), introduces thegraph information of dialogue topic transitionsand achieves the current state-of-the-art results.
note that these learnable metrics are trained in atwo-level training objective to separate the coher-ent dialogues from the incoherent ones, while ourquantidce models the task in a multi-level settingwhich is closer to the actual human rating..knowledge distillation.
knowledge distillation(kd) is a method that transfers the knowledge froma large trained teacher model to a smaller studentmodel by using the soft targets provided by theteacher (hinton et al., 2015).
in recent years, kdhas been applied to many speciï¬c tasks (sun et al.,2020; wei et al., 2019; kim and rush, 2016; sourtyet al., 2020).
unlike these previous works, we usekd to retain knowledge learned at the pre-training.
2719figure 2: the overall pipeline of our quantidce, consisting of two training stages which are marked by theblue and the black one-way arrows.
each input dialogue example contains one context with three-level candidateresponses and ï¬ve responses for each level, shown as red, orange and green rectangles respectively.
the solidcircle represents the centroid score for each level of the ith dialogue.
at mlr pre-training stage, the context-response pairs are encoded with bert and transformed into the coherence scores through the mlp predictionnetwork, and then mlr loss is applied to optimize the network.
the dotted two-way arrows indicate that bothends should be separated, while the solid two-way arrows indicate that both ends should be compact.
and at thekd ï¬ne-tuning stage, the student model is ï¬rst initialized with the teacher model and optimized by kd-mse loss..stage during ï¬ne-tuning and do not compress themodel size of the student model..network and a multi-layer perceptron (mlp) as thepredictor network..3 quantidce framework.
in this section, we present quantidce, a two-stageframework for dialogue coherence metric learn-ing, consisting of multi-level ranking (mlr)pre-training and knowledge distillation (kd) ï¬ne-tuning.
as illustrated in figure 2, given a met-ric model m (section 3.1), quantidce enablesm to learn multi-level representations for context-response pairs with different levels of coherencedegrees during the pre-training stage (section 3.2),and further to learn the rating standards of humanswith only a fraction of data during the ï¬ne-tuningstage (section 3.3).
after these two training stages,the quantiï¬able gap between automatic metrics andhumans can be obviously reduced..3.1 model architecture.
in our quantidce framework, the metric model mis composed of: (1) an encoder network for encod-ing the input context-response pairs into featuresand (2) a predictor network for transforming the en-coded features into coherence scores.
speciï¬cally,we adopt bert (devlin et al., 2019) as the encoder.
given a context c = {c1, Â· Â· Â· , cm} and aresponse r = {r1, Â· Â· Â· , rn} where ci and riare tokens of the context and the responsethe c and r are concatenatedrespectively,as {[cls], c1, Â· Â· Â· , cm, [sep], r1, Â· Â· Â· , rn, [sep]},denoted as [c; r].
then the coherence score Ë†s ofthe response r w.r.t.
the context c is predicted by:.
Ë†s = m lp (bert ([c; r])),.
(1).
where m lp is a three-layer fully-connected net-work in which the activation functions of the threelayers are two exponential linear units (clevertet al., 2016) and a sigmoid function, respectively..3.2 mlr pre-training.
for learning the coarse judgement of coherencedegrees without the direct supervision of score an-notations, the model m is ï¬rst pretrained by mini-mizing a new multi-level ranking (mlr) loss on alarge-scale dialogue dataset.
concretely, the mlrloss is composed of a separation loss, a compact-ness loss and an ordering loss..formally, given a training dataset dpt =i=1 where ci is a dialogue context and.
{(ci, ri)}n1.
2720stage one: mlr pre-trainingğ“›ğ‘šğ‘™ğ‘Ÿbertmlpbertmlphuman ratingstage two: kd fine-tuningğ“ğ‘˜ğ‘‘ğ“ğ‘šğ‘ ğ‘’teacher modelkdstudent modellevel-1 centroid scorelevel-2 centroid scorelevel-3 centroid scoreï¼œï¼œlevel-1 scoresğ’Šğ’•ğ’‰dialogue example contextlevel-1level-2level-3responseseparationcompactness level-2 scoreslevel-3 scoresğ“›kd_ğ‘šğ‘ ğ‘’ï¼œï¼œseparation  losscompactness lossordering lossconcatâ€¦â€¦â€¦â€¦â€¦â€¦concatconcati,k)}l.i,1, Â· Â· Â· , rj.
ri = {(rjj=1 is a response set withl coherence levels2 and k responses for each level,the model m is trained by minimizing the follow-ing mlr loss:.
lmlr =.
((cid:96)sep.i + (cid:96)com.
i + (cid:96)ord.
i.
),.
(2).
1n1.
n1(cid:88).
i=1.
i., (cid:96)comi., and (cid:96)ord.
where (cid:96)seprefer to the separationiloss, the compactness loss and the ordering loss ofthe ith example, respectively..the separation lossaims to separate the fea-tures of context-response pairs with different coher-ence levels by separating the coherence scores ofthe different pairs3.
moreover, to efï¬ciently com-pute the loss, we ï¬rst compute the centroids ofthe context-response pairs belonging to the samecoherence level for the ith dialogue example, i.e.,ei = {eji âˆˆ r} whereË†sji,k is the coherence score of the context-responsepair (ci, rji,k), and the separation loss between thecentroids is then computed as follows:.
i,k|j âˆˆ [1, l], ej.
i = (cid:80)k.k=1 Ë†sj.
(cid:96)sepi =.
lâˆ’1(cid:88).
l(cid:88).
j=1.
l=j+1.
max(0, w âˆ— Î» âˆ’ d(ej.
i , el.
i)),.
(3).
where d(Â·) is the l1 distance, Î» is the lower boundfor the distance between two centroids, and w =l âˆ’ j is the distance weight used for amplifying thelower bound w.r.t.
the coherence-level gap..the compactness lossaims to compact the pairswithin the same level, which served as a regular-ization role to avoid the occurrence of outliers foreach coherence level.
speciï¬cally, the coherencescore Ë†sji,k is forced to be closer to the correspondingcentroid ej.
i as follows:.
(cid:96)comi =.
max(0, d(ej.
i , Ë†sj.
i,k) âˆ’ Âµ),.
(4).
l(cid:88).
k(cid:88).
j=1.
k=1.
where Âµ is the upper bound for the distance be-tween the centroid of a certain coherence level andthe score within this level..2the coherence level is in ascending order, i.e., the re-sponse in a higher level is more coherent than the lower one.
3we also tried to directly restrict the features of different-level pairs to be separated, but the performance dropped com-pared with restricting the scores..the ordering lossis ï¬nally introduced to assurethat the rank order of the predicted scores satisï¬esthe pre-deï¬ned order of coherence degrees, i.e.,Ë†sji,k < Ë†sj+1i,k , j âˆˆ [1, lâˆ’1], k âˆˆ [1, k].
it is criticalsince the separation loss only restricts the scoresof the pairs from different coherence levels to beseparated and this restriction is also satisï¬ed whenthe scores of the highest level are lower than thescores of the lowest level.
similar to the separationloss, the ordering loss is also computed betweeneach two centroids as follows:.
(cid:96)ordi =.
lâˆ’1(cid:88).
l(cid:88).
j=1.
l=j+1.
max(0, el.
i âˆ’ ej.
i )..(5).
3.3 kd fine-tuning.
the model m pretrained by the mlr loss is fur-ther trained at the kd ï¬ne-tuning stage to directlylearn the actual human rating standards with only afraction of annotated data..formally, given a training dataset df t ={(ci, ri, si)}n2i=1 where ci, ri and si are the dia-logue context, the corresponding response and thehuman-annotated coherence score of ri w.r.t.
cirespectively, the previous ï¬ne-tuning approach forthe scoring task usually optimizes the model mwith an mse loss between the predicted score Ë†siand the human score si:.
(6).
i = (si âˆ’ Ë†si)2.
(cid:96)msehowever, by minimizing (cid:96)mse.
i.for each exam-ple, the model m will be easily over-ï¬tting onthe very few annotated data, and thus the modelgeneralizability will be dramatically reduced.
toovercome this issue, a novel knowledge distillation(kd) regularization is introduced for retaining theknowledge learned at the mlr pre-training stage.
concretely, the pretrained model m is treated asthe teacher model that provides the soft targets forthe student model Ë†m which is entirely copied fromm .
and we adopt the distillation objectives oftinybert (jiao et al., 2020), including the distil-lations of the embedding layer, the transformerlayers and the prediction layer.
the kd loss is thenformulated as:.
(cid:96)kdi =.
||ot.
i âˆ’ Ë†ot.
i||2.
2 +.
||at.
i âˆ’ Ë†at.
i||22,.
(7).
t +1(cid:88).
t=0.
t(cid:88).
t=1.
where || Â· ||2number of the transformer layers, ot.
2 indicates the squared l2 norm, t is thei are.
i and Ë†ot.
2721algorithm 1 training procedure of quantidce.
training datasets dpt and df t, metric.
input:model moutput: student model Ë†m1: initialize m with bertbase2: for all (ci, ri) âˆˆ dpt dosi = m (ci, ri)3:compute the centroids ei for sicompute (cid:96)sepcompute (cid:96)comcompute lmlrupdate m to minimize lmlr.
and (cid:96)ordbetween ei and si.
for ei.
4:.
5:.
6:.
7:.
i.i.i.
8:9: end for10: initialize Ë†m with m11: for all (ci, ri, si) âˆˆ df t do12:.
13:.
14:.
15:.
16:.
oi, ai = m (ci, ri)Ë†si, Ë†oi, Ë†ai = Ë†m (ci, ri)compute (cid:96)mseicompute (cid:96)kdcompute lkd mseupdate Ë†m to minimize lkd mse.
between si and Ë†sii between oi, ai and Ë†oi, Ë†ai.
17:18: end for19: return student model Ë†m.
the tth layer outputs of m and Ë†m respectively, atiand Ë†ati are the attention matrices of the tth trans-former layer.
note that the layer 0 and the layert+1 refer to the embedding layer and the predictionlayer respectively..overall, the loss function for kd ï¬ne-tuning,named as kd-mse loss, is the weighted sum of(cid:96)msei across the whole training dataset df t:i.and (cid:96)kd.
lkd mse =.
(Î± âˆ— (cid:96)mse.
i + Î² âˆ— (cid:96)kd.
i ),.
(8).
1n2.
n2(cid:88).
i=1.
where Î± and Î² are hyperparameters, and we empir-ically found that Î± = 1 and Î² = 5 performs well..the overall training procedure is summarized in.
algorithm 1..4 experiments.
4.1 experimental setup.
baseline metrics.
we compare the metric modeltrained by our quantidce with eight popu-lar automatic dialogue metrics, including threelexical word-overlap metrics: bleu (papineniet al., 2002), rouge (lin, 2004) and me-teor (banerjee and lavie, 2005), one seman-tic word-overlap metric, bertscore (zhang et al.,.
2020), and four learnable metrics: adem (loweet al., 2017), bert-ruber (ghazarian et al.,2019), bleurt (sellam et al., 2020) andgrade (huang et al., 2020)..evaluation.
our quantidce and the baselinesare evaluated by computing the correlations be-tween the model-predicted scores and the human-rated scores.
speciï¬cally, we adopt pearson, spear-man and kendall as the correlation measures and alarge-scale human judgement benchmark (huanget al., 2020) to provide the human-rated scores.
this benchmark contains 1,200 unique (context, re-sponse, human-rated score) triplets for metric eval-uation where the contexts were randomly selectedfrom the test set of three chit-chat datasets includ-ing dailydialog (li et al., 2017), convai2 (dinanet al., 2019) and empatheticdialogues (rashkinet al., 2019), and the responses were produced byboth the retrieval-based dialogue models and thegeneration-based ones to assure response diversity..training datasets.
we use two datasets, daily-dialog++4 and dailydialogeval5, to support thepre-training and ï¬ne-tuning of quantidce, respec-tively.
the dailydialog++ dataset (sai et al., 2020)contains over 11k conversations, which augmentsthe original dailydialog dataset with multiple re-sponses of different quality levels including ï¬vegolden reference responses, ï¬ve adversarial irrele-vant responses and ï¬ve random selected responsesfor each context.
therefore, in this work, we setthe number of coherence levels l = 3 where thepairs containing the random responses, the adver-sarial responses and the reference responses respec-tively belong to the levels from 1 to 3. as to theï¬ne-tuning data, we use the dailydialog humanjudgement dataset, denoted as dailydialogeval,which is a subset of the adopted evaluation bench-mark (huang et al., 2020), with 300 human ratingdata in total, and randomly split the data into train-ing (90%) and validation (10%) sets..implementation details.
we use bertbaseto initialize the encoder network, which is in linewith the current sota metric, grade.
for themlr pre-training, we pretrain our model for 5epochs with batch size 3 and learning rate 2e-5where the lower bound for the separation loss Î» =0.3 and the upper bound for the compactness loss.
4https://github.com/iitmnlp/dialogue-evaluation-with-bert.
5https://github.com/li3cmz/grade.
2722metric.
pearson.
spearman kendall average.
loss.
pearson spearman kendall average.
bleurougemeteorbertscoreadembert-ruberbleurtgradequantidce.
convai2.
0.003 *0.1360.1450.2250.026 *0.2660.1520.4960.554empatheticdialogues.
0.1280.1400.1810.2250.037 *0.2660.1490.5030.554.
0.0880.0970.1230.1540.049 *0.1850.1030.3560.395.
-0.051 *bleu0.029 *rouge0.118meteor0.046 *bertscoreadem0.007 *bert-ruber -0.022 *0.203bleurt0.350grade0.412quantidce.
0.002 *-0.013 *0.055 *0.033 *0.009 *-0.040 *0.1920.3440.393.
0.005 *-0.010 *0.04 *0.021 *0.040 *-0.029 *0.130.2430.274.
0.0730.1240.150.2010.0370.2390.1350.4520.501.
-0.0150.0020.0710.0330.019-0.0300.1750.3120.360.table 1: correlations between automatic evaluationmetrics and human judgements on two datasets (con-vai2 and empatheticdialogues).
the star * indicatesresults with p-value > 0.05, which are not statistically signiï¬-cant..Âµ = 0.1. for the kd ï¬ne-tuning, we further ï¬ne-tune the pretrained model for 20 epochs with batchsize 10 and learning rate 5e-6.
for all the training,bertadam is used as the optimizer with Î²1 = 0.9and Î²2 = 0.999. for the transformer-layer distilla-tion, we distill all the transformer layers since themodel architectures of the teacher and the studentare exactly the same..4.2 experimental results.
metric performance.
the correlation results ofquantidce and the other baseline metrics on thelarge-scale human judgement benchmark are pre-sented in table 1, including the convai2 and theempatheticdialogues datasets.6 for a fair com-parison, the learnable baseline metrics, adem,bert-ruber and grade, are trained on thetraining dataset we adopted, i.e., dailydialog++.7generally, quantidce achieves an absolute aver-aged correlation improvement by around 5% pointsover the current sota, grade.
besides, all theresults of quantidce are statistically signiï¬cantwith p-value <0.01..6the dailydialogeval dataset was not used for evalua-.
tion since we used it for ï¬ne-tuning..7bleurt was not trained on dailydialog++ since thisdataset is not suitable for the bleurt pre-training strategy.
instead, we trained bleurt with the ï¬ne-tuning data we used.
the training details of these baseline metrics are provided inappendix a..bcerankingsupconfatvanilla mlrmlr (ours).
bcerankingsupconfatvanilla mlrmlr (ours).
convai20.5050.5040.5230.5210.5360.554.
0.5050.5070.4950.5160.5220.554empatheticdialogues0.3540.3990.3320.3810.4030.412.
0.3530.3890.3150.3580.3870.393.
0.3610.3600.3670.3710.3790.395.
0.2430.2720.220.2450.2670.274.
0.4570.4570.4620.4690.4790.501.
0.3170.3530.2890.3280.3520.360.table 2: correlations between human judgements andthe metric models trained with different losses duringpre-training and the same kd-mse loss during ï¬ne-tuning.
ranking represents the margin ranking loss..pre-training objective.
to verify the superior-ity of our pre-training objective, namely the mlrloss, we investigated the performance of several ex-isting loss functions for pre-training compared withours.
speciï¬cally, two categories of loss functionsused for metric training are adopted, including (a)the two-level setting and (b) the multi-level setting.
the binary cross entropy (bce) loss and the marginranking loss are adopted for the two-level setting,while another three loss functions are adopted forthe multi-level setting, including the supervisedcontrastive (supcon) loss (khosla et al., 2020), thefast-approximated triplet (fat) loss (yuan et al.,2019) and the vanilla mlr loss (lin et al., 2020) 8.as shown in table 2, the performance of our mlrloss is the best among all the pre-training objec-tives.
and we also found that the multi-level set-ting losses perform better than the two-level ones,especially on the convai2 dataset.
moreover, inorder to more intuitively analyze the performancesof these pre-training objectives, we also visualizethe encoded features and the predicted scores ofthe model m after being pretrained by the aboveloss functions on the dailydialog++ dataset with-out ï¬ne-tuning.9 as shown in figure 3, (a) thebce loss cannot separate the level-1 scores fromthe level-2 ones and the corresponding features arealso mixed; (b) the fat loss, on the other hand,separates the features of different levels well, butdoes not consider the relative gaps where the dis-tances between the level-1 and level-3 features are.
8the details of these pre-training loss fucntions are pro-.
vided in appendix b..9the visualization results of the ranking loss, supcon loss.
and vanilla mlr loss are provided in appendix c..2723figure 3: visualizations of features (the scatter plots in the upper row) and scores (the violin plots in the lower row)on the dailydialog++ dataset.
the features and scores in each of the three columns are obtained from the metricmodel m only pretrained with the bce loss, the fat loss and our mlr loss, respectively..pearson spearman kendall averageconvai2 (best epoch).
loss.
msemse (ï¬x encoder)kd-mse (ours).
msemse (ï¬x encoder)kd-mse (ours).
msemse (ï¬x encoder)kd-mse (ours).
0.2720.4770.554.
0.2780.3840.412.
0.9340.3790.804.empatheticdialogues (best epoch).
dailydialogeval (last epoch).
0.3690.4770.554.
0.2760.3670.393.
0.9450.4020.832.
0.2550.3370.395.
0.1870.2530.274.
0.8670.2810.678.
0.2990.4300.501.
0.2470.3350.360.
0.9150.3540.771.table 3: correlations between human judgements andthe metric model m further trained with different ï¬ne-tuning losses after mlr pre-training..not larger than those between level-1 and level-2;(c) in contrast, our mlr loss separates both thefeatures and the scores well and also considers therelative gaps between different levels..fine-tuning objective.
furthermore, we alsoveriï¬ed the effectiveness of our kd-mse loss dur-ing ï¬ne-tuning by comparing with other ï¬ne-tuninglosses, including the pure mse loss without kdregularization as shown in equation 6 and the samemse loss except for freezing the encoder networkand only ï¬netuning the predictor network i.e.
themlp, denoted as mse (ï¬x encoder).
as the resultsshown in table 3, compared with the other twolosses, the model ï¬netuned by our kd-mse losshas the highest correlation results on both convai2and empatheticdialogues.
moreover, by compar-.
figure 4: score visualization on the dailydialog++dataset where the scores are predicted by our quan-tidce after kd ï¬ne-tuning..ing the results of mse and kd-mse, we can ï¬ndthat introducing kd regularization leads to obvi-ous averaged correlation improvements by 20.2%points on convai2 and 11.3% points on empa-theticdialogues, which veriï¬es the effectivenessof the kd loss.
besides, we also reported the last-epoch correlation results on the training dataset,dailydialogeval.
and the results of mse andmse (ï¬x encoder) indicate the phenomena of over-ï¬tting and under-ï¬tting into dailydialogeval re-spectively, which explain the reasons of their lowperformance on the two evaluation datasets.
incontrast, our kd-mse loss enables the model tolearn the actual human rating standards from thescarce annotated data and avoid overï¬tting it si-.
2724(c)mlr(ours)(a)bce(b)fat0.10.20.30.40.50.60.70.80.9score321levelmetricquantidcew/o mlr pre-training.
w/o separation lossw/o compactness lossw/o ordering loss.
w/o kd ï¬ne-tuning.
pearson spearman kendall average0.5540.3730.3880.526-0.4940.531.
0.5010.3250.3640.489-0.4620.484.
0.5540.3570.4160.550-0.5220.540.
0.3950.2460.2890.390-0.3710.381.table 4: ablation studies on the convai2 dataset by re-moving one of the component in quantidce, includingthe mlr loss (w/o mlr pre-training), the kd+mseloss (w/o kd ï¬ne-tuning), and three secondary lossesof the mlr loss..u1: i need to book a plane ticket to london.
u2: round-trip or one-way?
r: round trip or one way trip?
coherence score (human / quantidce / grade) : 2.10 / 2.85 / 4.52u1: yum.
you will ï¬nd me in the kitchen and if not i am ï¬shing.
u2: wow thatâ€™s pretty cool what else you do for fun?
r: probably ï¬sh it is great.
coherence score (human / quantidce / grade) : 2.50 / 3.94 / 4.27.table 5: two representative examples to show thestrength and weakness of our quantidce where u1and u2 are two utterances of the context and r is thecorresponding response..number of data for fine-tuning.
moreover,we also investigated how the scale of data for ï¬ne-tuning effects the model performance by increas-ing the number of ï¬ne-tuning data 5% each timefrom zero.
the trend of the model performance ispresented in figure 5. we observed that minimiz-ing our kd-mse loss made the correlation resultshave a gradually increasing trend after an initial de-crease.10 more speciï¬cally, the result achieved thestandard before ï¬ne-tuning at around the 70% datascale and continued increasing until 100% with aï¬nal improvement by around 2% points.
for com-parison, the performance trends of mse and mse(ï¬x encoder) are also provided.
and the resultspresent overall decreasing trends of the model per-formance, which indicates that the model trainedby mse or mse (ï¬x encoder) cannot beneï¬t fromthe increasing of data scale, due to the severe over-ï¬tting or under-ï¬tting.
therefore, to effectivelyutilize the limited data, it is important to enablethe update of the entire network and add some con-straints to avoid over-ï¬tting, such as our proposedkd regularization..4.4 case study.
to illustrate the performance of quantidce, tworepresentative examples are shown in table 5 .
theï¬rst example shows the strength of quantidcewhere the coherence score given by ours is closerto the human rating score compared with the ex-tremely high score given by grade.
however,in the second example, both our quantidce andgrade deviate from the human score, possiblybecause the number of coherence levels we adoptedin this work (l = 3) is insufï¬cient as humans usu-ally consider more levels of dialogue coherence..10the initial decrease probably attributes to the randomnessof data sampling where the smaller the sampling ratio is, thehigher the probability that noisy samples dominate the sam-pled data will be.
and overï¬tting into the noisy samples leadsto the performance decrease..figure 5: the performance trends when changing thenumber of annotated data used for different ï¬ne-tuningobjectives.
each point in the line chart indicates the av-eraged correlation of pearson, spearman and kendall..multaneously.
finally, in figure 4, we present thevisualization of the scores predicted by our quan-tidce after kd ï¬ne-tuning.
compared with thescore distributions before ï¬ne-tuning in figure 3(c),the ï¬netuned score distributions of the level-1 andlevel-3 are wider and partly overlap with the level-2 distribution.
it is predictable as the judgementsof coherence are always subjective and humanstend to give vague and middle scores instead ofextremely high or low scores..4.3 ablation studies.
component analysis.
to verify the contribu-tions of the core components in our quantidce, wefurther conducted ablation studies on the convai2dataset.
as shown in table 4, both the mlr pre-training and kd ï¬ne-tuning contribute to the betterperformance of quantidce.
besides, we also con-ducted ablations by removing one of the secondaryloss during mlr pre-training, including the sepa-ration loss, the compactness loss and the orderingloss.
the results show that the performance ben-eï¬ts from all these losses in which the separationloss and the ordering loss are crucial for training ametric with strong and positive human correlations..27250102030405060708090100data scale (%)0.200.250.300.350.400.450.50correlationmsemse (fix encoder)kd-mse (ours)5 conclusion.
in this paper, we propose quantidce, a novel train-ing framework aiming to bridge the gap betweenthe training objective and the actual human ratingand train a quantiï¬able dialogue coherence met-ric.
in general, quantidce includes two trainingstages, mlr pre-training for learning the coarsehuman judgements of dialogue coherence degrees,and kd ï¬ne-tuning for learning the actual humanrating standards.
experimental results show thatthe metric trained by quantidce presents strongcorrelations with human judgements.
for futurework, it is interesting to investigate a more efï¬cientway to obtain multi-level data and extend the multi-level setting into the general evaluation for naturallanguage generation..acknowledgments.
science.
we thank all anonymous reviewers for theirconstructive comments.
this work was supportedin part by national key r&d program of2020aaa0109700,china under grant no.
offoundationnational naturalchina(nsfc) under grant no.u19a2073and no.61976233, guangdong province ba-sic and applied basic research (regionaljoint fund-key) grant no.2019b1515120039,shenzhenprogramfundamental research(project no.
rcyx20200714114642083, no.
jcyj20190807154211365), zhijiang labâ€™s openfund (no.
2020aa3ab14) and csig youngfellow support fund..references.
satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65â€“72, ann ar-bor, michigan.
association for computational lin-guistics..alessandra cervone, evgeny stepanov, and giuseppericcardi.
2018. coherence models for dialogue.
in proceedings of the 19th annual conference ofthe international speech communcation associa-tion, pages 1011â€“1015..djork-arnÂ´e clevert, thomas unterthiner, and sepphochreiter.
2016. fast and accurate deep networklearning by exponential linear units (elus).
in inter-national conference on learning representations..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171â€“4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, varvara logacheva, valentin malykh,alexander h. miller, kurt shuster, jack urbanek,douwe kiela, arthur szlam, iulian serban, ryanlowe, shrimai prabhumoye, alan w. black, alexan-der i. rudnicky, jason williams, joelle pineau,mikhail burtsev, and jason weston.
2019. the sec-ond conversational intelligence challenge (convai2).
computing research repository, arxiv:1902.00098.version 1..sarik ghazarian, johnny wei, aram galstyan, andnanyun peng.
2019. better automatic evaluation ofopen-domain dialogue systems with contextualizedin proceedings of the workshop onembeddings.
methods for optimizing and evaluating neural lan-guage generation, pages 82â€“89, minneapolis, min-nesota.
association for computational linguistics..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
computing research repository, arxiv:1503.02531.version 1..lishan huang, zheng ye, jinghui qin, liang lin, andxiaodan liang.
2020. grade: automatic graph-enhanced coherence metric for evaluating open-in proceedings of thedomain dialogue systems.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 9230â€“9240,online.
association for computational linguistics..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in findings of the associationfor computational linguistics: emnlp 2020, pages4163â€“4174, online.
association for computationallinguistics..prannay khosla, piotr teterwak, chen wang, aaronsarna, yonglong tian, phillip isola, aaronmaschinot, ce liu, and dilip krishnan.
2020. su-pervised contrastive learning.
in advances in neu-ral information processing systems 33: annual con-ference on neural information processing systems2020, neurips 2020, december 6-12, 2020, virtual..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 1317â€“1327, austin,texas.
association for computational linguistics..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-.
2726ally labelled multi-turn dialogue dataset.
in proceed-ings of the eighth international joint conference onnatural language processing (volume 1: long pa-pers), pages 986â€“995, taipei, taiwan.
asian federa-tion of natural language processing..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74â€“81, barcelona, spain.
association for computational linguistics..zibo lin, deng cai, yan wang, xiaojiang liu, haitaozheng, and shuming shi.
2020. the world is notbinary: learning to rank with grayscale data for di-in proceedings of thealogue response selection.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 9220â€“9229,online.
association for computational linguistics..chia-wei liu, ryan lowe, iulian serban, mike nose-worthy, laurent charlin, and joelle pineau.
2016.how not to evaluate your dialogue system: anempirical study of unsupervised evaluation metricsfor dialogue response generation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2122â€“2132, austin,texas.
association for computational linguistics..ryan lowe, michael noseworthy, iulian vlad ser-ban, nicolas angelard-gontier, yoshua bengio, andjoelle pineau.
2017. towards an automatic tur-ing test: learning to evaluate dialogue responses.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1116â€“1126, vancouver,canada.
association for computational linguistics..jekaterina novikova, ondË‡rej duË‡sek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedingsnew evaluation metrics for nlg.
of the 2017 conference on empirical methods innatural language processing, pages 2241â€“2252,copenhagen, denmark.
association for computa-tional linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311â€“318, philadelphia,pennsylvania, usa.
association for computationallinguistics..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark andin proceedings of the 57th annual meet-dataset.
ing of the association for computational linguis-tics, pages 5370â€“5381, florence, italy.
associationfor computational linguistics..ananya b. sai, akash kumar mohankumar, sid-dhartha arora, and mitesh m. khapra.
2020.im-proving dialog evaluation with a multi-referenceadversarial dataset and large scale pretraining..computing research repository, arxiv:2009.11321.version 1..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881â€“7892, online.
association for computa-tional linguistics..raphaÂ¨el sourty, jose g. moreno, francÂ¸ois-paul ser-vant, and lynda tamine-lechani.
2020. knowledgebase embedding by cooperative knowledge distilla-tion.
in proceedings of the 28th international con-ference on computational linguistics, pages 5579â€“5590, barcelona, spain (online).
international com-mittee on computational linguistics..jingyuan sun, shaonan wang, jiajun zhang, andchengqing zong.
2020. distill and replay for con-tinual language learning.
in proceedings of the 28thinternational conference on computational linguis-tics, pages 3569â€“3579, barcelona, spain (online).
international committee on computational linguis-tics..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in aaai..hao-ran wei, shujian huang, ran wang, xin-yu dai,and jiajun chen.
2019. online distilling from check-in proceed-points for neural machine translation.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 1932â€“1941, min-neapolis, minnesota.
association for computationallinguistics..ye yuan, wuyang chen, yang yang, and zhangyanglossin defense ofwang.
2019.again: learning robust person re-identiï¬cation withfast approximated triplet loss and label distillation.
computing research repository, arxiv:1912.07863.version 2..the triplet.
tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..a training details of the learnable.
baseline metrics.
a) following sai et al.
(2020), we trained ademby ï¬rst initializing it with the ofï¬cial checkpointand further ï¬netuning on dailydialog++ with atarget of 5 for level-3 pairs and 1 for level-1 pairs;b) bert-ruber and grade were both trainedon dailydialog++ where level-3 pairs as positive.
2727samples and both level-1 and level-2 pairs as neg-ative samples, except that the former use cross-entropy loss while the latter use ranking loss; c)bleurt was initialized with the ofï¬cial recom-mended checkpoint bleurt-base and ï¬netunedon dailydilaogeval by following the ofï¬ce guide-lines11..vanilla mlr loss.
the vanilla mlr loss (linet al., 2020) is the extension of the margin rankingloss to a multi-level version by repeatedly applyingthe original margin ranking loss between differentlevels, which can be directly applied to our evalua-tion task..c visualizations of the pre-training.
b details of the pre-training losses.
losses.
we have already compared the visualization resultsof the bce loss and the fat loss.
for a supple-ment, here we mainly introduce the visualizationsof the margin ranking loss, the supcon loss andthe vanilla mlr loss in detail..as we can see in figure 6, (a) the margin rank-ing loss cannot separate the level-1 scores fromthe level-2 ones and the corresponding features arealso mixed, which is similar to the bce loss; (b)the supcon loss, on the other hand, can distinguishthe features and scores of the three levels to someextent, and the scores of different levels are alsoseparated but do not follow the real rank order, i.e.,level-1 < level-2 < level-3; (c) the ï¬nal vanillamlr loss can separate the context-response pairswith different coherence level in feature space andthe predicted scores also follow the actual rankorder.
however, its score distributions are not com-pact enough for the level-1 and level-3..bce loss.
the binary cross entropy (bce) lossis adopted for the experiments of the two-levelsetting, where both the adversarial irrelevant re-sponses and random selected responses of the dai-lydialog++ dataset (sai et al., 2020) are treated asnegative samples and labeled as 0, while the goldenreference responses are treated as positive samplesand labeled as 1..margin ranking loss.
similarly, the marginranking loss simpliï¬es the evaluation task as atwo-level setting and maximizes the differencesbetween the positive coherent dialogues and thenegative incoherent ones.
as the name suggests,the focus of the margin ranking loss is ranking,which aims at ranking the scores of positive co-herent dialogues ahead of the negative incoherentones..supcon loss.
the supervised contrastive (sup-con) loss (khosla et al., 2020), which pulls the pos-itive anchors closer and pushes the negatives fartheraway in representation space, can be adopted forthe multi-level setting.
here, for our multi-levelsetting, we consider the dialogues of level-1, level-2, and level-3 as positive anchors successively, andthe remaining two levels as corresponding nega-tives..fat loss.
the fast-approximated triplet (fat)loss (yuan et al., 2019) replaces the traditionalpoint-to-point distances of the triplet loss withpoint-to-cluster distances, through an upper boundrelaxation of the triplet form, which is ï¬rst appliedfor the classiï¬cation task and obviously reducesthe computation cost.
to use fat loss in our eval-uation task, we consider the different coherencelevels as different classes and perform the fat lossto separate the context-response pairs with differentcoherence levels..11https://github.com/google-research/.
bleurt.
2728figure 6: visualizations of features (the scatter plots in the upper row) and scores (the violin plots in the lower row)on the dailydialog++ dataset.
the features and scores in each of the three columns are obtained from the metricmodel m only pretrained with the margin ranking loss, the supcon loss and the vanilla mlr loss, respectively..2729(c)vanillamlr(b)supcon(a)ranking