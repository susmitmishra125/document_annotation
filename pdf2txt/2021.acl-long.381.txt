warp: word-level adversarial reprogramming.
karen hambardzumyan1, hrant khachatrian1,2, jonathan may31yerevann, 2yerevan state university,3information sciences institute, university of southern californiamahnerak@yerevann.com, hrant@yerevann.com, jonmay@isi.edu.
abstract.
transfer learning from pretrained languagemodels recently became the dominant ap-proach for solving many nlp tasks.
a com-mon approach to transfer learning for multipletasks that maximize parameter sharing trainsone or more task-speciﬁc layers on top of thelanguage model.
in this paper, we present analternative approach based on adversarial re-programming, which extends earlier work onautomatic prompt generation.
adversarial re-programming attempts to learn task-speciﬁcword embeddings that, when concatenated tothe input text, instruct the language model tosolve the speciﬁed task.
using up to 25ktrainable parameters per task, this approachoutperforms all existing methods with up to25m trainable parameters on the public leader-board of the glue benchmark.
our method,initialized with task-speciﬁc human-readableprompts, also works in a few-shot setting, out-performing gpt-3 on two superglue taskswith just 32 training samples..1.introduction.
language model pretraining has had a tremendousimpact on solving many natural language process-ing tasks (peters et al., 2018; radford et al., 2018;devlin et al., 2019; liu et al., 2019).
the mostpopular two approaches take a pretrained modeland use a straightforward supervised learning ob-jective.
in the ﬁrst approach, the parameters ofthe language model are frozen and a task-speciﬁchead is trained on top of them (peters et al., 2018).
the second approach ﬁne-tunes all model param-eters (radford et al., 2018).
the latter can some-times yield better results (peters et al., 2019),while the ﬁrst one usually offers better stability forsmaller datasets.
the approach based on frozenfeatures does not require storing task-speciﬁc lan-guage models..a recent alternative is based on so calledadapters (houlsby et al., 2019; pfeiffer et al.,2021), a technique that adds new weights at everylayer of the pretrained language model while theoriginal parameters are kept frozen.
this enablesa smaller set of task-speciﬁc parameters whileachieving results comparable to the ﬁne-tuning ap-proach..another approach of leveraging pretrained lan-guage models for downstream tasks, introducedby radford et al.
(2019), provides “task descrip-tions” without using any labeled examples.
gpt-3 (brown et al., 2020) demonstrates impressivefew-shot learning performance with priming: byproviding the language model a few inputs andoutputs (“analogies”) as a context.
the languagemodel contextually “learns” from these examplesand outputs the answer with a single forward passwithout any trainable parameters.
these methods,however, require huge language models (1.5b and175b parameters, respectively)..the success of task reformulation-based ap-proaches suggest that language models are capa-ble of solving various natural language processingtasks given a well-crafted prompt.
we hypothesizethat it is possible to ﬁnd such prompts.
in otherwords, we can discover extra tokens that, whenadded to the input, can exploit language model ca-pabilities better than the manually-designed ones.
in this paper, we introduce a novel technique toﬁnd optimal prompts.
we call our method warp:word-level adversarial reprograming1.
themethod is inspired by adversarial reprogramming(elsayed et al., 2019) — a method of adding ad-versarial perturbations to an input image that re-programs a pretrained neural network to performclassiﬁcation on a task other than the one it wasoriginally trained for..1our implementation is publicly available at: https:.
//github.com/yerevann/warp.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4921–4933august1–6,2021.©2021associationforcomputationallinguistics4921figure 1: an example of an adversarial program thatcauses inception v3 imagenet model to function as anmnist classiﬁer, from elsayed et al.
(2019).
figure 2: warp adds a few trainable embeddingsaround the input, which causes the masked languagemodel to predict the sentiment of the sentence..we show that our method, using up to 25ktrainable parameters per task, achieves 81.6 testscore on the glue leaderboard, outperformingall the other submissions that use up to three or-ders of magnitude more trainable parameters.
weshow that it is possible to inject knowledge intowarp models using manually designed initializa-tion of the prompt, which is especially useful ontasks with a small number of examples.
more-over, warp shows impressive few-shot perfor-mance on two tasks from the superglue bench-mark with just 32 examples, outperforming gpt-3results.
finally, we discuss the advantages of ourmethod in real-life applications..2 related work.
2.1 towards fewer trainable parameters.
jiao et al.
(2020) show that knowledge distillationmay help reduce the size of their model 7.5 timeswhile almost preserving the performance, but ﬁne-tuning such models still requires storage of sepa-rate task-speciﬁc models.
as seen in section 6,this approach does not scale when we want to ap-ply it to many tasks at once..another approach, called adapters (houlsbyet al., 2019; pfeiffer et al., 2021), introduces newtask-speciﬁc parameters that are added at everylayer of the transformer network.
only thesenewly initialized weights are trained, which allowsseparation of general and task-speciﬁc knowl-edge.
in contrast, our method does not inject task-speciﬁc knowledge inside the body of the pre-trained language model.
instead, it focuses onlearning task-speciﬁc input-level prompts..2.2 task reformulation.
in gpt-2, radford et al.
(2019) introduce a com-pletely unsupervised way for transferring knowl-edge to downstream tasks by reformulating vari-ous natural language understanding tasks into lan-guage modeling problems.
this approach doesnot make use of the available training examples.
brown et al.
(2020) demonstrate an effective few-shot transfer by reformulating downstream tasksinto input-output analogies in the context withouta need for further ﬁne-tuning.
nonetheless, thenumber of training examples is limited to the con-text size and is not scalable to a traditional super-vised learning scenario..schick and sch¨utze (2021b) show the effec-tiveness of reformulating a number of tasks intocloze-style tasks by ﬁne-tuning masked languagemodels (devlin et al., 2019).
the method,called pattern exploited training (pet), addition-ally uses training samples and performs few-shotlearning even without huge models such as gpt-3.
our method is also based on masked lan-guage models, but unlike pet, we focus on ﬁnd-ing the best prompt using the training examples.
this eliminates the need for manually-designedprompts, however, our method can also beneﬁtfrom similar prior knowledge about the task bycareful initialization of the prompts..2.3 adversarial reprogramming.
adversarial reprogramming (elsayed et al., 2019)demonstrates the reprogramming of pretrained im-agenet classiﬁers by adding input-level adversar-ial perturbations to make them perform well onmnist and cifar-10 image classiﬁcation tasks.
the adversarial perturbation is designed to be im-age padding added to the original input, as illus-.
4922a_lovely_film_...a_pretentious_mess_...two_hours_of_junk.exceeds_expectations.
[mask][mask][mask][mask]13%79%90%12%87%21%10%88%negativepositivefigure 3: illustration of warp.
the prompt tokens [p 1], [p 2], ..., [p n] are inserted before, between, andafter the sentences.
only the prompt and class embeddings are trainable (colored in green).
the masked languagemodeling head is applied without the decoder; instead, the matrix of [v 1], [v 2], ..., [v n] is applied as alinear layer.
finally, a regular task-speciﬁc loss is computed on the resulting logits..trated in figure 1. then the perturbation param-eter is trained to optimize the target classiﬁcationtask objective using the annotated image data..while in the case of image classiﬁcation it is notobvious why adversarial reprogramming shouldever work, e.g.
why a network trained on ima-genet should have the capacity to solve mnistwhen surrounded with a particular bitmap, fornlp tasks, there is more intuition.
many nlptasks can be reformulated as language models, ashared space for both program and data..adversarial reprogramming has been adapted totext classiﬁcation tasks with lstm networks in(neekhara et al., 2019).
they operate in the vo-cabulary space and reprogram a model trained forone task to perform another task.
more recently,autoprompt (shin et al., 2020a) attempts to ﬁndprompts for large language models automaticallywithout adding any parameters to the model.
un-like autoprompt, we perform gradient-based opti-mization in the space of word embeddings whichgives our model more degrees of freedom andeventually better performance on the downstreamtasks (section 6.2)..in a more general sense, guiding an nlp modelwith special tokens appended to the input is aneven older idea.
in particular, multilingual neu-ral machine translation models use special tokensin the input to control the target language (haet al., 2016; johnson et al., 2017) or politeness.
of the translation (sennrich et al., 2016).
anothermethod to reprogram a bert-based model is pro-posed by artetxe et al.
(2020), where a modeltuned on an english version of a particular taskis transformed to work in another language bychanging only the embedding matrices..in parallel work, li and liang (2021) proposea similar method and successfully apply it ontwo text generation tasks.
apart from the dif-ferent types of tasks and our characterization ofthe task as a form of adversarial reprogramming,the main difference between their approach andours is that they use an additional parameteriza-tion trick to stabilize the training..3 warp.
we follow a setup similar to elsayed et al.
(2019)with some nlp-speciﬁc modiﬁcations depicted infigure 2..our goal is to ﬁnd the best prompt that willmake a pretrained masked language model pre-dict the desired answer (verbalizer token) for atraining example’s masked token2.
we searchfor such prompts in the (continuous) embeddingspace.
in other words, we want to ﬁnd parametersθ = {θp , θv } for prompt and verbalizer embed-.
2this approach can be easily extended to autoregressive.
language modeling..4923transformer / encoder[cls][p_1]_oil_prices_rise[p_2][mask]_oil_prices_fall_back[p_5][sep]mlm headw/o decoder[v_1]entailment[v_2]contradiction[v_3]neutralloss[p_4][p_3]dings, respectively, such that:.
3.2.implementation details.
θ∗ = arg max.
(− log pθ(y|x)).
θ.and the probabilities are given by:.
pθ(y|x) =.
exp θv.
y f (tθp (x)).
exp θv.
i f (tθp (x)).
(cid:80)i∈c.
where tθp (x) is the template that inserts theprompt embeddings θp into predeﬁned positions,c is the set of classes, and f (x) is the maskedlanguage model output (without the last decoderlayer, which is simply the transposed word embed-ding matrix).
both θp and θv are vectors in thesame embeddings space as the word embeddings.
in figure 2, the template tθp (x) prepends θp1and appends θp3 , θp4 parameters to the wordembeddings and uses θv+ and θv− to calculate theprobabilities on the masked token position for pos-itive and negative classes..2 , θp.
3.1 method.
similar to elsayed et al.
(2019), we employstochastic gradient descent to ﬁnd the best adver-sarial perturbation on the text that will minimizethe task objective.
first, we insert special prompttokens [p 1], [p 2], ... [p k] and an additional[mask] token into the input sequence.
these to-kens might be placed before or after the sentences,depending on the prompt template..we set the optimization objective to a cross-entropy loss between the head output of themasked language model and the verbalizer tokens[v 1], [v 2], ..., [v c] for classes 1...c ac-cordingly..the only trainable parameters are the word em-beddings for [p 1], ..., [p k] and [v 1], ...[v c].
in case we want to train models for mul-tiple tasks, these are the only task-speciﬁc param-eters we need to store.
the entire “body” of thelarge language model (all attention layers, feed-forward layers, and all other word embeddings)remains untouched..note that, unlike most adversarial attacks, wedo not update the embeddings of the original to-kens of the input.
this follows the intuition fromelsayed et al.
(2019), when the pixels of mnistor cifar images are left untouched, and onlypadding pixels are updated..we train these parameters by minimizing the.
loss on the training set of the downstream task..warp is implemented in the allennlp frame-work.
for all the glue benchmark tasks weuse the roberta-large (liu et al., 2019)model from the pytorch implementation ofhuggingface transformers (wolf et al.,2020) library.
for the few-shot experiments, weuse albert-xxlarge-v2 in order to directlycompare to ipet (schick and sch¨utze, 2021b).
for the glue and superglue tasks we usedataset loaders and metrics implementations fromthe huggingface datasets library..the prompt tokens are initialized either withword embeddings of [mask] or similar to thevectors from the word embedding layer.
for theanswer prompts, we use the masked languagemodel head, which usually consists of a feed-forward network and a decoder on top of it, wherethe weights of the decoder are shared with theword embeddings used for the input.
we calcu-late the softmax over the verbalizer tokens [v 1],... [v c]..we choose the adam optimizer with a slantedtriangular schedule for the learning rate with 6%warm-up steps and train for 10-20 epochs on eachtask.
each batch consists of examples containingat most 1024 tokens and 8 examples..in order to speed up the training, we disable thedropout of the pretrained language model.
all theexperiments are performed on two titan vs andtwo rtx 3080 gpus, with mixed precision train-ing.
in practice, warp is 2.5-3 times faster thanregular ﬁne-tuning and 2 times slower than frozen-features experiments in terms of epoch durationwith the same batch sizes..details about the hyperparameters can be found.
in the supplementary material..4 experiments on glue.
following prior work, we evaluate our method onthe glue benchmark (wang et al., 2019b), whichconsists of 9 natural language understanding tasks.
generally, we perform single-task warp training,with early stopping and model selection using theoriginal validation sets, if not stated otherwise..4.1 tasks.
almost all the tasks from the glue benchmarkare either sentence classiﬁcation or sentence pairclassiﬁcation tasks, so warp requires very fewmodiﬁcations to adapt to each of the tasks..4924mnli92.0 / 92.8human baselinesdebert391.9 / 91.690.8 / 90.2roberta86.7 / 85.9bertlarge84.6 / 83.4bertbase84.6 / 83.2tinybert682.5 / 81.8tinybert481.6 / 81.2electrasmall85.4 / 85.0adapters (bert)warp (roberta) 88.0 / 88.2.qnli91.299.295.492.790.590.487.788.392.493.5.qqp59.5 / 80.476.2 / 90.874.3 / 90.272.1 / 89.371.2 / 89.271.6 / 89.171.3 / 89.270.4 / 88.071.5 / 89.468.6 / 87.7.rte sst mrpc93.693.288.270.166.470.066.663.671.684.3.
97.8 86.3 / 80.897.5 94.0 / 92.096.7 92.3 / 89.894.9 89.3 / 85.493.5 88.9 / 84.893.1 87.3 / 82.692.6 86.4 / 81.291.1 89.0 / 84.994.3 88.7 / 84.396.3 88.2 / 83.9.cola66.471.567.860.552.151.144.155.659.253.9.sts-b92.7 / 92.692.9 / 92.692.2 / 91.987.6 / 86.587.1 / 85.885.0 / 83.781.9 / 80.485.6 / 84.687.3 / 86.189.5 / 88.8.avg87.190.888.180.578.378.175.977.480.281.6.
#.
3 · 109355 · 106355 · 106110 · 10667 · 10615 · 10614 · 1061.2 · 106< 25k.
table 1: test set results on glue benchmark.
the results are obtained from the glue evaluation server.
thesubscript next to tinybert corresponds to the number of layers in the model.
warp for rte, sts-b and mrpcare intialized from the mnli parameters.
results for wnli are not shown, although they are counted in theaveraged glue score (avg column).
the last column # shows the number of trainable parameters.
warp’saverage performance is higher than all models with up to three orders of magnitude more trainable parameters.
fully ﬁne-tuned roberta and the current state-of-the-art method (debert) score higher by 6.5 and 9.2 points,respectively..sst-2 (sentence sentiment treebank, socheret al., 2013) is a single sentence binary classiﬁca-tion task.
for the prompt, we put a [mask] tokenafter the sentence, and the trainable prompt tokensare both appended and prepended to the sentence.
cola (corpus of linguistic acceptability,warstadt et al., 2019) is a single sentence classiﬁ-cation task as well, so we treat both the same waywith the only difference that as a validation metricwe use accuracy for sst-2, and matthew’s corre-lation for cola..mnli (multinli, multi-genre natural lan-guage inference, williams et al., 2018), qnli(question natural language inference, rajpurkaret al., 2016) and rte (recognizing textual en-tailment, dagan et al., 2006; bar haim et al.,2006; giampiccolo et al., 2007; bentivogli et al.,2009) are sentence pair classiﬁcation tasks.
sim-ilar to schick and sch¨utze (2021a), we may haveprompt tokens before, after and between the twosentences, but the [mask] token is always put be-tween the sentences.
for mnli, we use matchedaccuracy as a validation metric and use the samemodel for the mismatched version.
in our few-shotattempt for the rte task, we use a different train-ing and evaluation setup discussed in section 5.2.qqp (quora question pairs4) and mrpc (mi-crosoft research paraphrase corpus, dolan andbrockett, 2005) follow the same prompt pattern asnli tasks.
as a validation metric f1 score is used.
sts-b (semantic textual similarity bench-.
4https://www.quora.com/q/quoradata/first-quora-.
dataset-release-question-pairs.
mark, cer et al., 2017), unlike the other tasks inthe benchmark, is formulated as a regression task.
the prompt pattern is the same, but instead of in-troducing new embeddings for [v 1], [v 2],..., [v c] verbalizer tokens, we add a regres-sion head to the last hidden state of mlm headand use mean squares error optimization objec-tive, similar to (liu et al., 2019).
pearson cor-relation is used as the validation metric.
duringinference, we clip the scores within [1, 5]..we follow liu et al..and train models formrpc, sts-b, and rte tasks initialized with theparameters from the best mnli model but do notapply any task-speciﬁc tricks to wnli (winogradschema challenge nli, levesque et al., 2011) andalways predict the majority label..4.2 results.
table 1 presents the results on the test set obtainedfrom the glue evaluation server.
besides ourbest warp models, we also include the humanbaselines, current state-of-the-art model (he et al.,2020), the regular ﬁne-tuned pretrained model weuse, and also include relatively small languagemodels, including (jiao et al., 2020), (clark et al.,2020), (houlsby et al., 2019)..with the glue score, warp outperforms allthe models that train less than 25 million parame-ters on the leaderboard.
we explain the relativelystrong warp results on textual entailment tasksby the easier reformulation of such tasks.
like-wise, we explain the relatively weak performanceon cola by the difﬁculties of reformulating the.
4925train sizefine-tuningadapterslinear classiﬁerwarp0warp1warp2warp4warp8warpinitwarp20warpmnli.
mnli39270290.290.464.270.983.985.486.987.686.888.2.qnli10474394.794.778.178.887.688.092.493.090.493.5.qqp36384692.288.574.977.181.681.583.183.883.684.5.rte249086.683.459.272.272.669.768.272.980.175.886.3.sst6734996.496.388.489.893.894.395.995.496.096.0.mrpc366890.992.982.583.884.785.385.085.686.090.891.2.cola855168.067.448.932.846.154.456.057.451.760.6.sts-b574992.492.571.873.880.480.875.581.086.988.691.0.avg.
#.
88.988.371.072.478.879.980.482.182.784.886.4.
355 · 1063 · 106≤ 3072≤ 3072≤ 4096≤ 5120≤ 7168< 11k< 11k< 25k< 25k.
table 2: dev set results on glue tasks.
the last column shows the number of trainable parameters only.
warpicorresponds to warp training with prompt consisting of i prompt tokens.
warpmnli corresponds to warptraining initialized with the best mnli parameters.
all the models are based on pretrained roberta-large,and for adapters and warp-based approaches require to store 355 · 106 frozen parameters shared across allthe glue tasks.
we show the primary validation metric for each task, described at subsection 4.1. the avgcolumn shows the average of shown metrics and is not comparable to the test server glue score.
the numberof parameters for warp methods may vary because of a difference in the number of classes.
underlined numberscorrespond to our glue submission..task into a cloze task..to further analyze warp, we conduct severalexperiments and focus on dev set results.
in orderto directly compare warp with existing methods,we report in table 2 different methods that useroberta, including ﬁne-tuning, linear classiﬁerson top, autoprompt, and adapters.5 for warpexperiments, we compare performance with dif-ferent numbers of prompt tokens.
the warp0 model does not.
introduce anyprompt parameters.
the only difference betweenwarp0 and linear classiﬁer is that for warp0,[mask] is added to the input of each sample, andwe get sentence representations from the mlmhead at the masked position.
by contrast, in thecase of the linear classiﬁer, we use the average ofnon-special token embeddings as sentence repre-sentations.
as we can see, pooling with mlm issigniﬁcantly better..table 2 shows that, as we decrease the num-ber of trainable prompt parameters, the perfor-mance decreases, but the model still works.
simi-lar behavior was observed by elsayed et al.
(2019)in experiments with different padding parametersizes.
however, in contrast to warp, the num-ber of trainable parameters in that work are muchgreater than the size of the input..an important beneﬁt of using warp is that.
5unlike in table 2, adapters in table 1 are built on.
bert-large-uncased model..it can be initialized with manual prompts.
inaddition to the regular models where we initial-ize with [mask] tokens, we performed a runon the glue datasets with the same prompt[cls] “s1”?
[mask].
“s2”!
[sep]for all the tasks(without s2 for single-sentence tasks).
we denotethese results as warpinit in table 2. warpinitoutperforms warp8 on tasks with relatively fewtraining examples — rte, mrpc and sts-b, which indicates its potential in the low-dataregime..5 few-shot experiments.
the fact that warp can be initialized using man-ually designed natural prompts suggests that wecan similarly beneﬁt from such human attributionsimilar to ipet (schick and sch¨utze, 2021b), es-pecially in scenarios with limited training data..5.1 setup.
for our few-shot experiments we build warpon top of albert (lan et al., 2020),thesame pretrained model used by pet and ipet.
to initialize warp prompts, we use the sameprompt-verbalizer patterns (pvp) from schickand sch¨utze (2021b): the embeddings for [p 1],[p n] are initialized with pvp’s[p 2]...and embeddingspromptfor [v 1], [v 2]...[v c] are initializedwith verbalizer token embeddings for their corre-.
token embeddings,.
4926sponding classes.
unlike roberta-large, thealberta-xxlarge-v2 uses word embeddingsof size 128 (8 times smaller than roberta)..5.2 tasks.
in order to compare with gpt-3, pet, and ipet,we use two tasks from fewglue (schick andsch¨utze, 2021b), which is a few-shot subset of thesuperglue benchmark (wang et al., 2019a) con-sisting of 32 examples for each task.
the datasetalso provides 20000 additional unlabeled exam-ples, however, we do not make use of them andwork in a purely supervised setup..cb: commitmentbank (de marneffe et al.,2019) is a textual entailmenttask which wetreat like the other sentence pair classiﬁcationtasks.
to initialize the prompt we use thetemplate [cls] “h”?
[mask].
“p” [sep] .
we alsoinitialize [v 1], [v 2], [v 3] token embed-dings with yes, no and maybe (respec-tively for entailment, contradiction andneutral)..rte: unlike experiments on the rte task forthe full-sized training in the glue benchmark,we do not initialize the model with vectors frommnli.
instead, the prompt is initialized exactlythe same way as in the cb task.
the only differ-ence is that we have only the two tokens [v 1]and [v 2] initialized with yes and instead(for entailment and not entailment, re-spectively)..5.3 model selection.
although all trainable parameters are manuallyinitialized in this setup, different random seedscan yield different results because of the order thetraining examples appear during an epoch..in the few-shot setup we cannot access the orig-inal validation set.
thus, we disable early stoppingand simply pick the last checkpoint..in order to ﬁnd the best initial learning rate, weconduct 20 runs of warp with the same learn-ing rate each time by randomly choosing 16 train-ing examples and taking the rest for a developmentset.
we repeat this for all candidate learning ratesand choose the one with the best average valida-tion performance across all the random seeds..finally, in order to eliminate the effect of dif-ferent random seeds, we build an ensemble modelfrom 20 warp runs using simple majority vote..model.
gpt-3 smallgpt-3 medgpt-3pet (albert)ipet (albert)warpinit (albert)gpt-3pet (albert)ipet (albert)warpinit (albert).
ved.tset.cbf1 / acc.
26.1 / 42.940.4 / 58.957.2 / 82.159.4 / 85.192.4 / 92.984.0 / 87.552.0 / 75.660.2 / 87.279.9 / 88.870.2 / 82.4.rteacc.
52.348.472.969.874.071.869.067.270.869.1.table 3: results on superglue benchmark.
the re-sults for the test set are obtained from superglueevaluation server.
we only show systems performingin a similar few-shot training setup using 32 examples..5.4 results.
as seen in table 3, warp outperforms pet andgpt-3 baselines, but stays behind ipet on bothtasks.
gpt-3 has 170b parameters, but none ofthem is being trained for the given tasks.
pet andipet have 255m parameters, and all of them aretrained for these tasks.
additionally, they lever-age unlabeled examples using distillation.
warphas roughly the same 255m parameters, but only1024 of them are trained for any single model.
anensemble of 20 warp models has slightly morethan 20k trainable parameters..6 discussion.
6.1.interpreting tokens learned by warp.
warp learns prompt embeddings in a continuousspace.
in this section, we explore those embed-dings by looking at the nearby token vectors.
ta-ble 6 in the supplementary material lists the clos-est tokens (in terms of cosine similarity) to thelearned embeddings.
all glue tasks are initial-ized with [mask] token, except for rte, mrpc,and sts-b, which are initialized from the pre-trained mnli model.
the prompt tokens of thesolutions for those three tasks are quite close tothe ones from the mnli solution.
we have seensimilar behavior on superglue experiments withmanual initializations.
the solution for cola(which is one of the worst-performing tasks) isclose to the initialized point..we do not see any prompt tokens that are mean-ingful in the context of the tasks.
as expected,the verbalized tokens are more interpretable.
for.
4927# of parameters to store.
approachlinear probing m + ecnfull ﬁne-tuning m nsingle layertinybertadapterswarp.
m + n e(e + c)m0nm + n ee(cid:48)m + n e(c + k).
table 4: the number of parameters to be stored to serven text classiﬁcation tasks with at most c classes each,using a pretrained language model with m parameters.
e is the dimension of embeddings (1024 in the case ofroberta).
in tinybert, m0 can be up to 10 timesless than m .
in adapters, e(cid:48) is roughly equal to e,as the number of layers to which adapters are attachedroughly compensates the smaller size of the bottlenecklayer.
in warp, k is the number of prompts (usuallyfewer than 10)..shin et al.
(2020b) include results with a manu-ally designed prompt6 which performs pretty well(shown as a dashed line).
we also compare withthe manually initialized7 version of warp, whichperforms very well with just 100 examples..6.3 real-world applications.
the importance of nlp systems like warp canbe demonstrated by the following application.
suppose we want to build a system that needs toserve n >> 1 classiﬁcation tasks simultaneously.
let the number of classes for each task be boundedby c. the system can be based on a large pre-trained language model with m parameters, usingword embedding size e. how many parametersshould the system store in the device memory tobe able to serve all n tasks?.
if we take the approach with frozen features, wecan reuse m parameters for all tasks and store ad-ditional ecn task-speciﬁc parameters.
this isoptimal in terms of storage but will not performwell.
the other extreme is to ﬁne-tune the wholemodel for each task and store at least m n pa-rameters.
table 4 shows the trade-offs offered bythe other solutions.
methods like tinybert de-crease the number of parameters from m n byonly m .
warp, on the other hand, needs to storeonly m + n e(c + k) parameters, where k isthe number of trainable prompt tokens..6 sent.
this movie wasand “fantastic” as verbalizer tokens.
.
as a prompt, and “terrible”.
7 sent, and ﬁnally, the movie overall was veryas a prompt, and “good” and “bad” as verbalizer tokens.
!.
figure 4: the effect of the training data size for sst-2task (dev set).
horizontal axis is the number of trainingexamples.
solid lines represent median over 10 runs,and the error bars show minimum and maximum per-formance.
all methods use roberta-large model.
the results for autoprompt and ﬁne-tuning are takenfrom (shin et al., 2020b).
..example, the embedding for the “contradiction”class of mnli is close to the token “unless”.
theembeddings for “negative” and “positive” classesof sst-2 task are close to “defective” and “im-portant”, respectively.
other verbalized tokens arenon-interpretable (e.g.
“470” or word pieces withnon-latin characters)..6.2 comparison with autoprompt.
autoprompt (shin et al., 2020b) learns a promptfor the given task in the ﬁnite space of vocabu-lary tokens.
their best version uses 3 or 6 prompttokens and reaches 91.2% accuracy on the devel-opment set of sst-2.
the search space of warpis signiﬁcantly larger, which allows warp to getbetter performance with just a single prompt token(93.8%)..autoprompt does not achieve meaningful re-sults on rte or cb tasks.
warp succeeds onboth without manual initialization.
moreover,with manual initialization, warp gets good per-formance on both tasks even with just 32 examples(table 3)..figure 4 shows the dependence of the accu-racy on sst-2 development set from the numberof training samples.
both warp and autopromptuse 10 prompt tokens.
with a few hundred train-ing samples or fewer, the difference between thetwo algorithms is not signiﬁcant.
warp starts toperform better with more training samples..4928101102103numberoftraininginstances0.30.50.70.9accuracywarp10autopromptfine-tuningwarpinitmanualin practice, warp additionally allows perform-ing inference on inputs for different tasks in paral-lel, using samples of multiple tasks in the samebatch.
every input sentence can be concatenatedwith task-speciﬁc pretrained prompts in advance.
then, the forward pass of the network is identicalfor all tasks.
the ﬁnal task-speciﬁc linear layerscan be concatenated to form a single large linearlayer with at most n c output neurons..this approach can be especially useful in thesystems that provide machine learning models asa service.
by storing one copy of a pretrained lan-guage model, it is possible to serve a large numberof user-speciﬁc models in parallel with little over-head..7 conclusion.
in this paper we have proposed an alternative wayto transfer knowledge from large pretrained lan-guage models to downstream tasks by appendingcarefully optimized embeddings to the input text.
the method outperforms existing methods withsigniﬁcantly more trainable parameters on gluebenchmark tasks and shows an impressive perfor-mance in a few-shot setting on two supergluetasks.
on the sentiment analysis task, the perfor-mance is comparable to the fully ﬁne-tuned lan-guage models.
this method can save a lot ofstorage in software applications designed to servelarge numbers of sentence classiﬁcation tasks..acknowledgments.
this work is based in part on research sponsoredby air force research laboratory (afrl) underagreement number fa8750-19-1-1000.
the u.s.government is authorized to reproduce and dis-tribute reprints for government purposes notwith-standing any copyright notation therein.
theviews and conclusions contained herein are thoseof the authors and should not be interpreted asnecessarily representing the ofﬁcial policies or en-dorsements, either expressed or implied, of airforce laboratory, darpa or the u.s. govern-ment..the work was supported by the ra sciencecommittee, in the frames of the research projectno.
20ttat-aia024.
most experiments were per-formed on gpus donated by nvidia..references.
mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..roy bar haim, ido dagan, bill dolan, lisa ferro,danilo giampiccolo, bernardo magnini, and idanszpektor.
2006. the second pascal recognisingtextual entailment challenge..luisa bentivogli, ido dagan, hoa trang dang, danilogiampiccolo, and bernardo magnini.
2009. theﬁfth pascal recognizing textual entailment chal-lenge..t. brown, b. mann, nick ryder, melanie subbiah,j. kaplan, prafulla dhariwal, arvind neelakan-tan, pranav shyam, girish sastry, amanda askell,sandhini agarwal, ariel herbert-voss, g. kr¨uger,t. henighan, r. child, aditya ramesh, d. ziegler,jeffrey wu, clemens winter, christopher hesse,mark chen, e. sigler, mateusz litwin, scott gray,benjamin chess, j. clark, christopher berner, sammccandlish, a. radford, ilya sutskever, and dariolanguage models are few-shotamodei.
2020.learners.
arxiv, abs/2005.14165..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andin proceedingscrosslingual focused evaluation.
of the 11th international workshop on semanticevaluation (semeval-2017), pages 1–14, vancou-ver, canada.
association for computational lin-guistics..kevin clark, minh-thang luong, quoc le, andchristopher d. manning.
2020. pre-training trans-formers as energy-based cloze models.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages285–294, online.
association for computationallinguistics..ido dagan, oren glickman, and bernardo magnini.
2006. the pascal recognising textual entailmentchallenge.
in machine learning challenges.
evalu-ating predictive uncertainty, visual object classiﬁca-tion, and recognising tectual entailment, pages 177–190. springer..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..4929william b dolan and chris brockett.
2005. auto-matically constructing a corpus of sentential para-phrases.
in proceedings of the international work-shop on paraphrasing..gamaleldin f. elsayed, ian goodfellow, and jaschasohl-dickstein.
2019. adversarial reprogrammingof neural networks.
in international conference onlearning representations..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recog-nizing textual entailment challenge.
in proceedingsof the acl-pascal workshop on textual entailmentand paraphrasing, pages 1–9.
association for com-putational linguistics..thanh-le ha, jan niehues, and alexander waibel.
2016. toward multilingual neural machine transla-tion with universal encoder and decoder..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020.deberta: decoding-enhanced bert with disentangled attention.
arxivpreprint arxiv:2006.03654..n. houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and s. gelly.
2019.inparameter-efﬁcient transfer learning for nlp.
icml..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in findings of the associationfor computational linguistics: emnlp 2020, pages4163–4174, online.
association for computationallinguistics..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin interna-learning of language representations.
tional conference on learning representations..hector j levesque, ernest davis, and leora morgen-stern.
2011. the winograd schema challenge.
inaaai spring symposium: logical formalizations ofcommonsense reasoning, volume 46, page 47..xiang lisa li and percy liang.
2021..preﬁx-tuning: optimizing continuous prompts for genera-tion.
arxiv preprint arxiv:2101.00190..roberta: a robustly optimized bert pretrainingapproach.
arxiv, abs/1907.11692..marie-catherine de marneffe, mandy simons, and ju-dith tonhauser.
2019. the commitmentbank: in-vestigating projection in naturally occurring dis-proceedings of sinn und bedeutung,course.
23(2):107–124..paarth neekhara, shehzeen hussain, shlomo dubnov,and farinaz koushanfar.
2019. adversarial repro-gramming of text classiﬁcation neural networks.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5216–5225, hong kong, china.
association for computa-tional linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..matthew e. peters, sebastian ruder, and noah a.smith.
2019. to tune or not to tune?
adapting pre-trained representations to diverse tasks.
in proceed-ings of the 4th workshop on representation learn-ing for nlp (repl4nlp-2019), pages 7–14, flo-rence, italy.
association for computational linguis-tics..jonas pfeiffer, aishwarya kamath, andreas r¨uckl´e,kyunghyun cho,and iryna gurevych.
2021.adapterfusion: non-destructive task compositionin proceedings of the 16thfor transfer learning.
conference of the european chapter of the associ-ation for computational linguistics: main volume,pages 487–503, online.
association for computa-tional linguistics..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..y. liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, m. lewis,luke zettlemoyer, and veselin stoyanov.
2019..timo schick and hinrich sch¨utze.
2021a.
exploitingcloze-questions for few-shot text classiﬁcation andin proceedings of thenatural language inference..4930chapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
as-sociation for computational linguistics..16th conference of the european chapter of the as-sociation for computational linguistics: main vol-ume, pages 255–269, online.
association for com-putational linguistics..timo schick and hinrich sch¨utze.
2021b.
it’s not justsize that matters: small language models are alsofew-shot learners.
in proceedings of the 2021 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 2339–2352, online.
as-sociation for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. controlling politeness in neural machinein proceedings oftranslation via side constraints.
the 2016 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 35–40, sandiego, california.
association for computationallinguistics..taylor shin, yasaman razeghi, robert l. logan iv,eric wallace, and sameer singh.
2020a.
auto-prompt: eliciting knowledge from language mod-els with automatically generated prompts.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 4222–4235, online.
association for compu-tational linguistics..taylor shin, yasaman razeghi, robert l. logan iv,eric wallace, and sameer singh.
2020b.
auto-prompt: eliciting knowledge from language mod-els with automatically generated prompts.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 4222–4235, online.
association for compu-tational linguistics..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
in proceedings of emnlp, pages 1631–1642..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r. bowman.
2019a.
superglue:a stickier benchmark for general-purpose languageunderstanding systems.
in neurips..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019b.
glue: a multi-task benchmark and analysis plat-form for natural language understanding.
in inter-national conference on learning representations..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019. neural network acceptability judg-ments.
transactions of the association for compu-tational linguistics, 7:625–641..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north american.
4931a hyperparameters.
b learned tokens.
table 6 lists the closest vocabulary words to thelearned embeddings.
most tasks have two inputsentences, so the prompts consist of three parts:one is added before the ﬁrst sentence, the sec-ond one is added between the sentences and thethird one is appended next to the second sentence.
for the single-sentence tasks, the second and thirdparts of the prompt are simply concatenated.
eachtask has trainable verbalizer tokens, one per outputclass..the prompts of rte, mrpc and sts-b arepretty similar to mnli’s prompts, as the mod-els for these tasks were initialized from pretrainedmnli models.
the other tasks were initializedwith [mask] tokens.
the ﬁnal model for coladidn’t move too far from its initialization..for each of the tasks, we performed hyperparam-eter search in the following space:.
• learning rate is chosen from the set{10−2, 3 · 10−3, 10−3, 3 · 10−4, 10−4, 3 ·10−5},.
• number of epochs is chosen as either 10or 20. this determines the behavior of theslanted triangular learning rate scheduler..• initialization is performed either with theembedding of the [mask] token, or ran-domly initialized from a normal distribution,with the mean and variance taken from thematrix of roberta’s word embeddings..the hyperparameter search took roughly 4 dayson two titan v gpus.
the ﬁnal choices for eachtask are shown in table 5.initialization with[mask] performed better than the random initial-ization..we disable all dropouts inside transformer.
weuse huggingface implementation of adamw op-timizer with weight decay disabled.
the gradi-ent is normalized to the value 1.0. for the batchsampling we use bucketing with padding noise of0.1. in order to use the device memory more ef-fectively, we also set maximum number of tokensper batch to 2048. the maximum sequence lengthis truncated to 512 tokens.
we enable mixed preci-sion and pad all sequence lengths to the multiplesof 8 for the effective usage of tensorcores8..8https://docs.nvidia.com/deeplearning/performance/mixed-.
precision-training/index.html.
learning rate epochs.
taskmnliqnliqqprtesst-2mrpccolasts-b.
0.0010.0010.00030.0010.0030.0010.0010.001.init.
[mask][mask][mask]mnli[mask]mnli[mask]mnli.
1010202020202020.table 5: hyperparameters of our best-performing mod-els.
[mask] means the prompts are intialized with theword embedding of same token, and mnli means theprompt is initialized with the prompts of out best mnlirun..4932mnli.
prompts.
verbalizers.
prompts.
qnli.
verbalizers.
prompts.
qqp.
verbalizers.
prompts.
verbalizers.
prompts.
verbalizers.
prompts.
verbalizers.
rte.
sst-2.
mrpc.
prompts.
cola.
verbalizers.
sts-b.
prompts.
beforebetweenafterentailmentneutralcontradictionbeforebetween.
afterentailmentnot entailmentbeforebetween.
afternot duplicateduplicatebeforebetweenafterentailmentnot entailmentbeforebetweenafternegativepositivebeforebetweenafterentailmentneutralbeforebetween.
afterunacceptableacceptablebefore.
betweenafterregression.
gomery.
unless.
vide.
470.ende.
sugg.
e!.
blames.
a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a- tomorrow ale .agj *.
much irin [/ a (@ [mask] dl ahj e [mask] akh<!– informing inyl entit dimcategories.
*.
neigh [mask] u {{ag—ag— [mask] olitan pronouns [mask] [mask] [mask]@@@@ [mask] choi [mask].
resembling swarm paramount calm membershipderive rics [mask] alias iary [mask] omnip [mask] [mask][mask] sham.
[mask] forb [mask] fireﬂy they.
a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a- tomorrow ale .agj *.
much irin [/ a (@ [mask] ahj femin [mask] akahiahi.
informing # entit oooo.
choes charms sorely ”... akijakija afe pae charred masked [mask] fall babys smartest ik /dl forums bio mang a+-.
defective.
important.
gomery.
additionally.
o.a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a- tomorrow rison .agj *.
much irin [/ a jay [mask] dl ahj femin [mask] .?
> informing # entit oooocategories.
[mask] [mask] [mask] [mask] [mask][mask] [mask] [mask] [mask] [mask] [mask] [mask][mask] [mask] [mask] [mask].
[mask] [mask] [mask] [mask] [mask].
a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a-[mask]kers irin [/ a (@ [mask] dl ahahahah femin [mask] akha-a-a-a-a-a-a-a-a-a-a-a-a-a-a-a- repertoire inyl idea dim.
tomorrow ale.
.agj.
verbalizers.
ch.
table 6: the closest words to the prompt and verbalizer token embeddings for the best model for each task.
weuse cosine distance to measure the distance.
[mask] tokens highlighted in bold indicate the positions we use tooutput the prediction..4933