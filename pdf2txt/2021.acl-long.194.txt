otters:one-turn topic transitions for open-domain dialogue.
karin sevegnani, david m. howcroft, ioannis konstas, verena rieserthe interaction lab, macs heriot-watt universityedinburgh, scotland, uk{karin.sevegnani, i.konstas, v.t.rieser}@hw.ac.uk.
abstract.
user a source topic: i spend a lot of time outside..mixed initiative in open-domain dialogue re-quires a system to pro-actively introduce newtopics.
the one-turn topic transition task ex-plores how a system connects two topics in acooperative and coherent manner.
the goal ofthe task is to generate a “bridging” utteranceconnecting the new topic to the topic of the pre-vious conversation turn.
we are especially in-terested in commonsense explanations of howa new topic relates to what has been mentionedbefore.
we ﬁrst collect a new dataset of hu-man one-turn topic transitions, which we callotters1.
we then explore different strategiesused by humans when asked to complete sucha task, and notice that the use of a bridgingutterance to connect the two topics is the ap-proach used the most.
we ﬁnally show howexisting state-of-the-art text generation modelscan be adapted to this task and examine the per-formance of these baselines on different splitsof the otters data..1.introduction.
for a conversation to be truly engaging, we typi-cally assume that both participants take initiative,e.g.
by introducing a new topic.
we call this amixed-initiative dialogue.
open-domain systemstrained on vast amounts of data (jiang et al., 2020;zhang et al., 2020; gao et al., 2018; li et al., 2017,2016; vinyals and le, 2015), however, are oftenpurely responsive, make abrupt transitions, or failto take initiative (see examples in table 1).
inthis paper, we consider the case where the systempro-actively introduces a new topic in a conver-sation by providing a commonsense link of howthis new topic relates to what was mentioned pre-viously (see fig.1).
we call this transition strategy“bridging”.
humans deploy a range of strategies.
1https://github.com/karinseve/otters.
user b.user b.transition: i like the outdoors as well, espe-cially gardening.
it destressesme..target topic: i enjoy relaxing and getting.
ﬂowers.
entity path: outside - garden -flower.
transition: since you like seafood,.
isswedish ﬁsh a candy that youmight enjoy?.
target topic: i have no self control when itcomes to candy.
entity path: seafood - swedish fish -.
candy.
user a source topic: i like seafood a lot..user a source topic: i think i am getting engagedsoon..transition: i have two children from a pre-.
user b.vious marriage.
target topic: my children are my life.
entity path: engagement - marriage -.
child.
figure 1: example topic transitions from otters.
usera introduces a topic with a short sentence (main con-cept in bold).
then user b responds with a (option-ally multi-sentence) “bridging” transition before intro-ducing the new topic (the main concepts for the tran-sition and target topic are denoted with underline andbold, respectively).
each example is accompanied byan entity path, comprising knowledge graph entities(denoted with teletype) instantiating the main con-cepts of the dialogue turn..in addition to bridging, including disengagement,discourse markers or silence (riou, 2015).
we hy-pothesise that introducing a new topic by making aconnection with the previous dialogue turn can beperceived as a less abrupt transition..more speciﬁcally, we investigate bridging tran-sitions between two user utterances in the form ofone or more sentences that contain at least one mainlinking concept.
these inherently can allow for bet-ter grounding to external resources such as entities.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2492–2504august1–6,2021.©2021associationforcomputationallinguistics2492in large knowledge graphs (kg) (e.g., wikidata),or named entities mentioned in documents (e.g.,wikipedia, or news articles), ultimately leading tomore controlled and interpretable outputs..to this end, we crowdsource a corpus of human-written topic transitions focused on these “bridging”strategies, where humans introduce a “missing link”concept, given a source and target topic in the formof two short user utterances (fig.
1).
by groundingthe topics on a kg using automatically recognisedentities associated with each topic, we can thenidentify “commonsense” connections which aresimilar to these missing links..by modelling such topic transitions in the formof cause-effect relationships in a kg, we canthen perform abductive inference on commonsenseknowledge for which we provide a language gener-ation baseline.
in particular, we ﬁne-tune a multi-hop reasoning model (ji et al., 2020) which wastrained on a similar task called abductive nlg(αnlg) to generate an explanatory hypothesisgiven two observations.
we ﬁnd that combining areasoning module over a kg (conceptnet) with alanguage model achieves the best performance onour “topic transition” task for both the predictedentity path as well as the generated utterance.
in ad-dition, we show that existing multi-topic dialoguedatasets, such as personachat (zhang et al., 2018)and topicalchat(gopalakrishnan et al., 2019),cannot be easily adapted to this task, due to thedifferent nature of the tasks they were designed for.
our contributions are as follows:.
• we propose a new natural language gener-ation task based on one-turn topic transitionsfor open-domain dialogue based on a “bridg-ing” strategy, which promotes grounding onkg entities..• we collect a crowdsourced dataset, otters,and present a rigorous analysis in terms oftransition strategies, linguistic properties andentity linking to a kg..• we show that our kg-grounded dataset caneffectively leverage the reasoning componentof an existing transformer-based model (jiet al., 2020) to generate better output com-pared to a vanilla gpt-2 (radford et al., 2019)decoder, both in in-domain and out-of-domaindata splits..2 related work.
topic transitions in the linguistic literature.
there is no common deﬁnition for the term topic(goutsos, 1997; purver et al., 2011); however, thereare a number of deﬁnitions which are helpful forour purposes.
goutsos (1997) divide a “topic” intotwo main components: 1) what constitutes a topic(the “what”) and 2) how participants perceive andmanage a topic (the “how”).
an early work frombrown and yule (1983) declares that “topics shouldbe described as the most frequently used, unex-plained term in the analysis of discourse”.
in gen-eral, “discourse topics” can be explained as whata portion of the interaction is about, therefore the“aboutness” (berthoud and mondada, 1995; porhiel,2005).
more speciﬁcally chafe (1994) deﬁnes thenotion of topic as “the totality of information thatis semiactive at one time”..prior work has shown that the introduction ofa new topic usually co-occurs with cues such aswrapping things up about the current topic (may-nard, 1980), preceding silence, or the use of dis-course markers (riou, 2015).
also, backchannelsignals, e.g., yeah, right, you know, indicate thatboth agents are involved in the interaction andshow consent for the topic development (james,1995).
beyond these overt cues, james (1995) andgeluykens (1993) describe semantic topic transi-tions: “each topic has a tendency to lead to the next;to provide the opening for another” (james, 1995),and topics are typically “co-constructed”, requiringeach speaker to contribute to the conversation forfurther progression and development (geluykens,1993).
the identiﬁcation of topic transition is in-deed not an easy task.
it is not only about linguisticcues such as discourse markers and prosodic cues,as sometimes a topic switch can be identiﬁed withthe introduction of a new entity (james, 1995).
ad-ditionally, in a conversation topics are created andintroduced by participants themselves in real time,making topics participant- and interaction-speciﬁc(mondada, 2001, 2003).
moreover, “the entities infocus at a given point in the discourse will be thatpartially-ordered subset of activated entities whichare likely to be continued as topics of subsequentutterances” (gundel et al., 1993).
these coopera-tive elements emphasise the importance of mixed-initiative topic management for open-domain dia-logue systems..2493personachat.
topicalchat.
user a: i do not like carrots.
i throw them away.
user b: really.
but, i can sing pitch perfect.
user a: i also cook, and i ride my bike to work.
user b: great!
i had won an award for spelling bee..user a: yeah and saltwater ﬁsh are lucky becausethey can do that and drink through theirmouths..user b: seems like fresh water ﬁsh got the short endof the stick with that one.
have you everbeen to a cat cafe?.
table 1: examples of abrupt topic transitions from thepersonachat and topicalchat datasets..current multi-topic open-domain systems.
previous work in open-domain dialogue systemshas largely avoided explicitly modelling topic tran-sitions and instead focused on grounding systembehaviour in a “persona” (a set of statements abouthobbies, demographics, or preferences) (zhanget al., 2018; li et al., 2016) or by condition-ing conversations on knowledge sources such asnewspaper articles, fun facts or wikipedia articles(gopalakrishnan et al., 2019; dinan et al., 2019)to generate engaging responses while avoidinggeneric replies, improving coherence, and raisingnew and interesting topics.
these approaches oftenlead to poor topic transitions, as illustrated in table1. the personachat example shows neither initia-tive nor common sense while transitioning to a newtopic; it only displays passive acknowledgementfrom user b. whereas the topicalchat examplepresents a very abrupt topic shift by user b. ourdataset is the ﬁrst corpus focused speciﬁcally onone-turn topic transitions; however, there are sev-eral human-to-human dialogue corpora whereinparticipants discuss assigned topics.
two promi-nent such corpora are topicalchat (gopalakrishnanet al., 2019) and personachat (zhang et al., 2018).
in topicalchat both participants used source doc-uments from wikipedia to discuss a shared topic.
the dialogues in this corpus tend to ﬂow less natu-rally than those in personachat with participantsgenerally focusing on expressing the main facts,often by copy and pasting from their source doc-uments rather than having a natural conversation.
therefore we focus on personachat as a point ofcomparison..personachat dialogues consist of chit-chat con-versations based on a set of “persona traits” as-signed to each participant.
because participantsseek to express their persona to each other, the con-.
versations require mentioning various topics (i.e.
their persona traits) in a natural way.
indeed, zhanget al.
(2018, sec.
3.3) adjusted their design to en-courage users to engage with each other’s topicsand not simply state their own topics as quicklyas possible to end the dialogue.
personachat doesnot contain annotations for the topic of each turnand participants had the freedom to mention theirtopics (i.e.
persona traits) in any order..we use personachat in two different ways: 1)using their persona traits as starting and goal topicsfor our own data collection, and 2) as a point ofcomparison for our dataset..commonsense-aware neural text generation.
large language models still suffer in cases wherereasoning over underlying commonsense knowl-edge is required during generation, including dia-logue generation (zhou et al., 2018), story endinggeneration (guan et al., 2019), and topic-to-essaygeneration (yang et al., 2019).
recently, guanet al.
(2019); bhagavatula et al.
(2020) attemptedto integrate external commonsense knowledge intogenerative pretrained language models, which wewill also attempt in section 4 using the abductivenlg (αnlg) dataset (bhagavatula et al., 2020).
our setup is similar in spirit to αnlg, which is aconditional generation task for explanations givenobservations in natural language.
in particular, themodel has to generate an explanatory hypothesisgiven two observations: the cause (e.g.
the smithfamily went on a cruise for their summer vacation)and the consequence (e.g.
from then on, the smithswent to the beach each summer instead).
here, apossible explanation might be: the smith familygot seasick on the cruise.
the αnlg dataset con-tains 20k pairs observations and 200k explicativehypotheses, which we will later use for ﬁne-tuningour models (see section 4)..3 one-turn topic transitions.
3.1 task design and data collection.
task description.
we assume there are topics taand tb for utterances ua and ub (with u· = t· forthis paper).
the goal of the task is to generate a one-turn transition utterance ut to serve as a smoothlink between ta and tb so that its concatenationwith utterance ub is a sensible response to ua.
abridging transition occurs when one or more of theentities et ∈ et mentioned in ut lies on a path inthe knowledge graph between entities ea ∈ ea and.
2494eb ∈ eb mentioned in ua and ub, respectively..knowledge graph construction.
we use per-sonachat persona traits as the starting point for ourdata collection.
in order to model commonsenseconnections, we built a knowledge graph (kg) us-ing the entities found in each persona trait throughthe yahoo entity linker (blanco et al., 2015; pappuet al., 2017).
each entity is linked to its correspon-dent wikidata identiﬁer, while a sparql queryretrieved the entity’s super-classes and sub-classes,which were added to the kg.
furthermore, the kghas been augmented by retrieving the common-sense connections for each entity from conceptnet(speer et al., 2017) and by parsing wikipedia ab-stracts mentions..to select which traits to use for the data collec-tion, we ﬁrst selected all pairs of entities connectedwith k-hops (1 < k < 20) in the kg.
then, werecovered the entities mentions in the persona traitsand saved every pair (nearly 30k) as potential pairsfor our data collection..data collection.
we crowdsourced the data col-lection for otters on amazon mechanical turk(amt).
each user was provided with two topics a,b from the personachat persona traits, along withinstructions explaining the task.
the instructionsask the user to imagine they are having a conversa-tion where the ﬁrst topic a from the pair representsthe last turn of the other person, and the secondtopic b contains the ﬁnal topic the user wants totalk about.
the user then has to write a short ut-terance to transition to the new topic b in the leastabrupt way possible.
additionally, in order to en-courage crowd-workers to ground their utterancesin actual topics, we asked them to report the “topics”mentioned in their sentence (see figure 2)..for each topic pair in the study we collectedthree different transition utterances to provide moreinsight into the different strategies users adoptwhen transitioning to a new topic..3.2 corpus properties.
basic statistics.
table 2 provides summarystatistics describing otters.
our corpus consistsof 4,316 utterances for 1,421 unique topic pairs,with an average utterance length of 1.3 sentencesand 16.4 words.
the kg path statistics for ottersare based on all of the paths found by the yahooentity linker between the 1421 unique topic pairsin the corpus, a total of just over 12k paths..figure 2: screenshot of part of the interface users arepresented with when accepting the study on amt..propertyttrent.
ttrturn length—: mean—: modesent.s/turn—: mean—: modekg path—: mean—: mode.
otters5085 / 709353984 / 170542-6816.491-71.311-146.15.table 2: properties of otters.
type-token ratio (ttr)for the different splits of the dataset.
entity ttr refersto the number of (unique) entities appearing in that por-tion of the dataset.
turn length is the length of thetransition utterance in tokens, including punctuation.
number of sentences per turn measured by splitting onsentence-ﬁnal punctuation..kg coverage.
we calculated the distance be-tween each pair of topics in the knowledge graphdescribed in sec.
3.1 to facilitate analyses of therole of topic distance in transition strategy and tran-sition quality.
to extract entities from the utter-ances in our corpus, we extended the tagger built-into the yahoo entity linker with the spacy namedentity recognizer to include all nouns and adjec-tives as potential entities.2.
using these extracted entities we analyse theoverlap between entities mentioned in the given top-ics a, b and those mentioned in the crowdsourcedtransition utterances.
the jaccard distance betweenthese two sets is 1 for nearly a quarter of the topic-pairs and utterances in our dataset, with a mean of0.842, meaning that the overlap between entitiesmentioned in the utterances and entities mentionedin the topics is fairly low.
this indicates that users.
2this modiﬁed version allowed us to identify a wider range.
of topic-related entities..2495overlaputt.-topicsutt.-kg path.
jaccard dist.
mean mode (freq)1.0 (1274)0.8420.667 (451)0.751.table 3: overlap in entities between transition utter-ances and (1) the topic sentences (i.e.
persona traits)and (2) the path between those topics in the kg..cambridgepdtb3turn length.
spearman ρ0.039∗0.0030.139.pearson r0.046∗0.001−0.001.
table 4: correlations between the kg distance be-tween topics a, b and the number of discourse markersused, as deﬁned by cambridge dictionary and penndiscourse treebank, as well as correlation with turnlength.
* indicates statistical signiﬁcance p < 0.01with bonferroni correction for multiple comparisons..transition from topic a to topic b mentioningnew unseen entities, following a “path” that can begrounded on a knowledge graph..in contrast, the overlap between the entities inthe kg path between the topics and the entitiesmentioned in the transition utterances is higher:both the mean and the mode jaccard distances dropto below 0.8, suggesting that crowdworkers makesimilar connections to the ones we can ﬁnd in ourknowledge graph a substantial portion of the times.
this suggests that our kg-grounded approach canﬁnd plausible entities to be mentioned to bridgebetween topics, similar to the commonsense con-nections made by humans shifting between topics..3.3 transition strategies in otters.
to examine the strategies humans applied whilecompleting the otters task, we adapted the cate-gories of riou (2015) for a manual analysis of ourdata.
riou (2015) distinguishes between disjunc-tive and stepwise transitions between topics.
dis-junctive transitions make no attempt to relate thenew topic to the previous topic, switching abruptlyto the new topic without acknowledging the previ-ous topic, whereas stepwise transitions are akin tothe previously described transition strategies..we distinguish between bridging and acknowl-edge & continue strategies:in the former, thespeaker aims to produce an utterance which con-nects the previous and new topics directly; in thelatter, the speaker acknowledges the previous topicbefore introducing their own topic, without explic-itly relating the two to one another.
in additionto these categories, we also annotated utterances.
as off-task (e.g.
replying to or continuing the ﬁrsttopic without any attention paid to the second topic)or off-topic when the utterance had nothing to dowith either of the two topics (e.g.
random greetingsor generic questions)..two of the authors annotated 10 utterances from10 different users, resulting in 200 total annotations.
the initial inter-annotator agreement was 71%,classiﬁed as substantial (krippendorff’s α = 0.34),after which the annotators collaborated to reacha consensus annotation for each of the examplesthat presented a disagreement.
table 5 containsa prototypical example for each of the annotatedclasses..more than 80% of the data contains some formof transition to the second topic, with 79% contain-ing a bridging utterance, 5% applying an acknowl-edge and continue strategy, and only 2% using thedisjoint transition strategy.
12% of the data is con-nected to one or more of the topics in some waybut does not serve as a transition, and 2% of thedata is completely off-topic.
this analysis sug-gests that our corpus indeed represents the kind ofknowledge-based transitions we are interested in..kg distance and discourse markers.
we hy-pothesize that speakers are less likely to use explicittopic management strategies (e.g.
topic wrap-ups,discourse markers) when topics are more closelyrelated to each other, e.g.
as measured by graphdistance in a large knowledge graph.
this wouldbe in line with ﬁndings about the use of explicit dis-course markers versus leaving discourse relationsimplicit.
torabi asr and demberg (2012, 2013)found that explicit markers are more likely to beomitted when the discourse relation is highly pre-dictable based on the content of the arguments..based on riou (2015) we examined the fre-quency of discourse markers in utterances to testour hypothesis, examining both general conversa-tional discourse markers and those associated withspeciﬁc discourse relations.
for conversational dis-course markers we use the cambridge dictionary,which provides a list of spoken and written mark-ers, including “well”, “you know”, etc., while formarkers signalling particular discourse relationswe use the list from the penn discourse treebank(webber et al., 2019; prasad et al., 2008, pdtb);these include markers like “because” indicating acausal relationship or “in addition” for an additiverelationship.
we ﬁnd a small but signiﬁcant corre-lation (≈ 0.04) between conversational discourse.
2496acknowledge and continue.
i like to eat the same thing as ninja turtles.
i love pizza.
i eat it while i skateboard.
i enjoy riding around on a plank with wheels.
bridging: missing link.
i prefer things to be authentic.
i think children are the truest form of authenticitybecause they say things unﬁltered.
i am not a fan of children..disjunctive.
i like american made cars.
i like liver cooked in butter – just throwing that in!
i avoid eating broccoli..a:t:b:.
a:t:.
b:.
a:t:b:.
i prefer things to be authentic..a:t: my bro just made some authentic thai chicken.
b:.
i am not a fan of children..off-task.
off-topic.
a:t:.
b:.
i learnt to drive.
i had a rough night sleeping in my new bed lastnight.
i like making a salmon entree..table 5: prototypical examples of each annotation cat-egory for transition strategy or lack of transition in ot-ters.
(a) is the preceding topic or utterance; (t) is thecollected utterance; and (b) is the goal topic and poten-tial next utterance for speaker b..markers and no signiﬁcant correlations betweenthe use of pdtb3 discourse markers or the turnlength and kg distance.
this suggests that usersare somewhat more likely to use conversationaldiscourse markers as the distance between topicsin the knowledge graph increases, in line with ourhypothesis..3.4 validating the corpus.
we evaluate whether the transition strategies inotters are less abrupt than those found in per-sonachatby constructing a comparable subset ofpersonachatand performing a human evaluation..comparable corpus construction.
we ﬁrst ex-tract a subset of personachat where two consecu-tive turns contain different topics.
in other words:turns where one speaker changed the topic fromwhat the previous speaker has just said.
since per-sonachat turns do not incorporate topic annota-tions, we use a heuristic based on bertscore toassign a topic to each turn.
given topics t andturns u for a dialogue in personachat, we calcu-late the bertscore similarity between each u ∈ uand each t ∈ t. for each turn u we then assignt = argmaxt(bertscore(u, t)), if and only if.
figure 3: interface for crowdsourced validation..where t(cid:48) is the topic achieving the second highestbertscore relative to u, and d is a threshold toensure that we only assign a topic to a turn if it is asubstantially better ﬁt than the other topics.3 whilethis means that not every turn is assigned a topic,this is necessary to ensure that we do not assigntopics to, e.g., greetings like ‘hi, how are you?’..this way of assigning topics yields a subset con-sisting of 22,010 utterances which have a differenttopic from the preceding utterance.
most of thesetopic-pairs (20,491) are only expressed throughone utterance in the dataset, while 1,188 are ex-pressed by two utterances, 248 by three, and 83 bymore than 3 utterances.
moreover, there are 445topic-pairs which also occur in our corpus..crowdsourced validation.
using the compara-ble sub-corpus of personachat, we asked crowd-workers to vote which of two potential transitionutterances was “less abrupt” (fig.
3) for 49 topic-pairs occurring in both datasets.
we collected 3votes for each utterance and only counted instanceswhere 2/3 workers agreed on the same choice..the results conﬁrm that otters has less abrupttransitions: the utterances in otters were judgedas less abrupt in 44/49 cases, with the comparablepersonachat utterance judged less abrupt in onecase, and both utterances rated “bad” in another.
only 3 cases did not present a majority class..4 experiments.
having conﬁrmed the quality of our corpus, wenow adapt two existing text generation models asbaselines for this task.
we also explore differenttrain-dev-test splits and conduct an error analysis..bertscore(u, t)bertscore(u, t(cid:48)).
> d.(1).
3we set d = 1.27 for our dataset construction, since this isthe 50th percentile of bertscore values observed in our data..2497trainid693/1,929ood 677/2,034.
dev404/1,160372/1,152.
test303/1,158372/1,130.
table 6: num.
unique/total topic-pairs in each split..4.1 baselines.
the ﬁrst baseline we consider is a vanilla gpt-2language model (radford et al., 2019) ﬁne-tunedon otters (vgpt2).
next, we test the recent multi-gen (ji et al., 2020) on this task, which extendsgpt-2 with multi-hop reasoning on commonsenseknowledge graphs.
in particular, this model com-bines the vocabulary distribution generated by gpt-2 with a concept distribution in order to produceknowledge grounded responses.
the concept dis-tribution is given by reasoning performed on thecommonsense knowledge graph conceptnetio, us-ing the context modeled through gpt-2..4.2 train-dev-test splits.
the ﬁrst split is an out-of-domain split (ood),which ensures that none of the topics in the test-setare present in any of the topic-pairs in the train-set.
for the second split, this restriction is relaxed tocreate an in-domain split (id), allowing one of thetopics in each pair in the test-set to appear in thetrain-set, although with a different second topic..the ood split resembles a zero-shot scenario,where the model has to generate a shift betweentwo topics it has never been ﬁne-tuned on.
hence,we expect results to be lower than the ones fromid.
the number of unique and total topic pairs foreach split is illustrated in table 6..4.3 evaluation.
we evaluate two aspects of the transition task: 1)whether the model can ﬁnd a sensible path throughintermediate topics and 2) whether the model cangenerate a natural utterance which mentions suchintermediate topics..to evaluate the former, we assess the entitiesmentioned in the transition utterance to determinehow well they bridge the gap between topic a andtopic b. we use hits@k ratio as an automaticapproximation, which measures the number of rel-evant entities correctly predicted by the model, outof the k most important entities identiﬁed in thetarget references.
this metric shows how well themodels ground the concepts introduced in the twodialogue turns and how the reasoning compares to.
the human standard presented in otters..for (2) we adopt the same automated metricsused for evaluating multigen on the αnlg datasetfor comparability: rouge-l (lin, 2004), me-teor (banerjee and lavie, 2005), and cider(vedantam et al., 2015).
however, we report thefull bleu score (papineni et al., 2002)4 that ac-counts for the overlap across 1-4 ngrams insteadof just 4-grams (bleu-4).
as word-overlap basedmetrics have been widely criticised due to their lackof correlation with human judgements (novikovaet al., 2017; reiter, 2018), we also provide anexample-based error analysis in section 4.4..4.4 results.
for each aforementioned split we evaluated threedifferent models to compare performance: the pre-trained vgpt2 ﬁne-tuned on each split for otters,the multigen model ﬁne-tuned only on αnlg, andthe same model additionally ﬁne-tuned on otters(called αnlgft)..overview of results.
table 7 shows the resultsof these experiments.
vgpt2 performs poorly onthe one-turn transition task, regardless of the train-dev-test split, which we attribute to the small sizeof otters: with only a few thousand utterances,vgpt2 is unable to learn the task.
we notice, how-ever, that the system tends to repeat the main entityin topic a, therefore scoring surprisingly well onthe hits@k metric, despite the fact that the ut-terances themselves are of low quality (see table8)..the reasoning component added by multigenleads to substantial improvements in most of theevaluation metrics but not hits@k (αnlg in thetable).
therefore, the improvements in text qualitymetrics appear to be due primarily to the similaritybetween the structure of the abductive nlg taskand the increased amount of data for ﬁne-tuning(≈ 688k tokens) compared to ﬁne-tuning vgpt2on our ≈ 71k tokens alone..further ﬁne-tuning multigen on otters leads tosubstantial improvements on all metrics for both in-domain & out-of-domain splits.
the performanceimprovement is considerable especially becauseof the relatively small size of the training set (693unique topic pairs on in-domain, see table 6), fur-ther justifying the compatibility between the orig-inal task multigen was trained on and otters..4we use sacrebleu (post, 2018)..2498vgpt2vgpt2αnlgαnlgαnlgftαnlgft.
split bleu meteor rouge-l ciderood4.658.37id3.7510.26ood12.1216.35id11.3318.9ood21.722.8id18.4825.03.
12.414.6723.2625.5229.2731.54.
1.261.581.521.64.04.27.hits@122.9457.1421.8522.8935.3338.11.hits@312.5430.7912.0312.7218.6721.51.table 7: system performance on in-domain and out-of-domain data splits..ex.
1.ex.
2.ex.
3.ex.
4.topic a:otterstopic b:vgpt2:αnlg:αnlgft:.
topic a:otterstopic b:vgpt2:αnlg:αnlgft:topic a:.
i like babies.
i make babies laugh.
i enjoy making other people laugh.
i like other people making babies.
i decided to give birth to a baby.
i like to make babies laugh when i’mnot working.
i love those sleek italian cars.
i wish i was as sleek as those cars.
i am in good shape.
i love it.
i love it.
i bought a new car.
i like the look of italian cars.
i have prior agriculture experience..otters my parents were farmers.
i am a mom.
topic b:a lot of cooking.
i am a lot of cook-vgpt2:ing..αnlg: my mom taught me how to grow veg-.
αnlgft:topic a:.
etables.
i have been a farmer for many years.
i make blogs..otters do you know of any blogs that areon the subject of large families withbabies?
i have a large family with babies.
family.
i have a lot of babies.
i have a large family..topic b:vgpt2:αnlg:αnlgft:.
table 8: representative outputs from each model..nonetheless, the bleu scores from table 7 in-dicate there is still space for improvement.
we hy-pothesise meteor are higher than bleu scores,because they also consider paraphrases..these results conﬁrm that our newly introducedone-turn topic transition task needs a reliable lan-guage model combined with an advanced reasoningcomponent..detailed discussion and model limitations.
we further analyse the results to understand modellimitations.
first, we observe that multigen’shits@k ratio is quite low, especially when com-pared to vgpt2.
this is surprising consideringvgpt2’s generated sentences are mostly very shortand repetitive, and the predicted concepts mostlymatch the ones contained in the ‘topic a’ sen-tence.
one possible explanation is that multigen’s.
reasoning module uses a gate loss, which deter-mines whether to select a concept from the pro-vided knowledge graph or a word from the gpt2dictionary.
we observed that the majority of thetimes the model will use a word from the gpt2dictionary rather than selecting a concept from theknowledge graph..moreover, we observe that only 65% of the con-cepts found in the target sentences are actuallynodes in multigen’s subgraphs.
one possible ex-planation is that multigen’s reasoning model hasa limited input capacity of up to 100 nodes thatare at most 2 hops away in order to prune the verylarge knowledge graph from conceptnet.
the en-glish vocabulary from conceptnet contains approx-imately 1,500,000 nodes, which makes the processof determining the concept distributions very com-putationally expensive and time inefﬁcient.
there-fore, the pruning strategy adapted by ji et al.
(2020)overcomes these problems but cannot be appliedto the otters task, as the selection of the conceptsis just as important as the output sentence beingﬂuent.
contrary to our expectations, expandingthe size of the knowledge graphs from 100 nodesto 200 and 300 did not improve the hits@k ra-tio.
most likely because the concepts added to thegraphs are either not relevant or misleading for themodel.
this suggests that improving concept selec-tion is a promising future direction to improve theperformance of the reasoning module, leading tooverall better topic transitions..error analysis.
in addition, we preform anexample-based error analysis to further understandthe strengths and weaknesses of the individual mod-els.
table 8 shows representative system outputsfor each of the models on the in-domain data split.
first, we observe that vgpt2 often generates verysimple sentences (e.g., ‘family.’, in ex.
4), repeatednon-content bearing tokens (e.g., ‘i love it.’, in ex.
2), or incoherent and often not speciﬁc enough out-put to form a successful bridging transition (e.g.,‘a lot of cooking.’, in ex.
3, is not a well-formedsentence, and only loosely connected to topic a.
2499about ‘agricultural experience‘), contributing tolow bleu scores.
however, this also reinforcesthe idea that the hits@k scores are artiﬁcially in-ﬂated simply due to vgpt2 choosing to includeone of the entities from the ﬁrst topic..the outputs from multigen tested on ottersshow a better performance than vgpt2, given thatthe topic selection for the model is grounded onconceptnet.
however, since the abductive nlgtask is different than the ‘topic transition’ taskaddressed in otters, there is a discrepancy in theuse of the language.
the model often outputs co-herent sentences that use generic commonsensefacts which may not be related to topic b (e.g., ‘idecided to give birth to a baby’, in ex.
1)..the texts generated from multigen ﬁne-tunedon otters on the other hand, introduce interestingconnections between topic a and topic b (e.g., ‘ilike to make babies laugh when i’m not working.’,in ex.
1) and leverage commonsense (e.g., ‘i likethe look of italian cars’, in ex.
2, where ‘the look’creates a connection with ‘being in good shape’from topic b)..5 discussion & conclusion.
ethical considerations.
we recognise that anymixed-initiative dialogue system carries risks re-lated to dual-use: in addition to helpful systemswhich serve to help users explore a new topic ordiscover more about the world, a system which caneffectively change the topic of conversation couldalso be used to manipulate user behaviour.
forexample, bridging strategies for topic transitionscould be used by virtual assistants to encourageusers to make a purchase or to express their opin-ion or preference regarding sensitive subjects..conclusion.
we have deﬁned a new nlg taskexploring one-turn topic transitions for mixed-initiative in open-domain systems.
our otterscorpus provides training data for modelling topictransitions based on ‘missing link’ topics whichconnect the previous conversation subject to a newtopic.
baseline models based on state-of-the-artapproaches to text generation illustrate possible ap-proaches to the task and show that there is roomfor improvement.
in particular, we show that com-monsense knowledge grounding is necessary forthis task, outperforming ﬁne-tuned large languagemodels.
in future work, we will explore modelarchitectures speciﬁcally designed for topic transi-tions, as well as ﬁne-tuning strategies to deal with.
small datasets.
we also plan to evaluate the impactof bridging transitions on user (dis)engagement inan open-domain dialogue system..acknowledgements.
this research received funding from the epsrcproject madrigal (ep/n017536/1), as well asgoogle research grant to support nlu and dialogresearch at heriot-watt university..references.
satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..a-c berthoud and lorenza mondada.
1995. traite-ment du topic, processus ´enonciatifs et s´equenceslinguistiqueconversationnelles.
franc¸aise, (17):205–228..cahiers de.
chandra bhagavatula, ronan le bras, chaitanyamalaviya, keisuke sakaguchi, ari holtzman, han-nah rashkin, doug downey, wen-tau yih, and yejinchoi.
2020. abductive commonsense reasoning.
in8th international conference on learning represen-tations, iclr 2020, addis ababa, ethiopia, april26-30, 2020..roi blanco, giuseppe ottaviano, and edgar meij.
2015.fast and space-efﬁcient entity linking in queries.
inproceedings of the eight acm international confer-ence on web search and data mining, wsdm 15,new york, ny, usa.
acm..gillian brown and george yule.
1983. discourse anal-.
ysis.
cambridge university press..wallace chafe.
1994. discourse, consciousness, andtime: the ﬂow and displacement of conscious experi-ence in speaking and writing.
university of chicagopress..emily dinan, stephen roller, kurt shuster, angelafan, michael auli, and jason weston.
2019. wizardof wikipedia: knowledge-powered conversationalagents.
in proceedings of the international confer-ence on learning representations (iclr)..jianfeng gao, michel galley, and lihong li.
2018.in theneural approaches to conversational ai.
41st international acm sigir conference on re-search & developmentin information retrieval,pages 1371–1374..ronald geluykens.
1993. topic introduction in englishconversation.
transactions of the philological soci-ety, 91(2):181–214..2500karthik gopalakrishnan, behnam hedayatnia, qin-lang chen, anna gottardi, sanjeev kwatra, anuvenkatesh, raefer gabriel, dilek hakkani-t¨ur, andamazon alexa ai.
2019.topical-chat: to-wards knowledge-grounded open-domain conversa-tions.
proc.
interspeech 2019, pages 1891–1895..dionysis goutsos.
1997. modeling discourse topic:sequential relations and strategies in expository text,volume 59. greenwood publishing group..jian guan, yansen wang, and minlie huang.
2019.story ending generation with incremental encod-proceedingsing and commonsense knowledge.
of the aaai conference on artiﬁcial intelligence,33(01):6473–6480..jeanette k gundel, nancy hedberg, and ron zacharski.
1993. cognitive status and the form of referring ex-pressions in discourse.
language, pages 274–307..angela james.
1995. topic shift in casual conversa-tion.
the university of western ontario journal ofanthropology, 2(1)..haozhe ji, pei ke, shaohan huang, furu wei, xiaoyanzhu, and minlie huang.
2020. language generationwith multi-hop reasoning on commonsense knowl-edge graph.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing..bin jiang, wanyue zhou, jingxu yang, chao yang, shi-han wang, and liang pang.
2020. pednet: a per-sona enhanced dual alternating learning network forconversational response generation.
in proceedingsof the 28th international conference on computa-tional linguistics, pages 4089–4099..jiwei li, michel galley, chris brockett, georgios sp-ithourakis, jianfeng gao, and bill dolan.
2016. ain pro-persona-based neural conversation model.
ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 994–1003..jiwei li, will monroe, tianlin shi, s´ebastien jean,alan ritter, and dan jurafsky.
2017. adversariallearning for neural dialogue generation.
in proceed-ings of the 2017 conference on empirical methodsin natural language processing, pages 2157–2169..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..douglas w maynard.
1980..placement of topicchanges in conversation.
semiotica, 30(3-4):263–290..lorenza mondada.
2003. parler topical et organisations´equentielle: l’apport de l’analyse conversationnelle.
verbum (nancy), (2):193–219..jekaterina novikova, ondˇrej duˇsek, amanda cercascurry, and verena rieser.
2017. why we need newin proceedings of theevaluation metrics for nlg.
2017 conference on empirical methods in naturallanguage processing, pages 2241–2252..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..aasish pappu, roi blanco, yashar mehdad, amandastent, and kapil thadani.
2017. lightweight multi-lingual entity extraction and linking.
in proceedingsof the tenth acm international conference on websearch and data mining, wsdm 17, new york, ny,usa.
acm..sylvie porhiel.
2005. les marqueurs de th´ematisation:des th`emes phrastiques et textuels.
travaux de lin-guistique, (2):55–84..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191. association for computational linguistics..rashmi prasad, nikhil dinesh, alan lee, eleni milt-sakaki, livio robaldo, aravind joshi, and bon-nie webber.
2008. the penn discourse treebankthe sixth international2.0.conference on language resources and evaluation(lrec’08), marrakech, morocco.
european lan-guage resources association (elra)..in proceedings of.
matthew purver, arash eshghi, and julian hough.
incremental semantic construction in a dia-2011.in proceedings of the ninth inter-logue system.
national conference on computational semantics(iwcs 2011)..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..ehud reiter.
2018. a structured review of the validityof bleu.
computational linguistics, 44(3):393–401..marine riou.
2015. a methodology for the identiﬁca-tion of topic transitions in interaction.
discours.
re-vue de linguistique, psycholinguistique et informa-tique.
a journal of linguistics, psycholinguistics andcomputational linguistics, (16)..lorenza mondada.
2001. gestion du topic et organi-sation de la conversation.
cadernos de estudos lin-guisticos, 41:7–36..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge..2501fatemeh torabi asr and vera demberg.
2012. implicit-ness of discourse relations.
in proceedings of col-ing 2012, pages 2669–2684, mumbai, india.
thecoling 2012 organizing committee..fatemeh torabi asr and vera demberg.
2013. onthe information conveyed by discourse markers.
inproceedings ofthe fourth annual workshop oncognitive modeling and computational linguistics(cmcl), pages 84–93, soﬁa, bulgaria.
associationfor computational linguistics..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566–4575..oriol vinyals and quoc le.
2015. a neural conversa-tional model.
arxiv preprint arxiv:1506.05869..bonnie webber, rashmi prasad, alan lee, and ar-avind joshi.
2019. the penn discourse treebank3.0 annotation manual.
technical report..pengcheng yang, lei li, fuli luo, tianyu liu, andxu sun.
2019. enhancing topic-to-essay generationwith external commonsense knowledge.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 2002–2012,florence, italy.
association for computational lin-guistics..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018. per-sonalizing dialogue agents: i have a dog, do youin proceedings of the 56th an-have pets too?
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 2204–2213..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt: large-scalegenerative pre-training for conversational responsegeneration.
in acl, system demonstration..hao zhou, tom young, minlie huang, haizhou zhao,com-jingfang xu, and xiaoyan zhu.
2018.monsense knowledge aware conversation generationwith graph attention.
in proceedings of the twenty-seventh international joint conference on artiﬁcialintelligence, ijcai-18, pages 4623–4629..a otters data collection.
in order to avoid collecting noisy or out-of-taskdata, we established some worker requirements forturkers participating in our data collection.
workersneeded to:.
• be masters (label assigned by mechanicalturk to workers who achieve excellenceacross a variety of tasks),.
• have a number of hits approved greater than.
500,.
• have a hit approval rate (%) greater than 80,.
• being located in an english speaking coun-try, namely australia, canada, new zealand,united kingdom, and united states..a worker received a reward of $0.3 for eachcompleted assignment.
the reward was calculatedbased on an estimate of the time it would takea worker to read the instructions and completethe task.
the time for completing the task hasbeen estimated at 1.5 minutes, and the reward wascalculated accordingly to a $12 hourly payment.
each task had been assigned to 3 unique workers.
figure 4 shows the instructions that workerswere presented after opening the otters data col-lection task.
the instructions explain that the con-text is a conversation with a newly-met person.
af-ter writing the sentence for transitioning the currenttopic to the ‘ﬁnal’ one, workers are asked to list thetopics they covered for the transition.
additionally,we provided an example:.
current sentence: ‘i have a love of reptiles.’final sentence: ‘i want to travel to nyc.’topic shifting sentence: ‘i know there is a coolsnake species in the new york zoo.
this is why iwant to travel to nyc.’covered topics:.
• reptiles.
• zoo.
• nyc.
2502figure 4: instructions shown to workers for the otters data collection..current sentencetopic transitionfinal sentencecovered topics.
i like the pool.
i reward myself after going to the pool by eating a hearty meal.
i like vegetables.
• pool• hearty meal• vegetables.
table 9: example of topic transition collected for otters..i am a parent..current sentencetopic transition my kids each have a pet they take care of.
final sentencecovered topics.
i like animals.
• parent• kids• pet• animals.
table 10: example of topic transition collected for otters..current sentencei have gone across the ocean.
topic transition my mom was in a band when she lived in france.
final sentencecovered topics.
my mom is famous.
• ocean• mom• band• france• famous.
table 11: example of topic transition collected for otters..2503current sentencetopic transition.
final sentencecovered topics.
i love cuddling with my babies.
i wish i could still do that with my children, but after my back injury i have had tolisten to my physician and really change my ways.
the dr said no sitting up for me.
• children• medical care• disability.
table 12: example of topic transition collected for otters..current sentencetopic transition.
final sentencecovered topics.
i like going to concerts.
i like going to concerts which means i normally have to take a breakduring a workweek.
i do not go a full week of employment without a break.
• concerts• break• work.
table 13: example of topic transition collected for otters..current sentencetopic transitionfinal sentencecovered topics.
i play basketball and football.
i bet my ﬁancee while at the park playing basketball.
my signiﬁcant other and i will be having a wedding.
• basketball• football• fiancee• park• wedding.
table 14: example of topic transition collected for otters..2504