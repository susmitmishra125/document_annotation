data augmentation with adversarial training for cross-lingual nli.
xin dong1, yaxin zhu1, zuohui fu1, dongkuan xu2, gerard de melo31 rutgers university2 the pennsylvania state university3 hasso plattner institute / university of potsdam{xd48, yz956, zuohui.fu}@rutgers.edudux19@psu.edu, gdm@demelo.org.
abstract.
due to recent pretrained multilingual represen-tation models, it has become feasible to exploitlabeled data from one language to train a cross-lingual model that can then be applied to mul-tiple new languages.
in practice, however, westill face the problem of scarce labeled data,in this paper, weleading to subpar results.
propose a novel data augmentation strategy forbetter cross-lingual natural language inferenceby enriching the data to reﬂect more diversityin a semantically faithful way.
to this end, wepropose two methods of training a generativemodel to induce synthesized examples, andthen leverage the resulting data using an ad-versarial training regimen for more robustness.
in a series of detailed experiments, we showthat this fruitful combination leads to substan-tial gains in cross-lingual inference..1.introduction.
there is a growing need for nlp systems thatsupport low-resource languages, for which task-speciﬁc training data may be lacking, whiledomain-speciﬁc parallel corpora may be too scarceto train a reliable machine translation engine.
toovercome this, zero-shot cross-lingual systems canbe trained on a source language ls and subse-quently also be applied to other languages lt de-spite a complete lack of labelled training data forthose target languages.
in the past, such systemstypically drew on translation dictionaries, lexicalknowledge graphs, or parallel corpora, to build across-lingual model that exploits simple connec-tions between words and phrases across differentlanguages (de melo and siersdorfer, 2007; fu et al.,2020).
recently, pretrained language model archi-tectures such as bert (devlin et al., 2019) havebeen shown capable of learning joint multilingualrepresentations with self-supervised objectives un-der a shared vocabulary, simply by combining the.
input from multiple languages (devlin et al., 2019;artetxe and schwenk, 2019; conneau and lample,2019; conneau et al., 2019).
such representationsgreatly facilitate cross-lingual applications.
still,the success of such cross-lingual transfer hinges onhow close the involved languages are, with substan-tial drops observed for some more distant languagepairs (lauscher et al., 2020)..for our study, we focus on natural language infer-ence (nli), i.e., classifying whether a premise sen-tence entails, contradicts, or is neutral with regardto a hypothesis sentence (williams et al., 2017).
this is a useful building block for applications in-volving semantic understanding (zhu et al., 2018;reimers and gurevych, 2019).
however, the taskis also very challenging, as it not only requiresaccounting for very subtle differences in meaningbut also inferring presuppositions and implicationsthat are not explicitly stated.
due to these intricatesubtleties, zero-shot cross-lingual models are oftenfairly brittle, while obtaining in-language trainingdata is fairly costly..data augmentation.
to boost the performanceof cross-lingual models, an intuitive thought is todraw on unlabeled data from the target languageso as to enable the model to better account for thespeciﬁcs of that language, rather than just beingﬁne-tuned on the source language.
a natural wayof exploiting unlabeled data is to consider standardsemi-supervised learning methods that leverage amodel’s own predictions on unlabeled target lan-guage inputs (dong and de melo, 2019).
how-ever, this strategy fails when the predictions aretoo noisy to serve as reliable training signals.
inthis paper, we hence explore data augmentationto circumvent this problem.
the idea, widespreadin computer vision and speech recognition, is togenerate new training data from existing labeleddata.
for images, a common approach is to apply.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5158–5167august1–6,2021.©2021associationforcomputationallinguistics5158transformations such as rotation and ﬂipping, asthese typically preserve the original label assignedto an image (krizhevsky et al., 2012).
for text,in contrast, data augmentation is more challeng-ing, and straightforward techniques include simpleoperations on words within the original trainingsequences, such as synonym replacement, randominsertion, random swapping, or random deletion(wei and zou, 2019).
in practice, however, thereare two notable problems.
one is that the synthe-sized data from data augmentation techniques mayas well be noisy and unreliable.
second, new ex-amples may diverge from the distribution of theoriginal data..on nli, these problems are particularly pro-nounced, as the very nature of this task is to accountfor subtle differences between sentences.
modiﬁedversions of the original sentences may no longerhave the same meaning and entailments.
hence,existing data augmentation techniques often fail toboost the result quality..overview and contributions.
in this paper, wepropose a novel data augmentation scheme to syn-thesize controllable and much less noisy data forcross-lingual nli.
this augmentation consists oftwo parts.
one serves to encourage language adap-tation by means of reordering source languagewords based on word alignments to better copewith typological divergency between languages,denoted as reorder augmentation (ra).
anotherseeks to enrich the set of semantic relationships be-tween a premise and pertinent hypotheses, denotedas semantic augmentation (sa).
both are achievedby learning corresponding sequence-to-sequence(seq2seq) models..the resulting samples along with their new la-bels serve as an enriched training set for the ﬁnalcross-lingual training.
during this phase, we in-voke a special adversarial training regimen thatenables the model to better learn from such au-tomatically induced training samples and transfermore information to the target languages while bet-ter bridging the gap between typologically distinctlanguages.
our empirical study demonstrates thenecessity of incorporating adversarial training intotraining with synthetic samples and the superiorityof our new augmentation method on cross-lingualnatural language inference (conneau et al., 2018).
remarkably, our cross-lingual approach even out-performs in-language supervised learning..2 method.
our proposed method consists of two steps.
theﬁrst involves inducing training examples with twodata augmentation models.
next, a task-speciﬁcclassiﬁer is trained on both the original and thenewly generated training instances, with adversar-ial perturbation for improved robustness and gener-alization..2.1 data augmentation model.
2.1.1 reorder augmentationreorder augmentation is based on the intuition ofmaking a model more robust with respect to dif-ferences in word order typology.
if our trainingexamples consist entirely of instances from a lan-guage ls with a fairly strict subject–verb–object(svo) word order such as english, the model willbe less well equipped to pay attention to subtle se-mantic differences between sentences from a targetlanguage lt obeying subject–object–verb (sov)order.
to alleviate this problem, we can rely onauxiliary data to diversify the training data.
forthis, we obtain word alignments for unannotatedbilingual parallel sentence pairs covering ls and anauxiliary language la that need not be the same aslt. we then reorder all source sentences to matchthe word order of la based on the alignments, andtrain a model to apply such reordering on the nlitraining instances..figure 1: illustration of using a word-aligned parallelcorpus for reordering a source language text..formally, suppose we have obtained l unlabelledparallel sentences in the source language ls andin the auxiliary language la, c = {((cid:104)si, ai(cid:105) |i = 1, ..., l}, where (cid:104)s, a(cid:105) is a source–auxiliarylanguage sentence pair.
based on a word alignmentmodel, in our case fastalign (dyer et al., 2013),which uses expectation maximization to computethe lexical translation probabilities, we obtain aword pair table for each sentence pair (cid:104)s, t(cid:105), de-noted as a(s, a) = {(i1, j1), ..., (im, jm)}..following the word order of la, we then re-order the source sequence s by consulting the table.
5159source𝑥!𝑥"𝑥#𝑥$𝑥%𝑥&𝑥’𝑥(𝑥)𝑥*𝑥"!𝑥""𝑥"#word alignmenttarget𝑦!𝑦"𝑦#𝑦$𝑦%𝑦&𝑦’𝑦(𝑦)𝑦*𝑦"!𝑦""𝑦"#𝑦"$reordered source𝑥!𝑥"𝑥#𝑥(𝑥$𝑥%𝑥)𝑥*𝑥"!𝑥’𝑥"#a(s, t), yielding the new sentence pair (cid:104)s, ¯s(cid:105).
next,we consider a pretrained seq2seq model, denotedas r(·; θ).
the model is assumed to have beenpretrained with an encoder and a decoder in thesource language, and we ﬁne-tune this generativemodel by training on the new parallel corpus ¯c ={((cid:104)si, ¯si(cid:105) | i = 1, ..., l}.
this generative seq2seqmodel can then reorder the sequences in the la-beled training dataset d = {(xi, yi) | i = 1, ..., n},where n is the number of labeled instances, eachxi consists of a sequence pair (cid:104)s1, s2(cid:105), and eachyi ∈ y is the corresponding ground truth labeldescribing their relationship..2.1.2 semantic augmentationour second augmentation strategy involves traininga controllable model that, given a sentence and alabel describing the desired relationship, seeks toemit a second sentence that stands in said relation-ship to the input sentence.
thus, given an existingtraining sentence pair, we can consider differentvariations of one sentence in the pair and invoke themodel to generate a suitable second sentence.
how-ever, such automatically induced samples from saare inordinately noisy, precluding their immediateuse as training data, so we exploit a large pretrainedteacher model trained on available source languagesamples to rectify the labels of these synthetic sam-ples with appropriate strategies..generation.
as we wish to be able to control thelabel of a generated example, the requested label isprepended to the input as a (textual) preﬁx before itis fed into a seq2seq model.
we adopt the ground-truth label of each example as the respective preﬁx,resulting in a new input sequence (yi : s1) coupledwith s2 as the desired output forming a trainingpair for the generation model..given the resulting labeled training dataset dsa,we can ﬁne-tune a pretrained seq2seq model, de-noted as g(·; θ).
this generative seq2seq modelcan then be invoked for semantic data augmen-tation to generate new training instances.
foreach (¯y : s1) as a labeled input sequence, where¯y ∈ y \ {yi}, we generate an ˜s2 via the ﬁne-tunedseq2seq model, yielding a new training instance((cid:104)s1, ˜s2(cid:105), ¯y)..label rectiﬁcation.
the semantic augmentationinduces ˜s2 automatically based on s1 and the re-quested label ¯y.
however, the obtained ˜s2 may notalways genuinely have the desired relationship ¯y tos1.
thus, we treat this data as inherently noisy and.
propose a rectifying scheme based on a teachermodel.
we wish for this teacher to be as accurateas possible, so we start off with a large pretrainedlanguage model speciﬁcally for the source lan-guage ls, which we assume obtains a better perfor-mance on ls than a pretrained multilingual model.
we train the teacher network h(·; θ) in k epochsusing the set of original labeled data d. thisteacher model is then invoked to verify and poten-tially rectify labels from the automatically inducedaugmentation data d˜a = {(˜xi, yi) | i = 1, ..., m}obtained in the previous step (where m is the num-ber of instances).
we assume ( ˜yi, c) = h(˜xi; θ)denotes the predicted label along with the conﬁ-dence score c ∈ [0, 1] emitted by the classiﬁer, andassume a conﬁdence threshold t has been prede-termined.
there are several strategies to determinethe ﬁnal labels..• teacher strategy: we adopt dr = {(˜xi, ˜yi) |(˜xi, yi) ∈ d˜a, ( ˜yi, c) = h(˜xi), c > t }, i.e.,when the conﬁdence score is above t , we be-lieve the teacher model is sufﬁciently conﬁdentto ensure a reliable label, while other instancesare discarded..• tr strategy: an alternative scheme is to in-stead adopt dr = {(˜xi, φ(yi, ˜yi, c)) | (˜xi, yi) ∈d˜a, ( ˜yi, c) = h(˜xi)}, where.
φ(yi, ˜yi, c) =.
(cid:40).
˜yiyi.
c > totherwise.
here, labels remain unchanged when teacherpredictions match the originally requested labels.
in case of an inconsistency, we adopt the teachermodel’s label if it is sufﬁciently conﬁdent, andotherwise retain the requested label..2.2 adversarial training.
upon completing the two kinds of data augmenta-tion, we possess synthesized data that is substan-tially less noisy, denoted as dr, which can be incor-porated into the original training data d to yield theﬁnal augmented training set da = d ∪ dr. withthis, we proceed to train a new model f (·; θ) forthe ﬁnal cross-lingual sentence pair classiﬁcation.
as a special training regimen, we adopt adversar-ial training, which seeks to minimize the maximalloss incurred by label-preserving adversarial per-turbations (szegedy et al., 2014; goodfellow et al.,2015), thereby promising to make the model morerobust.
nonetheless, the gains observed from it.
5160in practice have been somewhat limited in bothmonolingual and cross-lingual settings.
we conjec-ture that this is because it has previously merelybeen invoked as an additional form of monolingualregularization (miyato et al., 2017)..in contrast, we hypothesize that adversarial train-ing is particularly productive in a cross-lingualframework when used to exploit augmented data, asit encourages the model to be more robust towardsthe divergence among similar words and word or-ders in different languages and to better adapt tothe new modestly noisy data.
this hypothesis islater conﬁrmed in our experimental results..adversarial training is based on the notion ofﬁnding optimal parameters θ to make the modelrobust against any perturbation r within a normball on a continuous multilingual (sub-)word em-bedding space.
hence, the loss function becomes:.
ladv(xi, yi) = l(f (xi + radv(xi, yi); θ), yi).
where radv(xi, yi) = argmaxr,||r||≤(cid:15).
(1)l(f (xi + r; ˜θ), yi).
generally, a closed form for the optimal perturba-tion radv(xi, yi) cannot be obtained for deep neu-ral networks.
goodfellow et al.
(2015) proposedapproximating this worst case perturbation by lin-earizing f (xi; ˜θ) around xi.
with a linear approx-imation and an l2 norm constraint in equation 2,the adversarial perturbation is.
radv(xi, yi) ≈ (cid:15).
g(xi, yi)||g(xi, yi)||2.
(2).
where g(xi, yi) = ∇xil(f (xi; ˜θ), yi)..however, neural networks are typically not lineareven over a relatively small region, so this approxi-mation cannot guarantee to achieve the best optimalpoint within the bound.
madry et al.
(2017) demon-strated that projected gradient descent (pgd) al-lows us to ﬁnd a better perturbation radv(xi, yi).
in particular, for the norm ball constraint ||r|| ≤ (cid:15),given a point r0, π||r||≤(cid:15) aims to ﬁnd a perturbationr that is closest to r0 as follows:.
π||r||≤(cid:15)(r0) = argmin||r||≤(cid:15).
||r − r0||.
(3).
to ﬁnd more optimal points, k-step pgd isneeded during training, which requires k forward–backward passes through the network.
with a lin-ear approximation and an l2 norm constraint, pgd.
takes the following step in each iteration:.
(cid:18).
rt+1 = π||r||≤(cid:15).
rt + α.g(xi, yi, rt)||g(xi, yi, rt)||2).
(cid:19).
(4).
where g(xi, yi, rt) = ∇rtl(f (xi + rt; ˜θ), yi).
here, α is the step size and t is the step index..3 experiments and analysis.
3.1 experimental setup.
tasks and datasets.
for evaluation, we usedxnli (conneau et al., 2018), the most promi-nent cross-lingual natural language inference cor-pus, which extends the multinli dataset (williamset al., 2017) to 15 languages.
in our experiments,we considered 20k training data, i.e., ∼5% of theoriginal training size to study lower-resource set-tings requiring augmentation.
following previouswork, we consider english as the source languagein our experiments..model details.
to show that our reorder aug-mentation strategy does not require auxiliary datafrom a low-resource target language, we only giveit access to parallel data for another closely re-lated high-resource language.
speciﬁcally, we usethe english–german bilingual parallel corpus fromjw300 (agi´c and vuli´c, 2019).
like english, ger-man commonly adopts an svo word order, but insome instances also mandates sov and is generallyless rigid than english.
this allows us to demon-strate the utility of reorder augmentation even inthe absence of data from a language similar to thetarget language.
we relied on fastalign1 to induce200k training pairs for seq2seq ﬁne-tuning on re-ordering..as the pre-trained seq2seq model, we usedgoogle’s t5-base (raffel et al., 2020), a uniﬁedtext-to-text transformer, to generate new trainingexamples.
during generation, we set the beam sizeas 1 and use sampling instead of greedy decoding.
for the teacher model in semantic augmentation,we relied on roberta-large (liu et al., 2019), arobustly optimized bert model, to ﬁne-tune nlion english.
as the multilingual model, we employxlm-roberta-base (xlm-r) (conneau et al.,2019), trained on over 100 different languages.
forpgd, the step size α, norm constraint size (cid:15), andnumber of steps k are 1.0, 3.0, 3, respectively.
allhyperparameter tuning is conducted based on the.
1https://github.com/clab/fast align.
5161table 1: hyper-parameters for pretrained models..parametermax.
sequence lengthtraining batch sizelearning ratemax.
grad.
norm.
roberta128161e-5-.
t5 xlm-r1281503281e-53e-4-1.0.accuracy on the english validation set.
the teacherstrategy for xnli then is used for the rectiﬁcationof semantically augmented texts, as inference re-quires particularly clean data.
the threshold t forthis is 0.8. an overview of the basic network pa-rameter values is given in table 1. we rely on earlystopping as a termination criterion.
for all nliclassiﬁcation results, we randomly repeat each ex-periment 5 times and report the averaged accuracy..3.2 main results.
cross-lingual inference classiﬁcation.
table 2compares our approach against several strong base-lines on xnli.
the ﬁrst part considers in-languagesupervised learning, where we relied on genuinetraining data from the target language rather thana cross-lingual setting.
these results are merelyprovided for comparison.
the second part consid-ers zero-shot cross-lingual transfer, i.e., the settingwe are targeting in this paper: we ﬁrst used en-glish training data to train the xlm-r model andthen applied it to non-english languages withoutany training data in the target language.
we alsotrained the model with pgd adversarial training toassess how well pgd works without any data aug-mentation.
next, we evaluate xlm-r when trainedon original and augmented examples from severalaugmentation methods, with and without adversar-ial training, respectively.
the ﬁrst of these is easyaugmentation (ea) by wei and zou (2019), a state-of-the-art method for data augmentation in nlp.
itmixes 4 strategies, namely synonym replacement,random insertion, random swapping, and randomdeletion, applying each of these to 20% of words ina sentence.
additionally, we consider our proposedra and sa strategies, as well as combinations ofea or ra with sa..compared with vanilla xlm-r without adver-sarial training, xlm-r with pgd works betteracross a range of non-english languages, whichshows the effectiveness of adversarial training formore robustness in cross-lingual settings.
we ob-serve that xlm-r, when trained with ea or ra,outperform the setting without augmentation forenglish and some non-english languages, though.
it does not achieve sufﬁciently stronger results interms of the average accuracy across different lan-guages.
this suggests that xlm-r struggles tobeneﬁt from the augmented instances from ra forbetter generalizability.
in contrast, when trainedwith sa, xlm-r performs better than without saexamples for most languages, conﬁrming that oursemantic augmentation is beneﬁcial.
remarkably,xlm-r with sa examples even succeeds at out-performing in-language training with an averageabsolute improvement of about 1.1% in accuracy,suggesting that cross-lingual models trained withautomatically generated english examples can bemore informative with regard to inference than tar-get language examples.2 next, we also observethat the accuracy of xlm-r with additional exam-ples from ea, ra, sa is boosted with pgd.
thissuggests that adversarial training is particularly use-ful to boost generalizability and robustness whenoperating on artiﬁcial augmented examples..beyond this, our full zero-shot approach furtheroutperforms all baselines across 14 languages, in-cluding in-language training.
this demonstratesthe value of improving generalizability and robust-ness by adding diverse forms of augmentation in anadversarial training framework that can cope withnoisy examples..3.3 ablation studies and analysis.
comparisons on different rectifying strate-gies.
one key part of our method is the labelrectiﬁcation mechanism.
we compare differentrectiﬁcation strategies in table 3. the results showthat the teacher and tr methods introduced insection 2.1.2 yield fairly similar results.
this con-ﬁrms the robustness of our approach with regard tothe choice of strategy.
the same also holds for anadditional option, agreement, which retains onlythose examples on which the prediction from theteacher agrees with the originally requested label.
finally, for comparison, we evaluated yet anotherstrategy, requested, which always adopts the orig-inally requested labels as chosen for generation.
we ﬁnd that this strategy introduces overly manyunreliable labels, so the model is unable to workwell.
this conﬁrms that rectifying labels with ateacher model is a crucial ingredient..comparisons on adversarial perturbations.
for assessing the value of pgd for adversarial per-.
2note that the in-language training data in xnli was cre-.
ated using machine translation..5162table 2: accuracy (in %) on xnli with augmented examples used for cross-lingual transfer.
the number ofaugmented examples from ea, ra and sa are 80k, 20k, 80k.
ea (wei and zou, 2019) is easy data augmentation.
the best cross-lingual transfer results under xlm-r are given in boldface..ru.
ar.
sw.ur.
bg.
el.
th.
tr.
vi.
hi.
avg.
f rapproachin-language supervised learning (translate–train).
zh.
en.
de.
es.
robertambertxlm-r.88.273.377.7.zero-shot cross-lingual transfer.
65.270.6.
69.073.0.
66.568.1.
66.572.8.
64.870.6.
61.767.4.
57.761.8.
56.360.5.
65.873.2.
63.471.0.
49.368.9.
61.569.3.
66.970.2.
59.364.9.
63.169.3.
77.7xlm-r78.9+pgd77.8+ea(80k)78.4+ra(20k)79.5+sa(80k)77.9+ea+pgd78.9+ra+pgd+sa+pgd80.4+ea+sa+pgd 80.0+ra+sa+pgd 80.8.
71.771.870.371.072.071.972.573.474.074.5.
72.674.573.173.174.474.474.775.776.177.3.
69.570.269.267.369.671.171.171.873.073.6.
72.773.572.973.074.173.574.574.075.575.8.
70.271.170.370.271.971.572.073.173.974.9.
67.767.367.567.167.568.868.669.370.270.0.
60.760.761.661.563.663.363.164.563.764.8.
61.062.063.561.162.764.463.663.765.565.7.
72.072.972.171.973.674.173.374.575.476.3.
70.271.370.170.371.968.37273.273.374.9.
67.468.768.165.569.069.569.070.370.571.6.
69.069.268.767.569.268.969.970.271.471.4.
71..071.369.569.571.070.471.772.372.974.5.
64.964.965.164.766.166.965.966.968.068.5.
69.169.969.368.870.470.370.771.572.273.0.table 3: accuracy (in %) on xlni with different rectifying strategies, training on xlm-r with sa and pgd.
tis the threshold.
p denotes the percentage of initial augmented examples retained for training..approachteacher (t = 0)teacher (t = 0.8)tr (t = 0.8)agreementrequested.
p.en100% 79.794% 80.4100% 79.166% 78.7100% 75.4.de72.873.472.971.367.5.es75.675.775.374.570.1.zh71.771.871.470.869.0.f r73.974.074.172.768.0.ru73.073.173.171.769.2.ar69.369.368.868.765.7.sw64.564.564.163.861.1.ur63.863.763.662.661.6.bg74.074.573.973.070.5.el72.673.273.172.068.3.th69.870.370.469.765.9.tr70.070.270.469.468.3.vi71.872.372.071.170.6.hi66.566.966.665.964.1.avg71.371.671.370.467.7.effectiveness on different training sizes.
data augmentation is an important approach todeal with scarce labels.
the results in table4 further show that when ﬁne-tuning t5 using10k xnli training instances with 80k semanticand 10k reorder augmented examples, we obtainsubstantially better results than when using 20ktraining instances without augmentation.
we canalso observe the improvement of xlm-r with ra,sa, and adversarial training over vanilla xlm-ron each language as plotted in figure 2. therelative gains with 10k training data are larger thanwith 20k training data across a range of languages,which shows that our method is consistently mostbeneﬁcial when training data is scarce..inﬂuence of amount of augmentation.
to as-sess the role of the amount of data augmentation,we conducted experiments on xnli with 20k train-ing examples, and evaluated the effect of addingeither 20k or 80k augmented examples from ea,ra, sa.
the results are given in table 5. whentrained without pgd, one can often beneﬁt fromusing up to 80k augmented examples.
due to theinherent reordering differences between englishand german, there are limits regarding the amountof such data one ought to incorporate.
we ﬁnd that20k instances from ra can sufﬁce.
we observe.
figure 2: relative improvements of xlm-r with aug-mentation and pgd over xlm-r. blue refers to the im-provement on 10k original instances plus 80k sa and10k ra, while orange refers to the improvement on 20koriginal instances plus 80k sa and 20k ra, and browndesignates the overlap between blue and orange..turbation, table 4 compares pgd with the standardfast gradient method (fgm) for adversarial per-turbation (goodfellow et al., 2015) as introduced insection 2.2. we ran experiments on xnli with 10kand 20k training data, each augmented with 80kinduced semantic examples.
we observe that fgmobtains a lower average accuracy than pgd withthe same amount of training data, conﬁrming thesuperiority of pgd in providing better adversarialperturbations than fgm to improve both general-ization and robustness..5163 h q g h h v ] k i u u x d u v z x u e j h o w k w u y l k l / d q j x d j h        , p s u r y h p h q w      .    .    .   .    .    .
table 4: accuracy (in %) on xnli experiments with different amounts of training and augmentation data, anddifferent adversarial training methods..table 5: accuracy (in %) on xnli experiments trained using 20k vs. 80k augmentation data from ea, ra, sa,with and without pgd..enapproachxlm-r (10k)74.5+sa +fgm77.578.2+sa +pgd+ra +sa +pgd 79.16.2improvement(%)77.7xlm-r (20k)+sa +fgm79.380.4+sa +pgd+ra +sa +pgd 80.84.0improvement(%).
approachxlm-r (20k)+ea (20k)+ea (80k)+ra (20k)+ra (80k)+sa (20k)+sa (80k)+pgd+ea +pgd (20k)+ea +pgd (80k)+ra +pgd (20k)+ra +pgd (80k)+sa +pgd (20k)+sa +pgd (80k).
en77.777.477.878.477.578.279.578.977.677.978.978.479.380.4.de68.070.971.473.07.470.072.473.474.56.4.de71.769.170.371.070.870.672.071.870.971.972.571.973.373.4.es70.373.673.775.27.072.574.775.777.36.6.es72.671.973.173.173.372.874.474.573.974.474.774.974.075.7.zh65.568.370.872.310.069.270.671.873.66.4.zh69.567.569.267.368.167.369.670.269.871.171.171.069.471.8.f r70.873.173.173.64.072.773.774.075.84.3.f r72.771.672.973.072.272.674.173.573.073.574.573.773.374.0.ru68.070.771.272.56.670.671.873.174.96.1.ar64.267.368.169.27.866.967.669.370.04.6.sw61.162.262.364.55.661.663.564.564.85.2.ur60.262.263.263.75.860.863.063.765.78.1.bg69.972.873.674.56.672.072.974.576.36.0.el68.970.571.872.34.970.271.973.274.96.7.th65.068.468.970.07.766.768.370.371.67.3.tr66.967.369.270.75.768.769.370.271.43.9.ru70.269.370.370.270.370.371.971.171.171.572.071.971.073.1.ar67.765.567.567.166.866.567.567.367.168.868.668.767.669.3.sw60.761.061.661.560.761.463.660.762.463.363.162.662.764.5.ur61.061.563.561.160.360.462.762.063.864.463.664.062.463.7.bg72.071.172.171.972.571.873.672.973.074.173.373.473.774.5.el70.269.270.170.370.569.671.971.371.368.37272.171.773.2.th67.467.168.165.566.066.969.068.768.969.569.068.968.370.3.tr69.067.168.767.567.667.669.269.269.168.969.969.969.2870.2.vi68.470.271.172.76.370.671.672.374.55.5.vi71..068.869.569.569.369.571.071.371.270.471.771.971.172.3.hi61.564.965.667.29.364.966.666.968.55.5.hi64.963.965.164.763.364.066.164.965.866.965.966.465.666.9.avg66.969.370.171.46.769.070.571.673.05.8.avg69.168.169.368.868.668.670.469.969.970.370.770.470.271.5.that ea with pgd requires up to 80k augmented in-stances, i.e., 3 times the size of the original trainingdata, to outperform xlm-r with pgd, whereasonly 20k augmented examples sufﬁce for ra withpgd to beat xlm-r with pgd..case studies.
to better illustrate the principlesof our data augmentation technique, we provideseveral examples.
table 6 shows two examples ofthe three data augmentation processes on xnli.
for the ﬁrst example, the original label is contra-diction, so entailment and neutral serve as re-quested labels to generate new training text.
next,our teacher model attempts to rectify these labels.
although our generative model treats vrenna andi fought him in a ﬁght, but he had just gotten usas neutral to s1 (vrenna and i both fought himand he nearly took us), the teacher model changesthe label to entailment.
for the second example,both the generative and teacher model are unableto conclude that the rice ripens in the summer iscontradictory with the premise.
from the two eaoutputs, we can observe him is randomly deleted inexample (1) and the and rice is swapped in exam-ple (2), which loses some information, whereas ra.
seq2seq generated examples maintain all crucialinformation despite the reordering..4 related work.
data augmentation.
data augmentation is apromising technique, especially when dealing withscarce data, imbalanced data, or semi-supervisedlearning problems.
back-translation (sennrichet al., 2015) has been considered as a techniqueto obtain alternative examples preserving the origi-nal semantics, by translating an existing examplein language la into another language lb and thentranslating it back into la to obtain an augmentedexample.
yu et al.
(2018) and xie et al.
(2020) ap-plied it to question answering and semi-supervisedmonolingual training scenarios.
however, this re-quires high-quality translation engines that oftendo not exist in the settings in which one wishes toapply cross-lingual systems..wei and zou (2019) instead combined synonymreplacement, random insertion, random swapping,and random deletion in a method named eda.
since insertion and deletion may affect the seman-tics of the utterance, some studies opt to control.
5164table 6: examples of xnli data augmentation.
v: version (o: original).
rl: requested label.
l: final (possiblyrectiﬁed) label..v.o.ea.
ra.
(1).
rl.
l.contradiction.
contradiction.
contradiction.
sa entailmentsa.
neutral.
entailmententailment.
o.contradiction.
(2).
ea.
contradiction.
–.
–.
–.
–.
–.
–.
ra.
contradiction.
sa entailmentsa.
neutral.
entailmententailment.
texts1: vrenna and i both fought him and he nearly took us.
s2: neither vrenna nor myself have ever fought him.
s1: vrenna and i both fought him and took nearly he us.
s2: neither vrenna nor myself have ever fought.
s1: vrenna and i both him fought and he us nearly took.
s2: neither me nor vrenna have him ever fought.
s2: it was the guy that nearly took the couple of us.
s2: vrenna and i fought him in a ﬁght, but he had just gotten us.
s1: in summer the rice forms a green velvety blanket, thenturns golden in autumn when it ripens and is harvested.
s2: the rice is golden and harvestable in the summer, but turns green in autumn.
s1: harvested summer the rice forms a green velvety blanket thenturns golden in autumn when is ripens and it in.
s2: the the is golden and harvestable in rice summer, but turns green in autumn.
s1: in summer forms the rice a green velvety blanket, turnsthen in autumn golden when it ripens and harvested is.
s2: the rice is golden and harvestable in the summer, but turns in autumn green.
s2: the rice turns golden in autumn when it ripens.
s2: the rice ripens in the summer and then turns golden in the autumn..the selection of words to be replaced with indi-cators such as tf-idf scores (xie et al., 2020).
fadaee et al.
(2017) use contextualized word em-beddings to replace the target word.
kobayashi(2018) proposed a bi-directional language-model-based augmentation method, and wu et al.
(2019)further improved its results by switching to bert.
another major category is text generation basedaugmentation.
anaby-tavor et al.
(2020) proposeda language model based data augmentation method,shown to improve classiﬁer performance on a vari-ety of english datasets.
it relies on gpt-2 (radfordet al., 2018) to generate a single new sequence ineach instance..our work, in contrast, presents a novel augmen-tation scheme designed to cope with the specialchallenges of sentence pair classiﬁcation, where aseq2seq transformer enables augmentation basedon a paired input sentence.
our method also in-troduces a teacher model to rectify labels.
apartfrom this, we expand the idea of language modelbased augmentation to cross-lingual settings andleverage noisy instances with adversarial training..adversarial training.
many approaches for im-proving the robustness of a machine learning sys-tem against adversarial perturbations (szegedyet al., 2014) have been advanced.
goodfellow et al.
(2015) proposed a fast gradient method based onlinear perturbation of non-linear models.
later,madry et al.
(2017) presented pgd-based adver-sarial training through multiple projected gradient.
ascent steps to adversarially maximize the loss.
innlp, belinkov and bisk (2017) exploited structure-invariant word manipulation and robust trainingiyyeron noisy texts for improved robustness.
et al.
(2018) proposed syntactically controlled para-phrase networks with back-translated data and usedthem to generate adversarial examples.
adversar-ial training also plays a role in improving a neu-ral model’s generalization.
for instance, chenget al.
(2019) used adversarial source examples toimprove a translation model.
dong et al.
(2020)exploit fgm-based adversarial training in self-learning for improved cross-lingual text classiﬁ-cation.
in our setting, we count on adversarialtraining in the word embedding space and showthat pgd-based adversarial training remains effec-tive when the adversarial perturbation is applied tonoisy augmented examples..5 conclusion.
while multilingual pretrained model have enabledbetter cross-lingual learning, we still often en-counter data scarcity issues due to the high costof collecting data, which weakens the generaliza-tion ability of the multilingual model..to address this, this paper proposes a novel dataaugmentation strategy with label rectiﬁcation tobuild synthetic examples, outperforming even mod-els trained with larger amounts of ground-truth data.
we show that we can best learn from such noisyinstances with adversarial training, which enables.
5165the classiﬁer to transfer more information from thesource language to other languages and to becomemore robust.
remarkably, with this, our modelstrained without any target language training data atall are able to outperform models trained fully onin-language training data.
moreover, the amount ofaugmented data from our seq2seq-based reorderaugmentation used in training is much less than thatrequired by the state-of-the-art eda method in or-der to achieve comparable performance.
finally, inour series of follow-up experiments comparing dif-ferent training regimens and variants, one notableﬁnding is that our overall augmented approach caneven outperform non-augmented supervision withtwice as many ground truth labels.
overall, thissuggests our combination of data augmentationwith adversarial training as a valuable way of learn-ing substantially more accurate and more robustmodels without any target-language training data..broader impact.
research on cross-lingual nlp is often motivatedby a desire to provide state-of-the-art advances tolinguistic communities that have been underserved.
such advances may enable better access to informa-tion as well as to products and services.
however,there is a risk that such technological advances maynot always be desired by the relevant communitiesand may indeed also cause harm to them (bird,2020).
moreover, cross-lingual systems in partic-ular may exhibit biases with regard to the sourcelanguage used for training and the general culturalassumptions reﬂected in such data.
in light of this,special care needs to be taken to analyze potentialoutcomes and risks before deploying cross-lingualsystems in real-world applications..references.
ˇzeljko agi´c and ivan vuli´c.
2019. jw300: a wide-coverage parallel corpus for low-resource languages.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages3204–3210, florence, italy.
association for compu-tational linguistics..ateret anaby-tavor, boaz carmeli, esther goldbraich,amir kantor, george kour, segev shlomov, naamatepper, and naama zwerdling.
2020. do not haveenough data?
deep learning to the rescue!
in aaai,pages 7383–7390..mikel artetxe and holger schwenk.
2019. massivelymultilingual sentence embeddings for zero-shot.
cross-lingual transfer and beyond.
transactionsof the association for computational linguistics,7:597–610..yonatan belinkov and yonatan bisk.
2017. syntheticand natural noise both break neural machine transla-tion.
arxiv preprint arxiv:1711.02173..steven bird.
2020. decolonising speech and lan-guage technology.
in proceedings of the 28th inter-national conference on computational linguistics,pages 3504–3519, barcelona, spain (online).
inter-national committee on computational linguistics..yong cheng, lu jiang, and wolfgang macherey.
2019.robust neural machine translation with doubly ad-versarial inputs.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 4324–4333, florence, italy.
associa-tion for computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2019. unsupervisedcross-lingual representation learning at scale.
arxivpreprint arxiv:1911.02116..alexis conneau and guillaume lample.
2019. cross-lingual language model pretraining.
in advances inneural information processing systems, volume 32.curran associates, inc..alexis conneau, guillaume lample, ruty rinott, ad-ina williams, samuel r bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluating cross-arxiv preprintlingual sentence representations.
arxiv:1809.05053..gerard de melo and stefan siersdorfer.
2007. multilin-gual text classiﬁcation using ontologies.
in proceed-ings of ecir 2007. springer..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..xin dong and gerard de melo.
2019. a robust self-learning framework for cross-lingual text classiﬁca-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages6306–6310, hong kong, china.
association forcomputational linguistics..xin dong, yaxin zhu, yupeng zhang, zuohui fu,dongkuan xu, sen yang, and gerard de melo.
2020.leveraging adversarial training in self-learning forin proceedings ofcross-lingual text classiﬁcation..5166the 43rd international acm sigir conference onresearch and development in information retrieval,sigir ’20, page 1541–1544, new york, ny, usa.
association for computing machinery..chris dyer, victor chahuneau, and noah a smith.
2013. a simple, fast, and effective reparameteriza-in proceedings of the 2013tion of ibm model 2.conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 644–648..marzieh fadaee, arianna bisazza,.
and christofmonz.
2017.low-resource neural machine translation.
arxiv preprintarxiv:1705.00440..data augmentation for.
zuohui fu, yikun xian, shijie geng, yingqiang ge,yuting wang, xin dong, guang wang, and gerardde melo.
2020. absent: cross-lingual sentence rep-inresentation mapping with bidirectional gans.
proceedings of the 34th aaai conference on arti-ﬁcial intelligence (aaai 2020).
aaai press..ian goodfellow,.
and christianjonathon shlens,szegedy.
2015. explaining and harnessing adversar-ial examples.
in international conference on learn-ing representations..mohit iyyer, john wieting, kevin gimpel, and lukezettlemoyer.
2018. adversarial example generationwith syntactically controlled paraphrase networks.
arxiv preprint arxiv:1804.06059..sosuke kobayashi.
2018. contextual augmentation:data augmentation by words with paradigmatic re-lations.
arxiv preprint arxiv:1805.06201..alex krizhevsky, ilya sutskever, and geoffrey e hin-ton.
2012.imagenet classiﬁcation with deep con-volutional neural networks.
in advances in neuralinformation processing systems, pages 1097–1105..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on thelimitations of zero-shot language transfer with mul-tilingual transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 4483–4499, on-line.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..aleksander madry, aleksandar makelov, ludwigschmidt, dimitris tsipras, and adrian vladu.
2017.towards deep learning models resistant to adversar-ial attacks.
arxiv preprint arxiv:1706.06083..takeru miyato, andrew m. dai, and ian goodfel-low.
2017. adversarial training methods for semi-supervised text classiﬁcation.
iclr..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translationarxiv preprint.
2015.models with monolingual data.
arxiv:1511.06709..christian szegedy, wojciech zaremba, ilya sutskever,joan bruna, dumitru erhan, ian goodfellow, androb fergus.
2014.intriguing properties of neuralnetworks.
in international conference on learningrepresentations..jason wei and kai zou.
2019..eda: easy dataaugmentation techniques for boosting performancein proceedings ofon text classiﬁcation tasks.
emnlp-ijcnlp 2019, pages 6382–6388, hongkong, china..adina williams, nikita nangia, and samuel r bow-man.
2017. a broad-coverage challenge corpus forarxivsentence understanding through inference.
preprint arxiv:1704.05426..xing wu, shangwen lv, liangjun zang, jizhong han,and songlin hu.
2019. conditional bert contextualaugmentation.
in international conference on com-putational science, pages 84–95.
springer..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in advances in neuralinformation processing systems, volume 33, pages6256–6268.
curran associates, inc..adams wei yu, david dohan, quoc le, thang luong,rui zhao, and kai chen.
2018. fast and accuratereading comprehension by combining self-attentionin international conference onand convolution.
learning representations..xunjie zhu, tingfeng li, and gerard de melo.
2018.exploring semantic properties of sentence embed-dings.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 2: short papers), pages 632–637, melbourne,australia.
association for computational linguis-tics..5167