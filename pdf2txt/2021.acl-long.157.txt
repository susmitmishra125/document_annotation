control image captioning spatially and temporally.
kun yan†∗, lei ji ‡§¶, huaishao luo(cid:107), ming zhou¶, nan duan¶, shuai ma††sklsde lab, beihang university, beijing, china‡institute of computing technology, cas, beijing, china§university of chinese academy of sciences, beijing, china¶microsoft research asia, beijing, china(cid:107)southwest jiaotong university, chengdu, china†{kunyan,mashuai}@buaa.edu.cn ¶{leiji,mingzhou,nanduan}@microsoft.com(cid:107)huaishaoluo@gmail.com.
abstract.
generating image captions with user inten-tion is an emerging need.
the recentlypublished localized narratives dataset takesmouse traces as another input to the image cap-tioning task, which is an intuitive and efﬁcientway for a user to control what to describe inthe image.
however, how to effectively em-ploy traces to improve generation quality andcontrollability is still under exploration.
thispaper aims to solve this problem by propos-ing a novel model called loopcag, whichconnects contrastive constraints and attentionguidance in a loop manner, engaged explicitspatial and temporal constraints to the gener-ating process.
precisely, each generated sen-tence is temporally aligned to the correspond-ing trace sequence through a contrastive learn-ing strategy.
besides, each generated text to-ken is supervised to attend to the correct visualobjects under heuristic spatial attention guid-ance.
comprehensive experimental resultsdemonstrate that our loopcag model learnsbetter correspondence among the three modal-ities(vision, language, and traces) and achievessota performance on trace controlled imagecaptioning task.
moreover, the controllabilityand explainability of loopcag are validatedby analyzing spatial and temporal sensitivityduring the generation process..1.introduction.
image captioning is a fundamental task to examinewhether an intelligent system can understand thevisual world by letting the system describe it withnatural language.
generating a reasonable captionrequires the model to link linguistic tokens to ob-jects, relationships, scenes of the visual world inthe input image.
thus, a great captioning modelwill help us better understand what characteristicspromise a good joint visual-linguistic representa-tion..∗contribution during internship at msra..figure 1: a showcase of trace controlled image cap-tion.
given an image together with a mouse trace rep-resenting user intention, the task is to generate the cor-responding captions aligned with each part of the trace.
in this case, the trace and the caption marked with thesame color correspond to each other..most previous attempts aim to describe the im-age indicating the salient objects and relations with-out considering user intention.
to generate con-trollable and explainable captions, recent worksdedicated to establishing a new controllable imagecaptioning task to generate the caption at will.
thecaptioning process can be controlled by pos tag-ging (deshpande et al., 2018), sentiment (you et al.,2018), length (deng et al., 2020), bounding boxes(cornia et al., 2019), and mouse traces (pont-tusetet al., 2020)..in this paper, we mainly investigate trace-controlled image captioning, since it is not only amore natural and interactive paradigm for real webapplications, e.g.
automatic presentation or helppeople with visual difﬁculties but also a new per-spective for us to better understand how the long-pursued cross-modality alignment is performed indeep learning models.
figure 1 presents a showcaseof the scenario.
given an image, users can easilydraw a trace to ask the ai agent to describe thescene in the image along the trace automatically..in the localized narratives dataset (pont-tusetet al., 2020), the annotators describe the image.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2014–2025august1–6,2021.©2021associationforcomputationallinguistics2014in this picture thereisastand on a ground.
on the backside there is aperson.heisridingonahorse.heiswearingacap.heisinbetweenthefence.thereisaflagsonawall.ontheleftsidethereisascore board on atableandflowerplants.
wecanseeinthebackgroundsky andtrees.
while drawing the traces of their attention move-ment, which presents a spatial alignment betweenvisual objects and caption tokens as well as a tem-poral alignment between user intention(by trace)and caption sentences.
from figure 1, we see thatthe caption tokens, e.g.
“person”, “horse”, “trees”can be grounded to the visual objects spatially, andthe order of caption sentences can be arranged toalign to the order of traces temporally.
although itis easy for humans to recognize which visual objectis indicated by the traces, it is a challenge for theagent to recognize, emphasize and arrange visualsemantics solely based on several tracepoints’ co-ordinates.
thereby, we mainly devote our effort tothe spatial grounding and temporal controllabilityof image captioning..inspired by the above observation, we designtwo novel approaches to tackle the above chal-lenges.
speciﬁcally, we design sentence-level con-trastive constraints to align the generated sentencesto the corresponding trace sequences temporally.
besides, we design a type of heuristic spatial at-tention guidance to supervise each generated texttokens to attend to the correct visual objects.
com-posing the above together, we propose a noveltrace-controlled image captioning model calledloopcag and demonstrate its superior capabilityon captioning quality and ﬂexible controllability..our contribution can be summarized as:1) we propose a novel model loopcag,which learns the caption tokens’ spatial groundingthrough attention guidance and temporal localiza-tion between trace input and the caption sentencesthrough contrastive constraints in an end-to-endloop manner among the three modalities(vision,language, and traces)..2) the quantitative results show that ourtrace-loopcag model can generate bettercontrolled captions and achieve sota performanceon automatic criteria.
the qualitative resultspresent that our model can generate highly rele-vant captions given users’ trace inputs..3) we intensively study the controllability andexplainability of trace-controlled image captioning..2 preliminary.
2.1 task deﬁnition.
v = {v1, .
.
.
, vn }, in which vi ∈ r2048 is thei-th object visual feature, and n is the number ofvisual objects.
the text description sequence isy = {y1, .
.
.
, yl}, in which yj is the j-th token andl is the text sequence length.
the output is condi-tioned on model parameters θ, and the optimizationprocess can be formulated as the following maxi-mum likelihood form:.
θ∗ = arg max.
log p(y | v ; θ)..(1).
θ.for trace-controlled image captioning, the rawtrace input is a sequence of tracepoints coordinateswith timestamps.
to reduce those tracepoints to anacceptable length due to the limit of gpu memory,we segment the tracepoints sequences uniformly bythe same time window τ , and then each trace seg-ment is converted to its minimal bounding rectan-gle.
every bounding rectangle can be representedby a 5d vector which contains normalized coordi-nates of the top-left and bottom-right corners, andthe area ratio with respect to the whole image.
wedenote the trace input as t = {t1, .
.
.
, tm }, whereti ∈ r5.
the trace controlled captioning objectivecan be formulated as follow:.
θ∗ = arg max.
log p(y | v , t ; θ).
(2).
θ.
3 method.
theour method consists of three components:caption generation module with a transformerencoder-decoder backbone, the attention guidancefor object-level spatial grounding, and the con-trastive constraints for sentence-level temporalalignment.
the overall model structure is illus-trated in figure 2. the model is trained by jointlyoptimizing the three objectives listed in the follow-ing subsections..3.1 caption generation.
the caption generation backbone is a transformer-based encoder-decoder proposed by vaswani et al.
(2017), which mainly employs a multi-head atten-tion mechanism and achieves top-tier performancein many sequential related tasks.
here, we high-light several task-oriented modiﬁcations..for image captioning, the task is to generate atext description y given an image i. we ﬁrstapply a pre-trained visual object detector on theimage and get an object level visual feature set.
vision-trace encoder the visual embeddingsv and traces embeddings t are encoded separatelyand then concatenated together as a single inputsequence feeding into a transformer encoder..2015figure 2: model architecture overview.
the model consists of three modules: (a) caption generation: wedirectly concatenate the visual object embedding and the trace embedding as encoder input, and then employ atransformer decoder for caption generation.
(b) attention guidance: we use a heuristic supervision attentionscore matrix to supervise the vision-linguistic cross-attention generated by the transformer backbone, groundingthe caption tokens to visual objects spatially (c) contrastive constraints: we split the hidden states of captiontokens and traces by sentence respectively and then apply the contrastive loss to make the representations of thesentence and trace segment with same order indices closer, thereby aligning caption sentences to trace segmentstemporally..• object visual embedding: we ﬁrst repre-sent the spatial info of each object proposalby a 5d vector (in the same way as thetraces), then project it into a spatial embed-ding pi ∈ rd, where d is the embedding sizeacross the model.
each object visual featurevi is projected into a lower dimension vec-tor ˆvi ∈ rd.
the ﬁnal visual embedding is˜v = {˜v1, .
.
.
, ˜vn }, where ˜vi = ˆvi + pi..• trace embedding: each trace input item tiis projected into ˆti ∈ rd.
we also generatesinusoidal positional embeddings (vaswaniet al., 2017) oi to capture the temporal orderof the traces.
the ﬁnal trace embedding ˜t ={˜t1, .
.
.
, ˜tm }, where ˜ti = ˆti + oi..caption decoder caption decoder combines vi-sion and trace information using cross attentionconnected to the hidden states of vision-trace en-coder’s last layer.
using a casual mask to encodegenerated token progressively, the transformer de-coder ensures that the predictions for position i candepend only on the known outputs at positions lessthan i. during training, the ground truth captiontokens are shifted right, and a special token (cid:104)bos(cid:105)(begin of the sentence) is inserted into the ﬁrst posi-tion.
a cross-entropy generation loss lgen is thencomputed with the logits transformed from the lastdecoder layer’s hidden states and un-shifted ground.
truth caption token ids with a special token (cid:104)eos(cid:105)(end of the sentence) appended..lgen = − eˆyi∼ˆy.
(cid:16).
log p.ˆyi | ˆy<i, ˜t , ˜v ; θ.
(cid:17).
..(3).
it is noted that ˆy is the masked version of theground-truth caption y. to make a fair compar-ison with the baseline (pont-tuset et al., 2020), weapply the same setting and do not employ commontechniques such as label smoothing(szegedy et al.,2016) or self-critical training(rennie et al., 2017)..3.2 attention guidance for spatial ground.
attention supervision construction to explic-itly guide the attention for object-level spatialgrounding, we align the semantic caption tokenswith the visual object by taking trace as an inter-mediate bridge.
in this way, we construct a super-vision matrix to guide the attention between thecaption tokens and visual objects by the two fol-lowing steps..1) language-trace temporal alignment.
in thelocalized narrative dataset, the caption ut-terances1 u and mouse traces are highlytemporal-aligned, i.e., every utterance u has a.
1we are following the naming tradition of pont-tuset et al.
(2020), where an utterance means one or several adjacenttokens, not a whole sentence..2016𝑣(cid:2869)𝑣(cid:2870)𝑣(cid:2871)𝑣..𝑣(cid:3015)𝑡(cid:2869)𝑡(cid:2870)𝑡(cid:2871)𝑡.𝑡(cid:3014)transformer encoder layersspatial embeddingpositional embeddingtrace embeddingobject vision embeddingvisionhiddenstatestracehiddenstatestransformer decoder layerstokentcaptionhiddenstatespositional embeddingword embedding…token2token1<bos>splitby sentenceaggregateaggregateseg1seg2seg3seg3seg2seg1similarity matrixgt +/-labelinstance contrastiveloss(c) contrastive constraintstoken1token2…tokent<eos>generationloss(a) caption generationweighted bce loss(b) attention guidanceobjectstokensmaskedcross attention matrixattentionsupervision score matrixsplitby sentenceattention-guided grounding a cross-attentionmatrix is generated in shape (n, t, l, h) duringthe transformer’s decoding steps.
here n denotesthe number of pre-detected visual objects, t de-notes the number of tokens in a caption sentenceafter padding, l denotes the number of transformerlayers, and h denotes the number of attentionheads in transformer layers.
two linear projec-tions and layer normalization (ba et al., 2016) areapplied sequentially on dimension l and h, re-spectively reducing the dimension to 1. thus, for asingle instance, we eventually calculate an attentionmatrix a ∈ rn ×t ..to train the model, the goal can be achieved byminimizing the following attention guidance lossfunction latt:.
latt = − e.a∼a,s∼s.
s · [s log a + (1 − s) log (1 − a)] ,.
(6).
which is a weighted binary cross entropy betweena and s. noted that we also choose to mask outsome stop-words columns of the matrix a and sto avoid introducing too much annotation noise..3.3 contrastive constraints for temporal.
alignment.
as illustrated on the left side of figure 4, weﬁrst use a “split by sentence” procedure to builda sentence-level alignment between caption andtraces, and then employ contrastive loss to con-strain the temporal order of the generation process..figure 4: a showcase of split by sentence and con-trastive constraints for temporal alignment.
split by sentence an annotated instance con-sists of an image, a tracepoint list, and a captionparagraph consisting of a list of ordered captionsentences.
here, we deﬁne a caption sentence as a.figure 3: a showcase of spatial attention scoring.
corresponding time window, every tracepointp has a timestamp.
to leverage this infor-mation, we ﬁrst assign each tracepoint p toa unique utterance u, where the tracepointtimestamp is in the utterance time window.
thus, every utterance u is aligned to a seriesof tracepoints pu = {p1, .
.
.
, pku}..2) language-vision spatial alignment.
give theutterance u and corresponding pu, we cal-culate the alignment score considering thespatial overlap between tracepoints pu andeach vision object vi .
every visual objectvi has a corresponding spatial bounding boxi , y2bi = (x1iare top-left and bottom-right horizontal andvertical coordinates respectively.
we set thealignment score s(uj ,bi) between utterance ujand bounding box bi as,.
i ), and the x1.
i , x2.
i , x2.
i , y2.
i , y1.
i , y1.
(cid:80)p∈puj.
ibi(p).
|puj |.
s(uj ,bi) =.
(4).
(5).
where i is an indicator of whether point p isin the bounding box bi:.
ibi(p) =.
.
.
i < xp < x2ii < yp < y2i.
1 if x1.
and y10 otherwise.
xp and yp are the coordinates of each trace-point in pu.
an example of the alignmentscore calculation is illustrated in figure 3..by calculating the alignment score, we establishthe spatial grounding supervision between captiontokens and auto-detected visual objects.
for everyword yi in the same utterance u, the s(yi,bj ) =s(u,bj ).
eventually, we get the supervision scorematrix s ∈ [0, 1]n ×t and sij = s(yi,bj )..2017vision box: persontrace utterance: personnumber of tracepointsin box:12total number of points:  12supervision attention score=12/12=1.0vision box: persontrace utterance: horsenumber of tracepointsin box:14total number of points:  67supervision attention score=14/67=0.209the personis riding a horse.in this picture there is a stand on a ground.on the backside there is a person.
heisridingonahorse.he is wearing a cap.
there is a flagon a wall.
on the left side there is a score board on a table and flower plants.
in this picture thereisastand on a ground.
on the backside there is aperson.heisridingonahorse.heiswearingacap.heisinbetweenthefence.thereisaflag onawall.ontheleftsidethereisascore board on atableandflowerplants.
contrastive constraintssplit by sentenceseries of utterances segmented out by a period(’.’).
in section 3.2, we already maintain an alignmentbetween utterances and tracepoints.
following thissetting, we can unite a list of ordered utteranceu = {u1, .
.
.
, uk} in the same caption sentence,and then orderly unite a list of tracepoints corre-sponding to u ’s elements into a so-called trace seg-ment.
the alignment between caption sentencesand trace segments can be established by simplyuniting the association between utterances and tra-cepoints with respect to the above sentence split.
we call this procedure as split by sentence..temporal contrastive constraints accordingto the split mentioned above, we aggregate thetransformer’s last layer hidden states of trace seg-ments and caption sentences respectively, and de-note them as hts = {h1ts, .
.
.
, hnts} and hcs ={h1cs}.
here n is the number of captionsentences..cs, .
.
.
, hn.
we adopt the nce loss to learn to discriminatethe positive from negative trace-caption pairs.
thepositive is deﬁned as all the temporal aligned corre-sponding caption sentence and trace segment pairsi.e.
with the same order indices, and other pairswithout temporal alignment in the same image asnegative samples.
this contrastive loss functionlcts is deﬁned as follows,.
lcts = − e.log.
hts∼hts.
ts, hiexp(s(hiz.cs)).
,.
z =.
exp(s(hi.
ts, hj.
cs)).
n(cid:88).
j=1.
(7).
(8).
where s(·, ·) means two linear layers and an l2normalization applied on the elements respectively,and a dot production between them.
by minimiz-ing the lcts, we force the model to learn a repre-sentation being aware of sentence-level temporalordering, which leads to more precise captioning..3.4 loss.
finally, the model is trained with three losses lgen,latt, and lcts, where lgen is the caption genera-tion loss, latt is the spatial attention guidance loss,and lcts is the temporal contrastive loss.we jointlyoptimize our model by minimizing all losses addedtogether:.
lall =lgen + latt + lcts..(9).
4 experiments.
4.1 dataset.
we use the annotated coco subset of localizednarratives to evaluate our method.
we call thisdataset split as ln-coco for short.
each imagehas one or several pairs of the captioning paragraphand corresponding mouse traces.
every single pairis a so-called localized narrative.
the training andvalidation splits are identical to pont-tuset et al.
(2020)’s setting.
there are 134,272 localized narra-tives in the training set and 8,573 in the validationset.
we train on the whole training set and eval-uate our model performance against the identicalvalidation set..4.2.implementation details.
for the visual feature, we adopt faster-rcnn(renet al., 2015) to extract 100 bounding box proposals.
for trace feature, we use τ = 0.4s to extract tracesegment for feature extraction.
the embedding sized, number of transformer layers, hidden size of thetransformer feed-forward layer are 768, 2, and 768,respectively.
the number of attention heads is 8,and the dropout rate is 0.1. we adopt the adam-w optimizer (loshchilov and hutter, 2019) withlearning rate of 7e-4(which is the best performancesetting of baseline, and adopted widely for othertrials), and set two momentum parameters β1= 0.9and β2= 0.99. we set the batch size to 256. allmodels are trained on 4 tesla v100 gpus with32gb memory for 10 to 12 hours..4.3 evaluation metrics.
this generation task adopts the traditional imagecaptioning evaluation metric using the open-sourcetool2 with a minor modiﬁcation3 to suit with ln-coco, including bleu(papineni et al., 2002),meteor (banerjee and lavie, 2005), rouge-l (lin and och, 2004), rouge-1-f1(pont-tusetet al., 2020), and cider-d (vedantam et al., 2015)..4.4 results.
baseline and +trace methods the baseline and+trace methods are our re-implementations follow-ing (pont-tuset et al., 2020)’s method description.
the baseline method only takes image feature asinput while the +trace model take image feature.
2https://github.com/tylin/coco-caption3we add an additional id to every trace-image-captiontriplet and adjust some code of the standard evaluation tool tomeet the ”1 trace-vs-1 caption” evaluation need..2018and trace both as input.
they employ the architec-ture in changpinyo et al.
(2019) with a few minordifferences.
first, they set the number of trans-formers’ layers for both the encoder and the de-coder to 2 instead of 6. second, their projectionlayers also consist of layer normalization(ba et al.,2016).
third, they set the maximum number of it-erations to 150k.
finally, they allow the maximumnumber of target captions to be as long as 225 toaccount for the narration’s longer nature..loopcag methods our model comprises offour components: 1) the transformer encoder-decoder framework; 2) the trace input; 3) attentionguidance(+ag for short) grounding loss; 4) con-trastive constraints(+c for short)..main results the table 1 shows the overall per-formance comparison on the ln-coco dataset.
to reduce the deviation caused by different im-plementation details, we ﬁrst present our imple-mentations’ performance (with *), which have ahigher score than pont-tuset et al.
(2020) reported.
thus, we have a more strict baseline to evaluatethe improvement purely coming from our innova-tive method.
compared to baseline* method, theperformance on all metrics improves signiﬁcantlywhen controlling captioning using the mouse trace(+trace*), it indicates that using the mouse traceenables the system to describe better those userintended parts of the image..most importantly, the results indicate that ourloopcag method achieves state of the art onall automatic criteria, outperforming the previousstate-of-art model by 2.4 and 7.5 on bleu-4 andcider-d, respectively.
this demonstrates our pro-posed attention guidance method helps the modelgenerate better spatially grounded and more pre-cise captions.
when considering the 2.0 risingon rouge-l score, we can conclude that con-trastive constraints can help the model better alignthe order of generated sentence to the user intentbecause rouge-l mainly employs an order mat-tered longest common sequence f-measure..ablations we perform three ablations to verifythe most improvements in-deed come from the at-tention guidance and contrastive constraints.
start-ing from standard captioning (baseline*), we addthe attention guidance to help the model betterspatially ground visual objects and caption tokens(table 2, “+ ag”).
this affects performance, sug-gesting that the model does beneﬁt from knowing.
where to ﬁnd the highly semantic related appear-ance feature in the image.
next, we add the tracefeature (table 2, “+ trace”).
this introduces userintention to the model.
we also take this line toshow the performance lift caused by contrastiveconstraints fairly.
then we add the contrastive mod-ule (table 2, “+c”) and see a good improvementon almost all criteria.
hence, we verify the signif-icance of the positive inﬂuence of temporal con-trastive constraints.
moreover, in the last line is ourfull loopcag model.
we can see the two proposedmethods are not exclusive to each other..4.5 quantitative analysis.
controllability analysis on temporal orderwe also design an experiment to further demon-strate loopcag’s superior controllability on thecaption sentences’ temporal order.
speciﬁcally,we split each localized narrative input by sentenceas described in sec3.3, and reverse the sequentialorder of the splits, i.e., the last sentence of a cap-tion paragraph will become the ﬁrst one, the sameprocessing is applied to trace segments, too.
weconduct an evaluation on the sentence&segment re-verted dataset, and the performance comparison isshown in table 3. with the contrastive constraintsmechanism’s help, the loopcag model is muchmore robust to trace input reversing, even compet-itive with the model trained on reverted data.
incontrast, the base models all face a dramatic dropon almost all metrics when the input trace order isreversed.
this also implies there are some biasedhabits of human annotators.
for example, they al-ways describe the salient objects ﬁrst and end witha sentence about the background of the image..controllability analysis on temporal fre-quency then, we analyze the controllability ofthe temporal frequency τ to present whether thecoarse-grained or ﬁne-grained tracepoints (sam-pling rate, in other words) affects the generationperformance.
as the table 4 shows, we change thetemporal frequency τ from 0.4 to 1.2. a perfor-mance drop is impressive with the τ getting larger.
the purpose of this experiment for various τ is tosimulate the trace drawing speed of users in a realapplication scenario, and a larger τ is equivalentto a faster drawing speed.
as deng et al.
(2020)has demonstrated, the length is one of the criticalfacts that impact quantitative performance.
thisresult implies we can further decide to generateeither a coarse-grained or ﬁne-grained caption by.
2019methodbaseline(pont-tuset et al., 2020)+trace(pont-tuset et al., 2020)baseline*+trace*loopcag(our).
rouge-l rouge-1-f1 bleu-1 bleu-4 cider-d meteor-32.2-52.216.436.025.255.426.057.2.
29.3106.529.5107.9114.0.
8.124.610.325.027.0.
47.960.754.068.169.8.
31.748.334.149.050.3.table 1: comparison with baseline methods results: baseline means an encoder-decoder model without takingtrace as input.
+trace means concatenating encoded trace feature to the encoder input, i.e., trace controlled captionperformance.
loopcag is our complete model.
the results with * are the baseline performance re-implementedby ourselves.
rouge-l rouge-1-f1 bleu-134.134.7(+0.6)49.0.methodbaseline*+ag+trace*+trace+c 50.1(+1.1)loopcag 50.3(+1.3).
54.055.5(+1.5)68.169.3(+1.2)69.8(+1.7).
36.037.4(+1.4)55.456.7(+1.3)57.2(+1.8).
bleu-410.310.5(+0.2)25.026.4(+1.4)27.0(+2.0).
cider-d29.530.1(+0.6)107.9113.6(+5.7)114.0(+6.1).
meteor16.416.6(+0.2)25.225.9(+0.7)26.0(+0.8).
table 2: ablation study results: baseline means an encoder-decoder model without taking trace as input.
+agmeans using attention guidance.
+trace means concatenating trace feature to the encoder input, i.e., trace con-trolled caption performance.
+c means applying the contrastive constraints method.
loopcag is our completemodel.the results with * are the baseline performance re-implemented by ourselves.
method.
baseline*+trace*+trace*+trace*loopcag.
reversetrained.
reverseevaluated(cid:88)(cid:88).
(cid:88)(cid:88).
(cid:88)(cid:88).
bleu-1 bleu-4 meteor rouge-l cider-d.36.050.850.253.453.7.
10.115.516.419.618.6.
16.319.920.121.621.7.
28.733.236.438.234.6.
29.143.445.255.152.2.table 3: analysis on temporal order results: model performance on caption sentence and trace segment reversedevaluation dataset.the results with * are the baseline performance re-implemented by ourselves..τ bleu-4 meteor rouge-l cider-d91.791.188.382.479.1.
26.926.726.124.824.1.
47.246.946.244.944.3.
25.525.525.324.724.4.
0.40.60.81.01.2.table 4: analysis on temporal frequency results..controlling the time-frequency τ ..controllability analysis on spatial semanticgrounding one of our important purposes ofusing attention guidance is introducing more inter-pretability to the model while improving the cap-tion performance.
when generating each token, themodel is forced to show which visual elements arethe most effective reason for the current generation.
and this effectiveness is supervised by our pseudo.
attention label.
in this way, we can hopefully ob-tain better visual-linguistic joint representation.
inappendix a, we showcase the attention values com-parison of models w/wo attention guidance.
weﬁnd that the ag model has a more diverse dis-tribution across all different types of tokens.
a”neater” activation is observed in appendix a (a)compared with (c), e.g., activations of ”who”, ”is”and ”on” are clearly suppressed.
we observe thatthese suppressions happen on most function word,so we add this illustration for further discussionand exploration by our research community..4.6 qualitative case study.
we present a showcase of a captioning result ofdifferent methods in figure 7. we can easily ﬁndthat the baseline captioning describes the imagein random order while the +trace captioning and.
2020loopcag captioning almost have the same or-der as ground truth captioning.
it is also awe-inspiring that the baseline captioning and +tracecaptioning both consist of some preposterous de-scription highlighted in red color.
in contrast, theloopcag captioning is all reasonable.
this is evi-dence of superior fact grounding advantage broughtby our attention guidance method..ground truth captioning.
in this picture there is a stand on a ground.
on the backsidethere is a person.
he is riding on a horse.
he is wearing a cap.
he is in between the fence.
there is a ﬂags on a wall.
on theleft side there is a score board on a table and ﬂower plants.
we can see in the background sky.
trees.
baseline captioning.
in this image i can see a horse which is in white color, at leftthere is a person sitting on the horse, at the back ground thereare some people standing, in the background there are fewbuildings, trees and sky.
+trace captioning.
this picture might be taken outside of the city.
in this image,in the middle there is a man sitting on horse and holding thecollar rope of a horse.
on the right side, we can also seeanother horse and a person is riding it.
in the background,there are group of people, ﬂags, trees, plants, metal fence,hoardings, trees.
on top there is s a sky, at the bottom thereare some grass and a land.
loopcag captioning.
there is a person sitting on a horse.
he is holding a horsethread and he is wearing a cap.
there are ﬂags, board on theleft side.
we can see in the background sky, trees..figure 5: controlled captioning qualitative examples1: ground truth captioning by annotator versus baselinecaptioning where the input is only the image, captioning con-trolled by mouse traces where the mouse traces are also aninput to the model (+trace and loopcag captioning).
gradi-ent.
indicates time..5 related work.
controllable image captioning is an emerg-ing research direction.
previous works aim tocontrol the captioning by part-of-speech tag-ging(deshpande et al., 2018), sentiment, (you et al.,2018), length (deng et al., 2020), bounding box(cornia et al., 2019) etc.
those works either triedto describe a semantic guided captioning.
otherworks relied on predeﬁned categories, e.g., bound-ing box or sentiment classes.
similar works (yuet al., 2018; cornia et al., 2019) control the cap-tion by a sequence of ordered topics and boundingboxes.
however, those methods limit the caption-ing on the pre-deﬁned or recognized objects inthe bounding box and hard to scale out.
besides,the trace is a more natural way to input than thebounding box.
the most similar work (pont-tusetet al., 2020) proposed a trace-controlled image cap-tioning task and designed a simple benchmark bydirectly concatenating the mouse trace coordinatesand size into a self-attention module.
althoughmouse trace is ﬂexible and interactive, it is easy forhumans to understand the trace’s semantic repre-sentation but hard for ai agents.
unlike previousworks, we propose a novel trace-controlled modelfor capturing the semantic representation of tracefrom both ﬁne-grained and coarse-grained spatialand temporal characteristics.
contrastive learning recently, contrastive learn-ing has been widely studied in unsupervised rep-resentation learning for vision, (he et al., 2020;chen et al., 2020; grill et al., 2020; caron et al.,2020; chen and he, 2020), language (mikolovet al., 2013; saunshi et al., 2019; chi et al., 2020;fang and xie, 2020; giorgi et al., 2020; konget al., 2020; gunel et al., 2021), or multi-modal(sun et al., 2019; luo et al., 2020).
the goal is tolearn semantic representation between two viewsby allowing the positive sample to be similar (insemantic space) and negatives to be dissimilar se-mantically simultaneously.
clip (radford et al.)
and mil-nce (miech et al., 2020) has demon-strated the effectiveness for learning the semanticmapping between vision and language.
previousattempts mainly exploit the infonce (oord et al.,2018) objective to maximize a lower bound of themutual information.
this paper extends the multi-modal contrastive learning between the trace in theimage and captioning sentence.
in the same image,they correspond to each other semantically.
thismotivates us to design a contrastive loss for better.
2021alignment between the trace and language..6 conclusion.
in this paper, we focus on the controlled imagecaptioning task and ﬁnd mouse traces provide anintuitive and efﬁcient way for a user to control thedescription.
we propose a novel caption generationmodel with contrastive constraints and attentionguidance called loopcag to control the captioningprocess spatially and temporally.
the experimentalresults demonstrate the our model’s effectiveness,and our work will inspire more future research onvision-linguistic understanding and generation..7 acknowledgement.
we thank botian shi, rongcheng tu for help-ful discussions.
this work is supported inpart by national key r&d program of china2018aaa0102301 and nsfc 61925203..references.
jimmy ba, j. kiros, and geoffrey e. hinton.
2016..layer normalization.
arxiv, abs/1607.06450..satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..mathilde caron, ishan misra, julien mairal, priyagoyal, piotr bojanowski, and armand joulin.
2020.unsupervised learning of visual features by contrast-ing cluster assignments.
in neurips..soravit changpinyo, bo pang, piyush sharma, andradu soricut.
2019. decoupled box proposal andfeaturization with ultraﬁne-grained semantic labelsimprove image captioning and visual question an-swering.
arxiv, abs/1909.02097..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inicml, volume 119, pages 1597–1607..xinlei chen and kaiming he.
2020. exploring sim-ple siamese representation learning.
arxiv preprintarxiv:2011.10566..zewen chi, l. dong, furu wei, n. yang, sak-sham singhal, wenhui wang, xia song, xian-ling mao, he yan huang, and m. zhou.
2020.infoxlm: an information-theoretic framework forcross-lingual language model pre-training.
arxivpreprint arxiv:2007.07834..m. cornia, l. baraldi, and r. cucchiara.
2019. show,control and tell: a framework for generating control-lable and grounded captions.
2019 ieee/cvf con-ference on computer vision and pattern recognition(cvpr), pages 8299–8308..chaorui deng, ning ding, mingkui tan, and qi wu.
2020. length-controllable image captioning.
incomputer vision – eccv 2020, pages 712–729,cham.
springer international publishing..aditya deshpande, jyoti aneja, liwei wang, alexan-der g schwing, and david a forsyth.
2018. di-verse and controllable image captioning with part-of-speech guidance..hongchao fang and pengtao xie.
2020. cert: con-trastive self-supervised learning for language under-standing.
arxiv preprint arxiv:2005.12766..john m giorgi, osvald nitski, gary d. bader, andbo wang.
2020. declutr: deep contrastive learn-ing for unsupervised textual representations.
arxivpreprint arxiv:2006.03659..jean-bastien grill, florian strub, florent altch´e,corentin tallec, pierre h. richemond, elenabuchatskaya, carl doersch, bernardo avila pires,zhaohan daniel guo, mohammad gheshlaghi azar,bilal piot, koray kavukcuoglu, r´emi munos, andmichal valko.
2020. bootstrap your own latent:a new approach to self-supervised learning.
inneurips..beliz gunel, jingfei du, alexis conneau, and veselinstoyanov.
2021. supervised contrastive learning forpre-trained language model ﬁne-tuning.
in iclr..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for un-supervised visual representation learning.
in cvpr,pages 9729–9738..lingpeng kong, cyprien de masson d’autume, lei yu,wang ling, zihang dai, and dani yogatama.
2020.a mutual information maximization perspective oflanguage representation learning.
in iclr..chin-yew lin and franz josef och.
2004. auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
in acl, page 605..ilya loshchilov and frank hutter.
2019. decoupled.
weight decay regularization.
in iclr..huaishao luo, lei ji, botian shi, haoyang huang, nanduan, tianrui li, jason li, taroon bharti, and mingzhou.
2020. univl: a uniﬁed video and languagepre-training model for multimodal understandingand generation.
arxiv preprint arxiv:2002.06353..antoine miech, jean-baptiste alayrac, lucas smaira,ivan laptev, josef sivic, and andrew zisserman.
2020. end-to-end learning of visual represen-tations from uncurated instructional videos.
incvpr..2022quanzeng you, hailin jin, and jiebo luo.
2018. imagecaptioning at will: a versatile scheme for effectivelyinjecting sentiments into image descriptions.
arxivpreprint arxiv:1801.10121..niange yu, xiaolin hu, binheng song, jian yang, andjianwei zhang.
2018. topic-oriented image caption-ing based on order-embedding.
ieee transactionson image processing, 28(6):2743–2754..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems, volume 26, pages 3111–3119..aaron van den oord, yazhe li, and oriol vinyals.
2018. representation learning with contrastive pre-dictive coding.
arxiv preprint arxiv:1807.03748..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl, pages 311–318..jordi pont-tuset, jasper uijlings, beer changpinyo,radu soricut, and vittorio ferrari.
2020. connect-ing vision and language with localized narratives.
ineccv..alec radford, jong wook kim, chris hallacy, adityaramesh, gabriel goh, sandhini agarwal, girishsastry, amanda askell, pamela mishkin, jack clark,et al.
learning transferable visual models from nat-ural language supervision.
image, 2:t2..shaoqing ren, kaiming he, ross girshick, and jiansun.
2015. faster r-cnn: towards real-time objectin pro-detection with region proposal networks.
ceedings of the 28th international conference onneural information processing systems - volume 1,nips’15, page 91–99, cambridge, ma, usa.
mitpress..steven j. rennie, e. marcheret, youssef mroueh,j. ross, and v. goel.
2017. self-critical sequencetraining for image captioning.
2017 ieee confer-ence on computer vision and pattern recognition(cvpr), pages 1179–1195..nikunj saunshi, orestis plevrakis, sanjeev arora,mikhail khodak, and hrishikesh khandeparkar.
2019. a theoretical analysis of contrastive unsuper-vised representation learning.
in proceedings of the36th international conference on machine learning,volume 97, pages 5628–5637..chen sun, fabien baradel, kevin murphy, andcordelia schmid.
2019. contrastive bidirectionaltransformer for temporal representation learning.
arxiv preprint arxiv:1906.05743..c. szegedy, v. vanhoucke, s. ioffe, j. shlens, andz. wojna.
2016. rethinking the inception architec-in 2016 ieee confer-ture for computer vision.
ence on computer vision and pattern recognition(cvpr), pages 2818–2826..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, l. kaiser,and illia polosukhin.
2017. attention is all you need.
in nips..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-scription evaluation.
in cvpr, pages 4566–4575..2023(a) attention activation (with attentionguidance).
(b) words activation comparison (withattention guidance).
(c) attention activation (withoutattention guidance).
(d) words activation comparison(without attention guidance).
figure 6: appendix a: controllability analysis on spatial semantic grounding.
2024original image.
image with trace.
ground truth captioning.
baseline captioning.
in this image i can see a person wearing whiteshirt, blue tie, blue blazer, skirt and black shoesis standing and holding a black colored bag inhis hand.
in the background i can see the whitecolored wall..in this picture we can see a man standing andholding a mobile in his hand, in the backgroundwe can ﬁnd a wall..+trace captioning.
loopcag captioning.
in the middle of the image a man is standing andsmiling and he is holding a tennis racket.
behindhim there is a cloth on the red color wall.
bottomleft side of the room there are two shoes..in this image i can see a person wearing bluecoat, black pant and black shoe is standing andholding a black colored bag in his hand.
in thebackground i can observe the white colored wall..figure 7: appendix b: controlled captioning qualitative examples 2: ground truth captioning by annotator versusbaseline captioning where the input is only the image (top left), captioning controlled by mouse traces where the mouse tracesare also an input to the model (+trace and loopcag captioning).
gradient.
indicates time..2025