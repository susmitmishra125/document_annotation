self-training sampling with monolingual data uncertaintyfor neural machine translation.
wenxiang jiao†∗ xing wang‡ zhaopeng tu‡ shuming shi‡ michael r. lyu†.
irwin king†.
†department of computer science and engineeringthe chinese university of hong kong, hksar, china‡tencent ai lab†{wxjiao,lyu,king}@cse.cuhk.edu.hk‡{brightxwang,zptu,shumingshi}@tencent.com.
abstract.
self-training has proven effective for improv-ing nmt performance by augmenting modelthetraining with synthetic parallel data.
common practice is to construct syntheticdata based on a randomly sampled subset oflarge-scale monolingual data, which we em-pirically show is sub-optimal.
in this work,we propose to improve the sampling proce-dure by selecting the most informative mono-lingual sentences to complement the paral-lel data.
to this end, we compute the un-certainty of monolingual sentences using thebilingual dictionary extracted from the paral-lel data.
intuitively, monolingual sentenceswith lower uncertainty generally correspond toeasy-to-translate patterns which may not pro-vide additional gains.
accordingly, we de-sign an uncertainty-based sampling strategyto efﬁciently exploit the monolingual data forself-training, in which monolingual sentenceswith higher uncertainty would be sampledwith higher probability.
experimental resultson large-scale wmt english⇒german andenglish⇒chinese datasets demonstrate the ef-fectiveness of the proposed approach.
ex-tensive analyses suggest that emphasizing thelearning on uncertain monolingual sentencesby our approach does improve the translationquality of high-uncertainty sentences and alsobeneﬁts the prediction of low-frequency wordsat the target side.1.
1.introduction.
leveraging large-scale unlabeled data has becomean effective approach for improving the perfor-mance of natural language processing (nlp) mod-els (devlin et al., 2019; brown et al., 2020; jiaoet al., 2020a).
as for neural machine translation.
∗work was mainly done when wenxiang jiao was intern-.
ing at tencent ai lab..com/wxjiao/uncsamp.
1the source code is available at https://github..(nmt), compared to the parallel data, the mono-lingual data is available in large quantities formany languages.
several approaches on boostingthe nmt performance with the monolingual datahave been proposed, e.g., data augmentation (sen-nrich et al., 2016a; zhang and zong, 2016), semi-supervised training (cheng et al., 2016; zhanget al., 2018; cai et al., 2021), pre-training (siddhantet al., 2020; liu et al., 2020).
among them, dataaugmentation with the synthetic parallel data (sen-nrich et al., 2016a; edunov et al., 2018) is the mostwidely used approach due to its simple and effec-tive implementation.
it has been a de-facto standardin developing the large-scale nmt systems (has-san et al., 2018; ng et al., 2019; wu et al., 2020;huang et al., 2021)..self-training (zhang and zong, 2016) is one ofthe most commonly used approaches for data aug-mentation.
generally, self-training is performed inthree steps: (1) randomly sample a subset from thelarge-scale monolingual data; (2) use a “teacher”nmt model to translate the subset data into thetarget language to construct the synthetic paralleldata; (3) combine the synthetic and authentic par-allel data to train a “student” nmt model.
recentstudies have shown that synthetic data manipula-tion (edunov et al., 2018; caswell et al., 2019) andtraining strategy optimization (wu et al., 2019b;wang et al., 2019) in the last two steps can boostthe self-training performance signiﬁcantly.
how-ever, how to efﬁciently and effectively sample thesubset from the large-scale monolingual data in theﬁrst step has not been well studied..intuitively, self-training simpliﬁes the complex-ity of generated target sentences (kim and rush,2016; zhou et al., 2019; jiao et al., 2020b), andeasy patterns in monolingual sentences with deter-ministic translations may not provide additionalgains over the self-training “teacher” model (shri-vastava et al., 2016).
related work on computer.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2840–2850august1–6,2021.©2021associationforcomputationallinguistics2840vision also reveals that easy patterns in unlabeleddata with the deterministic prediction may not pro-vide additional gains (mukherjee and awadallah,2020).
in this work, we investigate and identify theuncertain monolingual sentences which implicitlyhold difﬁcult patterns and exploit them to boost theself-training performance.
speciﬁcally, we mea-sure the uncertainty of the monolingual sentencesby using a bilingual dictionary extracted from theauthentic parallel data (§2.1).
experimental re-sults show that nmt models beneﬁt more fromthe monolingual sentences with higher uncertainty,except on those with excessively high uncertainty(§2.3).
by conducting the linguistic property anal-ysis, we ﬁnd that extremely uncertain sentencescontain relatively poor translation outputs, whichmay hinder the training of nmt models (§2.4)..inspired by the above ﬁnding, we proposean uncertainty-based sampling strategy for self-training, in which monolingual sentences withhigher uncertainty would be selected with higherprobability (§3.1).
large-scale experiments onwmt english⇒german and english⇒chinesedatasets show that self-training with the proposeduncertainty-based sampling strategy signiﬁcantlyoutperforms that with random sampling (§3.3).
ex-tensive analyses on the generated outputs conﬁrmour claim by showing that our approach improvesthe translation of uncertain sentences and the pre-diction of low-frequency target words (§3.4)..contributions.
our main contributions are:.
• we demonstrate the necessity of distinguish-ing monolingual sentences for self-training..• we propose an uncertainty-based samplingstrategy for self-training, which selects morecomplementary sentences for the authenticparallel data..• we show that nmt models beneﬁt morefrom uncertain monolingual sentences in self-training, which improves the translation qual-ity of uncertain sentences and the predictionaccuracy of low-frequency words..2 observing monolingual uncertainty.
in this section, we aimed to understand the effectof uncertain monolingual data on self-training.
weﬁrst introduced the metric for identifying uncertainmonolingual sentences, then the experimental setupand at last our preliminary results..(cid:88).
y∈y.
tx(cid:88).
t=1.
notations.
let x and y denote the source andtarget languages, and let x and y represent thesentence domains of corresponding languages.
letb = {(xi, yi)}ni=1 denote the authentic paralleldata, where xi ∈ x , yi ∈ y and n is the numberof sentence pairs.
let mx = {xj}mxj=1 denote thecollection of monolingual sentences in the sourcelanguage, where xj ∈ x and mx is the size of theset.
our objective is to obtain a translation modelf : x (cid:55)→ y, that can translate sentences fromlanguage x to language y ..2.1.identiﬁcation of uncertain data.
data complexity.
according to zhou et al.
(2019), the complexity of a parallel corpus canbe measured by adding up the translation uncer-tainty of all source sentences.
formally, the trans-lation uncertainty of a source sentence x with itstranslation candidates can be operationalized asconditional entropy:.
h(y|x = x) = −.
p(y|x) log p(y|x).
(1).
≈.
h(y|x = xt),.
(2).
where tx denotes the length of the source sentence,x and y represent a word in the source and tar-get vocabularies, respectively.
generally, a highh(y|x = x) denotes that a source sentence xwould have more possible translation candidates.
equation (2) estimates the translation uncertaintyof a source sentence with all possible translationcandidates in the parallel corpus.
it can not be di-rectly applied to the sentences in monolingual datadue to the lack of corresponding translation can-didates.
one potential solution to the problem isutilizing a trained model to generate multiple trans-lation candidates.
however, generation may leadto bias estimation due to the generation diversityissue (li et al., 2016; shu et al., 2019).
more im-portantly, generation is extremely time-consumingfor large-scale monolingual data..monolingual uncertainty.
to address the prob-lem, we modiﬁed equation (2) to reﬂect the uncer-tainty of monolingual sentences.
we estimate thetarget word distribution conditioned on each sourceword based on the authentic parallel corpus, andthen use the distribution to measure the translationuncertainty of the monolingual example.
speciﬁ-cally, we measure the uncertainty of monolingualsentences based on the bilingual dictionary..2841newscrawl data from year 2011 to 2019 for theenglish monolingual corpus, consisting of about200m sentences.
we randomly sampled 40mmonolingual data for en⇒de and 20m for en⇒zhunless otherwise stated.
we adopted newstest2018as the validation set and used newstest2019/2020as the test sets.
for each language pair, we appliedbyte pair encoding (bpe, sennrich et al., 2016b)with 32k merge operations..model.
we chose the state-of-the-art trans-former (vaswani et al., 2017) network as ourmodel, which consists of an encoder of 6 layers anda decoder of 6 layers.
we adopted the open-sourcetoolkit fairseq (ott et al., 2019) to implementthe model.
we used the transformer-basemodel for preliminary experiments (§2.3) and theconstrained scenario (§3.2) for efﬁciency.
forthe unconstrained scenario (§3.3), we adopted thetransformer-big model.
results on these mod-els with different capacities can also reﬂect the ro-bustness of our approach.
for the transformer-base model, we trained it for 150k steps with32k (4096 × 8) tokens per batch.
for thetransformer-big model, we trained it for 30ksteps with 460k (3600 × 128) tokens per batchwith the cosine learning rate schedule (wu et al.,2019a).
we used 16 nvidia v100 gpus to conductthe experiments and selected the ﬁnal model by thebest perplexity on the validation set..evaluation.
we evaluated the models by bleuscore (papineni et al., 2002) computed by sacre-bleu (post, 2018)2. for the en⇒zh task, weadded the option --tok zh to sacrebleu.
wemeasured the statistical signiﬁcance of improve-ment with paired bootstrap resampling (koehn,2004) using compare-mt3 (neubig et al., 2019)..figure 1: performance of self-training with increasedsize of monolingual data.
the bleu score is averagedon wmt en⇒de newstest2019 and newstest2020..for a given monolingual sentence xj ∈ mx, its.
uncertainty u is calculated as:.
u(xj|ab) =.
h(y|ab, x = xt),.
(3).
1tx.
tx(cid:88).
t=1.
which is normalized by tx to avoid the length bias.
a higher value of u indicates a higher translationuncertainty of the monolingual sentence..in equation 3,.the word level entropyh(y|ab, x = xt) captures the translation modal-ities of each source word by using the bilingualdictionary ab.
the bilingual dictionary records allthe possible target words for each source word, aswell as translation probabilities.
it can be built fromthe word alignments by external alignment toolk-its on the authentic parallel corpus.
for example,given a source word x with all three word transla-tions y1, y2 and y3 and the translation probabilitiesof p(y1|x), p(y2|x) and p(y3|x), respectively, theword level entropy can be calculated as follows:.
h(y|ab, xi) = −.
p(yj|xi) log p(yj|xi)..2.3 effect of uncertain data.
(cid:88).
yj ∈ab(xi).
2.2 experimental setup.
data.
weconducted experiments on twolarge-scale benchmark translation datasets, i.e.,wmt english⇒german (en⇒de) and wmtenglish⇒chinese (en⇒zh).
the authenticparallel data for the two tasks consists of about36.8m and 22.1m sentence pairs, respectively.
the monolingual data we used is from newscrawlreleased by wmt2020.
we combined the.
(4).
first of all, we investigated the effect of mono-lingual data uncertainty on the self-training per-formance in nmt.
we conducted the preliminaryexperiments on the wmt en⇒de dataset withthe transformer-base model.
we sampled8m bilingual sentence pairs from the authenticparallel data and randomly sampled 40m mono-lingual sentences for the self-training.
to ensurethe quality of synthetic parallel data, we trained.
2bleu+case.mixed+lang.
[task]+numrefs.1+smooth.exp++test.wmt[year]+tok.
[tok]+version.1.4.14, task=en-de/en-zh, year=19/20, tok=13a/zh3https://github.com/neulab/compare-mt.
28422021-01-23 uncertainty vs. performancebleu353637uncertainty1.52.53.5data bins12345uncertaintybleuall (36.5)bleu32343638additional monolingual data0m8m16m24m32m40mthem into 5 equally-sized bins (i.e., 8m sentencesper bin) according to their uncertainty scores.
atlast, we performed self-training with each bin ofmonolingual data..we reported the translation performance in fig-ure 2. as seen, there is a trend of performance im-provement with the increase of monolingual datauncertainty (e.g., bins 1 to 4) until the last bin.
thelast bin consists of sentences with excessively highuncertainty, which may contain erroneous synthetictarget sentences.
training on these sentences forcesthe models to over-ﬁt on these incorrect syntheticdata, resulting in the conﬁrmation bias issue (arazoet al., 2020).
these results corroborate with priorstudies (chang et al., 2017; mukherjee and awadal-lah, 2020) such that learning on certain examplesbrings little gain while on the excessively uncertainexamples may also hurt the model training..2.4 linguistic properties of uncertain data.
we further analyzed the differences between themonolingual sentences with varied uncertainty togain a deeper understanding of the uncertain data.
speciﬁcally, we performed linguistic analysis onthe ﬁve data bins in terms of three properties: 1)sentence length that counts the tokens in the sen-tence, 2) word rarity (platanios et al., 2019) thatmeasures the frequency of words in a sentencewith a higher value indicating a more rare sen-tence, and 3) translation coverage (khadivi andney, 2005) that measures the ratio of source wordsbeing aligned with any target words.
the ﬁrsttwo reﬂect the properties of monolingual sentenceswhile the last one reﬂects the quality of syntheticsentence pairs.
we also presented the results of thesynthetic target sentences for reference.
details ofthe linguistic properties are in appendix a.2..the results are reported in figure 3. for thelength property, we ﬁnd that monolingual sentenceswith higher uncertainty are usually longer exceptfor those with excessively high uncertainty (e.g.,bin 5).
the monolingual sentences in the last databin noticeably contain more rare words than otherbins in figure 3(b), and the rare words in the sen-tences pose a great challenge in the nmt train-ing process (gu et al., 2020).
in figure 3(c), theoverall coverage in bin 5 is the lowest among theself-training bins.
in contrast, bin 1 with the low-est uncertainty has the highest coverage.
theseobservations suggest that monolingual sentencesin bin 1 indeed contain the easiest patterns while.
figure 2: relationship between uncertainty of mono-lingual data and the corresponding nmt performance.
the bleu score is averaged on wmt en⇒de new-stest2019 and newstest2020..a transformer-big model for translating thesource monolingual data to the target language.
we generated translations using beam search withbeam width 5, and followed edunov et al.
(2018)4to ﬁlter the generated sentence pairs (see ap-pendix a.1)..self-training v.s.
data size.
we took a look atthe performance of standard self-training and itsrelationship with data size.
figure 1 showed theresults.
obviously, self-training with 8m syntheticdata can already improve the nmt performance bya signiﬁcant margin (36.2 averaged bleu score onwmt en⇒de newstest2019 and newstest2020).
increasing the size of added monolingual datadoes not bring much more beneﬁt.
with all the40m monolingual sentences, the ﬁnal performanceachieves only 36.5 bleu points.
it indicates thatadding more monolingual data only is not a promis-ing way to improve self-training, and more sophis-ticated approaches for exploiting the monolingualdata are desired..self-training v.s.
uncertainty.
in this experi-ment, we ﬁrst adopted fast-align5 to establish wordalignments between source and target words in theauthentic parallel corpus and used the alignmentsto build the bilingual dictionary ab.
then we usedthe bilingual dictionary to compute the data uncer-tainty expressed in equation (3) for the sentencesin the monolingual data set.
after that, we rankedall the 40m monolingual sentences and grouped.
4https://github.com/pytorch/fairseq/.
tree/master/examples/backtranslation.
5https://github.com/clab/fast_align.
28432021-01-23 uncertainty vs. performancebleu353637uncertainty1.52.53.5data bins12345uncertaintybleuall (36.5)bleu32343638additional monolingual data0m8m16m24m32m40muncertaincertain(a) sentence length.
(b) word rarity.
(c) coverage.
figure 3: comparison of monolingual sentences with varied uncertainty in terms of three properties, includingsentence length, word rarity, and coverage..tences with relatively high uncertainty..to ensure the data diversity and avoid the risk ofbeing dominated by the excessively uncertain sen-tences, we sample monolingual sentences accord-ing to the uncertainty distribution with the highestuncertainty penalized.
speciﬁcally, given a bud-get of ns sentences to sample, we set two hyper-parameters to control the sampling probability asfollows:.
p =.
α =.
(cid:2)α · u(xj|ab)(cid:3)β.
(cid:80).
(cid:40).
[α · u(xj|ab)]β ,xj ∈mx1, u(xj|ab) ≤ umax,max( 2umax.
u(xj |ab) − 1, 0), else,.
(5).
(6).
where α is used to penalize excessively high un-certainty over a maximum uncertainty thresholdumax (see figure 4(a)), the power rate β is usedto adjust the distribution such that a larger β givesmore probability mass to the sentences with highuncertainty (see figure 4(b))..the maximum uncertainty threshold umax isassigned to the uncertainty value such that r%of sentences in the authentic parallel corpus havemonolingual data uncertainty below than it.
ris assumed to be as high as 80 to 100. becausefor monolingual data with uncertainty higher thanthis threshold, they may not be translated correctlyby the “teacher” model as there are inadequatesuch sentences in the authentic parallel data forthe model to learn.
as a result, monolingual sen-tences with uncertainty higher than umax shouldbe penalized in terms of the sampling probability..overall framework.
figure 5 presents theframework of our uncertainty-based sampling for.
(a) uncertainty.
(b) sampling probability.
figure 4: distribution of modiﬁed monolingual uncer-tainty and sampling probability.
the sample with highuncertainty has more chance to be selected while thatwith excessively high uncertainty would be penalized..monolingual sentences in bin 5 are the most difﬁ-cult ones, which may explain their relatively weakperformance in figure 2..3 exploiting monolingual uncertainty.
by analyzing the effect of monolingual data uncer-tainty on self-training in section 2, we understoodthat monolingual sentences with relatively highuncertainty are more informative while also withhigh quality, which motivates us to emphasize thetraining on these sentences.
in this section, we in-troduced the uncertainty-based sampling strategyfor self-training and the overall framework..3.1 uncertainty-based sampling strategy.
with the aforementioned measure of monolin-gual data uncertainty in section 2.1, we proposethe uncertainty-based sampling strategy for self-training, which prefers to sample monolingual sen-.
28442021-01-23 uncertainty vs. properties02550data bins12345sourcetarget7.58.08.5data bins12345sourcetarget0.850.900.951.00data bins12345sourcetargetuncertaincertainuncertaincertainuncertaincertain2021-01-23 uncertainty vs. properties02550data bins12345sourcetarget7.58.08.5data bins12345sourcetarget0.850.900.951.00data bins12345sourcetargetuncertaincertainuncertaincertainuncertaincertain2021-01-23 uncertainty vs. properties02550data bins12345sourcetarget7.58.08.5data bins12345sourcetarget0.850.900.951.00data bins12345sourcetargetuncertaincertainuncertaincertainuncertaincertain2021-01-23 uncertainty distributionuncertainty01234monolingual dataw/o penaltyw/ penaltyprobabilitymonolingual datan1β=1β=2n1umax1n2n02021-01-23 uncertainty distributionuncertainty01234monolingual dataw/o penaltyw/ penaltyprobabilitymonolingual datan1β=1β=2n1ucmax1n2n0figure 5: framework of the proposed uncertainty-based sampling strategy for self-training.
procedures framedin the red dashed box corresponds to our approach integrated into the standard self-training framework.
“bitext”,“mono”, “synthetic” denotes authentic parallel data, monolingual data and synthetic parallel data, respectively..self-training, which includes four steps: 1) traina “teacher” nmt model and an alignment modelon the authentic parallel data simultaneously; 2)extract the bilingual dictionary from the alignmentmodel and perform uncertainty-based sampling formonolingual sentences; 3) use the “teacher” nmtmodel to translate the sampled monolingual sen-tences to construct the synthetic parallel data; 4)train a “student” nmt model on the combinationof synthetic and authentic parallel data..3.2 constrained scenario.
we ﬁrst validated the proposed sampling approachin a constrained scenario, where we followed theexperimental conﬁguration in section 2.3 with thetransformer-base model, the 8m bitext, andthe 40m monolingual data.
it allows the efﬁcientevaluation of our approach with varied combina-tions of hyper-parameters and also the comparisonwith related methods.
speciﬁcally, we performedour approach by sampling 8m sentences from the40m monolingual data and then combining the cor-responding 8m synthetic data with the 8m bitextto train the transformer-base model..table 1 reported the impact of β and r on thebleu score.
as shown, sampling with high uncer-tainty sentences and penalizing those with exces-sively high uncertainty improves translation perfor-mance from 36.6 to 36.9. in these experiments, theuncertainty threshold umax for penalizing are 2.90and 2.74, which are determined by the 90% and80% (r=90 and 80 in table 1) most certain sen-tences in the authentic parallel data, respectively..bleu.
β.
123.r.90.
36.736.936.5.
80.
36.636.636.5.
100.
36.636.736.5.table 1: translation performance with respect to differ-ent values of β and r. the bleu score is averaged onwmt en⇒de newstest2019 and newstest2020..obviously, the proposed uncertainty-based sam-pling strategy achieves the best performance withr at 90 and β at 2. in the following experiments,we use r = 90 and β = 2 as the default setting forour sampling strategy if not otherwise stated..effect of sampling.
some researchers maydoubt that the ﬁnal translation quality is affected bythe quality of the teacher model.
therefore, transla-tions of high-uncertainty sentences should containmany errors, and it is better to add the results oforacle translations to discuss the sampling effectand the quality of pseudo-sentences separately.
todispel the doubt, we still used the aforementioned8m bitext as the bilingual data, and used the rest ofwmt19 en-de data (28.8m) as the held-out data(with oracle translations) for sampling.
the resultsare listed in table 2..clearly, our uncertainty-based sampling strat-egy (uncsamp) outperforms the random samplingstrategy (randsamp) when manual translationsare used (rows 2 vs. 3), demonstrating the effec-tiveness of our sampling strategy based on the un-.
28452021-01-23 stteacher nmt modelxyalignment modelbitext(1.1) train(1.2) trainxy1y2y3p(y1|x)p(y2|x)p(y3|x)bilingual dictionaryxmmonouncertainty-based samplingtranslatexm’ym’synthetic(2) sample(3) generatexm’student nmt model(4) train+ bitextsystem.
data.
en⇒de.
en⇒zh.
2019.
2020 avg.
2019.
2020 avg.
wu et al.
(2019b).
shi et al.
(2020).
this work.
bitext+randsampbitext+randsamp.
bitext+randsamp+srclm+uncsamp.
37.339.8––.
39.641.641.742.5⇑.
––––.
––––.
––––.
31.033.133.134.4⇑.
35.337.337.438.4.
37.137.637.338.2⇑.
––38.641.9.
42.543.844.044.3↑.
––––.
39.840.740.741.3.table 4: translation performance on wmt en⇒de and wmt en⇒zh test sets.
the results are reported withde-tokenized case-sensitive sacrebleu.
we adopt the transformer-big with large batch training (ott et al.,2018) to achieve the strong performance.
“↑ / ⇑”: indicate statistically signiﬁcant improvement over randsampp < 0.05/0.01 respectively..# data.
2019.
2020 avg.
data.
2019.
2020 avg.
1 bitext2345.
+ randsamp ora+ uncsamp ora+ randsamp st+ uncsamp st.36.937.437.840.040.4.
27.728.028.230.130.5.
32.332.733.035.035.4.randsampdwfsrclm.
uncsamp.
+ filtering.
40.939.641.1.
41.641.5.
31.630.132.0.
32.332.7.
36.234.836.5.
36.937.1.table 2: comparison of our uncsamp and rand-samp with manual translations (ora: manual transla-tions; st: pseudo-sentences) on wmt en⇒de new-stest2019 and newstest2020..table 3: comparison of the proposed uncertainty-based sampling strategy with related methods on wmten⇒de newstest2019 and newstest2020..certainty.
another interesting ﬁnding is that usingthe pseudo-sentences outperforms using the manualtranslations (rows 4 vs. 2, 5 vs. 3).
one possiblereason is that the transformer-big model toconstruct the pseudo-sentences was trained on thewhole wmt19 en-de data that contains the held-out data, which serves as self-training to decentlyimprove the supervised baseline (he et al., 2019)..comparison with related work.
we com-pared our sampling approach with two relatedworks, i.e., difﬁcult word by frequency (dwf,fadaee and monz, 2018) and source languagemodel (srclm, lewis, 2010).
the former onewas proposed for monolingual data selection forback-translation, in which sentences with low-frequency words were selected to boost the perfor-mance of back-translation.
the latter one was pro-posed for in-domain data selection for in-domainlanguage models.
details of the implementation ofrelated work are in appendix a.3..table 3 listed the results.
for dwf, it brings noimprovement over randsamp, indicating that the.
technique developed for back-translation may notwork for self-training.
as for srclm, it achievesa marginal improvement over randsamp.
theproposed uncsamp approach outperforms thebaseline randsamp by +0.7 bleu point, whichdemonstrates the effectiveness of our approach.
inaddition to our uncsamp approach, we also uti-lized another n-gram language model at the targetside to further ﬁlter out the synthetic data withpotentially erroneous target sentences.
by ﬁlter-ing out 20% sentences from the sampled 8m sen-tences, our uncsamp approach achieves a furtherimprovement up to +0.9 bleu point..3.3 unconstrained scenario.
we extended our sampling approach to the uncon-strained scenario, where the scale of data and thecapacity of nmt models for self-training are in-creased signiﬁcantly.
we conducted experimentson the high-resource en⇒de and en⇒zh transla-tion tasks with all the authentic parallel data, includ-ing 36.8m sentence pairs for en⇒de and 22.1mfor en⇒zh, respectively.
for monolingual data,.
2846we considered all the 200m english newscrawlmonolingual data to perform sampling.
we trainedthe transformer-big model for experiments.
table 4 listed the main results of large-scaleself-training on high-resource language pairs.
asshown, our transformer-big models trained onthe authentic parallel data achieve the performancecompetitive with or even better than the submis-sions to wmt competitions.
based on such strongbaselines, self-training with randsamp improvesthe performance by +2.0 and +0.9 bleu pointson en⇒de and en⇒zh tasks respectively, demon-strating the effectiveness of the large-scale self-training for nmt models.
with our uncertainty-based sampling strategy uncsamp, self-trainingachieves further signiﬁcant improvement by +1.1and +0.6 bleu points over the random samplingstrategy, which demonstrates the effectiveness ofexploiting uncertain monolingual sentences..3.4 analysis.
in this section, we conducted analyses to under-stand how the proposed uncertainty-based sam-pling approach improved the translation perfor-mance.
concretely, we analyzed the translationoutputs of wmt en⇒de newstest2019 from thetransformer-big model in table 4..uncertain sentences.
as we propose to enhancehigh uncertainty sentences in self-training, one re-maining question is whether our uncsamp ap-proach improves the translation quality of high un-certainty sentences.
speciﬁcally, we ranked thesource sentences in the newstest2019 by the mono-lingual uncertainty, and divided them into threeequally sized groups, namely low, medium andhigh uncertainty..the translation performance on these threegroups is reported in table 5. the ﬁrst observa-tion is that sentences with high uncertainty arewith relatively low bleu scores (i.e., 31.0), in-dicating the higher difﬁculty for nmt models tocorrectly decode the source sentences with higheruncertainty.
our uncsamp approach improves thetranslation performance on all sentences, especiallyon the sentences with high uncertainty (+10.9%),which conﬁrms our motivation of emphasizing thelearning on uncertain sentences for self-training..unc.
bitext randsamp.
lowmedhigh.
38.134.231.0.
39.736.733.4.uncsamp.
bleu (cid:52)(%).
41.537.434.4.
8.99.310.9.table 5: translation performance on uncertain sen-tences.
the relative improvements over bitext foruncsamp are also presented..freq bitext randsamp.
lowmedhigh.
52.365.270.3.
53.866.571.6.uncsamp.
fmeas (cid:52)(%).
54.766.972.0.
4.52.62.4.table 6: prediction accuracy of low-frequency wordsin the translation outputs.
the relative improvementsover bitext for uncsamp are also presented..has the potential to improve the prediction of low-frequency words at the target side for the nmtmodels.
therefore, we investigated whether ourapproach has a further boost to the performance onthe prediction of low-frequency words.
we calcu-lated the word accuracy of the translation outputswith respect to the reference in newstest2019 bycompare-mt.
following wang et al.
(2020), wedivided words into three categories based on theirfrequency, including high: the most 3,000 frequentwords; medium: the most 3,001-12,000 frequentwords; low: the other words..table 6 listed the results of word accuracy onthese three groups evaluated by f-measure.
first,we observe that low-frequency words in bitextare more difﬁcult to predict than medium- andhigh-frequency words (i.e., 52.3 v.s.
65.2 and70.3), which is consistent with fadaee and monz(2018).
second, adding monolingual data by self-training improves the prediction performance oflow-frequency words.
our uncsamp approachoutperforms randsamp signiﬁcantly on the low-frequency words.
these results suggest that em-phasizing the learning on uncertain monolingualsentences also brings additional beneﬁts for thelearning of low-frequency words at the target side..4 related work.
low-frequency words.
partially motivated byfadaee and monz (2018), we hypothesized thatthe addition of monolingual data in self-training.
synthetic parallel data.
data augmentation bysynthetic parallel data has been the most simple andeffective way to utilize monolingual data for nmt,.
2847which can be achieved by self-training (he et al.,2019) and back-translation (sennrich et al., 2016a).
while back-translation has dominated the nmtarea for years (fadaee and monz, 2018; edunovet al., 2018; caswell et al., 2019), recent works ontranslationese (marie et al., 2020; graham et al.,2019) suggest that nmt models trained with back-translation may lead to distortions in automatic andhuman evaluation.
to address the problem, startingfrom wmt2019 (barrault et al., 2019), the test setsonly include naturally occurring text at the source-side, which is a more realistic scenario for practicaltranslation usage.
in this new testing setup, theforward-translation (zhang and zong, 2016), i.e.,self-training in nmt, becomes a more promisingmethod as it also introduces naturally occurringtext at the source-side.
therefore, we focus on thedata sampling strategy in the self-training scenario,which is different from these prior studies..data uncertainty in nmt.
data uncertaintyin nmt has been investigated in the last fewyears.
ott et al.
(2018) analyzed the nmt modelswith data uncertainty by observing the effective-ness of data uncertainty on the model ﬁtting andbeam search.
wang et al.
(2019) and zhou et al.
(2020) computed the data uncertainty on the back-translation data and the authentic parallel data andproposed uncertainty-aware training strategies toimprove the model performance, respectively.
weiet al.
(2020) proposed the uncertainty-aware seman-tic augmentation method to bridge the discrepancyof the data distribution between the training andthe inference phases.
in this work, we propose toexplore monolingual data uncertainty to performdata sampling for the self-training in nmt..5 conclusion.
in this work, we demonstrate the necessity of distin-guishing monolingual sentences for self-training innmt, and propose an uncertainty-based samplingstrategy to sample monolingual data.
by samplingmonolingual data with relatively high uncertainty,our method outperforms random sampling signiﬁ-cantly on the large-scale wmt english⇒germanand english⇒chinese datasets.
further analysesdemonstrate that our uncertainty-based samplingapproach does improve the translation quality ofhigh uncertainty sentences and also beneﬁts theprediction of low-frequency words at the targetside.
the proposed technology has been applied to.
transmart6 (huang et al., 2021), an interactive ma-chine translation system in tencent, to improve theperformance of its core translation engine.
futurework includes the investigation on the conﬁrmationbias issue of self-training and the effect of decodingstrategies on self-training sampling..acknowledgments.
this work is partially supported by the researchgrants council of the hong kong special admin-istrative region, china (cuhk 2410021, researchimpact fund (rif), r5034-18; cuhk 14210717,general research fund), and tencent ai lab rhi-nobird focused research program (gf202036).
we sincerely thank the anonymous reviewers fortheir insightful suggestions on various aspects ofthis work..references.
eric arazo, diego ortego, paul albert, noel eo’connor, and kevin mcguinness.
2020. pseudo-labeling and conﬁrmation biasin deep semi-supervised learning.
in ijcnn..lo¨ıc barrault, ondˇrej bojar, marta r costa-juss`a,christian federmann, mark fishel, yvette gra-ham, barry haddow, matthias huck, philipp koehn,shervin malmasi, et al.
2019. findings of the 2019conference on machine translation.
in wmt..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
in neurips..deng cai, yan wang, huayang li, wai lam, andlemao liu.
2021. neural machine translation withmonolingual translation memory.
in acl..isaac caswell, ciprian chelba, and david grangier..2019. tagged back-translation.
in wmt..haw-shiuan chang, erik g learned-miller, and an-drew mccallum.
2017. active bias: training moreaccurate neural networks by emphasizing high vari-ance samples.
in neurips..yong cheng, wei xu, zhongjun he, wei he, huawu, maosong sun, and yang liu.
2016. semi-supervised learning for neural machine translation.
in acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl..6https://transmart.qq.com/index.
2848sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atscale.
in emnlp..benjamin marie, raphael rubino, and atsushi fujita.
2020. tagged back-translation revisited: why doesit really work?
in acl..marzieh fadaee and christof monz.
2018. back-translation sampling by targeting difﬁcult words inneural machine translation.
in emnlp..subhabrata mukherjee and ahmed awadallah.
2020.text.
uncertainty-aware self-training for few-shotclassiﬁcation.
in neurips..yvette graham, barry haddow, and philipp koehn.
2019. translationese in machine translation evalu-ation.
arxiv..shuhao gu, jinchao zhang, fandong meng, yangfeng, wanying xie, jie zhou, and dong yu.
2020.token-level adaptive training for neural machinetranslation.
in emnlp..hany hassan, anthony aue, chang chen, vishalchowdhary,jonathan clark, christian feder-mann, xuedong huang, marcin junczys-dowmunt,william lewis, mu li, et al.
2018. achieving hu-man parity on automatic chinese to english newstranslation.
arxiv..junxian he, jiatao gu, jiajun shen, and marc’aurelioranzato.
2019. revisiting self-training for neuralsequence generation.
in iclr..kenneth heaﬁeld.
2011. kenlm: faster and smaller.
language model queries.
in emnlp..guoping huang, lemao liu, xing wang, longyuewang, huayang li, zhaopeng tu, chengyan huang,and shuming shi.
2021. transmart: a practical in-teractive machine translation system.
arxiv..wenxiang jiao, michael lyu, and irwin king.
2020a.
exploiting unsupervised data for emotion recogni-tion in conversations.
in emnlp: findings..wenxiang jiao, xing wang, shilin he, irwin king,michael r lyu, and zhaopeng tu.
2020b.
data reju-venation: exploiting inactive training examples forneural machine translation.
in emnlp..shahram khadivi and hermann ney.
2005. automaticﬁltering of bilingual corpora for statistical machinetranslation.
in nldb..yoon kim and alexander m rush.
2016. sequence-.
level knowledge distillation.
in emnlp..philipp koehn.
2004. statistical signiﬁcance tests for.
machine translation evaluation.
in emnlp..robert c moore william lewis.
2010. intelligent se-.
lection of language model training data.
acl..jiwei li, will monroe, and dan jurafsky.
2016. a sim-ple, fast diverse decoding algorithm for neural gen-eration.
arxiv..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
tacl..graham neubig, zi-yi dou, junjie hu, paul michel,danish pruthi, and xinyi wang.
2019. compare-mt:a tool for holistic comparison of language genera-tion systems.
in naacl (demonstrations)..nathan ng, kyra yee, alexei baevski, myle ott,michael auli, and sergey edunov.
2019. facebookinfair’s wmt19 news translation task submission.
wmt..myle ott, michael auli, david grangier,.
andmarc’aurelio ranzato.
2018. analyzing uncer-tainty in neural machine translation.
in icml..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andmichael auli.
2019. fairseq: a fast, extensibletoolkit for sequence modeling.
in naacl (demon-strations)..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl..emmanouil antonios platanios, otilia stretcu, grahamneubig, barnabas poczos, and tom mitchell.
2019.competence-based curriculum learning for neuralmachine translation.
in naacl..matt post.
2018. a call for clarity in reporting bleu.
scores.
wmt 2018..rico sennrich, barry haddow, and alexandra birch.
2016a.
improving neural machine translation mod-els with monolingual data.
in acl..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in acl..tingxun shi, shiyu zhao, xiaopu li, xiaoxue wang,qian zhang, di ai, dawei dang, xue zhengshan,and jie hao.
2020. oppo’s machine translation sys-tems for wmt20.
in wmt..abhinav shrivastava, abhinav gupta, and ross gir-shick.
2016. training region-based object detectorswith online hard example mining.
in cvpr..raphael shu, hideki nakayama, and kyunghyun cho.
2019. generating diverse translations with sentencecodes.
in acl..aditya siddhant, ankur bapna, yuan cao, orhan firat,mia xu chen, sneha kudugunta, naveen arivazha-gan, and yonghui wu.
2020. leveraging monolin-gual data with self-supervision for multilingual neu-ral machine translation.
in acl..2849zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neuralmachine translation.
in acl..1.5. the “teacher” nmt model for self-trainingis the transformer-big model to ensure thequality of synthetic data..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in neurips..shuo wang, yang liu, chao wang, huanbo luan, andimproving back-translationin.
maosong sun.
2019.with uncertainty-based conﬁdence estimation.
emnlp..shuo wang, zhaopeng tu, shuming shi, and yang liu.
2020. on the inference calibration of neural ma-chine translation.
in acl..xiangpeng wei, heng yu, yue hu, rongxiang weng,luxi xing, and weihua luo.
2020. uncertainty-aware semantic augmentation for neural machinetranslation.
in emnlp..felix wu, angela fan, alexei baevski, yann ndauphin, and michael auli.
2019a.
pay less atten-tion with lightweight and dynamic convolutions.
iniclr..lijun wu, yiren wang, yingce xia, qin tao, jian-huang lai, and tie-yan liu.
2019b.
exploitingmonolingual data at scale for neural machine trans-lation.
in emnlp..shuangzhi wu, xing wang, longyue wang, fangxuliu, jun xie, zhaopeng tu, shuming shi, and mu li.
2020. tencent neural machine translation systemsfor the wmt20 news translation task.
in wmt..jiajun zhang and chengqing zong.
2016. exploit-ing source-side monolingual data in neural machinetranslation.
in emnlp..zhirui zhang, shujie liu, mu li, ming zhou, and en-hong chen.
2018. joint training for neural machinetranslation models with monolingual data.
in aaai..chunting zhou, graham neubig, and jiatao gu.
2019. understanding knowledge distillation in non-autoregressive machine translation.
in iclr..yikai zhou, baosong yang, derek f wong, yu wan,and lidia s chao.
2020. uncertainty-aware curricu-lum learning for neural machine translation.
in acl..a appendix.
a.1 synthetic data.
when performing self-training, we constructed thesynthetic data by translating the monolingual sen-tences via beam search with beam width 5, andfollowed edunov et al.
(2018)7 to remove sen-tences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding.
a.2 linguistic properties.
word rarity.
word rarity measures the fre-quency of words in a sentence with a higher valueindicating a more rare sentence (platanios et al.,2019).
the word rarity of a sentence is calculatedas follows:.
wr(x) = −.
log p(xt),.
(7).
1tx.
tx(cid:88).
t=1.
where p(xt) denotes the normalized frequency ofword xt in the authentic parallel data, and tx is thesentence length..coverage.
coverage measures the ratio of sourcewords being aligned by any target words (tu et al.,2016).
firstly, we trained an alignment model onthe authentic parallel data by fast-align8.
then weused the alignment model to force-align the mono-lingual sentences and the synthetic target sentences.
next, we calculated the coverage of each sourcesentence, and report the averaged coverage of eachdata bin.
the lower coverage of monolingual sen-tences in bin 5 indicates that they are not alignedas well as the other bins..a.3 comparison with related work.
sampling approach withwe compared ourtwo related works,i.e., difﬁcult word by fre-quency (dwf, fadaee and monz, 2018) and sourcelanguage model (srclm, lewis, 2010).
the for-mer one was proposed for monolingual data selec-tion for back-translation, in which sentences withlow-frequency words were selected to boost theperformance of back-translation.
the latter onewas proposed for in-domain data selection for in-domain language models..for dwf, we ranked the monolingual data byword rarity (platanios et al., 2019) of sentencesand also selected the top 80m monolingual data forself-training.
for srclm, we trained an n-gramlanguage model (heaﬁeld, 2011)9 on the sourcesentences in the bitext and measured the distancebetween each monolingual sentence to the bitextsource sentences by cross-entropy.
similarly, we se-lected 8m monolingual data with the lowest cross-entropy for self-training..7https://github.com/pytorch/fairseq/.
tree/master/examples/backtranslation.
8https://github.com/clab/fast_align9https://kheafield.com/code/kenlm/.
2850