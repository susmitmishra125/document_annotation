citationie: leveraging the citation graph for scientiﬁc informationextraction.
vijay viswanathan, graham neubig, pengfei liulanguage technologies institute, carnegie mellon university{vijayv, gneubig, pfliu3}@cs.cmu.edu.
abstract.
automatically extracting key informationfrom scientiﬁc documents has the potential tohelp scientists work more efﬁciently and ac-celerate the pace of scientiﬁc progress.
priorwork has considered extracting document-level entity clusters and relations end-to-endfrom raw scientiﬁc text, which can improve lit-erature search and help identify methods andmaterials for a given problem.
despite the im-portance of this task, most existing works onscientiﬁc information extraction (sciie) con-sider extraction solely based on the content ofan individual paper, without considering thepaper’s place in the broader literature.
in con-trast to prior work, we augment our text rep-resentations by leveraging a complementarysource of document context: the citation graphof referential links between citing and cited pa-pers.
on a test set of english-language scien-tiﬁc documents, we show that simple ways ofutilizing the structure and content of the cita-tion graph can each lead to signiﬁcant gainsin different scientiﬁc information extractiontasks.
when these tasks are combined, we ob-serve a sizable improvement in end-to-end in-formation extraction over the state-of-the-art,suggesting the potential for future work alongthis direction.
we release software tools to fa-cilitate citation-aware sciie development.1.
1.introduction.
the rapid expansion in published scientiﬁc knowl-edge has enormous potential for good, if it canonly be harnessed correctly.
for example, duringthe ﬁrst ﬁve months of the global covid-19 pan-demic, at least 11000 papers were published onlineabout the novel disease (hallenbeck, 2020), witheach representing a potential faster end to a globalpandemic and saved lives.
despite the value ofthis quantity of focused research, it is infeasible.
1https://github.com/viswavi/scigraphie.
figure 1: example of using the citation graph to im-prove the task of salient entity classiﬁcation (jain et al.,2020).
in this task, each entity in the document is clas-siﬁed as salient or not, where a salient entity is deﬁnedas being relevant to its paper’s main ideas..for the scientiﬁc community to read this many pa-pers in a time-critical situation, and make accuratejudgements to help separate signal from the noise.
to this end, how can machines help researchersquickly identify relevant papers?
one step in thisdirection is to automatically extract and organizescientiﬁc information (e.g.
important concepts andtheir relations) from a collection of research arti-cles, which could help researchers identify newmethods or materials for a given task.
scientiﬁc in-formation extraction (sciie) (gupta and manning,2011; yogatama et al., 2011), which aims to extractstructured information from scientiﬁc articles, hasseen growing interest recently, as reﬂected in therapid evolution of systems and datasets (luan et al.,2018; g´abor et al., 2018; jain et al., 2020)..existing works on sciie revolve around extrac-tion solely based on the content of different partsof an individual paper, such as the abstract or con-clusion (augenstein et al., 2017; luan et al., 2019).
however, scientiﬁc papers do not exist in a vacuum— they are part of a larger ecosystem of papers,related to each other through different conceptualrelations.
in this paper, we claim a better under-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages719–731august1–6,2021.©2021associationforcomputationallinguistics719speech papersvision papersnlp paperscitation graphmlpapers  “ [...]  the very deep convolutional networks are inspired by the “vggnet” architecture introduced in [16] for the 2014 imagenet classification challenge, with the central idea to replace large convolutional kernels by small 3×3 kernels.
[…]   given the recent popularity of lstms for acoustic modeling, we have experimented with such models on the switchboard task[…] ”“the ibm 2016 english  conversational telephone speech recognition system”interspeech 2016salient?nosalient?yesstanding of a research article relies not only on itscontent but also on its relations with associatedworks, using both the content of related papers andthe paper’s position in the larger citation network.
we use a concrete example to motivate how in-formation from the citation graph helps with sciie,considering the task of identifying key entities in along document (known as “salient entity classiﬁca-tion”) in figure 1..in this example, we see a paper describing aspeech recognition system (saon et al., 2016).
fo-cusing on two speciﬁc entities in the paper (“ima-genet classiﬁcation challenge” and “switchboardtask”), we are tasked with classifying whether eachis critical to the paper.
this task requires reasoningabout each entity in relation to the central topic ofthe paper, which is a daunting task for nlp con-sidering that this paper contains over 3000 wordsacross 11 sections.
an existing state-of-the-artmodel (jain et al., 2020) mistakenly predicts thenon-salient entity “imagenet classiﬁcation chal-lenge” as salient due to the limited contextual infor-mation.
however, this problem is more approach-able when informed of the structure of the citationgraph that conveys how this paper correlates withother research works.
examining this example pa-per’s position in the surrounding citation networksuggests it is concerned with speech processing,which makes it unlikely that “imagenet” is salient.2the clear goal of incorporating inter-article in-formation, however, is hindered by a resource chal-lenge: existing sciie datasets that annotate paperswith rich entity and relation information fail toinclude their references in a ﬁne-grained, machine-readable way.
to overcome this difﬁculty, we buildon top of an existing sciie dataset and align itwith a source of citation graph information, whichﬁnally allows us to explore citation-aware sciie..architecturally, we adopt the neural multi-taskmodel introduced by jain et al.
(2020), and es-tablish a proof of concept by comparing simpleways of incorporating the network structure andtextual content of the citation graph into this model.
experimentally, we rigorously evaluate our meth-ods, which we call citationie, on three tasks: men-tion identiﬁcation, salient entity classiﬁcation, anddocument-level relation extraction.
we ﬁnd thatleveraging citation graph information provides sig-niﬁcant improvements in the latter two tasks, in-.
cluding a 10 point improvement on f1 score forrelation extraction.
this leads to a sizable increasein the performance of the end-to-end citationiesystem relative to the current state-of-the-art, jainet al.
(2020).
we offer qualitative analysis of whyour methods may work in §5.3..2 document-level scientiﬁc ie.
2.1 task deﬁnition.
we consider the task of extracting document-levelrelations from scientiﬁc texts..most work on scientiﬁc information extractionhas used annotated datasets of scientiﬁc abstracts,such as those provided for semeval 2017 and se-meval 2018 shared tasks (augenstein et al., 2017;g´abor et al., 2018), the scierc dataset (luan et al.,2018), and the biocreative v chemical diseaserelation dataset (wei et al., 2016)..we focus on the task of open-domain document-level relation extraction from long, full-text doc-uments.
this is in contrast to the above methodsthat only use paper abstracts.
our setting is alsodifferent from works that consider a ﬁxed set ofcandidate relations (hou et al., 2019; kardas et al.,2020) or those that only consider ie tasks otherthan relation extraction, such as entity recognition(verspoor et al., 2011)..we base our task deﬁnition and baseline modelson the recently released scirex dataset (jain et al.,2020), which contains 438 annotated papers,3 allrelated to machine learning research..each document consists of sections d ={s1, .
.
.
, sn }, where each section contains a se-quence of words si = {wi,1, .
.
.
, wi,ni}.
eachdocument comes with annotations of entities, coref-erence clusters, cluster-level saliency labels, and4-ary document-level relations.
we break downthe end-to-end information extraction process as asequence of these four related tasks, with each tasktaking the output of the preceding tasks as input..mention identiﬁcation for each span of textwithin a section, this task aims to recognize if thespan describes a task, dataset, method, ormetric entity, if any..coreference this task requires clustering all en-tity mentions in a document such that, in eachcluster, every mention refers to the same entity(varkel and globerson, 2020).
the scirex dataset.
2our proposed method actually makes correct predictionson both these samples, where the baseline model fails on both..3the dataset contains 306 documents for training, 66 for.
validation, and 66 for testing..720includes coreference annotations for each task,dataset, method, and metric mention..salient entity classiﬁcation given a cluster ofmentions corresponding to the same entity, themodel must predict whether the entity is key tothe work described in a paper.
we follow the deﬁ-nition from the scirex dataset (jain et al., 2020),where an entity in a paper is deemed salient if itplays a role in the paper’s evaluation..relation extraction the ultimate task in ourie pipeline is relation extraction.
we con-sider relations as 4-ary tuples of typed entities(etask, edataset, emethod, emetric), which arerequired to be salient entities.
given a set of candi-date relations, we must determine which relationsare contained in the main result of the paper..2.2 baseline model.
we base our work on top of the model of jain et al.
(2020), which was introduced as a strong baselineaccompanying the scirex dataset.
we refer thereader to their paper for full architectural details,and brieﬂy summarize their model here..this multi-task model performs three of ourtasks (mention identiﬁcation, saliency classiﬁca-tion, and relation extraction) in a sequence, treatingcoreference resolution as an external black box.
while word and span representations are sharedacross all tasks and updated to minimize multi-taskloss, the model trains each task on gold input.
fig-ure 2 summarizes the baseline model’s end-to-endarchitecture, and highlights the places where wepropose improvements for our citationie model..feature extraction the model extracts featuresfrom raw text in two stages.
first, contextualizedword embeddings are obtained for each section byrunning scibert (beltagy et al., 2019) on thatsection of text (up to 512 tokens).
then, the embed-dings from all words over all sections are passedthrough a bidirectional lstm (graves et al., 2005)to contextualize each word’s representation withthose from other sections..mention identiﬁcation the baseline modeltreats this named entity recognition task as aniobes sequence tagging problem (reimers andgurevych, 2017).
the tagger takes the scibert-bilstm (beltagy et al., 2019; graves et al., 2005)word embeddings (as shown in the figure 2),feeds them through two feedforward networks (not.
shown in figure 2), and produces tag potentialsat each word.
these are then passed to a crf(lafferty et al., 2001) which predicts discrete tags..span embeddings for a given mention span, itsspan embedding is produced via additive attention(bahdanau et al., 2014) over the tokens in the span..coreference using an external model, pairwisecoreference predictions are made for all entity men-tions, forming coreference clusters..salient entity classiﬁcation saliency is a prop-erty of entity clusters, but it is ﬁrst predicted at theentity mention level.
each entity mention’s spanembedding is simply passed through two feedfor-ward networks, giving a binary saliency prediction.
to turn these mention-level predictions intocluster-level predictions, the predicted saliencyscores are max-pooled over all mentions in a coref-erence cluster to give cluster-level saliency scores..relation extraction the model treats relationextraction as binary classiﬁcation, taking as inputa set of 4 typed salient entity clusters.
for eachentity cluster in the relation, per-section entity clus-ter representations are computed by taking the setof that entity’s mentions in a given section, andmax-pooling over the span embeddings of thesementions.
the four entity-section embeddings (onefor each entity in the relation) are then concate-nated and passed through a feedforward networkto produce a relation-section embedding.
then, therelation-section embeddings are averaged over allsections and passed through another feedforwardnetwork which returns a binary prediction..3 citation-aware sciie dataset.
although citation network information has beenshown to be effective in other tasks, few workshave recently tried using it in sciie systems.
onepotential reason is the lack of a suitable dataset..thus, as a ﬁrst contribution of this paper, weaddress this bottleneck by constructing a sciiedataset that is annotated with citation graph infor-mation.4 speciﬁcally, we combine the rich anno-tations of scirex with a source of citation graphinformation, s2orc (lo et al., 2020).
for each pa-per, s2orc includes parsed metadata about whichother papers cite this paper, which other papers are.
4we have released code to construct this dataset: https:.
//github.com/viswavi/scigraphie.
721figure 2: architecture of the model we use for neural information extraction.
light blue blocks indicate placeswhere we can incorporate information from the citation graph for the citation-aware citationie architecture..in addition to the 5 documents we could notmatch to the s2orc citation graph, 7 were incor-rectly recorded as containing no references and 5others were incorrectly recorded as having no ci-tations.
these errors are due to data issues in thes2orc dataset, which relies on pdf parsers toextract information (lo et al., 2020)..figure 3: degree statistics of scirex documents in thecitation graph..4 citationie.
cited by this paper, and locations in the body textwhere reference markers are embedded..to merge scirex with s2orc, we link recordsusing metadata obtained via the semantic scholarapi:5 paper title, doi string, arxiv id, and se-mantic scholar paper id.
for each document inscirex, we check against all 81m documents ins2orc for exact matches on any of these identi-ﬁers, yielding s2orc entries for 433 out of 438documents in scirex.
the ﬁnal mapping is in-cluded in our repository for the community to use.
though our work only used the scirex dataset,our methods can be readily extended to other sciiedatasets (including those mentioned in §2.1) usingour released software..statistics examining the distribution of citationsfor all documents in the scirex dataset (in fig-ure 3), we observe a long-tailed distribution of ci-tations per paper, and a bell-shaped distribution ofreferences per paper..5https://www.semanticscholar.org/.
we now describe our citation-aware scientiﬁc ie ar-chitecture, which incorporates citation informationinto mention identiﬁcation, salient entity classiﬁ-cation, and relation extraction.
for each task, weconsider two types of citation graph information,either separately or together: (1) structural infor-mation from the graph network topology and (2)textual information from the content of citing andcited documents..4.1 structural information.
the structure of the citation graph can contextual-ize a document within the greater body of work..prior works in scientiﬁc information extractionhave predominantly used the citation graph only toanalyze the content of citing papers, such as cite-textrank (das gollapalli and caragea, 2014) andcitation tf-idf (caragea et al., 2014), which isdescribed in detail in §4.2.2.
however, the citationgraph can be used to discover relationships betweennon-adjacent documents in the citation graph; priorworks struggle to capture these relationships..722bilstmtitle/abstracttheibm...recurrentneuralnetworksinputdocumentmention identificationsalient entity classificationcoreference clusteringrelation extractionintro.
(part 1)thelandscapeof...ourfindingsintro.
(part 2)recurrentnetswith...sigmoidactivationsfeature extractioncrf b-tasksciberti -taskb-methodi -methodl-methodooooob-methodl-methodb-methodl -methodo“ibm asr system”=task:“asr"method:“rnn”additive attention“rnn”“language model”ffffspan embeddingffffspan embeddingnon-salientffffffff✓dataset:“switchboard”metric:“wer”task:“asr"method:“language model”ffff×dataset:“switchboard”metric:“wer”[cite]usedrecurrent…citation sentencessalientffffsalientsalientrelation:{    task:        asr,    method:      rnn,    dataset:    switchboard,    metric:        wer,}relation:{    task:        asr,    method:      language       model,    dataset:    switchboard,    metric:        wer}010002000in-degree050100150frequency050out-degree02040architecture, though the input to these networksvaries from task to task (scibert-bilstm embed-dings for mention identiﬁcation, span embeddingsfor salient entity classiﬁcation, and per-section re-lation embeddings for relation extraction)..this architecture gives two options for where toconcatenate the graph embedding into the hiddenstate - stage 1 or stage 2 - marked with a lightblue block in figure 4. intuitively, concatenatingthe graph embedding in a later stage feeds it moredirectly into the ﬁnal prediction.
we ﬁnd stage1 is superior for relation extraction, and both per-form comparably for salient entity classiﬁcationand mention identiﬁcation.
we give details on thisexperiment in appendix a.3..4.2 textual information.
most prior work using the citation graph for sciiehas focused on using the text of citing papers.
weexamine how to use two varieties of textual infor-mation related to citations..4.2.1 citances.
citation sentences, also known as “citances”(nakov et al., 2004), provide an additional sourceof textual context about a paper.
they have seenuse in automatic summarization (yasunaga et al.,2019), but not in neural information extraction..in our work, we augment each document in ourtraining set with its citances, treating each citancein this way,as a new section in the document.
we incorporate citances into our citationie modelthrough the shared text representations used byeach task in our system, as shown in figure 5. if ourdocument has many citations, we randomly sample25 to use.
for each citing document, we selectcitances centered on the sentence containing theﬁrst reference marker pointing to our document ofinterest, and include the subsequent and consequentsentences if they are both in the same section..we ensure the mention identiﬁcation step doesnot predict entities in citance sections, which wouldlead to false positive entities in downstream tasks..4.2.2 citation tf-idf.
citation tf-idf (caragea et al., 2014), is a featurerepresenting the tf-idf value (jones, 1972) of agiven token in its document’s citances.
we con-sider a variant of this feature: for each token in adocument, we compute the tf-idf of that token ineach citance of the document, and average the per-citance tf-idf values over all citances.
we imple-.
figure 4: feedforward architecture in each task (withcitationie-speciﬁc parameters shown in light blue)..arnold and cohen (2009) are the only priorwork, to our knowledge, to explicitly use the cita-tion graph’s structure for scientiﬁc ie.
they predictkey entities related to a paper via random walks ona combined knowledge-and-citation-graph consist-ing of papers and entities, without considering adocument’s content.
this approach is simple butcannot generalize to new or unseen entities..a rich direction of recent work has studiedlearned representations of networks, such as so-cial networks (perozzi et al., 2014) and citationgraphs (sen et al., 2008; yang et al., 2015; buiet al., 2018; khosla et al., 2021).
in this paper,we show citation graph embeddings can improvescientiﬁc information extraction..construction of citation graph to constructour citation graph, we found all nodes in thes2orc citation graph within 2 undirected edgesof any document in the scirex dataset, includingall edges between those documents.
this processtook 10 hours on one machine due to the massivesize of the full s2orc graph, resulting in a graphwith ∼1.1m nodes and ∼5m edges..network representation learning we learnrepresentations for each node (paper) using deep-walk6 (perozzi et al., 2014) via the graphvitelibrary (zhu et al., 2019), resulting in a 128-dimensional “graph embedding” for each documentin our dataset.
for each task, we incorporate thedocument-level graph embedding into that task’smodel component, by simply concatenating thedocument’s graph embedding with the hidden statein that component.
we do not update the graphembedding values during training..incorporating graph embedding each task inour citationie system culminates in a pair of feed-forward networks.
figure 4 describes this general.
6an empirical comparison by khosla et al.
(2021) founddeepwalk to be quite competitive on two citation graph nodeclassiﬁcation datasets, despite its speed and simplicity..723outputstage 1stage 2relation extraction this is the ultimate task inour pipeline.
we use its output and metrics to evalu-ate the end-to-end system, but also evaluate relationextraction separately from upstream componentsto isolate its performance.
we speciﬁcally considertwo types of metrics:(1) document-level: for each document, given aset of ground truth 4-ary relations, we evaluate aset of predicted 4-ary relations as a sequence ofbinary predictions (where a matching relation is atrue positive).
we then compute precision, recall,and f1 scores for each document, and average eachover all documents.
we refer to this metric asthe “document-level” relation metric.
to comparewith jain et al.
(2020), this is the primary metric tomeasure the full system.
(2) corpus-level: when evaluating the relation ex-traction component in isolation, we are also able touse a more standard “corpus-level” binary classi-ﬁcation evaluation, where each candidate relationfrom each document is treated as a separate sample.
we also run both these metrics on a binary rela-tion extraction setup, by ﬂattening each set of 4-aryrelations into a set of binary relations and evaluat-ing these predictions as an intermediate metric..5.1.2 baselines.
for each task, we compare against jain et al.
(2020),whose architecture our system is built on.
no othermodel to our knowledge performs all the tasks weconsider on full documents.
for the 4-ary relationextraction task, we also compare against the doc-taet model (hou et al., 2019), which is consid-ered as state-of-the-art for full-text scientiﬁc rela-tion extraction (jain et al., 2020; hou et al., 2019)..signiﬁcance to improve the rigor of our eval-uation, we run signiﬁcance tests for each of ourproposed methods against its associated baseline,via paired bootstrap sampling (koehn, 2004).
in ex-periments where we trained multiple models withdifferent seeds, we perform a hierarchical bootstrapprocedure where we ﬁrst sample a seed for eachmodel and then sample a randomized test set..5.1.3 training details.
we build our proposed citationie methods on topof the scirex repository7 (jain et al., 2020) in theallennlp framework (gardner et al., 2018)..for each task, we ﬁrst train that component inisolation from the rest of the system to minimize.
7https://github.com/allenai/scirex.
figure 5: incorporating citances into the text represen-tation extractor..mented this feature only for saliency classiﬁcation,as it explicitly reasons about the signiﬁcance of atoken in citing texts.
as a local token-level feature,it also does not apply naturally to relation extrac-tion, which operates on entire clusters of spans..4.3 graph structure and text content.
we lastly consider using graph embeddings and ci-tances together in a single model for each task.
wedo this naively by including citances with the docu-ment’s input text when ﬁrst computing shared textfeatures, and then concatenating graph embeddingsinto downstream task-speciﬁc components..5 experiments.
5.1 metrics, baselines and training.
5.1.1 metrics.
the ultimate product of our work is an end-to-enddocument-level relation extraction system, but wealso measure each component of our system inisolation, giving end-to-end and per-task metrics.
all metrics, except where stated otherwise, are thesame as described by jain et al.
(2020)..mention identiﬁcation we evaluate mentionidentiﬁcation with the average f1 score of clas-sifying entities of each span type..salient entity classiﬁcation similar to jainet al.
(2020) we evaluate this task at the mentionlevel and cluster level.
we evaluate both metricson gold standard entity recognition inputs..724bilstmtheibm...recurrentneuralnetworksscibert[cite]usedrecurrent…citation sentence#1paper content...introducedby[cite]citation sentence#25model.
f1.
p.r.salient mention evaluation.
baseline (reported)baseline (reimpl.)
citationiew/ citation-tf-idfw/ citancesw/ graph embeddingsw/ graph + citance.
57.957.5.
57.158.7†59.2†58.4†.
57.550.5.
50.251.453.5†51.3.
58.466.8.
66.168.5†66.367.8†.
salient entity cluster evaluation.
baseline (reimpl.)
citationiew/ citation-tf-idfw/ citancesw/ graph embeddings.
39.1.
28.5.
75.8.
38.638.740.3.
28.428.229.8.
74.374.874.5.table 1: salient entity classiﬁcation results.
baseline(jain et al., 2020) and graph embedding model eval-uations are each trained with 3 different model seeds,then metrics averaged; rest are from single model dueto computational limitations.
† indicates signiﬁcance at95% conﬁdence.
best model is in bold for each metric..the task-speciﬁc loss.
we then take the best per-forming modiﬁcations and use them to train end-to-end ie models to minimize the sum of lossesfrom all tasks.
we train each model on a singlegpu with batch size 4 for up to 20 epochs.
weinclude detailed training conﬁguration informationin appendix a.1..for saliency classiﬁcation and relation extrac-tion, we trained the baseline and the strongest pro-posed models three times,8 to improve reliability ofour results.
for mention identiﬁcation, we did notretrain models, as the ﬁrst set of results stronglysuggested our proposed methods were not helpful..5.2 quantitative results.
mention identiﬁcation for mention identiﬁca-tion, we observe no major performance differencefrom using citation graphs, and include full resultsin appendix a.2..salient entity classiﬁcation table 1 shows theresults of our citationie methods.
we observe:(1) using citation graph embeddings signiﬁcantlyimproves the system with respect to the salient men-tion metric.
(2) graph embeddings do not improve cluster eval-uation signiﬁcantly (at 95%) due to the small test.
8see appendix a.1 for exact seeds used9reported as “component-wise binary and 4-ary rela-.
tions” in jain et al.
(2020).
size10 (66 samples) and inter-model variation.
(3) incorporating graph embeddings and citancessimultaneously is no better than using either.
(4) our reimplemented baseline differs from theresults reported by jain et al.
(2020) despite usingtheir published code to train their model.
this maybe because we use a batch size of 4 (due to computelimits) while they reported a batch size of 50..relation extraction table 2 shows that usinggraph embeddings here gives an 11.5 point im-provement in document-level f1 over the reportedbaseline,11 and statistically signiﬁcant gains onboth corpus-level f1 metrics..despite seemingly large gains on the document-level f1 metric, these are not statistically signiﬁcantdue to signiﬁcant inter-model variability and smalltest set size, despite the graph embedding modelperforming best at every seed we tried..end-to-end model from table 3, we observe:(1) using graph embeddings appears to have a posi-tive effect on the main task of 4-ary relation extrac-tion.
however, these gains are not statistically sig-niﬁcant (p = 0.235) despite our proposed methodoutperforming the baseline at every seed, for thesame reasons as mentioned above.
(2) on binary relation evaluation, we observesmaller improvements which had a lower p-value(p = 0.099) due to lower inter-model variation.
(3) using citances instead of graph embeddingsstill appears to outperform the baseline (though bya smaller margin than the graph embeddings)..5.3 analysis.
we analyzed our experimental results, guided bythe following four questions:.
do papers with few citations beneﬁt from cita-tion graph information?
our test set only con-tains two documents with zero citations, so we can-not characterize performance on such documents.
however, figure 6 shows that the gains providedby the proposed citationie model with graph em-beddings counterintuitively shrink as the numberof citations of a paper increases.
we also observe.
10the limited size of this test set is an area of concern whenusing the scirex dataset, and improving statistical power insciie evaluation is a crucial area for future work..11the large gap between reimplemented and reported base-lines is likely due to our reproduced results averaging over 3random seeds.
when using the same seed used by jain et al.
(2020), the baseline’s document-level test f1 score is almost20 points better than with two other random seeds..72557.049.865.5.
69.268.567.5.
61.150.8.
69.272.966.2.
0.70.23.
1.317.03.
6.54.09.
5.424.97.model.
f1.
p.r.f1.
p.r.document-level metric.
corpus-level metric.
4-ary relation extraction.
baseline (reported)9baseline (reimpl.)
doctaetcitationiew/ citancesw/ graph embeddingsw/ graph + citance.
baseline (reported)baseline (reimpl.)
citationiew/ citancesw/ graph embeddingsw/ graph + citance.
document-level metric.
corpus-level metric.
binary relation extraction.
82.050.162.4.
70.067.566.8.
53.151.1.
69.270.465.9.
44.050.185.1.
76.676.275.0.
71.851.1.
71.356.168.1.n/a48.039.9.
39.458.7†51.9.n/a41.2.
43.351.0†48.0†.
n/a48.155.7.
39.961.0†54.6.n/a48.4.
46.754.1†51.4.n/a48.256.8.
41.959.654.5.n/a44.6.
44.057.152.7.table 2: comparing methods on relation extraction.
baseline, graph embedding, and graph + citance modelswere evaluated over 3 model seeds, and the remainder with a single seed.
we use macro-f1 for corpus-levelevaluation.
† indicates signiﬁcance at 95% conﬁdence, and best implemented model in each metric is bolded.
graph embeddings signiﬁcantly improve over baseline on 4-ary and binary corpus-level f1 (p < 0.05), but are lesssigniﬁcant on document-level f1 metrics (p ≈ 0.11)..model.
f1.
p.r.4-ary relation extraction.
baseline (reported)baseline (reimpl.)
citationiew/ graph embeddingsw/ citances.
baseline (reported)baseline (reimpl.)
citationiew/ graph embeddingsw/ citances.
0.80.44.
1.480.75.
9.66.48.
7.707.61.
17.322.66.
20.0413.36.
41.143.83.
37.1743.57.binary relation extraction.
figure 6: document-level relation extraction f1 scoreof citationie models with graph embeddings (left) andcitances (right), compared with the baseline (red) ondocuments grouped by number of citations..table 3: end-to-end model evaluation.
each model wasevaluated over 3 model seeds..large swaths of text.
this is particularly usefulsince neural models still struggle to model long-range dependencies effectively (brown et al., 2020)..this with citances, to a lesser extent.
this suggestsmore work needs to be done to represent citationgraph nodes with many edges..how does citation graph information help re-lation extraction?
with relation extraction, wefound citation graph information provides strongestgains when classifying relations between distantentities in a document, seen in figure 7. for eachrelation in the test set, we computed the averagedistance between pairs of entity mentions in thatrelation, normalized by total document length.
weﬁnd models with graph embeddings or citances per-form markedly better when these relations span.
does citation graph information help contextu-alize important terms?
going back to our moti-vating example of a speech paper referring to ima-genet in passing §1, we hypothesized that addingcontext from citations helps deal with terms that areimportant in general, but not for a given document.
to measure this, we grouped all entities in ourtest dataset by their “global saliency rate” measuredon the test set: given a span, what is the probabilitythat this span is salient in any given occurrence?.
in figure 8, we observe that most of the improve-ment from graph embeddings and citances comesat terms which are labeled as salient in at least 20%.
726(0, 70)(70, 450)(450, 12k)0.00.20.40.60.81.0document-level f1baselinew/ graph(0, 70)(70, 450)(450, 12k)baselinew/ citancesfigure 7: corpus-level f1 of relation extraction mod-els, bucketed by the average distance between entitymentions in each relation..acknowledgments.
the authors thank sarthak jain for assisting with re-producing baseline results, bharadwaj ramachan-dran for giving advice on ﬁgures, and siddhantarora and rishabh joshi for providing suggestionson the paper.
the authors also thank the anony-mous reviewers for their helpful comments.
thiswork was supported by the air force researchlaboratory under agreement number fa8750-19-2-0200. the u.s. government is authorized toreproduce and distribute reprints for governmen-tal purposes notwithstanding any copyright nota-tion thereon.
the views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the ofﬁcialpolicies or endorsements, either expressed or im-plied, of the air force research laboratory or theu.s. government..figure 8: macro f1 of salient mention classiﬁcationmodels, evaluated on test-set spans, each bucketed bytheir training-set global saliency rate..references.
of their training-set mentions.
this suggests thatcitation graph information yields improvementswith reasoning about important terms, without neg-atively interfering with less-important terms..6.implications and future directions.
we explore the use of citation graph information inneural scientiﬁc information extraction with cita-tionie, a model that can leverage either the struc-ture of the citation graph or the content of citingor cited documents.
we ﬁnd that this information,combined with document text, leads to particularlystrong improvements for salient entity classiﬁca-tion and relation extraction, and provides an in-crease in end-to-end ie system performance over astrong baseline..our proposed methods reﬂect some of the sim-plest ways of incorporating citation graph informa-tion into a neural sciie system.
as such, theseresults can be considered a proof of concept.
inthe future we will explore ways to extract richer in-formation from the graph using more sophisticatedtechniques, hopefully better capturing the interplaybetween citation graph structure and content.
fi-nally, we evaluated our proof of concept here ona single dataset in the machine learning domain.
while our methods are not domain-speciﬁc, verify-ing that these methods generalize to other scientiﬁcdomains is important future work..andrew o. arnold and william w. cohen.
2009. infor-mation extraction as link prediction: using curatedin-citation networks to improve gene detection.
ternational conference on wireless algorithms, sys-tems, and applications (wasa), 5682:541–550..isabelle augenstein, mrinal das, sebastian riedel,lakshmi vikraman, and andrew mccallum.
2017.semeval 2017 task 10: scienceie - extractingkeyphrases and relations from scientiﬁc publica-in proceedings of the 11th internationaltions.
workshop on semantic evaluation (semeval-2017),pages 546–555, vancouver, canada.
association forcomputational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlylearning to align and translate.
iclr..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inemnlp/ijcnlp..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..thang d. bui, sujith ravi, and vivek ramavajjala.
2018. neural graph learning: training neural net-works using graphs.
proceedings of the eleventh.
727(0.0, 0.29)(0.29, 0.37)(0.37, 0.4)(0.4, 0.54)0.00.20.40.60.81.0corpus-level f1baselinew/ graph(0.0, 0.29)(0.29, 0.37)(0.37, 0.4)(0.4, 0.54)baselinew/ citances(0.0, 0.2)(0.2, 0.4)(0.4, 0.6)(0.6, 0.8)(0.8, 1.0)0.00.20.40.60.8macro-f1baselinew/ graph(0.0, 0.2)(0.2, 0.4)(0.4, 0.6)(0.6, 0.8)(0.8, 1.0)baselinew/ citancesacm international conference on web search anddata mining..cornelia caragea, florin adrian bulgarov, andreeagodea, and sujatha das gollapalli.
2014. citation-enhanced keyphrase extraction from research pa-in proceedings ofpers: a supervised approach.
the 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1435–1446, doha, qatar.
association for computationallinguistics..sujatha das gollapalli and cornelia caragea.
2014.extracting keyphrases from research papers using ci-tation networks.
proceedings of the aaai confer-ence on artiﬁcial intelligence, 28(1)..kata g´abor, davide buscaldi, anne-kathrin schu-mann, behrang qasemizadeh, ha¨ıfa zargayouna,and thierry charnois.
2018. semeval-2018 task7: semantic relation extraction and classiﬁcation inscientiﬁc papers.
in proceedings of the 12th inter-national workshop on semantic evaluation, pages679–688, new orleans, louisiana.
association forcomputational linguistics..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthew pe-ters, michael schmitz, and luke zettlemoyer.
2018.allennlp: a deep semantic natural language pro-in proceedings of workshop forcessing platform.
nlp open source software (nlp-oss), pages 1–6, melbourne, australia.
association for computa-tional linguistics..alex graves, santiago fern´andez, and j¨urgen schmid-huber.
2005. bidirectional lstm networks for im-inproved phoneme classiﬁcation and recognition.
proceedings of the 15th international conferenceon artiﬁcial neural networks: formal models andtheir applications - volume part ii, icann’05,page 799–804, berlin, heidelberg.
springer-verlag..sonal gupta and christopher manning.
2011. analyz-ing the dynamics of research by extracting key as-pects of scientiﬁc papers.
in proceedings of 5th in-ternational joint conference on natural languageprocessing, pages 1–9, chiang mai, thailand.
asianfederation of natural language processing..ken hallenbeck.
2020. the covid-19 deluge: is it timefor a new model of data disclosure?
asbmb today:the member magazine of the american society forbiochemistry and molecular biology..yufang hou, charles jochim, martin gleize, francescabonin, and debasis ganguly.
2019.identiﬁca-tion of tasks, datasets, evaluation metrics, and nu-meric scores for scientiﬁc leaderboards construction.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages5203–5213, florence, italy.
association for compu-tational linguistics..sarthak jain, madeleine van zuylen, hannaneh ha-jishirzi, and iz beltagy.
2020. scirex: a chal-lenge dataset for document-level information extrac-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 7506–7516, online.
association for compu-tational linguistics..karen sparck jones.
1972. a statistical interpretationof term speciﬁcity and its application in retrieval.
journal of documentation..marcin kardas, piotr czapla, pontus stenetorp, se-bastian ruder, sebastian riedel, ross taylor, androbert stojnic.
2020. axcell: automatic extractionof results from machine learning papers.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages8580–8594, online.
association for computationallinguistics..megha khosla, vinay setty, and avishek anand.
2021.a comparative study for unsupervised network rep-resentation learning.
ieee transactions on knowl-edge and data engineering, 33(5):1807–1818..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..john lafferty, andrew mccallum, and fernando c.n.
pereira.
2001. conditional random ﬁelds: prob-abilistic models for segmenting and labeling se-quence data.
in icml..kyle lo, lucy lu wang, mark neumann, rodney kin-ney, and daniel weld.
2020. s2orc: the semanticscholar open research corpus.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4969–4983, online.
as-sociation for computational linguistics..yi luan, luheng he, mari ostendorf, and hannanehhajishirzi.
2018. multi-task identiﬁcation of enti-ties, relations, and coreference for scientiﬁc knowl-edge graph construction.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3219–3232, brussels, bel-gium.
association for computational linguistics..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-in proceedings of the 2019namic span graphs.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3036–3046, minneapolis, minnesota.
association for computational linguistics..preslav i. nakov, ariel s. schwartz, and marti a.hearst.
2004. citances: citation sentences for se-.
728zhaocheng zhu, shizhen xu, meng qu, and jian tang.
2019. graphvite: a high-performance cpu-gpu hy-brid system for node embedding.
in the world wideweb conference, pages 2494–2504.
acm..in in proceed-mantic analysis of bioscience text.
ings of the sigir’04 workshop on search and dis-covery in bioinformatics..bryan perozzi, rami al-rfou, and steven skiena.
2014. deepwalk: online learning of social represen-tations.
in proceedings of the 20th acm sigkddinternational conference on knowledge discoveryand data mining, pages 701–710..nils reimers and iryna gurevych.
2017. optimal hy-perparameters for deep lstm-networks for sequencelabeling tasks.
arxiv, abs/1707.06799..george saon, tom sercu, steven rennie, and hong-kwang j. kuo.
2016. the ibm 2016 english con-versational telephone speech recognition system.
ininterspeech..prithviraj sen, galileo namata, mustafa bilgic, lisegetoor, brian gallagher, and tina eliassi-rad.
2008.collective classiﬁcation in network data.
ai maga-zine, 29:93–106..yuval varkel and amir globerson.
2020. pre-trainingmention representations in coreference models.
inemnlp..karin m. verspoor, k. cohen, arrick lanfranchi,c. warner, helen l. johnson, christophe roeder,jinho d. choi, christopher s. funk, yuriy malenkiy,miriam eckert, nianwen xue, w. baumgartner,m. bada, martha palmer, and l. hunter.
2011. acorpus of full-text journal articles is a robust evalua-tion tool for revealing differences in performance ofbiomedical natural language processing tools.
bmcbioinformatics, 13:207 – 207..chih-hsuan wei, yifan peng, robert leaman, al-lan peter davis, carolyn j marringly, jiao li,thomas c wiegers, and zhiyong lu.
2016. assess-ing the state of the art in biomedical relation extrac-tion: overview of the biocreative v chemical-diseaserelation (cdr) task.
database : the journal of biolog-ical databases and curation..cheng yang, zhiyuan liu, deli zhao, maosong sun,and edward y. chang.
2015. network representa-tion learning with rich text information.
in ijcai..michihiro yasunaga, jungo kasai, rui zhang, alexan-der r. fabbri,irene li, dan friedman, anddragomir r. radev.
2019. scisummnet: a largeannotated corpus and content-impact models for sci-entiﬁc paper summarization with citation networks.
proceedings of the aaai conference on artiﬁcial in-telligence, 33(01):7386–7393..dani yogatama, michael heilman, brendan o’connor,chris dyer, bryan r. routledge, and noah a. smith.
2011. predicting a scientiﬁc community’s responseto an article.
in proceedings of the 2011 conferenceon empirical methods in natural language process-ing, pages 594–604, edinburgh, scotland, uk.
asso-ciation for computational linguistics..729a.3 combining graph embeddings with.
word embeddings.
each of our task-speciﬁc components in the cita-tionie model contains two feedforward networkswhere we may concatenate graph embedding infor-mation.
we refer to these two options for where tofuse graph embedding information as ”early fusion”and ”late fusion”, illustrated in figure 4..here we show a detailed comparison of earlyfusion vs late fusion models on mention identiﬁ-cation (table 5), salient entity classiﬁcation (ta-ble 6), and relation extraction (table 7).
basedon these results, we used early fusion in our ﬁnalcitationie models for mention identiﬁcation andrelation extraction.
for saliency classiﬁcation, therelative performance of early fusion and late fu-sion differed across our two metrics, making thisinconclusive.
we used early fusion for saliencyclassiﬁcation in the end-to-end model due to strongempirical performance there..model.
f1.
p.r.mention identiﬁcation.
graph embed.
(early fusion)graph embed.
(late fusion).
74.4†74.1.
74.4†73.1.
74.375.1†.
table 5: comparing citationie models for mentionidentiﬁcation with early graph embedding fusion vslate fusion.
results are shown from single-model evalu-ation.
† indicates signiﬁcance at 95% conﬁdence.
bestmodel is in bold for each metric..model.
f1.
p.r.salient mention evaluation.
graph embed.
(early fusion)graph embed.
(late fusion).
57.159.2†.
54.4†53.5.
60.166.3†.
salient entity cluster evaluation.
graph embed.
(early fusion)graph embed.
(late fusion).
43.3†40.3.
33.8†29.8.
72.074.5†.
table 6: comparing citationie models for salient en-tity classiﬁcation with early graph embedding fusionvs late fusion.
the early fusion model was trained once,while late fusion numbers are reported over an averageof 3 runs.
† indicates signiﬁcance at 95% conﬁdence.
best model is in bold for each metric..a appendices.
a.1 training conﬁgurations.
we train each model on a single 11gb nvidiageforce rtx 2080 ti gpu with a batch sizeof 4. we train for up to 20 epochs, and set thepatience parameter in allennlp to 10; if thevalidation metric does not improve for 10 consecu-tive epochs, we stop training early.
for each task-speciﬁc model, we use a product of validation lossand corpus-level binary f1 score on the validationset as the validation metric.
for salient entity classi-ﬁcation and relation extraction, we choose the bestthreshold on the validation set using f1 score..in total, training with these conﬁgurations takesroughly 2 hours for salient entity classiﬁcation, 8hours for mention identiﬁcation, 18-24 hours forrelation extraction, and 24-30 hours for the end-to-end system.
our citationie models took roughlyas long to train as the baseline scirex models did.
for models that we trained three different times,.
we use different seeds for each software library:.
• for pytorch, we use seeds 133,12 11, and 22.
• for numpy, we use seeds 1337, 111, and 222.
• for python’s random library, we use seeds.
11370, 1111, and 2222.a.2 mention identiﬁcation results.
model.
f1.
p.r.mention identiﬁcation.
baseline (reported)13.
70.7.
71.7.
71.2.baseline (reimpl.)
w/ citancesw/ graph embeddingsw/ graph + citance.
74.6†74.074.473.6.
73.773.074.4†73.0.
75.6†75.074.374.3.table 4: mention identiﬁcation results.
† indicates sig-niﬁcance at 95% conﬁdence.
best model is in bold foreach metric..we include results from using citation graph in-formation for the mention identiﬁcation task in ta-ble 4. we observe no major improvements in thistask.
intuitively, recognizing a named entity in adocument may not require global context about thedocument (e.g.
“lstm” almost always refers to amethod, regardless of the paper where it is used),so the lack of gains in this task is unsurprising..12133/1337/13370 is the default seed setting in allennlp..730model.
f1.
p.r.f1.
p.r.graph embeddings (early fusion)graph embeddings (late fusion).
graph embeddings (early fusion)graph embeddings (late fusion).
document-level metrics.
corpus-level metrics.
4-ary relation extraction.
68.563.3.
72.958.3.
67.561.8.
70.458.0.
76.267.3binary relation extraction.
58.775.8†.
56.159.0.
51.053.6.
61.076.0†.
54.158.1†.
59.676.1†.
57.166.4.document-level metrics.
corpus-level metrics.
table 7: comparing citationie models for relation extraction with early graph embedding fusion vs late fusion.
early fusion models were trained 3 times, late fusion was trained once.
† indicates signiﬁcance at 95% conﬁdence,and the best model in each metric is bolded..731