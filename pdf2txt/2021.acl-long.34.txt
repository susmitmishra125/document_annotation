a training-free and reference-free summarization evaluation metric viacentrality-weighted relevance and self-referenced redundancy.
irwin king1.
wang chen1∗.
piji li21department of computer science and engineering,the chinese university of hong kong, shatin, n.t., hong kong2tencent ai lab1{wchen, king}@cse.cuhk.edu.hk2pijili@tencent.com.
abstract.
summarization.
in recent years, reference-based and super-vised summarization evaluation metrics havebeen widely explored.
however, collectinghuman-annotated references and ratings arecostly and time-consuming.
to avoid theselimitations, we propose a training-free andreference-freeevaluationmetric.
our metric consists of a centrality-weighted relevance score and a self-referencedredundancy score.
the relevance score iscomputed between the pseudo reference builtfrom the source document and the givensummary, where the pseudo reference contentis weighted by the sentence centrality toprovide importance guidance.
besides anf1-based relevance score, we also design anfβ-based variant that pays more attention tothe recall score.
as for the redundancy scoreof the summary, we compute a self-maskedsimilarity score with the summary itself toevaluate the redundantinformation in thesummary.
finally, we combine the relevanceand redundancy scores to produce the ﬁnalevaluation score of the given summary.
ex-tensive experiments show that our methodscan signiﬁcantly outperform existing methodson both multi-document and single-documentsummarization evaluation.
the source codeis released at https://github.com/chen-wang-cuhk/training-free-and-ref-free-summ-evaluation..1.introduction.
text summarization systems have been developedrapidly due to the appearance of sequence-to-sequence frameworks (sutskever et al., 2014; bah-danau et al., 2015; see et al., 2017; chan et al.,2020), transformer architectures (vaswani et al.,2017) and large-scale pre-training models (devlinet al., 2019; liu et al., 2019).
how to accurately.
∗this work was mainly done when wang chen was an.
intern at tencent ai lab..evaluate the summaries generated from these sys-tems also attracts more and more attention in thisresearch area.
one of the most accurate evaluationmethods is human evaluation.
however, humanevaluation is expensive, time-consuming, and non-reproducible.
thus, it is necessary to develop au-tomatic evaluation metrics for text summarizationsystems.
existing automatic summarization evalu-ation metrics can be roughly categorized into twogroups: reference-based metrics and reference-freemetrics.
in this work, we focus on reference-freemetrics..reference-free summarization evaluation met-rics have been developed in parallel in multi-document summarization and single-documentsummarization.
the sota reference-free methodfor multi-document summarization evaluation, su-pert (gao et al., 2020), predicts a relevance scorefor each (document, summary) pair to estimate theinformativeness of the summary and then averagesall the scores from multiple documents as the ﬁ-nal evaluation score.
for each pair, supert em-ploys the top-ranked sentences which are ranked bythe position or centrality as a pseudo reference ofthe document and then applies bertscore (zhanget al., 2020) to produce a relevance score betweenthe pseudo reference and the given summary.
thesota single-document summarization reference-free evaluation metric, ls score (wu et al., 2020),combines a learned linguistic scorer for the sum-mary and a cosine similarity scorer for the (docu-ment, summary) pair to produce the ﬁnal score..although supert and ls score achieve thesota performance on their own areas respectively,they still have several drawbacks.
for example,supert only considers the relevance score be-tween the document and the summary while ignor-ing the other aspects such as how much redundantinformation is contained in the summary.
besides,supert assumes that all pseudo reference sen-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages404–414august1–6,2021.©2021associationforcomputationallinguistics404tences are equally-important.
however, in the realworld, the key information of a document is un-evenly distributed over sentences.
therefore, suchan assumption may introduce extra noise for theevaluation.
note that although supert may em-ploy sentence centrality to select document sen-tences as a pseudo reference, they ignore the sen-tence centrality after the selection and still treatthe selected sentences equally-important.
as forls score, although it does not require a referenceduring the evaluation of a summary, it requiresa large-scale training dataset with reference sum-maries to train the linguistic scorer.
besides theintrinsic drawbacks in these sota methods, to ourbest knowledge, there is no reference-free evalua-tion metric showing that it can achieve the sotaperformance on both multi-document and single-document summarization..to solve the above limitations, based on su-training-free andpert, we propose a novelreference-free metric for both multiple and singledocument summarization evaluation.
our metric iscomposed of a centrality-weighted relevance scoreand a self-referenced redundancy score..for the relevance score which is employed toestimate the informativeness of the summary, weincorporate the following new features.
first, un-like previous work which only utilizes the token-level representations, motivated by clark et al.
(2019), we engage a hybrid way that contains bothtoken-level representations and sentence-level rep-resentations to encode the document and the sum-mary.
the purpose of the hybrid representationis to enable our method to consider richer map-ping styles (i.e., token-to-token, sentence-to-token,and sentence-to-sentence) and help to produce amore comprehensive evaluation score.
second,we utilize the sentence centrality computed fromsentence-level representations of the source doc-ument to produce the importance weights of thepseudo reference sentences and tokens.
based onthe weights, we compute a weighted relevancescore that is more precise by considering the rela-tive importance.
third, besides the f1 version ofour relevance score, we also propose an adaptivefβ version where recall is considered β times asimportant as precision.
β is computed based on thelength ratio between the pseudo reference and thegiven summary.
the motivation is to punish theshort summary that can easily get high precisionwhile covering very limited important information.
in the pseudo reference (i.e., low recall)..to measure the redundancy of a summary, wedesign a simple but effective self-referenced simi-larity score.
if a summary contains much redundantinformation, there must exist plenty of semanticallysimilar tokens or sentences.
based on this assump-tion, we use the summary itself as the referenceand input a (summary, summary) pair into a self-masked bertscore to produce a redundancy scorethat evaluates the averaged degree of semantic sim-ilarity of each token or sentence with other tokensor sentences..after obtaining the centrality-weighted rele-vance score and the self-referenced redundancyscore, we combine them to predict the ﬁnal evalua-tion score.
depending on either f1 or fβ is appliedin our relevance score, we propose two variants ofour method: the f1-based version and the fβ-basedversion.
extensive experiments are conducted onboth multi-document and single-document summa-rization datasets.
the results show that our f1-based method already outperforms all the sotabaselines on all datasets.
moreover, our fβ-basedmethod can further improve the performance onmulti-document summarization datasets..our contributions are summarized as follows:(1) a novel training-free and reference-free summa-rization evaluation metric which considers both rel-evance and redundancy; (2) a centrality-weightedrelevance score that effectively utilizes the sentencecentrality of the documents to provide importanceguidance for the pseudo reference tokens and sen-tences.
besides the f1 version, we also developan fβ based relevance score which pays more at-tention to recall; (3) a self-referenced redundancyscore that utilizes a self-masked bertscore todetect the duplicated information of the given sum-mary; (4) to the best of our knowledge, we arethe ﬁrst evaluation metric that can achieve sotaperformance on both multiple and single documentsummarization under the reference-free setting..2 preliminary.
notations.
we denote vectors as bold lowercasecharacters and matrices as bold uppercase charac-ters.
the characters that are not bold are used todenote scalars.
calligraphy uppercase charactersare utilized to represent sets..problem deﬁnition.
we formally deﬁne thereference-free summarization evaluation problemas follows.
give a set of documents d =.
405figure 1: overall framework of our method.
w and s are the token-level and sentence-level representations.
nand n (m and m ) are the token number and the sentence number of the summary (pseudo reference).
for multi-document summary (i.e., k > 1), we compute relevance scores between the summary x and each document dk,and then average them as the ﬁnal relevance score..{d1, d2, ..., dk} and a generated summary x, thegoal is to predict a score to represent the overallquality of the summary.
k = 1 and k > 1 indi-cate single-document and multi-document summa-rization respectively..3 our methodology.
the overall framework is illustrated in figure 1.our ﬁnal evaluation score of a summary consists ofan averaged centrality-weighted relevance scoreand a self-referenced redundancy score.
bothscores are calculated on a semantic-level instead ofutilizing n-gram overlapping.
the averaged rele-vance score is computed from the relevance scorebetween the summary and each document in thedocument set.
the redundancy score is calculatedbased on the summary itself..3.1 centrality-weighted relevance score.
our relevance score aims to estimate the informa-tiveness of the given summary.
we ﬁrst encodeeach document in the document set and the sum-mary into hidden representations.
then, for eachdocument, we select essential sentences by central-ity to build a pseudo reference.
next, we computea centrality-weighted relevance score between thesummary and each pseudo reference.
finally, weaverage all the relevance scores as the ﬁnal rel-evance score of the summary.
we use the k-thdocument dk and a summary x as an example toshow the workﬂow..encoding.
following supert (gao et al., 2020),we ﬁrst split the document dk and the summary xinto sentences.
then, the pre-trained sbert1 isemployed to encode the tokens of each sentenceinto token-level contextual hidden representations.
we also apply max-pooling on all the tokens of asentence to obtain the sentence-level hidden repre-sentation.
following previous work, when utilizingthe token-level representations to compute the rel-evance and redundancy scores, we will ﬁlter outthe non-informative tokens such as stop-words toimprove the efﬁciency..building pseudo reference.
we do not chooseall the document sentences of dk to evaluate therelevance of the summary.
because the whole doc-ument usually contains plenty of unimportant sen-tences which may introduce extra noise for the rel-evance evaluation.
thus, we select important docu-ment sentences to build a pseudo reference r for theevaluation.
the sentence selection is based on thecentrality of each sentence, which is computed bythe unsupervised algorithm, pacsum (zheng andlapata, 2019), using the sentence-level represen-tation.
after obtaining the centrality scores of allsentences of the document, we choose the top-m 2sentences as the pseudo reference.
besides, wenormalize the centrality scores to [0, 1] and denotethe normalized centrality scores of the selected sen-.
1bert-large-nli-stsb-mean-tokens2in experiments, we follow the default conﬁguration of.
supert and set m as 12 for all the datasets..406summary 𝑥document 𝑑#bert𝐰%&…𝐰’&𝐬%&…𝐬)&𝐰%*…𝐰+*𝐬%*…𝐬,*centrality-based sentence selectioncentrality-weighted bertscore(𝐹%or 𝐹.
)self-masked bertscore(𝐹%)sentence weightsmergefinal scoreaveraged relevance scoreredundancy score𝐾pseudoreference 𝑟1, ¯as.
m ] where ¯as.
tences as ¯as = [¯as2, ..., ¯asi ∈ [0, 1]and the superscript s means sentence-level.
wedenote the pseudo reference building process aspacsumtopm.
computing relevance score with one pseudoreference.
instead of only using token-level rep-resentations, we also leverage the sentence-levelrepresentations to provide multi-level information.
the hybrid representations of the summary x andthe pseudo reference r are denoted as follows:.
bertscore (zhang et al., 2020).
for brevity, wedenote the j-th element of x as xj, the i-th elementof rk as ri, and the i-th element of a as ai:.
recall =.
p recision =.
(cid:80).
(cid:80).
,.
(cid:80).
i ai maxj sim(ri, xj)i aij maxi sim(ri, xj)|x|.
,.
f1 =.
2 ∗ recall ∗ p recisionrecall + p recision.
,.
(8).
(9).
(10).
x = [wxrk = [wr.
1 , ..., wx1, ..., wr.
n, sxm, sr.1, ..., sx1, ..., sr.n ],m ],.
(1).
(2).
where n and n (m and m ) are the token numberand sentence number of the summary (pseudo ref-erence).
w and s represent the token and sentencehidden representations respectively..besides the hybrid representations, we also in-troduce a centrality weighting scheme to weightthe tokens and sentences of the pseudo reference,which is different from previous work that eithertreats them equally or uses the surface statisticslike idf as the weights.
based on the centralityscores of the selected pseudo reference sentencesi.e., ¯as = [¯as2, ..., ¯asm ], we assign the weightsof the pseudo reference tokens as follows:.
1, ¯as.
¯aw = [¯awj = ¯as¯aw.
1 , ¯awi:wj ∈si,.
2 , ..., ¯aw.
m],.
(3).
(4).
where ¯ai:wj ∈si indicates the token wj inherits thecentrality score from its sentence si.
since wehave already removed the non-informative tokensin the token-level representations of each sentence,the remaining tokens capture the key informationof the sentence and consequently it is reasonableto perform such a weight inheritance.
next, wecombine token weights ¯aw and sentence weights ¯asto get the ﬁnal normalized centrality-based weightsof the hybrid representations:.
a = [awj = ¯awawi = ¯asas.
1, ..., asm, as1 , ..., awj /sum([¯aw; ¯as]),i /sum([¯aw; ¯as]),.
m ],.
(5).
(6).
(7).
where “[·; ·]” represents concatenation..based on the hybrid representations (i.e., xand rk) and the centrality-based weights of thepseudo reference tokens and sentences (i.e., a),we compute the relevance score between the sum-mary and the pseudo reference by a weighted.
where “sim” denotes the cosine similarity and |x|equals to n + n .
recall, p recision, and f1 arein the range of [-1, 1]..besides the f1 version, we also propose an adap-.
tive fβ version of relevance score as follows:.
fβ =.
β2 =.
(1 + β2) ∗ recall ∗ p recisionrecall + β2 ∗ p recision.
,.
1,2,( |rk||x| )1/γ, otherwise.
if ( |rk|if ( |rk|.
|x| )1/γ ≤ 1|x| )1/γ ≥ 2.
(11).
,.
(12).
where |rk| = m+m , |x| = n+n , and γ is a pos-itive integer hyper-parameter.
in our experiments,γ is set as 2 after ﬁne-tuning on the validationdataset and is ﬁxed for all the testing datasets.
thephysical meaning of β is that the recall score isconsidered β times as important as the p recisionscore.
in summarization evaluation, the coverageof the key information is always the most importantquality indicator of the summary.
thus, we set thelower bound of β as 1. on the other hand, the met-ric should not only evaluate the key informationcoverage, containing less unimportant content inthe summary should also be considered.
therefore,2. as shown inwe set the upper bound of β as2], β adaptivelyeq.12, within the range of [1,changes according to the ratio between |rk| and|x|.
the intuition comes from that a longer pseudoreference implies more key information needs tobe covered by the summary.
besides, a shortersummary can easily get high precision but coversvery limited important information in the pseudoreference.
thus, we give recall a higher weightto punish such short summaries when the pseudoreference is long..√√.
final averaged relevance score.
after com-puting the centrality-weighted relevance score be-tween the summary and the pseudo reference ofeach source document, we employ the average as.
407the ﬁnal relevance score of the summary:∗ , ..., f k.scorerel = mean([f 1.
∗ , ..., f k.∗ ]),.
(13).
where * is 1 for the f1 variant and β for the fβvariant.
the superscript k indicates the f∗ scoreis computed with the k-th document.
note thatscorerel ∈ [−1, 1] and higher is better..3.2 self-referenced redundancy score.
in this section, we introduce our self-referencedredundancy score.
we engage the summary itselfas the reference to evaluate the degree of the se-mantic similarity between each summary token orsentence with the other tokens or sentences.
theaveraged semantic similarity degree is used as theredundancy score.
the computation is based on aself-masked bertscore as follows:(cid:80).
i maxj:i(cid:54)=j sim(xj, xi)|x|.
,.
(14).
scorered =.
where “j : i (cid:54)= j” means we do not consider thesimilarity between xi and itself, i.e, self-masked.
because of the symmetric property, the f1, preci-sion, and recall scores are equal with each other.
this is also the reason that we use precision ineq.14 as the ﬁnal redundancy score.
note thatscorered ∈ [−1, 1] and lower is better..3.3 final evaluation score.
after obtaining the relevance score and the redun-dancy score, we apply a linear combination to pro-duce the ﬁnal evaluation score of the summarybased on the document set:.
score =.
scorerel − λ ∗ scorered1 + λ.,.
(15).
where 0 < λ ≤ 1 is a hyper-parameter toscale the redundancy score and score ∈ [−1, 1].
higher score means better summary quality.
inour experiments, after ﬁne-tuning on the vali-dation set, λ is set as 0.6 and is ﬁxed for allthe testing datasets.
we denote the variants ofour ﬁnal method as ours(fβ)-pacsumtopm andours(f1)-pacsumtopm depending on whetherthe adaptive fβ is employed..4 experiment setup.
4.1 datasets.
for comprehensively investigating our summariza-tion evaluation methods, we test our methods onboth multi-document and single-document sum-marization datasets.
we leverage tac3 datasets.
dataset.
|t opic|.
valid.
tac-2010tac-2011tac-2009tac-2008cnndm.
test..46444448499.document|set| ave.s ave.t651.823.2560.520.1705.824.9660.023.3921.136.0.
101010101.summary|systems| ave.s ave.t118.9120.9117.6119.673.2.
4.34.34.14.23.5.
435055584.table 1: statistics of datasets.
“valid.” and “test.” in-dicate the dataset is used for validation and testing, re-spectively.
“|t opic|” is the number of topics.
undereach topic, a set of documents is given and summariesare from different systems associating with human-annotated quality scores.
“|set|” is the number of doc-uments in the document set.
“ave.s” and “ave.t” rep-resent the averaged sentence number and token numberper document or summary.
note that the token numberis counted after the tokenization.
“|systems|” denotesthe number of summarization systems in the dataset..for multi-document summarization evaluation test-ing.
we choose tac-2010 as the validation datasetand tac-2008/tac-2009/tac-2011 as the testingdatasets.
following previous work, we only uti-lize the initial summaries in tac datasets, i.e., thesummaries for the document set a. for the single-document summarization evaluation, we employcnndm4 (chaganty et al., 2018) as the testingdataset.
the statistics of these datasets are shownin table 1. note that the hyper-parameters of ourmethods are ﬁne-tuned on tac-2010 and then ﬁxedfor all the testing datasets..for tac datasets, we compute correlation coef-ﬁcients between predicted scores of an evaluationmethod and the annotated pyramid scores of sum-maries to measure the effectiveness of the method.
following gao et al.
(2020), a correlation is com-puted for each topic.
then, the averaged correlationfrom all the topics is engaged as the ﬁnal correla-tion of the method with human ratings..for cnndm dataset, correlations are calculatedwith the human scores in three dimensions includ-ing overall, grammar, and redundancy.
follow-ing wu et al.
(2020), the correlation is computedbetween predicted scores of the 499 × 4 = 1996(document, summary) pairs with corresponding hu-man ratings..4.2 baselines.
in this section, we brieﬂy introduce our baselines.
we choose tf-idf, js (louis and nenkova,2013), and repear (rioux et al., 2014) as tra-ditional reference-free baselines.
all these tradi-tional baselines do not build pseudo references and.
3https://tac.nist.gov/.
4https://bit.ly/price-of-debiasing.
408table 2: main results on multi-document summarization datasets.
pearson’s r, spearman’s ρ, and kendall’s τ withhuman scores are reported.
the best results are bold and the second-best results are underlined..method.
tac-2011ρ.τ.r.tac-2009ρ.τ.r.tac-2008ρ.τ.r.0.313tf-idf0.377js0.377reaperours(f1)-all0.495ours(fβ)-all0.4980.436rouge-1-pacsumtopm0.429rouge-2-pacsumtopmrouge-l-pacsumtopm0.436moverscore-pacsumtopm 0.5210.291s+wms-pacsumtopm0.386c-elmo-pacsumtopm0.332c-sbert-pacsumtopmsupert-pacsumtopm0.511supert-idf-pacsumtopm 0.507ours(f1)-pacsumtopm0.531ours(fβ)-pacsumtopm0.541.
0.2940.3330.3340.4510.4550.3770.3880.3700.4750.2920.3020.2930.4810.4760.4930.505.
0.2090.2400.2370.3290.3320.2740.2870.2720.3510.2110.2170.2070.3570.3530.3650.374.
0.3720.3760.3580.4780.4800.4180.3800.4270.4830.3500.3170.3140.4860.4850.5020.507.
0.3820.3810.3570.4760.4710.4060.4190.4150.4850.3580.2350.2770.4940.4920.5060.508.
0.2790.2790.2560.3530.3480.3010.3140.3060.3620.2640.1670.1970.3680.3670.3810.380.
0.3750.3850.2870.4660.4620.3970.4100.3850.4790.3640.2100.1830.4930.4890.4950.500.
0.3410.3380.2610.4260.4230.3480.3550.3360.4400.3580.1620.1960.4570.4500.4610.465.
0.2430.2420.1870.3100.3070.2520.2590.2450.3230.2600.1140.1430.3340.3280.3370.339.method.
overallρ.r.grammarρ.τ.redundancyρ.τ.r.0.264tf-idf0.265js0.036reaper−ls score (wu et al., 2020)ours(f1)-all0.390ours(fβ)-all0.3610.224rouge-1-pacsumtopm0.347rouge-2-pacsumtopmrouge-l-pacsumtopm0.235moverscore-pacsumtopm 0.3730.324s+wms-pacsumtopm0.355c-elmo-pacsumtopm0.405c-sbert-pacsumtopmsupert-pacsumtopm0.384supert-idf-pacsumtopm 0.382ours(f1)-pacsumtopm0.416ours(fβ)-pacsumtopm0.400.
0.2490.2320.0320.3340.3700.3370.2150.3350.2240.3410.3530.2970.3780.3740.3730.4040.381.τ.
0.1870.1740.024−0.2810.2550.1590.2530.1660.2590.2670.2230.2860.2840.2830.3080.290.r.0.1860.2100.004−0.3060.2730.1260.2540.1350.2640.2400.2320.2950.3180.3160.3410.314.
0.1700.180-0.0060.2660.3060.2700.1140.2400.1220.2400.2560.2010.2990.3170.3140.3410.311.
0.1270.136-0.005−0.2320.2040.0840.1810.0900.1810.1930.1510.2250.2400.2380.2590.235.
0.2810.317-0.020−0.4130.3950.2890.3980.3000.4110.3600.4250.4150.3810.3770.4280.427.
0.2530.278-0.0310.2880.3810.3560.2540.3690.2640.3590.3850.3540.3730.3690.3650.4080.395.
0.1870.208-0.024−0.2870.2680.1860.2740.1930.2670.2860.2620.2790.2770.2740.3080.298.table 3: main results on single-document summarization dataset (cnndm).
pearson’s r, spearman’s ρ, andkendall’s τ with human scores are reported.
the best results are bold and the second-best results are underlined..directly utilize the full content of the documents.
for fairness, we also show the performance of ourmethods without building pseudo reference.
wedenote them as ours(f1)-all and ours(fβ)-allsince they use the whole document as a reference..we also extend several popular reference-based methods as baselines.
we adapt rouge-1/2/l (lin, 2004), moverscore (zhao et al., 2019),and s+wms (clark et al., 2019) into the reference-free scenario via building the pseudo reference withthe pacsumtopm method.
we add the sufﬁx “-pacsumtopm” to these baseline names to indi-cate the pseudo reference building process..besides, the sota reference-free summary eval-uation metrics are also selected as our strong base-lines, including c-elmo/c-sbert (sun andnenkova, 2019), supert/supert-idf (gaoet al., 2020), and ls score (wu et al., 2020).
c-elmo (c-sbert) encodes the document and the.
summary using the pre-trained elmo (sbert)and then computes their cosine similarity.
supert-idf is an extension of supert, which utilizes theinverse document frequency (idf) as the impor-tance weight of each token.
for fair comparisons,we also apply the same pseudo reference build-ing process i.e., pacsumtopm, to c-elmo/c-sbert/supert/supert-idf and add the sufﬁx“-pacsumtopm” to the their names..5 results and analysis.
5.1 main results.
the main experimental results on multi-documentsummarization datasets are shown in table 2.we ﬁnd that our f1 version (i.e., ours(f1)-pacsumtopm) already consistently outperformsall the baselines, which indicates the effectivenessof our centrality-weighted relevance score and ourself-referenced redundancy score.
the results also.
409method.
ours(f1)ours(fβ)moverscore+centralityw.
+redundancy+both.
20110.4930.5050.4750.4720.2370.261.tac20090.5060.5080.4850.4670.2020.220.cnndm.
2008 overall grammar redundancy0.4610.4650.4400.4310.2210.241.
0.3410.3110.2400.2570.3260.341.
0.4080.3950.3590.3640.5460.545.
0.4040.3810.3410.3500.4480.455.table 4: spearman’s ρ of incorporating the central-ity weighting and redundancy score into moverscorebased framework.
“+both” means these two featuresare simultaneously applied..can consistently improve the performance for dif-ferent |set|.
for the single-document summariza-tion setting, i.e., |set|=1, it still obtains a positivegap.
nevertheless, when the |systems| is smallsuch as 4, applying our fβ leads to a dramatic per-formance dropping.
from table 1, we also seethat cnndm and tac-2011 have different sum-mary lengths (73.2 for cnndm and 120.9 for tac-2011).
however, when we limit the |systems| oftac-2011 to smaller numbers, the average lengthof generated summaries is still around 120, whichindicates the performance degeneration is indeedfrom the change of system numbers.
therefore, wesuggest using ours(fβ) when |systems| is largelike 12 and employing ours(f1) when |systems|is small like 4..5.2 ablation study.
for better understanding the contributions of ourproposed components, we conduct ablation stud-ies on the best-performed method on each dataset,i.e., ours(fβ) for the multi-document summariza-tion datasets and ours(f1) for the single-documentsummarization dataset.
we display results of therank-based spearman’s ρ in figure 3..as shown in the ﬁgure, after removing one ofthe three components (i.e., the centrality weight-ing, the hybrid representation, and the redundancyscore), the performance of our methods becomeworse in most cases.
this ﬁnding demonstratesthe effectiveness of our proposed components.
be-sides, we also note that removing the redundancyscore signiﬁcantly degrades the performance onthe redundancy evaluation on cnndm, which in-dicates our redundancy score effectively capturesthe redundancy degree of the summaries..5.3 apply centrality weighting and.
redundancy score into moverscore.
besides basing on bertscore, we also studywhether our key features i.e., the centrality weight-ing and redundancy score, can work well in a.figure 2: the gap of spearman’s ρ between ours(fβ)and ours(f1) on tac-2011 for different |set| and|systems|.
positive gaps mean our fβ can improve theperformance while negative gaps indicate our fβ de-grades the performance.
when changing one of them,the other is ﬁxed.
“all” means the full size is applied,i.e., 10 for |set| and 50 for |systems|..demonstrate that our fβ version can further im-prove the performance of multi-document sum-marization evaluation.
by comparing ours(fβ)-pacsumtopm and ours(fβ)-all, we see that thepseudo reference building process can signiﬁcantlyimprove the performance.
this is also the reasonwhy we apply the same pseudo reference buildingprocess into sota baselines for fair comparisons.
in the remaining part of this paper, we omit thesufﬁx “-pacsumtopm” for simplicity when wemention a method..we also test our methods on the single-documentsummarization dataset without further ﬁne-tuningthe hyper-parameters.
the main results are dis-played in table 3. we note that our f1 version stilloutperforms all the baselines, which manifests thehigh generalization ability of our f1-based method.
one interesting ﬁnding is that the performance sig-niﬁcantly drops after incorporating the fβ score..to study the reason for the performance degrada-tion on cnndm after incorporating fβ, we com-pare cnndm and tac datasets ﬁrst.
from table 1,we note the main differences between them are thesize of the document set for each topic (i.e., |set|)and the number of the summarization systems (i.e.,|systems|).
cnndm has much smaller |set| and|systems|.
we use the tac-2011 dataset as an ex-ample to investigate whether our fβ is unsuitablefor smaller |set| and |systems|.
we change |set|and |systems| respectively and report the gap ofspearman’s ρ between ours(fβ) and ours(f1) infigure 2. from the results, we observe that our fβ.
410123579alldifferent |set|−0.02−0.010.000.010.02ours(fβ)'s ρ - ours(f1)'s ρ0.0040.0110.0070.010.0080.0090.01248121620283644alldifferent |systems|−0.02−0.010.000.010.02ours(fβ)'s ρ - ours(f1)'s ρ-0.024-0.0070.0070.0140.0180.0180.0140.0140.012figure 3: ablation studies for ours(fβ) on tacdatasets and ours(f1) on cnndm.
“-centralityw.”means that we remove the centrality weighting whencomputing relevance scores.
“-hybridr.” representswe only utilize the token-level representations whencalculating relevance and redundancy scores.
“-redundancy” indicates we omit the redundancy score..moverscore based framework (i.e., the relevanceand redundancy scores are computed using mover-score).
note that our fβ is not applicable to mover-score since it is not an f -measure.
the results arelisted in table 4. we ﬁnd that these two featuressigniﬁcantly improve the performance of the orig-inal moverscore on single-document summariza-tion evaluation while degrading the performancedramatically on multi-document summarizationevaluation.
on cnndm, the enhanced mover-score even outperforms ours(f1) on the “overall”and “redundancy” aspects, which indicates mover-score is a promising basis for our proposed new fea-tures.
we leave solving the performance droppingof the enhanced moverscore on multi-documentsetting as future work..5.4 robustness analysis.
we investigate the robustness of our method onthe following factors and report the experimentalresults on the validation dataset (i.e., tac-2010) infigure 4: (1) the hyper-parameter λ for scaling theredundancy score; (2) the hyper-parameter γ in fβ;(3) the number of selected sentences for pseudo ref-erence i.e., m ; (4) different pre-trained contextualencoding models including bert-base5, bert-large6, roberta-base7, and roberta-large8..5bert-base-nli-stsb-mean-tokens6bert-large-nli-stsb-mean-tokens7roberta-base-nli-stsb-mean-tokens8roberta-large-nli-stsb-mean-tokens.
figure 4: the performance of ours(fβ) on tac-2010under different λ, γ, m , and encoding models.
whenwe change one of them, the others are ﬁxed.
the pear-son’s r and spearman’s ρ are reported..since both spearman’s ρ and kendall’s τare rank-based correlation coefﬁcients, we omitkendall’s τ for simplicity.
from this ﬁgure, weobserve that the performance of our method is rel-atively stable for different λ and γ. we also ﬁndthat a small m leads to lower correlations becausemuch important information may be abandonedwhen building the pseudo references.
but a largem will also degenerate the correlations since morenoises are introduced.
thus, a moderate m is better.
as for encoding models, we note that large encod-ing models obtain better performance than baseencoding models.
however, large models needmore computation resources and time to encodethe input text.
note that for our ﬁnal method, weonly ﬁne-tune λ and γ on the tac-2010 and setthem as 0.6 and 2. as for m and encoding mod-els, following the conﬁguration of supert (gaoet al., 2020), we directly set m as 12 and employthe bert-large as the encoding model.
all thesefactors are ﬁxed for all testing datasets..5.5 performance on bad/good summaries.
in this section, we evaluate the ability of ourmethod to distinguish bad and good summaries.
the bad and good summaries are selected by hu-man ratings.
we use tac-2011 as an example andchoose supert as a strong baseline.
the corre-sponding distributions of the reversed rank for badand good summaries are illustrated in figure 5. asmaller (larger) reversed rank represents the sum-mary is assigned with a lower (higher) score.
fromthe ﬁgure, we ﬁnd that compared with supert,our(fβ) has a better ability to assign bad sum-.
411tac2011tac2009tac20080.300.350.400.450.500.550.60spearman's ρ0.5050.5080.4650.5010.510.460.5020.5090.4680.4980.4960.466ablation study on tac datasetsours(fβ)-centralityw.-hybridr.-redundancyoverallgrammarredundancy0.200.250.300.350.400.450.50spearman's ρ0.4040.3410.4080.3990.3320.4060.40.3370.4060.3820.330.374ablation study on cnndm datasetours(f1)-centralityw.-hybridr.-redundancy0.10.20.30.40.50.60.70.80.91.0different λ0.5500.5750.6000.625correlationsrρ123456different γ0.5500.5750.6000.625correlationsrρ36912151821different m0.5500.5750.6000.625correlationsrρbert-basebert-largeroberta-baseroberta-large0.5500.5750.6000.625correlationsrρcombines a linguistic quality scorer trained fromthe built positive and negative summaries, and arelevance scorer based on cosine similarity.
theothers do not require training (louis and nenkova,2013; rioux et al., 2014; peyrard, 2019; sun andnenkova, 2019).
for instance, supert (gao et al.,2020) builds the pseudo references from the sourcedocument ﬁrst and then engages bertscore tocompute the relevance score between the pseudoreference and the summary..7 conclusion.
in this paper, we propose a novel training-freeand reference-free summarization evaluation met-ric consisting of a relevance score and a redun-dancy score.
experiments on multi-document andsingle-document summarization settings show theeffectiveness of our methods.
one promising fu-ture direction is to solve the performance droppingissue after applying our key features into mover-score and the other is to tackle the problem thatcurrent metrics struggle to assign higher scores forgood summaries..acknowledgements.
the work described in this paper was partiallysupported by the research grants council of thehong kong special administrative region, china(cuhk 2410021, research impact fund (rif),r5034-18)..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..florian b¨ohm, yang gao, christian m. meyer, orishapira, ido dagan, and iryna gurevych.
2019. bet-ter rewards yield better summaries: learning to sum-in proceedings of themarise without references.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3110–3120, hong kong,china.
association for computational linguistics..arun chaganty, stephen mussmann, and percy liang.
2018. the price of debiasing automatic metrics innatural language evalaution.
in proceedings of the56th annual meeting of the association for com-putational linguistics (volume 1: long papers),.
figure 5: distributions of the reversed rank from su-pert and ours(fβ) for bad and good summaries ontac-2011.
the bar in the middle indicates the median..maries lower scores and good summaries higherscores, which demonstrates the effectiveness ofour method again.
moreover, we also note thatboth supert and ours(fβ) are good at giving badsummaries lower scores while having difﬁculty inassigning good summaries higher scores.
we leavesolving this problem as another future work underthe reference-free setting..6 related work.
reference-based evaluation metrics mainlymeasure the relevance between the human-annotated references and the system-generatedtext, which are widely adopted in text summa-rization (lin, 2004; zhao et al., 2019), machinetranslation (papineni et al., 2002; zhang et al.,2020), and dialogue systems (papineni et al., 2002;gao et al., 2021; xiang et al., 2021).
for exam-ple, rouge (lin, 2004) evaluates the token se-quence overlapping.
bertscore (zhang et al.,2020), s+wms (clark et al., 2019), and mover-score (zhao et al., 2019) measure the semanticsimilarity between the references and the summaryvia a greedy or optimized minimum earth mover’sdistance..reference-free evaluation metrics have beendeveloped to avoid the dependency on human-annotated references, which obtain more and moreattention in recent years (b¨ohm et al., 2019; gaoet al., 2020; wu et al., 2020; chan et al., 2021).
some of them need to train a scorer (peyrard andgurevych, 2018; xenouleas et al., 2019; scialomet al., 2019; b¨ohm et al., 2019).
for example,ls score (wu et al., 2020) designs a metric which.
412supertours(fβ)010203040reversed rankreversed rank of bad summariessupertours(fβ)010203040reversed rankreversed rank of good summariespages 643–653, melbourne, australia.
associationfor computational linguistics..hou pong chan, wang chen, and irwin king.
2020.a uniﬁed dual-view model for review summariza-tion and sentiment classiﬁcation with inconsistencyloss.
in proceedings of the 43rd international acmsigir conference on research and development ininformation retrieval, sigir 2020, virtual event,china, july 25-30, 2020, pages 1191–1200.
acm..zhangming chan, lemao liu, juntao li, haisongzhang, dongyan zhao, shuming shi, and rui yan.
2021. enhancing the open-domain dialogue evalua-tion in latent space.
in proceedings of the 59th an-nual meeting of the association for computationallinguistics: findings..elizabeth clark, asli c¸ elikyilmaz, and noah a. smith.
2019. sentence mover’s similarity: automatic eval-in proceedings ofuation for multi-sentence texts.
the 57th conference of the association for compu-tational linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages2748–2760.
association for computational linguis-tics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jun gao, wei bi, ruifeng xu, and shuming shi.
2021.ream#: an enhancement approach to reference-based evaluation metrics for open-domain dialoggeneration.
in proceedings of the 59th annual meet-ing of the association for computational linguistics:findings..yang gao, wei zhao, and steffen eger.
2020. su-pert: towards new frontiers in unsupervised evalu-ation metrics for multi-document summarization.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 1347–1354.
associa-tion for computational linguistics..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..annie louis and ani nenkova.
2013. automaticallyassessing machine summary content without a goldstandard.
comput.
linguistics, 39(2):267–300..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..maxime peyrard.
2019. a simple theoretical model ofin proceedings ofimportance for summarization.
the 57th annual meeting of the association for com-putational linguistics, pages 1059–1073, florence,italy.
association for computational linguistics..maxime peyrard and iryna gurevych.
2018. objec-tive function learning to match human judgementsfor optimization-based summarization.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, naacl-hlt, new orleans, louisiana, usa, june 1-6, 2018,volume 2 (short papers), pages 654–660.
associa-tion for computational linguistics..cody rioux, sadid a. hasan, and yllias chali.
2014.fear the reaper: a system for automatic multi-document summarization with reinforcement learn-in proceedings of the 2014 conference oning.
empirical methods in natural language processing(emnlp), pages 681–690, doha, qatar.
associationfor computational linguistics..thomas scialom, sylvain lamprier, benjamin pi-wowarski, and jacopo staiano.
2019. answersunite!
unsupervised metrics for reinforced summa-in proceedings of the 2019 con-rization models.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3246–3256, hong kong, china.
as-sociation for computational linguistics..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..simeng sun and ani nenkova.
2019. the feasibilityof embedding based automatic evaluation for sin-in proceedings ofgle document summarization.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1216–1221, hong kong,china.
association for computational linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..413ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, 4-9 decem-ber 2017, long beach, ca, usa, pages 5998–6008..hanlu wu, tengfei ma, lingfei wu, tariromanyumwa, and shouling ji.
2020.unsuper-vised reference-free summary quality evaluation viacontrastive learning.
corr, abs/2010.01781..stratos xenouleas, prodromos malakasiotis, mariannaapidianaki, and ion androutsopoulos.
2019. sum-qe: a bert-based summary quality estimationmodel.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages6005–6011, hong kong, china.
association forcomputational linguistics..jiannan xiang, yahui liu, deng cai, huayang li, defulian, and lemao liu.
2021. assessing dialogue sys-tems with distribution distances.
in proceedings ofthe 59th annual meeting of the association for com-putational linguistics: findings..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in 8th inter-uating text generation with bert.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m. meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563–578, hongkong, china.
association for computational lin-guistics..hao zheng and mirella lapata.
2019. sentence cen-trality revisited for unsupervised summarization.
inproceedings of the 57th conference of the asso-ciation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 6236–6247.
association forcomputational linguistics..414