a human-machine collaborative framework forevaluating malevolence in dialogues.
yangjun zhanguniversity of amsterdamy.zhang6@uva.nl.
pengjie ren∗shandong universityrenpengjie@sdu.edu.cn.
maarten de rijkeuniversity of amsterdam& ahold delhaizem.derijke@uva.nl.
abstract.
conversational dialogue systems (cdss) arehard to evaluate due to the complexity of nat-ural language.
automatic evaluation of di-alogues often shows insufﬁcient correlationwith human judgements.
human evaluationis reliable but labor-intensive.
we introducea human-machine collaborative framework,hmceval, that can guarantee reliability of theevaluation outcomes with reduced human ef-fort.
hmceval casts dialogue evaluation as asample assignment problem, where we needto decide to assign a sample to a human ora machine for evaluation.
hmceval includesa model conﬁdence estimation module to esti-mate the conﬁdence of the predicted sample as-signment, and a human effort estimation mod-ule to estimate the human effort should thesample be assigned to human evaluation, aswell as a sample assignment execution mod-ule that ﬁnds the optimum assignment solu-tion based on the estimated conﬁdence and ef-fort.
we assess the performance of hmcevalon the task of evaluating malevolence in di-alogues.
the experimental results show thathmceval achieves around 99% evaluation ac-curacy with half of the human effort spared,showing that hmceval provides reliable eval-uation outcomes while reducing human effortby a large amount..introduction.
1conversational dialogue systems (cdss) are oftentrained to generate responses given unstructured,open-domain dialogues.
evaluation of cds re-sponses has drawn broad attention due to its cru-cial rule for cds development (deriu et al., 2020).
broadly speaking, there are two approaches to per-form dialogue evaluation: automatic evaluationand human judgements (finch and choi, 2020).
automatic evaluation metrics such as appropri-ateness (lowe et al., 2017), engagement (zhang.
∗∗ corresponding author..et al., 2020), are efﬁcient but have low agree-ment with human judgements due to the diver-sity of responses (liu et al., 2016), especially forword-overlap based metrics, such as bleu (pa-pineni et al., 2002) and rouge (lin and hovy,2002).
more recently, training based methods, e.g.,adem (lowe et al., 2017), ruber (tao et al.,2018) and contextualized methods, e.g.
bert-based ruber (ghazarian et al., 2019), have beenshown to have better agreement with human judge-ments.
however, these methods are still not reli-able enough: the pearson correlation with humanjudgments is 0.44 for appropriateness (lowe et al.,2017) and 0.55 for relevance (ghazarian et al.,2019).
to guarantee reliability of evaluation out-comes, our current best practice is to use humanjudgements.
in terms of most evaluation aspects,e.g., appropriateness (young et al., 2018), coher-ence (ram et al., 2018) and empathy (rashkin et al.,2019), human judgements simply show the highestreliability.
obviously, human judgments are morelabor-intensive than automatic evaluation (deriuet al., 2020)..the ﬂaws of automatic evaluation and the lackof speed and scalability of human evaluation lim-its the speed at which the community can developmore intelligent cdss.
for example, as part of thedaily research and development cycle of cdss, weneed to change the model design and retrain themodel multiple times, on a daily or even hourlybasis.
even if there is a minor change, we needto verify its performance again each time.
for an-other example, cds leaderboards are very popularrecently as a means to provide platforms for faircomparison (hou et al., 2019).
there are usuallydozens of models to evaluate, and new models areintroduced everyday.
practical scenarios like theabove two call for dialogue evaluation methods thatare both reliable and efﬁcient..in this paper, we propose the human-machine.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5612–5623august1–6,2021.©2021associationforcomputationallinguistics5612we demonstrate the effectiveness of hmcevalon dialogue malevolence evaluation (zhang et al.,2021).
the main reason we choose this partic-ular task is that dialogue malevolence is highlyrelated to social good (xu et al., 2020; shi et al.,2020), which is of vital importance for cdss, butit is hard to evaluate because of the need of deepsemantic understanding (das et al., 2020).
wecarry out experiments on the recently introducedmalevolent dialogue response detection and classi-fying (mdrdc) dataset (zhang et al., 2021).
ourresults show that the proposed hmceval frame-work signiﬁcantly surpasses machine evaluationand human judgement in terms of balancing relia-bility and effort.
hmceval achieves around 99%evaluation accuracy (compared to human evalua-tion) with as much as half of the human effort saved.
the results demonstrate that hmceval can be usedfor reliable and efﬁcient evaluation of cdss sincethe accuracy is high and the effort is signiﬁcantlyreduced compared to fully human evaluation..2 related work2.1 evaluation of cdssautomatic evaluation for cdss includes untrainedmethods and learning based methods.
early un-trained methods, such as perplexity (chen et al.,1998), and quality metrics bleu (papineni et al.,2002) and rouge (lin and hovy, 2002) arewidely used for cds but the aspects they evaluateare limited.
recent work based on word embed-dings cover more aspects, such as distinct-n fordiversity (li et al., 2016) or average word embed-ding similarity for coherence (luo et al., 2018).
most untrained methods have low agreement withhuman judgements (liu et al., 2016) because ma-chine responses are highly diversiﬁed, althougha few metrics have sufﬁcient agreement with hu-man, i.e., a pearson correlation of 0.69 for coher-ence (luo et al., 2018)..to address the problem of low agreement withhuman judgments, learning based methods havebeen developed (novikova et al., 2017; tao et al.,2018).
lowe et al.
(2017) propose adem to eval-uate the appropriateness of responses.
tao et al.
(2018) propose ruber, which shows better agree-ment with human judgments than adem.
ruberis designed for relevance and similarity by blend-ing relevance between the generated response withhuman ground truth and context.
several meth-ods utilize pretrained language models such asbert for automatic evaluation.
ghazarian et al..figure 1: human-machine collaborative evaluation(hmceval) framework.
r1, .
.
.
, rn are the generatedresponse samples to be evaluated.
r and e are reliabil-ity and efﬁciency, respectively..collaborative evaluation (hmceval) frameworkfor dialogue evaluation with the aim of balancingreliability and efﬁciency.
hmceval formulates thedialogue evaluation task as a sample assignmentproblem, i.e., if the machine can provide accurateoutcomes, most evaluation samples should be as-signed to the machine; otherwise, we should assignmore samples to human evaluators.
as shown infigure 1, automatic evaluation has low reliability al-though the efﬁciency is high; human judgement hashigh reliability but it is labor-intensive; hmcevalbeats the previous two methods in balancing reli-ability and efﬁciency.
finding a good balance be-tween reliability and efﬁciency is non-trivial as thetwo desiderata are often in conﬂict with each other.
usually, reliability is improved at the expense ofefﬁciency (chaganty et al., 2018)..there are three main modules in human-machinecollaborative evaluation (hmceval), namely themodel conﬁdence estimation (mce) module, thehuman effort estimation (hee) module, and thesample assignment execution (sae) module.
first,the mce module measures the conﬁdence of pre-dicted evaluation for each dialogue response basedsample.
our implementation of mce is based onthree estimation methods, namely, bert basedmaximum class probability (mcp), trust score(ts) (jiang et al., 2018), and true class probabil-ity (tcp) (corbi`ere et al., 2019).
ts and tcphave originally been introduced for images; weadd a bert layer to expand it to dialogues.
sec-ond, the hee module estimates the effort.
ourimplementation is based on annotation time costprediction by dialogue-related and worker-relatedfeatures.
third, the sae module decides whethera dialogue response sample should be assigned toa human or a machine for evaluation by maximiz-ing the conﬁdence and minimizing the (human)effort.
we implement the module by integer linearprogramming (ilp)..5613(2019) propose contextualized ruber, which out-performs ruber.
similarly, a predictive engage-ment metric is built by utilizing user engagementscore (ghazarian et al., 2020); quality is evalu-ated by transformer based language models with-out reference response (nedelchev et al., 2020).
the above methods cover more aspects and inte-grate linguistic features (tao et al., 2018), thus theagreement with human judgement is higher thanmost word-overlap based methods.
however, formost of the metrics, the model performance stillhas space to improve, for instance, the accuracy ofengagement is 0.76 (ghazarian et al., 2020).
ourproposed hmceval framework could be appliedto these metrics and improve general evaluationreliability with an acceptable amount of humaneffort..human judgement is applied in common evalu-ation aspects including ﬂuency, consistence, rel-evance, appropriateness, coherence, quality forcdss (finch and choi, 2020).
it is reliable, yetexpensive and time intensive, especially for largescale evaluation (hou et al., 2019).
in order toguarantee reliability, agreement among differentworkers is needed, which makes the high effortproblem more severe (das et al., 2020)..unlike the methods listed above, the hmcevalframework speciﬁcally aims to balance reliabilityand human effort for the evaluation of cdss..2.2 human-machine collaborationhuman-machine collaboration hybridizes machineprediction and human judgements.
previous re-search mostly focuses on using human judgmentsto help label the low reliability samples (callaghanet al., 2018; kyono et al., 2018; gates et al., 2020).
earlier research gives human the output of an au-tomatic model and lets human decide whether themodel prediction is reliable (lasecki et al., 2012).
however, people tend to ignore the predictions of amodel if it makes mistakes (dietvorst et al., 2015)since they are not tolerant to model mistakes.
insuch cases, predictive results are not fully utilizedand human effort increases.
at the same time, thereis a possibility that human annotators mistakenlyfollow the outputs of a model with errors (cum-mings, 2004).
both situations lead to failure ofhuman-machine collaboration..the core problem is to determine when a humanannotator should trust a model.
conﬁdence esti-mation for a model’s prediction has been proposedto help improve overall accuracy, correctness etc..for human-machine collaboration.
callaghan et al.
(2018) develop a hybrid cardiogram classiﬁcationhuman-machine collaborative (hmc) framework,which achieves better performance than a classiﬁerby itself and uses less expert resources compared toexpert classiﬁcation by itself.
kyono et al.
(2018)develop a man and machine mammography ora-cle that improves overall breast cancer diagnosticaccuracy, while reducing the number of radiolo-gist readings.
gates et al.
(2020) use abstrackrbased a hmc screening method to screen relevanttitle and abstract for paper reviews, which couldsave time of reviewers and have little risk of miss-ing relevant records.
however, the above methodsselect the top-k most unreliable samples and donot consider effort division between human andmachine.
chaganty et al.
(2018) are the ﬁrst tocombine machine and human evaluation to obtain areliable estimate at lower cost than human alone onsummarizing and open-source question answering,with cost reduction only 7–13%.
ravindranath et al.
(2020) build a highly cost-efﬁcient face recognitionhmc framework that outperforms both a machine-based method and a fully manual method, withboth reliability and effort considered.
however, themethods introduced previously are not suitable forhmc evaluation for dialogue because of focusingon non-dialogue tasks, low cost reduction, or notconsidering both reliability and effort..our proposed framework is purpose-built for di-alogue evaluation.
it leverages both human judge-ment and machine prediction by assigning low con-ﬁdence machine-generated samples to human work-ers, while minimizing overall human effort..3 methodology.
3.1 overview.
suppose we have a set of m samples {(ci, ˆxi)}mi=1to be evaluated.
here, ci is the dialogue contextand ˆxi is a response generated by a cds modelfg(c) → ˆx.
below, we propose a method toachieve reliable and efﬁcient evaluation of them samples under the constraint that a humancan annotate at most n (cid:28) m samples.
wepropose the human-machine collaborative eval-uation (hmceval) framework to solve this task.
hmceval is divided into three modules: sample as-signment execution (sae), model conﬁdence esti-mation (mce) and human effort estimation (hee)..56143.2 sae modulethe optimization problem of assigning m sam-ples to a human or machine can be solved bytractable integer linear programming, which is np-complete (papadimitriou and steiglitz, 1998).
first,we introduce the decision variable zi to denote thesample assignment to a human or machine:.
zi =.
(cid:40).
0,1,.sample i is assigned to a human;sample i is assigned to machine..(1).
second, we deﬁne two ilp objectives that try tomaximize the overall conﬁdence and minimize theoverall effort, respectively:.
max.
ˆaizi +.
bi(1 − zi),.
m(cid:88).
i=1m(cid:88).
i=1.
m(cid:88).
i=1m(cid:88).
i=1.
min.
kizi +.
ˆli(1 − zi),.
(2).
where (a) m is the total number of samples to evalu-ate generated by the generation model fg(c) → ˆx;(b) ˆai ∈ [0, 1] is the model conﬁdence for evalu-ating sample i; (c) bi is the human conﬁdence forevaluating sample i; (d) ki is the machine effortfor evaluating sample i; and (e) ˆli ∈ [0, 1] is thehuman effort for evaluating sample i..we use the weighted sum method (marler andarora, 2010) to solve eq.
2 so as to get the optimalzi.
the objective function in eq.
2 is transformedinto:.
max.
ˆaizi +.
bi(1 − zi)−.
(cid:34) m(cid:88).
i=1.
m(cid:88).
i=1.
(cid:32) m(cid:88).
λ.i=1.
m(cid:88).
i=1.
kizi +.
ˆli(1 − zi).
,.
(cid:33)(cid:35).
(3).
subject to.
m(cid:88).
i=1.
zi ≥ m − n.bi = 1 for i = 1, .
.
.
, mki = 0 for i = 1, .
.
.
, mλ ≥ 0..(4).
the constraints are motivated as follows: (a) thenumber of samples assigned to a human is less thanor equal to n ; (b) human conﬁdence is assumedto be 1; (c) machine effort is assumed to be 0; and(d) λ is greater than 0. n and λ are two parameters.
that we use to balance reliability and effort; λ is atrade-off parameter that controls the contributionof two objectives to the overall objective, as shownin eq.
3; and n controls the total samples assignedto a human.
as n gets larger or λ gets smaller, theoverall evaluation is more reliable but needs morehuman effort.
as n gets smaller or λ gets larger,the overall evaluation costs less human effort butgets less reliability..3.3 mce modulegiven a machine evaluation model (usually a classi-ﬁcation model (de mattei et al., 2020)) fc(c, ˆx) →ˆy, where ˆy is the evaluation result (usually a cate-gory, e.g., malevolence or non-malevolence), themce module aims to recognize how conﬁdent theevaluation ˆy is.
in this work, we investigate threeconﬁdence estimation methods, namely maximumclass probability (mcp), trust score (ts) and trueclass probability (tcp)..mcp is a basic method that directly uses theclassiﬁcation probabilities to measure the conﬁ-dence.
based on the dataset {(c (cid:48)j=1, webuild a bert-based classiﬁer as a machine evalua-tion model fc.
mcp is the softmax probability ofthe evaluation result ˆy.
formally, mcp(c (cid:48), x) =p (y = ˆy|w, c (cid:48), x)..j, xj), yj}q.ts is a conﬁdence measurement that estimateswhether the predicted category of a test sam-ple by a classiﬁer can be trusted.
it is calcu-lated as the ratio between the hausdorff distancefrom the sample to the non-predicted and the pre-dicted categories (jiang et al., 2018).
first, thetraining data is processed to ﬁnd k-nn radiusbased α-high-density-set ˆh( ˜c (cid:48)train, ˜xtrain), where{ ˜c (cid:48)train, ˜xtrain} is the output of feeding trainingsamples {(c (cid:48)train, xtrain)} into the bert layerof fc.
this part is different from the originalts work designed for images (yu et al., 2019).
then, for a given test sample, we predict the ra-tio of distances, which is the ts value.
formally,ˆa = d(c (cid:48)j, xj, ˆh2), where ˆh1 is thehigh density set of the non-predicted category, ˆh2is the high density set of the predicted category.
the estimated ts is normalized within 0 and 1 bymin-max normalization..j, xj, ˆh1)/d(c (cid:48).
as for tcp,.
the estimation is obtained bya learning-based method.
similar to ts,theoriginal conﬁdence network for tcp estimationis also built for images (corbi`ere et al., 2019).
we expand it into a bert-based conﬁdence net-work for cdss.
the tcp estimation part fconf.
5615is based on the bert-classiﬁer fc.
formally,fconf (c, ˆx, fc, fg) → ˆa ∈ [0, 1], where fg is thegeneration model.
we pass the features from thebert layer of fc and feed them into a conﬁdencenetwork implemented by a succession of dense lay-ers with a sigmoid activation to get the conﬁdencescalar..i, xi, y∗.
we deﬁne an mse loss to train tcp: lconf =(cid:80)qi=1(ˆa(c (cid:48)i, xi, θ) − a∗(c (cid:48)1i ))2, whereqa∗(c (cid:48)i, xi, y∗i ) is the target conﬁdence value.
dur-ing inference,the ground truth tcp score iscalculated based on the bert-based classiﬁer:tcp(c (cid:48), x, y∗) = p (y = y∗|w, c (cid:48), x), wherey∗ is the true category..3.4 hee module.
the hee module is designed for estimating thehuman effort ˆe.
in this work, we use time cost, i.e.,the time spent for each annotation, to represent hu-man effort.
we implement the time cost estimationmodel fl with random forest regression (liaw et al.,2002): fl(h(c, ˆx)) → ˆl ∈ [0, 1], h is the featureextraction function..there are two groups of features, namely dia-logue related features and worker related features;see table 5. the dialogue related features are:(a) ‘total turns’: total number of turns in a dialogue;(b) ‘malevolent turns’: total number of malevo-lent turns in a dialogue; for prediction, we usethe bert-classiﬁer results; (c) ‘non-malevolentturns’: total number of non-malevolent turns in adialogue; for prediction, we use the bert-classiﬁerresults.
(d) ‘ﬁrst submission or not’: if this is theﬁrst time the worker does this task, the value is1, else 0; (e) ‘paraphrased turns’: some turns areparaphrased; we calculate the total number of suchturns; (f) ‘total length’:total number of tokensthe result of ain the dialogue; (g) ‘fk score’:readability test, based on (kincaid et al., 1975);(h) ‘dc score’: the result of a readability test, basedon (dale and chall, 1948); (i) ‘contains malevolentturn or not’: if the dialogue contains a malevolentturn, the value is 1, else 0; and (j) ‘perplexity score’:we use bert as a language model to calculate theperplexity (gamon et al., 2005).
the worker relatedfeatures are: (a) ‘worker test score’: this is basedon a test designed to test workers’ ability to an-notate the dialogue according to the gold standardannotation (zhang et al., 2021); and (b) ‘approvalrate ranking’: we rank workers by their lifetimeapproval rate in ascending order, and use the index;.
lower approval rate workers (i.e., with a smallerindex) usually spend less time on annotations..to train the time cost estimation model fl, weneed the annotation time spent on each response.
however, for each individual response, the timespent is relatively short; as a consequence, the in-ﬂuence of noise such as attention, click time, maybe relatively large and make the data unreliable astraining data.
therefore, we use the annotation timespent on each dialogue instead of each responseas time cost target, and it is normalized within 0and 1 using min-max normalization.
for the saemodule and effort assessment, we use the averagetime per turn of each dialogue as the time cost ˆlfor each response.
in addition, there are multiplehuman annotator submissions for inter-annotatoragreement; we ﬁlter out the data points that dis-agree with the agreed annotation; then we choosethe data point with a higher annotator test score; ifthe test scores are same, we randomly choose one..4 experimental setup4.1 datasetwe carry out experiments on the mdrdc datasetwhich is initially built for malevolent dialogue de-tection and classiﬁcation (zhang et al., 2021).
thedataset consists of 6,000 dialogues, with 21,081non-malevolent utterances and 10,299 malevolentutterances.
the dataset also includes mturk infor-mation, e.g., the time spent on each annotation.
wefollow the original paper to split the dataset intotrain, validation and test with a ratio of 7:1:2..implementation details.
4.2in terms of the responses by the generation modelfg, in our implementation, we use the original re-sponses by a human for evaluation.
the mcemodule is implemented by a bert-based classiﬁerand a bert-based conﬁdence network.
first, forthe bert-based classiﬁer, we add a softmax layeron top of the ‘[cls]’ token.
it is ﬁne-tuned with4 epochs since it is already pretrained on a largedataset.
the vocabulary size is 30,522. dialoguecontext and the current response are concatenatedwith the ‘[sep]’ delimiter.
we consider the previ-ous three dialogue utterances (if any) as context.
we set the max sequence length to 128, the batchsize to 64, the dropout ratio to 0.1, and the learn-ing rate is 5e-5.
second, the bert-based conﬁ-dence network is attached to a bert-classiﬁer.
itis composed of 5 dense layers, following previouswork (corbi`ere et al., 2019).
as for max sequence.
5616length, batch size, dropout ratio, and learning rate,these are the same as for the classiﬁer.
the con-ﬁdence network is trained with a maximum of 30epochs, with early stopping if the validation lossdoes not improve for 10 epochs.
the hee mod-ule is implemented by a random forest regressionmodel; the max number of estimators in this studyis 100; only the features related to time cost areselected for annotation time cost prediction, witha maximum feature size of 10. we use the mippackage to implement ilp for the sae module1with the coin-or branch-and-cut solver (mitchell,2002).
the search stops when it reaches a feasi-ble solution.
all the neural models are trained ongeforce gtx titanx gpus..4.3 metricswe use reliability metrics and effort metrics to as-sess overall performance.
the reliability metricsare precision, recall, f1-score, and accuracy.
wecalculate the macro score of precision, recall andf1 as the categories are imbalanced (hossin andsulaiman, 2015).
the effort metrics include hu-man ratio and time cost.
human ratio is the ratioof samples assigned to a human.
time cost is thetotal time required for a human to annotate the sam-ples.
we use auc, and top-k accuracy to assessthe different mce implementations (ouni et al.,2017).
we rank the conﬁdence in descending orderand calculate the accuracy at top-50%.
top-50%accuracy measures how well the mce predictionswork for the top-50% most conﬁdent samples.
weuse mean square error (mse), rooted mean squareerror (rmse), mean absolute error (mae) and r2to assess the hee module.
mse, rmse, maeare calculated between the predicted time cost andreal time cost.
we also use the pearson and spear-man correlation scores to analyze the correlationbetween features and real time cost..5 results and analysis5.1 reliability and efﬁciencyto determine how hmceval compares to humanevaluation and machine evaluation in balancingreliability and efﬁciency, we report the results intable 1. hmceval outperforms both human andmachine evaluation in balancing reliability and ef-ﬁciency.
more importantly, hmceval, with halfof the human effort spared, achieves reliability thatis close to human reliability.
first, compared to.
1https://python-mip.com.
table 1: reliability and efﬁciency of hmceval w.r.t.
human and machine evaluation (n/m = 0.5)..metric.
machine human hmceval.
reliability.
precisionrecallf1-scoreaccuracy.
efﬁciency.
0.8180.8030.8100.862.human ratiotime cost.
00.
1111.
11.
0.9830.9760.9800.985.
0.5000.500.human evaluation, hmceval arrives at 98.5% ofhuman accuracy but the human effort decreases by50.0%.
this means that hmceval is much moreefﬁcient than human evaluation, while the relia-bility is close to human.
second, compared to ma-chine evaluation, the precision, recall, f1-score andaccuracy of hmceval increase by 20.2%, 21.5%,21.0%, and 14.3%, respectively.
this means thathmceval has higher reliability than machine eval-uation.
in sum, therefore, hmceval surpasses bothhuman and machine evaluation in balancing relia-bility and efﬁciency..inﬂuence of n and λ.
5.2to investigate how n and λ, two parameters for thesae module that balance the reliability and effort,inﬂuence the performance of hmceval, we ﬁrstﬁx λ and vary n/m from 0 to 1 with a step sizeof 0.05, where m is the total number of samplesto evaluate.
then, we ﬁx n and vary λ from 0 to45 with a step size of 0.1. the results are shown infigure 2 and 3.inﬂuence of n .
generally, as n increases,hmceval has better reliability, nevertheless thehuman effort increases.
from figure 2, we can seethat when λ is ﬁxed, as n gets larger, the precision,recall, f1-score and accuracy increase, but humanratio and time cost also increase.
with larger n ,more samples are assigned to a human, so the over-all evaluation results are more reliable, but thisrequires a bigger human annotation effort.
themarginal reliability beneﬁt of assigning more sam-ples to a human decreases as n gets larger.
fig-ure 2(a) shows that as n increases, the reliabilityincreases sharply at the beginning but the increaselevels off when n > 2, 500. the samples assignedto a human when n < 2, 500 have lower modelconﬁdence, i.e., it is very likely that those samplesare given inaccurate evaluation by machine.
butwhen n > 2, 500, samples with higher model con-ﬁdence are also assigned to human which yields a.
5617(a) reliability..(b) effort..figure 2: inﬂuence of n with λ = 0.1..table 2: analysis of the sae module..metric.
mce mce+hee.
hee.
reliability.
precisionrecallf1-scoreaccuracy.
efﬁciency.
0.9890.9820.9850.989.
0.9830.9760.9800.985.
0.8810.8580.8690.906.human ratiotime cost.
0.5000.650.
0.5000.500.
0.5000.135.ts has the best accuracy.
mcp has the best aucscore, which means for all the m samples, mcpis the best.
but the top-50% samples have moreinﬂuence on the sae module..(a) reliability..(b) effort..figure 3: inﬂuence of λ with ﬁxed n (n/m = 0.5)..limited return in terms of reliability..inﬂuence of λ. as λ increases, hmceval getsmore efﬁcient, while the reliability gets worse.
asshown in figure 3, when λ increases, the humanratio stays at 0.5, and after a certain pivotal point,it decreases sharply.
the time costs keep decreas-ing.
the precision, recall, f1-score and accuracydecreases rapidly.
with larger λ, the sae objectiveputs a bigger emphasis on efﬁciency, so hmcevalgets more efﬁcient but less reliable..5.3 module analysisanalysis of the sae module.
by adjusting theλ values, the sae module can degenerate intoa greedy algorithm (gates et al., 2020).
table 2shows the results with the human ratio set to a ﬁxedvalue of n/m , i.e., 0.5. when λ = 0, the heemodule has no effect, so it has the worst efﬁciencyand the best reliability.
when λ → ∞, i.e., 500,the mce module contributes little to the objective,so it has the best efﬁciency but the worst reliability..analysis of the mce module.
for the mce mod-ule, we analyze the effect of alternative implemen-tations.
as shown in figure 4, ts outperformsmcp and tcp.
speciﬁcally, when the human ra-tio is ﬁxed to 0.5, ts achieves the best accuracyfor different time costs.
this means that ts hasbetter model conﬁdence estimation for the sampleswith higher conﬁdence.
as shown in table 3, forthe top-50% samples ranked by model conﬁdence,.
figure 4: performance of hmceval with differentmce implementations (n/m = 0.5)..table 3: conﬁdence prediction results comparison ofmce methods..metric.
mcp tcp.
ts.
0.828 0.823 0.825aucaccuracy (top-50%) 0.977 0.975 0.978.analysis of the hee module.
for the hee mod-ule, we analyze the effect of different features.
adding worker related features helps to improveaccuracy.
as shown in figure 5, sae with both di-alogue and worker related features has better accu-racy than sae with only dialogue related featureswhen the human ratio is ﬁxed to 0.5. worker basedfeatures are useful for time cost estimation.
thisis conﬁrmed by the results in table 4. the resultswith both dialogue and worker related features arethe best, with mse, rmse and mae decreasingby 55.6%, 35.9%, 45.9%, and r2 increasing by76.2%.
the hee module is sufﬁcient for time costprediction since r2 greater than 0.26 is sufﬁcientfor behavior related models (cohen, 1988)..a correlation analysis between each feature andthe real time cost is shown in table 5. all the fea-tures, except perplexity, have signiﬁcant pearson orspearman scores with the real time cost by workers.
most features show positive correlation.
but twofeatures, namely ‘non-malevolent turns’ and ‘fk.
5618table 5: correlation analysis between time cost and dif-ferent features for hmc module.
∗∗ and ∗ indicate sig-niﬁcance p < 0.001, p < 0.05, respectively..feature.
pearson spearman.
dialogue related features (d).
total turnsmalevolent turnsnon-malevolent turnsfirst submissionparaphrased turnstotal lengthreadability (dc)readability (fk)contains malevolent turnbert-perplexity.
0.053∗∗0.445∗∗.
0.122∗∗0.600∗∗−0.236∗∗ −0.292∗∗0.263∗∗0.564∗∗0.100∗∗.
0.342∗∗0.555∗∗0.046∗∗0.042∗ −0.001−0.026∗ −0.053∗∗0.603∗∗0.001.
0.432∗∗.
−0.008.
worker related features (w).
worker test scoreapproval rate ranking.
0.162∗∗0.840∗∗.
0.049∗∗0.849∗∗.
figure 6: accuracy and effort per turn with half humaneffort spared in average..assigned to a human, 1.1–1.5% samples are eval-uated inaccurately.
this is due to contexts thatconsist of a small number of turns, or high conﬁ-dence for some dialogues where language is usedin a non-literal way.
although hmceval could begeneralized to several evaluation metrics of cds,e.g., bert-based ruber and bert-based en-gagement, for score-based metrics, suitable con-ﬁdence estimation is required.
in the future, weseek to improve the model conﬁdence and humaneffort estimation by considering better neural archi-tectures and more factors; we also plan to conducta comprehensive and reliable analysis of the perfor-mance of current state-of-the-art cds models byapplying hmceval to various evaluation aspects..acknowledgementsthis research was funded by the china scholar-ship council and the hybrid intelligence center,a 10-year program funded by the dutch ministryof education, culture and science through thenetherlands organisation for scientiﬁc research,https://hybrid-intelligence-centre.nl..codeour code is available at: https://github.com/repozhang/case_hmceval..figure 5: feature analysis w.r.t.
accuracy.
(d: dialoguerelated features, w: worker related features.).
table 4: direct evaluation of the hee module.
(d: di-alogue related features, w: worker related features.).
metric.
msermsemaer2.
d.0.0090.0920.0610.433.d+w.
0.0040.0590.0330.763.score’ have a negative correlation with time cost:(a) non-malevolent responses are relatively easyto identify; and (b) a higher flesch–kincaid (fk)score means that the dialogue is easier to under-stand, which requires less time to annotate..5.4 performance at different turnswe analyze the effectiveness of hmceval at dif-ferent dialogue turns in figure 6. as the dialogueevolves, hmceval gets more reliable.
it gets easierfor the mce module to detect malevolent responseswith high conﬁdence when more context informa-tion is available.
the exception for turn seven andnine might due to the fact that the total number ofutterances is small (less than 5% of the whole testset) and thus the results have high variance.
theeffort is not related to turn..we also look into the 1.5% cases whenhmceval gives inaccurate evaluation, and somecases that require human judgement but are notassigned to a human.
we ﬁnd that these casesmostly have meaning extension, which means anextension of meaning of words with reference.
forinstance, ‘i’ve commit 8 treasonous acts today andthey still haven’t put me in prison’, this is actuallya non-malevolent joke.
however, the mce moduleclassiﬁed it to be malevolent with high conﬁdence..6 conclusion and future workin this work, we have introduced a human-machinecollaborative evaluation framework (hmceval) forreliable and efﬁcient cds evaluation.
experimentson the task of evaluating malevolence in dialogueresponses show that hmceval can achieve around99% reliability with half human effort spared.
alimitation of hmceval is that given 50% samples.
5619references.
william callaghan, joslin goh, michael mohareb, an-drew lim, and edith law.
2018. mechanicalheart:a human-machine framework for the classiﬁcationof phonocardiograms.
proceedings of the acmon human-computer interaction, 2(cscw):28:1–28:17..arun chaganty, stephen mussmann, and percy liang.
2018. the price of debiasing automatic metrics innatural language evalaution.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages643–653..stanley f. chen, douglas beeferman, and roni rosen-feld.
1998. evaluation metrics for language mod-in darpa broadcast news transcription andels.
understanding workshop (bntuw)..jacob cohen.
1988. statistical power analysis for thebehavioral sciences (2nd edition).
hillsdale, l. erl-baum associates..charles corbi`ere, nicolas thome, avner bar-hen,matthieu cord, and patrick p´erez.
2019. address-ing failure prediction by learning model conﬁdence.
in advances in neural information processing sys-tems, pages 2902–2913..mary cummings.
2004. automation bias in intelli-gent time critical decision support systems.
in aiaa1st intelligent systems technical conference, page6313..edgar dale and jeanne s chall.
1948. a formula forpredicting readability: instructions.
educational re-search bulletin, pages 37–54..anubrata das, brandon dang, and matthew lease.
2020. fast, accurate, and healthier: interactive blur-ring helps moderators reduce exposure to harmfulcontent.
in proceedings of the aaai conference onhuman computation and crowdsourcing, volume 8,pages 33–42..lorenzo de mattei, michele cafagana, felicedell’orletta, malvina nissim, and albert gatt.
2020.changeit@evalita2020: change headlines, adaptnews, generate.
in proceedings of seventh evalua-tion campaign of natural language processing andspeech tools for italian.
final workshop (evalita2020), online.
ceur.org..jan deriu, alvaro rodrigo, arantxa otegi, guillermoechegoyen, sophie rosset, eneko agirre, and markcieliebak.
2020.survey on evaluation methodsfor dialogue systems.
artiﬁcial intelligence review,pages 1–56..sarah e finch and jinho d choi.
2020. towards uni-ﬁed dialogue system evaluation: a comprehensiveanalysis of current evaluation protocols.
in proceed-ings of the 21th annual meeting of the special inter-est group on discourse and dialogue, pages 236–245..michael gamon, anthony aue, and martine smets.
2005. sentence-level mt evaluation without refer-ence translations: beyond language modeling.
inproceedings of the 10th eamt conference: practi-cal applications of machine translation..allison gates, michelle gates, meghan sebastian-ski, samantha guitard, sarah a elliott, and lisahartling.
2020. the semi-automation of title and ab-stract screening: a retrospective exploration of waysto leverage abstrackr’s relevance predictions in sys-tematic and rapid reviews.
bmc medical researchmethodology, 20:1–9..sarik ghazarian, johnny wei, aram galstyan, andnanyun peng.
2019. better automatic evaluation ofopen-domain dialogue systems with contextualizedin proceedings of the workshop onembeddings.
methods for optimizing and evaluating neural lan-guage generation, pages 82–89..sarik ghazarian, ralph weischedel, aram galstyan,and nanyun peng.
2020. predictive engagement:an efﬁcient metric for automatic evaluation of open-in proceedings of thedomain dialogue systems.
aaai conference on artiﬁcial intelligence, vol-ume 34, pages 7789–7796..mohammad hossin and mn sulaiman.
2015. a re-view on evaluation metrics for data classiﬁcationevaluations.
international journal of data mining& knowledge management process, 5(2):1..yufang hou, charles jochim, martin gleize, francescabonin, and debasis ganguly.
2019.identiﬁcationof tasks, datasets, evaluation metrics, and numericinscores for scientiﬁc leaderboards construction.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5203–5213..heinrich jiang, been kim, melody guan, and mayagupta.
2018. to trust or not to trust a classiﬁer.
inadvances in neural information processing systems,pages 5541–5552..j. peter kincaid, robert p. fishburne jr, richard l.rogers, and brad s. chissom.
1975. derivation ofnew readability formulas (automated readability in-dex, fog count and ﬂesch reading ease formula) fornavy enlisted personnel.
technical report, navaltechnical training command millington tn re-search branch..berkeley j dietvorst, joseph p simmons, and cademassey.
2015. algorithm aversion: people er-roneously avoid algorithms after seeing them err.
journal of experimental psychology: general,144(1):114..trent kyono, fiona j. gilbert, and mihaela van derschaar.
2018. mammo: a deep learning solu-tion for facilitating radiologist-machine collabora-arxiv preprinttion in breast cancer diagnosis.
arxiv:1811.02661..5620walter lasecki, christopher miller, adam sadilek, an-drew abumoussa, donato borrello, raja kushalna-gar, and jeffrey bigham.
2012. real-time caption-ing by groups of non-experts.
in proceedings of the25th annual acm symposium on user interface soft-ware and technology, pages 23–34..jiwei li, michel galley, chris brockett, jianfeng gao,and william b. dolan.
2016. a diversity-promotingobjective function for neural conversation models.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119..andy liaw, matthew wiener, et al.
2002. classiﬁ-cation and regression by randomforest.
r news,2(3):18–22..chin-yew lin and eduard hovy.
2002. manual and au-tomatic evaluation of summaries.
in proceedings ofthe acl-02 workshop on automatic summarization-volume 4, pages 45–51..chia-wei liu, ryan lowe, iulian vlad serban, mikenoseworthy, laurent charlin, and joelle pineau.
2016. how not to evaluate your dialogue system:an empirical study of unsupervised evaluation met-in proceed-rics for dialogue response generation.
ings of the 2016 conference on empirical methodsin natural language processing, pages 2122–2132..ryan lowe, michael noseworthy, iulian vlad ser-ban, nicolas angelard-gontier, yoshua bengio, andjoelle pineau.
2017. towards an automatic turingtest: learning to evaluate dialogue responses.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1116–1126..liangchen luo, jingjing xu, junyang lin, qi zeng,and xu sun.
2018. an auto-encoder matchingmodel for learning utterance-level semantic depen-dency in dialogue generation.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 702–707..r. timothy marler and jasbir s. arora.
2010. theweighted sum method for multi-objective optimiza-tion: new insights.
structural and multidisciplinaryoptimization, 41(6):853–862..john e mitchell.
2002. branch-and-cut algorithms forcombinatorial optimization problems.
handbook ofapplied optimization, 1:65–77..rostislav nedelchev, jens lehmann, and ricardo us-beck.
2020. language model transformers as evalu-ators for open-domain dialogues.
in proceedings ofthe 28th international conference on computationallinguistics, pages 6797–6808..jekaterina novikova, ondˇrej duˇsek, amanda cercascurry, and verena rieser.
2017. why we need newevaluation metrics for nlg.
in proceedings of the2017 conference on empirical methods in naturallanguage processing, pages 2241–2252..ali ouni, raula gaikovina kula, marouane kessen-tini, takashi ishio, daniel m german, and katsuroinoue.
2017. search-based software library recom-in-mendation using multi-objective optimization.
formation and software technology, 83:55–75..christos h. papadimitriou and kenneth steiglitz.
1998.combinatorial optimization: algorithms and com-plexity.
courier corporation..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automaticevaluation of machine translation.
in proceedings ofthe 40th annual meeting of the association for com-putational linguistics, pages 311–318..ashwin ram, rohit prasad, chandra khatri, anuvenkatesh, raefer gabriel, qing liu, jeff nunn,behnam hedayatnia, ming cheng, ashish nagar,et al.
2018. conversational ai: the science behindthe alexa prize.
arxiv preprint arxiv:1801.03604..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark andin proceedings of the 57th annual meet-dataset.
ing of the association for computational linguistics,pages 5370–5381..saurabh ravindranath, rahul baburaj, vineeth n. bal-asubramanian, nageswararao namburu, sujit gu-jar, and c.v. jawahar.
2020. human-machine col-in proceedings oflaboration for face recognition.
the 7th acm ikdd cods and 25th comad, pages10–18..zheyuan ryan shi, claire wang, and fei fang.
2020.artiﬁcial intelligence for social good: a survey.
arxiv preprint arxiv:2001.01818..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in the 32nd aaai conference on artiﬁcial intelli-gence..jing xu, da ju, margaret li, y-lan boureau, ja-son weston, and emily dinan.
2020. recipes forarxiv preprintsafety in open-domain chatbots.
arxiv:2010.07079..tom young, erik cambria, iti chaturvedi, hao zhou,subham biswas, and minlie huang.
2018. aug-menting end-to-end dialogue systems with common-sense knowledge.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 32..kun yu, shlomo berkovsky, ronnie taib, jianlongzhou, and fang chen.
2019. do i trust my machineteammate?
an investigation from perception to deci-sion.
in proceedings of the 24th international con-ference on intelligent user interfaces, pages 460–468..5621yangjun zhang, pengjie ren, and maarten de rijke.
2021. a taxonomy, dataset and benchmark fordetecting and classifying malevolent dialogue re-sponses.
journal of the association for informationscience and technology..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt: large-scalegenerative pre-training for conversational responsegeneration.
acl system demonstrations..5622implemented by random forest regression and theruntime is less than 10 minutes for 5-fold cross-validation.
the sae module is implemented byilp and the runtime is around 2.5 hours..in terms of parameters, the mce module is neu-ral network based.
mcp and ts are estimated withthe bert-based classiﬁer, which has 109.5 millionparameters.
tcp has an additional conﬁdence net-work compared with mcp and ts.
the conﬁdencenetwork part has 2.4 million parameters.
the heemodule and the sae module are not neural networkbased, we have included most of the information inthe main manuscript.
to add up, the sae moduleis based on search.
there are a total number of 10thousand trials with different n and λ parameters.
the best n and λ are chosen by reliability metricsand efﬁciency metrics.
in table 1 (presented insection 5) and table 6, we choose the ﬁnal resultswith λ = 4.6 and n = 0.5m , where m is thenumber of the total samples to be evaluated..appendices.
we present additional details for reproducibilityto the appendices.
speciﬁcally, we include corre-sponding validation performance for the main re-sult (appendix a), average runtime of each moduleand detailed information of parameters (appendixb)..a reliability and efﬁciency of hmceval.
for validation.
as for validation performance, we report the vali-dation results of comparing hmceval to machineevaluation and human evaluation in balancing reli-ability and efﬁciency, as shown in table 6. hmce-val surpasses both human and machine evaluationin balancing reliability and efﬁciency for validation.
on the one hand, compared to human evaluation,hmceval achieves 98.2% of human accuracy with50% human effort spared.
this suggests that for thevalidation set, hmceval is efﬁcient than humanevaluation, while the reliability is close to humanevaluation.
on the other hand, compared to ma-chine evaluation, the precision, recall, f1-score andaccuracy of hmceval increase by 21.5%, 22.8%,22.0% and 15.3%, respectively.
moreover, the re-sults of the validation set and the test set are similar.
compared to results of the test set, reliability re-sults of the validation set is slightly lower, but thedifference is less than 0.5%, as shown in table 1(presented in section 5) and table 6..table 6: reliability and efﬁciency of hmceval w.r.t.
human and machine evaluation for validation (n/m =0.5)..metric.
machine human hmceval.
reliability.
precisionrecallf1-scoreaccuracy.
efﬁciency.
0.8060.7930.8000.852.human ratiotime cost.
00.
1111.
11.
0.9790.9740.9760.982.
0.5000.500.b runtime and parameters.
in terms of average runtime, we have three modules.
the time costs for all the modules are acceptable.
the mce module has thee methods: mcp, ts andtcp.
their time costs are 0.5 hours, 0.1 hours,and 3.5 hours, respectively.
the hee module is.
5623