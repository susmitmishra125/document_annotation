syntax-enhanced pre-trained modelzenan xu1∗ †, daya guo1∗, duyu tang2†, qinliang su1,4,5‡, linjun shou3,ming gong3, wanjun zhong1∗, xiaojun quan1, daxin jiang3, and nan duan21school of computer science and engineering, sun yat-sen university, guangzhou, china2microsoft research asia, beijing, china3microsoft search technology center asia, beijing, china4guangdong key laboratory of big data analysis and processing, guangzhou, china5key lab.
of machine intelligence and advanced computing, ministry of education, china{xuzn, guody5, zhongwj25}@mail2.sysu.edu.cn{suqliang, quanxj3}@mail.sysu.edu.cn{dutang,lisho,migon,djiang,nanduan}@microsoft.com.
abstract.
we study the problem of leveraging the syn-tactic structure of text to enhance pre-trainedmodels such as bert and roberta.
exist-ing methods utilize syntax of text either in thepre-training stage or in the ﬁne-tuning stage,so that they suffer from discrepancy betweenthe two stages.
such a problem would leadto the necessity of having human-annotatedsyntactic information, which limits the appli-cation of existing methods to broader scenar-ios.
to address this, we present a model thatutilizes the syntax of text in both pre-trainingand ﬁne-tuning stages.
our model is basedon transformer with a syntax-aware attentionlayer that considers the dependency tree of thetext.
we further introduce a new pre-trainingtask of predicting the syntactic distance amongtokens in the dependency tree.
we evaluate themodel on three downstream tasks, includingrelation classiﬁcation, entity typing, and ques-tion answering.
results show that our modelachieves state-of-the-art performance on sixpublic benchmark datasets.
we have two ma-jor ﬁndings.
first, we demonstrate that in-fusing automatically produced syntax of textimproves pre-trained models.
second, globalsyntactic distances among tokens bring largerperformance gains compared to local head re-lations between contiguous tokens.1.
1.introduction.
pre-trained models such as bert (devlin et al.,2019), gpt (radford et al., 2018), and roberta(liu et al., 2019) have advanced the state-of-the-artperformances of various natural language process-ing tasks.
the successful recipe is that a model isﬁrst pre-trained on a huge volume of unsupervised.
∗ work is done during internship at microsoft.
† for questions, please contact d. tang and z. xu.
‡ corresponding author.
1the source data is available at https://github.com/hi-.
zenanxu/syntax-enhanced pre-trained model..for clarity..data with self-supervised objectives, and then isﬁne-tuned on supervised data with the same datascheme.
dominant pre-trained models representa text as a sequence of tokens2.
the merits arethat such basic text representations are availablefrom vast amounts of unsupervised data, and thatmodels pre-trained and ﬁne-tuned with the sameparadigm usually achieve good accuracy in practice(guu et al., 2020).
however, an evident limitationof these methods is that richer syntactic structureof text is ignored..in this paper, we seek to enhance pre-trainedmodels with syntax of text.
related studies attemptto inject syntax information either only in the ﬁne-tuning stage (nguyen et al., 2020; sachan et al.,2020), or only in the pre-training stage (wang et al.,2020), which results in discrepancies.
when onlyfusing syntax information in the ﬁne-tuning phase,sachan et al.
(2020) ﬁnds that there is no perfor-mance boost unless high quality human-annotateddependency parses are available.
however, this re-quirement would limit the application of the modelto broader scenarios where human-annotated de-pendency information is not available..to address this, we conduct a large-scale studyon injecting automatically produced syntax of textin both the pre-training and ﬁne-tuning stages.
weconstruct a pre-training dataset by applying an off-the-shelf dependency parser (qi et al., 2020) toone billion sentences from common crawl news.
with these data, we introduce a syntax-aware pre-training task, called dependency distance predic-tion, which predicts the syntactic distance betweentokens in the dependency structure.
compared withthe pre-training task of dependency head prediction(wang et al., 2020) that only captures local syntac-tic relations among words, dependency distanceprediction leverages global syntax of the text.
in.
2such tokens can be words or word pieces.
we use token.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5412–5422august1–6,2021.©2021associationforcomputationallinguistics5412addition, we developed a syntax-aware attentionlayer, which can be conveniently integrated intotransformer (vaswani et al., 2017) to allow tokensto selectively attend to contextual tokens based ontheir syntactic distance in the dependency structure.
we conduct experiments on entity typing, ques-tion answering and relation classiﬁcation on sixbenchmark datasets.
experimental results showthat our method achieves state-of-the-art perfor-mance on all six datasets.
further analysis showsthat our model can indicate the importance of syn-tactic information on downstream tasks, and thatthe newly introduced dependency distance predic-tion task could capture the global syntax of thetext, performs better than dependency head pre-diction.
in addition, compared with experimentalresults of injecting syntax information in either thepre-training or ﬁne-tuning stage, injecting syntaxinformation in both stages achieves the best perfor-mance..in summary, the contribution of this paper isthreefold.
(1) we demonstrate that infusing auto-matically produced dependency structures into thepre-trained model shows superior performance overdownstream tasks.
(2) we propose a syntax-awareattention layer and a pre-training task for infusingsyntactic information into the pre-trained model.
(3) we ﬁnd that the newly introduced dependencydistance prediction task performs better than thedependency head prediction task..2 related work.
our work involves injecting syntax informationinto pre-trained models.
first, we will review re-cent studies on analyzing the knowledge presentedin pre-trained models, and then we will introducethe existing methods that enhance pre-trained mod-els with syntax information..2.1 probing pre-trained models.
with the huge success of pre-trained models (de-vlin et al., 2019; radford et al., 2018) in a widerange of nlp tasks, lots of works study to whatextent pre-trained models inherently.
here, wewill introduce recent works on probing linguisticinformation, factual knowledge, and symbolic rea-soning ability from pre-trained models respectively.
in terms of linguistic information, hewitt and man-ning (2019) learn a linear transformation to pre-dict the depth of each word on a syntax tree basedon their representation, which indicates that the.
syntax information is implicitly embedded in thebert model.
however, yaushian et al.
(2019) ﬁndthat the attention scores calculated by pre-trainedmodels seem to be inconsistent with human intu-itions of hierarchical structures, and indicate thatcertain complex syntax information may not be nat-urally embedded in bert.
in terms of probing fac-tual knowledge, petroni et al.
(2019) ﬁnd that pre-trained models are able to answer fact-ﬁlling clozetests, which indicates that the pre-trained modelshave memorized factual knowledge.
however, po-erner et al.
(2019) argue that bert’s outstandingperformance of answering fact-ﬁlling cloze testsis partly due to the reasoning of the surface formof name patterns.
in terms of symbolic reasoning,talmor et al.
(2020) test the pre-trained models oneight reasoning tasks and ﬁnd that the models com-pletely fail on half of the tasks.
although probingknowledge from pre-trained model is a worthwhilearea, it runs perpendicular to infusing knowledgeinto pre-trained models..2.2.integrating syntax into pre-trainedmodels.
recently, there has been growing interest in enhanc-ing pre-trained models with syntax of text.
existingmethods attempt to inject syntax information in theﬁne-tuning stage or only in the pre-training stage.
we ﬁrst introduce related works that inject syntaxin the ﬁne-tuning stage.
nguyen et al.
(2020) in-corporate a tree-structured attention into the trans-former framework to help encode syntax informa-tion in the ﬁne-tuning stage.
zhang et al.
(2020)utilize the syntax to guide the transformer modelto pay no attention to the dispensable words inthe ﬁne-tuning stage and improve the performancein machine reading comprehension.
sachan et al.
(2020) investigate two distinct strategies for incor-porating dependency structures in the ﬁne-tuningstage and obtain state-of-the-art results on the se-mantic role labeling task.
meanwhile, sachan et al.
(2020) argue that the performance boost is mainlycontributed to the high-quality human-annotatedsyntax.
however, human annotation is costly anddifﬁcult to extend to a wide range of applications.
syntax information can also be injected in the pre-training stage.
wang et al.
(2020) introduce headprediction tasks to inject syntax information intothe pre-trained model, while syntax information isnot provided during inference.
note that the headprediction task in wang et al.
(2020) only focuses.
5413figure 1: the dependency tree of the sentence, “my dog is playing frisbee outside the room,” after running thestanza parser..on the local relationship between two related to-kens, which prevents each token from being able toperceive the information of the entire tree.
despitethe success of utilizing syntax information, existingmethods only consider the syntactic informationof text in the pre-training or the ﬁne-tuning stageso that they suffer from discrepancy between thepre-training and the ﬁne-tuning stage.
to bridgethis gap, we conduct a large-scale study on inject-ing automatically produced syntax information inboth the two stages.
compared with the head pre-diction task (wang et al., 2020) that captures thelocal relationship, we introduce the dependencydistance prediction task that leverages the globalrelationship to predict the distance of two giventokens..3 data construction.
in this paper, we adopt the dependency tree to ex-press the syntax information.
such a tree structureis concise and only expresses necessary informa-tion for the parse (jurafsky, 2000).
meanwhile, itshead-dependent relation can be viewed as an ap-proximation to the semantic relationship betweentokens, which is directly useful for capturing se-mantic information.
the above advantages help ourmodel make more effective use of syntax informa-tion.
another available type of syntax informationis the constituency tree, which is used in nguyenet al.
(2020).
however, as pointed out in juraf-sky (2000), the relationships between the tokens independency tree can directly reﬂect important syn-tax information, which is often buried in the morecomplex constituency trees.
this property requiresextra techniques to extracting relation among thewords from a constituency tree (jurafsky, 2000)3.the dependency tree takes linguistic words asone of its basic units.
however, most pre-trainedmodels take subwords (also known as the wordpieces) instead of the entire linguistic words as theinput unit, and this necessitates us to extend the def-inition of the dependency tree to include subwords.
following wang et al.
(2020), we will add edges.
from the ﬁrst subword of v to all subwords of u, ifthere exists a relationship between linguistic wordv and word u..based on the above extended deﬁnition, webuild a pre-training dataset from open-domainsources.
speciﬁcally, we randomly collect 1b sen-tences from publicly released common crawl newsdatasets (zellers et al., 2019) that contain englishnews articles crawled between december 2016 andmarch 2019. considering its effectiveness and abil-ity to expand to multiple languages, we adopt off-the-shelf stanza4 to automatically generate the syn-tax information for each sentence.
the averagetoken length of each sentence is 25.34, and theaverage depth of syntax trees is 5.15..4 methodology.
in this section, we present the proposed syntax-enhanced pre-trained model (seprem).
weﬁrst deﬁne the syntax distance between two tokens.
based on the syntax distance, we then introduce asyntax-aware attention layer to learn syntax-awarerepresentation and a pre-training task to enablemodel to capture global syntactic relations amongtokens..4.1 syntax distance over syntactic tree.
intuitively, the distance between two tokens on thesyntactic tree may reﬂect the strength of their lin-guistic correlation.
if two tokens are far away fromeach other on the syntactic tree, the strength of theirlinguistic correlation is likely weak.
thus, we de-ﬁne the distance of two tokens over the dependencytree as their syntactic distance.
speciﬁcally, wedeﬁne the distance between the token v and tokenu as 1, i.e.
d(v, u) = 1, if v is the head of u. iftwo tokens are not directly connected in the depen-dency graph, their distance is the summation ofthe distances between adjacent nodes on the path.
if two tokens are separated in the graph, their dis-tance is set to inﬁnite.
taking the sentence “mydog is playing frisbee outside the room.” in fig 1 as.
3https://web.stanford.edu/˜jurafsky/slp3/.
4https://github.com/stanfordnlp/stanza.
5414my   dog   is   playing   frisbee   outside   the   room  .d(playing, frisbee )  = 1d(playing, outside)  = 2d(outside, frisbee )  = ∞···rootan example, d(playing, frisbee) equals 1 since thetoken “playing” is the head of the token “frisbee”..4.2 syntax-aware transformer.
we follow bert (devlin et al., 2019) and usethe multi-layer bidirectional transformer (vaswaniet al., 2017) as the model backbone.
the modeltakes a sequence x as the input and applies ntransformer layers to produce contextual represen-tation:.
h n = transf ormern((1 − α)h n−1 + α ˆh n−1)(1)where n ∈ [1, n ] denotes the n-th layer of themodel, ˆh is the syntax-aware representation whichwill be described in section 4.3, h 0 is embeddingsof the sequence input x, and α is a learnable vari-able..however, the introduction of syntax-aware rep-resentation ˆh in the equation 1 changes the ar-chitecture of transformer, invalidating the originalweights from pre-trained model, such as bert androberta.
instead, we introduce a learnable im-portance score α that controls the proportion ofintegration between contextual and syntax-awarerepresentation.
when α is equal to zero, the syntax-aware representation is totally excluded and themodel is architectural identical to vanilla trans-former.
therefore, we initialize the parameter α asthe small but not zero value, which can help betterfuse syntactic information into existing pre-trainedmodels.
we will discuss importance score α indetailed in section 5.6..each transformer layer transf ormern con-tains an architecturally identical transformer block,which is composed of a multi-headed self-attentionm ultiattn (vaswani et al., 2017) and a followedfeed forward layer f f n .
formally, the output h nof the transformer block transf ormern(h (cid:48)n−1) iscomputed as:.
n = ln (m ultiattn(h (cid:48).
g(cid:48)h n = ln (f f n (g(cid:48).
n−1) + h (cid:48)n) + g(cid:48)n).
n−1).
(2).
n−1 is (1 − α)h n−1 + α ˆh n−1where the input h (cid:48)and ln represents a layer normalization operation..4.3 syntax-aware attention layer.
in this section, we will introduce how to obtainthe syntax-aware representation ˆh used in syntax-aware transformer..tree structure encoding we adopt a distancematrix d to encode the tree structure.
the advan-tages of distance matrix d are that it can well pre-serve the hierarchical syntactic structure of text andcan directly reﬂect the distance of two given tokens.
meanwhile, its uniqueness property guarantees theone-to-one mapping of the tree structure.
givena dependency tree, the element di,j of distancematrix d in i-th row and j-th column is deﬁned as:.
di,j =.
(cid:26) d(i, j),0,.if exists a path from vi to vj,if i = j and otherwise..(3)where vi and vj are tokens on the dependency tree.
based on the concept that distance is inversely pro-portional to importance, we normalize the matrixd and obtain the normalized correlation strengthmatrix ˜d as follows:.
(cid:40).
˜di,j =.
(cid:80).
1/di,j.
z∈{y|di,y (cid:54)=0}(1/di,z) ,0,.if di,j (cid:54)= 0,.otherwise..(4).
syntax-aware representation given the treestructure representation ˜d and the contextual rep-resentation h n, we fuse the tree structure into thecontextual representation as:.
ˆh n = σ(w 1.n h n + w 2n.˜dh n).
(5).
n and w 2.where σ is the activation function, w 1n ∈rdh×dh are model parameters.
we can see that˜dh n allows one to aggregate information fromothers along the tree structure.
the closer theyare on the dependency tree, the larger the attentionweight, and thus more information will be propa-gated to each other, and vice verse..4.4 syntax-aware pre-training task.
to better understand the sentences, it is beneﬁcialfor model to be aware of the underlying syntax.
to this end, a new pre-training task, named depen-dency distance prediction task (dp), is designedto enhance the model’s ability of capturing globalsyntactic relations among tokens.
speciﬁcally, weﬁrst randomly mask some elements in the distancematrix d, e.g., supposed di,j.
afterwards, therepresentations of tokens i and j from sepremare concatenated and fed into a linear classiﬁer,which outputs the probabilities over difference dis-tances.
in all of our experiments, 15% of distanceare masked at random..5415similar to bert (devlin et al., 2019) androberta (liu et al., 2019), we conduct the follow-ing operations to boost the robustness.
the distancein matrix d will be masked at 80% probability orreplaced by a random integer with a probability of10%.
for the rest 10% probability, the distance willbe maintained..during pre-training, in addition to the dp pre-training task, we also use the dependency headprediction (hp) task, which is used in wang et al.
(2020) to capture the local head relation amongwords, and the dynamic masked language model(mlm), which is used in liu et al.
(2019) to capturecontextual information.
the ﬁnal loss for the pre-training is the summation of the training loss of dp,hp and mlm tasks..4.5.implementation details.
the implementation of seprem is based on hug-gingface’s transformer (wolf et al., 2019).
toaccelerate the training process, we initialize param-eters from roberta model released by hugging-face5, which contains 24 layers, with 1024 hiddenstates in each layer.
the number of parameters ofour model is 464m.
we pre-train our model with16 32g nvidia v100 gpus for approximatelytwo weeks.
the batch size is set to 2048, and thetotal steps are 500000, of which 30000 is the warmup steps..in both pre-training and ﬁne-tuning stages, ourmodel takes the syntax of the text as the additionalinput, which is pre-processed in advance.
specially,we obtain the dependency tree of each sentence viastanza and then generate the normalized distancematrix..5 experiments.
in this section, we evaluate the proposed sepremon six benchmark datasets over three downstreamtasks, i.e., entity typing, question answering andrelation classiﬁcation..5.1 entity typing.
the entity typing task requires the model to predictthe type of a given entity based on its context.
twoﬁne-grained public datasets, open entity (choiet al., 2018) and figer (ling et al., 2015), areemployed to evaluate our model.
the statistics ofthe aforementioned datasets are shown in table1. following wang et al.
(2020), special token.
5https://huggingface.co/transformers/.
dataset.
train.
dev.
test.
label.
open entityfiger.
2,0002,000,000.
2,00010,000.
2,000563.tacred.
68,124.
22,631.
15,509.
6113.
42.table 1: the statistics of the entity typing datasets,i.e., open entity and figer, and relation classiﬁcationdataset tacred.
label refers to type of a given entityor relation between two entities..“@” is added before and after a certain entity, thenthe representation of the ﬁrst special token “@” isadopted to predict the type of the given entity.
tokeep the evaluation criteria consistent with previousworks (shimaoka et al., 2016; zhang et al., 2019;peters et al., 2019; wang et al., 2019; xiong et al.,2020), we adopt loose micro precision, recall, andf1 to evaluate model performance on open entitydatasets.
as for figer datasets, we utilize strictaccuracy, loose macro-f1, and loose micro-f1 asevaluation metrics..baselines nfgec (shimaoka et al., 2016) recur-sively composes representation of entity contextand further incorporates an attention mechanismto capture ﬁne-grained category memberships ofan entity.
kepler (wang et al., 2019) infusesknowledge into the pre-trained models and jointlylearns the knowledge embeddings and languagerepresentation.
roberta-large (continue training)learns on the proposed pre-training dataset underthe same settings with seprem but only with dy-namic mlm task.
in addition, we also report theresults of bert-base (devlin et al., 2019), ernie(zhang et al., 2019), knowbert (peters et al.,2019), wklm (xiong et al., 2020), roberta-large, and k-adapter (wang et al., 2020) for a fullcomparison..experimental results as we can see in table2, our seprem outperforms all other baselineson both entity typing datasets.
in the open en-tity dataset, with the utility of the syntax of text,seprem achieves an improvement of 3.6% inmicro-f1 score comparing with roberta-large(continue training) model.
the result demonstratesthat the proposed syntax-aware pre-training tasksand syntax-aware attention layer help to capturethe syntax of text, which is beneﬁcial to predict thetypes more accurately.
as for the figer dataset,which contains more labels about the type of entity,seprem still brings an improvement in strict accu-racy, macro-f1, and micro-f1.
this demonstrates.
5416model.
openentity.
figer.
p.r mi-f1.
acc ma-f1 mi-f1.
nfgec (shimaoka et al., 2016)bert-base (zhang et al., 2019)ernie (zhang et al., 2019)knowbert (peters et al., 2019)kepler (wang et al., 2019)wklm (xiong et al., 2020)k-adapter (wang et al., 2020).
roberta-largeroberta-large (continue training)seprem.
68.8076.3778.4278.6077.20-79.25.
77.5577.6381.07.
53.3070.9672.9073.7074.20-75.00.
74.9575.0177.14.
60.1073.5675.5676.1075.70-77.06.
76.2376.3079.06.
55.6052.0457.19--60.2161.81.
56.3156.5263.21.
75.1575.1675.61--81.9984.87.
82.4382.3786.14.
71.7371.6373.39--77.0080.54.
77.8377.8182.05.table 2: results for entity typing task on the openentity and figer datasets..dataset.
train.
dev.
test.
searchqaquasar-t.99,81128,496.
13,8933,000.
27,2473,000.cosmosqa 25,588.
3,000.
7,000.table 3: the statistics of the question answeringdatasets: searchqa, quasar-t and cosmosqa..the effectiveness of leveraging syntactic informa-tion in tasks with more ﬁne-grained information.
speciﬁcally, compared with the k-adapter model,our seprem model brings an improvement of2.6% f1 score on open entity dataset.
it is worthnoting that seprem model is complementary tothe k-adapter model, both of which inject syntacticinformation into model during pre-training stage.
this improvement indicates that injecting syntacticinformation in both the pre-training and ﬁne-tuningstages can make full use of the syntax of the text,thereby beneﬁting downstream tasks..5.2 question answering.
we use open-domain question answering (qa)task and commonsense qa task to evaluate theproposed model.
open-domain qa requires mod-els to answer open-domain questions with the helpof external resources such as materials of collecteddocuments and webpages.
we use searchqa(dunn et al., 2017) and quasart (dhingra et al.,2017) for this task, and adopt exactmatch (em)and loose f1 scores as evaluation metrics.
inthis task, we ﬁrst retrieve related paragraphsaccording to the question from external materialsvia the information retrieval system, and then a.reading comprehension technique is adopted toextract possible answers from the above retrievedparagraphs.
following previous work (lin et al.,2018), we use the retrieved paragraphs provided bywang et al.
(2017b) for the two datasets.
for faircomparison, we follow wang et al.
(2020) to use[<sep>, quesiton,</sep>, paragraph,</sep>]as the input, where <sep> is a special token infront of two segmants and </sep> is a specialsymbol to split two kinds of data types.
we takethe task as a multi-classiﬁcation to ﬁne-tune themodel and use two linear layers over the lasthidden features from models to predict the startand end positions of the answer span..commonsense qa aims to answer questionswhich require commonsense knowledge that isnot explicitly expressed in the question.
weuse the public cosmosqa dataset(huanget al., 2019) for this task, and the accuracyscores are used as evaluation metrics.
the datastatistics of the above three datasets are shownin cosmosqa, each question hasin table 3.
4 candidate answers, and we concatenate thequestion together with each answer separately as[<sep>, context,</sep>, paragraph,</sep>]for input.
the representation of the ﬁrst token isadopted to calculate a score for this answer, andthe answer with the highest score is regarded asthe prediction answer for this question..baselines bidaf (seo et al., 2017) is a bidirec-tional attention network to obtain query-aware con-text representation.
aqa (buck et al., 2018) adoptsa reinforce-guide questions re-write system andgenerates answers according to the re-written ques-tions.
rˆ3 (wang et al., 2017a) selects the most.
5417model.
bidaf (seo et al., 2017)aqa (buck et al., 2018)rˆ3 (wang et al., 2017a)dsqa (lin et al., 2018)evidence agg.
(wang et al., 2018)bert (xiong et al., 2020)wklm (xiong et al., 2020)wklm + ranking (xiong et al., 2020)bert-ftrace+sw ag (huang et al., 2019)k-adapter (wang et al., 2020).
roberta-largeroberta-large (continue training)seprem.
searchqa.
quasar-t.cosmosqa.
f1.
accuracy.
em.
28.6040.5049.0049.0057.0057.1058.7061.70-61.96.
59.0159.3462.31.f1.
34.6047.4055.3055.3063.2061.9063.3066.70-67.31.
65.6265.7167.74.em.
25.90-35.3042.3042.3040.4043.7045.80-45.69.
40.8340.9146.37.
28.50-41.7049.3049.6046.1049.9052.20-52.48.
48.8449.0453.18.
--------68.7081.83.
80.5980.7582.37.table 4: results on qa datasets including: searchqa, quasar-t and cosmosqa..model.
c-gcn (zhang et al., 2018)bert-base (zhang et al., 2019)ernie (zhang et al., 2019)bert-large (baldini soares et al., 2019)bert+mtb (baldini soares et al., 2019)knowbert (peters et al., 2019)kepler (wang et al., 2019)k-adapter (wang et al., 2020).
roberta-largeroberta-large (continue training)seprem.
p.r.69.9067.2369.97--71.6070.4370.05.
70.1770.1970.57.
63.3064.8166.08--71.4073.0273.92.
72.3672.4174.36.f1.
66.4066.0067.9770.1071.5071.5071.7071.93.
71.2571.2872.42.table 5: results for relation classiﬁcation task on ta-cred dataset..conﬁdent paragraph with a designed reinforcementranker.
dsqa (lin et al., 2018) employs a para-graph selector to remove paragraphs with noiseand a paragraph reader to extract the correct an-swer from denoised paragraphs.
evidence agg.
(wang et al., 2018) makes use of multiple pas-sages to generate answers.
bert-ftrace+sw ag(huang et al., 2019) sequentially ﬁne-tunes thebert model on the race and swag datasets forknowledge transfer.
besides the aforementionedmodels, we also report the results of bert (xionget al., 2020), wklm (xiong et al., 2020), wklm+ ranking (xiong et al., 2020), roberta-large,roberta-large (continue training), and k-adapter(wang et al., 2020) for a detailed comparison..experimental results the results of the open-domain qa task are shown in table 4. we cansee that the proposed seprem model brings sig-niﬁcant gains of 3.1% and 8.4% in f1 scores,compared with roberta-large (continue training)model.
this may be partially attributed to the factthat, qa task requires a model to have readingcomprehension ability (wang et al., 2020), and.
the introduced syntax information can guide themodel to avoid concentrating on certain dispens-able words and improve its reading comprehen-sion capacity (zhang et al., 2020).
meanwhile,seprem achieves state-of-the-art results on thecosmosqa dataset, which demonstrates the effec-tiveness of the proposed seprem model.
it canbe also seen that the performance gains observedin cosmosqa are not as substantial as those in theopen-domain qa tasks.
we speculate that cos-mosqa requires capacity for contextual common-sense reasoning and the lack of explicitly injectionof commonsense knowledge into seprem modellimits its improvement..5.3 relation classiﬁcation.
a relation classiﬁcation task aims to predict the re-lation between two given entities in a sentence.
weuse a large-scale relation classiﬁcation dataset ta-cred (zhang et al., 2017) for this task, and adoptmicro-precision, recall, and f1 scores as evaluationmetrics.
the statistics of the tacred datasets areshown in table 1. following wang et al.
(2020),we add special tokens “@” and “#” before and afterthe ﬁrst and second entity respectively.
then, therepresentations of the former token “@” and “#”are concatenated to perform relation classiﬁcation..baselines c-gcn (zhang et al., 2018) encodesthe dependency tree via graph convolutional net-works for relation classiﬁcation.
bert+mtb (bal-dini soares et al., 2019) trains relation representa-tion by matching the blanks.
we also include thebaseline models of bert-base (zhang et al., 2019),ernie (zhang et al., 2019), bert-large (bal-dini soares et al., 2019), knowbert (peters et al.,2019), kepler (wang et al., 2019), roberta-.
5418(a) open entity.
(b) cosmosqa.
(c) tacred.
figure 2: ablation study of the seprem model on three different datasets over entity typing, question answering,and relation classiﬁcation tasks.
all the evaluation models are pre-trained on 10 million sentences..figure 3: case study results on the tacred dataset of relation classiﬁcation tasks.
models are required to predictthe relation between tokens in orange and blue colors.
predictions with mark (cid:88)are the same with true labels..large, roberta-large (continue training), and k-adapter (wang et al., 2020) for a comprehensivecomparison..experimental results table 5 shows the per-formances of baseline models and the proposedseprem on tacred.
as we can see that the pro-posed syntax-aware pre-training tasks and syntax-aware attention mechanism can continuously bringgains in relation classiﬁcation task and sepremoutperforms baseline models overall.
this furtherconﬁrms the outstanding generalization capacityof our proposed model.
it can be also seen thatcompared with k-adapter model, the performancegains of seprem model observed in the tacreddataset are not as substantial as that in open entitydataset.
this may be partially due to the fact thatk-adapter also injects factual knowledge into themodel, which may help in identifying relationships..5.4 ablation study.
to investigate the impacts of various componentsin seprem, experiments are conducted for en-.
tity typing, question answering and relation clas-siﬁcation tasks under the different correspondingbenchmarks, i .e., open entity, cosmosqa, andtacred, respectively.
note that due to the time-consuming issue of training the models on entiredata, we randomly sample 10 million sentencesfrom the whole data to build a small dataset in thisablation study..the results are illustrated in figure 2, in whichwe eliminate two syntax-aware pre-training tasks(i.e., hp and dp) and syntax-aware attention layerto evaluate their effectiveness.
it can be seen thatwithout using the syntax-aware attention layer, im-mediate performance degradation is observed, indi-cating that leveraging syntax-aware attention layerto learn syntax-aware representation could beneﬁtthe seprem.
another observation is that for allthree experiments, eliminating dp pre-training taskleads to worse empirical results.
in other words,compared with existing method (i.e., head predic-tion task), the proposed dependency distance pre-diction task is more advantageous to various down-stream tasks.
this observation may be attributed.
5419 + 3 ' 3 + 3    ' 3                 0 l f r u  )   v f r u h   ;     6 ( 3 5 ( 0  z  r  g l v w d q f h  d z d u h  o d \ h u 6 ( 3 5 ( 0  i x o o + 3 ' 3 + 3    ' 3                         $ f f x u d f \   ;     6 ( 3 5 ( 0  z  r  g l v w d q f h  d z d u h  o d \ h u 6 ( 3 5 ( 0  i x o o + 3 ' 3 + 3    ' 3                                    )   v f r u h   ;     6 ( 3 5 ( 0  z  r  g l v w d q f h  d z d u h  o d \ h u 6 ( 3 5 ( 0  i x o ocaseinput sequencemodelprediction1baldinowas born may 13 , 1953 , and grew up in new jersey ···robertaper:stateorprovince_of_birthsepremper:stateorprovinces_of_residence   (√)2alico, a member company of aigis looking for one j2ee developer ···robertaorg:parentssepremorg:member_of                                     (√)3and strangely enough , cain's short , three-year tenure at the nrais ···robertano_relationsepremorg:top_members/employees            (√)syntax tree of case 1syntax tree of case 2syntax tree of case 3baldinogrewbornjerseyalicocompanyaigmembertenurecainnrato the fact that leveraging global syntactic correla-tion is more beneﬁcial than considering local cor-relation.
moreover, signiﬁcant performance gainscan be obtained by simultaneously exploiting thetwo pre-training tasks and syntax-aware attentionlayer, which further conﬁrms superiority of ourpre-training architecture..5.5 case study.
we conduct a case study to empirically explore theeffectiveness of utilizing syntax information.
inthe case of relation classiﬁcation task, we need topredict the relationship of two tokens in a sentence.
as the three examples shown in figure 3, sepremcan capture the syntax information by the depen-dency tree and make correct predictions.
however,without utilizing syntax information, robertafails to recognize the correct relationship.
to givefurther insight of how syntax information affectsprediction, we also take case 1 for detailed analysis.
the extracted dependency tree captures the closecorrelation of “grew” and “jersey”, which indicatesthat “new jersey” is more likely to be a residenceplace.
these results reﬂects that our model can bet-ter understand the global syntax relations amongtokens by utilizing dependency tree..5.6 analysis of importance score α.under the syntax-enhanced pre-trained frameworkintroduced here,the contextual representation(h n) and syntax-aware representation ( ˆh n) arejointly optimized to abstract semantic informationfrom sentences.
an interesting question concernshow much syntactic information should be lever-in this regard,aged for our pre-trained model.
we further investigate the effect of the importancescore α on the aforementioned six downstreamtasks, and the learned weights α after ﬁne-tuningseprem model are shown in table 6. we observethat the values of α are in the range of 13% and15% on six downstream datasets, which indicatesthat those downstream tasks require syntactic in-formation to obtain the best performance and onceagain conﬁrms the effectiveness of utilizing syntaxinformation..to have a further insight of the effect broughtby importance score α, we conduct experimentson seprem w/o α, which eliminates the α inequation 1 and equally integrates the syntax-aware and contextual representation, i.e., h n =transf ormern(h n−1+ ˆh n−1).
the pre-trainingsettings of the seprem w/o α model are the same.
datasets.
model.
performance values of α.open entity.
figer.
searchqa.
quasar-t.cosmosqa.
tacred.
seprem.
seprem w/o α.seprem.
seprem w/o α.seprem.
seprem w/o α.seprem.
seprem w/o α.seprem.
seprem w/o α.seprem.
seprem w/o α.
79.06.
77.13.
82.05.
79.54.
67.74.
66.31.
53.18.
51.84.
82.37.
81.06.
72.42.
71.82.
0.1334.
0.1428.
0.1385.
0.1407.
0.1357.
0.1407.
-.
-.
-.
-.
-.
-.
table 6: the model’s performance and the correspond-ing values of importance score α after ﬁne-tuning onsix public benchmark datasets.
performance is underthe evaluate metrics of either mi-f1 or accuracy scores..with the proposed seprem model.
it can be seenin table 6 that, the performances drop 1%∼3% onthe six datasets when excluding the α. this obser-vation indicates the necessity of introducing the αto better integrate the syntax-aware and contextualrepresentation..6 conclusion.
in this paper, we present seprem that leveragesyntax information to enhance pre-trained mod-els.
to inject syntactic information, we introduce asyntax-aware attention layer and a newly designedpre-training task are proposed.
experimental re-sults show that our method achieves state-of-the-art performance over six datasets.
further analysisshows that the proposed dependency distance pre-diction task performs better than dependency headprediction task..acknowledgments.
we are grateful to yeyun gong, ruize wang andjunjie huang for fruitful comments.
we are obligedto zijing ou and wenxuan li for perfecting thisarticle.
we appreciate genifer zhao for beautifyingthe ﬁgures of this article.
zenan xu and qinliangsu are supported by the national natural sciencefoundation of china (no.
61806223, 61906217,u1811264), key r&d program of guangdongprovince (no.
2018b010107005), national nat-ural science foundation of guangdong province(no.
2021a1515012299).
zenan xu and qinliangsu are also supported by huawei mindspore..5420references.
livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-ing.
in acl, pages 2895–2905..christian buck, jannis bulian, massimiliano cia-ramita, andrea gesmundo, neil houlsby, wojciechgajewski, and wei wang.
2018. ask the rightquestions: active question reformulation with re-inforcement learning.
in iclr..eunsol choi, omer levy, yejin choi, and luke zettle-in acl,.
moyer.
2018. ultra-ﬁne entity typing.
pages 87–96..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..bhuwan dhingra, kathryn mazaitis, and william wcohen.
2017. quasar: datasets for question an-in arxiv preprintswering by search and reading.
arxiv:1707.03904..matthew dunn, levent sagun, mike higgins, v. ugurg¨uney, volkan cirik, and kyunghyun cho.
2017.searchqa: a new q&a dataset augmented within arxiv preprintcontext from a search engine.
arxiv:1704.05179..kelvin guu, kenton lee, zora tung, panupong pasu-pat, and ming-wei chang.
2020. realm: retrieval-arxivaugmented language model pre-training.
preprint arxiv:2002.08909..john hewitt and christopher d manning.
2019. astructural probe for ﬁnding syntax in word represen-in proceedings of the 2019 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4129–4138..lifu huang, ronan le bras, chandra bhagavatula, andyejin choi.
2019. cosmos qa: machine readingcomprehension with contextual commonsense rea-soning.
in emnlp, pages 2391–2401..dan jurafsky.
2000. speech & language processing..pearson education india..yankai lin, haozhe ji, zhiyuan liu, and maosong sun.
2018. denoising distantly supervised open-domainquestion answering.
in acl, pages 1736–1745..xiao ling, sameer singh, and daniel s. weld.
2015.design challenges for entity linking.
tacl, 3:315–328..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..xuan-phi nguyen, shaﬁq joty, steven hoi, andtree-structured attentionin international.
richard socher.
2020.with hierarchical accumulation.
conference on learning representations..matthew e peters, mark neumann,.
iv logan,l robert, roy schwartz, vidur joshi, sameer singh,and noah a smith.
2019. knowledge enhanced con-textual word representations.
in emnlp, pages 43–54..f. petroni, tim rockt¨aschel, patrick lewis, a. bakhtin,y. wu, alexander h. miller, and s. riedel.
2019.arxiv,language models as knowledge bases?
abs/1909.01066..nina poerner, ulli waltinger, and hinrich sch¨utze.
2019. bert is not a knowledge base (yet): fac-tual knowledge vs. name-based reasoning in unsu-pervised qa.
arxiv, abs/1911.03681..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..devendra singh sachan, yuhao zhang, peng qi, andwilliam hamilton.
2020. do syntax trees help pre-arxivtrained transformers extract information?
preprint arxiv:2008.09084..min joon seo, aniruddha kembhavi, ali farhadi, andhannaneh hajishirzi.
2017. bidirectional attentionin 5th inter-ﬂow for machine comprehension.
national conference on learning representations,iclr 2017..sonse shimaoka, pontus stenetorp, kentaro inui, andsebastian riedel.
2016. an attentive neural ar-chitecture for ﬁne-grained entity type classiﬁcation.
in proceedings of the 5th workshop on automatedknowledge base construction(akbc), pages 69–74..alon talmor, yanai elazar, yoav goldberg, andjonathan berant.
2020. olmpics-on what languagemodel pre-training captures.
transactions of the as-sociation for computational linguistics, 8:743–758..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..5421zhengyan zhang, xu han, zhiyuan liu, xin jiang,maosong sun, and qun liu.
2019. ernie: en-hanced language representation with informativeentities.
in acl, pages 1441–1451..zhuosheng zhang, yuwei wu, junru zhou, and sufengduan.
2020. sg-net: syntax-guided machine read-ing comprehension.
in thirty-fourth aaai confer-ence on artiﬁcial intelligence (aaai-2020)..ruize wang, duyu tang, nan duan, zhongyu wei,xuanjing huang, cuihong cao, daxin jiang, mingzhou, et al.
2020. k-adapter:infusing knowl-edge into pre-trained models with adapters.
arxivpreprint arxiv:2002.01808..shuohang wang, mo yu, xiaoxiao guo, zhiguo wang,tim klinger, wei zhang, shiyu chang, geraldtesauro, bowen zhou, and jing jiang.
2017a.
re-inforced reader-ranker for open-domain question an-swering.
arxiv preprint arxiv:1709.00023..shuohang wang, mo yu, jing jiang, wei zhang, xiaox-iao guo, shiyu chang, zhiguo wang, tim klinger,gerald tesauro, and murray campbell.
2018. ev-idence aggregation for answer re-ranking in open-domain question answering.
in iclr..wenhui wang, nan yang, furu wei, baobao chang,and ming zhou.
2017b.
gated self-matching net-works for reading comprehension and question an-swering.
in acl, pages 189–198..xiaozhi wang, tianyu gao, zhaocheng zhu, zhiyuanliu, juanzi li, and jian tang.
2019. kepler: auniﬁed model for knowledge embedding and pre-arxiv preprinttrained language representation.
arxiv:1911.06136..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, et al.
2019. huggingface’s transformers: state-of-the-art natural language processing.
arxiv, pagesarxiv–1910..wenhan xiong, jingfei du, william yang wang, andveselin stoyanov.
2020.pretrained encyclope-dia: weakly supervised knowledge-pretrained lan-guage model.
in iclr..wang yaushian, lee hung-yi, and chen yun-nung.
2019. multitree transformer: integrating tree struc-tures into self-attention.
in proceedings of the 2019conference on empirical methods in natural lan-guage processing..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019. defending against neural fakenews.
in h. wallach, h. larochelle, a. beygelz-imer, f. d'alch´e-buc, e. fox, and r. garnett, editors,advances in neural information processing systems32, pages 9054–9065.
curran associates, inc..yuhao zhang, peng qi, and christopher d. manning.
2018. graph convolution over pruned dependencyin emnlp,trees improves relation extraction.
pages 2205–2215..yuhao zhang, victor zhong, danqi chen, gabor an-geli, and christopher d. manning.
2017. position-aware attention and supervised data improve slotfilling.
in emnlp, pages 35–45..5422