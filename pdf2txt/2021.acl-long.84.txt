the art of abstention: selective prediction and error regularizationfor natural language processing.
ji xin, raphael tang, yaoliang yu, and jimmy lin.
david r. cheriton school of computer science, university of waterloovector institute for artiﬁcial intelligence{ji.xin,r33tang,yaoliang.yu,jimmylin}@uwaterloo.ca.
abstract.
in selective prediction, a classiﬁer is allowedto abstain from making predictions on low-conﬁdence examples.
though this setting isinteresting and important, selective predictionhas rarely been examined in natural languageprocessing (nlp) tasks.
to ﬁll this void inthe literature, we study in this paper selec-tive prediction for nlp, comparing differentmodels and conﬁdence estimators.
we fur-ther propose a simple error regularization trickthat improves conﬁdence estimation withoutsubstantially increasing the computation bud-get.
we show that recent pre-trained trans-former models simultaneously improve bothmodel accuracy and conﬁdence estimation ef-fectiveness.
we also ﬁnd that our proposedregularization improves conﬁdence estimationand can be applied to other relevant scenarios,such as using classiﬁer cascades for accuracy–efﬁciency trade-offs.
source code for this pa-per can be found at https://github.com/castorini/transformers-selective..1.introduction.
recent advances in deep learning models havepushed the frontier of natural language process-ing (nlp).
pre-trained language models based onthe transformer architecture (vaswani et al., 2017)have improved the state-of-the-art results on manynlp applications.
naturally, these models are de-ployed in various real-world applications.
however,one may wonder whether they are always reliable,as pointed out by guo et al.
(2017) that modernneural networks, while having better accuracy, tendto be overconﬁdent compared to simple networksfrom 20 years ago..in this paper, we study the problem of selectiveprediction (geifman and el-yaniv, 2017) in nlp.
under the setting of selective prediction, a modelis allowed to abstain from making predictions onuncertain examples (figure 1) and thereby reduce.
figure 1: example of a selective classiﬁer that makesa prediction for a conﬁdent example (left) and abstainsfor an uncertain one (right)..the error rate.
this is a practical setting in a lotof realistic scenarios, such as making entailmentjudgments for breaking news articles in search en-gines (carlebach et al., 2020) and making criticalpredictions in medical and legal documents (zhanget al., 2019).
in these cases, it is totally acceptable,if not desirable, for the models to admit their un-certainty and call for help from humans or better(but more costly) models..under the selective prediction setting, we con-struct a selective classiﬁer by pairing a standardclassiﬁer with a conﬁdence estimator.
the conﬁ-dence estimator measures how conﬁdent the modelis for a certain example, and instructs the classiﬁerto abstain on uncertain ones.
naturally, a good con-ﬁdence estimator should have higher conﬁdence forcorrectly classiﬁed examples than incorrect ones.
we consider two choices of conﬁdence estima-tors, softmax response (sr; hendrycks and gimpel,2017), and monte-carlo dropout (mc-dropout; galand ghahramani, 2016).
sr interprets the outputof the ﬁnal softmax layer as a probability distri-bution and the highest probability as conﬁdence.
mc-dropout repeats the inference process multipletimes, each time with a different dropout mask, andtreats the negative variance of maximum probabil-ity as conﬁdence.
conﬁdence estimation is criticalto selective prediction, and therefore studying thisproblem also helps relevant tasks such as active.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1040–1051august1–6,2021.©2021associationforcomputationallinguistics1040i enjoyed movies.positiveselective classifierselective classifieri enjoyed moviesbefore watching this.abstain?
learning (cohn et al., 1995; shen et al., 2018) andearly exiting (schwartz et al., 2020; xin et al., 2020;zhou et al., 2020; xin et al., 2021)..in this paper, we compare selective predictionperformance of different nlp models and conﬁ-dence estimators.
we also propose a simple trick,error regularization, which can be applied to anyof these models and conﬁdence estimators, and im-prove their selective prediction performance.
wefurther study the application of selective predic-tion on a variety of interesting applications, such asclassiﬁcation with no valid labels (no-answer prob-lem) and using classiﬁer cascades for accuracy–efﬁciency trade-offs.
experiments show that recentpowerful nlp models such as bert (devlin et al.,2019) and albert (lan et al., 2020) improvenot only accuracy but also selective prediction per-formance; they also demonstrate the effectivenessof the proposed error regularization by producingbetter conﬁdence estimators which reduce the areaunder the risk–coverage curve by 10%..2 related work.
selective prediction has been studied by the ma-chine learning community for a long time (chow,1957; el-yaniv and wiener, 2010).
more recently,geifman and el-yaniv (2017, 2019) study selec-tive prediction for modern deep learning models,though with a focus on computer vision tasks..selective prediction is closely related to conﬁ-dence estimation, as well as out-of-domain (ood)detection (sch¨olkopf et al., 2000; liang et al., 2018)and prediction error detection (hendrycks and gim-pel, 2017), albeit more remotely.
there have beenmany different methods for conﬁdence estimation.
bayesian methods such as markov chain montecarlo (geyer, 1992) and variational inference (hin-ton and van camp, 1993; graves, 2011) assumea prior distribution over model parameters and ob-tain conﬁdence estimates through the posterior.
ensemble-based methods (gal and ghahramani,2016; lakshminarayanan et al., 2017; geifmanet al., 2019) estimate conﬁdence based on statisticsof the ensemble model’s output.
these methods,however, are computationally practical for smallmodels only.
current large-scale pre-trained nlpmodels, such as bert (devlin et al., 2019) androberta (liu et al., 2019), are too expensive torun multiple times of inference, and therefore re-quire lightweight conﬁdence estimation..previously, selective prediction and conﬁdence.
estimation have been studied in limited nlp scenar-ios.
dong et al.
(2018) train a separate conﬁdencescoring model to explicitly estimate conﬁdence insemantic parsing.
kamath et al.
(2020) introduceselective prediction for ood question answering,where abstention is allowed for ood and difﬁ-cult questions.
however, selective prediction forbroader nlp applications has yet to be explored,and we hope to draw the attention of the nlp com-munity to this problem..there are two notable related topics, conﬁdencecalibration and unanswerable questions, but the dif-ference between them and selective prediction isstill nontrivial.
calibration (guo et al., 2017; jianget al., 2018; kumar et al., 2018; wang et al., 2020;desai and durrett, 2020) focuses on adjusting theoverall conﬁdence level of a model, while selectiveprediction is based on relative conﬁdence amongthe examples.
for example, the most widely usedcalibration technique, temperature scaling (platt,1999), globally increases or decreases the model’sconﬁdence on all examples, but the ranking of allexamples’ conﬁdence is unchanged.
unanswer-able questions are considered in previous datasets,e.g., squad2.0 (rajpurkar et al., 2018).
the unan-swerable questions are impossible to answer evenfor humans, while abstention in selective predic-tion is due to model uncertainty rather than model-agnostic data uncertainty..3 background.
we introduce relevant concepts about selectiveprediction and conﬁdence estimators, using multi-class classiﬁcation as an example..3.1 selective prediction.
given a feature space x and a set of labels y, astandard classiﬁer f is a function f : x → y. aselective classiﬁer is another function h : x →y ∪ {⊥}, where ⊥ is a special label indicating theabstention of prediction.
normally, the selectiveclassiﬁer is composed of a pair of functions h =(f, g), where f is a standard classiﬁer and g is theselective function g : x → {0, 1}.
given an inputx ∈ x , the output of the selective classiﬁer is asfollows:.
(cid:40).
h(x) =.
f (x),⊥,.
if g(x) = 1,if g(x) = 0,.
(1).
and we can see that the output of g controls predic-tion or abstention.
in most cases, g consists of a.
1041conﬁdence estimator ˜g : x → r, and a conﬁdencethreshold θ:.
g(x) = 1[˜g(x) > θ]..(2).
˜g(x) indicates how conﬁdent the classiﬁer f is onthe example x, and θ controls the overall predictionversus abstention level..a selective classiﬁer makes trade-offs betweencoverage and risk.
given a labeled dataset s ={(xi, yi)}ni=1 ⊂ x × y and an error function l tocalculate each example’s error li = l(f (xi), yi),the coverage and the selective risk of a classiﬁerh = (f, g) on s are, respectively,.
γ(h) =.
1|s|.
(cid:88).
g(xi),.
r(h) =.
(cid:80).
(xi,yi)∈s(xi,yi)∈s g(xi)li(cid:80)(xi,yi)∈s g(xi).
..(3).
(4).
the selective classiﬁer aims to minimize the selec-tive risk at a given coverage..the performance of a selective classiﬁer h =(f, g) can be evaluated by the risk–coveragecurve (rcc; el-yaniv and wiener, 2010), which isdrawn by varying the conﬁdence threshold θ (seefigure 2 for an example).
quantitatively, the areaunder curve (auc) of rcc measures the effective-ness of a selective classiﬁer.1.
in order to minimize the auc of rcc, the selec-tive classiﬁer should, intuitively, output g(x) = 1for correctly classiﬁed examples and g(x) = 0for incorrect ones.
therefore, an ideal ˜g hasthe following property: ∀(xi, yi), (xj, yj) ∈ s,˜g(xi) ≤ ˜g(xj) iff li ≥ lj.
we propose the follow-ing metric, reversed pair proportion (rpp), to eval-uate how far the conﬁdence estimator ˜g is to ideal,given the labeled dataset s of size n:.
n(cid:80)1≤i,j≤n.
rpp =.
1[˜g(xi) < ˜g(xj), li < lj].
n2.
..(5).
rpp measures the proportion of example pairs witha reversed conﬁdence–error relationship, and then2 in the denominator is used to normalize thevalue.
an ideal conﬁdence estimator has an rppvalue of 0..3.2 conﬁdence estimators.
in most cases for multi-class classiﬁcation, the lastlayer of the classiﬁer is a softmax activation, which.
1auc in this paper always corresponds to rccs..outputs a probability distribution p (y) over the setof labels y, where y ∈ y is a label.
in this case,the classiﬁer can be written as.
f (x) = ˆy = arg max.
p (y),.
(6).
y∈y.
where ˆy is the label with highest probability..perhaps the most straightforward and popularchoice for the conﬁdence estimator is softmax re-sponse (hendrycks and gimpel, 2017):.
˜gsr(x) = p (ˆy) = maxy∈y.
p (y)..(7).
alternatively, we can use the difference betweenprobabilities of the top two classes for conﬁdenceestimation.
we refer to this method as pd (proba-bility difference)..gal and ghahramani (2016) argue that “softmaxoutputs are often erroneously interpreted as modelconﬁdence”, and propose to use mc-dropout asthe conﬁdence estimator.
in mc-dropout, p (ˆy)is computed for a total of r times, using adifferent dropout mask at each time, producingp1(ˆy), p2(ˆy), · · · , pr(ˆy).
the variance of them isused to estimate the conﬁdence:.
˜gmc(x) = −var[p1(ˆy), · · · , pr(ˆy)]..(8).
we use the negative sign here because a larger vari-ance indicates a greater uncertainty, i.e., a lowerconﬁdence (geifman and el-yaniv, 2017; kamathet al., 2020).
by using different dropout masks,mc-dropout is equivalent to using an ensemble forconﬁdence estimation, but does not require actuallytraining and storing multiple models.
nevertheless,compared to sr, the inference cost of mc-dropoutis multiplied by r, which can be a problem whenmodel inference is expensive..4 error regularization.
4.1 regularizers.
sr and mc-dropout are often used directly outof the box as the conﬁdence estimator.
we pro-pose a simple regularization trick that can be easilyapplied at training (or ﬁne-tuning for pre-trainedmodels) time and can improve the effectiveness ofthe induced conﬁdence estimators..considering that a good conﬁdence estimatorshould minimize rpp deﬁned in equation 5, we.
1042figure 2: risk–coverage curves of bert-base and bert-large models with sr and mc conﬁdence estimators.
the legend applies to all sub-plots..add the following regularizer to the original train-ing loss function:.
ltotal =.
h(f (xi), yi) + λlreg,.
lreg =.
∆i,j 1[ei > ej],.
(9).
(10).
n(cid:88).
i=1.
(cid:88).
1≤i,j≤n.
∆i,j = max{0, ˜gsr(xi) − ˜gsr(xj)}2..(11).
here, h(·, ·) is the task-speciﬁc loss function suchas cross entropy (h is not the same with the errorfunction l), λ is the hyperparameter for regulariza-tion, ˜gsr is the maximum softmax probability de-ﬁned in equation 7, and ei is the error of example iat the current iteration—details to calculate it willbe explained in the next paragraph.
we use srconﬁdence here because it is easily accessible attraining time, while mc-dropout conﬁdence is not.
the intuition of this regularizer is as follows: if themodel’s error on example i is larger than its erroron example j (i.e., example i is considered more“difﬁcult” for the model), then the conﬁdence onexample i should not be greater than the conﬁdenceon example j..in practice, at each iteration of training (ﬁne-tuning), we can obtain the error ei in one of the twofollowing ways..• current iteration error we simply use theerror function l to calculate the error of the ex-ample at the current iteration, and use it as ei.
inthe case of multi-class classiﬁcation, l is oftenchosen as the 0–1 error..• history record error since we intend touse ei to quantify how difﬁcult an example.
is, we draw inspiration from forgettable exam-ples (toneva et al., 2019).
we calculate exampleerror with l throughout the training process, anduse the error averaged from the beginning to thecurrent iteration as ei.
in this case, ei takes valuefrom [0, 1]..4.2 practical approximations.
in practice, it is computationally prohibitive toeither strictly compute lreg from equation 10 forall example pairs, or to calculate history recorderror after every iteration.
we therefore make thefollowing two approximations..for lreg from equation 10, we only consider ex-amples from the mini-batch of the current iteration.
for current iteration error, where ei takes valuefrom {0, 1}, we consider all pairs where ei = 1and ej = 0. for history record error, where eitakes value from [0, 1], we sort all examples inthe mini-batch by their errors, and divide the mini-batch into 20% of examples with high error valuesand 80% of examples with low error values;2 thenwe consider all pairs where example i is from theformer 20% and j from the latter 80%..for calculating history record error, we computeand record the error values for the entire training set10 times per epoch (once after each 10% iterations).
at each training iteration, we use the average oferror values recorded so far as ei..5 experiments.
we conduct experiments of selective predictionon nlp tasks.
since the formulation of selec-tive prediction is model agnostic, we choose the.
2we choose this 20–80 division to mimic the current itera-tion error case, where roughly 20% of training examples havean error of 1 and 80% have an error of 0..10430.00.20.40.60.81.00.0000.0250.0500.0750.1000.1250.1500.175mrpc0.00.20.40.60.81.00.000.020.040.060.08qnli0.00.20.40.60.81.00.000.020.040.060.080.100.120.14mnlibert-base-mcbert-large-mcbert-base-srbert-large-srcoverageriskmodel.
lstm.
bertbase.
bertlarge.
albertbase.
srmc.
srmc.
srmc.
srmc.
conﬁdenceestimator.
mrpc.
qnli.
f1(↑) auc(↓) rpp(↓) acc(↑) auc(↓) rpp(↓).
acc(↑).
mnli-(m/mm)auc(↓).
rpp(↓).
101.5137.0.
9.011.3.
1539.12039.8.
9.011.6.
65.4/64.3.
1984.0/1990.03554.1/3548.1.
6.8/6.611.7/11.7.
81.8.
87.7.
89.0.
90.9.
33.838.3.
27.035.9.
16.043.9.
3.74.5.
3.24.3.
2.15.6.
62.7.
91.6.
92.0.
90.9.
111.9130.1.
105.6114.6.
122.8160.0.
1.21.3.
1.11.2.
1.21.6.
84.9/84.6.
86.4/86.0.
84.7/85.4.
514.8/491.7639.0/677.5.
486.1/470.0482.0/510.2.
469.3/453.5921.1/878.3.
2.6/2.43.4/3.5.
2.5/2.42.5/2.6.
2.3/2.35.0/4.7.
table 1: comparing selective prediction performance of different models and conﬁdence estimators.
all metricsexcept auc are in percentages..dataset.
#train.
#dev (m/mm).
#labels.
mrpcqnlimnlisst-5bmnlibsst-5.
3.7k104.7k392.7k8.5k261.8k6.9k.
0.4k5.5k9.8k/9.8k1.1k9.8k/9.8k1.1k.
22352+12+1.
table 2: dataset statistics.
bmnli/bsst-5 are bina-rized version of mnli/sst-5, with two normal labelsand a special no-answer label..following representative models: (1) bert-baseand bert-large (devlin et al., 2019), the dom-inant transformer-based models of recent years;(2) albert-base (lan et al., 2020), a variant ofbert featuring parameter sharing and memory ef-ﬁciency; (3) long short-term memory (lstm;hochreiter and schmidhuber, 1997), the popularpre-transformer model that is lightweight and fast.
in this section, we compare the performance ofselective prediction of these models, demonstratethe effectiveness of the proposed error regulariza-tion, and show the application of selective predic-tion in two interesting scenarios—the no-answerproblem and the classiﬁer cascades..5.1 experiment setups.
we conduct experiments mainly on three datasets:mrpc (dolan and brockett, 2005), qnli (wanget al., 2018), and mnli (williams et al., 2018).
insection 5.4, we will need an additional non-binarydataset sst-5 (socher et al., 2013).
statistics ofthese datasets can be found in table 2. followingthe setting of the glue benchmark (wang et al.,2018), we use the training set for training/ﬁne-tuning and the development set for evaluation (the.
test set’s labels are not publicly available); mnli’sdevelopment set has two parts, matched and mis-matched (m/mm).
these datasets include semanticequivalence judgments, entailment classiﬁcation,and sentiment analysis, which are important ap-plication scenarios for selective prediction as dis-cussed in section 1..the implementation is based on pytorch (paszkeet al., 2019) and the huggingface transformers li-brary (wolf et al., 2020).
training/ﬁne-tuning andinference are done on a single nvidia tesla v100gpu.
since we are evaluating the selective predic-tion performance of different models instead of pur-suing state-of-the-art results, we do not extensivelytune hyperparameters; instead, most experimentsettings such as hidden sizes, learning rates, andbatch sizes are kept unchanged from the hugging-face library.
further setup details can be found inappendix a..5.2 comparing different models.
we compare selective prediction performance ofdifferent models in table 1. for each model, wereport the performance given by the two conﬁdenceestimators, softmax response (sr) and mc-dropout(mc); the results of using pd for conﬁdence es-timation are very similar to those of sr, and wereport them in appendix b due to space limita-tions.
the accuracy and the f1 score3 measurethe effectiveness of the classiﬁer f , rpp measuresthe reliability of the conﬁdence estimator ˜g, andauc is a comprehensive metric for both the clas-siﬁer and the conﬁdence estimator.
the choice ofconﬁdence estimator does not affect the model’saccuracy.
we also provide risk–coverage curves(rccs) of different models and conﬁdence estima-.
3we henceforth refer to both accuracy and f1 scores simply.
as accuracy for the sake of conciseness..1044figure 3: selective prediction performance of mc-dropout with different numbers of repetitive runs (x-axis) anddropout rates (marked in the legend).
bert-base is used here.
the legend applies to all sub-plots..model.
reg..mrpc.
f1(↑) auc(↓) rpp(↓) acc(↑) auc(↓) rpp(↓).
acc(↑).
lstm.
bertbase.
bertlarge.
albertbase.
nonecurr.
hist..nonecurr.
hist..nonecurr.
hist..nonecurr.
hist..81.881.281.2.
87.788.187.9.
89.089.789.0.
90.991.491.0.
101.594.092.3.
33.831.230.3.
27.020.624.4.
16.013.216.2.
9.047.707.58.
3.703.493.51.
3.173.053.30.
2.131.822.18.
62.764.764.7.
91.691.991.4.
92.091.292.1.
90.990.991.2.qnli.
1539.11376.21368.0.
111.9100.1113.9.
105.698.299.4.
122.8104.3117.5.mnli-(m/mm)auc(↓).
1984.0/1990.01976.0/1990.51974.4/1987.4.
514.8/491.7479.1/461.8490.7/472.5.
486.1/470.0417.7/434.4404.9/400.6.
469.3/453.5451.2/463.9461.1/429.8.
65.4/64.365.5/64.465.3/64.6.
84.9/84.684.6/84.684.4/84.5.
86.4/86.086.5/85.585.5/85.9.
84.7/85.484.7/85.284.6/85.2.
rpp(↓).
6.81/6.606.80/6.646.74/6.71.
2.55/2.412.39/2.272.42/2.32.
2.45/2.392.17/2.172.25/2.25.
2.32/2.302.25/2.232.26/2.30.
8.998.518.42.
1.161.081.20.
1.061.040.94.
1.211.231.12.table 3: comparing different regularizers (reg.)
for different models and datasets.
selective prediction perfor-mance is measured by auc and rpp.
all metrics except auc are in percentages..tors in figure 2. mc in the table and the ﬁgure usesa dropout rate of 0.01 and repetitive runs r = 10.we ﬁrst notice that models with overall higher ac-curacy also have better selective prediction perfor-mance (lower auc and rpp).
for example, com-pared with lstm, bert-base has higher accuracyand lower auc/rpp on all datasets, and the sameapplies to the comparison between bert-base andbert-large.
since the classiﬁer’s effectivenessdoes not directly affect rpp, the consistency ofrpp’s and accuracy’s improvement indicates thatsophisticated models simultaneously improve bothmodel accuracy and conﬁdence estimation.
thisis in contrast to the discovery by guo et al.
(2017)that sophisticated neural networks, despite havingbetter accuracy, are more easily overconﬁdent andworse calibrated than simple ones..both auc and rpp.
this shows that for nlp tasksand models, model conﬁdence estimated by mc-dropout fails to align well with real example difﬁ-culty.
we further study and visualize in figure 3the effect of different dropout rates and differentnumbers of repetitive runs r on mc-dropout’s se-lective prediction performance.
we can see that (1)a dropout rate of 0.01 is a favorable choice: largerdropout rates lead to worse performance whilesmaller ones do not improve it; (2) mc-dropoutneeds at least 20 repetitions to obtain results com-parable to sr, which is extremely expensive.
al-though mc-dropout has a sound theoretical foun-dation, its practical application to nlp tasks needsfurther improvements..5.3 effect of error regularization.
we also notice that mc-dropout performs con-sistently worse than softmax response, shown by.
in this part, we show that our simple regularizationtrick improves selective prediction performance.
in.
10450204060801003040506070mrpc020406080100100150200250300350qnli020406080100500600700800900100011001200mnlimc-0.2mc-0.1mc-0.01mc-0.001srrepetitive runs (r)aucmodel.
reg..bsst5acc(↑) acc*(↑) auc(↓) rpp(↓).
bertbase.
bertlarge.
albertbase.
nonecurr.
hist..nonecurr.
hist..nonecurr.
hist..71.772.072.6.
73.273.373.5.
72.372.472.5.
74.073.874.7.
73.774.273.7.
73.573.273.2.
174.7173.5157.4.
158.4137.1148.8.
172.4168.0161.0.
5.345.355.17.
5.584.824.53.
5.615.325.63.bmnli-(m/mm).
acc(↑).
acc*(↑).
auc(↓).
rpp(↓).
63.9/64.263.8/64.263.8/64.1.
64.8/64.864.5/65.165.0/64.8.
64.0/64.363.8/64.263.9/64.4.
70.6/70.971.1/71.470.7/71.6.
72.9/72.572.7/73.273.1/73.2.
71.6/72.472.9/72.371.6/73.5.
1645.4/1630.71562.2/1562.11630.5/1583.2.
1861.0/1852.31476.8/1629.71695.9/1460.6.
1579.1/1534.41563.8/1550.91601.8/1496.3.
4.74/4.724.41/4.454.62/4.51.
5.18/5.164.91/4.684.14/4.11.
4.44/4.344.45/4.324.20/4.10.
table 4: selective prediction performance of different models and regularization methods (reg.)
on two datasetswith the no-answer label.
all metrics except auc are in percentages..table 3, we report the accuracy, auc, and rpp foreach model, paired with three different regularizers:no regularization (none), current error regularizer(curr.
), and history error regularizer (hist.
), as de-scribed in section 4..we ﬁrst see that applying error regularization(either current or history) does not harm modelaccuracy.
there are minor ﬂuctuations, but gener-ally speaking, error regularization has no negativeeffect on the models’ effectiveness..we can also see that error regularization im-proves models’ selective prediction performance,reducing auc and rpp.
as we mention in the pre-vious section, auc is a comprehensive metric forboth the classiﬁer f and the conﬁdence estimator˜g.
we therefore focus on this metric in this section,and we bold the lowest auc in table 3. we see thaterror regularization consistently achieve the lowestauc values, and on average, the best scores areapproximately 10% lower than the scores withoutregularization.
this shows that error regulariza-tion produces conﬁdence estimators that give betterconﬁdence rankings..the two regularization methods, current errorand history error, are similar in quality, with nei-ther outperforming the other across all models anddatasets.
therefore, we can conclude only that theerror regularization trick improves selective predic-tion, but the best speciﬁc method varies.
we leavethis exploration for future work..5.4 the no-answer problem.
in this section, we conduct experiments to see howselective classiﬁers perform on datasets that ei-ther allow abstention or, equivalently, provide theno-answer label.
this no-answer problem occurs.
whenever a trained classiﬁer encounters an examplewhose label is unseen in training, which is commonin practice.
for example, in the setting of ultraﬁneentity typing with more than 10,000 labels (choiet al., 2018), it is unsurprising to encounter exam-ples with unseen types.
ideally, in this case, theclassiﬁer should choose the no-answer label.
thissetting is important yet often neglected, and thereexist few classiﬁcation datasets with the no-answerlabel.
we therefore build our own datasets, bina-rized mnli and sst-5 (bmnli and bsst-5), toevaluate different models in this setting (table 2)..the mnli dataset is for sentence entailmentclassiﬁcation.
given a pair of sentences, the goalis to predict the relationship between them, amongthree labels: entailment, contradiction, and neutral.
the sst-5 dataset is for ﬁne-grained sentence sen-timent classiﬁcation.
given a sentence, the goalis to predict the sentiment of it, among ﬁve labels:strongly positive, mildly positive, strongly negative,mildly negative, and neutral.
to convert the orig-inal mnli and sst-5 datasets into our binarizedversions bmnli and bsst-5, we modify the fol-lowing: for sst-5, we merge strongly and mildlypositive/negative into one positive/negative class;for mnli, we simply regard entailment as positiveand contradictory as negative.
we then removeall neutral instances from the training set but keepthose in the development and test sets.
this way,neutral instances in the development and test setsshould be classiﬁed as no-answer by the model.
a good model is expected to assign neutral exam-ples in the development and test sets with low con-ﬁdence scores, thereby predicting the no-answerlabel for them..we report results for these two datasets with.
1046figure 4: accuracy–efﬁciency trade-offs by using classiﬁer cascades.
all examples are ﬁrst evaluated by lstm,and then we compare three ways of choosing examples to send to the more sophisticated model (bert-base):random selection (random), sr without regularization (sr), and sr with history error regularization (sr-hist.).
the legend applies to all sub-plots..the no-answer label in table 4. accuracy (acc),auc, and rpp have the same meaning from theprevious sections.
we also consider a new metricspeciﬁcally for the no-answer setting, augmentedaccuracy (acc*), which is calculated as follows:(1) we make a number of attempts by searching athreshold α from 0.7 to 1.0 in increments of 0.01;(2) for each attempt, we regard all examples withpredicted conﬁdence lower than α as neutral, andthen calculate the accuracy; (3) among all attempts,we take the highest accuracy as acc*.
choosingthe optimal α requires knowing the ground-truthanswers in advance and is not practical in reality.4instead, acc* indicates how well a model recog-nizes examples whose label is likely unseen in thetraining set..we ﬁrst see that acc* is consistently higher thanacc in all cases.
this is unsurprising, but it demon-strates that unseen samples indeed have lower con-ﬁdence and shows that introducing the abstentionoption is beneﬁcial in the no-answer scenario.
also,we observe that error regularization improves themodels’ selective prediction performance, produc-ing lower auc/rpp and higher acc* in most cases.
this further demonstrates the effectiveness of thesimple error regularization trick..secondly, we can see that the improvement ofacc* over acc is larger in bmnli than in bsst-5.
the reason is that in bmnli, neutral examples con-stitute about a third of the entire development set,while in bsst-5 they constitute only a ﬁfth.
the.
4alternatively, one may use a validation set to choose theoptimal α. in our experiments, however, we use the develop-ment set for evaluation, since the labels of the test set itselfare not publicly available.
holding out a part of the trainingset for validation is left for future exploration..improvement is positively correlated with the pro-portion of neutral examples, since they are assignedlower conﬁdence scores and provide the potentialfor abstention-based improvements..5.5 classiﬁer cascades.
in this section, we show how conﬁdence estimationand abstention can be used for accuracy–efﬁciencytrade-offs.
we use classiﬁer cascades: we ﬁrst usea less accurate classiﬁer for prediction, abstain onexamples with low conﬁdence, then send them tomore accurate but more costly classiﬁers.
herewe choose lstm and bert-base to constitute thecascade, but one can also choose other models andmore levels of classiﬁers..we ﬁrst use an lstm for all examples’ infer-ence, and then send “difﬁcult” ones to bert-base.
since the computational cost of lstm is negligi-ble5 compared to bert-base, the key to efﬁciencyhere is correctly picking the “difﬁcult” examples.
in figure 4, we show the results of accuracy/f1score versus average flops6 per inference exam-ple.
each curve represents a method to choosedifﬁcult examples: the blue curves are obtained byrandomly selecting examples, as a simple baseline.
the orange and green curves are obtained by usingsr of lstm as the indicator of example difﬁculty;the orange curves represent the lstm trained withno regularization while the green curves are withhistory error regularization.
different points onthe curves are chosen by varying the proportion ofexamples sent to the more accurate model, bert-.
5bert-base’s cost is ∼ 105 times larger than lstm here.
6we use the torchproﬁle toolkit to measure multiply–accumulate operations (macs), and then double the numberto obtain ﬂoating point operations (flops)..104705101520gflops0.820.840.860.88f1mrpc05101520gflops0.650.700.750.800.850.90acc.qnli05101520gflops0.650.700.750.800.85acc.mnlirandomsrsr-hist.
base.
a curve with a larger area under it indicatesa better accuracy–efﬁciency trade-off..we can see that the blue curves are basicallylinear interpolations between the lstm (the lower-left dot) and bert-base (the upper-right dot), andthis is expected for random selection.
orange andgreen curves are concave, indicating that using srfor conﬁdence estimation is, unsurprisingly, moreeffective than random selection.
between thesetwo, the green curves (history error regularization)have larger areas under themselves than orangeones (no regularization), i.e., green curves have bet-ter accuracy given the same flops.
this demon-strates the effectiveness of error regularization forbetter conﬁdence estimation..6 conclusion.
in this paper, we introduce the problem of selectiveprediction for nlp.
we provide theoretical back-ground and evaluation metrics for the problem, andalso propose a simple error regularization methodthat improves selective prediction performance fornlp models.
we conduct experiments to comparedifferent models under the selective prediction set-ting, demonstrate the effectiveness of the proposedregularization trick, and study two scenarios whereselective prediction and the error regularizationmethod can be helpful..we summarize interesting experimental observa-.
tions as follows:.
1. recent sophisticated nlp models not only im-prove accuracy over simple models, but alsoprovide better selective prediction results (betterconﬁdence estimation)..2. mc-dropout, despite having a solid theoreticalfoundation, has difﬁculties matching the effec-tiveness of simple sr in practice..3. the simple error regularization helps modelslower their auc and rpp, i.e., models trainedwith it produce better conﬁdence estimators..4. selective prediction can be applied to scenarioswhere estimating example difﬁculties is neces-in these cases, our proposed error reg-sary.
ularization trick can also be helpful, such asproviding better accuracy–efﬁciency trade-offs..future work (1) despite the effectiveness ofthe proposed error regularization trick, we are notcertain on the best way for computing the error.
(current or history); it is important to unify theminto one method that consistently does well.
(2) wehave only covered a selection of nlp tasks, andthere are still other unexplored categories: token-level classiﬁcation such as named entity recogni-tion and question answering, sequence generationsuch as summarization and translation, and so on;it would be interesting to extend selective predic-tion to these problems.
(3) there exists anothersetting for selective prediction where abstentioninduces a ﬁxed cost (bartlett and wegkamp, 2008)and the goal is to minimize the overall cost insteadof auc; it would also be interesting to investigatethis setting for nlp applications..acknowledgements.
we thank anonymous reviewers for their construc-tive suggestions.
this research is supported in partby the canada first research excellence fund andthe natural sciences and engineering researchcouncil (nserc) of canada..references.
peter l. bartlett and marten h. wegkamp.
2008. clas-siﬁcation with a reject option using a hinge loss.
journal of machine learning research, 9(59)..mark carlebach, ria cheruvu, brandon walker, cesarilharco magalhaes, and sylvain jaume.
2020. newsaggregation with diverse viewpoint identiﬁcation us-ing neural embeddings and semantic understandingmodels.
in proceedings of the 7th workshop on ar-gument mining, pages 59–66, online.
associationfor computational linguistics..eunsol choi, omer levy, yejin choi, and luke zettle-moyer.
2018. ultra-ﬁne entity typing.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 87–96, melbourne, australia.
associa-tion for computational linguistics..chi-keung chow.
1957. an optimum character recog-nition system using decision functions.
ire trans-actions on electronic computers, (4)..david cohn, zoubin ghahramani, and michael jordan.
1995. active learning with statistical models.
inadvances in neural information processing systems,volume 7. mit press..shrey desai and greg durrett.
2020. calibration ofpre-trained transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 295–302, online.
association for computational linguistics..1048jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..william b. dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in proceedings of the third international workshopon paraphrasing (iwp2005)..li dong, chris quirk, and mirella lapata.
2018. conﬁ-dence modeling for neural semantic parsing.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 743–753, melbourne, australia.
as-sociation for computational linguistics..ran el-yaniv and yair wiener.
2010. on the founda-tions of noise-free selective classiﬁcation.
journalof machine learning research, 11(53)..yarin gal and zoubin ghahramani.
2016. dropout asa bayesian approximation: representing model un-in proceedings of thecertainty in deep learning.
33rd international conference on machine learn-ing, volume 48 of proceedings of machine learningresearch, pages 1050–1059, new york, new york,usa.
pmlr..yonatan geifman and ran el-yaniv.
2017..selec-tive classiﬁcation for deep neural networks.
in ad-vances in neural information processing systems,volume 30. curran associates, inc..yonatan geifman and ran el-yaniv.
2019..selec-tivenet: a deep neural network with an integratedin proceedings of the 36th inter-reject option.
national conference on machine learning, vol-ume 97 of proceedings of machine learning re-search, pages 2151–2159.
pmlr..yonatan geifman, guy uziel, and ran el-yaniv.
2019.bias-reduced uncertainty estimation for deep neuralclassiﬁers.
in international conference on learningrepresentations..charles j. geyer.
1992. practical markov chain monte.
carlo.
statistical science, pages 473–483..alex graves.
2011. practical variational inference forneural networks.
in advances in neural informationprocessing systems, volume 24. curran associates,inc..dan hendrycks and kevin gimpel.
2017. a baselinefor detecting misclassiﬁed and out-of-distributionexamples in neural networks.
in international con-ference on learning representations..geoffrey e. hinton and drew van camp.
1993. keep-ing the neural networks simple by minimizing thein proceedingsdescription length of the weights.
of the sixth annual conference on computationallearning theory, pages 5–13..sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..heinrich jiang, been kim, melody guan, and mayagupta.
2018. to trust or not to trust a classiﬁer.
in s. bengio, h. wallach, h. larochelle, k. grau-man, n. cesa-bianchi, and r. garnett, editors, ad-vances in neural information processing systems,volume 31. curran associates, inc..amita kamath, robin jia, and percy liang.
2020. se-lective question answering under domain shift.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5684–5696, online.
association for computational lin-guistics..aviral kumar, sunita sarawagi, and ujjwal jain.
2018.trainable calibration measures for neural networksin proceedingsfrom kernel mean embeddings.
of the 35th international conference on machinelearning, volume 80 of proceedings of machinelearning research, pages 2805–2814.
pmlr..balaji lakshminarayanan, alexander pritzel, andcharles blundell.
2017. simple and scalable pre-dictive uncertainty estimation using deep ensembles.
in advances in neural information processing sys-tems, volume 30, pages 6402–6413.
curran asso-ciates, inc..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin interna-learning of language representations.
tional conference on learning representations..shiyu liang, yixuan li, and r. srikant.
2018. enhanc-ing the reliability of out-of-distribution image detec-tion in neural networks.
in international conferenceon learning representations..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..chuan guo, geoff pleiss, yu sun, and kilian q. wein-berger.
2017. on calibration of modern neural net-in proceedings of the 34th internationalworks.
conference on machine learning, volume 70 ofproceedings of machine learning research, pages1321–1330.
pmlr..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,.
1049junjie bai, and soumith chintala.
2019. pytorch:an imperative style, high-performance deep learn-ing library.
in advances in neural information pro-cessing systems, volume 32. curran associates, inc..john c. platt.
1999. probabilistic outputs for supportvector machines and comparisons to regularized like-lihood methods.
in advances in large margin clas-siﬁers..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..bernhard sch¨olkopf, robert c. williamson, alexsmola, john shawe-taylor, and john platt.
2000.support vector method for novelty detection.
in ad-vances in neural information processing systems,volume 12. mit press..roy.
gabriel.
schwartz,.
stanovsky,.
swabhaswayamdipta, jesse dodge, and noah a. smith.
2020. the right tool for the job: matching modelin proceedings of theand instance complexities.
58th annual meeting of the association for com-putational linguistics, pages 6640–6651, online.
association for computational linguistics..yanyao shen, hyokun yun, zachary c. lipton,yakov kronrod,and animashree anandkumar.
2018. deep active learning for named entity recogni-tion.
in international conference on learning rep-resentations..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..mariya toneva, alessandro sordoni, remi tachet descombes, adam trischler, yoshua bengio, and geof-frey j. gordon.
2019. an empirical study of exam-ple forgetting during deep neural network learning.
in international conference on learning represen-tations..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
the 2018 emnlp workshop black-ceedings of.
boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..shuo wang, zhaopeng tu, shuming shi, and yang liu.
2020. on the inference calibration of neural ma-chine translation.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 3070–3079, online.
association forcomputational linguistics..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..ji xin, raphael tang, yaoliang yu, and jimmy lin.
2021. berxit: early exiting for bert with betterﬁne-tuning and extension to regression.
in proceed-ings of the 16th conference of the european chap-ter of the association for computational linguistics:main volume, pages 91–104, online.
association forcomputational linguistics..ji xin, raphael tang, jaejun lee, yaoliang yu, andjimmy lin.
2020. deebert: dynamic early exitingin proceedingsfor accelerating bert inference.
of the 58th annual meeting of the association forcomputational linguistics, pages 2246–2251, on-line.
association for computational linguistics..xuchao zhang, fanglan chen, chang-tien lu, andnaren ramakrishnan.
2019. mitigating uncertaintyin proceedings of thein document classiﬁcation.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 3126–3136, minneapolis, min-nesota.
association for computational linguistics..wangchunshu zhou, canwen xu, tao ge, julianmcauley, ke xu, and furu wei.
2020. bert losespatience: fast and robust inference with early exit.
in h. larochelle, m. ranzato, r. hadsell, m. f.balcan, and h. lin, editors, advances in neuralinformation processing systems, volume 33, pages18330–18341.
curran associates, inc..1050model.
conﬁdenceestimator.
sst-5acc auc rpp.
mnli-(m/mm)auc.
acc.
rpp.
lstm.
bertbase.
bertlarge.
albertbase.
srpdmc.
srpdmc.
srpdmc.
srpdmc.
37.6.
51.6.
53.3.
50.2.
624.8 10.1646.5 10.8701.2 11.9.
439.9 10.1438.9 10.2493.5 10.9.
430.5 10.4434.2 10.5474.8 11.2.
474.2 10.5481.3 10.8524.4 11.8.
65.4/64.3.
84.9/84.6.
86.4/86.0.
84.7/85.4.
6.8/6.61984.0/1990.02001.8/2013.86.9/6.73554.1/3548.1 11.7/11.7.
514.8/491.7517.8/494.0639.0/677.5.
486.1/470.0489.2/473.0482.0/510.2.
469.3/453.5473.1/456.6921.1/878.3.
2.6/2.42.6/2.43.4/3.5.
2.5/2.42.5/2.42.5/2.6.
2.3/2.32.3/2.35.0/4.7.
table 5: adding pd as conﬁdence estimation to the comparison between different conﬁdence estimators on sst-5and mnli..model.
lstmbert-basebert-largealbert-base.
curr..0.50.050.10.01.hist..0.50.050.10.05.table 6: choices for λ for different models and regular-ization methods..binary classiﬁcation, and therefore pd’s resultsare identical to softmax response (sr).
sst-5 andmnli have more than two classes, and thereforepd’s results are different from sr’s.
we show themin table 5..we can see that the results of pd are very similarto those of sr. of course, mnli and sst-5 haveonly three/ﬁve labels respectively, and for datasetswith far more labels, pd will possibly show itsdifference from sr..a detailed experiment settings.
the lstm is randomly initialized without pre-training.
for models that require pre-trained, weuse the following ones provided by the hugging-face transformer library (wolf et al., 2020)..• bert-base-uncased.
• bert-large-uncased.
• albert-base-v2.
all these models are trained/ﬁne-tuned for 3epochs without early-stopping or checkpoint se-lection.
learning rate is 2 × 10−5.
a batch size of32 is used for training/ﬁne-tuning.
the maximuminput sequence length is 128. choices for the reg-ularization hyperparameter λ from equation 9 areshown in table 6..the numbers of parameters for the two modelsbert and albert can be found in the paper bylan et al.
(2020)..the lstm used in the paper is a two-layer bi-directional lstm, with a hidden size of 200. ontop of it there is a max-pooling layer and a fully-connected layer..b pd conﬁdence estimator.
probability difference (pd), the difference betweenprobabilities of the top two classes, can also beused as conﬁdence estimation.
among the fourdatasets used in the paper, mrpc and qnli are.
1051