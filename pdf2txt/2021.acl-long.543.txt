lexical semantic change discovery.
sinan kurtyigit♠ maike park♥ dominik schlechtweg♠jonas kuhn♠ sabine schulte im walde♠.
♠institute for natural language processing, university of stuttgart♥leibniz institute for the german language, mannheimsinan.kurtyigit@gmail.com, park@ids-mannheim.de{schlecdk,jonas.kuhn,schulte}@ims.uni-stuttgart.de.
abstract.
while there is a large amount of research inthe ﬁeld of lexical semantic change detec-tion, only few approaches go beyond a stan-dard benchmark evaluation of existing models.
in this paper, we propose a shift of focus fromchange detection to change discovery, i.e., dis-covering novel word senses over time from thefull corpus vocabulary.
by heavily ﬁne-tuninga type-based and a token-based approach on re-cently published german data, we demonstratethat both models can successfully be appliedto discover new words undergoing meaningchange.
furthermore, we provide an almostfully automated framework for both evaluationand discovery..1.introduction.
there has been considerable progress in lexical se-mantic change detection (lscd) in recent years(kutuzov et al., 2018; tahmasebi et al., 2018;hengchen et al., 2021), with milestones such asthe ﬁrst approaches using neural language mod-els (kim et al., 2014; kulkarni et al., 2015), theintroduction of orthogonal procrustes alignment(kulkarni et al., 2015; hamilton et al., 2016), de-tecting sources of noise (dubossarsky et al., 2017,2019), the formulation of continuous models (fr-ermann and lapata, 2016; rosenfeld and erk,2018; tsakalidis and liakata, 2020), the ﬁrst usesof contextualized embeddings (hu et al., 2019; giu-lianelli et al., 2020), the development of solid an-notation and evaluation frameworks (schlechtweget al., 2018, 2019; shoemark et al., 2019) andshared tasks (basile et al., 2020; schlechtweg et al.,2020)..however, only a very limited amount of workapplies the methods to discover novel instances ofsemantic change and to evaluate the usefulness ofsuch discovered senses for external ﬁelds.
that is,the majority of research focuses on the introduction.
of novel lscd models, and on analyzing and evalu-ating existing models.
up to now, these preferencesfor development and analysis vs. application repre-sented a well-motivated choice, because the qualityof state-of-the-art models had not been establishedyet, and because no tuning and testing data wereavailable.
but with recent advances in evaluation(basile et al., 2020; schlechtweg et al., 2020; ku-tuzov and pivovarova, 2021), the ﬁeld now ownsstandard corpora and tuning data for different lan-guages.
furthermore, we have gained experienceregarding the interaction of model parameters andmodelling task (such as binary vs. graded semanticchange).
this enables the ﬁeld to more conﬁdentlyapply models to discover previously unknown se-mantic changes.
such discoveries may be usefulin a range of ﬁelds (hengchen et al., 2019; jatowtet al., 2021), among which historical semantics andlexicography represent obvious choices (ljubeˇsi´c,2020)..in this paper, we tune the most successful mod-els from semeval-2020 task 1 (schlechtweg et al.,2020) on the german task data set in order to obtainhigh-quality discovery predictions for novel seman-tic changes.
we validate the model predictions in astandardized human annotation procedure and visu-alize the annotations in an intuitive way supportingfurther analysis of the semantic structure relatingword usages.
in this way, we automatically detectpreviously described semantic changes and at thesame time discover novel instances of semanticchange which had not been indexed in standard his-torical dictionaries before.
our approach is largelyautomated, by relying on unsupervized languagemodels and a publicly available annotation systemrequiring only a small set of judgments from anno-tators.
we further evaluate the usability of the ap-proach from a lexicographer’s viewpoint and showhow intuitive visualizations of human-annotateddata can beneﬁt dictionary makers..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6985–6998august1–6,2021.©2021associationforcomputationallinguistics69852 related work.
state-of-the-art semantic change detection modelsare vector space models (vsms) (schlechtweget al., 2020).
these can be divided into type-based(static) (turney and pantel, 2010) and token-based(contextualized) (sch¨utze, 1998) approaches.
forour study, we use both a static and a contextualizedmodel.
as mentioned above, previous work mostlyfocuses on creating data sets or developing, evalu-ating and analyzing models.
a common approachfor evaluation is to annotate target words selectedfrom dictionaries in speciﬁc corpora (tahmasebiand risse, 2017; schlechtweg et al., 2018; perroneet al., 2019; basile et al., 2020; rodina and kutu-zov, 2020; schlechtweg et al., 2020).
contrary tothis, our goal is to ﬁnd ‘undiscovered’ changingwords and validate the predictions of our modelsby human annotators.
few studies focus on thistask.
kim et al.
(2014), hamilton et al.
(2016),basile et al.
(2016), basile and mcgillivray (2018),takamura et al.
(2017) and tsakalidis et al.
(2019)evaluate their approaches by validating the topranked words through author intuitions or knownhistorical data.
the only approaches applying asystematic annotation process are gulordava andbaroni (2011) and cook et al.
(2013).
gulordavaand baroni ask human annotators to rate 100 ran-domly sampled words on a 4-point scale from 0(no change) to 3 (changed signiﬁcantly), howeverwithout relating this to a data set.
cook et al.
workclosely with a professional lexicographer to inspect20 lemmas predicted by their models plus 10 ran-domly selected ones.
gulordava and baroni andcook et al.
evaluate their predictions on the (macro)lemma level.
we, however, annotate our predic-tions on the (micro) usage level, enabling us tobetter control the criteria for annotation and theirinter-subjectivity.
in this way, we are also able tobuild clusters of usages with the same sense and tovisualise the annotated data in an intuitive way.
theannotation process is designed to not only improvethe quality of the annotations, but also lessen theburden on the annotators.
we additionally seek theopinion of a professional lexicographer to assessthe usefulness of the predictions outside the ﬁeldof lscd..in contrast to previous work, we obtain modelpredictions by ﬁne-tuning static and contextualizedembeddings on high-quality data sets (schlechtweget al., 2020) that were not available before.
weprovide a highly automated general framework for.
evaluating models and predicting changing wordson all kinds of corpora..3 data.
we use the german data set provided by thesemeval-2020 shared task (schlechtweg et al.,2020, 2021).
the data set contains a diachroniccorpus pair for two time periods to be compared,a set of carefully selected target words as well asbinary and graded gold data for semantic changeevaluation and ﬁne-tuning purposes..dta.
corpus.
corpora the(deutschestextarchiv, 2017) and a combination of thebz (berliner zeitung, 2018) and nd (neuesdeutschland, 2018) corpora are used.
dtacontains texts from different genres spanning the16th–20th centuries.
bz and nd are newspapercorpora jointly spanning 1945–1993.
schlechtweget al.
(2020) extract two time speciﬁc corpora c1(dta, 1800–1899) and c2 (bz+nd 1946–1990)and provide raw and lemmatized versions..target words a list of 48 target words, consist-ing of 32 nouns, 14 verbs and 2 adjectives is pro-vided.
these are controlled for word frequencyto minimize model biases that may lead to artiﬁ-cially high performance (dubossarsky et al., 2017;schlechtweg and schulte im walde, 2020)..4 models.
type-based models generate a single vector foreach word from a pre-deﬁned vocabulary.
in con-trast, token-based models generate one vector foreach usage of a word.
while the former do not takeinto account that most words have multiple senses,the latter are able to capture this particular aspectand are thus presumably more suited for the task oflscd (martinc et al., 2020).
even though contex-tualized approaches have indeed signiﬁcantly out-performed static approaches in several nlp tasksover the past years (ethayarajh, 2019), the ﬁeldof lscd is still dominated by type-based models(schlechtweg et al., 2020).
kutuzov and giulianelli(2020) yet show that the performance of token-based models (especially elmo) can be increasedby ﬁne-tuning on the target corpora.
laicher et al.
(2020, 2021) drastically improve the performanceof bert by reducing the inﬂuence of target wordmorphology.
in this paper, we compare both fami-lies of approaches for change discovery..69864.1 type-based approach.
most type-based approaches in lscd combinethree sub-systems: (i) creating semantic word rep-resentations, (ii) aligning them across corpora, and(iii) measuring differences between the alignedrepresentations (schlechtweg et al., 2019).
mo-tivated by its wide usage and high performanceamong participants in semeval-2020 (schlechtweget al., 2020) and diacr-ita (basile et al., 2020),we use the skip-gram with negative samplingmodel (sgns, mikolov et al., 2013a,b) to cre-ate static word embeddings.
sgns is a shallowneural language model trained on pairs of wordco-occurrences extracted from a corpus with a sym-metric window.
the optimized parameters can beinterpreted as a semantic vector space that con-tains the word vectors for all words in the vocabu-lary.
in our case, we obtain two separately trainedvector spaces, one for each subcorpus (c1 andc2).
following standard practice, both spaces arelength-normalized, mean-centered (artetxe et al.,2016; schlechtweg et al., 2019) and then alignedby applying orthogonal procrustes (op), becausecolumns from different vector spaces may not cor-respond to the same coordinate axes (hamiltonet al., 2016).
the change between two time-speciﬁcembeddings is measured by calculating their co-sine distance (cd) (salton and mcgill, 1983).
thestrength of sgns+op+cd has been shown in tworecent shared tasks with this sub-system combina-tion ranking among the best submissions (arefyevand zhikov, 2020; kaiser et al., 2020b; p¨omsl andlyapin, 2020; praˇz´ak et al., 2020)..4.2 token-based approach.
bidirectional encoder representations from trans-formers (bert, devlin et al., 2019)is atransformer-based neural language model designedto ﬁnd contextualized representations for text byanalyzing left and right contexts.
the base versionprocesses text in 12 different layers.
in each layer,a contextualized token vector representation is cre-ated for every word.
a layer, or a combination ofmultiple layers (we use the average), then servesas a representation for a token.
for every targetword we extract usages (i.e., sentences in whichthe word appears) by randomly sub-sampling up to100 sentences from both subcorpora c1 and c2.1these are then fed into bert to create contex-.
1we sub-sample as some words appear in 10,000 or more.
sentences..tualized embeddings, resulting in two sets of upto 100 contextualized vectors for both time peri-ods.
to measure the change between these sets weuse two different approaches: (i) we calculate theaverage pairwise distance (apd).
the idea is torandomly pick a number of vectors from both setsand measure their mutual distances (schlechtweget al., 2018; kutuzov and giulianelli, 2020).
thechange score corresponds to the mean average dis-tance of all comparisons.
(ii) we average bothvector sets and measure the cosine distance (cos)between the two resulting mean vectors (kutuzovand giulianelli, 2020)..5 discovery.
semeval-2020 task 1 consists of two subtasks:(i) binary classiﬁcation: for a set of target words,decide whether (or not) the words lost or gainedsense(s) between c1 and c2, and (ii) graded rank-ing: rank a set of target words according to theirdegree of lsc between c1 and c2.
these requireto detect semantic change in a small pre-selectedset of target words.
instead, we are interested in thediscovery of changing words from the full vocab-ulary of the corpus.
we deﬁne the task of lexicalsemantic change discovery as follows..given a diachronic corpus pair c1 and c2, de-cide for the intersection of their vocabularieswhich words lost or gained sense(s) betweenc1 and c2..this task can also be seen as a special case of se-meval’s subtask 1 where the target words equalthe intersection of the corpus vocabularies.
note,however, that discovery introduces additional difﬁ-culties for models, e.g.
because a large number ofpredictions is required and the target words are notpreselected, balanced or cleaned.
yet, discovery isan important task, with applications such as lexi-cography where dictionary makers aim to cover thefull vocabulary of a language..5.1 approach.
we start the discovery process by generatingoptimized graded value predictions using high-performing parameter conﬁgurations following pre-vious work and ﬁne-tuning.
afterwards, we inferbinary scores with a thresholding technique (seebelow).
we then tune the threshold to ﬁnd thebest-performing type- and token-based approach.
6987(cid:120).
4: identical3: closely related2: distantly related1: unrelated.
table 1: durel relatedness scale (schlechtweg et al.,2018)..for binary classiﬁcation.
these are used to generatetwo sets of predictions.2.
evaluation metrics we evaluate the gradedrankings in subtask 2 by computing spearman’srank-order correlation coefﬁcient ρ. for the binaryclassiﬁcation subtask we compute precision, recalland f0.5.
the latter puts a stronger focus on pre-cision than recall because our human evaluationcannot be automated, so we decided to weigh qual-ity (precision) higher than quantity (recall)..parameter tuning solving subtask 2 is straight-forward, since both the type-based and token-basedapproaches output distances between representa-tions for c1 and c2 for every target word.
likemany approaches in semeval-2020 task 1 anddiacr-ita we use thresholding to binarise thesevalues.
the idea is to deﬁne a threshold parame-ter, where all ranked words with a distance greateror equal to this threshold are labeled as changingwords..for cases where no tuning data is available,kaiser et al.
(2020b) propose to choose the thresh-old according to the population of cds of all wordsin the corpus.
kaiser et al.
set the threshold toµ + σ, where µ is the mean and σ is the standarddeviation of the population.
we slightly modify thisapproach by changing the threshold to µ + t ∗ σ.in this way, we introduce an additional parameter t,which we tune on the semeval-2020 test data.
wetest different values ranging from −2 to 2 in stepsof 0.1..population since sgns generates type-basedvectors for every word in the vocabulary, measuringthe distances for the full vocabulary comes with lowadditional computational effort.
unfortunately, thisis much more difﬁcult for bert.
creating up to 100vectors for every word in the vocabulary drasticallyincreases the computational burden.
we choose apopulation of 500 words for our work allowing us.
2find the code used for each step of.
the pre-diction process at https://github.com/seinan9/lscdiscovery..to test multiple parameter conﬁgurations.3 we sam-ple words from different frequency areas to havepredictions not only for low-frequency words.
forthis, we ﬁrst compute the frequency range (highestfrequency – lowest frequency) of the vocabulary.
this range is then split into 5 areas of equal fre-quency width.
random samples from these areasare taken based on how many words they contain.
if the lowest frequency area con-for example:tains 50% of all words from the vocabulary, then0.5 ∗ 500 = 250 random samples are taken fromthis area.
the semeval-2020 target words are ex-cluded from this sampling process.
the resultingpopulation is used to create predictions for bothmodels..filtering the predictions contain proper names,foreign language and lemmatization errors, whichwe aim to ﬁlter out, as such cases are usually notconsidered as semantic changes.
we only allownouns, verbs and adjectives to pass.
words whereover 10% of the usages are either non-german orcontain more than 25% punctuation are ﬁltered outas well..6 annotation.
the model predictions are validated by human an-notation.
for this, we apply the semeval-2020task 1 procedure, as described in schlechtweg et al.
(2020).
annotators are asked to judge the semanticrelatedness of pairs of word usages, such as the twousages of aufkommen in (1) and (2), on the scalein table 1..(1) es ist richtig, dass mit dem aufkommen dermanufaktur im unterschied zum handwerksich spuren der kinderexploitation zeigen.
‘it is true that with the emergence of themanufactory, in contrast to the handicraft,traces of child labor are showing.’.
(2) sie wissen, daß wir f¨ur das vieh mehr futter.
aus eigenem aufkommen brauchen.
‘they know that we need more feed from ourown production for the cattle.’.
the annotated data of a word is represented in aword usage graph (wug), where vertices repre-sent word usages, and weights on edges represent.
3in a practical setting where predictions have to be gener-ated only once, a much larger number may be chosen.
also,possibilities to scale up bert performance can be applied(montariol et al., 2021)..6988full.
c1.
c2.
figure 1: word usage graph of german aufkommen (left), subgraphs for ﬁrst time period c1 (middle) and forsecond time period c2 (right).
black/gray lines indicate high/low edge weights..the (median) semantic relatedness judgment of apair of usages such as (1) and (2).
the ﬁnal wugsare clustered with a variation of correlation cluster-ing (bansal et al., 2004; schlechtweg et al., 2020)(see figure 1, left) and split into two subgraphsrepresenting nodes from subcorpora c1 and c2,respectively (middle and right).
clusters are theninterpreted as word senses and changes in clustersover time as lexical semantic change..in contrast to schlechtweg et al.
we use theopenly available durel interface for annotationand visualization.4 this also implies a change insampling procedure, as the system currently imple-ments only random sampling of use pairs (withoutsemeval-style optimization).
for each target wordwe sample |u1| = |u2| = 25 usages (sentences)per subcorpus (c1, c2) and upload these to thedurel system, which presents use pairs to annota-tors in randomized order.
we recruit eight germannative speakers with university level education asannotators.
five have a background in linguistics,two in german studies, and one has an additionalprofessional background in lexicography.
similarto schlechtweg et al., we ensure the robustnessof the obtained clusterings by continuing the an-notation of a target word until all multi-clusters(clusters with more than one usage) in its wugare connected by at least one judgment.
we ﬁ-nally label a target word as changed (binary) if itgained or lost a cluster over time.
for instance,aufkommen in figure 1 is labeled as change as itgains the orange cluster from c1 to c2.
follow-ing schlechtweg et al.
(2020) we use k and n aslower frequency thresholds to avoid that small ran-dom ﬂuctuations in sense frequencies caused bysampling variability or annotation error be misclas-.
4https://www.ims.uni-stuttgart.de/.
data/durel-tool..siﬁed as change.
as proposed in schlechtweg andschulte im walde (submitted) for comparabilityacross sample sizes we set k = 1 ≤ 0.01 ∗ |ui| ≤ 3and n = 3 ≤ 0.1 ∗ |ui| ≤ 5, where |ui| is thenumber of usages from the respective time period(after removing incomprehensible usages from thegraphs).
this results in k = 1 and n = 3 for alltarget words..find an overview over the ﬁnal set of wugsin table 2. we reach a comparably high inter-annotator agreement (krippendorf’s α = .58).5.we now describe the results of the tuning and dis-covery procedures..7 results.
7.1 tuning.
sgns is commonly used (schlechtweg et al., 2020)and also highly optimized (kaiser et al., 2020a,b,2021), so it is difﬁcult to further increase the per-formance.
we thus rely on the work of kaiser et al.
(2020a) and test their parameter conﬁgurations onthe german semeval-2020 data set.6 we obtainthree slightly different parameter conﬁgurations(see table 3 for more details), yielding competitiveρ = .690, ρ = .710 and ρ = .710, respectively..in order.
to improve the performance ofbert, we test different layer combinations, pre-processings and semantic change measures.
fol-lowing laicher et al.
(2020, 2021), we are ableto drastically increase the performance of bert.
5we provide wugs as python networkx graphs, de-scriptive statistics, inferred clusterings, change values andinteractive visualizations for all target words and the respec-tive code at https://www.ims.uni-stuttgart.de/data/wugs..6all conﬁgurations use w = 10, d = 300, e = 5 and a.minimum frequency count of 39..6989data set n n/v/a |u| an jud av spr kri unc loss lscb lscg.
semeval 48 32/14/2 178 88predictions 75 39/16/20 49.
37k24k.
21.
.59.64.
.53.58.
00.
.12.26.
.35.48.
.31.40.table 2: overview target words.
n = no.
of target words, n/v/a = no.
of nouns/verbs/adjectives, |u | = avg.
no.
ofusages per word, an = no.
of annotators, jud = total no.
of judged usage pairs, av = avg.
no.
of judgments perusage pair, spr = weighted mean of pairwise spearman, kri = krippendorff’s α, unc = avg.
no.
of uncomparedmulti-cluster combinations, loss = avg.
of normalized clustering loss * 10, lscb/g = mean binary/gradedchange score..on the german semeval-2020 data.
in a pre-processing step, we replace the target word in ev-ery usage by its lemma.
in combination with layer12+1, both apd and cos perform competitivelywell on subtask 2 (ρ = .690 and ρ = .738)..after applying thresholding as described in sec-tion 5 we obtain f0.5-scores for a large range ofthresholds.
sgns achieves peak f0.5-scores of.692, .738 and .685, respectively (see table 3).
in-terestingly, the optimal threshold is at t = 1.0 inall three cases.
this corresponds to the thresh-old used in kaiser et al.
(2020b).
while the peakf0.5 of bert+apd is marginally worse (.598 att = −0.2), bert+cos is able to outperform thebest sgns conﬁguration with a peak of .741 att = 0.1..in order to obtain an estimate on the samplingvariability that is caused by sampling only up to 100usages per word for bert+apd and bert+cos(see section 4.2), we repeat the whole procedure9 times and estimate mean and standard deviationof performance on the tuning data.
in the begin-ning of every run the usages are randomly sampledfrom the corpora.
we observe a mean ρ of .657for bert+apd and .743 for bert+cos with astandard deviation of .015 and .012, respectively,as well as a mean f0.5 of .576 for bert+apd and.684 for bert+cos with a standard deviation of.013 and .038, respectively.
this shows that thevariability caused by sub-sampling word usages isnegligible..7.2 discovery.
we use the top-performing conﬁgurations (see ta-ble 3) to generate two sets of large-scale predic-tions.
while we use the lemmatized corpora forsgns, in bert’s case we choose the raw corporawith lemmatized target words instead.
the latterchoice is motivated by the previously described per-formance increases.
after the ﬁltering as describedin section 6, we obtain 27 and 75 words labeled.
as changing, respectively.
we further sample 30targets from the second set of predictions to ob-tain a feasible number for annotation.
we call theﬁrst set sgns targets and the second one berttargets, with an overlap of 7 targets.
additionally,we randomly sample 30 words from the population(with an overlap of 5 with the sgns and berttargets) in order to have an indication of what thechange distribution underlying the corpora is.
wecall these baseline (bl) targets.
this baseline willhelp us to put the results of the predictions in con-text and to ﬁnd out whether the predictions of thetwo models can be explained by pure randomness.
following the annotation process, binary gold datais generated for all three target sets, in order tovalidate the quality of the predictions..the evaluation of the predictions is presentedin table 3. we achieve a f0.5-score of .714 forsgns and .620 for bert.
out of the 27 wordspredicted by the sgns model, 18 (67 %) were ac-tually labeled as changing words by the humanannotators.
in comparison, only 17 out of the30 (57 %) bert predictions were annotated assuch.
the performance of sgns for prediction(sgns targets) is even higher than on the tuningdata (semeval targets).
in contrast, bert’s perfor-mance for prediction drops strongly in comparisonto the performance on the tuning data (.741 vs..620).
this reproduces previous results and con-ﬁrms that (off-the-shelf) bert generalises poorlyfor lscd and does not transfer well between datasets (laicher et al., 2020).
if we compare theseresults to the baseline, we can see that both mod-els perform much better than the random baseline(f0.5 of .349).
only 10 out of the 30 (30 %) ran-domly sampled words are annotated as changing.
this indicates, that the performance of sgns andbert is likely not a cause of randomness.
bothmodels considerably increase the chance of ﬁndingchanging words compared to a random model..figure 2 shows the detailed f0.5 developments.
6990parameters.
s k = 1, s = .005nk = 5, s = .001gs.k = 5, s = none.
treb.apd.
cos.l random samplingb.r.ρ.predictionspf0.5.
r..295.
.714 .667.
1.0.t.1.01.01.0.
−0.20.1.ρ.
.690.710.710.
.673.738.tuningp.f0.5.
.692.750.738 .818.714.685.
.598.560.741 .706.
.529.529.588.
.824.788.
.482.
.620.
.567.
.349.
.300.
1.0.
1.0.table 3: performance (spearman ρ, f0.5-measure, precision p and recall r) of different approaches on tuning data(semeval targets) and performance of best type- and token-based approach on respective predictions with optimaltuning threshold t, as well as the performance of a randomly sampled baseline..across different thresholds on the semeval targetsand the predicted words.
increasing the thresholdon the predicted words improves the f0.5 for boththe type-based and token-based approach.
a newhigh-score of .783 at t = 1.3 is achievable forsgns.
while bert’s performance also increasesto a peak of .714 at t = 1.0, it is still lower than inthe tuning phase..7.3 analysis.
for further insights into sources of errors, we takea close look at the false positives, their wugsand the underlying usages.
most of the wrongpredictions can be grouped into one out of twoerror sources (cf.
kutuzov, 2020, pp.
175–182)..context change the ﬁrst category includeswords where the context in the usages shifts be-tween time periods, while the meaning stays thesame.
the wug of angriffswaffe (‘offensiveweapon’) (see figure 5 in appendix a) shows a sin-gle cluster for both c1 and c2.
in the ﬁrst time pe-riod angriffswaffe is used to refer to a hand weapon(such as ‘sword’, ‘spear’).
in the second period,however, the context changes to nuclear weaponry.
we can see a clear contextual shift, while the mean-ing did not change.
in this case both models aretricked by the change of context.
further false posi-tives in this category are the sgns targets ¨achtung(‘ostracism’) and aussterben (‘to die out’) and thecos targets k¨onigreich (‘kingdom’) and waffen-ruhe (‘ceaseﬁre’)..context variety words that can be used in alarge variety of contexts form the second group offalse positives.
sgns falsely predicts neunj¨ahrigas a changing word.
we take a closer look at itswug (see figure 6 in appendix a).
we observe.
that there is only one and the same cluster in bothtime periods, and the meaning of the target does notchange, even though a large variety of contexts ex-ists in both c1 and c2.
for example: ‘which bearsoats at nine years fertilization’, ‘courageously, anine-year-old spaniard did something’ and ‘afternine years of work’.
both models are misguidedby this large context variety.
examples includethe sgns targets neunj¨ahrig (‘9-year-old’) andvorj¨ahrig (‘of the previous year’) and the cos tar-gets bemerken (‘to notice’) and durchdenken (‘tothink through’)..8 lexicographical evaluation.
we now evaluate the usefulness of the proposedsemantic change discovery procedure including theannotation system and wug visualization from alexicographer’s viewpoint.
the advantage of ourapproach lies in providing lexicographers and dic-tionary makers the choice to take a look into pre-dictions they consider promising with respect totheir research objective (disambiguation of wordsenses, detection of novel senses, detection of ar-chaisms, describing senses in regard to speciﬁcdiscourses etc.)
and the type of dictionary.
visual-ized predictions for target words may be analyzedin regard to single senses, clusters of senses, thesemantic proximity of sense clusters and a stylizedrepresentation of frequency.
random samplingof usages also offers the opportunity to judge un-derrepresented senses in a sample that might beinfrequent in a corpus or during a speciﬁc periodof time (although currently a high number of over-all annotations would be required in order to doso).
most importantly, the use of a variable numberof human annotators has the potential to ensure amore objective analysis of large amounts of corpus.
6991figure 2: f0.5 performance on semeval targets (orange) and respective predictions (green) across different thresh-olds.
left: sgns.
right: bert+cos.
gray vertical line indicates optimal performance on semeval targets..data.
in order to evaluate the potential of the ap-proach for assisting lexicographers with extendingdictionaries, we analyze statistical measures andpredictions of the models provided for the two setsof predictions (sgns, bert) and compare themto existing dictionary contents..we consider overall inter-annotator agreement(α >= .5) and annotated binary change label toselect 21 target words for lexicographical analy-sis.
in this way, we exclude unclear cases andnon-changing words.
the target words are ana-lyzed by inspecting cluster visualizations of wugs(such as in figure 1) and comparing them to entriesin general and specialized dictionaries in order todetermine:.
• whether a candidate novel sense is alreadyincluded in one of the reference dictionaries,.
• whether a candidate novel sense is included inone of the two reference dictionaries that areconsulted for c1 (covering the period between1800–1899) and c2 (covering the period be-tween 1946–1990), indicating the rise of anovel sense, the archaization of older sensesor a change in frequency..three dictionaries are consulted throughout theanalysis: (i) the dictionary of the german language(dwb) by jacob und wilhelm grimm (digitizedversion of the 1st print published between 1854–1961), (ii) the dictionary of contemporary ger-man (wgd), published between 1964–1977, nowcurated and digitized by the dwds and (iii) theduden online dictionary of german language (du-den), reﬂecting usage of contemporary german.
up until today.7 additionally, lemma entries in thewiktionary online dictionary (wiktionary) are con-sulted to verify genuinely novel senses describedin section 8.1..8.1 records of novel senses.
in the case of 17 target words, all senses identiﬁedby the system are included in at least one of thethree dictionaries consulted for the analysis.
inthe four remaining cases, at least one novel senseof a word is neither paraphrased nor given as anexample of semantically related senses in the dic-tionaries:.
einbinden reference to the integration or embed-ding of details on a topic, event, person in respect toa chronological order within written text or visualpresentation (e.g.
for an exhibition on an author)is judged as a novel sense in close semantic prox-imity to the old sense ‘to bind sth.
into sth.’, e.g.
ﬂowers into a bundle of ﬂowers.
einbinden is alsoused in technical contexts, meaning ‘to (physically)implement parts of a construction or machine intotheir intended slots’..niederschlagen in cases where the verb nieder-schlagen co-occurs with the verb particle auf andthe noun fl¨ugel, the verb refers to a bird’s action ofrepeatedly moving its wings up and down in orderto ﬂy..regelrecht used as an adverb, regelrecht may re-fer to something being the usual outcome that ought.
7only the fully-digitized version of the dwb’s ﬁrst printwas consulted for this evaluation, since a revised version hasnot been completed yet and is only available for lemmas start-ing with letters a–f..6992to be expected due to scientiﬁc principles, with anemphasis on the actual result of an action (such asthe dyeing of ﬁber of a piece of clothing followingthe bleaching process), whereas senses included indictionaries for general language emphasize eitherthe intended accordance with a rule or somethingusually happening (the latter being colloquial use)..zehner(see figure 3 in appendix a) themeaning ‘a winning sequence of numbers in thenational lottery’, predicted to have risen as a novelsense between c1 and c2, is not included in any ofthe reference dictionaries..in most of these cases, senses identiﬁed as novelreﬂect metaphoric use, indicating that deﬁnitionsin existing dictionary entries may need to be broad-ened, or example sentences would have to be added.
some of the senses described in this section mightbe included in specialized dictionaries, e.g.
techni-cal usage of einbinden..8.2 records of changes.
for 12 target words, semantic change predictedby the models (innovative, reductive or a salientchange of frequency of a sense) correlates withthe addition or non-inclusion of senses in dictio-nary entries consulted for the respective period oftime (dwb for c1, wgd for c2).
it should benoted though, that lemma lists of the two dictionar-ies might be lacking lemmas in the headword list,and lemma entries might be lacking paraphrasesor examples of senses of the lemma, simply be-cause corpus-based lexicography was not availableat the time of their ﬁrst print and revisions of thedictionaries are currently work in progress..additionally, we consult a dictionary for earlynew high german (fhd) in order to checkwhether discovered novel senses existed at an ear-lier stage and may be discovered due to low fre-quency or sampling error.
in two cases, discoverednovel senses that are not included in the dwb (forc1) are found to be included in the fhd..interestingly, one sense paraphrased for ausru-fung (‘a loud wording, a shout’) is included inneither of the two dictionaries consulted to judgesenses from c1 and c2, but in the fhd (earlier)and duden (as of now).
these ﬁndings suggestthat it might be reasonable to use more than tworeference corpora.
this would also alleviate thecorpus bias stemming from idiosyncratic data sam-pling procedures..9 conclusion.
we used two state-of-the-art approaches to lscdetection in combination with a recently publishedhigh-quality data set to automatically discover se-mantic changes in a german diachronic corpuspair.
while both approaches were able to discovervarious semantic changes with above-random prob-ability, some of them previously undescribed inetymological dictionaries, the type-based approachshowed a clearly better performance..we validated model predictions by an opti-mized human annotation process yielding highinter-annotator agreement and providing conve-nient ways of visualization.
in addition, we eval-uated the full discovery process from a lexicogra-pher’s point of view and conclude that we obtainedhigh-quality predictions, useful visualizations andpreviously unreported changes.
on the other hand,we discovered some issues with respect to the re-liability of predictions for semantic change andnumber and composition of reference corpora thatare going to be dealt with in the future.
the resultsof the analyses endorse that our approach might aidlexicographers with extending and altering existingdictionary entries..acknowledgments.
we thank the three reviewers for their insightfulfeedback and pedro gonz´alez bascoy for setting upthe durel annotation tool.
dominik schlechtwegwas supported by the konrad adenauer foundationand the creta center funded by the german min-istry for education and research (bmbf) duringthe conduct of this study..references.
deutsches w¨orterbuch von jacob grimm und wil-helm grimm, digitalized edition curated by thew¨orterbuchnetz at the trier center for digital hu-manities.
https://www.woerterbuchnetz.de/dwb.
accessed: 2021-01-07..fr¨uhneuhochdeutsches w¨orterbuch.
fwb-online.de.
accessed: 2021-01-07..https://.
w¨orterbuch der deutschen gegenwartssprache 1964–1977, curated and provided by the digital dictionaryof german language.
https://www.dwds.de/d/wb-wdg.
accessed: 2021-01-07..nikolay arefyev and vasily zhikov.
2020. bos atsemeval-2020 task 1: word sense induction vialexical substitution for lexical semantic change.
6993in proceedings of the 14th interna-detection.
tional workshop on semantic evaluation, barcelona,spain.
association for computational linguistics..mikel artetxe, gorka labaka, and eneko agirre.
2016.learning principled bilingual mappings of word em-beddings while preserving monolingual invariance.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages2289–2294, austin, texas.
association for compu-tational linguistics..nikhil bansal, avrim blum, and shuchi chawla.
2004.correlation clustering.
machine learning, 56(1-3):89–113..pierpaolo basile, annalina caputo, tommaso caselli,pierluigi cassotti, and rossella varvara.
2020.overview of the evalita 2020 diachronic lexi-cal semantics (diacr-ita) task.
in proceedings ofthe 7th evaluation campaign of natural languageprocessing and speech tools for italian (evalita2020), online.
ceur.org..pierpaolo basile, annalina caputo, roberta luisi, andgiovanni semeraro.
2016. diachronic analysisof the italian language exploiting google ngram,pages 56–60..pierpaolo basile and barbara mcgillivray.
2018. ex-ploiting the web for semantic change detection,pages 194–208..berliner zeitung.
diachronic newspaper corpus pub-lished by staatsbibliothek zu berlin [online].
2018..paul cook, jey han lau, michael rundell, diana mc-carthy, and timothy baldwin.
2013. a lexico-graphic appraisal of an automatic approach for de-in proceedings of elextecting new word senses.
2013, pages 49–65..deutsches textarchiv.
grundlage f¨ur ein referenzkor-pus der neuhochdeutschen sprache.
herausgegebenvon der berlin-brandenburgischen akademie derwissenschaften [online].
2017..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..haim dubossarsky, simon hengchen, nina tahmasebi,and dominik schlechtweg.
2019. time-out: tem-poral referencing for robust modeling of lexicalin proceedings of the 57th an-semantic change.
nual meeting of the association for computationallinguistics, pages 457–470, florence, italy.
associ-ation for computational linguistics..haim dubossarsky, daphna weinshall, and eitangrossman.
2017. outta control: laws of semanticchange and inherent biases in word representationmodels.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 1147–1156, copenhagen, denmark..duden.
duden online.
www.duden.de.
accessed:.
2021-02-01..dwds.
digitales w¨orterbuch der deutschen sprache.
das wortauskunftssystem zur deutschen sprachein geschichte und gegenwart, hrsg.
v. d. berlin-brandenburgischen akademie der wissenschaften.
https://www.dwds.de/.
accessed: 02.02.2021..kawin ethayarajh.
2019. how contextual are contex-tualized word representations?
comparing the geom-etry of bert, elmo, and gpt-2 embeddings.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 55–65,hong kong, china.
association for computationallinguistics..lea frermann and mirella lapata.
2016. a bayesianmodel of diachronic meaning change.
transactionsof the association for computational linguistics,4:31–45..mario giulianelli, marco del tredici, and raquelfern´andez.
2020.analysing lexical semanticchange with contextualised word representations.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3960–3973, online.
association for computational lin-guistics..kristina gulordava and marco baroni.
2011. a dis-tributional similarity approach to the detection ofsemantic change in the google books ngram cor-pus.
in proceedings of the workshop on geometri-cal models of natural language semantics, pages67–71, stroudsburg, pa, usa..william l. hamilton, jure leskovec, and dan jurafsky.
2016. diachronic word embeddings reveal statisti-cal laws of semantic change.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1489–1501, berlin, germany..simon hengchen, ruben ros, and jani marjanen.
2019.a data-driven approach to the changing vocabu-lary of the ’nation’ in english, dutch, swedish andin proceedingsfinnish newspapers, 1750-1950.of the digital humanities (dh) conference 2019,utrecht, the netherlands..simon hengchen, nina tahmasebi, dominikschlechtweg,2021.challenges for computational lexical semanticchange.
in nina tahmasebi, lars borin, adamjatowt, yang xu, and simon hengchen, editors,.
and haim dubossarsky..6994computational approaches to semantic change,volume language variation, chapter 11. languagescience press, berlin..renfen hu, shen li, and shichen liang.
2019. di-achronic sense modeling with deep contextualizedword embeddings: an ecological view.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3899–3908,florence, italy.
association for computational lin-guistics..adam jatowt, nina tahmasebi, and lars borin.
2021. computational approaches to lexical seman-tic change: visualization systems and novel appli-in nina tahmasebi, lars borin, adamcations.
jatowt, yang xu, and simon hengchen, editors,computational approaches to semantic change,language variation, chapter 10. language sciencepress, berlin..jens kaiser, sinan kurtyigit, serge kotchourko, anddominik schlechtweg.
2021. effects of pre- andpost-processing on type-based embeddings in lexi-cal semantic change detection.
in proceedings ofthe 16th conference of the european chapter of theassociation for computational linguistics, online.
association for computational linguistics..jens kaiser, dominik schlechtweg, sean papay, andsabine schulte im walde.
2020a.
ims at semeval-2020 task 1: how low can you go?
dimensionalityin lexical semantic change detection.
in proceed-ings of the 14th international workshop on semanticevaluation, barcelona, spain.
association for com-putational linguistics..jens kaiser, dominik schlechtweg, and sabine schulteim walde.
2020b.
op-ims @ diacr-ita: backto the roots: sgns+op+cd still rocks semanticin proceedings of the 7th eval-change detection.
uation campaign of natural language processingand speech tools for italian (evalita 2020), on-line.
ceur.org.
winning submission!.
yoon kim, yi-i chiu, kentaro hanaki, darshanhegde, and slav petrov.
2014. temporal analysisof language through neural language models.
inltcss@acl, pages 61–65.
association for compu-tational linguistics..vivek kulkarni, rami al-rfou, bryan perozzi, andsteven skiena.
2015. statistically signiﬁcant de-tection of linguistic change.
in proceedings of the24th international conference on world wide web,www, pages 625–635, florence, italy..andrey kutuzov.
2020. distributional word embed-.
dings in modeling diachronic semantic change..andrey kutuzov and mario giulianelli.
2020. uio-uva at semeval-2020 task 1: contextualised em-beddings for lexical semantic change detection.
in proceedings of the 14th international workshopon semantic evaluation, barcelona, spain.
associa-tion for computational linguistics..andrey kutuzov, lilja øvrelid, terrence szymanski,and erik velldal.
2018. diachronic word embed-dings and semantic shifts: a survey.
in proceedingsof the 27th international conference on computa-tional linguistics, pages 1384–1397, santa fe, newmexico, usa.
association for computational lin-guistics..andrey kutuzov and lidia pivovarova.
2021. rushifte-val: a shared task on semantic shift detection for rus-sian.
komp’yuternaya lingvistika i intellektual’nyetekhnologii: dialog conference..severin laicher, gioia baldissin, enrique castaneda,dominik schlechtweg, and sabine schulte imwalde.
2020. cl-ims @ diacr-ita: volente onolente: bert does not outperform sgns on se-mantic change detection.
in proceedings of the 7thevaluation campaign of natural language process-ing and speech tools for italian (evalita 2020),online.
ceur.org..severin.
laicher,.
sinan kurtyigit,.
dominikschlechtweg,jonas kuhn, and sabine schulteim walde.
2021. explaining and improving bertperformance on lexical semantic change de-in proceedings of the student researchtection.
workshop at the 16th conference of the europeanchapter ofthe association for computationallinguistics, online.
association for computationallinguistics..nikola ljubeˇsi´c.
2020.
“deep lexicography” – fad oropportunity?
“duboka leksikograﬁja” – pomodnostili prilika?
rasprave instituta za hrvatski jezik ijezikoslovlje, 46:839–852..matej martinc, syrielle montariol, elaine zosa, andlidia pivovarova.
2020. capturing evolution inword usage: just add more clusters?
in companionproceedings of the web conference 2020, www’20, pages 343—-349, new york, ny, usa.
asso-ciation for computing machinery..tomas mikolov, kai chen, greg corrado, and jeffreydean.
2013a.
efﬁcient estimation of word represen-in 1st international con-tations in vector space.
ference on learning representations, iclr 2013,scottsdale, arizona, usa, may 2-4, 2013, workshoptrack proceedings..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013b.
distributed repre-sentations of words and phrases and their composi-tionality.
in proceedings of nips..syrielle montariol, matej martinc, and lidia pivo-varova.
2021. scalable and interpretable semanticchange detection.
in 2021 annual conference of thenorth american chapter of the association for com-putational linguistics..neues deutschland.
diachronic newspaper corpus pub-lished by staatsbibliothek zu berlin [online].
2018..6995valerio perrone, marco palma, simon hengchen,jim q. smith, and barbaraalessandro vatri,mcgillivray.
2019. gasc: genre-aware semanticin proceedings of thechange for ancient greek.
1st international workshop on computational ap-proaches to historical language change, pages 56–66, florence, italy.
association for computationallinguistics..martin p¨omsl and roman lyapin.
2020. circe atsemeval-2020 task 1: ensembling context-freeand context-dependent word representations.
inproceedings of the 14th international workshop onsemantic evaluation, barcelona, spain.
associationfor computational linguistics..ondˇrej praˇz´ak, pavel pˇrib´aˇn, and stephen taylor.
2020.uwb @ diacr-ita: lexical semantic change de-tection with cca and orthogonal transformation.
in proceedings of the 7th evaluation campaign ofnatural language processing and speech tools foritalian (evalita 2020), online.
ceur.org..julia rodina and andrey kutuzov.
2020. rusemshift:a dataset of historical lexical semantic change inin proceedings of the 28th internationalrussian.
conference on computational linguistics (coling2020).
association for computational linguistics..alex rosenfeld and katrin erk.
2018. deep neuralin proceedings of themodels of semantic shift.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 474–484, neworleans, louisiana..gerard salton and michael j mcgill.
1983. introduc-tion to modern information retrieval.
mcgraw-hillbook company, new york..dominik schlechtweg, anna h¨atty, marco del tredici,and sabine schulte im walde.
2019. a wind ofchange: detecting and evaluating lexical seman-tic change across times and domains.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 732–746, flo-rence, italy.
association for computational linguis-tics..dominik schlechtweg, barbara mcgillivray, simonhengchen, haim dubossarsky, and nina tahmasebi.
2020. semeval-2020 task 1: unsupervised lexi-cal semantic change detection.
in proceedings ofthe 14th international workshop on semantic eval-uation, barcelona, spain.
association for computa-tional linguistics..dominik schlechtweg and sabine schulte im walde.
submitted.
clustering word usage graphs: a flex-ible framework to measure changes in contextualword meaning..dominik schlechtweg, sabine schulte im walde, andstefanie eckmann.
2018. diachronic usage relat-edness (durel): a framework for the annotation.
in proceedings of theof lexical semantic change.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 169–174, neworleans, louisiana..dominik schlechtweg, nina tahmasebi, simonhengchen, haim dubossarsky,and barbaramcgillivray.
2021. dwug: a large resource ofdiachronic word usage graphs in four languages..dominik schlechtweg and sabine schulte im walde.
2020. simulating lexical semantic change fromin the evolution of lan-sense-annotated data.
guage: proceedings of the 13th international con-ference (evolang13)..hinrich sch¨utze.
1998. automatic word sense discrim-ination.
computational linguistics, 24(1):97–123..philippa shoemark, farhana ferdousi liza, dongnguyen, scott hale, and barbara mcgillivray.
2019.room to glo: a systematic comparison of seman-tic change detection approaches with word embed-in proceedings of the 2019 conference ondings.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages66–76, hong kong, china.
association for compu-tational linguistics..nina tahmasebi, lars borin, and adam jatowt.
2018.survey of computational approaches to diachronicconceptual change.
arxiv e-prints..nina tahmasebi and thomas risse.
2017. finding in-dividual word sense changes and their delay in ap-pearance.
in proceedings of the international con-ference recent advances in natural language pro-cessing, pages 741–749, varna, bulgaria..hiroya takamura, ryo nagata,.
and yoshifumikawasaki.
2017. analyzing semantic change inin proceedings of the 15thjapanese loanwords.
conference of the european chapter of the associa-tion for computational linguistics: volume 1, longpapers, pages 1195–1204, valencia, spain.
associa-tion for computational linguistics..adam tsakalidis, marya bazzi, mihai cucuringu, pier-paolo basile, and barbara mcgillivray.
2019. min-ing the uk web archive for semantic change detec-tion.
in proceedings of the international conferenceon recent advances in natural language process-ing (ranlp 2019), pages 1212–1221, varna, bul-garia.
incoma ltd..adam tsakalidis and maria liakata.
2020. sequentialmodelling of the evolution of word representationsfor semantic change detection.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8485–8497,online.
association for computational linguistics..peter d. turney and patrick pantel.
2010. from fre-quency to meaning: vector space models of seman-tics.
j. artif.
int.
res., 37(1):141–188..6996wiktionary.
wiktionary, das freie w¨orterbuch.
https://de.wiktionary.org.
accessed: 2021-01-07..a additional plots.
please ﬁnd additional plots of word usage graphsin figures 3–6..6997full.
c1.
c2.
figure 3: word usage graph of german zehner (left), subgraphs for ﬁrst time period c1 (middle) and for secondtime period c2 (right)..full.
c1.
c2.
figure 4: word usage graph of german lager (left), subgraphs for ﬁrst time period c1 (middle) and for secondtime period c2 (right)..full.
c1.
c2.
figure 5: word usage graph of german anriffswaffe (left), subgraphs for ﬁrst time period c1 (middle) and forsecond time period c2 (right)..full.
c1.
c2.
figure 6: word usage graph of german neunj¨ahrig (left), subgraphs for ﬁrst time period c1 (middle) and forsecond time period c2 (right)..6998