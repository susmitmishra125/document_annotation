end-to-end training of neural retrievers foropen-domain question answering.
devendra singh sachan1,2∗, mostofa patwary3, mohammad shoeybi3, neel kant3,wei ping3, william l hamilton1,2,4, bryan catanzaro31mila - quebec ai institute; 2mcgill university; 3nvidia; 4canada cifar ai chairsachande@mila.quebec, mpatwary@nvidia.com.
abstract.
recent work on training neural retrievers foropen-domain question answering (openqa)has employed both supervised and unsuper-vised approaches.
however, it remains un-clear how unsupervised and supervised meth-ods can be used most effectively for neuralin this work, we systematicallyretrievers.
study retriever pre-training.
we ﬁrst proposean approach of unsupervised pre-training withthe inverse cloze task and masked salientspans, followed by supervised ﬁnetuning usingquestion-context pairs.
this approach leadsto absolute gains of 2+ points over the previ-ous best result in the top-20 retrieval accuracyon natural questions and triviaqa datasets.
we next explore two approaches for end-to-end training of the reader and retriever compo-nents in openqa models, which differ in themanner the reader ingests the retrieved docu-ments.
our experiments demonstrate the ef-fectiveness of these approaches as we obtainstate-of-the-art results.
on the natural ques-tions dataset, we obtain a top-20 retrieval ac-curacy of 84%, an improvement of 5 pointsover the recent dpr model.
we also showcasegood results on answer extraction, outperform-ing recent models such as realm and ragby 3+ points.
our code is available at: https://github.com/nvidia/megatron-lm..1.introduction.
the task of open-domain question answering(openqa) consists of ﬁnding answers to theinformation-seeking questions using a large knowl-edge source such as wikipedia.
this knowledgesource is also referred to as evidence and it typicallycontains millions of documents.
most approachesfor openqa consist of a two-stage pipeline (chenet al., 2017; chen, 2018).
in the ﬁrst stage, given.
∗ this work was done during an internship at nvidia.
corresponding authors: devendra sachan, mostofa patwary..figure 1: an example illustrating openqa pipeline..a question, a retriever module identiﬁes the mostrelevant documents, which is often a very smallsubset of the evidence known as context documents.
traditionally, approaches based on document rank-ing such as bm25 (robertson and zaragoza, 2009)in the secondhave been used for the retriever.
stage, these relevant documents are given as inputto the reader module, which understands them andextracts the answer for the question (figure 1)..the main drawback of the bm25 method is thatit is not trainable and hence it can’t be adapted totasks involving open-retrieval.
recent work has ad-dressed this limitation by building upon advancesin self-supervised learning, such as bert (devlinet al., 2019).
these approaches model both theretriever and reader using neural networks, allow-ing the retriever to be trained using task-speciﬁcdatasets (lee et al., 2019; guu et al., 2020).
typi-cally, the retriever model consists of a dual-encoderarchitecture (bromley et al., 1994), where one en-coder processes the question and the other encoderprocesses the context document.
prior work hasinvestigated both unsupervised and supervised ap-proaches to train the retriever.
unsupervised ap-proaches include separately training the retrieverwith inverse cloze task (ict) (lee et al., 2019)or training the retriever and reader jointly by pre-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6648–6662august1–6,2021.©2021associationforcomputationallinguistics6648document 1: bowling is a target sport and recreational activity in which a player rolls a ball towards pins (in pin bowling) or another target (in target bowling) …document 2: “hall of fame” is a song by irish pop rock band the script.
it is the lead single from their studio album #3. the track features american hip-hop artist will.i.amof the black eyed peas.document 3: the world bowling writers (wbw) international bowling hall of famewas established in 1993 and is located … on the international bowling campus in arlington, texas.question: where is the bowling hall of fame located?stage 1: retrieverstage 2: readeranswer: arlington, texasdicting masked salient spans (realm) (guu et al.,2020), while supervised approaches such as densepassage retrieval (dpr) (karpukhin et al., 2020)train the retriever using human-annotated sets ofquestion and context pairs..however, there is no study that investigates thecomparative advantages of using these two styles oftraining when the retrieval task is challenging, i.e.,when the evidence contains millions of documents.
it is unclear if the unsupervised approaches canfurther help to improve the performance of strongsupervised approaches, and, if so, under what con-ditions.
a core focus of this work is systematicallystudying these aspects of retriever training..we propose a uniﬁed approach to train the re-triever: unsupervised pre-training followed by su-pervised ﬁnetuning.
we also investigate key de-sign choices—such as relevance score scaling andlonger training—and showcase their effectiveness.
our results demonstrate that the proposed approachobtains substantial accuracy gains when evaluatedon benchmark openqa datasets.
extensive experi-ments also highlight the relative importance of dif-ferent pre-training strategies, revealing importanttrade-offs when varying the amount of superviseddata available to train the retriever..furthermore, motivated by recent work (guuet al., 2020; lewis et al., 2020a), we also exploretwo approaches for end-to-end supervised trainingof the reader and retriever components.
in the ﬁrstapproach, the reader considers each retrieved docu-ment separately while in the second approach, thereader takes as input all the retrieved documentstogether.
we compare the effectiveness of theseapproaches on both retrieval accuracy and answerextraction.
we show that the ﬁrst approach leadsto an improved retrieval performance, while thesecond approach results in an improved answer ex-traction.
with end-to-end training, we outperformprevious best models to obtain new state-of-the-artresults on retrieval accuracy and answer extraction.
we also perform experiments by scaling the modelsize to a large conﬁguration for both retriever andreader and observe consistent improvements, com-pared with smaller models..in summary, the contributions of this work are:.
• we demonstrate that our proposed method ofunsupervised pre-training of the retriever withict followed by supervised ﬁnetuning leads toabsolute gains of more than 2 points in the top-20retrieval accuracy over the previous best result.
on natural questions and triviaqa datasets.
• we show that masked salient spans-based pre-training of the retriever is more effective whenthe supervised dataset sizes are small..• our end-to-end training approach obtains newstate-of-the-art performance on retrieval accu-racy.
on natural questions, our top-20 accuracyis 84, which is a 5 points gain over dpr results.
• we achieve competitive results on answer extrac-tion with gains of more than 3 points over recentmodels such as realm (guu et al., 2020) andrag (lewis et al., 2020c)..• we scale up end-to-end training to large models.
and show consistent gains in performance..the rest of the paper is organized as follows.
sec.
2 and 3 explain the retriever model and end-to-end training, respectively.
sec.
4-6 describe theexperimental details with the results.
sec.
7 reviewsthe related work followed by conclusion in sec.
8..2 neural retriever.
in this section, we ﬁrst describe the retriever ar-chitecture and then discuss different approaches totrain it, including our proposed approach..2.1 background.
given a collection of documents in the evidencez = {z1, · · · , zm} and a question q, the task of theretriever is to select a relevant subset of documentsfor the question.
to do this, the retriever performs aranking of the evidence documents conditioned onthe question and outputs the top-ranked documents.
the retriever model consists of two modules: aquestion encoder (fq) and a context encoder (fz).
such a model is often referred to as a dual-encodermodel (bromley et al., 1994).
here, we detail thetraining methodology of the dual-encoder modelgiven a questions (q) and context documents (zi)from z. first, we compute the relevance scorebetween the question and context.
we deﬁne therelevance score to be the dot-product between thequestion and context representations.
s(q, zi; φ) = fq(q)(cid:62)fz(zi).
(1).
where fq(q) ∈ rd and fz(z) ∈ rd denote thequestion and context encoders, respectively, whichare parameterized by φ = [φq, φz].
we modelthe fq and fz using bert-style transformer net-works (devlin et al., 2019; vaswani et al., 2017).
we consider the hidden states of the ﬁrst token of.
6649the sequence (i.e.
[cls] token) as the encoder’soutput.
the probability of a context document zibeing relevant to the question q is calculated as.
p(zi | q, z; φ) =.
exp(s(q, zi; φ)/τ )j=1 exp(s(q, zj; φ)/τ ).
(cid:80)|z|.
(2).
√.
where τ is a scaling factor.
while previous workhad used the setting of τ = 1, in this work, we setd. bigger scaling factor helps in better opti-τ =mization when the model hidden size (d) is large.
we refer to this as relevance score scaling.
totrain the retriever, we maximize the log-likelihoodcomputed from eq.
2..in practice, as the evidence set consists of mil-lions of documents, the normalization term wouldbe expensive to compute.
hence, we approximatethe denominator of the above equation by using thecontext documents in the batch as negative exam-ples, a technique that has shown to perform well inpractice (chen et al., 2020)..2.2 training.
in this section, we discuss different approaches totrain the retriever.
in all the approaches, we initial-ize the parameters of both the question and contextencoders using bert weights as implemented inmegatron-lm (shoeybi et al., 2019).
we also ex-perimented with random initialization but it vastlyunderperformed bert initialization..2.2.1 supervised trainingin the supervised setting, human-annotated ques-tions, answers, and sometimes context are provided.
if the context is not included, then a common ap-proach is to use distant supervision (mintz et al.,2009) to obtain the context document.
speciﬁ-cally, we select the top-ranked document usingbm25 (robertson and zaragoza, 2009) from theevidence that contains the answer as the context.
we also select other top-ranked documents that donot contain the answer as additional hard negativeexamples.
this approach to train neural retrieverwas popularized by (karpukhin et al., 2020)..2.2.2 unsupervised traininginverse cloze task (ict):in this setup, we donot consider the human-annotated question-contextpairs.
instead, the retriever is trained in an unsu-pervised manner.
speciﬁcally, a randomly sampledsentence from a paragraph is considered as thequery while other sentences as the context.
thisapproach was ﬁrst proposed by (lee et al., 2019)..masked salient spans training:(guu et al.,2020) showcased that the ict initialized retrievercan be further improved by training it with an ob-jective where the reader predicts the masked salientspans such as named entities conditioned on theretrieved documents.
in this work, we adopt thesame approach.
however, unlike (guu et al., 2020)who use bert for the reader, we use a generativelanguage model based on t5 (raffel et al., 2020)..2.3 proposed approach: unsupervised.
pre-training and supervised finetuning.
to improve the retriever training, we propose theapproach of unsupervised pre-training of the re-triever followed by supervised ﬁnetuning.
in thisapproach, we ﬁrst pre-train the retriever weightswith ict training or masked salient spans training(sec.
2.2.2).
after pre-training, we ﬁnetune theretriever with supervised training (sec.
2.2.1)..3 end-to-end retriever and reader.
training.
in this section, we explore two supervised train-ing approaches to end-to-end train the reader andretriever components from the task-speciﬁc data.
in the ﬁrst approach, the reader considers each re-trieved document separately (sec.
3.1) while inthe second approach, the reader takes as input allretrieved documents together (sec.
3.2).
these ap-proaches are designed such that when predictingthe answer conditioned on the question, the learn-ing process improves both the reader and retriever..background and notation:in end-to-end train-the trainable components consists of theing,retriever (φ) and reader (θ) parameters.
forretriever, we use the dual-encoder architectureand train it as discussed previously in sec.
2.3.our reader is a generative model designed ac-cording to the sequence-to-sequence modelingparadigm (sutskever et al., 2014).
speciﬁcally, weuse pre-trained t5 as the reader.
the inputs to thetraining process are questions (q) and its answers(a), both in string form.
given a question, ﬁrst theretriever obtains the k relevant context documents(k) from the evidence (z) as.
k = arg sort.
s(q, zi; φ)[: k].
(3).
zi∈z.
the reader then takes the question and one or morecontext documents (zi) as input to predict the an-.
6650figure 2: a schematic diagram illustrating end-to-end supervised training of the retriever and reader components..swer, the likelihood of which is deﬁned as.
3.2 approach 2: joint top-k.p(a | q, zi; θ) =.
p (aj | a1:j−1, q, zi; θ) ,.
(4).
n(cid:89).
j=1.
where n is the number of answer tokens.
next,we describe the two proposed approaches.
a blockdiagram illustrating the end-to-end training processis shown in figure 2..3.1 approach 1: individual top-k.in this approach, similar to (guu et al., 2020), thereader’s likelihood is ﬁrst computed conditionedon the question and each retrieved document.
themarginal likelihood is deﬁned as the weighted av-erage of the individual likelihoods as.
(cid:88).
zi∈k.
p(a | q; θ, φ) =.
p(a | q, zi; θ)p(zi | q, z; φ),.
(5)where p(zi| q, z; φ) is computed using eq.
2.however, the normalization is done over k insteadof z. the ﬁnal loss is deﬁned as the negativemarginal log-likelihood.
l(q, a) = − log p(a | q; θ, φ)..(6).
in this approach, similar to (lewis et al., 2020a),the likelihood is deﬁned as the reader’s likelihoodconditioned on the question, all the retrieved docu-ments, and the retrieval score.
p(a | q; θ, φ) = p(a | q, z1:k, p(z | q, z; φ); θ)..(7)as the t5 reader consists of separate encoderand decoder modules, it provides the ﬂexibility tocustomize the input or output of the encoder.
weconcatenate each retrieved document with the ques-tion and feed them as input to the encoder, whichcomputes their hidden representations.
next, westack the hidden representations of all the retrieveddocuments, which the decoder jointly attends toduring the encoder-decoder attention, thus allowinga more powerful form of information aggregationfrom multiple retrieved documents.
we also add re-triever similarity score to bias the encoder-decoderattention as it helps facilitate end-to-end trainingand enables the reader to pay higher attention to therelevant documents.
the interaction score duringthe encoder-decoder attention is computed as.
we note that the rag model (lewis et al., 2020c)also proposed a similar approach, but there are twomain differences.
the ﬁrst is that while we updateall the parameters of the retriever (both the queryand context encoders), rag just updates the queryencoder.
the second is that we use t5 model asthe reader while rag uses bart model (lewiset al., 2020b).
these enhancements help us obtainsubstantial gains over the rag model, which wewill discuss in sec.
6..attn(q, a, z1:k) ∝ q(a)(cid:62)k(z1:k, q) + λp(z | q; φ),(8)where q is the query vector computed from de-coder’s input, k is the key vector computed fromencoder’s output, and λ is a trainable parameter..final loss is deﬁned according to eq.
6. wefurther note that a similar approach for openqawas proposed in (izacard and grave, 2020) but itonly optimizes the reader model and didn’t performend-to-end training of the retriever..6651question (q)evidence embeddingstop-k inner product searchencoderdecodert5 modeltop-k documents (z)readerxdot-productrelevance scorestale weights from previous checkpoint   evidence documentsasynchronousembeddingupdatelossanswer (a)retrieverdataset.
train.
filtered train.
dev.
test.
setting.
top-1 top-5 top-20 top-100.
79,168nqtriviaqa 78,785.
58,88060,413.
8,7578,837.
3,61011,313.table 1: openqa dataset statistics.
the training set isused for end-to-end training, while the ﬁltered versionis used for retriever training.
the ﬁltered set ignoresthose examples where the document retrieved from ev-idence does not align with the ground-truth document..4 experimental setup.
in this section, we describe the datasets and modelsettings.
for reproducibility, we provide trainingdetails and list the hyperparameters in appendix a..4.1 openqa datasets.
we perform experiments using two widely usedqa datasets whose details are provided below andtheir statistics are shown in table 1..natural questions (nq): this corpus con-sists of real questions asked from the googlesearch engine along with their long and short an-swer annotations from the top-ranked wikipediapages (kwiatkowski et al., 2019).
following priorwork (karpukhin et al., 2020), we use the samesubset of the short answer questions in our experi-ments, as it is more suited for openqa..triviaqa: this corpus consists of a collectionof trivia questions and their answers scraped frommultiple sources in the web (joshi et al., 2017)..evidence: following (karpukhin et al., 2020),we make use of their released preprocessed en-glish wikipedia dump from december 2018 as thesource of evidence documents.
overall, there are21, 015, 324 documents, each 100 words long..4.2 model details.
we use two models of different sizes, base andlarge, for the experiments.
the base conﬁgurationconsists of 12 layers, 768-d hidden size, and 12attention heads.
the bert-base contains 110mparameters while the t5-base contains 220m pa-rameters.
the large conﬁguration consists of 24layers, 1024-d hidden size, and 16 attention heads.
the bert-large contains 330m parameters whilethe t5-large contains 770m parameters..5 results: retriever training.
in this section, we compare different approaches totrain the retriever.
retrieval accuracy is evaluatedusing the top-k metric (k ∈ {1, 5, 20, 100})..base conﬁguration.
[cls], 40 epochs+ score scaling+ 80 epochs+ 1 hard negative.
dpr (ofﬁcial).
32.634.136.748.6.
–.
60.160.962.274.5.
67.1.
76.477.677.479.0.
78.4.
85.985.986.085.8.
85.4.table 2: effect of different factors on the supervisedtraining of retriever when evaluated on nq test set..5.1 effect of relevance score scaling, longer.
training, and hard negatives.
we explore the best training settings for supervisedtraining of the retriever.
to do so, we perform aseries of experiments on the nq dataset startingwith the training settings from the popular dprmodel and then progressively improve it.
dpr wasinitialized with bert, trained for 40 epochs, witha scaling factor of 1, and utilized [cls] token em-beddings from the retriever.
our result with thissetting is shown in table 2. we then observe thatincorporating relevance score scaling and longertraining till 80 epochs helps to improve the top-5and top-20 accuracy by 1.5-2 points.
these resultsalso signify that the original dpr model was sig-niﬁcantly undertrained and not fully optimized..in addition to score scaling, we further include 1additional hard-negative example (similar to dpr)for each question-context pair and train the modelfor 80 epochs.
our results, in sync with the re-sults of dpr, obtain substantial additional gainsin performance.
these ﬁndings highlight that rele-vance score scaling, longer training, and includinga hard negative example are essential to improvethe supervised retriever’s accuracy.
these super-vised training results can be considered as a verystrong baseline.
hence, we employ these settingsin subsequent experiments..5.2 effect of retriever initialization.
we ﬁrst characterize the zero-shot retriever’s perfor-mance when its weights are initialized with eitherbert or ict or masked salient spans pre-training(table 3).
as is understood that unsupervised lan-guage models do not perform well in informationretrieval tasks (lee et al., 2019), evidently, bertalso leads to a poor retrieval accuracy.
we note thatict initialization is quite effective in providing anon-trivial zero-shot accuracy which is further im-proved by masked salient spans training by morethan 8 points.
both being unsupervised approaches.
6652model.
nq.
triviaqa.
top-1 top-5 top-20 top-100 top-1.
top-5 top-20 top-100.
bert (zero-shot)ict (zero-shot)masked salient spans (zero-shot)bert + supervisedict + supervisedmasked salient spans + supervised.
ict (zero-shot)bert + supervisedict + supervised.
base conﬁguration.
0.912.620.048.648.450.3.
13.051.452.4.
3.932.341.768.872.171.9.
31.871.072.7.
9.450.659.879.081.882.1.
49.381.082.6.large conﬁguration.
20.366.874.985.888.087.8.
66.187.288.3.
0.619.231.757.558.460.6.
20.160.461.9.
2.840.253.372.273.974.8.
41.674.576.2.
7.257.568.280.081.781.8.
58.581.482.9.
17.873.679.485.186.386.6.
74.186.087.1.table 3: effect of unsupervised pre-training on retrieval accuracy when evaluated on nq and triviaqa test sets..demonstrate their utility in effectively bootstrap-ping the retriever almost from scratch..we next empirically analyze our proposed ap-proach of pre-training with ict and masked salientspans followed by supervised ﬁnetuning.
we ob-serve that it provides absolute improvements of2-3 points over the already strong supervised train-ing results, with the gains being consistent acrossboth the datasets.
these results highlight that evenafter ﬁnetuning the retriever with thousands of la-beled examples, it does not lead to catastrophicforgetting of the discriminative properties learnedby the retriever during ict and masked salientspans pre-training.
another merit is that being un-supervised, large text collections can be leveragedto pre-train the retriever, a considerable advantageover data-augmentation methods which rely on theavailability of human-annotated question-contextpairs.
furthermore, when comparing ict withmasked salient spans initialization, we note thattheir accuracy gains are roughly similar..5.3 effect of amount of training data.
we study the effect on accuracy when the retrieveris pre-trained with bert, ict, or masked salientspans and the amount of supervised training datais varied.
we train the retriever with 1%, 2%, 5%,10-50%, of nq’s training data and plot the top-20accuracy in figure 3. results reveal that in the low-resource regime, masked salient spans pre-trainingis much more effective than ict, consistently lead-ing to large gains.
as the fraction of training dataincreases to beyond 40% towards a high-resourcesetup, the gains from salient spans pre-trainingsaturates to that of ict.
we believe that these ﬁnd-ings will have important implications for futureresearch in openqa—with only a few hundred ex-.
figure 3: effect of amount of training data on retrievalaccuracy when evaluated on nq test set..amples, performing expensive masked salient spantraining is beneﬁcial while if the training data hasthousands of examples, ict is just as optimal asmasked salient spans training..5.4 effect of end-to-end training.
for end-to-end training, retriever weights are ini-tialized with the previous best setting of ict pre-training and supervised ﬁnetuning.
the numberof retrieved evidence documents for the reader isconsidered as a hyperparameter and is selected viaperformance on the dev set.
the focus here is toanalyze the effect on retrieval accuracy when up-dating the retriever weights using question-answerpairs in an end-to-end setting (sec.
3).
from theresults in table 4, we observe that for individualtop-k, when only the query encoder is updated, ittends to improve retrieval accuracy.
in addition,when the context encoder is also updated, the re-trieval accuracy improves to 75% at top-5, a biggain of 8 points over the previous best dpr re-triever.
larger models further help to improve theperformance leading to new state-of-the-art results.
on the other hand, in joint top-k, updating the.
66530.00.010.020.050.10.20.40.51.0fractionofthetrainingdata1020304050607080top-20accuracybertinitictinitmaskedsalientspansinitmodel.
nq.
triviaqa.
q c top-1 top-5 top-20 top-100 top-1 top-5.
top-20 top-100.
dpr (karpukhin et al., 2020).
ict + supervisedindividual top-kindividual top-kjoint top-k.ict + supervisedindividual top-kjoint top-k.base conﬁguration.
–.
48.454.556.851.1.
52.457.553.7.
67.1.
72.173.775.072.1.
72.776.273.3.
78.4.
81.883.284.081.8.
82.684.883.2.large conﬁguration.
85.4.
88.088.689.287.8.
88.389.888.0.
(cid:51) (cid:55)(cid:51) (cid:51)(cid:51) (cid:55).
(cid:51) (cid:51)(cid:51) (cid:55).
–.
58.461.463.559.1.
61.966.461.2.
–.
73.975.676.874.1.
76.278.775.9.
79.4.
81.782.183.181.3.
82.984.182.7.
85.0.
86.386.787.086.3.
87.187.887.0.table 4: effect of end-to-end training using question-answer pairs on retrieval accuracy.
q and c signify if thequery encoder and the context encoder are updated during training or not, respectively..√.
0.250.5124.
×.
d top-1 top-5.
top-20 top-100 avg..base conﬁguration.
48.851.451.150.250.6.
69.371.671.871.571.7.
78.781.582.181.981.7.
85.587.787.787.988.0.
70.673.173.272.973.0.table 5: effect of score scaling factor (τ ) on the re-trieval accuracy when evaluated on the nq test set.
theﬁrst column denotes the multiple (m) that is multipliedby.
d to obtain τ , i.e., τ = m ×.
d in equation 2..√.
√.
query encoder just improves the top-1 score butdoes not really lead to much accuracy gains forhigher top-k’s.
we also do not update the contextencoder for joint top-k as it did not result in im-provements during our initial experiments..these results showcase that when the retrieveris already well-initialized, the objective functionof individual top-k method is designed such that itsigniﬁcantly improves the retrieval accuracy whilethe joint top-k method does not result in improve-ments.
as we will show next, that the usefulnessof this method lies in answer extraction..here, we brieﬂy explain the intuition regardingthe usage of the scaling factor.
in our preliminaryexperiments on retriever training and end-to-endtraining without the scaling factor, we observedthat a few of the top-k document’s similarity scorewith the query was very high that in turn led to itbeing assigned a high retrieval probability score.
this high score was leading to a skewed probabilitydistribution with most of the mass being centeredover the top-1 or top-2 retrieved documents.
alarger value of scaling factor results in a more evendistribution of probability mass over the top-k doc-uments, which in turn leads to better results in bothretrieval accuracy and in the end-to-end training..6 results: answer extraction.
we next present the results of end-to-end trainingon answer extraction.
to train the model, retrieverweights are initialized with ict pre-training andsupervised ﬁnetuning while the reader is initializedwith pre-trained t5 weights.
the number of re-trieved evidence documents for the reader is tunedon the dev set.
results are reported using the con-ventional exact match (em) metric..5.5.intuition for retriever score scaling.
6.1.individual top-k approach.
retrieval score scaling is used when computing theprobability distribution of the retrieved documentsaccording to equation 2, where the retrieval scoreis normalized by the scaling factor (τ ).
to study theeffect of τ on the retrieval accuracy, we perform anablation study with different values of τ on the nqretrieval task, whose results can be seen in table 5.more speciﬁcally, we choose different values of τd, where d is the hidden size ofas a multiple ofthe model.
our results indicate that the choice ofτ =.
d works well in practice..√.
√.
we compare our results as presented in table 6 withthe recent related approaches in openqa.
for thebase conﬁguration on nq, our model outperformsboth realm and dpr by more than 4 points.
forthe large conﬁguration, we compare with the ragmodel (lewis et al., 2020c), where our approachoutperforms it by 3.5+ points on nq and by 2.8points on triviaqa.
our improved results are be-cause of a more accurate initial retriever, strongerreader, and updating both the query and contextencoders during training..6654model.
nq triviaqa.
model.
nq triviaqa.
base conﬁguration.
base conﬁguration.
orqa (lee et al., 2019)realm (guu et al., 2020)dpr (karpukhin et al., 2020)individual top-k.large conﬁguration.
rag (lewis et al., 2020c)individual top-k.33.340.441.545.9.
44.548.1.
45.0–56.856.3.
56.859.6.table 6: answer extraction results using individualtop-k approach.
the grouping under base and largeconﬁgurations is based on the size of the reader model..fid (izacard and grave, 2020)joint top-k.48.249.2.large conﬁguration.
fid (izacard and grave, 2020)joint top-k.51.451.4.
65.064.8.
67.668.3.table 7: results on answer extraction using joint top-k approach..figure 4: effect of increasing top-k documents on an-swer generation for individual top-k approach..our analysis in figure 4 reveals that updating thecontext encoder improves the results for both thebase and large conﬁgurations.
quite surprisingly,we also observe that the performance of individ-ual top-k approach is sensitive to the number oftop-k documents and can also decrease with an in-crease in top-k documents.
we leave an in-depthinvestigation of this as a future work..6.2.joint top-k approach.
we compare our results with the recent fusion-in-decoder (fid) approach (izacard and grave, 2020)that also performs joint encoder-decoder attention.
it consists of dpr as the retriever and t5 as thereader, which are initialized with their open-sourceweights.
however, unlike our approach, fid justﬁnetunes the reader weights.
our results in table 7show that for the base conﬁguration, joint top-k outperforms the fid model by 1 point on nq,highlighting the signiﬁcance of end-to-end training.
for the large conﬁguration, we obtain a gain of 0.7points on triviaqa..our analysis in figure 5 portrays that the emscores improve with more retrieved documents.
this highlights that in contrast to individual top-k,the joint top-k better aggregates the information.
figure 5: effect of increasing top-k documents on an-swer generation for joint top-k approach..contained in the retrieved documents.
this fig-ure also illustrates the effect of similarity enrichedattention on answer extraction for the base conﬁg-uration.
for values of top-k=5, 10, and 25, usingretrieval-similarity enriched encoder-decoder atten-tion, we consistently observe a gain of 0.8-1 empoints (comparing orange plot and blue plot in fig-ure 5), while there is a smaller gain when top-k=50.
this signiﬁes that with more retrieved documents,the utility of end-to-end training tends to dimin-ish, thus explaining the lower gains observed inretrieval performance for joint top-k in table 4..6.3 overall comparison.
based on the discussions in sec.
5.4 and sec.
6,we remark that end-to-end training using the twoapproaches has a complementary effect on the re-trieval accuracy and answer extraction.
while theindividual top-k approach helps to signiﬁcantlyimprove the retrieval performance, the joint top-kapproach is more useful for answer extraction..7 related work.
(yih et al., 2011) proposed a discriminative ap-proach to train a retriever by learning dense repre-sentations of query and context documents basedon word frequency.
however, this approach wasdata-hungry and not scalable.
recently, (lee et al.,.
66551020304050numberoftop-kdocuments4244464850exactmatchscorereader+queryenc.update(base)reader+query+contextenc.update(base)reader+query+contextenc.update(large)1020304050numberoftop-kdocuments42444648505254exactmatchscorereaderupdate(base)reader+queryenc.update(base)reader+queryenc.update(large)2019; karpukhin et al., 2020) address this by lever-aging pre-trained bert weights (devlin et al.,2019) to train a dual-encoder retriever by usingsmaller amounts of question-context pairs.
in par-ticular, (lee et al., 2019) ﬁrst pre-train the retrieverin an unsupervised manner using ict and thenjointly train the retriever and reader for openqa.
on the other hand, (karpukhin et al., 2020) per-form supervised training of the retriever using hard-negative examples, yielding impressive results onseveral retrieval benchmarks..to improve the retrieval accuracy of the dual-encoder model, (chang et al., 2020) explore severalparagraph-level pre-training strategies includingthe application of ict.
they demonstrated the ef-fectiveness of pre-training over sparse-retrieval ap-proaches such as bm25.
their evidence consistedof the training documents that was further increasedto 1m documents for openqa.
our work differsfrom them in several ways.
first, our openqasetup is more challenging as the evidence consistsof 21m documents.
second, we pre-train with twostrategies consisting of ict and masked salient-spans and ﬁnetune using strong supervised meth-ods, which leads to much improved results.
third,we further update the retriever with end-to-endtraining leveraging question-answer pairs, whichfurther improves the retrieval accuracy leading tonew state-of-the-art results..a new line of work investigates task-speciﬁc pre-training of language models.
for example, (guuet al., 2020) predicts masked salient spans consist-ing of named entities to pre-train the reader and re-triever components for openqa.
similarly, (lewiset al., 2020a) perform cross-lingual pre-trainingwhere the objective is to predict a sequence usingits paraphrases in different languages, demonstrat-ing improved zero-shot performance in documenttranslation tasks..8 conclusion.
we propose approaches to improve the retrieval ac-curacy of the dual-encoder model for the openqatask.
we ﬁrst perform a systematic investigationof the importance of pre-training with ict andmasked salient spans tasks for supervised trainingof the retriever.
we then present two approaches forend-to-end training of the reader and retriever com-ponents in openqa.
in one approach, the readerconsiders each retrieved document individuallywhile in the other approach where the reader con-.
siders all the retrieved documents jointly.
overall,these methods help achieve state-of-the-art resultson both retrieval and answer extraction..acknowledgements.
this work was done during the ﬁrst author’s intern-ship at nvidia.
it was also partially supported bycanada cifar ai chair held by prof. hamilton.
we would like to thank the anonymous reviewersfor providing valuable feedback and recommenda-tions.
we would also like to thank the adminis-trators of the selene supercomputer for their assis-tance in facilitating the large-scale runs..broader impact and ethics statement.
to understand the ethical context of our work onopen-domain question answering, it is importantto consider the real-world use cases and potentialindividuals who may interact with systems devel-oped based on our proposed methods.
the poten-tial real-world applications could be search enginesor virtual assistants, where our techniques can im-prove the question-answering ability.
however, itis worthwhile to mention that our trained systemscan not be deployed off-the-shelf for such appli-cations, given that our models were trained on thenatural questions and triviaqa datasets with thegoal of matching the speciﬁc training data distri-bution.
real-world applications building on ourwork should be re-trained using a custom trainingdataset that is relevant to the kind of queries thatoriginates in practice..our system represents a prototype model for an-swering questions over wikipedia and can easily beextended to be used in sensitive contexts such as le-gal or health-care settings.
however, extensive androbust quality assurance testing will be needed asour system was not designed to meet those criteria.
more generally, there is the possibility of social bi-ases which could be introduced by the training data.
since we did not control or regularize our modelto remove such biases, we would urge the users toundertake the necessary quality-assurance testingto evaluate and understand the extent to which suchbiases might be present.
user should also under-stand how much these biases are impacting theirtrained system and to make modiﬁcations to theirtraining data and procedures accordingly..6656references.
alexandr andoni, piotr indyk, thijs laarhoven, ilyarazenshteyn, and ludwig schmidt.
2015. practicaland optimal lsh for angular distance.
in advances inneural information processing systems..jane bromley, isabelle guyon, yann lecun, eduards¨ackinger, and roopak shah.
1994. signature veri-ﬁcation using a ”siamese” time delay neural network.
in advances in neural information processing sys-tems..wei-cheng chang, felix x. yu, yin-wen chang, yim-ing yang, and sanjiv kumar.
2020. pre-trainingtasks for embedding-based large-scale retrieval.
ininternational conference on learning representa-tions..danqi chen.
2018. neural reading comprehensionand beyond.
ph.d. thesis, stanford university..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers)..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers)..kelvin guu, kenton lee, zora tung, panupong pa-supat, and mingwei chang.
2020. retrieval aug-in proceed-mented language model pre-training.
ings of the 37th international conference on ma-chine learning..gautier izacard and edouard grave.
2020. lever-aging passage retrieval with generative models foropen domain question answering.
arxiv preprintarxiv:2007.01282..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers)..vladimir karpukhin, barlas o˘guz, sewon min, ledellwu, sergey edunov, danqi chen, and wen-tau yih.
2020. dense passage retrieval for open-domainin proceedings of the 2020question answering.
conference on empirical methods in natural lan-guage processing (emnlp)..diederik p kingma and jimmy ba.
2015. adam: ain the 2015method for stochastic optimization.
international conference for learning representa-tions..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
transactions of the association of compu-tational linguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervised learningin international con-of language representations.
ference on learning representations..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for compu-tational linguistics..mike lewis, marjan ghazvininejad, gargi ghosh, ar-men aghajanyan, sida wang, and luke zettlemoyer.
2020a.
pre-training via paraphrasing.
in advancesin neural information processing systems..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020b.
bart: denoising sequence-to-sequencepre-training for natural language generation, trans-in proceedings of thelation, and comprehension.
58th annual meeting of the association for compu-tational linguistics..patrick lewis, ethan perez, aleksandara piktus,f. petroni, v. karpukhin, naman goyal, heinrichkuttler, m. lewis, wen tau yih, tim rockt¨aschel,sebastian riedel,and douwe kiela.
2020c.
retrieval-augmented generation for knowledge-in advances in neuralintensive nlp tasks.
information processing systems..shen li, yanli zhao, rohan varma, omkar salpekar,pieter noordhuis, teng li, adam paszke, jeff smith,brian vaughan, pritam damania, and soumith chin-tala.
2020. pytorch distributed: experiences on ac-celerating data parallel training.
proc.
vldb en-dow..paulius micikevicius, sharan narang, jonah alben,gregory diamos, erich elsen, david garcia, borisginsburg, michael houston, oleksii kuchaiev,ganesh venkatesh, and hao wu.
2018. mixed preci-sion training.
in international conference on learn-ing representations..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation ex-traction without labeled data.
in proceedings of the.
6657joint conference of the 47th annual meeting of theacl and the 4th international joint conference onnatural language processing of the afnlp..sameer pradhan, alessandro moschitti, nianwen xue,olga uryupina, and yuchen zhang.
2012. conll-2012 shared task: modeling multilingual unre-stricted coreference in ontonotes.
in joint confer-ence on emnlp and conll - shared task..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..stephen robertson and hugo zaragoza.
2009. theprobabilistic relevance framework: bm25 and be-yond.
foundations and trends in information re-trieval..mohammad shoeybi, mostofa patwary, raul puri,patrick legresley, jared casper, and bryan catan-zaro.
2019. megatron-lm: training multi-billionparameter language models using gpu model paral-lelism.
arxiv preprint arxiv:1909.08053..anshumali shrivastava and ping li.
2014. asymmetriclsh (alsh) for sublinear time maximum inner productin advances in neural informationsearch (mips).
processing systems, volume 27. curran associates,inc..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems..wen-tau yih, kristina toutanova, john c. platt, andchristopher meek.
2011. learning discriminativeprojections for text similarity measures.
in proceed-ings of the fifteenth conference on computationalnatural language learning..6658a training details.
we provide the training details of all the experi-ments below.
we use the same training settings forboth the base and large model conﬁgurations anduse the open-source megatron-lm toolkit (shoeybiet al., 2019) to implement the models.1 to trainthe models, we employed mixed-precision train-ing (micikevicius et al., 2018) and leveraged dis-tributed training feature as implemented in the py-torch framework (li et al., 2020).
all of our exper-iments were performed on the selene cluster whichconsists of nvidia a100 gpus..a.1 language models training.
we train bert (devlin et al., 2019; lan et al.,2020) and t5 (raffel et al., 2020) language modelsfrom scratch, whose hyperparameters for both thebase and large conﬁgurations are detailed in table 8.we used 32 gpus to train the bert-large (330m)model and 128 gpus to train the t5-large (770m)model..a.2 retriever training.
supervised: we use adam optimizer (kingmaand ba, 2015), a batch size of 128, learning rate of2e-5 with a linear decay, and train for 80 epochs.
training was performed on 16 gpus..ict training: we initialize the parameters ofboth the question and context encoders using bertweights trained with megatron-lm.
we train themodel on wikipedia paragraphs with maximumlength of 256 tokens.
we use a batch size of 4, 096,learning rate of 1e-4 with linear decay, and trainthe model for 100, 000 steps using adam optimizer.
this corresponds to training the model for roughly20 epochs over the wikipedia dataset.
we set theweight decay to 0.01 and the warmup ratio of theoptimizer to 0.01. with a probability of 0.1, wealso keep the query sentence in the context.
wetrain the large ict model using 128 gpus..masked salient spans generative training: weinitialize the retriever with ict training and pre-train the t5 reader on an aggregated datasetfrom (shoeybi et al., 2019).
we use the pre-trainedmodels provided by the stanza toolkit (qi et al.,2020) to segment wikipedia paragraphs into sen-tences and extract named entities.2 the masked.
1https://github.com/nvidia/megatron-lm2we use the model trained on ontonotes (pradhan et al.,.
2012) to extract named entities for 10 selected categories..sentence is used as a query to retrieve evidencedocuments with the help of which the reader pre-dicts the masked words.
the model is trained ac-cording to equation 5 and 6. we train the modelfor 100, 000 steps with adam optimizer using alearning rate of 2e-5 and a warmup ratio of 0.05.similar to (guu et al., 2020), we also compute theevidence embeddings asynchronously and updatethe evidence index every 500 steps.
training wasperformed on 240 gpus..a.3 end-to-end supervised training.
as the performance of the ict pre-trained retrieverand masked salient spans pre-trained retriever issimilar when all the training data is used (sec.
5.2),we select the retriever pre-trained with ict initial-ization and ﬁnetuned with supervised data.
forthe reader, we use a pre-trained t5 model.
for allexperiments, we train for 10 epochs using a batchsize of 64, learning rate of 2e-5 with linear decay,and weight regularization of 0.1. for individualtop-k approach, during training, the evidence em-beddings index is refreshed after every 500 steps.
the number of retrieved evidence documents forthe reader is considered as a hyperparameter and isselected via performance on the dev set.
trainingof individual top-k was performed on 240 gpuswhile training of joint top-k was performed on 64gpus..for retrieving the top-k documents from ourevidence (∼21m documents), we perform exactsearch.
speciﬁcally, we utilize matrix multiplica-tion and top-k functionalities as provided by thepytorch framework.
this matrix multiplicationoperation is highly optimized for gpu computa-tions and we observed that performing exact searchwas not a bottleneck during training.
we there-fore did not optimize or approximate the similaritysearch using lsh (andoni et al., 2015) or efﬁcientmaximum inner product search (shrivastava andli, 2014)..nq and triviaqa speciﬁc details: for bothdatasets, we uniformly sample the target answerfrom the list of provided answers during the train-ing process.
for answer extraction, similar to (guuet al., 2020), we did not append the title of thewikipedia article with the corresponding top-k re-trieved document as the reader’s input..6659wikipedia, bookcorpus wikipedia, cc-stories, realnews, openwebtext.
hyperparameter.
bert.
datasethidden sizeattention headsdropoutattention dropoutoptimizertraining stepswarmup stepspeak learning rateweight decaybatch sizelearning rate decaygradient clipping.
{768, 1024}{12, 16}0.10.1adam1m10k1e-41e-2256linear1.0.t5.
{768, 1024}{12, 16}0.10.1adam1m10k1e-41e-22048linear1.0.table 8: hyperparameters for pre-training bert and t5 models..a.4.
individual top-k inference.
during inference, the reader model ﬁrst greedilygenerates an answer for each retrieved document.
we then score each generated answer using eq.
5and ﬁnally select the answer with the highest likeli-hood score..a.5 example outputs from retriever.
we present few examples in table 9 when the ict+ supervised retriever is evaluated on the nq testdataset..b reproducibility checklist.
b.1 for all reported experimental results.
• a clear description of the mathematical set-ting, algorithm, and/or model: this is pro-vided in the main paper in sec.
2 and sec.
3..• a link to a downloadable source code, withspeciﬁcation of all dependencies, includingexternal libraries (recommended for cameraready, though welcome for initial submission):as mentioned previously, we have developedour codebase over the open-source megatron-lm library (https://github.com/nvidia/megatron-lm).
our implementations overthis codebase are currently organized in differ-ent branches, that are better suited for walk-through with a git-based tool.
to preserveanonymity and in good faith, we are submit-ting the source codes from one branch of ourcodebase, with the caution that the codebasedoesn’t contain an exhaustive readme ﬁle..• a description of computing infrastructureused: we run experiments on nvidia’s se-lene cluster where each node’s speciﬁcations.
are: number of cpus: 256, physical memory:2.2tb, gpu model: 8 x nvidia a100, gpu ar-chitecture and memory: ampere/80gb, arch:x86 64, and disk size: 10tb..• the average runtime for each model or al-gorithm, or estimated energy cost: we pro-vide the average runtime and compute usedfor training different models in appendix a.however, we want to highlight that our codeswere not carefully optimized to minimize run-time or to make optimal use of the hardwareresources..• the number of parameters in each model: weprovide number of parameters in models insec.
4.2..• corresponding validation performance foreach reported test result: validation set per-formance is currently not reported in the mainpaper.
however, we followed rigorous ex-perimentation protocol, and selected the bestmodels by its performance on the validationset.
if the program committee or reviewers re-quire the validation set performance, we willinclude it in the ﬁnal version of the paper..• a clear deﬁnition of.
the speciﬁc evalua-tion measure or statistics used to report re-sults: our evaluation metrics are standard andwidely used by the question answering com-munity.
we provide their details in the mainpaper in sec.
5 and sec.
6..6660question from nq test.
answer.
top-1 document retrieved by ict + super-vised.
what parts make up the peripheral nervous sys-tem.
autonomic nervous system .
.
.
the connection between cns and organsallows the system to be in two different func-tional states: sympathetic and parasympathetic.
the peripheral nervous system is divided intothe somatic nervous system, and the autonomicnervous system.
the somatic nervous systemis under voluntary control, and transmits sig-nals from the brain to end organs such as mus-cles.
the sensory nervous system is part of thesomatic nervous system and transmits signalsfrom senses such as taste and touch (includingﬁne touch and gross touch) to the spinal cordand brain.
.
..when is the new season of wentworth comingout.
19 june 2018.who challenged the aristotelian model of a geo-centric universe.
copernicus.
.
.
.
in a similar manner, a 12-episode fourth sea-son was announced before the airing of thethird season on 27 february 2015.it beganairing from 10 may 2016. cormack conﬁrmeda ﬁfth season had been commissioned on 19july.
the twelve-part series premiered on 4april 2017. on 9 may 2017, showcase an-nounced that the series has been renewed fora sixth season, which premiered on 19 june2018. a seventh season was commissioned inapril 2018, before the sixth-season premiere,with ﬁlming commencing the following weekand a premiere set for 2019. .
...
.
.
(”on the revolutions of the heavenlyspheres”), which posited that the earth and theother planets instead revolved around the sun.
the geocentric system was still held for manyyears afterwards, as at the time the coperni-can system did not offer better predictions thanthe geocentric system, and it posed problemsfor both natural philosophy and scripture.
thecopernican system was no more accurate thanptolemy´s system, because it still used circularorbits.
this was not altered until johannes ke-pler postulated that they were elliptical (kepler´sﬁrst law of planetary motion).
.
.
..table 9: examples of top-1 retrieved documents from the nq test as outputted from the ict + supervised retriever.
if the answer exists in the document, it is highlighted in bold..b.2 for all results involving multiple.
parameter settings in appendix a..experiments, such as hyperparametersearch.
• the exact number of training and evaluationruns: we provide training details for all mod-els in appendix a. speciﬁcally, for the ﬁne-tuning experiments, we train the models untilconvergence, which is 80 epochs for retrievermodels and 10 epochs for answer extractionmodels.
we evaluate the model after eachepoch on the validation set and save the bestcheckpoint according to their performance onthe corresponding evaluation metric..• hyperparameter conﬁgurations for best-performing models: we provide the hyper-.
• the bounds for each hyperparameter: as de-scribed in appendix a, our model and trainingsetting uses standard hyperparameters such asdifferent dropouts ∈ [0, 1), warmup ratio ofoptimizer ∈ [0.01, 0.05], weight regulariza-tion ∈ [0, 1], and learning rate ∈ [1e−4, 1e−5].
the model hyperparameters includes modeldimensions d ∈ {768, 1024}, number of lay-ers ∈ {12, 24}..• the method of choosing hyperparameter val-ues (e.g., uniform sampling, manual tuning,etc.)
and the criterion used to select amongthem (e.g., accuracy): we performed manualhyperparameter tuning.
we also performed.
6661b.3 for all datasets used.
tuning of the number of warmup steps for theadam optimizer.
we selected the best hyper-parameter using performance on the valida-tion set..• summary statistics of the results (e.g.
mean,variance, error bars, etc.
): all of our ex-periments are compute expensive large-scaleruns utilizing a lot of resources such as cpus,gpus and take time ranging from tens ofhours to several days.
therefore, due to com-putational and time constraints performingmultiple runs for each experiment was notfeasible.
therefore, we adopted the approachof using the same seed value (1234) for all thetraining runs including both pre-training andﬁnetuning experiments..• details of train/validation/test splits: we usethe standard training / dev / test splits whosedetails are provided in sec.
4..• relevant statistics such as number of exam-ples and label distributions: we providedataset statistics details in table 1..• an explanation of any data that were excluded,and all pre-processing steps: we include therelevant details in sec.
4..• for natural language data, the name of thelanguage(s): our datasets are in english lan-guage..• a link to a downloadable version of thedataset or simulation environment: both thedatasets of nq and triviaqa are open-sourceand widely used by the community.
nqis available at: https://ai.google.com/research/naturalquestions/download.
triviaqa is available at: http://nlp.cs.
washington.edu/triviaqa/.
we makeuse of the nq, triviaqa, and wikipediadatasets as open-sourced by the dpr au-thors (karpukhin et al., 2020) here: https://github.com/facebookresearch/dpr/blob/master/data/download_data.py..• for new data collected, a complete descrip-tion of the data collection process, such asinstructions to annotators and methods forquality control: this is not applicable to thiswork..6662