dual reader-parser on hybrid textual and tabular evidencefor open domain question answering.
alexander hanbo li, patrick ng, peng xu, henghui zhu,zhiguo wang, bing xiangaws ai labs, amazon.
hanboli, patricng, pengx, henghui, zhiguow, bxiang}.
{.
@amazon.com.
abstract.
the current state-of-the-art generative modelsfor open-domain question answering (odqa)have focused on generating direct answersfrom unstructured textual information.
how-ever, a large amount of world’s knowledgeis stored in structured databases, and need tobe accessed using query languages such assql.
furthermore, query languages can an-swer questions that require complex reason-ing, as well as offering full explainability.
inthis paper, we propose a hybrid frameworkthat takes both textual and tabular evidenceas input and generates either direct answersor sql queries depending on which formcould better answer the question.
the gen-erated sql queries can then be executed onthe associated databases to obtain the ﬁnal an-swers.
to the best of our knowledge, this isthe ﬁrst paper that applies text2sql to odqatasks.
empirically, we demonstrate that onseveral odqa datasets, the hybrid methodsconsistently outperforms the baseline modelsthat only take homogeneous input by a largemargin.
speciﬁcally we achieve state-of-the-art performance on opensquad dataset us-ing a t5-base model.
in a detailed analysis,we demonstrate that the being able to gener-ate structural sql queries can always bringgains, especially for those questions that re-quires complex reasoning..1 introduction.
open-domain question answering (odqa) is atask to answer factoid questions without a pre-speciﬁed domain.
recently, generative models(roberts et al., 2020; lewis et al., 2020; min et al.,2020; izacard and grave, 2020) have achieved thestate-of-the-art performance on many odqa tasks.
these approaches all share the common pipelinewhere the ﬁrst stage is retrieving evidence fromthe free-form text in wikipedia.
however, a largeamount of world’s knowledge is not stored as plain.
text but in structured databases, and need to beaccessed using query languages such as sql.
fur-thermore, query languages can answer questionsthat require complex reasoning, as well as offer-ing full explainability.
in practice, an ideal odqamodel should be able to retrieve evidence from bothunstructured textual and structured tabular informa-tion sources, as some questions are better answeredby tabular evidence from databases.
for example,the current state-of-the-art odqa models struggleon questions that involve aggregation operationssuch as counting or averaging..one line of research on accessing databases, al-though not open domain, is translating natural lan-guage questions into sql queries (zhong et al.,2017; xu et al., 2017; yu et al., 2018c; guo et al.,2019; wang et al., 2018a, 2020; yu et al., 2018a;guo and gao, 2019; choi et al., 2020).
thesemethods all rely on knowing the associated tablefor each question in advance, and hence are not triv-ially applicable to the open-domain setting, wherethe relevant evidence might come from millions oftables..in this paper, we provide a solution to the afore-mentioned problem by empowering the currentgenerative odqa models with the text2sql abil-ity.
more speciﬁcally, we propose a dual reader-parser (durepa) framework that can take bothtextual and tabular data as input, and generate ei-ther direct answers or sql queries based on thecontext1.
if the model chooses to generate a sqlquery, we can then execute the query on the corre-sponding database to get the ﬁnal answer.
overall,our framework consists of three stages: retrieval,joint ranking and dual reading-parsing.
first weretrieve supporting candidates of both textual andtabular types, followed by a joint reranker that pre-dicts how relevant each supporting candidate is to.
1our code is available at https://github.com/.
alexanderyogurt/hybrid-open-qa.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4078–4088august1–6,2021.©2021associationforcomputationallinguistics4078the question, and ﬁnally we use a fusion-in-decodermodel (izacard and grave, 2020) for our reader-parser, which takes all the reranked candidates inaddition to the question to generate direct answersor sql queries..to evaluate the effectiveness of our durepa,we construct a hybrid datasetthat combinessquad (rajpurkar et al., 2016) and wikisql(zhong et al., 2017) questions.
we alsoconduct experiments on naturalquestions (nq)(kwiatkowski et al., 2019) and ott-qa (chenet al., 2020a) to evaluate durepa performance.
astextual and tabular open-domain knowledge, weused textual and tabular data from wikipedia viawikidumps (from dec. 21, 2016) and wikitables(bhagavatula et al., 2015).
we study the modelperformance on different kinds of questions, wheresome of them only need one supporting evidencetype while others need both textual and tabularevidence.
on all question types, durepa per-forms signiﬁcantly better than baseline models thatwere trained on a single evidence type.
we alsodemonstrate that durepa can generate human-interpretable sqls that answer questions requiringcomplex reasoning, such as calculations and su-perlatives..our highlighted contributions are as follows:• we propose a multi-modal framework that in-corporates hybrid knowledge sources with thetext2sql ability for odqa tasks.
to the bestof our knowledge, this is the ﬁrst work that inves-tigates text2sql in the odqa setting..• we propose a simple but effective generative ap-proach that takes both textual and tabular evi-dence and generates either direct answers or sqlqueries, automatically determined by the context.
with that, we achieve the state-of-the-art perfor-mance on opensquad using a t5-base model.
• we conduct comprehensive experiments todemonstrate the beneﬁts of text2sql for odqatasks.
we show that interpretable sql genera-tion can effectively answer questions that requirecomplex reasoning in the odqa setting..roberts et al., 2020; min et al., 2020; lewis et al.,2020; izacard and grave, 2020) that directly gener-ate the answers.
wang et al.
(2018b,c); nogueiraand cho (2019) proposed to rerank the retrievedpassages to get higher top-n recall..table parsing text2sql is a task to translatenatural questions to executable sql queries.
bradet al.
(2017) proposed senlidb dataset whichonly contains 29 tables and lacks annotation intheir training set.
recently, with datasets like wik-isql (zhong et al., 2017), spider (yu et al., 2018c)and cosql (yu et al., 2019) being introduced,many works have shown promising progress onthese dataset (yu et al., 2018b; he et al., 2019;hwang et al., 2019; min et al., 2019; wang et al.,2020; choi et al., 2020; guo et al., 2019; lyu et al.,2020; zhang et al., 2019; zhong et al., 2020; shiet al., 2020).
another line of work proposes toreason over tables without generating logical forms(neelakantan et al., 2015; lu et al., 2016; herziget al., 2020; yin et al., 2020).
however, they areall closed-domain and each question is given theassociated table..hybrid qa chen et al.
(2020a) also proposedan open-domain qa problem with textual and tab-ular evidence.
unlike our problem, they generatean answer directly from the tabular evidence in-stead of generating an sql query.
in addition,they assume some contextual information abouttable is available during retrieval stage (e.g.
theirfusion-retriever is pretrained using hyperlinks be-tween tables and paragraphs), whereas we don’tuse any link information between tables and pas-sages.
moreover, chen et al.
(2020b) proposed aclosed-domain hybrid qa dataset where each tableis linked to on average 44 passages.
different fromours, their purpose is to study multi-hop reasoningover both forms of information, and each questionis still given the associated table..2 related work.
3 method.
open domain question answering odqahas been extensively studied recently including ex-tractive models (chen et al., 2017; clark and gard-ner, 2018; wang et al., 2019; min et al., 2019; yanget al., 2019) that predict spans from evidence pas-sages, and generative models (raffel et al., 2020;.
in this section, we describe our method for hybridopen-domain question answering.
it mainly con-sists of three components: (1) a retrieval system; (2)a joint reranker and (3) a dual seq2seq model thatuses fusion-in-decoder (izacard and grave, 2020)to generate direct answer or sql query..4079figure 1: the pipeline of our proposed hybrid model.
the candidates are retrieved from knowledge source such aswikipedia including both paragraphs and tables.
then a generative seq2seq model reads the question and all thecandidates, and produces k outputs using beam search.
each output can be either a ﬁnal answer or an intermediatesql query.
the types and order of the outputs are automatically determined by the model itself..3.1 retrieval.
for the hybrid open-domain setting, we build twoseparate search indices – one for textual input andanother for tabular input.
for paragraphs, we splitthem into passages of at most 100 words.
for tables,we ﬂattened each table into passages by concate-nating cell values along each row.
if the ﬂattenedtable exceeds 100 words, we split it into a separatepassage, respecting row boundaries.
the columnheaders are concatenated to each tabular passage.
some examples of ﬂattened tables are given in theappendix a.1..given a natural language question, the retrievalsystem retrieves 100 textual and 100 tabular pas-sages as the support candidates from the textual andtabular indices, respectively, using bm25 (robert-son et al., 1995) ranking function..3.2.joint reranking.
the purpose of our reranking model is to producea score si of how relevant a candidate (either an un-structured passage or table) is to a question.
specif-ically, the reranker input is the concatenation ofquestion, a retrieved candidate-content,and its corresponding title if available2, sepa-rated by special tokens shown in figure 1. thecandidate content can be either the unstructured.
2wikipedia passages have page titles, and tables have table.
titles..text or ﬂattened table.
we use bertbase model inthis paper.
following nogueira and cho (2019),we ﬁnetune the bert (devlin et al., 2019) modelusing the following loss:.
l =.
 .
log(si).
 .
log(1.si)..(1).
 .
pos.
neg.
the.
xi2i.
xi2iipos is sampled from all relevant bm25ineg is sampled from allcandidates, and the setnon-relevant bm25 candidates.
different fromnogueira and cho (2019), during training, for eachquestion, we sample 64 candidates including onepositive candidate and 63 negative candidates, that= 63. if none of theis,200 candidates is relevant, we skip the question.
during inference, we use the hybrid reranker toassign a score to each of the 200 candidates, andchoose the top 50 candidates as the input to thenext module – the reader-parser model.
for the top50 candidates, we choose them from the joint poolof all candidates, according to the scores assignedby the reranker..= 1 and.
|ineg|.
|ipos|.
3.3 dual reading-parsing.
our dual reader-parser model is based on the fusion-in-decoder (fid) proposed in izacard and grave(2020), and is initialized using the pretrained t5(raffel et al., 2020) model.
the overall pipelineof the reader-parser is shown in figure 1. each.
4080retrieved candidate is represented by its title andcontent, in the following formats:.
tics of the open-domain qa datasets we use intable 1..textual candidate we represent each textualcandidate as the concatenation of the passage titleand content, appended by special tokens [texttitle] and [text content] respectively..in order to represent a struc-tabular candidatetured table as a passage, we ﬁrst ﬂatten each tableinto the following format: each ﬂattened table startswith the complete header names and then followedby rows.
figure 1 presents an example for thisconversion..finally, a tabular candidate is the concatenationof the table title and content ﬂattened as a passage,appended by special tokens [table title]and [table content] respectively.
we usethe table id as the title so that it can be copied tothe generated sql queries by the model..preﬁx of the targets during training, we alsoadd special tokens answer: or sql: to a tar-geted sentence depending on whether it is a plaintext or a sql query.
for those questions that haveboth textual answer and sql query annotations(for example, wikisql questions), we create twotraining examples for each question.
during infer-ence, the generated outputs will also contain thesetwo special preﬁxes, indicating which output typethe model has generated..dual reader-parser our generative seq2seqmodel has reader-parser duality.
during inference,the model reads the question and all the candidates,and produces k outputs using beam search.
eachoutput can be either a ﬁnal answer or an interme-diate sql query.
depending on the context, thetypes and order of the outputs are automaticallydetermined by the model itself.
all the generatedsql queries will then be executed to produce theﬁnal answers.
in this paper, we ﬁx k = 3 andalways generate three outputs for each question..4 experiments.
in this section, we report the performance of theproposed method on several hybrid open-domainqa datasets..4.1 datasets.
in this section, we describe all the datasets we usein our experiments.
first we summarize the statis-.
dataset.
#train&dev.
#test.
opensquadopennqott-qaopenwikisqlmix-squwikiwikisql-both.
82,59987,92541,46952,026134,625–.
5,0003,6102,2147,76412,7643,029.table 1: statistics of datasets.
opensquad is an open-domain qa datasetconstructed from the original squad-v1.1 (ra-jpurkar et al., 2016), which was designed forthe reading comprehension task, consisting of100,000+ questions posed by annotators on a set ofwikipedia articles, where the answer to each ques-tion is a span from the corresponding paragraph..opennq is an open-domain qa datasets con-structed from the naturalquestions (kwiatkowskiet al., 2019), which was desgined for the end-to-end question answering task.
the questions werefrom real google search queries and the answerswere from wikipedia articles annotated by humans..ott-qa (chen et al., 2020a) is a large-scaleopen table-and-text question answering dataset forevaluating open qa over both tabular and textualdata.
the questions were constructed through “de-contextualization” from hybridqa (chen et al.,2020b) with additional 2200 new questions mainlyused in dev/test set.
ott-qa also provides its owncorpus which contains over 5 million passages andaround 400k tables..openwikisql is an open-domain text2sqlqa dataset constructed from the original wikisql(zhong et al., 2017).
wikisql is a dataset of80,654 annotated questions and sql queries dis-tributed across 24,241 tables from wikipedia..mix-squwikiopenwikisql datasets..is the union of opensquad and.
wikisql-both is a subset of openwikisqlevaluation data that contains the questions that canbe answered by both textual and tabular evidences.
the purpose of this dataset is to study when bothtypes of evidence are possible to answer a ques-tion, whether the hybrid model can still choose thebetter one.
we select these questions in a weakly-supervised way by only keeping a question if the.
4081model.
evidence corpus type.
opensquad opennq ott-qa openwikisql.
fid(t5-base)fid(t5-large)ir+crfr+cruniﬁed model.
text-onlytext-onlytext+table w/o sqltext+table w/o sqltext+nq table w/o sql.
oursfid+fid+durepafid+durepa.
text-onlytable-only w/o sqltable-only with sqltext+table w/o sqltext+table with sql.
53.456.7---.
56.42.52.756.457.0.
48.251.4--54.64.
45.214.314.846.748.0.
--14.428.13-.
14.54.14.715.015.8.
-----.
13.930.340.230.942.6.table 2: comparison to the state-of-the-art on open-domain qa datasets.
the numbers reported are in em metric.
fid(t5-base & t5-large) is reported from (izacard and grave, 2020), ir+cr (iterative retrieval+cross-blockreader) and fr+cr (fusion retrieval+cross-block reader) are from (chen et al., 2020a), uniﬁed model is from(oguz et al., 2020).
comparing durepa with fid+ , we observe that having the ability to generate structuralqueries is always beneﬁcial even for questions with mostly extractive answers like squad and nq..groundtruth answer is contained in both textual andtabular bm25 candidates.
for example in figure1, the answer “richard marquand” can be found inboth types of passages.
we ﬁlter out some trivialcases where the answer shows up in more than halfof the candidates.
5.wikipedia passages and tables for the textualevidences, we process the wikipedia 2016 dumpand split the articles into overlapping passages of100 words following (wang et al., 2019).
to createthe tabular evidences, we combine 1.6m wikipediatables (bhagavatula et al., 2015) and all the 24,241wikisql tables, and ﬂatten and split each tableinto passages not exceeding 100 words, in the sameformat mentioned in the previous section.
we usethese two collections as the evidence sources forall the qa datasets except for ott-qa, where weuse its own textual and tabular collections..4.2.implementation details.
retriever and reranker.
we conduct bm25 re-trieval using elasticsearch 7.7 6 with the defaultsettings.
and we use a bert reranker initializedwith pretrained bert-base-uncased model..dual reader and parser with fusion-in-decoder.
similar to (izacard and grave, 2020), we initial-ize the fusion-in-decoders with the pretrained t5model (raffel et al., 2020).
we only explore t5-base model in this paper, which has 220m parame-ters..5for example, some numerical number like ”1” is a very.
common substring and shows up in most of the candidates..6https://www.elastic.co/.
for both reranker and fid models, we use adamoptimizer (kingma and ba, 2014) with a maximum4 and a dropout rate of 10%.
learning rate of 10 4 andthe learning rate linearly warms up to 10 then linearly anneals to zero.
we train models for10k gradient steps with a batch size of 32, and savea checkpoint every 1k steps.
for the fid model,when there are multiple answers for one question,we randomly sample one answer from the list.
forthe fid model, during inference, we generate 3answers for each question using beam search withbeam size 3..4.3 main results.
we present the end-to-end results on the open-domain qa task comparing with the baseline meth-ods as show in table 2..we build models with 5 different settings basedon the source evidence modality as well as the for-mat of model prediction.
speciﬁcally, we considersingle modality settings with only textual evidenceor tabular evidence and the hybrid setting with bothtextual and tabular evidence available.
for tabularevidence, the models either predict direct answertext or generate structure sql queries.
note wealso consider a baseline model, fid+ , a fid modelthat only generates direct answer text, but can makeuse of both textual and tabular evidence..3chen et al.
(2020a) uses a fusion-retriever to retrievedtable-passages blocks as evidences.
to construct the fusionblocks, they train a gpt-2 model using extra hyperlink infor-mation to link table cell to passages.
in contrast, we do notuse any hyperlink information..4oguz et al.
(2020) uses tables provided by nq trainingdata (less than 500k in total), whereas we use all the tablesextracted from wikipedia dumps (around 1.6m in total)..4082indexr@1r@10r@25r@50r@100.bm25textual34.4059.3865.9272.1676.50.rerankertextual69.7680.3081.6482.5083.44.bm25tabular1.606.348.8412.3615.04.rerankertabular10.1618.8821.2022.6223.72.rerankerhybrid69.9280.9082.4283.2684.10.table 3: recalls on top-k textual, tabular or the hybrid candidates for squad questions.
the recalls on hybridinputs are almost the same as or even better than the best recalls on individual textual or tabular inputs, meaningthat the reranker is able to jointly rank both types of candidates and provide better evidences to the next component– the reader-parser..first, in the single modality setting, we ob-serve that for opensquad, opennq and ott-qadatasets, textual qa model is performing signif-icantly better than tabular qa models, while foropenwikisql, it is the opposite.
this is expecteddue to the nature of the construction process ofthose datasets.
in the hybrid setting, the hybridmodels outperform single modality models con-sistently across all these datasets.
this indicateshybrid models are more robust and ﬂexible whendealing with questions of various types in practice..comparing durepa with fid+ , we observethat having the ability to generate structural queriesis always beneﬁcial even for extractive questionslike squad and nq.
and for wikisql-type ques-tions, the gain of sql generation is signiﬁcant..on opensquad dataset, our durepa modelusing hybrid evidences achieves a new state-of-the-art em score of 57.0. it is worth noting thatthe previous best score was attained by fid us-ing t5-large model, while our model is using t5-base, which has much fewer parameters.
on nqdataset, fid+ with text-only evidences has lowerem score compared with fid-base, despite hav-ing the same underlying model and inputs.
wesuspect that this is because (1) we truncate all pas-sages into at most 150 word pieces while in fidpaper they keep 250 word pieces, so the actual in-put (top-100 passages) to our fid model is muchless than that in the fid paper; and (2) we usebm25 to retrieve the initial pool of candidates in-stead of trained embedding-based neural retrievalmodel(karpukhin et al., 2020; izacard and grave,2020).
nevertheless, the durepa model with hy-brid evidences still improve the em by 2.8 pointscompared to fid+ using only text inputs.
on ott-qa questions, our full model also outperformsthe ir+cr baseline by 1.4 points.
the fr+crmodel is using a different setting where they usehyperlinks between tables and passages to train the.
fusion-retriever (fr), so the result is not directlycomparable to ours.
we provide more analysison ott-qa in the appendix.
on openwikisqldataset, enabling sql generation brings more than10 points improvement on the em scores.
this isbecause many questions therein require complexreasoning like count, average or sum on thetable evidences.
we provide more in-depth analysisin section 5.2 including some complex reasoningexamples in table 7..5 analysis.
5.1 retrieval and reranking performance.
in this section, we investigate the performance ofthe bm25 retriever and the bert reranker usingtop-k recalls as our evaluation metric..during both training and inference, for eachquestion,the textual and tabular passages arereranked jointly using a single reranker.
on themix-squwiki dataset, we report the reranking re-sults on squad questions in table 3. the resulton wikisql questions is in table 9 in appendix.
to provide better insights on the reranker’s per-formance, we show the top-k recalls on textual,tabular and hybrid evidences separately..from table 3, on both textual and tabular can-didates, recall@25 of the ranker is even higherthan recall@100 of the bm25 retriever.
this sug-gest that during inference, instead of providing 100bm25 candidates to the fusion-in-decoder (fid),only 25 reranked candidates would sufﬁce..in table 9 and 10 in appendix, we observe sim-ilar trend with top-25 recalls comparable to top-100 recalls on both wikisql and nq questions.
finally, across all datasets, the recalls on hybridinputs are almost the same as or even better thanthe best recalls on individual textual or tabular in-puts, meaning that the reranker is able to jointlyrank both types of candidates and provide better.
4083evidences to the next component – the dual reader-parser..5.2 performance of the reader-parser.
in this section, we discuss the performance of thedual reader-parser on different kinds of questions..sql prediction helps with complex reasoning.
in table 4, we compare the top-1 em execution ac-curacy of durepa and fid+ on openwikisql.
ifdurepa generated a sql, we execute the sql toobtain its answer prediction.
if the ground-truth an-swer is a list (e.g., what are the names of simpsonsepisodes aired in 2008?
), we use set-equivalence toevaluate accuracy.
durepa outperforms fid+ onthe test set in most of the settings.
we also comparetheir performance under a breakdown of differentcategories based on the ground-truth sql query.
durepa achieved close to 3x and 5x improve-ments on wikisql questions that have superlative(max/min) and calculation (sum/avg) opera-tions, respectively.
for count queries, fid+often predicted either 0 or 1. thus, these resultssupport our hypothesis that the sql generationhelps in complex reasoning and explainability fortabular question answering..}.
all0,1count2{2count min/maxsum/avgcomparison (< or >)and-conditionanswer is a listdirect answers.
durepa47.178.044.426.622.645.853.034.378.7.fid+29.382.90.09.34.732.031.80.075.6.
#test776477096543149392045160933.table 4: comparison of durepa and fid+ on open-wikisql dataset.
we compare their accuracy un-der a breakdown of different categories based on theground-truth sql query.
“direct answers” stands forthe questions that durepa predicts direct answers.
durepa signiﬁcantly outperforms on questions thatrequire complex reasoning such as superlatives and cal-culations..using hybrid evidence types leads to better per-formance.
shown in table 5 is the model per-formance on the mix-squwiki questions.
as thebaseline models, if we only use a single evidencetype, the best top-1 em is 34.0, achieved by themodel fid+ using only textual candidates.
how-ever, if we use both evidence types, the hybridmodel durepa attains a signiﬁcantly better top-.
1 em of 47.9, which implies that including bothtextual and tabular evidences leads a better modelperformance on mix-squwiki.
furthermore, weobserve that the model durepa has a better top-1em compared to fid+, suggesting that the answersfor some of these questions need to be obtained byexecuting sql queries instead of generated directly.
in table 7, we samples some questions on whichthe model durepa predicts the correct answersbut the model fid+ fails..what if the questions can be answered by bothtextual and tabular evidences?
table 6 showsthe model performance on wikisql-both dataset.
recall that all these questions in the dataset can beanswered by both type of evidence.
first of all, thedurepa model using tabular evidences behavesbetter than the fid+ model using textual evidences.
this implies on wikisql questions, using tabularinformation leads to better answers.
next, whenusing only one type of evidence, both durepaand fid+ models behave signiﬁcantly worse thantheir hybrid counterparts.
this indicates that thehybrid model can again ﬁgure out which evidencetype should be used to provide the correct ﬁnalanswer..6 discussion and future work.
our experiments consistently show that the pro-posed framework durepa brings signiﬁcant im-provement on answering questions using hybridtypes of evidence.
especially on the questions thatcan be answered by both supporting evidence types,our multi-modal method still shows clear advantageover models using single-type knowledge, imply-ing that our approach could ﬁgure out the mostrelevant evidence to answer a question.
we alsodemonstrate that the dual reader-parser is essentialto the good performance of durepa; the ability ofgenerating both direct answers and structural sqlqueries help durepa perform much better thanfid+ and other baselines on questions that requirecomplex reasoning like counting or averaging..we believe that our methods can be improved intwo aspects.
first, our general framework fig.
1can be improved by a better retrieval system.
forexample, instead of using bm25, we can use morepowerful neural retrieval models (karpukhin et al.,2020).
on the hybrid evidence, one can also use anentity linking module to link the entities betweenthe tables and passages (chen et al., 2020a) andutilize the structure information for better multi-.
4084model.
evidence corpus type.
% of sqlanswers.
acc of sqlanswers (%).
% of directanswers.
acc of directanswers (%).
em (overall).
table 5: detailed results on mix-squwiki dataset under various settings..fid+fid+durepafid+durepa.
text-onlytable-only w/o sqltable-only with sqltext+table w/o sqltext+table with sql.
fid+fid+durepafid+durepa.
text-onlytable-only w/o sqltable-only with sqltext+table w/o sqltext+table with sql.
0.00.053.90.033.5.
0.00.038.60.039.8.
--42.5-44.1.
--30.4-35.5.
100.0100.046.1100.066.5.
100.0100.061.4100.060.2.
34.019.38.440.049.8.
38.738.457.243.264.0.
34.019.326.840.047.9.
38.738.446.843.253.6.model.
evidence corpus type.
% of sqlanswers.
acc of sqlanswers (%).
% of directanswers.
acc of directanswers (%).
em (overall).
table 6: model performance on wikisql-both dataset.
the models are trained on mix-squwiki training data..question:groundtruth:top-1 generation by durepa:.
execution result:top-1 generation by durepa–question:.
groundtruth:top-1 generation by durepa:.
execution result:top-1 generation by durepa–analysis:question:.
groundtruth:top-1 generation by durepa:.
execution result:top-1 generation by durepa–analysis:question:groundtruth:top-1 generation by durepa:.
execution result:top-1 generation by durepa–analysis:question:groundtruth:top-1 generation by durepa:.
execution result:top-1 generation by durepa–analysis:.
select condition from table 1-14006-1 where partial.
select party from table 1-1342218-17 where district.
which party won in the election in voting district kentucky 5?
[’democratic’]sql:= "kentucky 5"[’democratic’]answer: republicanwhich condition has an unaffected partial thromboplastin time, platelet count, and aprothrombin time?
[’aspirin’, ’uremia’, ”glanzmann’s thrombasthenia”]sql:thromboplastin time = "unaffected" and platelet count ="unaffected" and prothrombin time = "unaffected"[’uremia’, ”glanzmann’s thrombasthenia”, ’aspirin’]answer: vitamin k deﬁciency or warfarinanswer is a list of medical conditionshow many wins have goals against smaller than 30, and goals for larger than 25, anddraws larger than 5?
[’3’]sql:goals against < 30 and goals for > 25 and draws > 5[3]answer: 0count operationwhat is the highest rd that tom sneva had the pole position in?
[’7’]sql:position = "tom sneva"[7]answer: 2.0max operationname the average erp w and call sign of w237br[110]sql:sign = "w237br"[110]answer: 1.0avg calculation.
select avg(erp w) from table 2-14208614-1 where call.
select max(rd) from table 1-10706961-2 where pole.
select count(wins) from table 2-18017970-2 where.
table 7: examples of the squwiki and openwikisql questions that are answered correctly by model durepabut incorrectly by model fid+..hop reasoning.
second, as we have demonstrated,having the ability of generating structural sql.
queries is a very powerful and necessary featurefor answering questions that require complex rea-.
4085soning.
given the limited text2sql data and thedifﬁculty of obtaining such sql supervision, twointeresting future work include (1) getting sql an-notations more efﬁciently and (2) adapting weakly-supervised approaches like discrete em (min et al.,2019) for model training..references.
chandra sekhar bhagavatula, thanapon noraset, anddoug downey.
2015. tabel: entity linking in webin international semantic web conference,tables.
pages 425–441.
springer..florin brad, radu iacob, ionel hosu, and traian rebe-dea.
2017. dataset for a neural natural languagearxiv preprintinterface for databases (nnlidb).
arxiv:1707.03172..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-domain questions.
in 55th annual meeting of the as-sociation for computational linguistics, acl 2017,pages 1870–1879.
association for computationallinguistics (acl)..wenhu chen, ming-wei chang, eva schlinger,william wang, and william w cohen.
2020a.
openarxivquestion answering over tables and text.
preprint arxiv:2010.10439..wenhu chen, hanwen zha, zhiyu chen, wenhanxiong, hong wang, and william yang wang.
2020b.
hybridqa: a dataset of multi-hop question answer-ing over tabular and textual data.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing: findings, pages 1026–1036..donghyun choi, myeong cheol shin, eunggyun kim,and dong ryeol shin.
2020. ryansql: recursivelyapplying sketch-based slot ﬁllings for complex text-arxiv preprintto-sql in cross-domain databases.
arxiv:2004.03125..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 845–855..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..jiaqi guo, zecheng zhan, yan gao, yan xiao,jian-guang lou, ting liu, and dongmei zhang.
2019. towards complex text-to-sql in cross-domain.
in pro-database with intermediate representation.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4524–4535..tong guo and huilin gao.
2019. content enhancedarxiv preprint.
bert-based text-to-sql generation.
arxiv:1910.07179..pengcheng he, yi mao, kaushik chakrabarti, andreinforce schemaarxiv preprint.
weizhu chen.
2019. x-sql:representation with context.
arxiv:1908.08113..jonathan herzig, pawel krzysztof nowak, thomasmueller, francesco piccinno, and julian eisensch-los.
2020. tapas: weakly supervised table parsingvia pre-training.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 4320–4333..wonseok hwang, jinyeong yim, seunghyun park, andminjoon seo.
2019. a comprehensive explorationon wikisql with table-aware word contextualization.
arxiv preprint arxiv:1902.01069..gautier izacard and edouard grave.
2020. lever-aging passage retrieval with generative models foropen domain question answering.
arxiv preprintarxiv:2007.01282..vladimir karpukhin, barlas o˘guz, sewon min, ledelland wen-wu, sergey edunov, danqi chen,fortau yih.
2020.open-domain question answering.
arxiv preprintarxiv:2004.04906..dense passage retrieval.
diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, jacob devlin,kenton lee, et al.
2019. natural questions: a bench-mark for question answering research.
transactionsof the association for computational linguistics,7:453–466..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich k¨uttler, mike lewis, wen-tau yih, timrockt¨aschel, et al.
2020. retrieval-augmented gen-arxiveration for knowledge-intensive nlp tasks.
preprint arxiv:2005.11401..zhengdong lu, hang li, and ben kao.
2016. neu-ral enquirer: learning to query tables in natural lan-guage.
ieee data eng.
bull., 39(3):63–73..qin lyu, kaushik chakrabarti, shobhit hathi, souvikkundu, jianwen zhang, and zheng chen.
2020. hy-brid ranking network for text-to-sql.
arxiv preprintarxiv:2008.04759..4086sewon min, danqi chen, hannaneh hajishirzi, andluke zettlemoyer.
2019. a discrete hard em ap-proach for weakly supervised question answering.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2844–2857..sewon min, julian michael, hannaneh hajishirzi, andluke zettlemoyer.
2020. ambigqa: answering am-biguous open-domain questions.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 5783–5797..arvind neelakantan, quoc v le, and ilya sutskever.
inducing latent pro-arxiv preprint.
2015. neural programmer:grams with gradient descent.
arxiv:1511.04834..rodrigo nogueira and kyunghyun cho.
2019. pas-arxiv preprint.
sage re-ranking with bert.
arxiv:1901.04085..barlas oguz, xilun chen, vladimir karpukhin,stan peshterliev, dmytro okhonko, michaelschlichtkrull, sonal gupta, yashar mehdad, andscott yih.
2020.uniﬁed open-domain ques-tion answering with structured and unstructuredknowledge.
arxiv preprint arxiv:2012.14610..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21:1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392..adam roberts, colin raffel, and noam shazeer.
2020.how much knowledge can you pack into the param-in proceedings of theeters of a language model?
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 5418–5426..stephen e robertson, steve walker, susan jones,micheline m hancock-beaulieu, mike gatford, et al.
1995. okapi at trec-3.
nist special publication sp,109:109..peng shi, patrick ng, zhiguo wang, henghuizhu, alexander hanbo li,jun wang, ciceronogueira dos santos, and bing xiang.
2020. learn-ing contextual representations for semantic pars-ing with generation-augmented pre-training.
arxivpreprint arxiv:2012.10309..bailin wang, richard shin, xiaodong liu, oleksandrpolozov, and matthew richardson.
2020. rat-sql:relation-aware schema encoding and linking for.
in proceedings of the 58th an-text-to-sql parsers.
nual meeting of the association for computationallinguistics, pages 7567–7578..chenglong wang,.
kedar.
tatwawadi, marcbrockschmidt, po-sen huang, yi mao, olek-sandr polozov, and rishabh singh.
2018a.
robusttext-to-sqlexecution-guidedgeneration withdecoding.
arxiv preprint arxiv:1807.03100..shuohang wang, mo yu, xiaoxiao guo, zhiguo wang,tim klinger, wei zhang, shiyu chang, gerrytesauro, bowen zhou, and jing jiang.
2018b.
r 3:reinforced ranker-reader for open-domain questionanswering.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 32..shuohang wang, mo yu, jing jiang, wei zhang, xiaox-iao guo, shiyu chang, zhiguo wang, tim klinger,gerald tesauro, and murray campbell.
2018c.
ev-idence aggregation for answer re-ranking in open-in international con-domain question answering.
ference on learning representations..zhiguo wang, patrick ng, xiaofei ma, ramesh nal-lapati, and bing xiang.
2019. multi-passagebert: a globally normalized bert model for open-domain question answering.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5881–5885..xiaojun xu, chang liu, and dawn song.
2017. sqlnet:generating structured queries from natural languagearxiv preprintwithout reinforcementarxiv:1711.04436..learning..wei yang, yuqing xie, aileen lin, xingyu li, luchentan, kun xiong, ming li, and jimmy lin.
2019.end-to-end open-domain question answering withbertserini.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics (demonstrations), pages72–77..pengcheng yin, graham neubig, wen-tau yih, and se-bastian riedel.
2020. tabert: pretraining for jointin pro-understanding of textual and tabular data.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8413–8426..tao yu, zifan li, zilin zhang, rui zhang, andtypesql: knowledge-dragomir radev.
2018a.
inbased type-aware neural text-to-sql generation.
proceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 2 (short papers), pages 588–594..tao yu, michihiro yasunaga, kai yang, rui zhang,dongxu wang, zifan li, and dragomir radev.
2018b.
syntaxsqlnet: syntax tree networks for com-plex and cross-domain text-to-sql task.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1653–1663..4087tao yu, rui zhang, heyang er, suyi li, eric xue,bo pang, xi victoria lin, yi chern tan, tianzeshi, zihan li, et al.
2019. cosql: a conversationaltext-to-sql challenge towards cross-domain naturallanguage interfaces to databases.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1962–1979..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma, irene li, qingn-ing yao, shanelle roman, et al.
2018c.
spider: alarge-scale human-labeled dataset for complex andcross-domain semantic parsing and text-to-sql task.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages3911–3921..rui zhang, tao yu, heyang er, sungrok shim,eric xue, xi victoria lin, tianze shi, caimingxiong, richard socher, and dragomir radev.
2019.editing-based sql query generation for cross-domaincontext-dependent questions.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5341–5352..victor zhong, mike lewis, sida i wang, and lukezettlemoyer.
2020. grounded adaptation for zero-shot executable semantic parsing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6869–6882..victor zhong, caiming xiong, and richard socher.
2017.seq2sql: generating structured queriesfrom natural language using reinforcement learning.
corr, abs/1709.00103..4088