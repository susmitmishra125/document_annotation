ruddit: norms of offensiveness for english reddit comments.
rishav hada1,∗, sohi sudhir1,∗, pushkar mishra2, helen yannakoudakis3,saif m. mohammad4, ekaterina shutova11illc, university of amsterdam2facebook ai, london3dept.
of informatics, king’s college london4national research council canadarishavhada@gmail.com, sohigre@gmail.com, pushkarmishra@fb.com,.
helen.yannakoudakis@kcl.ac.uk, saif.mohammad@nrc-cnrc.gc.ca, e.shutova@uva.nl.
abstract.
warning: this paper contains comments thatmay be offensive or upsetting..on social media platforms, hateful and of-fensive language negatively impact the men-tal well-being of users and the participationof people from diverse backgrounds.
auto-matic methods to detect offensive languagehave largely relied on datasets with categoricallabels.
however, comments can vary in theirdegree of offensiveness.
we create the ﬁrstdataset of english language reddit commentsthat has ﬁne-grained, real-valued scores be-tween -1 (maximally supportive) and 1 (max-imally offensive).
the dataset was annotatedusing best–worst scaling, a form of compara-tive annotation that has been shown to allevi-ate known biases of using rating scales.
weshow that the method produces highly reliableoffensiveness scores.
finally, we evaluate theability of widely-used neural models to predictoffensiveness scores on this new dataset..1.introduction.
social media platforms serve as a medium for ex-change of ideas on a range of topics, from the per-sonal to the political.
this exchange can, however,be disrupted by offensive or hateful language.
suchlanguage is pervasive online (statista, 2020b), andexposure to it may have numerous negative con-sequences for the victim’s mental health (munro,2011).
automated offensive language detection hasthus been gaining interest in the nlp community,as a promising direction to better understand thenature and spread of such content..there are several challenges in the automaticdetection of offensive language (wiedemann et al.,2018).
the nlp community has adopted variousdeﬁnitions for offensive language, classifying itinto speciﬁc categories.
for example, waseem and.
∗both authors contributed equally..hovy (2016) classiﬁed comments as racist, sex-ist, neither; davidson et al.
(2017) as hate-speech,offensive but not hate-speech, neither offensivenor hate-speech and founta et al.
(2018) as abu-sive, hateful, normal, spam.
schmidt and wiegand(2017); fortuna and nunes (2018); mishra et al.
(2019); kiritchenko and nejadgholi (2020) summa-rize the different deﬁnitions.
however, these cat-egories have signiﬁcant overlaps with each other,creating ill-deﬁned boundaries, thus introducingambiguity and annotation inconsistency (fountaet al., 2018).
a further challenge is that after en-countering several highly offensive comments, anannotator might ﬁnd subsequent moderately offen-sive comments to not be offensive (de-sensitization)(kurrek et al., 2020; soral et al., 2018)..at the same time, existing approaches do nottake into account that comments can be offensiveto a different degree.
knowing the degree of offen-siveness of a comment has practical implications,when taking action against inappropriate behaviouronline, as it allows for a more ﬁne-grained analysisand prioritization in moderation..the representation of the offensive class in adataset is often boosted using different strategies.
the most common strategy used is key-word basedsampling.
this results in datasets that are rich inexplicit offensive language (language that is un-ambiguous in its potential to be offensive, such asthose using slurs or swear words (waseem et al.,2017)) but lack cases of implicit offensive lan-guage (language with its true offensive nature ob-scured due to lack of unambiguous swear words,usage of sarcasm or offensive analogies, and oth-ers (waseem et al., 2017; wiegand et al., 2021))(waseem, 2016; wiegand et al., 2019).
further,wiegand et al.
(2019) show that key-word basedsampling often results in spurious correlations (e.g.,sports-related expressions such as announcer andsport occur very frequently in offensive tweets)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2700–2717august1–6,2021.©2021associationforcomputationallinguistics2700lastly, existing datasets consider offensive com-ments in isolation from the wider conversation ofwhich they are a part.
offensive language is, how-ever, inherently a social phenomenon and its analy-sis has much to gain from taking the conversationalcontext into account (gao and huang, 2017)..in this paper, we present the ﬁrst dataset of 6000english language reddit comments that has ﬁne-grained, real-valued scores between -1 (maximallysupportive) and 1 (maximally offensive) – norma-tive offensiveness ratings for the comments.
for theﬁrst time, we use comparative annotations to detectoffensive language.
in its simplest form, compara-tive annotations involve giving the annotators twoinstances at a time, and asking which exhibits theproperty of interest to a greater extent.
this allevi-ates several annotation biases present in standardrating scales, such as scale-region bias (presser andschuman, 1996; asaadi et al., 2019), and improvesannotation consistency (kiritchenko and moham-mad, 2017).
however, instead of needing to anno-tate n instances, one now needs to annotate n 2instance pairs—which can be prohibitive.
thus,we annotate our dataset using an efﬁcient form ofcomparative annotation called best–worst scaling(bws) (louviere, 1991; louviere et al., 2015; kir-itchenko and mohammad, 2016, 2017)..by eliminating different offensiveness cate-gories, treating offensiveness as a continuous di-mension, and eliciting comparative judgments fromthe annotators (based on their understanding ofwhat is offensive), we alleviate the issues regardingcategory deﬁnitions and arbitrary category bound-aries discussed earlier.
by obtaining real-valuedoffensiveness scores, different thresholds can beused in downstream applications to handle varyingdegrees of offensiveness appropriately.
by fram-ing the task as a comparative annotation task, weobtain consistent and reliable annotations.
we alsogreatly mitigate issues of annotator de-sensitizationas one will still be able to recognize if one com-ment is more offensive than another, even if theythink both comments are not that offensive..in contrast to existing resources, which provideannotations for individual comments, our datasetincludes conversational context for each comment(i.e.
the reddit thread in which the comment oc-curred).
we conduct quantitative and qualitativeanalyses of the dataset to obtain insights into howemotions, identity terms, swear words, are relatedto offensiveness.
finally, we benchmark several.
widely-used neural models in their ability to predictoffensiveness scores on this new dataset.1.
2 related work.
2.1 offensive language datasets.
surveys by schmidt and wiegand (2017); fortunaand nunes (2018); mishra et al.
(2019); vidgen andderczynski (2020) discuss various existing datasetsand their compositions in detail.
waseem and hovy(2016); davidson et al.
(2017); founta et al.
(2018)created datasets based on twitter data.
due toprevalence of the non-offensive class in naturally-occurring data (waseem, 2016; founta et al., 2018),the authors devised techniques to boost the pres-ence of the offensive class in the dataset.
waseemand hovy (2016) used terms frequently occurringin offensive tweets, while davidson et al.
(2017)used a list of hate-related terms to extract offen-sive tweets from the twitter search api.
park et al.
(2018), wiegand et al.
(2019), and davidson et al.
(2019) show that the waseem and hovy (2016)dataset exhibits topic bias and author bias due to theemployed sampling strategy.
founta et al.
(2018)boosted the representation of offensive class in theirdataset by analysing the sentiment of the tweets andchecking for the presence of offensive terms.
in ourwork, we employ a hybrid approach, selecting ourdata in three ways: speciﬁc topics, emotion-relatedkey-words, and random sampling..past work has partitioned offensive com-ments into explicitly offensive (those that includeprofanity—swear words, taboo words, or hateterms) and implicitly offensive (those that do not in-clude profanity) (waseem et al., 2017; caselli et al.,2020a; wiegand et al., 2021).
some other past workhas deﬁned explicitly and implicitly offensive in-stances a little differently: sap et al.
(2020) consid-ered factors such as obviousness, intent to offendand biased implications, breitfeller et al.
(2019)considered factors such as the context and the per-son annotating the instance, and razo and k¨ubler(2020) considered the kind of lexicon used.
re-gardless of the exact deﬁnition, implicit offensivelanguage, due to a lack of lexical cues, is harder toclassify not only for computational models, but alsofor humans.
in our work, we consider implicitlyoffensive comments as those offensive commentsthat do not contain any swear words..1dataset and code available at:.
https://github.com/hadarishav/ruddit..2701wulczyn et al.
(2016, 2017) created three differ-ent datasets from wikipedia talk pages, focusingon aggression, personal attacks and toxicity.
thecomments were sampled at random from a largedump of english wikipedia, and boosted by includ-ing comments from blocked users.
for the personalattacks dataset, wulczyn et al.
(2016) used two dif-ferent kinds of labels: ed (empirical distribution),oh (one hot).
in case of ed, the comments wereassigned real-valued scores between 0 and 1 repre-senting the fraction of annotators who consideredthe comment a personal attack.
while these labelswere introduced to create a separation between thenature of comments with a score of 1.0 and thosewith a score of 0.6 (which would otherwise be clas-siﬁed as attacks), they are discrete.
in our work,using the bws comparative annotation setup, weassign ﬁne-grained continuous scores to commentsto denote their degree of offensiveness..2.2 best–worst scaling (bws).
bws was proposed by louviere (1991).
kir-itchenko and mohammad (2017) have experimen-tally shown that bws produces more reliable ﬁne-grained scores than the scores acquired utilizingrating scales.
in the bws annotation setup, theannotators are given an n-tuple (where n > 1, andcommonly n = 4), and asked which item is thebest and which is the worst (best and worst corre-spond to the highest and the lowest with respectto a property of interest).
best–worst annotationsare particularly efﬁcient when using 4-tuples, aseach annotation results in inequalities for 5 of the6 item pairs.
for example, a 4-tuple with itemsa, b, c, and d, where a is the best, and d is theworst, results in inequalities: a>b, a>c, a>d,b>d, and c>d.
real-valued scores of associa-tions are calculated between the items and the prop-erty of interest from the best–worst annotations fora set of 4-tuples (orme, 2009; flynn and marley,2014).
the scores can be used to rank items bythe degree of association with the property of in-terest.
within the nlp community, bws has thusfar been used only for creating datasets for rela-tional similarity (jurgens et al., 2012), word-sensedisambiguation (jurgens, 2013), word–sentimentintensity (kiritchenko et al., 2014), phrase senti-ment composition (kiritchenko and mohammad,2016), and tweet-emotion intensity (mohammadand bravo-marquez, 2017; mohammad and kir-itchenko, 2018).
using bws, we create the ﬁrst.
dataset with degree of offensiveness scores for so-cial media comments..3 data collection and sampling.
we extracted reddit data from the pushshiftrepository (baumgartner et al., 2020) using googlebigquery.
reddit is a social news aggregation,web content rating, and discussion website.
itcontains forums called subreddits dedicated tospeciﬁc topics.
users can make a post on thesubreddit to start a discussion.
users can commenton existing posts or comments to participatein the discussion.
as users can also reply to acomment, the entire discussion has a hierarchicalstructure called the comment thread.
we dividedthe extracted comments into 3 categories based ontheir subreddit source:1. topics (50%): contains comments fromtopic-focused subreddits: askmen, askreddit,twoxchromosomes, vaxxhappened, worldnews,worldpolitics.
these subreddits were chosento cover a diverse range of topics.
askreddit,vaxxhappened, worldnews, worldpolitics dis-cuss generic themes.
twoxchromosomes con-tains women’s perspectives on various topicsand askmen contains men’s perspectives..2. changemyview (cmv) (25%): the cmvsubreddit (with over a million users) has postsand comments on controversial topics..3. random (25%): contains comments from ran-.
dom subreddits..we selected 808 posts from the subreddits based oncriteria such as date, thread length, and post length.
(further details in the appendix a.1.)
we took theﬁrst 25 and the last 25 comments per post (skippingcomments that had [deleted] or [removed] ascomment body).
the ﬁrst responses are likely tobe most relevant to the post.
the ﬁnal commentsindicate how the discussion ended.
we sampled6000 comments from this set for annotation..the goal of the sampling was to increase theproportion of offensive and emotional comments.
emotions are highly representative of one’s mentalstate, which in turn are associated with their be-haviour (poria et al., 2019).
for example, jay andjanschewitz (2008) show that people tend to swearwhen they are angry, frustrated or anxious..studies have shown that the primary dimen-sions of emotion are valence, arousal, and dom-inance (vad) (osgood et al., 1957; russell, 1980,.
27022003).
valence is the positive–negative or pleasure–displeasure dimension.
arousal is the excited–calm or active–passive dimension.
dominanceis powerful–weak or ‘have full control’–‘have nocontrol’ dimension (mohammad, 2018).
to boostthe representation of offensive and emotional com-ments in our dataset, we up-sampled commentsthat included low-valence (highly negative) wordsand those that included high-arousal words (as perthe nrc vad lexicon (mohammad, 2018)).
themanually constructed nrc vad lexicon includes20,000 english words, each with a real-valuedscore between 0 and 1 in the v, a, d dimensions.
in order to do this upsampling, we ﬁrst deﬁnedthe valence score of each comment as the averagevalence score of the negative words within the com-ment (a negative word is deﬁned as a word with avalence score less than 0.25 in the vad lexicon.).
similarly, we deﬁned the arousal score for a com-ment as the average arousal score of high-arousalwords in each comment (a high-arousal word isdeﬁned as a word with an arousal score greater than0.75.)2.
we selected comments from the comment poolsuch that 50% of the comments were from the top-ics category, 25% of the comments from the cmvcategory and 25% of the comments from the ran-dom category.
within each category, 33% of thecomments were those that had the lowest valencescores, 33% of the comments were those that hadthe highest arousal scores, and the remaining werechosen at random..4 annotation.
the perception of ‘offensiveness’ of a commentcan vary from person to person.
therefore, weused crowdsourcing to annotate our data.
crowd-sourcing helps us get an aggregation of varied per-spectives rather than expert opinions which canleave out offensiveness in a comment that lies out-side the ‘typical’ offensiveness norms (blackwellet al., 2017).
we carried out all the annotation taskson amazon mechanical turk (amt).
due to thestrong language, an adult content warning was is-sued for the task.
reddit is most popular in theus, which accounts for 50% of its desktop trafﬁc(statista, 2020a).
therefore, we restricted annota-tors to those residing in the us.
to maintain the.
2in some initial pilot experiments, we found this approachof sampling low valence and high arousal comments to resultin the highest number of offensive comments..quality of annotations, only annotators with highapproval rate were allowed to participate..4.1 annotation with best–worst scaling.
we followed the procedure described in kir-itchenko and mohammad (2016) to obtain bwsannotations.
annotators were presented with 4comments (4-tuple) at a time and asked to selectthe comment that is most offensive (least support-ive) and the comment that is least offensive (mostsupportive).
we randomly generated 2n distinct4-tuples (where n is the number of comments inthe dataset), such that each comment was seen ineight different 4-tuples and no two 4-tuples hadmore than 2 items in common.
we used the scriptprovided by kiritchenko and mohammad (2016)to obtain the 4-tuples to be annotated.3.
kiritchenko and mohammad (2016) show that ina word-level sentiment task, using just three anno-tations per 4-tuple produces highly reliable results.
however, since we work with long comments anda relatively more difﬁcult task, we got each tupleannotated by 6 annotators.
since each comment isseen in 8 different 4-tuples, we obtain 8 x 6 = 48judgements per comment..4.2 annotation task and process.
in our instructions to the annotators, we deﬁnedoffensive language as comments that include butare not limited to [being hurtful (with or withoutthe usage of abusive words)/ being intentionallyharmful/ treating someone improperly/ harmingthe ‘self-concept’ of another person/ aggressiveoutbursts/ name calling/ showing anger and hostil-ity/ bullying/ hurtful sarcasm].
we also encouragedthe annotators to follow their instincts.
by framingthe task in terms of comparisons and providing abroad deﬁnition of offensiveness, we avoided in-troducing artiﬁcial categories and elicit responsesguided by their intuition of the language..detailed annotation instructions are made pub-licly available (figure 6 in appendix a.2).4 asample questionnaire is shown in figure 7 in ap-pendix a.2.
for quality control purposes, we man-ually annotated around 5% of the data ourselvesbeforehand.
we will refer to these instances as goldquestions.
the gold questions were interspersedwith the other questions.
if a worker’s accuracy on.
3http://saifmohammad.com/webpages/.
bestworst.html.
4amt task interface with instructions: https://.
hadarishav.github.io/ruddit/.
2703# comments.
# annotations per tuple.
# annotations.
# annotators.
shr pearson.
shr spearman.
6000.
6.
95,255.
725.
0.8818 ± 0.0023.
0.8612 ± 0.0029.table 1: ruddit annotation statistics and split-half reliability (shr) scores..the gold questions fell below 70%, they were re-fused further annotation and all of their annotationswere discarded.
the discarded annotations werepublished again for re-annotation.
we received atotal of 95,255 annotations by 725 crowd workers.
the bws responses were converted to scoresusing a simple counting procedure (orme, 2009;flynn and marley, 2014).
for each item, the scoreis the proportion of times the item is chosen as themost offensive minus the proportion of times theitem is chosen as the least offensive.
we releasethe aggregated annotations as well as the individualannotations of ruddit, to allow further work onexamining and understanding the variability.5.
figure 1: a histogram of frequency of comments–degree of offensiveness.
degree of offensivenessscores are grouped in bins of size 0.05..4.3 annotation reliability.
5 data analysis.
we cannot use standard inter-annotator agreementmeasures to ascertain the quality of comparativeannotations.
the disagreement that arises in tupleshaving two items that are close together in theirdegree of offensiveness is a useful signal for bws(helping it give similar scores to the two items).
the quality of annotations can be measured bymeasuring the reproducibility of the end result –if repeated manual annotations from multiple an-notators can produce similar rankings and scores,then, one can be conﬁdent about the quality of an-notations received.
to assess this reproducibility,we computed average split-half reliability (shr)values over 100 trials.
shr is a commonly used ap-proach to determine consistency in psychologicalstudies..for computing shr values, the annotations foreach 4-tuple were randomly split in two halves.
using these two splits, two sets of rankings weredetermined.
we then calculated the correlationvalues between these two sets.
this procedurewas repeated 100 times and the correlations wereaveraged.
a high correlation value indicates thatthe annotations are of good quality.
table 1 showsthe shr for our annotations.
shr scores of over0.8 indicate substantial reliability..5we provide the comment ids and not the comment body,in accordance to the gdpr regulations.
comment body canbe extracted using the reddit api..in this section, we analyze various aspects of thedata, including: the distribution of scores, the as-sociation with identity terms, the relationship withemotion dimensions, the relationship with datasource, and the role of swear words..distribution of scores figure 1 shows a his-togram of frequency of comments vs. degree ofoffensiveness, over 40 equi-spaced score bins ofsize 0.05. we observe a normal distribution..to analyze the data, we placed the comments in5 equi-spaced score bins of size 0.4 (bin 1: −1.0to −0.6, bin 2: −0.6 to −0.2, and so on).
table 2shows some comments from the dataset (more ex-amples can be found in appendix a.3 table 6).
weobserved that bin 1 primarily contains supportivecomments while bin 2 shows a transition from sup-portive to neutral comments.
bin 3 is dominatedby neutral comments but as the score increases thecomments become potentially offensive and bins4 & 5 predominantly contain offensive comments.
it is interesting to note that bin 4 contains someinstances of implicit offensive language such as‘you look like a lesbian mechanic who has a shellcollection’.
in their paper, wiegand et al.
(2021)explore the category of such “implicity abusivecomparisons”, in depth.
more examples of implic-itly offensive comments present in our dataset canbe found in table 2 and table 6 (in appendix a.3).
to explore whether speciﬁc bins capture spe-.
27041.
2.
3.
4.
5.
12345.bin comment.
don’t worry, she’s going to be ﬁne.
i see you too are a man of culture;)this is so sexy!
love it!
“i live with my ex, but it’s totally cool, we’re just friends”not sure why im being down voted?
why does the truth bother so many people?
i presented a hypothetical question to you.
i did not even claim that you made that argument.
unfortunatelythat is not a straw man.
so, care to answer that question again?
don’t forget vaccines cause autism.
and torture is awesome.
we should murder the families of terrorists.
what is your angle, kim??
is this some hitler bs where you sign a peace treaty and then startwwiii?
or did you ﬁnally just grow a brain?
because neither sound particularly more likely thanthe other...if you support trump kill yourself, painfullyshut the fuck up bitch.
it’s bernie or bust nobody is voting for biden, now get the fuck out of here you cunt.
score.
−0.75−0.604−0.562−0.229−0.1910.083.
0.50.521.
0.6040.958.table 2: sample comments from ruddit for each of the 5 score bins.
comment in bold is implicitly offensive..bin words.
awesome, thanks, appreciatesongs, headphones, sweet, moviegap, sacriﬁce, employeemuslim, fucked, gay, ass, rapeddick, fuck, asshole, ass, shut.
table 3: top pmi scoring words for each of the 5 of-fensiveness score bins.
degree of offensiveness scoresare grouped in bins of size 0.4..ciﬁc topics or key-words, we calculated pointwisemutual information (pmi) scores of all the uniquewords in the comments (excluding stop words) withthe ﬁve score bins.
table 3 shows the top scoringwords for each bin.
we observed that bins 1, 2, and3 exhibit a strong association with supportive orneutral words, while bins 4 and 5 show a strongassociation with swear words and identity termscommonly found in offensive contexts..identity terms a common criticism of the ex-isting offensive language datasets is that in thosedatasets, certain identity terms (particularly thosereferring to minority groups) occur mainly in textsthat are offensive (sap et al., 2019; davidson et al.,2019; wiegand et al., 2019; park et al., 2018; dixonet al., 2018).
this leads to high association oftargeted minority groups (such as muslims, fe-males, black people and others) with the offensiveclass(es).
this bias, in turn, is captured by thecomputational models trained on such datasets.
asmentioned earlier, in ruddit, certain words such asgay, trans, male, female, black, white were foundto exhibit a relatively higher association with theoffensive bins than with the supportive bins.
inorder to probe the effect of this on the computa-tional models, we created a variant of ruddit by.
replacing all the identity terms (from the list givenin appendix a.4) in the comments with the [group]token and observed the effect on the models’ per-formance.
we refer to this variant of the datasetas the identity-agnostic dataset.
we analyse themodels’ performance in the next section..offensiveness vs. emotion as discussed earlier,our emotions impact the words we use in text.
we examined this relationship quantitatively us-ing ruddit and the nrc vad lexicon (which hasintensity scores along the valence, arousal, anddominance dimensions).
we ﬁrst identiﬁed sets ofwords in the vad lexicon that have high valencescores (>0.75), low valence scores (<0.25), higharousal scores (>0.75), low arousal scores (<0.25),high dominance scores (>0.75), and low domi-nance scores (<0.25), respectively.
we will referto them as the high and low intensity v/a/d words.
for each comment in ruddit, we calculated threescores that captured the intensities of the high v, a,d words (the averages of the intensities of the highv/a/d words in the comment) and three scores thatcaptured the intensities of the low v, a, d words(the averages of the intensities of the low v/a/dwords in the comment).
we then determined thecorrelation between each of these six scores and thedegree of offensiveness.
see table 4. from the ta-ble, we can observe that high valence, low arousal,high dominance and low dominance show no corre-lation with offensiveness whereas low valence andhigh arousal are somewhat correlated..in our dataset of 6000 comments, 33% (1990)comments are those that have the lowest valencescores (referred to as low valence comments), 33%(1990) of the comments are those that have the high-est arousal scores (referred to as high arousal com-.
2705emotion.
pearson’s r.high valencelow valencehigh arousallow arousalhigh dominancelow dominance.
0.03650.21400.35620.08590.07550.1004.table 4: pearson correlation values between the offen-siveness scores and the emotion dimension scores..figure 3: distribution of comments within each of the5 score bins over the comment types..figure 2: distribution of comments in each commenttype over the 5 offensiveness score bins..ments) and the remaining 34% (2020) commentswere chosen at random (referred to as random com-ments).
in figure 2, we see the distribution ofcomments from each type over the 5 score bins.
we observe that the majority of comments fromall types are situated in the center.
high arousalcomments are skewed towards the offensive end ofthe scale.
random comments are heavily skewedtowards the supportive end of the scale while lowvalence comments are slightly skewed towards thesupportive end.
figure 3 shows the distribution ofcomment types within each bin.
we can clearlysee that high arousal and low valence commentsdominate the bins on the offensive end of the scalewhile random comments dominate the bins on thesupportive end.
therefore, from both analyses, wecan infer that the low valence and high arousal emo-tion dimensions are useful signals for determiningthe offensiveness of a comment..offensiveness vs. data source as mentionedearlier, comments in our dataset come from threedifferent sources - topics, cmv, and random.
fig-ure 4 shows the distribution of comments fromeach source over the score bins.
we observed thatcomments from topics have near equal representa-tion on both sides of the scale, while for the othertwo sources, comments are more prevalent in the.
figure 4: distribution of comments in each commentcategory over the 5 offensiveness score bins..supportive bins.
the higher representation of com-ments from topics than the other two sources in theoffensive bins, is likely due to the fact that the top-ics category includes subreddits such as worldnewsand worldpolitics.
discussions on these subredditscovers controversial topics and lead to the usage ofoffensive language.
we observed that worldnewsand worldpolitics indeed have high representationin the offensive bins (figure 9 in appendix a.4)..swear words we identiﬁed 868 comments in ourdataset that contain at least one swear word fromthe cursing lexicon (wang et al., 2014).
commentscontaining swear words can have a wide range ofoffensiveness scores.
to visualize the distribution,we plot a histogram of the comments containingswear words vs. degree of offensiveness (see fig-ure 8 in appendix a.4).
the distribution is skewedtowards the offensive end of the scale.
an interest-ing observation is that some comments with lowoffensiveness scores contain phrases using swearwords to express enthusiasm or to lay more empha-sis, for example ‘hell yes’, ‘sure as hell love it’,‘uncomfortable as shit’ and others.
to study theimpact of comments containing swear words on.
2706computational models, we created another variantof ruddit in which we removed all the commentscontaining at least one swear word.
we refer to thisvariant as the no-swearing dataset.
this datasetcontains 5132 comments.
we analyse the models’performance on this dataset in the next section..offensiveness in different score rangesit ispossible that comments in the middle region ofthe scale may be more difﬁcult for the computa-tional models.
thus, we created a subset of rudditcontaining comments with scores from −0.5 to0.5. we call this subset (of 5151 comments), thereduced-range dataset.
we discuss the models’performance on this dataset in the next section..6 computational modeling.
in this section, we present benchmark experimentson ruddit and its variants by implementing somecommonly used model architectures.
the task ofthe models was to predict the offensiveness scoreof a given comment.
we performed 5-fold cross-validation for each of the models.6.
6.1 models.
bidirectional lstm we fed pre-trained 300 di-mensional glove word embeddings (penningtonet al., 2014) to a 2-layered bilstm to obtain asentence representation (using a concatenation ofthe last hidden state from the forward and back-ward direction).
this sentence representation wasthen passed to a linear layer with a tanh activationto produce a score between −1 and 1. we usedmean squared error (mse) loss as the objectivefunction, adam with 0.001 learning rate as the op-timizer, hidden dimension of 256, batch size of 32,and a dropout of 0.5. the model was trained for 7epochs..bert we ﬁne-tuned bertbase (devlin et al.,2019).
we added a regression head containinga linear layer to the pre-trained model.
we usedmse loss as the objective function, batch size of 16,and learning rate of 2e − 5 (other hyperparameterssame as (devlin et al., 2019)).
we used the adamwoptimizer with a linear learning rate scheduler withno warm up steps.
the model was trained for 3epochs.
(more details in appendix a.5.).
hatebert hatebert (caselli et al., 2020b) isa version of bert pretrained for abusive languagedetection in english.
hatebert was trained onral-e, a large dataset of english language red-dit comments from communities banned for beingoffensive or hateful.
hatebert has been shownto outperform the general purpose bert modelon the offensive language detection task when ﬁne-tuned on popular datasets such as offenseval 2019(zampieri et al., 2019), abuseval (caselli et al.,2020a), and hateval (basile et al., 2019)..we ﬁne-tuned hatebert on ruddit and its vari-ants.
the experimental setup for this model is thesame as that described for the bert model..6.2 results and analysis.
we report pearson correlation (r) and mse, aver-aged over all folds.
the performance of the modelson ruddit and its variants is shown in the table5. note that the performance values on the no-swearing and the reduced-range datasets are notdirectly comparable to the performance values onthe full ruddit as their score range is different.
we can see that on all the datasets, the hatebertmodel performs the best, followed by the bertmodel.
interestingly, the model performance (forall models) does not change substantially whentrained on ruddit or the identity-agnostic dataset.
this indicates that the computational models arenot learning to beneﬁt from the association of cer-tain identity terms with a speciﬁc range of scoreson the offensiveness scale.7.
the models show a performance drop on theno-swearing dataset, which suggests that swearwords are useful indicators of offensiveness andthat the comments containing them are easier toclassify.
yet, the fact that the models still obtainperformance of up to 0.8 (r) demonstrates that theynecessitate and are able to learn other types of of-fensiveness features.
it is also worth mentioningthat even if they encounter swear words in a com-ment, the task is not simply to label the commentas offensive but to provide a suitable score..finally, the models obtained the performance ofup to 0.78 (r) on the reduced-range dataset, whichshows that even if the comments from the extremeends of the offensiveness scale are removed, rudditstill presents an interesting and feasible offensive-ness scoring task..6since we have a linear regression task, we created foldsusing sorted stratiﬁcation (lowe, 2016) to ensure that thedistribution of all the partitions is similar..7it should be noted that since the list of identity terms andthe cursing lexicon we use is not exhaustive, our conclusionsare only limited to the scope of the respective lists..2707dataset.
hatebert.
bert.
bilstm.
r.mse.
r.mse.
r.mse.
a. rudditb. identity-agnostic.
0.886 ± 0.0030.883 ± 0.006.
0.025 ± 0.0010.025 ± 0.001.
0.873 ± 0.0050.869 ± 0.007.
0.027 ± 0.0010.027 ± 0.001.
0.831 ± 0.0050.824 ± 0.007.
0.035 ± 0.0010.036 ± 0.001.c. no-swearing.
0.808 ± 0.013.
0.023 ± 0.001.
0.783 ± 0.012.
0.027 ± 0.001.
0.704 ± 0.014.
0.036 ± 0.002.d. reduced-range.
0.781 ± 0.014.
0.022 ± 0.001.
0.757 ± 0.011.
0.025 ± 0.001.
0.659 ± 0.008.
0.033 ± 0.001.table 5: five-fold cross-validation results of the models on ruddit and its variants.
r = pearson’s r. note: scoresfor c. and d. are not directly comparable to scores for a. and b. as they involve different score ranges..figure 5: squared error values for the 3 models’ predictions over the offensiveness score range in ruddit..error analysis figure 5 shows the squared errorvalues of the 3 models over the offensiveness scorerange in ruddit.
as expected, for all the models,the error in predictions is lower on both the extremeends of the scale than in the middle region.
com-ments with very high or very low offensivenessscores are rich in obvious linguistic cues, makingit easier for the computational models to predictscores.
most of the not-obvious, indirect implicitlyoffensive, and neutral comments should be presentin the middle region of the offensiveness scale,making them more difﬁcult for the models.
it isinteresting to observe that hatebert, unlike theother two models, does not have high error valuesfor samples within the score range 0.25–0.75.
thisindicates that hatebert is efﬁcient in dealing withoffensive language that does not lie in the extremeoffensive end.
bilstm seems relatively less accu-rate for samples in the supportive range (−0.75 to−0.25).
this could be attributed to the less com-plex model architecture and the usage of gloveword embeddings..7 conclusion.
the ratings obtained are highly reliable (shr pear-son r ≈ 0.88).
we performed data analysis to gaininsight into the relation of emotions, data sources,identity terms, and swear words with the offensive-ness scores.
we showed that low valence and higharousal comments have a higher correlation withthe offensiveness scores.
finally, we presentedbenchmark experiments to predict the offensive-ness score of a comment, on our dataset.
we foundthat computational models are not beneﬁting fromthe association of identity terms with speciﬁc rangeof scores on the offensiveness scale.
in future work,it would be interesting to explore the use of con-versational context in computational modeling ofoffensiveness, as well as studying the interactionbetween offensiveness and emotions in more depth.
we make our dataset freely available to the researchcommunity..acknowledgements.
this research was funded by the facebook onlinesafety benchmark research award for the project“a benchmark and evaluation framework for abu-sive language detection.”.
we presented the ﬁrst dataset of online commentsannotated for their degree of offensiveness.
weused a comparative annotation technique calledbest–worst scaling, which addresses the limita-tions of traditional rating scales.
we showed that.
ethical considerations.
we create ruddit to study, understand and explorethe nature of offensive language.
any such datasetmight also be used to create automatic offensive.
2708language detection systems.
while we realisethe importance of such systems, we also acceptthat any moderation of online content is a threatto free speech.
offensive language datasetsor automatic systems can be misused to stiﬂedisagreeing voices.
our intent is solely to learnmore about the use of offensive language, learnabout the various degrees of offensive language,explore how computational models can beenabled to watch and contain offensive language,and encourage others to do so.
we follow theformat provided by bender and friedman (2018)to discuss the ethical considerations for our dataset..institutional review: this research was fundedby the facebook online safety benchmarkresearch award.
the primary objective of thisresearch award is the creation of publicly availablebenchmarks to improve online safety.
this awarddoes not directly beneﬁt facebook in any way.
this research was reviewed by facebook forvarious aspects, in particular:• legal review: evaluates whether the researchto be undertaken or the research performed canviolate intellectual property rights..• policy and ethics review: evaluates whether theresearch to be undertaken aligns with the bestethics practices.
this includes several aspectssuch as mitigating harm to people involved, im-proving data privacy, and informed consent..data redistribution / user privacy: we ex-tracted our data from the pushshift reddit datasetmade publicly available by baumgartner et al.
(2020) for research purposes.
the creators ofthe pushshift reddit dataset have provisions todelete comments from their dataset upon user’srequest.
we release data in a manner that is gdprcompliant.
we do not provide any user-speciﬁcinformation.
we release only the comment idsand post ids.
reddit’s terms of service do notprohibit the distribution of ids.8 the researchersusing the dataset need to retrieve the data using thereddit api..speaker and annotator demographic: no spe-ciﬁc speaker demographic information is availablefor the comments included in ruddit.
accordingto the october 2020 survey published by statista(statista, 2020a), 50% of the reddit’s desktop traf-ﬁc is from the united states.
they also state thatfrom the internet users in the us, 21% from ages.
18-24, 23% from ages 25-29 and 14% from ages30-49 use reddit..we restricted annotators to those residing in theus.
a total of 725 crowd-workers participated inthe task.
apart from the country of residence, noother information is known about the annotators.
the annotators are governed by amt’s privacypolicy.9 pew research center conducted a demo-graphic survey of amt workers in 2016. in thissurvey, 3370 workers participated.
they found outthat 80% of the crowd-workers on amt are fromthe us (prc, 2020).
more information about theworkers who participated in their survey can befound in their article..it.
is important.
to include the opinions oftargeted minorities and marginalized groups whendealing with the annotation of offensive language(kiritchenko and nejadgholi, 2020; blackwellet al., 2017).
however, we did not have our dataannotated by the speciﬁc target demographicbecause it poses certain challenges.
for example:identiﬁcation of the target of offensive language;ﬁnding people of the target demographic groupwho are willing to annotate offensive language;and others.
annotating such offensive data canbe even more traumatizing for the members offinally, ruddit wasthe targeted minorities.
created with the intention to look at wide rangingoffensive language of various degrees as opposedto detecting offensive language towards speciﬁctarget groups..annotation guidelines: we created our anno-tation guidelines drawing inspiration from thecommunity standards set for offensive language onseveral social media platforms.
these standardsare made after thorough research and feedbackfrom the community.
however, we are awarethe deﬁnitions in our guidelines are notthatrepresentative of all possible perspectives.
thedegree of offensiveness scores that we provide inruddit are a representation of what the majority ofour annotators think.
we would like to emphasizethat the scores provided are not the “correct” or theonly appropriate value of offensiveness.
differentindividuals and demographic groups may ﬁnd thesame comment to be more or less offensive thanthe scores provided..impact on annotators: annotation of harsh andoffensive language might impact the mental healthof the annotators negatively (vidgen et al., 2019;.
8https://www.reddit.com/wiki/api-terms.
9https://www.mturk.com/help.
2709roberts, 2016, 2019; kiritchenko and nejadgholi,2020).
the following minimized negative mentalimpact on the annotators participating in our task:• the comments that we included in our datasetare pre-moderated by reddit’s admins and sub-reddit speciﬁc moderators.
any comments thatdo not comply with reddit’s content policy arenot included.10.
• our goal was to annotate posts one sees on socialmedia (after content moderation).
unlike somepast work, we do not limit the data to include onlynegative comments.
we included a large sampleof posts that one normally sees on social media,and annotated it for degree of supportiveness ordegree of offensiveness..• amt provides a checkbox where requesters canindicate that some content in the task may beoffensive.
these tasks are not shown to annota-tors who have speciﬁed so in their proﬁle.
weused the checkbox to indicate that this task hasoffensive content..• we explicitly warned the annotators about thecontent of annotation, and advised worker discre-tion..• we provided detailed annotation instructions andinformed the annotators about how the anno-tations for offensive language will be used forstudying and understanding offensive language.
• the annotation of our data was crowdsourced,allowing for a large number of raters (725).
thisreduces the number of comments seen per rater.
we also placed a limit on how many posts onemay annotate.
annotators were not allowed tosubmit more than ∼ 5% of the total assignments.
• there are just 25 comments in the top 10% of theoffensiveness score range.
thus, most annotators(> 99.95%) do not see even one such comment..identity terms: as discussed in section 5, inruddit, certain identity terms show a higherassociation with offensive comments than within order to addressthe supportive comments.
this, we created a variant of ruddit, in which wereplaced all the identity terms (from the list givenin appendix a.4) with the [group] token.
wecall this variant the identity-agnostic dataset.
werelease the code for creating this variant from theoriginal dataset.
we evaluated our computationalmodels on this variant and observed that themodels did not learn to beneﬁt from the association.
10https://www.redditinc.com/policies/.
content-policy.
of the identity terms with the offensive comments..computational models: the models reportedin this paper are not intended to fully automateoffensive content moderation or to make judge-ments about speciﬁc individuals.
owing to privacyconcerns, we do not model user history to predictoffensiveness scores (mitchell et al., 2018)..feedback: we are aware that our dataset is sub-ject to the inherent bias of the data, the samplingprocedure and the opinion of the annotators whoannotated it.
finally, we acknowledge that this isnot a comprehensive listing of all the ethical con-siderations and limitations.
we welcome feedbackfrom the research community and anyone using ourdataset..references.
shima asaadi, saif mohammad, and svetlana kir-itchenko.
2019. big bird: a large, ﬁne-grained,bigram relatedness dataset for examining semanticin proceedings of the 2019 confer-composition.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 505–516, minneapolis, minnesota.
as-sociation for computational linguistics..valerio basile, cristina bosco, elisabetta fersini,debora nozza, viviana patti, francisco manuelrangel pardo, paolo rosso, and manuela san-guinetti.
2019.semeval-2019 task 5: multilin-gual detection of hate speech against immigrants andwomen in twitter.
in proceedings of the 13th inter-national workshop on semantic evaluation, pages54–63, minneapolis, minnesota, usa.
associationfor computational linguistics..jason baumgartner, savvas zannettou, brian keegan,megan squire, and jeremy blackburn.
2020. thepushshift reddit dataset..emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..lindsay blackwell, jill dimond, sarita schoenebeck,and cliff lampe.
2017. classiﬁcation and its con-sequences for online harassment: design insightsfrom heartmob.
proc.
acm hum.-comput.
interact.,1(cscw)..luke breitfeller, emily ahn, david jurgens, and yu-lia tsvetkov.
2019. finding microaggressions in thewild: a case for locating elusive phenomena in so-cial media posts.
in proceedings of the 2019 con-ference on empirical methods in natural language.
2710processing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1664–1674, hong kong, china.
as-sociation for computational linguistics..tommaso caselli, valerio basile, jelena mitrovi´c, ingakartoziya, and michael granitzer.
2020a.
i feel of-fended, don’t be abusive!
implicit/explicit messagesin offensive and abusive language.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 6193–6202, marseille, france.
euro-pean language resources association..tommaso caselli, valerio basile, jelena mitrovi´c, andmichael granitzer.
2020b.
hatebert: retraining bertfor abusive language detection in english..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech andabusive language detection datasets.
in proceedingsof the third workshop on abusive language online,pages 25–35, florence, italy.
association for com-putational linguistics..thomas davidson, dana warmsley, michael macy,and ingmar weber.
2017. automated hate speechdetection and the problem of offensive language.
inproceedings of the 11th international aaai confer-ence on web and social media, icwsm ’17, pages512–515..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and mitigat-in pro-ing unintended bias in text classiﬁcation.
ceedings of the 2018 aaai/acm conference on ai,ethics, and society, aies ’18, page 67–73, newyork, ny, usa.
association for computing machin-ery..t.n.
flynn and a.a.j.
marley.
2014. best-worst scal-ing: theory and methods.
in stephane hess and an-drew daly, editors, handbook of choice modelling,chapters, chapter 8, pages 178–201.
edward elgarpublishing..p. fortuna and s. nunes.
2018. a survey on automaticdetection of hate speech in text.
acm computingsurveys (csur), 51:1 – 30..antigoni-maria founta, constantinos djouvas, de-spoina chatzakou, ilias leontiadis, jeremy black-burn, gianluca stringhini, athena vakali, michaelsirivianos, and nicolas kourtellis.
2018. largescale crowdsourcing and characterization of twitterabusive behavior.
corr, abs/1802.00393..lei gao and ruihong huang.
2017. detecting onlinehate speech using context aware models.
pages 260–266..t. jay and kristin janschewitz.
2008. the pragmatics.
of swearing..david jurgens.
2013. embracing ambiguity: a com-parison of annotation methodologies for crowdsourc-ing word sense labels.
in proceedings of the 2013conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 556–562, atlanta,georgia.
association for computational linguistics..david a. jurgens, peter d. turney, saif m. moham-mad, and keith j. holyoak.
2012. semeval-2012task 2: measuring degrees of relational similarity.
inproceedings of the first joint conference on lexicaland computational semantics - volume 1: proceed-ings of the main conference and the shared task,and volume 2: proceedings of the sixth interna-tional workshop on semantic evaluation, semeval’12, page 356–364, usa.
association for computa-tional linguistics..svetlana kiritchenko and saif mohammad.
2017. best-worst scaling more reliable than rating scales: acase study on sentiment intensity annotation.
in pro-ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 465–470, vancouver, canada.
asso-ciation for computational linguistics..svetlana kiritchenko and saif m. mohammad.
2016.capturing reliable ﬁne-grained sentiment associa-tions by crowdsourcing and best–worst scaling.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 811–817, san diego, california.
associationfor computational linguistics..svetlana kiritchenko and isar nejadgholi.
2020. to-wards ethics by design in online abusive content de-tection..svetlana kiritchenko, xiaodan zhu, and saif moham-mad.
2014. sentiment analysis of short informaltext.
the journal of artiﬁcial intelligence research(jair), 50..jana kurrek, haji mohammad saleem, and derekruths.
2020. towards a comprehensive taxonomyand large-scale annotated corpus for online slur us-age.
in proceedings of the fourth workshop on on-line abuse and harms, pages 138–149, online.
as-sociation for computational linguistics..j. j. louviere.
1991. best-worst scaling: a model forthelargest difference judgments.
working paper..jordan j. louviere, terry n. flynn, and a. a. j. mar-ley.
2015. best-worst scaling: theory, methods andapplications.
cambridge university press..2711scott c. lowe.
2016. stratiﬁed validation splits for re-.
prc.
2020. research in the crowdsourcing age, a case.
gression problems..study.
accessed: 2020-10-10..pushkar mishra, helen yannakoudakis, and ekaterinashutova.
2019.tackling online abuse: a sur-vey of automated abuse detection methods.
corr,abs/1908.06024..stanley presser and howard schuman.
1996. ques-tions and answers in attitude surveys: experimentson question form, wording, and context.
sagepublications, inc..margaret mitchell, simone wu, andrew zaldivar,parker barnes, lucy vasserman, ben hutchinson,elena spitzer, inioluwa deborah raji, and timnitgebru.
2018. model cards for model reporting.
corr, abs/1810.03993..dante razo and sandra k¨ubler.
2020..investigatingsampling bias in abusive language detection.
in pro-ceedings of the fourth workshop on online abuseand harms, pages 70–78, online.
association forcomputational linguistics..saif mohammad.
2018. obtaining reliable human rat-ings of valence, arousal, and dominance for 20,000in proceedings of the 56th an-english words.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 174–184, melbourne, australia.
association for compu-tational linguistics..saif mohammad and felipe bravo-marquez.
2017.emotion intensities in tweets.
in proceedings of the6th joint conference on lexical and computationalsemantics (*sem 2017), pages 65–77, vancouver,canada.
association for computational linguistics..saif mohammad and svetlana kiritchenko.
2018. un-derstanding emotions: a dataset of tweets to studyinteractions between affect categories.
in proceed-ings of the eleventh international conference onlanguage resources and evaluation (lrec 2018),miyazaki, japan.
european language resources as-sociation (elra)..emily munro.
2011. the protection of children online:a brief scoping review to identify vulnerable groups..b. orme.
2009..counting,individual-levelsoftware, inc..maxdiff analysis:.
simplelogit, and hb.
sawtooth.
c.e.
osgood, g.j.
suci, and p.h.
tenenbaum.
1957.the measurement of meaning.
university of illinoispress, urbana:..ji ho park, jamin shin, and pascale fung.
2018. re-ducing gender bias in abusive language detection.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 2799–2804, brussels, belgium.
associationfor computational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..soujanya poria, navonil majumder, rada mihalcea,and eduard hovy.
2019. emotion recognition inconversation: research challenges, datasets, and re-cent advances..sarah t. roberts.
2016. chapter eight: commercialcontent moderation: digital laborers’ dirty work.
peter lang..sarah t. roberts.
2019. behind the screen: contentmoderation in the shadows of social media.
yaleuniversity press..james russell.
2003. core affect and the psychologi-cal construction of emotion.
psychological review,110:145–72..james a. russell.
1980. a circumplex model of af-fect.
journal of personality and social psychology,39(6):1161–1178..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for com-putational linguistics, pages 1668–1678, florence,italy.
association for computational linguistics..maarten sap, saadia gabriel, lianhui qin, dan juraf-sky, noah a smith, and yejin choi.
2020. socialbias frames: reasoning about social and power im-plications of language.
in acl..anna schmidt and michael wiegand.
2017. a surveyon hate speech detection using natural language pro-in proceedings of the fifth internationalcessing.
workshop on natural language processing for so-cial media, pages 1–10, valencia, spain.
associa-tion for computational linguistics..wiktor soral, m. bilewicz, and m. winiewski.
2018.exposure to hate speech increases prejudice throughdesensitization.
aggressive behavior, 44:13v–146..statista.
2020a.
regional distribution of desktop traf-ﬁc to reddit.com as of september 2020, by country.
accessed: 2021-01-04..statista.
2020b.
share of adult internet users in theunited states who have personally experienced on-line harassment as of january 2020..bertie vidgen and leon derczynski.
2020. direc-tions in abusive language training data: garbage in,garbage out..2712marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019. semeval-2019 task 6: identifying and catego-rizing offensive language in social media (offense-val).
in proceedings of the 13th international work-shop on semantic evaluation, pages 75–86, min-neapolis, minnesota, usa.
association for compu-tational linguistics..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019.challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online, pages 80–93, florence, italy.
association for computational linguistics..wenbo wang, lu chen, krishnaprasad thirunarayan,and amit p. sheth.
2014. cursing in english on twit-in proceedings of the 17th acm conferenceter.
on computer supported cooperative work & socialcomputing, cscw ’14, page 415–425, new york,ny, usa.
association for computing machinery..zeerak waseem.
2016. are you a racist or am i seeingthings?
annotator inﬂuence on hate speech detectionon twitter.
in proceedings of the first workshop onnlp and computational social science, pages 138–142, austin, texas.
association for computationallinguistics..zeerak waseem, thomas davidson, dana warmsley,and ingmar weber.
2017. understanding abuse: atypology of abusive language detection subtasks.
inproceedings of the first workshop on abusive lan-guage online, pages 78–84, vancouver, bc, canada.
association for computational linguistics..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatein proceedings of thespeech detection on twitter.
naacl student research workshop, pages 88–93,san diego, california.
association for computa-tional linguistics..gregor wiedemann, eugen ruppert, raghav jindal,and chris biemann.
2018. transfer learning fromlda to bilstm-cnn for offensive language detectionin twitter.
corr, abs/1811.02906..michael wiegand, maja geulig, and josef ruppen-hofer.
2021. implicitly abusive comparisons – a newdataset and linguistic analysis.
in proceedings of the16th conference of the european chapter of the as-sociation for computational linguistics: main vol-ume, pages 358–368, online.
association for com-putational linguistics..michael wiegand, josef ruppenhofer, and thomaskleinbauer.
2019. detection of abusive language:the problem of biased datasets.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 602–608, minneapolis,minnesota.
association for computational linguis-tics..ellery wulczyn, nithum thain, and lucas dixon.
2016.ex machina: personal attacks seen at scale.
corr,abs/1610.08914..ellery wulczyn, nithum thain, and lucas dixon.
2017..wikipedia talk corpus..2713a supplemental material.
a.2 annotation.
a.1 post and comment criteria.
we selected the posts from the subreddits based onthe following criteria:.
1. date: to extract comments from posts that dis-cuss current matters, we took comments fromthe time period of january, 2015 to september,2019 (last available month at the time of extrac-tion)..2. thread length: we chose posts with more than150 comments and less than 5000 comments.
this criteria ensured that the posts containedenough comments to capture meaningful discus-sion..3. post length: we chose posts containing morethan 5 words and less than 60 words in the postbody.
this was done to avoid posts that are tooshort to provide enough information or are toolong and have a possibility of being spam..4. url: often, posts on reddit contain urls redi-recting to images, videos, news articles and oth-ers.
we limited our posts to those containingat most one url to avoid issues arising due tomissing context..for each post, the hierarchical threads were re-constructed using the anytree python library.
weﬁltered comments from these posts based on thefollowing criteria:.
1. comment length: we chose comments con-taining more than 5 words and less than 150words in the comment body.
we did this to in-clude comments that are neither too long (canbe difﬁcult to annotate) nor too short (not veryvaluable)..2. no.
of users: in the ﬁrst and last 25 commentsof the thread, we ensured participation of atleast 4 users.
this was done to ensure that thecomments in our dataset are from a diverse setof users..3. url: we chose comments with no url inthem.
comments with url can be difﬁcultto annotate as the urls provide extra contextfor the comment..figure 6 shows the detailed annotation instructionsgiven to the crowd-workers for the task..a sample questionnaire for the ﬁnal annotation.
task is shown in figure 7..the hourly compensation rate for annotators onamazon mechanical turk was us$7.50/hr.
thetask received considerable attention with 725 par-ticipants in total..a.3 sample data.
table 6 contains comments from ruddit groupedaccording to the 5 score bins..a.4 data analysis.
trans, queer,.
we used the list of identity terms used by dixonet al.
(2018) with a few of our own additions.
the terms used are lesbian, gay, bisexual, trans-gender,lgbtq, homosexual,lgbt,straight, heterosexual, male, female, nonbinary,african, africanamerican, black, white, european,hispanic, latino, latina, latinx, mexican, cana-dian, american, asian, indian, middle eastern, chi-nese, japanese, christian, muslim, jewish, buddhist,catholic, protestant, sikh, taoist, old, older, young,younger, teenage, millenial, middle aged, elderly,blind, deaf, paralyzed, atheist, feminist, islam, mus-lim, man, woman, boy, girl..figure 8 shows a histogram of the commentscontaining swear words–degree of offensiveness,over 40 equi-spaced score bins of size 0.05..figure 9 shows a distribution of commentswithin each of the 5 score bins over the subred-dits that were included in the topics category..a.5 computational modeling.
hyperparameter tuning we tuned hyperpa-rameters for the bert and the bilstm models.
we performed grid search cross-validation on rud-dit and used pearson’s r to select the best hyperpa-rameter setting.
all experiments were performedon a ﬁxed seed value of 12..for the bilstm model, the batch size was ﬁxedat 32 and the number of epochs was set to 7. thehyperparameter search space is as follows:.
• number of layers (n): 1, 2.
• hidden size (h): 64, 128, 256.for the bert model, the batch size was ﬁxed at16 and bert tokenizer’s maximum length was set.
2714figure 6: detailed instructions for the ﬁnal annotation task..figure 7: sample questionnaire for the ﬁnal annotation task..to 200. we tune hyperparameters on the settingsthat devlin et al.
(2019) found to work best on alltasks.
the search space is as follows:.
• learning rate: 2e − 5, 3e − 5, 5e − 5.
• number of epochs: 3, 4.we reported the best setting for the models insection 6.1. the average r of the bert and thebilstm models across all hyperparameter search.
2715bin comment.
1.
2.
3.
4.
5.truly gave me a smileawwwwwe that’s was the best reveal i’ve seen.
i’m literally doing the same tonight!
have an awesome time :)my apologies for missing that in your post.
carry on.
i’m assuming this is a great response for i haven’t seen the video in question, but i do believe that thewoman in the video carried herself very elegantly!
could you not love a “purrfectly” healthy moggy?
one whose life you could save from a shelter and won’tbreak the bank.
handwritten letters are pure gold.
it is a simple but immensely pleasant gift to receive.
still have the onesmy ex gf would sent me when we were apart.
it would take way more time to get a screaming baby to stay still long enough to inject them.
i remembermy little sister throwing off doctors and nurses like a tiny she-hulk when she was a toddler.
she alsopunched my dad in the face.
oh man, that’s a great idea!
nuke a few large cities in the us, this will send the economy to the sky!
no it’s usually their kids they are killing.
they were compassionately raped too.
that’s how you feel the love.
you look like a lesbian mechanic who has a shell collectionyou know it sucks when you cough, and the all of a sudden you got whooping gaythe best thing about anti-vax is soon we wont have to do anything about them, the diseases will doit for usshe’s a hoe, plain and simple.
this guy is giving me fucking aidsfuck you, you’re just pretending to be blocked.
get back to work right now you piece of shit piping.
score.
−0.812−0.688−0.625.
−0.583−0.375.
−0.25.
−0.196.
−0.125.
0.1040.2080.3120.4380.4570.458.
0.6250.7920.938.table 6: more sample comments from ruddit for each of the 5 score bins.
comments in bold are implicitlyoffensive..the approximate average runtime for each modelon the ruddit dataset is as follows:.
• bilstm (n = 2, h = 256): 2 seconds per.
epoch.
• bert: 3 minutes per epoch.
• hatebert: 3.6 minutes per epoch.
figure 8: a histogram of frequency of comments con-taining swear words–degree of offensiveness.
degreeof offensiveness scores are grouped in bins of size 0.05..trials was 0.868 ± 0.005 and 0.827 ± 0.002 respec-tively..training times we trained all our models onthe tesla t4 gpu.
the number of gpu(s) used is1. the number of trainable parameters and thus,the training time varied for each model.
the ap-proximate number of trainable parameters for eachmodel is as follows:.
• bilstm (n = 2, h = 256): 7 million.
• bert: 108 million.
• hatebert: 109 million.
2716figure 9: distribution of comments within each of the ﬁve score bins over subreddits (that were included in thetopics category).
for each score bin, the graph shows how the comments are distributed across the subreddits..2717