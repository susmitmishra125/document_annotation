language model augmented relevance score.
ruibo liu1.
jason wei2.
soroush vosoughi1.
1dartmouth college.
2google ai language.
ruibo.liu.gr@dartmouth.edu jasonwei@google.comsoroush.vosoughi@dartmouth.edu.
abstract.
although automated metrics are commonlyused to evaluate nlg systems, they often cor-relate poorly with human judgements.
newermetrics such as bertscore have addressedmany weaknesses in prior metrics such asbleu and rouge, which rely on 𝑛-grammatching.
these newer methods, however, arestill limited in that they do not consider thegeneration context, so they cannot properly re-ward generated text that is correct but deviatesfrom the given reference..in this paper, we propose language modelaugmented relevance score (mars), a newcontext-aware metric for nlg evaluation.
mars leverages off-the-shelf language mod-els, guided by reinforcement learning, to cre-ate augmented references that consider boththe generation context and available humanreferences, which are then used as additionalreferences to score generated text.
comparedwith seven existing metrics in three commonnlg tasks, mars not only achieves highercorrelation with human reference judgements,but also differentiates well-formed candidatesfrom adversarial samples to a larger degree..1.introduction.
automated metrics such as bleu (papineni et al.,2002) and rouge (lin, 2004) are popular meth-ods for evaluating natural language generation(nlg) systems.
compared with human evalua-tion, they are cheaper and faster, and accordingly,they often serve as essential metrics for benchmark-ing the performance of nlg models (novikovaet al., 2017).
despite their widespread use, how-ever, these automated metrics often poorly correlatewith ratings given by human judges, particularlyfor datasets in which only a single human referenceexists (gupta et al., 2019; novikova et al., 2017).
moreover, these automated metrics only capture.
figure 1: existing metrics compare the candidate withthe human reference but ignore context.
mars (ourmethod) augments the human reference while consider-ing the context, which allows it to provide evaluationscores that correlate highly with human references..similarities between generated sentences and refer-ence candidates, crucially ignoring provided con-texts that are relevant for evaluating the answer incontextual nlg tasks, such as story generation,news summarization, and question-answering (taoet al., 2018; nema and khapra, 2018)..table 1 shows a story generation1 examplethat exempliﬁes some weaknesses of several com-mon metrics.
perplexity (ppl) (brown et al.,1992) successfully detects ungrammatical sen-tences, but it fails to distinguish legitimate novelcontinuations and copy-and-pasted ones.
rely-ing on surface-level 𝑛-gram matching, bleu-1and rouge-l2 cannot detect reordering effec-tively, and wrongly score the well-formed candi-date lower than its retrieval-based adversarial ex-ample.
bertscore (zhang et al., 2019) leveragescontextual embeddings from bert (devlin et al.,2019), thus mitigating the above challenges, butstill does not fairly evaluate candidates that cor-rectly align with the context but happen to differ.
1the roc story generation task asks systems to generate.
a legitimate ending for a four-sentence story..2l stands for longest common sequence matching..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6677–6690august1–6,2021.©2021associationforcomputationallinguistics6677contextexsitingmetrics:ppl/bleu/rouge/bert-score/etc.ours:marshumanreferencecandidatelmlmaugmentedreferencescontext.
wendy was driving down the road.
she heard her car making a noise.
she pulled over to examine the problem.
there was nothing but oil all on the road from her car..human reference.
she called for help and waited to get her car ﬁxed.
ppl bleu-1 rouge-l bertscore mars.
candidate.
her fears were conﬁrmed when her engine was smoking.
reorder.
her conﬁrmed engine fears her when was were smoking.
retrieve.
she heard her car making a noise..75.58405.6063.93.
0.2230.2230.337.
0.1820.1820.400.
0.3380.2650.406.
0.5740.3520.448.table 1: in this story generation example, mars is the only metric that gives the well-formed candidate a higherscore than two adversarial examples.
the human rating of the candidate averaged over 20 judgements is 5.05 out of6.00. two adversarial examples are generated by reordering the tokens of the candidate (as weak nlg systemswhose generation is not readable) and retrieveing a sentence from the context (as systems with no generationability).
we boxed the cases where the adversarial example does not score lower than the well-formed candidate..from the provided reference example.
in our exam-ple, the candidate “... her engine was smoking” isreasonable but deviates from the human reference,and so bertscore rates it relatively low (0.338 outof 1.0), thus correlating poorly with human rating,which was high (5.05 out of 6.00)..to address the above issues, prior studies haveproposed a number of promising remedies.
oneline of work has proposed to combine human rat-ings with automated metrics (durmus et al., 2020;chaganty et al., 2018, inter alia).
for instance, inhuse score, hashimoto et al.
(2019) leverages thedifferences between perplexity and human judge-ments to consider both quality and diversity ofgenerated text.
another line has proposed train-ing separate neural models to aid automated met-rics (mehri and eskenazi, 2020; yuma et al., 2020,inter alia).
for instance, bleurt (sellam et al.,2020) ﬁne-tunes bert (devlin et al., 2019) on syn-thetic reference-candidate pairs for machine trans-lation.
these methods, however, are often limitedin practical use, because the high-cost human rat-ings are not always available for every dataset, andthe data- or system-speciﬁc training is not easilyextended to other domains (zhang et al., 2019), andcan even bias the evaluation (freitag et al., 2020b)..in this paper, we present mars (languagemodel augmented relevance score), a new nlgevaluation metric that requires neither supervisionfrom human ratings nor additional training on spe-ciﬁc domains.
as shown in figure 1, instead ofcomparing candidates only with human written ref-erences, as many prior metrics do, mars uses amixture of both human and augmented references.
speciﬁcally, mars masks tokens in the referenceto create templates, and then uses the context andtemplates to generate augmented references by in-ﬁlling the masked parts with an lm guided by rein-forcement learning.
the augmented references thus.
incorporate information from both the context andthe human reference, and are enriched with lexicaland syntactic diversity, facilitating fairer evaluationof candidates.
finally, we compute the score asa weighted average of the similarity between thecandidate and the set of augmented references inthe contextual embedding space..the advantages of mars are three-fold.
first,mars correlates highly with human judgements.
we apply mars to three diverse nlg tasks, anddemonstrate that, compared with seven popularnlg metrics, mars better correlates with hu-man judgements and is robust against adversarialattacks.
second, mars is context-aware.
un-like existing metrics that only consider the givenhuman reference, we use a constrained nlg ap-proach to incorporate the generation context intoaugmented references, thus alleviating bias againstdiverse candidates.
third, mars is easy to deployand extend.
built on off-the-shelf lms, marsrequires neither human supervision nor additionaltraining for speciﬁc domains, and can thereforeserve as a general-purpose metric for a broad rangeof nlg applications, as we will demonstrate forthree common nlg tasks: story generation, newssummarization, and question-answering..2 approach.
mars comprises three steps.
first, we mask outnon-important tokens from the human reference toproduce templates for augmentation (§2.1).
sec-ond, we guide off-the-shelf lms to generate refer-ence augmentation on these templates via a rein-forced self-planning algorithm (§2.2).
finally, wecompute a weighted average score that reﬂects theoverall similarity between the candidate and the setof augmented references (§2.3)..66782.1 human reference token masking.
the ﬁrst step in mars is to take in the given hu-man reference and generate templates—maskedversions of the human reference—which can thenbe used to generate augmented references.
ourmasking procedure can be viewed as a reversedprocess of prior insertion- and template-based gen-eration approaches (zhang et al., 2020; miao et al.,2019); whereas these generation approaches startwith templates of important tokens and then ﬁllin the details to generate complete sentences, ourmasking procedure starts with the complete sen-tence (i.e., the human reference) and then masksout unimportant tokens to generate templates.
tobetter explain our masking procedure, we introducetwo concepts, mask priority and mask cost:.
mask priority.
we compute a mask priority 𝑣𝑖for each token 𝑥𝑖, which captures the priority ofmasking 𝑥𝑖, where non-important words shouldreceive higher priority.
we compute 𝑣𝑖 as a func-tion of two things: the inverse document frequency(idf) of 𝑥𝑖, and the part-of-speech (pos) of 𝑥𝑖:.
𝑣𝑖 =.
𝛼(pos [𝑥𝑖])idf(𝑥𝑖, 𝑋).
,.
(1).
where 𝛼 is a function that assigns a weight to eachpos tag.3 common tokens across the corpus 𝑋(e.g., stop words, with low idf) will receive highmask priority.
tokens responsible for descriptiondetails will also be assigned high mask prioritybased on their part-of-speech (e.g., adjectives aremainly used for details and so they are given higherpriority of being masked)..mask cost.
for each token 𝑥𝑖, we also computea mask cost 𝑤𝑖.
tokens that appear in both con-text and human reference should have high mask-ing cost as they are deemed context-carrying.
weuse the longest common sequence (lcs) match-ing between the context and the human referenceto identify these context-carrying tokens.
in ourexperiments, we set the 𝑤𝑖 of these tokens to 10and the default 𝑤𝑖 of all other tokens to 1. we use𝜆 to denote the ratio of tokens to be masked in asentence of 𝑁 tokens, and deﬁne 𝑊max = 𝜆 · 𝑁 asthe maximum cost allowed..3𝛼 varies for each task.
empirically, we ﬁnd that it workswell to assign adjectives, adverbs, and nouns higher weightsthan other parts-of-speech.
for our setting, we assign weightsof 4, 3, 2 to the above three types..dp-based token masking.
now that for eachtoken we have a mask priority and a mask cost,we aim to choose a set of tokens to mask with thehighest possible sum of priorities for which thesum of mask costs is not greater than 𝑊max.
givena function 𝜙(𝑥𝑖) = {1, 0} where 1 means token 𝑥𝑖is masked and 0 means it remains, the objective oftoken masking can be expressed as follows:.
max.
𝑣𝑖 · 𝜙(𝑥𝑖) ,.
s.t..𝑤𝑖 · 𝜙(𝑥𝑖) ≤ 𝑊max ..(2).
𝑁(cid:213).
𝑖=1𝑁(cid:213).
𝑖=1.
such a goal is actually a np-complete combina-torial optimization problem, called the knapsackproblem (pisinger, 1995), which we solve usingdynamic-programming (dp).
in general, the mask-ing strategy aggressively harvests tokens of highmask priority while keeping the cost of masked to-kens from exceeding the mask cost limitation 𝑊max.
the detailed dp algorithm for solving this problemis shown in appendix a..2.2 self-planning cloze augmentation.
after creating the templates described in §2.1, weproduce augmented reference examples based onboth the templates as well as the generation context.
this procedure can be seen as a mixture of hard-and soft-constrained nlg, where the template to-kens pre-exist with some blanks, and the system,conditioned on the context, aims to ﬁll in the blanks.
we henceforth refer this process of creating aug-mented references as cloze4 augmentation..background.
masked language models (mlm)such as roberta (liu et al., 2019) and bert (de-vlin et al., 2019) are trained to predict maskedtokens within sentences, and thus are able to docloze augmentation off-the-shelf.
however, with-out architecture-level modiﬁcation, mlms are onlyable to inﬁll a pre-determined number of missingtokens (zhu et al., 2019).
this is especially prob-lematic since—if they are directly used to augmentreferences—all the augmented references will havethe same number of tokens as that of the originalhuman reference.
we believe this unnecessarilyconstrains augmentation diversity, and thus con-sider it as a naive method in our evaluations (§4)..4a cloze test (taylor, 1953) is a language test where aportion of language is removed and the participant is asked toreplace the missing language item..6679𝑎𝑡 = 𝑥𝑡 ).
we take the softmax output of the lasthidden states (with parameter 𝜃) as the policy 𝜋 𝜃 ,since it is the probability of picking token 𝑥𝑡 (action𝑎𝑡 ) given the state 𝑠𝑡 = 𝑥<𝑡 .
similarly, we denotethe policy after reinforced self-planning as 𝜋 𝜃𝑑 ..typically, the rl objective is to maximize theexpectation of total reward 𝐽, summed over 𝑇 stepson the trajectory 𝜏 induced by 𝜋 𝜃 :.
𝐽 (𝜃) = e𝜏∼ 𝜋𝜃.
(3).
(cid:35).
𝛾𝑡𝑟𝑡.
,.
(cid:34) 𝑇(cid:213).
𝑡=0.
where 𝛾 ∈ (0, 1] is the discounting factor, and 𝑟 isthe single-step reward.
in text generation, however,such a reward deﬁnition requires sampling over thefuture generated sequence to estimate current stepreward (gong et al., 2019), which may cause thepolicy to end in zero reward region because of highvariance of the gradient (pang and he, 2021).
sincewe guide the generation in every step of decoding,we derive the 𝑡-th step policy gradient (cid:79)𝜃 𝐽𝑡 (𝜃) as:.
e𝑡.
𝜏∼ 𝜋𝜃.
(cid:2)𝜖𝑡 (cid:79)𝜃 log 𝜋 𝜃 (𝑎𝑡 |𝑠𝑡 ) · 𝑟 (𝑥𝑑with importance sampling weight 𝜖𝑡 to stabilize theoptimization (munos et al., 2016), which is:.
𝑡 )(cid:3) ,.
(4).
𝜖𝑡 =.
𝜋 𝜃𝑑 (𝑎𝑡 |𝑠𝑡 )𝜋 𝜃 (𝑎𝑡 |𝑠𝑡 ).
..if we denote a certain token in future contextas 𝑤 ∈ {𝑤future}, single-step self-planning reward𝑟 (𝑥𝑑𝑡 ) can be approximated by the cosine similaritybetween 𝑡-th step hidden state and the embeddedvector of 𝑤 by the lm embedding layers, which is<𝑡 ) · emb(𝑤)) ..log(softmax(ℎ 𝜃𝑑.
𝑟 (𝑥𝑑.
𝑡 ) =.
(cid:213).
𝑤 ∈𝑤future.
(5)given all above deﬁnitions, at 𝑡-th step, we up-.
date 𝜋 𝜃 towards the self-planned 𝜋 𝜃𝑑 as:.
𝜃𝑑 ← 𝜃 + 𝜂.𝑘(cid:213).
𝑖=1.
(cid:79)𝜃 𝐽𝑡 (𝜃𝑑/𝜉)(cid:107)(cid:79)𝜃 𝐽𝑡 (𝜃𝑑/𝜉) (cid:107).
,.
(6).
where 𝜂 is the learning rate and 𝜉 is the temperatureparameter to control the stochastic sampling dur-ing token decoding (keskar et al., 2019).
after 𝑘iterations of reinforced self-planning, the updatedpolicy 𝜋 𝜃𝑑 should produce tokens approaching thefuture context in embedding space, since futurecontext contributes to the calculation of reward 𝑟(eq.
5).5 more details about how we handle edgecases during reinforced self-planning are presentedin appendix b..5in our setting, 𝜂, 𝜉 and 𝑘 are 0.02, 1.3, and 3 respectively..figure 2: compared with the naive method, our rein-forced self-planning approach inﬁlls blanks with ([blk])varying-length tokens while considering both past andfuture tokens, which promote diversity and coherencerespectively.
the context is concatenated to the begin-ning of the reference template..autoregressive language models (alm) suchas gpt-2 (radford et al., 2019), on the other hand,are trained to predict current step token given pasttokens.
they can generate sequences of varyinglengths, but they cannot inﬁll missing tokens withinsentences effectively since they do not considerfuture context.
to enable alms to inﬁll blanksof unspeciﬁed length, prior work has proposedeither retraining a new lm from scratch (shenet al., 2020) or ﬁne-tuning on specially prepareddata (donahue et al., 2020), which are costly andnot easy to extend to new nlg tasks.
as shownin figure 2, we take a reinforcement learning (rl)approach that uses future words after the blank toguide current step inﬁlling generation.
since suchrl guidance only relies on the tokens within itsown to-be-inﬁlled template, we call it reinforcedself-planning.
our method combines the advan-tages of both mlms and alms, requiring neitherre-training nor collecting new data, and thus is eas-ier to extend to other off-the-shelf lms..reinforced self-planning.
at each decodingstep during generation, a vanilla alm will pickthe token 𝑥𝑡 that has the highest probability byapplying an argmax over the softmax output of hid-den states.
we add a self-planning stage betweenthe argmax and softmax function.
following therl framework, we deﬁne the state at step 𝑡 as thegenerated sequences before 𝑡 (i.e., 𝑠𝑡 = 𝑥<𝑡 ), andthe action at step 𝑡 as the 𝑡-th output token (i.e.,.
6680ireallyliketheshowperformedatthetheatre!ienjoyeveryminuteoftheshowatthetheatre!i[blk][blk]theshow[blk][blk]thetheatre!(a)naiveclozeaugmentation:maskedlm(b)self-planningclozeaugmentation:autoregressivelmienjoytheshowonlyperformedatthetheatre!contextcontextbi-directionalattentionuni-directionalattentionreinforcedself-planning++i[blk][blk]theshow[blk][blk]thetheatre!
2.3 computing contextual similarity.
after generating augmented reference sentences,the ﬁnal mars score is computed as a weightedaverage of the similarity between the candidate andeach reference in the augmentation set (includingthe original human reference).
one way to ob-tain similarity scores is using bertscore (zhanget al., 2019), but bertscore requires training onexternal resources to make its outputs more read-able.
therefore, in order to keep all the resourcesused by mars off-the-shelf, we utilize sentence-bert (reimers and gurevych, 2019), which usesthe mean of all token embeddings in a sentence asthe overall sentence-level encoding.
as the sen-tence encoder, we use roberta-large (liu et al.,2019), a common choice in the literature (zhanget al., 2019; reimers and gurevych, 2020).
asshown in eq.
7, we then compute mars score asthe average of the cosine similarities weighted us-ing a geometric progression with a common ratio𝑞 ∈ (0, 1] and a scale factor (start value) 𝑎 ≠ 0:.
mars =.
𝑎𝑞𝑖−1 candt · ref𝑖−1(cid:107)cand(cid:107)t (cid:107)ref𝑖−1(cid:107).
(7).
s.t..𝑎𝑞𝑖−1 = 1 ,.
#𝜆(cid:213).
𝑖=1#𝜆(cid:213).
𝑖=1.
where the candidate encoding is cand, the referenceencodings are ref𝑖 (𝑖 is the index of the augmentedreference under a certain 𝜆, and ref0 marks the zero-mask human reference), and #𝜆 is the number ofmasking ratios we use in §2.1.
different 𝑞 values,as deﬁned by the geometric progression, determinehow much weight each reference contributes.
bydefault, eq.
7 assigns the largest weight to the hu-man reference since it is the gold standard..3 tasks & datasets.
we evaluated mars and compared it with severalpopular nlg metrics on the following three tasks:.
story generation.
we use the roc storiesdataset6 for story generation, which requires candi-date nlg systems to generate coherent endings tofour-sentence stories (mostafazadeh et al., 2016).
the dataset consists of 96,198 examples of par-tially written stories; we take the human-ratedsubset (𝑁=300) released by huse (hashimotoet al., 2019), which contains continuances by (1).
avg.
|cntx.|.
avg.
|h ref.|.
# data(# hr / data).
𝛼.roc34.38newsroom 772.21mocha 161.92.
8.3734.704.69.
300 (20)540 (3)450 (5).
0.640.710.82.ω.
4.122.334.5.table 2: statistics of the three datasets with human rat-ings used in this work.
avg.
|cntx.| and |h ref.|: theaveraged number of tokens in contexts and human ref-erences.
ω: the ratio of the previous two terms (lowerω can indicate a more open-ended task).
# hr: thenumber of human ratings.
𝛼: krippendorff’s alphacoefﬁcient to measure inter-annotator agreement..an industry-level system based on apache solr7,and (2) an open-nmt model with global atten-tion (mccann et al., 2017)..news summarization.
for the news summariza-tion task, we use the newsroom summary dataset.8this dataset contains 1.3 million articles from 38major publications (grusky et al., 2018) and we usethe subset with human ratings (𝑁=540) released bythe authors.9 this dataset contains outputs fromsummarization models: (1) textrank: a sentence-level summarization system inspired by googlepagerank (page et al., 1999), (2) a seq2seq modelwith attention (rush et al., 2015), and (3) pointer-n: a pointer-based neural model (see et al., 2017)trained on newsroom dataset..question answering.
for question answering,we use the mocha dataset,10 which includeshuman ratings on outputs of ﬁve models trainedon six qa datasets (chen et al., 2020).
we con-sider a distributionally-balanced subset (𝑁=450)of these outputs from three systems:(1) ﬁne-tuned gpt-2 (radford et al., 2019), (2) a back-translation model (sennrich et al., 2016), and (3)a mhpg model (bauer et al., 2018) trained on nar-rativeqa (koˇcisk`y et al., 2018) and mcscript (os-termann et al., 2018) datasets..the detailed statistics of these three datasetswe used for this work are shown in table 2. forpre-processing, we removed hashtags and urls inthe text, but leave punctuation and stop words,which can affect lcs matching when computingmask costs.
for all tasks, we use gpt-2 (large,with 774m parameters) as the language model for.
7https://lucene.apache.org/solr8http://lil.nlp.cornell.edu/newsroom/9the subset includes human ratings on four perspectives:coherence, ﬂuency, informative and relevance.
we computethe average of the four scores as an overall human rating..6https://cs.rochester.edu/nlp/rocstories/.
10https://allennlp.org/mocha.
6681roc story generationω = 4.1.newsroom summarizationω = 22.7.mocha question answeringω = 34.5.existing metrics.
solr.
open-nmt.
textrank seq2seq pointer-n gpt-2 back-tran.
mhpg.
bleu-1meteorrouge-lsent.
mover sim.
moverscorebertscoreperplexity.
mars (default)- w/o.
self-plan.
- w/o.
context+- w/o.
both.
naive (mlm).
0.1980.1800.1180.0200.1810.245-0.104.
0.4760.3130.3600.276.
0.449.
0.1040.1160.1950.0150.3910.386-0.073.
0.3970.2120.3340.183.
0.197.
0.2240.2880.0410.1120.0750.154-0.385.
0.3720.2900.107-0.163.
0.201.
0.2680.235-0.1330.0990.3370.3020.011.
0.3360.2450.1600.149.
0.324.
0.1150.2560.0650.1770.2120.181-0.035.
0.3290.314-0.009-0.057.
0.3280.4660.4680.5100.5350.4440.014.
0.5260.4770.134-0.092.
0.114.
0.443.
0.0610.1790.0560.1660.1900.274-0.051.
0.6440.6310.2220.121.
0.307.
0.3180.4090.2470.6100.5920.458-0.128.
0.7410.7090.3030.299.
0.540.table 3: pearson’s 𝑟 correlations with human judgements for mars and seven existing metrics across systemoutputs for three generation tasks.
bleu-1 (papineni et al., 2002), meteor (lavie and agarwal, 2007), androuge-l (lin and och, 2004) use 𝑛-gram matching.
sentence mover’s similarity (clark et al., 2019) and mover-score (zhao et al., 2019) measure similarity using earth mover’s distance.
bertscore (zhang et al., 2019) lever-ages contextual embeddings from pre-trained lms.
as an ablation, we remove self-planning guidance, context,and both.
naive uses roberta-large for reference augmentation (see §2.2).
ω is deﬁned as in table 2..mars, and roberta-large for the naive method.
for the newsroom dataset, some news articles werelonger than the max sequence length of 1024 bpe,and so we cut off the tail end of these examples.
with a single rtx-2080 gpu, cloze augmentationwith 𝜆 = {0 (human ref.
), 20%, 40%, 60%, 80%}takes 0.8 seconds on average per reference, amount-ing to a total augmentation time of 17, 45, and 32minutes for the roc, newsroom and mochatasks respectively.
we show how we pick the mask-ing ratios for different tasks in §4.3..4 evaluation.
4.1 mars better correlates with humans.
as automated metrics are only helpful if they cor-relate sufﬁciently with human judgements, in thissection we examine how mars correlates withhuman judgements compared with prior metrics..system-level correlation.
table 3 shows thecorrelations between human judgements and au-tomated metrics for mars and seven other unsu-pervised metrics, across all nlg systems studiedin our three tasks.
compared with the other metrics,mars achieves the highest correlation with humanjudgements for ﬁve of the seven systems (and com-parable with the top in the other two systems), mak-ing considerable improvements over the next-bestmetric for many of the nlg systems (e.g., 0.370 ↑for back-translation, and 0.231 ↑ for solr).
we.
also notice that mars has greater improvementson more open-ended tasks (e.g., story generation,which has low ω), which corroborates mars’soriginal objective of judging diverse candidatesmore fairly.
as for the baselines, 𝑛-gram matchingmetrics such as bleu correlate poorly with humanratings on such open-ended tasks; bertscore per-forms better on short candidates and high-ω tasks(e.g., qa); and perplexity, as expected, correlatesweakly with human ratings.
the naive method,which uses multiple augmented references of thesame length, improves over bertscore, whichonly uses the original reference..ablation study.
as shown in the lower rows oftable 3, we see that the performance of marsdrops substantially when the crucial componentsare removed.
speciﬁcally, removing self-planninghurts performance more for tasks with longer refer-ences (e.g., story generation) since self-planning ismore helpful when there are more blanks to in-ﬁll,and removing context hurts performance more intasks that are less open-ended (high ω, such as qa)because there is no adequate input for a reasonableaugmentation.
we take these ablation study re-sults as evidence that the techniques we propose inmars are crucial for improving correlation withhuman judgements..task-level correlation visualization.
to visu-alize the correlation between automated metrics.
6682roc story generation.
newsroom summarization mocha question answering.
existing metrics.
reorder (δ) retrieve (δ).
ref.
reorder (δ) retrieve (δ).
ref.
reorder (δ) retrieve (δ).
ref..bleu-1meteorrouge-lsent.
mover sim.
moverscorebertscoreperplexity.
marsw/.
roberta emb.
w/.
glove emb..naive (mlm).
(=) 0(cid:72) 0.041(cid:72) 0.131(cid:72) 0.024(cid:72) 0.131(cid:72) 0.109(cid:72) 0.113.
(cid:72) 0.125(cid:72) 0.087(cid:72) 0.149.
(cid:72) 0.015(cid:72) 0.031(cid:72) 0.123(cid:72) 0.062(cid:72) 0.123(cid:72) 0.127(cid:78) 0.170.
(cid:72) 0.191(cid:72) 0.177(cid:72) 0.156.
0.1370.0940.1940.0190.2760.337-0.089.
0.4590.363.
0.350.
(=) 0(cid:72) 0.132(cid:78) 0.011(cid:72) 0.153(cid:78) 0.011(cid:72) 0.112(cid:72) 0.298.
(cid:72) 0.117(cid:72) 0.052(cid:72) 0.112.
(cid:72) 0.144(cid:72) 0.142(cid:72) 0.035(cid:72) 0.161(cid:72) 0.135(cid:72) 0.026(cid:78) 0.008.
(cid:72) 0.198(cid:72) 0.149(cid:72) 0.190.
0.1760.2440.0360.1360.2360.3440.234.
0.4230.409.
0.314.
(=) 0(cid:72) 0.012(cid:72) 0.032(cid:72) 0.232(cid:78) 0.027(cid:72) 0.101(cid:72) 0.035.
(cid:72) 0.092(cid:72) 0.085(cid:72) 0.098.
(cid:72) 0.424(cid:72) 0.379(cid:72) 0.363(cid:72) 0.161(cid:72) 0.495(cid:72) 0.461(cid:78) 0.026.
(cid:72) 0.504(cid:72) 0.426(cid:72) 0.247.
0.3440.4120.3360.5150.5000.462-0.032.
0.6670.602.
0.639.table 4: we test robustness of mars and seven other automated metrics under attacks from adversarial samplesgenerated by following two attack strategies: (1) reorder: randomly reorders 50% of tokens in the candidates; (2)retrieve: randomly retrieves a sentence from the context as a candidate.
ref.
: correlation of original candidateswith human judgements.
if a metric scores adversarial samples equal to (=) or higher ((cid:78)) than ref., we considersuch metrics not robust under attacks.
robust systems should assign decreased scores ((cid:72)) compared to ref..to the human reference, mars uses augmented ref-erences enriched with information from the contextto provide a fairer judgement..4.2.is mars robust?.
good evaluation metrics ought to also be able to de-tect adversarial examples by assigning them lowerscores than well-formed candidates.
as shown intable 4, uni-gram matching bleu-1 cannot de-tect reordered sequences, while rouge-l scoresreordered sequence higher occasionally if token-swapping leads to more lcs.
sentence mover’ssimilarity combines word and sentence embed-dings and thus is more capable of recognizing re-ordered samples than moverscore.
perplexity candetect reordered examples effectively, but is unableto detect retrieved sentences, as they are usuallywell-formed.
mars, on the other hand, has thebest robustness against adversarial samples, possi-bly because multiple context-infused augmentedreferences help mars detect adversarial samplesmore reliably.
we also study the effects of contex-tual embeddings we use in §2.3—when switchingto glove embeddings (pennington et al., 2014),which are not contextual, mars is less able todetect adversarial samples, especially reorderedones.
the naive method, which by default usesroberta embedding, achieves comparable robust-ness as mars but its task-level correlations withhumans (ref.)
are generally lower than mars, po-tentially because its ﬁxed-length cloze generationlimits the diversity of augmented references..figure 3: correlation between bertscore (left) andmars (right) with human judgements for mochaqa.
the 𝑥-axis is the automated metric score and 𝑦-axis is the human judgement.
points in different col-ors represent generation outputs of three nlg systems:gpt-2 (red circles), back-translation (green triangles),and mhpg (blue squares)..and human judgements, we consider the mochaqa task as an example and plot the correlations ofbertscore (left) and mars (right) with humanjudgements.
as shown in figure 3, compared withmars, bertscore has more candidates in theupper-left corner of the plot (i.e., low bertscorebut high human judgement).
many of these aregenerated by gpt-2 and mhpg, which, based onmanual examination, tend to provide more detailsin the answer than the human reference.
for in-stance, given a context about shopping, one ques-tion is “did they need to buy any meat?”.
thehuman reference answer is simply “yes, they did.”,but gpt-2 returns “yes, they bought chicken anda roast.”, which is more detailed, even containingitem names derived from the context.
whereasbertscore cannot evaluate such cases where thegenerated candidate is over-described with respect.
6683bertscore(r=0.46)humanjudgementgpt-2back-tranmhpg0.000.250.500.751.0012345gpt-2back-tranmhpg0.000.250.500.751.0012345mars(r=0.67)humanjudgementroc story generation.
error.
example.
{𝜆}maxpearson’s 𝑟avg.
𝜎.
{𝜆}maxpearson’s 𝑟avg.
𝜎.
{𝜆}maxpearson’s 𝑟avg.
𝜎.
0% (ref.).
20% 40% 60% 80%.
0.411-.
0.4320.027.
0.4440.046.
0.4590.055.
0.4520.059.newsroom summarization.
0% (ref.).
20% 40% 60% 80%.
0.395-.
0.4070.061.
0.4160.062.
0.4230.063.
0.4110.068.mocha question answering.
0% (ref.).
20% 40% 60% 80%.
0.658-.
0.6670.074.
0.6490.104.
0.6030.117.
0.5840.125.table 5: evaluating correlation with human judge-ments for various max masking ratios (𝜆max) used inmars.
0% masking (ref.)
means only the human ref-erence was used to score candidates.
we also showthe averaged standard deviation of the cosine similar-ities between the candidate and augmented referencesacross all samples..4.3 choosing masking ratios for mars.
the masking ratios for mars are set using the hy-perparameter {𝜆}max, which corresponds to marsusing masking ratios from 0% to {𝜆}max in in-crements of 20%, e.g., {𝜆}max = 40% indicates𝜆 ∈ {0%, 20%, 40%}.
in preliminary experi-ments, we observed that {𝜆}max varied for differ-ent datasets.
thus, for our three generation tasks,we evaluate mars performance given different{𝜆}max, as shown in table 5. we ﬁnd that tasks thatwere more open-ended (low ω; e.g., story genera-tion) beneﬁted from higher {𝜆}max, which created amore diverse set of augmented references, whereastasks that were less open-ended (high ω; e.g., qa)worked better with lower {𝜆}max, which kept theaugmented references more similar to the original..4.4 error analysis.
we analyzed cases where mars score substan-tially differed from human judgements.
from testset outputs, we found that errors could often be cat-egorized into one of three types (shown in table 6):(1) out of vocabulary errors, often induced byunknown tokens in the candidates, (2) confusionerrors, where candidates are simply copied fromcontext, and (3) inference errors, where the candi-dates are further inferences of the context based oncommonsense knowledge.
in these cases, humanannotators tended to assign higher scores, whereas,mars over-penalized them..oov(roc).
context: ...waltz dance at wedding...gold: all the guests gaspedwhen they saw the couples’ skill!
candidate: all the guests gaspedwhen they saw the unk unkhuman: 0.392 mars: 0.198.confusion(newsroom).
context: ...bidding on a neighborhood...gold: a neighborhood namedfor its former orchards inspires loyaltyand bidding wars.
candidate: living there cherrydale liesnorth of interstate... (a sentence extractedfrom context)human: 0.700 mars: 0.399.inference(mocha).
context: ...washing cloths...q: why did they do the laundry?
gold: to clean their clothescandidate: because they were dirty.
human: 0.400 mars: 0.083.table 6: error analysis of mars.
we investigated threetypical types of errors within the samples which re-ceived large differences between the mars score andhuman ratings.
gold: human written references..5 human judgement.
we conducted human evaluation on amazon me-chanical turk (mturk) to further study the qualityof mars augmentation.
in total 150 participantswere randomly assigned to evaluate the three tasks.
participants (61.3% male and 38.7% female) wereall from the united states and above 18 years old,with an average age of 34.7 years old.
each partici-pant was paid 75 cents for completing 14 questionsin each questionnaire (average completion time perquestionnaire was about 5.11 minutes)..results we conducted paired sample 𝑡-tests toexamine how much the augmentation samples re-semble the original human references regardingrelevance to context and readability.
as shown intable 7, in terms of relevance to context, marshad no statistically signiﬁcant difference comparedwith original human references in newsroom andmocha datasets, but was rated as even more rel-evant to the generation context than the humanreference in the roc dataset (mars mean = 5.07> human ref.
mean = 4.95), possibly because re-inforced self-planning guided the augmentation tobe more related to the context.
in terms of readabil-.
6684roc.
newsroom.
mocha.
ori.
naive mars ori.
naive mars ori.
naive mars.
relevance.
readability.
overall.
meanp.meanp.meanp.4.95-.
5.67-.
5.69-.
4.81.00*.
5.53.11.
5.31.12.
5.07.04*.
5.40.05.
5.42.30.
4.62-.
4.54-.
4.87-.
4.50.05.
4.31.12.
4.57.10.
4.61.95.
4.59.41.
4.75.22.
5.16-.
5.41-.
4.62-.
4.61.00*.
5.23.16.
4.44.07.
4.97.10.
5.33.29.
4.68.10.table 7: human evaluation results on relevance (to context), readability, and overall quality of mars andnaive augmentation method.
all results are compared with the original human reference (ori.).
text was scoredon a scale from 1-7. 𝑝 value describes the signiﬁcance of difference.
(* corresponds to 𝑝 < 0.05, ** to 𝑝 < 0.01and *** to 𝑝 < 0.001.).
ity, both mars and naive were rated lower thanthe original but not signiﬁcantly; we take this as acompromise of cloze style augmentation.
no sta-tistically signiﬁcant differences were seen betweenthe original and mars augmentation in overallratings across the three tasks.
these results furtherconﬁrm that augmented examples from mars areof similar quality to the original human references..6 related metrics.
unsupervised metrics.
in addition to the met-rics we directly compared with previously, otherunsupervised metrics have also been proposed.
ter (snover et al., 2006), character (wanget al., 2016), and chrf (popovi´c, 2017) focus oncharacter-level overlaps instead of 𝑛-gram match-ing.
similar to bertscore, yisi (lo, 2019) andbertr (mathur et al., 2019) leverage pre-trainedcontextual embeddings to better capture similarity.
δbleu (galley et al., 2015) adds human anno-tated sentences as negative references.
bawdenet al.
(2020) ﬁnd the gain from multiple referencescan be limited by inherent weaknesses in bleu.
we considered lessons from many of the aboveworks while designing mars..learned metrics.
compared with unsupervisedmetrics, learned metrics collect human supervi-sions (freitag et al., 2020a; chaganty et al., 2018)or train on specially prepared data of a certain do-main (sellam et al., 2020; rei et al., 2020).
otherapproaches train on related tasks and use these mod-els as metrics for the original task (goodrich et al.,2019; eyal et al., 2019).
whereas learned metricsmay have limited applicability on tasks where nosuch resources are available, mars fully exploitsthe few-shot learning abilities of off-the-shelf lms.
and therefore does not require additional training..task-speciﬁc metrics.
finally, many metricshave been proposed for task-speciﬁc evaluation,such as leic (cui et al., 2018) and cider (vedan-tam et al., 2015) for image captioning, par-ent (dhingra et al., 2019) for table-to-text, andeasse (alva-manchego et al., 2019) for sentencesimpliﬁcation.
mars, with some modiﬁcations,can potentially be extended to these tasks..7 limitations.
mars can be limited by the lm that it uses—for instance, the total length of context + refer-ence/candidate is limited by the max sequencelength of the lm used.
additionally, our workhas focused on english, and mars may requirenon-trivial modiﬁcations to handle cases where thecontext and reference/candidate are in different lan-guages, such as machine translation.
future work,could potentially extend mars to these scenariosusing multi-lingual sequence-to-sequence modelssuch as multilingual-t5 (xue et al., 2020).
we alsoanalyzed errors and found that mars sometimesunder-scores candidates that contained unknown to-kens or were copied directly from the context (seeappendix c for examples and further analysis)..8 conclusion.
we have proposed mars, a context-aware andeasy-to-deploy nlg metric built upon an off-the-shelf language model (gpt-2).
on three contextualnlg tasks, we show that mars better correlateswith human judgements compared with seven otherunsupervised metrics.
requiring neither costly hu-man supervision nor additional training, marscan be applied to a broad range of nlg tasks..6685ethical considerations.
the goal of mars is to aid the evaluation of nlgmodels, and hence we draw attention to several eth-ical considerations.
first, the augmented referencesof mars can be affected by certain biases from thelm it is based on (e.g., gpt-2) (liu et al., 2021),though those biases may be partially mitigated bythe relatively narrow scope of cloze completion andby generations being guided by given context andhuman references.
second, mars facilitates eval-uation and therefore development of nlg models,for which a major ethical consideration is that theycan mimic target properties in training data thatare undesirable.
this is especially true of modelstrained on non-contemporary data that does not rep-resent current norms and practices.
these biasescan lead to ethical concerns if users or deployers ofmodels are not aware of these issues or do not ac-count for them.
more generally, nlg models canalso be used in malicious ways such as to generatefake news or spam, which we strongly discourage.
finally, our experiments and analysis are done inenglish, and therefore we do not claim that ourﬁndings will generalize across all languages, al-though our framework has potential to be extendedto other languages with necessary modiﬁcations..references.
fernando alva-manchego, louis martin, carolinascarton, and lucia specia.
2019. easse: easier au-in pro-tomatic sentence simpliﬁcation evaluation.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp): system demonstra-tions, pages 49–54, hong kong, china.
associationfor computational linguistics..lisa bauer, yicheng wang, and mohit bansal.
2018.commonsense for generative multi-hop question an-swering tasks.
in proceedings of the 2018 confer-ence on empirical methods in natural languageprocessing, pages 4220–4230..arun chaganty, stephen mussmann, and percy liang.
2018. the price of debiasing automatic metrics innatural language evalaution.
in proceedings of the56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 643–653, melbourne, australia.
associationfor computational linguistics..anthony chen, gabriel stanovsky, sameer singh, andmatt gardner.
2020. mocha: a dataset for train-ing and evaluating generative reading comprehen-sion metrics.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 6521–6532..elizabeth clark, asli celikyilmaz, and noah a. smith.
2019. sentence mover’s similarity: automatic eval-in proceedings ofuation for multi-sentence texts.
the 57th annual meeting of the association for com-putational linguistics, pages 2748–2760, florence,italy.
association for computational linguistics..yin cui, guandao yang, andreas veit, xun huang,and serge belongie.
2018. learning to evaluate im-in proceedings of the ieee con-age captioning.
ference on computer vision and pattern recognition,pages 5804–5812..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..bhuwan dhingra, manaal faruqui, ankur parikh,ming-wei chang, dipanjan das, and william co-hen.
2019. handling divergent reference texts whenevaluating table-to-text generation.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4884–4895, flo-rence, italy.
association for computational linguis-tics..chris donahue, mina lee, and percy liang.
2020. en-abling language models to ﬁll in the blanks.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2492–2501, online.
association for computational lin-guistics..rachel bawden, biao zhang, lisa yankovskaya, an-dre t¨attar, and matt post.
2020. a study in im-proving bleu reference coverage with diverse au-in findings of the associa-tomatic paraphrasing.
tion for computational linguistics: emnlp 2020,pages 918–932, online.
association for computa-tional linguistics..esin durmus, he he, and mona diab.
2020. feqa: aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5055–5070, online.
association for computational lin-guistics..peter f brown, stephen a della pietra, vincent jdella pietra, jennifer c lai, and robert l mercer.
1992. an estimate of an upper bound for the entropyof english.
computational linguistics, 18(1):31–40..matan eyal, tal baumel, and michael elhadad.
2019.question answering as an automatic evaluation met-in proceed-ric for news article summarization.
ings of the 2019 conference of the north american.
6686chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3938–3948, min-neapolis, minnesota.
association for computationallinguistics..markus freitag, george foster, david grangier, andcolin cherry.
2020a.
human-paraphrased refer-in pro-ences improve neural machine translation.
ceedings of the fifth conference on machine trans-lation, pages 1183–1192, online.
association forcomputational linguistics..markus freitag, david grangier, and isaac caswell.
2020b.
bleu might be guilty but references are notin proceedings of the 2020 conferenceinnocent.
on empirical methods in natural language process-ing (emnlp), pages 61–71, online.
association forcomputational linguistics..michel galley, chris brockett, alessandro sordoni,yangfeng ji, michael auli, chris quirk, mar-garet mitchell, jianfeng gao, and bill dolan.
2015.deltableu: a discriminative metric for generationtasks with intrinsically diverse targets.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 2: short papers), pages 445–450,beijing, china.
association for computational lin-guistics..hongyu gong, suma bhat, lingfei wu, jinjun xiong,and wen-mei hwu.
2019. reinforcement learn-ing based text style transfer without parallel train-ing corpus.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 3168–3180, minneapolis, minnesota.
associ-ation for computational linguistics..ben goodrich, mohammad ahmad saleh, peter liu,and vinay rao.
2019. assessing the factual accu-racy of text generation..max grusky, mor naaman, and yoav artzi.
2018.newsroom: a dataset of 1.3 million summaries withdiverse extractive strategies.
in proceedings of the2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 708–719, new orleans, louisiana.
as-sociation for computational linguistics..prakhar gupta, shikib mehri, tiancheng zhao, amypavel, maxine eskenazi, and jeffrey p bigham.
investigating evaluation of open-domain di-2019.alogue systems with human generated multiple ref-in proceedings of the 20th annual sig-erences.
dial meeting on discourse and dialogue, pages 379–391..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation for.
natural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689–1701, minneapolis, min-nesota.
association for computational linguistics..nitish shirish keskar, bryan mccann, lav r varshney,caiming xiong, and richard socher.
2019. ctrl: aconditional transformer language model for control-lable generation.
arxiv preprint arxiv:1909.05858..tom´aˇs koˇcisk`y, jonathan schwarz, phil blunsom,chris dyer, karl moritz hermann, g´abor melis, andedward grefenstette.
2018. the narrativeqa readingcomprehension challenge.
transactions of the asso-ciation for computational linguistics, 6:317–328..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231, prague, czech repub-lic.
association for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..chin-yew lin and franz josef och.
2004. auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
in proceedings of the 42nd annual meet-ing of the association for computational linguistics(acl-04), pages 605–612, barcelona, spain..ruibo liu, chenyan jia, jason wei, guangxuan xu,lili wang, and soroush vosoughi.
2021. mitigatingpolitical bias in language models through reinforcedcalibration.
in proceedings of the aaai conferenceon artiﬁcial intelligence..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..chi-kiu lo.
2019. yisi - a uniﬁed semantic mt qualityevaluation and estimation metric for languages withdifferent levels of available resources.
in proceed-ings of the fourth conference on machine transla-tion (volume 2: shared task papers, day 1), pages507–513, florence, italy.
association for computa-tional linguistics..nitika mathur, timothy baldwin, and trevor cohn.
2019. putting evaluation in context: contextualembeddings improve machine translation evaluation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2799–2808, florence, italy.
association for compu-tational linguistics..6687bryan mccann, james bradbury, caiming xiong, andrichard socher.
2017. learned in translation: con-textualized word vectors.
in advances in neural in-formation processing systems, pages 6294–6305..shikib mehri and maxine eskenazi.
2020. usr: anunsupervised and reference free evaluation metricfor dialog generation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 681–707, online.
association forcomputational linguistics..ning miao, hao zhou, lili mou, rui yan, and leili.
2019. cgmh: constrained sentence generationby metropolis-hastings sampling.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 33, pages 6834–6842..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a cor-pus and cloze evaluation for deeper understanding ofin proceedings of the 2016commonsense stories.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 839–849, san diego,california.
association for computational linguis-tics..r´emi munos, tom stepleton, anna harutyunyan, andmarc bellemare.
2016. safe and efﬁcient off-policyin advances in neural in-reinforcement learning.
formation processing systems, pages 1054–1062..preksha nema and mitesh m. khapra.
2018. towards abetter metric for evaluating question generation sys-in proceedings of the 2018 conference ontems.
empirical methods in natural language processing,pages 3950–3959, brussels, belgium.
associationfor computational linguistics..jekaterina novikova, ondˇrej duˇsek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedingsnew evaluation metrics for nlg.
of the 2017 conference on empirical methods innatural language processing, pages 2241–2252,copenhagen, denmark.
association for computa-tional linguistics..simon ostermann, ashutosh modi, michael roth, ste-fan thater, and manfred pinkal.
2018. mcscript:a novel dataset for assessing machine comprehen-sion using script knowledge.
in proceedings of theeleventh international conference on language re-sources and evaluation (lrec 2018)..lawrence page, sergey brin, rajeev motwani, andterry winograd.
1999. the pagerank citation rank-ing: bringing order to the web.
technical report,stanford infolab..richard yuanzhe pang and he he.
2021. text gen-eration by learning from off-policy demonstrations.
international conference on learning representa-tions (iclr 21’)..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..david pisinger.
1995. algorithms for knapsack prob-.
lems..maja popovi´c.
2017. chrf++: words helping charac-in proceedings of the second con-ter n-grams.
ference on machine translation, pages 612–618,copenhagen, denmark.
association for computa-tional linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..ricardo rei, craig stewart, ana c farinha, and alonlavie.
2020. comet: a neural framework for mtevaluation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2685–2702, online.
associa-tion for computational linguistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processing.
association for computational linguistics..nils reimers and iryna gurevych.
2020. makingmonolingual sentence embeddings multilingual us-in proceedings of theing knowledge distillation.
2020 conference on empirical methods in natu-ral language processing.
association for computa-tional linguistics..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-in proceedings of the 2015tence summarization.
conference on empirical methods in natural lan-guage processing, pages 379–389, lisbon, portugal.
association for computational linguistics..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881–7892, online.
association for computa-tional linguistics..6688via insertion-based generative pre-training.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 8649–8670, online.
association for compu-tational linguistics..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m. meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563–578, hongkong, china.
association for computational lin-guistics..wanrong zhu, zhiting hu, and eric xing.
2019. text.
inﬁlling.
arxiv preprint arxiv:1901.00158..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96..tianxiao shen, victor quach, regina barzilay, andtommi jaakkola.
2020. blank language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 5186–5198, online.
association for computa-tional linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of association for machine transla-tion in the americas, volume 200. cambridge, ma..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 32..wilson l taylor.
1953..“cloze procedure”: a newtool for measuring readability.
journalism quarterly,30(4):415–433..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566–4575..weiyue wang, jan-thorsten peter, hendrik rosendahl,and hermann ney.
2016. character: translationin proceedings of theedit rate on character level.
first conference on machine translation: volume2, shared task papers, pages 505–510, berlin, ger-many.
association for computational linguistics..linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, adityabarua, and colin raffel.
2020. mt5: a mas-sively multilingual pre-trained text-to-texttrans-former.
arxiv preprint arxiv:2010.11934..tsuta yuma, naoki yoshinaga, and masashi toyoda.
2020. ubleu: uncertainty-aware automatic evalua-tion method for open-domain dialogue systems.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics: student re-search workshop, pages 199–206, online.
associa-tion for computational linguistics..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019. bertscore: eval-uating text generation with bert..yizhe zhang, guoyin wang, chunyuan li, zhegan, chris brockett,and bill dolan.
2020.pointer: constrained progressive text generation.
6689algorithm 2: generate, judge and reviseinput: template {𝜙(𝑥𝑖)} 𝑁.𝑖=1, max guess 𝜎,.
and lm perplexity checker ppl..𝑖=1 into [b] and [t];.
group {𝜙(𝑥𝑖)} 𝑁init ﬁnal output 𝑠;foreach block do.
𝑖 ← 0;init priority queue 𝑞, buffer 𝑠(cid:48);if [t] then.
append [t] to 𝑠;.
else if [b] then.
while 𝑖 < 𝜎 + |[b]| doif next is [t] then.
𝑤 ← self-planning gen.;.
else.
𝑤 ← open-ended gen.;.
end𝑠(cid:48) ← 𝑠 + 𝑤;record (ppl(𝑠(cid:48) + [t]), 𝑠(cid:48)) in 𝑞;𝑖 ← 𝑖 + 1;.
end𝑠 ← 𝑠 + lowest ppl 𝑠(cid:48) pop from 𝑞;.
end.
endreturn augmented reference 𝑠;.
modes: self-planning generation (if there is futurecontext) and open-ended generation (otherwise).
we use a priority queue to store each step genera-tion and its corresponding ppl for quick revisionsafterwards..appendix a: dp-based token maskingalgorithm.
as part of eq.1 in the main paper, we deﬁne theidf score given token 𝑥𝑖 and a corpus 𝑋 containing𝑀 documents as:.
idf(𝑥𝑖, 𝑋) = − log.
𝕀[𝑥𝑖 ∈ 𝑋 𝑗] ,.
1𝑀.𝑀(cid:213).
𝑗=1.
where 𝕀[·] is the indicator function.
we present ourdp-based masking algorithm in algorithm 1:.
algorithm 1: dp-based token maskinginput: human reference {𝑥𝑖 } 𝑁.𝑖=1, maskingratio 𝜆, and task-speciﬁc factor 𝛼..compute 𝑣𝑖 for each 𝑥𝑖 with 𝛼 (eq.
1);compute 𝑤𝑖 depending on lcs for each 𝑥𝑖;init dp-table 𝑇 [𝑁 + 1] [𝑊max + 1] with all 0;for 𝑖 = 1, 2, .
.
.
, 𝑁 do.
for 𝑗 = 1, 2, .
.
.
, 𝑊max doif 𝑗 − 𝑤𝑖−1 < 0 then.
𝑇 [𝑖] [ 𝑗] = 𝑇 [𝑖 − 1] [ 𝑗];record masking choice 𝜙(𝑥𝑖);.
𝑇 [𝑖] [ 𝑗] = max(𝑇 [𝑖 − 1] [ 𝑗],𝑇 [𝑖 − 1] [ 𝑗 − 𝑤𝑖−1] + 𝑣𝑖−1);record masking choice 𝜙(𝑥𝑖);.
else.
end.
end.
end{𝜙(𝑥𝑖) 𝑁𝑖=1return best masking strategy {𝜙(𝑥𝑖) 𝑁𝑖=1.}
← backtracking via records;};.
appendix b: generate, judge, and revisealgorithm.
the complete procedure for augmenting human ref-erences is presented in algorithm 2. for a giventemplate, we ﬁrst group the tokens into a block-by-block form with blank blocks ([b]) and textblocks ([t]).
then, we generate varying lengthsof tokens, iteratively concatenating them with nexttext block, and judging them based on ppl, and ﬁ-nally revising current generations accordingly.
weuse the language modeling ability of lm to checkthe perplexity of the current sequence, and set ahyper-parameter 𝜎 to control the maximum ex-tended generation (for a lower ppl)..depending on whether there is a subsequenttext block, the generation will switch between two.
6690