language model augmented relevance score.
ruibo liu1.
jason wei2.
soroush vosoughi1.
1dartmouth college.
2google ai language.
ruibo.liu.gr@dartmouth.edu jasonwei@google.comsoroush.vosoughi@dartmouth.edu.
abstract.
although automated metrics are commonlyused to evaluate nlg systems, they often cor-relate poorly with human judgements.
newermetrics such as bertscore have addressedmany weaknesses in prior metrics such asbleu and rouge, which rely on ğ‘›-grammatching.
these newer methods, however, arestill limited in that they do not consider thegeneration context, so they cannot properly re-ward generated text that is correct but deviatesfrom the given reference..in this paper, we propose language modelaugmented relevance score (mars), a newcontext-aware metric for nlg evaluation.
mars leverages off-the-shelf language mod-els, guided by reinforcement learning, to cre-ate augmented references that consider boththe generation context and available humanreferences, which are then used as additionalreferences to score generated text.
comparedwith seven existing metrics in three commonnlg tasks, mars not only achieves highercorrelation with human reference judgements,but also differentiates well-formed candidatesfrom adversarial samples to a larger degree..1.introduction.
automated metrics such as bleu (papineni et al.,2002) and rouge (lin, 2004) are popular meth-ods for evaluating natural language generation(nlg) systems.
compared with human evalua-tion, they are cheaper and faster, and accordingly,they often serve as essential metrics for benchmark-ing the performance of nlg models (novikovaet al., 2017).
despite their widespread use, how-ever, these automated metrics often poorly correlatewith ratings given by human judges, particularlyfor datasets in which only a single human referenceexists (gupta et al., 2019; novikova et al., 2017).
moreover, these automated metrics only capture.
figure 1: existing metrics compare the candidate withthe human reference but ignore context.
mars (ourmethod) augments the human reference while consider-ing the context, which allows it to provide evaluationscores that correlate highly with human references..similarities between generated sentences and refer-ence candidates, crucially ignoring provided con-texts that are relevant for evaluating the answer incontextual nlg tasks, such as story generation,news summarization, and question-answering (taoet al., 2018; nema and khapra, 2018)..table 1 shows a story generation1 examplethat exempliï¬es some weaknesses of several com-mon metrics.
perplexity (ppl) (brown et al.,1992) successfully detects ungrammatical sen-tences, but it fails to distinguish legitimate novelcontinuations and copy-and-pasted ones.
rely-ing on surface-level ğ‘›-gram matching, bleu-1and rouge-l2 cannot detect reordering effec-tively, and wrongly score the well-formed candi-date lower than its retrieval-based adversarial ex-ample.
bertscore (zhang et al., 2019) leveragescontextual embeddings from bert (devlin et al.,2019), thus mitigating the above challenges, butstill does not fairly evaluate candidates that cor-rectly align with the context but happen to differ.
1the roc story generation task asks systems to generate.
a legitimate ending for a four-sentence story..2l stands for longest common sequence matching..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6677â€“6690august1â€“6,2021.Â©2021associationforcomputationallinguistics6677contextexsitingmetrics:ppl/bleu/rouge/bert-score/etc.ours:marshumanreferencecandidatelmlmaugmentedreferencescontext.
wendy was driving down the road.
she heard her car making a noise.
she pulled over to examine the problem.
there was nothing but oil all on the road from her car..human reference.
she called for help and waited to get her car ï¬xed.
ppl bleu-1 rouge-l bertscore mars.
candidate.
her fears were conï¬rmed when her engine was smoking.
reorder.
her conï¬rmed engine fears her when was were smoking.
retrieve.
she heard her car making a noise..75.58405.6063.93.
0.2230.2230.337.
0.1820.1820.400.
0.3380.2650.406.
0.5740.3520.448.table 1: in this story generation example, mars is the only metric that gives the well-formed candidate a higherscore than two adversarial examples.
the human rating of the candidate averaged over 20 judgements is 5.05 out of6.00. two adversarial examples are generated by reordering the tokens of the candidate (as weak nlg systemswhose generation is not readable) and retrieveing a sentence from the context (as systems with no generationability).
we boxed the cases where the adversarial example does not score lower than the well-formed candidate..from the provided reference example.
in our exam-ple, the candidate â€œ... her engine was smokingâ€ isreasonable but deviates from the human reference,and so bertscore rates it relatively low (0.338 outof 1.0), thus correlating poorly with human rating,which was high (5.05 out of 6.00)..to address the above issues, prior studies haveproposed a number of promising remedies.
oneline of work has proposed to combine human rat-ings with automated metrics (durmus et al., 2020;chaganty et al., 2018, inter alia).
for instance, inhuse score, hashimoto et al.
(2019) leverages thedifferences between perplexity and human judge-ments to consider both quality and diversity ofgenerated text.
another line has proposed train-ing separate neural models to aid automated met-rics (mehri and eskenazi, 2020; yuma et al., 2020,inter alia).
for instance, bleurt (sellam et al.,2020) ï¬ne-tunes bert (devlin et al., 2019) on syn-thetic reference-candidate pairs for machine trans-lation.
these methods, however, are often limitedin practical use, because the high-cost human rat-ings are not always available for every dataset, andthe data- or system-speciï¬c training is not easilyextended to other domains (zhang et al., 2019), andcan even bias the evaluation (freitag et al., 2020b)..in this paper, we present mars (languagemodel augmented relevance score), a new nlgevaluation metric that requires neither supervisionfrom human ratings nor additional training on spe-ciï¬c domains.
as shown in figure 1, instead ofcomparing candidates only with human written ref-erences, as many prior metrics do, mars uses amixture of both human and augmented references.
speciï¬cally, mars masks tokens in the referenceto create templates, and then uses the context andtemplates to generate augmented references by in-ï¬lling the masked parts with an lm guided by rein-forcement learning.
the augmented references thus.
incorporate information from both the context andthe human reference, and are enriched with lexicaland syntactic diversity, facilitating fairer evaluationof candidates.
finally, we compute the score asa weighted average of the similarity between thecandidate and the set of augmented references inthe contextual embedding space..the advantages of mars are three-fold.
first,mars correlates highly with human judgements.
we apply mars to three diverse nlg tasks, anddemonstrate that, compared with seven popularnlg metrics, mars better correlates with hu-man judgements and is robust against adversarialattacks.
second, mars is context-aware.
un-like existing metrics that only consider the givenhuman reference, we use a constrained nlg ap-proach to incorporate the generation context intoaugmented references, thus alleviating bias againstdiverse candidates.
third, mars is easy to deployand extend.
built on off-the-shelf lms, marsrequires neither human supervision nor additionaltraining for speciï¬c domains, and can thereforeserve as a general-purpose metric for a broad rangeof nlg applications, as we will demonstrate forthree common nlg tasks: story generation, newssummarization, and question-answering..2 approach.
mars comprises three steps.
first, we mask outnon-important tokens from the human reference toproduce templates for augmentation (Â§2.1).
sec-ond, we guide off-the-shelf lms to generate refer-ence augmentation on these templates via a rein-forced self-planning algorithm (Â§2.2).
finally, wecompute a weighted average score that reï¬‚ects theoverall similarity between the candidate and the setof augmented references (Â§2.3)..66782.1 human reference token masking.
the ï¬rst step in mars is to take in the given hu-man reference and generate templatesâ€”maskedversions of the human referenceâ€”which can thenbe used to generate augmented references.
ourmasking procedure can be viewed as a reversedprocess of prior insertion- and template-based gen-eration approaches (zhang et al., 2020; miao et al.,2019); whereas these generation approaches startwith templates of important tokens and then ï¬llin the details to generate complete sentences, ourmasking procedure starts with the complete sen-tence (i.e., the human reference) and then masksout unimportant tokens to generate templates.
tobetter explain our masking procedure, we introducetwo concepts, mask priority and mask cost:.
mask priority.
we compute a mask priority ğ‘£ğ‘–for each token ğ‘¥ğ‘–, which captures the priority ofmasking ğ‘¥ğ‘–, where non-important words shouldreceive higher priority.
we compute ğ‘£ğ‘– as a func-tion of two things: the inverse document frequency(idf) of ğ‘¥ğ‘–, and the part-of-speech (pos) of ğ‘¥ğ‘–:.
ğ‘£ğ‘– =.
ğ›¼(pos [ğ‘¥ğ‘–])idf(ğ‘¥ğ‘–, ğ‘‹).
,.
(1).
where ğ›¼ is a function that assigns a weight to eachpos tag.3 common tokens across the corpus ğ‘‹(e.g., stop words, with low idf) will receive highmask priority.
tokens responsible for descriptiondetails will also be assigned high mask prioritybased on their part-of-speech (e.g., adjectives aremainly used for details and so they are given higherpriority of being masked)..mask cost.
for each token ğ‘¥ğ‘–, we also computea mask cost ğ‘¤ğ‘–.
tokens that appear in both con-text and human reference should have high mask-ing cost as they are deemed context-carrying.
weuse the longest common sequence (lcs) match-ing between the context and the human referenceto identify these context-carrying tokens.
in ourexperiments, we set the ğ‘¤ğ‘– of these tokens to 10and the default ğ‘¤ğ‘– of all other tokens to 1. we useğœ† to denote the ratio of tokens to be masked in asentence of ğ‘ tokens, and deï¬ne ğ‘Šmax = ğœ† Â· ğ‘ asthe maximum cost allowed..3ğ›¼ varies for each task.
empirically, we ï¬nd that it workswell to assign adjectives, adverbs, and nouns higher weightsthan other parts-of-speech.
for our setting, we assign weightsof 4, 3, 2 to the above three types..dp-based token masking.
now that for eachtoken we have a mask priority and a mask cost,we aim to choose a set of tokens to mask with thehighest possible sum of priorities for which thesum of mask costs is not greater than ğ‘Šmax.
givena function ğœ™(ğ‘¥ğ‘–) = {1, 0} where 1 means token ğ‘¥ğ‘–is masked and 0 means it remains, the objective oftoken masking can be expressed as follows:.
max.
ğ‘£ğ‘– Â· ğœ™(ğ‘¥ğ‘–) ,.
s.t..ğ‘¤ğ‘– Â· ğœ™(ğ‘¥ğ‘–) â‰¤ ğ‘Šmax ..(2).
ğ‘(cid:213).
ğ‘–=1ğ‘(cid:213).
ğ‘–=1.
such a goal is actually a np-complete combina-torial optimization problem, called the knapsackproblem (pisinger, 1995), which we solve usingdynamic-programming (dp).
in general, the mask-ing strategy aggressively harvests tokens of highmask priority while keeping the cost of masked to-kens from exceeding the mask cost limitation ğ‘Šmax.
the detailed dp algorithm for solving this problemis shown in appendix a..2.2 self-planning cloze augmentation.
after creating the templates described in Â§2.1, weproduce augmented reference examples based onboth the templates as well as the generation context.
this procedure can be seen as a mixture of hard-and soft-constrained nlg, where the template to-kens pre-exist with some blanks, and the system,conditioned on the context, aims to ï¬ll in the blanks.
we henceforth refer this process of creating aug-mented references as cloze4 augmentation..background.
masked language models (mlm)such as roberta (liu et al., 2019) and bert (de-vlin et al., 2019) are trained to predict maskedtokens within sentences, and thus are able to docloze augmentation off-the-shelf.
however, with-out architecture-level modiï¬cation, mlms are onlyable to inï¬ll a pre-determined number of missingtokens (zhu et al., 2019).
this is especially prob-lematic sinceâ€”if they are directly used to augmentreferencesâ€”all the augmented references will havethe same number of tokens as that of the originalhuman reference.
we believe this unnecessarilyconstrains augmentation diversity, and thus con-sider it as a naive method in our evaluations (Â§4)..4a cloze test (taylor, 1953) is a language test where aportion of language is removed and the participant is asked toreplace the missing language item..6679ğ‘ğ‘¡ = ğ‘¥ğ‘¡ ).
we take the softmax output of the lasthidden states (with parameter ğœƒ) as the policy ğœ‹ ğœƒ ,since it is the probability of picking token ğ‘¥ğ‘¡ (actionğ‘ğ‘¡ ) given the state ğ‘ ğ‘¡ = ğ‘¥<ğ‘¡ .
similarly, we denotethe policy after reinforced self-planning as ğœ‹ ğœƒğ‘‘ ..typically, the rl objective is to maximize theexpectation of total reward ğ½, summed over ğ‘‡ stepson the trajectory ğœ induced by ğœ‹ ğœƒ :.
ğ½ (ğœƒ) = eğœâˆ¼ ğœ‹ğœƒ.
(3).
(cid:35).
ğ›¾ğ‘¡ğ‘Ÿğ‘¡.
,.
(cid:34) ğ‘‡(cid:213).
ğ‘¡=0.
where ğ›¾ âˆˆ (0, 1] is the discounting factor, and ğ‘Ÿ isthe single-step reward.
in text generation, however,such a reward deï¬nition requires sampling over thefuture generated sequence to estimate current stepreward (gong et al., 2019), which may cause thepolicy to end in zero reward region because of highvariance of the gradient (pang and he, 2021).
sincewe guide the generation in every step of decoding,we derive the ğ‘¡-th step policy gradient (cid:79)ğœƒ ğ½ğ‘¡ (ğœƒ) as:.
eğ‘¡.
ğœâˆ¼ ğœ‹ğœƒ.
(cid:2)ğœ–ğ‘¡ (cid:79)ğœƒ log ğœ‹ ğœƒ (ğ‘ğ‘¡ |ğ‘ ğ‘¡ ) Â· ğ‘Ÿ (ğ‘¥ğ‘‘with importance sampling weight ğœ–ğ‘¡ to stabilize theoptimization (munos et al., 2016), which is:.
ğ‘¡ )(cid:3) ,.
(4).
ğœ–ğ‘¡ =.
ğœ‹ ğœƒğ‘‘ (ğ‘ğ‘¡ |ğ‘ ğ‘¡ )ğœ‹ ğœƒ (ğ‘ğ‘¡ |ğ‘ ğ‘¡ ).
..if we denote a certain token in future contextas ğ‘¤ âˆˆ {ğ‘¤future}, single-step self-planning rewardğ‘Ÿ (ğ‘¥ğ‘‘ğ‘¡ ) can be approximated by the cosine similaritybetween ğ‘¡-th step hidden state and the embeddedvector of ğ‘¤ by the lm embedding layers, which is<ğ‘¡ ) Â· emb(ğ‘¤)) ..log(softmax(â„ ğœƒğ‘‘.
ğ‘Ÿ (ğ‘¥ğ‘‘.
ğ‘¡ ) =.
(cid:213).
ğ‘¤ âˆˆğ‘¤future.
(5)given all above deï¬nitions, at ğ‘¡-th step, we up-.
date ğœ‹ ğœƒ towards the self-planned ğœ‹ ğœƒğ‘‘ as:.
ğœƒğ‘‘ â† ğœƒ + ğœ‚.ğ‘˜(cid:213).
ğ‘–=1.
(cid:79)ğœƒ ğ½ğ‘¡ (ğœƒğ‘‘/ğœ‰)(cid:107)(cid:79)ğœƒ ğ½ğ‘¡ (ğœƒğ‘‘/ğœ‰) (cid:107).
,.
(6).
where ğœ‚ is the learning rate and ğœ‰ is the temperatureparameter to control the stochastic sampling dur-ing token decoding (keskar et al., 2019).
after ğ‘˜iterations of reinforced self-planning, the updatedpolicy ğœ‹ ğœƒğ‘‘ should produce tokens approaching thefuture context in embedding space, since futurecontext contributes to the calculation of reward ğ‘Ÿ(eq.
5).5 more details about how we handle edgecases during reinforced self-planning are presentedin appendix b..5in our setting, ğœ‚, ğœ‰ and ğ‘˜ are 0.02, 1.3, and 3 respectively..figure 2: compared with the naive method, our rein-forced self-planning approach inï¬lls blanks with ([blk])varying-length tokens while considering both past andfuture tokens, which promote diversity and coherencerespectively.
the context is concatenated to the begin-ning of the reference template..autoregressive language models (alm) suchas gpt-2 (radford et al., 2019), on the other hand,are trained to predict current step token given pasttokens.
they can generate sequences of varyinglengths, but they cannot inï¬ll missing tokens withinsentences effectively since they do not considerfuture context.
to enable alms to inï¬ll blanksof unspeciï¬ed length, prior work has proposedeither retraining a new lm from scratch (shenet al., 2020) or ï¬ne-tuning on specially prepareddata (donahue et al., 2020), which are costly andnot easy to extend to new nlg tasks.
as shownin figure 2, we take a reinforcement learning (rl)approach that uses future words after the blank toguide current step inï¬lling generation.
since suchrl guidance only relies on the tokens within itsown to-be-inï¬lled template, we call it reinforcedself-planning.
our method combines the advan-tages of both mlms and alms, requiring neitherre-training nor collecting new data, and thus is eas-ier to extend to other off-the-shelf lms..reinforced self-planning.
at each decodingstep during generation, a vanilla alm will pickthe token ğ‘¥ğ‘¡ that has the highest probability byapplying an argmax over the softmax output of hid-den states.
we add a self-planning stage betweenthe argmax and softmax function.
following therl framework, we deï¬ne the state at step ğ‘¡ as thegenerated sequences before ğ‘¡ (i.e., ğ‘ ğ‘¡ = ğ‘¥<ğ‘¡ ), andthe action at step ğ‘¡ as the ğ‘¡-th output token (i.e.,.
6680ireallyliketheshowperformedatthetheatre!ienjoyeveryminuteoftheshowatthetheatre!i[blk][blk]theshow[blk][blk]thetheatre!(a)naiveclozeaugmentation:maskedlm(b)self-planningclozeaugmentation:autoregressivelmienjoytheshowonlyperformedatthetheatre!contextcontextbi-directionalattentionuni-directionalattentionreinforcedself-planning++i[blk][blk]theshow[blk][blk]thetheatre!
2.3 computing contextual similarity.
after generating augmented reference sentences,the ï¬nal mars score is computed as a weightedaverage of the similarity between the candidate andeach reference in the augmentation set (includingthe original human reference).
one way to ob-tain similarity scores is using bertscore (zhanget al., 2019), but bertscore requires training onexternal resources to make its outputs more read-able.
therefore, in order to keep all the resourcesused by mars off-the-shelf, we utilize sentence-bert (reimers and gurevych, 2019), which usesthe mean of all token embeddings in a sentence asthe overall sentence-level encoding.
as the sen-tence encoder, we use roberta-large (liu et al.,2019), a common choice in the literature (zhanget al., 2019; reimers and gurevych, 2020).
asshown in eq.
7, we then compute mars score asthe average of the cosine similarities weighted us-ing a geometric progression with a common ratioğ‘ âˆˆ (0, 1] and a scale factor (start value) ğ‘ â‰  0:.
mars =.
ğ‘ğ‘ğ‘–âˆ’1 candt Â· refğ‘–âˆ’1(cid:107)cand(cid:107)t (cid:107)refğ‘–âˆ’1(cid:107).
(7).
s.t..ğ‘ğ‘ğ‘–âˆ’1 = 1 ,.
#ğœ†(cid:213).
ğ‘–=1#ğœ†(cid:213).
ğ‘–=1.
where the candidate encoding is cand, the referenceencodings are refğ‘– (ğ‘– is the index of the augmentedreference under a certain ğœ†, and ref0 marks the zero-mask human reference), and #ğœ† is the number ofmasking ratios we use in Â§2.1.
different ğ‘ values,as deï¬ned by the geometric progression, determinehow much weight each reference contributes.
bydefault, eq.
7 assigns the largest weight to the hu-man reference since it is the gold standard..3 tasks & datasets.
we evaluated mars and compared it with severalpopular nlg metrics on the following three tasks:.
story generation.
we use the roc storiesdataset6 for story generation, which requires candi-date nlg systems to generate coherent endings tofour-sentence stories (mostafazadeh et al., 2016).
the dataset consists of 96,198 examples of par-tially written stories; we take the human-ratedsubset (ğ‘=300) released by huse (hashimotoet al., 2019), which contains continuances by (1).
avg.
|cntx.|.
avg.
|h ref.|.
# data(# hr / data).
ğ›¼.roc34.38newsroom 772.21mocha 161.92.
8.3734.704.69.
300 (20)540 (3)450 (5).
0.640.710.82.Ï‰.
4.122.334.5.table 2: statistics of the three datasets with human rat-ings used in this work.
avg.
|cntx.| and |h ref.|: theaveraged number of tokens in contexts and human ref-erences.
Ï‰: the ratio of the previous two terms (lowerÏ‰ can indicate a more open-ended task).
# hr: thenumber of human ratings.
ğ›¼: krippendorffâ€™s alphacoefï¬cient to measure inter-annotator agreement..an industry-level system based on apache solr7,and (2) an open-nmt model with global atten-tion (mccann et al., 2017)..news summarization.
for the news summariza-tion task, we use the newsroom summary dataset.8this dataset contains 1.3 million articles from 38major publications (grusky et al., 2018) and we usethe subset with human ratings (ğ‘=540) released bythe authors.9 this dataset contains outputs fromsummarization models: (1) textrank: a sentence-level summarization system inspired by googlepagerank (page et al., 1999), (2) a seq2seq modelwith attention (rush et al., 2015), and (3) pointer-n: a pointer-based neural model (see et al., 2017)trained on newsroom dataset..question answering.
for question answering,we use the mocha dataset,10 which includeshuman ratings on outputs of ï¬ve models trainedon six qa datasets (chen et al., 2020).
we con-sider a distributionally-balanced subset (ğ‘=450)of these outputs from three systems:(1) ï¬ne-tuned gpt-2 (radford et al., 2019), (2) a back-translation model (sennrich et al., 2016), and (3)a mhpg model (bauer et al., 2018) trained on nar-rativeqa (koË‡cisk`y et al., 2018) and mcscript (os-termann et al., 2018) datasets..the detailed statistics of these three datasetswe used for this work are shown in table 2. forpre-processing, we removed hashtags and urls inthe text, but leave punctuation and stop words,which can affect lcs matching when computingmask costs.
for all tasks, we use gpt-2 (large,with 774m parameters) as the language model for.
7https://lucene.apache.org/solr8http://lil.nlp.cornell.edu/newsroom/9the subset includes human ratings on four perspectives:coherence, ï¬‚uency, informative and relevance.
we computethe average of the four scores as an overall human rating..6https://cs.rochester.edu/nlp/rocstories/.
10https://allennlp.org/mocha.
6681roc story generationÏ‰ = 4.1.newsroom summarizationÏ‰ = 22.7.mocha question answeringÏ‰ = 34.5.existing metrics.
solr.
open-nmt.
textrank seq2seq pointer-n gpt-2 back-tran.
mhpg.
bleu-1meteorrouge-lsent.
mover sim.
moverscorebertscoreperplexity.
mars (default)- w/o.
self-plan.
- w/o.
context+- w/o.
both.
naive (mlm).
0.1980.1800.1180.0200.1810.245-0.104.
0.4760.3130.3600.276.
0.449.
0.1040.1160.1950.0150.3910.386-0.073.
0.3970.2120.3340.183.
0.197.
0.2240.2880.0410.1120.0750.154-0.385.
0.3720.2900.107-0.163.
0.201.
0.2680.235-0.1330.0990.3370.3020.011.
0.3360.2450.1600.149.
0.324.
0.1150.2560.0650.1770.2120.181-0.035.
0.3290.314-0.009-0.057.
0.3280.4660.4680.5100.5350.4440.014.
0.5260.4770.134-0.092.
0.114.
0.443.
0.0610.1790.0560.1660.1900.274-0.051.
0.6440.6310.2220.121.
0.307.
0.3180.4090.2470.6100.5920.458-0.128.
0.7410.7090.3030.299.
0.540.table 3: pearsonâ€™s ğ‘Ÿ correlations with human judgements for mars and seven existing metrics across systemoutputs for three generation tasks.
bleu-1 (papineni et al., 2002), meteor (lavie and agarwal, 2007), androuge-l (lin and och, 2004) use ğ‘›-gram matching.
sentence moverâ€™s similarity (clark et al., 2019) and mover-score (zhao et al., 2019) measure similarity using earth moverâ€™s distance.
bertscore (zhang et al., 2019) lever-ages contextual embeddings from pre-trained lms.
as an ablation, we remove self-planning guidance, context,and both.
naive uses roberta-large for reference augmentation (see Â§2.2).
Ï‰ is deï¬ned as in table 2..mars, and roberta-large for the naive method.
for the newsroom dataset, some news articles werelonger than the max sequence length of 1024 bpe,and so we cut off the tail end of these examples.
with a single rtx-2080 gpu, cloze augmentationwith ğœ† = {0 (human ref.
), 20%, 40%, 60%, 80%}takes 0.8 seconds on average per reference, amount-ing to a total augmentation time of 17, 45, and 32minutes for the roc, newsroom and mochatasks respectively.
we show how we pick the mask-ing ratios for different tasks in Â§4.3..4 evaluation.
4.1 mars better correlates with humans.
as automated metrics are only helpful if they cor-relate sufï¬ciently with human judgements, in thissection we examine how mars correlates withhuman judgements compared with prior metrics..system-level correlation.
table 3 shows thecorrelations between human judgements and au-tomated metrics for mars and seven other unsu-pervised metrics, across all nlg systems studiedin our three tasks.
compared with the other metrics,mars achieves the highest correlation with humanjudgements for ï¬ve of the seven systems (and com-parable with the top in the other two systems), mak-ing considerable improvements over the next-bestmetric for many of the nlg systems (e.g., 0.370 â†‘for back-translation, and 0.231 â†‘ for solr).
we.
also notice that mars has greater improvementson more open-ended tasks (e.g., story generation,which has low Ï‰), which corroborates marsâ€™soriginal objective of judging diverse candidatesmore fairly.
as for the baselines, ğ‘›-gram matchingmetrics such as bleu correlate poorly with humanratings on such open-ended tasks; bertscore per-forms better on short candidates and high-Ï‰ tasks(e.g., qa); and perplexity, as expected, correlatesweakly with human ratings.
the naive method,which uses multiple augmented references of thesame length, improves over bertscore, whichonly uses the original reference..ablation study.
as shown in the lower rows oftable 3, we see that the performance of marsdrops substantially when the crucial componentsare removed.
speciï¬cally, removing self-planninghurts performance more for tasks with longer refer-ences (e.g., story generation) since self-planning ismore helpful when there are more blanks to in-ï¬ll,and removing context hurts performance more intasks that are less open-ended (high Ï‰, such as qa)because there is no adequate input for a reasonableaugmentation.
we take these ablation study re-sults as evidence that the techniques we propose inmars are crucial for improving correlation withhuman judgements..task-level correlation visualization.
to visu-alize the correlation between automated metrics.
6682roc story generation.
newsroom summarization mocha question answering.
existing metrics.
reorder (Î´) retrieve (Î´).
ref.
reorder (Î´) retrieve (Î´).
ref.
reorder (Î´) retrieve (Î´).
ref..bleu-1meteorrouge-lsent.
mover sim.
moverscorebertscoreperplexity.
marsw/.
roberta emb.
w/.
glove emb..naive (mlm).
(=) 0(cid:72) 0.041(cid:72) 0.131(cid:72) 0.024(cid:72) 0.131(cid:72) 0.109(cid:72) 0.113.
(cid:72) 0.125(cid:72) 0.087(cid:72) 0.149.
(cid:72) 0.015(cid:72) 0.031(cid:72) 0.123(cid:72) 0.062(cid:72) 0.123(cid:72) 0.127(cid:78) 0.170.
(cid:72) 0.191(cid:72) 0.177(cid:72) 0.156.
0.1370.0940.1940.0190.2760.337-0.089.
0.4590.363.
0.350.
(=) 0(cid:72) 0.132(cid:78) 0.011(cid:72) 0.153(cid:78) 0.011(cid:72) 0.112(cid:72) 0.298.
(cid:72) 0.117(cid:72) 0.052(cid:72) 0.112.
(cid:72) 0.144(cid:72) 0.142(cid:72) 0.035(cid:72) 0.161(cid:72) 0.135(cid:72) 0.026(cid:78) 0.008.
(cid:72) 0.198(cid:72) 0.149(cid:72) 0.190.
0.1760.2440.0360.1360.2360.3440.234.
0.4230.409.
0.314.
(=) 0(cid:72) 0.012(cid:72) 0.032(cid:72) 0.232(cid:78) 0.027(cid:72) 0.101(cid:72) 0.035.
(cid:72) 0.092(cid:72) 0.085(cid:72) 0.098.
(cid:72) 0.424(cid:72) 0.379(cid:72) 0.363(cid:72) 0.161(cid:72) 0.495(cid:72) 0.461(cid:78) 0.026.
(cid:72) 0.504(cid:72) 0.426(cid:72) 0.247.
0.3440.4120.3360.5150.5000.462-0.032.
0.6670.602.
0.639.table 4: we test robustness of mars and seven other automated metrics under attacks from adversarial samplesgenerated by following two attack strategies: (1) reorder: randomly reorders 50% of tokens in the candidates; (2)retrieve: randomly retrieves a sentence from the context as a candidate.
ref.
: correlation of original candidateswith human judgements.
if a metric scores adversarial samples equal to (=) or higher ((cid:78)) than ref., we considersuch metrics not robust under attacks.
robust systems should assign decreased scores ((cid:72)) compared to ref..to the human reference, mars uses augmented ref-erences enriched with information from the contextto provide a fairer judgement..4.2.is mars robust?.
good evaluation metrics ought to also be able to de-tect adversarial examples by assigning them lowerscores than well-formed candidates.
as shown intable 4, uni-gram matching bleu-1 cannot de-tect reordered sequences, while rouge-l scoresreordered sequence higher occasionally if token-swapping leads to more lcs.
sentence moverâ€™ssimilarity combines word and sentence embed-dings and thus is more capable of recognizing re-ordered samples than moverscore.
perplexity candetect reordered examples effectively, but is unableto detect retrieved sentences, as they are usuallywell-formed.
mars, on the other hand, has thebest robustness against adversarial samples, possi-bly because multiple context-infused augmentedreferences help mars detect adversarial samplesmore reliably.
we also study the effects of contex-tual embeddings we use in Â§2.3â€”when switchingto glove embeddings (pennington et al., 2014),which are not contextual, mars is less able todetect adversarial samples, especially reorderedones.
the naive method, which by default usesroberta embedding, achieves comparable robust-ness as mars but its task-level correlations withhumans (ref.)
are generally lower than mars, po-tentially because its ï¬xed-length cloze generationlimits the diversity of augmented references..figure 3: correlation between bertscore (left) andmars (right) with human judgements for mochaqa.
the ğ‘¥-axis is the automated metric score and ğ‘¦-axis is the human judgement.
points in different col-ors represent generation outputs of three nlg systems:gpt-2 (red circles), back-translation (green triangles),and mhpg (blue squares)..and human judgements, we consider the mochaqa task as an example and plot the correlations ofbertscore (left) and mars (right) with humanjudgements.
as shown in figure 3, compared withmars, bertscore has more candidates in theupper-left corner of the plot (i.e., low bertscorebut high human judgement).
many of these aregenerated by gpt-2 and mhpg, which, based onmanual examination, tend to provide more detailsin the answer than the human reference.
for in-stance, given a context about shopping, one ques-tion is â€œdid they need to buy any meat?â€.
thehuman reference answer is simply â€œyes, they did.â€,but gpt-2 returns â€œyes, they bought chicken anda roast.â€, which is more detailed, even containingitem names derived from the context.
whereasbertscore cannot evaluate such cases where thegenerated candidate is over-described with respect.
6683bertscore(r=0.46)humanjudgementgpt-2back-tranmhpg0.000.250.500.751.0012345gpt-2back-tranmhpg0.000.250.500.751.0012345mars(r=0.67)humanjudgementroc story generation.
error.
example.
{ğœ†}maxpearsonâ€™s ğ‘Ÿavg.
ğœ.
{ğœ†}maxpearsonâ€™s ğ‘Ÿavg.
ğœ.
{ğœ†}maxpearsonâ€™s ğ‘Ÿavg.
ğœ.
0% (ref.).
20% 40% 60% 80%.
0.411-.
0.4320.027.
0.4440.046.
0.4590.055.
0.4520.059.newsroom summarization.
0% (ref.).
20% 40% 60% 80%.
0.395-.
0.4070.061.
0.4160.062.
0.4230.063.
0.4110.068.mocha question answering.
0% (ref.).
20% 40% 60% 80%.
0.658-.
0.6670.074.
0.6490.104.
0.6030.117.
0.5840.125.table 5: evaluating correlation with human judge-ments for various max masking ratios (ğœ†max) used inmars.
0% masking (ref.)
means only the human ref-erence was used to score candidates.
we also showthe averaged standard deviation of the cosine similar-ities between the candidate and augmented referencesacross all samples..4.3 choosing masking ratios for mars.
the masking ratios for mars are set using the hy-perparameter {ğœ†}max, which corresponds to marsusing masking ratios from 0% to {ğœ†}max in in-crements of 20%, e.g., {ğœ†}max = 40% indicatesğœ† âˆˆ {0%, 20%, 40%}.
in preliminary experi-ments, we observed that {ğœ†}max varied for differ-ent datasets.
thus, for our three generation tasks,we evaluate mars performance given different{ğœ†}max, as shown in table 5. we ï¬nd that tasks thatwere more open-ended (low Ï‰; e.g., story genera-tion) beneï¬ted from higher {ğœ†}max, which created amore diverse set of augmented references, whereastasks that were less open-ended (high Ï‰; e.g., qa)worked better with lower {ğœ†}max, which kept theaugmented references more similar to the original..4.4 error analysis.
we analyzed cases where mars score substan-tially differed from human judgements.
from testset outputs, we found that errors could often be cat-egorized into one of three types (shown in table 6):(1) out of vocabulary errors, often induced byunknown tokens in the candidates, (2) confusionerrors, where candidates are simply copied fromcontext, and (3) inference errors, where the candi-dates are further inferences of the context based oncommonsense knowledge.
in these cases, humanannotators tended to assign higher scores, whereas,mars over-penalized them..oov(roc).
context: ...waltz dance at wedding...gold: all the guests gaspedwhen they saw the couplesâ€™ skill!
candidate: all the guests gaspedwhen they saw the unk unkhuman: 0.392 mars: 0.198.confusion(newsroom).
context: ...bidding on a neighborhood...gold: a neighborhood namedfor its former orchards inspires loyaltyand bidding wars.
candidate: living there cherrydale liesnorth of interstate... (a sentence extractedfrom context)human: 0.700 mars: 0.399.inference(mocha).
context: ...washing cloths...q: why did they do the laundry?
gold: to clean their clothescandidate: because they were dirty.
human: 0.400 mars: 0.083.table 6: error analysis of mars.
we investigated threetypical types of errors within the samples which re-ceived large differences between the mars score andhuman ratings.
gold: human written references..5 human judgement.
we conducted human evaluation on amazon me-chanical turk (mturk) to further study the qualityof mars augmentation.
in total 150 participantswere randomly assigned to evaluate the three tasks.
participants (61.3% male and 38.7% female) wereall from the united states and above 18 years old,with an average age of 34.7 years old.
each partici-pant was paid 75 cents for completing 14 questionsin each questionnaire (average completion time perquestionnaire was about 5.11 minutes)..results we conducted paired sample ğ‘¡-tests toexamine how much the augmentation samples re-semble the original human references regardingrelevance to context and readability.
as shown intable 7, in terms of relevance to context, marshad no statistically signiï¬cant difference comparedwith original human references in newsroom andmocha datasets, but was rated as even more rel-evant to the generation context than the humanreference in the roc dataset (mars mean = 5.07> human ref.
mean = 4.95), possibly because re-inforced self-planning guided the augmentation tobe more related to the context.
in terms of readabil-.
6684roc.
newsroom.
mocha.
ori.
naive mars ori.
naive mars ori.
naive mars.
relevance.
readability.
overall.
meanp.meanp.meanp.4.95-.
5.67-.
5.69-.
4.81.00*.
5.53.11.
5.31.12.
5.07.04*.
5.40.05.
5.42.30.
4.62-.
4.54-.
4.87-.
4.50.05.
4.31.12.
4.57.10.
4.61.95.
4.59.41.
4.75.22.
5.16-.
5.41-.
4.62-.
4.61.00*.
5.23.16.
4.44.07.
4.97.10.
5.33.29.
4.68.10.table 7: human evaluation results on relevance (to context), readability, and overall quality of mars andnaive augmentation method.
all results are compared with the original human reference (ori.).
text was scoredon a scale from 1-7. ğ‘ value describes the signiï¬cance of difference.
(* corresponds to ğ‘ < 0.05, ** to ğ‘ < 0.01and *** to ğ‘ < 0.001.).
ity, both mars and naive were rated lower thanthe original but not signiï¬cantly; we take this as acompromise of cloze style augmentation.
no sta-tistically signiï¬cant differences were seen betweenthe original and mars augmentation in overallratings across the three tasks.
these results furtherconï¬rm that augmented examples from mars areof similar quality to the original human references..6 related metrics.
unsupervised metrics.
in addition to the met-rics we directly compared with previously, otherunsupervised metrics have also been proposed.
ter (snover et al., 2006), character (wanget al., 2016), and chrf (popoviÂ´c, 2017) focus oncharacter-level overlaps instead of ğ‘›-gram match-ing.
similar to bertscore, yisi (lo, 2019) andbertr (mathur et al., 2019) leverage pre-trainedcontextual embeddings to better capture similarity.
Î´bleu (galley et al., 2015) adds human anno-tated sentences as negative references.
bawdenet al.
(2020) ï¬nd the gain from multiple referencescan be limited by inherent weaknesses in bleu.
we considered lessons from many of the aboveworks while designing mars..learned metrics.
compared with unsupervisedmetrics, learned metrics collect human supervi-sions (freitag et al., 2020a; chaganty et al., 2018)or train on specially prepared data of a certain do-main (sellam et al., 2020; rei et al., 2020).
otherapproaches train on related tasks and use these mod-els as metrics for the original task (goodrich et al.,2019; eyal et al., 2019).
whereas learned metricsmay have limited applicability on tasks where nosuch resources are available, mars fully exploitsthe few-shot learning abilities of off-the-shelf lms.
and therefore does not require additional training..task-speciï¬c metrics.
finally, many metricshave been proposed for task-speciï¬c evaluation,such as leic (cui et al., 2018) and cider (vedan-tam et al., 2015) for image captioning, par-ent (dhingra et al., 2019) for table-to-text, andeasse (alva-manchego et al., 2019) for sentencesimpliï¬cation.
mars, with some modiï¬cations,can potentially be extended to these tasks..7 limitations.
mars can be limited by the lm that it usesâ€”for instance, the total length of context + refer-ence/candidate is limited by the max sequencelength of the lm used.
additionally, our workhas focused on english, and mars may requirenon-trivial modiï¬cations to handle cases where thecontext and reference/candidate are in different lan-guages, such as machine translation.
future work,could potentially extend mars to these scenariosusing multi-lingual sequence-to-sequence modelssuch as multilingual-t5 (xue et al., 2020).
we alsoanalyzed errors and found that mars sometimesunder-scores candidates that contained unknown to-kens or were copied directly from the context (seeappendix c for examples and further analysis)..8 conclusion.
we have proposed mars, a context-aware andeasy-to-deploy nlg metric built upon an off-the-shelf language model (gpt-2).
on three contextualnlg tasks, we show that mars better correlateswith human judgements compared with seven otherunsupervised metrics.
requiring neither costly hu-man supervision nor additional training, marscan be applied to a broad range of nlg tasks..6685ethical considerations.
the goal of mars is to aid the evaluation of nlgmodels, and hence we draw attention to several eth-ical considerations.
first, the augmented referencesof mars can be affected by certain biases from thelm it is based on (e.g., gpt-2) (liu et al., 2021),though those biases may be partially mitigated bythe relatively narrow scope of cloze completion andby generations being guided by given context andhuman references.
second, mars facilitates eval-uation and therefore development of nlg models,for which a major ethical consideration is that theycan mimic target properties in training data thatare undesirable.
this is especially true of modelstrained on non-contemporary data that does not rep-resent current norms and practices.
these biasescan lead to ethical concerns if users or deployers ofmodels are not aware of these issues or do not ac-count for them.
more generally, nlg models canalso be used in malicious ways such as to generatefake news or spam, which we strongly discourage.
finally, our experiments and analysis are done inenglish, and therefore we do not claim that ourï¬ndings will generalize across all languages, al-though our framework has potential to be extendedto other languages with necessary modiï¬cations..references.
fernando alva-manchego, louis martin, carolinascarton, and lucia specia.
2019. easse: easier au-in pro-tomatic sentence simpliï¬cation evaluation.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp): system demonstra-tions, pages 49â€“54, hong kong, china.
associationfor computational linguistics..lisa bauer, yicheng wang, and mohit bansal.
2018.commonsense for generative multi-hop question an-swering tasks.
in proceedings of the 2018 confer-ence on empirical methods in natural languageprocessing, pages 4220â€“4230..arun chaganty, stephen mussmann, and percy liang.
2018. the price of debiasing automatic metrics innatural language evalaution.
in proceedings of the56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 643â€“653, melbourne, australia.
associationfor computational linguistics..anthony chen, gabriel stanovsky, sameer singh, andmatt gardner.
2020. mocha: a dataset for train-ing and evaluating generative reading comprehen-sion metrics.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 6521â€“6532..elizabeth clark, asli celikyilmaz, and noah a. smith.
2019. sentence moverâ€™s similarity: automatic eval-in proceedings ofuation for multi-sentence texts.
the 57th annual meeting of the association for com-putational linguistics, pages 2748â€“2760, florence,italy.
association for computational linguistics..yin cui, guandao yang, andreas veit, xun huang,and serge belongie.
2018. learning to evaluate im-in proceedings of the ieee con-age captioning.
ference on computer vision and pattern recognition,pages 5804â€“5812..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171â€“4186..bhuwan dhingra, manaal faruqui, ankur parikh,ming-wei chang, dipanjan das, and william co-hen.
2019. handling divergent reference texts whenevaluating table-to-text generation.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4884â€“4895, flo-rence, italy.
association for computational linguis-tics..chris donahue, mina lee, and percy liang.
2020. en-abling language models to ï¬ll in the blanks.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2492â€“2501, online.
association for computational lin-guistics..rachel bawden, biao zhang, lisa yankovskaya, an-dre tÂ¨attar, and matt post.
2020. a study in im-proving bleu reference coverage with diverse au-in findings of the associa-tomatic paraphrasing.
tion for computational linguistics: emnlp 2020,pages 918â€“932, online.
association for computa-tional linguistics..esin durmus, he he, and mona diab.
2020. feqa: aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5055â€“5070, online.
association for computational lin-guistics..peter f brown, stephen a della pietra, vincent jdella pietra, jennifer c lai, and robert l mercer.
1992. an estimate of an upper bound for the entropyof english.
computational linguistics, 18(1):31â€“40..matan eyal, tal baumel, and michael elhadad.
2019.question answering as an automatic evaluation met-in proceed-ric for news article summarization.
ings of the 2019 conference of the north american.
6686chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3938â€“3948, min-neapolis, minnesota.
association for computationallinguistics..markus freitag, george foster, david grangier, andcolin cherry.
2020a.
human-paraphrased refer-in pro-ences improve neural machine translation.
ceedings of the fifth conference on machine trans-lation, pages 1183â€“1192, online.
association forcomputational linguistics..markus freitag, david grangier, and isaac caswell.
2020b.
bleu might be guilty but references are notin proceedings of the 2020 conferenceinnocent.
on empirical methods in natural language process-ing (emnlp), pages 61â€“71, online.
association forcomputational linguistics..michel galley, chris brockett, alessandro sordoni,yangfeng ji, michael auli, chris quirk, mar-garet mitchell, jianfeng gao, and bill dolan.
2015.deltableu: a discriminative metric for generationtasks with intrinsically diverse targets.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 2: short papers), pages 445â€“450,beijing, china.
association for computational lin-guistics..hongyu gong, suma bhat, lingfei wu, jinjun xiong,and wen-mei hwu.
2019. reinforcement learn-ing based text style transfer without parallel train-ing corpus.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 3168â€“3180, minneapolis, minnesota.
associ-ation for computational linguistics..ben goodrich, mohammad ahmad saleh, peter liu,and vinay rao.
2019. assessing the factual accu-racy of text generation..max grusky, mor naaman, and yoav artzi.
2018.newsroom: a dataset of 1.3 million summaries withdiverse extractive strategies.
in proceedings of the2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 708â€“719, new orleans, louisiana.
as-sociation for computational linguistics..prakhar gupta, shikib mehri, tiancheng zhao, amypavel, maxine eskenazi, and jeffrey p bigham.
investigating evaluation of open-domain di-2019.alogue systems with human generated multiple ref-in proceedings of the 20th annual sig-erences.
dial meeting on discourse and dialogue, pages 379â€“391..tatsunori hashimoto, hugh zhang, and percy liang.
2019. unifying human and statistical evaluation for.
natural language generation.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1689â€“1701, minneapolis, min-nesota.
association for computational linguistics..nitish shirish keskar, bryan mccann, lav r varshney,caiming xiong, and richard socher.
2019. ctrl: aconditional transformer language model for control-lable generation.
arxiv preprint arxiv:1909.05858..tomÂ´aË‡s koË‡cisk`y, jonathan schwarz, phil blunsom,chris dyer, karl moritz hermann, gÂ´abor melis, andedward grefenstette.
2018. the narrativeqa readingcomprehension challenge.
transactions of the asso-ciation for computational linguistics, 6:317â€“328..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228â€“231, prague, czech repub-lic.
association for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74â€“81..chin-yew lin and franz josef och.
2004. auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
in proceedings of the 42nd annual meet-ing of the association for computational linguistics(acl-04), pages 605â€“612, barcelona, spain..ruibo liu, chenyan jia, jason wei, guangxuan xu,lili wang, and soroush vosoughi.
2021. mitigatingpolitical bias in language models through reinforcedcalibration.
in proceedings of the aaai conferenceon artiï¬cial intelligence..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..chi-kiu lo.
2019. yisi - a uniï¬ed semantic mt qualityevaluation and estimation metric for languages withdifferent levels of available resources.
in proceed-ings of the fourth conference on machine transla-tion (volume 2: shared task papers, day 1), pages507â€“513, florence, italy.
association for computa-tional linguistics..nitika mathur, timothy baldwin, and trevor cohn.
2019. putting evaluation in context: contextualembeddings improve machine translation evaluation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2799â€“2808, florence, italy.
association for compu-tational linguistics..6687bryan mccann, james bradbury, caiming xiong, andrichard socher.
2017. learned in translation: con-textualized word vectors.
in advances in neural in-formation processing systems, pages 6294â€“6305..shikib mehri and maxine eskenazi.
2020. usr: anunsupervised and reference free evaluation metricfor dialog generation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 681â€“707, online.
association forcomputational linguistics..ning miao, hao zhou, lili mou, rui yan, and leili.
2019. cgmh: constrained sentence generationby metropolis-hastings sampling.
in proceedings ofthe aaai conference on artiï¬cial intelligence, vol-ume 33, pages 6834â€“6842..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james allen.
2016. a cor-pus and cloze evaluation for deeper understanding ofin proceedings of the 2016commonsense stories.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 839â€“849, san diego,california.
association for computational linguis-tics..rÂ´emi munos, tom stepleton, anna harutyunyan, andmarc bellemare.
2016. safe and efï¬cient off-policyin advances in neural in-reinforcement learning.
formation processing systems, pages 1054â€“1062..preksha nema and mitesh m. khapra.
2018. towards abetter metric for evaluating question generation sys-in proceedings of the 2018 conference ontems.
empirical methods in natural language processing,pages 3950â€“3959, brussels, belgium.
associationfor computational linguistics..jekaterina novikova, ondË‡rej duË‡sek, amanda cer-cas curry, and verena rieser.
2017. why we needin proceedingsnew evaluation metrics for nlg.
of the 2017 conference on empirical methods innatural language processing, pages 2241â€“2252,copenhagen, denmark.
association for computa-tional linguistics..simon ostermann, ashutosh modi, michael roth, ste-fan thater, and manfred pinkal.
2018. mcscript:a novel dataset for assessing machine comprehen-sion using script knowledge.
in proceedings of theeleventh international conference on language re-sources and evaluation (lrec 2018)..lawrence page, sergey brin, rajeev motwani, andterry winograd.
1999. the pagerank citation rank-ing: bringing order to the web.
technical report,stanford infolab..richard yuanzhe pang and he he.
2021. text gen-eration by learning from off-policy demonstrations.
international conference on learning representa-tions (iclr 21â€™)..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311â€“318..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532â€“1543, doha,qatar.
association for computational linguistics..david pisinger.
1995. algorithms for knapsack prob-.
lems..maja popoviÂ´c.
2017. chrf++: words helping charac-in proceedings of the second con-ter n-grams.
ference on machine translation, pages 612â€“618,copenhagen, denmark.
association for computa-tional linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..ricardo rei, craig stewart, ana c farinha, and alonlavie.
2020. comet: a neural framework for mtevaluation.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2685â€“2702, online.
associa-tion for computational linguistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processing.
association for computational linguistics..nils reimers and iryna gurevych.
2020. makingmonolingual sentence embeddings multilingual us-in proceedings of theing knowledge distillation.
2020 conference on empirical methods in natu-ral language processing.
association for computa-tional linguistics..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-in proceedings of the 2015tence summarization.
conference on empirical methods in natural lan-guage processing, pages 379â€“389, lisbon, portugal.
association for computational linguistics..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073â€“1083..thibault sellam, dipanjan das, and ankur parikh.
2020. bleurt: learning robust metrics for textgeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 7881â€“7892, online.
association for computa-tional linguistics..6688via insertion-based generative pre-training.
in pro-ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 8649â€“8670, online.
association for compu-tational linguistics..wei zhao, maxime peyrard, fei liu, yang gao, chris-tian m. meyer, and steffen eger.
2019. moverscore:text generation evaluating with contextualized em-beddings and earth mover distance.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 563â€“578, hongkong, china.
association for computational lin-guistics..wanrong zhu, zhiting hu, and eric xing.
2019. text.
inï¬lling.
arxiv preprint arxiv:1901.00158..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86â€“96..tianxiao shen, victor quach, regina barzilay, andtommi jaakkola.
2020. blank language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 5186â€“5198, online.
association for computa-tional linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of association for machine transla-tion in the americas, volume 200. cambridge, ma..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in proceedings of the aaai conference on artiï¬cialintelligence, volume 32..wilson l taylor.
1953..â€œcloze procedureâ€: a newtool for measuring readability.
journalism quarterly,30(4):415â€“433..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566â€“4575..weiyue wang, jan-thorsten peter, hendrik rosendahl,and hermann ney.
2016. character: translationin proceedings of theedit rate on character level.
first conference on machine translation: volume2, shared task papers, pages 505â€“510, berlin, ger-many.
association for computational linguistics..linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, adityabarua, and colin raffel.
2020. mt5: a mas-sively multilingual pre-trained text-to-texttrans-former.
arxiv preprint arxiv:2010.11934..tsuta yuma, naoki yoshinaga, and masashi toyoda.
2020. ubleu: uncertainty-aware automatic evalua-tion method for open-domain dialogue systems.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics: student re-search workshop, pages 199â€“206, online.
associa-tion for computational linguistics..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019. bertscore: eval-uating text generation with bert..yizhe zhang, guoyin wang, chunyuan li, zhegan, chris brockett,and bill dolan.
2020.pointer: constrained progressive text generation.
6689algorithm 2: generate, judge and reviseinput: template {ğœ™(ğ‘¥ğ‘–)} ğ‘.ğ‘–=1, max guess ğœ,.
and lm perplexity checker ppl..ğ‘–=1 into [b] and [t];.
group {ğœ™(ğ‘¥ğ‘–)} ğ‘init ï¬nal output ğ‘ ;foreach block do.
ğ‘– â† 0;init priority queue ğ‘, buffer ğ‘ (cid:48);if [t] then.
append [t] to ğ‘ ;.
else if [b] then.
while ğ‘– < ğœ + |[b]| doif next is [t] then.
ğ‘¤ â† self-planning gen.;.
else.
ğ‘¤ â† open-ended gen.;.
endğ‘ (cid:48) â† ğ‘  + ğ‘¤;record (ppl(ğ‘ (cid:48) + [t]), ğ‘ (cid:48)) in ğ‘;ğ‘– â† ğ‘– + 1;.
endğ‘  â† ğ‘  + lowest ppl ğ‘ (cid:48) pop from ğ‘;.
end.
endreturn augmented reference ğ‘ ;.
modes: self-planning generation (if there is futurecontext) and open-ended generation (otherwise).
we use a priority queue to store each step genera-tion and its corresponding ppl for quick revisionsafterwards..appendix a: dp-based token maskingalgorithm.
as part of eq.1 in the main paper, we deï¬ne theidf score given token ğ‘¥ğ‘– and a corpus ğ‘‹ containingğ‘€ documents as:.
idf(ğ‘¥ğ‘–, ğ‘‹) = âˆ’ log.
ğ•€[ğ‘¥ğ‘– âˆˆ ğ‘‹ ğ‘—] ,.
1ğ‘€.ğ‘€(cid:213).
ğ‘—=1.
where ğ•€[Â·] is the indicator function.
we present ourdp-based masking algorithm in algorithm 1:.
algorithm 1: dp-based token maskinginput: human reference {ğ‘¥ğ‘– } ğ‘.ğ‘–=1, maskingratio ğœ†, and task-speciï¬c factor ğ›¼..compute ğ‘£ğ‘– for each ğ‘¥ğ‘– with ğ›¼ (eq.
1);compute ğ‘¤ğ‘– depending on lcs for each ğ‘¥ğ‘–;init dp-table ğ‘‡ [ğ‘ + 1] [ğ‘Šmax + 1] with all 0;for ğ‘– = 1, 2, .
.
.
, ğ‘ do.
for ğ‘— = 1, 2, .
.
.
, ğ‘Šmax doif ğ‘— âˆ’ ğ‘¤ğ‘–âˆ’1 < 0 then.
ğ‘‡ [ğ‘–] [ ğ‘—] = ğ‘‡ [ğ‘– âˆ’ 1] [ ğ‘—];record masking choice ğœ™(ğ‘¥ğ‘–);.
ğ‘‡ [ğ‘–] [ ğ‘—] = max(ğ‘‡ [ğ‘– âˆ’ 1] [ ğ‘—],ğ‘‡ [ğ‘– âˆ’ 1] [ ğ‘— âˆ’ ğ‘¤ğ‘–âˆ’1] + ğ‘£ğ‘–âˆ’1);record masking choice ğœ™(ğ‘¥ğ‘–);.
else.
end.
end.
end{ğœ™(ğ‘¥ğ‘–) ğ‘ğ‘–=1return best masking strategy {ğœ™(ğ‘¥ğ‘–) ğ‘ğ‘–=1.}
â† backtracking via records;};.
appendix b: generate, judge, and revisealgorithm.
the complete procedure for augmenting human ref-erences is presented in algorithm 2. for a giventemplate, we ï¬rst group the tokens into a block-by-block form with blank blocks ([b]) and textblocks ([t]).
then, we generate varying lengthsof tokens, iteratively concatenating them with nexttext block, and judging them based on ppl, and ï¬-nally revising current generations accordingly.
weuse the language modeling ability of lm to checkthe perplexity of the current sequence, and set ahyper-parameter ğœ to control the maximum ex-tended generation (for a lower ppl)..depending on whether there is a subsequenttext block, the generation will switch between two.
6690