on finding the k-best non-projective dependency trees.
ran zmigrod.
tim vieira.
ryan cotterell.
,.
university of cambridge.
johns hopkins university.
eth z¨urich.
rz279@cam.ac.uk tim.f.vieira@gmail.comryan.cotterell@inf.ethz.ch.
abstract.
the connection between the maximum span-ning tree in a directed graph and the best de-pendency tree of a sentence has been exploitedby the nlp community.
however, for manydependency parsing schemes, an important de-tail of this approach is that the spanning treemust have exactly one edge emanating fromthe root.
while work has been done to ef-ﬁciently solve this problem for ﬁnding theone-best dependency tree, no research has at-tempted to extend this solution to ﬁnding thek-best dependency trees.
this is arguably amore important extension as a larger propor-tion of decoded trees will not be subject to theroot constraint of dependency trees.
indeed,we show that the rate of root constraint viola-tions increases by an average of 13 times whendecoding with k = 50 as opposed to k = 1. inthis paper, we provide a simpliﬁcation of thek-best spanning tree algorithm of cameriniet al.
(1980).
our simpliﬁcation allows usto obtain a constant time speed-up over theoriginal algorithm.
furthermore, we present anovel extension of the algorithm for decodingthe k-best dependency trees of a graph whichare subject to a root constraint.1.
1.introduction.
non-projective, graph-based dependency parsersare widespread in the nlp literature.
(mcdonaldet al., 2005; dozat and manning, 2017; qi et al.,2020).
however, despite the prevalence of k-bestdependency parsing for other parsing formalisms—often in the context of re-ranking (collins and koo,2005; sangati et al., 2009; zhu et al., 2015; do andrehbein, 2020) and other areas of nlp (shen et al.,2004; huang and chiang, 2005; pauls and klein,2009; zhang et al., 2009), we have only foundthree works that consider k-best non-projective.
figure 1: violation rate of the root constraint whenusing regular k-best decoding (camerini et al., 1980)on pre-trained models of qi et al.
(2020) for languageswith varying training set sizes..dependency parsing (hall, 2007; hall et al., 2007;agi´c, 2012).
all three papers utilize the k-bestspanning tree algorithm of camerini et al.
(1980).
despite the general utility of k-best methods innlp, we suspect that the relative lack of interest ink-best non-projective dependency parsing is dueto the implementation complexity and nuances ofcamerini et al.
(1980)’s algorithm.2.
we make a few changes to camerini et al.
(1980)’s algorithm, which result in both a sim-pler algorithm and simpler proof of correctness.3firstly, both algorithms follow the key propertythat we can ﬁnd the second-best tree of a graphby removing a single edge from the graph (the-orem 1); this property is used iteratively to enu-merate the k-best trees in order.
our approachto ﬁnding the second-best tree (see §3) is fasterbecause of it performs half as many of the expen-sive cycle-contraction operations (see §2).
overall,this change is responsible for our 1.39x speed-up.
2in fact, an anonymous reviewer called it “one of the most.
‘feared’ algorithms in dependency parsing.”.
1our implementation is available at https://github..3while our algorithm is by no means simple, an anony-.
com/rycolab/spanningtrees..mous reviewer called it “a big step in that direction.”.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1324–1337august1–6,2021.©2021associationforcomputationallinguistics1324102103104trainingsetsize(log-scale)05101520253035rootconstraintviolationrate(%)k=50k=1(see §4).
secondly, their proof of correctness isbased on reasoning about a complicated orderingon the edges in the kth tree (camerini et al., 1980,section 4); our proof side-steps the complicatedordering by directly reasoning over the ancestryrelations of the kth tree.
consequently, our proofsof correctness are considerably simpler and shorter.
throughout the paper, we provide the statementsof all lemmas and theorems in the main text, butdefer all proofs to the appendix..in addition to simplifying camerini et al.
(1980)’s algorithm, we offer a novel extension.
for many dependency parsing schemes such asthe universal dependency (ud) scheme (nivreet al., 2018), there is a restriction on dependencytrees to only have one edge emanate from the root.4finding the maximally weighted spanning tree thatobeys this constraint was considered by gabow andtarjan (1984) who extended the o(n 2) maximumspanning tree algorithm of tarjan (1977); cameriniet al.
(1979).
however, no algorithm exists for k-best decoding of dependency trees subject to a rootconstraint.
as such, we provide the ﬁrst k-bestalgorithm that returns dependency trees that obeythe root constraint..to motivate the practical necessity of our exten-sion, consider fig.
1. fig.
1 shows the percentageof trees that violate the root constraint when doingone-best and 50-best decoding for 63 languagesfrom the ud treebank (nivre et al., 2018) usingthe pre-trained model of qi et al.
(2020).5,6 weﬁnd that decoding without the root constraint hasa much more extreme effect when decoding the50-best than the one-best.
speciﬁcally, we observethat on average, the number of violations of the rootconstraint increased by 13 times, with the worst in-crease being 44 times.
the results thus suggest thatﬁnding k-best trees that obey the root constraintfrom a non-projective dependency parser requiresa specialist algorithm.
we provide a more detailedresults table in app.
a, including root constraintviolation rates for k = 5, k = 10, and k = 20. fur-thermore, we note that the k-best algorithm mayalso be used for marginalization of latent variables(correia et al., 2020) and for constructing parserswith global scoring functions (lee et al., 2016)..4there are certain exceptions to this such as the prague.
treebank (bejˇcek et al., 2013)..5zmigrod et al.
(2020) conduct a similar experiment for.
only the one-best tree..6we note that qi et al.
(2020) do apply the root constraint.
for one-best decoding, albeit with a sub-optimal algorithm..ρ.
90.
1.
20.
40.
4.
10.
60.
70.
30 50.
2.
3.figure 2: example graph g (taken from zmigrod et al.
(2020)).
edges that are part of both the best tree g(1)and the best dependency tree g[1] are marked as thicksolid edges.
edges only in g(1) are dashed and edgesonly in g[1] are dotted..2 finding the best tree.
the study of rooted directedwe considerweighted graphs, which we will abbreviate to sim-ply graphs.7 a graph is given by g = (ρ, n , e)where n is a set of n + 1 nodes with a designatedroot node ρ ∈ n and e is a set of directed weightededges.
each edge e = (i (cid:65) j) ∈ e has a weightw(e) ∈ r+.
we assume that self-loops are not al-lowed in the graph (i.e., (i (cid:65) i) (cid:54)∈ e).
additionally,we assume our graph is not a multi-graph, there-fore, there can exist at most one edge from node ito node j.8 when it is clear from context, we abusenotation and use j ∈ g and e ∈ g for j ∈ n ande ∈ e respectively.
when discussing runtimes, wewill assume a fully connected graph (|e| = n 2).9an arborescence (henceforth called a tree) of gis a subgraph d = (ρ, n , e (cid:48)) such that e (cid:48) ⊆ e andthe following is true:.
1. for all j ∈ n (cid:114) {ρ}, |{( (cid:65) j) ∈ e (cid:48)}| = 1..2. d does not contain any cycles..other deﬁnitions of trees can also include thatthere is at least one edge emanating from the root.
however, this condition is immediately satisﬁedby the above two conditions.
a dependency tree.
7as we use the algorithm in zmigrod et al.
(2020) as ourbase algorithm, we borrow their notation wherever convenient.
8we make this assumption for simplicity, the algorithmspresented here will also work with multi-graphs.
this might bedesirable for decoding labeled dependency trees.
however, wenote that in most graph-based parsers such as qi et al.
(2020)and ma and hovy (2017), dependency labels are extractedafter the unlabeled tree has been decoded..9we make this assumption as in the context of dependencyparsing, we generate scores for each possible edge.
further-more, (tarjan, 1977) prove that the runtime of ﬁnding the besttree for dense graphs is o(n 2).
this is o(|e| log n ) in thenon-dense case..1325d = (ρ, n , e (cid:48)) is a tree with the extra constraint.
3.
|{(ρ (cid:65) ) ∈ n (cid:48)}| = 1.the set of all trees and dependency trees in a graphare given by a(g) and d(g) respectively.
theweight of a tree is given by the sum of its edgeweights10.
(cid:88).
w(d) =.
w(e).
(1).
e∈dthis paper concerns ﬁnding the k highest-weighted (henceforce called k-best) tree or de-pendency tree, these are denoted by g(k) and g[k]respectively.
tarjan (1977); camerini et al.
(1979)provided the details for an o(n 2) algorithm fordecoding the one-best tree.
this algorithm was ex-tended by gabow and tarjan (1984) to ﬁnd the bestdependency tree in o(n 2) time.
we borrow thealgorithm (and notation) of zmigrod et al.
(2020),who provide an exposition and proofs of these algo-rithms in the context of non-projective dependencyparsing.
the pseudocode for ﬁnding g(1) and g[1]is given in fig.
3. we brieﬂy describe the key com-ponents of the algorithm.11.
the greedy graph of g is denoted by.
−(cid:65)g =(ρ, n , e (cid:48)) where e (cid:48) contains the highest weightedincoming edge to each non-root node.
therefore, if−(cid:65)−(cid:65)−(cid:65)g = g(1).
a cycle c ing has no cycles, thengis called a critical cycle.
if we encounter a criticalcycle in the algorithm, we contract the graph bythe critical cycle.
a graph contraction, g/c, by acycle c replaces the nodes in c by a mega-nodec such that the nodes of g/c are n (cid:114) c ∪ {c}.
furthermore, for each edge e = (i (cid:65) j) ∈ g:.
1. if i (cid:54)∈ c and j ∈ c, then e(cid:48) = (i (cid:65) c) ∈(cid:16)−(cid:65)(cid:17)g/c such that w(e(cid:48)) = w(e) + wcjwhere cj is the subgraph of c rooted at j.
2. if i ∈ c and j (cid:54)∈ c, then e(cid:48) = (c (cid:65) j) ∈.
g/c such that w(e(cid:48)) = w(e).
3. if i (cid:54)∈ c and j (cid:54)∈ c, then e ∈ g/c..4. if i ∈ c and j ∈ c, then there is no edge.
related to (i (cid:65) j) in g/c..there also exists a bookkeeping function π such.
10for inference, the weight of a trees often decomposesmultiplicatively rather than additively over the edges.
one cantake the exponent (or logarithm) of the original edge weightsto make the weights distribute additively (or multiplicative).
11for a more complete and detailed description as well as aproof of correctness, please refer to the original manuscripts..1: def opt(g) :−(cid:65)g has a cycle c :return opt(cid:0)g/c.
if.
2:.
3:.
(cid:1) (cid:35) c.else.
(cid:46) recursive case.
(cid:46) base case.
if we require a dependency tree :.
return constrain(g).
else.
return.
−(cid:65)g.9: def constrain(g) :.
−(cid:65)g.σ ← set of ρ’s outgoing edges inif |σ| = 1 : return.
−(cid:65)g (cid:46) constraint satisﬁed(cid:18)−−−(cid:65)g\\e(cid:48).
(cid:19).
e ← argmax.
w.e(cid:48)∈σ.
if.
−−(cid:65)g\\e has cycle c :return constrain(cid:0)g/c.
(cid:1) (cid:35) c.else.
return constrain(g\\e).
4:.
5:.
6:.
7:.
8:.
10:.
11:.
12:.
13:.
14:.
15:.
16:.
figure 3: algorithms for ﬁnding g(1) and g[1].
theseare from zmigrod et al.
(2020).
that for all e(cid:48) ∈ g/c, π(e(cid:48)) ∈ g. this bookkeepingfunction returns the edge in the original graph thatled to the creation of the edge in the contractedgraph using one of the constructions above..finding g(1) is then the task of ﬁnding a con-−(cid:65)tracted graph g(cid:48) such thatg(cid:48) = g(cid:48)(1).
once this isdone, we can stitch back the cycles we contracted.
if g(cid:48) = g/c, for any d ∈ a(g/c), d (cid:35) c ∈a(g) is the tree made with edges π(d) (π applied−(cid:65)to each edge d) andcj where cj is the subgraph ofthe nodes in c rooted at node j and π(e) = (i (cid:65) j)for e = (i (cid:65) c) ∈ d. the contraction weight-ing scheme means that w(d) = w(d (cid:35) c) (geor-giadis, 2003).
therefore, g(1) = (g(cid:48)(1) (cid:35) c)(1).
the strategy for ﬁnding g[1] is to ﬁnd the con-tracted graph for g(1) and attempt to remove edgesemanating from the root.
this was ﬁrst proposedby gabow and tarjan (1984).
when we considerremoving an edge emanating from the root, we aredoing this in a possibly contracted graph, and so anedge (ρ (cid:65) j) may exist multiple times in the graph.
we denote g\\e to be the graph g with all edgeswith the same end-points as e removed.
fig.
2 givesan example of a graph g, its best tree g(1), and itsbest dependency tree g[1]..the runtime complexity of ﬁnding g(1) or g[1]is o(n 2) for dense graphs by using efﬁcient pri-ority queues and sorting algorithms (tarjan, 1977;gabow and tarjan, 1984).
we assume this runtime.
1326ρ.ρ.
1.
1.
2.
3.
2.
3.
4.
4.
(d).
(a)b(g, e, g(1)).
e(cid:48)(cid:48)f.b(g, e, g(1)).
ρ.b(g, e, g(1)).
ρ.
2.
3.
2.f (cid:48).
3.
1.
1.e.4.r(g, e, g(1)).
(b)b(g, e, g(1)).
ρ.e(cid:48)(cid:48)f.4.
(e).
2.
3.
2.f (cid:48).
3.ρ.
1.
1.
4.e(cid:48).
r(g, e, g(1)).
(c).
4.
(f).
e.e(cid:48)(cid:48)f.r(g, e, g(1)).
r(g, e, g(1)).
−(cid:65)g. therefore, g(1) =.
figure 4: worked example of lemma 1. consider a fully connected graph, g, of the example given in fig.
2 as−(cid:65)g. next, suppose that we knowgiven in (a).
suppose that the solid edges in (a) representthat e = (2 (cid:65) 4) ∈ g(1) is not in g(2).
then one of the dashed edges in (b) must be in g(2) as 4 must have anincoming edge.
the edges emanating from ρ and 1 make up the set of blue edges, b(g, e, g(1)) while the edgeemanating from 3 makes the set of red edges, r(g, e, g(1)).
if e(cid:48) ∈ b(g, e, g(1)) is in g(2) as in (c), then thesolid lines in (c) make a tree and g(2) differs from g(1) by exactly one blue edge of e. otherwise, we know thate(cid:48)(cid:48) ∈ r(g, e, g(1)) is in g(2) as in (d).
however, the solid edges in (d) contain a cycle between 3 and 4 withedges e(cid:48)(cid:48) and f .
we could break the cycle at 3 and include edge f (cid:48) in our tree as in (e).
however, while the solidedges in (e) make a valid tree, as w(e) > w(e(cid:48)(cid:48)) and w(f ) > w(f (cid:48)), the tree given by the solid lines of (f) will havea higher weight.
this would mean that e ∈ g(2) which leads to a contradiction.
therefore, we must break the cycleat 4 , which leads us to a tree as in (c).
consequently, g(2) will differ from g(1) by exactly one blue edge of e..for the remainder of the paper..3 finding the second best tree.
in the following two sections, we provide a simpli-ﬁed reformulation of camerini et al.
(1980) to ﬁndthe k-best trees.
the simpliﬁcations additionallyprovide a constant time speed-up over cameriniet al.
(1980)’s algorithm.
we discuss the differ-ences throughout our exposition..the underlying concept behind ﬁnding the k-best tree, is that g(k) is the second best tree g(cid:48)(2)of some subgraph g(cid:48) ⊆ g. in order to explore thespace of subgraphs, we introduce the concept ofedge inclusion and exclusion graphs.
deﬁnition 1 (edge inclusion and exclusion).
forany graph g and edge e ∈ g, the edge-inclusiongraph g + e ⊂ g is the graph such that forany d ∈ a(g + e), e ∈ d. similarly, the edge-exclusion graph g − e ⊂ g is the graph such thatfor any d ∈ a(g − e), e (cid:54)∈ d..when we discuss ﬁnding the k-best dependencytrees in §5, we implicitly change the above deﬁ-nition to use d(g + e) and d(g − e) instead ofa(g + e) and a(g − e) respectively..in this section, we will speciﬁcally focus on ﬁnd-ing g(2), we extend this to ﬁnding the g(k) in §4.
finding g(2) relies on the following fundamentaltheorem.
theorem 1. for any graph g and e ∈ g(1).
g(2) = (g − e)(1).
where.
e = argmaxe(cid:48)∈g(1).
(cid:16).
(g − e(cid:48))(1)(cid:17).
w.(6).
(7).
theorem 1 states that we can ﬁnd g(2) byidentifying an edge e ∈ g(1) such that g(2) =(g − e)(1).
we next show an efﬁcient method foridentifying this edge, as well as the weight of g(2)without actually having to ﬁnd g(2).
deﬁnition 2 (blue and red edges).
for any graph.
1327(cid:46) recursive case.
3:.
2:.
if.
1: def next(g) :−(cid:65)g has a cycle c :d, (cid:104)w, e(cid:105) ← next(cid:0)g/cd(cid:48) ← d (cid:35) ce(cid:48) ← argmine(cid:48)(cid:48)∈c∩d(cid:48).
5:.
4:.
(cid:1).
wg,d(cid:48)(e(cid:48)(cid:48))w(cid:48) ← w(d(cid:48)) − wg,d(cid:48)(e)return d(cid:48), max((cid:104)w, π(e)(cid:105), (cid:104)w(cid:48), e(cid:48)(cid:105)).
requiring much more complicated proofs.
deﬁnition 3 (swap cost).
for any graph g, treed ∈ a(g), and edge e ∈ d, the swap cost denotesthe minimum change to a tree weight to replace eby a single edge in d. it is given by.
wg,d(e) = min.
e(cid:48)∈b(g,e,d).
(cid:0)w(e) − w(cid:0)e(cid:48)(cid:1)(cid:1).
(4).
else.
(cid:46) base case.
we will shorthand wg(e) to mean wg,g(1)(e)..6:.
7:.
8:.
9:.
10:.
11:.
wg(e(cid:48)).
e ← argmin−(cid:65)e(cid:48)∈g(cid:16)−(cid:65)g−(cid:65)g, (cid:104)w, e(cid:105).
w ← w.return.
(cid:17).
− wg(e).
figure 5: algorithm for ﬁnding g(1), the best edge eto delete to ﬁnd g(2), and w(cid:0)g(2)(cid:1).
g, tree d ∈ a(g), and edge e = (i (cid:65) j) ∈ d,the set of blue edges b(g, e, d) and red edgesr(g, e, d) are deﬁned by12.
b(g, e, d).
= {e(cid:48) =(i(cid:48) (cid:65) j) | w(cid:0)e(cid:48)(cid:1) ≤ w(e),defd (cid:114) {e} ∪ {e(cid:48)} ∈ a(g)}.
r(g, e, d).
def= {e(cid:48) =(i(cid:48) (cid:65) j) | e(cid:48) (cid:54)∈ b(g, e, d)}.
(2).
(3).
an example of blue and red edges are given infig.
4..lemma 1. for any graph g, if g(1) =for some e ∈ g(1) and e(cid:48) ∈ b(g, e, g(1)).
−(cid:65)g, then.
g(2) = g(1) (cid:114) {e} ∪ {e(cid:48)}.
(8).
lemma 1 can be understood more clearly byfollowing the worked example in fig.
4. the moralof lemma 1 is that in the base case where there areno critical cycles, we only need to examine the blueedges of the greedy graph to ﬁnd the second besttree.
furthermore, our second best tree will onlydiffer from our best tree by exactly one blue edge.
camerini et al.
(1980) make use of the concepts ofthe blue and red edge sets, but rather than considera base case as lemma 1, they propose an orderingin which to visit the edges of the graph.
this resultsin several properties about the possible orderings,.
(i(cid:48) (cid:65) j).
12we can also deﬁne b(g, e, d) as.
∈b(g, e, d) ⇐⇒ i(cid:48) is an ancestor of j in d and r(g, e, d)as (i(cid:48) (cid:65) j) ∈ r(g, e, d) ⇐⇒ i(cid:48) is a descendant of j ind. this equivalence exists as we can only swap an incomingedge to j in d without introducing a cycle if the new edge em-anates from an ancestor of j. the exposition using ancestorsand descendants is more similar to the exposition originallypresented by camerini et al.
(1980)..−(cid:65)g, then.
corollary 1. for any graph g, if g(1) =g(2) = (g − e)(1) where e is given by.
e = argmine(cid:48)∈g(1)furthermore, w(cid:0)g(2)(cid:1) = w(cid:0)g(1)(cid:1) − wg(e)..wg.
(cid:0)e(cid:48)(cid:1).
(5).
corollary 1 provides us a procedure for ﬁndingthe best edge to remove to ﬁnd g(2) as well asits weight in the base case of g having no criticalcycles.
we next illustrate what must be done in therecursive case when a critical cycle exists.
lemma 2. for any g with a critical cy-cle c, either g(2) = (g/c)(2) (cid:35) c (withw(cid:0)g(2)(cid:1) = w(cid:0)(g/c)(2)(cid:1)) or g(2) = (g − e)(1)(with w(cid:0)g(2)(cid:1) = w(cid:0)g(1)(cid:1) − wg(e)) for somee ∈ c ∩ g(1)..combining corollary 1 and lemma 2, we candirectly modify opt to ﬁnd the weight of g(2) andthe edge we must remove to obtain it.
we detailthis algorithm as next in fig.
5.theorem 2. for any graph g, executing next(g)returns g(1) and (cid:104)w, e(cid:105) such that g(2) =(g − e)(1) and w(cid:0)g(2)(cid:1) = w.runtime analysis.
we know that without lines5, 6, 9 and 10, next is identical to opt and sowill run in o(n 2).
we call w at most n + 2times during a full call of next: n times fromlines 5 and 9 combined, once from line 6, andonce from line 10. to ﬁnd w, we ﬁrst need toﬁnd the set of blue edges, which can be done ino(n ) by computing the reachability graph.
then,we need another o(n ) to ﬁnd the minimisingvalue.
therefore, next does o(n 2) extra workthan opt and so retains the runtime of o(n 2).
camerini et al.
(1980) require g(1) to be knownahead of time.
this results in having to run theoriginal algorithm in o(n 2) time and then havingto do the same amount of work as next becausethey must still contract the graph.
therefore, nexthas a constant-time speed-up over its counterpartin camerini et al.
(1979)..1328g(1), w : 260.ρ.
1.
4.
2.
3.g(2), w : 220.
+e.
ρ.
1.
2.
4.
3e : (4 (cid:65) 3).
−e.
g(5), w : 190.
+e.
g(6), w : 150.
+e.
−e.
3e : (ρ (cid:65) 1).
g(3), w : 210.
+e.
2.
2.ρ.
1.ρ.
1.
4.
4.
3e : (ρ (cid:65) 2).
−e.
+e.
3e : (ρ (cid:65) 1).
g(7), w : 130.
−e.
+e.
2.
2.ρ.
1.ρ.
1.
4.
4.
3e : (ρ (cid:65) 1).
−e.
g(4), w : 200.ρ.
1.
2.
4.
3e : (2 (cid:65) 3).
−e.
figure 6: example of running through kbest using the graph of fig.
2. we start with g(1) that has a weight of 260and consider the best edge to remove to ﬁnd g(2).
using next we ﬁnd that g(2) = (g − e)(1) for e = (4 (cid:65) 3).
we then know that either e ∈ g(3) or e (cid:54)∈ g(3).
we can push these two possibilities to the queue using two callsto next.
we ﬁnd that g(3) comes from the graph without e, and also removes the edge e(cid:48) = (ρ (cid:65) 2).
we attemptto push two new elements to the queue, but we see that only by including e(cid:48) in the graph can we ﬁnd another tree.
we repeat this process until we have found g(k) or the queue is empty..1: def kbest(g, k) :2:.
(cid:104)g(1), (cid:104)w, e(cid:105)(cid:105) ← next(g)yield g(1).
3:4: q ← priority queue([(cid:104)w, e, g(cid:105)])5:.
for k = 2, .
.
.
, k :.
6:.
7:.
8:.
9:.
10:.
11:.
12:.
if q.empty() : return(cid:104)w, e, g(cid:48)(cid:105) ← q.pop()(cid:104)g(k), (cid:104)w(cid:48), e(cid:48)(cid:105)(cid:105) ← next(g(cid:48) − e)yield g(k)q.push((cid:104)w(cid:48), e(cid:48), g(cid:48) − e(cid:105))(cid:104) · , (cid:104)w(cid:48)(cid:48), e(cid:48)(cid:48)(cid:105)(cid:105) ← next(g(cid:48) + e)q.push((cid:104)w(cid:48)(cid:48), e(cid:48)(cid:48), g(cid:48) + e(cid:105)).
figure 7: k-best tree enumeration algorithm..4 finding the k th best tree.
in the previous section, we found an efﬁcientmethod for ﬁnding g(2).
we now utilize thismethod to efﬁciently ﬁnd the k-best trees.
lemma 3. for any graph g and k > 1, thereexists a subgraph g(cid:48) ⊆ g and 1 ≤ l < k suchthat g(l) = g(cid:48)(1) and g(k) = g(cid:48)(2)..lemma 3 suggests that we can ﬁnd the k-besttrees by only examining the second best trees ofsubgraphs of g. this idea is formalized as algo-rithm kbest in fig.
7. a walk-through of theexploration space using kbest for our examplegraph in fig.
2 is shown in fig.
6.theorem 3. for any graph g and k > 0, at anyiteration 1 ≤ k ≤ k, kbest(g, k) returns g(k).
runtime analysis.
we call next once at the.
k = 10 k = 20 k = 50.camerini et al.
kbest.
6.954.89.
14.0410.10.
35.1125.63.speed-up.
1.42×.
1.39×.
1.37×.
table 1: runtime experiment for parsing the k-bestspanning trees in the english ud test set (nivre et al.,2018).
times are given in 10−2 seconds for the averageparse of the k-best spanning trees..start of the algorithm, then every subsequent itera-tion we make two calls to next.
as we have k −1iterations , the runtime of kbest is o(kn 2).
theﬁrst call to next in each iteration ﬁnds the kthbest tree as well as an edge to remove.
cameriniet al.
(1980) make one call to of opt and two callsto next which only ﬁnds the weight-edge pair ofour algorithm.
therefore, kbest has a constanttime speed-up on the original algorithm.13a short experiment.
we empirically measurethe constant time speed-up between kbest andthe original algorithm of camerini et al.
(1980).
we take the english ud test set (as used for fig.
1)and ﬁnd the 10, 20, and 50 best spanning treesusing both algorithms.14 we give the results of theexperiment in tab.
1.15 we note that on averagekbest leads to a 1.39 times speed-up.
this is.
13in practice, we maintain a set of edges to include and.
exclude to save space..14implementations for both versions can be found in our.
code release (see footnote 1).
15the experiment was conducted using an intel(r).
core(tm) i7-7500u processor with 16gb ram..1329g[1], w : 210ρ.
2.
4.
1.
3e : (ρ (cid:65) 1).
+e.
−e.
g[3], w : 150ρ.
2.
4.
1.
3e : (4 (cid:65) 3).
+e.
−e.
g[2], w : 190ρ.
2.
4.
1.
3e : (ρ (cid:65) 2).
+e.
−e.
g[4], w : 130ρ.
2.
4.
1.
3e : (2 (cid:65) 3).
+e.
−e.
figure 8: example of running through kbest dep using the graph of fig.
2. we start with g[1] that has a weightof 210 and consider the best edge to remove to ﬁnd g(2).
we consider removing the best dependency tree with thesame edge emanating from the root e = (ρ (cid:65) 1) using next.
however, no such dependency tree exists, and so weonly need to push the graph g − e. when we next pop from the queue, we see that we have removed root edge e,and so must consider removing the new root edge e(cid:48) = (ρ (cid:65) e).
in this case, no dependency tree exists without eand e(cid:48), and so we only push to the queue the results of running next.
we repeat this process until we have foundg[k] or the queue is empty..lower than we anticipated as we have to make halfas many calls to next than the original algorithm.
however, in the original next of camerini et al.
(1980), we do not require to stitch together the tree,which may explain the slightly smaller speed-up..5 finding the k th best dependency tree.
in this section, we present a novel extension to thealgorithm presented thus far, that allows us to efﬁ-ciently ﬁnd the k-best dependency trees.
recallthat we consider dependency trees to be spanningtrees with a root constraint such that only one edgemay emanate from ρ. na¨ıvely, we can use kbestwhere we initialize the queue with (g + eρ)(1) foreach eρ = (ρ (cid:65) j) ∈ g. however, this adds ao(n 3) component to our runtime as we have tocall opt n times.
instead, our algorithm main-tains the o(kn 2) runtime as the regular k-bestalgorithm.
we begin by noting that we can ﬁndsecond best dependency tree, by ﬁnding either thebest dependency tree with a different root edge orthe second best tree with the same root edge.
lemma 4. for any graph g and edge eρ =(ρ (cid:65) j) ∈ g[1], g[2] = (g − eρ)[1] or g[2] =(g + eρ)[2].
lemma 5. for any graph g and k > 1, if e =(ρ (cid:65) j) ∈ g[k], then either e is not in any of thek−1-best trees or there exists a subgraph g(cid:48) ⊆ gand 1 ≤ l < k such that g[l] = g(cid:48)[1], e ∈ g(cid:48)[1]and g[k] = g(cid:48)[2]..lemma 5 suggests that we can ﬁnd the k-bestdependency trees, by examining the second bestdependency trees of subgraphs of g or ﬁnding thebest dependency tree with a unique root edge.
this.
4:.
1: def kbest dep(g, k) :2: g[1] ← opt(g)yield g[1]3:eρ ← outgoing edge from ρ in g[1](cid:104) ·, (cid:104)w, e(cid:105)(cid:105) ← next(g + eρ)d ← opt(g − eρ).
6:7: q ← priority queue([(cid:104)w(d), eρ, g(cid:105)])8: q.push((cid:104)w, e, g + eρ(cid:105))for k = 2, .
.
.
, k :9:.
5:.
10:.
11:.
12:.
13:.
14:.
15:.
16:.
17:.
18:.
19:.
20:.
21:.
22:.
23:.
24:.
if q.empty() : return(cid:104)w, e, g(cid:48)(cid:105) ← q.pop()if e does not emanate from ρ :.
g[k], (cid:104)w(cid:48), e(cid:48)(cid:105) ← next(g(cid:48) − e)q.push((cid:104)w(cid:48), e(cid:48), g(cid:48) − e(cid:105))(cid:104) · , (cid:104)w(cid:48)(cid:48), e(cid:48)(cid:48)(cid:105)(cid:105) ← next(g(cid:48) + e)q.push((cid:104)w(cid:48)(cid:48), e(cid:48)(cid:48), g(cid:48) + e(cid:105)).
else.
g[k] ← opt(g(cid:48))eρ ← outgoing edge from ρ in g[k]d ← opt(g(cid:48) − eρ)q.push((cid:104)w(d), eρ, g(cid:48) − e(cid:105))(cid:104) ·, (cid:104)w(cid:48), e(cid:48)(cid:105)(cid:105) ← next(g(cid:48) + eρ)q.push((cid:104)w(cid:48), e(cid:48), g + eρ(cid:105)).
yield g(k).
figure 9: k-best dependency tree enumeration algo-rithm..idea is formalized as algorithm kbest dep infig.
9. a walk-through of the exploration spaceusing kbest dep for our example graph in fig.
2is shown in fig.
8.theorem 4. for any graph g and k ≥ 1, at it-eration 1 ≤ k ≤ k, kbest dep(g, k) returnsg[k]..1330runtime analysis.
at the start of the algorithm,we call opt twice and next once.
then, at eachiteration we either make two calls two next, ortwo calls to opt and one call to next.
as bothalgorithms have a runtime of o(n 2), each iterationhas a runtime of o(n 2).
therefore, running kiterations gives a runtime of o(kn 2)..6 conclusion.
in this paper, we provided a simpliﬁcation tocamerini et al.
(1980)’s o(kn 2) k-best spanningtrees algorithm.
furthermore, we provided a novelextension to the algorithm that decodes the k-bestdependency trees in o(kn 2).
we motivated theneed for this new algorithm as using regular k-bestdecoding yields up to 36% trees which violationthe root constraint.
this is a substantial (up to 44times) increase in the violation rate from decodingthe one-best tree, and thus such an algorithm iseven more important than in the one-best case.
wehope that this paper encourages future research ink-best dependency parsing..acknowledgments.
we would like to thank the reviewers for theirvaluable feedback and suggestions to improve thiswork.
the ﬁrst author is supported by the uni-versity of cambridge school of technology vice-chancellor’s scholarship as well as by the univer-sity of cambridge department of computer sci-ence and technology’s epsrc..ethical concerns.
paolo m. camerini, luigi fratta, and francesco maf-ﬁoli.
1980. the k best spanning arborescences of anetwork.
networks, 10..michael collins and terry koo.
2005. discriminativereranking for natural language parsing.
computa-tional linguistics, 31..gonc¸alo m. correia, vlad niculae, wilker aziz, andandr´e f. t. martins.
2020. efﬁcient marginalizationof discrete and structured latent variables via spar-sity.
in advances in neural information processingsystems: annual conference on neural informationprocessing systems..bich-ngoc do and ines rehbein.
2020. neural rerank-ing for dependency parsing: an evaluation.
in pro-ceedings of the annual meeting of the associationfor computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in proceedings of the international conferenceon learning representations..harold n. gabow and robert endre tarjan.
1984. efﬁ-cient algorithms for a family of matroid intersectionproblems.
journal of algorithms, 5..leonidas georgiadis.
2003. arborescence optimiza-tion problems solvable by edmonds’ algorithm.
theoretical computer science, 301..keith hall.
2007. k-best spanning tree parsing.
in pro-ceedings of the annual meeting of the association ofcomputational linguistics..keith hall, jiˇr´ı havelka, and david a. smith.
2007.log-linear models of non-projective trees, k-bestin proceedings ofmst parsing and tree-ranking.
the joint conference on empirical methods in nat-ural language processing and computational natu-ral language learning..we do not foresee how the more efﬁcient algo-rithms presented this work exacerbate any existingethical concerns with nlp systems..liang huang and david chiang.
2005. better k-bestparsing.
in proceedings of the international work-shop on parsing technology..referencesˇzeljko agi´c.
2012. k-best spanning tree dependencyparsing with verb valency lexicon reranking.
in pro-ceedings of coling..eduard bejˇcek, eva hajiˇcov´a, jan hajiˇc, pavl´ınaj´ınov´a, v´aclava kettnerov´a, veronika kol´aˇrov´a,marie mikulov´a, jiˇr´ı m´ırovsk´y, anna nedoluzhko,lucie pol´akov´a, magdajarmila panevov´a,ˇsevˇc´ıkov´a,jan ˇstˇep´anek, and ˇs´arka zik´anov´a.
2013. prague dependency treebank 3.0..paolo m. camerini, luigi fratta, and francesco maf-ﬁoli.
1979. a note on ﬁnding optimum branchings.
networks, 9..kenton lee, mike lewis, and luke zettlemoyer.
2016.global neural ccg parsing with optimality guaran-tees.
in proceedings of the conference on empiricalmethods in natural language processing..xuezhe ma and eduard hovy.
2017. neural probabilis-tic model for non-projective mst parsing.
in pro-ceedings of the international joint conference onnatural language processing..ryan mcdonald, fernando pereira, kiril ribarov, andjan hajiˇc.
2005. non-projective dependency pars-ing using spanning tree algorithms.
in proceedingsof human language technology conference andconference on empirical methods in natural lan-guage processing..1331joakim nivre, mitchell abrams,.
ˇzeljko agi´c, larsahrenberg, lene antonsen, katya aplonova,maria jesus aranzabe, gashaw arutie, masayukiasahara, luma ateyah, mohammed attia, aitz-iber atutxa, liesbeth augustinus, elena badmaeva,miguel ballesteros, esha banerjee, sebastian bank,verginica barbu mititelu, victoria basmov, johnbauer, sandra bellato, kepa bengoetxea, yev-geni berzak, irshad ahmad bhat, riyaz ahmadbhat, erica biagetti, eckhard bick, rogier blok-land, victoria bobicev, carl b¨orstell, cristinabosco, gosse bouma, sam bowman, adrianeboyd, aljoscha burchardt, marie candito, bernardcaron, gauthier caron, g¨uls¸en cebiro˘glu eryi˘git,flavio massimiliano cecchini, giuseppe g. a.
ˇc´epl¨o, savas cetin, fabriciocelano, slavom´ırchalub, jinho choi, yongseok cho, jayeol chun,silvie cinkov´a, aur´elie collomb, c¸ a˘grı c¸ ¨oltekin,miriam connor, marine courtin, elizabeth david-son, marie-catherine de marneffe, valeria de paiva,arantza diaz de ilarraza, carly dickerson, pe-ter dirix, kaja dobrovoljc, timothy dozat, kiradroganova, puneet dwivedi, marhaba eli, alielkahky, binyam ephrem, tomaˇz erjavec, alineetienne, rich´ard farkas, hector fernandez al-calde, jennifer foster, cl´audia freitas, katar´ınagajdoˇsov´a, daniel galbraith, marcos garcia, moag¨ardenfors, sebastian garza, kim gerdes, filipginter, iakes goenaga, koldo gojenola, memduhg¨okırmak, yoav goldberg, xavier g´omez guino-vart, berta gonz´ales saavedra, matias grioni, nor-munds gr¯uz¯ıtis, bruno guillaume, c´eline guillot-barbance, nizar habash, jan hajiˇc, jan hajiˇc jr.,linh h`a m˜y, na-rae han, kim harris, dag haug,barbora hladk´a,jaroslava hlav´aˇcov´a, florinelhociung, petter hohle, jena hwang, radu ion,elena irimia, o. l´aj´ıd´e ishola, tom´aˇs jel´ınek, an-ders johannsen, fredrik jørgensen, h¨uner kas¸ıkara,sylvain kahane, hiroshi kanayama, jenna kan-erva, boris katz, tolga kayadelen, jessica ken-ney, v´aclava kettnerov´a, jesse kirchner, kamilkopacewicz, natalia kotsyba, simon krek, sooky-oung kwak, veronika laippala, lorenzo lam-bertino, lucia lam, tatiana lando, septina dianlarasati, alexei lavrentiev, john lee, phuonglˆe h`ˆong, alessandro lenci, saran lertpradit, her-man leung, cheuk ying li, josie li, keyingli, kyungtae lim, nikola ljubeˇsi´c, olga logi-nova, olga lyashevskaya, teresa lynn, vivienmacketanz, aibek makazhanov, michael mandl,christopher manning, ruli manurung, c˘at˘alinam˘ar˘anduc, david mareˇcek, katrin marheinecke,h´ector mart´ınez alonso, andr´e martins,janmaˇsek, yuji matsumoto, ryan mcdonald, gus-tavo mendonc¸a, niko miekka, margarita misir-pashayeva, anna missil¨a, c˘at˘alin mititelu, yusukemiyao, simonetta montemagni, amir more, lauramoreno romero, keiko sophie mori, shinsukemori, bjartur mortensen, bohdan moskalevskyi,kadri muischnek, yugo murawaki, kaili m¨u¨urisep,pinkey nainwani, juan ignacio navarro hor˜niacek,anna nedoluzhko, gunta neˇspore-b¯erzkalne, lu-ong nguy˜ˆen thi., huy`ˆen nguy˜ˆen thi.
minh, vitaly.
nikolaev, rattima nitisaroj, hanna nurmi, stinaojala, ad´edayo.
ol´u`okun, mai omura, petya osen-ova, robert ¨ostling, lilja øvrelid, niko partanen,elena pascual, marco passarotti, agnieszka pate-juk, guilherme paulino-passos, siyao peng, cenel-augusto perez, guy perrier, slav petrov, jussi piitu-lainen, emily pitler, barbara plank, thierry poibeau,martin popel, lauma pretkalnin¸a, sophie pr´evost,prokopis prokopidis, adam przepi´orkowski, ti-ina puolakainen, sampo pyysalo, andriela r¨a¨abis,alexandre rademaker, loganathan ramasamy,taraka rama, carlos ramisch, vinit ravishankar,livy real, siva reddy, georg rehm, michaelrießler, larissa rinaldi, laura rituma, luisarocha, mykhailo romanenko, rudolf rosa, daviderovati, valentin ros, ca, olga rudina, jack rueter,shoval sadde, benoˆıt sagot, shadi saleh, tanjasamardˇzi´c, stephanie samson, manuela sanguinetti,baiba saul¯ıte, yanin sawanakunanon, nathanschneider, sebastian schuster, djam´e seddah, wolf-gang seeker, mojgan seraji, mo shen, atsuko shi-mada, muh shohibussirri, dmitry sichinava, na-talia silveira, maria simi, radu simionescu, katalinsimk´o, m´aria ˇsimkov´a, kiril simov, aaron smith,isabela soares-bastos, carolyn spadine, antoniostella, milan straka, jana strnadov´a, alane suhr,umut sulubacak, zsolt sz´ant´o, dima taji, yutatakahashi, takaaki tanaka, isabelle tellier, trondtrosterud, anna trukhina, reut tsarfaty, francistyers, sumire uematsu, zdeˇnka ureˇsov´a, larraitzuria, hans uszkoreit, sowmya vajjala, daniel vanniekerk, gertjan van noord, viktor varga, ericvillemonte de la clergerie, veronika vincze, larswallin, jing xian wang, jonathan north washing-ton, seyi williams, mats wir´en, tsegay wolde-mariam, tak-sum wong, chunxiao yan, marat m.yavrumyan, zhuoran yu, zdenˇek ˇzabokrtsk´y, amirzeldes, daniel zeman, manying zhang, and hanzhizhu.
2018. universal dependencies 2.3.lin-dat/clarin digital library at the institute of for-mal and applied linguistics ( ´ufal), faculty ofmathematics and physics, charles university..adam pauls and dan klein.
2009. k-best a* parsing.
in proceedings of the joint conference of the an-nual meeting of the acl and the international jointconference on natural language processing of theafnlp..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyhuman languages.
in proceedings of the associationfor computational linguistics: system demonstra-tions..federico sangati, willem zuidema, and rens bod.
2009. a generative re-ranking model for depen-dency parsing.
in proceedings of the internationalconference on parsing technologies..libin shen, anoop sarkar, and franz josef och.
2004.discriminative reranking for machine translation.
inproceedings of the human language technology.
1332conference of the north american chapter of theassociation for computational linguistics..robert endre tarjan.
1977. finding optimum branch-.
ings.
networks, 7..hui zhang, min zhang, chew lim tan, and haizhouli.
2009. k-best combination of syntactic parsers.
in proceedings ofthe conference on empiricalmethods in natural language processing..chenxi zhu, xipeng qiu, xinchi chen, and xuanjinghuang.
2015. a re-ranking model for dependencyparser with recursive convolutional neural network.
in proceedings of the annual meeting of the associ-ation for computational linguistics and the interna-tional joint conference on natural language pro-cessing, volume 1..ran zmigrod, tim vieira, and ryan cotterell.
2020.please mind the root: decoding arborescences forin proceedings of the con-dependency parsing.
ference on empirical methods in natural languageprocessing..1333a supplementary materials for section 1 (introduction).
results table for fig.
1.language.
|train|.
|test|.
root constraint violation rate (%)k = 1 k = 5 k = 10 k = 20 k = 50.czechrussianestoniankoreanlatinnorwegianancient greekfrenchspanishold frenchgermanpolishhindicatalanitalianenglishdutchfinnishclassical chineselatvianbulgarianslovakportugueseromanianjapanesecroatianslovenianarabicukrainianbasquehebrewpersianindonesiandanishswedishold church slavonicurduchineseturkishgothicserbiangaliciannorth samiarmeniangreekuyghurvietnameseafrikaanswolofmalteseteluguscottish gaelichungarianirishtamilmarathibelarusianlithuaniankazakhupper sorbiankurmanjiburyatlivvi.
6849548814246332301016809156961501414450143051390913814137741330413123131211254312264122171100410156890784838328804371256914647860755496539652414798447743834303412440433997366433873328227222571975166216561400131511881123105110159108584003733191533123201919.
10148649132142287210119391047416172119279771727168418464822077596155520731823111610614777295501136788680892179949160055756512191141535500983102952086186527845690080042547051814653644945412047253551047623734908106.
0.450.490.930.960.520.520.571.680.170.521.540.000.180.540.210.480.670.390.960.880.270.380.420.410.000.880.380.290.900.671.020.671.260.531.231.051.121.802.540.780.191.161.270.000.440.563.386.351.490.580.000.754.232.420.002.130.797.272.586.4223.576.6112.26.
5.075.075.596.685.174.264.743.852.256.815.124.761.342.324.029.123.394.7222.527.054.664.813.311.265.132.902.663.797.493.642.812.434.064.354.6314.322.474.8012.478.652.042.077.497.343.207.186.7813.656.895.1727.817.167.447.141.1720.855.619.827.979.3427.0610.3714.15.
6.186.587.029.515.575.207.004.953.259.416.377.862.192.975.6610.734.186.1225.958.776.735.344.151.666.243.713.534.159.155.064.013.475.485.596.0817.643.085.9015.5311.182.602.3610.158.424.199.648.2514.738.326.7032.818.978.668.681.5021.709.0510.3610.6810.7229.2213.0015.00.
6.767.668.2411.916.256.228.385.813.9611.387.6310.112.983.687.2511.124.827.3928.099.958.165.294.762.167.204.444.594.7210.136.675.044.286.656.357.0919.883.607.6817.0913.103.162.8812.439.644.8012.249.5616.129.918.1236.1610.209.8210.231.8327.348.9910.8213.4511.7830.8715.4815.99.
7.678.999.4214.747.627.3810.696.984.8913.019.0613.004.044.519.1911.345.599.1529.9111.3110.295.295.752.818.795.625.795.2711.728.715.905.258.257.458.7322.054.399.3118.7314.864.233.4615.5410.815.8215.5711.3918.2612.179.7336.9911.7510.7511.733.0533.367.2712.4717.4113.4533.3319.1317.68.
1334b supplementary materials for section 3 (finding the second best tree).
theorem 1. for any graph g and e ∈ g(1).
where.
g(2) = (g − e)(1).
e = argmaxe(cid:48)∈g(1).
(cid:16).
(g − e(cid:48))(1)(cid:17).
w.proof.
there must be at least one edge e ∈ g(1) such that e (cid:54)∈ g(2).
therefore, there exists ane ∈ g(1) such that g(2) = (g − e)(1).
now suppose by way of contradiction that e is not as givenin (7).
if we choose an e(cid:48) that satisﬁes (7), then by deﬁnition w(cid:0)(g − e(cid:48))(1)(cid:1) > w(cid:0)(g − e)(1)(cid:1).
as(cid:4)(g − e(cid:48))(1) (cid:54)= g(1), we arrive at a contradiction..lemma 1. for any graph g, if g(1) =.
−(cid:65)g, then for some e ∈ g(1) and e(cid:48) ∈ b(g, e, g(1))g(2) = g(1) (cid:114) {e} ∪ {e(cid:48)}(8)proof.
by theorem 1, we have g(2) = (g − e)(1) where e = (i (cid:65) j) is chosen according to (7).
consider−−−(cid:65)g − e = g(1) (cid:114) {e} ∪ {e(cid:48)} where e(cid:48) is the second best incoming edge to jthe graph g − e; we have thatin g by the deﬁnition of the greedy graph.
−−−(cid:65)g − e is a tree and (g − e)(1) =−−−(cid:65)g − e has a cycle c by construction.
since this is a greedy graph,.
2. case e(cid:48) ∈ r(g, e, g(1)): then,.
1. case e(cid:48) ∈ b(g, e, g(1)): then.
−−−(cid:65)g − e..cycle c is critical.
in the expansion phase of the 1-best algorithm, we will break the cycle c..(a) case break c at j: then, e(cid:48) (cid:54)∈ (g − e)(1) and we must choose an edge e(cid:48)(cid:48) = (i(cid:48) (cid:65) j) to be in(g − e)(1).
we require that e(cid:48)(cid:48) ∈ b(g, e, g(1)) as we would otherwise re-introduce a cycle inthe expansion phase, which is not possible.
therefore, g(2) = g(1) (cid:114) {e} ∪ {e(cid:48)(cid:48)}..(b) case break c at j(cid:48).
(cid:54)= j: then, there exists an edge f = (i(cid:48)(cid:48) (cid:65) j(cid:48)) ∈ c (and in g(1))which is not in g(2).
instead, we choose f (cid:48) = (i(cid:48) (cid:65) j(cid:48)) to be in g(2).
therefore, g(2) =g(1) (cid:114) {e, f } ∪ {e(cid:48), f (cid:48)}.
however, it is not possible for f (cid:48) and e to form a cycle and sod = g(1) (cid:114) {f } ∪ {f (cid:48)} ∈ a(g) and w(d) > w(cid:0)g(2)(cid:1).
this is a contradiction as onlyw(cid:0)g(1)(cid:1) > w(cid:0)g(2)(cid:1)..lemma 2. for any g with a critical cycle c, either g(2) = (g/c)(2) (cid:35) c (with w(cid:0)g(2)(cid:1) = w(cid:0)(g/c)(2)(cid:1))or g(2) = (g − e)(1) (with w(cid:0)g(2)(cid:1) = w(cid:0)g(1)(cid:1) − wg(e)) for some e ∈ c ∩ g(1).
proof.
it must be that g(2) = (g/c)(2) (cid:35) c or g(2) (cid:54)= (g/c)(2) (cid:35) c..1. case g(2) = (g/c)(2) (cid:35) c: since the weight of a tree is preserved during expansion, we are done..2. case g(2) (cid:54)= (g/c)(2) (cid:35) c: then, for all e(cid:48) ∈ (g/c)(1), π(e(cid:48)) ∈ g(2).
therefore, if j is the entrancej = (cj − e)(1)− wcj (e).
thus, g(2) = (g − e)(1) where e ∈ c ∩ d and.
site of c in (g/c)(1), g(2) = π((g/c)(1)) ∪ c(2)c(2)for e ∈ c(1)and w= wjw(cid:0)g(2)(cid:1) = w(cid:0)g(1)(cid:1) − wg(e)..−(cid:65)cj, by corollary 1, c(2).
.
as c(1).
c(1)j.j =.
(cid:17).
(cid:16).
(cid:16).
(cid:17).
j.j.
(6).
(7).
(cid:4).
(cid:4).
theorem 2. for any graph g, executing next(g) returns g(1) and (cid:104)w, e(cid:105) such that g(2) = (g − e)(1)and w(cid:0)g(2)(cid:1) = w..1335proof.
next(g) returns g(1) by the correctness of opt.
we prove that w, e satisfy the above conditions..1. case g(1) =.
2. case g(1) (cid:54)=next(cid:0)g/c.
−(cid:65)g: then, by corollary 1 we can ﬁnd the best edge to remove and the weight of g(2).
−(cid:65)g: then, g has a critical cycle c. by lemma 2, we can either recursively call(cid:1) or examine the edges in c ∩ g(1) to ﬁnd the best edge to remove and the weight of g(2)..c supplementary materials for section 4 (finding the k th best tree).
lemma 3. for any graph g and k > 1, there exists a subgraph g(cid:48) ⊆ g and 1 ≤ l < k such thatg(l) = g(cid:48)(1) and g(k) = g(cid:48)(2)..proof.
there must exist some subgraph g(cid:48) ⊆ g such that g(k) = g(cid:48)(2).
suppose by way of contradictionthat there does not exist an l < k such that g(l) = g(cid:48)(1).
however, since w(cid:0)g(cid:48)(1)(cid:1) > w(cid:0)g(k+1)(cid:1), g(cid:48)(1)(cid:4)must be in the k-highest weighted trees.
therefore, there must exist an l such that g(l) = g(cid:48)(1).
theorem 3. for any graph g and k > 0, at any iteration 1 ≤ k ≤ k, kbest(g, k) returns g(k)..proof.
we prove this by induction on k.base case: then, k = 1 and g(1) is returned by theorem 2.inductive step: assume that for all l ≤ k, at iteration l, g(l) is returned.
now consider iteration k + 1, bylemma 3, we know that g(k+1) = g(cid:48)(2) where g(cid:48)(1) = g(l) for some l ≤ k. by the induction hypothesis,g(l) is returned at the lth iteration, and by theorem 2, we have pushed g(cid:48)(2) onto the queue.
therefore,(cid:4)we will return g(k+1)..d supplementary materials for section 5 (finding the k th best dependency tree).
lemma 4. for any graph g and edge eρ = (ρ (cid:65) j) ∈ g[1], g[2] = (g − eρ)[1] or g[2] = (g + eρ)[2].
proof.
if eρ (cid:54)∈ g[2], then clearly g[2] = (g − eρ)[1].
otherwise, eρ ∈ g[2].
as eρ ∈ g[1], g[2] =(cid:4)(g + eρ)[2]..lemma 5. for any graph g and k > 1, if e = (ρ (cid:65) j) ∈ g[k], then either e is not in any of thek −1-best trees or there exists a subgraph g(cid:48) ⊆ g and 1 ≤ l < k such that g[l] = g(cid:48)[1], e ∈ g(cid:48)[1] andg[k] = g(cid:48)[2]..proof.
it must be that either there exists an 1 ≤ l < k such that e ∈ g[l] (case 1) or no such l exists(case 2)..1. consider the graph g + e. under our deﬁnition of edge-inclusion graphs for dependency trees,a(g + e) = d(g + e).
then, by lemma 3, there exists a l(cid:48) and g(cid:48) such that (g[l(cid:48)] = g(cid:48)[1] andg[k] = g(cid:48)[2]..2. then, e is not in any of the (k −1)-best trees..(cid:4).
(cid:4).
theorem 4. for any graph g and k ≥ 1, at iteration 1 ≤ k ≤ k, kbest dep(g, k) returns g[k]..proof.
we prove this by induction on k.base case: then, k = 1 and g(1) is returned by the correctness of opt.
inductive step: assume that for all l ≤ k, at iteration l, g[l] was returned.
now consider iteration k + 1,by lemma 5, we know that either g[k+1] has a unique root edge to the k-best trees (case 1) or e = (ρ (cid:65) j)and there exists a g(cid:48) and l ≤ k such that g(cid:48)(1) = g(l), e ∈ g(l), and g[k+1] = g(cid:48)[2] (case 2)..13361. there always exists a tree in the queue that has a unique root edge to all trees that came before it..furthermore, it is the highest such tree by the correctness of opt..2. by our induction hypothesis, g[l] is returned at the lth iteration, and by theorem 2, we have pushed.
g(cid:48) + e[2] onto the queue.
therefore, we will return g[k+1]..(cid:4).
1337