augnlg: few-shot natural language generation using self-traineddata augmentation.
xinnuo xu1, guoyin wang2, young-bum kim2, sungjin lee21the interaction lab, heriot-watt university, edinburgh2amazon alexa ai, seattle, wa, usaxx6@hw.ac.uk, guoyiwan, youngbum, sungjinl@amazon.com.
abstract.
natural language generation (nlg) is a keycomponent in a task-oriented dialogue system,which converts the structured meaning repre-sentation (mr) to the natural language.
forlarge-scale conversational systems, where it iscommon to have over hundreds of intents andthousands of slots, neither template-based ap-proaches nor model-based approaches are scal-able.
recently, neural nlgs started lever-aging transfer learning and showed promis-ing results in few-shot settings.
this paperproposes augnlg, a novel data augmenta-tion approach that combines a self-trained neu-ral retrieval model with a few-shot learnednlu model, to automatically create mr-to-text data from open-domain texts.
the pro-posed system mostly outperforms the state-of-the-art methods on the fewshotwoz datain both bleu and slot error rate.
we fur-ther conﬁrm improved results on the few-shotsgd data and provide comprehensiveanalysis results on key components of our sys-tem.
our code and data are available at https://github.com/xinnuoxu/augnlg..1.introduction.
large-scale conversational systems provide a nat-ural interface to achieve various daily-life tasks.
natural language generation (nlg) is a key com-ponent in such a system to convert the structuredmeaning representation (mr) to the natural lan-guage, as shown in figure 1. in task-oriented dia-logue systems, nlg is typically accomplished byﬁlling out a basic set of developer-provided tem-plates, leading to a conversational system generat-ing unnatural, robotic responses.
in order to makethe system sound more human-like, model-basednlg approaches, in particular neural models, haverecently been gaining an increasing traction (gaoet al., 2018; wen et al., 2015).
however, neitherthe template-based approaches nor the model-based.
figure 1: an example of nlg task.
the model takesin the system mr, which consists of an intent with slotvalue pairs, and outputs text in natural language..approaches are sufﬁciently scalable for large-scaleconversational systems, where it is common to haveover hundreds of intents and thousands of slots..with the rise of neural transfer learning fornlp using pretrained lms, recently, neural nlgsstarted to leverage transfer learning and showedsome promising results (radford et al., 2019;brown et al., 2020; dai et al., 2019; edunov et al.,2019).
in particular, peng et al.
(2020) proposedfewshotwoz, the ﬁrst nlg benchmark test infew-shot learning settings, and achieved a sotaperformance by leveraging existing mr-to-textdata sets via task-speciﬁc continued pre-training.
despite the improved result, their approach leaveslittle room for further improvements as mr-to-textdata are expensive to obtain for new domains, prac-tically circling back to the same scalability problemafter exhausting the existing data..in order to go beyond this restriction, this pa-per proposes augnlg, a novel data augmenta-tion approach, that automatically creates mr-to-text data from open-domain texts by combining aself-trained neural retrieval model with a few-shotlearned nlu model.
since our data augmenta-tion approach is orthogonal to the prior transferlearning approaches, one can use our approach inconjunction with other approaches.
in experiments,we empirically show that augnlg mostly booststhe performance of both the ﬁne-tuned gpt-2 (ft-gpt) (radford et al., 2019) and sc-gpt (penget al., 2020), the continued pretraining approachwith existing mr-to-text data, on the fewshot-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1183–1195august1–6,2021.©2021associationforcomputationallinguistics1183system mrintent: requestslot-value pairs: [city =?]
generated textwhich city are you interested in?
nlgto augment data for nlu tasks by retrieving sen-tences from data crawled on the web.
however,their method cannot be directly applied to the nlgproblem since it does not yield mr annotations.
our approach, in contrast, generates mr-to-textdata by jointly employing a self-trained neural re-trieval model with a few-shot learned nlu model..3 few-shot transfer learning for nlg.
the goal of nlg is to translate an mr a intoits natural language response x = (cid:2)x1, .
.
.
, xt (cid:3),where xi is the ith token in the sequence x and t isthe sequence length.
a is deﬁned as the combina-tion of intent i and slot-value pairs {(si, vi)}pi=1:.
(1).
a = {i, (s1, v1), .
.
.
, (sp , vp )},where the intent stands for the illocutionary typeof the system action while slot-value pairs indicatecategory names and their values to embed in theutterance.
for example, in the mr, inform (food =chinese ; price = cheap), inform is the intent, foodand price are two slot keys and chinese and cheapare the corresponding slot values..given in-domain mr-to-text data d ={(an, xn)}nn=1 for training, where n is the num-ber of examples, a statistical neural language modelparameterized by θ is adopted to characterize theconditional probability pθ(x|a).
by adopting thechain rule on auto-regressive generation, the jointprobability of x conditioned on a is decomposedas (cid:81)tt=1 pθ(xt|x<t, a).
the training process, i.e.
the learning of θ, is then deﬁned as maximizing thelog-likelihood of the conditional probabilities overthe entire training dataset:.
lθ(d) =.
log pθ(xn|an)..|d|(cid:88).
n=1.
in the few-shot learning setup, the number oftraining examples n is extremely small (e.g.
≤ 50),which easily leads to non-ﬂuent generated sen-tences with many grammar mistakes or missingpieces of information.
in order to combat the datasparseness problem, inspired by prior transfer learn-ing approaches, we introduce a three-step pipelineto gradually evolve a general large-scale languagemodel to a domain-speciﬁc nlg model (shown infigure 2): (1) pre-training a base language modelwith massive amounts of text, (2) nlg-speciﬁccontinued pre-training with auto-augmented mr-to-text data, and (3) ﬁnal ﬁne-tuning with the lim-ited in-domain mr-to-text ground-truth data..figure 2: the training procedure for augnlg..woz task.
furthermore, we construct another few-shot learning testbed, fewshotsgd, out of theschema-guided dialogue (sgd) corpus (rastogiet al., 2020) and conﬁrm improved results by apply-ing augnlg to the ft-gpt.
1 finally, we providecomprehensive analysis results on the key compo-nents of our system to gain detailed insights intothe relationship between component-wise behaviorand various parameters..2 related work.
nlg for dialogue response generation therehas been a body of work on neural nlg models,adopting various architectures, such as rnns (wenet al., 2015), attention rnns (duˇsek and jurˇc´ıˇcek,2016), sc-lstm (wen et al., 2016), t2g2 (kaleand rastogi, 2020), adaptercl (madotto et al.,2020) and associated variants (tran and le nguyen,2017; tran et al., 2017).
despite the improved ﬂex-ibility and naturalness over template-based meth-ods, neural approaches require large amounts ofannotated data to reach good performance.
data augmentation data augmentation has beenwidely applied to a variety of nlp tasks, includingsentence classiﬁcation (xie et al., 2020), naturallanguage inference (hu et al., 2019) and spokenlanguage understanding (li et al., 2019; quan andxiong, 2019; zhao et al., 2019).
prior approachesfor text data utilized back-translation (sennrichet al., 2016; edunov et al., 2018), c-bert wordreplacement (jiao et al., 2020), mixed labels andrepresentations (guo et al., 2019; chen et al., 2020)and paraphrase data (gao et al., 2020).
however,the range of augmented data will be inherently lim-ited, particularly in few-shot learning settings dueto the nature of prior approaches, which only lever-ages in-domain data.
in contrast, we take a rarelyexplored approach, tapping into a wealth of open-domain text that covers almost all topics.
recently,du et al.
(2021) proposed a self-training method.
1since sgd accounts for a large portion of the existingmr-to-text data that sc-gpt utilized in training, we couldnot apply augnlg to sc-gpt for the fewshotsgd task..1184auto-augmented  mr-to-text pairsnlg pre-trainingnlg fine-tuningplain textlm pre-trainingin-domain mr-to-text pairsfigure 3: the overall pipeline for mr-to-text data augmentation..speciﬁcally, in step (1), we adopt gpt-2 (rad-ford et al., 2019) as our base language model sincegpt-2 has demonstrated a remarkable performanceon auto-regressive text generation tasks, which isclose to mr-to-text generation, in a variety ofdomains.
however, gpt-2 is pre-trained on open-webtext and the language style and topics thereofare quite different from those of daily conversa-tions in a target domain.
furthermore, the gen-eration task in nlg is conditioned on the inputmr, as opposed to the unconditioned generationof the underlying gpt-2 pre-training task.
thus,to bring the model a step closer to the ﬁnal nlgmodel in the target domain, in step (2), we contin-uously pre-train the gpt-2 model on an automat-ically constructed set of augmented mr-to-textpairs d(cid:48) = {(am, xm)}mm=1, where m is the num-ber of augmented examples, which is much largerthan the amount of in-domain ground-truth data.
data augmentation is achieved by retrieving a largeamount of relevant text from reddit (hendersonet al., 2019) with a self-trained neural retrievalmodel and then synthesizing mrs with a few-shotlearned nlu model.
the details of data augmenta-tion is described in section 4. finally, in step (3),we ﬁne-tune the nlg model on a limited amountof in-domain ground-truth mr-to-text pairs d fora ﬁnal adaptation..4 data augmentation.
the data augmentation procedure aims to con-struct a large amount of mr-to-text pairs d(cid:48)from open-domain texts that are relevant to thein-domain ground-truth mr-to-text pairs d. theaugmentation process consists of two stages: (1).
retrieving keyword-matching utterances and ﬁl-tering out domain-irrelevant instances, (2) gen-erating synthetic mr annotations.
figure 3 il-lustrates the overall pipeline with some exam-ples.
for further analysis and studies, we re-lease the data from all intermediate steps foreach domain at https://github.com/xinnuoxu/augnlg/tree/master/augmented_data..4.1 retrieval and filtering.
the utterance retrieval and ﬁltering procedure con-sists of three steps: (1) keyword extraction thatcollects n-gram keywords from all in-domain utter-ances x = {xn}nn=1; (2) keyword-based retrievalthat searches the open-domain texts for utterancesthat match any keywords extracted in the previ-ous step, yielding a set of utterances x(cid:48)cand; (3)self-trained neural classiﬁer that ﬁlters out someretrieved utterances that are semantically irrelevantto the target domain.
after the ﬁltering, we form anaugmented set of utterances x(cid:48) with the unﬁlteredutterances..keywords extraction.
to efﬁciently extractkeywords, we ﬁrst gather all n-gram phrases thatappear in x. since some phrases are too general tobe effective, e.g.
“i cannot”, “is your”, we use tf-idf scores to measure the speciﬁcity of a phrase(see appendix a for more detail).
we ﬁrst rank thecollected n-grams according to their tf-idf scoresand ﬁlter out those n-gram phrases with relativelylow tf-idf score..keyword-based retrieval.
having extractedthe keywords, we retrieve utterances from the open-domain utterance pool that contains at least one.
1185in-domain mr-to-text pairs      in the restaurant domain (texts are     ) mr: inform_no_match (kidsallowed = yes)text: i cannot ﬁnd restaurants with kids allowed.keywords: ... restaurants with, with kids ... unlabelled open-domain textsyou are a war hero.
we love food in chicago.how clean is your room now?
i prefer it to gingers.why aren't they good?
with kids movies ?
whats your solution then?
keyword-based candidates retrievalyou are a war hero.
we love food in chicago.how clean is your room now?
i prefer it to gingers.why aren't they good?
with kids movies ?
whats your solution then?
self-trainedclassiﬁcation✅we love food in chicago.
✅i prefer it to gingers.🚫with kids movies ?mr: inform (food = chinese ; price = cheap) text: it serves chinese food in the cheap price.keywords: ... chinese food, food in ... mr: select (near = civic center ; near = dont_care)text: would you prefer it near the civic centre?keywords: ...prefer it, it near ... synthetic mrannotationmr: conﬁrm ( near = chicago)text: we love food in chicago.
mr: conﬁrm ( food = gingers) text: i prefer it to gingers.
augmented dataretrieval and filteringsynthetic mr annotationfiltered textsretrieved textscand, cl) ≥ σ}.
4.2 synthetic mr annotation.
algorithm 1 self-trained neural filteringrequire: in-domain utterances x in the target do-.
main; retrieved utterances x(cid:48).
cand.
0 = u +; e −.
1: u + ← positive examples x2: u − ← randomly selected negative examples3: c0 ← train(u +, u −)4: l ← maximum number of iterations5: l = 1; e +6: while l ≤ l doe +l ← {x(cid:48) if predict(x(cid:48)7:e −l ← {x(cid:48) if predict(x(cid:48)l ← e +e +l + u +(cid:12) − (cid:12)(cid:12)(cid:12)if (cid:12)(cid:12)e +(cid:12)e +(cid:12) ≤ δ thenll−1converged; break.
cand, cl−1) ≥ σ+}cand, cl−1) ≤ σ−}.
0 = u −.
9:.
8:.
10:.
11:.
12:.
13:.
end ifcl ← train(e +ll ← l + 1., e −l ).
14:15: end while16: x(cid:48) ← {x(cid:48) if predict(x(cid:48).
extracted keyword in it.
the aim of this step isto source a large amount of domain-relevant utter-ances x(cid:48).
cand based on the surface-level overlap..self-trained neural filtering.
although thekeyword-based retrieval is efﬁcient, the retrieved ut-terances x(cid:48)cand can be quite noisy since an n-gramkeyword only matches some part of the utterance,failing to detect the existence of irrelevant pieces inother parts.
for example, in figure 3, even thoughthe utterance “with kids movies?” contains thekeyword “with kids”, it is irrelevant to the targetdomain restaurant given the word movies.
thus,we introduce a self-trained neural classiﬁer to ﬁl-ter out domain-irrelevant utterances from x(cid:48)cand byconsidering the semantic representation of an entireutterance and yield a domain-relevant set x(cid:48)..the algorithm of the self-training and ﬁlteringprocess is listed in algorithm 1. we adopt a bert(devlin et al., 2019) model with a binary classiﬁ-cation layer atop as the base model and then trainthe classiﬁer with in-domain utterances x and ran-domly selected open-domain utterances2 , servingas positive and negative examples (u + and u −), re-spectively.
after that, the self-training and ﬁlteringcycle starts.
at each iteration, we make predic-tions on the utterances in x(cid:48)cand with the classiﬁer.
2all utterances in x(cid:48).
cand are excluded from the open-domain utterance pool.
to balance the precision and re-call, we control the size of the initial negative set such that(cid:12)u −(cid:12)(cid:12).
(cid:12), where λ1 = 10..(cid:12) = λ1 · (cid:12).
(cid:12)u +(cid:12).
trained in the previous iteration.
all utteranceswith a score over the threshold σ+, together withthe in-domain utterances x, are then taken as anew set of positive examples e +, whereas all utter-ances with a score less than the threshold σ− arecollected as a new set of negative examples e −.3the self-training loop terminates if either the in-crement of positive examples at the last iteration isless than the threshold δ or the iterations is over thepre-deﬁned maximum number of iterations.
other-wise, a new classiﬁer is trained on e + and e − andthe algorithm keeps going on the loop.
once theloop terminated, we label all utterances in x(cid:48)candwith the classiﬁer from the last iteration.
finally,we build a domain-relevant set of augmented utter-ances x(cid:48) by taking all utterances with a score overthe threshold σ.4.
having built the domain-relevant set of augmentedutterances x(cid:48), we now proceed to synthesize mrlabels to produce a complete mr-to-text datasetd(cid:48).
to this end, we build a few-shot nlu model byﬁne-tuning a bert model with in-domain ground-truth data.
to put the data in the right format forthe nlu task, we take mrs and utterances as la-bels and model inputs, respectively.
each token isannotated with the slot name if it is a part of theassociated slot value and the ﬁnal hidden state ofthe special token [cls] is used to predict the intent(see figure 5 in appendix b).
finally, we gener-ate an mr-to-text dataset d(cid:48) by concatenating theutterances in x(cid:48) with the synthetic mr labels pre-dicted by the few-shot nlu model..5 experimental setup.
5.1 dataset.
fewshot nlg data fewshotwoz is a few-shot nlg benchmark, built upon rnnlg andmultiwoz (budzianowski et al., 2018).
in eachdomain, mr-to-text pairs are grouped accordingto their delexicalized mrs (i.e.
slot values beingmasked) and a training set is created by takinga pair each from 50 random groups and then therest are taken as the test set.
we also construct anew dataset fewshotsgd by applying the same.
3to guarantee the precision of the positive examples, weuse σ+ = 0.99 and σ− = 0.5. also, we sub-sample negative(cid:12) = λ2 · (cid:12)examples such that (cid:12).
(cid:12), where λ2 = 5..(cid:12)e −(cid:12).
(cid:12)e +(cid:12).
4to harvest a large amount of utterances, we set the thresh-.
old σ to 0.5..1186statistics# domainsavg.
# intentsavg.
# slotsavg.
# delex mrs in trainingavg.
# delex mrs in testingavg.
# training instancesavg.
# test instancesavg.
# test instances per mravg.
# test novelty uni-gram (%)avg.
# test novelty bi-gram(%)avg.
# test novelty tri-gram(%)avg.
# test novelty four-gram(%)avg.
# keywords (k)avg.
# retrieved utterances (k)avg.
# augmented pairs (k)avg.
# delex.
mrs in aug. pairs (k).
-woz -sgd.
78.1416.250473504731.1412.9744.4268.2082.700.20854.834.02.12.
166.4411.33331355618472.923.9065.2984.4492.750.12731.325.60.57.table 1: comparison of fewshotwoz and few-shotsgd.
the bottom section shows the statistics foraugmented data.
the unit for all statistics in the bottomsection is thousand(k)..preparation steps to the sgd corpus.
the com-parison of fewshotwoz and fewshotsgdis presented in the top section in table 1. com-paring to fewshotwoz, fewshotsgd has (1)more domains, (2) less intents, slots and delexi-calized mrs5 (3) more testing examples for eachdelexicalized mr, (4) more novel n-grams6 in testutterances..augmented data since reddit has shown to pro-vide natural conversational english data, we adoptreddit (henderson et al., 2019) as the open-domainutterance pool after ﬁltering for utterances of lengthbetween 2 and 40, totalling about 0.7b utterances.
the average number of extracted keywords, re-trieved utterances, ﬁnal augmented mr-to-textpairs and delexicalized mrs over all domains infewshotwoz and fewshotsgd are shown inthe bottom section of table 1. the detailed break-downs of each domain are listed in table 9 andtable 10 in appendix c..5.2 evaluation metrics.
following wen et al.
(2015) and peng et al.
(2020),we use bleu score and slot error rate (err)for automatic evaluation.
bleu score measuresthe surface-level similarity between generated re-sponses and human-authored references.
whereas,.
5note that, the average number of delexicalized mrs inthe training set is 33, which means the number of trainingexamples in some domains are less than 50..6the novelty is calculated by dividing the number of n-grams in the test set that does not appear in the training set bythe number of n-grams in the test set..7https://github.com/pengbaolin/sc-gpt..err measures the semantic alignment in termsof slot-value insertion and omission.
speciﬁcally,err = (p + q)/m , where m is the total numberof slots in the mr and p, q are the number of miss-ing and redundant slots in the surface realisation.
since the sgd dataset does not provide enoughinformation to compute err, we report err onlyon fewshotwoz..5.3 systems.
we apply our data augmentation approachaugnlg to two baseline systems,.
• ft-gpt gpt-2 is directly ﬁne-tuned on thein-domain ground-truth mr-to-text data.
weintroduce augnlg-ft, which further pre-trains gpt-2 on the augmented mr-to-textdata and performs a ﬁnal ﬁne-tuning on thein-domain data..• sc-gpt (peng et al., 2020) further pre-trainsgpt-2 on existing mr-to-text data borrowedfrom other nlg corpora and ﬁne-tunes onthe in-domain data.
we introduce augnlg-sc, which pre-trains gpt-2 on both exist-ing mr-to-text data and automatically aug-mented data, and ﬁnally ﬁne-tunes on the in-domain data..6 results.
fewshotwoz table 2 reports the results onfewshotwoz.
augnlg-ft substantially out-performs ft-gpt across all domains in both bleuand err.
similarly, augnlg-sc performs bet-ter than sc-gpt and achieves the state-of-the-art performance in most domains.
remarkably,augnlg-ft achieves a competitive performancewith sc-gpt in many domains without leveragingany existing mr-to-text data.
it even outperformssc-gpt in “tv” and “attraction” domain in bothbleu and err..fewshotsgd table 3 shows the results infewshotsgd.
due to the higher novelty of thetest examples and the smaller amount of trainingexamples (see avg.
# test novelty n-gram and #training instances in table 1), ft-gpt performsworse than on fewshotwoz.
this indicates thatthe few-shot settings on fewshotsgd are evenmore challenging.
but augnlg-ft managed tooutperform ft-gpt by a large margin via the con-tinued pre-training on the augmented examples..1187model.
laptop.
restauranttvbleu err bleu err bleu err bleu err bleu err bleu err bleu err17.45 22.83 13.06 25.59 14.84 28.5728.15 15.87ft-gpt20.61 13.58 14.95 10.64 16.70 10.7132.16 4.79augnlg-ft19.21 4.7622.24 16.62 17.06 8.82sc-gpt30.48 6.8917.81 3.5722.50 10.40 16.35 6.13augnlg-sc 34.20 2.99.
28.83 11.82 36.51 14.29 33.73 9.2834.80 6.9236.99 9.8933.64 5.1438.30 8.2433.82 7.3233.51 5.3834.99 5.5334.96 6.5934.32 2.83.attraction.
train.
hotel.
taxi.
table 2: evaluation results on fewshotwoz (bleu↑, err↓).
note that, the sc-gpt model reported here waspre-trained and ﬁne-tuned using the code and only the sgd data shared by the original authors 7..modelft-gptaugnlg-ftmodelft-gptaugnlg-ft.flights calendarrestaurants hotels12.1808.8417.2317.58media movies10.0503.1711.9608.62.
08.9817.83homes03.7512.27.
05.2710.45music05.7912.76.banks06.0908.94.
06.7913.32.weather10.5213.75.
13.8715.54.buses07.7714.26services09.7916.82.events09.1718.68travel02.0814.35.rentalcars ridesharing.
table 3: evaluation results in bleu on fewshotsgd..qualitative evaluation table 4 compares somegenerated utterances by different models on few-shotwoz (examples in fewshotsgd areshown in table 16 in appendix e).
both ft-gptand sc-gpt are prone to omit important slots.
comparing to sc-gpt, ft-gpt tends to over-generate and introduces hallucinations.
however,augnlg and augnlg-sc managed to generateﬂuent, natural text while precisely reﬂecting the theinput mr. we further examined 70 randomly sam-pled utterances generated by augnlg-sc, whosebleu scores are lower than those generated by sc-gpt, in the “hotel”, “train” and “taxi” domainto understand some potential factors causing thelower bleu scores we found that the lower bleuscores are mainly driven by bleu penalizing se-mantically correct paraphrases due to the nature ofbleu only checking surface-level matches.
someexamples of such penalization are provided in ta-ble 15 in appendix e. only 7 out of the 70 manu-ally checked examples generated by augnlg-scare actually worse than sc-gpt.8.
in sum, the results (1) verify the effectiveness ofcomplementing existing transfer learning methodswith our novel data augmentation approach; (2) re-veal that automatically augmented mr-to-text dataalone can lead to a competitive performance, previ-ously only achieved with existing mr-to-text data.
since existing mr-to-text data is not a scalabledata source, our approach brings more practicalvalues to real-world applications; (3) indicate that.
8we also examined 70 randomly sampled utterances gener-ated by augnlg-sc, whose bleu scores are equal/higherthan those generated by sc-gpt.
among these examples, 35examples are actually better and 7 examples are worse thanthe sc-gpt generations..leveraging augmented mr-to-text data on top ofexisting mr-to-text data yields a new sota per-formance on the benchmark test..7.in-depth analysis.
in this section, we provide comprehensive analy-sis results on the key components and parametersof our system to gain detailed insights: (1) intrin-sic evaluation on augmented data, (2) inﬂuenceof nlu quality, and (3) performance trends overvarying amounts of augmented data..7.1.intrinsic evaluation on augmented data.
for intrinsic evaluation of augmented data, we ﬁrstintroduce four metrics:.
• mr coverage (mr cov.)
evaluates the cov-erage of delexicalized mrs of the test set in theaugmented set:.
mr cov.
=.
# delexicalized mrs ∈ a(cid:48) ∩ atest# delexicalized mrs ∈ atest.
,.
where a(cid:48) and atest denote delexicalized mrs in theaugmented set and the test set, respectively.
highermr cov.
values indicate that more delexicalizedmrs of the test set appear in the augmented set..• slot coverage (sl cov.)
evaluates the coverage.
of slot keys of the test set in the augmented set..• language model perplexity (ppl) is the per-plexity of augmented utterances calculated by agpt-2 language model ﬁne-tuned on the test set.
lower ppl values indicate that the distribution ofaugmented utterances is close to that of the testutterances..• average n-gram novelty (nvt.)
n-gram noveltymeasures the fraction of the n-grams in the test set.
1188domain: restaurant.
input mrreferenceft-gptsc-gptaugnlg-ft marlowe is a good restaurant that serves good dinner in mission bay.
augnlg-sc marlowe is good for dinner in mission bay..inform(name=marlowe; goodformeal=dinner; area=mission bay)marlowe serves dinner in the mission bay area.
there is a restaurant marlowe in the mission bay area called dinner, it is good for dinner, a good area.
marlowe is a good restaurant in mission bay that serves dinner..domain: laptop.
input mrreferenceft-gptsc-gptaugnlg-ftaugnlg-sc tecra proteus 23 laptop - 9 hour battery - 4 gb memory - not used for business computing..inform(name=tecra proteus 23;type=laptop;battery=9 hour;memory=4 gb;isforbusiness=false)the tecra proteus 23 laptop has a 9 hour battery life with 4 gb memory but is not for business computing.
tecra proteus 23 laptop with 9 hour battery and 4 gb memory.
% miss {isforbusiness=false}tecra proteus 23 laptop with 4 gb memory and 9 hour battery.
% miss {isforbusiness=false}the tecra proteus 23 laptop has a 9 hour battery, 4 gb of memory, and is not for business computing..domain: tv.
input mrreferenceft-gptsc-gptaugnlg-ftaugnlg-sc the pontus 45 tv has a medium screen size of 2 hdmi ports and a+ eco rating..inform(name=pontus 45; type=tv;ecorating=a+; screensizerange=medium; hdmiport=2)the pontus 45 tv has a a+ eco rating and a medium screen size and 2 hdmi ports.
the pontus 45 tv has 2 hdmi ports, a medium screen size , and 2 screensize.
% miss {ecorating=a+}the pontus 45 tv has a medium screen size and 2 hdmi ports.
% miss {ecorating=a+}the pontus 45 tv has a+ eco rating, 2 hdmi ports, and a medium screen size..domain: attraction.
input mrreferenceft-gptsc-gptaugnlg-ftaugnlg-sc yes, the park is in the north.
no entrance fee.
phone number 01223..inform(area=north;price=no entrance fee; type=park; phone=01223)yes, it’s a park located in the north area of town and has no entrance fee.
the phone number is 01223.yes, there is a free entrance fee of £1.
phone 01223 for more information.
% miss {area=north}no problem.
01223 is the phone number and the park is north.
the entrance fee is free.
yes, the entrance fee is no entrance fee.
the park is in the north.
phone is 01223..table 4: example utterances generated by different models on fewshotwoz (better viewed in color).
errorsare shown in three colors.
the red text starting with “%” denotes omission.
the blue text indicates hallucination.
the green text means non-ﬂuent generation..metricsmr cov.
↑sl cov.
↑.
re la ho tv at tr ta.59.40.70.86.941.0.
.711.0.
.66.89.
.21.95.
.44.92.table 5: augmented data evaluation of mr cov.
andsl cov.
on fewshotwoz.
the domain names arerepresented by the ﬁrst two letters..metricsmr cov ↑sl cov ↑metricsmr cov ↑sl cov ↑.
.72.85.
.66.89.
.65.75.re ho fl ca ba we bu ev.80.70.80.58.88.92.93.86se trho me mo mu re ri.55.88.77.59.71.93.78.75.
.43.57.
.81.80.
.67.75.
.74.80.
.58.67.table 6: augmented data evaluation of mr cov.
andsl cov.
on fewshotsgd.
the domain names arerepresented by the ﬁrst two letters..that do not appear in the augmented set:.
n-gram novelty = 1 −.
# n-grams ∈ x(cid:48) ∩ xtest# n-grams ∈ xtest.
,.
where x(cid:48) and xtest denote utterances in the aug-mented set and test set, respectively.
lower nvt.
values indicate that more n-grams of the test setappear in the augmented set.
we consider from1-grams to 4-grams and report the average value..the results of mr cov.
/ sl cov.
on fewshot-.
woz and fewshotsgd are shown in table 5and table 6, respectively.
sl cov.
achieves 70%in most domains on both datasets while mr cov.
has a wide range of values across domains.
note-worthily, table 6 strongly correlates with table 3– “banks” and “media” domains are worse thanother domains in both coverage metrics and nlgperformance.
on the other hand, “restaurants” and“events” domains are better than the others in bothaspects.
although we do not see the same patternon fewshotwoz, it could be attributed to thelarge variance in the number of delexicalized mrsin each domain (see table 2 in (peng et al., 2020)).
the results of ppl and nvt.
on fewshotwozare shown in table 7. we compare the augmenteddata (aug) with the existing mr-to-text data (ex-ist).
the top section shows that aug achieveslower ppl values in all seven domains comparedto exist.
the bottom section again demonstratesthat aug achieves lower nvt.
values in most do-mains.
however, in the “train” and “taxi” do-mains exist attains lower novelty values, whichmatches the results in table 2, sc-gpt outperform-ing augnlg-sc in these two domains.9.
9detailed breakdowns of novelty scores from 1-grams to4-grams are provided in table 11 in appendix c. the nvt.
re-.
1189metrics.
ppl ↓.
nvt.
(%) ↓.
dataexistaugexistaug.restaurant laptop hotel04.0902.8955.2148.39.
04.1403.4857.3654.50.
22.9208.4671.1150.73.tv19.5305.7772.3444.93.attraction train09.0406.7753.4556.24.
08.2804.7355.3739.83.taxi06.7406.7246.9455.38.table 7: language model perplexity (ppl) and average n-gram novelty (nvt.)
on augmented data..figure 4: the inﬂuence of nlu on four domains in fewshotsgd.
the top row shows nlu f1 scores with50, 100, 200, 500, 1500 training examples.
the bottom row shows the bleu scores of augnlg-ft pre-trainedusing these nlu models.
all experiments are repeated for 5 times with different samples..7.2.inﬂuence of nlu.
few-shot nlu performance since few-shotnlu models are a key component of our system,we report their performance in f1 score.
for eachdomain, we evaluate the few-shot nlu model onthe text-to-mr test set, prepared in section 4.2.the average f1 over all domains on fewshot-woz and fewshotsgd are 0.77 and 0.68, re-spectively.
a further breakdown over the domainsare provided in table 13 and table 14 in ap-pendix d..inﬂuence of nlu quality the mediocre nluperformance on fewshotsgd leads to the fol-lowing research question: can better nlu modelsboost nlg performance?
to answer this question,we select four domains from fewshotsgd withrelatively low nlu performance: “buses (0.63)”,“flights (0.74)”, “movies (0.44)”, and ridesharing(0.63).
in each domain, we construct a new testset by randomly sampling 500 mr-to-text pairsfrom the original test set, and take the rest as thenlu training pool.
to obtain nlu models of vary-ing quality, we train a set of models while varyingthe amount of training data with stratiﬁed sam-pling.
the top row in figure 4 clearly shows thatf1 score increases in proportion to the training size,reaching 0.95 in f1 in all four domains.
we thenannotate the augmented utterances with different.
sults on fewshotsgd are shown in table 12 in appendix c,demonstrating similar trends..fl.
bu.
50do modft7.87aug 14.37ft10.40aug 14.07ft13.30aug 17.13ft12.32aug 17.18.mo.
ri.
10010.3815.3612.9315.5016.1317.5516.9922.06.
20015.2117.0619.9121.5521.9923.6823.2524.76.
50021.8322.1825.9725.3829.7629.1427.9926.87.
150024.9124.9829.1826.6234.0433.5529.0228.60.table 8: bleu scores for ft-gpt (ft) and augnlg-ft (aug) with different training sizes (50, 100, 200,500, 1500).
“bu”, “fl”, “mo” and “ri” are shortfor the domain names “buses”, “flights”, “movies”,“ridesharing”.
all experiments are repeated for 5 timeswith different samples..nlu models and pre-train the nlg models withthe augmented mr-to-text data updated with newmr labels.
finally, we ﬁne-tune the nlg mod-els on the in-domain training set d and performevaluation on the newly constructed 500 test set.
the bottom row in figure 4 conﬁrms that thereis a general proportional relationship between theperformances of nlu and nlg..7.3 varying amounts of augmentation.
lastly, we investigate the relationship between theamount of in-domain ground-truth data and theeffect of augmentation.
as in the previous section,we build new test sets by randomly taking 500examples and vary the size of training set to trainboth nlu and nlg models.
table 8 shows that,in all four domains, the performance difference.
11900.70.80.9nlu f1 scoresgd_buses0.60.70.80.91.0sgd_flights0.60.8sgd_movies0.70.80.9sgd_ridesharing501002005001500# training examples for nlg13141516bleu * 100501002005001500# training examples for nlg15161718501002005001500# training examples for nlg101214501002005001500# training examples for nlg1416between augnlg-ft and ft-gpt culminates atthe smallest training set and gradually diminishesas more training data become available..8 conclusion.
in this paper, we proposed augnlg, a novel dataaugmentation approach that combines a self-trainedretrieval model with a few-shot learned nlu, toautomatically create mr-to-text data from open-domain texts.
experimental results verify the ef-fectiveness of our approach by establishing newsota performances on two benchmark tests.
moreimportantly, we showed how our approach comple-ments the previous sota approach, which hingeson unscalable data sources, with unlimited open-domain data.
future work includes (1) technicalinnovations on each component of our system forfurther performance improvements, (2) exploringself-training on the nlu side too to evolve boththe nlu and nlg model at the same time..acknowledgments.
we would like to thank the ﬁrst author of penget al.
(2020), baolin peng, for his generous help.
we also thank the anonymous reviewers for theirhelpful comments..references.
tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..paweł budzianowski, tsung-hsien wen, bo-hsiangtseng, inigo casanueva, stefan ultes, osman ra-madan, and milica gaˇsi´c.
2018. multiwoz-alarge-scale multi-domain wizard-of-oz dataset forarxiv preprinttask-oriented dialogue modelling.
arxiv:1810.00278..jiaao chen, zichao yang, and diyi yang.
2020. mix-text: linguistically-informed interpolation of hid-den space for semi-supervised text classiﬁcation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2147–2157..zihang dai, zhilin yang, yiming yang, jaime g car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jingfei du, edouard grave, beliz gunel, vishravchaudhary, onur celebi, michael auli, veselinstoyanov, and alexis conneau.
2021. self-trainingimproves pre-training for natural language under-standing.
in proceedings of the 2021 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 5408–5418, online.
association forcomputational linguistics..ondˇrej duˇsek and filip jurˇc´ıˇcek.
2016. sequence-to-sequence generation for spoken dialogue via deepin proceedings of thesyntax trees and strings.
54th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages45–51, berlin, germany.
association for computa-tional linguistics..sergey edunov, alexei baevski, and michael auli.
2019. pre-trained language model representationsfor language generation.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 4052–4059..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atin proceedings of the 2018 conference onscale.
empirical methods in natural language processing,pages 489–500..jianfeng gao, michel galley, and lihong li.
2018.neural approaches to conversational ai.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics: tutorial abstracts,pages 2–7, melbourne, australia.
association forcomputational linguistics..silin gao, yichi zhang, zhijian ou, and zhou yu.
2020. paraphrase augmented task-oriented dialoggeneration.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 639–649..hongyu guo, yongyi mao, and richong zhang.
2019.augmenting data with mixup for sentence clas-arxiv preprintsiﬁcation: an empirical study.
arxiv:1905.08941..matthew henderson, paweł budzianowski,.
i˜nigocasanueva, sam coope, daniela gerz, girish ku-mar, nikola mrkˇsi´c, georgios spithourakis, pei-hao su, ivan vulic, and tsung-hsien wen.
2019..119154th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96..van-khanh tran and minh le nguyen.
2017. naturallanguage generation for spoken dialogue system us-ing rnn encoder-decoder networks.
in proceedingsof the 21st conference on computational naturallanguage learning (conll 2017), pages 442–451..van-khanh tran, minh le nguyen, and satoshi tojo.
2017. neural-based natural language generation indialogue using rnn encoder-decoder with semanticaggregation.
in proceedings of the 18th annual sig-dial meeting on discourse and dialogue, pages 231–240..tsung-hsien wen, milica gaˇsi´c, nikola mrkˇsi´c,lina m. rojas-barahona, pei-hao su, davidvandyke, and steve young.
2016. multi-domainneural network language generation for spoken di-in proceedings of the 2016 con-alogue systems.
ference of the north american chapter of the as-sociation for computational linguistics: humanlanguage technologies, pages 120–129, san diego,california.
association for computational linguis-tics..tsung-hsien wen, milica gaˇsi´c, nikola mrkˇsi´c, pei-hao su, david vandyke, and steve young.
2015.semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages1711–1721, lisbon, portugal.
association for com-putational linguistics..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
advances in neuralinformation processing systems, 33..zijian zhao, su zhu, and kai yu.
2019. data augmen-tation with atomic templates for spoken languageunderstanding.
in proceedings of the 2019 confer-ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3628–3634..in pro-a repository of conversational datasets.
ceedings of the workshop on nlp for conversa-tional ai.
data available at github.com/polyai-ldn/conversational-datasets..zhiting hu, bowen tan, russ r salakhutdinov, tom mmitchell, and eric p xing.
2019. learning data ma-nipulation for augmentation and weighting.
in ad-vances in neural information processing systems,volume 32, pages 15764–15775.
curran associates,inc..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in findings of the associationfor computational linguistics: emnlp 2020, pages4163–4174, online.
association for computationallinguistics..mihir kale and abhinav rastogi.
2020. templateguided text generation for task-oriented dialogue.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 6505–6520, online.
association for computa-tional linguistics..juntao li, lisong qiu, bo tang, dongmin chen,dongyan zhao, and rui yan.
2019. insufﬁcient datalearning to converse using smallercan also rock!
data with augmentation.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 33,pages 6698–6705..andrea madotto, zhaojiang lin, zhenpeng zhou, se-ungwhan moon, paul crook, bing liu, zhou yu, eu-njoon cho, and zhiguang wang.
2020. continuallearning in task-oriented dialogue systems.
arxivpreprint arxiv:2012.15504..baolin peng, chenguang zhu, chunyuan li, xiujunli, jinchao li, michael zeng, and jianfeng gao.
few-shot natural language generation for2020.in findings of the associa-task-oriented dialog.
tion for computational linguistics: emnlp 2020,pages 172–182, online.
association for computa-tional linguistics..jun quan and deyi xiong.
2019. effective data aug-mentation approaches to end-to-end task-oriented di-alogue.
in 2019 international conference on asianlanguage processing (ialp), pages 47–52.
ieee..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..abhinav rastogi, xiaoxue zang, srinivas sunkara,raghav gupta, and pranav khaitan.
2020. schema-guided dialogue state tracking task at dstc8.
arxivpreprint arxiv:2002.01359..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-in proceedings of the.
2016.els with monolingual data..1192a the calculation of tf-idf.
to calculate the tf-idf score for a n-gram phrase, we take all in-domain texts x as one document d tocalculate its tf (term frequency) score, and randomly selected open-domain texts as the set of documentsd to calculate the idf (inverse document frequency) score10.
thus, we formulate the tf-idf score forn-gram phrase phi as:.
tf-idf (phi, d, d) = tf (phi, d) · idf (phi, d) ,.
where,.
tf (phi, d) = log (1 + freq (phi, d))(cid:18).
(cid:19).
idf (phi, d) = log.
|d||{phi ∈ d}|.
,.
in which, freq (phi, d) denotes the raw count of the phrase phi appears in the document d..b the structure of the bert-based nlu annotation.
figure 5: the structure of the bert-based nlu annotation.
the mr for the text “we love food in chicago” is“conﬁrm ( near = chicago )”.
each slot-value token is annotated with the slot-name.
the rest tokens are annotatedwith “o”..c statistics for the augmented data.
domains# ind pairs# keywords (k)# rtv texts (k)# aug pairs (k)# delex mrs (k).
restaurant510.23885.4630.970.78.laptop510.061000.1336.625.84.hotel510.22760.6140.460.91.tv510.28850.0049.766.39.attraction500.271262.6965.480.33.train500.18650.539.600.54.taxi400.17573.934.950.05.table 9: fewshotwoz statistics of the augmented pairs over 7 different domains.
ind is short for in-domain..domains# ind pairs# keywords (k)# rtv texts (k)# aug pairs (k)# delex mrs (k)model# ind pairs# keywords (k)# rtv texts (k)# aug pairs (k)# delex mrs (k).
restaurants500.231021.6461.511.15homes210.07403.918.040.15.hotels500.151068.4320.640.77media movies.
flights500.251195.4139.591.64.
140.04335.423.900.05.
300.08538.685.900.06.calendar250.09582.2256.870.19music210.081033.6329.690.13.banks230.05112.781.270.03.
500.12469.956.410.20.weather110.04387.906.390.04.
480.181180.0227.020.23.buses500.17749.4611.151.31services500.22953.5160.092.00.events500.171305.4156.551.05travel140.04362.4514.800.05.rentalcars ridesharing.
table 10: fewshotsgd statistics of the augmented pairs over 16 domains.
ind is short for in-domain..10here, each open-domain text represents a document..1193welove[cls]foodinchicagotransformer encoderinput text (x)contextualembeddingscomﬁrmoooonearnlu tagstsixe.nvt.
(%) ↓ data restaurant laptop hotel11.36nvt.
uni46.68nvt.
bi74.33nvt.
tri88.46nvt.
four06.13nvt.
uni31.37nvt.
bi66.98nvt.
tri89.10nvt.
four.
12.4648.7077.2191.0711.2039.4573.6393.73.
28.9369.8288.6897.0206.3337.8669.1089.63.gua.tv27.5572.5391.3397.9404.5125.7261.1088.39.attraction08.8446.6675.4890.4904.0921.6053.7279.92.train12.1946.1370.7584.7409.8038.6880.2196.28.taxi09.2235.4062.6980.4607.5639.0079.1095.87.table 11: n-gram novelty (↓) breakdowns in fewshotwoz..gua.sriapdni.nvt.
(%) ↓ data restaurants hotelsflights calendar14.2314.16nvt.
uni59.3253.84nvt.
bi84.0874.84nvt.
tri85.0093.75nvt.
four01.7803.64nvt.
uni19.5421.63nvt.
bi61.2062.72nvt.
tri89.6288.40nvt.
fournvt.
(%) ↓ datamedia movies32.0630.85nvt.
uni73.5373.92nvt.
bi88.9087.20nvt.
tri92.7793.68nvt.
four08.8610.57nvt.
uni47.5360.10nvt.
bi81.1793.48nvt.
tri93.1298.75nvt.
four.
18.4158.0879.8291.6202.9718.9451.7281.62homes30.7377.0890.7895.2208.2237.8575.6492.74.
25.4169.4586.6794.4902.3017.2249.5179.57music35.9177.8491.3596.5503.5932.3873.0393.37.sriapdni.gua.banks16.0649.9172.3684.2508.4537.9168.7387.10.
19.8157.0876.9187.0901.9226.6868.7591.65.weather25.6072.7388.4796.4304.2234.6175.7793.30.
15.7151.6078.2789.7403.6825.4868.6092.50.buses18.1166.1687.0794.1402.1728.0972.0593.15services21.5960.5681.4492.3703.7624.8255.2680.79.events21.5162.2285.0993.1002.3819.8853.8282.53travel42.1183.7693.6897.5906.4738.2973.4590.90.rentalcars ridesharing.
table 12: nvt.
(↓) breakdowns in fewshotsgd.
exist are from the sgd, we compare with in-domain pairs.
d few-shot nlu performancerestaurantmetrics1.000precision1.000recall1.000f1 score.
attraction train.6552.6706.6628.laptop hotel.7500.8229.7500.8490.7500.8357.tv.7904.8382.8136.taxi.6178.7239.6667.
.6050.6904.6449.table 13: nlu evaluation for fewshotwoz (precision↑), (recall↑), (f1 score↑).
metricsprecisionrecallf1 scoremetricsprecisionrecallf1 score.
flights calendarrestaurants hotels.7229.6516.7560.6866.6686.7391media movies.4787.6404.3996.5060.4356.5653.
.6346.6635.6487homes.8201.8166.8183.
.8332.8897.8605music.8011.7889.7950.banks.8971.8684.8825.
.5183.5288.5235.weather.7177.7183.7180.
.6145.6546.6340.buses.6289.6372.6330services.5470.6146.5788.events.5333.5870.5589travel.8132.8713.8412.rentalcars ridesharing.
table 14: nlu evaluation for fewshotsgd (precision↑), (recall↑), (f1 score↑)..e generation examples.
input mrreferencesc-gptaugnlg-sc the parker guest house in mastro’s address is 520 church street.
the phone number is 4156213222..inform(name=parker guest house; area=mastro; address=520 church street; phone=4156213222)parker guest house is near mastro at 520 church street.
their phone number is 4156213222.the parker guest house is located in mastro and is at 520 church street.
the phone number is 4156213222..input mrreferencesc-gptaugnlg-sc tr5413 arrives at 00:04 on sunday.
would you like to book ?.
inform(day=sunday; id=tr5413; arriveby=00:04 ) @ book (none)train tr5413 arrives at 24:44 or 00:04 on sunday.
would you like to book it now ?
tr5413 arrives at 00:04. would you like me to book it for you ?.
domain: hotel.
domain: train.
domain: taxi.
input mrreferencesc-gptaugnlg-sc i ’m on it , the taxi will arrive by 16:00..inform(arriveby=16:00)yes , the taxi is due to arrive at 16:00.yes , the taxi will arrive by 16:00..table 15: generation examples in “hotel”, “train” and “taxi” domain on fewshotwoz.
in the same example,the bleu scores for the sc-gpt generations are higher than the augnlg-sc generations..1194input mrreferenceft-gptaugnlg-ft.offer(balance=2,890.54;account type=checking)your checking balance is 2,890.54 .
now you have 2,890.54 to spend on goods and services .
balance of checking account is 2,890.54.domain: banks.
domain: buses.
input mrreferenceft-gptaugnlg-ft.offer(departure time=10:50 am;price=42;fare type=economy)there is a bus leaving at 10:50 am .
its economy ticket costs 42please conﬁrm that you want to leave from 10:50 am to 10:50 pm .
sure , when do you want the bus to leave at ?
10:50 am , economy ticket or 42 ?.
domain:calendar.
input mrreferenceft-gptaugnlg-ft.offer(available start time=8 am;available end time=7:30 pm ) @ inf count (count)yes , you have 2 empty slots from 8 am to 7:30 pm .
you have 1 available seat .
is your event scheduled to start at 8 am and end at 7:30 pm ?.
input mrreferenceft-gptaugnlg-ft.confirm(number of tickets=1;city=new york)no problem , i ’ll make it 1 ticket for new york .
please conﬁrm your number of tickets .
1 ticket to the event in new york ..domain: events.
domain: flights.
input mrreferenceft-gptaugnlg-ft.confirm(airlines=southwest airlines;return date=march 12th)so you want a southwest airlines ﬂight that returns on march 12th ?
it arrives on march 12th at southwest airlines .
it’s not an upgrade , it’s a southwest airlines ﬂight that leaves on march 12th ..input mrreferenceft-gptaugnlg-ft.inform(phone number=650-961-2975;furnished=false)the phone number is 650-961-2975 .
and the apartment is not furnished .
you are willing to call 650-961-2975 to schedule an appointment .
650-971-2975 is the phone number for the furnished apartment ..domain: homes.
offer(movie name=hackers, high life, sunset)input mri found hackers , high life , and sunset for youreferenceft-gptwhat movie would you like ?
augnlg-ft what movies are you interested in seeing then ?.
domain: movies.
domain: music.
input mrreferenceft-gptaugnlg-ft what is your favorite song from the mile high club ?
( or any song for that matter ).
confirm(song name=mile high;playback device=bedroom speaker)just to make sure , you want to listen to mile high on the bedroom speaker .
you are listening to the song mile high by mile high ..domain: rentalcars.
input mrreferenceft-gptaugnlg-ft will you be picking it up on march 5th and returning it on march 9th ?.
request(pickup date=march 5th;dropoff date=march 9th)would you like to rent it on march 5th and return it on march 9th ?
what date would you like the car to arrive from ?.
input mrreferenceft-gptaugnlg-ft.inform(wait time=2;ride fare=36.01)the cab will arrive in 2 minutes and it’s going to cost 36.01 .
what is the fare for 2 ?
please wait 2 hours and 36.01 will be added to your cart ..domain: ridesharing.
domain: services.
input mrreferenceft-gptaugnlg-ft.inform(is unisex=true;phone number=925-446-4144 ) @ notify success ( none)it is uniesex and you have an appointment set .
phone them at 925-446-4144 .
your appointment has been scheduled .
the salon is unisex and phone is 925-446-4144 .
your appointment is conﬁrmed .
the unisex bathroom is located at 925-446-4144 ..domain: weather.
input mrreferenceft-gptaugnlg-ft.inform(humidity=43)it will be around 43 % .
the humidity will be 43 percent .
the humidity is 43 percent ..input mrreferenceft-gptaugnlg-ft.inform(directed by=roxann dawson ) @ notify success ( none)okay , i started the movie .
the director is roxann dawson .
the movie has started .
it’s directed by roxann dawson and it’s calledn’tify success ..domain: media.
table 16: randomly sampled generation examples from fewshotsgd..1195