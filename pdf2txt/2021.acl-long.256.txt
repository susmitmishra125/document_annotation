evidence-based factual error correction.
james thornedepartment of computer scienceuniversity of cambridgejt719@cam.ac.uk.
andreas vlachosdepartment of computer scienceuniversity of cambridgeav308@cam.ac.uk.
abstract.
this paper introduces the task of factual er-ror correction: performing edits to a claim sothat the generated rewrite is better supportedby evidence.
this extends the well-studiedtask of fact veriﬁcation by providing a mech-anism to correct written texts that are refutedor only partially supported by evidence.
wedemonstrate that it is feasible to train fac-tual error correction systems from existing factchecking datasets which only contain labeledclaims accompanied by evidence, but not thecorrection.
we achieve this by employing atwo-stage distant supervision approach that in-corporates evidence into masked claims whengenerating corrections.
our approach, basedon the t5 transformer and using retrieved ev-idence, achieved better results than existingwork which used a pointer copy network andgold evidence, producing accurate factual er-ror corrections for 5x more instances in humanevaluation and a .125 increase in sari score.
the evaluation is conducted on a dataset of65,000 instances based on a recent fact veri-ﬁcation shared task and we release it to enablefurther work on the task.1.
1.introduction.
fact veriﬁcation is the task of predicting whetherclaims are true or false using evidence.
with theavailability of a number of resources (wang, 2017;karadzhov et al., 2017; thorne et al., 2018; au-genstein et al., 2019; wadden et al., 2020), the taskhas attracted signiﬁcant attention and spawned thedevelopment of new models, architectures and ap-proaches.
with potentially sensitive applications,recent works have focused on building explain-able variants of fact checking (atanasova et al.,2020; stammbach and ash, 2020; kotonya andtoni, 2020).
exposing the evidence source and.
1https://github.com/j6mes/.
2021-acl-factual-error-correction.
figure 1: factual error correction uses evidence tomake corrections to claims, in contrast to fact veriﬁca-tion, which instead classiﬁes the veracity of the claim..decision making process may help the reader un-cover subtle issues that cause automated systemsto fail.
additionally, using such evidence to contin-uously update news articles as facts change formspart of the vision outlined by cohen et al.
(2011)for automated newsrooms..in this paper, we propose factual error correc-tion, as an explainable alternative for fact veriﬁca-tion.
rather than merely assigning a truth label,possibly accompanied by evidence, our goal is torewrite claims so that they are better supported bythe retrieved evidence.
for example, in figure 1,a claim that would be refuted by the evidenceusing a fact veriﬁcation system is rewritten so thatit becomes supported by evidence retrieved fromwikipedia.
this work extends fact guided sentencemodiﬁcation (shah et al., 2020), which uses shortfactoid claims to introduce changes to wikipediapassages.
however, they assume that the claim and.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3298–3309august1–6,2021.©2021associationforcomputationallinguistics3298system outputsbrown recluse spiders do not bitethe brown recluse spider'sbite sometimes requiresmedical attention.input claimsimilar to other recluse spiderbites, their bite sometimesrequires medical attention.retrieved evidencefact veriﬁcationwikipediarefutederror correctioninformation retrievalwikipedia text are always incongruous and requirea meaning-altering change, our proposal makes noassumptions over the veracity, and is applicableto claims both supported and refuted by evidence.
additionally, we incorporate a retrieval componentto select evidence for a given claim from a corpus(in our case, wikipedia) rather than requiring goldstandard evidence to be explicitly provided..a challenge for factual error correction is thelack of datasets consisting of claims paired withtheir corrections.
however, with recent develop-ments in fact checking, there is an abundance ofnew datasets consisting of claims paired with ev-idence.
to address this data scarcity, we makeuse of distant supervision to incorporate retrievedevidence into generating the corrections..we release a dataset of 65,000 claims, containingthe intermediate annotations from fever (thorneet al., 2018).
these consist of factoid sentences thatwere used to construct the supported and refutedclaims in the dataset, and use these as reference tar-gets for automated evaluation.we further verify theﬁndings through a ﬁnal round of annotation usinghuman raters.
our evaluation ﬁnds high correla-tion between manual scores and the sari metric(xu et al., 2016) and our best performing distantly-supervised system generated corrected claims for24% of instances when using retrieved evidence,with a sari final score of .419. a fully-supervisedsystem with gold evidence generated correctionsfor 69% of instances, indicating plenty of opportu-nities for future work to extend our contributions..2 related work.
a number of related works offer methods to makecorrections to sentences.
however, their use of ex-ternal information differs.
this can be placed on acontinuum from only using the knowledge capturedduring language model pre-training, to condition-ing generation based on a context sentence.
webrieﬂy outline key methods and approaches below.
grammatical error correction (gec) (knightand chander, 1994; han et al., 2010; ng et al.,2014) is the task of making meaning-preservingchanges to sentences such that grammatical errorsmade by language learners are removed.
no ex-ternal information is required as the sentence isundergoing a surface-level transformation wherethe (intended) semantic content of the sentenceshould remain unchanged..in contrast, the semantic content of sentences.
undergoing factual error correction will be altered,if needed, to better align the meaning with groundtruth evidence.
shah et al.
(2020) make meaning-altering updates to sentences in wikipedia in atwo step process that does not require referencecorrections in training: salient tokens are maskedand a corrector conditionally replaces the maskswith ground truth evidence.
in this approach, to-ken salience is predicted by querying a model thatis trained to perform fact veriﬁcation for a claimagainst evidence.
cao et al.
(2020) generate correc-tions as a post-editing step for outputs from abstrac-tive summarization so that they are consistent withthe source text.
their approach uses a sequence-to-sequence model trained to restore artiﬁcially gener-ated corruptions of a reference summary..one potential way to introduce knowledge is touse information stored in the parameters of large-scale pre-trained language models (petroni et al.,2019).
the language model can be used recovertokens responsible for causing factual errors thatare masked out as a variant of cloze-style evaluation(taylor, 1953).
while such approaches have beenemployed for fact veriﬁcation (lee et al., 2020),these approaches share the following limitations.
without explicit control (nie et al., 2019), the mostlikely token when decoded may not be factuallyaccurate, or supported by the retrieved evidence,commonly referred to as a hallucination (rohrbachet al., 2018; zhou et al., 2020).
furthermore, evenif the information stored within language modelparameters could be reliably retrieved for factualerror correction, facts change over time and theneed to obtain information from up-to-date sourcesbecomes greater as the state of the world divergesfrom the information captured within the modelparameters.
recent language models augmentedwith a retrieval component such as realm (guuet al., 2020) and rag (lewis et al., 2020) could beapplied, however, task-speciﬁc ﬁne-tuning wouldstill be required to condition the generation basedon the factual error to mitigate hallucination..3 task deﬁnition.
training let a claim c be the input sentence un-dergoing correction to yield c(cid:48).
the correctionrequires incorporating knowledge from retrievedevidence e(c) such that c(cid:48) is supported by this ev-idence, e(c) (cid:15) c(cid:48).
factual error correction issubject to the following 3 requirements:.
3299figure 2: the corrector is trained to reconstruct masked claims, conditioned on retrieved evidence, indicated by thedashed arrow.
at test time, the corrector is able to incorporate new facts from the evidence to generate corrections..r1 - intelligible similar to other language gen-eration tasks, our ﬁrst requirement is that generatedoutputs are ﬂuent and intelligible.
they must befree of grammatical mistakes and the meaning mustbe understandable without the aid of additional con-text or evidence so that their factual correctness canbe assessed..r2 - supported by evidence the generated cor-rection must be supported by the retrieved evidence.
this property follows from previous work (thorneet al., 2018) and also requires models to conditiongeneration on the retrieved evidence – penalizingmodels that hallucinate (holtzman et al., 2020)..r3 - error correction speciﬁc to our task, thecorrections should be targeted to the errors presentin the inputted claim.
while this, in part, can beassessed by r2 we need to compare the correctionto the inputted claim to ensure the output is not in-troducing new unrelated information.
for example,an erroneous claim: france is in south americacould be supported by evidence if it were rewrit-ten as france is a republic.
however, the desiredcorrection should instead state france is in europe..4 task decomposition.
the choice of supervision for the error correctionsystem inﬂuences the task decomposition.
for ex-ample, with full supervision, the system can be.
constructed with an information retrieval moduleand a sequence-to-sequence module that condition-ally generates a correction given the claim and ev-idence.
however, large datasets of claims pairedwith corrections are not available.
the absence offull supervision requires that we distantly-superviseour systems using fact veriﬁcation datasets, whichare an abundant resource.
fact veriﬁcation datasetscontain claims labeled with evidence but do notcontain corrections.
with this resource, we proposea task decomposition that generated corrections bytraining models to reconstruct claims with maskedtokens using retrieved evidence..4.1 distantly-supervised corrections.
test time corrections are generated by a two-stage process, illustrated in figure 2. tokens fromthe claim, c, are ﬁrst masked, yielding ˜c, and theninput to the corrector c(cid:48) = corr(˜c, e(c)).
themasker, ˜c = m ask(c, e(c)), replaces a subsetof tokens in the claim with a blank placeholder,conditioned on e(c).
its purpose is to remove to-kens that are salient to the claim being supported orrefuted by the evidence.
using the masked claim, ˜c,the corrector replaces the blank placeholders withtokens conditionally generated using retrieved evi-dence.
to correct errors, evidence refuting a claim(e(c) (cid:50) c) conditions generation of a correctionsupported by it e(c) (cid:15) c(cid:48).
this extends the pro-.
3300john goodman had thelead role in the babe.john goodman had thelead role in # #.john goodman had thelead role in the babe.claimmasked claimsupervision targetcorrectionwikipage john goodman.
context his other film performances include lead roles in the babe (1992) and the flintstones (1992)maskercorrectorevidencetrainingjohn goodman acted in star warsjohn goodman acted in # #john goodmanacted in the babeclaimmasked claimcorrectionwikipage john goodman context his other film performances include lead roles in the babemaskercorrectorevidencetestingpage star wars context star wars is an american epic space opera media franchise...tocol shah et al.
(2020) by conditioning both themasker and corrector with multiple retrieved evi-dence sentences, rather than a single gold factoid..training the corrector similar to masked lan-guage modeling, the training objective is to gen-erate the input claim c(cid:48) = c conditioned on themasked claim ˜c and evidence e(c).
by trainingthe model to generate the input claim, we expectthe model to generate the input claim only if itwas in complete agreement with the evidence (as-suming the masking and the evidence are correct).
otherwise, the generated correction will contain ev-idence pertinent to the correcting the masked claim,which enables us to generate corrections satisfyingrequirements r2 and r3..masker when applied to factual error correction,masking the tokens from the claim acts as a proxyto which tokens need to be removed to correct anerror.
parallels can be drawn between masking andgenerating token-level explanations.
we brieﬂysummarize common approaches to generating ex-planations in section 5.2..5 model.
5.1 evidence retrieval.
we use genre (cao et al., 2021) and dense pas-sage retrieval (karpukhin et al., 2020) togetherto retrieve evidence for claims e(c).
both haveshown success for a number of language under-standing tasks over wikipedia (petroni et al., 2020).
genre is a pre-trained seq2seq model, trained topredict a wikipedia page name for a claim.
dprencodes ﬁxed length passages from wikipedia intovectors using a bert encoder to build a static in-dex.
at test-time, the claim is encoded and themost-similar passages are returned using an inner-product search.
we return the top-k passages re-turned by dpr from pages predicted by genre..5.2 token-level explanations as masks.
at test time, the purpose of the masker is to selec-tively remove tokens that contribute to the factualerrors within a claim.
we study how the choice ofmasker inﬂuences the quality of corrections.
thisconsiders varying levels of access to model infor-mation and different run-time complexity.
boththe black- and white-box methods, outlined below,require querying a model trained to classify theveracity of claims given evidence whereas the thelanguage model masker and baselines do not..black-box masker we evaluate perturbing theinput to a classiﬁer that is trained to predict theveracity of a claim given evidence.
we use lime(ribeiro et al., 2016), a diagnostic that trains alocally linear model to score the importance ofinput features (in our case, tokens in the claim)with respect to the predicted labels.
the modelunder test is a bert classiﬁer where evidence andthe claim are concatenated in the input.
this isreferred to as black-box because the model doesnot undergo modiﬁcation and no information aboutinternal values or states is exposed..in contrast, to obtain white-white-box maskerbox model explanations, the model has undergonemodiﬁcation to expose internal information.
weuse the neutrality masker from (shah et al., 2020)to predict which tokens, when masked, are likelyto cause a label ﬂip from supports or refuted to notenough information.
this masker exposes encodedinput of an esim classiﬁer (chen et al., 2017), andadds a linear classiﬁer over the hidden states topredict per-token masking probability.
at test time,masks can be generated through a single query tothe model (unlike lime in the black-box maskerwhich requires multiple queries to the model), how-ever this requires an additional step to train, usingpredictions from the classiﬁer as signal..language model masker we evaluate whetherit is possible to generate masks without the needfor a fact veriﬁcation model.
we use a bert pre-trained language model (devlin et al., 2019) tomeasure the surprisal of tokens in the claim.
ourintuition is to identify tokens which introduce mis-information under the hypothesis that the worldknowledge (petroni et al., 2019) captured in re-training would assign lower probabilities to tokenscontradictory to the world state.
this languagemodel has no additional task-speciﬁc ﬁne-tuning.
we independently predict the cross-entropy foreach token under a masked language modellingobjective using bert and return the top-k tokens..baselines we additionally consider two simplebaseline maskers: random masking of a subsetof tokens and also a heuristic method of maskingtokens which are not in common between the claimand the retrieved evidence..5.3 corrections.
we train an encoder-decoder transformer modelto generate corrections from masked claims and.
3301evidence.
our model uses a pre-trained t5 trans-former (raffel et al., 2020) which we ﬁne-tune withthe distant supervision protocol described in sec-tion 4.1. this model jointly encodes the maskedclaim and evidence by concatenating these two in-puts in the input..we also compare against a baseline model froma related task of fact guided sentence modiﬁcation(shah et al., 2020) which uses a pointer genera-tor network (see et al., 2017).
unlike our model,which captures long-range dependencies betweenclaim and evidence through the transformer self-attention (vaswani et al., 2017), the baseline inde-pendently encodes the evidence and masked claimusing lstms (hochreiter and schmidhuber, 1997)before decoding using a pointer-copy mechanism.
in order to evaluate the impact of conditioning onevidence, we decode tokens from masked claims us-ing a language model without ﬁne-tuning or condi-tioning, similar to the language models as knowl-edge bases hypothesis introduced by petroni et al.
(2019).
this would consider correcting claims us-ing the implicit knowledge stored within the modelparameters rather than using external evidence..6 data.
we make use of fever (thorne et al., 2018), acommonly used fact veriﬁcation dataset, as thebasis for our experiments.
fever is one of thelargest resources consisting of claims paired withevidence from wikipedia.
there are 185k instanceswith corresponding evidence sentences and a la-bel as to whether the claim is supported or re-futed by it.
claims where no information couldbe found are labeled as notenoughinfo..to comprehensively evaluate the corrections gen-erated manual evaluation is required.
however, thisis expensive and not suitable for system develop-ment and hyper-parameter optimization.
to auto-mate system evaluation or to train a seq2seq modelwith full supervision, a reference “gold standard”correction is also required.
for this, we releaseannotations from the fever shared task as fol-lows.
the claims in fever were generated in atwo-stage process: annotators extracted facts fromwikipedia and then performed meaning alteringperturbations called mutations over these extractedfacts.
each claim was independently labeled usingretrieved evidence.
our reference corrections arethe unmodiﬁed facts extracted from wikipedia..the class balance and size of the dataset is re-.
ported in table 1. the training and test splits aredisjoint by entity.
the additional hidden sharedtask test set was not used.
the claims labelled asnotenoughinfo.
are used for training fact ver-iﬁcation classiﬁers, but they will not be used fortraining the error correction systems in this paperas there is no labeled evidence to make correctionsfrom.
for completeness, we also release these un-used notenoughinfo instances, as they haveclaims paired unmodiﬁed extracted facts (21934training, 1870 development and 2037 test)..label.
instance count.
train validation.
test.
supportsrefutes.
3796120075.total training.
58036.
14772091.
3568.
15932289.
3891.table 1: instance counts by class and dataset partitions.
7 evaluation.
while it’s convenient to use an automatic metricduring development, these metrics compute tokenoverlap against a single reference sentence and can-not capture the nuances required to assess the ve-racity of the generated corrections against evidence.
thus, our primary evaluation will use human ratersto label whether the model predictions meet thetask requirements stated in section 3..human raters are asked three questions aboutsystem outputs to assess whether the correctionsmeet the requirements of intelligibility, supportedby evidence, and error correction introduced in sec-tion 3. for the ﬁrst 2 requirements, the questionhas a binary answer.
for the third requirement oferror correction, the question has 3 answer choices:(1) the information content w.r.t.
the evidence im-proved, (2) information unrelated to the claim wasadded (i.e.
the claim was ignored), (3) no correc-tion was needed (i.e.
the claim was already sup-ported by evidence).
the raters were shown eachquestion in this sequence without knowledge ofwhich system generated the correction.
negativeanswers to a question automatically assigned nega-tive answers to subsequent ones (prescribing thatan unintelligible sentence could not contain a factsupported by evidence or introduce a correction).
20% of the tasks are assigned to two raters to mea-sure inter-annotator agreement.
we used 4 expertparticipants from our lab (none of them co-authorsof the paper) who were familiar with fact veriﬁca-.
3302tion, but not with error correction.
responses werecalibrated using a pilot study on the validation set.
for automated evaluation, we use sari (xuet al., 2016) which is a metric used for sentence sim-pliﬁcation.
sari considers ngrams retained fromthe source as well added or deleted ngrams throughcomparison against a reference sentence.
we ad-ditionally report bleu (papineni et al., 2002) androuge (lin, 2004) to indicate precision and re-call of the correction.
in section 9, we report cor-relation of automated metrics against our manualevaluation..8.implementation.
t5 masker-corrector we ﬁne-tuned the t5-base pre-trained models released by huggingface(wolf et al., 2020).
the number of training epochsand learning rate was selected through optimizingthe overall sari score.
the search space for learn-ing rate was {10−5, 5 · 10−5, 104, 5 · 10−4}.
weused 5 · 10−5 for all experiments.
we found dimin-ishing returns in sari after 4 epochs and stoppedtraining..fully supervised ceiling we use this model toestimate the ceiling performance of a factual errorcorrection system (assuming a reasonable amountof training data is available) that other methodscan be compared against.
we ﬁne-tune a t5-basemodel with supervision of the correction (see sec-tion 6), using the same hyper-parameter choices asthe t5 masker-corrector..automated scoring a single reference sentencefrom the fever dataset is used for automatedscoring.
we consider bleu, rouge, and sari.
sari considers the f1 of added tokens, f1 of kepttokens, precision of deletions, and the mean ofthese 3 scores (denoted ﬁnal).
we use code madeavailable by xu et al.
(2016)..evidence retrieval we use the facebook imple-mentation of dpr (karpukhin et al., 2020) with-out ﬁne-tuning and constructed an index over thewikipedia version released with fever (thorneet al., 2018), chunked into passages of 50 tokens.
for genre, the original authors’ implementationwas used.
we selected the top matching 2 passages.
this resulted in the highest scores on the down-stream corrections; sari was lower when using 1or 3 passages..maskers for the white-box masker, we use theimplementation provided by shah et al.
(2020)applied to our dataset retaining original hyper-parameters trained on fever.
for the black-boxmasker, we use the lime implementation from(ribeiro et al., 2016) to probe a bert classiﬁer(devlin et al., 2019) ﬁne-tuned on fever.
for thelm and random baseline maskers, where the num-ber of masks was tunable, we masked 50% of thetokens, which was similar to the number of tokensmasked by the black- and white-box maskers..language model as correctors?
we greedilydecode masked tokens using a bert-base-casedlanguage model using the huggingface implemen-tation (wolf et al., 2020) without ﬁne-tuning..comparison to previous work for comparisonto previous work, we use the dual-encoder pointernetwork implementation from (shah et al., 2020),retaining the original hyper-parameter choices..9 results.
we ﬁrst report results from a manual evaluation,assessing the requirements that corrections are in-telligible, supported by evidence, and improve thefactuality of the claim, as listed in section 3. ourevaluation considers a sample of 200 instances persystem.
we report the results in table 2. for inter-annotator agreement control, 20% of instanceswere annotated by two annotators: the cohen’sκ scores for the 3 questions are 0.92 for intelligible,0.92 for supported, and 0.86 for corrected.
whenusing retrieved evidence, the white-box masker gen-erated no masks for 41% of instances.
withoutmasked tokens, the t5 corrector copied the inputclaim to the output.
this ﬁts the assumption that,if the claim is already supported well by evidence,no correction is required..the fully supervised models had the highest rateof satisfactory corrections that improved the fac-tuality of the claim (requirement 3), indicating aperformance ceiling for the distantly-supervisedmodels.
incorporating retrieved evidence in thesesupervised models (rather than gold) reduced thenumber of corrections supported by evidence from88.9% to 64.7% and the number of satisfactorycorrections from 68.9% to 48.9% showing the chal-lenges of incorporating (possibly noisy) retrievedevidence when generating the corrections..when using the masker and corrector distant su-pervision strategy, different maskers could be used.
3303system.
evidence.
trainingmasks.
testmasks.
aggregated score (%).
intelligible.
supported corrected.
t5 fully supervisedt5 fully supervised.
goldretrieved.
--.
--.
heuristict5 masker + corrector retrievedrandomheuristict5 masker + corrector retrieved heuristict5 masker + corrector retrievedblack-boxrandomt5 masker + corrector retrieved black-box black-boxt5 masker + corrector retrieved white-box white-box.
bert language modelbert language modelshah et al.
(2020) m+c.
--gold.
--.
heuristicblack-boxwhite-box white-box.
98.997.7.
89.390.093.191.490.6.
48.030.132.2.
88.964.7.
57.938.042.237.041.7.
20.74.910.7.
68.948.9.
40.020.024.019.823.9.
15.03.45.0.table 2: aggregated scores from human evaluation considering intelligibility, whether generated instances weresupported by evidence and errors corrected..to train the corrector to the masker used at testtime.
we observed that training the corrector withrandom masks yielded both a higher rate of satis-factory corrections and corrections supported byevidence when using either the black-box or heuris-tic masker at test time.
we further evaluate othermaskers with automated metrics in section 9.2..using a heuristic masker at test time, which re-moved tokens from the claim not present in theevidence, generated more claims meeting the sup-ported and corrected requirements than masks gen-erated by querying a fact veriﬁcation model (bothblack-box and white-box).
an analysis of themasker’s inﬂuence on the corrections is providedin section 9.1. the two baseline systems, dualencoder m+c, based on shah et al.
(2020), anda pre-trained bert language model, generatedcorrections that were intelligible or supported byevidence at a lower rate than the aforementionedmodels, further discussed in sections 9.3 and 9.4..we report the correlation between automatedscoring metrics and our manual evaluation in ta-ble 3. the keep component of sari, which mea-sures the f1 of n-grams from the claim retained inthe output, had the highest correlation with all threerequirements.
overly aggressive maskers which re-move too much content from the claim can result inunintelligible outputs, or corrections unrelated tothe claim.
rouge2, which measures the recall ofbigrams in the correction w.r.t.
the reference, exhib-ited reasonable correlation to the manual evaluationagainst the supported and corrected requirements,however does not correlate as well with intelligibil-.
ity.
the add and delete components of sariprovide further information but do not correlateas strongly with the human judgements.
havingonly one reference correction reduces the utilityof precision-oriented metrics, like bleu, as validcorrections can differ from the reference..metric.
sari keepsari finalsari deletesari addrouge2rouge1bleu2bleu1.
correlation (pearson r).
intelligible.
supported corrected.
.87.78.72.52.75.71−.05−.46.
.95.92.82.84.90.87.32−.10.
.93.91.91.79.91.88.45.05.table 3: both sari and rouge automated scoringmetrics have high correlation to manual evaluation..9.1 choice of masker.
when training the corrector with the same maskerthat is used at test time, both the heuristic and black-box maskers yielded comparable scores under hu-man evaluation.
inspection of sari breakdown intable 4 indicates that more tokens were kept whenusing the heuristic masker (keep=.651) whereasthe black box model was more aggressive in mask-ing, resulting in less information from the claim be-ing retained (keep=.594).
this correlated well withhuman judgements as more information retainedgives a richer context for generating the correctionand prevents erasure of claims already (partially)supported by the evidence..3304both the black-box (lime) and white-box (themasker from shah et al.
(2020)) methods requirequerying a veracity classiﬁer to generate the masks.
using retrieved evidence for the veracity classi-ﬁer, which was used to generate the masks in con-junction with these two methods, had a negativeimpact on most components of the sari score.
for the black-box masker, using retrieved evidencereduced the number of masked tokens from an aver-age of 4.7 per claim to 3.9. whereas the number ofmasked tokens by the white-box masker remainedunchanged at 4.7 (approximately 50% of number oftokens in the claim).
most notably, the white-boxmethod of mask generation (row 4 in table 4) didnot to generate masks for 41% of instances whenusing retrieved evidence, whereas all instances hadat least one mask when using gold evidence – anartefact of the noise introduced by retrieval..masker.
sari score.
keep delete add.
final.
black-box (gold)white-box (gold)black-box (ir)white-box (ir)heuristic (ir)masked lmrandom.
.630.652.594.628.651.538.619.
.582.559.526.535.574.509.475.
.088.128.090.107.041.062.087.
.433.447.412.426.422.370.390.table 4: extrinsic evaluation of maskers, varying theuse of evidence when generating the masks, evaluatedusing the t5 masker+corrector model..9.2 corrector trained with random masks.
generating large quantities of masked training datathrough querying a model, such as with the black-box model explanation techniques, can be compu-tationally expensive.
in contrast, random maskscan be generated without querying a model.
us-ing a corrector trained on random masks resultedin higher quality outputs at test time when pairedthe black-box and heuristic maskers.
training withrandom masks promotes good exploration of thetask.
in contrast, while the black-box and heuristicapproaches worked well during testing, correctorstrained on these maskers generated worse outputsdue to the limited exploration of the task space.
ad-ditionally, generating training data using the black-and white-box methods requires making predic-tions using the model’s training data which mayresult in different outcomes to making predictionson unseen test data..masker.
sari score.
keep delete add.
final.
black-box (gold)white-box (gold)black-box (ir)white-box (ir)heuristic (ir)masked lm.
.618.640.611.618.652.561.
.622.570.543.590.627.529.
.102.114.194.144.155.078.
.447.441.419.452.478.389.table 5: using random masks at training resulted inhigher scores when testing with different maskers.
9.3 comparison to previous work.
previous work uses a dual encoder pointer network(shah et al., 2020) to make corrections, reportedin table 6. the corrector tended to copy portionsof claim rather than correct it, resulting in a sarikeep score of .452 which is lower than the t5model using the same white-box masker (table 4).
human evaluation considered these correctionsmostly unintelligible, even when using gold evi-dence (table 2).
this was especially the case forrarer entities.
hyper-parameter tuning of the cor-rector’s coverage ratio, as suggested by the authors,did not yield improvements..system.
sari score.
keep delete add.
final.
dual enc ptr (gold)dual enc ptr (ir).
.452.345.
.569.481.
.039.017.
.353.281.table 6: results using a dual encoder pointer network(shah et al., 2020) were low, despite the strong masker..9.4 language models as correctors?.
with the exception of the heuristic masker, usinga pre-trained language model, without ﬁne-tuning,to correct claims resulted in low sari scores (ta-ble 7).
without conditioning on the evidence, thecorrection is not related to the claim or supportedby evidence to verify the claim, which is indicatedby the low sari add scores which consider theprecision of the added tokens.
as these maskersdeleted most tokens, retaining only stop-words, de-coding most likely tokens without a prompt or con-text tokens resulted in unintelligible outputs.
forthe heuristic masker, more content words were re-tained yielding more intelligible outputs.
however,these were not always supported by evidence, indi-cated in the human evaluation in table 2..3305masker.
sari score.
keep delete add.
final.
masked lmheuristic (ir)white-box (ir)black-box (ir).
.360.629.232.364.
.472.651.446.003.
.019.034.005.001.
.289.438.228.122.table 7: correcting claims using a language modeldoes not condition the generation on evidence..rect tv station.
similarly, where incorrect maskswere made, additional retrieval retrieval may berequired to prevent the corrector from hallucinatinginformation to cover the knowledge missing fromthe evidence.
for example, the name of the tvshow was masked in the claim “two and a halfmen starred jamie fox[sic]”, but as no mention ofjamie fox was present in the evidence, the modelhallucinated a different tv show name..10 qualitative error analysis.
11 conclusions and future work.
in this section we discuss the following issueswhich were present in all master-corrector systems:.
over-erasurein some instances, the masker re-moved most or all of the non-stopword tokens fromthe claim.
this resulted in the original meaning ofthe claim being erased.
without this informationthe corrector could not reconstruct the claim, result-ing in corrections that were unrelated to the inputclaim.
this issue was most prevalent for the black-box masker, where 15% of instances had more than5 consecutive tokens masked and 32% of instanceshad 4 consecutive tokens masked.
in contrast, theheuristic masker, which identiﬁes the tokens notpresent in the retrieved evidence had 5 consecutivetokens masked for 3% of instances and 4 consecu-tive tokens masked for 9% of instances.
while, insome cases, appropriate corrections could be madedespite the aggressive masking (e.g.
the claim “exitthe king is by man[sic].” was fully masked, but cor-rected to include the author’s name), others werere-written focusing on a different fact, e.g.
a claimabout the length of reign of maria theresa wasrewritten to be about her date of birth..incorrect masking when the erroneous tokensin a claim were not masked, the corrector wouldgenerate outputs not supported by evidence.
forexample the following claim, which has an in-correct year, was masked but retaining the error:“ghost, the ﬁlm was released in 1994” as “[mask], [mask] [mask] [mask] [mask] [mask]in 1994”.
even with suitable retrieved evidence,indicating the release year is 1990, no appropriatecorrection could be made..inadequate evidence retrieval where the evi-dence retrieved was related, but not speciﬁcallysupporting or refuting the claim, the generatedcorrections were vague: the claim “poldark airedon hbo” was corrected to “poldark premiered ontv” as the evidence lacked the name of the cor-.
going beyond simply identifying errors, factualerror correction presents a number of challengesfor information retrieval, fact veriﬁcation and ab-stractive summarization communities alike.
in thispaper, we demonstrated that the task can be per-formed with distant supervision in the form ofclaims labeled by evidence supporting or refutingthem.
however, there are a number of outstand-ing challenges that must be addressed.
the datawe used from the fever task was re-purposed toevaluate whether systems can undo mutations intro-duced by human annotators and may not be repre-sentative of the range of factual errors that would bepresent in real-world documents.
while some auto-mated metrics correlated well with human judge-ments, future work should consider how automatedscoring can be better used to discriminate the ade-quacy of the generated corrections going beyondsimilarity to the reference sentence.
from a mod-elling perspective, the masks strongly inﬂuencedthe corrector and further work is required to gen-erate masks that result in better corrections.
weobserved where masks mismatched the evidence,the correction was vague, hallucinated or did notcorrect the factual errors in the claim.
this couldbe addressed through joint training of both com-ponents to enable them to avoid error propagationfrom masking to correction..acknowledgements.
the authors wish to thank: tal schuster for hishelpful comments and feedback; nicola de caofor providing the genre predictions for fever;amrith krishna, guy aglionby, rami aly and zhi-jiang guo for manual evaluation of the model pre-dictions.
this research was supported by donationof compute resources from google cloud.
jamesthorne is supported by an amazon alexa gradu-ate research fellowship.
andreas vlachos is sup-ported by the erc grant averitec (ga 865958)..3306broader impact statement.
our experiments were performed on publicly avail-able data about common facts from wikipedia.
these data are released under a creative-commonslicense.
the expert raters from our lab who manu-ally reviewed the generated instances were volun-teers and were compensated through quid-pro-quohelp on their own projects..the intended use of this project is to help explainreasoning using evidence, going beyond single-label classiﬁcation.
this adds an additional safe-guard, making the decision process more transpar-ent as poor predictions by our model expose limi-tations that would be hidden by classiﬁcation.
ourdata is synthetic in nature and is biased towardssynthetic facts from popular entities.
applicationto political or scientiﬁc domains would require ad-ditional work.
misinformation about populationsthat are under-represented in our data may not beaccurately identiﬁed or corrected without furthermitigation.
one positive ﬁnding in our paper wasthat some of biases perpetuated in the hallucina-tions of language models were mitigated when con-ditioning the generation on retrieved evidence..model ﬁne-tuning took approximately 2 hoursper experiment on a single p100 gpu.
generatinglime explanations of the training dataset took ap-proximately one day – motivating our experimentsthat used models trained on random or heuristicmaskers which required fewer resources by severalorders of magnitude..references.
pepa atanasova, jakob grue simonsen, christina li-oma, and isabelle augenstein.
2020. generatingin proceedings of thefact checking explanations.
58th annual meeting of the association for compu-tational linguistics, pages 7352–7364.
associationfor computational linguistics..isabelle augenstein, christina lioma, dongshengwang, lucas chaves lima, casper hansen, chris-tian hansen, and jakob grue simonsen.
2019.multifc: a real-world multi-domain dataset forevidence-based fact checking of claims.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4685–4697, hongkong, china.
association for computational lin-guistics..meng cao, yue dong, jiapeng wu, and jackie chi kitcheung.
2020. factual error correction for abstrac-.
tive summarization models.
in empirical methodsin natural language processing, pages 6251–6258..nicola de cao, gautier izacard, sebastian riedel, andfabio petroni.
2021. autoregressive entity retrieval.
in international conference on learning represen-tations..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstmin proceedings offor natural language inference.
the 55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1657–1668, vancouver, canada.
associationfor computational linguistics..sarah cohen, chengkai li, jun yang, and cong yu.
2011. computational journalism: a call to armsto database researchers.
proceedings of the 5thbiennial conference on innovative data systemsresearch (cidr 2011) asilomar, california, usa.,(january):148–151..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..kelvin guu, kenton lee, zora tung, panupongpasupat, and ming-wei chang.
2020. realm:retrieval-augmentedpre-training..language model.
na-rae han, joel tetreault, soo-hwa lee, and jin-young ha.
2010. using an error-annotated learnercorpus to develop an esl/efl error correction sys-in proceedings of the seventh internationaltem.
conference on language resources and evaluation(lrec’10), valletta, malta.
european language re-sources association (elra)..sepp hochreiter and jurgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..ari holtzman, jan buys, li du, maxwell forbes, andyejin choi.
2020. the curious case of neural text de-in international conference on learn-generation.
ing representations..georgi karadzhov, preslav nakov, lluís màrquez,alberto barrón-cedeño, and ivan koychev.
2017.fully automated fact checking using externalsources.
in proceedings of the international confer-ence recent advances in natural language process-ing, ranlp 2017, pages 344–353.
incoma ltd..vladimir karpukhin, barlas o˘guz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering..3307kevin knight and ishwar chander.
1994. automatedin proceedings of thepostediting of documents.
national conference on artiﬁcial intelligence, vol-ume 1, pages 779–784..processing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..neema kotonya and francesca toni.
2020. explain-able automated fact-checking for public healthclaims.
in the 2020 conference on empirical meth-ods in natural language processing..nayeon lee, belinda li, sinong wang, wen-tau yih,hao ma, and madian khabsa.
2020. languagein proceedings of themodels as fact checkers?
third workshop on fact extraction and veriﬁcation(fever), volume 2, pages 36–41.
association forcomputational linguistics..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich küttler, mike lewis, wen-tau yih, tim rock-täschel, sebastian riedel, and douwe kiela.
2020.retrieval-augmented generation for knowledge-intensive nlp tasks..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..hwee tou ng, siew mei wu, ted briscoe, christianhadiwinoto, raymond hendy susanto, and christo-pher bryant.
2014. the conll-2014 shared taskon grammatical error correction.
in proceedings ofthe eighteenth conference on computational natu-ral language learning: shared task, july, pages 1–14. association for computational linguistics..feng nie, jin-ge yao, jinpeng wang, rong pan, andchin-yew lin.
2019. a simple recipe towards re-ducing hallucination in neural surface realisation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2673–2679, florence, italy.
association for compu-tational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, july, pages 311–318.
associa-tion for computational linguistics..fabio petroni, aleksandra piktus, angela fan, patricklewis, majid yazdani, nicola de cao, jamesthorne, yacine jernite, vassilis plachouras, timrocktäschel, and sebastian riedel.
2020. kilt:a benchmark for knowledge intensive languagetasks..fabio petroni, tim rocktäschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural language.
colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learningresearch, 21:1–67..marco ribeiro, sameer singh, and carlos guestrin.
2016.
“why should i trust you?”: explaining the pre-dictions of any classiﬁer.
in proceedings of the 2016conference of the north american chapter of theassociation for computational linguistics: demon-strations, volume 39, pages 97–101.
association forcomputational linguistics..anna rohrbach, lisa anne hendricks, kaylee burns,trevor darrell, and kate saenko.
2018. object hal-in proceedings oflucination in image captioning.
the 2018 conference on empirical methods in nat-ural language processing, pages 4035–4045, brus-sels, belgium.
association for computational lin-guistics..abigail see, peter j. liu, and christopher d. man-ning.
2017. get to the point: summarization within proceedings of thepointer-generator networks.
55th annual meeting of the association for compu-tational linguistics (volume 1: long papers), vol-ume 1, pages 1073–1083.
association for computa-tional linguistics..darsh j shah, tal schuster, and regina barzilay.
2020.automatic fact-guided sentence modiﬁcation.
inproceedings of the aaai conference on artiﬁcial in-telligence..dominik stammbach and elliott ash.
2020. e-fever:explanations and summaries for automated factin proceedings of the 2020 truth andchecking.
trust online conference (tto 2020), page 32.hacks hackers..wilson l. taylor.
1953.
“cloze procedure”: a newtool for measuring readability.
journalism quar-terly, 30(4):415–433..james.
andreas vlachos,.
and arpit mittal..christosthorne,2018.christodoulopoulos,fever: a large-scale dataset for fact extractionin proceedings of the 2018and veriﬁcation.
conference ofthe north american chapter ofthe association for computational linguistics:human language technologies, volume 1 (longpapers), pages 809–819, new orleans, louisiana.
association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, lilon jones, aidan gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in 31st conference on neural information.
3308processing systems (nips 2017), long beach, ca,usa..david wadden, shanchuan lin, kyle lo, lucy luwang, madeleine van zuylen, arman cohan, andhannaneh hajishirzi.
2020. fact or fiction: verify-ing scientiﬁc claims..william yang wang.
2017.
“liar, liar pants on ﬁre”:a new benchmark dataset for fake news detection.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume2: short papers), pages 422–426.
association forcomputational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics, 4:401–415..chunting zhou, jiatao gu, mona diab, paco guzman,luke zettlemoyer, and marjan ghazvininejad.
2020.detecting hallucinated content in conditional neu-ral sequence generation.
pages 1–21..3309