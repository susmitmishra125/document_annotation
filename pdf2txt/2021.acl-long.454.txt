lexicon enhanced chinese sequence labeling using bert adapter.
wei liu1, xiyan fu2, yue zhang3, wenming xiao11damo academy, alibaba group, china2college of computer science, nankai university, china3school of engineering, westlake university, china3institute of advanced technology, westlake institute for advanced studyhezan.lw@alibaba-inc.com, fuxiyan@mail.nankai.edu.cn,yue.zhang@wias.org.cn, wenming.xiaowm@alibaba-inc.com.
abstract.
lexicon information and pre-trained models,such as bert, have been combined to explorechinese sequence labeling tasks due to theirrespective strengths.
however, existing meth-ods solely fuse lexicon features via a shal-low and random initialized sequence layer anddo not integrate them into the bottom layersof bert.
in this paper, we propose lexiconenhanced bert (lebert) for chinese se-quence labeling, which integrates external lex-icon knowledge into bert layers directly bya lexicon adapter layer.
compared with ex-isting methods, our model facilitates deep lex-icon knowledge fusion at the lower layers ofbert.
experiments on ten chinese datasets ofthree tasks including named entity recogni-tion, word segmentation, and part-of-speechtagging, show that lebert achieves state-of-the-art results..1.introduction.
sequence labeling is a classic task in natural lan-guage processing (nlp), which is to assign a labelto each unit in a sequence (jurafsky and martin,2009).
many important language processing taskscan be converted into this problem, such as part-of-speech (pos) tagging, named entity recognition(ner), and text chunking.
the current state-of-the-art results for sequence labeling have been achievedby neural network approaches (lample et al., 2016;ma and hovy, 2016; chiu and nichols, 2016; guiet al., 2017)..chinese sequence labeling is more challengingdue to the lack of explicit word boundaries in chi-nese sentences.
one way of performing chinesesequence labeling is to perform chinese word seg-mentation (cws) ﬁrst, before applying word se-quence labeling (sun and uszkoreit, 2012; yanget al., 2016).
however, it can suffer from the seg-mentation errors propagated from the cws system.
(a) model-level fusion.
(b) bert-level fusion.
figure 1: comparison of fusing lexicon features andbert at different levels for chinese sequence labeling.
for simplicity, we only show two transformer layers inbert and truncate the sentence to three characters.
cidenotes the i-th chinese character, wj denotes the j-thchinese word..(zhang and yang, 2018; liu et al., 2019).
there-fore, some approaches (cao et al., 2018; shenet al., 2016) perform chinese sequence labelingdirectly at the character level, which has been em-pirically proven to be more effective (ng and low,2004; liu et al., 2010; zhang and yang, 2018)..there are two lines of recent work enhancingcharacter-based neural chinese sequence labeling.
the ﬁrst considers integrating word informationinto a character-based sequence encoder, so thatword features can be explicitly modeled (zhangand yang, 2018; yang et al., 2019; liu et al., 2019;ding et al., 2019; higashiyama et al., 2019).
thesemethods can be treated as designing different vari-ants to neural architectures for integrating discretestructured knowledge.
the second considers the in-tegration of large-scale pre-trained contextualizedembeddings, such as bert (devlin et al., 2019),which has been shown to capture implicit word-level syntactic and semantic knowledge (goldberg,2019; hewitt and manning, 2019)..the two lines of work are complementary toeach other due to the different nature of discreteand neural representations.
recent work considers.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5847–5858august1–6,2021.©2021associationforcomputationallinguistics5847!"#$%&’"()"!"#$%&’"()"!!"!""!""#!##!#"##*+%,’$-#.)"/$&)")$0)-#.)"123!!"#$%&’"()"!"#$%&’"()"!!"!""!""#!##!#"##*$&)")$+),#-)"./0!
the combination of lexicon features and bert forchinese ner (ma et al., 2020; li et al., 2020), chi-nese word segmentation (gan and zhang, 2020),and chinese pos tagging (tian et al., 2020b).
themain idea is to integrate contextual representationsfrom bert and lexicon features into a neural se-quence labeling model (shown in figure 1 (a)).
however, these approaches do not fully exploit therepresentation power of bert, because the exter-nal features are not integrated into the bottom level.
inspired by the work about bert adapter(houlsby et al., 2019; bapna and firat, 2019; wanget al., 2020), we propose lexicon enhanced bert(lebert) to integrate lexicon information be-tween transformer layers of bert directly.
specif-ically, a chinese sentence is converted into a char-words pair sequence by matching the sentence withan existing lexicon.
a lexicon adapter is designedto dynamically extract the most relevant matchedwords for each character using a char-to-word bi-linear attention mechanism.
the lexicon adapteris applied between adjacent transformers in bert(shown in figure 1 (b)) so that lexicon features andbert representation interact sufﬁciently throughthe multi-layer encoder within bert.
we ﬁne-tuneboth the bert and lexicon adapter during trainingto make full use of word information, which is con-siderably different from the bert adapter (it ﬁxesbert parameters)..we investigate the effectiveness of lebert onthree chinese sequence labeling tasks1, includingchinese ner, chinese word segmentation2, andchinese pos tagging.
experimental results on tenbenchmark datasets illustrate the effectiveness ofour model, where state-of-the-art performance isachieved for each task on all datasets.
in addi-tion, we provide comprehensive comparisons anddetailed analyses, which empirically conﬁrm thatbottom-level feature integration contributes to spanboundary detection and span type determination..2 related work.
our work is related to existing neural methods us-ing lexicon features and pre-trained models to im-prove chinese sequence labeling.
lexicon-based.
lexicon-based models aim to en-hance character-based models with lexicon infor-mation.
zhang and yang (2018) introduced a lat-.
1https://github.com/liuwei1206/lebert2we follow the mainstream methods and regard chinese.
word segmentation as a sequence labeling problem..tice lstm to encode both characters and wordsfor chinese ner.
it is further improved by fol-lowing efforts in terms of training efﬁciency (guiet al., 2019a; ma et al., 2020), model degrada-tion (liu et al., 2019), graph structure (gui et al.,2019b; ding et al., 2019), and removing the depen-dency of the lexicon (zhu and wang, 2019).
lex-icon information has also been shown helpful forchinese word segmentation (cws) and part-of-speech (pos) tagging.
yang et al.
(2019) applieda lattice lstm for cws, showing good perfor-mance.
zhao et al.
(2020) improved the resultsof cws with lexicon-enhanced adaptive attention.
tian et al.
(2020b) enhanced the character-basedchinese pos tagging model with a multi-channelattention of n-grams..pre-trained model-based.
transformer-basedpre-trained models, such as bert (devlin et al.,2019), have shown excellent performance for chi-nese sequence labeling.
yang (2019) simply addeda softmax on bert, achieving state-of-the-art per-formance on cws.
meng et al.
(2019); hu and ver-berne (2020) showed that models using the char-acter features from bert outperform the staticembedding-based approaches by a large margin forchinese ner and chinese pos tagging..hybrid model.
recent work tries to integrate thelexicon and pre-trained models by utilizing theirrespective strengths.
ma et al.
(2020) concatenatedseparate features, bert representation and lexiconinformation, and input them into a shallow fusionlayer (lstm) for chinese ner.
li et al.
(2020)proposed a shallow flat-lattice transformer to han-dle the character-word graph, in which the fusionis still at model-level.
similarly, character n-gramfeatures and bert vectors are concatenated forjoint training cws and pos tagging (tian et al.,2020b).
our method is in line with the above ap-proaches trying to combine lexicon informationand bert.
the difference is that we integrate lexi-con into the bottom level, allowing in-depth knowl-edge interaction within bert..there is also work employing lexicon to guidepre-training.
ernie (sun et al., 2019a,b) ex-ploited entity-level and word-level masking tointegrate knowledge into bert in an implicitway.
jia et al.
(2020) proposed entity enhancedbert, further pre-training bert using a domain-speciﬁc corpus and entity set with a carefully de-signed character-entity transformer.
zen (diaoet al., 2020) enhanced chinese bert with a multi-.
5848layered n-gram encoder but is limited by the smallsize of the n-gram vocabulary.
compared to theabove pre-training methods, our model integrateslexicon information into bert using an adapter,which is more efﬁcient and requires no raw texts orentity set.
bert adapter.
bert adapter (houlsby et al.,2019) aims to learn task-speciﬁc parameters for thedownstream tasks.
speciﬁcally, they add adaptersbetween layers of a pre-trained model and tuneonly the parameters in the added adapters for acertain task.
bapna and firat (2019) injected task-speciﬁc adapter layers into pre-trained models forneural machine translation.
mad-x (pfeiffer et al.,2020) is an adapter-based framework that enableshigh portability and parameter-efﬁcient transfer toarbitrary tasks.
wang et al.
(2020) proposed k-adapter to infuse knowledge into pre-trainedmodels with further pre-training.
similar to them,we use a lexicon adapter to integrate lexicon in-formation into bert.
the main difference is thatour goal is to better fuse lexicon and bert atthe bottom-level rather than efﬁcient training.
toachieve it, we ﬁne-tune the original parametersof bert instead of ﬁxing them, since directly in-jecting lexicon features into bert will affect theperformance due to the difference between that twoinformation..3 method.
the main architecture of the proposed lexicon en-hanced bert is shown in figure 2. compared tobert, lebert has two main differences.
first,lebert takes both character and lexicon featuresas the input given that the chinese sentence is con-verted to a character-words pair sequence.
second,a lexicon adapter is attached between transformerlayers, allowing lexicon knowledge integrated intobert effectively..in this section we describe: 1) char-words pairsequence (section 3.1), which incorporates wordsinto a character sequence naturally; 2) lexiconadapter (section 3.2), by injecting external lexiconfeatures into bert; 3) lexicon enhanced bert(section 3.3), by applying the lexicon adapter tobert..3.1 char-words pair sequence.
a chinese sentence is usually represented as a char-acter sequence, containing character-level featuressolely.
to make use of lexicon information, we.
figure 2: the architecture of lexicon enhanced bert,in which lexicon features are integrated between k-th and (k + 1)-th transformer layer using lexiconadapter.
where ci denote the i-th chinese character inthe sentence, and xwsi denotes matched words assignedto character ci..extend the character sequence to a character-wordspair sequence..given a chinese lexicon d and a chinese sen-tence with n characters sc = {c1, c2, ..., cn}, weﬁnd out all the potential words inside the sentenceby matching the character sequence with d. specif-ically, we ﬁrst build a trie based on the d, thentraverse all the character subsequences of the sen-tence and match them with the trie to obtain allpotential words.
taking the truncated sentence “美国人民 (american people)” for example, we canﬁnd out four different words, namely “美国 (amer-ica)”, “美国人 (american)”, “国人 (compatriot)”,“人民 (people)”.
subsequently, for each matchedword, we assign it to the characters it contains.
asshown in figure 3, the matched word “美国 (amer-ica)” is assigned to the character “美” and “国”since they form that word.
finally, we pair eachcharacter with assigned words and convert a chi-nese sentence into a character-words pair sequence,.
5849!"#$%&’()**)+!!!"!#!!!!,))*,-+./+*0**1234$5%678)/*0%%)"%6-"0**123"!$#"$##$!!"#!$"#!%"#!!!!,))*,-+./+*0**1234$5%678)/*0%%)"%6-"0**123!!$!$"$#%#!$%#"$%##$#!%#"%##%!"#$%&’()*+,"-!"#$%&’()*+,"-!
"#$%&’()*+,"-"#$%&’(#figure 3: character-words pair sequence of a trun-cated chinese sentence “美 国 人 民 (american peo-ple)”.
there are four potential words, namely “美国 (america)”, “美国人 (american)”, “国人 (compa-triot)”, “人民 (people)”.
“<pad>” denotes paddingvalue and each word is assigned to the characters it con-tains..i.e.
scw = {(c1, ws1), (c2, ws2), ..., (cn, wsn)},where ci denotes the i-th character in the sentenceand wsi denotes matched words assigned to ci..3.2 lexicon adapter.
each position in the sentence consists of two typesof information, namely character-level and word-level features.
in line with the existing hybrid mod-els, our goal is to combine the lexicon feature withbert.
speciﬁcally, inspired by the recent worksabout bert adapter (houlsby et al., 2019; wanget al., 2020), we propose a novel lexicon adapter(la) shown in figure 4, which can directly injectlexicon information into bert..i , xws.
i ), where hc.
a lexicon adapter receives two inputs, a char-acter and the paired words.
for the i-th positionin a char-words pair sequence, the input is denotedas (hci is a character vector, theoutput of a certain transformer layer in bert, andxwsi1, xwi = {xwim} is a set of word embed-dings.
the j-th word in xwsis represented as fol-lowing:.
i2, ..., xw.
i.ij = ew(wij)xwwhere ew is a pre-trained word embedding lookuptable and wij is the j-th word in wsi..(1).
to align those two different representations, weapply a non-linear transformation for the word vec-tors:.
ij = w2(tanh(w1xwvw.
ij + b1)) + b2.
(2).
where w1 is a dc-by-dw matrix, w2 is a dc-by-dcmatrix, and b1 and b2 are scaler bias.
dw and dcdenote the dimension of word embedding and thehidden size of bert respectively..as figure 3 shows, each character is paired withmultiple words.
however, the contribution to eachtask varies from word to word.
for example, as.
figure 4: structure of lexicon adapter (la).
theadapter takes as input a character vector and the pairedword features.
subsequently, a bilinear attention overboth character and words is used to weighted the lex-icon feature into a vector, which is then added to theinput character-level vector and followed by a layer nor-malization..for chinese pos tagging, words “美 国 (amer-ica)” and “人民 (people)” are superior to “美国人(american)” and “国人 (compatriot)”, since theyare ground-truth segmentation of the sentence.
topick out the most relevant words from all matchedwords, we introduce a character-to-word attentionmechanism..speciﬁcally, we denote all vw.
ij assigned to i-thcharacter as vi = (vwi1, ..., vwim), which has the sizem-by-dc and m is the total number of the assignedword.
the relevance of each word can be calculatedas:.
ai = softmax(hc.
iwattnvi.
t ).
(3).
where wattn is the weight matrix of bilinear atten-tion.
consequently, we can get the weighted sumof all words by:.
finally, the weighted lexicon information is in-.
jected into the character vector by:.
zwi =.
aijvwij.
m(cid:88).
j=1.
˜hi = hc.
i + zwi.
(4).
(5).
it is followed by a dropout layer and layer normal-ization..3.3 lexicon enhanced bert.
lexicon enhanced bert (lebert) is a combina-tion of lexicon adapter (la) and bert, in which.
5850!!"#$%&!"’(")*+#!"#’(")*+#,-.’/0"12$,%)&!"’(")*+#!"#’(")*+#,"#12(3#%)*2%#."234"!"#’(")*+#,"#12(3#%)*2%#$."234"$1*%*5",#$."234"-.’/0-.’/0!!""!#$#!#$%!!"#$%&#’()*)(#+,")-(.&"#/&0&#’()1..’#.&"#1223%!!
"#$#%&’$()*+&,-$./()*+&la is applied to a certain layer of bert shown infigure 2. concretely, la is attached between cer-tain transformers within bert, thereby injectingexternal lexicon knowledge into bert..given a chinese sentence with n charac-ters sc = {c1, c2, ..., cn}, we build the corre-sponding character-words pair sequence scw ={(c1, ws1), (c2, ws2), ..., (cn, wsn)} as describedin section 3.1. the characters {c1, c2, ..., cn} areﬁrst input into input embedder which outputse = {e1, e2, ..., en} by adding token, segment andposition embedding.
then we input e into trans-former encoders and each transformer layer actsas following:.
g = ln(h l−1 + mhattn(h l−1))h l = ln(g + ffn(g)).
(6).
1, hl.
2, ..., hl.
where h l = {hln} denotes the output ofthe l-th layer and h 0 = e; ln is layer normal-ization; mhattn is the multi-head attention mech-anism; ffn is a two-layer feed-forward networkwith relu as hidden activation function..to inject the lexicon information between thek-th and (k + 1)-th transformer, we ﬁrst get theoutput h k = {hk2, ..., hk1, hkn} after k successivetransformer layers.
then, each pair (hki ) arepassed through the lexicon adapter which trans-forms the ith pair into ˜hki :.
i , xws.
dataset.
ner.
weibo.
ontonotes.
msra.
resume.
pku.
cws.
msr.
ctb6.
ctb5.
ud.
pos.
ctb6.
type train1.4ksent73.8kchar15.7ksent491.9kchar46.4ksent2169.9kchar3.8ksent124.1kchar19.1ksent1826kchar86.9ksent4050kchar23ksent1056kchar18ksent805kchar23ksent1056kchar4ksent156kchar.
dev0.27k14.5k4.3k200.5k--0.46k13.9k----2k100k35012k2k100k50020k.
test0.27k14.8k4.3k208.1k4.4k172.6k0.48k15.1k1.9k173k4.0k184k3k134k34814k3k134k50019k.
table 1: the statistics of the datasets..given n labelled data {sj, yj}|n.
j=1, we train themodel by minimize the sentence-level negative log-likelihood loss as:.
l = −.
log(p(y|s)).
(10).
(cid:88).
j.while decoding, we ﬁnd out the label sequenceobtaining the highest score using the viterbi algo-rithm..˜hki = la(hk.
i , xwsi ).
(7).
4 experiments.
since there are l = 12 transformer layers inthe bert, we input (cid:101)h k = {˜hkn} to theremaining (l − k) transformers.
at the end, weget the output of l-th transformer h l for the se-quence labeling task..2, ..., ˜hk.
1, ˜hk.
3.4 training and decoding.
considering the dependency between successivelabels, we use a crf layer to make sequence la-beling.
given the hidden outputs of the last layerh l = {hln }, we ﬁrst calculate scoresp as:.
2 , ..., hl.
1 , hl.
o = woh l + bofor a label sequence y = {y1, y2, ..., yn}, we de-ﬁne its probability to be:.
(8).
p(y|s) =.
exp((cid:80)˜y exp((cid:80).
(cid:80).
i(oi,yi + tyi−1,yi)).
i(oi,˜yi + t˜yi−1,˜yi)).
(9).
where t is the transition score matrix and ˜y denotesall possible tag sequences..we carry out an extensive set of experiments to in-vestigate the effectiveness of lebert.
in addition,we aim to empirically compare model-level andbert-level fusion in the same setting.
standardf1-score (f1) is used as evaluation metrics..4.1 datasets.
we evaluate our method on ten datasets of three dif-ferent sequence labeling tasks, including chinesener, chinese word segmentation, and chinesepos tagging.
the statistics of the datasets is shownin table 1.chinese ner.
we conduct experiments on fourbenchmark datasets, including weibo ner (pengand dredze, 2015, 2016), ontonotes (weischedelet al., 2011), resume ner (zhang and yang,2018), and msra (levow, 2006).
weibo neris a social media domain dataset, which is drawnfrom sina weibo; while ontonotes and msradatasets are in the news domain.
resume ner.
5851dataset consists of resumes of senior executives,which is annotated by zhang and yang (2018).
chinese word segmentation.
for chinese wordsegmentation, we employ three benchmark datasetsin our experiments, namely pku, msr, and ctb6,where the former two are from sighan 2005bakeoff (emerson, 2005) and the last one is fromxue et al.
(2005).
for msr and pku, we followtheir ofﬁcial training/test data split.
for ctb6, weuse the same split as that stated in yang and xue(2012); higashiyama et al.
(2019).
chinese pos tagging.
for pos-tagging, threechinese benchmark datasets are used, includingctb5 and ctb6 from the penn chinese tree-bank (xue et al., 2005) and the chinese gsdtreebank of universal dependencies(ud) (nivreet al., 2016).
the ctb datasets are in simpliﬁedchinese while the ud dataset is in traditional chi-nese.
following shao et al.
(2017), we ﬁrst convertthe ud dataset into simpliﬁed chinese before thepos-tagging experiments3.
besides, ud has bothuniversal and language-speciﬁc pos tags, we fol-low previous works (shao et al., 2017; tian et al.,2020a), referring to the corpus with two tagsets asud1 and ud2, respectively.
we use the ofﬁcialsplits of train/dev/test in our experiments..4.2 experimental settings.
our model is constructed based on bertbase(devlin et al., 2019), with 12 layers of trans-former, and is initialized using the chinese-bertcheckpoint from huggingface4.
we use the 200-dimension pre-trained word embedding from songet al.
(2018), which is trained on texts of newsand webpages using a directional skip-gram model.
the lexicon d used in this paper is the vocab of thepre-trained word embedding.
we apply the lexi-con adapter between the 1-st and 2-nd transformerin bert and ﬁne-tune both bert and pre-trainedword embedding during training.
hyperparameters.
we use the adam optimizerwith an initial learning rate of 1e-5 for originalparameters of bert, and 1e-4 for other parametersintroduced by lebert, and a maximum epochnumber of 20 for training on all datasets.
the maxlength of the sequence is set to 256, and the trainingbatch size is 20 for msra ner and 4 for otherdatasets.
baselines.
to evaluate the effectiveness of the pro-.
3the conversion tool we used is opencc.
4https://github.com/huggingface/transformers.
modelzhang and yang (2018)*zhu and wang (2019)liu et al.
(2019)*ding et al.
(2019)ma et al.
(2020)* †li et al.
(2020)* †bertbert+worderinezenlebert.
weibo ontonotes msra resume94.5163.3494.9459.3194.4965.3059.50-95.5469.1195.7868.0795.3367.2795.4668.3294.8267.9695.4066.7196.0870.75.
92.8492.9793.5094.4095.3595.4694.7195.3295.0895.2095.70.
75.4973.6475.7975.2081.3480.5679.9381.0377.6579.0382.08.table 2: results on chinese ner..posed lebert, we compare it with the followingapproaches in the experiments..• bert.
directly ﬁne-tuning a pre-trained chi-nese bert on chinese sequence labeling tasks..• bert+word.
a strong model-level fusion base-line method, which inputs the concatenation ofbert vector and bilinear attention weightedword vector, and uses lstm5 and crf as fu-sion layer and inference layer respectively..• ernie (sun et al., 2019a).
an extension ofbert using a entity-level mask to guide pre-training..• zen.
diao et al.
(2020) explicitly integrate n-gram information into bert through an extramulti-layers of n-gram transformer encoder andpre-training..further, we also compare with the state-of-the-.
art models of each task..4.3 overall results.
chinese ner.
table 2 shows the experimental re-sults on chinese ner datasets6.
the ﬁrst fourrows (zhang and yang, 2018; zhu and wang, 2019;liu et al., 2019; ding et al., 2019) in the ﬁrstblock show the performance of lexicon enhancedcharacter-based chinese ner models, and the lasttwo rows (ma et al., 2020; li et al., 2020) in thesame block are the state-of-the-art models usingshallow fusion layer to integrate lexicon informa-tion and bert.
the hybrid models, including ex-isting state-of-the-art models, bert + word, and.
5we also evaluated with other fusion layers, such as trans-.
former, but we found lstm is consistently better..6for a fair comparison, in table 2, we use * denotes train-ing the model with the same pre-trained word embedding asours; † means the model is also initialized using the chinesebert checkpoint from huggingface and evaluated using theseqeval tool..5852modelyang et al.
(2017)ma et al.
(2018)yang et al.
(2019)qiu et al.
(2020)tian et al.
(2020c)(with bert)tian et al.
(2020c)(with zen)bertbert+worderinezenlebert.
pku msr ctb695.4096.8095.0096.7097.4096.1096.1097.8095.8096.9998.0596.4197.1698.2896.5197.2598.4096.5396.9897.9496.2597.2598.4196.5597.0298.1796.3397.1398.3696.3697.5298.6996.91.table 3: results on chinese word segmentation..the proposed lebert, achieve better performancethan both lexicon enhanced models and bert base-line.
this demonstrates the effectiveness of com-bining bert and lexicon features for chinese ner.
compared with model-level fusion models ((maet al., 2020; li et al., 2020), and bert+word), ourbert-level fusion model, lebert, improves in f1score on all four datasets across different domains,which shows that our approach is more efﬁcientin integrating word and bert.
the results alsoindicate that our adapter-based method, lebert,with an extra pre-trained word embedding solely,outperforms those two lexicon-guided pre-trainingmodels (ernie and zen).
this is likely becauseimplicit integration of lexicon in ernie and re-stricted pre-deﬁned n-gram vocabulary size in zenlimited the effect.
chinese word segmentation.
we report the f1score of our model and the baseline methods onchinese word segmentation in table 3. yanget al.
(2019) applied a lattice lstm to integrateword feature to character-based cws model.
qiuet al.
(2020) investigated the beneﬁt of multipleheterogeneous segmentation criteria for single cri-terion chinese word segmentation.
tian et al.
(2020c) designed a wordhood memory networkto incorporate wordhood information into a pre-trained-based cws model and showed good perfor-mance.
compared with those approaches, the mod-els (bert+word and lebert) that combine lexi-con features and bert perform better.
moreover,our proposed lebert outperforms both model-level fusion baseline (bert+word) and lexicon-guided pre-training models (ernie and zen),achieving the best results.
chinese pos tagging.
we report the f1 score onfour benchmarks of chinese pos tagging in table4. the state-of-the-art model (tian et al., 2020a)jointly trains chinese word segmentation and chi-.
modelshao et al.
(2017)zhang et al.
(2018)tian et al.
(2020a)(bert)tian et al.
(2020a)(zen)tian et al.
(2020b)(bert)tian et al.
(2020b)(zen)bertbert+worderinezenlebert.
ctb5 ctb6 ud1 ud289.4294.3894.95-95.4696.7795.4996.8695.3896.6095.4196.8294.7396.2595.4196.7795.1496.5195.0596.6095.7497.14.
89.75-95.5195.5295.5095.5994.8395.3995.1095.1596.06.
-92.5194.8294.8794.7494.8294.6494.7594.7694.7095.18.table 4: results on chinese pos tagging..weiboontonote4msraresumepkumsrctb6ctb5ctb6ud1ud2.
bert10.63%10.71%18.71%16.06%17.60%36.41%17.88%23.73%10.07%23.79%19.17%.
stoa (with bert)5.31%3.97%5.28%7.11%11.46%23.84%12.68%11.46%6.95%12.25%6.17%.
ner.
cws.
pos.
table 5: the relative error reductions over differentbase models..nese pos tagging using a two-way attention toincorporate auto-analyzed knowledge, such as poslabels, syntactic constituents, and dependency re-lations.
similar to bert+word baseline, tianet al.
(2020b) integrated character-ngram featureswith bert at model-level using a multi-channelattention.
as shown in table 4, hybrid models((tian et al., 2020b), bert+word, lebert) thatcombine words information and bert outperformbert baseline, indicating that lexicon featurescan further improve the performance of bert.
lebert achieves the best results among theseapproaches, which demonstrates the effectivenessof bert-level fusion.
consistent with results onchinese ner and cws, our bert adapter-basedapproach is superior to lexicon-guided pre-trainingmethods (ernie and zen)..our proposed model has achieved state-of-the-art results across all datasets.
to better show thestrength of our method, we also summarize therelative error reduction over bert baseline andbert-based state-of-the-art models in table 5. theresults show that the relative error reductions aresigniﬁcant compared with baseline models..5853span f1.
type acc.
ontonotes ud1 ontonotes ud196.9997.5197.72.
97.1697.2497.84.
82.6883.3884.16.
97.9998.0998.47.bertbert+wordlebert.
table 6: span f1 and type acc of different models..4.4 model-level fusion vs. bert-level fusion.
compared with model-levelfusion models,lebert directly integrates lexicon features intobert.
we evaluate those two types of models interms of span f1, type acc, and sentence length,choosing the bert+word as the model-level fu-sion baseline due to its good performance acrossall the datasets.
we also compare with a bertbaseline since both lebert and bert+word areimproved based on it.
span f1 & type acc.
span f1 means the correct-ness of the span for an entity in ner or a wordin pos-tagging, while type acc denotes the pro-portion of full-correct predictions to span-correctpredictions.
table 6 shows the results of three mod-els on the ontonotes and ud1 datasets.
we canﬁnd that both bert+word and lebert performbetter than bert in terms of span f1 and typeacc on the two datasets.
the results indicate thatlexicon information contributes to span boundarydetection and span classiﬁcation.
speciﬁcally, theimprovement of span f1 is larger than type accon ontonotes, but smaller on ud1.
compared withbert+word, lebert achieves more improve-ment, demonstrating the effectiveness of lexiconfeature enhanced via bert-level fusion.
sentence length.
figure 5 shows the f1-valuetrend of the baselines and lebert on ontonotesdataset.
all the models show a similar performance-length curve, decreasing as the sentence lengthincrease.
we speculate that long sentences aremore challenging due to complicated semantics.
even lexicon enhanced models may fail to choosethe correct words because of the increased numberof matched words as the sentence become longer.
the f1-score of bert is relatively low, whilebert+word achieves better performance due tothe usage of lexicon information.
compared withbert+word, lebert performs better and showsmore robustness when sentence length increases,demonstrating the more effective use of lexiconinformation.
case study.
table 8 shows examples of chi-nese ner and chinese pos tagging results on.
figure 5: f1-value against the sentence length..layer.
f1.
f1.
layer.
182.08.
1,381.54.
381.43multi1,3,681.28.one681.24.
1,3,6,981.23.
981.10.
1280.64.allall78.54.table 7: results of variations of lebert with lexi-con adapter applied at different layers of bert model.
one, multi, all mean applying la after one layer, mul-tiply layers, all layers of transformer in bert..ontonotes and ud1 datasets respectively.
in theﬁrst example, bert can not determine the entityboundary, but bert+word and lebert can seg-ment it correctly.
however, the bert+word modelfails to predict the type of the entity “呼伦贝尔盟(hulunbuir league)” while lebert makes thecorrect prediction.
this is likely because fusionat the lower layer contributes to capturing morecomplex semantics provided by bert and lexi-con.
in the second example, the three models canﬁnd the correct span boundary, but both bertand bert+word make incorrect predictions of thespan type.
although bert+word can use the wordinformation, it is disturbed by the irrelevant word“七八 (seven and eight)” predicting it as num.
in contrast, lebert can not only integrate lexi-con features but also choose the correct word forprediction..4.5 discussion.
adaptation at different layers.
we explore theeffect of applying the lexicon adapter (la) be-tween different transformer layers of bert onontonotes dataset.
different settings are evaluated,including applying la after one, multiple, and alllayers of transformer.
as for one layer, we ap-plied la after k ∈ {1, 3, 6, 9, 12} layer; and {1, 3},{1, 3, 6}, {1, 3, 6, 9} layers for multiple layers.
all.
585420<406080100>100sentence length0.780.800.820.840.860.88f1 valuebertbert+wordlebert#1 example of chinese nersentence (truncated) 内蒙古呼伦贝尔盟 (hulunbuir league, inner mongolia).
matched words.
内蒙, 内蒙古, 内蒙古呼伦贝尔, 蒙古, 呼伦, 呼伦贝尔, 呼伦贝尔盟, 贝尔inner mongolia, inner mongolia, inner mongolia hulunbuir, mongolia, hulun,hulunbuir, hulunbuir league, buir呼.
古.charactersgold labelsbertbert+wordlebert#2 example of chinese pos taggingsentence (truncated) 乱七八糟的关系 (messy relationship).
伦e-gpe b-gpei-gpei-gpei-gpei-gpee-gpe b-org i-orgi-gpee-gpe b-gpe.
内b-gpeb-gpeb-gpeb-gpe.
蒙i-gpei-gpei-gpei-gpe.
贝i-gpei-gpei-orgi-gpe.
尔i-gpei-gpei-orgi-gpe.
盟e-gpee-gpee-orge-gpe.
matched words.
charactersgold labelsbertbert+wordlebert.
乱七八糟, 七八, 八糟, 关系mess, seven and eight, bad news, relationship糟七八i-adje-adji-adji-num i-num e-adji-num i-num e-adje-adji-adji-adj.
乱b-adjb-adjb-adjb-adj.
的.
系.
关s-part b-noun e-nouns-part b-noun e-nouns-part b-noun e-nouns-part b-noun e-noun.
table 8: examples of tagging result..layers represents la used after every transformerlayer in bert.
the results show in table 7. theshallow layer achieves better performance, whichcan be due to the fact that the shallow layer pro-motes more layered interaction between lexiconfeatures and bert.
applying la at multi-layers ofbert hurts the performance and one possible rea-son is that integration at multi-layers causes over-ﬁtting.
tuning bert or not.
intuitively, integrating lex-icon into bert without ﬁne-tuning can be faster(houlsby et al., 2019) but with lower performancedue to the different characteristics of lexicon fea-ture and bert (discrete representation vs. neuralrepresentation).
to evaluate its impact, we conductexperiments with and without ﬁne-tuning bert pa-rameters on ontonotes and ud1 datasets.
from theresults, we ﬁnd that without ﬁne-tuning the bert,the f1-score shows a decline of 7.03 points (82.08→ 75.05) on ontonotes and 3.75 points (96.06 →92.31) on ud1, illustrating the importance of ﬁne-tuning bert for our lexicon integration..5 conclusion.
in this paper, we proposed a novel method to in-tegrate lexicon features and bert for chinese se-quence labeling, which directly injects lexicon in-formation between transformer layers in bertusing a lexicon adapter.
compared with model-level fusion methods, lebert allows in-depthfusion of lexicon features and bert representa-tion at bert-level.
extensive experiments showthat the proposed lebert achieves state-of-the-art performance on ten datasets of three chinese.
sequence labeling tasks..acknowledgments.
we would like to thank the anonymous review-ers for their valuable comments and suggestions.
moreover, we sincerely thank dr. zhiyang tengfor his constructive collaboration during the devel-opment of this paper, and dr. haixia chai, dr. jieyang, and my colleague junfeng tian for their helpin polishing our paper.
in addition, we appreciatezifeng cheng for point out the error in figure 3..references.
ankur bapna and orhan firat.
2019. simple, scal-able adaptation for neural machine translation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1538–1548, hong kong, china.
association for computa-tional linguistics..pengfei cao, yubo chen, kang liu, jun zhao, andshengping liu.
2018. adversarial transfer learn-ing for chinese named entity recognition with self-in proceedings of the 2018attention mechanism.
conference on empirical methods in natural lan-guage processing, pages 182–192, brussels, bel-gium.
association for computational linguistics..jason chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
transac-tions of the association for computational linguis-tics, 4(0):357–370..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding..5855of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..shizhe diao, jiaxin bai, yan song, tong zhang, andyonggang wang.
2020. zen: pre-training chinesetext encoder enhanced by n-gram representations.
infindings of the association for computational lin-guistics: emnlp 2020, pages 4729–4740, online.
association for computational linguistics..ruixue ding, pengjun xie, xiaoyan zhang, wei lu,linlin li, and luo si.
2019. a neural multi-digraphin pro-model for chinese ner with gazetteers.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 1462–1467, florence, italy.
association for computationallinguistics..thomas emerson.
2005. the second international chi-nese word segmentation bakeoff.
in proceedings ofthe fourth sighan workshop on chinese languageprocessing..l. gan and y. zhang.
2020. investigating self-attentionnetwork for chinese word segmentation.
ieee/acmtransactions on audio, speech, and language pro-cessing, 28:2933–2941..yoav goldberg.
2019. assessing bert’s syntactic abili-.
ties.
corr, abs/1901.05287..tao gui, ruotian ma, qi zhang, lujun zhao, yu-gangjiang, and xuanjing huang.
2019a.
cnn-based chi-nese ner with lexicon rethinking.
in proceedings ofthe twenty-eighth international joint conference onartiﬁcial intelligence, ijcai-19, pages 4982–4988.
international joint conferences on artiﬁcial intelli-gence organization..tao gui, qi zhang, haoran huang, minlong peng, andxuanjing huang.
2017. part-of-speech tagging forin pro-twitter with adversarial neural networks.
ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 2411–2420, copenhagen, denmark.
association for com-putational linguistics..tao gui, yicheng zou, qi zhang, minlong peng, jin-lan fu, zhongyu wei, and xuanjing huang.
2019b.
a lexicon-based graph neural network for chinesein proceedings of the 2019 conference onner.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages1040–1050, hong kong, china.
association forcomputational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human language.
technologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..shohei higashiyama, masao utiyama, eiichiro sumita,masao ideuchi, yoshiaki oida, yohei sakamoto,incorporating word atten-and isaac okada.
2019.tion into character-based word segmentation.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 2699–2709,minneapolis, minnesota.
association for computa-tional linguistics..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in proceedings of the 36th international conferenceon machine learning..yuting hu and suzan verberne.
2020. named en-tity recognition for chinese biomedical patents.
in proceedings of the 28th international confer-ence on computational linguistics, pages 627–637,barcelona, spain (online).
international committeeon computational linguistics..chen jia, yuefeng shi, qinrong yang, and yue zhang.
2020. entity enhanced bert pre-training for chi-nese ner.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 6384–6396, online.
associa-tion for computational linguistics..daniel jurafsky and james h. martin.
2009. speechand language processing (2nd edition).
prentice-hall, inc., usa..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..gina-anne levow.
2006. the third international chi-nese language processing bakeoff: word segmen-in proceed-tation and named entity recognition.
ings of the fifth sighan workshop on chinese lan-guage processing, pages 108–117..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020. flat: chinese ner using ﬂat-latticein proceedings of the 58th annualtransformer.
meeting of the association for computational lin-guistics, pages 6836–6842, online.
association forcomputational linguistics..wei liu, tongge xu, qinghua xu, jiayu song, andyueran zu.
2019. an encoding strategy based word-in proceed-character lstm for chinese ner.
ings of the 2019 conference of the north american.
5856chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2379–2389, min-neapolis, minnesota.
association for computationallinguistics..zhangxun liu, conghui zhu, and tiejun zhao.
2010.chinese named entity recognition with a sequencelabeling approach: based on characters, or basedon words?
springer berlin heidelberg..ji ma, kuzman ganchev, and david weiss.
2018.state-of-the-art chinese word segmentation with bi-lstms.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 4902–4908, brussels, belgium.
associationfor computational linguistics..ruotian ma, minlong peng, qi zhang, zhongyu wei,and xuanjing huang.
2020. simplify the usage ofin proceedings of thelexicon in chinese ner.
58th annual meeting of the association for compu-tational linguistics, pages 5951–5960, online.
as-sociation for computational linguistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..yuxian meng, wei wu, fei wang, xiaoya li, ping nie,fan yin, muyu li, qinghong han, xiaofei sun, andjiwei li.
2019. glyce: glyph-vectors for chinesein advances in neuralcharacter representations.
information processing systems, volume 32, pages2746–2757.
curran associates, inc..hwee tou ng and jin kiat low.
2004. chinese part-of-speech tagging: one-at-a-time or all-at-once?
word-in proceedings of thebased or character-based?
2004 conference on empirical methods in naturallanguage processing, pages 277–284, barcelona,spain.
association for computational linguistics..joakim nivre, marie-catherine de marneffe, filip gin-ter, yoav goldberg, jan hajiˇc, christopher d. man-ning, ryan mcdonald, slav petrov, sampo pyysalo,natalia silveira, reut tsarfaty, and daniel zeman.
2016. universal dependencies v1: a multilingualtreebank collection.
in proceedings of the tenth in-ternational conference on language resources andevaluation (lrec’16), pages 1659–1666, portoroˇz,slovenia.
european language resources associa-tion (elra)..nanyun peng and mark dredze.
2015. named entityrecognition for chinese social media with jointlyin proceedings of the 2015trained embeddings.
conference on empirical methods in natural lan-guage processing, pages 548–554, lisbon, portugal.
association for computational linguistics..nanyun peng and mark dredze.
2016..improvingnamed entity recognition for chinese social mediawith word segmentation representation learning.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 2:short papers), pages 149–155, berlin, germany.
as-sociation for computational linguistics..jonas pfeiffer, ivan vuli´c, iryna gurevych, and sebas-tian ruder.
2020. mad-x: an adapter-based frame-work for multi-task cross-lingual transfer..xipeng qiu, hengzhi pei, hang yan, and xuanjinghuang.
2020. a concise model for multi-criteriachinese word segmentation with transformer en-coder.
in findings of the association for computa-tional linguistics: emnlp 2020, pages 2887–2897,online.
association for computational linguistics..yan shao, christian hardmeier, j¨org tiedemann, andjoakim nivre.
2017. character-based joint segmen-tation and pos tagging for chinese using bidirec-tional rnn-crf.
in proceedings of the eighth in-ternational joint conference on natural languageprocessing (volume 1: long papers), pages 173–183, taipei, taiwan.
asian federation of naturallanguage processing..mo shen, wingmui li, hyunjeong choe, chenhuichu, daisuke kawahara, and sadao kurohashi.
2016. consistent word segmentation, part-of-speechtagging and dependency labelling annotation forchinese language.
in proceedings of coling 2016,the 26th international conference on computationallinguistics: technical papers, pages 298–308, os-aka, japan.
the coling 2016 organizing commit-tee..yan song, shuming shi, jing li, and haisong zhang.
2018. directional skip-gram: explicitly distinguish-ing left and right context for word embeddings.
inproceedings of the 2018 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 2 (short papers), pages 175–180, neworleans, louisiana.
association for computationallinguistics..weiwei sun and hans uszkoreit.
2012. capturingparadigmatic and syntagmatic lexical relations: to-wards accurate chinese part-of-speech tagging.
inproceedings of the 50th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 242–252, jeju island, korea.
association for computational linguistics..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019a.
ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..yu sun, shuohuan wang, yukun li, shikun feng, haotian, hua wu, and haifeng wang.
2019b.
ernie 2.0:a continual pre-training framework for language un-derstanding.
arxiv preprint arxiv:1907.12412..5857yaqin yang and nianwen xue.
2012. chinese commadisambiguation for discourse analysis.
in proceed-ings of the 50th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 786–794, jeju island, korea.
associa-tion for computational linguistics..m. zhang, n. yu, and g. fu.
2018. a simple andeffective neural model for joint word segmentationieee/acm transactions on au-and pos tagging.
dio, speech, and language processing, 26(9):1528–1538..yue zhang and jie yang.
2018. chinese ner us-in proceedings of the 56th an-ing lattice lstm.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1554–1564, melbourne, australia.
association for compu-tational linguistics..xiaoyan zhao, min yang, qiang qu, and yang sun.
improving neural chinese word segmen-2020.tation with lexicon-enhanced adaptive attention,page 1953–1956.
association for computing ma-chinery, new york, ny, usa..yuying zhu and guoxin wang.
2019. can-ner: con-volutional attention network for chinese namedin proceedings of the 2019entity recognition.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3384–3393, minneapolis, minnesota.
association for computational linguistics..yuanhe tian, yan song, xiang ao, fei xia, xiao-jun quan, tong zhang, and yonggang wang.
2020a.
joint chinese word segmentation and part-of-speechtagging via two-way attentions of auto-analyzedknowledge.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 8286–8296, online.
association for computa-tional linguistics..yuanhe tian, yan song, and fei xia.
2020b.
joint chi-nese word segmentation and part-of-speech taggingvia multi-channel attention of character n-grams.
inproceedings of the 28th international conferenceon computational linguistics, pages 2073–2084,barcelona, spain (online).
international committeeon computational linguistics..yuanhe tian, yan song, fei xia, tong zhang, andyonggang wang.
2020c.
improving chinese wordsegmentation with wordhood memory networks.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8274–8285, online.
association for computational lin-guistics..ruize wang, duyu tang, nan duan, zhongyu wei, xu-anjing huang, jianshu ji, guihong cao, daxin jiang,and ming zhou.
2020. k-adapter: infusing knowl-edge into pre-trained models with adapters..ralph weischedel, sameer pradhan, lance ramshaw,martha palmer, nianwen xue, mitchell marcus,ann taylor, craig greenberg, eduard hovy, robertbelvin, et al.
2011.ontonotes release 4.0.ldc2011t03, philadelphia, penn.
: linguistic dataconsortium..naiwen xue, fei xia, fu-dong chiou, and martapalmer.
2005. the penn chinese treebank: phrasestructure annotation of a large corpus.
natural lan-guage engineering, 11(2):207..haiqin yang.
2019. bert meets chinese word segmen-.
tation.
corr, abs/1909.09292..jie yang, zhiyang teng, meishan zhang, and yuezhang.
2016. combining discrete and neural fea-in international con-tures for sequence labeling.
ference on intelligent text processing and computa-tional linguistics, pages 140–154.
springer..jie yang, yue zhang, and fei dong.
2017. neuralin pro-word segmentation with rich pretraining.
ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 839–849, vancouver, canada.
asso-ciation for computational linguistics..jie yang, yue zhang, and shuailong liang.
2019. sub-word encoding in lattice lstm for chinese wordin proceedings of the 2019 confer-segmentation.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 2720–2725, minneapolis, minnesota.
association for computational linguistics..5858