arbert & marbert: deep bidirectional transformers for arabic.
muhammad abdul-mageed† abdelrahim elmadany† el moatez billah nagoudi†natural language processing labthe university of british columbia{muhammad.mageed,a.elmadany,moatez.nagoudi}@ubc.ca.
abstract.
pre-trained language models (lms) are cur-rently integral to many natural language pro-cessing systems.
although multilingual lmswere also introduced to serve many languages,these have limitations such as being costly atinference time and the size and diversity ofnon-english data involved in their pre-training.
we remedy these issues for a collection ofdiverse arabic varieties by introducing twopowerful deep bidirectional transformer-basedmodels, arbert and marbert.
to eval-uate our models, we also introduce arlue,a new benchmark for multi-dialectal arabiclanguage understanding evaluation.
arlueis built using 42 datasets targeting six differ-ent task clusters, allowing us to offer a se-ries of standardized experiments under richconditions.
when ﬁne-tuned on arlue, ourmodels collectively achieve new state-of-the-art results across the majority of tasks (37 outof 48 classiﬁcation tasks, on the 42 datasets).
our best model acquires the highest arluescore (77.40) across all six task clusters, out-performing all other models including xlm-rlarge (∼ 3.4× larger size).
our models arepublicly available at https://github.com/ubc-nlp/marbert and arlue will be releasedthrough the same repository..1.introduction.
language models (lms) exploiting self-supervisedlearning such as bert (devlin et al., 2019) androberta (liu et al., 2019a) have recently emergedas powerful transfer learning tools that help im-prove a very wide range of natural language pro-cessing (nlp) tasks.
multilingual lms such asmbert (devlin et al., 2019) and xlm-roberta(xlm-r) (conneau et al., 2020) have also beenintroduced, but are usually outperformed by mono-lingual models pre-trained with larger vocabularyand bigger language-speciﬁc datasets (virtanenet al., 2019; antoun et al., 2020; dadas et al., 2020;.
† all authors contributed equally..de vries et al., 2019; le et al., 2020; martin et al.,2020; nguyen and tuan nguyen, 2020)..since lms are costly to pre-train, it is importantto keep in mind the end goals they will serve oncedeveloped.
for example, (i) in addition to their util-ity on ‘standard’ data, it is useful to endow themwith ability to excel on wider real world settingssuch as in social media.
some existing lms do notmeet this need since they were trained on datasetsthat do not sufﬁciently capture the nuances of socialmedia language (e.g., frequent use of abbreviations,emoticons, and hashtags; playful character repeti-tions; neologisms and informal language).
it is alsodesirable to build models able to (ii) serve diversecommunities (e.g., speakers of dialects of a givenlanguage), rather than focusing only on mainstreamvarieties.
in addition, once created, models shouldbe (iii) usable in energy efﬁcient scenarios.
thismeans that, for example, medium-to-large modelswith competitive performance should be preferredto large-to-mega models..a related issue is (iv) how lms are evalu-ated.
progress in nlp hinges on our ability tocarry out meaningful comparisons across tasks,on carefully designed benchmarks.
although sev-eral benchmarks have been introduced to evaluatelms, the majority of these are either exclusivelyin english (e.g., decanlp (mccann et al., 2018),glue (wang et al., 2018), superglue (wanget al., 2019)) or use machine translation in theirtraining splits (e.g., xtreme (hu et al., 2020)).
again, useful as these benchmarks are, this circum-vents our ability to measure progress in real-worldsettings (e.g., training and evaluation on native vs.translated data) for both cross-lingual nlp and inmonolingual, non-english environments..context.
our objective is to showcase a sce-nario where we build lms that meet all four needslisted above.
that is, we describe novel lms that (i)excel across domains, including social media, (ii)can serve diverse communities, and (iii) performwell compared to larger (more energy hungry) mod-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7088–7105august1–6,2021.©2021associationforcomputationallinguistics7088els (iv) on a novel, standardized benchmark.
wechoose arabic as the context for our work sinceit is a widely spoken language (∼ 400m nativespeakers), with a large number of diverse dialectsdiffering among themselves and from the standardvariety, modern standard arabic (msa).
arabicis also covered by the popular mbert (devlinet al., 2019) and xlm-r (conneau et al., 2020),which provides us a setup for meaningful com-parisons.
that is, not only are we able to empir-ically measure monolingual vs. multilingual per-formance under robust conditions using our newbenchmark, arlue, but we can also demonstratehow our base-sized models outperform (or at leastare on par with) larger models (i.e., xlm-rlarge,which is ∼ 3.4× larger than our models).
in thecontext of our work, we also show how the cur-rently best-performing model dedicated to arabic,arabert (antoun et al., 2020), suffers from anumber of issues.
these include (a) not makinguse of easily accessible data across domains and,more seriously, (b) limited ability to handle arabicdialects and (c) narrow evaluation.
we rectify allthese limitations..our contributions.
with our stated goalsin mind, we introduce arbert and mar-bert, two arabic-focused lms exploiting large-to-massive diverse datasets.
for evaluation, wealso introduce a novel arabic natural languageunderstanding evaluation benchmark (arlue).
arlue is composed of 42 different datasets, mak-ing it by far the largest and most diverse arabicnlp benchmark we know of.
we arrange ar-lue into six coherent cluster tasks and methodi-cally evaluate on each independent dataset as wellas each cluster task, ultimately reporting a singlearlue score.
our models establish new state-of-the-art (sota) on the majority of tasks, acrossall cluster tasks.
our goal is for arlue to servethe critical need for measuring progress on arabic,and facilitate evaluation of multilingual and ara-bic lms.
to summarize, we offer the followingcontributions:.
1. we develop arbert and marbert,two novel arabic-speciﬁc transformer lmspre-trained on very large and diverse datasetsto facilitate transfer learning on msa as wellas arabic dialects..2. we introduce arlue, a new benchmark de-veloped by collecting and standardizing splits.
on 42 datasets across six different arabic lan-guage understanding cluster tasks, thereby fa-cilitating measurement of progress on arabicand multilingual nlp..3. we ﬁne-tune our new powerful models onarlue and provide an extensive set of com-parisons to available models.
our modelsachieve new sota on all task clusters in 37out of 48 individual datasets and a sota ar-lue score..the rest of the paper is organized as follows:in section 2, we provide an overview of arabiclms.
section 3 describes our arabic pre-tainedmodels.
we evaluate our models on downstreamtasks in section 4, and present our benchmark ar-lue and evaluation on it in section 5. section 6is an overview of related work.
we conclude insection 7. we now introduce existing arabic lms..2 arabic lms.
the term arabic refers to a collection of languages,language varieties, and dialects.
the standard va-riety of arabic is msa, and there exists a largenumber of dialects that are usually deﬁned at thelevel of the region or country (abdul-mageedet al., 2020a, 2021a,b).
a number of arabic lmshas been developed.
the most notable amongthese is arabert (antoun et al., 2020), whichis trained with the same architecture as bert (de-vlin et al., 2019) and uses the bertbase conﬁg-uration.
arabert is trained on 23gb of ara-bic text, making ∼ 70m sentences and 3b words,from arabic wikipedia, the open source inter-national dataset (osian) (zeroual et al., 2019)(3.5m news articles from 24 arab countries), and1.5b words corpus from el-khair (2016) (5m ar-ticles extracted from 10 news sources).
antounet al.
(2020) evaluate arabert on three arabicdownstream tasks.
these are (1) sentiment anal-ysis from six different datasets: hard (elnagaret al., 2018), astd (nabil et al., 2015), arsentd-lev (baly et al., 2019), labr (aly and atiya,2013), and arsas (elmadany et al., 2018).
(2)ner, with the anercorp (benajiba and rosso,2007), and (3) arabic qa, on arabic-squadand arcd (mozannar et al., 2019) datasets.
an-other arabic lm that was also introduced is ara-bicbert (safaya et al., 2020), which is similarlybased on bert architecture.
arabicbert was pre-trained on two datasets only, arabic wikipedia and.
7089arabic osacar (su´arez et al., 2019).
since bothof these datasets are already included in arabert,and arabic osacar1 has signiﬁcant duplicates,we compare to arabert only.
gigabert (lanet al., 2020), an arabic and english lm designedwith code-switching data in mind, was also intro-duced.2.
3 our models.
3.1 arbert.
3.1.1 training datawe train arbert on 61gb of msa text (6.5btokens) from the following sources:.
• books (hindawi).
we collect and pre-process 1, 800 arabic books from the publicarabic bookstore hindawi.3.
• el-khair.
this is a 5m news articles datasetfrom 10 major news sources covering eightarab countries from el-khair (2016)..• gigaword.
we use arabic gigaword 5thedition from the linguistic data consor-tium (ldc).4 the dataset is a comprehensivearchive of newswire text from multiple arabicnews sources..• oscar.
this is the msa and egyptian ara-bic portion of the open super-large crawledalmanach corpus (su´arez et al., 2019),5a huge multilingual subset from commoncrawl6 obtained using language identiﬁcationand ﬁltering..• osian.
the open source international ara-bic news corpus (osian) (zeroual et al.,2019) consists of 3.5 million articles from 31news sources in 24 arab countries..• wikipedia arabic.
we download and use thedecember 2019 dump of arabic wikipedia.
we use wikiextractor7 to extract articles andremove markup from the dump..1https://oscar-corpus.com.
2since gigabert is very recent, we could not compare toit.
however, we note that our pre-training datasets are muchlarger (i.e., 15.6b tokens for marbert vs. 4.3b arabictokens for gigabert) and more diverse.
3https://www.hindawi.org/books/.
4https://catalog.ldc.upenn.edu/ldc2011t11.
5https://oscar-corpus.com/.
6https://commoncrawl.org.
7https://github.com/attardi/wikiextractor..sourcebooks (hindawi)el-khairgigawordsosianoscar-msaoscar-egyptianwikitotal.
size650mb16gb10gb2.8gb31gb32mb1.4gb61gb.
#tokens72.5m1.6b1.1b292.6m3.4b3.8m156.5m6.5b.
table 1: arbert ’s pre-train resources..we provide relevant size and token count statisticsabout the datasets in table 1..3.1.2 training procedurepre-processing.
to prepare the raw data for pre-training, we perform light pre-processing.
thishelps retain a faithful representation of the natu-rally occurring text.
we only remove diacriticsand replace urls, user mentions, and hashtagsthat may exist in any of the collections with thegeneric string tokens url, user, and hashtag,respectively.
we do not perform any further pre-processing of the data before splitting the text off towordpieces (schuster and nakajima, 2012).
multi-lingual models such as mbert and xlm-r have5k (out of 110k) and 14k (out of 250k) ara-bic wordpieces, respectively, in their vocabularies.
arabert employs a vocabulary of 60k (out of64k).8 for arbert, we use a larger vocabularyof 100k wordpieces.
for tokenization, we use thewordpiece tokenizer (wu et al., 2016) providedby devlin et al.
(2019).
pre-training.
for arbert, we follow devlinet al.
(2019)’s pre-training setup.
to generate eachtraining input sequence, we use the whole wordmasking, where 15% of the n input tokens are se-lected for replacement.
these tokens are replaced80% of the time with the [mask] token, 10% witha random token, and 10% with the original token.
we use the original implementation of bert inthe tensorflow framework.9 as mentioned, weuse the same network architecture as bertbase:12 layers, 768 hidden units, 12 heads, for a totalof ∼ 163m parameters.
we use a batch size of256 sequences and a maximum sequence lengthof 128 tokens (256 sequences × 128 tokens =32, 768 tokens/batch) for 8m steps, which is ap-proximately 42 epochs over the 6.5b tokens.
forall our models, we use a learning rate of 1e−4..8the empty 4k vocabulary bin is reserved for additional.
wordpieces, if needed..9https://github.com/google-research/bert..7090we pre-train the model on one google cloud tpuwith eight cores (v2.8) from tensorflow researchcloud (tfrc).10 training took ∼ 16 days, for42 epochs over all the tokens.
table 2 shows acomparison of arbert with mbert, xlm-r,arabert, and marbert (see section 3.2) interms of data sources and size, vocabulary size,and model parameters..3.2 marbert.
as we pointed out in sections 1 and 2, arabic hasa large number of diverse dialects.
most of thesedialects are under-studied due to rarity of resources.
multilingual models such as mbert and xlm-rare trained on mostly msa data, which is also thecase for arabert and arbert.
as such, thesemodels are not best suited for downstream tasksinvolving dialectal arabic.
to treat this issue, weuse a large twitter dataset to pre-train a new model,marbert, from scratch as we describe next..3.2.1 training data.
to pre-train marbert, we randomly sample 1barabic tweets from a large in-house dataset ofabout 6b tweets.
we only include tweets withat least three arabic words, based on characterstring matching, regardless whether the tweet hasnon-arabic string or not.
that is, we do not re-move non-arabic so long as the tweet meets thethree arabic word criterion.
the dataset makes up128gb of text (15.6b tokens)..3.2.2 training procedure.
pre-processing.
we employ the same pre-processing as arbert.
pre-training.
we use the same network archi-tecture as bertbase, but without the next sen-tence prediction (nsp) objective since tweets areshort.11 we use the same vocabulary size (100kwordpieces) as arbert, and marbert also has∼ 160m parameters.
we train marbert for17m steps (∼ 36 epochs) with a batch size of 256and a maximum sequence length of 128. trainingtook ∼ 40 days on one google cloud tpu (eightcores).
we now present a comparison between ourmodels and popular multilingual models as well asarabert..10https://www.tensorﬂow.org/tfrc.
11it was also shown that nsp is not crucial for model per-.
formance (liu et al., 2019a)..3.3 model comparison.
our models compare to mbert (devlin et al.,2019), xlm-r (conneau et al., 2020) (base andlarge), and arabert (antoun et al., 2020) in termsof training data size, vocabulary size, and overallmodel capacity as we summarize in table 2. interms of the actual arabic variety involved, de-vlin et al.
(2019) train mbert with wikipediaarabic data, which is msa.
xlm-r (conneauet al., 2020) is trained on common crawl data,which likely involves a small amount of arabicdialects.
arabert is trained on msa data only.
arbert is trained on a large collection of msadatasets.
unlike all other models, our mar-bert model is trained on twitter data, which in-volves both msa and diverse dialects.
we nowdescribe our ﬁne-tuning setup..3.4 model fine-tuning.
we evaluate our models by ﬁne-tuning them on awide range of tasks, which we thematically orga-nize into six clusters: (1) sentiment analysis (sa),(2) social meaning (sm) (i.e., age and gender, dan-gerous and hateful speech, emotion, irony, and sar-casm), (3) topic classiﬁcation (tc), (4) dialect iden-tiﬁcation (di), (5) named entity recognition (ner),and (6) question answering (qa).
for all classi-ﬁcation tasks reported in this paper, we compareour models to four other models: mbert, xlm-rbase, xlm-rlarge, and arabert.
we note thatxlm-rlarge is ∼ 3.4× larger than any of our ownmodels (∼ 550m parameters vs. ∼ 160m).
we of-fer two main types of evaluation: on (i) individualtasks, which allows us to compare to other workson each individual dataset (48 classiﬁcation taskson 42 datasets), and (ii) arlue clusters (six taskclusters)..for all reported experiments, we follow the samelight pre-processing we use for pre-training.
for allindividual tasks and arlue task clusters, we ﬁne-tune on the respective training splits for 25 epochs,identifying the best epoch on development data,and reporting on both development and test data.12we typically use the exact data splits provided byoriginal authors of each dataset.
whenever no clear.
12a minority of datasets came with no development splitfrom source, and so we identify and report the best epochonly on test data for these.
this allows us to compare allthe models under the same conditions (25 epochs) and reporta fair comparison to the respective original works.
for allarlue cluster tasks, we identify the best epoch exclusivelyon our development sets (shown in table 10)..7091models.
training data.
conﬁgurationvocabularytokns (ar/all) tok size (ar/all) b / l param..source.
mbertxlm-rbxlm-rlarabertarbertmarbert ara.
tweets.
wiki.
cccc3 sources6 sources.
153m/1.5b wp2.9b/295bsp2.9b/295bsp2.5b/2.5bsp6.2b/6.2b wp15.6b/15.6b wp.
5k/110k14k/250k14k/250k60k/64k100k/100k100k/100k.
bblbbb.
110m270m550m135m163m163m.
table 2: models compared.
b: base, l: large, cc: common crawel, sp: sentencepiece, wp: wordpiece..splits are available, or in cases where expensivecross-validation was used in source, we divide thedata following a standard 80% training, 10% de-velopment, and 10% test split.
for all experiments,whether on individual tasks or arlue task clus-ters, we use the adam optimizer (?)
with inputsequence length of 256, a batch size of 32, and alearning rate of 2e−6.
these values were identiﬁedin initial experiments based on development data ofa few tasks.13 we now introduce individual tasks..4.individual downstream tasks.
4.1 sentiment analysis.
datasets.
we ﬁne-tune the language modelson all publicly available sa datasets we couldﬁnd in addition to those we acquired directlyfrom authors.
in total, we have the follow-ing 17 msa and da datasets: ajgt (alo-mari et al., 2017), aranetsent (abdul-mageedet al., 2020b), arasenti-tweet (al-twaireshet al., 2017), arsarcasmsent (farha and magdy,2020), arsas (elmadany et al., 2018), arsend-lev (baly et al., 2019), astd (nabil et al., 2015),awatif (abdul-mageed and diab, 2012), bbns& syts (salameh et al., 2015), camelsent (obeidet al., 2020), hard (elnagar et al., 2018),labr (aly and atiya, 2013), twitterabdullah (ab-dulla et al., 2013), twittersaad,14 and semeval-2017 (rosenthal et al., 2017).
details about thedatasets and their splits are in section a.1.
baselines.
we compare to the stoa listed in ta-ble 3 and table 4 captions.
for all datasets withno baseline in table 3, we consider arabert ourbaseline.
details about sa baselines are in sec-tion a.2.
results.
to facilitate comparison to previousworks with the appropriate evaluation metrics, we.
13ner and qa are expetions, where we use sequencelengths of 128 and 384, respectively; a batch sizes of 16 forboth; and a learning rate of 2e−6 and 3e−5, respectively.
14www.kaggle.com/mksaad/arabic-sentiment-twitter..dataset (classes).
sota mbert xlm-rb xlm-rl arabert arbert marbert.
87.5067.0057.0084.0060.5089.5055.5067.0079.0022.5060.5081.50.
92.00(cid:63)73.00(cid:63)69.00(cid:63)76.20†--------.
arsas (3)astd (3)semeval (3)aranetsent (2)arsarcsent (3)arasenti (3)bbn (3)syts (3)twsaad (2)samar (5)awatif (4)twabdullah (2)table 3: sa results (i) in f1mageed et al.
(2020b).
default baseline is arabert..91.5067.6767.0093.0070.0093.5072.0076.5095.0057.0068.5092.00.
92.0076.5069.0089.0068.0090.0076.5079.0090.0043.5071.5091.50.
91.0072.0062.0086.5063.5091.0070.0075.5081.0036.5066.5089.50.
90.0060.6764.0092.0063.5092.0069.5078.0095.0054.0063.5091.00.
93.0078.0071.0092.0071.5090.0079.0076.5096.0055.5072.5095.00.pn.
(cid:63) obeid et al.
(2020); † abdul-.
dataset (classes).
sota mbert xlm-rb xlm-rl arabert arbert marbert.
ajgt (2)hard (2)arsentd-lev (5)labr (2)astd-b(2).
93.8096.2059.4086.7092.60.
86.6795.5450.5091.2079.32.
89.4495.7455.2591.2387.59.
91.9495.9662.0092.2077.44.
92.2295.8956.1391.9783.08.
94.4496.1261.3892.5193.23.
96.1196.1760.3892.4996.24.table 4: sa results (ii) in acc.
sota by antoun et al.
(2020)..split our results into two tables: we show resultspn in table 3 and f1 in table 4. we typicallyin f1bold the best result on each dataset.
our modelsachieve best results in 13 out of the 17 classiﬁ-cation tasks reported in the two tables combined,while xlm-r (which is a much larger model)outperforms our models in the 4 remaining tasks.
we also note that xlm-r acquires better resultsthan arabert in the majority of tasks, a trendthat continues for the rest of tasks.
results alsoclearly show that marbert is more powerfulthan than arbert.
this is due to marbert’slarger and more diverse pre-training data, espe-cially that many of the sa datasets involve dialectsand come from social media..4.2 social meaning tasks.
we collectively refer to a host of tasks as socialmeaning.
these are age and gender detection; dan-gerous, hateful, and offensive speech detection;emotion detection; irony detection; and sarcasmdetection.
we now describe datasets we use foreach of these tasks.
datasets..for both age and gender, we use.
7092task (classes).
sota mbert xlm-rb xlm-rl arabert arbert marbert.
dataset (classes).
mbert xlm-rb xlm-rl arabert arbert marbert.
age (3)dangerous (2)emotion (8)gender (2)hate (2)irony (2)offensive (2)sarcasm (2).
51.42 ‡‡59.60 †60.32 ‡‡65.30 ‡‡82.28∗∗82.40 ‡90.51∗46.60 ‡‡.
56.3562.6665.7968.0672.8180.9684.2568.20.
59.7362.7670.6771.0071.3381.9785.2666.76.
53.6065.0174.8971.1479.3182.5288.2869.23.
57.7264.3765.6867.7578.8983.0186.5772.23.
58.95.
63.2167.7369.8683.0285.5990.3875.04.
62.27.
67.5375.8372.62.
84.7985.33.
92.4176.30.table 5: results on social meaning tasks.
f1 score is theevaluation metric.
(cid:63) hassan et al.
(2020), (cid:63)(cid:63) djandji et al.
(2020), ‡ zhang and abdul-mageed (2019a), † ?, †† farhaand magdy (2020), ‡‡ abdul-mageed et al.
(2020b)..arap-tweet (zaghouani and charﬁ, 2018).
weuse aradan (alshehri et al., 2020) for dangerousspeech.
for offensive language and hate speech,we use the dataset released in the shared task (sub-tasks a and b) of offensive speech by mubaraket al.
(2020).
we also use aranetemo (abdul-mageed et al., 2020b), idat@fire2019 (ghanemet al., 2019), and arsarcasm (farha and magdy,2020) for emotion, irony and sarcasm, respectively.
more information about these datasets and theirsplits is in appendix b.1.
baselines.
baselines for social meaning tasks arethe sota listed in table 5 caption.
details abouteach baseline is in appendix b.2.
results.
as table 5 shows, our models acquirebest results on all eight tasks.
of these, mar-bert achieves best performance on seven tasks,while arbert is marginally better than mar-bert on one task (irony@fire2019).
the size-able gains marbert achieves reﬂects its su-periority on social media tasks.
on average,our models are 9.83 f1 better than all previoussota..4.3 topic classiﬁcation.
classifying documents by topic is a classical taskthat still has practical utility.
we use four tcdatasets, as follows:datasets.
we ﬁne-tune on arabic news text(ant) (chouigui et al., 2017) under three pre-taining settings (title only, text only, and title+text.
),khaleej (abbas et al., 2011), and osac (saad andashour, 2010).
details about these datasets and theclasses therein are in appendix c.1.
baselines.
since, to the best of our knowledge,there are no published results exploiting deep learn-ing on tc, we consider arabert a strong baseline.
results.
as table 6 shows, arbert acquiresbest results on both osac and khaleej, and thetitle-only setting of ant.
arabert slightly out-performs our models on the text-only and title+text.
anttext (5)anttitle (5)anttext+title (5)khallej (4)osac (10).
86.7281.2586.9693.5698.20table 6: performance on tc tasks.
our baseline is arabert..86.8781.7087.2194.5397.50.
88.1781.0387.2293.8397.03.
85.7779.9686.2191.8797.15.
85.2781.1985.6093.6397.23.
84.8978.2984.6792.8196.84.dataset (classes) task.
sota mbert xlm-rb xlm-rl arabert arbert marbert.
arsarcdia(5)madar (21)aoc (4)aoc (3)aoc (2)qadi (18)nadi (21)nadi (100).
regoincountryregionregionbinarycountrycountryprovince.
--82.45(cid:63)78.81(cid:63)87.23(cid:63)60.60†26.78‡06.06††.
43.8134.9277.2785.7686.1966.5713.3202.13.
41.7135.9177.3486.3986.8577.0016.3604.12.
41.8335.1478.7787.5687.3082.7317.175.30.
47.5434.8779.2087.6887.7672.2317.4603.13.
54.7037.9081.0989.0688.4688.6322.5606.10.
51.2740.4082.3790.8588.5990.8929.1406.28.table 7: dia results in f1.
(cid:63) elaraby and abdul-mageed(2018), † abdelali et al.
(2020), ‡ el mekki et al.
(2020), ††talafha et al.
(2020).
default baseline is arabert..settings of ant..4.4 dialect identiﬁcation.
(farha.
arabic dialect identiﬁcation can be performed atdifferent levels of granularity, including binary (i.e.,msa-da), regional (e.g., gulf, levantine), coun-try level (e.g., algeria, morocco), and recentlyprovince level (e.g., the egyptian province of cairo,the saudi province of al-madinah) (abdul-mageedet al., 2020a, 2021b).
datasets.
we ﬁne-tune our models on thefollowing datasets: arabic online commentary(zaidan and callison-burch, 2014),(aoc)and magdy, 2020),15arsarcasmdiamadar (sub-task 2) (bouamor et al., 2019),nadi-2020 (abdul-mageed et al., 2020a), andqadi (abdelali et al., 2020).
details about thesedatasets are in table d.1.
baselines.
our baselines are marked in table 7 cap-tion.
details about the baselines are in table d.2.
results.
as table 7 shows, our models outper-form all sota as well as the baseline arabertacross all classiﬁcation levels with sizeable mar-gins.
these results reﬂect the powerful and di-verse dialectal representation of marbert, en-abling it to serve wider communities.
althougharbert is developed mainly for msa, it alsooutperforms all other models..4.5 named entity recognition.
we ﬁne-tune the models on ﬁve ner datasets.
datasets.
we use ace03nw and ace03bn(mitchell et al., 2004), ace04nw (mitchell et al.,2004), anercorp (benajiba and rosso, 2007), andtw-ner (darwish, 2013).
table e.1 shows the.
15arsarcasmdia carries regional dialect labels..7093dataset.
sota mbert xlm-rb xlm-rl arabert arbert marbert.
88.77.
86.7886.3791.2381.4036.83.anercorpace04nw 91.4794.92ace03bnace03nw 91.2065.34tw-nertable 8: ner results in f1.
sota by khalifa and shaalan(2019)..89.9489.8985.4190.6254.44.
89.1389.0391.9488.0941.26.
87.2489.9353.9787.2449.16.
80.6485.0279.0587.76.
96.1890.0959.17.
84.3888.24.
66.67.distribution of named entity classes across the ﬁvedatasets.
baseline.
we compare our results with sota pre-sented by khalifa and shaalan (2019) and followthem in focusing on person (per), location (loc)and organization (org) named entity labels whilesetting other labels to the unnamed entity (o).
de-tails about khalifa and shaalan (2019) sota mod-els are in appendix e.2.
results.
as table 8 shows, our models outperformsota on two out of the ﬁve ner datasets.
wenote that even though sota (khalifa and shaalan,2019) employ a complex combination of cnns andcharacter-level lstms, which may explain theirbetter results on two datasets, marbert stillachieves highest performance on the social me-dia dataset (tw-ner)..4.6 question answering.
datasets.
we use arcd (mozannar et al., 2019)and the three human translated arabic test sec-tions of the xtreme benchmark (hu et al., 2020):mlqa (lewis et al., 2020), xquad (artetxe et al.,2020), and tydi qa (artetxe et al., 2020).
detailsabout these datasets are in table f.1.
baselines.
we compare to antoun et al.
(2020)and consider their system a baseline on arcd.
wefollow the same splits they used where we ﬁne-tuneon arabic squad (mozannar et al., 2019) and50% of arcd and test on the remaining 50% ofarcd (arcd-test).
for all other experiments, weﬁne-tune on the arabic machine translated squad(ar-xtreme) from the xtreme multilingualbenchmark (hu et al., 2020) and test on the humantranslated test sets listed above.
our baselines inthese is hu et al.
(2020)’s mbertbase model ongold (human) data.
results.
as is standard, we report qa results interms of both exact match (em) and f1.
we ﬁndthat results with arbert and marbert on qaare not competitive, a clear discrepancy from whatwe have observed thus far on other tasks.
wehypothesize this is because the two models arepre-trained with a sequence length of only 128,which does not allow them to sufﬁciently capture.
both a question and its likely answer within thesame sequence window during the pre-training.16to rectify this, we further pre-train the strongermodel, marbert, on the same msa data as ar-bert in addition to aranews dataset (nagoudiet al., 2020) (8.6gb), but with a bigger sequencelength of 512 tokens for 40 epochs.
we call thisfurther pre-trained model marbert-v2, notingit has 29b tokens.
as table 9 shows, marbert-v2 acquires best performance on all but one testset, where xlm-rlarge marginally outperforms us(only in f1)..5 arlue.
5.1 arlue categories.
we concatenate the corresponding splits of the in-dividual datasets to form arlue, which is a con-glomerate of task clusters.
that is, we concatenateall training data from each group of tasks into asingle train, all development into a single dev,and all test into a single test.
one exception isthe social meaning tasks whose data we keep inde-pendent (see arluesm below).
table 10 showsa summary of the arlue datasets.17 we nowbrieﬂy describe how we merge individual datasetsinto arlue.
arluesenti.
to construct arluesenti, we col-lapse the labels very negative into negative, verypositive into positive, and objective into neu-tral, and remove the mixed class.
this gives usthe 3 classes negative, positive, and neutral forarluesenti.
details are in table a.1.
arluesm.
we refer to the different social mean-ing datasets collectively as arluesm.
we do notmerge these datasets to preserve the conceptual co-herence speciﬁc to each of the tasks.
details aboutindividual datasets in arluesm are in b.1.
arluetopic.
we straightforwardly merge the tcdatasets to form arluetopic, without modifyingany class labels.
details of arluetopic data are intable c.1.
arluedia.
we construct three arluedia cat-egories.
namely, we concatenate the aocand arasarcasmdia msa-da classes to formarluedia-b (binary) and the region level classesfrom the same two datasets to acquire arluedia-r(4-classes, region).
we then merge the country.
16in addition, marbert is not trained on wikipedia data.
from where some questions come..17again, arluesm datasets are kept independent, but toprovide a summary of all arlue datasets we collate thenumbers in table 10..7094em.
sota.
dataset.
em30.10†arcd-test(cid:63)-arcd-test39.00‡ar-mlqaar-xquad 54.20‡ar-tyidqa 39.00‡table 9: qa results.
(cid:63) results on this test set are with models using the same training data as antoun et al.
(2020), while rest ofrows report models trained with ar-xtreme (hu et al., 2020).
† antoun et al.
(2020); ‡ hu et al.
(2020)..f154.06 36.7555.14 29.6345.14 39.2358.46 56.5557.51 47.45.f161.20†-58.90 ‡71.00 ‡58.90‡.
f168.8663.0559.3972.4867.67.xlm-rlf130.2064.7732.0528.1125.6462.0838.11 60.00 35.4351.6072.1951.8544.1967.0644.41.arabertf1em62.3059.9255.4268.7964.39.xlm-rbf1em59.5559.6153.3564.9160.99.arbertf1em63.8960.7353.6567.9066.94.mbertf160.9358.8651.5766.2664.02.marbert marbert(v2)em.
29.6326.6432.9348.6646.36.
21.6523.2228.0241.0938.98.
30.2027.3132.9345.8839.41.
30.3427.2134.1549.9246.80.em.
em.
178522354.testdev#datasets task train190.9k44.2k6.5ksa1.51m 162.5k 166.1ksm5.9k5.9k47.5ktc12.9k10.8k94.9kdi5.3k4.5k38.5kdi52.1k31.5k711.9kdi66.5k44.1kner 227.7k7.45k517101.6kqa.
datasetarluesenti(cid:63)arluesmarluetopicarluedia-barluedia-rarluedia-carluener†‡arlueqatable 10: arlue categories across the different data splits.
(cid:63) refer to table b.1 for details about individual social mean-ing datasets in arluesm.
† statistics are at the token level.
‡ number of question-answer pairs..classes from the rest of datasets to get arluedia-c(21-classes, country).
details are in table d.1.
arluener & arlueqa.
we straightforwardlyconcatenate all corresponding splits from the dif-ferent ner and qa datasets to form arluenerand arlueqa, respectively.
details of each ofthese task clusters data are in tables e.1 and f.1,respectively..5.2 evaluation on arlue.
we present results on each task cluster indepen-dently using the relevant metric for both the devel-opment split (table 11) and test split (table 12).
inspired by mccann et al.
(2018) and wang et al.
(2018) who score nlp systems based on their per-formance on multiple datasets, we introduce anarlue score.
the arlue score is simply themacro-average of the different scores across alltask clusters, weighting each task equally.
fol-lowing wang et al.
(2018), for tasks with multi-ple metrics (e.g., accuracy and f1), we use an un-weighted average of the metrics as the score for thetask when computing the overall macro-average.
as table 12 shows, our marbert-v2 modelachieves the highest arlue score (77.40), fol-lowed by xlm-rl (76.55) and arbert (76.07).
we also note that in spite of its superiority on socialdata, marbert ranks top 4. this is due to mar-bert suffering on the qa tasks (due to its shortinput sequence length), and to a lesser extent onner and tc..6 related work.
english and multilingual lms.
pre-trained lmsexploiting a self-supervised objective with mask-ing such as bert (devlin et al., 2019) androberta (liu et al., 2019b) have revolutionizednlp.
multilingual versions of these models suchas mbert and xlm-roberta (conneau et al.,2020) were also pre-trained.
other models withdifferent objectives and/or architectures such asalbert (lan et al., 2019), t5 (raffel et al., 2020)and its multilingual version, mt5 (xue et al., 2021),and gpt3 (brown et al., 2020) were also intro-duced.
more information about bert-inspiredlms can be found in rogers et al.
(2020).
non-english lms.
several models dedicatedto individual languages other than english havebeen developed.
these include arabert (an-toun et al., 2020) and arabicbert (safaya et al.,2020) for arabic, bertje for dutch (de vries et al.,2019), camembert (martin et al., 2020) andflaubert (le et al., 2020) for french, phobertfor vietnamese (nguyen and tuan nguyen, 2020),and the models presented by virtanen et al.
(2019) for finnish, dadas et al.
(2020) for polish,and malmsten et al.
(2020) for swedish.
pyysaloet al.
(2020) also create monolingual lms for 42languages exploiting wikipedia data.
our modelscontributed to this growing work of dedicated lms,and has the advantage of covering a wide rangeof dialects.
our marbert and marbert-v2models are also trained with a massive scale socialmedia dataset, endowing them with a remarkableability for real-world downstream tasks.
nlp benchmarks.
in recent years, several nlpbenchmarks were designed for comparative eval-uation of pre-trained lms.
for english, mccannet al.
(2018) introduced nlp decathlon (decanlp)which combines 10 common nlp datasets/tasks.
wang et al.
(2018) proposed glue, a popularbenchmark for evaluating nine nlp tasks.
wanget al.
(2019) also presented superglue, a morechallenging benchmark than glue covering seventasks.
in the cross-lingual setting, hu et al.
(2020).
7095dataset.
mbert.
xlm-rb.
xlm-rl.
arabert.
arbert.
marbert marbert (v2).
(cid:63).
arluesenti†arluesmarluetopicarluedia-barluedia-rarluedia-carluener‡arlueqaaverage.
79.02 / 79.5066.84 / 61.7691.10 / 91.6787.83 / 87.5086.45 / 85.8941.08 / 32.0396.81 / 76.9132.30 / 51.1472.68 / 70.80.
92.17 / 93.00 93.18 / 94.00 78.26 / 78.5069.18 / 64.1267.63 / 62.1168.79 / 64.2091.57 / 92.24 92.66 / 93.53 92.42 / 93.1789.30 / 89.0688.92 / 88.5788.20 / 87.9387.30 / 86.9286.97 / 86.5486.00 / 85.4640.59 / 31.7537.90 / 30.4139.73 / 31.5197.74 / 84.09 97.97 / 85.56 97.79 / 83.6731.72 / 51.8735.18 / 58.0832.30 / 52.4375.75 / 71.9675.43 / 75.7974.72 / 73.88.
87.96 / 88.50 93.30 / 94.0069.12 / 64.23 71.64 / 68.3890.48 / 92.0191.06 / 92.2389.53 / 89.2389.80 / 89.5088.85 / 88.49 90.94 / 90.6543.54 / 34.2542.51 / 34.2696.89 / 76.5897.46 / 81.2127.27 / 43.6734.04 / 54.3475.48 / 73.6375.07 / 74.06.
92.82 / 93.5070.43 / 66.2691.52 / 92.5090.05 / 89.7290.04 / 89.6745.37 / 35.9497.18 / 79.3437.14 / 57.9376.82 / 75.61.arluescore.
71.74.
74.30.
75.34.
72.38.
74.56.
74.56.
76.21.table 11: performance of our models on the dev splits of arlue.
(cid:63) metric for arluesenti is f1the average score across the social meaning tasks described in table b.2.
‡ metric for arlueqa is exact match (em) / f1..pn.
† arluesm results is.
dataset.
mbert.
xlm-rb.
xlm-rl.
arabert.
arbert.
marbert marbert (v2).
(cid:63).
arluesenti†arluesmarluetopicarluedia-barluedia-rarluedia-carluener‡arlueqaaverage.
79.02 / 79.5077.76 / 69.8890.88 / 92.1285.52 / 84.8886.45 / 85.8942.80 / 35.2395.90 / 69.0634.34 / 55.7474.08 / 71.54.
92.17 / 93.0093.18 / 94.0078.26 / 78.5078.84 / 72.0380.01 / 73.0079.81 / 71.1990.90 / 91.81 92.24 / 93.40 92.15 / 92.9787.74 / 87.2187.82 / 87.1786.54 / 85.9887.30 / 86.9286.97 / 86.5486.00 / 85.4639.71 / 33.5641.94 / 34.9842.67 / 35.4096.76 / 76.1996.13 / 74.9496.02 / 73.2736.31 / 58.1039.37 / 63.1234.62 / 56.6774.63 / 73.1977.21 / 75.8976.09 / 74.10.
87.96 / 88.50 93.30 / 94.0080.39 / 74.22 82.35 / 77.1390.81 / 92.6589.67 / 90.9788.31 / 87.74 88.72 / 88.1988.85 / 88.49 90.94 / 90.6545.89 / 37.6944.44 / 36.8796.38 / 71.9397.00 / 76.8329.13 / 48.8336.29 / 57.8177.05 / 74.9276.76 / 75.39.
93.30 / 94.0076.34 / 77.1390.07 / 91.5488.47 / 87.8790.04 / 89.6747.49 / 38.5396.75 / 74.7040.47 / 62.0977.87 / 76.94.arluescore.
72.81.
75.09.
76.55.
73.91.
76.07.
75.99.
77.40.table 12: performance of our models on the test splits of arlue (acc / f1).
(cid:63) metric for arluesenti is acc/ f1pn.
†arluesm results is the average score across the social meaning tasks described in table 5.
‡ metric for arlueqa is exactmatch (em) / f1..provide a cross-lingual transfer evaluation ofmultilingual encoders (xtreme) benchmark forthe evaluation of cross-lingual transfer learningcovering nine tasks for 40 languages (12 languagefamilies).
arlue complements these bench-marking efforts, and is focused on arabic andits dialects.
arlue is also diverse (involves42 datasets) and challenging (our best arluescore is at 77.40)..7 conclusion.
we presented our efforts to develop two power-ful transformer-based language models for arabic.
our models are trained on large-to-massive datasetsthat cover different domains and text genres, includ-ing social media.
by pre-training marbert andmarbert-v2 on dialectal arabic, we aim at en-abling downstream nlp technologies that servewider and more diverse communities.
our bestmodels perform better than (or on par with) xlm-rlarge (∼ 3.4× larger than our models), and henceare more energy efﬁcient at inference time.
ourmodels are also signiﬁcantly better than arabert,.
the currently best-performing arabic pre-trainedlm.
we also introduced aralu, a large and di-verse benchmark for arabic nlu composed of 42datasets thematically organized into six main taskclusters.
arlue ﬁlls a critical gap in arabic andmultilingual nlp, and promises to help propel inno-vation and facilitate meaningful comparisons in theﬁeld.
our models are publicly available.
we alsoplan to publicly release our arlue benchmark.
in the future, we plan to explore self-training ourlanguage models as a way to improve performancefollowing khalifa et al.
(2021).
we also plan to in-vestigate developing more energy efﬁcient models..acknowledgements.
we gratefully acknowledges support from the nat-ural sciences and engineering research councilof canada, the social sciences and humanities re-search council of canada, canadian foundationfor innovation, compute canada and ubc arc-sockeye (https://doi.org/10.14288/sockeye).
we also thank the google tfrc program for pro-viding us with free tpu access..7096ethical considerations.
although our language models are pre-trained us-ing datasets that were public at the time of collec-tion, parts of these datasets might become privateor get removed (e.g., tweets that are deleted byusers).
for this reason, we will not release or re-distribute any of the pre-training datasets.
datacoverage is another important consideration: ourdatasets have wide coverage, and one of our con-tributions is offering models that can serve morediverse communities in better ways than existingmodels.
however, our models may still carry bi-ases that we have not tested for and hence we rec-ommend they be used with caution.
finally, ourmodels deliver better performance than larger-sizedmodels and as such are more energy conserving.
however, smaller models that can achieve simply‘good enough’ results should also be desirable.
thisis part of our own future research, and the commu-nity at large is invited to develop novel methodsthat are more environment friendly..references.
mourad abbas, kamel sma¨ıli, and daoud berkani.
2011. evaluation of topic identiﬁcation methods onarabic corpora.
jdim, 9(5):185–192..ahmed abdelali, hamdy mubarak, younes samih,sabit hassan, and kareem darwish.
2020. arabicdialect identiﬁcation in the wild.
proceedings ofthe sixth arabic natural language processing work-shop..muhammad abdul-mageed, mona diab, and sandrak¨ubler.
2014. samar: subjectivity and sentimentanalysis for arabic social media.
computer speech& language, 28(1):20–37..muhammad abdul-mageed and mona t diab.
2012.awatif: a multi-genre corpus for modern stan-dard arabic subjectivity and sentiment analysis.
inlrec, volume 515, pages 3907–3914.
citeseer..muhammad abdul-mageed, shady elbassuoni, jaddoughman, abdelrahim elmadany, el moatez bil-lah nagoudi, yorgo zoughby, ahmad shaher, iskan-der gaba, ahmed helal, and mohammed el-razzaz.
2021a.
dialex: a benchmark for evaluating multi-dialectal arabic word embeddings.
in proceedingsof the sixth arabic natural language processingworkshop, pages 11–20, kyiv, ukraine (virtual).
as-sociation for computational linguistics..shared task.
in proceedings of the fifth arabic nat-ural language processing workshop, pages 97–110,barcelona, spain (online).
association for compu-tational linguistics..muhammad abdul-mageed, chiyu zhang, abdel-rahim elmadany, houda bouamor, and nizarhabash.
2021b.
nadi 2021: the second nuancedarabic dialect identiﬁcation shared task.
in proceed-ings of the sixth arabic natural language process-ing workshop, pages 244–259, kyiv, ukraine (vir-tual).
association for computational linguistics..muhammad abdul-mageed, chiyu zhang, azadehhashemi, and el moatez billah nagoudi.
2020b.
aranet: a deep learning toolkit for arabic socialin proceedings of the 4th workshop onmedia.
open-source arabic corpora and processing tools,with a shared task on offensive language detec-tion, pages 16–23, marseille, france.
european lan-guage resource association..nawaf abdulla, n mahyoub, m shehab, and mah-moud al-ayyoub.
2013. arabic sentiment analysis:in proceedingscorpus-based and lexicon-based.
of the ieee conference on applied electrical en-gineering and computing technologies (aeect)..nora al-twairesh, hend al-khalifa, abdulmalik al-salman, and yousef al-ohali.
2017. arasenti-tweet:a corpus for arabic sentiment analysis of sauditweets.
procedia computer science, 117:63–72..hassan alhuzali, muhammad abdul-mageed, andlyle ungar.
2018. enabling deep learning of emo-in pro-tion with ﬁrst-person seed expressions.
ceedings of the second workshop on computationalmodeling of people’s opinions, personality, andemotions in social media, pages 25–35..khaled mohammad alomari, hatem m elsherif, andkhaled shaalan.
2017. arabic tweets sentimentalin internationalanalysis using machine learning.
conference on industrial, engineering and otherapplications of applied intelligent systems, pages602–610.
springer..ali alshehri, el moatez billah nagoudi, and muham-mad abdul-mageed.
2020. understanding and de-in pro-tecting dangerous speech in social media.
ceedings of the 4th workshop on open-source ara-bic corpora and processing tools, with a sharedtask on offensive language detection, pages 40–47,marseille, france.
european language resource as-sociation..mohamed aly and amir atiya.
2013. labr: a largein proceed-scale arabic book reviews dataset.
ings of the 51st annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), volume 2, pages 494–498..muhammad abdul-mageed, chiyu zhang, houdabouamor, and nizar habash.
2020a.
nadi2020: the ﬁrst nuanced arabic dialect identiﬁcation.
wissam antoun, fady baly, and hazem hajj.
2020.arabert: transformer-based model for arabic lan-in proceedings of the 4thguage understanding..7097workshop on open-source arabic corpora and pro-cessing tools, with a shared task on offensive lan-guage detection, pages 9–15..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637..ramy baly, alaa khaddaj, hazem hajj, wassim el-hajj, and khaled bashir shaban.
2019. arsentd-lev: a multi-topic corpus for target-based senti-arxivment analysis in arabic levantine tweets.
preprint arxiv:1906.01830..yassine benajiba and paolo rosso.
2007. anersys2.0: conquering the ner task for the arabic lan-guage by combining the maximum entropy withpos-tag information.
in iicai, pages 1814–1823..houda bouamor, sabit hassan, and nizar habash.
2019. the madar shared task on arabic ﬁne-grained dialect identiﬁcation.
in proceedings of thefourth arabic natural language processing work-shop, pages 199–207..tom brown, benjamin mann, nick ryder, melaniesubbiah, jared d kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotin advances in neural information pro-learners.
cessing systems, volume 33, pages 1877–1901.
cur-ran associates, inc..amina chouigui, oussama ben khiroun, and bilelelayeb.
2017. ant corpus : an arabic newstext collection for textual classiﬁcation.
in 2017ieee/acs 14th international conference on com-puter systems and applications (aiccsa), pages135–142.
ieee..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, et al.
2020.unsupervised cross-lingual representation learn-ing at scale.
proceedings of the 58th annual meet-ing of the association for computational linguis-tics..sławomir dadas, michał perełkiewicz, and rafałpo´swiata.
2020. pre-training polish transformer-based language models at scale.
artiﬁcial intelli-gence and soft computing..kareem darwish.
2013. named entity recognition us-ing cross-lingual resources: arabic as an example.
in proceedings of the 51st annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1558–1567..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human language.
technologies, volume 1 (long and short papers),pages 4171–4186..marc djandji, fady baly, wissam antoun, and hazemhajj.
2020. multi-task learning using arabert forin proceedings ofoffensive language detection.
the 4th workshop on open-source arabic corporaand processing tools, with a shared task on offen-sive language detection, pages 97–101, marseille,france.
european language resource association..ibrahim abu el-khair.
2016.
1.5 billion words arabic.
corpus.
arxiv preprint arxiv:1611.04033..abdellah el mekki, ahmed alami, hamza alami,and ismail berrada.
2020.ahmed khoumsi,weighted combination of bert and n-gram fea-tures for nuanced arabic dialect identiﬁcation.
inproceedings of the fourth arabic natural languageprocessing workshop, barcelona, spain..mohamed elaraby and muhammad abdul-mageed.
2018. deep models for arabic dialect identiﬁcationin proceedings of the fifthon benchmarked data.
workshop on nlp for similar languages, varietiesand dialects (vardial 2018), pages 263–274, santafe, new mexico, usa.
association for computa-tional linguistics..abdelrahim elmadany, hamdy mubarak, and walidmagdy.
2018. arsas: an arabic speech-act andsentiment corpus of tweets.
osact, 3:20..ashraf elnagar, yasmin s khalifa, and anas einea.
2018. hotel arabic-reviews dataset constructionfor sentiment analysis applications.
in intelligentnatural language processing: trends and applica-tions, pages 35–52.
springer..ibrahim abu farha and walid magdy.
2020. from ara-bic sentiment analysis to sarcasm detection: thearsarcasm dataset.
in proceedings of the 4th work-shop on open-source arabic corpora and process-ing tools, with a shared task on offensive languagedetection, pages 32–39..bilal ghanem,.
jihen karoui, farah benamara,v´eronique moriceau,and paolo rosso.
2019.idat@fire2019: overview of the track on ironydetection in arabic tweets.
.
in mehta p., rosso p.,majumder p., mitra m.
(eds.)
working notes of theforum for information retrieval evaluation (fire2019).
ceur workshop proceedings.
in: ceur-ws.org, kolkata, india, december 12-15..sabit hassan, younes samih, hamdy mubarak, ahmedabdelali, ammar rashed, and shammur absarchowdhury.
2020. alt submission for osactshared task on offensive language detection.
inproceedings of the 4th workshop on open-sourcearabic corpora and processing tools, with a sharedtask on offensive language detection, pages 61–65,marseille, france.
european language resource as-sociation..7098junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th internationalconference on machine learning, volume 119 ofproceedings of machine learning research, pages4411–4421.
pmlr..fatemah husain.
2020..osact4 shared taskintensiveon offensive language detection:preprocessing-based approach.
in proceedings ofthe 4th workshop on open-source arabic corporaand processing tools, with a shared task on offen-sive language detection, pages 53–60, marseille,france.
european language resource association..muhammad khalifa, muhammad abdul-mageed, andkhaled shaalan.
2021. self-training pre-trained lan-guage models for zero- and few-shot multi-dialectalin proceedings of thearabic sequence labeling.
16th conference of the european chapter of the as-sociation for computational linguistics: main vol-ume, pages 769–782, online.
association for com-putational linguistics..muhammad khalifa and khaled shaalan.
2019. char-acter convolutions for arabic named entity recog-nition with long short-term memory networks.
computer speech & language, 58:335–346..svetlana kiritchenko, saif mohammad, and moham-mad salameh.
2016. semeval-2016 task 7: deter-mining sentiment intensity of english and arabicin proceedings of the 10th internationalphrases.
workshop on semantic evaluation (semeval-2016),pages 42–51..wuwei lan, yang chen, wei xu, and alan ritter.
2020.an empirical study of pre-trained transformers forin proceedings ofarabic information extraction.
the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 4727–4734..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..hang le, lo¨ıc vial, jibril frej, vincent segonne, max-imin coavoux, benjamin lecouteux, alexandre al-lauzen, benoit crabb´e, laurent besacier, and didierschwab.
2020. flaubert: unsupervised languagein proceedings ofmodel pre-training for french.
the 12th language resources and evaluation con-ference, pages 2479–2490..patrick lewis, barlas o˘guz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
pages 7315–7330..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019a.
roberta: a robustly optimized bert pretrain-ing approach.
arxiv preprint arxiv:1907.11692..zihan liu, yan xu, genta indra winata, and pascalefung.
2019b.
incorporating word and subwordunits in unsupervised machine.
translation usinglanguage model rescoring..martin malmsten, love b¨orjeson, and chris haffenden.
2020. playing with words at the national library ofsweden–making a swedish bert.
arxiv preprintarxiv:2007.01658..louis martin, benjamin muller, pedro javier or-tiz su´arez, yoann dupont, laurent romary, ´ericde la clergerie, djam´e seddah, and benoˆıt sagot.
2020. camembert: a tasty french languagein proceedings of the 58th annual meet-model .
ing of the association for computational linguistics,pages 7203–7219, online.
association for computa-tional linguistics..bryan mccann, nitish shirish keskar, caiming xiong,and richard socher.
2018. the natural languagedecathlon: multitask learning as question answer-ing.
arxiv preprint arxiv:1806.08730..alexis mitchell, stephanie strassel, mark przybocki,j davis, george doddington, ralph grishman, andb sundheim.
2004. tides extraction (ace) 2003multilingual training data.
linguistic data consor-tium, philadelphia web download..s. bravo-marquez mohammad, m. f. salameh, ands. kiritchenko.
2018. semeval-2018 task 1: affectin proceedings of international work-in tweets.
shop on semantic evaluation (semeval-2018).
as-sociation for computational linguistics..hussein mozannar, karl el hajal, elie maamary, andhazem hajj.
2019. neural arabic question answer-in proceedings of the fourth arabic naturaling.
language processing workshop, florence, italy.
as-sociation for computational linguistics..hamdy mubarak, kareem darwish, walid magdy,tamer elsayed,and hend al-khalifa.
2020.overview of osact4 arabic offensive languagein proceedings of the 4thdetection shared task.
workshop on open-source arabic corpora and pro-cessing tools, with a shared task on offensive lan-guage detection, pages 48–52, marseille, france.
european language resource association..mahmoud nabil, mohamed aly, and amir f atiya.
2015. astd: arabic sentiment tweets dataset.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages2515–2519..el moatez billah nagoudi, abdelrahim elmadany,muhammad abdul-mageed, and tariq alhindi..70992020. machine generation and detection of ara-bic manipulated and fake news.
in proceedings ofthe fifth arabic natural language processing work-shop, pages 69–84, barcelona, spain (online).
asso-ciation for computational linguistics..dat quoc nguyen and anh tuan nguyen.
2020.phobert: pre-trained language models for viet-namese.
in findings of the association for computa-tional linguistics: emnlp 2020, pages 1037–1042,online.
association for computational linguistics..ossama obeid, nasser zalmout, salam khalifa, dimataji, mai oudah, bashar alhafni, go inoue, fadhleryani, alexander erdmann, and nizar habash.
2020. camel tools: an open source pythontoolkit for arabic natural language processing.
inproceedings of the 12th language resources andevaluation conference, pages 7022–7032..sampo pyysalo, jenna kanerva, antti virtanen, andfilip ginter.
2020. wikibert models: deep trans-arxiv preprintfer learning for many languages.
arxiv:2006.01538..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j liu.
2020. exploring thelimits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21:1–67..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in nat-ural language processing, austin, texas.
associa-tion for computational linguistics..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
transactions of the associ-ation for computational linguistics, 8:842–866..sara rosenthal, noura farra, and preslav nakov.
2017.semeval-2017 task 4: sentiment analysis in twitter.
in proceedings of the 11th international workshopon semantic evaluation (semeval-2017), pages 502–518..motaz k saad and wesam m ashour.
2010. osac:open source arabic corpora .
in 6th archeng int.
symposiums, eeecs, volume 10..case-study on arabic social media posts.
in pro-ceedings of the 2015 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages767–777, denver, colorado.
association for compu-tational linguistics..mike schuster and kaisuke nakajima.
2012. japanesein 2012 ieee interna-and korean voice search.
tional conference on acoustics, speech and signalprocessing (icassp), pages 5149–5152.
ieee..pedro javier ortiz su´arez, benoˆıt sagot, and laurentromary.
2019. asynchronous pipeline for process-ing huge corpora on medium to low resource in-in 7th workshop on the challengesfrastructure.
in the management of large corpora (cmlc-7).
leibniz-institut f¨ur deutsche sprache..bashar talafha, mohammad ali, muhy eddin za’ter,haitham seelawi, ibraheem tuffaha, mostafa samir,wael farhan, and hussein t al-natsheh.
2020.multi-dialect arabic bert for country-level di-alect identiﬁcation.
in proceedings of the fifth ara-bic natural language processing workshop, pages111–118, barcelona, spain (online).
association forcomputational linguistics..antti virtanen, jenna kanerva, rami ilo, jouni lu-oma, juhani luotolahti, tapio salakoski, filip gin-ter, and sampo pyysalo.
2019. multilingual isarxiv preprintnot enough: bert for finnish.
arxiv:1912.07076..wietse de vries, andreas van cranenburgh, ariannabisazza, tommaso caselli, gertjan van noord, andmalvina nissim.
2019. bertje: a dutch bertmodel.
arxiv preprint arxiv:1912.09582..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r bowman.
2019.super-glue: a stickier benchmark for general-purposearxiv preprintlanguage understanding systems.
arxiv:1905.00537..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2018.glue: a multi-task benchmark and analysis platformfor natural language understanding.
in proceedingsof the 2018 emnlp workshop blackboxnlp: an-alyzing and interpreting neural networks for nlp,pages 353–355, brussels, belgium.
association forcomputational linguistics..ali safaya, moutasem abdullatif, and deniz yuret.
2020. kuisail at semeval-2020 task 12: bert-cnn for offensive speech identiﬁcation in social me-dia.
in proceedings of the fourteenth workshop onsemantic evaluation, pages 2054–2059, barcelona(online).
international committee for computationallinguistics..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap betweenhuman and machine translation.
arxiv preprintarxiv:1609.08144..mohammad salameh, saif mohammad, and svetlanakiritchenko.
2015. sentiment after translation: a.linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, aditya.
7100barua, and colin raffel.
2021. mt5: a massivelymultilingual pre-trained text-to-text transformer.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 483–498, online.
association for computa-tional linguistics..wajdi zaghouani and anis charﬁ.
2018. arap-tweet:a large multi-dialect twitter corpus for gen-inder, age and language variety identiﬁcation.
proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..omar f zaidan and chris callison-burch.
2014. ara-bic dialect identiﬁcation .
computational linguis-tics, 40(1):171–202..imad zeroual, dirk goldhahn, thomas eckart, and ab-delhak lakhouaja.
2019. osian: open source in-ternational arabic news corpus - preparation andintegration into the clarin-infrastructure.
in pro-ceedings of the fourth arabic natural languageprocessing workshop, pages 175–182, florence,italy.
association for computational linguistics..chiyu zhang and muhammad abdul-mageed.
2019a.
multi-task bidirectional transformer representationsfor irony detection.
corr..chiyu zhang and muhammad abdul-mageed.
2019b.
no army, no navy: bert semi-supervised learn-in proceedings of theing of arabic dialects.
fourth arabic natural language processing work-shop, pages 279–284, florence, italy.
associationfor computational linguistics..7101appendices.
a sentiment analysis.
a.1 sa datasets.
• ajgt.
the arabic jordanian general tweets(ajgt) dataset (alomari et al., 2017) coversmsa and jordanian arabic, with 900 positiveand 900 negative posts..• aranetsent.
abdul-mageed et al.
(2020b)collect 15 datasets in both msa and di-alects from abdul-mageed and diab (2012)(awatif), abdul-mageed et al.
(2014)(samar), abdulla et al.
(2013); nabil et al.
(2015); kiritchenko et al.
(2016); aly andatiya (2013); salameh et al.
(2015); rosen-thal et al.
(2017); alomari et al.
(2017); mo-hammad et al.
(2018), and baly et al.
(2019).
these datasets carry both binary (negativeand positive) and three-way (negative, neu-tral, and positive) labels, but abdul-mageedet al.
(2020b) map them into binary sentimentonly..• arasenti-tweet.
this comprises 17, 573gold labeled msa and saudi arabic tweetsby al-twairesh et al.
(2017)..• arsarcasmsent this sarcasm dataset is la-beled with sentiment tags by farha and magdy(2020) who extract it from astd (nabil et al.,2015) (10, 547 tweets) and semeval-2017task 4 (rosenthal et al., 2017) (8, 075 tweets)..• arsas.
this arabic speech act and senti-ment (arsas) corpus (elmadany et al., 2018)consists of tweets annotated with sentimenttags..• arsend-lev.
the arabic sentiment twit-ter dataset for levantine dialect (arsend-lev) (baly et al., 2019) has 4, 000 tweets re-trieved from the levant region..• astd.
this is a collection of 10, 006 egyp-.
tian tweets by nabil et al.
(2015)..• awatif.
this is an msa dataset fromnewswire, wikipedia, and web fora intro-duced by abdul-mageed and diab (2012)..sentiment (syts) are introduced by salamehet al.
(2015)..• camelsent.
obeid et al.
(2020) merge train-ing and development data from arsas (el-madany et al., 2018), astd (nabil et al.,2015), semeval (rosenthal et al., 2017), andarsentd (al-twairesh et al., 2017) to createa new training dataset (∼ 24k) and evaluateon the independent test sets from each of thesesources..• hard.
the hotel arabic reviews dataset(hard) (elnagar et al., 2018) consists of93, 700 msa and dialect hotel reviews..• labr.
the large arabic book review cor-pus (aly and atiya, 2013) has 63, 257 bookreviews from goodreads,18 each rated with a1-5 stars scale..• twitterabdullah.19 this is a dataset of 2, 000msa and jordanian arabic tweets manuallylabeled by abdulla et al.
(2013)..• twittersaad.
this dataset is collected usingan emoji lexicon by moatez saad in 2019 andis available on kaggle.20.
• semeval-2017.
this is the semeval-2017sentiment analysis in arabic twitter taskdatasetby rosenthal et al.
(2017)..a.2 sa baselines.
for sa, we compare to the following stoa:.
• antoun et al.
(2020).
we compare to bestresults reported by the authors on ﬁve sadatasets: hard, balanced astd (which werefer to as astd-b), arsentd-lev, ajgt,and the unbalanced positive and negativeclasses for labr.
they split each dataset into80/20 for train/test, respectively, and reportin accuracy using the best epoch identiﬁed ontest data.
for a valid comparison, we followtheir data splits and evaluation set up..• obeid et al.
(2020).
they ﬁne-tune mbertand arabert on the merged camelsent.
18www.goodreads.com.
19for ease of reference, we assign a name to this and other.
unnamed datasets..20www.kaggle.com/mksaad/arabic-sentiment-twitter-.
• bbns & syts.
the bbn blog postsand syria tweets.
sentiment.
(bbns).
corpus..7102dataset (classes).
classes.
train dev test.
-.
1.4k.
{neg, pos}{neg, pos}{neg, neut, pos}{neg, neut, pos}{neg, neut, pos}{neg, neut, pos, neg+, pos+}{neg, neut, pos}{neg, pos}{neg, neut, obj, pos }{neg, neut, pos}{neg, pos}{neg, pos}{mix, neg, neut, obj, pos}{neg, neut, pos}{neg, neut, pos}{neg, pos}{neg, pos}{neg, pos, neut}.
361ajgt (2)100.5k 14.3k 11.8karanetsent (2)1.4k1.4k11.1karasenti-tweet (3)2.1k8.4karsarsent (3)-3.7k24.8karsas (3)-8013.2karsend-lev (5)-66424.8kastd (3)-2671.1kastd-b (2)-2842882.3kawatif (4)116125960bbn (3)21.1k84.5khard (2)-3.3k13.2klabr (2)-3163102.5ksamar (5)6.1k24.8ksemeval (3)-199202960syts (3)1901.6k202twabdullah (2)5.8k47k5.8ktwsaad (2)6.5k 44.2k190.9karluesenti (3)table a.1: sentiment analysis datasets.
neg+: “very neg-ative”; pos+: “very positive”.
we construct arluesenti bymerging the different datasets and collapsing, or removing,the less frequent classes (details in text)..pn, which is thedatasets and report in f1macro f1 score over the positive and nega-tive classes only (while neglecting the neutralclass)..• abdul-mageed et al.
(2020b).
they ﬁne-tune mbert on the aranetsent data and re-port results in f1 score on test data..a.3.
sa evaluation on dev.
table a.2 shows results of sa on dev for datasetswhere there is a development split..dataset (classes) mbert xlm-rb xlm-rl arabert arbert marbert.
aranetsent(2)arasenti(3)bbn(3)syts(3)twittersaad(2)samar(5)awatif(4)twitterabdullah(2).
89.0092.00.
92.0093.5075.00.
84.0093.0068.0062.0080.0026.0063.5087.50.
93.0095.0077.0066.0095.5061.0067.5095.5099.00table a.2: sa results (f1) on dev..86.5091.5070.0065.0081.5042.5065.0092.50.
80.5095.5054.5062.0091.00.
79.5069.0090.0050.5070.50.
92.0093.5078.5072.50.
96.0062.5072.0097.00.b social meaning.
b.1 sm tasks & datasets.
• age and gender.
for both age and gender,we use the arap-tweet dataset (zaghouani andcharﬁ, 2018), which covers 17 different coun-tries from 11 arab regions.
we follow the80-10-10 data split of aranet (abdul-mageedet al., 2020b)..• dangerous speech.
we use the dangerousspeech aradang dataset from alshehri et al.
(2020), which is composed of tweets manuallylabeled with dangerous and safe tags..task.
dataset (classes).
classes.
train dev.
test.
arap-tweet (3)aradang (2)aranetemo (8)arap-tweet (2).
agedangerousemotiongenderhate speech hs@osact (2)ironyoffensivesarcasm.
fire2019 (2)off@osact (2)arasarcasm (2).
{ ≤ 24 yrs, 25 − 34 yrs, ≥ 35 yrs }{dangerous, not-dangerous}{ang, anticip, disg, fear, joy, sad, surp, trust}{female, male}{hate, not-hate}{irony, not-irony}{offensive, not-offensive}{sarcasm, not-sarcasm}.
1.3m 160.7k 160.7k3.5k664616942911190k1.3m 160.7k 160.7k2k1k10k4043.6k-2k1k10k2.1k8.4k-.
table b.1: social meaning datasets..• offensive language and hate speech.
weuse manually labeled data from the shared taskof offensive speech (mubarak et al., 2020).21the shared task is divided into two sub-tasks:sub-task a: detecting if a tweet is offensiveor not-offensive, and sub-task b: detecting ifa tweet is hate-speech or not-hate-speech..• emotion.
we use the aranetemo datasetfrom abdul-mageed et al.
(2020b), whichis created by merging two datasets from al-huzali et al.
(2018)..• irony..we use the irony identiﬁca-tion dataset for arabic tweets released byidat@fire2019 shared task (ghanem et al.,2019), following abdul-mageed et al.
(2020b)data splits..• sarcasm.
we use the arsarcasm dataset de-.
veloped by farha and magdy (2020)..more details about these datasets are in ta-.
ble b.1..b.2 sm baselines.
• age and gender..we compare toaranet abdul-mageed et al.
(2020b) age andgender models, trained by ﬁne-tuning mbert.
the authors report 51.42 and 65.30 f1 on ageand gender, respectively..• dangerous speech.
we compare to alshehriet al.
(2020), who report a best of 59.60 f1on test with an mbert model ﬁned-tuned onemotion data..• emotion.
we compare to abdul-mageed et al.
(2020b), who acquire 60.32 f1 on test with aﬁne-tuned mbert..• hate speech.
the best results on the offen-sive and hate speech shared task (mubaraket al., 2020) are at 95 f1 score and are re-ported by husain (2020), who employ heavy.
21http://edinburghnlp.inf.ed.ac.uk/workshops/osact4..7103task (classes) mbert xlm-rb xlm-rl arabert arbert marbert.
dataset (classes).
mbert xlm-rb xlm-rl arabert arbert marbert.
age (3)dangerous (2)emotion (8)gender (2)hate (2)irony (2)offensive (2).
59.7065.0972.0971.1076.5683.1285.26.
56.3367.3561.3468.0675.9181.0884.04.
53.6369.9572.7871.2378.0081.2984.8386.7288.77table b.2: sm results in f1 on dev..57.6767.7365.4667.6172.0979.1287.21.
58.6068.5868.0569.9775.01.
62.1975.5075.1872.8182.9186.7791.68.anttext (5).
anttitle (5).
anttext+title (5).
khallej (4).
osac (10).
85.04.
79.46.
87.24.
94.48.
86.74.
80.77.
86.36.
95.32.
87.41.
82.04.
88.45.
96.09.
87.98.
83.56.
88.76.
95.65.
87.06.
81.10.
87.27.
96.16.
97.87.
97.61table c.2: tc results tasks (f1) on dev..97.75.
97.56.
97.94.
85.80.
82.36.
85.99.
96.31.
97.66.feature engineering with svms.
since our fo-cus is on methods exploiting language models,we compare to djandji et al.
(2020) who ranksecond in the shared task with a ﬁne-tunedarabert (83.41 f1 on test)..• irony.
we compare to zhang and abdul-mageed (2019a) who ﬁne-tune mbert onthe irony task, with an auxiliary author proﬁl-ing task, and report 82.4 f1 on test..• offensive language.
we compare to the bestresults on the offensive sub-task (mubaraket al., 2020) reported by hassan et al.
(2020).
they propose an ensemble of svms, cnn-bilstm, and mbert with majority votingand acquire 90.51 f1..• sarcasm.
we compare to farha and magdy(2020) who train a bilstm model using thearasarcasm dataset, reporting 46.00 f1 score..b.3.
sm evaluation on dev.
table b.2 shows results of the social meaning taskson development splits..c topic classiﬁcation.
c.1 tc datasets.
• arabic news text.
chouigui et al.
(2017)build the arabic news text (ant) dataset fromtranscribed tunisian radio broadcasts..• khaleej.
abbas et al.
(2011) created the.
khaleej from gulf arabic websites..• osac.
saad and ashour (2010) collect.
osac from news articles..dataset (classes).
classes.
train dev test.
{c, e, i, me, s, t}{e, i, loc, s}{e, f, h, hist, l, r, rlg, sps, s, str}{all classes}.
3.2kant (5)570khallej (4)2.2kosac (10)5.9karluetopic (16)table c.1: tc datasets.
c: culture, e: economy, f: family,h: health, hist: history, i: international news, l: law, loc,local news, me: middle east, r: recipes, rlg: religion, sps:space, s: sports, str: stories, t: technology..25.2k 3.2k4.6k57018k 2.2k47.7k 5.9k.
c.2 tc evaluation on dev.
results of tc tasks on dev data are in table c.2..d dialect identiﬁcation.
d.1 dia datasets.
we introduce each dataset brieﬂy here and providea description summary of all datasets in table d.1..• arabic online commentary (aoc).
this isa repository of 3m arabic comments on on-line news (zaidan and callison-burch, 2014).
it is labeled with msa and three regional di-alects (egyptian, gulf, and levantine)..• arsarcasmdia.
this dataset is developedby farha and magdy (2020) for sarcasm de-tection but also carries regional dialect la-bels from the set {egyptian, gulf, levantine,maghrebi}..• madar.
sub-task 2 of the madar sharedtask (bouamor et al., 2019)22 is focused onuser-level dialect identiﬁcation with manually-curated country labels (n=21)..• nadi-2020..the ﬁrst nuanced arabicdialect identiﬁcation shared task (nadi2020) (abdul-mageed et al., 2020a)23 tar-gets country level (n=21) as well as provincelevel (n=100) dialects..• qadi.
the qcri arabic dialect identiﬁca-tion (qadi) dataset (abdelali et al., 2020) islabeled at the country level (n=18)..details of the datasets are in table d.1..d.2 dia baselines.
• elaraby and abdul-mageed (2018) reportthree levels of classiﬁcation on aoc data: (1)msa vs. da (87.23 accuracy), (2) regional(i.e., egyptian, gulf, and levantine) (87.81 ac-curacy), and (3) msa, egyptian, gulf, and.
22https://camel.abudhabi.nyu.edu/madar-shared-task-.
2019/..23https://github.com/ubc-nlp/nadi..7104train dev.
test.
86.5k 10.8k 10.8k4.5k4.5k35.7k86.5k 10.8k 10.8k2.1k8.4k-44k.
193.1k 26.6k.
2.1k.
5k.
5k.
task (classes).
dataset classes.
binaryaoc (2)regionaoc (3)regionaoc (4)arsarcasmdia (5)regoinmadar-tl (21) country.
nadi (21).
country.
{da, msa}{egypt, gulf, levnt}{egypt, gulf, levnt, msa}{egypt, gulf, levnt, magreb, msa}{multiple countries(cid:63)}{multiple countries(cid:63)}{multiple countries†}{da, msa}{egypt, gulf, levnt, magreb}{multiple countries(cid:63)}.
497.8k.
country.
qadi (18)94.9k 10.8k 12.9karluedia-b (2)binary38.5k5.3k4.5karluedia-r (4)region711.9k 31.5k 52.1karluedia-c (21) countrytable d.1: dialect datasets.
(cid:63) all arab countries exceptcomoros.
† all arab countries except comoros, djibouti,mauritania, and somalia..3.5k.
-.
dataset (classes) task.
mbert xlm-rb xlm-rl arabert arbert marbert.
madar(21)aoc(4)aoc(3)aoc(2)nadi(21)nadi(100).
countryregoinregoinbinarycountryprovince.
33.7580.0787.0787.8914.4902.32.
34.5478.9786.8087.6317.3003.91.
33.2879.5588.2188.3818.624.00.
33.4780.8588.4688.7616.1803.04.
39.2481.9689.5789.3223.7306.05.
40.6183.5691.5689.6626.4005.23.table d.2: dia results on dev in f1..levantine (accuracy of 82.45).
their bestresults are based on bilstm..datasetanercorpace03bnace03nwace04bntw-nerarluener.
traintestdevtokens95.5k 24.8k 29.9k150.2k2k11.6k15.6k3k21.3k27k7k56.5k70.5k74.8k7.4k 24.5k42.9k338.3k 227.7k 44.1k 66.5k.
2k2.7k7k.
table e.1: distribution of the arabic ner datasets..dataset (classes) mbert xlm-rb xlm-rl arabert arbert marbert.
anercorpace03nwace03bnace04nwtw-ner.
86.2080.5780.3587.2152.60.
87.2488.2180.3690.0873.61.
89.6483.2488.1790.4983.3990.9191.9489.3370.7877.70table e.2: ner results (f1) on dev..90.2489.7681.0589.7073.61.
80.8685.0279.0586.8067.39.e.2 ner baselines.
khalifa and shaalan (2019) apply cnns and bil-stms and report f1 scores on test sets, as fol-lows: 88.77 (anercorp), 91.47 (ace03nw),94.92 (ace03bn), 91.20 (ace04nw), and 65.34(twitter).
we use their exact data splits..• abdelali et al.
(2020) ﬁne-tune arabert on.
f question answering datasets.
the qadi dataset.
they report 60.6 f1..• zhang and abdul-mageed (2019b) devel-oped the top ranked system in madar sub-task 2, with 48.76 accuracy and 34.87 f1 attweet level..• talafha et al.
(2020) developed nadi sub-task 1 (country level) winning system, an en-semble of ﬁne-tuned arabert (26.78 f1)..• el mekki et al.
(2020) developed nadi sub-task 2 (province level) winning system usinga combination of word and character n-gramsto ﬁne-tune arabert (6.08 f1)..• arabert.
for arsarcasmdia, where no di-alect id system was previously developed, weconsider a ﬁne-tuned arabert a baseline..d.3 dia evaluation on dev.
table d.2 shows results of the dialect identiﬁcationtasks on development splits..e named entity recognition.
e.1 ner datasets.
table e.1 and table e.2 show the data splits acrossour ner datasets, and the results of all our modelson the development splits..• arcd.
mozannar et al.
(2019) use crowd-sourcing to develop the arabic reading com-prehension dataset.
we use the same arcddata splits used by antoun et al.
(2020)..• mlqa.
this multilingual question answer-ing benchmark is proposed by lewis et al.
it consists of over 5k extractive(2020).
question-answer instances in squad formatin seven languages, including arabic..• xquad.
this cross-lingual question an-swering dataset artetxe et al.
(2020) consistsof 1, 190 question-answer pairs and 240 para-graphs from squad v1.1 (rajpurkar et al.,2016) translated into ten languages (includingarabic) by professional translators..• tydi qa.
the tydi qa dataset artetxe et al.
(2020) is manually curated and covers 11 lan-guages (including arabic).
we focus on the“gold” passage task only..datasetar-xtremearcdar-mlqaar-xquadar-tydi-qaarlueqa.
train86.7k (mt)---14.8k (h)101.6k.
dev--517 (ht)--517.test-1.4k (h)5.3k (ht)1.2k (ht)921 (h)11.6k.
table f.1: multilingual & arabic qa datasets.
h: humancreated.
ht: human translated.
mt: machine translated..7105