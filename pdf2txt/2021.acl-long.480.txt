good for misconceived reasons: an empirical revisiting on the need forvisual context in multimodal machine translation.
zhiyong wu† ∗, lingpeng kong†¶, wei bi‡, xiang li§, ben kao††the university of hong kong,‡tencent ai lab, ¶shanghai artiﬁcial intelligence laboratory, §east china normal university†{zywu,lpk,kao}@cs.hku.hk, ‡victoriabi@tencent.com, §xiangli@dase.ecnu.edu.cn.
abstract.
a neural multimodal machine translation(mmt) system is one that aims to perform bet-ter translation by extending conventional text-only translation models with multimodal infor-mation.
many recent studies report improve-ments when equipping their models with themultimodal module, despite the controversy ofwhether such improvements indeed come fromthe multimodal part.
we revisit the contribu-tion of multimodal information in mmt by de-vising two interpretable mmt models.
to oursurprise, although our models replicate sim-ilar gains as recently developed multimodal-integrated systems achieved, our models learnto ignore the multimodal information.
uponfurther investigation, we discover that the im-provements achieved by the multimodal mod-els over text-only counterparts are in fact re-sults of the regularization effect.
we report em-pirical ﬁndings that highlight the importanceof mmt models’ interpretability, and discusshow our ﬁndings will beneﬁt future research..1.introduction.
multimodal machine translation (mmt) aims atdesigning better translation systems by extendingconventional text-only translation systems to takeinto account multimodal information, especiallyfrom visual modality (specia et al., 2016; wanget al., 2019).
despite many previous success inmmt that report improvements when models areequipped with visual information (calixto et al.,2017; helcl et al., 2018; ive et al., 2019; lin et al.,2020; yin et al., 2020), there have been continuingdebates on the need for visual context in mmt..in particular, specia et al.
(2016); elliott et al.
(2017); barrault et al.
(2018) argue that visual con-text does not seem to help translation reliably, at.
∗the majority of this work was done while the ﬁrst author.
was interning at tencent ai lab..least as measured by automatic metrics.
elliott(2018); gr¨onroos et al.
(2018a) provide further ev-idence by showing that mmt models are, in fact,insensitive to visual input and can translate withoutsigniﬁcant performance losses even in the pres-ence of features derived from unrelated images.
amore recent study (caglayan et al., 2019), however,shows that under limited textual context (e.g., nounwords are masked), models can leverage visual in-put to generate better translations.
but it remainsunclear where the gains of mmt methods comefrom, when the textual context is complete..the main tool utilized in prior discussion is ad-versarial model comparison — explaining the be-havior of complex and black-box mmt models bycomparing performance changes when given adver-sarial input (e.g., random images).
although suchan opaque tool is an acceptable beginning to in-vestigate the need for visual context in mmt, theyprovide rather indirect evidence (hessel and lee,2020).
this is because performance differencescan often be attributed to factors unrelated to vi-sual input, such as regularization (kukaˇcka et al.,2017), data bias (jabri et al., 2016), and some oth-ers (dodge et al., 2019)..from these perspectives, we revisit the needfor visual context in mmt by designing two in-terpretable models.
instead of directly infusingvisual features into the model, we design learnablecomponents, which allow the model to voluntarilydecide the usefulness of the visual features and re-inforce their effects when they are helpful.
to oursurprise, while our models are shown to be effectiveon multi30k (elliott et al., 2016) and vatex (wanget al., 2019) datasets, they learn to ignore the mul-timodal information.
our further analysis suggeststhat under sufﬁcient textual context, the improve-ments come from a regularization effect that is sim-ilar to random noise injection (bishop, 1995) andweight decay (hanson and pratt, 1989).
the addi-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6153–6166august1–6,2021.©2021associationforcomputationallinguistics6153tional visual information is treated as noise signalsthat can be used to enhance model training and leadto a more robust network with lower generalizationerror (salamon and bello, 2017).
repeating theevaluation under limited textual context further sub-stantiates our ﬁndings and complements previousanalysis (caglayan et al., 2019)..our contributions are twofold.
first, we revisitthe need for visual context in the popular task ofmultimodal machine translation and ﬁnd that: (1)under sufﬁcient textual context, the mmt models’improvements over text-only counterparts resultfrom the regularization effect (section 5.2).
(2)under limited textual context, mmt models canleverage visual context to help translation (sec-tion 5.3).
our ﬁndings highlight the importance ofmmt models’ interpretability and the need for anew benchmark to advance the community..second, for the mmt task, we provide a strongtext-only baseline implementation and two mod-els with interpretable components that replicatesimilar gains as reported in previous works.
differ-ent from adversarial model comparison methods,our models are interpretable due to the speciﬁcallydesigned model structure and can serve as stan-dard baselines for future interpretable mmt stud-ies.
our code is available at https://github.
com/lividwo/revisit-mmt..2 background.
one can broadly categorize mmt systems into twotypes: (1) conventional mmt, where there is goldalignment between the source (target) sentence pairand a relevant image and (2) retrieval-based mmt,where systems retrieve relevant images from an im-age corpus as additional clues to assist translation..conventional mmt most mmt systems re-quire datasets consist of images with bilingual an-notations for both training and inference.
manyearly attempts use a pre-trained model (e.g.,resnet (he et al., 2016)) to encode images intofeature vectors.
this visual representation canbe used to initialize the encoder/decoder’s hid-den vectors (elliott et al., 2015; libovick´y andhelcl, 2017; calixto et al., 2016).
it can also beappended/prepended to word embeddings as ad-ditional input tokens (huang et al., 2016; calixtoand liu, 2017).
recent works (libovick´y et al.,2018; zhou et al., 2018; ive et al., 2019; lin et al.,2020) employ attention mechanism to generate avisual-aware representation for the decoder.
for.
instance, doubly-att (calixto et al., 2017; helclet al., 2018; arslan et al., 2018) insert an extravisual attention sub-layer between the decoder’ssource-target attention sub-layer and feed-forwardsub-layer.
while there are more works on engi-neering decoders, encoder-based approaches arerelatively less explored.
to this end, yao and wan(2020) and yin et al.
(2020) replace the vanillatransformer encoder with a multi-modal encoder.
besides the exploration on network structure, re-searchers also propose to leverage the beneﬁts ofmulti-tasking to improve mmt (elliott and k´ad´ar,2017; zhou et al., 2018).
the imagination archi-tecture (elliott and k´ad´ar, 2017; helcl et al., 2018)decomposes multimodal translation into two sub-tasks: translation task and an auxiliary visual recon-struction task, which encourages the model to learna visually grounded source sentence representation..retrieval-based mmt the effectiveness of con-ventional mmt heavily relies on the availabilityof images with bilingual annotations.
this couldrestrict its wide applicability.
to address this is-sue, zhang et al.
(2020) propose uvr-nmt thatintegrates a retrieval component into mmt.
theyuse tf-idf to build a token-to-image lookup ta-ble, based on which images sharing similar topicswith a source sentence are retrieved as relevantimages.
this creates image-bilingual-annotationinstances for training.
retrieval-based models havebeen shown to improve performance across a vari-ety of nlp tasks besides mmt, such as questionanswering (guu et al., 2020), dialogue (westonet al., 2018), language modeling (khandelwal et al.,2019), question generation (lewis et al., 2020), andtranslation (gu et al., 2018)..3 method.
in this section we introduce two interpretablemmt models: (1) gated fusion for conventionalmmt and (2) dense-retrieval-augmented mmt(rmmt) for retrieval-based mmt.
our design phi-losophy is that models should learn, in an inter-pretable manner, to which degree multimodal in-formation is used.
following this principle, wefocus on the component that integrates multimodalinformation.
in particular, we use a gating matrix λ(yin et al., 2020; zhang et al., 2020) to control theamount of visual information to be blended into thetextual representation.
such a matrix facilitates in-terpreting the fusion process: a larger gating valueλij ∈ [0, 1] indicates that the model exploits more.
6154visual context in translation, and vice versa..3.1 gated fusion mmt.
given a source sentence x of length t and an as-sociated image z, we compute the probability ofgenerating target sentence y of length n by:.
p(y|x, z) =.
pθ (yi | x, z, y<i) ,.
(1).
n(cid:89).
i.where pθ (yi | x, z, y<i) is implemented with atransformer-based (vaswani et al., 2017) network.
speciﬁcally, we ﬁrst feed x into a vanilla trans-former encoder to obtain a textual representationhtext ∈ rt ×d, which is then fused with visualrepresentation embed image (z) before fed into thetransformer decoder.
for each image z, we usea pre-trained resnet-50 cnn (he et al., 2016) toextract a 2048-dimensional average-pooled visualrepresentation, which is then projected to the samedimension as htext:.
embed image (z) = wz resnetpool (z) ..(2).
we next generate a gating matrix λ ∈ [0, 1]t ×dto control the fusion of htext and embed image (z):.
λ = sigmoid (cid:0)wλ embed image (z) + uλhtext.
(cid:1) ,.
where wλ and uλ are model parameters.
notethat this gating mechanism has been a buildingblock for many recent mmt systems (zhang et al.,2020; lin et al., 2020; yin et al., 2020).
we are,however, the ﬁrst to focus on its interpretability.
finally, we generate the output vector h by:.
h = htext + λ embed image (z)..(3).
h is then fed into the decoder directly for transla-tion as in vanilla transformer..3.2 retrieval-augmented mmt (rmmt).
rmmt consists of two sequential components:(1) an image retriever p(z|x) that takes x as in-put and returns top-k most relevant images froman image database; (2) a multi-modal translatorp(y|x, z) = (cid:81)ni pθ (yi | x, z, y<i) that generateseach yi conditioned on the input sentence x, theimage set z returned by the retriever, and the pre-viously generated tokens y<i..image retriever based on the tf-idf model,searching in existing retrieval-based mmt (zhanget al., 2020) ignores the context information of agiven query, which could lead to poor performance.
to improve the recall of our image retriever, wecompute the similarity between a sentence x andan image z with inner product:.
sim(x, z) = embed text (x)(cid:62) embed image(z),.
where embedtext(x) and embedimage(z) are d-dimensional representations of x and z, respec-tively.
we then retrieve top-k images that areclosest to x. for embedimage(z), we compute it byeq.
2. for embedtext(x), we implement it usingbert (devlin et al., 2019):.
embed text (x) = wtext bertcls (x) ..(4).
following standard practices, we use a pre-trainedbert model1 to obtain the “pooled” representationof the sequence (denoted as bertcls(x)).
here,wtext is a projection matrix..multimodal translator different from gatedfusion, p(y|x, z) now is conditioning on a set ofimages rather than one single image.
for each z inz, we represent it using embedimage(z) ∈ rd as inequation 2. the image set z then forms a featurematrix embedimage(z) ∈ rk×d, where k = |z|and each row corresponds to the feature vector ofan image.
we use a transformation layer fθ(∗) toextract salient features from embedimage(z) andobtain a compressed representation rd of z. af-ter the transformation, ideally, we can implementp(y|x, z) using any existing mmt models.
forinterpretability, we follow the gated fusion modelto fuse the textual and visual representations witha learnable gating matrix λ:.
h = htext + λfθ( embed image (z))..(5).
here, fθ(∗) denotes a max-pooling layer with win-dow size k × 1..4 experiment.
in this section, we evaluate our models on themulti30k and vatex benchmark..4.1 dataset.
we perform experiments on the widely-used mmtdatasets: multi30k.
we follow a standard split.
1here we use bert-base-uncased version..6155of 29,000 instances for training, 1,014 for valida-tion and 1,000 for testing (test2016).
we alsoreport results on the 2017 test set (test2017) withextra 1,000 instances and the mscoco test setthat includes 461 more challenging out-of-domaininstances with ambiguous verbs.
we merge thesource and target sentences in the ofﬁcially pre-processed version of multi30k2 to build a jointvocabulary.
we then apply the byte pair encod-ing (bpe) algorithm (sennrich et al., 2016) with10,000 merging operations to segment words intosubwords, which generates a vocabulary of 9,712(9,544) tokens for en-de (en-fr).
retriever pre-training.
we pre-train the retrieveron a subset of the flickr30k dataset (plummer et al.,2015) that has overlapping instances with multi30kremoved.
we use multi30k’s validation set to eval-uate the retriever.
we measure the performance byrecall-at-k (r@k), which is deﬁned as the frac-tion of queries whose closest k images retrievedcontain the correct images.
the pre-trained re-triever achieves r@1 of 22.8% and r@5 of 39.6%..4.2 setup.
we experiment with different model sizes (base,small, and tiny, see appendix a for details).
baseis a widely-used model conﬁguration for trans-former in both text-only translation (vaswani et al.,2017) and mmt (gr¨onroos et al., 2018b; ive et al.,2019).
however, for small datasets like multi30k,training such a large model (about 50 million pa-rameters) could cause overﬁtting.
in our prelimi-nary study, we found that even a small conﬁgura-tion, which is commonly used for low-resourcedtranslation (zhu et al., 2019), can still overﬁt onmulti30k.
we therefore perform grid search on theen→de validation set in multi30k and obtain atiny conﬁguration that works surprisingly well..we use adam with β1 = 0.9, β2 = 0.98 formodel optimization.
we start training with a warm-up phase (2,000 steps) where we linearly increasethe learning rate from 10−7 to 0.005. thereafter wedecay the learning rate proportional to the numberof updates.
each training batch contains at most4,096 source/target tokens.
we set label smoothingweight to 0.1, dropout to 0.3. we follow (zhanget al., 2020) to early-stop the training if validationloss does not improve for ten epochs.
we averagethe last ten checkpoints for inference as in (vaswaniet al., 2017) and (wu et al., 2018).
we perform.
2https://github.com/multi30k/dataset.
beam search with beam size set to 5. we report4-gram bleu and meteor scores for all test sets.
all models are trained and evaluated on one singlemachine with two titan p100 gpus..4.3 baselines.
our baselines can be categorized into three types:.
• the text-only transformer;• the conventional mmt models: doubly-attand imagination;• the retrieval-based mmt models: uvr-nmt..details of these methods can be found in section 2.for fairness, all the baselines are implemented byourselves based on fairseq (ott et al., 2019).
weuse top-5 retrieved images for both uvr-nmtand our rmmt.
we also consider two more recentstate-of-the-art conventional methods for reference:gmnmt (yin et al., 2020) and dccn (lin et al.,2020), whose results are reported as in their papers.
note that most mmt methods are difﬁcult (oreven impossible) to interpret.
while there existsome interpretable methods (e.g., uvr-nmt) thatcontain gated fusion layers similar to ours, theyperform sophisticated transformations on visualrepresentation before fusion, which lowers the in-terpretability of the gating matrix.
for example, inthe gated fusion layer of uvr-nmt, we observethat the visual vector is order-of-magnitude smallerthan the textual vector.
as a result, interpretinggating weight is meaningless because visual vectorhas negligible inﬂuence on the fused vector..4.4 results.
table 1 shows the bleu scores of these methodson the multi30k dataset.
from the table, we seethat although we can replicate similar bleu scoresof transformer-base as reported in (gr¨onroos et al.,2018b; ive et al., 2019), these scores (row 1) aresigniﬁcantly outperformed by transformer-smalland transformer-tiny, which have fewer parame-ters.
this shows that transformer-base could over-ﬁt the multi30k dataset.
transformer-tiny, whosenumber of parameters is about 20 times smallerthan that of transformer-base, is more robust andefﬁcient in our test cases.
we therefore use it asthe base model for all our mmt systems in thefollowing discussion..based on the transformer-tiny model, both ourproposed models (gated fusion and rmmt) andbaseline mmt models (doubly-att, imagina-tion and uvr-nmt) signiﬁcantly outperform the.
6156# model.
#params test2016 test2017 mscoco #params test2016 test2017 mscoco.
en→de.
en→fr.
123.transformer-basetransformer-smalltransformer-tiny.
4 gmnmt♠5 dccn♠6 doubly-att♠imagination♠78 uvr-nmt♦.
9 gated fusion♠10 rmmt♦.
49.1m36.5m2.6m.
4.0m17.1m3.2m7.0m2.9m.
2.9m2.9m.
text-only transformer27.5428.5029.88.
31.3632.9933.36.existing mmt systems.
28.732.226.731.033.9529.6329.9032.8932.1629.02our mmt systems29.0433.5930.0132.94.
38.3339.6841.02.
39.839.741.4541.3140.79.
41.9641.45.
49.0m36.4m2.6m.
-16.9m3.2m6.9m2.9m.
2.8m2.9m.
60.6061.3161.80.
60.961.261.9961.9061.00.
61.6962.12.
53.1653.8553.46.
53.954.353.7254.0753.20.
54.8554.39.
42.8344.0344.52.
-45.445.1644.8143.71.
44.8644.52.table 1: bleu scores on multi30k.
results in row 4 and 5 are taken from the original papers.
♠ indicates conven-tional mmt models, while ♦ refer to retrieval-based models.
without further speciﬁed, all our implementationsare based on the tiny conﬁguration..state-of-the-arts (gmnmt and dccn) on en→detranslation.
however, the improvement of all thesemethods (rows 4-10) over the base transformer-tiny model (row 3) is very marginal.
this showsthat visual context might not be as important aswe expected for translation, at least on datasets weexplored..we further evaluate all the methods on the me-teor scores (see appendix c).
we also run ex-periments on the vatex dataset (see appendix b).
similar results are observed as table 1. althoughvarious mmt systems have been proposed recently,a well-tuned model that uses text only remain com-petitive.
this motivates us to revisit the importanceof visual context for translation in mmt models..5 model analysis.
taking a closer look at the results given in the pre-vious section, we are surprised by the observationthat our models learn to ignore visual context whentranslating (sec 5.1).
this motivates us to revisitthe contribution of visual context in mmt systems(sec 5.2).
our adversarial evaluation shows thatadding model regularization achieves comparableresults as incorporating visual context.
finally, wediscuss when visual context is needed (sec 5.3) andhow these ﬁndings could beneﬁt future research..5.1 probe the need for visual context in mmt.
to explore the need for visual context in ourmodels, we focus on the interpretable compo-nent: the gated fusion layer (see equation 3 andintuitively, a larger gating weight λij indi-5).
cates the model learns to depend more on vi-.
multi30k.
en→de.
en→fr.
test2016test2017mscoco.
test2016test2017mscoco.
gated fusion rmmt.
4.5e-217.0e-179.7e-21.
1.6e-187.2e-152.3e-18.
8.6e-134.0e-133.5e-14.
1.1e-115.0e-125.3e-13.
table 2: micro-averaged gating weight λ on multi30k..sual context to perform better translation.
wequantify the degree to which visual context isused by the micro-averaged gating weight λ =(cid:80)mm=1 sum(λm)/(d × v ).
here m , v are the to-tal number of sentences and words in the corpus,respectively.
sum(·) add up all elements in a givenmatrix, and λ is a scalar value ranges from 0 to1. a larger λ implies more usage of the visualcontext..we ﬁrst study models’ behavior after conver-gence.
from table 2, we observe that λ is neg-ligibly small, suggesting that both models learnto discard visual context.
in other words, visualcontext may not be as important for translation aspreviously thought.
since λ is insensitive to out-liers (e.g., large gating weight at few dimensions),we further compute p(λij > 1e-10): percentageof gating weight entries in λ that are larger than1e-10.
with no surprise, we ﬁnd that on all testsplits p(λij > 1e-10) are always zero, which againshows that visual input is not used by the model ininference..the gated fusion’s training process also shed.
61571995) and weight decay (hanson and pratt, 1989).
the former simulates the effects of assumably un-informative visual representations and the later isa more principled way of regularization that doesnot get enough attention in the current hyperpa-rameter tuning stage.
inspecting the results, weﬁnd that applying these regularization techniquesachieves similar gains over the text-only baselineas incorporating multimodal information does..for random noise injection, we keep all hyper-parameters unchanged but replace visual featuresextracted using resnet with randomly initializedvectors, which are noise drawn from a standardgaussian distribution.
a mmt model equippedwith resnet features is denoted as a resnet-basedmodel, while the same model with random initial-ization is denoted as a noise-based model.
werun each experiment three times and report the av-eraged results.
note that values in parenthesesindicate the performance gap between the resnet-based model and its noise-based adversary..table 3 shows bleu scores on the multi30kdataset.
each column in the table correspondsto a test set “contest”.
from the table, we ob-serve that, among 18 (3 methods × 3 test sets ×2 tasks) contests with the transformer model (row1), noise-based models (rows 2-4) achieve betterperformance 13 times, while resnet-based modelswin 14 cases.
this shows that noise-based modelsperform comparably with resnet-based models.
afurther comparison between noise-based modelsand resnet-based models shows that they are com-patible after 18 contests, in which the former wins8 times and the latter wins 10 times..we observe similar results when repeating aboveevaluation using meteor (tabel 9 ) and on vatex(table 7 ).
these observations deduce that randomnoise could function as visual context.
in mmtsystems, adding random noise or visual contextcan help reduce overﬁtting (bishop et al., 1995)when translating sentences in multi30k, which areshort and repetitive (caglayan et al., 2019).
more-over, we ﬁnd that the (cid:96)2 norm of model weights inresnet-based gated fusion and noise-based gatedfusion are only 97.7% and 95.2% of that in trans-former on en→de, respectively.
this further ver-iﬁes our speculation that, as random noise injec-tion (an, 1996), visual context can help weightsmoothing and improve model generalization..further, we regularize the models with weight de-cay.
we consider three models: the text-only trans-.
(a) en→de..(b) en→fr..figure 1: training dynamic of multi30k en→de anden→fr translation, from epoch 1..some light on how the model accommodates thevisual information during training.
figure 1 (a) and(b) shows how λ changes during training, from theﬁrst epoch.
we ﬁnd that, gated fusion starts witha relatively high λ (>0.5), but quickly decreases to≈ 0.48 after the ﬁrst epoch.
as the training contin-ues, λ gradually decreases to roughly zero.
in theearly stages, the model relies heavily on images,possibly because they could provide meaningfulfeatures extracted from a pre-trained resnet-50cnn, while the textual encoder is randomly ini-tialized.
compared with text-only nmt, utilizingvisual features lowers mmt models’ trust in thehidden representations generated from the textualencoders.
as the training continues, the textual en-coder learns to represent source text better and theimportance of visual context gradually decreases.
in the end, the textual encoder carries sufﬁcientcontext for translation and supersedes the contribu-tions from the visual features.
nevertheless, thisdoesn’t explain the superior performance of themultimodal systems (table 1).
we speculate thatvisual context is acting as regularization that helpsmodel training in the early stages.
we further ex-plore this hypothesis in the next section..5.2 revisit need for visual context in mmt.
in the previous section, we hypothesize that thegains of mmt systems come from some regulariza-tion effects.
to verify our hypothesis, we conductexperiments based on two widely used regulariza-tion techniques: random noise injection (bishop,.
61580204060800.00.20.40.60.81.00510152025303540bleubleu0204060800.00.20.40.60.81.00102030405060bleubleu# model.
1 transformer2 doubly-attimagination34 gated fusion.
test2016.
41.0241.53(+0.08)41.20(-0.11)41.53(-0.45).
en→detest2017.
mscoco.
test2016.
en→frtest2017.
33.3633.90(-0.05)33.32(+0.42)33.52(-0.07).
29.8829.76(+0.15)29.92(+0.02)29.87(+0.83).
61.8061.85(-0.35)61.28(-0.62)61.58(-0.11).
53.4654.61(+0.46)53.74(-0.33)54.21(-0.64).
mscoco.
44.5244.85(-0.80)44.89(+0.08)44.88(+0.02).
table 3: bleu scores on multi30k with randomly initialized visual representation.
numbers in parenthesesindicate the relative improvement/deterioration compared with the same model with resnet feature initialization..bleu meteor.
12.
34.
56.transformer.
+weight decay 0.1.w. resnet featuresgated fusionrmmt.
w. random noisegated fusionrmmt.
11.3911.66.
14.7916.67.
11.4012.08.
35.5335.95.
40.4143.62.
35.4437.60.λ.
--.
0.0470.011.
0.0320.010.table 4: adversarial evaluation with limited textualcontext on multi30k en-de test2016..former, the representative existing mmt methoddoubly-att, and our gated fusion method.
fig-ure 2 and 3 (in appendix c) show the bleuand meteor scores of these methods on en→detranslation as weight decay rate changes, respec-tively.
we see that the best results of the text-onlytransformer model with ﬁne-tuned weight decayare comparable or even better than that of the mmtmodels doubly-att and gated fusion that utilizevisual context.
this again shows that visual contextis not as useful as we expected and it essentiallyplays the role of regularization..5.3 when is visual context needed in mmt.
despite the less importance of visual informationwe showed in previous sections, there also existworks that support its usefulness.
for example,caglayan et al.
(2019) experimentally show that,with limited textual context (e.g., masking someinput tokens), mmt models will utilize the visualinput for translation.
this further motivates us toinvestigate when visual context is needed in mmtmodels.
we conduct experiment with a new mask-ing strategy that does not need any entity linking an-notations as in caglayan et al.
(2019).
speciﬁcally,we follow tan and bansal (2020) to collect a listof visually grounded tokens.
a visually groundedtoken is the one that has more than 30 occurrencesin the multi30k dataset with stop words removed..masking all visually grounded tokens will affectaround 45% of tokens in multi30k..table 4 shows the adversarial study with visu-ally grounded tokens masked.
in particular, weselect transformer, gated fusion and rmmt asrepresentative methods.
from the table, we see thatrandom noise injection (row 5,6) and weight de-cay (row 2) can only bring marginal improvementover the text-only transformer model.
however,resnet-based models that utilize visual context sig-niﬁcantly improve the translation results.
for ex-ample, rmmt achieves almost 50% gain over thetransformer on the bleu score.
moreover, bothgated fusion and rmmt using resnet featureslead to a larger λ value than that when textual con-text is sufﬁcient as shown in table 2. those resultsfurther suggest that visual context is needed whentextual context is insufﬁcient.
in addition to tokenmasking, sentences with incorrect, ambiguous andgender-neutral words (frank et al., 2018) mightalso need visual context to help translation.
there-fore, to fully exert the power of mmt systems, weemphasize the need for a new mmt benchmark,in which visual context is deemed necessary togenerate correct translation..interestingly, even with resnet features, we ob-serve a signiﬁcant drop in both bleu and me-teor scores compared with those in table 1 and8, similar to that reported in (chowdhury and el-liott, 2019).
the reason could be two-fold.
on theone hand, there are many words that can not be visu-alized.
for example, in table 5 (a), although gatedfusion can successfully identify the main objectsin the image (“little boys pose with a puppy”), itfails to generate the more abstract concept “familypicture”.
on the other hand, when translating differ-ent words, it is difﬁcult to capture correct regionsin images.
for example, in table 5 (b), we seethat gated fusion incorrectly generates the wordfrauen (women) because it captures the woman atthe top-right corner of the image..6159(a) test2016..(b) test2017..(c) mscoco..figure 2: bleu score curves on en→de translation with different weight decay rate..src:nmt:.
mmt:.
ref:.
src:nmt:.
mmt:.
ref:.
two young boys pose with a puppy for a family picturezwei braune hunde spielen mit einem spielzeug f¨ur einen tennisball(two brown dogs play with a toy for a tennis ball)zwei kleine jungen posieren mit einem welpen f¨ur ein foto(two little boys pose with a puppy for a photo)zwei kleine jungen posieren mit einem welpen f¨ur eine familienfoto(two little boys pose with a puppy for a family photo)two men sitting in a restaurantzwei kinder spielen in einem springbrunnen(two children are playing in a fountain)zwei frauen sitzen in einem restaurant(two women are sitting in a restaurant)zwei m¨anner sitzen in einem restaurant(two men are sitting in a restaurant).
(a).
(b).
table 5: case studies under limited textual input.
we use underline to denote masked tokens, and strikethrough(bold) font to denote incorrect (correct) lexical choices.
we use gated fusion for analysis..5.4 discussion.
finally, we discuss how our ﬁndings might bene-ﬁt future mmt research.
first, a benchmark thatrequires more visual information than multi30kto solve is desired.
as shown in section 5.2, sen-tences in multi30k are rather simple and easy-to-understand.
thus textual context could providesufﬁcient information for correct translation, mak-ing visual modules relatively redundant in thesesystems.
while the mscoco test set in multi30kcontains ambiguous verbs and encourages modelsto use image sources for disambiguation, we stilllack a corresponding training set..second, our methods can serve as a veriﬁca-tion tool to investigate whether visual grounding isneeded in translation for a new benchmark..third, we ﬁnd that visual feature selection isalso critical for mmt’s performance.
while mostmethods employ the attention mechanism to learnto attend relevant regions in an image, the shortageof annotated data could impair the attention mod-ule (see table 5 (b)).
some recent efforts (yin.
et al., 2020; lin et al., 2020; caglayan et al.,2020) address the issue by feeding models with pre-extracted visual objects instead of the whole image.
however, these methods are easily affected by thequality of the extracted objects.
therefore, a moreeffective end-to-end visual feature selection tech-nique is needed, which can be further integratedinto mmt systems to improve performance..6 conclusion.
in this paper we devise two interpretable mod-els that exhibit state-of-the-art performance on thewidely adopted mmt datasets — multi30k and thenew video-based dataset — vatex.
our analysison the proposed models, as well as on other ex-isting mmt systems, suggests that visual contexthelps mmt in the similar vein as regularizationmethods (e.g., weight decay), under sufﬁcient tex-tual context.
those empirical ﬁndings, however,should not be understood as us downplaying theimportance existing datasets and models; we be-lieve that sophisticated mmt models are necessary.
616000.10.010.0010.0001weight decay rate40.5040.7541.0041.2541.5041.7542.0042.2542.50bleutransformerdoubly-attgated fusion00.10.010.0010.0001weight decay rate32.032.533.033.534.034.5bleutransformerdoubly-attgated fusion00.10.010.0010.0001weight decay rate28.5028.7529.0029.2529.5029.7530.0030.2530.50bleutransformerdoubly-attgated fusionfor effective grounding of visual context into trans-lation.
our goal, rather, is to (1) provide additionalclarity on the remaining shortcomings of currentdataset and stress the need for new datasets to movethe ﬁeld forward; (2) emphasise the importance ofinterpretability in mmt research..7 acknowledgement.
zhiyong wu is partially supported by a researchgrant from the hku-tcl joint research centrefor artiﬁcial intelligence..references.
guozhong an.
1996. the effects of adding noise dur-ing backpropagation training on a generalization per-formance.
neural computation, 8(3):643–674..hasan sait arslan, mark fishel, and gholamreza an-barjafari.
2018. doubly attentive transformer ma-chine translation.
arxiv preprint arxiv:1807.11605..satanjeev banerjee and alon lavie.
2005. meteor: anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
in proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, pages 65–72..lo¨ıc barrault, fethi bougares, lucia specia, chiraaglala, desmond elliott, and stella frank.
2018. find-ings of the third shared task on multimodal machinetranslation.
in wmt18..chris m bishop.
1995. training with noise is equiva-lent to tikhonov regularization.
neural computation,7(1):108–116..christopher m bishop et al.
1995. neural networks for.
pattern recognition.
oxford university press..ozan caglayan,.
julia ive, veneta haralampieva,pranava madhyastha, lo¨ıc barrault, and lucia spe-cia.
2020. simultaneous machine translation with vi-sual context.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 2350–2361, online.
associa-tion for computational linguistics..ozan caglayan, pranava madhyastha, lucia specia,and lo¨ıc barrault.
2019. probing the need for vi-sual context in multimodal machine translation.
innaacl-hlt (1)..iacer calixto, desmond elliott, and stella frank.
2016.dcu-uva multimodal mt system report.
in proceed-ings of the first conference on machine translation:volume 2, shared task papers, pages 634–638..iacer calixto and qun liu.
2017. sentence-level multi-lingual multi-modal embedding for natural language.
processing.
in proceedings of the international con-ference recent advances in natural language pro-cessing, ranlp 2017, pages 139–148, varna, bul-garia.
incoma ltd..iacer calixto, qun liu, and nick campbell.
2017.doubly-attentive decoder for multi-modal neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1913–1924, vancouver, canada.
association for computa-tional linguistics..koel dutta chowdhury and desmond elliott.
2019.understanding the effect of textual adversaries inin proceedingsmultimodal machine translation.
of the beyond vision and language: integratingreal-world knowledge (lantern), pages 35–40..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..jesse dodge, suchin gururangan, dallas card, royschwartz, and noah a. smith.
2019. show yourwork: improved reporting of experimental results.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2185–2194, hong kong, china.
association for computa-tional linguistics..desmond elliott.
2018. adversarial evaluation of mul-timodal machine translation.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 2974–2978..desmond elliott, stella frank, lo¨ıc barrault, fethibougares, and lucia specia.
2017. findings of thesecond shared task on multimodal machine transla-tion and multilingual image description.
in proceed-ings of the second conference on machine transla-tion, pages 215–233..desmond elliott, stella frank, and eva hasler.
2015.multilingual image description with neural sequencemodels.
arxiv preprint arxiv:1510.04709..desmond elliott, stella frank, khalil sima’an, and lu-cia specia.
2016. multi30k: multilingual english-in proceedings of thegerman image descriptions.
5th workshop on vision and language, pages 70–74..desmond elliott and ´akos k´ad´ar.
2017. imaginationimproves multimodal translation.
in proceedings ofthe eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 130–141..6161stella frank, desmond elliott, and lucia specia.
2018.assessing multilingual multimodal image descrip-tion: studies of native speaker preferences andtranslator choices.
natural language engineering,24(3):393–413..stig-arne gr¨onroos, benoit huet, mikko kurimo,jorma laaksonen, bernard merialdo, phu pham,mats sj¨oberg, umut sulubacak, j¨org tiedemann,raphael troncy, and ra´ul v´azquez.
2018a.
thememad submission to the wmt18 multimodaltranslation task.
in proceedings of the third confer-ence on machine translation: shared task papers,pages 603–611, belgium, brussels.
association forcomputational linguistics..stig-arne gr¨onroos, benoit huet, mikko kurimo,jorma laaksonen, bernard merialdo, phu pham,mats sj¨oberg, umut sulubacak, j¨org tiedemann,raphael troncy, et al.
2018b.
the memad sub-mission to the wmt18 multimodal translation task.
arxiv..jiatao gu, yong wang, kyunghyun cho, and vic-tor ok li.
2018. search engine guided neural ma-chine translation.
in aaai, pages 5133–5140..kelvin guu, kenton lee, zora tung, panupong pasu-pat, and ming-wei chang.
2020. realm: retrieval-augmented language model pre-training.
icml..stephen jos´e hanson and lorien y pratt.
1989. com-paring biases for minimal network construction within advances in neural informa-back-propagation.
tion processing systems, pages 177–185..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..jindˇrich helcl, jindˇrich libovick`y, and duˇsan variˇs.
2018. cuni system for the wmt18 multimodal trans-lation task.
arxiv preprint arxiv:1811.04697..jack hessel and lillian lee.
2020. does my multi-modal model learn cross-modal interactions?
it’sin proceed-harder to tell than you might think!
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages861–877..po-yao huang, frederick liu, sz-rung shiang, jeanoh, and chris dyer.
2016. attention-based multi-in proceedingsmodal neural machine translation.
of the first conference on machine translation: vol-ume 2, shared task papers, pages 639–645, berlin,germany.
association for computational linguis-tics..julia ive, pranava madhyastha, and lucia specia.
2019.distilling translations with visual awareness.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 6525–6538, florence, italy.
association for computa-tional linguistics..allan jabri, armand joulin,.
and laurens vander maaten.
2016. revisiting visual question an-swering baselines.
in european conference on com-puter vision, pages 727–739.
springer..urvashi khandelwal, omer levy, dan jurafsky, lukezettlemoyer, and mike lewis.
2019. generalizationthrough memorization: nearest neighbor languagein international conference on learningmodels.
representations..jan kukaˇcka, vladimir golkov, and daniel cremers.
2017. regularization for deep learning: a taxon-omy.
arxiv preprint arxiv:1710.10686..patrick lewis, ethan perez, aleksandara piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich k¨uttler, mike lewis, wen-tau yih, timrockt¨aschel, et al.
2020. retrieval-augmented gen-eration for knowledge-intensive nlp tasks.
34th con-ference on neural information processing systems(neurips 2020),..jindˇrich libovick´y and jindˇrich helcl.
2017. attentionstrategies for multi-source sequence-to-sequencelearning.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 2: short papers), pages 196–202, vancou-ver, canada.
association for computational linguis-tics..jindˇrich libovick´y, jindˇrich helcl, and david mareˇcek.
2018. input combination strategies for multi-sourcein proceedings of the thirdtransformer decoder.
conference on machine translation: research pa-pers, pages 253–260, belgium, brussels.
associa-tion for computational linguistics..huan lin, fandong meng, jinsong su, yongjing yin,zhengyuan yang, yubin ge, jie zhou, and jieboluo.
2020. dynamic context-guided capsule net-in pro-work for multimodal machine translation.
ceedings of the 28th acm international conferenceon multimedia, pages 1320–1329..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..bryan a plummer, liwei wang, chris m cervantes,juan c caicedo, julia hockenmaier, and svetlanalazebnik.
2015.flickr30k entities: collectingregion-to-phrase correspondences for richer image-in proceedings of the ieeeto-sentence models.
international conference on computer vision, pages2641–2649..justin salamon and juan pablo bello.
2017. deep con-volutional neural networks and data augmentationfor environmental sound classiﬁcation.
ieee signalprocessing letters, 24(3):279–283..6162rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725..mingyang zhou, runxiang cheng, yong jae lee, andzhou yu.
2018. a visual attention grounding neuralmodel for multimodal machine translation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 3643–3653..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2019.incorporating bert into neural machine translation.
in international conference on learning represen-tations..lucia specia, stella frank, khalil sima’an, anddesmond elliott.
2016. a shared task on multi-modal machine translation and crosslingual imagedescription.
in proceedings of the first conferenceon machine translation: volume 2, shared task pa-pers, pages 543–553..hao tan and mohit bansal.
2020. vokenization: im-proving language understanding via contextualized,in proceedings ofvisually-grounded supervision.
the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 2066–2080..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..xin wang, jiawei wu, junkun chen, lei li, yuan-fang wang, and william yang wang.
2019. vatex:a large-scale, high-quality multilingual dataset forvideo-and-language research.
in the ieee interna-tional conference on computer vision (iccv)..jason weston, emily dinan, and alexander miller.
2018. retrieve and reﬁne: improved sequence gen-eration models for dialogue.
in proceedings of the2018 emnlp workshop scai: the 2nd interna-tional workshop on search-oriented conversationalai, pages 87–92..felix wu, angela fan, alexei baevski, yann dauphin,and michael auli.
2018. pay less attention within interna-lightweight and dynamic convolutions.
tional conference on learning representations..shaowei yao and xiaojun wan.
2020. multimodaltransformer for multimodal machine translation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4346–4350..yongjing yin, fandong meng, jinsong su, chulunzhou, zhengyuan yang, jie zhou, and jiebo luo.
2020. a novel graph-based multi-modal fusion en-in proceed-coder for neural machine translation.
ings of the 58th annual meeting of the associationfor computational linguistics, pages 3025–3035..zhuosheng zhang, kehai chen, rui wang, masaoutiyama, eiichiro sumita, zuchao li, and hai zhao.
2020. neural machine translation with universal vi-sual representation.
in international conference onlearning representations..6163a training settings.
table 6 shows the conﬁguration of different modelsizes..model componentnumber of encoder/decoder layersinput/output layer dimensioninner feed-forward layer dimensionnumber of attention heads.
base651220488.small tiny.
651210244.
41282564.table 6: model conﬁgurations for base, small, andtiny..b results on vatex.
vatex is a video-based mmt corpus that con-tains 129,955 english-chinese sentence pairs fortraining, 15,000 sentence pairs for validation, and30,000 sentence pairs for testing.
each pair of sen-tences is associated with a video clip.
since thetesting set is not publicly available, we use half ofthe validation set for validating and the other halffor testing.
we apply the byte pair encoding al-gorithm on the lower-cased english sentences andsplit chinese sentences into sequences of charac-ters, resulting in a vocabulary of 17,216 englishtokens and 3,384 chinese tokens.
we use the videofeatures provided along with the vatex dataset, inwhich each video is represented as rk∗1024, wherek is the number of segments.
since some mmtsystems take a “global” visual feature as input, weuse 3d-max-pooling to extract the pooled repre-sentation r1024 for each video..model.
transformer.
+weight decay 0.1+weight decay 0.01+weight decay 0.001.bleu.
meteor.
35.8236.3236.0735.92.
59.0259.3859.1459.22.doubly-attimaginationgated fusionrmmt.
36.05 (35.46)36.25 (36.10)36.06 (36.01)36.35 (36.43).
59.26 (58.84)59.26 (59.15)59.34 (59.33)59.44 (59.57).
table 7: results on vatex en-zh translation.
numbersin parentheses are the performance of the same modelwith random noise initialization..the results are shown in table 7. we observethat although most mmt systems show improve-ment over the transformer baseline, the gains arequite marginal.
indicating that although image-based mmt models can be directly applied to.
video-based mmt, there is still room for improve-ment due to the challenge of video understand-ing.
we also note that (a) regularize the text-onlytransformer with weight decay demonstrates sim-ilar gains as injecting video information into themodels; (b) replacing video features with randomnoise replicate comparable performance, which fur-ther supports our ﬁndings in section 5.2..c results on meteor.
we also report ourresults based on me-teor (banerjee and lavie, 2005), which con-sistently demonstrates higher correlation with hu-man judgments than bleu does in independentevaluations such as in emnlp wmt 2011 3.from table 8, we can see that on en-fr transla-tion, mmt systems demonstrate similar improve-ments over text-only baselines in both meteorand bleu(see table 1).
on en-de translation,however, mmt systems are mostly on-par withtransformer-tiny on meteor and do not showconsistent gains as bleu.
we hypothesis the rea-son being that en-de sets are created in a image-blind fashion, in which the crowd-sourcing work-ers produce translations without seeing the im-ages (frank et al., 2018).
such that source sen-tence can already provide sufﬁcient context fortranslation.
when creating the en-fr corpus, theimage-blind issue is ﬁxed (elliott et al., 2017), thusimages are perceived as “needed” in the translationfor whatever reason.
although bleu is unableto elicit this difference, evaluation based on me-teor captured it and conﬁrmed previous research.
we also compute meteor scores for our exper-iments that regularize models with random noise(see table 9) and weight decay (see figure 3).
theresults are consistent with those evaluated usingbleu and further complement our early ﬁndings..d results on iwslt’14.
we also evaluate the retrieval-based model rmmton text-only corpus — iwslt’14.
the iwslt’14dataset contains 160k bilingual sentence pairs foren-de translation task.
following the commonpractice, we lowercase all words, split 7k sentencepairs from the training dataset for validation andconcatenate dev2010, dev2012, tst2010, tst2011,tst2012 as the test set.
the number of bpe opera-tions is set to 20,000. we use the small conﬁgura-tion in all our experiments.
the dropout and label.
3http://statmt.org/wmt11/papers.html.
6164# model.
#params test2016 test2017 mscoco #params test2016 test2017 mscoco.
en→de.
en→fr.
123.transformer-basetransformer-smalltransformer-tiny.
4 gmnmt♠5 dccn♠6 doubly-att♠imagination♠7.
9 gated fusion♠10 rmmt♦.
49.1m36.5m2.6m.
4.0m17.1m3.2m7.0m.
2.9m2.9m.
text-only transformer54.7355.9556.64.
60.0260.8062.05.existing mmt systems.
47.651.945.749.956.2161.8361.2956.57our mmt systems56.1561.9456.3361.71.
65.9266.0168.22.
57.656.868.0468.06.
67.8467.97.
49.0m36.4m2.6m.
-16.9m3.2m6.9m.
2.8m2.9m.
80.0980.7181.02.
74.976.481.1281.2.
80.9781.29.
74.9375.7475.62.
69.370.375.7176.03.
76.3476.09.
68.5769.1069.43.
-65.070.2570.35.
70.5170.24.table 8: meteor scores on multi30k.
results in row 4 and 5 are taken from the original papers.
♠ indicatesconventional mmt models, while ♦ refers to retrieval-based models.
without further speciﬁcation, all our imple-mentations are based on the tiny conﬁguration..# model.
test2016.
mscoco.
test2016.
en→detest2017.
en→frtest2017.
1 transformer68.222 doubly-att68.39(+0.35)imagination367.93(-0.13)4 gated fusion 68.25(+0.41).
62.0561.83(+0.0)61.84(+0.55)61.5(-0.44).
56.6456.46(+0.25)56.49(-0.08)55.93(-0.22).
81.0281.27(+0.15)80.75(-0.45)81.22(+0.25).
75.6276.22(+0.51)76.57(+0.54)76.01(-0.33).
mscoco.
69.4370.21(-0.04)69.88(-0.47)70.33(-0.18).
table 9: meteor scores on multi30k with randomly initialized visual representation.
numbers in parenthesesindicate the relative improvement/deterioration compared with the original model with resnet features..(a) test2016..(b) test2017..(c) mscoco..figure 3: meteor score curves on en→de translation with different weight decay rate..616500.10.010.0010.0001weight decay rate67.0067.2567.5067.7568.0068.2568.5068.7569.00meteortransformerdoubly-attgated fusion00.10.010.0010.0001weight decay rate61.0061.2561.5061.7562.0062.2562.5062.7563.00meteortransformerdoubly-attgated fusion00.10.010.0010.0001weight decay rate55.0055.2555.5055.7556.0056.2556.5056.7557.00meteortransformerdoubly-attgated fusionmodel.
transformer-small.
+weight decay 0.0001.rmmt-small.
bleu.
28.6229.1429.03.table 10: bleu score on iwslt’14 en→de transla-tion..smoothing rate are set to 0.3 and 0.1, respectively.
since there is no images associated with iwslt,we follow (zhang et al., 2020) and retrieve top-5images from multi30k corpus..from table 10, we see that transformer with-out weight decay is marginally outperformed byrmmt, but achieves slightly higher bleu scoreswhen trained with a 0.0001 weight decay.
ourdiscussion in section 5.2 sheds light on whyvisual context is helpful on non-grounded low-resourced datasets like iwslt’14 — for low-resourced dataset like iwslt’14, injecting visualcontext help regularize model training and avoidoverﬁtting..6166