dynasent: a dynamic benchmark for sentiment analysis.
christopher potts∗stanford universitycgpotts@stanford.edu.
zhengxuan wu∗stanford universitywuzhengx@stanford.edu.
atticus geigerstanford universityatticusg@stanford.edu.
douwe kielafacebook ai researchdkiela@fb.com.
abstract.
we introduce dynasent(‘dynamic senti-ment’), a new english-language benchmarktask for ternary (positive/negative/neutral) sen-timent analysis.
dynasent combines natu-rally occurring sentences with sentences cre-ated using the open-source dynabench plat-form, which facilities human-and-model-in-the-loop dataset creation.
dynasent has a totalof 121,634 sentences, each validated by ﬁvecrowdworkers, and its development and testsplits are designed to produce chance perfor-mance for even the best models we have beenable to develop; when future models solve thistask, we will use them to create dynasent ver-sion 2, continuing the dynamic evolution ofthis benchmark.
here, we report on the datasetcreation effort, focusing on the steps we tookto increase quality and reduce artifacts.
wealso present evidence that dynasent’s neutralcategory is more coherent than the compara-ble category in other benchmarks, and we mo-tivate training models from scratch for eachround over successive ﬁne-tuning..1.introduction.
sentiment analysis is an early success story fornlp, in both a technical and an industrial sense.
it has, however, entered into a more challengingphase for research and technology development:while present-day models achieve outstanding re-sults on all available benchmark tasks, they stillfall short when deployed as part of real-world sys-tems (burn-murdoch, 2013; grimes, 2014, 2017;gossett, 2020) and display a range of clear short-comings (kiritchenko and mohammad, 2018; han-wen shen et al., 2018; wallace et al., 2019; tsaiet al., 2019; jin et al., 2019; zhang et al., 2020)..in this paper, we seek to address the gap betweenbenchmark results and actual utility by introduc-.
model 0roberta ﬁne-tuned onsentiment benchmarks.
model 0 used to ﬁndchallenging naturallyoccurring sentences.
round 1 dataset.
human validation.
model 1roberta ﬁne-tuned onsentiment benchmarks+ round 1 dataset.
dynabench used tocrowdsource sentencesthat fool model 1.round 2 dataset.
human validation.
figure 1: the dynasent dataset creation process.
thehuman validation task is the same for both rounds; ﬁveresponses are obtained for each sentence.
on dyn-abench, we explore conditions with and without promptsentences that workers can edit to achieve their goal..ing version 1 of the dynasent dataset for english-language ternary (positive/negative/neutral) senti-ment analysis.1 dynasent is intended to be a dy-namic benchmark that expands in response to newmodels, new modeling goals, and new adversarialattacks.
we present the ﬁrst two rounds here andmotivate some speciﬁc data collection and mod-eling choices, and we propose that, when futuremodels solve these rounds, we use those modelsto create additional dynasent rounds.
this is aninstance of “the ‘moving post’ dynamic target” fornlp that nie et al.
(2020) envision..figure 1 summarizes our method, which incor-porates both naturally occurring sentences and sen-tences created by crowdworkers with the goal offooling a top-performing model.
the starting pointis model 0, which is trained on standard sentiment.
∗equal contribution..1https://github.com/cgpotts/dynasent.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2388–2404august1–6,2021.©2021associationforcomputationallinguistics2388benchmarks and used to ﬁnd challenging sentencesin existing data.
these sentences are fed into a hu-man validation task, leading to the round 1 dataset.
next, we train model 1 on round 1 in addition topublicly available datasets.
in round 2, this modelruns live on the dynabench platform for human-and-model-in-the-loop dataset creation;2 crowd-workers try to construct examples that fool model 1.these examples are human-validated, which resultsin the round 2 dataset.
taken together, rounds 1and 2 have 121,634 sentences, each with ﬁve hu-man validation labels.
thus, with only two roundscollected, dynasent is already a substantial newresource for sentiment analysis..in addition to contributing dynasent, we seekto address a pressing concern for any dataset col-lection method in which workers are asked to con-struct original sentences: human creativity has in-trinsic limits.
individual workers will happen uponspeciﬁc strategies and repeat them, and this willlead to dataset artifacts.
these artifacts will cer-tainly reduce the value of the dataset, and they arelikely to perpetuate and amplify social biases..we explore two methods for mitigating thesedangers.
first, by harvesting naturally occurringexamples for round 1, we tap into a wider popula-tion than we can via crowdsourcing, and we bringin sentences that were created for naturalistic rea-sons, rather than the more artiﬁcial goals presentduring crowdsourcing.
second, for the dynabenchcases created in round 2, we employ a ‘prompt’setting, in which crowdworkers are asked to modifya naturally occurring example rather than writingone from scratch.
we compare these sentenceswith those created without a prompt, and we ﬁndthat the prompt-derived sentences are more likenaturally occurring sentences in length and lexicaldiversity.
of course, fundamental sources of biasremain – we seek to identify these in the datasheet(gebru et al., 2018) distributed with our dataset –but we argue that these steps help, and can informcrowdsourcing efforts in general..as noted above, dynasent presently uses thelabels positive, negative, and neutral.
this isa minimal expansion of the usual binary (posi-tive/negative) sentiment task, but a crucial one,as it avoids the false presupposition that all textsconvey binary sentiment.
we chose this versionof the problem to show that even basic sentimentanalysis poses substantial challenges for our ﬁeld..2https://dynabench.org/.
we ﬁnd that the neutral category is especially dif-ﬁcult.
while it is common to synthesize such acategory from middle-scale product and servicereviews, we use an independent validation of thestanford sentiment treebank (socher et al., 2013)dev set to argue that this tends to blur neutralitytogether with mixed sentiment and uncertain senti-ment (section 5.2).
dynasent can help tease thesephenomena apart, since it already has a large num-ber of neutral examples and a large number ofexamples displaying substantial variation in valida-tion.
finally, we argue that the variable nature ofthe neutral category is an obstacle to ﬁne-tuning(section 5.3), which favors our strategy of trainingmodels from scratch for each round..2 related work.
sentiment analysis was one of the ﬁrst natural lan-guage understanding tasks to be revolutionized bydata-driven methods.
rather than trying to surveythe ﬁeld (see pang and lee 2008; liu 2012; grimes2014), we focus on the benchmark tasks that haveemerged in this space, and then seek to situate thesebenchmarks with respect to challenge (adversarial)datasets and crowdsourcing methods..2.1 sentiment benchmarks.
many sentiment datasets are derived from customerreviews of products and services (pang and lee,2004, 2005; socher et al., 2013; maas et al., 2011;jindal and liu, 2008; ni et al., 2019; mcauleyet al., 2012; zhang et al., 2015).
this is an appeal-ing source of data, since such texts are accessibleand abundant in many languages and regions of theworld, and they tend to come with their own author-provided labels (star ratings).
on the other hand,over-reliance on such texts is likely also limitingprogress; dynasent begins moving away from suchtexts, though it remains rooted in this domain..not all sentiment benchmarks are based in re-view texts.
the mpqa opinion corpus of wiebeet al.
(2005) contains news articles labeled at thephrase-level for a variety of subjective states; itpresents an exciting vision for how sentiment anal-ysis might become more multidimensional.
se-meval 2016 and 2017 (nakov et al., 2016; rosen-thal et al., 2017) offered twitter-based sentimentdatasets.
and of course there are numerous addi-tional datasets for speciﬁc languages, domains, andemotional dimensions; google’s dataset searchcurrently reports over 100 datasets for sentiment..23892.2 challenge and adversarial datasets.
challenge and adversarial datasets (winograd,1972; levesque, 2013) have risen to prominencein response to the sense that benchmark resultsare over-stating the quality of the models we aredeveloping (linzen, 2020).
these efforts seek todetermine whether models have met speciﬁc learn-ing targets (alzantot et al., 2018; glockner et al.,2018; naik et al., 2018; nie et al., 2019), exploit rel-atively superﬁcial properties of their training data,(jia and liang, 2017; kaushik and lipton, 2018;zhang et al., 2020), or inherit social biases in thedata they were trained on (kiritchenko and moham-mad, 2018; rudinger et al., 2017, 2018; sap et al.,2019; schuster et al., 2019)..for the most part, challenge and adversarialdatasets are meant to be used primarily for eval-uation (though liu et al.
(2019a) show that evensmall amounts of training on them can be fruitfulin some scenarios).
however, there are existing ad-versarial datasets that are large enough to supportfull-scale training efforts (zellers et al., 2018, 2019;chen et al., 2019; dua et al., 2019; bartolo et al.,2020).
dynasent falls into this class; it has largetrain sets that can support from-scratch training aswell as ﬁne-tuning.
our approach is closest to, anddirectly inspired by, the adversarial nli (anli)project, which is reported on by nie et al.
(2020)and which continues on dynabench.
in anli, hu-man annotators construct new examples that foola top-performing model but make sense to otherhuman annotators.
this is an iterative process thatallows the annotation project itself to organicallyﬁnd phenomena that fool current models.
the re-sulting dataset has, by far, the largest gap betweenestimated human performance and model accuracyof any benchmark in the ﬁeld right now.
we hopedynasent follows a similar pattern, and that itsnaturally occurring sentences and prompt-derivedsentences bring beneﬁcial diversity..2.3 crowdsourcing methods.
within nlp, snow et al.
(2008) helped establishcrowdsourcing as a viable method for collectingdata for at least some core language tasks.
sincethen, it has become the dominant mode for datasetcreation throughout all of ai, and the scientiﬁcstudy of these methods has in turn grown rapidly.
for our purposes, a few core ﬁndings from researchinto crowdsourcing are centrally important..first, crowdworkers are not fully representative.
of the general population (hube et al., 2019), andany crowdsourcing project will reach only a smallpopulation of workers (gadiraju et al., 2017).
thisnarrowness seems to be an underlying cause ofmany of the artifacts that have been identiﬁed inprominent nlu benchmarks (poliak et al., 2018;gururangan et al., 2018; tsuchiya, 2018; belinkovet al., 2019).
dynasent’s naturally occurring sen-tences and prompt sentences can help, but we ac-knowledge that those texts come from people whowrite online reviews, which is also a special group.
second, as with all work, quality varies acrossworkers and examples, which raises the question ofhow best to infer individual labels from responsedistributions.
dawid and skene (1979) is an earlycontribution to this problem leveraging expecta-tion maximization (dempster et al., 1977).
muchsubsequent work has pursued similar strategies; fora full review, see zheng et al.
2017. our corpusrelease uses the true majority (3/5 labels) as thegold label where such a majority exists, leavingexamples unlabeled otherwise, but we include thefull response distributions in our corpus releaseand make use of those distributions when trainingmodel 1. for additional details, see section 3.3..3 round 1: naturally occurring.
sentences.
we now begin to describe our method for construct-ing dynasent (figure 1).
the current section fo-cuses on model 0 and round 1, and section 4explains how these feed into model 1 and round 2..3.1 model 0.our model 0 begins with the roberta-base pa-rameters (liu et al., 2019b) and adds a three-waysentiment classiﬁer head.
the model was trainedon a number of publicly-available datasets, as sum-marized in table 2. see appendix a for detailson these datasets and how we processed them forour ternary task.
we evaluate this and subsequentmodels on three datasets (table 1): sst-3 dev andtest, and the assessment portion of the yelp andamazon datasets from zhang et al.
2015. for yelpand amazon, the original distribution containedonly (very large) test ﬁles.
we split them in half(by line number) to create dev and test splits..in table 3, we summarize our model 0 assess-ments on these datasets.
across the board, ourmodel does extremely well on the positive and neg-ative categories, and less well on neutral.
we trace.
2390sst-3.
yelp.
dev.
test.
dev.
test.
amazondev.
test.
3.3 validation.
posnegneu.
444428228.
909912 10,2225,201389.
9,577 10,423 130,631 129,3699,778 129,108 130,89264,73965,2614,799.total 1,100 2,210 25,000 25,000 325,000 325,000.table 1: external assessment datasets..cr.
imdb.
sst-3.
yelp amazon.
posnegneutotal.
2,4051,36603,771.
12,50012,500025,000.
42,67234,94481,658159,274.
260k260k130k650k.
1.2m1.2m600k3m.
table 2: model 0 training data..this to the fact that the neutral categories for allthese corpora were derived from three-star reviews,which actually mix a lot of different phenomena:neutrality, mixed sentiment, and (in the case of thereader judgments in sst) uncertainty about theauthor’s intentions.
we return to this issue in sec-tion 5.2, arguing that dynasent marks progress oncreating a more coherent neutral category..finally, table 3 includes results for our round 1dataset, as we are deﬁning it.
performance is at-chance across the board by construction (see sec-tion 3.4 below).
we include these columns to helpwith tracking the progress we make with model 1.we also report performance of this model on ourround 2 dataset (described below in section 4),again to help with tracking progress and under-standing the two rounds..3.2 harvesting sentences.
our ﬁrst round of data collection focused on ﬁndingnaturally occurring sentences that would challengeour model 0. to do this, we harvested sentencesfrom the yelp academic dataset, using the versionof the dataset that contains 8,021,122 reviews.3the sampling process was designed so that 50%of the sentences fell into two groups: those thatoccurred in 1-star reviews but were predicted bymodel 0 to be positive, and those that occurredin 5-star reviews but were predicted by model 0to be negative.
the intuition here is that thesewould likely be examples that fooled our model.
ofcourse, negative reviews can (and often do) containpositive sentences, and vice-versa.
this motivatesthe validation stage that we describe next..3https://www.yelp.com/dataset.
our validation task was conducted on mechanicalturk.
workers were shown ten sentences and askedto label them according to the categories positive,negative, neutral, and mixed.
see appendix bfor the full interface, including glosses for the cate-gories and the task instructions..for this round, 1,978 workers participated inthe validation process.
in the ﬁnal version of thecorpus, each sentence is validated by ﬁve differ-ent workers.
to obtain these ratings, we employedan iterative strategy.
sentences were uploaded inbatches of 3–5k and, after each round, we mea-sured each worker’s rate of agreement with themajority.
we then removed from the potential poolthose workers who disagreed more than 80% of thetime with their co-annotators, using a method of‘unqualifying’ workers that does not involve reject-ing their work or blocking them (turk, 2017).
wethen obtained additional labels for examples thatthose ‘unqualiﬁed’ workers annotated.
the ﬁnalversion of dynasent keeps only the responses fromthe highest-rated workers.
this led to a substan-tial increase in dataset quality by removing a lot oflabels that seemed to us to be randomly assigned.
appendix b describes the process in more detail,and our datasheet enumerates the known unwantedbiases that this process can introduce..3.4 round 1 dataset.
the round 1 dataset is summarized in table 5, andtable 4 gives randomly selected short examples.
because each sentence has ﬁve ratings, there aretwo perspectives we can take on the dataset:.
distributional labels we can repeat each exam-ple with each of its labels (de marneffe et al., 2012;pavlick and kwiatkowski, 2019).
for instance, theﬁrst sentence in table 4 would be repeated threetimes with ‘mixed’ as the label and twice with‘negative’.
for many classiﬁer models, this re-duces to labeling each example with its probabilitydistribution over the labels.
this is an appealingapproach to creating training data, since it allowsus to make use of all the examples,4 even those thatdo not have a majority label, and it allows us tomake maximal use of the labeling information.
inour experiments, we found that training on the dis-tributional labels consistently led to slightly better.
4for ‘mixed’ labels, we create two copies of the example,.
one labeled ‘positive’, the other ‘negative’..2391sst-3dev test.
85.184.145.471.5.
89.084.143.572.2.yelpdev test.
88.388.858.278.4.
90.589.159.479.7.amazondev test.
89.186.653.976.5.
89.486.653.776.6.round 1dev test.
33.333.333.333.3.
33.333.333.333.3.round 2dev test.
58.461.038.452.6.
63.063.144.356.8.positivenegativeneutralmacro avg.
table 3: model 0 performance (f1 scores) on external assessment datasets (table 1).
we also report on ourround 1 dataset (section 3.4), where performance is at chance by construction, and we report on our round 2dataset (section 4) to further quantify the challenging nature of that dataset..sentence.
model 0.responses.
good food nasty attitude by hostesses .
not much of a cocktail menu that i saw.
i scheduled the work for 3 weeks later.
i was very mistaken, it was much more!.
it is a gimmick, but when in rome, i get it.
probably a little pricey for lunch.
but this is strictly just my opinion.
the price was okay, not too pricey..the only downside was service was a little slow.
however there is a 2 hr seating time limit.
with alex, i never got that feeling.
its ran very well by management..negnegnegneg.
neuneuneuneu.
pospospospos.
mix, mix, mix, neg, negneg, neg, neg, neg, negneu, neu, neu, neu, posneg, pos, pos, pos, pos.
mix, mix, mix, neu, neumix, neg, neg, neg, negneu, neu, neu, neu, posmix, neu, pos, pos, pos.
mix, mix, mix, neg, negmix, neg, neg, neg, neuneu, neu, neu, neu, pospos, pos, pos, pos, pos.
table 4: round 1 train set examples, randomly selected from each combination of model 0 prediction and majoritylabel, but limited to examples with 30–50 characters..disttrain.
majority labeltrain dev test.
positivenegativeneutralmixedno majoritytotal.
130,04586,486215,93539,829–472,295.
21,391 1,200 1,20014,021 1,200 1,20045,076 1,200 1,20003,90010,071094,459 3,600 3,600.
00.table 5: round 1 dataset..models, suggesting that annotator disagreement isstable and informative..majority label we can take a more traditionalroute and infer a label based on the distribution oflabels.
in table 5, we show the labels inferred byassuming that an example has a label just in caseat least three of the ﬁve annotators chose that la-bel.
this is a conservative approach that creates afairly large ‘no majority’ category.
more sophis-ticated approaches might allow us to make fulleruse of the examples and account for biases relatingto annotator quality and example complexity (seesection 2.3).
we set these options aside for now.
because our validation process placed more weighton the best workers we could recruit (section 3.3).
the majority label splits given by table 5 aredesigned to ensure ﬁve properties: (1) the classesare balanced, (2) model 0 performs at chance, (3)the review-level rating associated with the sentencehas no predictive value, (4) at least four of the ﬁveworkers agreed, and (5) the majority label is posi-tive, negative, or neutral.
(this excludes examplesthat received a mixed majority and examples with-out a majority label at all.).
over the entire round, 47% of cases are such thatthe validation majority label is positive, negative,or neutral and model 0 predicted a different label..3.5 estimating human performance.
table 6a provides a conservative estimate of humanf1 in order to have a quantity that is comparable toour model assessments.
to do this, we randomizethe responses for each example to create ﬁve syn-thetic annotators, and we calculate the precision,recall, and f1 scores for each of these annotatorswith respect to the gold label.
we average thosescores.
this heavily weights the single annotatorwho disagreed for the cases with 4/5 majorities.
we.
2392dev test.
88.189.286.688.0.
87.889.386.988.0.posnegneuavg.
dev test.
posnegneuavg.
91.091.288.990.4.
90.991.088.290.0.
(a) round 1. fleiss κ: 0.62dev, 0.62 test.
614 of 1,280workers never disagreed withthe gold label..(b) round 2. fleiss κ: 0.68dev, 0.67 test.
116 of 244workers never disagreed withthe gold label..table 6: estimates of human performance (f1 scores)from comparing random synthesized human annotatorsagainst the gold labels using the response distributions.
these are conservative estimates, offered as a way oftracking model performance to determine when theround is “solved” and a new round should begin..cr imdb sst-3 yelp amazon round 1.
339,748pos2,405 12,500 128,016 29,841 133,411252,630neg 1,366 12,500 104,832 30,086 133,267431,8700 244,974 30,073 133,322neutotal 3,771 25,000 477,822 90,000 400,000 1,024,248.
0.table 7: model 1 training data.
cr and imdb are un-changed from table 2. sst-3 is repeated 3 times.
foryelp and amazon, we sample 1-, 3-, and 5-star reviewswith the goal of down-weighting them and removingambiguous reviews.
round 1 uses distributional labelsand is copied twice..can balance this against the fact that 614 of 1,280workers never disagreed with the majority label(see appendix b for the full distribution).
how-ever, it seems reasonable to say that a model hassolved the round if it achieves comparable scoresto our aggregate f1 – a signal to start a new round..4 round 2: dynabench.
in round 2, we leverage dynabench to begin creat-ing a new dynamic sentiment benchmark..4.1 model 1.model 1 was created using the same general meth-ods as for model 0 (section 3.1): we begin withroberta parameters and add a three-way senti-ment classiﬁer head.
the differences between thetwo models lie in the data they were trained on.
thetrain set is summarized in table 7, and appendix aprovides additional details..table 8 summarizes the performance of ourmodel on the same evaluation sets as are reportedin table 8 for model 0. overall, we see a smallperformance drop on the external datasets, but a.huge jump in performance on our dataset (round 1).
while it is unfortunate to see a decline in perfor-mance on the external datasets, this is expected ifwe are shifting the label distribution with our newdataset – it might be an inevitable consequence ofhill-climbing in our intended direction..4.2 dynabench interface.
our data distribution provides the dynabench inter-face we created for dynasent as well the completeinstructions and training items given to workers.
the essence of the task is that the worker chooses alabel y to target and then seeks to write an examplethat the model (currently, model 1) assigns a labelother than y but that other humans would label y.workers can try repeatedly to fool the model, andthey get feedback on the model’s predictions as aguide for how to fool it..4.3 methods.
we consider two conditions.
in the prompt con-dition, workers are shown a sentence and giventhe opportunity to modify it as part of achievingtheir goal.
prompts are sampled from parts of theyelp academic dataset not used for round 1. inthe no prompt condition, workers wrote sentencesfrom scratch, with no guidance beyond their goalof fooling the model.
we piloted both versions andcompared the results.
our analyses are summa-rized in section 5.1. the ﬁndings led us to drop theno prompt condition and use the prompt conditionexclusively, as it clearly leads to examples that aremore naturalistic and linguistically diverse..for round 2, our intention was for each promptto be used only once, but prompts were repeatedin a small number of cases.
we have ensured thatour dev and test sets contain only sentences derivedfrom unique prompts (section 4.5)..4.4 validation.
we used the identical validation process as de-scribed in section 3.3, getting ﬁve responses foreach example as before.
this again opens up thepossibility of using label distributions or inferringindividual labels.
395 workers participated in thisround.
see appendix b for additional details..4.5 round 2 dataset.
table 10 summarizes our round 2 dataset, andtable 9 provides train examples from round 2 sam-pled using the same criteria we used for table 4.overall, workers’ success rate in fooling model 1.
2393sst-3dev test.
84.682.740.069.1.
88.684.445.272.7.yelpdev test.
80.079.556.772.1.
83.179.656.673.1.amazondev test.
83.378.755.572.5.
83.378.855.472.5.round 1dev test.
81.080.583.181.5.
80.480.283.581.4.round 2dev test.
33.333.333.333.3.
33.333.333.333.3.positivenegativeneutralmacro avg.
table 8: model 1 performance (f1 scores) on external assessment datasets (table 1), as well as our round 1 andround 2 datasets.
chance performance for this model on round 2 is by design (section 4.5)..sentence.
model 1.responses.
the place was somewhat good and not welli bought a new car and met with an accident.
the retail store is closed for now at least.
prices are basically like garage sale prices..that book was good.
i need to get rid of it.
i really wanted to like this placebut i’m going to leave my money for the next vet.
once upon a time the model made a super decision..i cook my caribbean food and it was okaythis concept is really cool in name only.
wow, it’d be super cool if you could join usknife cut thru it like butter!
it was great..negnegnegneg.
neuneuneuneu.
pospospospos.
mix, mix, mix, mix, negneg, neg, neg, neg, negneu, neu, neu, neu, neuneg, neu, pos, pos, pos.
mix, mix, mix, neg, posmix, neg, neg, neg, posneg, neu, neu, neu, neupos, pos, pos, pos, pos.
mix, mix, mix, pos, posmix, neg, neg, neg, neuneu, neu, neu, neu, pospos, pos, pos, pos, pos.
table 9: round 2 train set examples, randomly selected from each combination of model 1 prediction and majoritylabel, but limited to examples with 30–50 characters..disttrain.
32,55124,99416,36518,765–92,675.majority labeltrain dev test.
6,0384,5792,4483,3342,13618,535.
24024024000720.
24024024000720.positivenegativeneutralmixedno majoritytotal.
table 10: round 2 dataset..is about 19%, which is much lower than the compa-rable value for round 1 (47%).
there seem to bethree central reasons for this.
first, model 1 is hardto fool, so many workers reach the maximum num-ber of attempts.
we retain the examples they enter,as many of them are interesting in their own right.
second, some workers seem to get confused aboutthe true goal and enter sentences that the modelin fact handles correctly.
some non-trivial rate ofconfusion here seems inevitable given the cognitivedemands of the task, but we have taken steps to im-prove the interface to minimize this factor.
third, acommon strategy is to create examples with mixedsentiment; the model does not predict this label,.
but it is chosen at a high rate in validation..despite these factors, we can construct splitsthat meet our core goals: (1) model 1 performs atchance on the dev and test sets, and (2) the dev andtest sets contain only examples where the majoritylabel was chosen by at least four of the ﬁve workers.
in addition, (3) our dev and test sets contain only ex-amples from the prompt condition (the no promptcases are in the train set, and ﬂagged as such), and(4) all the dev and test sentences are derived fromunique prompts to avoid leakage between train andassessment sets and reduce unwanted correlationswithin the assessment sets..4.6 estimating human performance.
table 6b provides estimates of human f1 forround 2 using the same methods as described insection 3.5. we again emphasize that these are con-servative estimates.
a large percentage of workers(116 of 244) never disagreed with the gold labelon the examples they rated, suggesting that humanperformance can approach perfection.
nonetheless,the estimates we give here seem useful for helpingus decide whether to continue hill-climbing on thisround or begin creating new rounds..23945 discussion.
we now address a range of issues that our methodsraise but that we have so far deferred in the interestof succinctly reporting on the methods themselves..is just the sort of behavior that we know can cre-ate persistent dataset artifacts.
for this reason, weinclude no prompt examples in the training dataonly, and we make it easy to identify them in caseone wants to handle them specially..5.1 the role of prompts.
5.2 the neutral category.
as discussed in section 4, we explored two meth-ods for collecting original sentences on dynabench:with and without a prompt sentence that workerscould edit to achieve their goal.
we did small pilotrounds in each condition and assessed the results.
this led us to use the prompt condition exclusively.
this section explains our reasoning more fully..first, we note that workers did in fact make useof the prompts.
in figure 2a, we plot the leven-shtein edit distance between the prompts providedto annotators and the examples the annotators pro-duced, normalized by the length of the promptor the example, whichever is longer.
there is aroughly bimodal distribution in this plot, where thepeak on the right represents examples generated bythe annotator tweaking the prompt slightly and thepeak on the left represents examples where theydeviated signiﬁcantly from the prompt.
essentiallyno examples fall at the extreme ends (literal reuseof the prompt; complete disregard for the prompt).
second, we observe that examples generatedin the prompt condition are generally longer thanthose in the no prompt condition, and more likeour round 1 examples.
figure 2b summarizes forstring lengths; the picture is essentially the samefor tokenized word counts.
in addition, the promptexamples have a more diverse vocabulary overall.
figure 2c provides evidence for this: we sampled100 examples from each condition 500 times, sam-pled ﬁve words from each example, and calculatedthe vocabulary size (unique token count) for eachsample.
(these measures are intended to controlfor the known correlation between token countsand vocabulary sizes; baayen 2001.)
the prompt-condition vocabularies are much larger, and againmore similar to our round 1 examples..third, a qualitative analysis further substantiatesthe above picture.
for example, many workers re-alized that they could fool the model by attributinga sentiment to another group and then denying it,as in “they said it would be great, but they werewrong”.
as a result, there are dozens of exam-ples in the no prompt condition that employ thisstrategy.
individual workers hit upon more idiosyn-cratic strategies and repeatedly used them.
this.
for both model 0 and model 1, there is consistentlya large gap between performance on the neutralcategory and performance on the other categories,but only for the external datasets we use for evalua-tion.
for our dataset, performance across all threecategories is fairly consistent.
we hypothesizedthat this traces to semantic diversity in the neutralcategories for these external datasets.
in reviewcorpora, three-star reviews can signal neutrality,but they are also likely to signal mixed sentimentor uncertain overall assessments.
similarly, wherethe ratings are assigned by readers, as in the sst,it seems likely that the middle of the scale will alsobe used to register mixed and uncertain sentiment,along with a real lack of sentiment..to further support this hypothesis, we ran thesst dev set through our validation pipeline.
thisleads to a completely relabeled dataset (distributedwith dynasent) with ﬁve ratings for each exampleand a richer array of categories.
the new labels areclosely aligned with sst’s for positive and neg-ative, but the sst-3 neutral category has a largepercentage of cases falling into mixed and no ma-jority.
appendix d provides the full comparisonmatrix and gives a random sample of cases wherethe two label sets differ with regard to the neutralcategory.
it also provides all seven cases of senti-ment confusion.
we think these comparisons favorour labels over sst’s original labels..5.3 fine-tuning.
our model 1 was trained from scratch (beginningwith roberta parameters)d. an appealing alterna-tive would be to begin with model 0 and ﬁne-tuneit on our round 1 data.
this would be more efﬁ-cient, and it might naturally lead to the round 1data receiving the desired overall weight relative tothe other datasets.
unfortunately, our attempts atthis led to worse models, and the problems tracedto very low performance on the neutral category.
to study the effect of our dataset on model 1performance, we employ the “ﬁne-tuning by in-oculation” method of liu et al.
(2019a).
we ﬁrstdivide our round 1 train set into small subsets viarandom sampling.
then, we ﬁne-tune our model 0.
2395(a) normalized edit distances between theprompt and the example..(b) string lengths.
the picture is essen-tially the same for tokenized word counts..(c) vocabulary sizes in samples of 100 ex-amples (500 samples with replacement)..figure 2: the ‘prompt’ and ‘no prompt’ conditions..figure 3: inoculation by ﬁne-tuning results with different number of ﬁne-tuning examples..using these subsets of round 1 train with non-distributional labels.
we early-stop our ﬁne-tuningprocess if performance on the round 0 dev set ofmodel 0 (sst-3 dev) has not improved for ﬁveepochs.
lastly, we measure model performancewith round 1 dev (sst-3 dev plus round 1 dev)and our external evaluation sets (table 1)..figure 3 presents f1 scores for our three classlabels using this method.
model performance onround 1 dev increases for all three labels givenmore training examples.
the f1 scores for the pos-itive and negative classes remain high, but theybegin to drop slightly with larger samples.
thef1 scores on sst-3 dev show larger perturbations.
the most striking trends are for the neutral cate-gory, where the f1 score on round 1 dev increasessteadily while the f1 scores on the three originaldevelopment sets for model 0 decrease drastically.
this is the pattern that liu et al.
(2019a) associatewith dataset artifacts or label distribution shifts..our current hypothesis is that the pattern we ob-serve can be attributed, at least in large part, to labelshift – speciﬁcally, to the difference between ourneutral category and the other neutral categories,as discussed in the preceding section.
our strategyof training from scratch seems less susceptible to.
these issues, though the label shift is still arguablya factor in the lower performance we see on thiscategory with our external validation sets..6 conclusion.
we presented dynasent, as the ﬁrst stage in anongoing effort to create a dynamic benchmark forsentiment analysis.
to date, the best future-lookingmodel 2 we have developed achieves 83.1 f1 onround 1 and 70.8 f1 on round 2 while maintaininggood performance on our external benchmarks.
ap-pendix e provides details on this model and others,and the dynabench platform offers a detailed andup-to-date leaderboard.
we hope and expect thatthe community will ﬁnd models that solve bothrounds.
that will be our cue to launch anotherround of data collection to fool those models andpush the ﬁeld of sentiment forward by another step..acknowledgements.
our thanks to the developers of the dynabenchplatform, and special thanks to our amazon me-chanical turk workers for their essential contribu-tions to this project.
this research is supported inpart by faculty research grants from facebook andgoogle..2396impact statement.
dynasent is distributed with a detailed datasheet(gebru et al., 2018) that describes the data collec-tion process and its motivations, and seeks to artic-ulate known limitations of the resource.
the datadistribution also includes a model card (mitchellet al., 2019) that seeks to provide similar disclo-sures concerning model 0 and model 1. takentogether, these documents further articulate ourcentral goals for these resources and provide guid-ance on responsible use.
these documents willbe upated appropropriately as dynasent and ourassociated models evolve..references.
moustafa alzantot, yash sharma, ahmed elgohary,bo-jhang ho, mani srivastava, and kai-wei chang.
2018. generating natural language adversarial ex-amples.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 2890–2896, brussels, belgium.
associationfor computational linguistics..r. harald baayen.
2001. word frequency distribu-tions.
kluwer academic publishers, dordrecht..max bartolo, alastair roberts, johannes welbl, sebas-tian riedel, and pontus stenetorp.
2020. beat theai: investigating adversarial human annotation forreading comprehension.
transactions of the associ-ation for computational linguistics, 8:662–678..yonatan belinkov, adam poliak, stuart shieber, ben-jamin van durme, and alexander rush.
2019.don’t take the premise for granted: mitigating ar-in proceed-tifacts in natural language inference.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 877–891, flo-rence, italy.
association for computational linguis-tics..john burn-murdoch.
2013. social media analytics:.
are we nearly there yet?
the guardian..michael chen, mike d’arcy, alisa liu, jared fer-nandez, and doug downey.
2019. codah: anadversarially-authored question answering datasetfor common sense.
in proceedings of the 3rd work-shop on evaluating vector space representationsfor nlp, pages 63–69, minneapolis, usa.
associ-ation for computational linguistics..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2019. electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..a. p. dawid and a. m. skene.
1979. maximum like-lihood estimation of observer error-rates using the.
em algorithm.
journal of the royal statistical soci-ety.
series c (applied statistics), 28(1):20–28..marie-catherine de marneffe, christopher d. man-ning, and christopher potts.
2012. did it happen?
the pragmatic complexity of veridicality assessment.
computational linguistics, 38(2):301–333..arthur p dempster, nan m laird, and donald b rubin.
1977. maximum likelihood from incomplete datavia the em algorithm.
journal of the royal statisti-cal society: series b (methodological), 39(1):1–22..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..ujwal gadiraju, besnik fetahu, ricardo kawase,patrick siehndel, and stefan dietze.
2017. usingworker self-assessments for competence-based pre-selection in crowdsourcing microtasks.
acm trans-actions of computer–human interaction, 24(4)..timnit gebru, jamie morgenstern, briana vecchione,jennifer wortman vaughan, hanna wallach, haldaume´e iii, and kate crawford.
2018. datasheetsfor datasets.
arxiv preprint arxiv:1803.09010..max glockner, vered shwartz, and yoav goldberg.
2018. breaking nli systems with sentences that re-in proceedings ofquire simple lexical inferences.
the 56th annual meeting of the association for com-putational linguistics (volume 2: short papers),pages 650–655, melbourne, australia.
associationfor computational linguistics..stephen gossett.
2020. emotion ai has great promise.
(when used responsibly).
built in blog..seth grimes.
2014. text analytics 2014: user perspec-tives on solutions and providers.
technical report,alta plana..seth grimes.
2017. data frontiers: subjectivity, senti-.
ment, and sense.
brandwatch..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah a.smith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data..2397conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107–112, new orleans, louisiana.
associa-tion for computational linguistics..judy hanwen shen, lauren fratamico, iyad rahwan,and alexander m. rush.
2018. darling or babygirl?
investigating stylistic bias in sentiment analysis.
infairness, accountability, and transparency in ma-chine learning..minqing hu and bing liu.
2004. mining and summa-rizing customer reviews.
in proceedings of the 10thacm sigkdd international conference on knowl-edge discovery and data mining, pages 168–177.
acl..christoph hube, besnik fetahu, and ujwal gadiraju.
2019. understanding and mitigating worker biasesin the crowdsourced collection of subjective judg-ments.
in proceedings of the 2019 chi conferenceon human factors in computing systems, chi ’19,pages 1–12.
association for computing machinery..robin jia and percy liang.
2017. adversarial exam-ples for evaluating reading comprehension systems.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2021–2031, copenhagen, denmark.
association forcomputational linguistics..di jin, zhijing jin, joey tianyi zhou, and peterszolovits.
2019.is bert really robust?
naturallanguage attack on text classiﬁcation and entailment.
arxiv preprint arxiv:1907.11932, 2..nitin jindal and bing liu.
2008. opinion spamin proceedings of the 2008 interna-and analysis.
tional conference on web search and data mining,wsdm ’08, pages 219–230, new york, ny, usa.
association for computing machinery..divyansh kaushik and zachary c. lipton.
2018. howmuch reading does reading comprehension require?
ina critical investigation of popular benchmarks.
proceedings of the 2018 conference on empiricalmethods in natural language processing, pages5010–5015, brussels, belgium.
association forcomputational linguistics..svetlana kiritchenko and saif mohammad.
2018. ex-amining gender and race bias in two hundred sen-in proceedings of thetiment analysis systems.
seventh joint conference on lexical and compu-tational semantics, pages 43–53, new orleans,louisiana.
association for computational linguis-tics..hector j. levesque.
2013. on our best behaviour.
inproceedings of the twenty-third international con-ference on artiﬁcial intelligence, beijing..tal linzen.
2020. how can we accelerate progress to-wards human-like linguistic generalization?
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5210–5217, online.
association for computational lin-guistics..bing liu.
2012. sentiment analysis and opinion min-.
ing.
morgan & claypool..nelson f. liu, roy schwartz, and noah a. smith.
2019a.
inoculation by ﬁne-tuning: a method foranalyzing challenge datasets.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 2171–2179, minneapolis, min-nesota.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts.
2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..julian mcauley, jure leskovec, and dan jurafsky.
2012. learning attitudes and attributes from multi-aspect reviews.
in 12th international conference ondata mining, pages 1020–1025, washington, d.c.ieee computer society..margaret mitchell, simone wu, andrew zaldivar,parker barnes, lucy vasserman, ben hutchinson,elena spitzer, inioluwa deborah raji, and timnitgebru.
2019. model cards for model reporting.
inproceedings of the conference on fairness, account-ability, and transparency, fat* ’19, pages 220–229, new york, ny, usa.
association for comput-ing machinery..aakanksha naik, abhilasha ravichander, normansadeh, carolyn rose, and graham neubig.
2018.stress test evaluation for natural language inference.
in proceedings of the 27th international conferenceon computational linguistics, pages 2340–2353,santa fe, new mexico, usa.
association for com-putational linguistics..preslav nakov, alan ritter, sara rosenthal, fabriziosebastiani, and veselin stoyanov.
2016. semeval-2016 task 4: sentiment analysis in twitter.
inproceedings of the 10th international workshop onsemantic evaluation (semeval-2016), pages 1–18,san diego, california.
association for computa-tional linguistics..2398jianmo ni, jiacheng li, and julian mcauley.
2019.justifying recommendations using distantly-labeledin proceedingsreviews and ﬁne-grained aspects.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 188–197, hongkong, china.
association for computational lin-guistics..yixin nie, yicheng wang, and mohit bansal.
2019.analyzing compositionality-sensitivity of nli mod-els.
in proceedings of the aaai conference on arti-ﬁcial intelligence, volume 33, pages 6867–6874..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..bo pang and lillian lee.
2004. a sentimental edu-cation: sentiment analysis using subjectivity sum-in proceed-marization based on minimum cuts.
ings of the 42nd annual meeting of the associationfor computational linguistics (acl-04), pages 271–278, barcelona, spain..workshop on ethics in natural language process-ing, pages 74–79, valencia, spain.
association forcomputational linguistics..rachel rudinger, jason naradowsky, brian leonard,and benjamin van durme.
2018. gender bias incoreference resolution.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 8–14, new orleans, louisiana.
associationfor computational linguistics..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for com-putational linguistics, pages 1668–1678, florence,italy.
association for computational linguistics..tal schuster, darsh shah, yun jie serene yeo, danielroberto filizzola ortiz, enrico santus, and reginabarzilay.
2019. towards debiasing fact veriﬁcationmodels.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3419–3425, hong kong, china.
association forcomputational linguistics..bo pang and lillian lee.
2005. seeing stars: ex-ploiting class relationships for sentiment categoriza-in proceed-tion with respect to rating scales.
ings of the 43rd annual meeting of the associationfor computational linguistics (acl’05), pages 115–124, ann arbor, michigan.
association for compu-tational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..bo pang and lillian lee.
2008. opinion mining andsentiment analysis.
foundations and trends in in-formation retrieval, 2(1):1–135..ellie pavlick and tom kwiatkowski.
2019..inherentdisagreements in human textual inferences.
transac-tions of the association for computational linguis-tics, 7:677–694..adam poliak, jason naradowsky, aparajita haldar,rachel rudinger, and benjamin van durme.
2018.hypothesis only baselines in natural language in-in proceedings of the seventh joint con-ference.
ference on lexical and computational semantics,pages 180–191, new orleans, louisiana.
associa-tion for computational linguistics..rion snow, brendan o’connor, daniel jurafsky, andandrew ng.
2008. cheap and fast – but is it good?
evaluating non-expert annotations for natural lan-guage tasks.
in proceedings of the 2008 conferenceon empirical methods in natural language process-ing, pages 254–263, honolulu, hawaii.
associationfor computational linguistics..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..sara rosenthal, noura farra, and preslav nakov.
2017.semeval-2017 task 4: sentiment analysis in twit-the 11th internationalter.
workshop on semantic evaluation (semeval-2017),pages 502–518, vancouver, canada.
association forcomputational linguistics..in proceedings of.
yi-ting tsai, min-chu yang, and han-yu chen.
2019.adversarial attack on sentiment classiﬁcation.
inthe 2019 acl workshop black-proceedings ofboxnlp: analyzing and interpreting neural net-works for nlp, pages 233–240, florence, italy.
as-sociation for computational linguistics..rachel rudinger, chandler may,.
and benjaminvan durme.
2017. social bias in elicited natural lan-guage inferences.
in proceedings of the first acl.
masatoshi tsuchiya.
2018..performance impactcaused by hidden bias of training data for recog-in proceedings of thenizing textual entailment..2399rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019. hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..wei emma zhang, quan z. sheng, ahoud alhazmi,and chenliang li.
2020. adversarial attacks ondeep-learning models in natural language process-ing: a survey.
acm transactions on intelligent sys-tems and technology, 11(3)..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
in c. cortes, n. d. lawrence, d. d. lee,m. sugiyama, and r. garnett, editors, advances inneural information processing systems 28, pages649–657.
curran associates, inc..yudian zheng, guoliang li, yuanbing li, caihua shan,and reynold cheng.
2017. truth inference in crowd-sourcing: is the problem solved?
proceedings ofvldb endowment, 10(5):541–552..eleventh international conference on language re-sources and evaluation (lrec 2018), miyazaki,japan.
european language resources association(elra)..amazon mechanical turk.
2017. tutorial: best prac-tices for managing workers in follow-up surveysor longitudinal studies.
amazon mechanical turkblog..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial trig-gers for attacking and analyzing nlp.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2153–2162, hongkong, china.
association for computational lin-guistics..janyce wiebe, theresa wilson, and claire cardie.
2005. annotating expressions of opinions and emo-tions in language.
language resources and evalua-tion, 39(2–3):165–210..terry winograd.
1972. understanding natural lan-.
guage.
cognitive psychology, 3(1):1–191..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems, volume 32, pages5753–5763.
curran associates, inc..rowan zellers, yonatan bisk, roy schwartz, andyejin choi.
2018. swag: a large-scale adversar-ial dataset for grounded commonsense inference.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages 93–104, brussels, belgium.
association for computa-tional linguistics..2400appendix.
a model training.
to train our model 0, we import weights from thepretrained roberta-base model.5 as in the origi-nal roberta-base model (liu et al., 2019b), ourmodels have 12 heads and 12 layers, with hiddenlayer size 768. they use byte-pair encoding asthe tokenizer (sennrich et al., 2016), with a maxi-mum sequence length of 128. the initial learningrate is 2e−5 for all trainable parameters, with abatch size of 8 per device (gpu).
we ﬁne-tune for3 epochs with a dropout probability of 0.1 for bothattention weights and hidden states.
to foster re-producibility, our training pipeline is adapted fromthe hugging face library (wolf et al., 2020).6 weused 6 × geforce rtx 2080 ti gpu each with11gb memory.
the training process takes about15 hours..to train model 0, we pooled a number of publicsentiment benchmarks, as summarized in table 2.the customer reviews (cr; hu and liu 2004)and imdb (maas et al., 2011) datasets have onlybinary labels.
the other datasets have ﬁve star-rating categories.
we bin these ratings by taking thelowest two ratings to be negative, the middle ratingto be neutral, and the highest two ratings to bepositive.
the yelp and amazon datasets are thoseused in zhang et al.
2015; the ﬁrst is derived froman earlier version of the yelp academic dataset,and the second is derived from the dataset used bymcauley et al.
(2012).
sst-3 is the ternary versionof the stanford sentiment treebank (socher et al.,2013) (labels 0–1 = neg; 2 = neu; 3–4 = pos).
wetrain on the phrase-level version of the dataset (andalways evaluate only on its sentence-level labels).
to train model 1, we used the same externaldatasets as we use for model 0, but with a fewcrucial changes, as seen in table 7. first, we sub-sample the large yelp and amazon datasets to en-sure that they do not dominate the dataset, and weinclude only 1-star, 3-star, and 5-star reviews totry to reduce the number of ambiguous examples.
second, we upsample sst-3 by a factor of 3 andour own dataset by a factor of 2, using the distri-butional labels for our dataset (section 3.4).
thisgives roughly equal weight, by example, to ourdataset as to all the others combined.
this makes.
5https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz6https://github.com/huggingface/.
transformers.
sense given our general goal of doing well on ourdataset and, especially, of shifting the nature of theneutral category to something more semanticallycoherent than what the other corpora provide..b additional details on validation.
b.1 validation interface.
figure 4 shows the interface for the validation taskused for both round 1 and round 2. the top pro-vides the instructions, and then one item is shown.
the full task had ten items per human interfacetask (hit).
workers were paid us$0.25 per hit,and all workers were paid for all their work, regard-less of whether we retained their labels..b.2 worker selection.
examples were uploaded to amazon’s mechani-cal turk in batches of 3–5k examples.
after eachround, we assessed workers by the percentage ofexamples they labeled for which they agreed withthe majority.
for example, a worker who selectsnegative where three of the other workers chosepositive disagrees with the majority for that exam-ple.
if a worker disagreed with the majority morethan 80% of the time, we removed that worker fromthe annotator pool and revalidated the examplesthey labeled.
this process was repeated iterativelyover the course of the entire validation process forboth rounds.
thus, many examples received morethan 5 labels; we collected a total of 808,289 re-sponses, of which 608,170 (75%) are used in theﬁnal dataset, as we keep only those by the top-ranked workers according to agreement with themajority.
we observed that this iterative processled to substantial improvements to the validationlabels according to our own intuitions..to remove workers from our pool, we used amethod of ‘unqualifying’, as described in turk2017. this method does no reputational damage toworkers and is often used in situations where the re-quester must limit responses to one per worker (e.g.,surveys).
we do not know precisely why workerstend to disagree with the majority.
the reasonsare likely diverse.
possible causes include inatten-tiveness, poor reading comprehension, a lack ofunderstanding of the task, and a genuinely differentperspective on what examples convey.
while wethink our method mainly increased label quality,we recognize that it can introduce unwanted biases.
we acknowledge this in our datasheet, which isdistributed with the dataset..2401b.3 worker distribution.
figure 5 show the distribution of workers for thevalidation task for both rounds.
in the ﬁnal versionof round 1, the median number of examples perworker was 45 and the mode was 11. for round 2,the median was 20 and the mode was 1..b.4 worker agreement with gold labels.
figure 6 summarizes the rates at which individualworkers agree with the gold label.
across the devand test sets for both rounds, substantial numbersof workers agreed with the gold label on all of thecases they labeled, and more than half were above95% for this agreement rate for both rounds..c additional details on dynabench task.
c.1.
interface for the prompt condition.
figure 7 shows an example of the dynabench in-terface in the prompt condition..c.2.
instructions.
our data distribution includes the complete instruc-tions for the dynabench task, and the list of compre-hension questions we required workers to answercorrectly before starting..c.3 data collection pipeline.
for each task, a worker has ten attempts in total toﬁnd an example that fools the model.
a worker canimmediately claim their payment after submitting asingle fooling example, or running out of attempts.
the average number of attempts per task is twobefore the worker generates an example that theyclaim fools the model.
workers are paid us$0.30per task.
a conﬁrmation step is required if themodel predicts incorrectly: we explicitly ask work-ers to conﬁrm the examples they come up with aretruly fooling examples..to incentivize workers, we pay a bonus ofus$0.30 for each truly fooling example accord-ing to our separate validation phase.
we temporar-ily disallow a worker to do our task if they failto correctly answer all our onboarding questionswithin ﬁve attempts.
we also temporarily disallowa worker to do our task if they consistently cannotcome up with truly fooling examples according toour validation task..a worker must meet the following qualiﬁcationsbefore accepting our tasks.
first, a worker mustreside in the u.s. and speak english.
second, aworker must have completed at least 1,000 tasks on.
amazon mechanical turk with an approval ratingof 98%.
lastly, a worker must not be in any of ourtemporarily disallowing worker pools..we adapt the open-source software package.
mephisto as our data collection tool.7.
d sst-3 validation examples.
table 11 compares the sst-3 labels with the labelsfrom our separate validation task.
there are justseven cases of polarity (positive/negative and neg-ative/positive) disagreement.
these are includedin table 12. the rate of disagreement is muchhigher where the sst-3 neutral category is in-volved, which we trace (in section 5.2) to the natureof the sst-3 category.
table 12 gives a randomselection of cases involving the neutral category tosupport these claims qualitatively..sst-3positive negative neutral.
positivenegativeneutralmixedno majority.
3675233415.
235983524.
6457443925.table 11: comparison of the sst-3 labels (dev set)with labels derived from our separate validation..e a future-looking model 2.as we say in section 6, we hope that dynasentcontinues to grow.
a future round 3 would usea future model 2 (or set of such models), ei-ther to harvest naturally occurring examples or todrive another round of adversarial example cre-ation on dynabench.
we have explored a vari-ety of transformer-based architectures (vaswaniet al., 2017) for model 2, designed and optimizedaccording to the protocols given in appendix a:roberta (liu et al., 2019b), bert (devlin et al.,2019), xlnet (yang et al., 2019), and elec-tra (clark et al., 2019).
electra has yieldedthe best results so far, with 83.1 f1 on round 1 and70.8 on round 2. we do not think these are the bestpossible models; we offer these very preliminaryresults in the hope that they provide some usefulguidance..7https://github.com/facebookresearch/.
mephisto.
2402figure 4: validation interface..(a) round 1..(b) round 2..figure 5: worker distribution for the validation task..(a) round 1..(b) round 2..figure 6: rates at which individual worker agree with the majority label.
the y-axis gives, for each worker, thetotal number of examples for which they chose the majority label divided by the total number of cases they labeledover all..2403figure 7: dynabench interface..sst-3.
responses.
neg.
neu, pos, pos, pos, pos.
neg.
neg, neg, pos, pos, pos.
sst-3.
responses.
pospospos.
pospos.
mix, mix, neg, neg, negmix, neg, neg, neg, negmix, neg, neg, neg, neg.
mix, mix, neg, neg, negmix, neg, neg, neg, neg.
moretti ’s compelling anatomy of grief and the difﬁcult process of adaptingto loss.
nothing is sacred in this gut-buster..(a) all examples for which the sst-3 label is negative and our majority label is positive..... routine , harmless diversion and little else.
hilariously inept and ridiculous.
reign of fire looks as if it was made without much thought – and is bestwatched that way.
so much facile technique, such cute ideas, so little movie.
while there ’s something intrinsically funny about sir anthony hopkinssaying ’get in the car, bitch,’ this jerry bruckheimer production has little elseto offer.
(b) all examples for which the sst-3 label is positive and our majority label is negative..should be seen at the very least for its spasms of absurdist humor.
van wilder brings a whole new meaning to the phrase ‘ comedy gag .
’‘ they’ begins and ends with scenes so terrifying i’m still stunned.
barely gets off the ground.
as a tolerable diversion, the ﬁlm sufﬁces; a triumph, however, it is not..sst-3.
responses.
neuneuneuneuneu.
pos, pos, pos, pos, posmix, neu, pos, pos, posneu, neu, pos, pos, posneg, neg, neg, neg, negmix, mix, mix, mix, neg.
(c) a random selection of examples for which sst-3 label is neutral and our validation label is not..table 12: comparisons between the sst-3 labels and our new validation labels..2404