exploiting document structures and cluster consistenciesfor event coreference resolution.
hieu minh tran1, duy phung1 and thien huu nguyen21 vinai research, vietnam2 department of computer and information science, university of oregon,eugene, or 97403, usa{v.hieutm4,v.duypv1}@vinai.io, thien@cs.uoregon.edu.
abstract.
we study the problem of event coreferenceresolution (ecr) that seeks to group coref-erent event mentions into the same clusters.
deep learning methods have recently been ap-plied for this task to deliver state-of-the-artperformance.
however, existing deep learn-ing models for ecr are limited in that theycannot exploit important interactions betweenrelevant objects for ecr, e.g., context wordsand entity mentions, to support the encodingof document-level context.
in addition, con-sistency constraints between golden and pre-dicted clusters of event mentions have not beenconsidered to improve representation learningin prior deep learning models for ecr.
thiswork addresses such limitations by introduc-ing a novel deep learning model for ecr.
atthe core of our model are document structuresto explicitly capture relevant objects for ecr.
our document structures introduce diverseknowledge sources (discourse, syntax, seman-tics) to compute edges/interactions betweenstructure nodes for document-level representa-tion learning.
we also present novel regular-ization techniques based on consistencies ofgolden and predicted clusters for event men-tions in documents.
extensive experimentsshow that our model achieve state-of-the-artperformance on two benchmark datasets..1.introduction.
event coreference resolution (ecr) is the task ofclustering event mentions (i.e., trigger words thatevoke an event) in a document such that each clus-ter represents a unique real world event.
for ex-ample, the three event mentions in figure 1, i.e.,“refuse to sign, “raised objections”, and “doesn’tsign”, should be grouped into the same cluster toindicate their coreference to the same event..a common component in prior ecr models in-volves a binary classiﬁer that receives a pair of.
event mentions and predict their coreference (chenet al., 2009; lu et al., 2016; lu and ng, 2017).
to this end, an important step in ecr models isto transform event mention pairs into representa-tion vectors to encode discriminative features forcoreference prediction.
early work on ecr hasachieved feature representation via feature engi-neering where multiple features are hand-designedfor input event mention pairs (lu and ng, 2017).
amajor problem with feature engineering is the spar-sity of the features that limits the generalizationto unseen data.
representation learning in deeplearning models has recently been introduced toaddress this issue, leading to more robust methodswith better performance for ecr (nguyen et al.,2016; choubey and huang, 2018; huang et al.,2019; barhom et al., 2019).
as such, there are atleast two limitations in existing deep learning mod-els for ecr that will be addressed in this work toimprove the performance..first, as event mentions pairs for coreferenceprediction might belong to long-distance sentencesin documents, capturing document-level context be-tween the event mentions (i.e., beyond the two sen-tences that host the event mentions) might presentuseful information for ecr.
as their ﬁrst limita-tion, prior deep learning models for ecr has onlyattempted to encode document-level context viahand-designed features (kenyon-dean et al., 2018;barhom et al., 2019) that still suffer from the fea-ture sparsity issue.
in addition, such prior work isunable to exploit ecr-related objects in documents(e.g., entity mentions, context words) and theirconnections/interactions (possibly beyond sentenceboundary) to aid representation learning.
an exam-ple for the importance of context words, entity men-tions, and their interactions for ecr can be seen infigure 1. here, to decisively determine the coref-erence of “raised objections” and “doesn’t sign”,ecr systems should recognize “trump” and “the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4840–4850august1–6,2021.©2021associationforcomputationallinguistics4840figure 1: an example for event coreference resolution..$900bn relief bill” as the arguments of “raised ob-jections”, and “trump” and “the damn bill” as thearguments of “doesn’t sign”.
the systems shouldalso be able to realize the coreference relation be-tween the two entity mentions “trump”, and be-tween “the $900bn relief bill” and “the damn bill”to conclude the same identity for the event men-tions (i.e., as they involve the same arguments).
as such, it is helpful to identify relevant entitymentions, context words and leverage their rela-tions/interactions to improve representation vectorsfor event mentions in ecr.
motivated by this issue,we propose to form graphs for documents (calleddocument structures) to explicitly capture relevantobjects and interactions for ecr that will be con-sumed to learn representation vectors for eventmentions.
in particular, context words, entity men-tions, and event mentions will serve as the nodesin our document structures due to their intuitiverelevance to ecr.
different types of knowledgesources will then be exploited to connect the nodesfor the document structures, featuring discourse in-formation (e.g., to connect coreferring entity men-tions), syntactic information (e.g., to directly linkevent mentions and their arguments), and seman-tic similarity (e.g., to connect words/event men-tions with similar meanings).
such rich documentstructures allows us to model the interactions ofrelevant objects for ecr beyond sentence levelfor document-level context.
using graph convolu-tional neural networks (gcn) (kipf and welling,2017; nguyen and grishman, 2018) for represen-tation learning, we expect enriched representationvectors from the document structures can furtherimprove the performance of ecr systems.
to ourknowledge, this is the ﬁrst time that rich documentstructures are employed for ecr..second, prior deep learning models for ecrfails to leverage consistencies between golden clus-.
ters (provided by human) and predicted clusters(generated by models) to promote representationlearning.
in particular, it is intuitive that ecr mod-els can achieve better performance if their predictedevent clusters are more similar to the golden eventclusters in the data.
to this end, we propose toobtain different inconsistency measures betweengolden and predicted clusters that will be incorpo-rated into the overall loss function for minimization.
as such, we expect that the consistency/similarityregularization between two types of clusters canprovide useful training signals to improve repre-sentation vectors for event mentions in ecr.
toour knowledge, this is also the ﬁrst work to ex-ploit cluster consistency-based regularization forrepresentation learning in ecr.
finally, we con-duct extensive experiments for ecr on the kbpbenchmark datasets.
the experiments demonstratethe beneﬁts of the proposed methods and lead tostate-of-the-art performance for ecr..2 related work.
event coreference resolution is broadly related toworks on entity coreference resolution that aim toresolve nouns phrases/mentions for entities (raghu-nathan et al., 2010; ng, 2010; durrett and klein,2013; lee et al., 2017a; joshi et al., 2019b,a).
how-ever, resolving event mentions has been consideredas a more challenging task than entity coreferenceresolution due to the more complex structures ofevent mentions (yang et al., 2015)..our work focuses on the within-document set-ting for ecr where input event mentions are ex-pected to appear in the same input documents;however, we also note prior works on cross-document ecr (lee et al., 2012a; adrian bejanand harabagiu, 2014; choubey and huang, 2017;kenyon-dean et al., 2018; barhom et al., 2019;cattan et al., 2020).
as such, for within-document.
4841donaldtrumpcontinuedtorefusetosignareliefpackageagreedincongressandheadedinsteadtothegolfcourse….trump,whoisspendingthechristmasandnewyearholidayathismar-a-lagoresortinflorida,raisedobjectionstothe$900bnreliefbillonlyafteritwaspassedbycongresslastweek,havingbeennegotiatedbyhisowntreasurysecretarystevenmnuchin…allthesefolksandtheirfamilieswillsufferiftrumpdoesn’tsignthedamnbill.coreferential eventmentionscoreferential entity mentionsecr, previous methods have applied feature-basedmodels for pairwise classiﬁers (ahn, 2006; chenet al., 2009; cybulska and vossen, 2015; penget al., 2016), spectral graph clustering (chen and ji,2009), information propagation (liu et al., 2014),markov logic networks (lu et al., 2016), joint mod-eling of ecr with event detection (araki and mi-tamura, 2015; lu et al., 2016; chen and ng, 2016;lu and ng, 2017), and recent deep learning models(nguyen et al., 2016; choubey and huang, 2018;huang et al., 2019; lu et al., 2020; choubey et al.,2020).
compared to previous deep learning worksfor ecr, our model presents a novel representationlearning framework based on document structuresto explicitly encode important interactions betweenrelevant objects, and representation regularizationto exploit the cluster consistency between goldenand predicted clusters for event mentions..3 model.
formally, in ecr, given an input document d =w1, w2, .
.
.
, wn (of n words/tokens) with a set ofevent mentions e = {e1, e2, .
.
.
, e|e|}, the goal isto group the event mentions in e into clusters tocapture the coreference relation between mentions.
our ecr model consists of four major components:(i) document encoder to words into representationvectors, (ii) document structure to create graphsfor documents and learn rich representation vec-tors for event mentions, (iii) end-to-end resolu-tion to simultaneously resolve the coreference forthe entity mentions in d, and (iv) cluster consis-tency regularization to regularize representationvectors based on consistency constraints betweengolden and predict event mention clusters.
figure2 presents an overview of our model for ecr..3.1 document encoder.
in the ﬁrst step, we transform each word wi ∈d into a representation vector xi by feeding dinto the pre-trained language model bert (devlinet al., 2019).
in particular, as bert might split wiinto several word-pieces, we average the hiddenvectors of the word-pieces of wi in the last layer ofbert to obtain the representation vector xi for wi.
to handle long documents with bert, we divide dinto segments of 512 consecutive word-pieces thatwill be encoded separately.
the resulting sequencex = x1, x2, .
.
.
, xn for d is then sent to the nextsteps for further computation..3.2 document structure.
this component aims to learn representation vec-tors for the event mentions in e using an interactiongraph g = {n , e} for d that facilitates the enrich-ment of representation vectors for event mentionswith relevant objects and interactions at documentlevel.
as such, the nodes and edges in g for ourecr problem are constructed as follows:.
nodes: the node set n for our interaction graphg should capture relevant objects for the corefer-ence between event mentions in d. toward thisgoal, we consider all the context words (i.e., wi),event mentions, and entity mentions in d as rel-evant objects for our ecr problem.
for conve-nience, let m = {m1, m2, .
.
.
, m|m |} be the setof entity mentions in d. the node set n for gis thus created by the union of d, e, and m :n = d ∪ e ∪ m = {n1, n2, .
.
.
, n|n |}.
toachieve a fair comparison, we use the predictedevent mentions that are provided by (choubey andhuang, 2018) in the datasets for e. the stanfordcorenlp toolkit is employed to obtain the entitymentions m ..edges: the edges between the nodes in nfor g will be represented by an adjacency matrixa = {aij}i,j=|n | (aij ∈ r) in this work.
asa will be consumed by graph convolutional net-works (gcn) to learn representation vectors forecr, the value/score aij between two nodes niand nj in n is expected to estimate the importance(or the level of interaction) of nj for the represen-tation computation of ni.
this structure allows niand nj of n to directly interact and inﬂuence therepresentation computation of each other even ifthey are sequentially far away from each other ind. as presented in the introduction, we explorethree types of information to design the edges e(or compute the interaction scores aij) for g in ourmodel, including discourse-based, syntax-basedand semantic-based information.
discourse-based edges: due to multiple sen-tences and event/entity mentions involved in theinput document d, we need to understand wheresuch objects span and how they relate to each otherto effectively encode document context for ecr.
to this end, we propose to exploit three types of dis-course information to obtain the interaction graphg, i.e., sentence boundary, coreference structure,and mention span for event/entity mentions in d.sentence boundary: our motivation for this in-formation is that event/entity mentions appearing.
4842figure 2: an overview of the proposed ecr model..in the same sentences tend to be more contextuallyrelated to each other than those in different sen-tences.
as such, event/entity mentions in the samesentences might involve more helpful informationfor the representation computation of each otherin our problem.
to capture this intuition, we com-pute the sentence boundary-based interaction scoreasentfor the nodes ni and nj in n where asentij = 1ijif ni and nj are the event/entity mentions of thesame sentences in d (i.e., ni, nj ∈ e ∪ m ); and 0otherwise.
we will use asentas an input to computethe overall interaction score aij for g later..ij.
ij.
entity coreference structure: instead of con-sidering within-sentence information as in asent,coreference structure focuses on the connection ofentity mentions across sentences to enrich their rep-resentations with the contextual information of thecoreferring ones.
as such, to enable the interactionof representations for coreferring enity mentions,we compute the conference-based score acorefforeach pair of nodes ni and nj to contribute to theoverall score aij for representation learning.
here,acorefis set to 1 if ni and nj are coreferring entityijmentions in d, and 0 otherwise.
note that we alsouse the stanford corenlp toolkit to determine thecoreference of entity mentions in this work..ij.
mention span: the sentence boundary and coref-erence structure scores model interactions of eventand entity mentions in d based on discourse struc-ture.
to connect event and entity mentions tocontext words wi for representation learning, weemploy the mention span-based interaction scoreaspanisij.
as another input for aij.
here, aspan.
ij.
ij.
only set to 1 (i.e., 0 otherwise) if ni is a word(ni ∈ d) in the span of the entity/event mention nj(nj ∈ e ∪m ) or vice verse.
aspanis important as ithelps ground representation vectors of event/entitymentions to the contextual information in d.syntax-based edges: we expect the dependencytrees of the sentences in d to provide beneﬁcialinformation to connect the nodes in n for effec-tive representation learning in ecr.
for example,dependency trees have been used to retrieve im-portant context words between an event mentionsand their arguments in prior work (li et al., 2013;veyseh et al., 2020a,b).
to this end, we proposeto employ the dependency relations/connectionsbetween the words in d to obtain a syntax-basedinteraction score adepfor each pair of nodes ni andijnj in n , serving as an additional input for aij.
inparticular, by inheriting the graph structures of thedependency trees of the sentences in d, we setadepto 1 if ni and nj are two words in the sameijsentence (i.e., ni, nj ∈ d) and there is an edge be-tween them in the corresponding dependency tree1,and 0 otherwise.
semantic-based edges: this information lever-ages the semantic similarity of the nodes in n toenrich the overall interaction scores aij for g. ourmotivation is that a node ni will contribute moreto the representation computation of another nodenj for ecr if ni is more semantically related tonj.
in particular, as the representation vectors forthe nodes in n have captured the contextual se-mantics of the words in d, we propose to explore.
1we use stanford corenlp to parse sentences..4843donald trump continue   refuse        to      sign  ….raise objectionsto the....doesn’t sign….bert    structure combinationg5document structure initial word representationg1g2...gcninitial entity mentionrepresentationabstractevent mention representationinitial event mention representationend-to-endcoreference resolutioncluster consistencygolden clustercomponent structuresa novel source of semantic information that relieson external knowledge for the words to computeinteraction scores between the nodes n in our doc-ument structures for ecr.
we expect the externalknowledge for the words to provide complemen-tary information to the contextual information ind, thus further enriching the overall interactionscores aij for the nodes in n .
to this end, we pro-pose to utilize wordnet (miller, 1995), a rich net-work of word meanings, to obtain external knowl-edge for the words in d. the word meanings (i.e.,synsets) in wordnet are connected to each othervia different semantic relations (e.g., synonyms,hyponyms).
in particular, our ﬁrst step to generateknowledge-based similarity scores involves map-ping each word node ni ∈ d ∩ n to a synsetnode mi in wordnet using a word sense disam-biguation (wsd) tool.
in particular, we employwordnet 3.0 and the state-of-the-art bert-basedwsd model in (blevins and zettlemoyer, 2020) toperform the word-synset mapping in this work.
af-terward, we compute a knowledge-based similarityscore astructfor each pair of word nodes ni and njin d ∩ n using the structure-based similarity oftheir linked synsets mi and mj in wordnet (i.e.,astruct= 0 if either ni or nj is not a word node inijd ∩ n ).
accordingly, the lin similarity measure(lin et al., 1998) for synset nodes in wordnet is uti-= 2∗ic(lcs(mi,mj ))lized for this purpose: astruct.
ijic(mi)+ic(mj )here, ic and lcs represent the information con-tent of synset nodes and the least common sub-sumer of two synsets in the wordnet hierarchy(the most speciﬁc ancestor node) respectively2.
structure combination: up to now, ﬁve scoreshave been generated to capture the level of inter-actions in representation learning for each pair ofnodes ni and nj in n according to different in-, adepformation sources (i.e., asentijand astruct).
for convenience, we group the ﬁveijscores for each node pair ni and nj into a vectordij = [asent] of size 5.to combine the scores in dij into an overall richinteraction score aij for ni and nj in g, we use thefollowing normalization:.
ij , astructij.
, acorefij.
, acorefij.
, aspanij.
, aspanij.
, adep.
ij.
ij.
ij.
aij = exp(dijqt )/.
exp(diuqt ).
(1).
(cid:88).
u=1..|n |.
2we use the nltk tool.
to obtain the lin sim-ilarity: https://www.nltk.org/howto/wordnet.
html.
we tried other wordnet-based similarities availablein nltk (e.g., wu-palmer similarity), but the lin similarityproduced the best results in our experiments..where q is a learnable vector of size 5.representation learning: given the combinedinteraction graph g with the adjacency matrixa = {aij}i,j=|n |, we use gcns to induce rep-resentation vectors for the nodes in n for ecr.
inparticular, our gcn model takes the initial repre-sentation vectors vi of the nodes ni ∈ n as theinput.
here, the initial representation vector vi fora word node ni ∈ d is directly obtained from thebert-based representation vector xc ∈ x (i.e.,vi = xc) of the corresponding word wc for ni.
incontrast, for event and entity mentions, their initialrepresentation vectors are obtained by max-poolingthe contextualized embedding vectors in x thatcorrespond to the words in the event/entity men-tions’ spans.
for convenience, we organize vi intorows of the input matrix h0 = [v1, .
.
.
, v|n |].
thegcn model then involves g layers that generatethe matrix hl at the l-th layer for the nodes in n(1 ≤ l ≤ g) via: hl = relu (ahl−1wl) (wl isthe weight matrix for the l-th layer).
the output ofthe gcn model after g layers is hg whose rowsare denoted by hg = [h1, .
.
.
, h|n |], serving asmore abstract representation vectors for the nodesni in the coreference prediction for event mentions.
also, for convenience, let {re1, .
.
.
, re|e|} ⊂ hgbe the set of gcn-induced representation vectorsfor the event mention nodes in e1, .
.
.
, e|e| in e..3.3 end-to-end coreference resolution.
to facilitate the incorporation of the consistencyregularization between golden and predicted clus-ters into the training process, we perform and end-to-end procedure that seeks to simultaneously re-solve the coreference for the event mentions ine in a single process.
motivated by the entitycoreference resolution in (lee et al., 2017b), weimplement the end-to-end resolution via a set ofantecedent assignments for the event mentions ine. in particular, we assume that the event mentionsin e are enumerated in their appearance order in d.as such, our model aims to link each event mentionei ∈ e to one of its prior event mention in the setyi = {(cid:15), e1, .
.
.
, ei−1} ((cid:15) is a dumpy antecedent).
here, a link of ei to a non-dumpy antecedent ej inyi represents a coreference relation between ei andej.
in contrast, a dumpy assignment for ei indicatesthat ei is not coreferent with any prior event men-tion.
by forming a coreference graph with ei asthe nodes, the non-dumpy antecedent assignmentsfor every event mention in e can be utitlized to.
4844connect coreference event mentions.
connectedcomponents from the coreference graph can thenbe returned to serve as predicted event mentionclusters in d..in order to predict the coreferent antecedentyi ∈ y for an event mention ei, we compute thedistribution over the possible antecedents in yies(ei,yi)for ei via: p (yi| ei, yi) =y(cid:48)∈y(i) es(ei,y(cid:48)) wheres(ei, ej) is a score function to determine the coref-erence likelihood between ei and ej in d. to thisend, we set s(ei, (cid:15)) = 0 for all ei ∈ e. inspiredby (lee et al., 2017b), we obtain the score functions(ei, ej) for ei and ej by leveraging their gcn-induced representation vectors rei and rej via:.
(cid:80).
mffm(rei ).
s(ei, ej) = sm(ei) + sm(ej) + sc(ei, ej) + sa(ei, ej)sm(ei) = w(cid:62)sc(ei, ej) = w(cid:62)sa(ei, ej) = r(cid:62).
a ffc([rei , rej , rei (cid:12) rej ])eiw crej.
m and w(cid:62).
where fm and ffc are two-layer feed-forward net-works, w(cid:62)a are learnable vectors, w cis a weight matrix, and (cid:12) is the element-wisemultiplication.
at the inference time, we em-ploy the greedy decoding to predict the antecedentˆyi for ei: ˆyi = argmaxp (yi|ei, yi).
for train-ing, we use the negative log-likelihood as the lossfunction in our end-to-end framework: lpred =− (cid:80)|e|i=0 log p (y∗i ∈ yi is the goldenantecedent for ei)..i |ei, yi) (y∗.
3.4 cluster consistency regularization.
to further improve representation learning for ecr,we propose to regularize the induced representa-tion vectors of the event mentions in e to explicitlyenforce the consistency/similarity between goldenand predicted event mention clusters in d. thisis based on our motivation that ecr models willperform better if they can produce more similarevent mention clusters to the golden ones.
as such,for convenience, let t = {t1, t2, .
.
.
, t|t |} andp = {p1, p2, .
.
.
, p|p|} be the golden and pre-dicted sets of event mentions in e respectively,i.e., ti, pj ⊂ e, and t1 ∪ t2 ∪ .
.
.
∪ t|t | =p1∪p2∪.
.
.∪p|p| = e. also, for each cluster c int or p, we compute a centroid vector rc for it byaveraging the representation vectors of the eventmention members: rc = averagee∈c(re).
thisleads to the centroid vectors {rt1, rt2, .
.
.
, rt|t |}and {rp1, rp2, .
.
.
, rp|p|} for t and p respectively.
we propose the following regularization terms forcluster consistency:.
i.i.i.i.i.i.i , ei ∈ p (cid:48).
i ∈ t , p (cid:48).
(cid:107)22 − (cid:107)rei − rp (cid:48).
i ∈ p, ei ∈ t (cid:48).
intra-cluster consistency: this constraint con-cerns the inner information of each cluster, char-acterizing the structure of each individual eventmention in its golden and predicted clusters in tand p. in particular, for each event mention ei ∈ e,we expect its distances to the centroid vectors ofthe corresponding golden and predicted clusterst (cid:48)i and p (cid:48)i in t and p (respectively) to be similar,i.e., t (cid:48)i .
as such,we compute the distances between the representa-tion vector rei of ei to the centroid vectors rt (cid:48)and(cid:107)2via the euclidean distances (cid:107)rei − rt (cid:48)rp (cid:48)2 and(cid:107)2(cid:107)rei − rp (cid:48)2. afterward, the differences linnerbetween the two distances for golden and predictedclusters are aggregated over all event mentions andadded into the overall loss function for minimiza-tion: linner = (cid:80)|e|(cid:107)22|.
i=1 |(cid:107)rei − rt (cid:48)inter-cluster consistency: in this constraint, weexpect that the structure among the clusters tiin the golden set t is consistent with those forthe predicted event cluster set p (i.e., inter-clusterregulation).
to implement this idea, we encodethe structure of the clusters in a set via the av-erage of the pairwise distances between the cen-troid vectors of the clusters.
in particular, theinter-cluster structure scores for the golden andpredicted clusters in t and p are computed via:(cid:80)|t |st =2, andsp =2. thedifference between the structure scores for goldenand predicted clusters t and p is then includedinto the overall loss function for minimization:linter = |st − sp |.
inter-set similarity: this constraint aims to di-rectly promote the similarity between the goldenclusters in t and the predicted clusters in p. assuch, for the golden and predicted cluster sets tand p, we ﬁrst obtain the overall centroid vec-tors ut and up (respectively) by averaging thecentroid vectors of their member clusters: ut =averaget ∈t (rt ) and up = averagep ∈p (rp ).
theeuclidean distance lsim is then integrated into theoverall loss for minimization: lsim = (cid:107)ut −up (cid:107)22.note that linner, linter, and lsim will be zero ifthe predicted clusters in p are the same as those inthe golden clusters in t ..j=i+1 (cid:107)rti − rtj (cid:107)2j=i+1 (cid:107)rpi − rpj (cid:107)2.
2|t |(|t |−1)2|p|(|p|−1).
(cid:80)|t |i=1(cid:80)|p|i=1.
(cid:80)|p|.
to summarize, the overall loss function l totrain our ecr model in this work is: l =lpred + αinnerlinner + αinterlinter + αsimlsimwith αinner, αinter, and αsim as the trade-off pa-rameters..48454 experiments.
4.1 dataset & hyperparameters.
following prior work (choubey and huang, 2018),we train our ecr models on the kbp 2015 dataset(mitamura et al., 2015) and evaluate the modelson the kbp 2016 and kbp 2017 datasets for ecr(mitamura et al., 2016, 2017).
in particular, thekbp 2015 dataset includes 360 annotated docu-ments for ecr (181 documents from discussionforum and 179 documents from news articles).
weuse the same 310 documents from kbp 2015 as in(choubey and huang, 2018) for the training dataand the remaining 50 documents for the develop-ment data.
also, similar to (choubey and huang,2018), the news articles in kbp 2016 (85 docu-ments) and kbp 2017 (83 documents) are lever-aged for test datasets.
to ensure a fair comparison,we use the predicted event mentions provided by(choubey and huang, 2018) in all the datasets.
fi-nally, we report the ecr performance based onthe ofﬁcial kbp 2017 scorer (version 1.8)3. thescorer employs four coreference scoring measures,including muc (vilain et al., 1995), b3 (baggaand baldwin, 1998), ceaf-e (luo, 2005), blanc(lee et al., 2012b), and the unweighted average oftheir f1 scores (avgf 1)..hyper-parameters for the models are ﬁne-tunedby the avgf 1 scores over development data.
theselected values from the tuning process include: 1e-5 for the learning rate of the adam optimizer (se-lected from [1e-5, 2e-5, 3e-5, 4e-5, 5e-5]); 8 for themini-batch size (selected from [8, 16, 32, 64]); 128hidden units for all the feed-forward network andgcn layers (selected from [64, 128, 256, 512]);2 layers for the gcn model, g = 2 (selectedfrom [1, 2, 3, 4]), and αinner = 0.1, αinter =0.1, and αsim = 0.1 for the trade-off parame-ters in the overall loss function l (selected from[0.1, 0, 2, .
.
.
, 0.9]).
finally, we use the bertbasemodel (of 768 dimensions) for the pre-trained wordembeddings (updated during the training)..4.2 performance evaluation.
we compare the proposed model for ecr withdocument structures and cluster consistency regu-larization (called structecr) with prior work ecrmodels in the same evaluation setting, includingthe joint model between ecr and event detection(lu and ng, 2017), the integer linear programming.
3https://github.com/hunterhector/.
evmeval.
approach in (choubey and huang, 2018), and thediscourse structure proﬁling model in (choubeyet al., 2020) (also the model with the best reportedperformance in kbp datasets).
in addition, weexamine the following baselines of structecr tohighlight the beneﬁts of the proposed components:.
e2e-only: this variant implements the end-to-end resolution model described in section 3.3where all event mentions in a document are re-solved simultaneously in a single process.
how-ever, different from our full model structecr, e2e-only does not include the document structure com-ponent with gcn for representation learning, i.e.,it directly uses the initial representation vectors vi(induced from bert) for the event mentions in thecomputation of the distribution p (yi|ei, yi).
also,the cluster consistency regularization in section 3.4is also not included in this model..pairwise: this model is similar to e2e-only inthat it does not applies the document structures andregularization terms in structecr.
in addition, in-stead of simultaneously resolving event mentionsin documents, pairwise predicts the coreference forevery pair of event mentions separately.
in particu-lar, the representation vectors vei and vej for twoevent mentions ei and ej (included from bert)are combined via [vei, vej , vei (cid:12) vej ].
this vectoris then sent into a feed-forward network to pro-duce a distribution over possible coreference labelsbetween ei and ej (i.e., two labels for being coref-erent or not).
the coreference labels for every pairof event mentions are then gathered in a corefer-ence graphs among event mentions; the connectedcomponents will be returned for the event clusters..table 1 reports the performance of the ecr mod-els on the kbp 2016 and kbp 2017 datasets.
ascan be seen from the table, e2e-only performscomparably or better than prior state-of-the-artmodels for ecr, e.g., (choubey and huang, 2018)and (choubey et al., 2020), that employ extensivefeature engineering.
in addition, the better perfor-mance of e2e-only over pairwise (for both kbp2016 and kbp 2017) illustrates the beneﬁts of end-to-end coreference resolution for event mentions indocuments.
most importantly, the proposed modelstructecr signiﬁcantly outperforms all the base-line models for which the performance improve-ment over e2e-only is 1.94% and 1.26% (i.e.,avgf 1 scores) over the kbp 2016 and kbp 2017datasets respectively.
this clearly demonstratesthe beneﬁts of the proposed ecr model with rich.
4846kbp 2016.model(lu and ng, 2017)(choubey and huang, 2018)(choubey et al., 2020)pairwisee2e-onlystructecr.
b350.1651.6752.7852.1650.8952.77.ceafe muc blanc avgf 140.9742.2342.9041.2542.8344.77.
32.7234.0834.4932.2133.9335.66.
48.5949.1049.7049.8450.4352.29.
32.4134.0834.6230.7936.0538.37.b3-50.3551.6850.9751.6051.93.kbp 2017.ceafe muc blanc avgf 1.
-48.6150.5748.8052.0352.82.
-37.2437.836.9238.5340.73.
-31.9433.3931.8633.0234.75.
-42.0443.3642.1443.8045.06.table 1: models’ performance on the kbp 2016 and kbp 2017 datasets.
the performance improvement ofstructecr over e2e-only is signiﬁcant with p < 0.01..document structures and cluster consistency regu-larization for representation learning..4.3 ablation study.
ij.
, adep.
, aspanij.
, acorefij.
ij , and astructij.
two major components in the proposed modelstructecr involve the document structures andthe cluster consistency regularization.
this sec-tion performs an ablation study to reveal the con-tribution of such components for the full model.
first, for the document structures, we examinethe following ablated models: (i) “structecr -x”: where x is one of the ﬁve interaction scoresused to compute the uniﬁed score aij for g (i.e.,asent).
for exam-ijple, “structecr - aspan” implies a variant ofstructecr where the span-based interaction scoreaspanis not included in the compuation of the over-ijall score aij; (ii) “structecr - entity nodes: thismodel excludes the entity mention nodes from theinteraction graph g in structecr (i.e., n = d ∪ eonly); (iii) “structecr - graphcombine”: in-stead of unifying the ﬁve interaction scores in dijinto an overall score aij in equation 1, this modelconsiders each of the ﬁve generated interactionscores as forming a separate interaction graph, thusproducing six different graphs.
the gcn model isthen applied over those ﬁve graphs (using the sameinitial representation vectors vi for the nodes ni inn ).
the outputs of the gcn model for the samenode ni (with different graphs) are then concate-nated to compute the ﬁnal representation vector hifor ni; and (iv) structecr - doc structures: thismodel removes the gcn model from structecr.
as such, the interaction graph g is not used andthe gcn-induced representation vectors hi are re-placed by the initial bert-induced representationvectors vi in the computation for end-to-end reso-lution and consistency regularization..second, for the cluster consistency regular-ization, we evaluate the following ablated mod-els for structecr: (v) structecr - y (y ∈.
ij.
ij.
modelstructecr (full)structecr - asentstructecr - acorefstructecr - aspanijstructecr - adepijstructecr - astructstructecr - entity nodesstructecr - graphcombinestructecr - doc structuresstructecr - linnerstructecr - linterstructecr - lsimstructecr - regularization.
ij.
b376.8675.3775.0775.3274.6675.4474.6775.4174.1575.0974.8075.1374.46.ceafe muc blanc avgf 170.5769.2569.3669.0169.0769.5768.6969.1166.8768.4568.1068.5667.76.
66.4062.4262.9763.4462.7261.8263.0162.3860.2462.2561.9262.0360.74.
69.9969.7369.7470.3269.7669.5369.7169.7466.7868.4467.9868.1267.55.
69.0269.4969.6766.9769.1471.4867.3568.9066.3268.0167.7168.9568.28.table 2: performance on the kbp 2015 dev set..kbp 2016.kbp 2017.nw → df df → nw nw → df df → nw.
pairwisee2e-onlystructecr.
60.5165.8268.19.
58.1162.0165.83.
59.2262.5665.19.
59.1062.5265.12.table 3: cross-domain performance (avgf 1).
nwand df represent news articles and discussion forumdocuments respectively.
x → y implies models trainedon domain x and tested on domain y..{linner, linter, lsim}): these models exclude oneof the regularization terms for the consistency be-tween golden and predicted clusters from the over-all loss function l; and (vi) structecr - regular-ization: this model completely ignores the consis-tency regularization component from structecr.
table 2 shows the performance of the modelson the development data of the kbp 2015 dataset.
as can be seen, the elimination of any componentfrom structecr would signiﬁcantly hurt the per-formance, thus clearly demonstrating the beneﬁtsof the designed document structures and clusterconsistency regularization in structecr..4.4 cross-domain evaluation.
to further demonstrate the beneﬁts for the pro-posed model structecr, we evaluate structecrand the baseline models pairwise and e2e-only inthe cross-domain setting.
in this setting, we aim.
4847to train the models on one domain (the source do-main) and evaluate them on another domain (thetarget domain).
we leverage the kbp 2016 andkbp 2017 datasets for this experiment.
in particu-lar, kbp 2016 annotates ecr data for 85 newswireand 84 discussion forum documents (i.e., two do-mains/genres) while kbp 2017 provides annotateddata for ecr on 83 news articles and 84 discus-sion forum documents.
as such, for each dataset,we consider two setups where documents in onedomain (i.e., newswire or discussion forum) areused for the source domain, leaving documents inthe other domain for the target domain data.
weuse the same hyper-parameters that are tuned onthe development set of kbp 2015 for the modelsin this experiment.
table 3 presents the perfor-mance of the models.
it is clear from the table thatstructecr are signiﬁcantly and substantially betterthan the baseline models (p < 0.01) over differ-ent datasets and settings for the source and targetdomains, thereby conﬁrming the domain general-ization advantages of structecr for ecr..5 conclusion.
we present a novel end-to-end coreference resolu-tion framework for event mentions based on deeplearning.
the novelty in our model is twofold.
first,document structures are introduced to explicitlycapture relevant objects and their interactions indocuments to aid representation learning.
second,several regularization techniques are proposed toexploit the consistencies between human-providedand machine-generated clusters of event mentionsin documents.
we perform extensive experimentson two benchmark datasets for ecr to demonstratethe advantages of the proposed model.
in the future,we plan to extend our models to related problemsin information extraction, e.g., event extraction..acknowledgments.
this research has been supported by the army re-search ofﬁce (aro) grant w911nf-21-1-0112.
this research is also based upon work supported bythe ofﬁce of the director of national intelligence(odni), intelligence advanced research projectsactivity (iarpa), via iarpa contract no.
2019-19051600006 under the better extraction from texttowards enhanced retrieval (better) program.
the views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the ofﬁcial policies, ei-.
ther expressed or implied, of aro, odni, iarpa,the department of defense, or the u.s. govern-ment.
the u.s. government is authorized to re-produce and distribute reprints for governmentalpurposes notwithstanding any copyright annotationtherein.
this document does not contain technol-ogy or technical data controlled under either theu.s. international trafﬁc in arms regulations orthe u.s. export administration regulations..references.
cosmin adrian bejan and sanda harabagiu.
2014. un-supervised event coreference resolution.
in compu-tational linguistics (cl)..david ahn.
2006. the stages of event extraction.
inproceedings of the workshop on annotating andreasoning about time and events..jun araki and teruko mitamura.
2015. joint event trig-ger identiﬁcation and event coreference resolutionin proceedings of thewith structured perceptron.
2015 conference on empirical methods in naturallanguage processing (emnlp)..amit bagga and breck baldwin.
1998..entity-based cross-document coreferencing using the vec-the inter-tor space model.
national conference on computational linguistics(coling)..in proceedings of.
shany barhom, vered shwartz, alon eirew, michaelbugert, nils reimers, and ido dagan.
2019. re-visiting joint modeling of cross-document entity andevent coreference resolution.
in proceedings of the57th annual meeting of the association for compu-tational linguistics (acl)..terra blevins and luke zettlemoyer.
2020. movingdown the long tail of word sense disambiguationwith gloss informed bi-encoders.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics (acl)..arie cattan, alon eirew, gabriel stanovsky, mandarjoshi, and ido dagan.
2020. streamlining cross-document coreference resolution: evaluation andmodeling.
arxiv preprint arxiv:2009.11032..chen chen and vincent ng.
2016. joint inference overa lightly supervised information extraction pipeline:towards event coreference resolution for resource-in proceedings of the associa-scarce languages.
tion for the advancement of artiﬁcial intelligence(aaai)..zheng chen and heng ji.
2009. graph-based eventcoreference resolution.
in proceedings of the work-shop on graph-based methods for natural lan-guage processing..4848zheng chen, heng ji, and robert haralick.
2009. apairwise event coreference model, feature impactand evaluation for event coreference resolution.
inproceedings of the workshop on events in emergingtext types..prafulla kumar choubey and ruihong huang.
2017.event coreference resolution by iteratively unfold-in proceed-ing inter-dependencies among events.
ings of the 2017 conference on empirical methodsin natural language processing (emnlp)..prafulla kumar choubey and ruihong huang.
2018.improving event coreference resolution by modelingcorrelations between event coreference chains andin proceedings of thedocument topic structures.
56th annual meeting of the association for compu-tational linguistics (acl)..prafulla kumar choubey, aaron lee, ruihong huang,and lu wang.
2020. discourse as a function ofevent: proﬁling discourse structure in news arti-in proceedings of thecles around the main event.
58th annual meeting of the association for compu-tational linguistics (acl)..agata cybulska and piek t. j. m. vossen.
2015.
”bagof events” approach to event coreference resolution.
supervised classiﬁcation of event templates.
in int.
j. comput.
linguistics appl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl-hlt)..greg durrett and dan klein.
2013. easy victories anduphill battles in coreference resolution.
in proceed-ings of the 2013 conference on empirical methodsin natural language processing (emnlp)..yin jou huang, jing lu, sadao kurohashi, and vincentng.
2019.improving event coreference resolutionby learning argument compatibility from unlabeleddata.
in proceedings of the 2019 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies (naacl-hlt)..mandar joshi, danqi chen, y. liu, daniel s. weld,luke zettlemoyer, and omer levy.
2019a.
spanbert:improving pre-training by representing and predict-in transactions of the association foring spans.
computational linguistics (tacl)..mandar joshi, omer levy, luke zettlemoyer, anddaniel weld.
2019b.
bert for coreference resolu-tion: baselines and analysis.
in proceedings of the2019 conference on empirical methods in naturallanguage processing (emnlp)..kian kenyon-dean, jackie chi kit cheung, and doinaprecup.
2018. resolving event coreference withsupervised representation learning and clustering-theoriented regularization.
eighth joint conference on lexical and computa-tional semantics (*sem)..in proceedings of.
thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in proceedings of the international con-ference on learning representations (iclr)..heeyoung lee, marta recasens, angel chang, mihaisurdeanu, and dan jurafsky.
2012a.
joint entity andevent coreference resolution across documents.
inproceedings of the 2012 conference on empiricalmethods in natural language processing (emnlp)..heeyoung lee, marta recasens, angel chang, mihaisurdeanu, and dan jurafsky.
2012b.
joint entity andinevent coreference resolution across documents.
proceedings of the 2012 conference on empiricalmethods in natural language processing (emnlp)..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017a.
end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing(emnlp)..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017b.
end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing(emnlp)..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-in proceedings of the 51th annual meet-tures.
ing of the association for computational linguistics(acl)..dekang lin et al.
1998. an information-theoretic def-inition of similarity.
in proceedings of the interna-tional conference on machine learning (icml)..zhengzhong liu, jun araki, eduard hovy, and terukomitamura.
2014. supervised within-document eventcoreference using information propagation.
in pro-ceedings of the international conference on lan-guage resources and evaluation (lrec)..jing lu and vincent ng.
2017..joint learning forevent coreference resolution.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (acl)..jing lu, deepak venugopal, vibhav gogate, and vin-cent ng.
2016. joint inference for event coreferencein transactions of the association forresolution.
computational linguistics (tacl)..yaojie lu, hongyu lin, jialong tang, xianpei han,and le sun.
2020. end-to-end neural event coref-erence resolution.
in corr abs/2009.08153..4849xiaoqiang luo.
2005. on coreference resolution per-formance metrics.
in proceedings of the 2005 con-ference on empirical methods in natural languageprocessing (emnlp)..marc vilain, john burger, john aberdeen, dennis con-nolly, and lynette hirschman.
1995. a model-theoretic coreference scoring scheme.
in sixth mes-sage understanding conference (muc-6)..bishan yang, claire cardie, and peter frazier.
2015. ahierarchical distance-dependent bayesian model forevent coreference resolution.
in transactions of theassociation for computational linguistics (tacl)..george a miller.
1995. wordnet: a lexical database forenglish.
communications of the acm, 38(11):39–41..teruko mitamura, zhengzhong liu, and eduard h.hovy.
2015. overview of tac-kbp 2015 eventin proceedings of the text analysisnugget track.
conference (tac)..teruko mitamura, zhengzhong liu, and eduard h.hovy.
2016. overview of tac-kbp 2016 eventin proceedings of the text analysisnugget track.
conference (tac)..teruko mitamura, zhengzhong liu, and eduard h.hovy.
2017. events detection, coreference and se-quencing: what’s next?
overview of the tac kbp2017 event track.
in proceedings of the text analy-sis conference (tac)..vincent ng.
2010. supervised noun phrase coreferenceresearch: the ﬁrst ﬁfteen years.
in proceedings ofthe 48th annual meeting of the association for com-putational linguistics (acl)..thien huu nguyen, , adam meyers, and ralph grish-man.
2016. new york university 2016 system forkbp event nugget: a deep learning approach.
in pro-ceedings of the text analysis conference (tac)..thien huu nguyen and ralph grishman.
2018. graphconvolutional networks with argument-aware pool-ing for event detection.
in proceedings of the asso-ciation for the advancement of artiﬁcial intelligence(aaai)..haoruo peng, yangqiu song, and dan roth.
2016.event detection and co-reference with minimal su-in proceedings of the 2016 conferencepervision.
on empirical methods in natural language process-ing (emnlp)..karthik raghunathan, heeyoung lee, sudarshan ran-garajan, nathanael chambers, mihai surdeanu, danjurafsky, and christopher manning.
2010. a multi-in proceed-pass sieve for coreference resolution.
ings of the 2010 conference on empirical methodsin natural language processing (emnlp)..amir pouran ben veyseh, franck dernoncourt, dejingdou, and thien huu nguyen.
2020a.
exploiting thesyntax-model consistency for neural relation extrac-tion.
in proceedings of the annual meeting of theassociation for computational linguistics (acl)..amir pouran ben veyseh, tuan ngo nguyen, andthien huu nguyen.
2020b.
graph transformer net-works with syntactic and semantic structures forin proceedings of theevent argument extraction.
findings of the conference on empirical methodsin natural language processing (emnlp)..4850