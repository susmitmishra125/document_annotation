descgen: a distantly supervised datasetfor generating abstractive entity descriptions.
weijia shi1, mandar joshi1, luke zettlemoyer11paul g. allen school of computer scienceengineering, university of washington, seattle, wa{swj0419, mandar90, lsz}@cs.washington.edu.
abstract.
short textual descriptions of entities providesummaries of their key attributes and havebeen shown to be useful sources of back-ground knowledge for tasks such as entity link-ing and question answering.
however, generat-ing entity descriptions, especially for new andlong-tail entities, can be challenging since rele-vant information is often scattered across mul-tiple sources with varied content and style.
weintroduce descgen: given mentions spreadover multiple documents, the goal is to gen-erate an entity summary description.
desc-gen consists of 37k entity descriptions fromwikipedia and fandom, each paired with nineevidence documents on average.
the docu-ments were collected using a combination ofentity linking and hyperlinks to the wikipediaand fandom entity pages, which together pro-vide high quality distant supervision.
the re-sulting summaries are more abstractive thanthose found in existing datasets, and providea better proxy for the challenge of describingnew and emerging entities.
we also proposea two-stage extract-then-generate baseline andshow that there exists a large gap (19.9% inrouge-l) between state-of-the-art modelsand human performance, suggesting that thedata will support signiﬁcant future work.1.
1.introduction.
entity knowledge has been shown to play an im-portant role in various applications including lan-guage modeling (peters et al., 2019), open-domainquestion answering (xu et al., 2016), and dialoguegeneration (qin et al., 2019).
recent studies sug-gest that such entity knowledge can be providedby simple textual descriptions (chen et al., 2019),which can be incorporated to improve downstreamtask performance (nie et al., 2018; logeswaran.
doc 1...are bitcoins, then, really worth anything?
accordingto carl menger’s subjective theory of value, they areworth whatever individuals choose to believe they areworth.
it is clear that many individuals value this newmedium of exchange highly...doc 2...the austrian school of economics has its roots out-side of austria — particularly in the french economistsjean baptiste say and claude-frederic bastiat.
theaustrian school proper began with carl menger, whochallenged the british labor theory of value.
to learnmore about austrian economics go to the website ofthe ludwig von mises institute...doc 3...karl menger was born on january 13, 1902, in vi-enna.
his father was the famous austrian economistcarl menger (1840–1921) who was one of the foundersof marginal utility theory....entity descriptioncarl menger (february 23, 1840 – february 26, 1921)was an austrian economist and the founder of the aus-trian school of economics.
he contributed to the devel-opment of the marginal utility theory and to the formula-tion of a subjective theory of value..table 1: an example from descgen exhibiting the di-versity of source documents and the abstractive natureof the entity description summaries..et al., 2019).
however, manually curating entitydescriptions is labor-intensive and it is challeng-ing to keep pace with the ever growing emergenceof new entities.
in this paper, we present a newdataset descgen for automatically generating en-tity descriptions from relevant documents and men-tions, which provides high quality supervision fora highly abstractive version of this task that targetsearly description of new entities as they emerge.
for example, in table 13, machines are requiredto generate a description of carl menger, givenmultiple documents mentioning him..descgen contains 37k entity descriptions ex-tracted from wikipedia and fandom2.
fandom.
1data.
and.
code.
available.
at.
2fandom is a set of encyclopedias centered around forms.
github.com/swj0419/descgen.
of entertainment such as movies, games etc..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages415–427august1–6,2021.©2021associationforcomputationallinguistics415allows us to capture the key challenge of gener-ating descriptions for emerging entities that arenot in wikipedia because they are less popular orhave just been introduced to the public.
to obtainsource documents of the entities, we collect webdocuments and news articles where entity mentionsare linked using web hyperlinks or an entity linker.
our dataset is distantly supervised in that theseheuristically collected documents are not guaran-teed to contain all the facts required to generatethe description—as would be seen for natural textcollections describing emerging entities.
we alsocarefully annotate a subset of 1,000 examples tosupport more reliable evaluation (see table 2 fordataset statistics)..unlike multi-document summarization thatmakes the assumption that a set of documents tobe summarized are written on the same topic (zopfet al., 2016), descgen only assumes that sourcedocuments mention the entity.
in contrast to an ex-isting entity summarization benchmark (liu et al.,2018, wikisum), descgen is more abstractiveand better approximates challenges faced when de-scribing new entities.
section 4.4 provides moredetails on these comparisons.
overall, our doc-uments for generating a description can cover amuch wider range of topics as well as text genres,including news, blog posts, and scientiﬁc articles.
for instance, the documents 1 and 2 mentioningcarl menger in figure 13 discuss topics on bitcoinsand the austrian school of economics..finally, we also propose a two-stage method thatﬁrst extracts salient sentences relevant to the en-tity and then abstracts them into a description.
wetest a range of models to establish baseline resultswith both automatic and human evaluation.
thebest model based on bart (lewis et al., 2020b)achieves 28.2% in the rouge-l f measure witha signiﬁcant gap compared to the human perfor-mance 48.1%, suggesting there was great room forfuture improvement.
in summary, our contributionsinclude:.
• we propose a new dataset descgen thatincludes challenging, abstractive entity sum-maries.
our dataset contains over 37k pairsof entity descriptions and their associated doc-uments, along with a human-annotated subsetof 1,000 pairs..• we conduct an extensive analysis of proper-ties of the dataset and identify its challenges—selection from largeextractive content.
entitiesdocumentsinput sizeoutput sizehuman-authored descriptions.
wikipedia26,585177,45411,56853598.fandom11,366170,2041,87232403.table 2: basic statistics for descgen.
input size andoutput size refer to the average number of words in thedescription and source documents respectively..amounts of text and abstractive generationfrom it, particularly for emerging entities..• we present a two-stage method and bench-mark various models on our dataset, aimingto facilitate future work on this dataset..2 related work.
existing entity description generation taskand dataset previous works (novikova et al.,2017; cheng et al., 2020; trisedya et al., 2020)mainly take as input some structured data suchas knowledge graphs to generate entity descrip-tions.
however, knowledge graphs, often minedfrom text corpora, are overwhelmingly incompleteon real-world entities and may not be updated inreal-time (dong et al., 2014).
therefore, we focuson generating descriptions from natural languagesources such as web texts and news because theyare often primary sources for entities and have bet-ter coverage of entities across multiple domains.
descgen is most related to wikisum, a recentdataset for generating wikipedia summaries fromtextual sources (liu et al., 2018).
wikisum sourcedocuments primarily come from high-quality ar-ticles cited in the wikipedia pages which makestheir data more extractive (section 4.4).
in contrast,we collect our source documents heuristically us-ing web texts and news, providing a better proxyfor emerging entities where high-quality citationsources may not be available.
in addition, their eval-uation is conducted only on distantly supervisedtest data.
however, our experiments demonstratethat manually annotated data allows for much betterevaluation of model performance (table 7)..multi-document summarization aims to con-dense a cluster of thematically-related documentsinto a short and informative summary.
a widerange of multi-document summarization datasetshave been built for the document understandingand text analysis conferences (over and yen,.
4162004; owczarzak and dang, 2011), news (fab-bri et al., 2019), events (gholipour ghalandariet al., 2020) and wikipedia summaries (liu et al.,2018).
recent work has studied both extractive (ya-sunaga et al., 2017; nallapati et al., 2017; tohalinoand amancio, 2018) and abstractive summariza-tion (banerjee et al., 2015; chali et al., 2017; nay-eem et al., 2018).
however, existing datasets typi-cally are not entity focused and assume the inputdocuments are at least loosely centered around acoherent topic or event..sources we paired entity descriptions withsource documents from three sources: wikilinks,realnews, and fandom using distant supervision.
to capture the challenge of emerging entities, weretrieve source documents that are not in wikipediausing wikilinks and realnews.
we also includespecialized entities in fandom that do not havewikipedia pages.
for quality control, we ﬁlter outentities for which the unigram recall of the entitydescription against its concatenated source docu-ments is lower than 0.6..wikipedia generation our work is also relatedto research on generating wikipedia articles.
forinstance, sauper and barzilay (2009) learn to buildcontent templates using an integer linear programto generate full articles.
similarly, banerjee andmitra (2016) generate wikipedia pages by buildinga topic classiﬁer to assign web retrieved contentsinto relevant sections.
we focus on a different task –generating a short text description that can identifyand best summarize an entity..3 dataset collection.
task deﬁnition given a collection of documentsd = {di|i = 1...n} with mentions linked to thesame entity e, the goal is to generate a descriptionof e. for example, table 13 shows a descriptionof an entity (carl menger) and three source docu-ments with mentions..distant supervision we make use of existingknowledge bases, such as wikipedia and fandom,to collect entity descriptions.
to obtain sourcedocuments and mentions for each entity, we use acombination of hyperlinks to wikipedia pages andan entity linker that links entity mentions in text.
our dataset is distantly supervised in that theseheuristically collected documents are not guaran-teed to contain all the facts required to generate thedescription.
to analyze the quality of distant su-pervision, we collect a smaller veriﬁed set of entitydescriptions using human annotators.
in contrastwith our work, wikisum (liu et al., 2018) useddocuments cited in the wikipedia pages or webpages returned by google as source documents togenerate wikipedia lead sections.
because high-quality citation sources constitute a substantial partof overall documents (75%), their dataset is less ab-stractive than descgen and unsuited for emergingentities where citations are not available..3.1 distantly supervised data collection.
wikilinks wikilinks (singh et al., 2012) is alarge dataset designed for cross-document coref-erence.
it consists of non-wikipedia web pages(discovered using the google search index) con-taining entities that are hyperlinked to wikipedia.
for each entity, we retrieve a collection of webpages in wikilink with the anchor text linked to itand use the lead section of target wikipedia page asits description.
we further parse the html textsof the web pages and extract contents as sourcedocuments..real news to expand the collection of sourcedocuments, we extract entity mentions in real-news (zellers et al., 2019), a large corpus of newsarticles from common crawl.
we ﬁrst conducta longest preﬁx match between the entity surfaceform and text tokens via trie, a preﬁx tree struc-ture that supports efﬁcient string searching.
morespeciﬁcally, we build a trie of entity names whereeach node is a word and its children indicate allpossible continuations from the preﬁx.
after retriv-ing candidates for entity mentions, we use an off-the-shelf entity linking model (gupta et al., 2017)to rank the candidates and add the correspondingnews articles as source documents of the rank-1candidate..fandom fandom3 is a collection of encyclo-pedias, centered around particular subjects andthemes such as movies, tv shows, and games.
itcontains specialized entities that require domainexperts with background knowledge to make ed-its.
entities and their source documents can beautomatically extracted by internal links.
we ﬁlterout entities and only keep those without wikipediapages, which can be viewed as new or emergingentities.
the description of the entity is extracted.
3https://www.fandom.com/.
417entity sourcewikipedia(wikilinks + real news) veriﬁeddistantveriﬁed.
train dev testdistant 21,267 2,659 2,6592992999,092 1,137 1,137201202.fandom.
-.
-.
table 3: number of entities for train, dev and test set..metricsiaa.
r-145.8.r-236.1.r-l47.7.meteor23.3.table 4: inter-annotator agreement (iaa) for human-authored descriptions..from the lead section of its fandom page.
we col-lect data from the 32 largest fandom wikis..3.2 human-authored entity descriptions.
entity descriptions extracted from wikipedia andfandom have been authored and edited by multi-ple community contributors largely independentlyof our source documents.
we collected additionalentity descriptions via upwork,4 a freelancing plat-form, to better analyze how descriptions sourcedfrom documents in our dataset contrast with thosefrom wikipedia and fandom.
we provided theentity and its source documents to annotators onupwork, and asked them to write the entity de-scriptions.
the annotators are also asked to marksentences they used to write the description.
eachentity was assigned to 2 annotators.
we collected500 entity descriptions for dev examples and 500descriptions for test examples..we control the quality of the crowdsourced de-scriptions by ﬁltering annotators who producedlow-quality descriptions.
we ask every candidateto annotate the same 20 examples and use two crite-ria for narrowing down candidates: (1) missing keyinformation in descriptions (2) unjustiﬁed informa-tion in descriptions that cannot be inferred fromsource documents alone.
eventually, we ﬁlteredout 4 annotators and accepted 7 qualiﬁed annota-tors.
the total annotation cost was around $3500..3.3 experimental setup.
all 37k entity description and document pairs inthe dataset are randomly split into train, develop-ment and test sets.
in addition to automaticallycollected descriptions from wikipedia and fan-dom, we use the human-authored descriptions (sec-tion 3.2) as veriﬁed subsets into dev and test splits.
table 3 shows basic statistics of the ﬁnal dataset.
we report model performance on automatically col-lected descriptions (distant) and human-authoreddescriptions (veriﬁed)..the next section provides a detailed analysis ofthe data quality, including annotator agreement and.
4https://www.upwork.com/.
figure 1: distribution of entity domains (outer level)and knowledge sources (inner level)..other aggregate statistics..4 dataset analysis.
an analysis of the data shows that descgen con-tains a high proportion of emerging entities fromdiverse domains, and is more extractive comparedto other multi-document summarization datasets..4.1 statistics.
table 2 shows data statistics.
descgen containsabout 37k entity descriptions from wikipedia andfandom.
on average, each entity has nine sourcedocuments.
we can see that 36% percent of entitiescome from fandom, and therefore have never hada wikipedia page written about them..domain diversity figure 1 shows that desc-gen covers a diverse set of entity domains.
foranalysis, we associate entities in wikipedia withdomains (gpe, loc, per, org, event, com-pany, group and misc) by querying the dbpe-dia knowledge-base (lehmann et al., 2015).
eachentity in fandom is manually categorized into 5 do-mains: movie, game, ﬁction, tv series and cartoonbased on its source wiki.
an analysis of base-line performance by entity type and domain (sec-tion 7.3) reveals a notable drop for less populardomains such as games and fiction, highlightinggeneralization challenges..418wikipediafandom.
r-134.745.6.r-217.827.8.r-l35.844.5.category paraphrasing missing info.
extra detailswikipediafandom.
1615.
2932.
2226.table 5: rouge results on human reference againstwikipedia/fandom descriptions..4.2.inter-annotator agreement.
each entity in the veriﬁed subset has two descrip-tions written by two annotators.
following previ-ous work (chen et al., 2015), we quantify inter-annotator agreement on descriptions by treatingone of the descriptions as the prediction and theother as the reference to compute rouge (lin,2004) and meteor (denkowski and lavie, 2014).
table 4 shows high inter-annotator agreement of47.7 in terms of rouge-l..we additionally measure the agreement on con-tent selection using sentences marked by annota-tors.
in particular, agreement is achieved when bothannotators selected the exact same sentences in allsource documents for an entity.
cohen’s kappais 0.38, which indicates high agreement (brennanand prediger, 1981) considering the strict criterionof reaching agreement..4.3 comparison between human-authored.
and wikipedia/fandom descriptions.
to understand how human-authored descriptionsdiffer with wikipedia and fandom descriptions interms of content and style, we compare them usingautomatic metrics (rouge) and manual evalua-tion..rouge table 5 shows the averaged rougescores of human-authored descriptions againstwikipedia and fandom descriptions.
human-authored descriptions have higher word overlapwith wikipedia descriptions than with fandom de-scriptions..pairwise comparison can humans distinguishbetween wikipedia/fandom and human-authoreddescriptions?
we have two human assessors evalu-ate 50 randomly sampled pairs of human-authoredand wikipedia/fandom descriptions in a blind pair-wise comparison, and ask them to classify de-scriptions into two categories: human-authored orwikipedia/fandom.
the classiﬁcation accuracy inwikipedia and fandom is 64.4% and 61.1% respec-tively and the inter-annotator agreement is 0.67in cohen’s kappa.
the relatively low classiﬁca-tion accuracy suggests that there is no substantial.
table 6: number of times a human-authored de-scription is classiﬁed into error categories withwikipedia/fandom descriptions as reference.
the sam-ple size is 40..quality and style difference in human-authored andwikipedia/fandom descriptions..quality analysis of distant supervision we areinterested in understanding if automatically gath-ered documents can provide enough signals forwriting the entity descriptions.
to study the qual-ity of distant supervision, we manually analyze40 human-authored descriptions that have low n-grams overlap with wikipedia/fandom descrip-tions, in terms of paraphrasing (does the human-authored description express the same meaning butuse different words?
), missing information (doesthe human-authored description miss any informa-tion in wikipedia/fandom description?)
and ex-tra details (does the human-authored descriptioncontain extra details not included in the wikpe-dia/fandom description?).
we use wikipedia andfandom descriptions as the ground truth and clas-sify each human-authored description into one ormore categories.
the results are shown in table 6.we ﬁnd that the difference between the two sourcesof descriptions are mainly caused by paraphrasingand missing information.
this suggests that evenfor entities that have very different human-authoredand extracted descriptions, most of the informationin the wikipedia/fandom descriptions is present inthe documents..4.4 extraction vs abstraction.
generating entity descriptions involves extractingessential information about the entity and condens-ing them into a short description.
to measure howmuch descgen requires paraphasing and com-pressing, we quantify the extractive nature of ourdataset by the measuring extractive fragment cov-erage and density deﬁned in grusky et al.
(2018).
extractive fragment coverage computes the per-centage of words in summary that appear in sourcedocuments:.
coverage(a, s) =.
1|s|.
(cid:88).
f ∈f.
|f |.
419model is used to fuse the selected sentences to adescription of the entity.
we compare a number ofdifferent approaches for each stage, as summarizedin the subsections below..5.1 extractive stage.
trivial concatenates all sentences that mention theentity, along with one sentence before and aftereach.
the content is truncated to the ﬁrst 1,000tokens to ﬁt the token limit of models in the ab-stractive stage..cheating ranks sentences according to their uni-gram recall against the description and selects thetop 15 sentences.
this heuristic demonstrates theeffect of extraction on ﬁnal performance..bert (devlin et al., 2019) with a classiﬁer uses alinear layer stacked on top of the bert outputs andpredict whether a sentence should be selected.
themodel is trained on our training dataset in whichsentences are labeled by the cheating method..5.2 abstractive stage.
we compare three pre-trained language generationmodels, including bart (lewis et al., 2020b),t5 (raffel et al., 2019) and marge (lewis et al.,2020a) to generate abstractive entity descriptions.
we ﬁne-tuned these models on our training datasetin a sequence-to-sequence fashion..t5 is a text-to-text transformer pre-trained on amulti-task mixture of unsupervised and supervisedtasks.
we consider models of two sizes: base andlarge containing 220m and 770m parameters re-spectively.
we use the hugging face version.5.
bart introduces a denoising autoencoder com-bining a bidirectional encoder and auto-regressivedecoder.
it is trained by reconstructing text cor-rupted with a noising function.
we consider thebase model with 139m parameters..marge is a multi-lingual sequence-to-sequencemodel trained by reconstructing target documentsretrieving paraphrased documents in other lan-guages.
it has around 960m parameters..6 experiments.
6.1 evaluation metrics.
following other summarization tasks, we evaluatethe quality of generated descriptions by rouge.
5https://github.com/huggingface/transformers.
figure 2: density and coverage on different datasets.
large variability in y-axis reﬂects the variation in aver-age length of shared token sequences..where a is a concatenation of the source docu-ments, s is the description and f is the set ofshared token sequences in a and s. likewise, ex-tractive fragment density is related to the averagelength of shared token sequences.
for example,an entity description with high coverage and lowdensity shares many individual words with sourcedocuments but almost no long phrases..density(a, s) =.
1|s|.
(cid:88).
f ∈f.
|f |2.
we compare our dataset with several multi-document summarization datasets, including cnn/ daily mail, multi-news (fabbri et al., 2019) andwikisum (liu et al., 2018).
figure 2 presents thedensity and coverage distribution.
the density ofmulti-news, cnn / daily mail and wikisum arehigh, showing that there is much copying of longsequences with respect to source documents.
de-scgen shows high coverage but low density, sug-gesting it is not common to copy long sequencesand the data overall is much more abstractive..5 baselines.
in this section, we introduce several new baselinemethods, building on state-of-the-art pre-trainedmodels.
the input documents can be long (sec-tion 8), making it computationally infeasible totrain end-to-end models.
we instead introduce apipelined approach to generate an entity descriptionin two stages.
in the ﬁrst extractive stage, a selectoris used to identify representative sentences relevantto the entity from multiple source documents.
inthe second abstractive stage, a neural generation.
420distant supervision.
veriﬁed.
extract.
abstract..trivial.
bartt5-baset5-largemargebartt5-baset5-largemargehuman performance.
bert.
r-124.521.723.923.226.923.426.825.140.7.devr-211.39.312.710.613.910.115.113.821.9.r-l22.120.623.421.827.623.927.426.239.9.r-123.421.124.223.026.323.025.424.939.1.testr-210.710.511.110.213.211.614.811.921.8.r-l23.821.123.522.126.624.425.925.039.3.r-127.225.127.726.428.924.927.126.745.2.devr-214.212.815.913.916.911.716.615.736.7.r-l26.424.727.225.827.324.127.525.948.7.r-127.124.926.926.226.725.027.326.345.3.testr-215.913.215.614.016.412.216.114.835.4.r-l26.623.727.325.828.224.827.325.848.1.table 7: experimental results of different baselines evaluated on distantly supervised and veriﬁed dev/test sets..distant supervision.
veriﬁed.
method.
dev.
test.
dev.
test.
uni.
bi.
uni.
bi.
uni.
bi.
uni.
bi.
trivial60.5 23.8 59.9 23.4 78.8 50.4 76.9 43.2bert 65.1 26.1 66.9 27.7 80.4 50.6 77.5 43.8cheating 72.4 31.7 72.3 31.4 81.6 51.9 79.2 44.6.table 8: unigram (uni.)
recall (%) and bigram (bi.)
recall (%) for extractive methods..modelsnon-redundancyfluencyinformativenessfaithfulness.
bart3.84.63.52.7.t5-large3.54.73.22.5.t5-base3.64.63.12.6.table 9: manual evaluation scores on a scale from 1(very poor) to 5 (very high).
all these models usebert in the extractive stage..f1-score (lin, 2004), which measures the overlapof unigram (r-1), bigram (r-2), and the longestmatching sequence of words (r-l).
in addition,we evaluate content selection by unigram and bi-gram recall to assess the importance of the extrac-tive stage.
lastly, in addition to automatic evalu-ation, we also conduct human evaluation for non-redudancy, ﬂuency, informativeness, and accuracy..6.2 experimental results.
automatic evaluation in table 8, we report theexperimental results in the extractive stage.
weobserve that bert consistently outperforms theunsupervised method trivial, suggesting that train-ing a model to predict sentence relevance can bringin immediate improvement in content selection.
meanwhile, the performance of bert still lagsbehind the upper bound deﬁned by cheating by1.7-7.3% in unigram..table 7 presents rouge scores of various base-lines in the abstractive stage.
t5-large and bart.
show similar performance and outperform othermodels for both distant supervision and veriﬁedsubsets, by a small margin.
increasing model sizefrom t5-base (220m) to t5-large (770m) parame-ters leads to a relatively large performance gain.
the human baseline is superior to all the mod-els and maintains a r-l score over 33 in distantsupervision and 48 in the veriﬁed subset.
thelarge gap between the human baseline and the best-performing model shows there is much room forfuture work..manual evaluation we present two human as-sessors with source documents and descriptionsgenerated from different abstractive models andasked them to rate descriptions in terms of non-redundancy (does the description avoid repeat-ing information?
), ﬂuency (is the description well-formed and gramatically correct?
), informativeness(does the description capture the salient informa-tion about the entity?)
and faithfulness (is the de-scription faithful to the source text?).
we comparedbart, t5-large, and t5-base.
for each model,we selected 100 descriptions and showed outputsof models to assessors side by side without reveal-ing which model generates them.
the score foreach description was averaged between two asses-sors.
as can be seen from table 9, bart showsstrong performance on all dimensions, except forﬂuency.
overall, all three models can generate ﬂu-ent descriptions (high ﬂuency) but struggle withproducing accurate statements (low faithfulness).
in most cases of low faithfulness, we observe thatthe model directly copies words from the input thatare not relevant to the entity as part of the descrip-tion or synthesize information that are not directlyinferable from the input..421wikipedia descriptioncarl menger (february 23, 1840 – february 26, 1921)was an austrian economist and the founder of the aus-trian school of economics.
he contributed to the devel-opment of the marginal utility theory and to the formula-tion of a subjective theory of value.
human-authored descriptioncarl menger is an austrian economist and one offounders of marginal utility theory.
he challengedthe british labor theory of value and proposed subjec-tive theory of value.
he founded the austrian school ofeconomics.
bartcarl menger was an austrian economist and one of thefounders of marginal utility theory.
t5-basecarl menger was born on january 13, 1902, in vienna.
he was one of the founders of marginal utility theory.
t5-largecarl menger was an austrian economist.
margecarl menger (born january 13, 1902) was an austrianeconomist..table 10: entity descriptions for carl menger gener-ated by different models.
red text indicates incorrectinformation in predictions while green text indicatesinformation in the wikipedia and human-authored de-scriptions that was not covered by any of the modelpredictions..7 analysis.
in this section, we perform qualitative and quantita-tive analysis of baseline results to better understandstrengths and weaknesses of models, and hypothe-size avenues for future work..7.1 case study.
a qualitative analysis of model predictions sug-gests that these models tend not to generate novelwords in the description, and mostly copy wordsfrom the original text.
the entity-centric natureof descgen makes extractive content selectiondifﬁcult as evidenced by the gap between bertextraction and the cheating model (section 6.2).
for example, table 10 shows the model-generatedentity descriptions for carl menger using sourcedocuments from table 13. bart, one of the bestperforming baselines, generates a description thathas highest overlap with the wikipedia description,but it still misses some important facts.
t5-baseand marge confuse carl menger and his son,and incorrectly include information that does notdescribe the target entity..models.
bartt5-baset5-large.
name-onlyfandom wiki.
16.616.216.8.
12.712.511.7.regularfandom wiki.
28.424.527.6.
27.525.826.1.table 11: rouge-l scores for models evaluated on theveriﬁed test set.
name-only and regular refer to mod-els using only the entity name as the input and modelsusing source documents respectively..wikipedia rouge-lgpelocperorgeventgroupcompany.
28.628.523.726.425.620.221.4.fandom rouge-lmoviegamefictioncartoonstv series.
28.122.525.326.427.6.table 12: rouge-l scores for bert+bart evalu-ated on different entity domains in the veriﬁed test set..7.2 entity knowledge in pre-trained models.
bart, t5, and marge are language models pre-trained on text corpora including wikipedia andcommon crawl.
the parameters of the models ap-pear to contain substantial linguistic and factual in-formation (petroni et al., 2019; peters et al., 2018).
in particular, we wonder if entity-related knowl-edge is captured in the pretraining stage and inves-tigate the following questions: (a) can the modelmemorize entity descriptions in pretraining stage?
(b) does the memorized knowledge improve modelperformance on generating entity descriptions?.
to investigate the questions, we test the model’sability to write a description given only the entityname instead of source documents.
we train themodel on our training dataset to adapt to the style ofwikipedia in a similar way.
the results are shownin table 11. considering the name-only baselines,we can see that all of them perform worse on fan-dom entities than wikipedia entities.
however, theregular baselines perform similarly on fandom andwikipedia.
this result suggests that facts aboutentities learnt in pretraining stage have much lessinﬂuence on model performance when source doc-uments are provided..7.3 entity type.
to understand how the performance of the modelsvaries with different types of entities, we report theperformance breakdown for different entity types intable 12. among domains in wikipedia, our modelobtains low scores on group and company, suggest-.
422ing that they are more challenging than other do-mains.
in fandom, entities from the game domainprove to be most difﬁcult..in summary, our analysis suggests there is roomfor improvement in extractive content selection andabstractive generation, particularly for new andemerging entities from less popular domains..8 conclusion.
in this work, we introduce descgen, a newdataset for generating entity descriptions from men-tions.
descgen contains 37k pairs of entity de-scriptions from wikipedia and fandom, and 481kautomatically gathered source documents basedon distant supervision.
we also present a cleanhuman-authored subset of 1,000 pairs for test.
weshow that, as compared to existing benchmarks,descgen requires more abstractive summaries,which we argue better approximate the challengeof describing emerging entities.
we also show thatthe performance of state-of-art models is far fromhuman levels, suggesting that our task remains asigniﬁcant challenge with room for improvement.
our study points to an interesting research direc-tion on modeling entity knowledge from contexts.
we hope it will facilitate future work on incorpo-rating entity knowledge into downstream tasks andgenerating descriptions for emerging entities..acknowledgements.
this work wassupported in part by thearo (arow911nf-16-1-0121) and the nsf(iis1562364).
the authors would like to thank ariholtzman, bhargavi paranjape, elizabeth clark,terra blevins and anonymous reviewers for helpfulcomments..references.
siddhartha banerjee and prasenjit mitra.
2016. wiki-write: generating wikipedia articles automatically.
in ijcai, pages 2740–2746..siddhartha banerjee, prasenjit mitra, and kazunarisugiyama.
2015. multi-document abstractive sum-marization using ilp based multi-sentence compres-sion.
in ijcai..yllias chali, moin tanvee, and mir tafseer nayeem.
2017. towards abstractive multi-document sum-marization using submodular function-based frame-in pro-work, sentence compression and merging.
ceedings ofthe eighth international joint con-ference on natural language processing (volume2: short papers), pages 418–424, taipei, taiwan.
asian federation of natural language processing..mingda chen, zewei chu, yang chen, karl stratos,and kevin gimpel.
2019. enteval: a holistic eval-uation benchmark for entity representations.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 421–433..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr dollar, andc lawrence zitnick.
2015. microsoft coco captions:data collection and evaluation server.
arxiv e-prints,pages arxiv–1504..liying cheng, dekun wu, lidong bing, yan zhang,zhanming jie, wei lu, and luo si.
2020. ent-desc: entity description generation by exploringknowledge graph.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1187–1197, online.
as-sociation for computational linguistics..michael denkowski and alon lavie.
2014. meteoruniversal: language speciﬁc translation evaluationfor any target language.
in proceedings of the ninthworkshop on statistical machine translation, pages376–380..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..xin dong, evgeniy gabrilovich, geremy heitz, wilkohorn, ni lao, kevin murphy, thomas strohmann,shaohua sun, and wei zhang.
2014. knowledgevault: a web-scale approach to probabilistic knowl-in proceedings of the 20th acmedge fusion.
sigkdd international conference on knowledgediscovery and data mining, pages 601–610..alexander r fabbri, irene li, tianwei she, suyi li,and dragomir r radev.
2019. multi-news: alarge-scale multi-document summarization datasetand abstractive hierarchical model.
arxiv preprintarxiv:1906.01749..robert l brennan and dale j prediger.
1981. co-efﬁcient kappa: some uses, misuses, and alterna-tives.
educational and psychological measurement,41(3):687–699..demian gholipour ghalandari, chris hokamp,nghia the pham, john glover, and georgiana ifrim.
2020. a large-scale multi-document summarizationdataset from the wikipedia current events portal..423in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages1302–1308, online.
association for computationallinguistics..max grusky, mor naaman, and yoav artzi.
2018.newsroom: a dataset of 1.3 million summaries withdiverse extractive strategies.
in naacl-hlt, pages708–719..nitish gupta, sameer singh, and dan roth.
2017. en-tity linking via joint encoding of types, descrip-in proc.
of the conference ontions, and context.
empirical methods in natural language processing(emnlp)..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..jens lehmann, robert isele, max jakob, anja jentzsch,dimitris kontokostas, pablo n mendes, sebastianhellmann, mohamed morsey, patrick van kleef,sören auer, et al.
2015. dbpedia–a large-scale, mul-tilingual knowledge base extracted from wikipedia.
semantic web, 6(2):167–195..mike lewis, marjan ghazvininejad, gargi ghosh, ar-men aghajanyan, sida wang, and luke zettlemoyer.
2020a.
pre-training via paraphrasing.
advances inneural information processing systems, 33..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020b.
bart: denoising sequence-to-sequencepre-training for natural language generation, trans-in proceedings of thelation, and comprehension.
58th annual meeting of the association for compu-tational linguistics, pages 7871–7880, online.
as-sociation for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..peter j liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser,andnoam shazeer.
2018. generating wikipedia byarxiv preprintsummarizing long sequences.
arxiv:1801.10198..lajanugen logeswaran, ming-wei chang, kenton lee,kristina toutanova, jacob devlin, and honglak lee.
2019. zero-shot entity linking by reading entity de-scriptions.
arxiv preprint arxiv:1906.07348..ramesh nallapati, feifei zhai, and bowen zhou.
2017.summarunner: a recurrent neural network based se-quence model for extractive summarization of docu-ments.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 31..mir tafseer nayeem, tanvir ahmed fuad, and yl-lias chali.
2018. abstractive unsupervised multi-document summarization using paraphrastic sen-in proceedings of the 27th inter-tence fusion.
national conference on computational linguistics,pages 1191–1204, santa fe, new mexico, usa.
as-sociation for computational linguistics..feng nie, yunbo cao, jinpeng wang, chin-yew lin,and rong pan.
2018. mention and entity descriptionco-attention for entity disambiguation.
in aaai..jekaterina novikova, ondˇrej dušek, and verena rieser.
2017. the e2e dataset: new challenges for end-in proceedings of the 18th an-to-end generation.
nual sigdial meeting on discourse and dialogue,pages 201–206, saarbrücken, germany.
associationfor computational linguistics..paul over and james yen.
2004. an introduction to.
duc-2004..karolina owczarzak and hoa trang dang.
2011.overview of the tac 2011 summarization track:guided task and aesop task.
in proceedings of thetext analysis conference (tac 2011), gaithersburg,maryland, usa, november..matthew peters, mark neumann, luke zettlemoyer,and wen-tau yih.
2018. dissecting contextualword embeddings: architecture and representation.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 1499–1509, brussels, belgium.
associationfor computational linguistics..matthew e peters, mark neumann, robert logan, royschwartz, vidur joshi, sameer singh, and noah asmith.
2019. knowledge enhanced contextual wordin proceedings of the 2019 con-representations.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 43–54..fabio petroni, tim rocktäschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..libo qin, yijia liu, wanxiang che, haoyang wen,yangming li, and ting liu.
2019. entity-consistentend-to-end task-oriented dialogue system with kbretriever.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages133–142, hong kong, china.
association for com-putational linguistics..424colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..christina joan sauper and regina barzilay.
2009.automatically generating wikipedia articles: astructure-aware approach.
association for compu-tational linguistics..sameer singh, amarnag subramanya, fernandopereira, and andrew mccallum.
2012. wikilinks:a large-scale cross-document coreference corpus la-beled via links to wikipedia.
university of mas-sachusetts, amherst, tech.
rep. um-cs-2012, 15..jorge v tohalino and diego r amancio.
2018. ex-tractive multi-document summarization using mul-tilayer networks.
physica a: statistical mechanicsand its applications, 503:526–539..bayu trisedya, jianzhong qi, and rui zhang.
2020.sentence generation for entity description withcontent-plan attention.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 34,pages 9057–9064..thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pier-ric cistac, morgan funtowicz, joe davison, samshleifer, et al.
2020. transformers: state-of-the-art natural language processing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing: system demonstrations,pages 38–45..kun xu, siva reddy, yansong feng, songfang huang,and dongyan zhao.
2016. question answering onfreebase via relation extraction and textual evidence.
in proceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 2326–2336, berlin, germany.
association for computational linguistics..michihiro yasunaga, rui zhang, kshitijh meelu,ayush pareek, krishnan srinivasan, and dragomirradev.
2017. graph-based neural multi-documentsummarization.
in proceedings of the 21st confer-ence on computational natural language learning(conll 2017), pages 452–462, vancouver, canada.
association for computational linguistics..rowan zellers, ari holtzman, hannah rashkin,yonatan bisk, ali farhadi, franziska roesner, andyejin choi.
2019. defending against neural fakenews.
in advances in neural information process-ing systems, pages 9054–9065..markus zopf, maxime peyrard, and judith eckle-kohler.
2016. the next step for multi-documentsummarization: a heterogeneous multi-genre cor-pus built with a novel construction approach.
in pro-ceedings of coling 2016, the 26th internationalconference on computational linguistics: techni-cal papers, pages 1535–1545..425a appendix.
a.1 experimental details.
all the abstractive models are initialized fromthe pretrained models.
the bart, t5-base andt5-large are adopted by the huggingface frame-work (wolf et al., 2020).
the marge model isadopted by the ofﬁcial authors (lewis et al., 2020a).
we apply the adam optimizer (kingma and ba,2014) with β1 = 0.9, β2 = 0.999, (cid:15) = 1e − 08.the learning rate is selected from {1e-3, 0.5e-3,1e-4, 0.5e-4, 1e-5, 0.5e-5}.
the best learning ratefor bart, t5-base, t5-large and marge is 1e-5,1e-5, 0.5e-5,0.5e-4.
we use beam searching withbeam-size 5 as decoding algorithm, which is se-lected from {5, 10, 15, 20}.
we use the batch sizeof 5 for all models due to memory limit..a.2 more examples.
see next page..426doc 1...it sometimes gets confusing in the global village , where technology, ﬁnance, cross-cultural interactions, and expandingethnic diasporas are tearing apart the relationship between borders and making multiple identities possible.
hence, ang leeis a taiwanese artist who directs american ﬁlms, but he is also an american ﬁlm director of chinese movies.
as a memberof the sinosphere, enlarged by ﬁfty million overseas chinese, ang is not only a creative individual who makes our worldmore interesting and prosperous.
he also helps to bridge between nations and cultures and to produce a sino-americansynergy that is more conducive to peace than a contingency of chinese and u.s. diplomats...doc 2...the life of pi.
one of the most interesting ﬁlm adaptations set for release in 2012 is brokeback mountain fame.
surajsharma, who has no previous acting experience, will play the central character, piscine patel.
based on the novel by yannmartel, it is being brought to the big screen by ang lee...doc 3comic character hulk is dr. bruce banner, who becomes a green monster with powerful strength after an experiment wentbad, or well, depending on who you ask.
in 2003, director ang lee’s ﬁlm hulk brought this character to the big screen, butwas poorly received by hulk’s fans...wikipedia descriptionang lee, (born october 23, 1954, p’ing-tung county, taiwan), is an taiwan-born ﬁlm director who transitioned fromdirecting chinese ﬁlms to major english-language productions.
human-authored descriptionang lee is a taiwanese director who directs american and chinese ﬁlms.
he is a director of the life of pi and hulk andregarded as second new wave of taiwanese directors.
bartang lee is a taiwanese ﬁlm director and screenwriter.
t5-baseang lee is a taiwanese ﬁlm director.
t5-largeang lee is a taiwanese ﬁlm directors and screenwriter.
doc 1...in the summer of 1994, arthur managed to get himself and his family (as well as harry and hermione) tickets for the1994 quidditch world cup from ludovic bagman because arthur had helped otto bagman, ludo’s brother, out of a minorscrape.
arthur was among the weasleys who fetched harry from the dursley family via the floo network.
while there, heexpressed his fascination at various muggle artefacts in the dursley’s house.the group sat in the top box, where they wereconfronted by the malfoy family, who were there by a personal invitation from the minister himself, though both arthurand lucius were able to restrain themselves out of respect for cornelius fudge...doc 2...before working at the ministry, he was a beater for both the wimbourne wasps and the english national quidditch team.
he had a brother named otto bagman.
he also tended to play dirty when gambling and betting as he tried to ﬁnd loopholesor even pay in fake money/gold...doc 3...a lawn mower is found in the muggle studies classroom at hogwarts school of witchcraft and wizardry.
arthur oncehelped ludovic bagman’s brother, otto bagman, by smoothing over a problem involving a lawn mower enchanted withmagical powers.
as thanks, ludo got arthur prime tickets to the 1994 quidditch world cup...fandom descriptionotto bagman was the brother of ludovic bagman.
he once had a problem with a magical lawn mower, a muggle artifact.
arthur weasley helped him out with the problem, and was rewarded by ludo with tickets to the 1994 quidditch world cupﬁnal.
human-authored descriptionotto bagman is the brother of ludovic bagman.
he had a problem involving a lawn mower enchanted with magical powers.
he was helped by arthur and gave arthur prime tickets to the 1994 quidditch world cup.
bartotto bagman was a ﬁctional character in the 1994 ﬁlm harry potter.
t5-baseotto bagman was an english footballer who played for the wimbourne wasps and the english national quidditch team.
he also played dirty when gambling and betting as he tried to ﬁnd loopholes or even pay in fake money.
t5-largeotto bagman was a brother of ludovic bagman..table 13: examples of entity descriptions generated by our model.
red text indicates incorrect information inpredictions while green text indicates information in the wikipedia and human-authored descriptions that was notcovered by any of the model predictions..427