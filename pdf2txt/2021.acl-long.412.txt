ctfn: hierarchical learning for multimodal sentiment analysis usingcoupled-translation fusion networkjiajia tang1∗, kang li1∗, xuanyu jin1, andrzej cichocki2 3, qibin zhao4, wanzeng kong1†1key laboratory of brain machine collaborative intelligence of zhejiang province,school of computer science and technology, hangzhou dianzi university, china2skolkovo institute of science and technology, moscow, russia3systems research institute, polish academy of science, warsaw, poland4center for advanced intelligence project, riken{hdutangjiajia, jxuanyu599}@163.com{likang bro, kongwanzeng}@hdu.edu.cn{a.cichocki, qibin.zhao}@riken.jp.
abstract.
multimodal sentiment analysis is the challeng-ing research area that attends to the fusion ofmultiple heterogeneous modalities.
the mainchallenge is the occurrence of some missingmodalities during the multimodal fusion pro-cedure.
however, the existing techniques re-quire all modalities as input, thus are sensi-tive to missing modalities at predicting time.
in this work, the coupled-translation fusionnetwork (ctfn) is ﬁrstly proposed to modelbi-direction interplay via couple learning, en-suring the robustness in respect to missingmodalities.
speciﬁcally, the cyclic consistencyconstraint is presented to improve the transla-tion performance, allowing us directly to dis-card decoder and only embraces encoder oftransformer.
this could contribute to a muchlighter model.
due to the couple learning,ctfn is able to conduct bi-direction cross-modality intercorrelation parallelly.
based onctfn, a hierarchical architecture is further es-tablished to exploit multiple bi-direction trans-lations, leading to double multimodal fusingembeddings compared with traditional trans-lation methods.
moreover, the convolutionblock is utilized to further highlight explicitinteractions among those translations.
forevaluation, ctfn was veriﬁed on two mul-timodal benchmarks with extensive ablationstudies.
the experiments demonstrate that theproposed framework achieves state-of-the-artor often competitive performance.
addition-ally, ctfn still maintains robustness whenconsidering missing modality..introduction.
1sentiment analysis has witnessed many signiﬁcantadvances in the artiﬁcial intelligence community, inwhich text (yadollahi et al., 2017), visual (kahouet al., 2016), and acoustic (luo et al., 2019) modal-ities are primarily employed to the related research.
∗∗equal contribution††corresponding author: wanzeng kong.
respectively, allowing to exploit the human emo-tional characteristic and intention effectively (denget al., 2018).
intuitively, due to the consistency andcomplementarity among different sources, the jointrepresentation attend to reason about multimodalmessages, which are capable of boosting the perfor-mance of the speciﬁc task (pan et al., 2016; gebruet al., 2017; al hanai et al., 2018)..multimodal fusion procedure is to incorporatemultiple knowledge for predicting a precise andproper outcome (baltruˇsaitis et al., 2018).
histori-cally, the existing fusion has been done generallyby leveraging the model-agnostic process, consid-ering the early fusion, late fusion, and hybrid fu-sion technique (poria et al., 2017a).
among those,early fusion focussed on the concatenation of theunimodal presentation (d’mello and kory, 2015).
on the contrast, late fusion performs the integra-tion at the decision level, by voting among all themodel results (shutova et al., 2016).
as to the hy-brid fusion, the output comes from the combinationof the early fusion and unimodal prediction (lanet al., 2014).
nevertheless, multimodal sentimentsequences often consists of unaligned properties,and the traditional fusion manners are failed to takethe heterogeneity and misalignment into accountcarefully, which raises a question on investigatingthe more sophisticated models and estimating emo-tional information.
(tsai et al., 2020; niu et al.,2017)..recently, transformer-based multimodal fusionframework has been developed to address the aboveissues with the help of multi-head attention mech-anism (rahman et al., 2020; le et al., 2019; tsaiet al., 2019).
by introducing the standard trans-former network (vaswani et al., 2017) as the basis,tsai et al.
(tsai et al., 2019) captured the integra-tions directly from unaligned multimodal streamsin an end-to-end fashion, latently adapted streamsfrom one modality to another with the cross-modal.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5301–5311august1–6,2021.©2021associationforcomputationallinguistics5301(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2778).
cycle consistencyconstraint.
(cid:1779)(cid:1814)(cid:1803)(cid:1815)(cid:1804)(cid:1805)(cid:1818).
(cid:1779)(cid:1814)(cid:1803)(cid:1815)(cid:1804)(cid:1805)(cid:1818).
decoder.
fusion embedding.
fusion embedding.
cycle consistencyconstraint.
(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2779).
(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2778).
fusion embedding.
decoder.
encoder.
fusion embedding.
encoder.
fusion embedding.
encoder.
decoder.
(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2779).
(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2778).
(cid:1787)(cid:1815)(cid:1804)(cid:1801)(cid:1812)(cid:1809)(cid:1820)(cid:1825)(cid:2779).
(a) ctfn model..(b) mctn model..(c) transmodality model..figure 1: comparison of ctfn with existing translation-based models.
in our model, the cyclic consistencyconstraint is presented to improve the translation performance, allowing us directly to discard decoder and onlyembrace encoder of transformer.
this could contribute to a much lighter model.
due to the couple learning,ctfn is able to conduct bi-direction cross-modality intercorrelation parallelly, ensuring the robustness in respectto missing modalities..attention module, regardless of the need for align-ment.
furthermore, wang et al.
(wang et al., 2020)proposed a parallel transformer unit, allowing toexplore the correlation between multimodal knowl-edge effectively.
however, the decoder componentof standard transformer is employed to improvethe translation performance, which may lead tosome redundancy.
moreover, the explicit interac-tion among cross-modality translations were notconsidered.
essentially, compared to our ctfn,their architecture require access to all modalities asinputs for exploring multimodal interplay with thesequential fusion strategy, thus are rather sensitivein the case of multiple missing modalities..in this paper, ctfn is proposed to model bi-directional interplay based on coupled learning, en-suring the robustness in respect to missing modali-ties.
speciﬁcally, the cyclic consistency constraintis proposed to improve the translation performance,allowing us directly to discard decoder and onlyembrace encoder of transformer.
this could con-tribute to a much lighter model.
thanks to the cou-ple learning, ctfn is able to conduct bi-directioncross-modality intercorrelation parallelly.
takectfn as a basis, a hierarchical architecture is es-tablished to exploit modality-guidance translation.
then, the convolution fusion block is presented tofurther explore the explicit correlation among theabove translations.
importantly, based on the paral-lel fusion strategy, our ctfn model still providesﬂexibility and robustness when considering onlyone input modality..for evaluation, ctfn was veriﬁed on two multi-modal sentiment benchmarks, cmu-mosi (zadeh.
et al., 2016) and meld (poria et al., 2019).
theexperiments demonstrate that ctfn could achievethe state-of-the-art or even better performance com-pared to the baseline models.
we also provideseveral extended ablation studies, to investigateintrinsic properties of the proposed model..2 related work.
the off-the-shelf multimodal sentiment fusiontwo leading groups:architecture comprisestranslation-based and non-translation based model.
non-translation based: recently, rnn-basedmodels, considering gru and lstm, have re-ceived signiﬁcant advances in exploiting thecontext-aware information across the data (yanget al., 2016; agarwal et al., 2019).
bc − lst m(poria et al., 2017b) and gm e − lst m (chunget al., 2014) presented a lstm-based model to re-trieve contextual information, where the unimodalfeatures are concatenated into a unit one as theinput information.
similarly, m eld − base (po-ria et al., 2019) leveraged the concatenation of au-dio and textual features on the input layer, andemployed gru to model sentimental context.
incontrast, chf usion (majumder et al., 2018) em-ployed the rnn-based hierarchical structure todraw ﬁne-grained local correlations among themodalities, and the empirical evidence illustratessuperior advances compared to the simple concate-nation of unimodal presentation.
on the basis ofrnn, m m m u −ba (ghosal et al., 2018) furtheremployed multimodal attention block to absorbthe contribution of all the neighboring utterances,which demonstrates that the attention mechanism.
5302(cid:2180)(cid:3028).
bigru & dense.
layer 1…layer l.(cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1775)(cid:1372)(cid:1796)(cid:4666)(cid:2180)(cid:3028)(cid:1314)(cid:4667).
(cid:2180)(cid:3049)(cid:1314).
reconstruction error(cid:1865)(cid:1861)(cid:1866) (cid:513) (cid:2180)(cid:2183) (cid:3398) (cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1796)(cid:1372)(cid:1775)(cid:4666)(cid:2180)(cid:3049)(cid:1314)(cid:4667) (cid:513).
discriminator(cid:1865)(cid:1861)(cid:1866) (cid:513) (cid:2180)(cid:2183) (cid:3398) (cid:2180)(cid:2183)(cid:1314) (cid:513).
(cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1796)(cid:1372)(cid:1775)(cid:4666)(cid:2180)(cid:3049)(cid:1314)(cid:4667).
(cid:2180)(cid:3028)(cid:1314).
layer l…layer 1.
(cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1775)(cid:1372)(cid:1796).
(cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1796)(cid:1372)(cid:1775).
bigru & dense.
reconstruction error(cid:1865)(cid:1861)(cid:1866) (cid:513) (cid:2180)(cid:2204) (cid:3398) (cid:1794)(cid:1818)(cid:1801)(cid:1814)(cid:1775)(cid:1372)(cid:1796)(cid:4666)(cid:2180)(cid:3028)(cid:1314)(cid:4667) (cid:513).
discriminator(cid:1865)(cid:1861)(cid:1866) (cid:513) (cid:2180)(cid:2204) (cid:3398) (cid:2180)(cid:2204)(cid:1314) (cid:513).
(cid:2180)(cid:3049).
layer (cid:2191) (cid:4666)(cid:2235) (cid:1372) (cid:2236)(cid:4667).
(cid:2171)(cid:2203)(cid:2202)(cid:2198)(cid:2203)(cid:2202)(cid:2235)(cid:1372)(cid:2236).
addition.
positionfeed-forward.
layernorm.
addition.
multi-head.
(cid:2173)(cid:2235) (cid:2167)(cid:2235)layernorm.
(cid:2178)(cid:2235).
(cid:2180)(cid:2235).
figure 2: ctfn: xa and xv refer to the features of modality audio and video respectively.
the blue lineindicates the primal process, and the yellow line indicates the dual procedure.
note that the cyclic consistencyconstraint is presented to improve the translation performance, allowing us directly to discard decoder and onlyembrace encoder of transformer.
and thanks to couple learning, ctfn could combine primal and dual processinto a coupled structure, ensuring the robustness in respect to missing modalities..can utilize the neighborhood contribution for inte-grating the contextual information.
however, allthese methods are suitable for the low-level pre-sentation within the single modality with a non-translation manner, which may be easily sensitiveto the noisy terms and missing information in thesources..translation-based model: inspired by the re-cent success of sequence to sequence (seq2seq)models (lin et al., 2019; ?)
in machine translation,(pham et al., 2019) and (pham et al., 2018) pre-sented multimodal fusion model via the essentialinsight that translates from a source modality toa target modality, which is able to capture muchmore robust associations across multiple modalities.
m ct n model incorporated a cyclic translationmodule to retrieve the robust joint representationbetween modalities in a sequential manner, e.g.,the language information ﬁrstly associated withthe visual modality, and latently translated into theacoustic modality.
compared with the m ct n ,seq2seq2sent introduced a hierarchical fusionmodel using the seq2seq methods.
for the ﬁrstlayer, the joint representation of a modality pair istreated as an input sequence for the next seq2seqlayer in an attempt to decode the third modality.
inspired by the success of the transformer-basedmodel, tsai et al.
introduced a directional cross-modality attention module to extend the standardtransformer network.
follow the basic idea of tsaiet al., wang et al.
provided a novel multimodalfusion cell which is comprised of two standardtransformers, embracing the association with amodality pair during the forward and backwardtranslation implicitly.
however, all existing modelsadopt sequential multimodal fusion architecture,.
which requires all modalities as input, thereforethey can be sensitive to the case of multiple miss-ing modalities.
moreover, the explicit interactionsamong cross-modality translations were not con-sidered..3 methodology.
in this section, we ﬁrstly present ctfn (figure 2),which is capable of exploring bi-direction cross-modality translation via couple learning.
on thebasis of ctfn, a hierarchical architecture is estab-lished to exploit multiple bi-direction translations,leading to double multimodal fusing embeddings(figure 4).
then, the convolutional fusion block(figure 3) is applied to further highlight explicitcorrelation among cross-modality translations..3.1 preliminaries.
the two benchmarks consist of three modalities,audio, video and textual modality.
speciﬁcally,the above utterance-level modalities are denoted asxa ∈ rta×da, xv ∈ rtv×dv and xt ∈ rtt×dt,respectively.
the number of utterances is presentedas ti(i ∈ {a, v, t}), and di(i ∈ {a, v, t}) standsfor the dimension of the unimodality features..3.2 coupled-translation fusion network.
for simplicity, we consider two unimodality pre-sentation xa and xv explored from audio (a)in the primal pro-and video (v), respectively.
cess of ctfn, we focus on learning a directionaltranslator t rana→v (xa, xv) for translating themodality audio to video.
then, the dual pro-cess aims to learn an inverse directional translatort ranv →a(xv, xa), allowing for the translationfrom modality video to audio.
inspired by the suc-.
5303modality t.modality a.modality v.tran(cid:2157) (cid:1374) (cid:2178).
(cid:1839)(cid:3028)(cid:3049).
concatenate.
feature domain.
tran(cid:2157) (cid:1374) (cid:2176).
(cid:1839)(cid:3028)(cid:3047).
niamod. laropmet.figure 3: multimodal convolutional fusion block: mat ∈ rt ×fat and mav ∈ rt ×fav refer to the cross-modality translations, where t and f∗ are the size of time and feature domain respectively.
subsequently, matand mav are concatenated along the feature domain, and the convolution operation is utilized to exploit the localand explicit interplay between cross-modality translations..convolutional fusion block.
cess of transformer in natural language process-ing, the encoder of transformer is introduced to ourmodel as the translation block, which is an efﬁcientand adaptive manner for retrieving the long-rangeinterplay along the temporal domain.
importantly,the cyclic consistency constraint is presented to im-prove the translation performance.
and due to thecouple learning, ctfn is able to combine primaland dual process into a coupled structure, ensuringthe robustness in respect to missing modalities..for the primal task, xa ∈ rta×da is ﬁrstly de-livered to a densely connected layer for receiving alinear transformation xa ∈ rta×la, where la isthe output dimension of the linear layer.
and thecorresponding query matrix, key matrix and valuematrix are denoted as qa = xawqa ∈ rta×la ,ka = xawka ∈ rta×la, va = xawva ∈rta×la, where wqa ∈ rla×la , wka ∈rla×la and wva ∈ rla×la are weight matrixes.
the translation from modality a to v is performed, = t rana→v (xa, xv) ∈ rta×lv , whereas xvxvla is the scalecoefﬁcient.
note that the input xa is directly de-livered to the translation process, while the inputxv is used to analyze the difference between real,,.
subsequently, xvdata xv and fake output xvis passed through the t ranv →a, leading to the,, xa),reconstruct output xaand the xa is only used to calculate the diversitybetween the real and reconstruct data.., refers to the fake xv, and.
, = t ranv →a(xv.
√.
xv.
, = t rana→v (xa, xv).
= sof tmax(.
t.qaka√la.
)va.= sof tmax(.
xawqawka.
t xa.
t.√.
la.
)xawva ..(1).
analogously,.
in the dual process, xv ∈rtv×lv is captured based on the input xv ∈.
, = t ranv →a(xv, xa)rtv×dv , xa∈, =rta×la, and reconstructed representation xv,, xv) ∈ rtv×lv .
essentially,t rana→v (xat rana→v and t ranv →a are implemented byseveral sequential encoder layers.
during the trans-lation period, we hypothesize that intermediate en-coder layer contains the cross-modality fusion in-formation and effectively balance the contributionof two modalities.
hence, the output of the middle[l/2]encoder layer t rana→vstand for the multimodal fusion knowledge, wherel refers to the number of layers, and when l isodd number, then l = l + 1..[l/2] and t ranv →a.
as.
for.
reward,.
the model.
the primalprocess has an immediate reward rp =,)(cid:4)f , and the dual step re-(cid:4)xa − t ranv →a(xv,)(cid:4)f ,lated reward is rd = (cid:4)xv − t rana→v (xaindicating the similarity between the real data andthe reconstructed output of the translator.
for sim-plicity, a linear transformation module is adoptedto combine the primal and dual step reward into atotal model reward, e.g., rall = αrp + (1 − α)rd,where α is employed to balance the contributionbetween dual and primal block.
additionally, theloss functions utilized in the coupled-translationmultimodal fusion block are deﬁned as follows:.
la→v (xa, xv) =(cid:3)t rana→v (xa, xv) − xv(cid:3)f +,, xv) − xv(cid:3)f(cid:3)t rana→v (xalv →a(xv, xa) =(cid:3)t ranv →a(xv, xa) − xa(cid:3)f +,, xa) − xa(cid:3)f(cid:3)t ranv →a(xvla↔v = αla→v (xa, xv) + (1 − α)lv →a(xv, xa),.
(2).
where la→v (xa, xv) and lv →a(xv, xa) re-fer to the training loss of the primal and dual trans-lator respectively, and la↔v stands for the loss ofbi-directional translator unit.
essentially, when thetraining process of all coupled-translation blocks.
5304oediv.tran(cid:2157) (cid:1372) (cid:2178).
tran(cid:2157) (cid:1370) (cid:2178).
tran(cid:2176) (cid:1372) (cid:2178).
tran(cid:2176) (cid:1370) (cid:2178).
oidua.
(cid:1839)(cid:2183)(cid:1372)(cid:2204).
(cid:1839)(cid:2204)(cid:1372)(cid:2183).
(cid:1839)(cid:2202)(cid:1372)(cid:2204).
(cid:1839)(cid:2202)(cid:1372)(cid:2183).
(cid:1839)(cid:2204)(cid:1372)(cid:2202).
(cid:1839)(cid:2183)(cid:1372)(cid:2202).
t it is very .
xet.interesting!.
tran(cid:2157) (cid:1372) (cid:2176).
tran(cid:2157) (cid:1370) (cid:2176).
convolutional fusion block.
figure 4: the hierarchical framework associated with three ctfns during the training period.
each ctfn isutilized to explore the speciﬁc bi-direction cross-modality interplay.
on the basis of this, three ctfn are stackedinto a united one for exploiting multiple bi-direction translations, leading to double multimodal fusing embeddings.
then, multiple multimodal fusing embeddings are delivered to the multimodal convolutional fusion block..are ﬁnished, our model only needs one input modal-ity at predicting time, without the help of targetmodalities..indeed, la↔v indicates the cycle-consistencyconstraint in our couple learning model.
the cycle-consistency is well-known, which refers to combi-nation of forward and backward cycle-consistency.
however, our goal is to solve missing modalityproblem in multi-modal learning, which cannot beachieved by applying cycle-consistency straightfor-ward.
this is because that introducing this strictcycle-consistency to ctfn fail to effectively asso-ciate primal task with dual task of the couple learn-ing model.
to solve this problem, we relaxed con-straint of original cycle-consistency by using a pa-rameter ‘α’ to balance the contribution of forwardand backward cycle-consistency, leading to a muchmore ﬂexible cycle-consistency.
thanks to thegreat ﬂexibility of new proposed cycle-consistency,we could adaptively and adequately associate pri-mal with dual task, resulting in much more bal-anced consistency among modalities..3.3 multimodal convolutional fusion block.
{t ranmodality source→modality m}m.based on ctfn, each modality is treated as thesource moment for (m − 1) times, which meansthat each modality holds (m − 1) directional trans-lations,m=1,where m refers to the total number of modalities.
for instance, given modality audio, we can retrievethe following two modality-guidance translations:l/2, video,] = t rana→v(audio, video)[t rana→vl/2, text,] = t rana→t(audio, text).
(3)note that audio plays a key role in different cross-modality translations, and provides the strong guid-.
[t rana→t.
ance for capturing various cross-modality interplay.
for blending the contribution of source modality(audio) effectively, a convolution fusion block is in-corporated to explore explicit and local correlationamong modality-guidance translations..initially,.
correlations.
the two cross-modality interme-t ranaudio→vedioanddiatel/2 are concatenated along thet ranaudio→texttemporal domain into a unit representation, wherethe size of time sequence is equal (ta = tv = tt),thus the concatenation is of size ta × (lv + lt):.
l/2.
l/2..l/2 ⊕ t rana→t.
zconcat = t rana→v(4)subsequently, the temporal convolution is em-ployed to further retrieve explicit interactionsamong cross-modality translations.
speciﬁcally,we adopt a 1d temporal convolutional layer to ex-ploit the local patten in a light manner:ˆzconcat = conv1d(zconcat, kconcat) ∈ rta×ld,(5)where kconcat is the size of the convolutionalkernel, and ld is the length of the cross-modalityintegration dimension.
the temporal kernel is usedto perform the convolutional operation along thefeature dimension, allowing to further exploit localinterplay among cross-modality translations.
thatis to say, the local interplay fully exploits the con-tribution from modality-guidance translations..3.4 hierarchical architecture.
on the basis of ctfn and convolutional mul-timodal fusion network, a hierarchical architec-ture was proposed for exploiting multiple bi-direction translations, leading to double multi-modal fusing embeddings.
for instance, givenm modalities, our model could achieve double.
5305input modality.
audio.
(cid:1839)(cid:2183)(cid:1372)(cid:2204).
(cid:2180)(cid:3028).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2157)(cid:1372)(cid:2178).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2157)(cid:1372)(cid:2176).
(cid:2180)(cid:3028).
(cid:1839)(cid:2183)(cid:1372)(cid:2202).
(cid:2180)(cid:3049)(cid:1314).
(cid:2180)(cid:3049)(cid:1314).
(cid:2180)(cid:3047)(cid:1314).
(cid:2180)(cid:3047)(cid:1314).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2178)(cid:1372)(cid:2157).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2178)(cid:1372)(cid:2176).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2176)(cid:1372)(cid:2178).
(cid:2176)(cid:2200)(cid:2183)(cid:2196)(cid:2176)(cid:1372)(cid:2157).
(cid:1839)(cid:2204)(cid:1372)(cid:2183).
(cid:1839)(cid:2204)(cid:1372)(cid:2202).
(cid:1839)(cid:2202)(cid:1372)(cid:2204).
(cid:1839)(cid:2202)(cid:1372)(cid:2183).
multimodal convolution fusion.
,.
,.
,.
,.
and xt.
.
then, xv.
4 experiments.
t →a, t ranl/2.
v →t , t ranl/2.
a→v , t ranl/2.
4.1 experimental setups.
is transmitted to t ranv →a and t ranv →t respectively.
and, xt.
is used for training the classiﬁer on the 3 ctfns’soutputs..figure 5: we only employ a single input modality (audio) to do the multimodal fusion task during the predicting pe-riod.
initially, audio presentation xa is sent to the pre-trained translators t rana→v and t rana→t for retrievingxvis sent to t rant →vand t rant →a respectively.
hence, the tree structure only need one input modality to do the multimodal fusiontask.
c2m embeddings.
as illustrated in figure 4, theproposed architecture consists of three ctfnst rana↔v , t rana↔t and t ranv ↔t .
consider-ing the contribution of the guidance (source) modal-ity, the modality-guidance translations are denotedas t rant ←a→v = [t ranl/2a→t ],t rant ←v →a = [t ranl/2v →a], andt rana←t →v = [t ranl/2t →v ], respec-tively.
similarly, when taking the contributionof target modalities into account, correspond-ing modality-guidance translations are illustratedas t rant →a←v = [t ranl/2t →a],t rant →v ←a = [t ranl/2a→v ], andt rana→t ←v = [t ranl/2v →t ], respec-tively.
subsequently, the convolutional fusion layeris used to further exploit explicit local interplayamong modality-guidance translations associatedwith the same source/target modality, which canfully leverage the contribution of source/targetmodality..datasets.
cmu-mosi consists of 2199 opin-ion video clips from online sharing websites (e.g.,youtube).
each utterance of the video clip is anno-tated with a speciﬁc sentimental label of positiveor negative in the range scale of [−3, +3].
thecorresponding training, validation, and testing sizerefer to division set (1284, 229, 686).
addition-ally, the same speaker will not appear in both train-ing and testing sets, allowing to exploit speaker-independent joint representations.
meld datasetcontains 13000 utterances from the famous tv-series f riends.
each utterance is annotated withemotion and sentiment labels, considering 7 classesof emotion tag (anger, disgust, fear, joy, neutral,sadness, and surprise) and 3 sentimental tendencylevels (positive, neutral, and negative).
hence, theoriginal dataset can be denoted as meld (senti-ment) and meld (emotion) with respect to thedata annotation, we only veriﬁed our model onthe meld (sentiment).
note that cmu-mosiand meld are the public and widely-used datasetswhich have been aligned and segmented already..v →a, t ranl/2.
t →v , t ranl/2.
a→t , t ranl/2.
essentially, as demonstrated in figure 4, ourmodel has “12+1” loss constraints in total, whichincludes 3 ctfns, each one has 4 training loss(primal & dual translator training loss), and 1 clas-siﬁcation loss.
however, we do not need to bal-ance these targets together, which is achieved byour training strategy that 3 ctfns are trained in-dividually.
for each ctfn, one hyper-parameter‘α’ is introduced to balance the loss of primal trans-lator and dual translator, and this hyper-parameteris shared among 3 ctfns.
hence, 3 ctfns onlyneed 1 hyper-parameter to balance the training loss,which is easy to be tuned.
the classiﬁcation loss.
features.
for cmu-mosi dataset, we adoptthe same preprocess manner mentioned in mfn(zadeh et al., 2018) to extract the low-level rep-resentation of multimodal data, and synchronizedat the utterance level that in consistent with textfor meld benchmark, we followmodality..5306models.
cmu-mosi.
meld (sentiment).
bi-modality.
tri-modality.
bi-modality.
(video, audio).
(text, video).
(text, audio).
(text, audio, video).
(text, audio).
gme-lstm (chung et al., 2014)bc-lstm (poria et al., 2017b)meld-based (poria et al., 2019)chfusion (majumder et al., 2018)mmmu-ba (ghosal et al., 2018)seqseq2sent (pham et al., 2018)mctn (pham et al., 2019)transmodality (wang et al., 2020).
ctfn (ours, l=1)ctfn (ours, l=3)ctfn (ours, l=6).
52.9056.5254.7954.4957.4558.0053.1059.97.
62.2063.1164.48.
74.3078.5976.6074.7780.8567.0076.8080.58.
80.4981.5580.79.
73.5078.8676.9978.5479.9266.0076.4081.25.
81.482.1681.71.
76.5079.2679.1976.5181.2570.0079.3082.71.
80.1882.7781.10.
66.4666.0966.6865.8565.5663.8466.2767.04.
67.8267.7867.24.table 1: comparison of performance results for sentiment analysis on cmu-mosi and meld (sentiment) bench-mark using various sota models..the related work of meld, in which the 300-dimensional glove (pennington et al., 2014) textvectors are fed into a 1d-cnn (chen et al., 2017)layer to extract textual representation, and audio-based descriptors are explored with the populartoolkit opensmile (eyben et al., 2010), while vi-sual features were not taken into account for thesentiment analysis..comparisons.
we introduced the translation-based and non-translation based models to thiswork as the baselines.
translation-based: mul-timodal cyclic translation network (mctn), se-quence to sequence for sentiment (seq2seq2sent),multimodal sentiment analysis with transformer(transmodality).
and non-translation based: bi-directional contextual lstm (bc-lstm), gatedembedding lstm (gme-lstm), multimodalemotionlines dataset baseline model (meld-base), hierarchical fusion with context model-ing (chfusion), multi-modal multi-utterance -bi-modal attention (mmmu-ba)..4.2 experiment results and analysis.
performance comparison with state-of-the-artmodels.
firstly, we analyzed the performance be-tween state-of-the-art baselines and our proposedmodel.
the bottom rows in table 1 indicate theeffectiveness and superiority of our model.
particu-larly, on cmu-mosi dataset, ctfn exceeded theprevious best transmodality on (video, audio) bya margin of 4.51. additionally, on meld (senti-ment) dataset, the empirical improvement of ctfnwas 0.78. it is interesting to note that the improve-ment of (video, audio) is more signiﬁcant than(text, video) and (text, audio).
this implies thatcoupled-translation structure is capable of decreas-ing the risk of interference between video and audioefﬁciently, and further leverage the explicit con-.
sistency between auxiliary features.
as for (text,audio, video), ctfn exceeds the previous besttransmodality with an improvement of 0.06, lead-ing to a comparable performance.
indeed, for thesame tri-modality fusion task, transmodality needs4 encoders and 4 decoders, while ctfn only re-quires 6 encoders.
it should be emphasized that thecyclic consistency mechanism could contribute toa much lighter model, as well as the more effectivebi-directional translation.
in addition, comparedto the bi-modality setting, the tri-modality caseachieved the improvement of 0.61, indicating thebeneﬁts brought by hierarchical architecture andconvolution fusion..settings.
cmu-mosi.
ctfn.
seqseq2sent.
f1.
acc.
f1.
acc.
1 missing modality.
2 missing modalities.
(audio, video, text)(audio, video, text)(audio, video, text).
(audio,video, text)(audio, video, text)(audio, video, text).
81.8282.2366.79.
80.7862.8263.94.
81.5582.1661.59.
80.7961.4360.98.
67.0065.0058.00.
76.0056.0048.00.
67.0066.0058.00.
77.0056.0057.00.
0 missing modality.
(text, audio, video).
82.85.
82.77.
66.00.
70.00.table 2: multimodal fusion results of seqseq2sentand ctfn with missing modalities.
the setting (au-dio, video, text) refers to the process that ctfn onlyemploys a single input modality (audio) to do the mul-timodal fusion task, shown in figure 5..effect of ctfn with missing modalities.
ex-isting translation-based manners focus only on thejoin representation between modalities, and ignorethe potential occurrence of the missing modalities.
therefore, we analyzed how does missing modalitymay affect the ﬁnal performance of ctfn and thesequential translation-based model seqseq2sent.
note that seqseq2sent only employs lstm toanalyze uni-modality rather than the translation-based method.
speciﬁcally, we take the hierarchi-cal architecture combined with three ctfns as.
5307the testing model.
from the table 2, we observethat compared to the setting (text, audio, video),the text-based settings {(audio, video, text), (au-dio, video, text), (audio,video, text)} seem to reachthe comparable result with only a relatively smallperformance drop.
on the contrast, when text wasmissing, the model has a relatively large perfor-mance drop, which implies that language modal-ity contains much more discriminative sentimentalmessage than audio and video, leading to the sig-niﬁcantly better performance.
essentially, the per-formance of (audio,video, text) demonstrates thathierarchical ctfn is able to maintain robustnessand consistency when considering only a single in-put modality.
in other words, the cyclic consistencymechanism allows ctfn to fully exploit the cross-modality interplay, thus hierarchical ctfn couldtransmit the single modality to various pre-trainedctfns for retrieving multimodal fusion message..mosi dataset.
primal taskdual task.
mosi dataset.
primal taskdual task.
a → t t → a a → v v → a t → v v → t.ycarucca.erocs1f. .
90.
67.5.
45.
22.5.
0.
90.
67.5.
45.
22.5.
0.a → t t → a a → v v → a t → v v → t.figure 6: effect of the translation direction..effect of the translation direction.
in this pa-per, we propose a coupled-translation block, whichaims to embrace fusion messages from the bi-directional translation process.
hence, we are in-terested to investigate the impact of translationdirection.
figure 6 depicts the performance ofvarious translations, considering (audio, text), (au-dio, video), and (text, video) translation.
for the(audio, text) instance, the translation text→audioachieves better performance than audio→text .
similarly, the translation text→video surpassesthe result of video→text.
however, the perfor-mance of audio→video and video→audio seemsto be quite similar.
the superiority of text→videoand text→audio may demonstrate that text modal-.
ity possesses much more sentimental information.
moreover, the prospects of text modality allow textto be the strong backbone of the translation..mosi dataset.
accuracy.
f1 score.
82.
81.5.
81.
80.5.ecnamrofrep.80.
1.ecnamrofrep.67.9.
67.65.
67.4.
67.15.
66.9.
2.
3.
4.
5.
6.encoder layer.
meld dataset.
accuracyf1 score.
1.
2.
3.
4.
5.
6.figure 7: effect of the translator layer..encoder layer.
effect of the translator layer.
as each trans-lator is comprised of several sequential encoderin this part, we assume that the outputlayers.
representation of a speciﬁc layer may affect theperformance of the proposed model.
for simplicity,we perform the related task on cmu-mosi withthe setting of (a, v, t), as well as the (t, a) on meld(sentiment).
initially, we retrieve the embeddingfrom the speciﬁc layer, where the layer ranges from1 to l (l is the total number of the layer).
in figure7, it is interesting to note that the model reachesthe peak value at layer 5 on cmu-mosi, whichmeans that the output of the ﬁfth layer embraces themost discriminative fusion message.
in compari-son, on meld (sentiment), the model achieves thebest performance at layer 1, which may imply thatthe simple translator associated with only one layeris able to capture the joint representation for thesimple case (text, audio).
in conclusion, the lowerencoder layer may involve low-level characteristicsof interplay, while the higher encoder layer mayembrace the explicit messages.
additionally, theoutput of the speciﬁc layer of the encoder lies onthe corresponding task and dataset.
we tried also(text, audio) on mosi, and ctfn maximizes theperformance at layer 3. compared to (text, audio,video), (text, audio) is the relatively simple case,thus the lower encoder layer may is sufﬁcient todemonstrate the interaction between text and audio.
effect of concatenation strategy of transla-tion.
in our work, those translations associated.
5308mosi dataset.
source-based.
target-based .
[.
a.
[.
t.[.
v.[.
t.[.
t.[.
a.
→.
→.
→.
→.
→.
→.
t,.
a.a.,.
a.,.
v.v.v.,.
a.a.,.
t,.
t.v.→.
→.
→.
→.
→.
→.
v.].
a.
].
t.].
v.].
v.].
t.].
mosi dataset.
source-based.
target-based .
ycarucca.
90.
67.5.
45.
22.5.
0.erocs 1f.90.
67.5.
45.
22.5.
0.
[.
a.
[.
t.[.
v.[.
t.[.
t.[.
a.
→.
→.
→.
→.
→.
→.
t,.
a.a.,.
a.,.
v.v.v.,.
a.a.,.
t,.
t.v.→.
→.
→.
→.
→.
→.
v.].
a.
].
t.].
v.].
v.].
t.].
effect of concatenation strategy viafigure 8:[a→t,a→v]source/target modality on mosi.
indicatesconcatenationsource[(a→t)⊕(a→v)], and [t→a,v→a] indicates theaudio-based target concatenation..audio-based.
the.
with the same guidance (source) modality are con-catenated along the feature domain.
as eachmodality serves as the source and target modal-ity in turn, we are interested to analyze the im-pact of the distinct concatenation strategies, e.g.,concatenate the translations via the same sourceor target modality.
as shown in figure 8, itis obvious to ﬁnd that audio-based target con-catenation [(t→a) ⊕ (v→a)] performs signiﬁ-cantly better than [(a→t)⊕(a→v)] with a largemargin.
analogously, video-based target con-catenation [(t→v)⊕(a→v)] works better than[(v→a)⊕(v→t)].
the above performance mayindicate that joint presentation is able to achievethe signiﬁcantly improved beneﬁts with the help ofguidance modality text.
in conclusion, when textmodality serves as the guidance modality, whichmay effectively leverage the contribution from au-dio and video, and further boost the task perfor-mance in a robust and consistent way..5 conclusion.
in this paper, we present a novel hierarchical multi-modal fusion architecture using coupled-translationfusion network (ctfn).
initially, ctfn is uti-lized for exploiting bi-directional interplay via cou-ple learning, ensuring the robustness in respect tomissing modalities.
speciﬁcally, the cyclic mech-anism directly discards the decoder and only em-braces the encoder of transformer, which could.
contribute to a much lighter model.
due to the cou-ple learning, ctfn is able to conduct bi-directioncross-modality intercorrelation parallelly.
basedon ctfn, a hierarchical architecture is further es-tablished to exploit multiple bi-direction transla-tions, leading to double multimodal fusing embed-dings compared with traditional translation meth-ods.
additionally, a multimodal convolutionalfusion block is employed to further explore thecomplementarity and consistency between cross-modality translations.
essentially, the parallel fu-sion strategy allows the model maintains robust-ness and ﬂexibility when considering only oneinput modality.
ctfn was veriﬁed on two pub-lic multimodal sentiment benchmarks, the exper-iments demonstrate the effectiveness and ﬂexi-bility of ctfn, and ctfn achieves state-of-the-art or comparable performance on cmu-mosiand meld (sentiment).
for future work, welike to evaluate ctfn on more multimodal fu-sion tasks.
the source code can be obtained fromhttps://github.com/deepsuperviser/ctfn..acknowledgments.
we sincerely thank the anonymous reviewers fortheir insightful comments and valuable sugges-tions.
this work was supported by national keyr&d program of china for intergovernmental in-ternational science and technology innovation co-operation project (2017yfe0116800), nationalnatural science foundation of china (u20b2074,u1909202), science and technology program ofzhejiang province (2018c04012), key laboratoryof brain machine collaborative intelligence of zhe-jiang province (2020e10010), jsps kakenhi(grant no.
20h04249), and supported by the min-istry of education and science of the russian fed-eration (grant 14.756.31.0001)..references.
ayush agarwal, ashima yadav, and dinesh kumarvishwakarma.
2019. multimodal sentiment analysisvia rnn variants.
in 2019 ieee international confer-ence on big data, cloud computing, data science& engineering (bcd), pages 19–23..tuka al hanai, mohammad m ghassemi, and james rglass.
2018. detecting depression with audio/textsequence modeling of interviews.
in interspeech,pages 1716–1720..tadas baltruˇsaitis, chaitanya ahuja,.
and louis-philippe morency.
2018. multimodal machine learn-.
5309ing: a survey and taxonomy.
ieee transac-tions on pattern analysis and machine intelligence,41(2):423–443..tao chen, ruifeng xu, yulan he, and xuan wang.
2017.improving sentiment analysis via sentencetype classiﬁcation using bilstm-crf and cnn.
expertsystems with applications, 72:221–230..junyoung chung, caglar gulcehre, kyunghyun cho,and yoshua bengio.
2014. empirical evaluation ofgated recurrent neural networks on sequence model-ing.
arxiv preprint arxiv:1412.3555..didan deng, yuqian zhou, jimin pi, and bertram e shi.
2018. multimodal utterance-level affect analysis us-ing visual, audio and text features.
arxiv preprintarxiv:1805.00625..sidney k d’mello and jacqueline kory.
2015. a re-view and meta-analysis of multimodal affect detec-tion systems.
acm computing surveys (csur),47(3):1–36..florian eyben, martin w¨ollmer, and bj¨orn schuller.
2010. opensmile:the munich versatile and fastopen-source audio feature extractor.
in proceedingsof the 18th acm international conference on multi-media, pages 1459–1462..israel d gebru, sileye ba, xiaofei li, and radu ho-raud.
2017. audio-visual speaker diarization basedon spatiotemporal bayesian fusion.
ieee transac-tions on pattern analysis and machine intelligence,40(5):1086–1099..deepanway ghosal, md shad akhtar, dushyantchauhan, soujanya poria, asif ekbal, and pushpakbhattacharyya.
2018. contextual inter-modal atten-tion for multi-modal sentiment analysis.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 3454–3466,brussels, belgium.
association for computationallinguistics..samira ebrahimi kahou, xavier bouthillier, pas-cal lamblin, caglar gulcehre, vincent michalski,kishore konda, s´ebastien jean, pierre froumenty,yann dauphin, nicolas boulanger-lewandowski,et al.
2016. emonets: multimodal deep learning ap-proaches for emotion recognition in video.
journalon multimodal user interfaces, 10(2):99–111..zhen-zhong lan, lei bao, shoou-i yu, wei liu, andalexander g hauptmann.
2014. multimedia classiﬁ-cation and event detection using double fusion.
mul-timedia tools and applications, 71(1):333–347..hung le, doyen sahoo, nancy chen, and steven hoi.
2019. multimodal transformer networks for end-to-end video-grounded dialogue systems.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 5612–5623,florence, italy.
association for computational lin-guistics..yan-bo lin, yu-jhe li, and yu-chiang frank wang.
2019. dual-modality seq2seq network for audio-visual event localization.
in ieee internationalconference on acoustics, speech and signal pro-cessing, icassp 2019, brighton, united kingdom,may 12-17, 2019, pages 2002–2006.
ieee..ziqian luo, hua xu, and feiyang chen.
2019. au-dio sentiment analysis by heterogeneous signal fea-tures learned from utterance-based parallel neuralnetwork.
in affcon@ aaai..navonil majumder, devamanyu hazarika, alexandergelbukh, erik cambria, and soujanya poria.
2018.multimodal sentiment analysis using hierarchical fu-sion with context modeling.
knowledge-based sys-tems, 161:124–133..zhenxing niu, mo zhou, le wang, xinbo gao, andgang hua.
2017. hierarchical multimodal lstmin ieee in-for dense visual-semantic embedding.
ternational conference on computer vision, iccv2017, venice, italy, october 22-29, 2017, pages1899–1907.
ieee computer society..yingwei pan, tao mei, ting yao, houqiang li, andyong rui.
2016. jointly modeling embedding andtranslation to bridge video and language.
in 2016ieee conference on computer vision and patternrecognition, cvpr 2016, las vegas, nv, usa, june27-30, 2016, pages 4594–4602.
ieee computer so-ciety..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..hai pham, paul pu liang, thomas manzini, louis-philippe morency, and barnab´as p´oczos.
2019.found in translation: learning robust joint repre-sentations by cyclic translations between modalities.
in the thirty-third aaai conference on artiﬁcialintelligence, aaai 2019, honolulu, hawaii, usa,january 27 - february 1, 2019, pages 6892–6899.
aaai press..hai pham, thomas manzini, paul pu liang, andbarnab´as pocz´os.
2018. seq2seq2sentiment: mul-timodal sequence to sequence models for senti-ment analysis.
in proceedings of grand challengeand workshop on human multimodal language(challenge-hml), pages 53–63, melbourne, aus-tralia.
association for computational linguistics..soujanya poria, erik cambria, rajiv bajpai, and amirhussain.
2017a.
a review of affective computing:from unimodal analysis to multimodal fusion.
in-formation fusion, 37:98–125..soujanya poria, erik cambria, devamanyu hazarika,navonil majumder, amir zadeh, and louis-philippemorency.
2017b.
context-dependent sentiment anal-ysis in user-generated videos.
in proceedings of the.
5310xiaodong yang, pavlo molchanov, and jan kautz.
2016. multilayer and multimodal fusion of deepneural networks for video classiﬁcation.
in pro-ceedings of the 2016 acm conference on multime-dia conference, mm 2016, amsterdam, the nether-lands, october 15-19, 2016, pages 978–987..amir zadeh, paul pu liang, navonil mazumder,soujanya poria, erik cambria, and louis-philippemorency.
2018. memory fusion network for multi-in proceedings of theview sequential learning.
thirty-second aaai conference on artiﬁcial intel-ligence, (aaai-18), new orleans, louisiana, usa,february 2-7, 2018, pages 5634–5641.
aaai press..amir zadeh, rowan zellers, eli pincus, and louis-philippe morency.
2016. mosi: multimodal cor-intensity and subjectivity anal-pus of sentimentarxiv preprintysis in online opinion videos.
arxiv:1606.06259..55th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages873–883, vancouver, canada.
association for com-putational linguistics..soujanya poria, devamanyu hazarika, navonil ma-jumder, gautam naik, erik cambria, and rada mi-halcea.
2019. meld: a multimodal multi-partydataset for emotion recognition in conversations.
inproceedings of the 57th annual meeting of the as-sociation for computational linguistics, pages 527–536, florence, italy.
association for computationallinguistics..wasifur rahman, md kamrul hasan, sangwu lee,amirali bagher zadeh, chengfeng mao, louis-philippe morency, and ehsan hoque.
2020.inte-grating multimodal information in large pretrainedtransformers.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 2359–2369, online.
association forcomputational linguistics..ekaterina shutova, douwe kiela, and jean maillard.
2016. black holes and white rabbits: metaphor iden-tiﬁcation with visual features.
in proceedings of the2016 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 160–170, sandiego, california.
association for computationallinguistics..yao-hung hubert tsai, shaojie bai, paul pu liang,j. zico kolter, louis-philippe morency, and rus-lan salakhutdinov.
2019. multimodal transformerinfor unaligned multimodal language sequences.
proceedings ofthethe 57th annual meeting ofassociation for computational linguistics, pages6558–6569, florence, italy.
association for compu-tational linguistics..yao-hung hubert tsai, martin q ma, muqiaoyang, ruslan salakhutdinov, and louis-philippeinterpretable multimodal routingmorency.
2020.for human multimodal language.
arxiv preprintarxiv:2004.14198..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..zilong wang, zhaohong wan, and xiaojun wan.
2020.transmodality: an end2end fusion method withtransformer for multimodal sentiment analysis.
inwww ’20: the web conference 2020, taipei, tai-wan, april 20-24, 2020, pages 2514–2520.
acm /iw3c2..ali yadollahi, ameneh gholipour shahraki, and os-mar r zaiane.
2017. current state of text sentimentanalysis from opinion to emotion mining.
acmcomputing surveys (csur), 50(2):1–33..5311