how good is your tokenizer?
on the monolingual performance of multilingual language models.
phillip rust∗1†, jonas pfeiffer∗1,ivan vuli´c2, sebastian ruder3, iryna gurevych11ubiquitous knowledge processing lab, technical university of darmstadt2language technology lab, university of cambridge3deepmindwww.ukp.tu-darmstadt.de.
abstract.
in this work, we provide a systematic andcomprehensive empirical comparison of pre-trained multilingual language models versustheir monolingual counterparts with regard totheir monolingual task performance.
we studya set of nine typologically diverse languageswith readily available pretrained monolingualmodels on a set of ﬁve diverse monolingualdownstream tasks.
we ﬁrst aim to establish,via fair and controlled comparisons, if a gapbetween the multilingual and the correspond-ing monolingual representation model of thatlanguage exists, and subsequently investigatethe reason for any performance difference.
todisentangle conﬂating factors, we train newmonolingual models on the same data, withmonolingually and multilingually trained tok-enizers.
we ﬁnd that while the pretraining datasize is an important factor, a designated mono-lingual tokenizer plays an equally importantrole in the downstream performance.
our re-sults show that languages that are adequatelyrepresented in the multilingual model’s vocab-ulary exhibit negligible performance decreasesover their monolingual counterparts.
we fur-ther ﬁnd that replacing the original multilin-gual tokenizer with the specialized monolin-gual tokenizer improves the downstream per-formance of the multilingual model for almostevery task and language..1.introduction.
following large transformer-based language mod-els (lms, vaswani et al., 2017) pretrained on largeenglish corpora (e.g., bert, roberta, t5; devlinet al., 2019; liu et al., 2019; raffel et al., 2020),similar monolingual language models have been in-troduced for other languages (virtanen et al., 2019;.
∗both authors contributed equally to this work.
† pr is now afﬁliated with the university of copenhagen..our code is available at https://github.com/adapter-hub/hgiyt..antoun et al., 2020; martin et al., 2020, inter alia),offering previously unmatched performance in allnlp tasks.
concurrently, massively multilingualmodels with the same architectures and trainingprocedures, covering more than 100 languages,have been proposed (e.g., mbert, xlm-r, mt5;devlin et al., 2019; conneau et al., 2020; xue et al.,2021)..the “industry” of pretraining and releasing newmonolingual bert models continues its operationsdespite the fact that the corresponding languagesare already covered by multilingual models.
thecommon argument justifying the need for mono-lingual variants is the assumption that multilingualmodels—due to suffering from the so-called curseof multilinguality (conneau et al., 2020, i.e., thelack of capacity to represent all languages in an eq-uitable way)—underperform monolingual modelswhen applied to monolingual tasks (virtanen et al.,2019; antoun et al., 2020; r¨onnqvist et al., 2019,inter alia).
however, little to no compelling em-pirical evidence with rigorous experiments and faircomparisons have been presented so far to supportor invalidate this strong claim.
in this regard, muchof the work proposing and releasing new mono-lingual models is grounded in anecdotal evidence,pointing to the positive results reported for othermonolingual bert models (de vries et al., 2019;virtanen et al., 2019; antoun et al., 2020)..monolingual bert models are typically eval-uated on downstream nlp tasks to demonstratetheir effectiveness in comparison to previous mono-lingual models or mbert (virtanen et al., 2019;antoun et al., 2020; martin et al., 2020, inter alia).
while these results do show that certain monolin-gual models can outperform mbert in certaintasks, we hypothesize that this may substantiallyvary across different languages and language prop-erties, tasks, pretrained models and their pretrain-ing data, domain, and size.
we further argue that.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3118–3135august1–6,2021.©2021associationforcomputationallinguistics3118conclusive evidence, either supporting or refutingthe key hypothesis that monolingual models cur-rently outperform multilingual models, necessitatesan independent and controlled empirical compari-son on a diverse set of languages and tasks..while recent work has argued and validated thatmbert is under-trained (r¨onnqvist et al., 2019;wu and dredze, 2020), providing evidence of im-proved performance when training monolingualmodels on more data, it is unclear if this is the onlyfactor relevant for the performance of monolin-gual models.
another so far under-studied factor isthe limited vocabulary size of multilingual modelscompared to the sum of tokens of all correspondingmonolingual models.
our analyses investigatingdedicated (i.e., language-speciﬁc) tokenizers revealthe importance of high-quality tokenizers for theperformance of both model variants.
we also shedlight on the interplay of tokenization with otherfactors such as pretraining data size..contributions.
1) we systematically comparemonolingual with multilingual pretrained languagemodels for 9 typologically diverse languages on 5structurally different tasks.
2) we train new mono-lingual models on equally sized datasets with differ-ent tokenizers (i.e., shared multilingual versus ded-icated language-speciﬁc tokenizers) to disentanglethe impact of pretraining data size from the vocabu-lary of the tokenizer.
3) we isolate factors that con-tribute to a performance difference (e.g., tokenizers’“fertility”, the number of unseen (sub)words, datasize) and provide an in-depth analysis of the im-pact of these factors on task performance.
4) ourresults suggest that monolingually adapted tokeniz-ers can robustly improve monolingual performanceof multilingual models..2 background and related work.
multilingual lms.
the widespread usage of pre-trained multilingual transformer-based lms hasbeen instigated by the release of multilingual bert(devlin et al., 2019), which followed on the successof the monolingual english bert model.
mbertadopted the same pretraining regime as mono-lingual bert by concatenating the 104 largestwikipedias.
exponential smoothing was used whencreating the subword vocabulary based on word-pieces (wu et al., 2016) and a pretraining corpus.
by oversampling underrepresented languages andundersampling overrepresented ones, it aims tocounteract the imbalance of pretraining data sizes..the ﬁnal shared mbert vocabulary comprises atotal of 119,547 subword tokens..other multilingual models followed mbert,such as xlm-r (conneau et al., 2020).
con-currently, many studies analyzed mbert’s andxlm-r’s capabilities and limitations, ﬁnding thatthe multilingual models work surprisingly well forcross-lingual tasks, despite the fact that they do notrely on direct cross-lingual supervision (e.g., par-allel or comparable data, translation dictionaries;pires et al., 2019; wu and dredze, 2019; artetxeet al., 2020; hu et al., 2020; k et al., 2020)..however, recent work has also pointed to somefundamental limitations of multilingual lms.
con-neau et al.
(2020) observe that, for a ﬁxed modelcapacity, adding new languages increases cross-lingual performance up to a certain point, afterwhich adding more languages results in perfor-mance drops.
this phenomenon, termed the curseof multilinguality, can be attenuated by increas-ing the model capacity (artetxe et al., 2020; pfeif-fer et al., 2020b; chau et al., 2020) or throughadditional training for particular language pairs(pfeiffer et al., 2020b; ponti et al., 2020).
anotherobservation concerns substantially reduced cross-lingual and monolingual abilities of the modelsfor resource-poor languages with smaller pretrain-ing data (wu and dredze, 2020; hu et al., 2020;lauscher et al., 2020).
those languages remain un-derrepresented in the subword vocabulary and themodel’s shared representation space despite over-sampling.
despite recent efforts to mitigate thisissue (e.g., chung et al.
(2020) propose to clusterand merge the vocabularies of similar languages,before deﬁning a joint vocabulary across all lan-guages), the multilingual lms still struggle withbalancing their parameters across many languages..language-speciﬁc models.
monolingual versus multilingual lms.
newmonolingualalsoemerged for many languages, following bert’sarchitecture and pretraining procedure.
there aremonolingual bert variants for arabic (antounet al., 2020), french (martin et al., 2020), finnish(virtanen et al., 2019), dutch (de vries et al.,2019), to name only a few.
pyysalo et al.
(2020)released 44 monolingual wikibert modelstrained on wikipedia.
however, only a fewstudies have thus far, either explicitly or implicitly,attempted to understand how monolingual andmultilingual lms compare across languages..3119nozza et al.
(2020) extracted task results fromthe respective papers on monolingual berts tofacilitate an overview of monolingual models andtheir comparison to mbert.1 however, they havenot veriﬁed the scores, nor have they performed acontrolled impartial comparison..vuli´c et al.
(2020) probed mbert and monolin-gual bert models across six typologically diverselanguages for lexical semantics.
they show thatpretrained monolingual bert models encode sig-niﬁcantly more lexical information than mbert..zhang et al.
(2020) investigated the role of pre-training data size with roberta, ﬁnding that themodel learns most syntactic and semantic featureson corpora spanning 10m–100m word tokens, butstill requires massive datasets to learn higher-levelsemantic and commonsense knowledge..mulcaire et al.
(2019) compared monolingualand bilingual elmo (peters et al., 2018) lmsacross three downstream tasks, ﬁnding that contex-tualized representations from the bilingual modelscan improve monolingual task performance relativeto their monolingual counterparts.2 however, it isunclear how their ﬁndings extend to massively mul-tilingual lms potentially suffering from the curseof multilinguality..r¨onnqvist et al.
(2019) compared mbert tomonolingual bert models for six languages(german, english, swedish, danish, norwegian,finnish) on three different tasks.
they ﬁnd thatmbert lags behind its monolingual counterpartsin terms of performance on cloze and generationtasks.
they also identiﬁed clear differences amongthe six languages in terms of this performance gap.
they speculate that mbert is under-trained withrespect to individual languages.
however, their setof tasks is limited, and their language sample istypologically narrow; it remains unclear whetherthese ﬁndings extend to different language familiesand to structurally different tasks..despite recent efforts, a careful, systematic studywithin a controlled experimental setup, a diverselanguage sample and set of tasks is still lacking.
we aim to address this gap in this work..3 controlled experimental setup.
we compare multilingual bert with its monolin-gual counterparts in a spectrum of typologically.
1https://bertlang.unibocconi.it/2mulcaire et al.
(2019) clearly differentiate between mul-tilingual and polyglot models.
their deﬁnition of polyglotmodels is in line with what we term multilingual models..diverse languages and across a variety of down-stream tasks.
by isolating and analyzing crucialfactors contributing to downstream performance,such as tokenizers and pretraining data, we canconduct unbiased and fair comparisons..3.1 language and task selection.
our selection of languages has been guided by sev-eral (sometimes competing) criteria: c1) typologi-cal diversity; c2) availability of pretrained mono-lingual bert models; c3) representation of thelanguages in standard evaluation benchmarks for asufﬁcient number of tasks..regarding c1, most high-resource languages be-long to the same language families, thus sharinga majority of their linguistic features.
neglectingtypological diversity inevitably leads to poor gener-alizability and language-speciﬁc biases (gerz et al.,2018; ponti et al., 2019; joshi et al., 2020).
fol-lowing recent work in multilingual nlp that paysparticular attention to typological diversity (clarket al., 2020; hu et al., 2020; ponti et al., 2020, in-ter alia), we experiment with a language samplecovering a broad spectrum of language properties.
regarding c2, for computational tractability, weonly select languages with readily available bertmodels.
unlike prior work, which typically lackseither language (r¨onnqvist et al., 2019; zhanget al., 2020) or task diversity (wu and dredze,2020; vuli´c et al., 2020), we ensure that our ex-perimental framework takes both into account, thusalso satisfying c3.
we achieve task diversity andgeneralizability by selecting a combination of tasksdriven by lower-level syntactic and higher-levelsemantic features (lauscher et al., 2020)..finally, we select a set of 9 languages from 8language families, as listed in table 1.3 we evalu-ate mbert and monolingual bert models on ﬁvedownstream nlp tasks: named entity recognition(ner), sentiment analysis (sa), question answer-ing (qa), universal dependency parsing (udp),and part-of-speech tagging (pos).4.
3note that, since we evaluate monolingual performanceand not cross-lingual transfer performance, we require train-ing data in the target language.
therefore, we are unable toleverage many of the available multilingual evaluation datasuch as xquad (artetxe et al., 2020), mlqa (lewis et al.,2020), or xnli (conneau et al., 2018).
these evaluation setsdo not provide any training portions for languages other thanenglish.
additional information regarding our selection ofpretrained models is available in appendix a.1..4information on which datasets are associated with whichlanguage and the dataset sizes (examples per split) are pro-vided in appendix a.4..3120language.
iso language family pretrained bert model.
arabicenglishfinnishindonesianjapanesekoreanrussianturkishchinese.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
afroasiaticindo-europeanuralicaustronesianjaponickoreanicindo-europeanturkicsino-tibetan.
arabert (antoun et al., 2020)bert (devlin et al., 2019)finbert (virtanen et al., 2019)indobert (wilie et al., 2020)japanese-char bert5kr-bert (lee et al., 2020)rubert (kuratov and arkhipov, 2019)berturk (schweter, 2020)chinese bert (devlin et al., 2019).
table 1: overview of selected languages and their re-spective pretrained monolingual bert models..named entity recognition (ner).
we rely on:conll-2003 (tjong kim sang and de meulder,2003), finer (ruokolainen et al., 2020), chi-nese literature (xu et al., 2017), kmou ner,6wikiann (pan et al., 2017; rahimi et al., 2019)..sentiment analysis (sa).
we employ: hard(elnagar et al., 2018), imdb movie reviews(maas et al., 2011), indonesian prosa (purwari-anti and crisdayanti, 2019), yahoo movie re-views,7 nsmc,8 rureviews (smetanin and ko-marov, 2019), turkish movie and product reviews(demirtas and pechenizkiy, 2013), chnsenticorp.9.
question answering (qa).
we use: squadv1.1(rajpurkar et al., 2016), korquad 1.0 (lim et al.,2019), sberquad (eﬁmov et al., 2020), tquad,10drcd (shao et al., 2019), tydiqa-goldp (clarket al., 2020)..dependency parsing (udp).
we rely on univer-sal dependencies (nivre et al., 2016, 2020) v2.6(zeman et al., 2020) for all languages..part-of-speech tagging (pos).
we again utilizeuniversal dependencies v2.6..3.2 task-based fine-tuning.
fine-tuning setup.
for all tasks besides udp,we use the standard ﬁne-tuning setup of devlinet al.
(2019).
for udp, we use a transformer-basedvariant (glavaˇs and vuli´c, 2021) of the standarddeep biafﬁne attention dependency parser (dozatand manning, 2017).
we distinguish between fullyﬁne-tuning a monolingual bert model and fullyﬁne-tuning mbert on the task.
for both settings,we average scores over three random initializationson the development set.
on the test set, we report.
5https://github.com/cl-tohoku/bert-japanese6https://github.com/kmounlp/ner7 https://github.com/dennybritz/sentiment-analysis8 https://www.lucypark.kr/docs/2015-pyconkr/#399https://github.com/pengming617/bert classiﬁcation10https://tquad.github.io/turkish-nlp-qa-dataset/.
lg model.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
avg.
monolingualmbert.
nertestf1.
91.190.0.
91.591.2.
92.088.2.
91.093.5.
72.473.4.
88.886.6.
91.090.0.
92.893.8.
76.576.1.
87.487.0.satestacc.
95.995.4.
91.689.8.
—–—–.
96.091.4.
88.087.8.
89.786.7.
95.295.0.
88.886.4.
95.393.8.
92.491.0.qadevem / f1.
68.3 / 82.466.1 / 80.6.
80.5 / 88.080.9 / 88.4.
69.9 / 81.666.6 / 77.6.
66.8 / 78.171.2 / 82.1.
—– / —–—– / —–.
74.2 / 91.169.7 / 89.5.
64.3 / 83.763.3 / 82.6.
60.6 / 78.157.9 / 76.4.
82.3 / 89.382.0 / 89.3.
70.8 / 84.069.7 / 83.3.udptestuas / las.
90.1 / 85.688.8 / 83.8.
92.1 / 89.791.6 / 89.1.
95.9 / 94.491.9 / 88.7.
85.3 / 78.185.9 / 79.3.
94.7 / 93.094.0 / 92.3.
90.3 / 87.289.2 / 85.7.
93.1 / 89.991.9 / 88.5.
79.8 / 73.274.5 / 67.4.
88.6 / 85.688.1 / 85.0.
90.0 / 86.388.4 / 84.4.postestacc.
96.896.8.
97.096.9.
98.496.2.
92.193.5.
98.197.8.
97.096.0.
98.498.2.
96.995.7.
97.296.7.
96.996.4.table 2: performance on named entity recognition(ner), sentiment analysis (sa), question answering(qa), universal dependency parsing (udp), and part-of-speech tagging (pos).
we use development (dev)sets only for qa.
finnish (fi) sa and japanese (ja)qa lack respective datasets..the results of the initialization that achieved thehighest score on the development set..evaluation measures.
we report f1 scores forner, accuracy scores for sa and pos, unlabeledand labeled attachment scores (uas & las) forudp, and exact match and f1 scores for qa..hyper-parameters and technical details.
weuse adamw (kingma and ba, 2015) in all experi-5.11 we train forments, with a learning rate of 3e10 epochs with early stopping (prechelt, 1998).12.
−.
11preliminary experiments indicated this to be a well per-forming learning rate.
due to the large volume of our exper-iments, we were unable to tune all the hyper-parameters foreach setting.
we found that a higher learning rate of 5e − 4works best for adapter-based ﬁne-tuning (see later) since thetask adapter parameters are learned from scratch (i.e., they arerandomly initialized)..12we evaluate a model every 500 gradient steps on thedevelopment set, saving the best-performing model based onthe respective evaluation measures.
we terminate training ifno performance gains are observed within ﬁve consecutiveevaluation runs (= 2,500 steps).
for qa and udp, we use thef1 scores and las, respectively.
for fi and id qa, we trainfor 20 epochs due to slower convergence.
we train with batchsize 32 and max sequence length 256 for all tasks except qa.
in qa, the batch size is 24, max sequence length 384, querylength 64, and document stride is set to 128..3121initial results.
3.3we report our ﬁrst set of results in table 2.13 weﬁnd that the performance gap between monolingualmodels and mbert does exist to a large extent,conﬁrming anecdotal evidence from prior work.
however, we also notice that the score differencesare largely dependent on the language and task athand.
the largest performance gains of monolin-gual models over mbert are found for fi, tr, ko,and ar.
in contrast, mbert outperforms the in-dobert (id) model in all tasks except sa, andperforms competitively with the ja and zh mono-lingual models on most datasets.
in general, thegap is particularly narrow for pos tagging, whereall models tend to score high (in most cases north of95% accuracy).
id aside, we also see a clear trendfor udp, with monolingual models outperformingfully ﬁne-tuned mbert models, most notably forfi and tr.
in what follows, we seek to understandthe causes of this behavior in relation to differentfactors such as tokenizers, corpora sizes, as well aslanguages and tasks in consideration..4 tokenizer versus corpus size.
4.1 pretraining corpus size.
the size of the pretraining corpora plays an impor-tant role in the performance of transformers (liuet al., 2019; conneau et al., 2020; zhang et al.,2020, inter alia).
therefore, we compare howmuch data each monolingual model was trained onwith the amount of data in the respective languagethat mbert has seen during training.
given thatmbert was trained on entire wikipedia dumps,we estimate the latter by the total number of wordsacross all articles listed for each wiki.14 for themonolingual lms, we extract information on pre-training data from the model documentation.
if noexact numbers are explicitly stated, and the pretrain-ing corpora are unavailable, we make estimationsbased on the information provided by the authors.15the statistics are provided in figure 1a.
for en, ja,ru, and zh, both the respective monolingual bertand mbert were trained on similar amounts ofmonolingual data.
on the other hand, monolingualberts of ar, id, fi, ko, and tr were trained onabout twice (ko) up to more than 40 times (tr) asmuch data in their language than mbert..13see appendix table 8 for the results on development sets.
14based on the numbers from.
https://meta.m.wikimedia.org/wiki/list of wikipedias.
15we provide further details in appendix a.2..4.2 tokenizer.
compared to monolingual models, mbert is sub-stantially more limited in terms of the parameterbudget that it can allocate for each of its 104 lan-guages in its vocabulary.
in addition, monolingualtokenizers are typically trained by native-speakingexperts who are aware of relevant linguistic phe-nomena exhibited by their target language.
wethus inspect how this affects the tokenizations ofmonolingual data produced by our sample of mono-lingual models and mbert.
we tokenize examplesfrom universal dependencies v2.6 treebanks andcompute two metrics ( ´acs, 2019).16 first, the sub-word fertility measures the average number of sub-words produced per tokenized word.
a minimumfertility of 1 means that the tokenizer’s vocabu-lary contains every single word in the text.
weplot the fertility scores in figure 1b.
we ﬁnd thatmbert has similar fertility values as its mono-lingual counterparts for en, id, ja, and zh.
incontrast, mbert has a much higher fertility forar, fi, ko, ru, and tr, indicating that such lan-guages are over-segmented.
mbert’s fertility isthe lowest for en; this is due to mbert havingseen the most data in this language during training,as well as english being morphologically poor incontrast to languages such as ar, fi, ru, or tr.17.
the second metric we employ is the proportionof words where the tokenized word is continuedacross at least two sub-tokens (denoted by contin-uation symbols ##).
whereas the fertility is con-cerned with how aggressively a tokenizer splits,this metric measures how often it splits words.
in-tuitively, low scores are preferable for both metricsas they indicate that the tokenizer is well suited tothe language.
the plots in figure 1c show similartrends as with the fertility statistic.
in addition toar, fi, ko, ru, and tr, which already displayeddifferences in fertility, mbert also produces a pro-portion of continued words more than twice as highas the monolingual model for id.18.
16we provide further details in appendix a.3.
17the ja model is the only monolingual bert with a fertil-ity score higher than mbert; its tokenizer is character-basedand thus by design produces the maximum number of sub-words..18we discuss additional tokenization statistics, further high-lighting the differences (or lack thereof) between the indi-vidual monolingual tokenizers and the mbert tokenizer, inappendix b.1..3122(a) pretraining corpus size.
(b) subword fertility.
(c) proportion of continued words.
figure 1: comparison of monolingual models with mbert w.r.t.
pretraining corpus size (measured in billions ofwords), subword fertility (i.e., the average number of subword tokens produced per tokenized word ( ´acs, 2019)),and proportion of continued words (i.e., words split into multiple subword tokens ( ´acs, 2019))..4.3 new pretrained models.
the differences in pretraining corpora and tok-enizer statistics seem to align with the variationsin downstream performance across languages.
inparticular, it appears that the performance gains ofmonolingual models over mbert are larger forlanguages where the differences between the re-spective tokenizers and pretraining corpora sizesare also larger (ar, fi, ko, ru, tr vs. en, ja,zh).19 this implies that both the data size andthe tokenizer are among the main driving forces ofdownstream task performance.
to disentangle theeffects of these two factors, we pretrain new mod-els for ar, fi, id, ko, and tr (the languages thatexhibited the largest discrepancies in tokenizationand pretraining data size) on wikipedia data..we train four model variants for each language.
first, we train two new monolingual bert modelson the same data, one with the original monolingualtokenizer (monomodel-monotok) and one withthe mbert tokenizer (monomodel-mberttok).20second, similar to artetxe et al.
(2020), we re-train the embedding layer of mbert, once with therespective monolingual tokenizer (mbertmodel-monotok) and once with the mbert tokenizer(mbertmodel-mberttok).
we freeze the trans-former and only retrain the embedding weights,thus largely preserving mbert’s multilingual-ity.
the reason we retrain mbert’s embed-ding layer with its own tokenizer is to furthereliminate confounding factors when comparingto the version of mbert with monolinguallyretrained embeddings.
by comparing models.
trained on the same amount of data, but withdifferent tokenizers (monomodel-monotok vs.monomodel-mberttok, mbertmodel-mberttokvs. mbertmodel-monotok), we disentangle theeffect of the dataset size from the tokenizer, bothwith monolingual and multilingual lm variants..pretraining setup.
we pretrain new bert mod-els for each language on its respective wikipediadump.21 we apply two preprocessing steps toobtain clean data for pretraining.
first, we usewikiextractor (attardi, 2015) to extract text pas-sages from the raw dumps.
next, we followpyysalo et al.
(2020) and utilize udpipe (strakaet al., 2016) parsers pretrained on ud data to seg-ment the extracted text passages into texts withdocument, sentence, and word boundaries..following liu et al.
(2019); wu and dredze(2020), we only use the masked language mod-eling (mlm) objective and omit the next sen-tence prediction task.
besides that, we largelyfollow the default pretraining procedure by de-vlin et al.
(2019).
we pretrain the new monolin-gual lms (monomodel-*) from scratch for 1msteps.22 we enable whole word masking (devlinet al., 2019) for the fi monolingual models, follow-ing the pretraining procedure for finbert (virta-nen et al., 2019).
for the retrained mbert mod-els (mbertmodel-*), we train for 250,000 stepsfollowing artetxe et al.
(2020).23 we freeze allparameters outside the embedding layer.24.
results.
we perform the same evaluations ondownstream tasks for our new models as described.
19the only exception is id, where the monolingual modelhas seen signiﬁcantly more data and also scores lower on thetokenizer metrics, yet underperforms mbert in most tasks.
we suspect this exception is because indobert is uncased,whereas the remaining models are cased..20the only exception is id; instead of relying on the uncasedindobert tokenizer by wilie et al.
(2020), we introduce anew cased tokenizer with identical vocabulary size (30,521)..21we use wiki dumps from june 20, 2020 (e.g., ﬁwiki-.
20200720-pages-articles.xml.bz2 for fi)..22the batch size is 64; the sequence length is 128 for theﬁrst 900,000 steps, and 512 for the remaining 100,000 steps.
23we train with batch size 64 and sequence length 512,otherwise using the same hyper-parameters as for the mono-lingual models..24for more details see appendix a.5..3123arenfiidjakorutrzh01234pretrainingcorpussize[1bwords]arenfiidjakorutrzh0.00.51.01.52.0fertilityarenfiidjakorutrzh0.00.10.20.30.40.50.60.7proportionofcontinuedwordsmodelmonombertnertestf1.
qadevem / f1.
udptestuas / las.
postestacc.
lg model.
monolingual.
ar.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
fi.
id.
ko.
tr.
mbert.
monolingual.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
mbert.
monolingual.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
mbert.
monolingual.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
mbert.
monolingual.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
mbert.
monolingual.
avg.
monomodel-monotok.
monomodel-mberttok.
mbertmodel-monotok.
mbertmodel-mberttok.
mbert.
91.1.
91.790.0.
91.289.7.
90.0.
92.0.
89.190.0.
88.188.1.
88.2.
91.0.
92.593.2.
93.993.9.
93.5.
88.8.
87.185.8.
86.686.2.
86.6.
92.8.
93.493.3.
93.793.8.
93.8.
91.1.
90.890.5.
90.790.3.
90.4.satestacc.
95.9.
95.695.5.
95.495.6.
95.4.
—–.
—–—–.
—–—–.
—–.
96.0.
96.094.8.
94.694.6.
91.4.
89.7.
88.887.2.
88.186.6.
86.7.
88.8.
87.084.8.
85.386.1.
86.4.
92.6.
91.990.6.
90.990.7.
90.0.
68.3 / 82.4.
90.1 / 85.6.
67.7 / 81.664.1 / 79.4.
66.9 / 81.866.3 / 80.7.
89.2 / 84.488.8 / 84.0.
89.3 / 84.589.1 / 84.2.
66.1 / 80.6.
88.8 / 83.8.
69.9 / 81.6.
95.9 / 94.4.
66.9 / 79.565.1 / 77.0.
66.4 / 78.365.9 / 77.3.
93.7 / 91.593.6 / 91.5.
92.4 / 89.692.2 / 89.4.
66.6 / 77.6.
91.9 / 88.7.
66.8 / 78.1.
85.3 / 78.1.
73.1 / 83.667.0 / 79.2.
74.1 / 83.871.9 / 82.7.
85.0 / 78.584.9 / 78.6.
86.4 / 80.286.2 / 79.6.
71.2 / 82.1.
85.9 / 79.3.
74.2 / 91.1.
90.3 / 87.2.
72.8 / 90.368.9 / 88.7.
72.9 / 90.269.3 / 89.3.
89.8 / 86.688.9 / 85.6.
90.1 / 87.089.2 / 85.9.
69.7 / 89.5.
89.2 / 85.7.
60.6 / 78.1.
79.8 / 73.2.
56.2 / 73.755.3 / 72.5.
59.4 / 76.758.7 / 76.6.
76.1 / 68.975.3 / 68.3.
77.1 / 70.276.2 / 69.2.
57.9 / 76.4.
74.5 / 67.4.
68.0 / 82.3.
88.3 / 83.7.
67.3 / 81.764.1 / 79.4.
68.0 / 82.266.4 / 81.3.
86.8 / 82.086.3 / 81.6.
87.1 / 82.386.6 / 81.7.
66.3 / 81.2.
86.1 / 81.0.
96.8.
96.697.0.
96.496.8.
96.8.
98.4.
97.397.0.
96.696.7.
96.2.
92.1.
93.993.6.
93.893.7.
93.5.
97.0.
96.796.4.
96.596.2.
96.0.
96.9.
96.396.5.
96.396.3.
95.7.
96.2.
96.296.1.
95.995.9.
95.6.table 3: performance of our new monomodel-* andmbertmodel-* models (see §a.5) ﬁne-tuned for thener, sa, qa, udp, and pos tasks (see §3.1), com-pared to the monolingual models from prior work andfully ﬁne-tuned mbert.
we group model counterpartsw.r.t.
tokenizer choice to facilitate a direct comparisonbetween respective counterparts.
we use developmentsets only for qa.
bold denotes best score across allmodels for a given language and task.
underlined de-notes best score compared to its respective counterpart..in §3, and report the results in table 3.25.the results indicate that the models trained withdedicated monolingual tokenizers outperform theircounterparts with multilingual tokenizers in mosttasks, with particular consistency for qa, udp,and sa.
in ner, the models trained with multilin-gual tokenizers score competitively or higher thanthe monolingual ones in half of the cases.
over-all, the performance gap is the smallest for postagging (at most 0.4% accuracy).
we observe the.
25full results including development set scores are available.
in table 9 of the appendix..largest gaps for qa (6.1 em / 4.4 f1 in id), sa(2.2% accuracy in tr), and ner (1.7 f1 in ar).
although the only language in which the monolin-gual counterpart always comes out on top is ko,the multilingual counterpart comes out on top atmost 3/10 times (for ar and tr) in the other lan-guages.
the largest decrease in performance of amonolingual tokenizer relative to its multilingualcounterpart is found for sa in tr (0.8% accuracy).
overall, we ﬁnd that for 38 out of 48 task, model,and language combinations, the monolingual tok-enizer outperforms the mbert counterpart.
wewere able to improve the monolingual performanceof the original mbert for 20 out of 24 languagesand tasks by only replacing the tokenizer and, thus,leveraging a specialized monolingual version.
sim-ilar to how the chosen method of tokenization af-fects neural machine translation quality (domingoet al., 2019), these results establish that, in fact,the designated pretrained tokenizer plays a funda-mental role in the monolingual downstream taskperformance of contemporary lms..in 18/24 language and task settings, the mono-lingual model from prior work (trained on moredata) outperforms its corresponding monomodel-monotok model.
4/6 settings in which ourmonomodel-monotok model performs better arefound for id, where indobert uses an uncasedtokenizer and our model uses a cased one, whichmay affect the comparison.
expectedly, these re-sults strongly indicate that data size plays a majorrole in downstream performance and corroborateprior research ﬁndings (liu et al., 2019; conneauet al., 2020; zhang et al., 2020, inter alia)..4.4 adapter-based training.
another way to provide more language-speciﬁc ca-pacity to a multilingual lm beyond a dedicated to-kenizer, thereby potentially making gains in mono-lingual downstream performance, is to introduce¨ust¨un et al.,adapters (pfeiffer et al., 2020b,c;2020), a small number of additional parameters atevery layer of a pretrained model.
to train adapters,usually all pretrained weights are frozen, while onlythe adapter weights are ﬁne-tuned.26 the adapter-based approaches thus offer increased efﬁciencyand modularity; it is crucial to verify to which ex-tent our ﬁndings extend to the more efﬁcient and.
26pfeiffer et al.
(2020b) propose to stack task-speciﬁcadapters on top of language adapters and extend this approachin pfeiffer et al.
(2020c) by additionally training new embed-dings for the target language..3124lg model.
ner satesttestf1acc.
qadevem / f1.
udptest.
postestuas / las acc.
ar.
fi.
id.
ko.
tr.
avg.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
90.089.689.791.1.
95.495.695.795.7.
88.2 —–88.5 —–88.4 —–88.1 —–.
93.593.593.593.4.
86.686.286.286.5.
93.893.093.592.7.
90.490.290.390.4.
91.490.693.693.8.
86.786.586.387.9.
86.483.984.885.3.
90.089.290.190.7.
66.1 / 80.666.7 / 81.166.9 / 81.067.7 / 82.1.
66.6 / 77.665.2 / 77.365.7 / 77.166.7 / 79.0.
71.2 / 82.170.6 / 82.570.8 / 82.274.4 / 84.4.
69.7 / 89.569.8 / 89.770.0 / 89.873.1 / 90.4.
57.9 / 76.455.3 / 75.156.9 / 75.860.0 / 77.0.
66.3 / 81.265.5 / 81.166.1 / 81.268.4 / 82.6.
88.8 / 83.887.8 / 82.688.0 / 82.888.5 / 83.4.
91.9 / 88.790.8 / 87.091.8 / 88.592.8 / 90.1.
85.9 / 79.384.8 / 77.485.4 / 78.185.1 / 78.3.
89.2 / 85.787.8 / 83.988.3 / 84.388.9 / 85.2.
74.5 / 67.472.4 / 64.173.0 / 64.775.7 / 68.1.
86.0 / 81.084.7 / 79.085.3 / 79.786.2 / 81.0.
96.896.896.896.5.
96.295.796.697.3.
93.593.493.493.5.
96.096.296.296.5.
95.795.795.996.3.
95.695.695.896.0.table 4: performance on the different tasks leveragingmbert with different adapter components (see §4.4)..more versatile adapter-based ﬁne-tuning setup..we evaluate the impact of different adapter com-ponents on the downstream task performance andtheir complementarity with monolingual tokenizersin table 4.27 here, +at ask and +alang impliesadding task- and language-adapters respectively,whereas +monotok additionally includes a newembedding layer.
as mentioned, we only ﬁne-tuneadapter weights on the downstream task, leveragingthe adapter architecture proposed by pfeiffer et al.
(2021).
for the +at ask + alang setting we lever-age pretrained language adapter weights availableat adapterhub.ml (pfeiffer et al., 2020a).
lan-guage adapters are added to the model and frozenwhile only task adapters are trained on the targettask.
for the +at ask +alang+ monotok we trainlanguage adapters and new embeddings with thecorresponding monolingual tokenizer equally as de-scribed in the previous section (e.g.
mbertmodel-monotok), task adapters are trained with a learningrate of 5e4 and 30 epochs with early stopping..−.
results.
similar to previous ﬁndings, adapters im-prove upon mbert in 18/24 language, and tasksettings, 13 of which can be attributed to the im-proved mbertmodel-monotok tokenizer.
figure 2illustrates the average performance of the differentadapter components in comparison to the mono-lingual models.
we ﬁnd that adapters with dedi-cated tokenizers reduce the performance gap con-.
27see appendix table 10 for the results on dev sets..figure 2: task performance averaged over all lan-guages for different models:fully ﬁne-tuned mono-lingual (mono), fully ﬁne-tuned mbert (mbert),mbert with task adapter (+atask), with task andlanguage adapter (+atask + alang), with task andlanguage adapter and embedding layerretraining(+atask + alang+ monotok)..siderably without leveraging more training data,and even outperform the monolingual models inqa.
this ﬁnding shows that adding additionallanguage-speciﬁc capacity to existing multilinguallms, which can be achieved with adapters in aportable and efﬁcient way, is a viable alternative tomonolingual pretraining..5 further analysis.
at ﬁrst glance, our results displayed in table 2seem to conﬁrm the prevailing view that mono-lingual models are more effective than multilin-gual models (r¨onnqvist et al., 2019; antoun et al.,2020; de vries et al., 2019, inter alia).
however,the broad scope of our experiments reveals certainnuances that were previously undiscovered.
un-like prior work, which primarily attributes gapsin performance to mbert being under-trained(r¨onnqvist et al., 2019; wu and dredze, 2020),our disentangled results (table 3) suggest that alarge portion of existing performance gaps can beattributed to the capability of the tokenizer..with monolingual tokenizers with lower fertil-ity and proportion-of-continued-words values thanthe mbert tokenizer (such as for ar, fi, id,ko, tr), consistent gains can be achieved, irre-spective of whether the lms are monolingual (themonomodel-* comparison) or multilingual (a com-parison of mbertmodel-* variants)..whenever the differences between monolingualmodels and mbert with respect to the tokenizerproperties and the pretraining corpus size are small(e.g., for en, ja, and zh), the performance gap istypically negligible.
in qa, we even ﬁnd mbertto be favorable for these languages.
therefore, weconclude that monolingual models are not superiorto multilingual ones per se, but gain advantage indirect comparisons by incorporating more pretrain-ing data and using language-adapted tokenizers..3125nersaudppos8090100scoresqa606570modelmonombert+atask+atask+alang+atask+alang+monotok6 conclusion.
we have conducted the ﬁrst comprehensive em-pirical investigation concerning the monolingualperformance of monolingual and multilingual lan-guage models (lms).
while our results support theexistence of a performance gap in most but not alllanguages and tasks, further analyses revealed thatthe gaps are often substantially smaller than whatwas previously assumed.
the gaps exist in certainlanguages due to the discrepancies in 1) pretrainingdata size, and 2) chosen tokenizers, and the levelof their adaptation to the target language..further, we have disentangled the impact of pre-trained corpora size from the inﬂuence of the tok-enizers on the downstream task performance.
wehave trained new monolingual lms on the samedata, but with two different tokenizers; one beingthe dedicated tokenizer of the monolingual lmprovided by native speakers; the other being theautomatically generated multilingual mbert tok-enizer.
we have found that for (almost) every taskand language, the use of monolingual tokenizersoutperforms the mbert tokenizer..consequently, in line with recent work by chunget al.
(2020), our results suggest that investing moreeffort into 1) improving the balance of individ-ual languages’ representations in the vocabularyof multilingual lms, and 2) providing language-speciﬁc adaptations and extensions of multilingualtokenizers (pfeiffer et al., 2020c) can reduce thegap between monolingual and multilingual lms.
another promising future research direction is com-pletely disposing of any (language-speciﬁc or mul-tilingual) tokenizers during pretraining (clark et al.,2021)..our code, pretrained models, and adapters areavailable at https://github.com/adapter-hub/hgiyt..acknowledgments.
jonas pfeiffer is supported by the loewe initia-tive (hesse, germany) within the emergencitycenter.
the work of ivan vuli´c is supported bythe erc consolidator grant lexical: lexicalacquisition across languages (no 648909)..we thank nils reimers, prasetya ajie utama,and adhiguna kuncoro for insightful feedback andsuggestions on a draft of this paper..figure 3: spearman’s ρ correlation of a relative de-crease in the proportion of continued words (cont.
pro-portion), a relative decrease in fertility, and a rela-tive increase in pretraining corpus size with a relativeincrease in downstream performance over fully ﬁne-tuned mbert.
for the proportion of continued wordsand the fertility, we consider fully ﬁne-tuned mbert,the monomodel-* models, and the mbertmodel-*models.
for the pretraining corpus size, we considerthe original monolingual models and the monomodel-monotok models.
we exclude the id models (see ap-pendix b.2 for the clariﬁcation)..correlation analysis.
to uncover additional pat-terns in our results (tables 2, 3, 4), we performa statistical analysis assessing the correlation be-tween the individual factors (pretraining data size,subword fertility, proportion of continued words)and the downstream performance.
although ourframework may not provide enough data pointsto be statistically representative, we argue that thecorrelation coefﬁcient can still provide reasonableindications and reveal relations not immediatelyevident by looking at the tables..figure 3 shows that both decreases in the propor-tion of continued words and the fertility correlatewith an increase in downstream performance rel-ative to fully ﬁne-tuned mbert across all tasks.
the correlation is stronger for udp and qa, wherewe ﬁnd models with monolingual tokenizers tooutperform their counterparts with the mbert to-kenizer consistently.
the correlation is weaker forner and pos tagging, which is also expected,considering the inconsistency of the results.28.
overall, we ﬁnd that the fertility and the pro-portion of continued words have a similar effecton the monolingual downstream performance asthe corpus size for pretraining; this indicates thatthe tokenizer’s ability of representing a languageplays a crucial role; consequently, choosing a sub-optimal tokenizer typically results in deteriorateddownstream performance..28for further information, see appendix b.2..3126allnerposqasaudptaskcont.proportionfertilitypre-trainsizemetric0.360.280.090.690.370.480.360.280.090.690.370.480.380.210.500.620.390.65−101referencesjudit ´acs.
2019. exploring bert’s vocabulary.
blog.
in natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..post..wissam antoun, fady baly, and hazem hajj.
2020.arabert: transformer-based model for arabic lan-in proceedings of the 4thguage understanding.
workshop on open-source arabic corpora and pro-cessing tools, with a shared task on offensive lan-guage detection, pages 9–15, marseille, france.
eu-ropean language resource association..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..giusepppe attardi.
2015. wikiextractor.
github.
repository..ethan c. chau, lucy h. lin, and noah a. smith.
2020.parsing with multilingual bert, a small corpus, andin findings of the associationa small treebank.
for computational linguistics: emnlp 2020, pages1324–1334, online.
association for computationallinguistics..hyung won chung, dan garrette, kiat chuan tan, andjason riesa.
2020. improving multilingual modelsin proceed-with language-clustered vocabularies.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4536–4546, online.
association for computationallinguistics..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
transactions of theassociation for computational linguistics, 8:454–470..jonathan h. clark, dan garrette, iulia turc, and johnwieting.
2021. canine: pre-training an efﬁcienttokenization-free encoder for language representa-tion.
arxiv preprint..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..erkin demirtas and mykola pechenizkiy.
2013. cross-lingual polarity detection with machine translation.
in proceedings of the second international work-shop on issues of sentiment discovery and opinionmining (wisdom ’13), pages 9:1–8, chicago, usa.
association for computing machinery..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..miguel domingo, mercedes garcıa-martınez, alexan-dre helle, francisco casacuberta, and manuel her-ranz.
2019. how much does tokenization affect neu-ral machine translation?
arxiv preprint..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in proceedings of the 5th international confer-ence on learning representations (iclr), toulon,france.
openreview.net..pavel eﬁmov, andrey chertok, leonid boytsov, andpavel braslavski.
2020. sberquad – russian read-ing comprehension dataset: description and analy-sis.
in clef 2020: experimental ir meets multilin-guality, multimodality, and interaction, pages 3–15.
springer, cham, switzerland..ashraf elnagar, yasmin s. khalifa, and anas einea.
2018. hotel arabic-reviews dataset constructionfor sentiment analysis applications.
in intelligentnatural language processing: trends and applica-tions, pages 35–52.
springer, cham, switzerland..daniela gerz, ivan vuli´c, edoardo maria ponti, roireichart, and anna korhonen.
2018. on the relationbetween linguistic typology and (limitations of) mul-tilingual language modeling.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 316–327, brussels, bel-gium.
association for computational linguistics..goran glavaˇs and ivan vuli´c.
2021. is supervised syn-tactic parsing beneﬁcial for language understandingtasks?
an empirical investigation.
in proceedings ofthe 16th conference of the european chapter of theassociation for computational linguistics: mainvolume, pages 3090–3104, online.
association forcomputational linguistics..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methods.
junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th international.
3127conference on machine learning, pages 4411–4421,virtual.
pmlr..pratik joshi, sebastin santy, amar budhiraja, kalikabali, and monojit choudhury.
2020. the state andfate of linguistic diversity and inclusion in the nlpin proceedings of the 58th annual meet-world.
ing of the association for computational linguistics,pages 6282–6293, online.
association for computa-tional linguistics..karthikeyan k, zihan wang, stephen mayhew, anddan roth.
2020. cross-lingual ability of multilin-gual bert: an empirical study.
in proceedings ofthe 8th international conference on learning rep-resentations (iclr), addis ababa, ethiopia.
open-review.net..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the 3rd international conference on learningrepresentations (iclr), san diego, ca, usa..yuri kuratov and mikhail arkhipov.
2019. adaptationof deep bidirectional multilingual transformers forrussian language.
arxiv preprint..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on thelimitations of zero-shot language transfer with mul-tilingual transformers.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 4483–4499, on-line.
association for computational linguistics..sangah lee, hansol jang, yunmee baik, suzi park,and hyopil shin.
2020. kr-bert: a small-scalekorean-speciﬁc language model.
arxiv preprint..patrick lewis, barlas oguz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7315–7330, online.
association for computational lin-guistics..seungyoung lim, myungji kim, and jooyoul lee.
2019. korquad1.0: korean qa dataset for ma-chine reading comprehension.
arxiv preprint..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint..ilya loshchilov and frank hutter.
2019. decoupledweight decay regularization.
in proceedings of the7th international conference on learning represen-tations (iclr), new orleans, la, usa.
openre-view.net..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts..2011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..louis martin, benjamin muller, pedro javier or-tiz su´arez, yoann dupont, laurent romary, ´ericde la clergerie, djam´e seddah, and benoˆıt sagot.
camembert: a tasty french language2020.in proceedings of the 58th annual meet-model.
ing of the association for computational linguistics,pages 7203–7219, online.
association for computa-tional linguistics..phoebe mulcaire, jungo kasai, and noah a. smith.
2019. polyglot contextual representations improvein proceedings of the 2019crosslingual transfer.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3912–3918, minneapolis, minnesota.
association for computational linguistics..joakim nivre, marie-catherine de marneffe, filip gin-ter, yoav goldberg, jan hajiˇc, christopher d. man-ning, ryan mcdonald, slav petrov, sampo pyysalo,natalia silveira, reut tsarfaty, and daniel zeman.
2016. universal dependencies v1: a multilingualtreebank collection.
in proceedings of the tenth in-ternational conference on language resources andevaluation (lrec’16), pages 1659–1666, portoroˇz,slovenia.
european language resources associa-tion (elra)..joakim nivre, marie-catherine de marneffe, filip gin-ter, jan hajiˇc, christopher d. manning, sampopyysalo, sebastian schuster, francis tyers, anddaniel zeman.
2020. universal dependencies v2:an evergrowing multilingualtreebank collection.
in proceedings of the 12th language resourcesand evaluation conference, pages 4034–4043, mar-seille, france.
european language resources asso-ciation..debora nozza, federico bianchi, and dirk hovy.
2020.the [mask]?
making sense of language-.
whatspeciﬁc bert models.
arxiv preprint..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages.
31282227–2237, new orleans, louisiana.
associationfor computational linguistics..jonas pfeiffer, aishwarya kamath, andreas r¨uckl´e,kyunghyun cho,and iryna gurevych.
2021.adapterfusion: non-destructive task compositionin proceedings of the 16thfor transfer learning.
conference of the european chapter of the associ-ation for computational linguistics: main volume,pages 487–503, online.
association for computa-tional linguistics..jonas pfeiffer, andreas r¨uckl´e, clifton poth, aish-ivan vuli´c, sebastian ruder,warya kamath,kyunghyun cho,and iryna gurevych.
2020a.
adapterhub: a framework for adapting transform-ers.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 46–54, online.
asso-ciation for computational linguistics..jonas pfeiffer, ivan vuli´c, iryna gurevych, and se-bastian ruder.
2020b.
mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7654–7673, online.
association for computa-tional linguistics..jonas pfeiffer, ivan vuli´c, iryna gurevych, and se-bastian ruder.
2020c.
unks everywhere: adapt-ing multilingual language models to new scripts.
arxiv preprint..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..edoardo maria ponti, goran glavaˇs, olga majewska,qianchu liu, ivan vuli´c, and anna korhonen.
2020.xcopa: a multilingual dataset for causal common-in proceedings of the 2020 con-sense reasoning.
ference on empirical methods in natural languageprocessing (emnlp), pages 2362–2376, online.
as-sociation for computational linguistics..advanced informatics: concepts, theory and appli-cations (icaicta), pages 1–5, yogyakarta, indone-sia.
ieee..sampo pyysalo, jenna kanerva, antti virtanen, andfilip ginter.
2020. wikibert models: deep trans-fer learning for many languages.
arxiv preprint..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..teemu ruokolainen, pekka kauppinen, miikka silfver-berg, and krister lind´en.
2020. a finnish newscorpus for named entity recognition.
language re-sources and evaluation, 54(1):247–272..samuel r¨onnqvist, jenna kanerva, tapio salakoski,and filip ginter.
2019.is multilingual bert ﬂu-ent in language generation?
in proceedings of thefirst nlpl workshop on deep learning for naturallanguage processing, pages 29–36, turku, finland.
link¨oping university electronic press..stefan schweter.
2020. berturk - bert models for.
turkish.
zenodo..chih chieh shao, trois liu, yuting lai, yiying tseng,and sam tsai.
2019. drcd: a chinese machinereading comprehension dataset.
arxiv preprint..edoardo maria ponti, helen o’horan, yevgeni berzak,ivan vuli´c, roi reichart, thierry poibeau, ekaterinashutova, and anna korhonen.
2019. modeling lan-guage variation and universals: a survey on typo-logical linguistics for natural language processing.
computational linguistics, 45(3):559–601..sergey smetanin and michail komarov.
2019. sen-timent analysis of product reviews in russian us-ing convolutional neural networks.
in proceedingsof the 2019 ieee 21st conference on business in-formatics (cbi), pages 482–486, moscow, russia.
ieee..lutz prechelt.
1998. early stopping-but when?.
inneural networks: tricks of the trade, pages 55–69.
springer, berlin, germany..ayu purwarianti and ida ayu putu ari crisdayanti.
2019. improving bi-lstm performance for indone-sian sentiment analysis using paragraph vector.
inproceedings of the 2019 international conference of.
milan straka, jan hajiˇc, and jana strakov´a.
2016. ud-pipe: trainable pipeline for processing conll-uﬁles performing tokenization, morphological analy-in proceedings ofsis, pos tagging and parsing.
the tenth international conference on language re-sources and evaluation (lrec’16), pages 4290–4297, portoroˇz, slovenia.
european language re-sources association (elra)..3129erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147, edmonton, canada.
association for com-putational linguistics..ahmet ¨ust¨un, arianna bisazza, gosse bouma, andgertjan van noord.
2020. udapter: language adap-intation for truly universal dependency parsing.
proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2302–2315, online.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention isin advances in neural informationall you need.
processing systems, pages 5998–6008, long beach,ca, usa.
curran associates, inc..antti virtanen, jenna kanerva, rami ilo, jouni luoma,juhani luotolahti, tapio salakoski, filip ginter, andsampo pyysalo.
2019. multilingual is not enough:bert for finnish.
arxiv preprint..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..shijie wu and mark dredze.
2020. are all languagescreated equal in multilingual bert?
in proceedingsof the 5th workshop on representation learning fornlp, pages 120–130, online.
association for com-putational linguistics..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, lukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation.
arxiv preprint..wietse de vries, andreas van cranenburgh, ariannabisazza, tommaso caselli, gertjan van noord, andmalvina nissim.
2019. bertje: a dutch bertmodel.
arxiv preprint..jingjing xu, ji wen, xu sun, and qi su.
2017. adiscourse-level named entity recognition and rela-tion extraction dataset for chinese literature text.
arxiv preprint..ivan vuli´c, edoardo maria ponti, robert litschko,goran glavaˇs, and anna korhonen.
2020. probingpretrained language models for lexical semantics.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7222–7240, online.
association for computa-tional linguistics..bryan wilie, karissa vincentio, genta indra winata,samuel cahyawijaya, xiaohong li, zhi yuan lim,sidik soleman, rahmad mahendra, pascale fung,syafri bahar, and ayu purwarianti.
2020. indonlu:benchmark and resources for evaluating indonesiannatural language understanding.
in proceedings ofthe 1st conference of the asia-paciﬁc chapter of theassociation for computational linguistics and the10th international joint conference on natural lan-guage processing, pages 843–857, suzhou, china.
association for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, adityabarua, and colin raffel.
2021. mt5: a massivelymultilingual pre-trained text-to-text transformer.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 483–498, online.
association for computa-tional linguistics..daniel zeman, joakim nivre, mitchell abrams, eliaackermann, no¨emi aepli, ˇzeljko agi´c, lars ahren-berg, chika kennedy ajede, gabriel˙e aleksan-draviˇci¯ut˙e, lene antonsen, katya aplonova, an-gelina aquino, maria jesus aranzabe, gashaw aru-tie, masayuki asahara, luma ateyah, furkan at-maca, mohammed attia, aitziber atutxa, lies-beth augustinus, elena badmaeva, miguel balles-teros, esha banerjee, sebastian bank, verginicabarbu mititelu, victoria basmov, colin batchelor,john bauer, kepa bengoetxea, yevgeni berzak, ir-shad ahmad bhat, riyaz ahmad bhat, erica bi-agetti, eckhard bick, agn˙e bielinskien˙e, rogierblokland, victoria bobicev, lo¨ıc boizou, emanuelborges v¨olker, carl b¨orstell, cristina bosco, gossebouma, sam bowman, adriane boyd, kristinabrokait˙e, aljoscha burchardt, marie candito,bernard caron, gauthier caron, tatiana cavalcanti,g¨uls¸en cebiro˘glu eryi˘git, flavio massimiliano cec-ˇc´epl¨o,chini, giuseppe g. a. celano, slavom´ırsavas cetin, fabricio chalub, ethan chi, jinhochoi, yongseok cho, jayeol chun, alessandra t..3130juk, guilherme paulino-passos, angelika peljak-łapi´nska, siyao peng, cenel-augusto perez, guyperrier, daria petrova, slav petrov, jason phelan,jussi piitulainen, tommi a pirinen, emily pitler,barbara plank, thierry poibeau, larisa ponomareva,martin popel, lauma pretkalnin¸a, sophie pr´evost,prokopis prokopidis, adam przepi´orkowski, ti-ina puolakainen, sampo pyysalo, peng qi, an-driela r¨a¨abis, alexandre rademaker, loganathanramasamy, taraka rama, carlos ramisch, vinitravishankar, livy real, petru rebeja, siva reddy,georg rehm,ivan riabov, michael rießler,erika rimkut˙e, larissa rinaldi, laura rituma,luisa rocha, mykhailo romanenko, rudolf rosa,valentin ros, ca, davide rovati, olga rudina, jackrueter, shoval sadde, benoˆıt sagot, shadi saleh,alessio salomoni, tanja samardˇzi´c, stephaniesamson, manuela sanguinetti, dage s¨arg, baibasaul¯ıte, yanin sawanakunanon, salvatore scarlata,nathan schneider, sebastian schuster, djam´e sed-dah, wolfgang seeker, mojgan seraji, mo shen,atsuko shimada, hiroyuki shirasu, muh shohibus-sirri, dmitry sichinava, aline silveira, natalia sil-veira, maria simi, radu simionescu, katalin simk´o,m´aria ˇsimkov´a, kiril simov, maria skachedubova,aaron smith, isabela soares-bastos, carolyn spa-dine, antonio stella, milan straka, jana strnadov´a,alane suhr, umut sulubacak, shingo suzuki, zsoltsz´ant´o, dima taji, yuta takahashi, fabio tam-burini, takaaki tanaka, samson tella,isabelletellier, guillaume thomas, liisi torga, marsidatoska, trond trosterud, anna trukhina, reut tsar-faty, utku t¨urk, francis tyers, sumire uematsu,roman untilov, zdeˇnka ureˇsov´a, larraitz uria,hans uszkoreit, andrius utka, sowmya vajjala,daniel van niekerk, gertjan van noord, viktorvarga, eric villemonte de la clergerie, veronikavincze, aya wakasa, lars wallin, abigail walsh,jing xian wang, jonathan north washington, max-imilan wendt, paul widmer, seyi williams, matswir´en, christian wittern, tsegay woldemariam,tak-sum wong, alina wr´oblewska, mary yako,kayo yamashita, naoki yamazaki, chunxiao yan,koichi yasuoka, marat m. yavrumyan, zhuoran yu,zdenˇek ˇzabokrtsk´y, amir zeldes, hanzhi zhu, andanna zhuravleva.
2020. universal dependencies2.6. lindat/clariah-cz digital library at theinstitute of formal and applied linguistics ( ´ufal),faculty of mathematics and physics, charles uni-versity..yian zhang, alex warstadt, haau-sing li, andsamuel r. bowman.
2020. when do you need bil-lions of words of pretraining data?
arxiv preprint..cignarella, silvie cinkov´a, aur´elie collomb, c¸ a˘grıc¸ ¨oltekin, miriam connor, marine courtin, eliza-beth davidson, marie-catherine de marneffe, vale-ria de paiva, elvis de souza, arantza diaz de ilar-raza, carly dickerson, bamba dione, peter dirix,kaja dobrovoljc, timothy dozat, kira droganova,puneet dwivedi, hanne eckhoff, marhaba eli, alielkahky, binyam ephrem, olga erina, tomaˇz er-javec, aline etienne, wograine evelyn, rich´ardfarkas, hector fernandez alcalde, jennifer fos-ter, cl´audia freitas, kazunori fujita, katar´ınagajdoˇsov´a, daniel galbraith, marcos garcia, moag¨ardenfors, sebastian garza, kim gerdes, filipginter, iakes goenaga, koldo gojenola, memduhg¨okırmak, yoav goldberg, xavier g´omez guino-vart, berta gonz´alez saavedra, bernadeta grici¯ut˙e,matias grioni, lo¨ıc grobol, normunds gr¯uz¯ıtis,bruno guillaume, c´eline guillot-barbance, tungag¨ung¨or, nizar habash, jan hajiˇc, jan hajiˇc jr.,mika h¨am¨al¨ainen, linh h`a m˜y, na-rae han,kim harris, dag haug, johannes heinecke, oliverhellwig, felix hennig, barbora hladk´a, jaroslavahlav´aˇcov´a, florinel hociung, petter hohle, jenahwang, takumi ikeda, radu ion, elena irimia,o. l´aj´ıd´e ishola, tom´aˇs jel´ınek, anders johannsen,hildur j´onsd´ottir, fredrik jørgensen, markus juu-tinen, h¨uner kas¸ıkara, andre kaasen, nadezhdakabaeva, sylvain kahane, hiroshi kanayama,jenna kanerva, boris katz, tolga kayadelen, jes-sica kenney, v´aclava kettnerov´a, jesse kirchner,elena klementieva, arne k¨ohn, abdullatif k¨oksal,kamil kopacewicz, timo korkiakangas, nataliakotsyba, jolanta kovalevskait˙e, simon krek, sooky-oung kwak, veronika laippala, lorenzo lam-bertino, lucia lam, tatiana lando, septina dianlarasati, alexei lavrentiev, john lee, phuonglˆe h`ˆong, alessandro lenci, saran lertpradit, her-man leung, maria levina, cheuk ying li, josieli, keying li, kyungtae lim, yuan li, nikolaljubeˇsi´c, olga loginova, olga lyashevskaya,teresa lynn, vivien macketanz, aibek makazhanov,michael mandl, christopher manning, ruli ma-nurung, c˘at˘alina m˘ar˘anduc, david mareˇcek, ka-trin marheinecke, h´ector mart´ınez alonso, andr´emartins, jan maˇsek, hiroshi matsuda, yuji mat-sumoto, ryan mcdonald, sarah mcguinness, gus-tavo mendonc¸a, niko miekka, margarita misir-pashayeva, anna missil¨a, c˘at˘alin mititelu, mariamitrofan, yusuke miyao, simonetta montemagni,amir more, laura moreno romero, keiko sophiemori, tomohiko morioka, shinsuke mori, shigekimoro, bjartur mortensen, bohdan moskalevskyi,kadri muischnek, robert munro, yugo murawaki,kaili m¨u¨urisep, pinkey nainwani,juan igna-cio navarro hor˜niacek, anna nedoluzhko, guntaneˇspore-b¯erzkalne, luong nguy˜ˆen thi., huy`ˆennguy˜ˆen thi.
minh, yoshihiro nikaido, vitaly niko-laev, rattima nitisaroj, hanna nurmi, stina ojala,atul kr.
ojha, ad´edayo.
ol´u`okun, mai omura,emeka onwuegbuzia, petya osenova, robert¨ostling, lilja øvrelid, s¸ aziye bet¨ul ¨ozates¸, arzu-can ¨ozg¨ur, balkız ¨ozt¨urk bas¸aran, niko partanen,elena pascual, marco passarotti, agnieszka pate-.
3131a reproducibility.
a.1 pretrained models.
all of the pretrained language models we use areavailable on the huggingface model hub29 andcompatible with the huggingface transformerspython library (wolf et al., 2020).
table 5 displaysthe model hub identiﬁers of our selected models..a.2 estimating the pretraining corpora.
sizes.
since mbert was pretrained on the entirewikipedia dumps of all languages it covers (de-vlin et al., 2019), we estimate the language-speciﬁcshares of the mbert pretraining corpus by wordcounts of the respective raw wikipedia dumps, ac-cording to numbers obtained from wikimedia30:327m words for ar, 3.7b for en, 134m for fi,142m for id, 1.1b for ja, 125m for ko, 781m forru, 104m for tr, 482m for zh.31 devlin et al.
(2019) only included text passages from the arti-cles, and used older wikipedia dumps, so thesenumbers should serve as upper limits, yet be rea-sonably accurate.
for the monolingual models, werely on information provided by the authors.32.
a.3 data for tokenizer analyses.
we tokenize the training and development splitsof the ud (nivre et al., 2016, 2020) v2.6 (zemanet al., 2020) treebanks listed in table 6..a.4 fine-tuning datasets.
we list the datasets we used, including the numberof examples per dataset split, in the table 7..a.5 training procedure of new models.
we pretrain our models on single nvidia teslav100, a100, and titan rtx gpus with 32gb,40gb, and 24gb of video memory, respectively.
to support larger batch sizes, we train in mixed-precision (fp16) mode.
following wu and dredze(2020), we only use masked language modeling(mlm) as pretraining objective and omit the nextsentence prediction task as liu et al.
(2019) ﬁnd itdoes not yield performance gains.
we otherwise.
29https://huggingface.co/models30https://meta.m.wikimedia.org/wiki/list of wikipedias31we obtained the numbers for id and tr on dec 10, 2020.and for the remaining languages on sep 10, 2020..32for ja, ru, and zh, the authors do not provide exact wordcounts.
therefore, we estimate them using other providedinformation (ru, zh) or scripts for training corpus reconstruc-tion (ja)..−.
mostly follow the default pretraining procedure bydevlin et al.
(2019).
we pretrain the new monolingual models(monomodel-*) from scratch for 1m steps withbatch size 64. we choose a sequence length of128 for the ﬁrst 900,000 steps and 512 for thein both phases, weremaining 100,000 steps.
warm up the learning rate to 1e4 over the ﬁrst10,000 steps, then decay linearly.
we use theadam optimizer with weight decay (adamw)(loshchilov and hutter, 2019) with defaulthyper-parameters and a weight decay of 0.01. weenable whole word masking (devlin et al., 2019)for the fi monolingual models, following thepretraining procedure for finbert (virtanen et al.,2019).
to lower computational requirements forthe monolingual models with mbert tokenizers,we remove all tokens from mbert’s vocabularythat do not appear in the pretraining data.
we,thereby, obtain vocabularies of size 78,193 (ar),60,827 (fi), 72,787 (id), 66,268 (ko), and 71,007(tr), which for all languages reduces the numberof parameters in the embedding layer signiﬁcantly,compared to the 119,547 word piece vocabulary ofmbert.
for(i.e.,thembertmodel-*), we run mlm for 250,000steps (similar to artetxe et al.
(2020)) with batchsize 64 and sequence length 512, otherwise usingthe same hyper-parameters as for the monolingualmodels.
in order to retrain the embedding layer,we ﬁrst resize it to match the vocabulary ofthe respective tokenizer.
for the mbertmodel-mberttok models, we use the mbert tokenizerswith reduced vocabulary as outlined above.
weinitialize the positional embeddings, segmentembeddings, and embeddings of special tokens([cls], [sep], [pad], [unk], [mask])frommbert, and reinitialize the remaining embeddingsrandomly.
we freeze all parameters outside theembedding layer.
for all pretraining runs, we setthe random seed to 42..retrained mbert models.
a.6 code.
our code with usage instructions for ﬁne-tuning, pretraining, data preprocessing, and cal-culating the tokenizer statistics is available athttps://github.com/adapter-hub/hgiyt.
the repos-itory also contains further links to a collection ofour new pretrained models and language adapters..3132kenizer consistently.
the correlation is weaker forner and pos tagging, which is also expected,considering the inconsistency of the results..somewhat surprisingly, the tokenizer metricsseem to be more indicative of high downstreamperformance than the size of the pretraining cor-pus.
we believe that this in parts due to the overallpoor performance of the uncased indobert model,which we (in this case unfairly) compare to ourcased id-monomodel-monotok model.
therefore,we plot the same correlation matrix excluding idin figure 3..compared to figure 6b, the overall correlationsfor the proportion of continued words and the fer-tility remain mostly unaffected.
in contrast, thecorrelation for the pretraining corpus size becomesmuch stronger, conﬁrming that the subpar perfor-mance of indobert is indeed an outlier in thisscenario.
leaving out indonesian also strengthensthe indication that the performance in pos taggingcorrelates more with the data size than with thetokenizer, although we argue that this indicationmay be misleading.
the performance gap is gen-erally very minor in pos tagging.
therefore, thespearman correlation coefﬁcient, which only takesthe rank into account, but not the absolute scoredifferences, is particularly sensitive to changes inpos tagging performance..finally, we plot the correlation between the threemetrics and the downstream performance underconsideration of all languages and models, includ-ing the adapter-based ﬁne-tuning settings, to gainan understanding of how pronounced their effectsare in a more “noisy” setting..as figure 6a shows, the three factors still corre-late with the downstream performance in a similarmanner even when not isolated.
this correlationtells us that even when there may be other factorsthat could have an inﬂuence, these three factorsare still highly indicative of the downstream perfor-mance..we also see that the correlation coefﬁcients forthe proportion of continued words and the fertilityare nearly identical, which is expected based onthe visual similarity of the respective plots (seen infigures 1b and 1c)..b further analyses and discussions.
b.1 tokenization analysis.
in our tokenization analysis in §4.2 of the main text,we only include the fertility and the proportion ofcontinued words as they are sufﬁcient to illustrateand quantify the differences between tokenizers.
insupport of the ﬁndings in §4.2 and for complete-ness, we provide additional tokenization statisticshere..for each tokenizer, table 5 lists the respectivevocabulary size and the proportion of its vocabu-lary also contained in mbert.
it shows that thetokenizers scoring lower in fertility (and accord-ingly performing better) than mbert are often notadequately covered by mbert’s vocabulary.
forinstance, only 5.6% of the arabert (ar) vocabu-lary is covered by mbert..figure 4 compares the proportion of unknowntokens ([unk]) in the tokenized data.
it shows thatthe proportion is generally extremely low, i.e., thetokenizers can typically split unknown words intoknown subwords..similar to the work by ´acs (2019), figure 5compares the tokenizations produced by the mono-lingual models and mbert with the reference to-kenizations provided by the human dataset anno-tators with respect to their sentence lengths.
weﬁnd that the tokenizers scoring low in fertility andthe proportion of continued words typically exhibitsentence length distributions much closer to thereference tokenizations by human ud annotators,indicating they are more capable than the mberttokenizer.
likewise, the monolingual models’ andmbert’s sentence length distributions are closerfor languages with similar fertility and proportionof continued words, such as en, ja, and zh..b.2 correlation analysis.
to uncover some of the hidden patterns in our re-sults (tables 2, 3, 4), we perform a statistical analy-sis assessing the correlation between the individualfactors (pretraining data size, subword fertility, pro-portion of continued words) and the downstreamperformance..figure 6b shows that both decreases in the pro-portion of continued words and the fertility corre-late with an increase in downstream performancerelative to fully ﬁne-tuned mbert across all tasks.
the correlation is stronger for udp and qa, wherewe found models with monolingual tokenizers tooutperform their counterparts with the mbert to-.
3133c full results.
for compactness, we have only reported the perfor-mance of our models on the respective test datasetsin the main text.33 for completeness, we also in-clude the full tables, including development (dev)dataset performance averaged over three randominitializations, as described in §3.
table 8 showsthe full results corresponding to table 2 (initialresults), table 9 shows the full results correspond-ing to table 3 (results for our new models), andtable 10 shows the full results corresponding totable 4 (adapter-based training)..lang model.
reference.
v. size % voc.
multi.
bert-base-multilingual-cased.
devlin et al.
(2019).
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
antoun et al.
(2020)devlin et al.
(2019).
aubmindlab/bert-base-arabertv01bert-base-casedturkunlp/bert-base-ﬁnnish-cased-v1 virtanen et al.
(2019)indobenchmark/indobert-base-p2cl-tohoku/bert-base-japanese-charsnunlp/kr-bert-char16424deeppavlov/rubert-base-caseddbmdz/bert-base-turkish-casedbert-base-chinese.
lee et al.
(2020)kuratov and arkhipov (2019)schweter (2020)devlin et al.
(2019).
wilie et al.
(2020)5.
119547.
640002899650105305214000164241195473200021128.
100.
5.666.414.340.599.147.421.123.079.4.table 5: selection of pretrained models used in our ex-periments.
we display the respective vocabulary sizesand the proportion of tokens that are also covered bymbert’s vocabulary..task lang dataset.
reference.
ner.
sa.
qa.
ud.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
wikiannconll-2003finerwikiannwikiannkmou nerwikiannwikiannchinese literature.
pan et al.
(2017); rahimi et al.
(2019)tjong kim sang and de meulder (2003)ruokolainen et al.
(2020)pan et al.
(2017); rahimi et al.
(2019)pan et al.
(2017); rahimi et al.
(2019)6.pan et al.
(2017); rahimi et al.
(2019)pan et al.
(2017); rahimi et al.
(2019)xu et al.
(2017).
elnagar et al.
(2018)maas et al.
(2011)—purwarianti and crisdayanti (2019)7.hardimdb movie reviews—indonesian prosayahoo movie reviewsnsmcrureviewsmovie & product reviews demirtas and pechenizkiy (2013)chnsenticorp.
smetanin and komarov (2019).
8.
9.tydiqa-goldpsquad v1.1tydiqa-goldptydiqa-goldp—korquad 1.0sberquadtquaddrcd.
padtewtftbgsdgsdgsdgsdimstgsd.
clark et al.
(2020)rajpurkar et al.
(2016)clark et al.
(2020)clark et al.
(2020)—lim et al.
(2019)eﬁmov et al.
(2020)10.shao et al.
(2019).
(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020)(zeman et al., 2020).
train / dev / test.
20000 / 10000 / 1000014041 / 3250 / 345313497 / 986 / 351220000 / 10000 / 1000020202 / 10100 / 1011323056 / 468 / 46320000 / 10000 / 1000020000 / 10000 / 1000024270 / 1902 / 2844.
84558 / 10570 / 1057020000 / 5000 / 25000—6853 / 763 / 40930545 / 3818 / 3819120000 / 30000 / 5000048000 / 6000 / 600013009 / 1627 / 16299600 / 1200 / 1200.
14805 / 92187599 / 105706855 / 7825702 / 565—60407 / 577445328 / 50368308 / 89226936 / 3524.
6075 / 909 / 68012543 / 2002 / 207714981 / 1875 / 18674477 / 559 / 5577027 / 501 / 5434400 / 950 / 9893850 / 579 / 6013664 / 988 / 9833997 / 500 / 500.table 7: named entity recognition (ner), sentimentanalysis (sa), question answering (qa), and universaldependencies (ud) datasets used in our experimentsand the number of examples in their respective train-ing, development, and test portions.
ud datasets wereused for both universal dependency parsing and postagging experiments..lang treebank.
# words.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
padt254192lines, ewt, gum, partut 449977324680ftb, tdt110141gsd179571gsd390369gsd1130482gsd, syntagrus, taiga47830imst222558gsd, gsdsimp.
table 6: ud v2.6 (zeman et al., 2020) treebanks usedfor our tokenizer analyses.
we use training and devel-opment portions only and display the total number ofwords per language..(a) ar.
(b) en.
(c) fi.
(d) id.
(e) ja.
(f) ko.
figure 4: proportion of unknown tokens in respectivemonolingual corpora tokenized by monolingual modelsvs. mbert..33except for qa, where we do not use any test data.
(g) ru.
(h) tr.
(i) zh.
figure 5: sentence length distributions of monolin-gual ud corpora tokenized by respective monolingualbert models and mbert, compared to the referencetokenizations by human ud treebank annotators..3134arenfiidjakorutrzh0.00.0040.0080.0120.016proportionunkmodelmonombert050100150200sentencelength[tokens]0.0000.0050.0100.015proportionreferencemonombert050100150200sentencelength[tokens]0.000.010.020.03proportionreferencemonombert050100150200sentencelength[tokens]0.000.020.040.060.08proportionreferencemonombert050100150200sentencelength[tokens]0.000.010.020.030.04proportionreferencemonombert050100150200sentencelength[tokens]0.000.010.020.03proportionreferencemonombert050100150200sentencelength[tokens]0.000.020.040.06proportionreferencemonombert050100150200sentencelength[tokens]0.000.010.020.030.04proportionreferencemonombert050100150200sentencelength[tokens]0.000.020.040.060.08proportionreferencemonombert050100150200sentencelength[tokens]0.000.010.020.030.04proportionreferencemonombertlg model.
ner.
sa.
devf1.
test dev testf1acc acc.
qadevem / f1.
udp.
postestdev testuas / las uas / las acc acc.
dev.
ar.
fi.
id.
ko.
tr.
monolingual.
91.5.
91.1.
96.1.
95.9.
68.3 / 82.4.
89.4 / 85.0.
90.1 / 85.6.
97.5.
96.8.monomodel-monotokmonomodel-mberttok.
mbertmodel-monotokmbertmodel-mberttok.
88.690.1.
91.990.0.
91.790.0.
91.289.7.
96.095.9.
95.995.8.
95.695.5.
95.495.6.
67.7 / 81.664.1 / 79.4.
88.4 / 83.787.8 / 83.2.
89.2 / 84.488.8 / 84.0.
66.9 / 81.866.3 / 80.7.
88.2 / 83.587.8 / 83.0.
89.3 / 84.589.1 / 84.2.
97.397.4.
97.297.3.
96.697.0.
96.496.8.mbert.
90.3.
90.0.
95.8.
95.4.
66.1 / 80.6.
87.8 / 83.0.
88.8 / 83.8.
97.2.
96.8.monolingual.
93.3.
92.0 —– —–.
69.9 / 81.6.
95.7 / 93.9.
95.9 / 94.4.
98.1.
98.4.monomodel-monotokmonomodel-mberttok.
89.1 —– —–90.0 —– —–.
66.9 / 79.565.1 / 77.0.
93.6 / 91.093.1 / 90.6.
93.7 / 91.593.6 / 91.5.mbertmodel-monotokmbertmodel-mberttok.
88.1 —– —–88.1 —– —–.
66.4 / 78.365.9 / 77.3.
92.2 / 89.392.1 / 89.2.
92.4 / 89.692.2 / 89.4.
91.991.8.
91.092.0.
97.096.2.
96.396.6.
97.397.0.
96.696.7.mbert.
90.9.
88.2 —– —–.
66.6 / 77.6.
91.1 / 88.0.
91.9 / 88.7.
96.0.
96.2.monolingual.
90.9.
91.0.
94.6.
96.0.
66.8 / 78.1.
84.5 / 77.4.
85.3 / 78.1.
92.0.
92.1.monomodel-monotokmonomodel-mberttok.
mbertmodel-monotokmbertmodel-mberttok.
93.093.3.
93.893.9.
92.593.2.
93.993.9.
93.993.9.
94.493.7.
96.094.8.
94.694.6.
73.1 / 83.667.0 / 79.2.
83.4 / 76.884.0 / 77.4.
85.0 / 78.584.9 / 78.6.
74.1 / 83.871.9 / 82.7.
85.5 / 78.885.3 / 78.6.
86.4 / 80.286.2 / 79.6.
93.693.4.
93.593.4.
93.993.6.
93.893.7.mbert.
93.7.
93.5.
93.1.
91.4.
71.2 / 82.1.
85.0 / 78.4.
85.9 / 79.3.
93.3.
93.5.monolingual.
88.6.
88.8.
89.8.
89.7.
74.2 / 91.1.
88.5 / 85.0.
90.3 / 87.2.
96.4.
97.0.monomodel-monotokmonomodel-mberttok.
mbertmodel-monotokmbertmodel-mberttok.
87.986.9.
87.986.7.
87.185.8.
86.686.2.
89.087.3.
88.286.6.
88.887.2.
88.186.6.
72.8 / 90.368.9 / 88.7.
87.9 / 84.286.9 / 83.2.
89.8 / 86.688.9 / 85.6.
72.9 / 90.269.3 / 89.3.
87.9 / 83.987.2 / 83.3.
90.1 / 87.089.2 / 85.9.
96.496.1.
96.295.9.
96.796.4.
96.596.2.mbert.
87.3.
86.6.
86.7.
86.7.
69.7 / 89.5.
86.9 / 83.2.
89.2 / 85.7.
95.8.
96.0.monolingual.
93.1.
92.8.
89.3.
88.8.
60.6 / 78.1.
78.0 / 70.9.
79.8 / 73.2.
97.0.
96.9.monomodel-monotokmonomodel-mberttok.
mbertmodel-monotokmbertmodel-mberttok.
93.593.2.
93.593.9.
93.493.3.
93.793.8.
87.585.8.
86.186.0.
87.084.8.
85.386.1.
56.2 / 73.755.3 / 72.5.
74.4 / 67.373.2 / 66.0.
76.1 / 68.975.3 / 68.3.
59.4 / 76.758.7 / 76.6.
74.7 / 67.673.2 / 66.1.
77.1 / 70.276.2 / 69.2.
95.996.4.
96.195.9.
96.396.5.
96.396.3.mbert.
93.7.
93.8.
86.4.
86.4.
57.9 / 76.4.
72.6 / 65.2.
74.5 / 67.4.
95.5.
95.7.monolingual.
91.5.
91.1.
92.5.
92.6.
68.0 / 82.3.
87.2 / 82.4.
88.3 / 83.7.
96.2.
96.2.avg.
monomodel-monotokmonomodel-mberttok.
mbertmodel-monotokmbertmodel-mberttok.
91.091.1.
91.691.3.
90.890.5.
90.790.3.
91.690.7.
91.290.5.
91.990.6.
90.990.7.
67.3 / 81.764.1 / 79.4.
85.5 / 80.685.0 / 80.1.
86.8 / 82.086.3 / 81.6.
68.0 / 82.266.4 / 81.3.
85.7 / 80.685.1 / 80.0.
87.1 / 82.386.6 / 81.7.
96.095.9.
95.995.8.
96.296.1.
95.995.9.mbert.
91.2.
90.4.
90.5.
90.0.
66.3 / 81.2.
84.7 / 79.6.
86.1 / 81.0.
95.6.
95.6.table 9: full results - performance of our newmonomodel-* and mbertmodel-* models (see§a.5) ﬁne-tuned for the ner, sa, qa, udp, and postasks (see §3.1), compared to the monolingual mod-els from prior work and fully ﬁne-tuned mbert.
wegroup model counterparts w.r.t.
tokenizer choice to fa-cilitate a direct comparison between respective counter-parts.
we use development sets only for qa.
bold de-notes best score across all models for a given languageand task.
underlined denotes best score compared toits respective counterpart..lg model.
ar.
fi.
id.
ko.
tr.
avg.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
mbert+ atask+ atask + alang+ atask + alang + monotok.
ner.
sa.
devf1.
90.390.090.291.5.
90.991.291.690.8.
93.793.393.693.0.
87.387.187.387.7.
93.793.093.392.7.
91.290.991.291.1.test dev testf1acc acc.
90.089.689.791.1.
95.896.196.196.0.
95.495.695.795.7.
88.2 —– —–88.5 —– —–88.4 —– —–88.1 —– —–.
93.593.593.593.4.
86.686.286.286.5.
93.893.093.592.7.
90.490.290.390.4.
93.192.993.194.5.
86.786.786.687.9.
86.486.186.286.1.
90.590.590.591.1.
91.490.693.693.8.
86.786.586.387.9.
86.483.984.885.3.
90.089.290.190.7.qadevem / f1.
66.1 / 80.666.7 / 81.166.9 / 81.067.7 / 82.1.
66.6 / 77.665.2 / 77.365.7 / 77.166.7 / 79.0.
71.2 / 82.170.6 / 82.570.8 / 82.274.4 / 84.4.
69.7 / 89.569.8 / 89.770.0 / 89.873.1 / 90.4.
57.9 / 76.455.3 / 75.156.9 / 75.860.0 / 77.0.
66.3 / 81.265.5 / 81.166.1 / 81.268.4 / 82.6.udp.
postestdev testuas / las uas / las acc acc.
dev.
87.8 / 83.086.7 / 81.687.0 / 81.987.7 / 82.8.
91.1 / 88.090.2 / 86.391.1 / 87.792.8 / 89.9.
85.0 / 78.483.7 / 76.584.3 / 77.484.6 / 77.6.
86.9 / 83.285.5 / 81.185.9 / 81.687.0 / 82.7.
72.6 / 65.270.4 / 62.071.1 / 63.073.5 / 65.6.
84.7 / 79.683.3 / 77.583.9 / 78.385.1 / 79.7.
88.8 / 83.887.8 / 82.688.0 / 82.888.5 / 83.4.
91.9 / 88.790.8 / 87.091.8 / 88.592.8 / 90.1.
85.9 / 79.384.8 / 77.485.4 / 78.185.1 / 78.3.
89.2 / 85.787.8 / 83.988.3 / 84.388.9 / 85.2.
74.5 / 67.472.4 / 64.173.0 / 64.775.7 / 68.1.
86.0 / 81.084.7 / 79.085.3 / 79.786.2 / 81.0.
97.297.397.397.3.
96.095.896.396.9.
93.393.593.693.7.
95.895.996.096.3.
95.595.596.096.4.
95.695.695.896.1.
96.896.896.896.5.
96.295.796.697.3.
93.593.493.493.5.
96.096.296.296.5.
95.795.795.996.3.
95.695.695.896.0.table 10: full results - performance on the differenttasks leveraging mbert with different adapter compo-nents (see §4.4)..(a) we consider all languages and models..(b) for the proportion of continued words and the fertility, weconsider fully ﬁne-tuned mbert, the monomodel-* mod-els, and the mbertmodel-* models.
for the pretrainingcorpus size, we consider the original monolingual models andthe monomodel-monotok models..figure 6: spearman’s ρ correlation of a relative de-crease in the proportion of continued words (cont.
pro-portion), a relative decrease in fertility, and a rela-tive increase in pretraining corpus size with a relativeincrease in downstream performance over fully ﬁne-tuned mbert..lg model.
ar.
en.
fi.
id.
ja.
ko.
ru.
tr.
zh.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
monolingualmbert.
devf1.
91.590.3.
95.495.7.
93.390.9.
90.993.7.
72.173.4.
88.687.3.
91.990.2.
93.193.7.
77.076.0.
88.287.9.qadevem / f1.
udp.
postestdev testuas / las uas / las acc acc.
dev.
ner.
sa.
test dev testf1acc acc.
91.190.0.
91.591.2.
96.195.8.
91.690.1.
95.995.4.
91.689.8.
68.3 / 82.466.1 / 80.6.
89.4 / 85.087.8 / 83.0.
80.5 / 88.080.9 / 88.4.
92.6 / 90.392.1 / 89.6.
92.0 —– —–88.2 —– —–.
69.9 / 81.666.6 / 77.6.
95.7 / 93.991.1 / 88.0.
96.091.4.
66.8 / 78.171.2 / 82.1.
84.5 / 77.485.0 / 78.4.
88.0 —– / —–87.8 —– / —–.
96.0 / 94.795.5 / 94.2.
74.2 / 91.169.7 / 89.5.
88.5 / 85.086.9 / 83.2.
64.3 / 83.763.3 / 82.6.
92.4 / 90.191.5 / 88.8.
60.6 / 78.157.9 / 76.4.
78.0 / 70.972.6 / 65.2.
82.3 / 89.382.0 / 89.3.
88.1 / 84.987.1 / 83.7.
91.093.5.
72.473.4.
88.886.6.
91.090.0.
92.893.8.
76.576.1.
87.487.0.
94.693.1.
88.788.8.
89.886.7.
95.295.2.
89.386.4.
94.893.1.
92.591.2.
89.786.7.
95.295.0.
88.886.4.
95.393.8.
92.491.0.
90.1 / 85.688.8 / 83.8.
92.1 / 89.791.6 / 89.1.
95.9 / 94.491.9 / 88.7.
85.3 / 78.185.9 / 79.3.
94.7 / 93.094.0 / 92.3.
90.3 / 87.289.2 / 85.7.
93.1 / 89.991.9 / 88.5.
79.8 / 73.274.5 / 67.4.
88.6 / 85.688.1 / 85.0.
97.597.2.
97.197.0.
98.196.0.
92.093.3.
98.398.1.
96.495.8.
98.698.4.
97.095.5.
96.696.1.
96.996.4.
96.896.8.
97.096.9.
98.496.2.
92.193.5.
98.197.8.
97.096.0.
98.498.2.
96.995.7.
97.296.7.
96.996.4.avg.
monolingualmbert.
70.8 / 84.069.7 / 83.3.
89.5 / 85.887.7 / 83.8.
90.0 / 86.388.4 / 84.4.table 8: full results - performance on named entityrecognition (ner), sentiment analysis (sa), ques-tion answering (qa), universal dependency parsing(udp), and part-of-speech tagging (pos).
we use de-velopment (dev) sets only for qa.
finnish (fi) sa andjapanese (ja) qa lack respective datasets..3135allnerposqasaudptaskcont.proportionfertilitypre-trainsizemetric0.390.280.300.610.410.480.420.310.340.640.400.520.200.200.130.320.210.23−101allnerposqasaudptaskcont.proportionfertilitypre-trainsizemetric0.340.200.180.670.320.450.350.230.200.650.330.480.230.060.270.340.310.43−101