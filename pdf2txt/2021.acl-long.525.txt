learning latent structures for cross action phrase relations in wet labprotocols.
chaitanya kulkarni, jany chan, eric fosler-lussier, raghu machirajudepartment of computer science and engineeringohio state university{kulkarni.132,chan.206,fosler-lussier.1,machiraju.1}@osu.edu.
abstract.
wet laboratory protocols (wlps) are criticalfor conveying reproducible procedures in bi-ological research.
they are composed of in-structions written in natural language describ-ing the step-wise processing of materials byspeciﬁc actions.
this process ﬂow descrip-tion for reagents and materials synthesis inwlps can be captured by material state trans-fer graphs (mstgs), which encode global tem-poral and causal relationships between actions.
here, we propose methods to automaticallygenerate a mstg for a given protocol by ex-tracting all action relationships across multiplesentences.
we also note that previous corporaand methods focused primarily on local intra-sentence relationships between actions and en-tities and did not address two critical issues:(i) resolution of implicit arguments and (ii) es-tablishing long-range dependencies across sen-tences.
we propose a new model that incre-mentally learns latent structures and is bettersuited to resolving inter-sentence relations andimplicit arguments.
this model draws upona new corpus wlp-mstg which was createdby extending annotations in the wlp corporafor inter-sentence relations and implicit argu-ments.
our model achieves an f1 score of54.53% for temporal and causal relations inprotocols from our corpus, which is a signiﬁ-cant improvement over previous models - dy-gie++:28.17%; spert:27.81%.
we make ourannotated wlp-mstg corpus available to theresearch community.
1.
1.introduction.
wet laboratory protocols (wlps) play an integralrole in bioscience and biomedical research by serv-ing as a vehicle to communicate experimental in-structions that allow for standardization and repli-cation of experiments.
these procedures, typicallywritten in natural language, prescribe actions (fig-ure 1) to be conducted on materials that generally.
1the dataset and code is available on the authors’ websites.
isolation of temperate phages by plaque agar overlay1. grow the bacteria overnight.
2.
...3.
...4. remove one tube of soft agar from the water bath.
5. add 1.0 ml host culture and either 1.0 or 0.1 ml viralconcentrate..figure 1: extraction of a mstg from an example wlp.
the mstg is composed of action graphs (in grey),connected by temporal and causal relationships (e.g.,temporal relation ”site” between remove and add).
the arrow indicate the direction of material ﬂow..produce new materials which, in turn, are used byfuture actions to make newer materials.
however,wlps can be unclear, composed of disconnectedand distant parts, and built upon implicit informa-tion that were referenced earlier or omitted entirely.
lack of careful documentation has led to a repro-ducibility crisis (baker, 2016) in the biosciencesand also poses considerable challenges for automa-tion of laboratory procedures: gleaning the effectand semantics of actions requires understandingthe underlying experiment, the sentence structureand rationale behind implicitly stated arguments..currently, there is a dearth of annotated re-sources for natural language instructions in lab-oratory protocols.
the wlp corpus initially col-lected by kulkarni et al.
(2018) and later updatedby tabassum et al.
(2020) focused solely on rela-tions within sentences.
however, actions in wlpsare more complex, containing additional relationsbetween actions (e.g., temporal and causal rela-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6737–6750august1–6,2021.©2021associationforcomputationallinguistics6737tions).
we propose using material state transfergraphs (mstg), which are a natural extension ofaction graphs (kulkarni et al., 2018).
mstgslink together several action graphs into a largerstructure by utilizing global temporal and causalrelationships that can span several sentences in or-der to describe the ﬂow of materials from actionto action (section 3).
an example of a mstg isshown in figure 1. the action phrase grow the bac-teria overnight in step 1 consists of an action growthat acts-on the reagent bacteria for an amount oftime speciﬁed as overnight.
this action graph isthen connected to other such graphs (like in step5) through temporal and causal relationships (e.g.,grow action’s product is host culture thus we usea product link to establish a temporal relation be-tween step 1 and step 5)..to automate the generation of mstgs, wemust overcome two distinct challenges prevalentin wlps.
first, the result of a preceding step maynot be immediately used by the next step, result-ing in long-range dependencies.
second, an actionmay involve implicit information, which is eithermentioned earlier or omitted entirely.
current mod-els usually fail to make accurate predictions forlong-range relations, as seen in figure 1 when es-tablishing a temporal relation between step 1 andstep 5. these methods rely on relation propagation(dygie++ wadden et al.
(2019)) or use contextualembeddings (spert eberts and ulges (2019)).
fur-thermore, neither successfully establish complexrelations involving implicit arguments.
in step5, the host culture and viral concentrate must beadded to the tube containing soft agar that was re-moved in step 4. however, the location tube instep 5 is implicit and has to be correctly inferredto make the site relation between remove and add..we propose a novel and effective neural net-work model that: (i): uses a series of relationalconvolutions to learn from relations within andacross multiple action phrases and (ii): iterativelyenriches entity representations with learned latentstructures using a multi-head r-gcn model.
ourmodel achieves an f1 score of 54.5% for temporaland causal relations, signiﬁcantly improving uponprevious methods dygie++ and spert for suchlong-range relations by 26.4% and 26.7% respec-tively.
we analyze our model for intra- and inter-sentence relation extraction and show substantialimprovements.
further, we also show the model’sability in resolving implicit arguments to improve.
temporal relation extraction over the best baselinemethod by 23.3%..this paper is organized around two main con-tributions: (i): the wlp-mstg corpus that ex-tends the wlp corpus (kulkarni et al., 2018) byincluding intra- and cross-sentence temporal andcausal relationships and (ii): a novel model thatbuilds upon latent structures to resolve implicitarguments and long-range relations spanning mul-tiple sentences.
in section 2, we describe relatedworks and in section 3, we introduce mstgs high-lighting the two challenges.
next, we describe ourproposed model in section 4 and demonstrate itsperformance in section 5..2 related work.
temporal and causal relation extraction:prior efforts have shown great promise in learn-ing local and global features (leeuwenberg andmoens, 2017; ning et al., 2017).
neural-network-based methods have proven effective (meng et al.,2017; meng and rumshisky, 2018).
notably, hanet al.
(2019) use neural support vector machinewhich can be difﬁcult to train.
early methods forextracting causal relations resorted to feature engi-neering (bethard and martin, 2008; yang and mao,2014).
recently several researchers (zeng et al.,2014; nguyen and grishman, 2015; santos et al.,2015) used convolutional neural networks (cnns)for extracting causal features.
notably, li and mao(2019) addressed scarcity of training data thoroughknowledge-based cnn.
however, such methodsare not scalable to multiple sentences..cross sentence relation extraction: longrange relations are understudied in literature.
priorwork focused on relations within a sentence or atbest between pairs of sentences (peng et al., 2017;lee et al., 2018; song et al., 2018; guo et al., 2019).
in addition to joint entity and relation extractionmodels, wadden et al.
(2019) proposed a modelthat passes useful information across graphs overcross-sentence contexts while eberts and ulges(2019) encoded per sentence contextual informa-tion for relation extraction over longer sentences..implicit arguments: early methods selectedspeciﬁc features to build linear classiﬁers (ger-ber and chai, 2010, 2012).
others incorporatedadditional, manually-constructed resources likenamed entity taggers and wordnet (gerber andchai, 2012; laparra and rigau, 2013; fellbaum,.
67382012).
in contrast, a few notable studies used un-labeled training data to resolve implicit arguments(chiarcos and schenk, 2015; schenk et al., 2016).
finally, do et al.
(2017) explored the full proba-bility space of semantic arguments; however, themethod does not scale well..3 task formulation: material state.
transfer graph.
data split.
#docs.
#entities.
#iap.
#cap-tac.
traindevtest.
total.
38799128.
615.
34,35513,71316,869.
32,58512,57815,679.
64,937.
60,842.
5,0492,2092,724.
9,982.table 1: statistics of the wet lab protocol-materialstate transfer graph corpus extended with cross ac-tion phrase temporal and causal relationships..a.
(i-i) dilute 5x buffer to 1x (???).
store (???)
in a tube.
prod(i-e) dilute 5x buffer to 1x (???).
store the sol in a tube..(e-i) dilute 5x buffer to 1x sol .
store (???)
in a tube..(e-e) dilute 5x buffer to 1x sol.
store the sol in a tube..a.coref.
(a) four different cases of implicit arguments.
etc.)
represented as directed edges, which we shallrefer to as inter-action phrase (iap) relations here-after.
(ii) temporal relations: inspired from priorwork (allen, 1984), we deﬁne temporality as a re-lationship between two action phrases such that anaction’s product (output) is connected to anotheraction’s source (input), thereby imposing a partialor total order.
it is also necessary to determinewhether an action is executed before or simultane-ously with respect to other actions.
we use 5 tem-poral relations, (namely ”acts-on”, ”site”, ”coref-erence”, ”product”, and ”overlaps”) to capture theﬂow of materials.
(iii) causal relations: following(barbey and wolff, 2007), we deﬁne causality asthe relationship between two actions where one ac-tion directly affects the execution of another action(e.g., if a given action enables or prevents2 anotheraction).
(iv) implicit arguments: we characterizeimplicit arguments into four cases (figure 2a) de-pending on whether the source or product of theconnected actions is implicit or explicit.
four of theﬁve temporal relations in wlp-mstg are deﬁnedto handle implicit arguments: ”acts-on”, ”site”,”coreference”, and ”product”..3.1 corpus for cross sentence relations.
annotation process: we annotate six-hundred-and-ﬁfteen (615) protocols derived from the wlpcorpus to include the 6 global cross-action phrasetemporal and causal (cap-tac) relationships.
wesplit the annotation task into two phases.
in theﬁrst phase, we worked with 7 expert annotatorsto develop the guidelines over 8 iterations.
eachiteration consisted of 10 protocols that were indi-vidually annotated by each expert annotator, andthe inter-annotator agreement (iaa) was measuredfor each of the 10 protocols.
at the end of each iter-.
2due to the limited instances of ”prevents” relations foundin wlps, we replace these with the relation ”enables”.
e.g.,mix regents carefully to not spill contents, implies a ”prevents”relation from mix to spill which is equivalent to an ”enables”relation from mix to not spill..(b) distribution of implicit arguments.
(a) classiﬁ-figure 2: implicit arguments in wlps.
cation of implicit arguments into four cases.
using thesame two actions, we denote the presence of implicit ar-guments by ”(???
)” for clarity.
(i-i): both product andsource are implicit.
(i-e): only the product is implied.
(e-i): the source is implied.
(e-e): both product andsource are explicit.
(b) distribution of relations thatcapture a speciﬁc category of implicit arguments.
(n =90 wlps).
to construct a mstg from an input protocol,we deﬁne the following four concepts.
(i) actiongraphs: introduced by kulkarni et al.
(2018), theyare extracted from action phrases as seen in fig-ure 1. forming the fundamental unit of a mstg,action graphs are composed of an action, 17types of named entities as explicit arguments (e.g,”reagent”, ”location”, etc.
), and 13 local seman-tic relations (e.g., ”using”, ”measure”, ”acts-on”,.
6739123456+distance in #sentences(i-i)(i-e)(e-i)(e-e)temporal relation sub-groups93.14.31.40.00.01.279.612.82.81.50.23.276.012.012.00.00.00.036.59.08.37.14.534.6020406080% #cap temporal relations in each sub-groupstatistics.
wlp-mstg w-nut 2020 wlpc.
# docs# entities# relations# rels/doc# cap-tac.
61564,93770,824115.169,982.
61580,65954,21288.14-.
62260,72142,42568.20-.
arguments; otherwise, they are relatively fartherapart figure 2b.
this analysis provides valuable in-sight about the challenges in the form of long-rangerelations and implicit arguments that are present inextracting mstgs from wlps..table 2: comparison of existing wlp corpora.
thewlp-mstg corpus expands relation coverage by in-cluding temporal and causal relationships..4 a latent structure model for jointentity and relation extraction.
ation, we reﬁned the set of rules to reduce the guide-lines’ ambiguity.
the agreement measured acrossall annotators using krippendorff’s alpha (krip-pendorff, 2004) on the last iteration was 78.23%.
with a good iaa attained, we began the secondphase to collect the train, dev, and test datasets.
to ensure the highest quality of the test data, weemployed all 7 annotators to work on the same128 protocols and merged the resulting annotationsbased on majority voting.
in contrast, individual an-notators collected the train and dev sets separatelyto speed up the annotation process.
a typical proto-col of 30 steps required 25 minutes on average foran annotator to identify all the cap-tac relations..comparison with previous corpora: our cor-pus, wlp-mstg, extends the wlp corpus (kulka-rni et al., 2018) which was later updated for awnut 2020 shared task (tabassum et al., 2020).
wnut 2020 was primarily designed to facili-tate supervised named entity taggers and within-sentence relation extraction methods.
we extendthe 615 protocols therein to include intra- and inter-sentence temporal and causal relations.
to ensurea fully connected graph, we exclude entities andrelations annotated for spurious descriptive sen-tences that do not prescribe any actions (e.g., title,notations, deﬁnitions, etc.).
table 2 provides acomparison of statistics among the three corpora..analysis: we conducted a distribution analysisof 90 protocols that would typically serve as thedev set for machine learning models.
actions con-nected by temporal and causal relations tend to beconsecutive (78.4%); however, a non-trivial num-ber are considerably spaced apart (21.6%) with1.08% of the total at least 8 actions apart.
forimplicit arguments, we observed: (i) implicit argu-ments are unusually prevalent in wlps (88.44%),(ii) a higher percentage (55.98%) of the productsof an action are implied, and (iii) temporally con-nected actions are closer if they contain implicit.
we develop a latent structure model for jointlylearning entity and relations within and across mul-tiple sentences.
a schematic of the model is shownin figure 3. in section 4.1 we describe constructionof span representation (figure 3a) from protocoltext that incorporates critical features necessary forlong-range relation extraction.
section 4.2 explainshow the transcoder block (figure 3b) builds uponlatent structures (as illustrated in figure 3d) to im-prove entity and relation representations.
finally, insection 4.3 we discuss training and regularizationstrategies to jointly learn span, entity, and relationsthrough a multi-task loss function derived fromspan, entity, and relation scores (figure 3c).
weshall use figure 1 as a running example throughoutthe model description..4.1 span representation.
following prior span-based approaches (waddenet al., 2019; eberts and ulges, 2019), our goal isto (i): collect a series of tokens from the protocoltext, (ii): enumerate all spans, and (iii): rank top-scoring spans for considerations as candidates forentity and relation extraction..token embeddings: we use scibert (beltagyet al., 2019) for learning token representations fora given protocol p. as shown in figure 3, theinput is a protocol p represented as a collectionof sentences s = {s1, ..., sp }.
each sentence siis composed of a sequence of tokens {t1, ..., tn}.
for example, within the sentence, add 1.0 ml hostculture and either 1.0 or 0.1 ml viral concentrate(figure 1, step 5), we identify host, culture, andetc., as the tokens to be passed to the scibertmodel.
we batch process sentences in the protocolto generate context-aware embeddings {t1, ..., tn}for each sentence..span enumeration: the spans between twotokens ti and tjis represented as sij ={ti, ti+1, ...tj}.
we enumerate all possible spans ofupto a size of 10 tokens.
for each enumerated span,the span representation eij ∈ rde is derived from.
6740figure 3: overview of latent structure model.
the model ﬁrst builds (a) span representations (section 4.1), whichare passed into the (b) transcoder block (section 4.2) that leverages (d) latent structures to improve entity andrelation representations which are scored alongside spans in the (c) multi-task loss function (section 4.3).
asindicated in (d), the model ﬁrst learns simple structures like action graphs.
the next set of layers discovers simpletemporal and causal relations and uses these connections to discover more complex relations in the ﬁnal layers..applying a feed-forward neural network (ffnn)on a concatenation of tokens representations andembeddings:.
eij = ffnn([ti; tj; φsh(sij); φpos(sij);.
(1).
φstep(sij); φw(sij)]).
where, ti and tj are the ﬁrst and last token rep-resentation.
note, φsh(sij) is a soft head repre-sentation (bahdanau et al., 2014) and, φw(sij) isa learnt span width embedding respectively.
fur-ther, φpos(sij) and φstep(sij) are two positionalembeddings, the former for within sentence whilethe latter deﬁnes the step position within the pro-tocol respectively.
hence, host culture and hostculture and are two valid spans that are enumeratedthrough this process..span pruning: next,low scoring spans areﬁltered out during both training and evaluationphases.
following (lee et al., 2017), the scoringfunction is implemented as a feed-forward networkφs(eij) = wts ffnns(eij).
we rank and pick anumber of top scoring spans per sentence by usinga combination of (i): a maximum fraction λp = 0.1of spans per sentence, and (ii): a minimum scorethreshold λt = 0.5. thus, the span host culturereceives a signiﬁcantly higher score than host cul-ture and, indicating that the former is the correctreagent entity in the prescribed step.
these spancandidates are then passed to the transcoder block..4.2 transcoder block.
in the transcoder block, we propose a novel archi-tecture to improve relation and entity representationfrom latent structures.
the objective is two fold:(i): to leverage localized features at phrase and sen-tence levels to resolve long range relations througha relation convolutions, and (ii): to learn from la-tent structures how to resolve implicit argumentsthrough a multi-head relational graph convolutionnetwork (multi-head r-gcn)..each transcoder block is composed of a rela-tion encoder (section 4.2.1), convolution (sec-tion 4.2.2) and decoder (section 4.2.3) compo-nents, to discover local relationships between theinput entities.
these relations (represented as la-tent structures a ∈ rm×m×r) are then passed tothe multi-head r-gcn (section 4.2.4) componentof the same transcoder block to enrich the entityrepresentation with information about those discov-ered local relationships.
these enriched entitiescan now be used to predict more complex crosssentence relationships in the next transcoder block.
to facilitate deeper networks, we make use of resid-ual connections (he et al., 2016) followed by layernormalization (ba et al., 2016) as denoted by add+ norm in figure 3b..we shall make use of the example (figure 1),focusing on the long range relationships betweenstep 1 (i.e., grow the bacteria overnight.)
and step5 (i.e., add 1.0 ml host culture and either 1.0 or.
6741relationencoderadd + normrelationdecoderadd + normadd + norm2-layerconvolutionslaplaciansmoothingscibert (fine-tuned)1. grow the bacteria overnight...5. add 1.0 ml host culture and ...scibert (fine-tuned)scibert (fine-tuned)span encoderspan filtermulti-head r-gcnr-gcnconcatlinearrelationdecoderrelationscoreslinear +softmaxentityscoreslinear +softmaxspanscores(a) span representation(b) transcoder block(c) scoring functions......(d) latent structuresthrough the network transcoder transcoder transcoder ......0.1 ml viral concentrate.)
to illustrate the ﬂowof information throughout the transcoder block.
the ﬁrst transcoder block takes as input m highscoring candidate entity span representations (ase(0) ∈ rm×de) as determined by the pruner 3. forinstance, from step 1 we identify the followinghigh scoring candidate entities grow, bacteria, andovernight and from step 5 we ﬁnd add, 1.0 ml,host culture, 0.1 ml, and viral concentrate..4.2.1 relation encoder:following (nguyen and verspoor, 2019), we makeuse of a bi-afﬁne pairwise function to encode re-lations for every pair of entity span representa-tion.
that is, we generate relational embeddingsfor entity pairs like grow and bacteria, grow andovernight, etc.
each entity span eij ∈ rde is ﬁrstprojected using two ffnns to generate the rep-ij ∈ rdt indicatingresentations ehthe ﬁrst (head) and the second (tail) argument of arelation:.
ij ∈ rdh and et.
ij = ffnnh(eij); eteh.
ij = ffnnt(eij).
in practice, we batch process all entities to gener-ate eh ∈ rm×dh and et ∈ rm×dt where m is thenumber of candidate spans.
in our experiments, welet dh = dt then use a bi-afﬁne operator to calculatea tensor (cid:101)r(l) ∈ rm×dr×m for relational embed-t .
here l ∈ rdh×dr×dt isdings: (cid:101)r(l) = (ehl)eta learned parameter tensor and dr is the relationembedding size..4.2.2 relation convolutions:we enrich the relational embeddings (cid:101)r(l) with lo-cal relational features within a single phrase (foundnear the diagonal) and across multiple phrases(found in the upper and lower triangle) using astack of convolutional layers.
we denote cw(.)
tobe a 2d convolutional operator applying a kernelwidth of size w × w. in our model, we make useof a two-layer convolution:.
t(0) = relu(c3( (cid:101)r(l)))r(l) = relu(c3(t(0))).
the input (cid:101)r(l) is reshaped as rm×m×dr such thatthe dimensions dr acts as the channel dimensionin the convolutions.
the dimensions of t(0) is inrm×m×2dr with the ﬁnal output r(l) ∈ rm×m×dr ..3the entity span representation from the entire sub-protocol, (i.e., from steps 1 to 5), are passed as a bag ofentities e(0) ∈ rm×de .
however, there aren’t any relations(i.e., r(0)) to be passed to the ﬁrst transcoder block.
4.2.3 relation decoder:the relational embeddings r(l) are decoded us-ing a 2-layer ffnn.
the decoded scores a ∈rm×m×r captures the latent structures (as shown infigure 3b).
this is re-encoded using the multi-headr-gcn to strengthen the model’s ability to predictmore complex relations in the next transcoder layer..4.2.4 multi-head r-gcn:for each predicted relation score ar ∈ rm×m, weadd self loops and perform laplacian smoothing(kipf and welling, 2017; li et al., 2018) for nor-malization following: ˆar = (cid:101)d− 12 (cid:101)ar (cid:101)d− 12 where(cid:101)ar = ar + i and ˜d = (cid:80)j (cid:101)aijr.
then, usingˆar as an adjacency matrix, we learn multi-head,direction-speciﬁc graph convolution transforma-tions.
each head corresponding to a given relationr performs graph convolutions on the entity rep-resentation e(l−1) ∈ rm×de to generate e(l)r ∈rm×(dr/r).
a single r-gcn(i)r (.)
(schlichtkrullet al., 2018) operation for a given relation type rand ith gcn layer corresponds to:.
r-gcn(i).
r ( ˆar, e(i−1)r+ σ( ˆat.)
= σ( ˆare(i−1)r w(i)r e(i−1).
r w(i)f r )br ) + b(i).
r.(2).
f r ∈ rdi−1×di, w(i).
where w(i)br ∈ rdi−1×di arelearnable parameters for incoming and outgoingedge directions respectively and b(i)is the bias.
rwe use the relu activation function σ in our net-works.
as shown in figure 3b, the outputs ofthe individual r-gcn heads are concatenated andpassed through a ffnn layer to compute the ﬁnaloutput e(l)..for instance, suppose we discovered a local rela-tion in step 1 between grow and bacteria after therelation decoder component in the ﬁrst transcoderblock.
the multi-head r-gcn takes in the dis-covered relation (through the latent structure a)and enriches grow’s entity embeddings, enablingthe next transcoder layer to predict a more com-plex cross sentence relation between grow (step1) and host culture (step 5).
since bacteria andhost culture are semantically related, they have sim-ilar entity embeddings, and therefore the enrichedrepresentation of grow (now containing informa-tion about bacteria) allows for establishing the re-lation between grow and host culture in the nexttranscoder block..67424.3 training and regularization.
the loss function is a linear combination of crossentropy losses for each of the tasks.
we addition-ally apply label smoothing (szegedy et al., 2016).
the relation extraction is trained on gold entityspans.
for regularization, we apply dropout (sri-vastava et al., 2014) to the output of each ffnnlayer.
we make use of dropedge (rong et al., 2019)for the adjacency matrix ar before it is passed tothe multi-head r-gcn model..5 experiments.
in contrast to general language models, domain-speciﬁc methods have resulted in more competitivebaselines and are better suited (tabassum et al.,2020; wadden et al., 2019; eberts and ulges, 2019)for simultaneously resolving and predicting entitiesand relations over longer contexts.
thus, we eval-uate our model against two state-of-the-art mod-els for jointly predicting entities and relations inscientiﬁc-text domain, namely dygie++ (waddenet al., 2019) and spert (eberts and ulges, 2019),on the wlp-mstg..we conduct ﬁve (5) runs with random initial-izations for each evaluation and report the test setperformance on the model that achieved the me-dian relation f1 score on the dev(elopment) set.
all models are evaluated end-to-end, where themodel takes as input tokenized sentences and pre-dicts all the entities and the relations generating amstg.
we use the standard precision, recall andf1 metrics.
an entity is considered correct if itspredicted span and label match the ground truth.
relation extraction is performed on the predictedentity spans.
a relation is correct if its relation typeand the entity pairs are both correct (in span andtype) against the ground truth.
we also evaluateour model’s performance on wnut 2020 (tabas-sum et al., 2020) corpus.
to fairly evaluate relationextraction, we use gold entities to make relationpredictions4 by modifying the loss function to onlytrain on relation scores.
we additionally concate-nate entity label embeddings to the span represen-tation in equation (1)..5.1 results.
on the wlp-mstg corpus, table 3 shows our bestmodel with n = 8 transcoder block layers making.
modest improvement on entity extraction at 82.0%but improving signiﬁcantly upon the previous state-of-the-art methods (i.e.
dygie++ and spert) inpredicting relations.
our model outperforms thebaselines for relation extraction with an f1 scoreon predicting inter-action phrase (iap) relationsat 68.0% and cross-action phrase temporal andcausal (cap-tac) relations at 54.5%.
we furtherenhanced the performance of our model by sharingthe relational decoders’ parameters across all layersof the transcoder block (section 4.2.3).
this en-ables the latent structures to be grounded in outputrelation types, which also lends itself to be inter-pretable.
the shared relation decoder marginallyoutperforms the not-shared conﬁguration by 0.5%for iap relations and 1.1% for cap-tac relations..short and long range relations: on thewnut 2020 corpus, which only includes intra-sentence relations, table 4 shows that our modeloutperforms the best single model that used theoriginal data by 1.0%.
we also report that ourmodel is competitive against the ensemble ap-proach that included models trained on an alteredversion of the original corpus where they removedduplicate text after clustering.
on the wlp-mstgcorpus, we can evaluate both short and long rangerelations: from table 3 we see a 3.5% improvementin f1 score over dygie++ for iap relations.
thisshows that our model leverages the cross-sentencetemporal and causal relations that were addition-ally annotated in wlp-mstg to improve localiap relations.
our model outperforms dygie++and spert on intra-sentence by 4.3% and 26.1%respectively, and signiﬁcantly improves for inter-sentence cap-tac relations by 45.5% and 21.5%respectively.
this is attributed to positional em-beddings along with the relational convolutionswhich enables the model to learn intra and interaction phrase relations effectively.
we see spertperforming better for ”overlaps” which is largelyattributed to the ’cls’ token that spert embedsto make relation predictions.
figure 4 shows per-formance on varying the number of sentences inbetween entities involved in a relation.
we observeour model performing the best for all distancesbetween sentences.
this is once again attributedto the relational convolution component which iseffective in capturing far away relations..4the best models on wnut2020 make direct use of goldentities during the training and inference and only focus onrelation extraction task..temporal and implicit arguments:in table 6we show our model outperforming the baselines for.
6743action + entities.
iap relations (85.2%).
cap-tac relations (14.8%).
models.
dygie++ (wadden et al., 2019)spert (eberts and ulges, 2019).
our model (no-sharing)our model (shared decoder).
p.85.076.4.
82.982.8.r.78.583.1.
81.281.3.f1.
81.679.6.
82.182.0.p.66.134.3.
66.967.9.r.62.959.0.
68.268.2.f1.
64.543.4.
67.568.0.p.61.520.1.
60.157.8.r.18.245.1.
48.151.5.f1.
28.127.8.
53.454.5.table 3: micro f1 scores for actions + entities and relation extraction (split into iap and cap-tac relations) onthe wlp-mstg test set..models.
miller and vosoughi (2020)single (sohrab et al., 2020)ensemble (sohrab et al., 2020).
our model (single).
p.45.480.380.8.
80.4.r.86.577.480.1.
79.3.f1.
59.678.980.5.
79.9.table 4: micro f1 scores for relation extraction onwnut 2020 shared task based on gold entities..spertdygie++our model.
60.
40.
20.serocs.1fgva-orci.m.0.
0.
1.
2.
3.
#sentences in between entities.
figure 4: micro f1 scores for cap-tac relation extrac-tion on the test set split by the distance between headand tail entities as measured by number of sentences..temporal relations at 53.4% f1 score.
we also ob-serve signiﬁcant improvements across the board forresolving implicit arguments.
we see the highestgains (at 55.6%) compared to the baseline mod-els (1.6% for dygie++ and 10.2% for spert) for(e-i) case (figure 2a) which only contains 169 sam-ples in the test set.
our model is able to correctlyresolve the implicit source (input) to an action byutilizing simple relations that is typically connectedto explicit arguments..causal relations: the performance for causalrelations for our model against dygie++ is com-parable as seen in table 6. causal relations arerelatively easier for the baseline models to capture,as they tend to have speciﬁc prepositions in be-.
cap-tac relations.
dygie++.
spert.
ours.
acts-onsitecoreference-linkproductenablesoverlaps.
micro f1.
acts-onsitecoreference-linkproductenablesoverlaps.
micro f1.
62.830.16.651.762.314.7.
52.6.
13.75.41.96.80.00.0.
7.4.
25.422.88.845.048.525.4.
30.8.
35.745.64.634.40.02.7.
31.4.
66.949.323.659.561.929.1.
56.9.
65.458.214.256.20.01.7.
52.9.
(a) intra-sentence cap-tac relations.
cap-tac relations.
dygie++.
spert.
ours.
(b) inter-sentence cap-tac relations.
table 5: cap-tac relation extraction performance onthe test set, split into (a) intra- and (b) inter-sentencerelations and presented as per class and micro averagedf1 scores.
bold indicates best performance per row..tween action phrases.5 however, more complexcausal relations are hard.
still, our model is ableto deal with such examples, presenting about 0.7%performance gain compared to dygie++ and about10.9% improvement against spert.
this is pri-marily attributed to the multi-head r-gcn whichbuilds upon simple relations that provide clues toestablish harder causal relations.
cross-sentential’enables’ relations (as seen in table 5) are chal-lenging even for our model as once again we donot encode any contextual features..model ablation: table 7 presents the results ofthe ablation test of our model on the developmentset of wlp-mstg.
all three components (i.e.,positional embeddings, relation convolutions and.
5for instance, in step resuspend by vortexing the pelletsbaseline models can easily identify an ”enables” relation fromvortexing to resuspend with the help of the preposition ’by’..6744groups.
dygie++.
spert.
ours.
temporal (5034)- (i-i) (1608)- (i-e) (2331)- (e-i) (169)- (e-e) (546).
causal (608).
23.441.515.51.63.1.
55.5.
30.133.035.710.25.7.
45.3.
53.456.467.555.631.0.
56.2.table 6: micro f1 scores for cap-tac relations splitinto temporal (and subgroups) and causal relations onwlp-mstg test set.
refer to figure 2a for acronymdeﬁnitions.
bold indicates best performance per row..n=2n=4n=6n=8.
serocs.1fgva-orci.m.50.
40.
30.
20.
10.
0.
0.
1.
2.
3.distance in #sentences.
figure 5: micro f1 scores for cap-tac relation ex-traction on the dev set for different number (n ) oftranscoder blocks as illustrated in figure 3..multi-head r-gcn) play a signiﬁcant role in im-proving cap-tac performance.
relation convo-lutions contributes the most to iap and cap-tacrelations by about 1.2% and 2.4% respectively.
po-sitional embeddings impacts iap relations more (by1.1%) whereas multi-head r-gcn only impactsthe more complex relations (captac by 1.1%) anddoes not help in improving simpler relations..how many layers?
: figure 5 shows that morelayers generally improve far away relations withoutimproving closer ones.
this shows that althoughour model can build upon simple relations that aretypically close by, it cannot do the opposite, i.e.,.
model.
final model- pos + step embedding- relation convolutions- multi-head r-gcn- all above.
all.
59.558.358.059.246.3.iap.
64.363.263.164.352.3.cap.
47.546.645.146.426.0.table 7: ablation test of proposed latent structuremodel evaluated on wlp-mstg dev set.
we presentmicro f1 scores for both iap and cap-tac relation ex-traction..leverage far away relations (which are typicallymore complicated) to improve more challengingcloser relations.
our model discovers those com-plex, distant relations too deep into the network tobe utilized to predict the challenging local relations..6 conclusions and future work.
we present the wlp-mstg corpus, an extensionof the wlp corpus that includes cap-tac relation-ships for building mstgs.
this corpus highlightstwo unique challenges: (i) the implicit argumentproblem and (ii) long-range relations.
to addressthese issues, our model builds upon latent struc-tures thus outperforming previous state-of-the-artmodels for predicting iap and cap-tac relations.
we also report signiﬁcant improvements in under-standing implicit arguments and identifying longrange relationships across multiple sentences.
how-ever, our model’s lower absolute performance in-dicates that we have not fully captured the infor-mation needed to facilitate modeling end-to-endworkﬂows, which will have a lasting impact in im-proving automation in the life sciences and otherdomains..references.
james f allen.
1984. towards a general theory ofaction and time.
artiﬁcial intelligence, 23(2):123–154..jimmy lei ba, jamie ryan kiros, and geoffrey e hin-arxiv preprint.
ton.
2016. layer normalization.
arxiv:1607.06450..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..monya baker.
2016. reproducibility crisis.
nature,.
533(26):353–66..aron k barbey and philip wolff.
2007. learningcausal structure from reasoning.
in proceedings ofthe annual meeting of the cognitive science society,volume 29..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3606–3611..steven bethard and james h martin.
2008. learningsemantic links from a corpus of parallel temporal.
6745and causal relations.
hlt, short papers, pages 177–180..in proceedings of acl-08:.
linguistics: human language technologies, vol-ume 2 (short papers), pages 97–106..christian chiarcos and niko schenk.
2015. a min-imalist approach to shallow discourse parsing andimplicit relation recognition.
in proceedings of thenineteenth conference on computational naturallanguage learning-shared task, pages 42–49..egoitz laparra and german rigau.
2013..impar: adeterministic algorithm for implicit semantic role la-in proceedings of the 51st annual meet-belling.
ing of the association for computational linguistics(volume 1: long papers), pages 1180–1189..quynh ngoc thi do,.
steven bethard,.
andimproving im-marie francine moens.
2017.plicit semantic role labeling by predicting semanticframe arguments.
in proceedings of the eighth in-ternational joint conference on natural languageprocessing (volume 1: long papers), pages 90–99..kenton lee, luheng he, mike lewis, and luke zettle-moyer.
2017. end-to-end neural coreference reso-in proceedings of the 2017 conference onlution.
empirical methods in natural language processing,pages 188–197..markus eberts and adrian ulges.
2019. span-basedjoint entity and relation extraction with transformerpre-training.
24th european conference on artiﬁ-cial intelligence..christiane fellbaum.
2012. wordnet.
the encyclope-.
dia of applied linguistics..kenton lee, luheng he, and luke zettlemoyer.
2018.higher-order coreference resolution with coarse-to-ﬁne inference.
in proceedings of the 2018 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 2 (short papers), pages687–692..matthew gerber and joyce chai.
2010. beyond nom-bank: a study of implicit arguments for nominalpredicates.
in proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 1583–1592..matthew gerber and joyce y chai.
2012. semanticrole labeling of implicit arguments for nominal pred-icates.
computational linguistics, 38(4):755–798..zhijiang guo, yan zhang, and wei lu.
2019. atten-tion guided graph convolutional networks for rela-tion extraction.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 241–251..rujun han, qiang ning, and nanyun peng.
2019. jointevent and temporal relation extraction with sharedin pro-representations and structured prediction.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 434–444..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..thomas n kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
iclr..klaus krippendorff.
2004. content analysis: an intro-duction to its methodology.
sage publications..chaitanya kulkarni, wei xu, alan ritter, and raghumachiraju.
2018. an annotated corpus for machinereading of instructions in wet lab protocols.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computational.
artuur leeuwenberg and marie francine moens.
2017.structured learning for temporal relation extractionin proceedings of the 15thfrom clinical records.
conference of the european chapter of the associa-tion for computational linguistics: volume 1, longpapers, pages 1150–1158..pengfei li and kezhi mao.
2019. knowledge-orientedconvolutional neural network for causal relation ex-traction from natural language texts.
expert systemswith applications, 115:512–523..qimai li, zhichao han, and xiao-ming wu.
2018.deeper insights into graph convolutional networksin proceedings offor semi-supervised learning.
the aaai conference on artiﬁcial intelligence, vol-ume 32..yuanliang meng and anna rumshisky.
2018. context-aware neural model for temporal information extrac-tion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 527–536..yuanliang meng, anna rumshisky, and alexey ro-manov.
2017. temporal information extraction forquestion answering using syntactic dependencies inin proceedings of thean lstm-based architecture.
2017 conference on empirical methods in naturallanguage processing, pages 887–896..chris miller and soroush vosoughi.
2020. big green atwnut 2020 shared task-1: relation extraction as con-textualized sequence classiﬁcation.
arxiv preprintarxiv:2012.04538..dat quoc nguyen and karin verspoor.
2019. end-to-end neural relation extraction using deep biafﬁne at-tention.
in european conference on information re-trieval, pages 729–738.
springer..6746thien huu nguyen and ralph grishman.
2015. rela-tion extraction: perspective from convolutional neu-ral networks.
in proceedings of the 1st workshop onvector space modeling for natural language pro-cessing, pages 39–48..jeniya tabassum, wei xu, and alan ritter.
2020.wnut-2020 task 1 overview: extracting entities andrelations from wet lab protocols.
in proceedings ofthe sixth workshop on noisy user-generated text(w-nut 2020), pages 260–267..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5788–5793..xuefeng yang and kezhi mao.
2014. multi levelcausal relation identiﬁcation using extended features.
expert systems with applications, 41(16):7171–7181..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiﬁcation via con-volutional deep neural network.
in proceedings ofcoling 2014, the 25th international conferenceon computational linguistics: technical papers,pages 2335–2344..qiang ning, zhili feng, and dan roth.
2017. a struc-tured learning approach to temporal relation extrac-in proceedings of the 2017 conference ontion.
empirical methods in natural language processing,pages 1027–1037..nanyun peng, hoifung poon, chris quirk, kristinatoutanova, and wen-tau yih.
2017. cross-sentencen-ary relation extraction with graph lstms.
transac-tions of the association for computational linguis-tics, 5:101–115..yu rong, wenbing huang, tingyang xu, and junzhouhuang.
2019. dropedge: towards deep graph con-volutional networks on node classiﬁcation.
in inter-national conference on learning representations..cicero nogueira dos santos, bing xiang, and bowenzhou.
2015. classifying relations by ranking witharxiv preprintconvolutional neural networks.
arxiv:1504.06580..niko schenk, christian chiarcos, kathrin donandt,samuel r¨onnqvist, evgeny stepanov, and giuseppericcardi.
2016. do we really need all those rich lin-guistic features?
a neural network-based approachin proceedings of theto implicit sense labeling.
conll-16 shared task, pages 41–49..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convo-lutional networks.
in 15th international conferenceon extended semantic web conference, eswc 2018,pages 593–607.
springer/verlag..mohammad golam sohrab, anh-khoa duong nguyen,makoto miwa, and hiroya takamura.
2020. mg-sohrab at wnut 2020 shared task-1: neural exhaus-tive approach for entity and relation recognition overwet lab protocols.
in proceedings of the sixth work-shop on noisy user-generated text (w-nut 2020),pages 290–298..linfeng song, yue zhang, zhiguo wang, and danielgildea.
2018. n-ary relation extraction using graph-state lstm.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 2226–2235..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..67477 appendix.
7.1 material state transfer graph example.
we describe a full material state transfer graph (asseen in figure 7) designed for the protocol in fig-ure 6. each action phrase found in the protocol textis converted into an action graph (as seen in greyboxes in figure 7).
for example, the action phrase:grow the bacteria overnight, we identify an ”ac-tion” grow and all of its arguments like ”reagent”bacteria and ”time” overnight.
these actions andentities are interconnected with local relations thatwe call iap (inter-action phrase) relations.
forinstance, relations like ”acts-on” between growto bacteria and ”setting” from grow to overnightto indicate that is how long we should be grow-ing the bacteria.
then, the action phrases as actiongraphs are interconnected with cross-action phrasetemporal and causal (cap-tac) relations.
theserelations can connect to any action or entity in theaction graph as seen in figure 7. for example, the”product” relation from grow to host culture in-dicates two things, (i) that the actual product ofthe grow ”action” is host culture and (ii) the stepsinvolving grow must take place ﬁrst before the”action” add.
we carefully deﬁne each relationused as cap-tac relations below:.
7.2 temporal relations.
the following four relations behave as ”before”temporal relations (ie ”acts-on”, ”site”, ”product”,”coreference”).
whereas the ﬁfth relation ”over-laps” is used when any two actions have any degreeof overlap in time.
the four relations also deﬁnewhich implicit argument relation group do they fallunder.
the four groups are (i)(i-i) as in both theproduct and the sources are implicit.
(ii)(e-i) theproduct is explicit, but the source is implied.
(iii)(i-e) the product is implied but the source is ex-plicit.
and, (iv) (e-e) both the source and productare explicit..acts-on: connects to a previous ”action” if theproduct of that ”action” is implicit.
otherwisedirectly connects to the named entity (which canbe a ”reagent”, ”location”, ”device” etc) thatthe previous action has a ”product” relation to.
if directly connected to an ”action”, this wouldfall under (i-i) case of implicit argument types.
ifconnected to any named entity which is a productof the previous action then it would fall under (e-i)case..isolation of temperate phages by plaque agar overlay.
1. grow the bacteria overnight.
2. melt soft agar overlay tubes in boiling water.
3. place in the 47c water bath.
4. remove one tube of soft agar from the water bath.
5. add 1.0 ml host culture and either 1.0 or 0.1 ml viral concentrate to.
the tube..6. mix the culture contents in the tube well by rolling back and forth.
between two hands.
immediately empty the tube contents onto an agar plate..7.
8. sit rt for 5 min.
9. gently spread the top agar over the agar surface by sliding the plate on.
the bench surface using a circular motion..10. harden the top agar by not disturbing the plates for 30 min.
11.incubate the plates (top agar side down) overnight to 48 h.12. temperate phage plaques will appear as turbid or cloudy plaques,whereas purely lytic phage will appear as sharply deﬁned, clearplaques..figure 6: an example experimental protocol.
the ﬁrst11 steps contain imperative phrases, while the last sen-tence describes the end results and their subsequent uti-lization.
a full material state transfer graph for thisprotocol is shown in figure 7.site: similar to ”acts-on”, this relation links tothe previous ”action” if the product of that ”ac-tion” is implicit, and that product is where thecurrent ”action” is taking place.
otherwise wedirectly connect to the appropriate named entity.
once again, similar to ”acts-on”, if directly con-nected to ”action”, it falls under (i-i) case, other-wise its (e-i) case..product: this relation is used to identify theproduct of the current ”action”, either its foundin its own action phrase or in some future actionphrase.
if the product is identiﬁed within its actionphrase (which is quite rare) it would be consideredan iap relation.
otherwise, this would fall under(i-e) case..coreference: this is used when the objects orarguments are in the same state.
we connect theobject to the same object referred before only ifthat object has not undergone any transformationsby any actions in between.
this relation falls under(e-e) case..overlaps: this relation is used to indicate whichtwo actions are being performed simultaneously orthat have any degree of overlap between them interms of time..7.3 causal relation.
we only make use of one causal relation type ”en-ables”.
due to low numbers on ”prevents” rela-tions, we turn them into an ”enables” relation by.
6748simply negating the ”action” involved in the re-lationship.
for example, mix regents carefully tonot spill contents, we replace a ”prevents” rela-tion from mix to spill with an ”enables” relationfrom mix to not spill.
in many elaborate negativewords we make use of ”mod-link” to connect tothe additional descriptors to the relevant action..7.4.implementation details.
in evaluating on wlp-mstg, we overcome mem-ory limitations in baseline models during trainingand inferencing, we sub-divide long protocols intooverlapping windows of 5 sentences each, with astride of 2 (i.e., each consecutive window shares3 sentences).
to ensure fair comparison we alsoincorporate this restriction to our model, althoughour models is capable of a much larger window size.
the ﬁnal evaluation is done by merging the predic-tions in the form of sub-graphs into one completematerial state transfer graph (mstg) and resolvingduplicate predictions through majority voting.
weidentify duplicates through exact match of spansboundaries for entities and exact match of entityspan and its types for relations..hyperparameters we make use of adam opti-mizer with a initial learning rate of 2.13 × 10−5.
for generating span candidates we only enumeratethem upto 10 tokens in width.
we set the positionalembedding φpos(sij) table size to 100. for step em-bedding φstep(sij) we only learn embeddings for5 steps.
both embeddings use embedding dimen-sions as 50. the span embedding size de = 340,and the relational embedding size dr is set to 100.label smoothing [symbol] is set to the default valueof 0.1. dropout used in every ffnn has p = 0.2and the dropedge used right before multi-head r-gcn model is set with p = 0.5..6749figure 7: a full material state transfer graphical representation of the example protocol in figure 6..6750