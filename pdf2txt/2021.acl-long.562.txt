beyond noise: mitigating the impact of fine-grainedsemantic divergences on neural machine translation.
eleftheria briakou and marine carpuatdepartment of computer scienceuniversity of marylandcollege park, md 20742, usaebriakou@cs.umd.edu, marine@cs.umd.edu.
abstract.
while it has been shown that neural machinetranslation (nmt) is highly sensitive to noisyparallel training samples, prior work treats alltypes of mismatches between source and tar-get as noise.
as a result, it remains unclearhow samples that are mostly equivalent butcontain a small number of semantically diver-gent tokens impact nmt training.
to close thisgap, we analyze the impact of different typesof ﬁne-grained semantic divergences on trans-former models.
we show that models trainedon synthetic divergences output degeneratedtext more frequently and are less conﬁdent intheir predictions.
based on these ﬁndings, weintroduce a divergent-aware nmt frameworkthat uses factors to help nmt recover from thedegradation caused by naturally occurring di-vergences, improving both translation qualityand model calibration on en↔fr tasks..1.introduction.
while parallel texts are essential to neural ma-chine translation (nmt), the degree of parallelismvaries widely across samples in practice, for rea-sons ranging from noise in the extraction pro-cess (roziewski and stokowiec, 2016) to non-literal translations (zhai et al., 2019b, 2020a).
forinstance (figure 1), a french source could bepaired with an exact translation into english (eq),with a mostly equivalent translation where onlya few tokens convey divergent meaning (ﬁne-div), or with a semantically unrelated, noisy ref-erence (coarse-div).
yet, prior work treats par-allel samples in a binary fashion: coarse-graineddivergences are viewed as noise to be excludedfrom training (koehn et al., 2018), whilst othersare typically regarded as gold-standard equivalenttranslations.
as a result, the impact of ﬁne-graineddivergences on nmt remains unclear..this paper aims to understand and mitigate theimpact of ﬁne-grained semantic divergences in.
figure 1: equivalent vs. divergent references onnmt training.
fine-grained divergences (i.e., ref (ﬁne-div)) provide an imperfect yet potentially useful signaldepending on the time step t..nmt.
we ﬁrst contribute an analysis of how ﬁne-grained divergences in training data affect nmtquality and conﬁdence.
starting from a set of equiv-alent english-french wikimatrix sentence pairs,we simulate divergences by gradually “corrupting”them with synthetic ﬁne-grained divergences.
fol-lowing khayrallah and koehn (2018)—who, incontrast, study the impact of noise on mt—wecontrol for different types of ﬁne-grained semanticdivergences and different ratios of equivalent vs.divergent data.
our ﬁndings indicate that these im-perfect training references: hurt translation quality(as measured by bleu and meteor) once theyoverwhelm equivalents; output degenerated textmore frequently; and increase the uncertainty ofmodels’ predictions..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7236–7249august1–6,2021.©2021associationforcomputationallinguistics7236based on these ﬁndings, we introduce adivergent-aware nmt framework that incorpo-rates information about which tokens are indicativeof semantic divergences between the source andtarget side of a training sample.
source-side diver-gence tags are integrated as feature factors (had-dow and koehn, 2012; sennrich and haddow,2016; hoang et al., 2016), while target-side di-vergence tags form an additional output sequencegenerated in a multi-task fashion (garc´ıa-mart´ınezet al., 2016, 2017).
results on en↔fr transla-tion show that our approach is a successful miti-gation strategy: it helps nmt recover from thenegative impact of ﬁne-grained divergences ontranslation quality, with fewer degenerated hypothe-ses, and more conﬁdent and better calibrated pre-dictions.
we make our code publicly available:https://github.com/elbria/xling-semdiv-nmt..2 background & motivation.
cross-lingual semantic divergences we usethis term to refer to meaning differences in alignedbilingual text (vyas et al., 2018; carpuat et al.,2017).
divergences in manual translation mightarise due to the translation process (zhai et al.,2018) and result in non-literal translations (zhaiet al., 2020a).
divergences might also arise in par-allel text extracted from multilingual comparableresources.
for instance, in wikipedia, documentsaligned across languages might contain parallelsegments that share important content, yet theyare not perfect translations of each other, yield-ing ﬁne-grained semantic divergences (smith et al.,2010).
finally coarse-grained divergences mightresult from the process of automatically miningand aligning corpora from monolingual data (fungand cheung, 2004; munteanu and marcu, 2005),or web-scale parallel text (smith et al., 2013; el-kishky et al., 2020; espl`a et al., 2019)..noise vs. semantic divergencesin the contextof mt, noise often refers to mismatches in web-crawled parallel corpora that are collected with-out guarantees about their quality.
khayrallah andkoehn (2018) deﬁne ﬁve frequent types of noisefound in the german-english paracrawl corpus:misaligned sentences, disﬂuent text, wrong lan-guage, short segments, and untranslated sentences.
they examine the impact of noise on translationquality and ﬁnd that untranslated training instancescause nmt models to copy the input sentence atinference time.
their ﬁndings motivated a shared.
task dedicated to ﬁltering noisy samples from web-crawled data at wmt, since 2018 (koehn et al.,2018, 2019, 2020).
this work moves beyond suchcoarse divergences and focuses instead on ﬁne-grained divergences that affect a small number oftokens within mostly equivalent pairs and that canbe found even in high-quality parallel corpora..training assumptions nmt models are typi-cally trained to maximize the log-likelihood ofthe training data, d ≡ {(x(n), y(n))}nn=1, where(x(n), y(n)) is the n-th sentence pair consisting ofsentences that are assumed to be translations ofeach other.
under this assumption, model parame-ters are updated to maximize the token-level cross-entropy loss:.
j (θ) =.
log p(y(n).
t.| y(n).
<t , x(n); θ).
(1).
n(cid:88).
t(cid:88).
n=1.
t=1.
t.in figure 1, we illustrate how semantic diver-gences interact with nmt training.
in the case ofcoarse divergences, both the preﬁxes (cid:101)y(n)t<1 and tar-gets (cid:101)y(n), yield a noisy training signal at each timestep t, which motivates excluding them from thetraining pool entirely.
in the case of ﬁne-graineddivergences, the assumption of semantic equiva-lence is only partially broken.
depending on thetime step t, we might thus condition the predictionof the next token on partially corrupted preﬁxes,encourage the model to make a wrong prediction,or do a combination of the above.
this suggeststhat ﬁne-grained divergent samples provide a noisyyet potentially useful training signal depending onthe time step.
meanwhile, ﬁne-grained divergencesincrease uncertainty in the training data, and asa result might impact models’ conﬁdence in theirpredictions, as noisy untranslated samples do (ottet al., 2018).
this work seeks to clarify and mit-igate their impact on nmt, accounting for bothtranslation quality and model conﬁdence..3 analyzing the impact of divergences.
3.1 method.
we evaluate the impact of semantic divergenceson nmt by injecting increasing amounts of syn-thetic divergent samples during training, followingthe methodology of khayrallah and koehn (2018)for noise.
we focus on three types of divergences,which were found to be frequent in parallel cor-pora.
they are ﬁne-grained as they represent dis-crepancies between the source and target segments.
7237at a word or phrase level: lexical substitu-tion aims at mimicking particularization and gen-eralization operations resulting from non-literaltranslations (zhai et al., 2019a, 2020b); phrasereplacement mimics phrasal mistranslations;subtree deletion simulates missing phrasalcontent from the source or target side..synthetic divergent samples are automaticallygenerated by corrupting semantically equivalentsentence pairs, following the methodology intro-duced by briakou and carpuat (2020).
equiva-lents are identiﬁed by their divergent mbert clas-siﬁer that yields an f1 score of 84, on manuallyannotated wikimatrix data, despite being trainedon synthetic data.
for lexical substitutionwe corrupt equivalents by substituting words withtheir hypernyms or hyponyms from wordnet, forphrase replacement we replace sequences ofwords with phrases of matching pos tags, and forsubtree deletion we randomly delete subtreesin the dependency parse tree of either the sourceor the target.
having access to those 4 versions ofthe same corpus (one initial equivalent and threesynthetic divergences), we mix equivalents and di-vergent pairs introducing one type of divergence ata time (corpora statistics are included in d).
finally,we evaluate the translation quality and uncertaintyof the resulting translation models..3.2 experimental set-up.
training data we train our models on the paral-lel wikimatrix french-english corpus (schwenket al., 2019), which consists of sentence pairsmined from wikipedia pages using language-agnostic sentence embeddings (laser) (artetxeand schwenk, 2019).
previous annotations showthat 40% of sentence pairs in a random sample con-tain ﬁne-grained divergences (briakou and carpuat,2020)..after cleaning noisy samples using simple rules(i.e., exclude pairs that are a) too short or too long,b) mostly numbers, c) almost copies based on editdistance), we extract equivalent samples using thedivergent mbert model.
table 1 presents statis-tics on the extracted pairs, along with the corpuscreated if we threshold the laser score at 1.04, assuggested by schwenk et al.
(2019)..development and test data we use the ofﬁcialdevelopment and test splits of the ted corpus (qiet al., 2018), consisting of 4,320 and 4,866 gold-standard translation pairs, respectively.
all models.
corpus.
wikimatrix.
+ heuristic filtering+ laser filtering+ divergentmbert filtering.
#sentences.
6,562,3602,437,1081,250,683751,792.table 1: wikimatrix en-fr corpus statistics..share the same bpe vocabulary.
we average resultsacross runs with 3 different random seeds..preprocessing we use the standard mosesscripts (koehn et al., 2007) for punctuation nor-malization, true-casing, and tokenization.
we learn32k bpes (sennrich et al., 2016c) using sentence-piece (kudo and richardson, 2018)..models we use the base transformer architec-ture (vaswani et al., 2017), with embedding size of512, transformer hidden size of 2,048, 8 attentionheads, 6 transformer layers, and dropout of 0.1.target embeddings are tied with the output layerweights.
we train with label smoothing (0.1).
weoptimize with adam (kingma and ba, 2015) witha batch size of 4,096 tokens and checkpoint mod-els every 1,000 updates.
the initial learning rateis 0.0002, and it is reduced by 30% after 4 check-points without validation perplexity improvement.
we stop training after 20 checkpoints without im-provement.
we select the best checkpoint based onvalidation bleu (papineni et al., 2002).
all modelsare trained on a single geforce gtx 1080 gpu..3.3 findings.
translation quality table 2 presents the impactof semantic divergences on bleu and meteor.
corrupting equivalent bitext with ﬁne-grained di-vergences hurts translation quality across the board.
in most cases, the degradation is proportional tothe percentage of corrupted training samples.
lex-ical substitution causes the largest degrada-tion for both metrics.
the degradation is relativelysmaller for meteor than bleu, which we attributeto the fact that meteor allows matches betweensynonyms when comparing references to hypothe-ses.
subtree deletion and lexical substi-tution corruptions lead to signiﬁcant degradationat ≥ 50% (bleu; standard deviations across re-runs are < 0.4).
by contrast, transformers aremore robust to phrase replacement corrup-tions, as degradations are only signiﬁcant after cor-rupting ≥ 70% (bleu) of equivalents..7238phrase.
lexical.
0%.
10%.
20%.
50%.
70%.
100%.
0%.
10%.
20%.
50%.
70%.
100%.
bleu.
meteor.
replacement.
30.89+0.00.
31.00+0.11.
30.82-0.07.
30.40-0.49.
29.74-1.15.
27.01-3.88.
33.74+0.00.
33.63-0.11.
33.66-0.08.
33.54-0.20.
33.12-0.62.
31.02-2.72.subtree.
deletion.
30.89+0.00.
30.80-0.09.
30.62-0.27.
28.95-1.94.
29.00-1.89.
27.50-3.39.
33.74+0.00.
33.61-0.13.
33.38-0.36.
32.17-1.57.
32.09-1.65.
31.44-2.30.substitution.
30.89+0.00.
30.72-0.17.
30.49-0.40.
25.04-5.85.
26.57-4.32.
25.18-5.71.
33.74+0.00.
33.56-0.18.
33.50-0.24.
29.59-4.15.
31.58-2.16.
30.75-2.99.table 2: results for fr→en translation on the ted test set (means of 3 runs).
bars denote degradation over equiv-alents (i.e., 0%) across different % of corruption.
divergences hurt bleu and meteor when they overwhelmthe training data.
transformers are particularly sensitive to ﬁne nuances introduced by lexical substitution..figure 2: average token probabilities of predictions conditioned on gold references (left) and beam search (5)preﬁxes (right).
training on ﬁne-grained divergences (100% corruption) increase nmt model’s uncertainty..more conﬁdent in their token level predictions bothat inference and training time.
subtree dele-tion mismatches affect models’ conﬁdence lessthan other types, while phrase replacementhurts conﬁdence the most both at inference and attraining time.
finally, we observe that differencesacross divergence types are larger in early decodingsteps, while at later steps, they all converge belowthe equivalents..degenerated hypotheses when models aretrained on 50% or more divergent samples, thetotal length of their hypotheses is longer than thereferences.
manual analysis on models trained with100% of divergent samples suggests that this lengtheffect is partially caused by degenerated text.
fol-lowing holtzman et al.
(2019)—who study this phe-nomenon for unconditional text generation—wedeﬁne degenerations as “output text that is bland,incoherent, or gets stuck in repetitive loops”.1.
1for instance, “i’ve never studied sculpture, engineering.
and architecture, and the engineering and architecture”..figure 3: % of degenerated outputs as a function ofbeam size.
nmt training on ﬁne-grained divergences(100% corruptions) increase the frequency of degener-ated hypotheses across beams..token uncertainty we measure the impact ofdivergences on model uncertainty at training timeand at test time.
for the ﬁrst, we extract the proba-bility of a reference token conditioned on referencepreﬁxes at each time step.
for the latter, we com-pute the probability of the token predicted by themodel given its own history of predictions.
figure 2shows that models trained on equivalents are.
7239123451020beam size23456789% degenerated outputsequivalentsreplacedeletesubstitutewe automatically detect degenerated text inmodel outputs by checking whether they containrepetitive loops of n-grams that do not appear inthe reference (details on the algorithm are in c).
figure 3 shows that exposing nmt to divergencesincreases the percentage of degenerated outputs.
even with large beams, the models trained on di-vergent data yield more repetitions than the equiv-alents.
moreover, divergences due to phrasalmismatches (phrase replacement and sub-tree deletion) yield more frequent repetitionsthan token-level mismatches (lexical substi-tution).
interestingly, the latter almost matchesthe frequency of repetitions in equivalents withlarger beams (≥ 5)..summary synthetic divergences hurt translationquality, as expected.
more surprisingly, our studyalso reveals that this degradation is partially dueto more frequent degenerated outputs, and that di-vergences impact models’ conﬁdence in their pre-dictions.
different types of divergences have dif-ferent effects: lexical substitution causesthe largest degradation in translation quality, sub-tree deletion and phrase replacement in-crease the number of degenerated beam hypotheses,while phrase replacement also hurts the mod-els’ conﬁdence the most.
nevertheless, the impactof divergences on bleu appears to be smaller thanthat of noise (khayrallah and koehn, 2018).2 thissuggests that noise ﬁltering techniques are subopti-mal to deal with ﬁne-grained divergences..4 mitigating the impact of fine-grained.
divergences.
we now turn to naturally occurring divergencesin wikimatrix.
we will see that their impact onmodel quality and uncertainty is consistent withthat of synthetic divergences (§ 4.3).
we proposea divergent-aware framework for nmt (§ 4.1) thatsuccessfully mitigates their impact (§ 4.3)..4.1 factorizing divergences for nmt.
we use semantic factors to inform nmt oftokens that are indicative of meaning differ-ences in each sentence pair.
we tag divergentsource and target tokens in parallel segmentsas equivalent (eq) or divergent (div) using anmbert-based classiﬁer trained on synthetic data..2while the absolute scores are not directly comparableacross settings, khayrallah and koehn (2018) report that noisehas a more striking impact of −8 to −25 bleu..the classiﬁer has a 45 f1 score on a ﬁne-graineddivergence test set (briakou and carpuat, 2020).
the predicted tags are thus noisy, as expectedon this challenging task, yet we will see thatthey are useful.
an example is illustrated below:.
src.
tgt.
tokens.
votre p`ere est francais.
factors eq div eq eq.
tokens.
your parent is french.
factors eq div eq eq.
source factors we follow sennrich and had-dow (2016) who represent the encoder input asa combination of token embeddings and linguis-tic features.
concretely, we look up separate em-beddings vectors for tokens and source-side diver-gent predictions, which are then concatenated.
thelength of the concatenated vector matches the totalembedding size..t.target factors target-side divergence tags arean additional output sequence, as in garc´ıa-mart´ınez et al.
(2016).
at each time step the modelproduces two distributions: one over the token tar-get vocabulary and one over the target factors.
themodel is trained to minimize a divergent-aware loss(equation 2).
terms in red (also, underlined) corre-spond to modiﬁcations to the traditional nmt loss.
at time step t, the model is rewarded to match thereference target y(n), conditioned on the source se-quence of tokens (x(n)), the source factors (ω(n)),the token target preﬁx (y(n)<t ), and the target fac-tors preﬁx (z(n)<t ).
at the same time (t), the modelis rewarded to match the factored predictions forthe previous time step τ = t − 1. the time shiftbetween the two target sequences is introduced sothat the model learns to ﬁrstly predict the referencetoken at τ and then its corresponding eq vs. divlabel, at the same time step.
the factored predic-tions are conditioned again on x(n), ω(n), the targetfactor preﬁx z(n).
<τ and the token preﬁx (y(n)≤τ )..l = −.
log p(y(n).
t.| y(n).
<t , z(n).
<t , x(n), ω(n); θ).
n(cid:88).
(cid:32) t.(cid:88).
n=1.
t=1(cid:124).
t(cid:88).
+.
τ =t−1(cid:124).
log p(z(n).
τ.
| z(n).
<τ , y(n).
≤τ , x(n), ω(n); θ).
(cid:123)(cid:122)(n)˜lmt.
(cid:123)(cid:122)(n)f actor.
l.(cid:125).
(cid:33).
(cid:125).
(2).
7240inference at test time, input tokens are taggedwith eq to encourage the model to predict an equiv-alent translation.
we decode using beam searchfor predicting the translation sequence.
the tokenpredictions are conditioned on both the token andthe factors preﬁxes.
the factor preﬁxes are greedilydecoded and thus do not participate in beam search..4.2 experimental set-up.
divergences we conduct an extensive compar-ison of models exposed to different amountsof equivalent and divergent wikimatrix samples.
starting from the pool of examples identiﬁedas divergent at §3.2, we rank and select themost ﬁne-grained divergences by thresholding thebicleaner score (ram´ırez-s´anchez et al., 2020)at 0.5, 0.7 and 0.8. for details, see a..models we compare the factored models (div-factorized) for incorporating divergent tokens(§4.1) against: 1. laser models are trained onwikimatrix pairs with a laser score greater than1.04 – the noise ﬁltering strategy recommended byschwenk et al.
(2019).
our prior work shows thatthresholding laser might introduce a number ofdivergent data in the training pool varying from ﬁneto coarse mismatches (briakou and carpuat, 2020).
2. equivalents models are trained on wiki-matrix pairs detected as exact translations (§3.2);3. div-agnostic models are trained on equiv-alent and ﬁne-grained divergent data without in-corporating information that distinguishes betweenthem; 4. div-tagged models distinguish equiva-lences from divergences by appending <eq> vs.<div> tags as source-side constraints (sennrichet al., 2016a)..models’ details our models are implemented inthe sockeye2 toolkit (domhan et al., 2020).3 weset the size of factor embeddings to 8, the sourcetoken embeddings to 504 and target embeddingsto 514, yielding equal model sizes across exper-iments.
all other parameters are kept the sameacross models, as discussed in §3.2, except thattarget embeddings are not tied with output layerweights for factored models.
more details are in-cluded in b..other data & preprocessing we use the samepreprocessing as well as development and testsets as in §3.2, except we learn 5k bpes as in.
3https://github.com/awslabs/sockeye.
schwenk et al.
(2019).
div-factorized, div-agnostic, and div-tagged models are com-pared in controlled setups that use the same train-ing data.
we also evaluate out-of-domain onthe khresmoi-summary test set for the wmt2014medical translation task (bojar et al., 2014)..evaluation we evaluate translation quality withbleu (papineni et al., 2002) and meteor (baner-jee and lavie, 2005).4,5 we compute inferenceexpected calibration error (infece) as wanget al.
(2020), which measures the difference in ex-pectation between conﬁdence and accuracy.6 wemeasure token-level translation accuracy based ontranslation error rate (ter) alignments betweenhypotheses and references.7 unless mentioned oth-erwise, we decode with a beam size of 5..4.3 results.
we discuss the impact of real divergences along thedimensions surfaced by the synthetic data analysis..translation quality table 3 presents bleu andmeteor scores across model conﬁgurations anddata settings on the ted test sets.
first, the modeltrained on equivalents represents a very com-petitive baseline as it performs better or statisti-cally comparable to all models.
this result is inline with prior evidence of vyas et al.
(2018) whoshow that ﬁltering out the most divergent pairsin noisy corpora (e.g., opensubtitles and com-moncrawl) does not hurt translation quality.
in-terestingly, the equivalents model outperformslaser across metrics and translation directions,despite the fact that it is exposed to only abouthalf of the training data.
gradually adding diver-gent data (div-agnostic) hurts translation qualityacross the board compared to the equivalentsmodel.
the drops are signiﬁcantly larger when di-vergences overwhelm the equivalent translations,which is consistent with our ﬁndings on syntheticdata..second, div-factorized is the most effectivemitigation strategy.
with segment-level constraints(div-tagged), models can recover from the degra-dation caused by divergences (div-agnostic), butnot consistently.
by contrast, token-level factors(div-factorized) help nmt recover from the im-pact of divergences across data setups and reach.
4https://github.com/mjpost/sacrebleu5https://www.cs.cmu.edu/˜alavie/meteor/6https://github.com/shuo-git/infece7http://www.cs.umd.edu/˜snover/tercom/.
7241method.
laser.
equivalents.
(cid:40) agnostic.
+div.
tagged.
factorized.
(cid:40) agnostic.
+div.
tagged.
factorized.
(cid:40) agnostic.
+div.
tagged.
factorized.
fr→en.
en→fr.
training size.
bleu ↓.
meteor ↓.
bleu ↓.
meteor ↓.
1.25m.
31.80 ±0.36 ↓.
34.00 ±0.17 ↓.
32.16 ±0.29 ↓.
56.49 ±0.24 ↓.
0.75m.
32.88 ±0.07 ↓.
34.75 ±0.10 ↓.
33.53 ±0.35 ↓.
57.38 ±0.28 ↓.
0.93m.
1.12m.
1.68m.
32.47 ±0.40 ↓31.76 ±1.61 ↓32.73 ±0.38 ↓.
32.53 ±0.46 ↓32.38 ±0.40 ↓32.79 ±0.24 ↓.
31.40 ±0.21 ↓31.97 ±0.26 ↑32.57 ±0.19 ↑.
34.56 ±0.20 ↓34.17 ±0.91 ↓34.84 ±0.21 ↑.
34.40 ±0.21 ↓34.52 ±0.13 ↓34.89 ±0.12 ↑.
33.79 ±0.11 ↓34.30 ±0.10 ↑34.70 ±0.11 ↑.
33.19 ±0.30 ↓33.43 ±0.39 ↓33.92 ±0.38 ↑.
31.47 ±0.61 ↓33.35 ±0.17 ↑33.22 ±0.35 ↑.
29.53 ±0.39 ↓31.37 ±0.12 ↑31.60 ±0.42 ↑.
57.10 ±0.30 ↓57.55 ±0.27 ↑57.63 ±0.28 ↑.
56.25 ±0.46 ↓57.33 ±0.14 ↑57.31 ±0.30 ↑.
54.29 ±0.44 ↓55.87 ±0.18 ↑56.10 ±0.22 ↑.
table 3: results for en↔fr translation on the ted test set (averages and stdev of 3 runs).
we underline the topscores among all models and boldface the scores lying within one stdev from equivalents.
↑ denotes (onestdev) improvements of div-tagged and div-factorized over div-agnostic.
factorizing divergences helpsnmt recover from the degradation caused by divergences, while it achieves comparable scores to equivalents..method.
train.
size (m).
fr→en.
en→fr.
laser.
equivalents.
div-agnostic.
div-factorized.
1.250.75.
0.931.121.68.
0.931.121.68.
38.27 ±0.4939.47 ±0.24.
39.27 ±0.4539.63 ±0.52.
39.45 ±0.5040.00 ±0.1439.90 ±0.14.
39.78 ±0.3739.20 ±0.5038.00 ±0.50.
40.27 ±0.4940.13 ±0.4640.03 ±0.42 40.30 ±0.2939.30 ±0.1639.97 ±0.26.
table 4: bleu scores on the medical domain.
we un-derline top scores and boldface (one stdev) improve-ments over equivalents.
divergences improve trans-lation quality when modeled by div-factorized..training divergences.
beam.
0%.
20%.
33%.
55%.
1.93.
1.55 1.09.
1.21 1.16.
2.92 1.81.
1.53.
1.19 0.71.
0.84 0.84.
2.78 1.49.
1.
5.
10.
1.48.
1.06 0.73.
0.84 0.76.
2.80 1.28.table 5: percentage of degenerated outputs for fr→enmodels exposed to difference percentage of divergenttraining data (0% corresponds to equivalents; darkgray columns correspond to div-agnostic).
div-factorized (grid-columns) help recover from degen-erations, yielding fewer repetitions across beams..translation quality comparable to that of the equiv-alents model, successfully mitigating the impactof the noisy training signals from divergent sam-ples..third, when translating the out-of-domain testset, div-factorized improves over the equiv-alents model, as presented in table 4. div-agnostic models perform comparably to equiv-alents, while factorizing divergences improveson the latter by ≈ +1 bleu, for both directions.8mitigating the impact of divergences is thus impor-tant for nmt to beneﬁt from the increased coverageof out-of-domain data provided by the divergentsamples..degenerated hypotheses we check for degen-erated outputs across models, data setups (we ac-count for different percentages of divergences inthe training data), and different beam sizes (ta-ble 5).
as with synthetic divergences, we observethat when real divergences overwhelm the trainingdata (55%), degenerated loops are almost twice asfrequent for all beam sizes.
this phenomenon isconsistently mitigated by div-factorized mod-els across the board.9 furthermore, in some set-tings (20%, 33%), div-factorized models de-crease the amount of degenerated text by half com-pared to the equivalents models.10.
8we include meteor results in appendix e.9we observe similar trends for en→fr in appendix f10 laser models degenerate more frequently than equiva-.
lents and div-factorized..7242(a) fr→en.
(b) fr→en.
(c) en→fr.
(d) en→fr.
figure 4: average token probability across time steps on ted test set.
div-agnostic yield the least conﬁdent pre-dictions (for reference and inference preﬁxes); div-factorized help recover from this drop (55% divergences)..fr→en.
↓ ↓.
en→fr.
↓ ↓.
train.
size.
conf.
(↑).
acc.
(↑).
infece (↓).
conf.% (↑).
acc.
(↑).
infece (↓).
69.09 ±0.67.
62.55 ±0.29.
12.34 ±0.38.
71.88 ±0.30.
60.20 ±0.18.
15.10 ±0.12.
equivalents.
70.96 ±0.94.
63.49 ±0.10.
12.37 ±0.24.
74.35 ±0.23.
61.81 ±0.30.
15.09 ±0.18.
71.19 ±0.3372.16 ±0.10*.
63.54 ±0.5464.29 ±0.44*.
12.00 ±0.0611.81 ±0.04*.
73.67 ±0.1174.50 ±0.02*.
61.44 ±0.2262.26 ±0.27*.
15.19 ±0.1714.70 ±0.25*.
71.65 ±0.1871.83 ±0.03.
61.34 ±0.3364.38 ±0.08*.
11.98 ±0.2211.86 ±0.01.
71.72 ±0.3874.09 ±0.14*.
59.29 ±0.4861.65 ±0.19*.
15.62 ±0.1914.84 ±0.18*.
68.01 ±0.3771.01 ±0.39*.
61.34 ±0.2363.65 ±0.07*.
12.63 ±0.2311.75 ±0.35*.
68.38 ±0.2571.81 ±0.49*.
56.89 ±0.3459.78 ±0.39*.
16.24 ±0.2714.95 ±0.02*.
method.
laser.
div-agnosticdiv-factorized.
div-agnosticdiv-factorized.
div-agnosticdiv-factorized.
1.25m.
0.75m.
0.93m.
1.12m.
1.68m.
table 6: average token conﬁdence, accuracy, and inference calibration results for en↔fr translation on theted test set (average and stdev of 3 runs).
we underline top scores and boldface (one stdev) improvements overequivalents.
* denotes (one stdev) improvements of div-factorized over div-agnostic.
div-factorizedyield more conﬁdent and accurate predictions compared to div-agnostic, yielding the smallest calibration errors..uncertainty figures 4a and 4c show that thegold-standard references are assigned lower proba-bilities by the div-agnostic models than all othermodels, especially in early time steps (t < 30).
weobserve similar drops in conﬁdence based on theprobabilities of predicted tokens at inference time(4b and 4d).
this conﬁrms that exposing models toﬁne-grained semantic divergences hurts their conﬁ-dence, whether the divergences are synthetic or not.
furthermore, factorizing divergences helps miti-gate the impact of naturally occurring divergenceson uncertainty in addition to translation quality..we conduct a calibration analysis to measure thedifferences between the conﬁdence (i.e., probabil-ity) and the correctness (i.e., accuracy) of the gener-ated tokens in expectation.
given that deep neuralnetworks are often mis-calibrated in the direction ofover-estimation (conﬁdence>accuracy) (guo et al.,2017), we check whether the increased conﬁdenceof div-factorized hurts calibration (table 6).
div-factorized models are on average more con-ﬁdent and more accurate than their div-agnosticinterestingly, div-agnostic hascounterparts.
smaller calibration errors than equivalents andlaser models across the board..5 related work.
we discuss work related to cross-lingual semanticdivergences and noise effects in section 2 and nowturn to the literature that connects with the methodsused in this paper..factored models factored models are intro-duced to inject word-level linguistic annotations(e.g., part-of-speech tags, lemmas) in translation.
source-side factors have been used in statisticalmt (haddow and koehn, 2012) and in nmt (sen-nrich et al., 2016b; hoang et al., 2016).
target-sidefactors are used by garc´ıa-mart´ınez et al.
(2017)as an extension to the traditional nmt frameworkthat outputs multiple sequences.
although theirmain motivation is to enable models to handlelarger vocabularies, wilken and matusov (2019)propose a list of novel applications of target-sidefactors beyond their initial purpose, such as word-case prediction and subword segmentation.
ourapproach draws inspiration from all the aforemen-tioned works, yet it is unique in its use of bothsource and target factors to incorporate semanticsin nmt..7243calibration kumar and sarawagi (2019) ﬁndthat nmt models are miscalibrated, even when con-ditioned on gold-standard preﬁxes.
they attributethis behavior to the poor calibration of the eostoken and the uncertainty of attention and designa recalibration model to improve calibration.
ottet al.
(2018) argue that miscalibration can be at-tributed to the “extrinsic” uncertainty of the noisy,untranslated references found in the training data.
m¨uller et al.
(2019) investigate the effect of labelsmoothing on calibration.
on a similar spirit, wanget al.
(2020) propose graduated label smoothing toimprove calibration at inference time.
they alsolink miscalibration to linguistic properties of thedata (e.g., frequency, position, syntactic roles).
ourwork, in contrast, focuses on the semantic proper-ties of the training data that affect calibration..6 conclusion.
this work investigates the impact of semantic mis-matches beyond noise in parallel text on nmt qual-ity and conﬁdence.
our experiments on en↔frtasks show that ﬁne-grained semantic divergenceshurt translation quality when they overwhelm thetraining data.
models exposed to ﬁne-grained di-vergences at training time are less conﬁdent in theirpredictions, which hurts beam search and producesdegenerated text (repetitive loops) more frequently.
furthermore, we also show that, unlike noisysamples, ﬁne-grained divergences can still providea useful training signal for nmt when they are mod-eled via factors.
evaluated on en↔fr translationtasks, our divergent-aware nmt framework miti-gates the negative impact of divergent referenceson translation quality, improves the conﬁdence andcalibration of predictions, and produces degener-ated text less frequently..more broadly, this work illustrates how under-standing the properties of training data can helpbuild better nmt models.
in future work, we willextend our analysis to other properties of paral-lel text and to other language pairs, focusing onlow-resource conditions where divergences are ex-pected to be even more prevalent..acknowledgements.
we thank sweta agrawal, doug oard, suraj rajap-pan nair, the anonymous reviewers and the cliplab at umd for helpful comments.
this materialis based upon work supported by the national sci-ence foundation under award no.
1750695. any.
opinions, ﬁndings, and conclusions or recommen-dations expressed in this material are those of theauthors and do not necessarily reﬂect the views ofthe national science foundation..references.
m. artetxe and holger schwenk.
2019. massively mul-tilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transactions of the as-sociation for computational linguistics, 7:597–610..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, michigan.
association for computational lin-guistics..rachel bawden, kevin bretonnel cohen, cristiangrozea, antonio jimeno yepes, madeleine kittner,martin krallinger, nancy mah, aurelie neveol, mar-iana neves, felipe soares, amy siu, karin verspoor,and maika vicente navarro.
2019. findings of thewmt 2019 biomedical translation shared task: eval-uation for medline abstracts and biomedical ter-in proceedings of the fourth confer-minologies.
ence on machine translation (volume 3: sharedtask papers, day 2), pages 29–53, florence, italy.
association for computational linguistics..rachel bawden, giorgio maria di nunzio, cris-tian grozea,inigo jauregi unanue, antonio ji-meno yepes, nancy mah, david martinez, aur´elien´ev´eol, mariana neves, maite oronoz, olatz perez-de vi˜naspre, massimo piccardi, roland roller, amysiu, philippe thomas, federica vezzani, maika vi-cente navarro, dina wiemann, and lana yeganova.
2020. findings of the wmt 2020 biomedical trans-lation shared task: basque, italian and russian asin proceedings of thenew additional languages.
fifth conference on machine translation, pages660–687, online.
association for computationallinguistics..ondˇrej bojar, christian buck, christian federmann,barry haddow, philipp koehn, johannes leveling,christof monz, pavel pecina, matt post, hervesaint-amand, radu soricut, lucia specia, and aleˇstamchyna.
2014. findings of the 2014 workshop onstatistical machine translation.
in proceedings of theninth workshop on statistical machine translation,pages 12–58, baltimore, maryland, usa.
associa-tion for computational linguistics..eleftheria briakou and marine carpuat.
2020. de-tecting fine-grained cross-lingual semantic diver-gences without supervision by learning to rank.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 1563–1580, online.
association for computa-tional linguistics..7244marine carpuat, yogarshi vyas, and xing niu.
2017.detecting cross-lingual semantic divergence for neu-ral machine translation.
in proceedings of the firstworkshop on neural machine translation, pages 69–79, vancouver.
association for computational lin-guistics..tobias domhan, michael denkowski, david vilar,xing niu, felix hieber, and kenneth heaﬁeld.
2020.the sockeye 2 neural machine translation toolkit atamta 2020. in proceedings of the 14th conferenceof the association for machine translation in theamericas (volume 1: research track), pages 110–115, virtual.
association for machine translation inthe americas..ahmed el-kishky, philipp koehn,.
and holgersearching the web for cross-schwenk.
2020.lingual parallel data, page 2417–2420.
associ-ation for computing machinery, new york, ny,usa..miquel espl`a, mikel forcada, gema ram´ırez-s´anchez,and hieu hoang.
2019. paracrawl: web-scale paral-lel corpora for the languages of the eu.
in proceed-ings of machine translation summit xvii volume 2:translator, project and user tracks, pages 118–119,dublin, ireland.
european association for machinetranslation..miquel espl`a-gomis, v´ıctor m. s´anchez-cartagena,jaume zaragoza-bernabeu, and felipe s´anchez-mart´ınez.
2020. bicleaner at wmt 2020: uni-versitat d’alacant-prompsit’s submission to the par-in proceedingsallel corpus ﬁltering shared task.
of the fifth conference on machine translation,pages 952–958, online.
association for computa-tional linguistics..pascale fung and percy cheung.
2004. multi-levelbootstrapping for extracting parallel sentences fromin coling 2004:a quasi-comparable corpus.
proceedings of the 20th international conferenceon computational linguistics, pages 1051–1057,geneva, switzerland.
coling..mercedes garc´ıa-mart´ınez, lo¨ıc barrault, and fethibougares.
2016. factored neural machine transla-tion.
corr, abs/1609.04621..mercedes garc´ıa-mart´ınez, lo¨ıc barrault, and fethibougares.
2017. neural machine translation bycorr,generating multiple linguistic factors.
abs/1712.01821..chuan guo, geoff pleiss, yu sun, and kilian q. wein-berger.
2017. on calibration of modern neural net-in proceedings of the 34th internationalworks.
conference on machine learning, volume 70 ofproceedings of machine learning research, pages1321–1330, international convention centre, syd-ney, australia.
pmlr..barry haddow and philipp koehn.
2012. interpolatedin asso-backoff for factored translation models.
ciation for machine translation in the americas,amta..cong duy vu hoang, gholamreza haffari, and trevorimproving neural translation modelscohn.
2016.in proceedings of the aus-with linguistic factors.
tralasian language technology association work-shop 2016, pages 7–14, melbourne, australia..ari holtzman, jan buys, maxwell forbes, and yejinchoi.
2019. the curious case of neural text degener-ation.
corr, abs/1904.09751..antonio jimeno yepes, aur´elie n´ev´eol, mariananeves, karin verspoor, ondˇrej bojar, arthur boyer,cristian grozea, barry haddow, madeleine kit-tner, yvonne lichtblau, pavel pecina, roland roller,rudolf rosa, amy siu, philippe thomas, andsaskia trescher.
2017. findings of the wmt 2017biomedical translation shared task.
in proceedingsof the second conference on machine translation,pages 234–247, copenhagen, denmark.
associationfor computational linguistics..huda khayrallah and philipp koehn.
2018. on theimpact of various types of noise on neural machinetranslation.
in proceedings of the 2nd workshop onneural machine translation and generation, pages74–83, melbourne, australia.
association for com-putational linguistics..diederik p. kingma and jimmy ba.
2015. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..philipp koehn, vishrav chaudhary, ahmed el-kishky,naman goyal, peng-jen chen, and franciscoguzm´an.
2020. findings of the wmt 2020 sharedtask on parallel corpus ﬁltering and alignment.
inproceedings of the fifth conference on machinetranslation, pages 726–742, online.
association forcomputational linguistics..philipp koehn, francisco guzm´an, vishrav chaud-hary, and juan pino.
2019. findings of the wmt2019 shared task on parallel corpus ﬁltering forin proceedings of thelow-resource conditions.
fourth conference on machine translation (volume3: shared task papers, day 2), pages 54–72, flo-rence, italy.
association for computational linguis-tics..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..philipp koehn, huda khayrallah, kenneth heaﬁeld,and mikel l. forcada.
2018. findings of the wmt.
72452018 shared task on parallel corpus ﬁltering.
in pro-ceedings of the third conference on machine trans-lation: shared task papers, pages 726–739, bel-gium, brussels.
association for computational lin-guistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..aviral kumar and sunita sarawagi.
2019. calibrationof encoder decoder models for neural machine trans-lation.
corr, abs/1903.00802..rafael m¨uller, simon kornblith, and geoffrey e hin-ton.
2019. when does label smoothing help?
inadvances in neural information processing systems,volume 32, pages 4694–4703.
curran associates,inc..dragos stefan munteanu and daniel marcu.
2005.improving machine translation performance by ex-ploiting non-parallel corpora.
comput.
linguist.,31(4):477–504..mariana neves, antonio jimeno yepes, aur´elien´ev´eol, cristian grozea, amy siu, madeleine kit-tner, and karin verspoor.
2018. findings of thewmt 2018 biomedical translation shared task: eval-uation on medline test sets.
in proceedings of thethird conference on machine translation: sharedtask papers, pages 324–339, belgium, brussels.
as-sociation for computational linguistics..myle ott, michael auli, david grangier,.
andmarc’aurelio ranzato.
2018. analyzing uncer-tainty in neural machine translation.
arxiv preprintarxiv:1803.00047..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ye qi, devendra sachan, matthieu felix, sarguna pad-manabhan, and graham neubig.
2018. when andwhy are pre-trained word embeddings useful for neu-ral machine translation?
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 529–535, new orleans, louisiana.
associa-tion for computational linguistics..gema ram´ırez-s´anchez, jaume zaragoza-bernabeu,marta ba˜n´on, and sergio ortiz rojas.
2020. bi-ﬁxer and bicleaner: two open-source tools to cleanyour parallel data.
in proceedings of the 22nd an-nual conference of the european association for.
machine translation, pages 291–298, lisboa, portu-gal.
european association for machine translation..szymon roziewski and wojciech stokowiec.
2016.languagecrawl: a generic tool for building lan-in proceed-guage models upon common-crawl.
ings of the tenth international conference on lan-guage resources and evaluation (lrec’16), pages2789–2793, portoroˇz, slovenia.
european languageresources association (elra)..holger schwenk, vishrav chaudhary, shuo sun,and francisco guzm´an.
2019.hongyu gong,wikimatrix: mining 135m parallel sentences incorr,1620 language pairs from wikipedia.
abs/1907.05791..rico sennrich and barry haddow.
2016. linguisticinput features improve neural machine translation.
in proceedings of the first conference on machinetranslation: volume 1, research papers, pages 83–91, berlin, germany.
association for computationallinguistics..rico sennrich, barry haddow, and alexandra birch.
2016a.
controlling politeness in neural machinein proceedings oftranslation via side constraints.
the 2016 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 35–40, sandiego, california.
association for computationallinguistics..rico sennrich, barry haddow, and alexandra birch.
2016b.
improving neural machine translation mod-in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages86–96, berlin, germany.
association for computa-tional linguistics..rico sennrich, barry haddow, and alexandra birch.
2016c.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..jason r. smith, chris quirk, and kristina toutanova.
2010. extracting parallel sentences from compara-ble corpora using document level alignment.
in hu-man language technologies: the 2010 annual con-ference of the north american chapter of the associ-ation for computational linguistics, pages 403–411,los angeles, california.
association for computa-tional linguistics..jason r. smith, herve saint-amand, magdalena pla-mada, philipp koehn, chris callison-burch, andadam lopez.
2013. dirt cheap web-scale paralleltext from the common crawl.
in proceedings of the51st annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1374–1383, soﬁa, bulgaria.
association for compu-tational linguistics..7246ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..yogarshi vyas, xing niu, and marine carpuat.
2018.textidentifying semantic divergences in parallelin proceedings of the 2018without annotations.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 1503–1515, new orleans, louisiana.
associ-ation for computational linguistics..shuo wang, zhaopeng tu, shuming shi, and yang liu.
2020. on the inference calibration of neural ma-chine translation.
in acl..patrick wilken and evgeny matusov.
2019. novelapplications of factored neural machine translation.
corr, abs/1910.03912..fangzhou zhai, vera demberg, pavel shkadzko, weishi, and asad sayeed.
2019a.
a hybrid model forglobally coherent story generation.
in proceedingsof the second workshop on storytelling, pages 34–45, florence, italy.
association for computationallinguistics..yuming zhai, gabriel illouz, and anne vilnat.
2020a.
translations by ﬁne-tuningdetecting non-literalcross-lingual pre-trained language models.
inproceedings of the 28th international conferenceon computational linguistics, pages 5944–5956,barcelona, spain (online).
international committeeon computational linguistics..yuming zhai, lufei liu, xinyi zhong, gbariel il-louz, and anne vilnat.
2020b.
building anenglish-chinese parallel corpus annotated with sub-sentential translation techniques.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 4024–4033, marseille, france.
euro-pean language resources association..yuming zhai, aur´elien max, and anne vilnat.
2018.construction of a multilingual corpus annotated within proceedings of the firsttranslation relations.
workshop on linguistic resources for natural lan-guage processing, pages 102–111, santa fe, newmexico, usa.
association for computational lin-guistics..yuming zhai, pooyan safari, gabriel illouz, alexandreallauzen, and anne vilnat.
2019b.
towards recog-nizing phrase translation processes: experiments onenglish-french.
corr, abs/1904.12213..7247a wikimatrix fine-grained divergences.
b sockeye2 conﬁguration details.
starting from the pool of examples identiﬁed asdivergent under the divergentmbert classiﬁer, wewant to focus on the subset of samples that con-tain ﬁne meaning differences.
therefore, we usebicleaner to ﬁlter out training data that are likelyto contain coarse meaning differences.
espl`a-gomis et al.
(2020) report better nmt resultson english↔portuguese translation after cleaningwikimatrix data with thresholds of 0.5 and 0.7..we conduct a preliminary experiment to un-derstand how the bicleaner scores of english-french wikimatrix sentences are distributed.
fig-ure 5(a) shows the distribution of scores amongthe three classes of the refresd dataset, a datasetthat distinguishes ﬁne meaning differences (“somemeaning difference”), coarse divergences (“unre-lated”), and equivalent translation pairs (“no mean-ing difference”).11 we observe that thresholdingthe bicleaner score at > 0.5 ﬁlters out most ofthe unrelated pairs.
we conduct three experimentswith thresholds at 0.8, 0.7, and 0.5 to graduallyadd more ﬁne-grained divergences.
figure 5(b)presents the number of english-french wikimatrixdivergences, binned by bicleaner scores..(a) refresd.
(b) wikimatrix divergences.
figure 5: distribution of bicleaner score on refresd,and english-french wikimatrix divergences..11https://github.com/elbria/xling-semdiv/.
tree/master/refresd.
tables 7 and 8 present details of nmt trainingwith sockeye2..--weight-tying-type="trg softmax"--num-words 5000:5000--label-smoothing 0.1--encoder transformer--decoder transformer--num-layers 6--transformer-attention-heads 84--transformer-model-size 512--num-embed 512--transformer-feed-forward-num-hidden 2048--transformer-preprocess n--transformer-postprocess dr--gradient-clipping-type none--transformer-dropout-attention 0.1--transformer-dropout-act 0.1--transformer-dropout-prepost 0.1--max-seq-len 80:80--batch-type word--batch-size 2048--min-num-epochs 3--initial-learning-rate 0.0002--learning-rate-reduce-factor 0.7--learning-rate-reduce-num-not-improved 4--checkpoint-interval 1000--keep-last-params 30--max-num-checkpoint-not-improved 20--decode-and-evaluate 1000.table 7: nmt conﬁgurations on sockeye2 for equiv-alents, laser, div-agnostic, and div-tagged..--weight-tying-type none--source-factors-num-embed 8--source-factors-combine concat--target-factors-num-embed 8--target-factors-combine concat--transformer-model-size 504:512--num-embed 504:504.table 8: nmt conﬁgurations on sockeye2 for div-factorized; for missing settings refer to table 7..c measuring degenerated hypotheses.
we include the pseudo-algorithm that checks if ahypothesis falls under odd repetitions not supportedby the reference in algorithm 1. when measuringrepeated n-grams we exclude punctuation and con-junctions.
the repeated function checks whetheran n-gram is repeated (number of occurrences > 1)in the hypothesis h, or reference r..7248wikimatrix version.
equivalents.
subtree deletion.
phrase replacementlexical substitution (hypernyms)lexical substitution (hyponyms).
#sents..#tokens.
#types.
length %corr.
751,792749,973750,527724,326617,913.
22,723,54320,783,05622,735,14322,014,60918,970,039.
515,154483,336475,567497,658442,299.
30.227.730.330.430.7.
0%9.32%16.11%12.33%7.42%.
table 9: wikimatrix statistics corresponding to extracted equivalents and the ﬁne-grained corruptions intro-duced in the synthetic setting (en-side).
%corr denotes the average % of corrupted tokens in a sentence..wikimatrix version.
equivalents.
subtree deletion.
phrase replacementlexical substitution (hypernyms)lexical substitution (hyponyms).
#sents..#tokens.
#types.
length %corr.
751,792749,973750,527724,326617,913.
25,554,54923,822,95825,554,54924,737,60421,387,650.
515,194486,908515,194499,423445,871.
34.031.834.034.234.6.
0%7.21%12.74%9.82%5.78%.
table 10: wikimatrix statistics corresponding to extracted equivalents and the ﬁne-grained corruptions intro-duced in the synthetic setting (fr-side)..3:.
4:.
5:.
6:.
7:.
8:.
end if.
end if.
end forreturn false.
9:10: end function.
method.
laser.
equivalents.
div-agnostic.
div-factorized.
1.250.75.
0.931.121.68.
0.931.121.68.algorithm 1 degenerated hypothesis checkinput: h, r (hypothesis, reference)output: deg (true for degenerated hypothesis).
1: function degenerationcheck(h[ ],r[ ])2:.
for n-gram ∈ h do.
if repeated(n-gram, h) then.
if not repeated(n-gram, r) then.
return true.
e meteor results (addition).
for completeness, we present meteor scoresthe bleu evaluation of §4.3,to complementwhich consists the ofﬁcial evaluation metric ofwmt biomedical translation tasks (jimeno yepeset al., 2017; neves et al., 2018; bawden et al.,the average improvements of2019, 2020).
div-factorized over equivalents and div-agnostic are smaller compared to the differenceshighlighted by bleu.
however, we note that me-teor results might be misleading when evaluatingmedical translations, as in this domain we mightnot want to account for synonyms when comparingreferences to hypotheses..training data (m).
fr→en.
en→fr.
f degenerated hypotheses (addition).
40.67 ±0.2841.23 ±0.16.
65.17 ±0.4065.60 ±0.45.
41.25 ±0.1741.19 ±0.3141.07 ±0.13.
41.34 ±0.2341.33 ±0.3141.25 ±0.08.
65.67 ±0.2265.23 ±0.3363.97 ±0.56.
66.03 ±0.3865.88 ±0.3565.08 ±0.11.
div-factorized decreases the % of degeneratedoutputs caused by divergent data (table 12)..beam.
0%.
20%.
33%.
55%.
divergences.
1.41.
1.95 1.15.
2.09 1.85.
2.24 2.17.table 11: meteor scores on medical translation task..d synthetic divergences statistics.
1.10.
1.19 0.71.
1.87 1.29.
2.30 1.59.tables 9 and 10 contain corpus statistics for the 3versions of synthetic divergences we create, start-ing from equivalents.
lexical substitu-tion are sampled at random from the pools ofsubstitutions based on hypernyms and hyponyms..0.89.
1.21 0.65.
1.78 1.10.
2.30 1.80.table 12: % of degenerated outputs across beams(en→fr).
div-factorized (grid-columns) help re-cover from degenerations, yielding fewer repetitions..1.
5.
10.
7249