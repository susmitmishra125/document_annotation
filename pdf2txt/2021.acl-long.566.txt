scientiﬁc credibility of machine translation research:a meta-evaluation of 769 papers.
benjamin marie.
atsushi fujitanational institute of information and communications technology3-5 hikaridai, seika-cho, soraku-gun, kyoto, 619-0289, japan{bmarie,atsushi.fujita,raphael.rubino}@nict.go.jp.
raphael rubino.
abstract.
this paper presents the ﬁrst large-scale meta-evaluation of machine translation (mt).
weannotated mt evaluations conducted in 769research papers published from 2010 to 2020.our study shows that practices for automaticmt evaluation have dramatically changed dur-ing the past decade and follow concerningtrends.
an increasing number of mt evalua-tions exclusively rely on differences betweenbleu scores to draw conclusions, withoutperforming any kind of statistical signiﬁcancetesting nor human evaluation, while at least108 metrics claiming to be better than bleuhave been proposed.
mt evaluations in recentpapers tend to copy and compare automaticmetric scores from previous work to claim thesuperiority of a method or an algorithm with-out conﬁrming neither exactly the same train-ing, validating, and testing data have been usednor the metric scores are comparable.
further-more, tools for reporting standardized metricscores are still far from being widely adoptedby the mt community.
after showing how theaccumulation of these pitfalls leads to dubiousevaluation, we propose a guideline to encour-age better automatic mt evaluation along witha simple meta-evaluation scoring method to as-sess its credibility..1.introduction.
new research publications in machine translation(mt) regularly introduce new methods and algo-rithms to improve the translation quality of mt sys-tems.
in the literature, translation quality is usuallyevaluated with automatic metrics such as bleu(papineni et al., 2002) and, more rarely, by humans.
to assess whether an mt system performs betterthan another mt system, their scores given by anautomatic metric are directly compared.
whilesuch comparisons between mt systems are exhib-ited in the large majority of mt papers, there are.
no well-deﬁned guideline nor clear prerequisitesunder which a comparison between mt systemsis considered valid.
consequently, we assume thatevaluation in mt is conducted with different de-grees of thoroughness across papers and that evalu-ation practices have evolved over the years.
whatcould be considered, by the research community,as a good evaluation methodology ten years agomay not be considered good today, and vice versa.
this evolution has not been studied and whethermt evaluation has become better, or worse, is de-batable..on the other hand, several requirements for mtevaluation have been well-identiﬁed.
for instance,the limitations of bleu are well-known (callison-burch et al., 2006; reiter, 2018; mathur et al.,2020) and the necessity to report automatic metricscores through standardized tools, such as sacre-bleu, has been recognized (post, 2018).
more-over, a trustworthy evaluation may adopt statisticalsigniﬁcance testing (koehn, 2004) and strong base-lines (denkowski and neubig, 2017).
however, towhat extent these requirements have been met inmt publications is unclear..in this paper, we propose the ﬁrst large-scalemeta-evaluation of mt in which we manually an-notated 769 research papers published from 2010to 2020. our study shows that evaluation in mt hasdramatically changed since 2010. an increasingnumber of publications exclusively rely on bleuscores to draw their conclusions.
the large major-ity of publications do not perform statistical sig-niﬁcance testing, especially since 2016. moreover,an increasing number of papers copy and comparebleu scores published by previous work whiletools to report standardized metric scores are stillfar from being extensively adopted by the mt com-munity.
we also show that compared systems areoften trained, validated, or even evaluated, on datathat are not exactly the same.
after demonstrating.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7297–7306august1–6,2021.©2021associationforcomputationallinguistics7297how the accumulation of these pitfalls leads to dubi-ous evaluation, we propose a general guideline forautomatic evaluation in mt and a simple scoringmethod to meta-evaluate an mt paper.
we believethat the adoption of these tools by authors or re-viewers have the potential to reverse the concerningtrends observed in this meta-evaluation..a4.
whether it makes comparisons with automaticmetric scores directly copied from previouswork to support its conclusion: yes or no.
most papers copying scores (mostly bleu)clearly mention it.
if there is no evidence thatthe scores have been copied, we annotatedthese papers with “no” for this question..2 a survey on mt evaluation.
we manually annotated the mt evaluation in re-search papers published from 2010 to 2020 at*acl conferences.1 to identify mt papers, wesearched the acl anthology website2 for the terms“mt” or “translation” in their titles3 and analyzedamong them the 769 papers that make comparisonsof translation quality between at least two mt sys-tems.
for each year between 2010 and 2020, werespectively annotated the following numbers ofpapers: 53, 40, 59, 80, 67, 45, 51, 62, 94, 115, and103..we annotated each paper as follows:.
a1.
all the automatic metrics used to evaluate thetranslation quality of mt systems.
we did notlist variants of the same metric: e.g., chrf3and chrf++ are labeled chrf (popovi´c, 2015).
moreover, we did not consider metrics whichonly target speciﬁc aspects of the translationquality, such as pronoun translation and rareword translation..a2.
whether a human evaluation of the translationquality has been conducted: yes or no.
if thehuman evaluation only targets speciﬁc typesof errors and did not evaluate the translationquality of the entire text, we answered “no.”4.
a3.
whether any kind of statistical signiﬁcancetesting of the difference between automaticmetric scores has been performed: yes or no.
potentially, some papers did perform signiﬁ-cance testing without mentioning it, but dueto the lack of evidences such papers have beenannotated with “no” for this question..1we considered only *acl main conferences, namelyacl, naacl, eacl, emnlp, conll, and aacl, as theyare the primary venues for publishing mt papers..2www.aclweb.org/anthology/3there are potentially mt papers falling outside thesesearch criteria but we considered the 769 papers we obtainedto be representative enough for the purpose of this study..4note that we only check here whether the automatic eval-uation is supported by a human evaluation.
previous workalready studied pitfalls in human evaluation (l¨aubli et al.,2020)..a5.
whether sacrebleu has been used: yes orno.
if there is no mention or reference to“sacrebleu,” we assume that it has not beenused.
note that “yes” does not mean that thepaper used sacrebleu for all the mt systemsevaluated..a6.
if previous work has not been reproduced butcopied, whether it has been conﬁrmed that allthe compared mt systems used exactly thesame pre-processed training, validating, andtesting data: yes or no..except for a6, the annotation was straightforwardsince most papers present a dedicated section forexperimental settings with most of the informa-tion we searched for.
answering a6 required tocheck the data exploited in the previous work usedfor comparison.
note that answering “yes” to thequestions from a2 to a6 may only be true for atleast one of the comparisons between mt systems,while we did not evaluate how well it applies.
forinstance, answering “yes” to a5 only means thatat least one of the systems has been evaluated withsacrebleu but not that the sacrebleu signaturehas been reported nor that sacrebleu scores havebeen correctly compared with other bleu scoresalso computed with sacrebleu..our annotations are available as a supplementalmaterial of this paper.
to keep track of the evolu-tion of mt evaluation, we will periodically updatethe annotations and will make it available online.5.
3 pitfalls and concerning trends.
this section discusses the four main pitfalls identi-ﬁed in our meta-evaluation of mt: the exclusive useof bleu, the absence of statistical signiﬁcance test-ing, the comparison of incomparable results fromprevious work, and the reliance on comparison be-tween mt systems that do not exploit exactly thesame data.
we report on how often they affectedmt papers and recent trends.
based on previous.
5the up-to-date version can be found here: github..com/benjamin-marie/meta_evaluation_mt..7298figure 1: percentage of papers using each evaluation metric per year.
metrics displayed are used in more thanﬁve papers.
“other” denotes all other automatic metrics.
“human” denotes that a human evaluation has beenconducted..work and supporting experiments, we show howeach of these problems and their accumulation leadto scientiﬁcally dubious mt evaluation..3.1 the 99% bleu.
automatic metrics for evaluating translation qual-ity have numerous advantages over a human eval-uation.
they are very fast and virtually free torun provided that a reference translation is alreadyavailable.
their scores are also reproducible.
assuch, automatic metrics remained at the center ofmt evaluation for the past two decades.
new met-rics that better correlate with human judgments areregularly introduced.
we propose in this sectionto analyze the use of automatic metrics in mt re-search, relying on our annotations for a1 and a2.
this is probably the most expected ﬁnding in ourstudy: the overwhelming majority of mt publica-tions uses bleu.
precisely, 98.8% of the annotatedpapers report on bleu scores.
as shown in fig-ure 1, the ratio of papers using bleu remainedstable over the years.
on the other hand, bleuscores used to be more often supported by scoresfrom other metrics, such as ter (snover et al.,2006) and meteor (banerjee and lavie, 2005),than they are now.
the large majority of papers,74.3%, only used bleu scores to evaluate mt sys-tems, i.e., without the support of any other metricsnor human evaluation.
it increases to 82.1% if weconsider only the years 2019 and 2020..this tendency looks surprising considering thatno less than 108 new metrics6 have been proposedin the last decade.
they have been shown to bet-ter correlate with human judgments than bleu.
some are even easier to use and more reproducible.
by being tokenization agnostic, such as chrf.
wecounted 29 metrics proposed at *acl conferencessince 2010 while the remaining metrics were pro-posed at the wmt metrics shared tasks.
89%of these 108 new metrics have never been used inan *acl publication on mt (except in the papersproposing the metrics).
among these metrics, onlyribes (isozaki et al., 2010) and chrf have beenused in more than two mt research paper..when properly used, bleu is a valid met-ric for evaluating translation quality of mt sys-tems (callison-burch et al., 2006; reiter, 2018).
nonetheless, we argue that better metrics proposedby the research community should be used to im-prove mt evaluation.
to illustrate how wrong anevaluation can become by only relying on one met-ric, we computed with bleu and chrf scores7 ofwmt20 submissions to the news translation sharedtask8 (barrault et al., 2020) using sacrebleu andshow rankings given by both metrics in table 1.results show that bleu and chrf produce two dif-ferent rankings.
for instance, for the ja→en task,niutrans system is the best according to bleu bybeing 1.1 points better than the tohoku-aip-nttsystem ranked second.
in most mt papers, sucha difference in bleu points would be consideredas a signiﬁcant evidence of the superiority of anmt system and as an improvement in translationquality.
relying only on these bleu scores with-out any statistical signiﬁcance testing nor humanevaluation would thus lead to the conclusion thatniutrans system is the best.
however, accordingto another metric that better correlates with human.
7sacrebleu (short).
chrf2+l.
{ja-en,zh-en}+n.6+s.false+t.wmt20+v.1.5.0 and bleu+c.mixed+l.
{ja-en,zh-en}+#.1+s.exp+t.wmt20+tok.13a+v.1.5.0.
signatures:.
6we did not count variants of the same metric and excluded.
8data.statmt.org/wmt20/.
metrics only proposed for an evaluation at segment level..translation-task/.
7299 0 10 20 30 40 50 60 70 80 90 10020102011201220132014201520162017201820192020% publicationsbleutermeteorribesnistchrfotherhumanrank.
12345.bleu.
(cid:7).
26.6♠25.524.822.822.2.
(cid:7).
(cid:7).
japanse-to-english (ja→en)system.
chrf.
system.
bleu.
system.
system.
chinese-to-english (zh→en).
niutranstohoku-aip-nttopponict kyotoetranslation.
0.5360.5350.5230.5070.504.
(cid:7).
(cid:7).
(cid:7).
tohoku-aip-nttniutransoppoonline-aonline-b.
36.936.836.636.635.9.
(cid:7).
wechat aitencent translationdidi nlpvolctransthunlp.
chrf.
0.6530.6480.6450.6440.643.
(cid:7).
(cid:7).
(cid:7).
(cid:7).
volctranstencent translationdidi nlpdeepmindthunlp.
table 1: rankings of wmt20 top 5 submissions for the news translation shared tasks according to bleu andchrf scores.
superscripts indicate systems that are signiﬁcantly worse ((cid:7)) and better (♠) according to each metric(p-value < 0.05) than tohoku-aip-ntt and volctrans systems for ja→en and zh→en, respectively..judgment, i.e., chrf, this does not hold: tohoku-aip-ntt system is better.
similar observationsare made for the zh→en task.9 these observa-tions have often been made by the mt community,for instance at wmt shared tasks, but nonethelessrarely seen in research papers..we assume that mt researchers largely ignorenew metrics in their research papers for the sakeof some comparability with previous work or sim-ply because differences between bleu scores mayseem more meaningful or easier to interpret thandifferences between scores of a rarely used met-ric.
most papers even qualify differences betweenbleu scores as “small,” “large,” or “signiﬁcant”(not necessarily statistically), implying that thereis a scientiﬁc consensus on the meaning of dif-ferences between bleu scores.
as we show inthe following sections, all these considerations areillusory.
moreover, bleu may also be directly re-quested by reviewers, or even worse, other metricsmay be requested to be dropped.10 we believe thatthe exclusive reliance on bleu can be ended andthe use of better metrics should be encouraged, inaddition to or in lieu of bleu, by the adoptionof a guideline for automatic mt evaluation (seesection 4)..3.2 the disappearing statistical signiﬁcance.
testing.
statistical signiﬁcance testing is a standard method-ology designed to ensure that experimental resultsare not coincidental.
in mt, statistical signiﬁcancetesting has been used on automatic metric scoresand more particularly to assess whether a partic-ular difference of metric scores between two mt.
9for both ja→en and zh→en tasks, systems ranked ﬁrst.
by chrf were also ranked ﬁrst by the human evaluation..10examples of such requests or related comments byreviewers can be found in the acl 2017 review corpus(github.com/allenai/peerread), e.g., in the reviewid 369 we read: “i am also rather suspicious of the fact thatthe authors present only meteor results and no bleu.”.
figure 2: percentage of papers testing statistical signif-icance of differences between metric scores..systems is not coincidental.
two methods are preva-lent in mt: the paired bootstrap test (koehn, 2004)and the approximate randomization test (riezlerand maxwell, 2005), for instance respectively im-plemented in moses11 and multeval.12.
dror et al.
(2018) report that while the naturallanguage processing (nlp) community assigns agreat value to experimental results, statistical sig-niﬁcance testing is rarely used.
we veriﬁed if thisapplies to mt evaluations based on our annotationsfor a3.
figure 2 shows the percentage of papersthat performed statistical signiﬁcance testing.
wefound out that the observations by dror et al.
(2018)apply to mt since never more than 65.0% of thepublications in a year (2011) performed statisti-cal signiﬁcance testing.
furthermore, our meta-evaluation shows a sharp decrease of its use since2016. most papers did not check whether theirresults are not coincidental but drew conclusionsfrom them..mt papers mainly relied on the amplitude of thedifferences between metric scores to state whetherthey are signiﬁcant or not.
this was also observedby dror et al.
(2018) for nlp in general..for illustration, we also performed statisticalsigniﬁcance testing13 with bleu and chrf scores.
11github.com/moses-smt/mosesdecoder12github.com/jhclark/multeval13for all the statistical signiﬁcance testing performed in thispaper, we used the paired bootstrap test with 1,000 samplesand 1,000 iterations..7300 0 10 20 30 40 50 60 7020102011201220132014201520162017201820192020% publicationsja→en.
system.
bleu.
chrf.
system.
tohoku-aip-ntt.
0.536.volctrans.
custom 1custom 2.
0.5360.503.custom 1custom 2.
25.5.
25.518.7.zh→enbleu.
36.6.
36.632.2.chrf.
0.653.
0.6530.638.table 2: bleu and chrf scores of the customizedtohoku-aip-ntt and volctrans outputs from whichonly one sentence has been modiﬁed.
the ﬁrst rowshows the results of the original wmt20 submissions.
custom 1 replaced the last sentence with an empty line,while custom 2 replaced the last sentence with a se-quence repeating 10k times the same token.
none ofthese systems are signiﬁcantly different according tostatistical signiﬁcance testing on these scores..on the wmt20 submissions in table 1. for ja→en,niutrans system is signiﬁcantly better in bleuthan tohoku-aip-ntt system.
in contrast, theyare not signiﬁcantly different in chrf.
using onlybleu, we would conclude that niutrans systemis signiﬁcantly the best.
this is not conﬁrmed bychrf hence we need to report on more than onemetric score to conduct a credible evaluation, evenwhen performing statistical signiﬁcance testing..furthermore, to show that the signiﬁcance of adifference between metric scores is independentfrom its amplitude, we performed additional exper-iments by modifying only one sentence, replacingit with an empty line or by the repetition of thesame token many times,14 from tohoku-aip-nttand volctrans systems’ outputs.
results in bleuand chrf are reported in table 2. we observe thata difference in only one sentence can lead to a dif-ference in bleu of 6.8 points (ja→en, custom2).15 nonetheless, our statistical signiﬁcance testsdid not ﬁnd any system signiﬁcantly better than theothers..while the importance of statistical signiﬁcancetesting is regularly debated by the scientiﬁc com-munity (wasserstein et al., 2019), it remains oneof the most cost-effective tools to check how trust-worthy a particular difference between two metricscores is.16.
14this could be considered as a simulation of potential de-fects from an mt framework or model, e.g., when translatingextremely long sequences..15for “custom 2,” bleu greatly penalized the increase ofthe number of tokens in the output.
this is indicated by thelength ratio reported by sacrebleu but rarely shown in mtpapers..16wasserstein et al.
(2019) give several recommendations.
for a better use of statistical signiﬁcance testing..figure 3: percentage of papers copying scores fromprevious work (“copied scores”), using sacrebleu(“sacrebleu”), and copying scores without usingsacrebleu (“copied w/o sacrebleu”)..3.3 the copied results.
an mt paper may compare the automatic metricscores of proposed mt systems with the scoresreported in previous work.
this practice has theadvantage to save the time and cost of reproducingcompeting methods.
based on our annotations fora4, we counted how often papers copied the scoresfrom previous work to compare them with theirown scores.
as pointed out by figure 3, copy-ing scores (mostly bleu) from previous workwas rarely done before 2015. in 2019 and 2020,nearly 40% of the papers reported on comparisonswith scores from other papers.
while many pa-pers copied and compared metric scores acrosspapers, it is often unclear whether they are actu-ally comparable.
as demonstrated by post (2018),bleu, as for most metrics, is not a single metric.
it requires several parameters and is dependent onthe pre-processing of the mt output and referencetranslation used for scoring.
in fact, post (2018)pointed out that most papers do not provide enoughinformation to enable the comparability of theirscores with other work.
post (2018) proposed atool, sacrebleu, to standardize metrics17 in or-der to guarantee this comparability, provided thatall the scores compared are computed with sacre-bleu.18 this is the only tool of this kind used bythe papers we annotated.
however, based on our an-notations for a5, figure 3 shows that sacrebleuis still far from widely adopted by the mt com-munity, even though it is gradually getting morepopular since its emergence in 2018. moreover,papers that copy bleu scores do not always usesacrebleu, even in 2020..to illustrate how deceiving a comparison ofcopied scores can be, we report on bleu and chrfscores using different processing,19 commonly.
17currently bleu, chrf, and ter.
18sacrebleu also generates a “signature” to further ensurethis comparability: two scores computed through sacrebleuwith an identical signature are comparable..19for all our processing, we used moses (code version mmt-.
7301 0 10 20 30 4020102011201220132014201520162017201820192020% publicationscopied scoressacrebleucopied w/o sacrebleuprocessing.
originalfully lowercasednorm.
punct..tokenized+ norm.
punct.
+ aggressive.
25.526.925.5.
26.726.827.8.tohoku-aip-ntt (ja→en)bleu.
chrf.
volctrans (zh→en)bleu.
chrf.
0.5360.5490.537.
0.5410.5410.541.
36.638.237.8.
37.138.539.5.
0.6530.6640.657.
0.6530.6590.659.table 3: bleu and chrf scores computed by sacre-bleu after applying different processing on somewmt20 mt system outputs (from tohoku-aip-nttand volctrans) and on the reference translations.
noneof these rows are comparable..adopted by mt researchers, applied to some mtsystem outputs and reference translations of thewmt20 news translation shared tasks.
our resultsare presented in table 3. the ﬁrst row presents orig-inal sacrebleu scores, i.e., detokenized.
secondand third rows respectively show the impact of low-ercasing and punctuation normalization on metricscores.
scores are increased.
last three rows showthe results on tokenized mt outputs.
applyingboth punctuation normalization and aggressive tok-enization with moses scripts leads to bleu scoresseveral points higher than the original sacrebleuscores.
obviously, none of the scores in differentrows are comparable.
nonetheless, mt papers stilloften report on tokenized bleu scores comparedwith tokenized, or even detokenized, bleu scoresfrom other papers without exactly knowing how to-kenization has been performed.
tokenized bleuscores reported in mt papers are often computedusing the multi-bleu script of moses even thoughit displays the following warning:20 “the scoresdepend on your tokenizer, which is unlikely to bereproducible from your paper or consistent acrossresearch groups.”.
even though the work of post (2018) is a well-acclaimed initiative towards better mt evaluation,we believe that it can only be a patch for ques-tionable evaluation practices.
a comparison witha copied score is de facto associated with the ab-sence of statistical signiﬁcance testing since themt output used to compute the copied score isnot available.
we also observed several misusesof sacrebleu, such as the comparison of scoresobtained by sacrebleu against scores obtained by.
mvp-v0.12.1-2851-gc054501) scripts..20this warning has been added on 20 oct. 2017. insightful.
discussions on this commit can be found there:github.com/moses-smt/mosesdecoder/commit/545eee7e75487aeaf45a8b077c57e189e50b2c2e..other tools.
sacrebleu signatures are also oftennot reported despite being required to ensure thecomparability between sacrebleu scores..ultimately, comparisons with copied scores mustbe avoided.
as we will show in the next section,copying scores also calls for more pitfalls..3.4 the data approximation.
in mt, datasets are mostly monolingual or paralleltexts used in three different steps of an experiment:training a translation model, tuning/validating themodel, and evaluating it.
henceforth, we denotethese datasets as training, validating, and testingdata, respective to these three steps.
how thesedatasets are pre-processed strongly inﬂuences trans-lation quality.
mt papers regularly propose newmethods or algorithms that aim at better exploit-ing training and/or validating data.
following thescientiﬁc method, we can then deﬁne these newmethods/algorithms and datasets as independentvariables of an mt experiment while the transla-tion quality, approximated by metric scores, wouldbe our dependent variable that we want to mea-sure.
testing the impact of a new algorithm onour dependent variable requires to keep all otherindependent variables, such as datasets, unchanged.
in other words, changing datasets (even slightly)and methods/algorithms in the same experimentcannot answer whether the change in metric scoresis due to the datasets, methods/algorithms, or thecombination of both..relying on our annotation for a6, we examinedhow often mt papers compared mt systems forwhich the datasets and/or their pre-processing21 de-scribed in the papers are not exactly identical.
notethat we only performed this comparison for pa-pers that copied and compared metric scores fromprevious work.
here, we also excluded compar-isons between systems performed to speciﬁcallyevaluate the impact of new datasets, pre-processingmethods, and human intervention or feedback (e.g.,post-editing and interactive mt).
if we had anydoubt whether a paper belongs or not to this cate-gory, we excluded it.
consequently, our estimationcan be considered as the lower bound..to illustrate the impact of modiﬁcations of thesedatasets on metric scores, we conducted experi-ments using the training, validating, and testingdata of the wmt20 news translation tasks.
we.
21for pre-processing, we checked, for instance, tokeniza-tion (framework and parameters), casing, subword segmenta-tions (method and vocabulary size), data ﬁltering, etc..7302--type transformer --mini-batch-fit --valid-freq5000 --save-freq 5000 --workspace 10000--disp-freq 500 --beam-size 12 --normalize1 --valid-mini-batch 16 --overwrite--early-stopping 5 --cost-type ce-mean-words--valid-metrics bleu --keep-best --enc-depth6 --dec-depth 6 --transformer-dropout0.1 --learn-rate 0.0003 --lr-warmup 16000--lr-decay-inv-sqrt 16000 --lr-report--label-smoothing 0.1 --devices 0 1 2 3 4 5 67 --optimizer-params 0.9 0.98 1e-09 --clip-norm5 --sync-sgd --exponential-smoothing --seed 1234.data.
all.
allallallall.
maxlen..tc.
120 (cid:88).
en→de.
ja→en.
bleu.
chrf.
bleu.
chrf.
30.9.
0.599.
20.4.
0.478.
120100 (cid:88)8060.
31.5♠30.8(cid:7)(cid:88) 29.8(cid:88) 26.6.
(cid:7).
lid ﬁltered.
120 (cid:88) 30.3.
(cid:7).
- 1 corpus.
120 (cid:88).
30.7.
0.604♠0.597(cid:7)0.5840.549.
(cid:7).
0.596.
0.600.
21.1♠20.520.0(cid:7)18.5.
20.7.
(cid:7).
19.6.
0.4810.476(cid:7)0.4710.453.
(cid:7).
0.480.
(cid:7).
0.468.table 4: hyper-parameters of marian used for trainingour nmt systems..table 5: bleu and chrf scores of systems using dif-ferently pre-processed parallel data for training.
“maxlen.” denotes that sentence pairs with sentence longerthan the speciﬁed number, in terms of subword tokens,are removed.
“tc” denotes whether truecasing is doneor not.
if not, original case of the data is kept for alldatasets.
“lid ﬁltered” denotes that the training data areﬁltered with language identiﬁcation tools.
last row de-notes that we remove one corpus from the training data:“rapid” for en→de and “opensubtitles” for ja→en.
(cid:7) and ♠ respectively denote systems that are signiﬁ-cantly worse and better (p-value < 0.05), according tothe metric, than the system in the ﬁrst row..our meta-evaluation an increasing amount of mtpapers (38.5% for the 2019–2020 period) draw-ing conclusions of the superiority of a particularmethod or algorithm while also using different data.
while their conclusions may be valid, the evalu-ation conducted in these papers is scientiﬁcallyﬂawed and cannot support the conclusions.
weassume that this is mainly due to a rather com-mon lack of detailed experimental settings.
con-sequently, it makes a speciﬁc experiment oftenimpossible to be reproduced identically.
in mostcases, ensuring the comparability with the pub-lished scores of an mt system is only possible byreplicating the mt system by ourselves.
therehave been initiatives towards the release of pre-processed datasets for mt, for instance by thewmt conference that released pre-processed datafor wmt19.23 nonetheless, we only noticed a verysmall number of papers exploiting pre-processedtraining/validating/testing data publicly released byprevious work.24 we believe that the current trendshould be reversed.
reviewers should also requestmore rigor to the authors by checking the conﬁgu-rations of the compared mt systems to make surethat their comparison can, indeed, answer whetherthe proposed method/algorithm improves mt inde-.
23this effort has not been conducted for wmt20.
24for instance, ma et al.
(2020) and kang et al.
(2020) usedexactly the same pre-processed data for research on document-level nmt released by maruf et al.
(2019)..figure 4: percentage of papers that compared mt sys-tems using data that are not identical..trained neural mt (nmt) systems with marian22(junczys-dowmunt et al., 2018), using the hyper-parameters in table 4, on all the provided paralleldata (“all” conﬁgurations) and removed sentencepairs based on their length (“max len.”).
thissimple ﬁltering step is usually applied for a moreefﬁcient training or due to some limits of the frame-work, method, or algorithm used.
yet, it is socommon as a pre-processing step that it is rarelydescribed in papers.
as shown in table 5, we ob-served that bleu scores vary by several pointsdepending on the maximum length used for ﬁl-tering.
another common pre-processing step isthe truecasing of the datasets.
while it is rathercommonly performed by participants in the wmttranslation shared tasks, how casing is handled israrely mentioned in research papers.
in our experi-ments, applying this step changed bleu scores bymore than 0.5 points.
further experiments applyinglanguage identiﬁcation ﬁltering or removing onecorpus from the training data also lead to variationsin metric scores.
the best conﬁgurations accordingto metric scores do not use truecasing and has amaximum sentence length set at 120 (second row).
a comparison of this conﬁguration with the thirdrow, which uses truecasing and a different maxi-mum sentence length, cannot lead to the conclusionthat truecasing decreases translation quality, sincewe changed two variables at the same time..while these observations may be expected oreven obvious, figure 4 shows that we found in.
22version: v1.7.6 1d4ba73 2019-05-11 17:16:31 +0100.
7303 0 10 20 30 4020102011201220132014201520162017201820192020% publicationsa better evaluation but not a ﬂawless evaluation..4.2 the guideline.
this guideline and the scoring method that followsare proposed for mt papers that rely on automaticmetric scores for evaluating translation quality..1. an mt evaluation may not exclusively relyon bleu.
other automatic metrics that bettercorrelate with human judgments, or a humanevaluation, may be used in addition or in lieuof bleu..2. statistical signiﬁcance testing may be per-formed on automatic metric scores to ensurethat the difference between two scores, what-ever its amplitude, is not coincidental..3. automatic metric scores copied from previ-ous work may not be compared.
if inevitable,copied scores may only be compared withscores computed in exactly the same way,through tools guaranteeing this comparability,while providing all the necessary informationto reproduce them..4. comparisons between mt systems throughtheir metric scores may be performed todemonstrate the superiority of a method oran algorithm only if the systems have beentrained, validated, and tested with exactly thesame pre-processed data, unless the proposedmethod or algorithm is indeed dependent on aparticular dataset or pre-processing..the purpose of the following scoring methodis to assess the trustworthiness of an automaticevaluation performed in an mt paper.
ultimately,it can be used for authors’ self-assessment or bymt program committees to identify trustworthypapers..each “yes” answer to the following questionsbrings 1 point to the paper for a maximum of 4points..1. is a metric that better correlates with humanjudgment than bleu used or is a human eval-uation performed?.
2. is statistical signiﬁcance testing performed?.
3. are the automatic metric scores computed forthe paper and not copied from other work?
if copied, are all the copied and comparedscores computed through tools that guaranteetheir comparability (e.g., sacrebleu)?.
figure 5: percentage of papers affected by the accu-mulation of pitfalls.
each bar considers only the paperscounted by the previous bar, e.g., the last bar considersonly papers that compared mt systems exploiting dif-ferent datasets while exclusively using bleu (“bleuonly”), without performing statistical signiﬁcance test-ing (“w/o sigtest”), to measure differences with bleuscores copied from other papers (“w/ copied scores”)while not using sacrebleu (“w/o sacrebleu”)..pendently of the data and their pre-processing..4 a guideline for mt meta-evaluation.
4.1 motivation.
the mt community is well-aware of all the pitfallsdescribed in section 3. they have all been de-scribed by previous work.
nonetheless, our meta-evaluation shows that most mt publications areaffected by at least one of these pitfalls.
more puz-zling are the trends we observed.
figure 5 showsthat an increasing number of publications accu-mulate questionable evaluation practices.
in theperiod 2019–2020, 17.4% (38 papers) of the anno-tated papers exclusively relied for their evaluationon differences between bleu scores of mt sys-tems, of which at least some have been copiedfrom different papers, without using sacrebleunor statistical signiﬁcance testing, while exploitingdifferent datasets..while these pitfalls are known and relativelyeasy to avoid, they are increasingly ignored andaccumulated.
we believe that a clear, simple, andwell-promoted guideline must be deﬁned for au-tomatic mt evaluation.
such a guideline wouldbe useful only if it is adopted by authors and itsapplication is checked by reviewers.
for the latter,we also propose a simple scoring method for themeta-evaluation of mt..note that the proposed guideline and scoringmethod only cover the aspects discussed in this pa-per.
thus, their strict adherence can only guarantee.
7304 0 10 20 30 40 50 60 70 80 90 10020102011201220132014201520162017201820192020% publicationsbleu only w/o sigtest  w copied scores   w/o sacrebleu    w/ diff.
dataresearchers to perform a similar meta-evaluation intheir respective area of expertise.
as we showed, itcan unveil pitfalls and concerning trends that canbe reversed before becoming prevalent..acknowledgments.
we would like to thank the reviewers for their in-sightful comments and suggestions.
this work waspartly supported by jsps kakenhi grant num-bers 20k19879 and 19h05660..references.
satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, usa.
association for computational linguis-tics..lo¨ıc barrault, magdalena biesialska, ondˇrej bojar,marta r. costa-juss`a, christian federmann, yvettegraham, roman grundkiewicz, barry haddow,matthias huck, eric joanis, tom kocmi, philippkoehn, chi-kiu lo, nikola ljubeˇsi´c, christofmonz, makoto morishita, masaaki nagata, toshi-aki nakazawa, santanu pal, matt post, and mar-cos zampieri.
2020. findings of the 2020 confer-in pro-ence on machine translation (wmt20).
ceedings of the fifth conference on machine trans-lation, pages 1–55, online.
association for compu-tational linguistics..chris callison-burch, miles osborne, and philippkoehn.
2006. re-evaluating the role of bleu in ma-in proceedings of thechine translation research.
11th conference of the european chapter of theassociation for computational linguistics, trento,italy.
association for computational linguistics..michael denkowski and graham neubig.
2017.stronger baselines for trustable results in neural ma-chine translation.
in proceedings of the first work-shop on neural machine translation, pages 18–27,vancouver, canada.
association for computationallinguistics..rotem dror, gili baumer, segev shlomov, and roi re-ichart.
2018. the hitchhiker’s guide to testing statis-tical signiﬁcance in natural language processing.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1383–1392, melbourne, aus-tralia.
association for computational linguistics..hideki isozaki, tsutomu hirao, kevin duh, katsuhitosudoh, and hajime tsukada.
2010. automatic eval-uation of translation quality for distant languagein proceedings of the 2010 conference onpairs..figure 6: average meta-evaluation score per year..4. if comparisons between mt systems are per-formed to demonstrate the superiority of amethod or an algorithm that is independentfrom the datasets exploited and their pre-processing, are all the compared mt systemsexploiting exactly the same pre-processeddata for training, validating, and testing?
(ifnot applicable, give 1 point by default).
we scored all the annotated papers, and reporton the average score and score distribution for eachyear in figure 6. based on this meta-evaluation,mt evaluation worsens..5 conclusion.
our meta-evaluation identiﬁed pitfalls in the mtevaluation in most of the annotated papers.
theaccumulation of these pitfalls and the concerningtrends we observed lead us to propose a guidelinefor automatic mt evaluation.
we hope this guide-line, or a similar one, will be adopted by the mtcommunity to enhance the scientiﬁc credibility ofmt research..this work also has its limitations since it doesnot cover all the pitfalls of mt evaluation.
forinstance, we noticed that mt papers regularly relyon the same language pairs to claim general im-provements of mt.
they also almost exclusivelyfocus on translation from or into english.
another,more positive observation, is that mt papers tendto use stronger baseline systems, following someof the recommendations by denkowski and neu-big (2017), than at the beginning of the last decadewhen baseline systems were mostly vanilla mtsystems.
for future work, we plan to extend ourmeta-evaluation of mt to publications at confer-ences in other research domains, such as machinelearning and artiﬁcial intelligence..as a ﬁnal note, we would like to encourage nlp.
7305 0 10 20 30 40 50 60 70 80 90 100201020112012201320142015201620172018201920201.51.61.71.81.92.02.12.22.32.42.5% publicationspoints0 point1 point2 points3 points4 pointsmaja popovi´c.
2015. chrf: character n-gram f-scorefor automatic mt evaluation.
in proceedings of thetenth workshop on statistical machine translation,pages 392–395, lisbon, portugal.
association forcomputational linguistics..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..ehud reiter.
2018. a structured review of the validityof bleu.
computational linguistics, 44(3):393–401..stefan riezler and john t. maxwell.
2005. on somepitfalls in automatic evaluation and signiﬁcance test-ing for mt.
in proceedings of the acl workshopon intrinsic and extrinsic evaluation measures formachine translation and/or summarization, pages57–64, ann arbor, michigan.
association for com-putational linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of the 7th biennial conference of theassociation for machine translation in the ameri-cas, pages 223–231, cambridge, usa.
associationfor machine translation of the americas..ronald l. wasserstein, allen l. schirm, and nicole a.lazar.
2019. moving to a world beyond “p < 0.05”.
the american statistician, 73(sup1):1–19..empirical methods in natural language processing,pages 944–952, cambridge, usa.
association forcomputational linguistics..marcin junczys-dowmunt, roman grundkiewicz,tomasz dwojak, hieu hoang, kenneth heaﬁeld,tom neckermann, frank seide, ulrich germann,alham fikri aji, nikolay bogoychev, andr´e f. t.martins, and alexandra birch.
2018. marian: fastneural machine translation in c++.
in proceedingsof acl 2018, system demonstrations, pages 116–121, melbourne, australia.
association for compu-tational linguistics..xiaomian kang, yang zhao,.
jiajun zhang, andchengqing zong.
2020. dynamic context selectionfor document-level neural machine translation via re-in proceedings of the 2020inforcement learning.
conference on empirical methods in natural lan-guage processing (emnlp), pages 2242–2254, on-line.
association for computational linguistics..philipp koehn.
2004..statistical signiﬁcance testsin proceed-for machine translation evaluation.
ings of the 2004 conference on empirical meth-ods in natural language processing, pages 388–395, barcelona, spain.
association for computa-tional linguistics..samuel l¨aubli, sheila castilho, graham neubig, ricosennrich, qinlan shen, and antonio toral.
2020.a set of recommendations for assessing human–machine parity in language translation.
journal ofartiﬁcial intelligence research, 67:653–672..shuming ma, dongdong zhang, and ming zhou.
2020.a simple and effective uniﬁed encoder for document-in proceedings of thelevel machine translation.
58th annual meeting of the association for compu-tational linguistics, pages 3505–3511, online.
as-sociation for computational linguistics..sameen maruf, andr´e f. t. martins, and gholamrezahaffari.
2019. selective attention for context-awarein proceedings of theneural machine translation.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 3092–3102, minneapolis, usa.
association for computational linguistics..nitika mathur, timothy baldwin, and trevor cohn.
2020. tangled up in bleu: reevaluating the eval-uation of automatic machine translation evaluationin proceedings of the 58th annual meet-metrics.
ing of the association for computational linguistics,pages 4984–4997, online.
association for computa-tional linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,usa.
association for computational linguistics..7306