unleash gpt-2 power for event detection.
amir pouran ben veyseh1, viet dac lai1,franck dernoncourt2, and thien huu nguyen11 department of computer and information science, university of oregon,eugene, or 97403, usa2 adobe research, san jose, ca, usa{apouranb,vietl,thien}@cs.uoregon.edu,franck.dernoncourt@adobe.com.
abstract.
event detection (ed) aims to recognize men-tions of events (i.e., event triggers) and theirtypes in text.
recently, several ed datasetsin various domains have been proposed.
how-ever, the major limitation of these resources isthe lack of enough training data for individualevent types which hinders the efﬁcient train-ing of data-hungry deep learning models.
toovercome this issue, we propose to exploit thepowerful pre-trained language model gpt-2to generate training samples for ed.
to pre-vent the noises inevitable in automatically gen-erated data from hampering training process,we propose to exploit a teacher-student archi-tecture in which the teacher is supposed tolearn anchor knowledge from the original data.
the student is then trained on combinationof the original and gpt-generated data whilebeing led by the anchor knowledge from theteacher.
optimal transport is introduced to fa-cilitate the anchor knowledge-based guidancebetween the two networks.
we evaluate theproposed model on multiple ed benchmarkdatasets, gaining consistent improvement andestablishing state-of-the-art results for ed..1.introduction.
an important task of information extraction (ie)involves event detection (ed) whose goal is torecognize and classify words/phrases that evokeevents in text (i.e., event triggers).
for instance,in the sentence “the organization donated 2 mil-lion dollars to humanitarian helps.”, ed systemsshould recognize “donated” as an event trigger oftype pay.
we differentiate two subtasks in ed,i.e., event identiﬁcation (ei): a binary classiﬁca-tion problem to predict if a word in text is an eventtrigger or not, and event classiﬁcation (ec): amulti-class classiﬁcation problem to classify eventtriggers according to predeﬁned event types..several methods have been introduced for ed,.
extending from feature-based models (ahn, 2006;liao and grishman, 2010a; miwa et al., 2014) toadvanced deep learning methods (nguyen and gr-ishman, 2015; chen et al., 2015; nguyen et al.,2016c; sha et al., 2018; zhang et al., 2020b;nguyen et al., 2021).
although deep learning mod-els have achieved substantial improvement, theirrequirement of large training datasets together withthe small sizes of existing ed datasets constitutes amajor hurdle to build high-performing ed models.
recently, there have been some efforts to enlargetraining data for ed models by exploiting unsu-pervised (huang et al., 2016; yuan et al., 2018) ordistantly-supervised (keith et al., 2017; nguyenand nguyen, 2018; araki and mitamura, 2018)techniques.
the common strategy in these meth-ods is to exploit unlabeled text data that are rich inevent mentions to aid the expansion of training datafor ed.
in this work, we explore a novel approachfor training data expansion in ed by leveraging theexisting pre-trained language model gpt-2 (rad-ford et al., 2019) to automatically generate trainingdata for models.
motivated by the promising per-formance of gpt models for text generation, weexpect our approach to produce effective data fored in different domains..speciﬁcally, we aim to ﬁne-tune gpt-2 on ex-isting training datasets so it can generate new sen-tences annotated with event triggers and/or eventtypes, serving as additional training data for edmodels.
one direction to achieve this idea is to ex-plicitly mark event triggers along with their eventtypes in sentences of an existing ed dataset thatcan be used to ﬁne-tune the gpt model for newdata generation.
however, one issue with this di-rection is that in existing ed datasets, numbers ofexamples for some rare event types might be small,potentially leading to the poor tuning performanceof gpt and impairing the quality of generated ex-amples for such rare events.
in addition, large num-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6271–6282august1–6,2021.©2021associationforcomputationallinguistics6271bers of event types in some ed datasets might makeit more challenging for the ﬁne-tuning of gpt todifferentiate event types and produce high-qualitydata.
to this end, instead of directly generatingdata for ed, we propose to use gpt-2 to only gen-erate samples for the event identiﬁcation task tosimplify the generation and achieve data with bet-ter annotated labels (i.e., output sentences only areonly marked with positions of event triggers).
assuch, to effectively leverage the generated ei datato improve ed performance, we propose a multi-task learning framework to train the ed models onthe combination of the generated ei data and theoriginal ed data.
in particular, for every event trig-ger candidate in a sentence, our framework seeksto perform two tasks, i.e., ei to predict a binarylabel for being an event trigger or not, and ed topredict the event type (if any) evoked by the wordvia a multi-class classiﬁcation problem.
an inputencoder is shared for both tasks that allow trainingsignals from both generated ei data and originaled data to contribute to the representation learn-ing in the encoder (i.e., transferring knowledge ingenerated ei data to ed models)..despite the simpliﬁcation to ei for better anno-tated labels of data, the generated sentences mightstill involve noises due to the inherent nature of thelanguage generation, e.g., grammatically wrongsentences, inconsistent information, or incorrectevent trigger annotations.
as such, it is crucial tointroduce mechanisms to ﬁlter the noises in gener-ated data to enable effective transfer learning fromgenerated ei data.
to this end, prior works for gpt-based data generation for other tasks has attemptedto directly remove noisy generated examples beforeactual usage for model training via some heuris-tic rules (anaby-tavor et al., 2020; yang et al.,2020).
however, heuristic rules are brittle and re-stricted in their coverage so they might overly ﬁlterthe generated data or incorrectly retain some noisygenerated samples.
to address this issue, we pro-pose to preserve all generated data for training anddevise methods to explicitly limit impacts of noisygenerated sentences in the models.
in particular,we expect the inclusion of generated ei data intothe training process for ed models might help toshift the representations of the models to betterregions for ed.
as such, we argue that this repre-sentation transition should only occur at a reason-able rate as drastic divergence of representationsdue to the generated data might be associated with.
noises in the data.
motivated by this intuition, wepropose a novel teacher-student framework for ourmulti-task learning problem where the teacher istrained on the original clean ed datasets to induceanchor representation knowledge for data.
the stu-dent, on the other hand, will be trained on bothgenerated ei data and original ed data to accom-plish transfer learning.
here, the anchor knowledgefrom the teacher will be leveraged to guide the stu-dent to prevent drastic divergence of representationvectors for noisy information penalization.
conse-quently, we propose a novel anchor information toimplement this idea, seeking to maintain the samelevel of differences between the generated and orig-inal data (in terms of representation vectors) forboth the teacher and the student (i.e., generated-vs-original data difference as the anchor).
at the coreof this techniques involves the computation of dis-tance/difference between samples in generated andoriginal data.
in this work, we envision two typesof information that models should consider whencomputing such distances for our problem: (1) rep-resentation vectors of the models for the examples,and (2) event trigger likelihood scores of exam-ples based on the models (i.e., two examples in thegenerated and original data are more similar if theyboth correspond to event triggers).
as such, we pro-pose to cast this distance computation problem ofgenerated and original data into an optimal trans-port (ot) problem.
ot is an established methodto compute the optimal transportation between twodata distributions based on the probability massesof data points and their pair-wise distances, thus fa-cilitating the integration of the two criteria of eventtrigger likelihoods and representation vectors intothe distance computation between data point sets.
extensive experiments and analysis reveal theeffectiveness of the proposed approach for ed indifferent domains, establishing new state-of-the-art performance on the ace 2005, cyseced andrams datasets..2 model.
we formulate the task of event detection as aword-level classiﬁcation problem as in prior work(nguyen and grishman, 2015; ngo et al., 2020).
formally, given the sentence s = [w1, w2, .
.
.
, wn]and the candidate trigger word wt, the goal is to pre-dict the event type l from a pre-deﬁned set of eventtypes l. note that if the word wt is not a triggerword, the gold event type is n one.
our proposed.
6272approach for this task consist of two stages: (1)data augmentation: to employ natural languagegeneration to augment existing training datasets fored, (2) task modeling: to propose a deep learningmodel for ed, exploiting available training data..2.1 data augmentation.
as presented in the introduction, our motivation inthis work is to explore a novel approach for train-ing data augmentation for ed based on the power-ful pre-trained language model for text generationgpt2.
our overall strategy involves using someexisting training dataset o for ed (i.e., originaldata) to ﬁne-tune gpt-2.
the ﬁne-tuned model isthen employed to generate a new labeled trainingset g (i.e., synthetic data) that will be combinedwith the original data o to train models for ed..to simplify the training data generation task andenhance the quality of the synthetic data, we seek togenerate data only for the subtask ei of ed wheresynthesized sentences are annotated with positionsof their event triggers (i.e., event types for triggersare not required for the generation to avoid the com-plication with rare event types for ﬁne-tuning).
tothis end, we ﬁrst enrich each sentence s ∈ o withpositions of event triggers that it contains to facili-tate the gpt ﬁne-tuning process.
formally, assumethat s = w1, w2, .
.
.
, wn is a sentence of n wordswith only one event trigger word located at wt, theenriched sentence s(cid:48) for s would have the form:s(cid:48) = [bos, w1, .
.
.
, t rgs, wt, t rge, .
.
.
, wn,eos] where t rgs and t rge are special tokensto mark the position of the event trigger, and bosand eos are special tokens to identify the begin-ning and the end of the sentence.
next, the gpt-2model will be ﬁne-tuned on the enriched sentencess(cid:48) of o in an auto-regressive fashion (i.e., predict-ing the next token in s(cid:48) given prior ones).
finally,using the ﬁne-tuned gpt-2, we generate a newdataset g of |o| sentences (|g| = |o|) to achieve abalanced size.
here, we ensure that only generatedsentences that contain the special tokens t rgsand t rge (i.e., involving event trigger words) areadded into g, allowing us to identify the candi-date trigger word in our word-level classiﬁcationformulation for ed.
as such, the combination aof the synthetic data g and the original data o(a = o ∪ g) will be leveraged to train our edmodel in the next step..to assess the quality of the synthetic data, werandomly select 200 sentences from g (generated.
by the ﬁne-tuned gpt-2 model over the popularace 2005 training set for ed) and evaluate themregarding grammatical soundness, meaningfulness,and inclusion and correctness of annotated eventtriggers (i.e., whether the words between the tokenst rgs and t rge evoke events or not).
amongthe sampled set, we ﬁnd that 17% of the sentencescontains at least one type of such errors..2.2 task modeling.
this section describes our model for ed to over-come the noises in the generated data g for modeltraining.
as discussed in the introduction, we em-ploy the teacher-student framework with multi-task learning to achieve this goal.
in the proposedframework, the teacher and student employs a basedeep learning model with the same architecture anddifferent parameters.
base model: following the prior work (wanget al., 2019), our base model consists of thebertbase model to represent each word wi in theinput sentence s with a vector ei.
formally, theinput sentence [[cls], w1, w2, .
.
.
, wn, [sep ]] isfed into the bertbase model and the hidden statesof the last layer of bert are taken as the con-textualized embeddings of the input words, i.e.,e = [e1, e2, .
.
.
, en].
note that if wi contains morethan one word-piece, the average of its word-pieceembeddings is used for ei.
in our experiments, weﬁnd that ﬁxing the bertbase parameters achievehigher performance.
as such, to ﬁne-tune thecontextualized embeddings e for ed, we employa bi-directional long short-term memory (bil-stm) network to consumes e; its hidden states,i.e., h = [h1, h2, .
.
.
, hn], are then employed asthe ﬁnal representations for the words in s. fi-nally, to create the ﬁnal vector v for ed prediction,the max-pooled representation of the sentence, i.e.,¯h = m ax p ool(h1, h2, .
.
.
, hn), is concate-nated with the representation of the trigger candi-date, i.e., ht.
v is consumed by a feed-forward net-work, whose last layer has |l| neurons, followed bya softmax layer to predict the distribution p (·|s, t)over possible event types in l. to train the model,we use negative log-likelihood as the loss function:lpred = − log p (l|s, t) where l is the gold label.
as the synthetic sentences in g only involve in-formation about positions of event triggers (i.e., noevent types included), we cannot directly combineg with o to train ed models with the loss lpred.
to facilitate the integration of g into the training.
6273process, we introduce an auxiliary task of ei for themulti-task learning in the training process, seekingto predict the binary label laux for the trigger candi-date wt in s, i.e., laux = 1 if wt is an event trigger.
to perform this auxiliary task, we employ anotherfeed-forward network, i.e., ffaux, which also con-sumes the overall vector v as input.
this feed-forward network has one neuron with the sigmoidactivation function in the last layer to estimate theevent trigger likelihood score: p (laux = 1|s, t) =ffaux(v ).
finally, to train the base model withthe auxiliary task, we exploit the binary cross-entropy loss: laux = −(laux log(ffaux(v )) +(1 − laux) log(1 − ffaux(v ))).
note that the mained task and the auxiliary ei task are done jointlyin a single training process where the loss lpred fored is computed only for the original data o. theloss laux, in contrast, will be obtained for bothoriginal and synthetic data in a..knowledge consistency: the generated data g isnot noise-free.
as such, training the ed model ona could lead to inferior performance.
to addressthis issue, as discussed in the introduction, we pro-pose to ﬁrst learn the anchor knowledge from theoriginal data o, then use that to lead the modeltraining on a to prevent drastic divergence fromthe anchor knowledge (i.e., knowledge consistencypromotion), thus constraining the noises.
hence,we propose a teacher-student network, in whichthe teacher is ﬁrst trained on o to learn the anchorknowledge.
the student network will be trainedon a afterward leveraging the consistency guid-ance with the induced anchor knowledge from theteacher.
we will also use the student network asthe ﬁnal model for our ed problem in this work..in our framework, both teacher and student net-works will be trained in the multi-task setting withed and ei tasks.
in particular, the training lossesfor both ed and ei will be computed based ono for the teacher (the loss to train the teacher is:lpred + τ laux where τ is a trade-off parameter).
in contrast, the combined data a will be used tocompute the ei loss for the student while the edloss for the student can only be computed on theoriginal data o. as such, we propose to enforcethe knowledge consistency between the two net-works for both the main task ed and the auxiliarytask ei during the training of the student model.
first, to achieve the knowledge consistency fored, we seek to minimize the kl divergence be-tween the teacher-predicted label-probability distri-.
bution and the student-predicted label-probabilitydistributions.
formally, for a sentence s ∈ o, thelabel-probability distributions of the teacher andthe student, i.e., pt(·|s, t) and ps(·|s, t) respec-tively, are employed to compute the kl-divergenceloss lkl = −σl∈lpt(l|s, t) log( pt(l|s,t)ps(l|s,t) ).
by de-creasing the kl-divergence during the student’straining, the model is encouraged to make simi-lar predictions as the teacher for the same originalsentence, thereby preventing noises to mislead thestudent.
note that different from traditional teacher-student networks that employ kl to achieve knowl-edge distillation on unlabelled data (hinton et al.,2015), the kl divergence in our model is leveragedto enforce knowledge consistency to prevent noisesin labeled data automatically generated by gpt-2.
second, for the auxiliary task ei, instead of en-forcing the student-teacher knowledge consistencyvia similarity predictions, we argue that it will bemore beneﬁcial to leverage the difference betweenthe original data o and the generated data g as ananchor knowledge to promote consistency.
in par-ticular, we expect that the student which is trainedon a, should discern the same difference betweeng and o as the teacher which is trained only onthe original data o. formally, during student train-ing, for each mini-batch, the distances between theoriginal data and the generated data detected by theteacher and the student are denoted by dto,g anddso,g, respectively.
to enforce the o-g distanceconsistency between the two networks, the follow-ing loss is added into the overall loss function:.
|dt.
o,g −ds|b|.
o,g |.
ldist =, where |b| is the mini-batchsize.
the advantage of this novel knowledge consis-tency enforcement compared to the kl-divergenceis that it explicitly exploits the different nature ofthe original and generated data to facilitate the mit-igation of noises in the generated data..a remaining question for our proposed knowl-edge consistency concerns how to assess the differ-ence between the original and the generated datafrom the perspective of the teacher, i.e., dto,g, andthe student networks, i.e., dso,g.
in this section,we will describe our method from the perspectiveof the student (the same method is employed forthe teacher network).
in particular, we deﬁne thedifference between the original and the generateddata as the cost of transforming o to g such thatfor the transformed data the model will make thesame predictions as g. how can we compute thecost of such transformation?
to answer this ques-.
6274tion, we propose to employ optimal transport (ot)which is an established method to ﬁnd the efﬁcienttransportation (i.e., transformation with the lowestcost) of one probability distribution to another one.
formally, given the probability distributions p(x)and q(y) over the domains x and y, and the costfunction c(x, y) : x × y → r+ for mapping x toy, ot ﬁnds the optimal joint distribution π∗(x, y)(over x × y) with marginals p(x) and q(y), i.e.,the cheapest transportation from p(x) to q(y), bysolving the following problem:.
π∗(x, y) = min.
π(x, y)c(x, y)dxdy.
(cid:90).
(cid:90).
π∈π(x,y).
xs.t.
x ∼ p(x) and y ∼ q(y),.
y.
(1).
where π(x, y) is the set of all joint distributionswith marginals p(x) and q(y).
note that if thedistributions p(x) and q(y) are discrete, the inte-grals in equation 1 are replaced with a sum andthe joint distribution π∗(x, y) is represented bya matrix whose entry (x, y) represents the prob-ability of transforming the data point x ∈ x toy ∈ y to convert the distribution p(x) to q(y).
bysolving the problem in equation 11, the cost oftransforming the discrete distribution p(x) to q(y)(i.e., wasserstein distance distw ) is deﬁned as:distw = σx∈x σy∈y π∗(x, y)c(x, y)..in order to utilize ot to compute the transforma-tion cost between o and g, i.e., dso,g, we proposeto deﬁne the domain x and y as the representa-tion spaces of the sentences in o and g, respec-tively, obtained from the student network.
in par-ticular, a data point x ∈ x represents a sentencexo ∈ o. similarly, a data point y ∈ y standsfor a sentence yg ∈ g. to deﬁne the cost functionc(x, y) for ot, we compute the euclidean distancebetween the representation vectors of the sentencesxo and yg (obtained by max-pooling over repre-sentations of their words): c(x, y) = (cid:13)(cid:13)o − ¯hy(cid:13)¯hx(cid:13)gwhere ¯hxo,1, .
.
.
, hxo,|xo|),¯hyg,|yg|), and hxg = m ax p ool(hyo,iand hyg,i are the representation vectors of the i-th words of xo and yg, respectively, obtainedfrom the student’s bilstm.
also, to deﬁne thediscrete distribution p(x) for ot over x , we em-ploy the event trigger likelihood scorexo for thetrigger candidate of each sentence xo in x thatis returned by the feed-forward network ffs.
o = m ax p ool(hxg,1, .
.
.
, hy.
aux.
1it is worth mentioning that this problem is intractable sowe solve its entropy-based approximation using the sinkhornalgorithm (peyre and cuturi, 2019)..o = ffs.
for the auxiliary task ei in the student model,i.e, scorexaux(xo).
afterward, we ap-ply the softmax function over the scores of theoriginal sentences in the current mini-batch to ob-tain p(x), i.e., p(x) = sof tmax(scorexo ).
sim-ilarly, the discrete distribution q(y) is deﬁned asq(y) = sof tmax(scoreyg ).
to this end, by solv-ing the ot problem in equation 1 and obtainingthe efﬁcient transport plan π∗(x, y) using this setup,we can obtain the distance dso,g.
in the same way,the distance dto,g can be computed using the rep-resentations and event trigger likelihoods from theteacher network.
note that in this way, we can inte-grate both representation vectors of sentences andevent trigger likelihoods into the distance computa-tion between data as motivated in the introduction.
finally, to train the student model, the followingcombined loss function is used in our framework:l = lpred + αlaux + βlkl + γldist, where α,β, and γ are the trade-off parameters..3 experiments.
3.1 datasets, baselines & hyper-parameters.
to evaluate the effectiveness of the proposed model,called the gpt-based data augmentation model fored with ot (gptedot), we conduct experimentson the following ed datasets:.
ace 2005 (walker et al., 2006): this datasetannotates 599 documents for 33 event types thatcover different text domains(e.g., news, weblog orconversation documents).
we use the same pre-processing script and data split as prior works (laiet al., 2020c; tong et al., 2020b) to achieve faircomparisons.
in particular, the data split involves529/30/40 articles for train/dev/test sets respec-tively.
for this dataset, we compare our modelwith prior state-of-the-art models reported in therecent works (lai et al., 2020c; tong et al., 2020b),including bert-based models such as dmbert,ad-dmbert (wang et al., 2019), drmm, ekd(tong et al., 2020b), and gatedgcn (lai et al.,2020c)..cyseced (man duc trong et al., 2020): thisdataset provides 8,014 event triggers for 30 eventtypes from 300 articles of the cybersecurity do-main (i.e., cybersecurity events).
we follow the thesame pre-processing and data split as the originalwork (man duc trong et al., 2020) with 240/30/30documents for the train/dev/test sets.
to be consis-tent with other experiments and facilitate the datageneration based on gpt-2, the experiments on cy-.
6275seced are conducted at the sentence level whereinputs for models involve sentences.
as such, weemploy the state-of-the-art sentence-level modelsreported in (man duc trong et al., 2020), i.e., dm-bert (wang et al., 2019), bert-ed (yang et al.,2019), as the baselines for cyseced..rams (ebner et al., 2020): this dataset anno-tates 9,124 event triggers for 38 event types.
weuse the ofﬁcial data split with 3,194, 399, and 400documents for training, development, and testingrespectively for rams.
we also perform ed at thesentence level in this dataset.
for the baselines, weutilize recent state-of-the-art bert-based modelsfor ed, i.e., dmbert (wang et al., 2019) andgatedgcn (lai et al., 2020c).
for a fair compar-ison, the performance of such baseline models isobtained via their ofﬁcial implementations fromthe original papers that are ﬁne-tuned for rams.
for each dataset, we use its training and devel-opment data to ﬁne-tune the gpt-2 model.
wetune the hyperparameters for the proposed teacher-student architecture using a random search.
allthe hyperparameters are selected based on the f1scores on the development set of the ace 2005dataset.
the same hyper-parameters from this ﬁne-tuning are then applied for other datasets for con-sistency.
in our model we use the small versionof gpt-2 to generate data.
in the base model, weuse bertbase, 300 dimensions in the hidden statesof bilstm and 2 layers of feed-forward neuralnetworks with 200 hidden dimensions to predictevents.
the trade-off parameters τ , α, β and γ areset to 0.1, 0.1, 0.05, and 0.08, respectively.
thelearning rate is set to 0.3 for the adam optimizerand the batch size of 50 are employed during train-ing.
finally, note that we do not update the bertmodel for word embeddings in this work due toits better performance on the development data oface 2005..3.2 results.
results of experiments on the ace 2005 test set areshown in table 1. the most important observationis that the proposed model gptedot signiﬁcantlyoutperforms all the baseline models (p < 0.01),thus showing the beneﬁts of gpt-generated dataand the teacher-student framework with knowledgeconsistency for ed in this work.
in particular,compared to the bert-based models that lever-age data augmentation, i.e., ad-dmbert (wanget al., 2019) with semi-supervised and adversarial.
modelcnn (nguyen and grishman, 2015)dmcnn (chen et al., 2015)dlrnn (duan et al., 2017)ann-s2 (liu et al., 2017)gmlatt (liu et al., 2018)gcn-ed (nguyen and grishman, 2018)lu’s distill (lu et al., 2019)ts-distill (liu et al., 2019)dmbert* (wang et al., 2019)ad-dmbert* (wang et al., 2019)drmm* (tong et al., 2020a)gatedgcn* (lai et al., 2020c)ekd* (tong et al., 2020b)gptedot*.
modelcnn (nguyen and grishman, 2015)dmcnn (chen et al., 2015)gcn-ed (nguyen and grishman, 2018)moganed (yan et al., 2019)cyberlstm (satyapanich et al., 2020)dmbert (wang et al., 2019)bert-ed (man duc trong et al., 2020)gptedot.
p71.875.677.278.078.977.976.376.877.677.977.978.879.182.3.p51.847.546.353.742.559.460.265.9.r66.463.664.966.366.968.871.972.971.872.574.876.378.076.3.r36.738.751.859.629.051.356.164.1.f169.069.170.571.772.473.174.074.874.675.176.377.678.679.2.f143.043.248.956.534.555.158.165.0.table 1: performance on the on ace 2005 test set.
*indicates models that use bert for the encoding..table 2: comparison with state-of-the-art models oncyseced.
all the models in this table use bert..modeldmbert (wang et al., 2019)gatedgcn (lai et al., 2020c)gptedot.
p62.666.555.5.r44.059.078.6.f151.762.565.1.table 3: model’s performance on rams.
all the mod-els use bert in this table..learning, drmm (tong et al., 2020a) with image-enhanced models, and ekd (tong et al., 2020b)with external open-domain event triggers, the betterperformance of gptedot highlights the advan-tages of gpt-2 to generate data for ed models..results of experiments on the cyseced test setare presented in table 2. this table reveals thatthe teacher-student architecture gptedot signif-icantly improves the performance over previousstate-of-the-art models for ed in cybersecurity do-main.
this is important as it shows that the pro-posed model is effective in different domains.
inaddition, our results also suggest that gpt-2 canbe employed to generate effective data for ed indomains where data annotation for ed requiresextensive domain expertise and expensive cost toobtain such as the cybersecurity events.
moreover,the higher margin of improvement for gptedoton cyseced compared to the those on the ace.
62763.3 ablation study.
table 4: ablation study on the ace 2005 dev set..2005 dataset suggests the necessity of using moretraining data for ed in technical domains..finally, results of experiments on the ramstest set are reported in table 3. consistent withour experiments on ace 2005 and cyseced,our proposed model achieve signiﬁcantly higherperformance than existing state-of-the-art models(p < 0.01), thus further conﬁrming the advantagesof gptedot for ed..this ablation study evaluates the effectiveness ofdifferent components in gptedot for ed.
first,for the importance of the generated data g fromgpt-2 and the teacher-student architecture to mit-igate noises, we examine the following baselines:(1) baseo: the baseline is the base model trainedonly on the original data o, thus being equivalent tothe teacher model and not using the student model;and (2) basea: this baseline trains the base modelon the combination of the original and generateddata, i.e., a, using the multi-learning setting (i.e.,the teacher model is excluded)..second, for the multi-task learning design in theteacher network, we explore the following ablatedmodels: (3) teacher−a: this baseline removes theauxiliary task ei in the teacher from gptedot.
assuch, the ot-based knowledge consistency for ei isalso eliminated; (4) teacher−m : in this model, themain task ed is utilize to train the teacher, so thecorresponding kl-based knowledge consistencyfor ed is also removed..third, for the design of the knowledge consis-tency losses in the student network, we evaluatethe following baselines: (5) student−ot : this ab-lated model eliminates the ot-based knowledgeconsistency loss for the auxiliary task ei in the stu-dent’s training of gptedot (the auxiliary task isstill employed for the teacher and the student); (6)student−kl: for this model, the kl-based knowl-edge consistency for the main task ed is ignoredin the student’s training; (7) student+ot : in thisbaseline, we use ot for the knowledge consistencyon both the main and the auxiliary tasks.
here,for the main task ed, the cost function c(x, y)for ot is still obtained via the euclidean distancesbetween representation vectors while the distribu-tions p(x) and p(y) are based on the maximumprobabilities of the label-probability distributionsps(.|xo, to) and ps(yg, tg) for the ed task; and(8) student+kl: this baseline employs the kl di-.
modelgptedot (full)baseobaseateacher−ateacher−mstudent−otstudent−klstudent+otstudent+klot−repot−score.
p82.478.275.876.975.875.476.876.177.176.878.0.r75.073.773.978.177.979.377.376.676.777.377.1.f178.575.974.977.576.977.377.076.476.977.077.6.vergence between models’ predicted distributionsto enforce the teacher-student consistency for boththe main task and the auxiliary task.
to this end, forthe auxiliary task ei, we convert the ﬁnal activationof ffaux into a distribution with two data points(i.e., [ffaux(x), 1 − ffaux(x)]) to compute thekl divergence between the teacher and the student.
finally, for the importance of euclidean dis-tances and event trigger likelihoods in the ot-based distance between o and g for knowledgeconsistency in ei, we investigate two baselines:(9) ot−rep: here, to compute ot, we use con-stant cost between every pair of sentences, i.e.,c(x, y) = 1 (i.e., ignoring representation-baseddistances); and (10) ot−score: this model usesuniform distributions for p(x) and q(y) to computethe ot (i.e., ignoring event trigger likelihoods)..we report the performance of the models (onthe ace 2005 development set) for the ablationstudy in table 4. there are several observationsfrom this table.
first, the generated data g andthe teacher-student architecture are necessary forgptedot to achieve the highest performance.
inparticular, comparing with baseo, the better perfor-mance of gptedot indicates the beneﬁts of thegpt-generated data.
moreover, the better perfor-mance of baseo over basea reveals that the simplecombination of the synthetic and original data with-out any effective method to mitigate noises mightbe harmful.
second, the lower performance ofteacher−a and teacher−m shows that both the aux-iliary and the main task (i.e., multi-task learning)in the teacher are integral to produce the best per-formance.
third, the choice of methods to promoteknowledge consistency is important and the pro-posed combination of kl and ot for the ed andei tasks (respectively) are necessary.
in particular,removing or replacing each of them with the otherone (i.e., student+ot and student+kl) would de-.
6277sentencei was totally shocked by the court’s decision to agree with sam sloan after he trgs sued trge his children..datasetace 2005cyseced according to the last update by the company, the following techniques are used to protect against such trgs malware trge.
rams.
the russian ofﬁcials trgs vowed trge to bomb the isis bases after the last week’s trgs attack trge..table 5: generated sentences by gpt-2 for different datasets.
event triggers are shown in boldface that aresurrounded by the special tokens trgs and trge generated by gpt-2..sentence example.
error typeincompleteness a federal judge on monday settled trgs charges trge against seven members ofrepetitioninconsistencymissing labelsincorrect labels the sec is a very good place to trgs hide trge money..do you think the trgs attack trge will happen to you or do you think the trgs attack trge will happen to you?
this morning we were watching the news and heard the news about the tragic trgs death trge of a young boy and her mother in iraq.
aaron tramailer’s story is the story of a woman who was forced into suicide..proportion18%15%12%29%26%.
table 6: samples of noisy generated sentences for the ace 2005 dataset from gpt-2.
event triggers are shown inboldface and the special tokens trgs and trge are generated by gpt-2..|g|0.5 * |o|1.0 * |o|2.0 * |o|3.0 * |o|.
p80.382.481.378.4.r72.475.073.371.8.f176.278.577.175.0.table 7: the performance of gptedot on the ace2005 dev set with different sizes of the generated datag..crease the performance signiﬁcantly.
finally, in theproposed consistency method based on ot for ei,it is beneﬁcial to employ both representation-leveldistances (i.e., ot−rep) and models’ predictionsfor event trigger likelihoods (i.e., ot−score) as re-moving any of them hurts the performance..3.4 analysis.
to provide more insights into the quality of thesynthetic data g, we provide samples of sentencesthat are generated by the ﬁne-tuned gpt-2 modelon each dataset in table 5. this table illustrates thatthe generated sentences also belong to the domainsof the original data (i.e., the cybersecurity domain).
as such, combining synthetic data with originaldata is promising for improving ed performanceas demonstrated in our experiments..as discussed earlier, the generated data g is notin order to better understand thefree of noise.
types of errors existing in generated sentences, wemanually assess 200 sentences randomly selectedfrom the set g generated by the ﬁne-tuned gpt-2model on the ace 2005 dataset.
we categorize theerrors into ﬁve types and provide their proportionsalong with example for each error type in table6. this table shows that the majority of errors aredue to missing labels (i.e., no special tokens trgsand trge are generated) or incorrect labels (i.e.,marked words are not event triggers of interested.
types) generated by the language model..finally, to study the importance of the size ofthe generated data to augment training set for ed,we conduct an experiment in which different num-bers of generated samples in g (for the ace 2005dataset) are combined with the original data o.the results are shown in table 7. according tothis table, the highest performance of the proposedmodel is achieved when the numbers of the gener-ated and original data are equal.
more speciﬁcally,decreasing the number of generated samples po-tentially limits the beneﬁts of data augmentation.
on the other hand, increasing the size of generateddata might introduces extensive noises and becomeharmful to the ed models..4 related work.
early methods for ed have employed feature-based techniques (ahn, 2006; ji and grishman,2008; patwardhan and riloff, 2009; liao and grish-man, 2010a,b; hong et al., 2011; mcclosky et al.,2011; li et al., 2013; miwa et al., 2014; yang andmitchell, 2016).
later, advanced deep learningmethods (nguyen and grishman, 2015; chen et al.,2015; nguyen et al., 2016a,b; sha et al., 2018;zhang et al., 2019; yang et al., 2019; nguyen andnguyen, 2019; zhang et al., 2020b) have been ap-plied for ed.
one challenge for ed research is thelimited size of existing datasets that hinder the train-ing of effective models.
prior works have attemptedto address this issue via unsupervised (huang et al.,2016; yuan et al., 2018), semi-supervised (liaoand grishman, 2010a; huang and riloff, 2012;ferguson et al., 2018), distantly supervised (keithet al., 2017; nguyen and nguyen, 2018; zeng et al.,2017; araki and mitamura, 2018), and few/zero-shot (huang et al., 2018; lai et al., 2020a,b) learn-.
6278ing.
in this work, we propose a novel method toaugment training data for ed by exploiting thepowerful language model gpt-2 to automaticallygenerate new samples..leveraging gpt-2 for augmenting training datahas also been studied for other nlp tasks recently(e.g., relation extraction, commonsense reasoning)(papanikolaou and pierleoni, 2020; zhang et al.,2020a; yang et al., 2020; madaan et al., 2020;bosselut et al., 2019; kumar et al., 2020; anaby-tavor et al., 2020; peng et al., 2020).
however,none of those works has explored gpt-2 for ed.
inaddition, existing methods only resort to heuristicsto ﬁlter out noisy samples generated by gpt-2.
incontrast, we propose a novel differentiable methodcapable of preventing noises from diverging repre-sentation vectors of the models for ed..5 conclusion.
we propose a novel method for augmenting train-ing data for ed using the samples generated bythe language model gpt-2.
to avoid noises in thegenerated data, we propose a novel teacher-studentarchitecture in a multi-task learning framework.
we introduce a mechanism for knowledge consis-tency enforcement to mitigate noises from gener-ated data based on optimal transport.
experimentson various ed benchmark datasets demonstrate theeffectiveness of the proposed method..acknowledgments.
this research has been supported by the army re-search ofﬁce (aro) grant w911nf-21-1-0112and the nsf grant cns-1747798 to the iu-crc center for big learning.
this research isalso based upon work supported by the ofﬁceof the director of national intelligence (odni),intelligence advanced research projects activ-ity (iarpa), via iarpa contract no.
2019-19051600006 under the better extraction from texttowards enhanced retrieval (better) program.
the views and conclusions contained herein arethose of the authors and should not be interpretedas necessarily representing the ofﬁcial policies, ei-ther expressed or implied, of aro, odni, iarpa,the department of defense, or the u.s. govern-ment.
the u.s. government is authorized to re-produce and distribute reprints for governmentalpurposes notwithstanding any copyright annotationtherein.
this document does not contain technol-ogy or technical data controlled under either the.
u.s. international trafﬁc in arms regulations orthe u.s. export administration regulations..references.
david ahn.
2006. the stages of event extraction.
inproceedings of the workshop on annotating andreasoning about time and events..ateret anaby-tavor, boaz carmeli, esther goldbraich,amir kantor, george kour, segev shlomov, naamatepper, and naama zwerdling.
2020. do not havein pro-enough data?
deep learning to the rescue!
ceedings of the association for the advancement ofartiﬁcial intelligence (aaai)..jun araki and teruko mitamura.
2018. open-domainin pro-event detection using distant supervision.
ceedings of the international conference on compu-tational linguistics (coling)..antoine bosselut, hannah rashkin, maarten sap, chai-tanya malaviya, asli celikyilmaz, and yejin choi.
2019. comet: commonsense transformers for au-tomatic knowledge graph construction.
in proceed-ings of the annual meeting of the association forcomputational linguistics (acl)..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the annual meeting of the association forcomputational linguistics (acl)..shaoyang duan, ruifang he, and wenli zhao.
2017.exploiting document level information to improveinevent detection via recurrent neural networks.
proceedings of the international joint conferenceon natural language processing (ijcnlp)..seth ebner, patrick xia, ryan culkin, kyle rawlins,and benjamin van durme.
2020. multi-sentence ar-gument linking.
in proceedings of the annual meet-ing of the association for computational linguistics(acl)..james ferguson, colin lockard, daniel s weld, andhannaneh hajishirzi.
2018. semi-supervised eventextraction with paraphrase clusters.
in proceedingsof the conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies (naacl-hlt)..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
inthe deep learning workshop atproceedings ofneurips..yu hong, jianfeng zhang, bin ma, jianmin yao,guodong zhou, and qiaoming zhu.
2011. usingcross-entity inference to improve event extraction.
in proceedings of the annual meeting of the asso-ciation for computational linguistics (acl)..6279lifu huang, taylor cassidy, xiaocheng feng, heng ji,clare voss, jiawei han, and avirup sil.
2016. lib-eral event extraction and event schema induction.
inproceedings of the annual meeting of the associa-tion for computational linguistics (acl)..shasha liao and ralph grishman.
2010b.
using doc-ument level cross-event inference to improve eventin proceedings of the annual meet-extraction.
ing of the association for computational linguistics(acl)..lifu huang, heng ji, kyunghyun cho, and clare r.voss.
2018. zero-shot transfer learning for eventin proceedings of the annual meet-extraction.
ing of the association for computational linguistics(acl)..ruihong huang and ellen riloff.
2012. bootstrappedtraining of event extraction classiﬁers.
in proceed-ings of the conference of the european chapterof the association for computational linguistics(eacl)..heng ji and ralph grishman.
2008. reﬁning event ex-traction through cross-document inference.
in pro-ceedings of the annual meeting of the associationfor computational linguistics (acl)..katherine keith, abram handler, michael pinkham,cara magliozzi, joshua mcdufﬁe, and brendano’connor.
2017. identifying civilians killed by po-lice with distantly supervised entity-event extrac-tion.
in proceedings of the conference on empiricalmethods in natural language processing (emnlp)..varun kumar, ashutosh choudhary, and eunah cho.
2020. data augmentation using pre-trained trans-former models.
arxiv preprint arxiv:2003.02245..viet dac lai, franck dernoncourt, and thien huunguyen.
2020a.
exploiting the matching informa-tion in the support set for few shot event classiﬁca-in proceedings of the 24th paciﬁc-asia con-tion.
ference on knowledge discovery and data mining(pakdd)..viet dac lai, franck dernoncourt, and thien huunguyen.
2020b.
extensively matching for few-shotlearning event detection.
in proceedings of the 1stjoint workshop on narrative understanding, story-lines, and events (nuse) at acl 2020..viet dac lai, tuan ngo nguyen, and thien huunguyen.
2020c.
event detection: gate diversity andsyntactic importance scores for graph convolutionneural networks.
in proceedings of the conferenceon empirical methods in natural language process-ing (emnlp)..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-tures.
in proceedings of the annual meeting of theassociation for computational linguistics (acl)..shasha liao and ralph grishman.
2010a.
filteredranking for bootstrapping in event extraction.
inproceedings ofthe international conference oncomputational linguistics (coling)..jian liu, yubo chen, and kang liu.
2019. exploit-ing the ground-truth: an adversarial imitation basedknowledge distillation approach for event detection.
in proceedings of the association for the advance-ment of artiﬁcial intelligence (aaai)..jian liu, yubo chen, kang liu, and jun zhao.
2018.event detection via gated multilingual attentionmechanism.
in proceedings of the association forthe advancement of artiﬁcial intelligence (aaai)..shulin liu, yubo chen, kang liu, and jun zhao.
2017.exploiting argument information to improve eventdetection via supervised attention mechanisms.
inproceedings of the annual meeting of the associa-tion for computational linguistics (acl)..yaojie lu, hongyu lin, xianpei han, and le sun.
2019.distilling discrimination and generaliza-tion knowledge for event detection via delta-in proceedings of the an-representation learning.
nual meeting of the association for computationallinguistics (acl)..aman madaan, dheeraj rajagopal, yiming yang, ab-hilasha ravichander, eduard hovy, and shrimaiprabhumoye.
2020. eigen: event inﬂuence gen-eration using pre-trained language models.
arxivpreprint arxiv:2010.11764..hieu man duc trong, duc trong le, amir pouranben veyseh, thuat nguyen, and thien huu nguyen.
2020. introducing a new dataset for event detectionin proceedings of the con-in cybersecurity texts.
ference on empirical methods in natural languageprocessing (emnlp)..david mcclosky, mihai surdeanu, and christophermanning.
2011. event extraction as dependencyparsing.
in bionlp shared task workshop..makoto miwa, paul thompson, ioannis korkontzelos,and sophia ananiadou.
2014. comparable studyof event extraction in newswire and biomedical do-mains.
in proceedings of the international confer-ence on computational linguistics (coling)..nghia ngo, tuan ngo nguyen, and thien huu nguyen.
2020. learning to select important context wordsin proceedings of the 24thfor event detection.
paciﬁc-asia conference on knowledge discoveryand data mining (pakdd)..minh van nguyen, viet dac lai, and thien huunguyen.
2021. cross-task instance representationinteractions and label dependencies for jointin-formation extraction with graph convolutional net-in proceedings of the conference of theworks.
north american chapter of the association for com-putational linguistics: human language technolo-gies (naacl-hlt)..6280minh van nguyen and thien huu nguyen.
2018. whois killed by police: introducing supervised attentionfor hierarchical lstms.
in proceedings of the inter-national conference on computational linguistics(coling)..taneeya satyapanich, francis ferraro, and tim finin.
2020. casie: extracting cybersecurity event infor-in proceedings of the associa-mation from text.
tion for the advancement of artiﬁcial intelligence(aaai)..thien huu nguyen, kyunghyun cho, and ralph grish-man.
2016a.
joint event extraction via recurrent neu-ral networks.
in proceedings of the conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl-hlt)..lei sha, feng qian, baobao chang, and zhifang sui.
2018.jointly extracting event triggers and argu-ments by dependency-bridge rnn and tensor-basedargument interaction.
in proceedings of the associ-ation for the advancement of artiﬁcial intelligence(aaai)..thien huu nguyen, lisheng fu, kyunghyun cho, andralph grishman.
2016b.
a two-stage approach forextending event detection to new types via neuralnetworks.
in proceedings of the 1st acl workshopon representation learning for nlp (repl4nlp)..thien huu nguyen and ralph grishman.
2015. eventdetection and domain adaptation with convolutionalneural networks.
in proceedings of the annual meet-ing of the association for computational linguistics(acl)..thien huu nguyen and ralph grishman.
2018. graphconvolutional networks with argument-aware pool-ing for event detection.
in proceedings of the asso-ciation for the advancement of artiﬁcial intelligence(aaai)..thien huu nguyen, adam meyers, and ralph grish-man.
2016c.
new york university 2016 system forkbp event nugget: a deep learning approach.
in pro-ceedings of text analysis conference (tac)..trung minh nguyen and thien huu nguyen.
2019.one for all: neural joint modeling of entities andevents.
in proceedings of the association for the ad-vancement of artiﬁcial intelligence (aaai)..yannis papanikolaou and andrea pierleoni.
2020.dare: data augmented relation extraction with gpt-2. in scinlp workshop at the conference on auto-mated knowledge base construction (akbc)..siddharth patwardhan and ellen riloff.
2009. a uni-ﬁed model of phrasal and sentential evidence forinformation extraction.
in proceedings of the con-ference on empirical methods in natural languageprocessing (emnlp)..baolin peng, chenguang zhu, michael zeng, and jian-feng gao.
2020. data augmentation for spoken lan-guage understanding via pretrained models.
arxivpreprint arxiv:2004.13952..gabriel peyre and marco cuturi.
2019. computationaloptimal transport: with applications to data science.
in foundations and trends in machine learning..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..meihan tong, shuai wang, yixin cao, bin xu, juanzili, lei hou, and tat-seng chua.
2020a.
image en-hanced event detection in news articles.
in proceed-ings of the association for the advancement of arti-ﬁcial intelligence (aaai)..meihan tong, bin xu, shuai wang, yixin cao, leihou, juanzi li, and jun xie.
2020b.
improvingevent detection via open-domain trigger knowledge.
in proceedings of the annual meeting of the associ-ation for computational linguistics (acl)..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilingualtraining corpus.
in technical report, linguistic dataconsortium..xiaozhi wang, xu han, zhiyuan liu, maosong sun,and peng li.
2019. adversarial training for weaklyin proceedings of thesupervised event detection.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies (naacl-hlt)..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-tion.
in proceedings of the conference on empiricalmethods in natural language processing (emnlp)..bishan yang and tom m. mitchell.
2016. joint extrac-tion of events and entities within a document con-text.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..sen yang, dawei feng, linbo qiao, zhigang kan,and dongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
inproceedings of the annual meeting of the associa-tion for computational linguistics (acl)..yiben yang, chaitanya malaviya, jared fernandez,swabha swayamdipta, ronan le bras, ji-pingwang, chandra bhagavatula, yejin choi, and dougdowney.
2020. generative data augmentation forin proceedings of thecommonsense reasoning.
findings of the conference on empirical methodsin natural language processing (emnlp)..6281quan yuan, xiang ren, wenqi he, chao zhang, xinhegeng, lifu huang, heng ji, chin-yew lin, and ji-awei han.
2018. open-schema event proﬁling formassive news corpora.
in proceedings of the confer-ence on information and knowledge management(cikm)..ying zeng, yansong feng, rong ma, zheng wang, ruiyan, chongde shi, and dongyan zhao.
2017. scaleup event extraction learning via automatic trainingin proceedings of the associa-data generation.
tion for the advancement of artiﬁcial intelligence(aaai)..danqing zhang, tao li, haiyang zhang, and bingon data augmentation for ex-arxiv preprint.
yin.
2020a.
treme multi-label classiﬁcation.
arxiv:2009.10778..junchi zhang, yanxia qin, yue zhang, mengchi liu,and donghong ji.
2019. extracting entities andevents as a single task using a transition-based neu-ral model.
in proceedings of the international jointconference on artiﬁcial intelligence (ijcai)..yunyan zhang, guangluan xu, yang wang, daoyu lin,feng li, chenglong wu, jingyuan zhang, and tin-glei huang.
2020b.
a question answering-basedframework for one-step event argument extraction.
in ieee access, vol 8, 65420-65431..6282