knowing the no-match: entity alignment with dangling cases.
zequn sun1 muhao chen2,3 wei hu11state key laboratory for novel software technology, nanjing university, china2department of computer science, university of southern california, usa3information sciences institute, university of southern california, usazqsun.nju@gmail.com, muhaoche@usc.edu, whu@nju.edu.cn.
abstract.
source kg entities.
dangling entitiesin the target kg .
this paper studies a new problem setting ofentity alignment for knowledge graphs (kgs).
since kgs possess different sets of entities,there could be entities that cannot ﬁnd align-ment across them, leading to the problem ofdangling entities.
as the ﬁrst attempt to thisproblem, we construct a new dataset and de-sign a multi-task learning framework for bothentity alignment and dangling entity detection.
the framework can opt to abstain from pre-dicting alignment for the detected dangling en-tities.
we propose three techniques for dan-gling entity detection that are based on thedistribution of nearest-neighbor distances, i.e.,nearest neighbor classiﬁcation, marginal rank-ing and background ranking.
after detectingand removing dangling entities, an incorpo-rated entity alignment model in our frameworkcan provide more robust alignment for remain-ing entities.
comprehensive experiments andanalyses demonstrate the effectiveness of ourframework.
we further discover that the dan-gling entity detection module can, in turn, im-prove alignment learning and the ﬁnal perfor-mance.
the contributed resource is publiclyavailable to foster further research..1.introduction.
knowledge graphs (kgs) have evolved to be thebuilding blocks of many intelligent systems (jiet al., 2020).
despite the importance, kgs areusually costly to construct (paulheim, 2018) andnaturally suffer from incompleteness (gal´arragaet al., 2017).
hence, merging multiple kgs throughentity alignment can lead to mutual enrichment oftheir knowledge (chen et al., 2020), and providedownstream applications with more comprehensiveknowledge representations (trivedi et al., 2018;chen et al., 2020).
entity alignment seeks to dis-cover identical entities in different kgs, such as en-glish entity thailand and its french counterpart.
dangling entitiesin the source kg .
target kg entities.
figure 1: illustration of entity alignment between twokgs with dangling cases.
paired red and black squaresin the overlap region denote entity alignment while oth-ers are dangling entities without counterparts..tha¨ılande.
to tackle this important problem,literature has attempted with the embedding-basedentity alignment methods (chen et al., 2017; wanget al., 2018; cao et al., 2019; fey et al., 2020; wuet al., 2020a; liu et al., 2020; sun et al., 2020a).
these methods jointly embed different kgs andput similar entities at close positions in a vectorspace, where the nearest neighbor search can re-trieve entity alignment.
due to its effectiveness,embedding-based entity alignment has drawn ex-tensive attention in recent years (sun et al., 2020c)..nonetheless, to practically support the alignmentof kgs as a real-world task, existing studies sufferone common problem of identifying entities with-out alignment across kgs (called dangling entities).
speciﬁcally, current methods are all built upon theassumption that any source entity has a counterpartin the target kg (sun et al., 2020c), and are ac-cordingly developed with learning resources thatenforce the same assumption.
hence, given everyentity in a source kg, a model always tends to pre-dict a counterpart via the nearest neighbor searchin the embedding space.
however, since each kgmay be independently created based on separatecorpora (lehmann et al., 2015) or contributed bydifferent crowds (speer et al., 2017; carlson et al.,2010), it is natural for kgs to possess different sets.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3582–3593august1–6,2021.©2021associationforcomputationallinguistics3582of entities (collarana et al., 2017), as illustratedin fig.
1. essentially, this problem overlooked inprior studies causes existing methods to fall shortof distinguishing between matchable and danglingentities, hence hinders any of such methods to alignkgs in a real-world scenario..towards more practical solutions of entity align-ment for kgs, we provide a redeﬁnition of the taskwith the incorporation of dangling cases (§2.1), asthe ﬁrst contribution of this work.
given a sourceentity, our setting does not assume that it must havea counterpart in the target kg as what previousstudies do.
instead, conducting entity alignmentalso involves identifying whether the counterpartof an entity actually exists in another kg.
hence,a system to tackle this realistic problem setting ofentity alignment is also challenged by the require-ment for justifying the validity of its prediction..to facilitate the research towards the new prob-lem, the second contribution of this work is to con-struct a new dataset dbp2.0 for entity alignmentwith dangling cases (§2.2).
as being discussed, ex-isting benchmarks for entity alignment, includingdbp15k (sun et al., 2017), wk3l (chen et al.,2017) and the more recent openea (sun et al.,2020c), are set with the constraint that any entityto be aligned should have a valid counterpart.
weuse the full dbpedia (lehmann et al., 2015) tobuild a new dataset and the key challenge lies inthat we need to guarantee the selected danglingentities actually do not have counterparts.
we ﬁrstextract two subgraphs with one-to-one entity align-ment (i.e., all entities have counterparts).
then, werandomly remove some entities to make their leftcounterparts in the peer kg dangling..although embedding-based entity alignment hasbeen investigated for several years, handling withdangling entities has not been studied yet.
as thethird contribution, we present a multi-task learningframework for the proposed task (§3).
it consistsof two jointly optimized modules for entity align-ment and dangling entity detection, respectively.
while the entity alignment module can basicallyincorporate any existing techniques from prior stud-ies (sun et al., 2020c), in this paper, we experimentwith two representative techniques, i.e., relationalembedding based (chen et al., 2017) and neighbor-hood aggregation based (sun et al., 2020b) meth-ods.
for dangling entity detection, our frameworkincorporates an auxiliary learning objective, whichseeks to learn a conﬁdence metric for the inferred.
entity alignment.
the principle to realize suchmetric learning is that the embeddings of danglingentities should be isolated and are distant from oth-ers.
according to this principle, we exploit severaltechniques to distinguish between matchable anddangling entities based on their distance distribu-tion with their neighbors (§3), including nearestneighbor classiﬁcation, marginal ranking and back-ground ranking (dhamija et al., 2018)..we conduct comprehensive experiments on thenew dbp2.0 dataset, which demonstrate the pro-posed techniques to solve the dangling entity de-tection problem to different extents.
moreover, weobserve that training the dangling detection model(marginal ranking) provides an effective indirect su-pervision that improves the detection of alignmentfor matchable entities.
we hope our task, datasetand framework can foster further investigation ofentity alignment techniques in the suggested realscenario, leading to more effective and practical so-lutions to this challenging but important problem..2 task and dataset.
we hereby describe the problem setting of our taskand introduce the new dataset..2.1 task deﬁnitiona kg is a set of relational triples t ⊆ e × r × e,where e and r denote vocabularies of entities andrelations, respectively.
without loss of generality,we consider entity alignment between two kgs,i.e., a source kg k1 = (t1, e1, r1) and a target kgk2 = (t2, e2, r2).
given a small set of seed entityalignment a12 = {(e1, e2) ∈ e1 × e2(cid:4)e1 ≡ e2}along with a small set of source entities d ⊂ e1known to have no counterparts as training data, thetask seeks to ﬁnd the remaining entity alignment.
different from the conventional entity alignmentsetting (sun et al., 2017), a portion (with an antic-ipated quantity) of entities in e1 and e2 may haveno counterparts.
our training and inference stagestake such dangling entities into consideration..2.2 dataset construction.
as discussed, previous testbeds for entity align-ment do not contain dangling entities (sun et al.,2017; chen et al., 2018; sun et al., 2020c).
there-fore, we ﬁrst create a new dataset to support thestudy of the proposed problem setting.
same asthe widely used existing benchmark dbp15k (sun.
3583datasets.
# entities # rel..# triples # align..zh-en.
ja-en.
fr-en.
zhen.
jaen.
fren.
84,996118,996.
100,860139,304.
3,7063,402.
3,2433,396.
286,067586,868.
347,204668,341.
33,183.
39,770.
221,327278,411.
2,841802,6784,598 1,287,231.
123,952.table 1: statistics of the dbp2.0 dataset..et al., 2017), we choose dbpedia 2016-101 as theraw data source.
following dbp15k, we also useenglish (en), french (fr), japanese (ja) and chi-nese (zh) versions of dbpedia to build three entityalignment settings of zh-en, ja-en and fr-en.
for each monolingual kg, the triples are extractedfrom the infobox data of dbpedia, where relationsare not mapped to a uniﬁed ontology.
the referenceentity alignment data is from the inter-languagelinks (ills) of dbpedia across these three bridgesof languages.
such reference data is later used asalignment labels for training and testing, and alsoserves as references to recognize dangling entities..construction.
the key challenge of building ourdataset lies in that we need to ensure the selecteddangling entities are indeed without counterparts.
specifcally, we cannot simply regard entities with-out ills as dangling ones, since the ills are alsoincomplete (chen et al., 2017).
under this circum-stance, we use a two-step dataset extraction process,which ﬁrst samples two subgraphs whose entitiesall have counterparts based on ills, and randomlyremoves a disjoint set of entities in the source andtarget graphs to make their counterparts dangling.
for the ﬁrst step, we iteratively delete unlinked enti-ties and their triples from the source and target kgsuntil the left two subgraphs are one-to-one aligned.
in the second step for entity removal, while theremoved entities are disjoint in two kgs, the pro-portion of the removed entities also complies withthe proportion of unaligned entities in each kg..statistics and evaluation.
tab.
1 lists the statis-tics our dataset.
the three entity alignment settingshave different data scales and each is much largerthan the same setting in dbp15k, thus can beneﬁtbetter scalability analysis of models.
for danglingentity detection, we split 30% of dangling entitiesfor training, 20% for validation and others for test-.
1downloaded from https://wiki.dbpedia.org/downloads-2016-10.
the latest 2020 version has notprovided updated data for some languages other than englishwhen this study is conducted..ing.
the splits of reference alignment follow thesame partition ratio, which is also consistent withthat of dbp15k to simulate the weak alignmentnature of kgs (chen et al., 2017; sun et al., 2017).
we also compare the degree distribution of match-able and dangling entities in our dataset againstdbp15k in fig.
7 of appx.
§a.
we ﬁnd the match-able and unlabeled entities in dbp15k have biaseddegree distribution, which has an adverse effecton dangling entity detection and leads to unrealevaluation.
by contrast, in dbp2.0, matchable anddangling entities have similar degree distribution..3 entity alignment with dangling cases.
we propose a multi-task learning framework forentity alignment with dangling cases, as illustratedin fig.
2. it has two jointly optimized modules,i.e., entity alignment and dangling entity detection.
the entity alignment module takes as input rela-tional triples of two kgs (for kg embedding) andseed entity alignment (for alignment learning).
asfor the detection of dangling entities, the moduleuses a small number of labeled dangling entitiesto jump-start the learning of a conﬁdence metricfor distinguishing between matchable and danglingentities.
in the inference stage for entity alignment,our framework is able to ﬁrst identify and removedangling entities, then predict alignment for thosethat are decided to be matchable..3.1 entity alignment.
our framework can incorporate any entity align-ment technique.
for the sake of generality, we con-sider two representative techniques in our frame-work.
one technique is based on mtranse (chenet al., 2017), which is among the earliest studies forembedding-based entity alignment.
it employs thetranslational model transe (bordes et al., 2013) toembed kgs in separate spaces, meanwhile jointlylearns a linear transformation between the embed-ding spaces to match entity counterparts.
specif-ically, given an entity pair (x1, x2) ∈ a12, let x1and x2 be their embeddings learned by the trans-lational model.
mtranse learns the linear trans-formation induced by a matrix m by minimizing(cid:4)mx1−x2(cid:4), where (cid:4)·(cid:4) denotes the l1 or l2 norm.
the other technique is from alinet (sun et al.,2020b), which is one of the sota methods basedon graph neural networks.
alinet encodes entitiesby performing a multi-hop neighborhood aggre-gation, seeking to cope with heteromorphism of.
3584input.
source kg.
seed entity alignmenttraining data.
dangling source entities.
target kg.
learning.
entity alignment.
dangling entity detection.
inference.
source entity verification.
remove danglingentities.
alignment search.
output.
figure 2: framework of entity alignment w/ abstention..their neighborhood structures.
for alignment learn-ing, different from mtranse that only minimizesthe transformed embedding distance, alinet addi-tionally optimizes a margin-based ranking loss forentity counterparts with negative samples.
speciﬁ-cally, let x be a matchable source entity in the seedentity alignment, and x(cid:2) is a randomly-sampledentity in the target kg, alinet attempts to ensure(cid:4)x − x(cid:2)(cid:4) > λ1 > 0, where λ1 is a distance margin..3.2 dangling entity detection.
we propose three techniques to implement the dan-gling detection module based on the distribution ofthe nearest neighbor distance in embedding space..3.2.1 nn classiﬁcation.
this technique is to train a binary classiﬁer to dis-tinguish between dangling entities (labeled 1, i.e.,y = 1) and matchable ones (y = 0).
speciﬁcally,we experiment with a feed-forward network (ffn)classiﬁer.
given a source entity x, its input featurerepresentation is the difference vector between itsembedding x and its transformed nn embeddingxnn in the target kg embedding space2.
the con-ﬁdence of x being a dangling entity is given byp(y = 1|x) = sigmoid(ffn(mx − xnn)).
let dbe the training set of dangling source entities and adenotes the set of matchable entities in the trainingalignment data.
for every x ∈ d ∪a, we minimizethe cross-entropy loss:(cid:2).
lx = −.
yx log(p(y = 1|x))+ (1 − yx) log(1 − p(y = 1|x)).
(cid:3).
,.
(1).
where yx denotes the truth label for entity x. in areal-world entity alignment scenario, the danglingentities and matchable ones usually differ greatly inquantity, leading to unbalanced label distribution.
in that case, we apply label weights (huang et al.,2016) to balance between the losses for both labels..2we use transformed nearest neighbor (nn) to denote thethe nn of a source kg entity after it is transformed to thetarget embedding space..3.2.2 marginal rankingconsidering that dangling entities are the noises forﬁnding entity alignment based on embedding dis-tance, we are motivated to let dangling entities havesolitary representations in the embedding space,i.e., they should keep a distance away from theirsurrounding embeddings.
hence, we seek to puta distance margin between dangling entities andtheir sampled nns.
for every input dangling entityx ∈ d, we minimize the following loss:.
(2).
lx = max(0, λ − (cid:4)mx − xnn(cid:4)),where λ is a distance margin.
this loss and the en-tity alignment loss (e.g., that of mtranse) conductjoint learning-to-rank, i.e., the distance between un-aligned entities should be larger than that of alignedentities while dangling entities should have a lowerranking in the candidate list of any source entity..3.2.3 background rankingin the two aforementioned techniques, searchingfor the nn of an entity is time-consuming.
fur-thermore, selecting an appropriate value for the dis-tance margin of the second technique is not trivial.
based on empirical studies, we ﬁnd that the marginhas a signiﬁcant inﬂuence on the ﬁnal performance.
hence, we would like to ﬁnd a more efﬁcient andself-driven technique.
inspired by the open-set clas-siﬁcation approach (dhamija et al., 2018) that letsa classiﬁer equally penalize the output logits forsamples of classes that are unknown to training (i.e.
background classes), we follow a similar principleand let the model equally enlarge the distance ofa dangling entity from any sampled target-spaceentities.
this method is to treat all dangling entitiesas the “background” of the embedding space, sincethey should be distant from matchable ones.
wealso decrease the scale of the dangling entity em-beddings to further provide a separation betweenthe embeddings of matchable and dangling entities.
for the dangling entity x ∈ d, let x vx be the setof randomly-sampled target entities with size of v.the loss is deﬁned as.
(cid:5)(cid:5)λx − (cid:4)mx − x(cid:2)(cid:4).
(cid:5)(cid:5) + α(cid:4)x(cid:4),.
(3).
lx =.
(cid:4).
x(cid:2)∈x vx.where | · | denotes the absolute value and α is aweight hyper-parameter for balance.
λx is the av-erage distance, i.e., λx = 1(cid:4)mx − x(cid:2)(cid:4).
vthis objective can push the relatively close entitiesaway from the source entity without requiring apre-deﬁned distance margin..x(cid:2)∈x vx.
(cid:6).
35853.3 learning and inference.
the overall learning objective of the proposedframework is a combination of the entity align-ment loss (e.g., mtranse’s loss) and one of thedangling entity detection loss as mentioned above.
the two losses are optimized in alternate batches.
more training details are presented in §4.1..like the training phase, the inference phase isalso separated into dangling entity detection andentity alignment.
the way of inference for dan-gling entities differs with the employed technique.
the nn classiﬁcation uses the jointly trained ffnclassiﬁer to estimate whether the input entity is adangling one.
the marginal ranking takes the pre-set margin value in training as a conﬁdence thresh-old, and decides whether an entity is a dangling onebased on if its transformed nn distance is higherthan the threshold.
the inference of backgroundranking is similar to that of marginal ranking, withonly the difference, by its design, to be that the con-ﬁdence threshold is set as the average nn distanceof entities in the target embedding space.
afterdetecting dangling entities, the framework ﬁndsalignment in the remaining entities based on thetransformed nn search among the matchable enti-ties in the embedding space of the target kg..accelerated nn search.
the ﬁrst and second tech-niques need to search nns.
we can use an efﬁcientsimilarity search library faiss (johnson et al., 2017)for fast nn retrieval in large embedding space.
wealso maintain a cache to store the nns of entitiesbackstage and update it every ten training epochs..4 experiments.
in this section, we report our experimental results.
we start with describing the experimental setups(§4.1).
next, we separately present the experimen-tation under two different evaluation settings (§4.2-§4.3), followed by an analysis on the similarityscore distribution of the obtained representationsfor matchable and dangling entities (§4.4).
to facil-iate the use of the contributed dataset and software,we have incorporated these resources into the ope-nea benchmark3 (sun et al., 2020c)..4.1 experimental settings.
we consider two evaluation settings.
one settingis for the proposed problem setting with danglingentities, for which we refer as the consolidated.
3https://github.com/nju-websoft/openea.
dbp15k.
dbp2.0.
41.8%.
41.9%.
41.3%.
31.4%.
31.6%.
28.0%.
60%.
40%.
20%.
z h -e n.j a -e n.fr -e n.figure 3: average neighbor overlap ratio of aligned en-tities in dbp15k and our dbp2.0..evaluation setting.
we ﬁrst detect and remove thedangling source entities and then search alignmentfor the left entities.
for this evaluation setting,we also separately assess the performance of thedangling detection module.
the other simpliﬁedsetting follows that in previous studies (sun et al.,2017, 2020c) where the source entities in test setall have counterparts in the target kg, so no dan-gling source entities are considered.
in this relaxedevaluation setting, we seek to evaluate the effect ofdangling entity detection on entity alignment andmake our results comparable to previous work..evaluation protocol.
for the relaxed evaluationsetting, given each source entity, the candidatecounterpart list is selected via nn search in theembedding space.
the widely-used metrics on theranking lists are hits@k (k = 1, 10, h@k forshort) and mean reciprocal rank (mrr).
higherh@k and mrr indicate better performance..for the consolidated setting, we report preci-sion, recall and f1 for dangling entity detection.
as for assessing the eventual performance of re-alistic entity alignment, since the dangling entitydetection may not be perfect.
it is inevitable forsome dangling entities to be incorrectly sent to theentity alignment module for aligning, while somematchable ones may be wrongly excluded.
in thiscase, h@k and mrr are not applicable for the con-solidated entity alignment evaluation.
followinga relevant evaluation setting for entity resolutionin database (mudgal et al., 2018; ebraheem et al.,2018), we also use precision, recall and f1 as met-rics.
more speciﬁcally, if a source entity is danglingand is not identiﬁed by the detection module, theprediction is always regarded as incorrect.
simi-larly, if a matchable entity is falsely excluded bythe dangling detection module, this test case is alsoregarded as incorrect since the alignment model hasno chance to search for alignment.
otherwise, thealignment module searches for the nn of a sourceentity in the target embedding space and assessesif the predicated counterpart is correct..model conﬁguration.
as described in §3.2, ourdangling detection module has three variants, i.e.,.
3586methods.
zh-en.
en-zh.
ja-en.
en-ja.
fr-en.
en-fr.
h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr h@1 h@10 mrr.
mtranse .358 .675 .463 .353 .670 .461 .348 .661 .453 .342 .670 .452 .245 .524 .338 .247 .531 .342w/ nnc .350 .668 .457 .356 .664 .460 .340 .657 .441 .336 .630 .445 .253 .539 .343 .251 .536 .343w/ mr .378 .693 .487 .383 .699 .491 .373 .686 .476 .374 .707 .485 .259 .541 .348 .265 .553 .360w/ br .360 .678 .468 .357 .675 .465 .344 .660 .451 .346 .675 .456 .251 .525 .342 .249 .531 .343.alinet.
.332 .594 .421 .359 .629 .451 .338 .596 .429 .363 .630 .455 .223 .473 .306 .246 .495 .329w/ nnc .321 .598 .415 .335 .608 .428 .330 .602 .422 .344 .627 .439 .212 .467 .294 .230 .476 .312w/ mr .343 .606 .433 .364 .637 .459 .349 .608 .438 .377 .646 .469 .230 .477 .312 .252 .502 .335w/ br .333 .599 .426 .357 .632 .451 .341 .608 .431 .369 .636 .461 .214 .468 .298 .238 .487 .321.table 2: entity alignment results (relaxed setting) of mtranse and alinet on dbp2.0..methods.
zh-en.
en-zh.
ja-en.
en-ja.
fr-en.
en-fr.
prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1.
e nnc .604 .485 .538 .719 .511 .598 .622 .491 .549 .686 .506 .583 .459 .447 .453 .557 .543 .550.781 .702 .740 .866 .675 .759 .799 .708 .751 .864 .653 .744 .482 .575 .524 .639 .613 .625.811 .728 .767 .892 .700 .785 .816 .733 .772 .888 .731 .801 .539 .686 .604 .692 .735 .713.mrbr.
snartm.t nnc .676 .419 .517 .738 .558 .634 .597 .482 .534 .761 .120 .207 .466 .365 .409 .545 .162 .250en.752 .538 .627 .828 .505 .627 .779 .580 .665 .854 .543 .664 .552 .570 .561 .686 .549 .609.762 .556 .643 .829 .515 .635 .783 .591 .673 .846 .546 .663 .547 .556 .552 .674 .556 .609.mrbr.
a.il.table 3: dangling entity detection results on dbp2.0..nn classiﬁcation (nnc), marginal ranking (mr),and background ranking (br).
we report the imple-mentation details of the entity alignment module(w/ mtranse or alinet) in appendices b and c.we initialize kg embeddings and model parame-ters using the xavier initializer (glorot and bengio,2010), and use adam (kingma and ba, 2015) to op-timize the learning objectives with the learning rate0.001 for mtranse and 0.0005 for alinet.
notethat we do not follow some methods to initializewith machine translated entity name embeddings(wu et al., 2020a).
as being pointed out by recentstudies (chen et al., 2021; liu et al., 2021, 2020),this is necessary to prevent test data leakage.
entitysimilarity is measured by cross-domain similaritylocal scaling (lample et al., 2018) for reduced hub-ness effects, as being consistent to recent studies(sun et al., 2020b; chen et al., 2021).
we use a two-layer ffn in nnc.
for mr, the margin is set asλ = 0.9 for mtranse and 0.2 for alinet.
br ran-domly samples 20 target entities for each entity perepoch and α = 0.01. training is terminated basedon f1 results of entity alignment on validation data..4.2 relaxed evaluation.
we ﬁrst present the evaluation under the relaxed en-tity alignment setting based on tab.
2. this settingonly involves matchable source entities to test en-tity alignment, which is an ideal (but less realistic)scenario similar to prior studies (sun et al., 2020c)..we also examine if jointly learning to detect dan-gling entities can indirectly improve alignment..as observed, mtranse, even without danglingdetection, can achieve promising performance ondbp2.0.
the results are even better than those ondbp15k as reported by sun et al.
(2017).
we at-tribute this phenomenon to the robustness of thissimple embedding method and our improved imple-mentation (e.g., more effective negative sampling).
by contrast, although we have tried our best intuning, the latest gnn-based alinet falls behindmtranse.
unlike mtranse that learns entity em-beddings from a ﬁrst-order perspective (i.e., basedon triple plausibility scores), alinet represents anentity from a high-order perspective by aggregat-ing its neighbor embeddings, and entities with sim-ilar neighborhood structures would have similarrepresentations.
however, the dangling entities indbp2.0 inevitably become spread noises in entityneighborhoods.
to further probe into this issue, wecount the average neighbor overlap ratio of alignedentities in dbp15k and our dbp2.0.
given an en-tity alignment pair (x1, x2), let π(x1) and π(x2) bethe sets of their neighboring entities respectively,where we also merge their aligned neighbors asone identity based on reference entity alignment.
then the neighbor overlap ratio of x1 and x2 iscalculated as |π(x1) ∩ π(x2)|/|π(x1) ∪ π(x2)|.
weaverage such a ratio for both dbp15k and dbp2.0as given in fig.
3. we can see that the three settings’.
3587methods.
zh-en.
en-zh.
ja-en.
en-ja.
fr-en.
en-fr.
prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1.
e nnc .164 .215 .186 .118 .207 .150 .180 .238 .205 .101 .167 .125 .185 .189 .187 .135 .140 .138.302 .349 .324 .231 .362 .282 .313 .367 .338 .227 .366 .280 .260 .220 .238 .213 .224 .218.312 .362 .335 .241 .376 .294 .314 .363 .336 .251 .358 .295 .265 .208 .233 .231 .213 .222.mrbr.
snartm.t nnc .121 .193 .149 .085 .138 .105 .113 .146 .127 .067 .208 .101 .126 .148 .136 .086 .161 .112en.207 .299 .245 .159 .320 .213 .231 .321 .269 .178 .340 .234 .195 .190 .193 .160 .200 .178.203 .286 .238 .155 .308 .207 .223 .306 .258 .170 .321 .222 .183 .181 .182 .164 .200 .180.mrbr.
a.il.table 4: entity alignment results on dbp2.0..nnc mr.br.
nnc mr.br.
0.8.
0.6.
0.4.
60.
40.
20.
0.z h -e n e n -z h j a -e n e n -j a fr -e n e n -fr.
figure 4: accuracy of dangling entity detection..overlap ratios in dbp2.0 are all much lower thanthose in dbp15k.
thus, dbp2.0 poses additionalchallenges, as compared to dbp15k, speciﬁcallyfor those methods relying on neighborhood aggre-gation.
based on results and analysis, we argue thatmethods performing well on the previous syntheticentity alignment dataset may not robustly general-ize to the more realistic dataset with dangling cases.
the performance of both mtranse and alinet isrelatively worse on fr-en, which has more entities(i.e., larger candidate search space) and a low neigh-borhood overlap ratio (therefore, more difﬁcult tomatch entities based on neighborhood similarity).
meanwhile, we ﬁnd that the dangling detectionmodule can affect the performance of entity align-ment.
in details, mr consistently leads to improve-ment to both mtranse and alinet.
br can alsonoticeably boost entity alignment on most settings.
this shows that learning to isolate dangling entitiesfrom matchable ones naturally provides indirecthelp to discriminate the counterpart of a matchableentity from irrelevant ones.
on the other hand, suchindirect supervision signals may be consumed bythe additional trainable parameters in nnc, caus-ing its effect on entity alignment to be negligible.
overall, the observation here calls for more robustentity alignment methods and dangling detectiontechniques, and lead to further analysis (§4.3)..4.3 consolidated evaluation.
we now report the experiment on the more realisticconsolidated evaluation setting.
tab.
3 gives theprecision, recall and f1 results of dangling entitydetection, and the ﬁnal entity alignment perfor-mance is presented in tab.
4. in addition, fig.
4.z h -e n e n -z h j a -e n e n -j a fr -e n e n - fr.
figure 5: average training time (seconds) of one epochfor dangling entity detection (mtranse variants)..shows the accuracy of dangling entity detection.
we analyze the results from the following aspects..dangling entity detection.
regardless of whichalignment module is incorporated, nnc performsthe worst (e.g., the low recall and accuracy around0.5) among the dangling detection techniques,whereas br generally performs the best.
nncdetermines whether an entity is dangling based onthe difference vector of the entity embedding andits nn, instead of directly capturing the embeddingdistance which is observed to be more importantbased on the results by the other two techniques.
by directly pushing dangling entities away fromtheir nns in the embedding space, both mr andbr offer much better performance.
besides, broutperforms mr in most cases.
by carefully check-ing their prediction results and the actual distanceof nns, we ﬁnd that the induced distance marginin br better discriminates dangling entities frommatchable ones than the pre-deﬁned margin..efﬁciency.
we compare the average epoch timeof training the three dangling detection modulesfor mtranse in fig.
5. we conduct the experi-ment using a workstation with an intel xeon e5-1620 3.50ghz cpu and a nvidia geforce rtx2080 ti gpu.
since nnc and mr need to searchfor nns of source entities, both techniques spendmuch more training time that is saved by randomsampling in br.
overall, br is an effective andefﬁcient technique for dangling entity detection..entity alignment.
generally, for both mtranseand alinet variants, mr and br lead to betterentity alignment results than nnc.
mr and br.
3588(cid:80)(cid:68)(cid:87)(cid:70)(cid:75)(cid:68)(cid:69)(cid:79)(cid:72)(cid:3)(cid:72)(cid:81)(cid:87)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86)(cid:71)(cid:68)(cid:81)(cid:74)(cid:79)(cid:76)(cid:81)(cid:74)(cid:3)(cid:72)(cid:81)(cid:87)(cid:76)(cid:87)(cid:76)(cid:72)(cid:86).
figure 6: kernel density estimate plot of the test match-able and dangling entities’ similarity distribution withtheir nearest target neighbors in zh-en..obtain higher precision and recall performance ondetecting dangling entities as listed in tab.
3, re-sulting in less noise that enters the entity alignmentstage.
by contrast, nnc has a low accuracy andthus introduces many noises.
as br outperformsmr in dangling detection, it also achieves higherentity alignment results than mr on most settings.
we also notice that mr in a few settings, mr offercomparible or slightly better performance than br.
this is because mr can enhance the learning ofalignment modules (see §4.2 for detailed analysis),thus delivering improvement to the ﬁnal perfor-mance.
mtranse variants generally excels alinetvariants in both entity alignment (see tab.
2) anddangling entity detection (see tab.
3) than alinet,similar to the observation in §4.2..alignment direction.
we ﬁnd that the alignmentdirection makes a difference in both dangling entitydetection and entity alignment.
using en kg asthe source is coupled with easier dangling detectionthan in other languages, as the most populated enkg contributes more dangling entities and triplesto training than other kgs.
as for entity alignment,we ﬁnd the observation to be quite the opposite,as using the en kg as a source leads to notice-able drops in results.
for example, the precision ofmtranse-br is 0.312 on zh-en, but only 0.241on en-zh.
this is because the en kg has a largerportion of dangling entities.
although the dan-gling detection module performs well on the enkg than on others, there are still much more dan-gling entities entering the alignment search stage,thus reducing the entity alignment precision.
thisobservation suggests that choosing the alignmentdirection from a less populated kg to the morepopulated en kg can be a more effective solution..we plot in fig.
6 the distribution of similarity scoresof each test entity and its nn.
the plot illustratesbr has the expected effect to isolate dangling en-tities from their nns, whereas matchable entitiesare generally placed closer to their nns.
yet, wecan still see a modest overlap between the two nnsimilarity distributions of dangling and matchableentities, and a number of dangling entities still havea quite large nn similarity.
this also reveals thefact that the proposed problem setting of entityalignment with dangling cases has many remainingchallenges that await further investigation..5 related work.
we discuss two topics of relevant work..5.1 entity alignment.
embedding-based entity alignmentis ﬁrst at-tempted in mtranse (chen et al., 2017), whichjointly learns a translational embedding model anda transform-based alignment model for two kgs.
later studies generally follow three lines of im-provement.
(i) the ﬁrst line improves the embed-ding technique to better suit the alignment task,including contextual translation techniques (sunet al., 2019), long-term dependency techniques(guo et al., 2019) and neighborhood aggregation(or gnn-based) ones (wang et al., 2018; cao et al.,2019; li et al., 2019; sun et al., 2020b,a; fey et al.,2020).
(ii) the second line focuses on effectivealignment learning with limited supervision.
someleverage semi-supervised learning techniques toresolve the training data insufﬁciency issue, includ-ing self-learning (sun et al., 2018; mao et al., 2020)and co-training (chen et al., 2018).
(iii) anotherline of research seeks to retrieve auxiliary or indi-rect supervision signals from proﬁle informationor side features of entities, such as entity attributes(sun et al., 2017; trisedya et al., 2019; zhang et al.,2019; pei et al., 2019), literals (wu et al., 2019,2020b; liu et al., 2020), free text (chen et al.,2021), pre-trained language models (yang et al.,2019; tang et al., 2020) or visual modalities (liuet al., 2021).
due to the large body of recent ad-vances, we refer readers to a more comprehensivesummarization in the survey (sun et al., 2020c)..5.2 learning with abstention.
4.4 similarity score distribution.
to illustrate how well the br technique distin-guishes between matchable and dangling entities,.
learning with abstention is a fundamental machinelearning, where the learner can opt to abstain frommaking a prediction if without enough decisive.
3589conﬁdence (cortes et al., 2016, 2018).
relatedtechniques include thresholding softmax (stefanoet al., 2000), selective classiﬁcation (geifman andel-yaniv, 2017), open-set classiﬁcation with back-ground classes (dhamija et al., 2018) and out-of-distribution detection (liang et al., 2018; vyaset al., 2018).
the idea of learning with abstentionalso has applications in nlp, such as unanswerableqa, where correct answers of some questions arenot stated in the given reference text (rajpurkaret al., 2018; zhu et al., 2019; hu et al., 2019)..to the best of our knowledge, our task, dataset,and the proposed dangling detection techniquesare the ﬁrst contribution to support learning withabstention for entity alignment and structured rep-resentation learning..6 conclusion and future work.
in this paper, we propose and study a new entityalignment task with dangling cases.
we constructa dataset to support the study of the proposed prob-lem setting, and design a multi-learning frameworkfor both entity alignment and dangling entity detec-tion.
three types of dangling detection techniquesare studied, which are based on nearest neighborclassiﬁcation, marginal ranking, and backgroundranking.
comprehensive experiments demonstratethe effectiveness of the method, and provide in-sights to foster further investigation on this newproblem.
we further ﬁnd that dangling entity de-tection can, in turn, effectively provide auxiliarysupervision signals to improve the performance ofentity alignment..for future work, we plan to extend the bench-marking on dbp2.0 with results from more basemodels of entity alignment as well as more absten-tion inference techniques.
extending our frame-work to support more prediction tasks with absten-tion, such as entity type inference (hao et al., 2019)and relation extraction (alt et al., 2020), is anotherdirection with potentially broad impact..acknowledgments.
we thank the anonymous reviewers for their in-sightful comments.
this work is supported by thenational natural science foundation of china (no.
61872172), and the collaborative innovation cen-ter of novel software technology & industrializa-tion.
muhao chen’s work is supported by the na-tional science foundation of united states grantiis-2105329..references.
christoph alt, aleksandra gabryszak, and leonhardhennig.
2020. tacred revisited: a thorough evalu-ation of the tacred relation extraction task.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics (acl), pages1558–1569..antoine bordes, nicolas usunier, alberto garc´ıa-dur´an,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in proceedings of the 27th annualconference on neural information processing sys-tems (neurips), pages 2787–2795..yixin cao, zhiyuan liu, chengjiang li, zhiyuan liu,juanzi li, and tat-seng chua.
2019. multi-channelgraph neural network for entity alignment.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics (acl), pages1452–1461..andrew carlson, justin betteridge, bryan kisiel, burrsettles, estevam hruschka, and tom mitchell.
2010.toward an architecture for never-ending languagelearning.
in proceedings of the 24th aaai confer-ence on artiﬁcial intelligence (aaai)..muhao chen, weijia shi, ben zhou, and dan roth.
2021. cross-lingual entity alignment with inciden-tal supervision.
in proceedings of the conference ofthe european chapter of the association for compu-tational linguistics (eacl)..muhao chen, yingtao tian, kai-wei chang, stevenskiena, and carlo zaniolo.
2018. co-training em-beddings of knowledge graphs and entity descrip-tions for cross-lingual entity alignment.
in proceed-ings of the 27th international joint conference onartiﬁcial intelligence (ijcai), pages 3998–4004..muhao chen, yingtao tian, mohan yang, and carlozaniolo.
2017. multilingual knowledge graph em-beddings for cross-lingual knowledge alignment.
inproceedings of the 26th international joint confer-ence on artiﬁcial intelligence (ijcai), pages 1511–1517..xuelu chen, muhao chen, changjun fan, ankith up-punda, yizhou sun, and carlo zaniolo.
2020. mul-tilingual knowledge graph completion via ensembleknowledge transfer.
in findings of the associationfor computational linguistics: emnlp 2020, pages3227–3238..diego collarana, mikhail galkin, ignacio traversorib´on, christoph lange, maria-esther vidal, ands¨oren auer.
2017. semantic data integration forknowledge graph construction at query time.
in pro-ceedings of the 11th ieee international conferenceon semantic computing (icsc), pages 109–116..corinna cortes, giulia desalvo, claudio gentile,mehryar mohri, and scott yang.
2018. online learn-ing with abstention.
in proceedings of the 35th inter-.
3590national conference on machine learning (icml),pages 1067–1075..corinna cortes, giulia desalvo, and mehryar mohri.
2016. boosting with abstention.
in proceedings ofthe 30th annual conference on neural informationprocessing systems (neurips), pages 1660–1668..akshay raj dhamija, manuel g¨unther, and terrance e.boult.
2018.reducing network agnostophobia.
in proceedings of the 32nd annual conference onneural information processing systems (neurips),pages 9175–9186..muhammad ebraheem, saravanan thirumuruganathan,shaﬁq r. joty, mourad ouzzani, and nan tang.
2018. distributed representations of tuples for entityresolution.
proceedings of the vldb endowment,11(11):1454–1467..matthias fey, jan eric lenssen, christopher morris,jonathan masci, and nils m. kriege.
2020. deepgraph matching consensus.
in proceedings of the8th international conference on learning represen-tations (iclr)..luis gal´arraga, simon razniewski, antoine amarilli,and fabian m. suchanek.
2017. predicting com-pleteness in knowledge bases.
in proceedings of the10th acm international conference on web searchand data mining (wsdm), pages 375–383..yonatan geifman and ran el-yaniv.
2017. selectiveclassiﬁcation for deep neural networks.
in proceed-ings of the 31st annual conference on neural infor-mation processing systems (neurips), pages 4878–4887..xavier glorot and yoshua bengio.
2010. understand-ing the difﬁculty of training deep feedforward neuralnetworks.
in proceedings of the 13th internationalconference on artiﬁcial intelligence and statistics(aistats), pages 249–256..lingbing guo, zequn sun, and wei hu.
2019. learn-ing to exploit long-term relational dependencies inknowledge graphs.
in proceedings of the 36th inter-national conference on machine learning (icml),pages 2505–2514..junheng hao, muhao chen, wenchao yu, yizhousun, and wei wang.
2019. universal representa-tion learning of knowledge bases by jointly embed-ding instances and ontological concepts.
in proceed-ings of the acm sigkdd international conferenceon knowledge discovery and data mining (kdd),pages 1709–1719..chen huang, yining li, chen change loy, and xiaooutang.
2016. learning deep representation for im-balanced classiﬁcation.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion (cvpr), pages 5375–5384..shaoxiong ji, shirui pan, erik cambria, pekka martti-nen, and philip s. yu.
2020. a survey on knowledgegraphs: representation, acquisition and applications.
corr, abs/2002.00388..jeff johnson, matthijs douze, and herv´e j´egou.
2017.billion-scale similarity search with gpus.
corr,abs/1702.08734..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the 3rd international conference on learningrepresentations (iclr)..guillaume lample, alexis conneau, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018.word translation without parallel data.
in proceed-ings of the 6th international conference on learningrepresentations (iclr)..jens lehmann, robert isele, max jakob, anja jentzsch,dimitris kontokostas, pablo n. mendes, sebastianhellmann, mohamed morsey, patrick van kleef,s¨oren auer, and christian bizer.
2015. dbpedia - alarge-scale, multilingual knowledge base extractedfrom wikipedia.
semantic web, 6(2):167–195..chengjiang li, yixin cao, lei hou, jiaxin shi, juanzisemi-supervisedli, and tat-seng chua.
2019.entity alignment via joint knowledge embeddingmodel and cross-graph model.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2723–2732..shiyu liang, yixuan li, and r srikant.
2018. enhanc-ing the reliability of out-of-distribution image detec-tion in neural networks.
in proceedings of the 6thinternational conference on learning representa-tions (iclr)..fangyu liu, muhao chen, dan roth, and nigel col-lier.
2021. visual pivoting for (unsupervised) entityalignment.
in proceedings of the aaai conferenceon artiﬁcial intelligence (aaai)..zhiyuan liu, yixin cao, liangming pan, juanzi li, andtat-seng chua.
2020. exploring and evaluating at-tributes, values, and structures for entity alignment.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 6355–6364..minghao hu, furu wei, yuxing peng, zhen huang,nan yang, and dongsheng li.
2019. read + verify:machine reading comprehension with unanswerablequestions.
in proceedings of the 33rd aaai confer-ence on artiﬁcial intelligence (aaai), pages 6529–6537..xin mao, wenting wang, huimin xu, man lan, andyuanbin wu.
2020. mraea: an efﬁcient and robustentity alignment approach for cross-lingual knowl-edge graph.
in proceedings of the 13th interna-tional conference on web search and data mining(wsdm), pages 420–428..3591sidharth mudgal, han li, theodoros rekatsinas, an-hai doan, youngchoon park, ganesh krishnan, ro-hit deep, esteban arcaute, and vijay raghavendra.
2018. deep learning for entity matching: a designspace exploration.
in proceedings of the 2018 inter-national conference on management of data (sig-mod), pages 19–34..heiko paulheim.
2018. how much is a triple?
esti-mating the cost of knowledge graph creation.
inproceedings of the 17th international semantic webconference (iswc)..shichao pei, lu yu, robert hoehndorf, and xiangliangzhang.
2019. semi-supervised entity alignment viaknowledge graph embedding with awareness of de-gree difference.
in proceedings of the world wideweb conference (www), pages 3130–3136..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (acl), pages 784–789..robyn speer, joshua chin, and catherine havasi.
2017.conceptnet 5.5: an open multilingual graph of gen-eral knowledge.
in proceedings of the 31st aaaiconference on artiﬁcial intelligence (aaai), vol-ume 31..claudio de stefano, carlo sansone, and mario vento.
2000. to reject or not to reject: that is the question-an answer in case of neural classiﬁers.
ieee trans-actions on systems, man, and cybernetics - part c:applications and reviews, 30(1):84–94..zequn sun, muhao chen, wei hu, chengming wang,jian dai, and wei zhang.
2020a.
knowledge associ-ation with hyperbolic knowledge graph embeddings.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 5704–5716..zequn sun, wei hu,.
and chengkai li.
2017.cross-lingual entity alignment via joint attribute-preserving embedding.
in proceedings of the 16thinternational semantic web conference (iswc),pages 628–644..zequn sun, wei hu, qingheng zhang, and yuzhongqu.
2018. bootstrapping entity alignment withknowledge graph embedding.
in proceedings of the27th international joint conference on artiﬁcial in-telligence (ijcai), pages 4396–4402..zequn sun, jiacheng huang, wei hu, muhao chen,lingbing guo, and yuzhong qu.
2019. transedge:translating relation-contextualized embeddings forknowledge graphs.
in proceedings of the 18th inter-national semantic web conference (iswc), pages612–629..zequn sun, chengming wang, wei hu, muhao chen,jian dai, wei zhang, and yuzhong qu.
2020b.
knowledge graph alignment network with gated.
multi-hop neighborhood aggregation.
in proceed-ings of the 34th aaai conference on artiﬁcial in-telligence (aaai), pages 222–229..zequn sun, qingheng zhang, wei hu, cheng-ming wang, muhao chen, farahnaz akrami, andchengkai li.
2020c.
a benchmarking study ofembedding-based entity alignment for knowledgegraphs.
the vldb endowment,13(11):2326–2340..proceedings of.
xiaobin tang, jing zhang, bo chen, yang yang, hongchen, and cuiping li.
2020. bert-int: a bert-based interaction model for knowledge graph align-ment.
in proceedings of the 29th international jointconference on artiﬁcial intelligence (ijcai), pages3174–3180..bayu distiawan trisedya, jianzhong qi, and ruizhang.
2019. entity alignment between knowledgegraphs using attribute embeddings.
in proceedingsof the 33rd aaai conference on artiﬁcial intelli-gence (aaai), pages 297–304..rakshit trivedi, bunyamin sisman, xin luna dong,christos faloutsos, jun ma, and hongyuan zha.
2018. linknbed: multi-graph representation learn-ing with entity linkage.
in proceedings of the 56thannual meeting of the association for computa-tional linguistics (acl), pages 252–262..apoorv vyas, nataraj jammalamadaka, xia zhu, di-pankar das, bharat kaul, and theodore l willke.
2018. out-of-distribution detection using an ensem-ble of self supervised leave-out classiﬁers.
in pro-ceedings of the european conference on computervision (eccv), pages 550–564..zhichun wang, qingsong lv, xiaohan lan, andyu zhang.
2018. cross-lingual knowledge graphalignment via graph convolutional networks.
inproceedings of the 2018 conference on empiricalmethods in natural language processing (emnlp),pages 349–357..yuting wu, xiao liu, yansong feng, zheng wang, anddongyan zhao.
2019. jointly learning entity and re-lation representations for entity alignment.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 240–249..yuting wu, xiao liu, yansong feng, zheng wang, anddongyan zhao.
2020a.
neighborhood matching net-work for entity alignment.
in proceedings of the58th annual meeting of the association for compu-tational linguistics (acl), pages 6477–6487..yuting wu, xiao liu, yansong feng, zheng wang, anddongyan zhao.
2020b.
neighborhood matching net-work for entity alignment.
in proceedings of the58th annual meeting of the association for compu-tational linguistics (acl), pages 6477–6487..3592hsiu-wei yang, yanyan zou, peng shi, wei lu, jimmylin, and xu sun.
2019. aligning cross-lingual enti-ties with multi-aspect information.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4431–4441..0.8.
0.6.
0.4.
0.2.nnc mr.br.
a degree distribution.
c hyper-parameter settings.
z h -e n e n -z h j a - e n e n -j a fr -e n e n -fr.
figure 8: recall@10 results of entity alignment..framework is extended based on openea (sunet al., 2020c).
we adopt the truncated negativesampling method by bootea (sun et al., 2018)to generate negative triples for mtranse and neg-ative alignment links for alinet, which leads toimproved performance.
the embedding size is 128for mtranse and 256 for alinet.
the batch sizeof mtranse is 20, 480 on zh-en and ja-en, and102, 400 on fr-en.
the batch size of alinet is8, 192 on zh-en and ja-en, and 20, 480 on fr-en.
λ1 = 1.4 in alinet..we select each hyper-parameter setting within awide range of values as follows:.
• learning rate: {0.0001, 0.0002, 0.0005, 0.001}• embedding dimension: {64, 128, 256, 512}• batch size: {4096, 8192, 10240, 20480, 102400}• # fnn layers: {1, 2, 3, 4}• # random targets: {1, 10, 20, 30, 40, 50}• λ: {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.
d recall@10 of entity alignment.
fig.
8 gives the recall@10 results of the mtransevariants with dangling entity detection in the con-solidated evaluation setting.
we can see that therecall@10 results on fr-en are lower than that onzh-en and ja-en, which is similar to the observa-tion in entity alignment §4.3.
from the results, wethink existing embedding-based entity alignmentmethods are still far from being usable in practice..qingheng zhang, zequn sun, wei hu, muhao chen,lingbing guo, and yuzhong qu.
2019. multi-viewknowledge graph embedding for entity alignment.
in proceedings of the 28th international joint con-ference (ijcai), pages 5429–5435..haichao zhu, li dong, furu wei, wenhui wang, bingqin, and ting liu.
2019. learning to ask unanswer-able questions for machine reading comprehension.
in proceedings of the 57th annual meeting of theassociation for computational linguistics (acl),pages 4238–4248..appendices.
fig.
7 shows the degree distribution of the match-able and dangling entities in our dataset againstdbp15k.
although dbp15k contains some enti-ties that are not labeled to have counterparts, bychecking the ills in the recent update of dbpedia,we ﬁnd many of these entities to have counterpartsin the target kg.
hence, these entities in dbp15kcannot act as dangling entities that are key to themore realistic evaluation protocol being proposedin this work.
from the comparison, we can see thatthese unlabeled entities in dbp15k have muchfewer triples than matchable entities.
this biaseddegree distribution will have an adverse effect ondangling entity detection and lead to unreal evalu-ation.
by contrast, in our dataset, matchable anddangling entities have similar degree distribution..figure 7: degree distribution of matchable and dan-gling entities in dbp15k fr-en and our fr-en..b conﬁguration of mtranse and alinet.
for entity alignment, we experiment with mtranse(chen et al., 2017) and the sota method alinet(sun et al., 2020b).
the implementation of our.
3593