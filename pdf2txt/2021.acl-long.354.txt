one2set: generating diverse keyphrases as a set.
jiacheng ye1, tao gui2∗, yichao luo1, yige xu1 and qi zhang1∗1school of computer science, fudan university2institute of modern languages and linguistics, fudan university{yejc19, tgui16, ycluo18, ygxu18, qz}@fudan.edu.cn.
abstract.
recently,the sequence-to-sequence modelshave made remarkable progress on the task ofkeyphrase generation (kg) by concatenatingmultiple keyphrases in a predeﬁned order asa target sequence during training.
however,the keyphrases are inherently an unorderedset rather than an ordered sequence.
imposinga predeﬁned order will introduce wrong biasduring training, which can highly penalizeshifts in the order between keyphrases.
in thiswork, we propose a new training paradigmone2set without predeﬁning an orderto concatenate the keyphrases.
to ﬁt thisparadigm, we propose a novel modelthatutilizes a ﬁxed set of learned control codesas conditions to generate a set of keyphrasesin parallel.
to solve the problem that there isno correspondence between each predictionand target during training, we propose ak-step target assignment mechanism viabipartite matching, which greatly increasesthe diversity and reduces the duplication ratioof generated keyphrases.
the experimentalresults on multiple benchmarks demonstratethat our approach signiﬁcantly outperformsthe state-of-the-art methods..1.introduction.
keyphrase generation (kg) aims to generate of aset of keyphrases that expresses the high-level se-mantic meaning of a document.
these keyphrasescan be further categorized into present keyphrasesthat appear in the document and absent keyphrasesthat do not.
meng et al.
(2017) proposed asequence-to-sequence (seq2seq) model with acopy mechanism (gu et al., 2016) to predict bothpresent and absent keyphrases.
however, themodel needs beam search during inference toovergenerate multiple keyphrases, which cannotdetermine the dynamic number of keyphrases.
to.
∗∗ corresponding authors..figure 1: an example of ground-truth keyphrases(upper) and predictions (lower) under one2seq andone2set training paradigm.
for the one2seqtraining paradigm, although the predictions are correctin each keyphrase, they will still be considered wrongdue to the shift in keyphrase order, and the model willreceive a large penalty..address this, yuan et al.
(2020) proposed theone2seq training paradigm where each sourcetext corresponds to a sequence of keyphrases thatare concatenated with a delimiter (cid:104)sep(cid:105) and aterminator (cid:104)eos(cid:105).
as keyphrases must be orderedbefore being concatenated, yuan et al.
(2020)sorted the present keyphrases by their order of theﬁrst occurrence in the source text and appendedthe absent keyphrases to the end.
during inference,the decoding process terminates when generating(cid:104)eos(cid:105), and the ﬁnal keyphrase predictions areobtained after splitting the sequence by (cid:104)sep(cid:105).
thus, a model trained with one2seq paradigmcan generate a sequence of multiple keyphraseswith dynamic numbers as well as considering thedependency between keyphrases..however, as the keyphrases are inherently anunordered set rather than an ordered sequence,imposing a predeﬁned order usually leads to the fol-lowing intractable problems.
first, the predeﬁnedorder will give wrong bias during training, which.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4598–4608august1–6,2021.©2021associationforcomputationallinguistics4598  deep learning <sep>    topic  model   <eos>   topic model   <sep>     deep learning <eos>  deep learning <eos>    topic  model  <eos>   topic   model  <eos>    deep learning <eos>(a)                   (b)                   one2seqone2setcan highly penalize shifts in the order betweenkeyphrases.
as shown in figure 1 (a), the modelmakes correct predictions in each keyphrase but canstill receive a large loss during training.
second,this increases the difﬁculty of model training.
forexample, the absent keyphrases are appended to theend in an author-deﬁned order in yuan et al.
(2020),however, different authors can have various sortingbases, which makes it difﬁcult for the model tolearn a uniﬁed pattern.
third, the model is highlysensitive to the predeﬁned order, as shown in menget al.
(2019), and can suffer from error propagationduring inference when previously having generatedkeyphrases with an incorrect order.
lately, chanet al.
(2019) proposed a reinforcement learning-based ﬁne-tuning method, which ﬁne-tunes thepre-trained models with metric-based rewards (i.e.,recall and f1) for generating more sufﬁcient andaccurate keyphrases.
however, this method canalleviate the impact of the order problems whenﬁne-tuning but needs to be pre-trained under theone2seq paradigm to initialize the model, whichcan still introduce wrong biases..to address this problem, we propose a new train-ing paradigm one2set where the ground-truthtarget is a set rather than a keyphrase-concatenatedsequence.
however, the vanilla seq2seq model cangenerate a sequence but not a set.
hence, we intro-duce a set prediction model that adopts transformer(vaswani et al., 2017) as the main architecturetogether with a ﬁxed set of learned control codes asadditional decoder inputs to perform controllablegeneration.
for each code, the model generates acorresponding keyphrase for the source documentor a special ∅ token that represents the meaningof “no corresponding keyphrase”.
during training,the cross-entropy loss cannot be directly used sincewe do not know the correspondence between eachprediction and target.
hence, we introduce ak-step target assignment mechanism, where weﬁrst auto-regressively generate k words for eachcode and then assign targets via bipartite matchingbased on the predicted words.
after that, we cantrain each code using teacher forcing as before.
compared with the previous models, the proposedmethod has the following advantages: (a) there isno need to predeﬁne an order to concatenate thekeyphrases, thus the model will not be affected bythe wrong biases in the whole training stage; and (b)the bipartite matching forces unique predictions foreach code, which greatly reduces the duplication.
ratio and increases the diversity of predictions..we summarize our main contributions as follows:(1) we propose a new training paradigm one2setwithout predeﬁning an order to concatenate thekeyphrases; (2) we propose a novel set predictionmodel that can generate a set of diverse keyphrasesin parallel and a dynamic target assignment mech-anism to solve the intractable training problemunder the one2set paradigm; (3) our methodconsistently outperforms all the state-of-the-artmethods and greatly reduces the duplication ratio.
our codes are publicly available at github1..2 related work.
2.1 keyphrase extraction.
existing approaches for keyphrase prediction canbe broadly divided into extraction and generationmethods.
early work mostly focuses on thekeyphrase extraction task, and a two-step strategyis typically designed (hulth, 2003; mihalcea andtarau, 2004; nguyen and kan, 2007; wan andxiao, 2008).
first, they extract a large set ofcandidate phrases by hand-crafted rules (mihalceaand tarau, 2004; medelyan et al., 2009; liuet al., 2011).
then, these candidates are scoredand reranked based on unsupervised methods(mihalcea and tarau, 2004; wan and xiao, 2008)or supervised methods (hulth, 2003; nguyen andkan, 2007).
other extractive approaches utilizeneural-based sequence labeling methods (zhanget al., 2016; gollapalli et al., 2017)..2.2 keyphrase generation.
compared to extractive approaches, generativeones have the ability to consider the absentkeyphrase prediction.
meng et al.
(2017) proposeda generative model copyrnn, which employsa encoder-decoder framework (sutskever et al.,2014) with attention (bahdanau et al., 2015) andcopy mechanisms (gu et al., 2016).
many worksare proposed based on the copyrnn architecture(chen et al., 2018; zhao and zhang, 2019; chenet al., 2019b,a)..in previous copyrnn based works, each sourcetext corresponds to a single target keyphrase.
thus,the model needs beam search during inference toovergenerate multiple keyphrases, which cannotdetermine the dynamic number of keyphrasesand consider the inter-relation among keyphrases..1https://github.com/jiacheng-ye/kg_.
one2set.
4599figure 2: architecture of settrans model with n learned control codes as input conditions.
a k-step targetassignment mechanism is used during training, where we ﬁrst predict k words for each code, and then ﬁnd anoptimal allocation among the predictions and targets.
in the ﬁgure, n = 8 and k = 2 are used..to this end, yuan et al.
(2020) proposed anone2seq training paradigm where each sourcetext corresponds to a sequence of concatenatedkeyphrases.
thus, the model can capture thecontextual information between the keyphrasesas well as determines the dynamic number ofkeyphrases for different source texts.
the recentworks (chan et al., 2019; chen et al., 2020; swami-nathan et al., 2020) mostly follow the one2seqtraining paradigm.
chan et al.
(2019) proposed anrl-based ﬁne-tuning method using f1 and recallmetrics as rewards.
swaminathan et al.
(2020)proposed an rl-based ﬁne-tuning method using adiscriminator to produce rewards.
all the abovemodels need to be trained or pre-trained underthe one2seq paradigm.
as keyphrases mustbe ordered before concatenating and keyphrasesare inherently an unordered set, the model can betrained with wrong signal.
our one2set trainingparadigm aims to solve this problem..3 methodology.
this paper proposes a new training paradigmone2set for keyphrase generation.
a set predic-tion model based on transformer (settrans) isproposed to ﬁt this paradigm, as shown in figure 2.given a ﬁxed set of learned control codes as inputconditions, the model generates a keyphrase or aspecial ∅ token for each code in parallel.
duringtraining, a k-step target assignment mechanismis proposed to dynamically determine the targetcorresponding to each code.
the main idea is thatthe model ﬁrst freely predicts k steps without anysupervision to see what keyphrase each code canroughly generate, and then use bipartite matchingto ﬁnd the optimal allocation based on the model’s.
conjecture and target.
given the correspondence ofeach code and target, a separate set loss is then usedto correct the model’s conjecture, where half of thecodes are trained to predict the present keyphraseset and the others are trained to predict the absentkeyphrase set..3.1 the one2set training paradigm.
we ﬁrst formally describe the keyphrase generationtask as follows.
given a document x, it’s aimedto predict a set of keyphrases y = {yi}i=1,...,|y|,where |y| is the number of keyphrases.
to solvethe kg task, previous works typically adopted anone2one training paradigm (meng et al., 2017)or one2seq training paradigm (yuan et al., 2020).
the difference between the two training paradigmsis that the form of training samples is different.
speciﬁcally, in the one2one training paradigm,each original sample pair (x, y) is divided intomultiple pairs {(x, yi)}i=1,...,|y| to perform train-ing independently.
in the one2seq trainingparadigm, each original sample pair is processed as(x, f (y)), where f (y) is a sequence of keyphrasesafter the reordering and concatenating operation..to solve the wrong bias problem caused bythe one2seq training paradigm, we proposethe one2set training paradigm, where eachoriginal sample pair is kept still as (x, y).
hence,the sample used in training is consistent withthe original sample, which avoids the intractableproblem introduced by the additional processing(i.e., dividing or concatenating)..3.2 the settrans model.
we adopt the transformer (vaswani et al., 2017) asthe backbone encoder-decoder framework.
how-.
4600transformer encodertransformer decoder1bos+1+2+1+1bos+4+2deep+4+...1bos+5+2neural+5+1bos+8+2neural+8+...topictopicmodeldeeplearning...neuralmodelneuralmodel.........set of present keyphraseset of absent keyphrasedeep learningtopic modelneural networkvector quantizationsource documentpositioncontrol codek-step target assignmenttoken????
ever, the vanilla transformer can only generate a se-quence but not a set.
to predict a set of keyphrases,we propose settrans model that utilizes a set oflearned control codes as additional decoder inputs.
by performing generation conditioned on eachcontrol code, we can generate a set of keyphrases inparallel.
to decide suitable numbers of keyphrasesfor different given documents, we ﬁx the totallength of the control codes to a sufﬁcient numbern , and introduce a special ∅ token that representsthe meaning of “no corresponding keyphrase”.
hence, we can determine the appropriate numberof keyphrases for an input document after removingall the ∅ tokens from the n predictions..formally, the decoder input at time step t for.
control code n is deﬁned as follows:.
t = ewdnynt−1.
+ ep.
t + cn,.
(1).
is the embedding of word yn.
t−1, epwhere ewyntt−1is the t-th sinusoid positional embedding as in(vaswani et al., 2017) and cn is the n-th learnedcontrol code embedding.
the decoder outputs thepredictive distribution pnt , which is used to getthe next word yn.
as some keyphrases containtwords that do not exist in the predeﬁned vocabularybut appear in the input document, we also employa copy mechanism (see et al., 2017), which isgenerally adopted for many previous kg works(meng et al., 2017; chan et al., 2019; chen et al.,2020; yuan et al., 2020)..3.3 training.
the main difﬁculty of training under the one2setparadigm is that the correspondence between eachprediction and ground-truth keyphrase is unknown,so that the cross-entropy loss cannot be directlyused.
hence, we introduce a k-step target as-signment mechanism to assign the ground-truthkeyphrase for each prediction, and a separate setloss to train the model in an end-to-end way..3.3.1 k-step target assignmentwe ﬁrst generate k words for each control codeand collect the corresponding predictive probabilitydistributions of each step.
formally, we denotep = {pn}n=1,...,n , where pn = {pnt }t=1,...,kand pnt is the predictive distribution at time step tfor control code n..then, we ﬁnd a bipartite matching between theground-truth keyphrases and predictions.
assum-ing the predeﬁned number of control codes n is.
larger than the number of ground-truth keyphrases,we consider the ground-truth keyphrases also asa set of size n padded with ∅.
note that thebipartite matching enforces permutation-invariance,and guarantees that each target element has aunique match.
thus, it reduces the duplication ratioof predictions.
speciﬁcally, as shown in figure2, both the ﬁfth and eighth control code predictthe same keyphrase “neural model”, but one ofthem is assigned with ∅.
the eighth code canperceive that this keyphrase has been generated byanother code.
hence, the control codes can learntheir mutual dependency during training and notgenerate duplicated keyphrases..formally, to ﬁnd a bipartite matching betweensets of ground-truth keyphrases and predictions, wesearch for a permutation ˆπ with the lowest cost:.
ˆπ = arg minπ∈π(n ).
n(cid:88).
n=1.
cmatch.
(cid:16).
yn, pπ(n)(cid:17).
,.
(2).
where π(n ) is the space of all n -length permuta-(cid:0)yn, pπ(n)(cid:1) is a pair-wise matchingtions, cmatchcost between the ground truth yn and distributionsof a prediction sequence with index π(n).
thisoptimal assignment is computed efﬁciently with thehungarian algorithm (kuhn, 1955).
the matchingcost takes into account the class predictions, whichcan be deﬁned as follows:.
s(cid:88).
t=1.
cmatch.
(cid:16).
yn, pπ(n)(cid:17).
= −.
1{yn.
t (cid:54)=∅}pπ(n).
t.(yn.
t ) ,.
(3)where s = min(|yn|, k) is the minimum sharedlength between the target and predicted sequence,pπ(n)t ) denotes the probability of word yn(yninttpπ(n), and we ignore the score from matchingtpredictions with ∅, which ensures that valid targets(i.e., non-∅ targets) can be allocated to predictionswith as higher predictive probability as possible..3.3.2 separate set loss.
given the correspondence between each code andtarget, we can train the model to predict a singletarget set, which is deﬁned as follows:.
l(θ) = −.
log pˆπ(n)t.(yn.
t ) ,.
(4).
n(cid:88).
|yn|(cid:88).
n=1.
t=1.
t.where pˆπ(n)is the predictive probability distribu-tion using teacher forcing.
however, predictingpresent and absent keyphrases requires the model.
4601to have different capabilities, we propose a separateset loss to ﬂexibly take this bias into account in auniﬁed model.
speciﬁcally, we ﬁrst separate thecontrol codes into two ﬁxed sets with equal size ofn/2, which is denoted as c1 and c2, and the targetkeyphrase set y into present target keyphrase sety pre and absent target keyphrase set y abs.
finally,the bipartite matching is performed on the twosets separately, namely, we ﬁnd a permutation ˆπpreusing y pre and predictions from c1, and ˆπabs usingy abs and predictions from c2.
thus, we can modifythe ﬁnal loss in equal 4 as follows:.
l(θ) = −(.
log pˆπpre(n)t.(ynt ).
n/2(cid:88).
|yn|(cid:88).
n=1.
t=1.
n(cid:88).
|yn|(cid:88).
+.
n=n/2+1.
t=1.
(5).
log pˆπabs(n)t.(yn.
t ))..in practice, we down-weight the log-probabilityterm when ynt = ∅ by scale factors λpre and λabsfor present keyphrase set and absent keyphrase setto account for the class imbalance..4 experimental setup.
4.1 datasets.
we conduct our experiments on ﬁve scientiﬁc arti-cle datasets, including inspec (hulth, 2003), nus(nguyen and kan, 2007), krapivin (krapivin et al.,2009), semeval (kim et al., 2010) and kp20k(meng et al., 2017).
each sample from thesedatasets consists of a title, an abstract, and somekeyphrases.
following previous works (menget al., 2017; chen et al., 2019b,a; yuan et al.,2020), we concatenate the title and abstract as asource document.
we use the largest dataset (i.e.,kp20k) to train all the models.
after preprocessing(i.e., lowercasing, replacing all the digits with thesymbol (cid:104)digit(cid:105) and removing the duplicated data),the ﬁnal kp20k dataset contains 509,818 samplesfor training, 20,000 for validation, and 20,000 fortesting.
the dataset statistics are shown in table 1..4.2 baselines.
we focus on the comparisons with the followingstate-of-the-art methods as our baselines:.
• catseq (yuan et al., 2020).
the rnn-basedseq2seq model with copy mechanism trainedunder one2seq paradigm..• catseqtg (chen et al., 2019b).
an extensionof catseq with additional title encoding andcross-attention..datasetinspecnuskrapivinsemevalkp20k.
#samples avg.
#kp avg.
|kp| % of abs.kp.
50021140010020,000.
9.7910.815.8314.435.26.
2.482.222.212.382.04.
26.4245.3644.3355.6137.23.table 1: statistics of the testing set on ﬁve datasets.
#kp: number of keyphrases.
|kp|: length of keyphrase.
abs.kp: absent keyphrases..• catseqtg-2rf1 (chan et al., 2019).
anextension of catseqtg with rl-based ﬁne-tuning using f1 and recall metrics as rewards.
• ganm r (swaminathan et al., 2020).
anextension of catseq with rl-based ﬁne-tuningusing a discriminator to produce rewards.
• exhird-h (chen et al., 2020).
an extensionof catseq with a hierarchical decoding methodand an exclusion mechanism to avoid generat-ing duplicated keyphrases..in this paper, we propose two transformer-based.
models that are denoted as follows:.
• transformer..a transformer-basedmodel with copy mechanism trained underone2seq paradigm..• settrans.
an extension of transformerwith additional control codes trained underone2set paradigm..4.3.implementation details.
following previous works (chan et al., 2019; chenet al., 2020; yuan et al., 2020), when training underthe one2seq paradigm,the target keyphrasesequence is the concatenation of present and absentkeyphrases, with the present keyphrases are sortedaccording to the orders of their ﬁrst occurrencesin the document and the absent keyphrase kept intheir original order.
we use a transformer structuresimilar to vaswani et al.
(2017), with six layersand eight self-attention heads, 2048 dimensions forhidden states.
in the training stage, we choose thetop 50,002 frequent words to form the predeﬁnedvocabulary and set the embedding dimension to512. we use the adam optimization algorithm(kingma and ba, 2015) with a learning rate of0.0001, and a batch size of 12. during testing, weuse greedy search as the decoding algorithm.
weset the number of control codes to 20 as we ﬁndit covers 99.5% of the samples in the validationset.
we use a number of two for target assignmentsteps k based on the average keyphrase length onthe validation set, a factor of 0.2 and 0.1 for λpre.
4602model.
catseq (yuan et al., 2020)catseqtg (chen et al., 2019b)catseqtg-2rf1 (chan et al., 2019)ganm r (swaminathan et al., 2020)exhird-h (chen et al., 2020)transformer (one2seq)settrans (one2set).
inspec.
nus.
krapivin.
semeval.
kp20k.
f1@50.2250.2290.2530.2580.2530.28150.2853.f1@m f1@50.3230.2620.3250.2700.3750.3010.3480.299-0.2910.32560.37070.406120.3243.f1@m f1@50.2690.3970.2820.3930.3000.4330.2880.4170.286-0.31580.419100.326120.4507.f1@m f1@50.2420.3540.2460.3660.3690.2870.369-0.2840.3470.287140.36550.331200.36412.f1@m f1@50.2910.2830.2920.2900.3210.3290.303-0.3110.3350.33210.325150.35850.35713.f1@m0.3670.3660.3860.3780.3740.37710.3924.table 2: present keyphrases prediction results of all models.
the best results are bold.
the subscript represents thecorresponding standard deviation (e.g., 0.3924 indicates 0.392±0.004)..model.
catseq (yuan et al., 2020)catseqtg (chen et al., 2019b)catseqtg-2rf1 (chan et al., 2019)ganm r (swaminathan et al., 2020)exhird-h (chen et al., 2020)transformer (one2seq)settrans (one2set).
inspec.
nus.
krapivin.
semeval.
kp20k.
f1@50.0040.0050.0120.0130.0110.01020.0211.f1@m f1@50.0160.0080.0110.0110.0190.0210.0260.019-0.0220.02820.01940.04220.0343.f1@m f1@50.0180.0280.0180.0180.0300.0310.0420.0380.022-0.03210.04820.04770.0604.f1@m f1@50.0160.0360.0110.0340.0210.053-0.0570.0170.0430.02050.06040.02630.07311.f1@m f1@50.0150.0280.0150.0180.0270.0300.032-0.0160.0250.02310.02330.03620.0345.f1@m0.0320.0320.0500.0450.0320.04610.0583.table 3: absent keyphrases prediction results of all models.
the best results are bold.
the subscript represents thecorresponding standard deviation (e.g., 0.0583 indicates 0.058±0.003)..and λabs respectively based on the validation set.
we conduct the experiments on a geforce rtx2080ti gpu, repeat three times using differentrandom seeds, and report the averaged results..4.4 evaluation metrics.
we follow previous works (chan et al., 2019;chen et al., 2020) and use macro-averaged f1@5and f1@m for both present and absent keyphrasepredictions.
f1@m compares all the keyphrasespredicted by the model with the ground-truthkeyphrases, which means it considers the numberof predictions.
for f1@5, when the predictionnumber is less than ﬁve, we randomly appendincorrect keyphrases until it obtains ﬁve predictions.
if we do not adopt such an appending operation,f1@5 will become the same with f1@m whenthe prediction number is less than ﬁve as shown inchan et al.
(2019).
we apply the porter stemmerbefore determining whether two keyphrases areidentical and remove all the duplicated keyphrasesafter stemming..5 results and analysis.
5.1 present and absent keyphrase.
predictions.
table 2 and table 3 show the performance evalua-tions of the present and absent keyphrase, respec-tively.
we observe that the proposed settransmodel consistently outperforms almost all the.
previous state-of-the-art models on both f1@5and f1@m metrics by a large margin, whichdemonstrates the effectiveness of our methods.
asnoted by previous works (chan et al., 2019; yuanet al., 2020) that predicting absent keyphrases fora document is an extremely challenging task, thusthe performance is much lower than that of presentkeyphrase prediction.
regarding the comparisonof our transformer model trained under one2seqparadigm and settrans model trained underone2set paradigm, we ﬁnd settrans modelconsistently improves both keyphrase extractiveand generative ability by a large margin on almostall the datasets, and maintains the performanceof present keyphrase prediction on the inspecand krapivin datasets, which demonstrates theadvantages of one2set training paradigm..5.2 diversity of predicted keyphrases.
to investigate the model’s ability to generatediverse keyphrases, we measure the average num-bers of unique present and absent keyphrases,and the average duplication ratio of all the pre-dicted keyphrases.
the results are reported intable 4. based on the results, we observe thatour settrans model generates more uniquekeyphrases than other baselines by a large margin,as well as achieves a signiﬁcantly lower duplicationratio.
note that exhird-h speciﬁcally designed adeduplication mechanism to remove duplication inthe inference stage.
in contrast, our model achieves.
4603model.
oraclecatseqcatseqtgcatseqtg-2rf1exhird-htransformersettrans.
krapivin#pk #ak dup2.593.240.673.500.833.823.281.561.024.411.394.442.204.83.
-0.460.410.290.140.290.08.semeval#pk #ak dup8.316.120.773.481.093.821.503.570.993.651.524.302.184.62.
-0.530.630.250.090.270.08.kp20k#pk #ak dup1.953.310.553.710.673.773.551.440.813.971.164.642.015.10.
-0.390.360.280.110.260.08.table 4: number and duplication ratio of predictedkeyphrases on three datasets.
“#pk” and “#ak” arethe average number of unique present and absentkeyphrases respectively.
“dup” refers to the averageduplication ratio of predicted keyphrases.
“oracle”refers to the gold average keyphrase number..a lower duplication ratio without any deduplica-tion mechanism, which proves its effectiveness.
however, we also observe that our model tendsto overgenerate more present keyphrases than theground-truth on the krapivin and kp20k datasets.
we analyze that different datasets have differentpreferences for the number of keyphrases, whichwe leave as our future work..5.3 ablation study.
to understand the effects of each component of thesettrans model, we conduct an ablation studyon it and report the results on the kp20k dataset intable 5..effects of model architecture to verify theeffectiveness of the model architecture of set-trans, we remove the control codes and ﬁnd themodel is completely broken.
the duplication ratioincreases to 0.95, which means all the 20 controlcodes predict the same keyphrase.
this occursbecause when the control codes are removed, allthe predictions depend on the same condition(i.e., the source document) without any distinction.
this demonstrates that the control codes play anextremely important role in the settrans model..effects of target assignment the major difﬁ-culty for successfully training under one2setparadigm is the target assignment between pre-dictions and targets.
an attempt is ﬁrst made toremove the k-step target assignment mechanism,which means that we employ a ﬁxed sequentialmatching strategy as in the one2seq paradigm.
from the results, we observe that both the presentand absent keyphrase performances degrade, thenumber of predicted keyphrases also drops dramat-ically, and the duplication ratio increased greatlyby 18%.
we analyze the reasons as follows: (1).
model.
oraclesettransmodel architecture- control codestarget assignment- k-step assign+ random assignset loss- teacher forcing- separate set loss.
present.
absent.
f1@5 f1@m #pk f1@5 f1@m #ak1.952.01.
-0.358.
-0.036.
-0.392.
-0.058.
3.315.10.dup.
-0.08.
0.001.
0.002.
0.01.
0.000.
0.000.
0.00.
0.95.
0.2650.005.
0.3810.010.
2.641.05.
0.0200.001.
0.0450.002.
0.0010.355.
0.0020.383.
0.015.31.
0.0000.016.
0.0000.031.
0.810.04.
0.000.55.
0.260.95.
0.890.05.
“-.
teacher.
table 5: ablation study of settrans on kp20kdataset.
to directlycalculating the loss after target assignment in a studentforcing schema.
“- separate set loss” refers to using asingle set loss..forcing” refers.
the dynamic characteristics of the k-step targetassignment remove unnecessary position constraintduring training, which encourages the model togenerate more keyphrases.
speciﬁcally, the modelcan generate a keyphrase in any location ratherthan only in the given position.
thus, the modeldoes not need to consider the position constraintduring the generation and encourages all the controlcodes to predict keyphrases rather than only theﬁrst few codes, which will be veriﬁed in section(2) the bipartite characteristics of the k-5.6.step target assignment forces the model to predictunique keyphrases, which reduces the duplicationratio of predictions.
when predictions from twocodes are similar, only one code may be assigneda target keyphrase, and the other is assigned a∅ token.
thus, the model can be very carefulabout each prediction to prevent duplication.
wefurther experiment that replacing the k-step targetassignment with a random assignment, and we ﬁndthat the results are similar to those when removingthe control codes.
this is because the randomassignment misleads the learning of the controlcodes and causes them to become invalid..effects of set loss as discussed in section3.3.2,teacher forcing and a separate set lossare used to train the model after assigning atarget for each prediction.
we investigate theireffects in detail.
the results show the following.
(1) teaching forcing can alleviate the cold startproblem.
after removing teaching forcing, themodel faces a cold start problem, in other words,the lack of supervision information leads to a poorprediction, and the target assignment is thereforenot ideal, which causes the model to fail at theearly stage of training.
(2) a separate set loss helpsin both present and absent keyphrase predictions.
4604figure 3: performance and number of predictions forpresent and absent keyphrase under different loss scalefactors λ for ∅ token on kp20k dataset.
we set bothλpre and λabs to λ to simplify the comparison..figure 4: performance and training/inference speedupcompared with transformer over differenttargetassignment steps k on kp20k dataset..but also increases the duplication ratio slightlycompared with a single set loss.
as producingcorrect present keyphrases is an easier task, themodel tends to generate present keyphrases onlywhen using a single set loss.
our separate setloss can infuse different inductive biases into thetwo sets of control codes, which makes them morefocused on generating one type of keyphrase (i.e.,the present one or absent one).
thus, it increasesthe accuracy of the predictions and encouragesmore absent keyphrase predictions.
however,because bipartite matching is performed separately,the constraint of unique prediction does not existbetween the two sets, which leads to a slightincrease in the duplication ratio..5.4 performance over scale factors.
in this section, we conduct experiments on kp20kdataset to evaluate performance under different lossscale factors λ for ∅ token.
the results are shownin figure 3..the left part of the ﬁgure shows that when λ =0.2, the performances on both present and absentkeyphrases are consistently better than the resultswhen λ = 0.1. however, a scale factor larger than0.1 improves the present keyphrase performance,but also harms the absent keyphrase performance.
as we can see from the right part of the ﬁgure,the number of predictions decreases consistentlyfor both the present and absent keyphrases whenthe scale factor becomes larger.
this is becausea larger scale factor causes the model to predictmore ∅ tokens to reduce the loss penalty duringtraining.
moreover, we also ﬁnd that the precisionmetric p @m will increases when the number ofpredictions decreases.
while the effect of thedecrease in the recall metric r@m is even greaterwhen the number is too small, which leads to adegradation in the overall metric f1@m ..5.5 efﬁciency over assignment steps.
in this section, we study the inﬂuence of targetassignment steps k on the prediction performanceand efﬁciency compared with transformer..as shown in the left part of figure 4, we notethat when k is equal to 1, the improvement ofsettrans over transformer is relatively lowerthan when it is equal to 2 (i.e.,the averagelength of keyphrase).
this is mainly becausesome keyphrases that have the same ﬁrst wordcannot be distinguished during training, whichcould interfere with the learning of control codes.
the right part of figure 4 shows the training andinference speedup with various k compared withthe transformer.
we note settrans could beslower than transformer at the training stage, anda smaller k could alleviate this problem.
forperformance and efﬁciency considerations, weconsider 2 to be an appropriate value for stepsk. moreover, as k is only used in the trainingstage, settrans is 6.44 times invariably fasterthan transformer on the inference stage.
this isbecause that with different control codes as inputcondition, all the keyphrases can be generated inparallel on the gpu.
hence, in addition to betterperformance than transformer, settrans alsohas great advantages in the inference efﬁciency..5.6 analysis of learned control codes.
our analysis here is driven by two questions fromsection 5.3:.
(1) whether the k-step target assignment mech-anism encourages all the control codes to predictkeyphrases rather than only the ﬁrst few codes?.
(2) whether the separate set loss makes thecontrol codes more focused on generating one typeof keyphrase (i.e., present or absent) compared tothe single set loss?.
to investigate these two questions, we measurethe ratio of present and absent keyphrase predic-.
4605present absentpresent f1@m0.250.300.350.400.45absent f1@m0.010.020.030.040.050.06scale factor λ00.10.20.30.40.5present  absentnumber of predictions0246810scale factor λ00.10.20.30.40.5training inferencetraining speedup0.500.550.600.650.70inference speedup6.26.36.46.56.66.7target assignment steps k123456present absentδf1@m00.0050.0100.0150.020target assignment steps k123456existing state-of-the-art models.
we also show thatsettrans has great advantages in the inferenceefﬁciency compared with the transformer underone2seq paradigm..acknowledgments.
the authors wish to thank the anonymous reviewersfor their helpful comments.
this work was partiallyfunded by china national key r&d program(no.
2017yfb1002104), national natural sciencefoundation of china (no.
62076069, 61976056),shanghai municipal science and technology ma-jor project (no.2021shzdzx0103)..references.
dzmitry bahdanau, kyunghyun cho, and yoshuabengio.
2015. neural machine translation by jointlylearning to align and translate.
in 3rd internationalconference on learning representations,iclr2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..hou pong chan, wang chen, lu wang, and irwinneural keyphrase generation viaking.
2019.inreinforcement learning with adaptive rewards.
proceedings ofthethe 57th annual meeting ofassociation for computational linguistics, pages2163–2174, florence, italy.
association for compu-tational linguistics..jun chen, xiaoming zhang, yu wu, zhao yan,keyphrase generationand zhoujun li.
2018.in proceedings ofwith correlation constraints.
the 2018 conference on empirical methods innatural language processing, pages 4057–4066,brussels, belgium.
association for computationallinguistics..wang chen, hou pong chan, piji li, lidong bing,and irwin king.
2019a.
an integrated approachfor keyphrase generation via exploring the power ofretrieval and extraction.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 2846–2856, minneapolis, minnesota.
association for computational linguistics..wang chen, hou pong chan, piji li, and irwinexclusive hierarchical decodingking.
2020.in proceedingsfor deep keyphrase generation.
ofthe associationfor computational linguistics, pages 1095–1105,online.
association for computational linguistics..the 58th annual meeting of.
wang chen, yifan gao, jiani zhang, irwin king, andmichael r. lyu.
2019b.
title-guided encoding forin the thirty-third aaaikeyphrase generation.
conference on artiﬁcial intelligence, aaai, pages6268–6275.
aaai press..figure 5: ratio of present and absent keyphrasepredictions for all the control codes on kp20k dataset.
thethe subgraphs from top to bottom are for“w/o k-step target assignment”, “a single set loss”,and “a separate set loss” cases, respectively.
thesummation of the ratios of the present keyphrases,absent keyphrases and ∅ equals to 100% for each code..tions for all the control codes on the kp20k dataset,which is shown in figure 5. as shown in the topand middle subﬁgures, we observe that withoutthe target assignment mechanism, many controlcodes are invalid (i.e., only predicting ∅), andonly the ﬁrst small part performs valid predictions.
moreover, when there are already very few validpredictions, the model still has a duplication ratioof up to 26%, as shown in table 5, resulting in aneven smaller number of ﬁnal predictions.
after theintroduction of the target assignment mechanism,most of the codes can generate valid keyphrases,which increases the number of predictions..however, as shown in the middle subﬁgure,most of the control code tends to generate morepresent keyphrases than absent keyphrases whenusing a single set loss.
when using a separateset loss in the bottom subﬁgure, the two parts aremore inclined to predict only present and absentkeyphrases respectively, which also increases thenumber of absent keyphrase predictions..6 conclusions.
in this paper, we propose a new training paradigmone2set without predeﬁning an order to concate-nate the keyphrases, and a novel model settransthat predicts a set of keyphrases in parallel.
tosuccessfully train under one2set paradigm, wepropose a k-step target assignment mechanismand a separate set loss, which greatly increases thenumber and diversity of the generated keyphrases.
experiments show that our method gains signif-icantly huge performance improvements against.
4606pre.kp ratioabs.kp ratio050100050100050100012345678910111213141516171819sujatha das gollapalli, xiaoli li, and peng yang.
2017.incorporating expert knowledge into keyphraseextraction.
in proceedings of the thirty-first aaaiconference on artiﬁcial intelligence, february 4-9,2017, san francisco, california, usa, pages 3180–3187. aaai press..jiatao gu, zhengdong lu, hang li, and victor o.k.
incorporating copying mechanism inli.
2016.in proceedings ofsequence-to-sequence learning.
the 54th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1631–1640, berlin, germany.
association forcomputational linguistics..anette hulth.
2003..improved automatic keywordextraction given more linguistic knowledge.
inproceedings of the 2003 conference on empiricalmethods in natural language processing, pages216–223..su nam kim, olena medelyan, min-yen kan, andtimothy baldwin.
2010.semeval-2010 task 5: automatic keyphrase extraction from scientiﬁcin proceedings of the 5th internationalarticles.
workshop on semantic evaluation, pages 21–26,uppsala, sweden.
association for computationallinguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..mikalai krapivin, aliaksandr autaeu, and mauriziolarge dataset for keyphrases.
marchese.
2009.extraction.
technical report, university of trento..harold w kuhn.
1955. the hungarian method forthe assignment problem.
naval research logisticsquarterly, 2(1-2):83–97..rui meng, sanqiang zhao, shuguang han, daqinghe, peter brusilovsky, and yu chi.
2017. deepthekeyphrase generation.
55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 582–592, vancouver, canada.
association forcomputational linguistics..in proceedings of.
rada mihalcea and paul tarau.
2004..textrank:in proceedings of thebringing order into text.
2004 conference on empirical methods in naturallanguage processing, pages 404–411, barcelona,spain.
association for computational linguistics..thuy dung nguyen and min-yen kan. 2007.keyphrase extraction in scientiﬁc publications.
ininternational conference on asian digital libraries,pages 317–326.
springer..get.
abigail see, peter j. liu, and christopher d. manning.
to the point: summarization with2017.in proceedings ofpointer-generator networks.
the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 1073–1083, vancouver, canada.
associationfor computational linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informationprocessing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..avinash swaminathan, haimin zhang, debanjanmahata, rakesh gosangi, rajiv ratn shah, andamanda stent.
2020. a preliminary exploration ofin proceedingsgans for keyphrase generation.
ofthe 2020 conference on empirical methodsin natural language processing (emnlp), pages8021–8030, online.
association for computationallinguistics..zhiyuan liu, xinxiong chen, yabin zheng, andmaosong sun.
2011. automatic keyphrase extrac-in proceedingstion by bridging vocabulary gap.
the fifteenth conference on computationalofnatural language learning, pages 135–144, port-land, oregon, usa.
association for computationallinguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..olena medelyan, eibe frank, and ian h. witten.
2009. human-competitive tagging using automaticthekeyphrase extraction.
2009 conference on empirical methods in naturallanguage processing, pages 1318–1327, singapore.
association for computational linguistics..in proceedings of.
rui meng, xingdi yuan, tong wang,.
peterbrusilovsky, adam trischler, and daqing he.
2019. does order matter?
an empirical study ongenerating multiple keyphrases as a sequence.
inarxiv:1909.03590 [cs]..xiaojun wan and jianguo xiao.
2008..singledocument keyphrase extraction using neighborhoodknowledge.
in aaai, volume 8, pages 855–860..xingdi yuan, tong wang, rui meng, khushboothaker, peter brusilovsky, daqing he, and adamtrischler.
2020. one size does not ﬁt all: generatinginand evaluating variable number of keyphrases.
thethe 58th annual meeting ofproceedings ofassociation for computational linguistics, pages7961–7975, online.
association for computationallinguistics..4607qi zhang, yang wang, yeyun gong, and xuanjinghuang.
2016. keyphrase extraction using deepin proceed-recurrent neural networks on twitter.
ings of the 2016 conference on empirical methodsin natural language processing, pages 836–845,austin, texas.
association for computational lin-guistics..jing zhao and yuxiang zhang.
2019..incorporatinglinguistic constraints into keyphrase generation.
intheproceedings ofassociation for computational linguistics, pages5224–5233, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
4608