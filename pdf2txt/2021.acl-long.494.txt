dual graph convolutional networks for aspect-based sentiment analysis.
ruifan li1∗, hao chen1, fangxiang feng1,zhanyu ma1, xiaojie wang1, and eduard hovy21 school of artiﬁcial intelligence, beijing university of posts and telecommunications, china2 language technologies institute, carnegie mellon university, usa{rfli, ccchenhao997, fxfeng, mazhanyu, xjwang}@bupt.edu.cnhovy@cmu.edu.
abstract.
aspect-based sentiment analysis is a ﬁne-grained sentiment classiﬁcation task.
re-cently, graph neural networks over depen-dency trees have been explored to explicitlymodel connections between aspects and opin-ion words.
however, the improvement is lim-ited due to the inaccuracy of the dependencyparsing results and the informal expressionsand complexity of online reviews.
to over-come these challenges, in this paper, we pro-pose a dual graph convolutional networks (du-algcn) model that considers the complemen-tarity of syntax structures and semantic cor-relations simultaneously.
particularly, to al-leviate dependency parsing errors, we designa syngcn module with rich syntactic knowl-edge.
to capture semantic correlations, wedesign a semgcn module with self-attentionmechanism.
furthermore, we propose or-thogonal and differential regularizers to cap-ture semantic correlations between words pre-cisely by constraining attention scores in thesemgcn module.
the orthogonal regular-izer encourages the semgcn to learn seman-tically correlated words with less overlap foreach word.
the differential regularizer encour-ages the semgcn to learn semantic featuresthat the syngcn fails to capture.
experimen-tal results on three public datasets show thatour dualgcn model outperforms state-of-the-art methods and verify the effectiveness of ourmodel..1.introduction.
sentiment analysis has become a popular topic innatural language processing (liu, 2012; li andhovy, 2017).
aspect-based sentiment analysis(absa) talks an entity-level oriented ﬁne-grainedsentiment analysis task that aims to determine sen-timent polarities of given aspects in a sentence.
in.
∗corresponding author..figure 1: an example sentence with its dependencytree from the restaurant reviews.
this sentence containstwo aspects but with opposite sentiment polarities..figure 1, the comment is about a restaurant review.
the sentiment polarity of the two aspects “price”and “service” are positive and negative, respec-tively.
thus, absa can precisely identify user’sattitudes towards a certain aspect, rather than sim-ply assigning a sentiment polarity for a sentence..the key point in solving the absa task is tomodel the dependency relationship between an as-pect and its corresponding opinion expressions.
nevertheless, there probably exist multiple aspectsand different opinion expressions in a sentence.
tojudge the sentiment of a particular aspect, previousstudies (wang et al., 2016; tang et al., 2016a; maet al., 2017; chen et al., 2017; fan et al., 2018;huang et al., 2018; gu et al., 2018) have proposedvarious recurrent neural networks (rnns) with at-tention mechanisms to generate aspect-speciﬁc sen-tence representations and have achieved appealingresults.
however, an inherent defect makes theattention mechanism vulnerable to noise in the sen-tence.
take figure 1 as an example; for the aspect“service”, the opinion word “reasonable” may re-ceive more attention than the opinion word “poor”.
however, the “reasonable” refers to another as-pect, i.e., “price”..more recent efforts (zhang et al., 2019; sunet al., 2019b; huang and carley, 2019; zhang andqian, 2020; chen et al., 2020; liang et al., 2020;wang et al., 2020; tang et al., 2020) have been de-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6319–6329august1–6,2021.©2021associationforcomputationallinguistics6319voted to graph convolutional networks (gcns) andgraph attention networks (gats) over dependencytrees, which explicitly exploit the syntactic struc-ture of a sentence.
consider the dependency treein figure 1; the syntactic dependency can establishconnections between the words in a sentence.
forexample, a dependency relation exists between theaspect “price” and the opinion word “reasonable”.
however, two challenges arise when applying syn-tactic dependency knowledge to the absa task: 1)the inaccuracy of the dependency parsing resultsand 2) gcns over dependency trees do not workwell as expected on datasets that are not sensitiveto syntactic dependency due to the informal expres-sion and complexity of online reviews..in this paper, we propose a novel architecture,the dual graph convolution network (dualgcn), asshown in figure 2, to solve the aforementionedchallenges.
for the ﬁrst challenge, we use theprobability matrix of all dependency arcs from adependency parser to build a syntax-based graphconvolutional network (syngcn).
the idea behindthis approach is that the probability matrix rep-resenting dependencies between words containsrich syntactic information compared with the ﬁnaldiscrete output of a dependency parser.
for thesecond, we construct a semantic correlation-basedgraph convolutional network (semgcn) by utiliz-ing a self-attention mechanism.
the idea behindthis approach is that the attention matrix shapedby self-attending, also viewed as an edge-weighteddirected graph, can represent semantic correlationsbetween words.
moreover, motivated by the workof dgedt (tang et al., 2020), we utilize a biafﬁnemodule to bridge relevant information between thesyngcn and semgcn modules..furthermore, we design two regularizers to en-hance our dualgcn model.
we observe that thesemantically related terms of each word should notoverlap.
therefore, we encourage the attentionprobability distributions over words to be orthog-onal.
to this end, we incorporate an orthogonalregularizer on the attention probability matrix forthe semgcn module.
moreover, the two represen-tations learned from the syngcn and semgcnmodules should contain signiﬁcantly distinct infor-mation captured by the syntactic dependency andthe semantic correlation.
therefore, we expect thatthe semgcn module could learn semantic repre-sentations different from syntactic representations.
thus, we propose a differential regularizer between.
the syngcn and semgcn modules..our contributions are highlighted as follows:.
• we propose a dualgcn model for the absatask.
our dualgcn considers both the syntacticstructure and the semantic correlation within agiven sentence.
speciﬁcally, our dualgcn in-tegrates the syngcn and semgcn networksthrough a mutual biafﬁne module..• we propose orthogonal and differential regular-izers.
the orthogonal regularizer encouragesthe semgcn network to learn an orthogonal se-mantic attention matrix, whereas the differentialregularizer encourages the semgcn network tolearn semantic features distinct from the syntac-tic ones built from the syngcn network..• we conduct extensive experiments on the se-meval 2014 and twitter datasets.
the experi-mental results demonstrate the effectiveness ofour dualgcn model.
additionally, the sourcecode and preprocessed datasets used in our workare provided on github1..2 related work.
traditional sentiment analysis tasks are sentence-in contrast,level or document-level oriented.
absa is an entity-level oriented and a more ﬁne-grained task for sentiment analysis.
earlier meth-ods (titov and mcdonald, 2008; jiang et al., 2011;kiritchenko et al., 2014; vo and zhang, 2015) areusually based on handcrafted features and fail tomodel the dependency between the given aspectand its context..recently, various attention-based neural net-works have been proposed to implicitly model thesemantic relation of an aspect and its context to cap-ture the opinion expression component (wang et al.,2016; tang et al., 2016a,b; ma et al., 2017; chenet al., 2017; fan et al., 2018; huang et al., 2018; guet al., 2018; li et al., 2018a; tan et al., 2019).
forinstance, (wang et al., 2016) proposed attention-based lstms for aspect-level sentiment classiﬁca-tion.
(tang et al., 2016b) and (chen et al., 2017)both introduced a hierarchical attention network toidentify important sentiment information relatedto the given aspect.
(fan et al., 2018) exploited amulti-grained attention mechanism to capture theword-level interaction between aspects and theircontext.
(tan et al., 2019) designed a dual attention.
1https://github.com/ccchenhao997/dualgcn-absa.
6320network to recognize conﬂicting opinions.
in addi-tion, the pre-trained language model bert (devlinet al., 2019) has achieved remarkable performancein many nlp tasks, including absa.
(sun et al.,2019a) transformed absa task into a sentence pairclassiﬁcation task by constructing an auxiliary sen-tence.
(xu et al., 2019) proposed a post-trainingapproach on the bert to enhance the performanceof ﬁne-tuning stage for the absa task..another trend explicitly leverages syntacticknowledge.
this type of knowledge helps to es-tablish connections between the aspects and theother words in a sentence to learn syntax-awarefeature representations of aspects.
(dong et al.,2014) proposed a recursive neural network to adap-tively propagate the sentiment of words to the as-pect along the dependency tree.
(he et al., 2018)introduced an attention model that incorporatedsyntactic information to compute attention weights.
(phan and ogunbona, 2020) utilized the syntacticrelative distance to reduce the impact of irrelevantwords..following this line, a few works extend the gcnand gat models by means of a syntactical depen-dency tree and develop several outstanding mod-els (zhang et al., 2019; sun et al., 2019b; huangand carley, 2019; wang et al., 2020; tang et al.,2020).
these works explicitly exploit the syntacticstructure information to learn node representationsfrom adjacent nodes.
thus, the dependency treeshortens the distance between the aspects and opin-ion words of a sentence and alleviates the problemof long-range dependency..most recently, several works explore the ideaof combining different types of graph for absatask.
for instance, (chen et al., 2020) combined adependency graph and a latent graph to generatethe aspect representation.
(zhang and qian, 2020)observed the characteristics of word co-occurrencein linguistics and designed hierarchical syntacticand lexical graphs.
(liang et al., 2020) constructedaspect-focused and inter-aspect graphs to learn de-pendency feature of the key aspect words and sen-timent relations between different aspects..in this paper, we propose a gcn based methodcombining syntactic and semantic features.
we usea dependency probability matrix with richer syntac-tic information and elaborately design orthogonaland differential regularizers to enhance the abilityto precisely capture the semantic associations..3 graph convolutional network (gcn).
motivated by conventional convolutional neuralnetworks (cnns) and graph embedding, a gcn isan efﬁcient cnn variant that operates directly ongraphs (kipf and welling, 2017).
for graph struc-tured data, a gcn can apply the convolution oper-ation on directly connected nodes to encode localinformation.
through the message passing of mul-tilayer gcns, each node in a graph can learn moreglobal information.
given a graph with n nodes,the graph can be represented as an adjacency ma-trix a ∈ rn×n.
most previous work (zhang et al.,2019; sun et al., 2019b) extend gcn models byencoding dependency trees and incorporating de-pendency paths between words.
they build the ad-jacency matrix a over the syntactical dependencytree of a sentence.
thus, an element aij in a in-dicates whether the i-th node is connected to thej-th node.
speciﬁcally, aij = 1 if the i-th node isconnected to the j-th node, and aij = 0 otherwise.
in addition, the adjacency matrix a, composed of0 and 1, can be deemed as the ﬁnal discrete outputof a dependency parser.
for the i-th node at thel-th layer, formally, its hidden state representation,denoted as hli, is updated by the following equation:.
hli = σ.
.
aijw lhl−1.
j + bl.
.
(1).
.
.
n(cid:88).
j=1.
where w l is a weight matrix, bl is a bias term, andσ is an activation function (e.g., relu)..4 proposed dualgcn.
figure 2 provides an overview of dualgcn.
inthe absa task, a sentence-aspect pair (s, a) isgiven, where a = {a1, a2, ..., am} is an aspect.
it is also a sub-sequence of the entire sentences = {w1, w2, ..., wn}.
then, we utilize bilstmor bert as sentence encoder to extract hidden con-textual representations, respectively.
for the bil-stm encoder, we ﬁrst obtain the word embeddingsx = {x1, x2, ..., xn} of the sentence s from an em-bedding lookup table e ∈ r|v |×de, where |v | isthe size of vocabulary and de denotes the dimen-sionality of word embeddings.
next, the word em-beddings of the sentence are fed into a bilstm toproduce hidden state vectors h = {h1, h2, ..., hn},where hi ∈ r2d is the hidden state vector at time tfrom the bilstm.
the dimensionality of a hiddenstate vector d is output by a unidirectional lstm..6321figure 2: the overall architecture of dualgcn, which is composed primarily of syngcn and semgcn.
syngcnuses the probability matrix generated by the dependency parser, while semgcn leverages the attention scorematrix generated by the self-attention layer.
the orthogonal and differential regularizers are designed to furtherimprove the ability of capturing semantic correlations.
details of these components are described in the main text..for the bert encoder, we construct a sentence-aspect pair “[cls] sentence [sep] aspect [sep]”as input to obtain aspect-aware hidden representa-tions of the sentence.
moreover, in order to matchthe wordpiece-based representations of bert withthe result of syntactic dependency based on word,we expand dependencies of a word into its all ofsubwords.
then, the hidden representations of sen-tence are input into the syngcn and semgcnmodules, respectively.
a biafﬁne module is thenadopted for effective information ﬂow.
finally, weaggregate all the aspect nodes’ representations fromthe syngcn and semgcn modules via poolingand concatenation to form the ﬁnal aspect repre-sentation.
next, we elaborate on the details of ourproposed dualgcn model..4.1 syntax-based gcn (syngcn).
the syngcn module takes the syntactic encodingas input.
to encode syntactic information, we uti-lize the probability matrix of all dependency arcsfrom a dependency parser.
compared to the ﬁnaldiscrete output of a dependency parser, the depen-dency probability matrix could capture rich struc-tural information by providing all latent syntactic.
structures.
therefore, the dependency probabilitymatrix is used to alleviate dependency parsing er-rors.
here, we use the state-of-the-art dependencyparsing model lal-parser (mrini et al., 2019)..with the syntactic encoding of an adjacencymatrix asyn ∈ rn×n,the syngcn moduletakes the hidden state vectors h from bilstmas initial node representations in the syntacticgraph.
the syntactic graph representation h syn ={hsynn } is then obtained from the syn-gcn module using eq.
(1).
here, hsyni ∈ rd is ahidden representation of the ith node.
note that fora2 , ..., hsynaspect nodes, we use symbols {hsynam}to denote their hidden representations..2 , ..., hsyn.
1 , hsyn.
a1 , hsyn.
4.2 semantic-based gcn (semgcn).
instead of utilizing additional syntactic knowledge,as in syngcn, semgcn obtains an attention ma-trix as an adjacency matrix via a self-attentionmechanism.
on the one hand, self-attention cancapture the semantically related terms of each wordin a sentence, which is more ﬂexible than the syn-tactic structure.
one the other hand, semgcn canadapt to online reviews that are not sensitive tosyntactic information..6322self-attention self-attention (vaswani et al.,2017) computes the attention score of each pairof elements in parallel.
in our dualgcn, we com-pute the attention score matrix asem ∈ rn×n usinga self-attention layer.
we then take the attentionscore matrix asem as the adjacency matrix of oursemgcn module, which can be formulated as:.
asem = softmax.
(cid:32).
qw q × (cid:0)kw k(cid:1)t√d.(cid:33).
(2).
where matrices q and k are both equal to the graphrepresentations of previous layer of our semgcnmodule, while w q and w k are learnable weightmatrices.
in addition, d is the dimensionality ofthe input node feature.
note that we use only oneself-attention head to obtain an attention score ma-trix for a sentence.
similar to the syngcn module,the semgcn module obtains the graph represen-tation h sem.
additionally, we use the symbols{hsema2 , ..., hsemam } to denote the hidden repre-sentations of all aspect nodes.
biafﬁne module to effectively exchange relevantfeatures between the syngcn and semgcn mod-ules, we adopt a mutual biafﬁne transformation asa bridge.
we formulate the process as follows:.
a1 , hsem.
h syn(cid:48) = softmax.
h sem(cid:48) = softmax.
(cid:16).
h synw1(h sem)t(cid:17)h semw2(h syn)t(cid:17)(cid:16).
h sem (3).
h syn (4).
where w1 and w2 are trainable parameters..finally, we apply average pooling and concatena-tion operations on the aspect nodes of the syngcnand semgcn modules.
thus, we obtain the ﬁnalfeature representation for the absa task, i.e.,.
a = f (cid:0)hsyna1 , hsynhsyna = f (cid:0)hsemhsema1 , hsema , hsemr = [hsyna ].
(cid:1).
a2 , ..., hsynama2 , ..., hsemam.
(cid:1).
(5).
(6).
(7).
where f (·) is an average pooling function appliedover the aspect node representations.
then, theobtained representation r is fed into a linear layer,followed by a softmax function to produce a senti-ment probability distribution p, i.e.,.
4.3 regularizer.
to improve the semantic representation, we pro-pose two regularizers for the semgcn module,i.e., orthogonal and differential regularizers.
orthogonal regularizer intuitively, the relateditems of each word should be in different regionsin a sentence, so the attention score distributionsrarely overlap.
therefore, we expect a regularizerto encourage orthogonality among the attentionscore vectors of all words.
given an attention scorematrix asem ∈ rn×n, the orthogonal regularizer isformulated as follows:.
ro = (cid:107)asemasemt − i(cid:107)f.(9).
where i is an identity matrix.
the subscript fdenotes the frobenius norm.
as a result, eachnondiagonal element of asemasemt is minimizedto maintain the matrix asem orthogonal.
differential regularizer we expect that two typesof feature representations learned from the syn-gcn and semgcn modules represent distinct in-formation contained within the syntactic depen-dency trees and semantic correlations.
therefore,we adopt a differential regularizer between the twoadjacency matrices of the syngcn and semgcnmodules.
note that the regularizer is only restric-tive to asem and is given as.
rd =.
1(cid:107)asem − asyn(cid:107)f...(10).
4.4 loss function.
our training goal is to minimize the following totalobjective function:.
(cid:96)t = (cid:96)c + λ1ro + λ2rd + λ3(cid:107)θ(cid:107)2.
(11).
where λ1, λ2 and λ3 are regularization coefﬁcientsand θ represents all trainable model parameters.
(cid:96)c is a standard cross-entropy loss and is deﬁnedfor the absa task as follows:.
(cid:96)c = −.
(cid:88).
(cid:88).
(s,a)∈d.
c∈c.
log p(a).
(12).
where d contains all sentence-aspect pairs and cis the collection of distinct sentiment polarities..5 experiments.
p(a) = softmax (wpr + bp).
(8).
5.1 datasets.
where wp and bp are the learnable weight and bias..we conduct experiments on three public standarddatasets.
the restaurant and laptop datasets.
6323dataset.
division # positive.
# negative.
# neutral.
the syngcn and semgcn modules..restaurant.
laptop.
twitter.
trainingtestingtrainingtestingtrainingtesting.
21647279763371507172.
8071968511281528169.
6371964551673016336.table 1: statistics for the three experimental datasets..are made public from the semeval absa chal-lenge (pontiki et al., 2014).
following (chen et al.,2017), we remove the instances using the “conﬂict”label.
in addition, the twitter dataset is a collectionof tweets (dong et al., 2014).
all three datasetshave three sentiment polarities: positive, negativeand neutral.
each sentence in these datasets is an-notated with marked aspects and their correspond-ing polarities.
statistics for the three datasets areshown in table 1..5.2.implementation details.
the lal-parser (mrini et al., 2019), which is usedfor dependency parsing, provides an off-the-shelfparser2.
for all the experiments, we use pretrained300-dimensional glove3 vectors (pennington et al.,2014) to initialize the word embeddings.
the di-mensionality of the position (i.e., the relative po-sition of each word in a sentence with respect tothe aspect) embeddings and part-of-speech (pos)embeddings is set to 30. thus, we concatenate theword, pos and position embeddings and then inputthem into a bilstm model, whose hidden size isset to 50. to alleviate overﬁtting, we apply dropoutat a rate of 0.7 to the input word embeddings ofthe bilstm.
the dropout rate of the syngcn andsemgcn modules is set to 0.1, and the number ofsyngcn and semgcn layers is set to 2. all themodel weights are initialized from a uniform distri-bution.
we use the adam optimizer with a learningrate of 0.002. the dualgcn model is trained in 50epochs with a batch size of 16. the regularizationcoefﬁcients, λ1 and λ2 are set to (0.2, 0.3), (0.2,0.2) and (0.3, 0.2) for the three datasets, respec-tively, and λ3 is set to 10−4.
for dualgcn+bert,we use the bert-base-uncased4 english version.
seeour code for more details about bert’s experi-ments.
additionally, following (marcheggiani andtitov, 2017), we add a self-loop for each node in.
2https://github.com/khalilmrini/lal-parser3https://nlp.stanford.edu/projects/glove/4https://github.com/huggingface/transformers.
5.3 baseline methods.
we compare dualgcn with state-of-the-art base-lines.
the models are brieﬂy described as follows.
1) atae-lstm (wang et al., 2016) utilizes aspectembedding and the attention mechanism in aspect-level sentiment classiﬁcation.
2) ian (ma et al., 2017) employs two lstmsand an interactive attention mechanism to generaterepresentations for the aspect and sentence.
3) ram (chen et al., 2017) uses multiple atten-tion and memory networks to learn the sentencerepresentation.
4) mgan (fan et al., 2018) designs a multigrainedattention mechanism to capture word-level interac-tions between the aspect and context.
5) tnet (li et al., 2018b) transforms bilstm em-beddings into target-speciﬁc embeddings and usescnn to extract ﬁnal embeddings for classiﬁcation.
6) asgcn (zhang et al., 2019) ﬁrst proposed us-ing gcn to learn the aspect-speciﬁc representa-tions for aspect-based sentiment classiﬁcation.
7) cdt (sun et al., 2019b) utilizes a gcn overa dependency tree to learn aspect representationswith syntactic information.
8) bigcn (zhang and qian, 2020) uses hierarchi-cal graph structure to integrate word co-occurrenceinformation and dependency type information.
9) kumagcn (chen et al., 2020) employs a latentgraph structure to complement syntactic features.
10) intergcn (liang et al., 2020) utilizes a gcnover a dependency tree to learn aspect representa-tions with syntactic information.
11) r-gat (wang et al., 2020) proposes a aspect-oriented dependency tree structure and then en-codes new dependency trees with a relational gat.
12) dgedt (tang et al., 2020) proposes a depen-dency graph enhanced dual-transformer network byjointly considering ﬂat representations and graph-based representations.
13) bert (devlin et al., 2019) is the vanilla bertmodel by feeding the sentence-aspect pair and us-ing the representation of [cls] for predictions.
14) r-gat+bert (wang et al., 2020) is the r-gat model that uses a pre-trained bert to replacebilstm as an encoder.
15) dgedt+bert (tang et al., 2020) is thedgedt model that uses a pre-trained bert toreplace bilstm as an encoder..63245.4 comparison results.
to evaluate the absa models, we use the accu-racy and macro-averaged f1-score as the mainevaluation metrics.
the main experimental resultsare reported in table 2. our dualgcn modelconsistently outperforms all attention-based andsyntax-based methods on the restaurant, laptopand twitter datasets.
these results demonstratesthat our dualgcn effectively integrates syntacticknowledge and semantic information.
in addition,the dualgcn accurately ﬁts datasets that containformal, informal or complicated reviews.
com-pared to attention-based methods such as atae-lstm, ian and ram, our dualgcn model uti-lizes syntactic knowledge to establish dependenciesbetween words, so it can avoid noises introducedby the attention mechanism.
moreover, the syntax-based methods, such as asgcn, cdt, r-gat andso on, achieve better performance than attention-based methods, but they ignore the semantic cor-relation between words.
however, when consider-ing informal or complicated sentences, using onlysyntactic knowledge results in poor performance.
in table 2, on the other side, the results from thelast group shows that the basic bert outperformsmost of the models based on static word embedding.
moreover, based on bert, our dualgcn+bertachieves better performance..5.5 ablation study.
to further investigate the role of modules in thedualgcn model, we conduct extensive ablationstudies.
the results are reported in table 2. thesyngcn-head model uses the discrete outputs ofa dependency parser to construct the adjacencymatrix of the gcns.
in contrast, syngcn lever-ages the probability matrix generated in a depen-dency parser as the adjacency matrix.
the syn-gcn model outperforms the syngcn-head onthe restaurant and laptop datasets, which demon-strates that rich syntactic knowledge can alleviatedependency parsing errors.
the semgcn modelutilizes a self-attention layer to construct the adja-cency matrix of the semantic graph.
this semgcnmodel outperforms the syngcn on the twitterdataset because the reviews from twitter, comparedto those from restaurant and laptop datasets, arelargely informal and insensitive to syntactic infor-mation.
dualgcn w/o biafﬁne means that weremove the biafﬁne module so that the syngcnand semgcn modules cannot interact with each.
other.
therefore, the performance degrades sub-stantially on the restaurant and laptop datasets.
dualgcn w/o ro&rd indicates that we removeboth the orthogonal and differential regularizers.
similarly, dualgcn w/o ro or rd denotes thatwe remove only one of the regularizers.
the ex-perimental results show that our two regularizersencourage the dualgcn to capture semantic cor-relations precisely.
overall, our dualgcn with allmodules achieves the best performance..5.6 case study.
table 4 shows a few sample cases analyzed us-ing different models.
the notations p, n and orepresent positive, negative and neutral sentiment,respectively.
we highlight the aspect words in redand in blue.
for the aspect “food” in the ﬁrstsample, the attention-based methods, i.e., atae-lstm and ian, are prone to attend to the noisyword “dreadful”.
although the syntactic depen-dency can establish direct connections between anaspect and some words, no association exists be-tween the aspect and the opinion words for com-plicated sentences.
take the second sample as anexample; the aspect “apple os” is far from the opin-ion word “happy” in terms of syntactic distance.
thus, the syngcn model fails.
additionally, inthe third sample, feature representations of the keywords “did not” are not captured by the syngcnmodel.
in contrast, the semgcn model can attendto the semantic correlation between words.
the lasttwo samples demonstrate that our dualgcn, whichfully considers the complementarity of syntacticknowledge and semantic information, can addresscomplicated and informal sentences with the helpof the orthogonal and differential regularizers..5.7 attention visualization.
to investigate the effectiveness of the two regulariz-ers in capturing the semantic correlations betweenwords, we visualized the attention score matrix ofthe dualgcn w/o ro&rd and the intact dual-gcn.
consider the sample sentence, i.e., “webbrowsing is very quick with safari browser.” with“safari browser” as an aspect.
as shown in figure 3(a), the attention score matrix is dense, and the re-lated terms of each word overlap in the dualgcnw/o ro&rd model.
this result is attributed to thelack of semantic constraints in the self-attentionlayers.
the overlap of semantic correlations willlead to redundancy and noise during informationpropagation.
the seventh and eighth rows of the.
6325--67.3070.8173.6070.4073.6673.3570.77-73.8273.40.
74.29.
74.0174.8875.4076.02.
73.8572.8673.8673.9273.2073.5572.8274.29.models.
restaurant.
laptopaccuracy macro-f1 accuracy macro-f1 accuracy macro-f1.
twitter.
atae-lstm (wang et al., 2016)ian (ma et al., 2017)ram (chen et al., 2017)mgan (fan et al., 2018)tnet (li et al., 2018b)asgcn (zhang et al., 2019)cdt (sun et al., 2019b)bigcn (zhang and qian, 2020)kumagcn (chen et al., 2020)intergcn (liang et al., 2020)r-gat (wang et al., 2020)dgedt (tang et al., 2020).
our dualgcn.
bert-spc (devlin et al., 2019)r-gat+bert (wang et al., 2020)dgedt+bert (tang et al., 2020)our dualgcn+bert.
77.2078.6080.2381.2580.6980.7782.3081.9781.4382.2383.3083.90.
84.27.
86.1586.6086.3087.13.
--70.8071.9471.2772.0274.0273.4873.6474.0176.0875.10.
78.08.
80.2981.3580.0081.16.
68.7072.1074.4975.3976.5475.5577.1974.5976.1277.8677.4276.80.
78.48.
81.0178.2179.8081.80.
--71.3572.4771.7571.0572.9971.8472.4274.3273.7672.30.
74.74.
76.6974.0775.6078.10.
--69.3672.5474.9072.1574.6674.1672.45-75.5774.80.
75.92.
75.1876.1577.9077.40.table 2: experimental results comparison on three publicly available datasets..models.
restaurant.
laptopaccuracy macro-f1 accuracy macro-f1 accuracy macro-f1.
twitter.
syngcn-headsyngcnsemgcndualgcn w/o biafﬁnedualgcn w/o ro&rddualgcn w/o rodualgcn w/o rddualgcn.
82.9383.7483.2982.8482.9383.5683.6584.27.
75.2976.9776.3075.3175.7977.4376.3478.08.
76.2776.5876.9076.9076.5876.5877.5378.48.
72.3973.1773.7273.2372.0372.7873.7274.74.
75.0474.5975.1875.3374.5975.1874.4575.92.table 3: experimental results of ablation study..attention score matrix are the attention probabil-ity distributions of “safari” and “browser”, respec-tively.
the information to which “safari browser”pays attention is redundant and it does not pay moreattention to the key opinion word “quick”.
thus,the dualgcn w/o ro&rd failed.
in comparison,in figure 3 (b), the attention score matrix producedby our dualgcn is relatively sparse.
both “safari”and “browser” are semantically related to “quick”,and their other attended items are also semanticallyreasonable.
in addition, the attention scores of therelated terms of each words tend to be distinct andprecise due to the semantic constraints of these tworegularizers.
therefore, our dualgcn model canreadily predict the correct sentiment polarity of theaspect “safari browser”..5.8.impact of the dualgcn layer number.
to investigate the impact of the dualgcn layernumber, we evaluate our dualgcn model withone to eight layers on the restaurant and laptopdatasets.
as shown in figure 4, our model withtwo dualgcn layers performs the best.
on onethe hand, node representations cannot propagatefar when the number of layers is small.
on theother hand, if the number of layers is excessive, themodel will become unstable due to the vanishinggradient and information redundancy..6 conclusion.
in this paper, we propose a dualgcn architectureto address the disadvantages of attention-based anddependency-based methods for absa tasks.
our.
6326# review.
atae-lstm.
ian.
syngcn semgcn dualgcn.
1 great food but the service was dreadful!
2 works well, and i am extremely happy to be back to an apple os.
3 did not enjoy the new windows 8 and touchscreen functions.
4.i never tried any external mics with that imac.
in mi burrito, here was nothing but dark chicken that had thatcooked last week and just warmed up in a microwave taste..5.
(n(cid:55), n(cid:51))(p(cid:51), p(cid:51))(o(cid:55), p(cid:55))o(cid:51).
(n(cid:55), n(cid:51))(p(cid:51), p(cid:51))(o(cid:55), n(cid:51))n(cid:55).
(p(cid:51), n(cid:51))(p(cid:51), o(cid:55))(p(cid:55), o(cid:55))n(cid:55).
(p(cid:51), n(cid:51))(p(cid:51), p(cid:51))(n(cid:51), n(cid:51))n(cid:55).
(p(cid:51), n(cid:51))(p(cid:51), p(cid:51))(n(cid:51), n(cid:51))o(cid:51).
(n(cid:51), p(cid:55)).
(n(cid:51), n(cid:51)).
(n(cid:51), o(cid:55)).
(n(cid:51), o(cid:55)).
(n(cid:51), n(cid:51)).
table 4: case studies of our dualgcn model compared with state-of-the-art baselines..(a) the attention score matrix of dualgcnw/o ro&rd.
figure 4: effect of the number of dualgcn layers..acknowledgments.
this work was supported in part by the na-tional key r&d program of china under grant2019yff0303300 and subject ii under grant2019yff0303302, in part by the national nat-ural science foundation of china under grants61906018 and 62076032, in part by the 111 projectunder grant b08004, and in part by the funda-mental research funds for the central universitiesunder grant 2021rc36..(b) the attention score matrix of dualgcn.
figure 3: an illustration on how orthogonal and differ-ential regularizers contribute to the self-attention layer..dualgcn model integrates syntactic knowledgeand semantic information by means of the syngcnand semgcn modules.
moreover, to effectivelycapture the semantic correlation between words,we propose orthogonal and differential regularizersin the semgcn module.
these regularizers canattend to the semantically related items with lessoverlap of each word and capture feature represen-tations that differ from the syntactic structure.
ex-tensive experiments on benchmark datasets showthat our dualgcn model outperforms baselines..references.
chenhua chen, zhiyang teng, and yue zhang.
2020.inducing target-speciﬁc latent structures for aspectsentiment classiﬁcation.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 5596–5607, on-line.
association for computational linguistics..peng chen, zhongqian sun, lidong bing, and weiyang.
2017. recurrent attention network on mem-ory for aspect sentiment analysis.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing, pages 452–461, copen-.
6327hagen, denmark.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong, furu wei, chuanqi tan, duyu tang, mingzhou, and ke xu.
2014. adaptive recursive neuralnetwork for target-dependent twitter sentiment clas-siﬁcation.
in proceedings of the 52nd annual meet-ing of the association for computational linguistics(volume 2), pages 49–54, baltimore, maryland.
as-sociation for computational linguistics..feifan fan, yansong feng, and dongyan zhao.
2018.multi-grained attention network for aspect-level sen-in proceedings of the 2018timent classiﬁcation.
conference on empirical methods in natural lan-guage processing, pages 3433–3442, brussels, bel-gium.
association for computational linguistics..shuqin gu, lipeng zhang, yuexian hou, and yin song.
2018. a position-aware bidirectional attention net-in pro-work for aspect-level sentiment analysis.
ceedings of the 27th international conference oncomputational linguistics, pages 774–784, santafe, new mexico, usa.
association for computa-tional linguistics..ruidan he, wee sun lee, hwee tou ng, and danieldahlmeier.
2018. effective attention modeling forin proceed-aspect-level sentiment classiﬁcation.
ings of the 27th international conference on com-putational linguistics, pages 1121–1131, santa fe,new mexico, usa.
association for computationallinguistics..binxuan huang and kathleen carley.
2019. syntax-level sentiment classiﬁcation withaware aspectin proceedings of thegraph attention networks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5469–5477, hong kong,china.
association for computational linguistics..binxuan huang, yanglan ou, and kathleen m. car-ley.
2018. aspect level sentiment classiﬁcation withcorr,attention-over-attention neural networks.
abs/1804.06536..long jiang, mo yu, ming zhou, xiaohua liu, andtiejun zhao.
2011. target-dependent twitter sen-timent classiﬁcation.
in proceedings of the 49th an-nual meeting of the association for computationallinguistics: human language technologies, pages151–160, portland, oregon, usa.
association forcomputational linguistics..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations, iclr 2017, toulon, france,april 24-26, 2017..svetlana kiritchenko, xiaodan zhu, colin cherry, andsaif mohammad.
2014. nrc-canada-2014: detect-ing aspects and sentiment in customer reviews.
inproceedings of the 8th international workshop onsemantic evaluation (semeval 2014), pages 437–442, dublin, ireland.
association for computationallinguistics..jiwei li and eduard hovy.
2017. reﬂections on senti-ment/opinion analysis.
in a practical guide to sen-timent analysis, pages 41–59, cham.
springer inter-national publishing..lishuang li, yang liu, and anqiao zhou.
2018a.
hier-archical attention based position-aware network forin proceedings ofaspect-level sentiment analysis.
the 22nd conference on computational natural lan-guage learning, pages 181–189, brussels, belgium.
association for computational linguistics..xin li, lidong bing, wai lam, and bei shi.
2018b.
transformation networks for target-oriented senti-ment classiﬁcation.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 946–956, melbourne, australia.
association for compu-tational linguistics..bin liang, rongdi yin, lin gui, jiachen du, andruifeng xu.
2020. jointly learning aspect-focusedand inter-aspect relations with graph convolutionalnetworks for aspect sentiment analysis.
in proceed-ings of the 28th international conference on com-putational linguistics, pages 150–161, barcelona,spain (online).
international committee on compu-tational linguistics..bing liu.
2012. sentiment analysis and opinion min-ing.
synthesis lectures on human language technolo-gies, 5(1):1–167..dehong ma, sujian li, xiaodong zhang, and houfenginteractive attention networks forwang.
2017.in proceed-aspect-level sentiment classiﬁcation.
ings of the 26th international joint conference onartiﬁcial intelligence, ijcai’17, page 4068–4074.
aaai press..diego marcheggiani and ivan titov.
2017. encodingsentences with graph convolutional networks for se-in proceedings of the 2017mantic role labeling.
conference on empirical methods in natural lan-guage processing, pages 1506–1515, copenhagen,denmark.
association for computational linguis-tics..khalil mrini, franck dernoncourt, trung bui, wal-re-an interpretable self-arxiv preprint.
ter chang, and ndapa nakashole.
2019.thinking self-attention:attentive encoder-decoder parser.
arxiv:1911.03875..6328jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..minh hieu phan and philip o. ogunbona.
2020. mod-elling context and syntactical features for aspect-in proceedings of thebased sentiment analysis.
58th annual meeting of the association for compu-tational linguistics, pages 3211–3220, online.
as-sociation for computational linguistics..maria pontiki, dimitris galanis, john pavlopoulos,ion androutsopoulos, andharris papageorgiou,suresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in proceedings of the8th international workshop on semantic evaluation(semeval 2014), pages 27–35, dublin, ireland.
as-sociation for computational linguistics..chi sun, luyao huang, and xipeng qiu.
2019a.
uti-lizing bert for aspect-based sentiment analysis viain proceedings ofconstructing auxiliary sentence.
the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1, pages380–385, minneapolis, minnesota.
association forcomputational linguistics..kai sun, richong zhang, samuel mensah, yongyimao, and xudong liu.
2019b.
aspect-level senti-ment analysis via convolution over dependency tree.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5679–5688, hong kong, china.
association for computa-tional linguistics..xingwei tan, yi cai, and changxi zhu.
2019. rec-ognizing conﬂict opinions in aspect-level sentimentclassiﬁcation with dual attention networks.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 3426–3431,hong kong, china.
association for computationallinguistics..duyu tang, bing qin, and ting liu.
2016a.
aspectlevel sentiment classiﬁcation with deep memory net-in proceedings of the 2016 conference onwork.
empirical methods in natural language processing,pages 214–224, austin, texas.
association for com-putational linguistics..duyu tang, bing qin, and ting liu.
2016b.
aspectlevel sentiment classiﬁcation with deep memory net-in proceedings of the 2016 conference onwork.
empirical methods in natural language processing,pages 214–224, austin, texas.
association for com-putational linguistics..hao tang, donghong ji, chenliang li, and qijizhou.
2020. dependency graph enhanced dual-transformer structure for aspect-based sentimentin proceedings of the 58th annualclassiﬁcation.
meeting of the association for computational lin-guistics, pages 6578–6588, online.
association forcomputational linguistics..ivan titov and ryan mcdonald.
2008. modeling on-line reviews with multi-grain topic models.
in pro-ceedings of the 17th international conference onworld wide web, page 111–120, new york, ny,usa.
association for computing machinery..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in i. guyon, u. v. luxburg, s. bengio,h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..duy-tin vo and yue zhang.
2015. deep learning forevent-driven stock prediction.
in proceedings of ij-cai, buenos aires, argentina..kai wang, weizhou shen, yunyi yang, xiaojun quan,and rui wang.
2020. relational graph attention net-in pro-work for aspect-based sentiment analysis.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3229–3238, online.
association for computational lin-guistics..yequan wang, minlie huang, xiaoyan zhu, andli zhao.
2016. attention-based lstm for aspect-in proceedings oflevel sentiment classiﬁcation.
the 2016 conference on empirical methods in nat-ural language processing, pages 606–615, austin,texas.
association for computational linguistics..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1, pages2324–2335, minneapolis, minnesota.
associationfor computational linguistics..chen zhang, qiuchi li, and dawei song.
2019.aspect-based sentiment classiﬁcation with aspect-speciﬁc graph convolutional networks.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th interna-tional joint conference on natural language pro-cessing, pages 4568–4578, hong kong, china.
as-sociation for computational linguistics..mi zhang and tieyun qian.
2020. convolution overhierarchical syntactic and lexical graphs for aspectlevel sentiment analysis.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 3540–3549, on-line.
association for computational linguistics..6329