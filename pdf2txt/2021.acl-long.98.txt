what ingredients make for an effective crowdsourcing protocol fordifﬁcult nlu data collection tasks?.
nikita nangia1∗.
saku sugawara2∗.
harsh trivedi3.
alex warstadt1.
clara vania4†.
samuel r. bowman1.
1new york university, 2national institute of informatics, 3stony brook university, 4amazon.
correspondence: {nikitanangia, bowman}@nyu.edu, saku@nii.ac.jp.
abstract.
crowdsourcing is widely used to create datafor common natural language understandingtasks.
despite the importance of these datasetsfor measuring and reﬁning model understand-ing of language, there has been little focus onthe crowdsourcing methods used for collectingthe datasets.
in this paper, we compare the efﬁ-cacy of interventions that have been proposedin prior work as ways of improving data qual-ity.
we use multiple-choice question answer-ing as a testbed and run a randomized trial byassigning crowdworkers to write questions un-der one of four different data collection proto-cols.
we ﬁnd that asking workers to write ex-planations for their examples is an ineffectivestand-alone strategy for boosting nlu exam-ple difﬁculty.
however, we ﬁnd that trainingcrowdworkers, and then using an iterative pro-cess of collecting data, sending feedback, andqualifying workers based on expert judgmentsis an effective means of collecting challengingdata.
but using crowdsourced, instead of ex-pert judgments, to qualify workers and sendfeedback does not prove to be effective.
weobserve that the data from the iterative proto-col with expert assessments is more challeng-ing by several measures.
notably, the human–model gap on the unanimous agreement por-tion of this data is, on average, twice as largeas the gap for the baseline protocol data..1.introduction.
crowdsourcing is a scalable method for construct-ing examples for many natural language processingtasks.
platforms like amazon’s mechanical turkgive researchers access to a large, diverse pool ofpeople to employ (howe, 2006; snow et al., 2008;callison-burch, 2009).
given the ease of data col-lection with crowdsourcing, it has been frequently.
∗equal contribution.
† work done while at new york university..used for collecting datasets for natural languageunderstanding (nlu) tasks like question answer-ing (mihaylov et al., 2018), reading comprehension(rajpurkar et al., 2016; huang et al., 2019), naturallanguage inference (dagan et al., 2005; bowmanet al., 2015; williams et al., 2018; nie et al., 2020a),and commonsense reasoning (talmor et al., 2019).
there has been substantial research devoted tostudying crowdsourcing methods, especially in thehuman-computer interaction literature (kittur et al.,2008, 2011; bernstein et al., 2012).
however, mostprior research investigates methods for collectingaccurate annotations for existing data, for examplelabeling objects in images or labeling the sentimentof sentences (hsueh et al., 2009; liu et al., 2019a;sun et al., 2020).
there are some small-scale stud-ies that use writing tasks, like writing product re-views, to compare crowdsourcing methodologies(dow et al., 2012).
however, we are unaware ofany prior work that directly evaluates the effectsof crowdsourcing protocol design choices on thequality of the resulting data for nlu tasks..decisions around methodology and task designused to collect datasets dictate the quality of thedata collected.
as models become stronger and areable to solve existing nlu datasets, we have anincreasing need for difﬁcult, high-quality datasetsthat are still reliably solvable by humans.
as aresult, our thresholds for what makes a dataset ac-ceptable become stricter: the data needs to be chal-lenging, have high human-agreement, and avoid se-rious annotation artifacts (gururangan et al., 2018).
to make collecting such large-scale datasets feasi-ble, making well-informed crowdsourcing designdecisions becomes crucial..existing nlp datasets have been crowdsourcedwith varying methods.
the prevailing standard isto experiment with task design during pilots thatare run before the main data collection (vaughan,2018).
this piloting process is essential to design-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1221–1235august1–6,2021.©2021associationforcomputationallinguistics1221figure 1: the initial pool of crowdworkers are randomly assigned to one of four protocols and the datasets arecollected in parallel..ing good crowdsourcing tasks with clear instruc-tions, but the ﬁndings from these pilots are rarelydiscussed in published corpus papers, and the pilotsare usually not large enough or systematic enoughto yield deﬁnitive conclusions.
in this paper, weuse a randomized trial to directly compare crowd-sourcing methodologies to establish general bestpractices for nlu data collection..we compare the efﬁcacy of three types of crowd-sourcing interventions that have been used in previ-ous work.
we use multiple-choice question answer-ing in english as a testbed for our study and collectfour small datasets in parallel including a base-line dataset with no interventions.
we choose qaas our test-bed over the similarly popular testbedtask of natural language inference (nli) becauseof our focus on very high human-agreement exam-ples which calls for minimizing label ambiguity.
in multiple-choice qa, the correct label is the an-swer choice that is most likely to be correct, even ifthere is some ambiguity in whether that choice isgenuinely true .
in nli however, if more than onelabel is plausible, then resolving the disagreementby ranking labels may not be possible (pavlick andkwiatkowski, 2019).
in the trial, crowdworkersare randomly assigned to one of four protocols:baseline, justification, crowd, or expert.1in baseline, crowdworkers are simply asked towrite question-answering examples.
in justifica-tionthey are tasked with also writing explanationsfor their examples, prompting self-assessment.
forthe expert and crowd protocols, we train work-.
1all.
the data is available at https://github.com/nyu-.
mll/crowdsourcing-protocol-comparison..ers using an iterative process of collecting data,sending feedback, and qualifying high perform-ing workers to subsequent rounds.
we use expert-curated evaluations in expert, and crowdsourcedevaluations in crowd for generating feedback andassigning qualiﬁcations.
we use a a standard ofhigh pay and strict qualiﬁcations for all protocols.
we also validate the data to discard ambiguous andunanswerable examples.
the experimental pipelineis sketched in figure 1..to quantify the dataset difﬁculty, we collect addi-tional label annotations to establish human perfor-mance on each dataset and compare these to modelperformance.
we also evaluate the difﬁculty of thedatasets for typical machine learning models usingirt (baker and kim, 1993; lalor et al., 2016)..we ﬁnd that the expert protocol dataset is themost challenging.
the human–model gap withrobertalarge (liu et al., 2019b) on the unani-mous agreement portion of expert is 13.9 per-centage point, compared to 7.0 on the baselineprotocol.
the gap with uniﬁedqa (khashabi et al.,2020) is 6.7 on expert, compared to 2.9 on base-line.
however, the crowd evaluation data isfar less challenging than expert, suggesting thatexpert evaluations are more reliable than crowd-sourced evaluations for sending feedback and as-signing qualiﬁcations..we also ﬁnd that the justification interven-tion is ineffective as a stand-alone method for in-creasing nlu data quality.
a substantial propor-tion of the explanations submitted are duplicates,reused for multiple examples, or give trivial reason-ing that is not speciﬁc to the example..1222lastly, to evaluate the datasets for serious anno-tation artifacts we test the guessability of answersby omitting the questions from the model input.
this partial-input baseline achieves the lowest ac-curacy on expert, showing that the interventionsused to successfully boost example difﬁculty mayalso reduce annotation artifacts..2 related work.
creating nlu corpora existing nlu datasetshave been collected using a multitude of methods,ranging from expert-designed, to crowdsourced,to automatically scraped.
the widely used wino-grad schema dataset by levesque et al.
(2012) isconstructed manually by specialists and it has 273examples.
larger nlu datasets, more appropri-ate for training neural networks, are often crowd-sourced, though the crowdsourcing methods usedvary widely.
popular datasets, such as squad (ra-jpurkar et al., 2016) for question answering andsnli (bowman et al., 2015) for natural languageinference, are collected by providing crowdworkerswith a context passage and instructing workers towrite an example given the context.
rogers et al.
(2020) crowdsource quail, a qa dataset, by us-ing a more constrained data collection protocolwhere they require workers to write nine speciﬁctypes of question for each passage.
quac (choiet al., 2018) is crowdsourced by pairing crowd-workers, providing one worker with a wikipediaarticle, and instructing the second worker to askquestions about the hidden article..recently, there has been a ﬂurry of corpora col-lected using adversarial models in the crowdsourc-ing pipeline.
dua et al.
(2019), nie et al.
(2020a),and bartolo et al.
(2020) use models in the loop dur-ing data collection, where crowdworkers can onlysubmit examples that cannot be solved by the mod-els.
however, such datasets can be biased towardsquirks of the model used during data collection(zellers et al., 2019; gardner et al., 2020)..crowdsourcing methods while crowdsourcingmakes it easy to collect large datasets quickly, thereare some clear pitfalls: crowdworkers are generallyless knowledgeable than ﬁeld experts about therequirements the data needs to meet, crowdworkcan be monotonous resulting in repetitive and noisydata, and crowdsourcing platforms can create a“market for lemons” where fast work is incentivizedover careful, creative work because of poor qualityrequesters (akerlof, 1978; chandler et al., 2013)..daniel et al.
(2018) give a broad overview ofthe variables at play when trying to crowdsourcehigh-quality data, discussing many strategies avail-able to requesters.
motivated by the use of self-assessment in teaching boud (1995), dow et al.
(2012) study the effectiveness of self-assessmentand external assessment when collecting data forproduct reviews.
they ﬁnd that both strategiesare effective for improving the quality of submit-ted work.
however, gadiraju et al.
(2017) ﬁndthat crowdworker self-assessment can be unreli-able since poor-performing workers overestimatetheir ability.
drapeau et al.
(2016) test a justify-reconsider strategy: crowdworkers justify theirannotations in a relation extraction task, they areshown a justiﬁcation written by a different crowd-worker, or an expert, and are asked to reconsidertheir annotation.
they ﬁnd that this method signiﬁ-cantly boosts the accuracy of annotations..another commonly used strategy when crowd-sourcing nlp datasets is to only qualify workerswho pass an initial quiz or perform well in prelim-inary crowdsourcing batches (wang et al., 2013;cotterell and callison-burch, 2014; ning et al.,2020; shapira et al., 2020; roit et al., 2020).
inaddition to using careful qualiﬁcations, roit et al.
(2020) send workers feedback detailing errors theymade in their qa-srl annotation.
writing suchfeedback is labor-intensive and can become unten-able as the number of workers grows.
dow et al.
(2011) design a framework of promoting crowd-workers into “shepherding roles” to crowdsourcesuch feedback.
we compare expert and crowd-sourced feedback in our expert and crowd pro-tocols..3 data collection protocols.
we run our study on amazon mechanical turk.2at launch, crowdworkers are randomly assignedto one of four data collection protocols, illustratedin figure 1.3 to be included in the initial pool,workers need to have an approval rating of 98% orhigher, have at least 1,000 approved tasks, and belocated in the us, the uk, or canada..3.1 writing examples.
this task is used for collecting question-answerpairs in the crowdsourcing pipeline for all four pro-.
2https://www.mturk.com/3screenshots of the task interfaces, and code to replicate.
them, are provided in the git repository..1223tocols.
crowdworkers assigned to the baselineprotocol are presented with only this task..in this writing task, we provide a context passagedrawn from the open american national corpus(ide and suderman, 2006).4 inspired by hu et al.
(2020), we ask workers to write two questions perpassage with four answer choices each.
we directworkers to ensure that the questions are answerablegiven the passage and that there is only one correctanswer for each question.
we instruct them to limitword overlap between their answer choices and thepassage and to write distracting answer choices thatwill seem plausibly correct to someone who hasn’tcarefully read the passage.
to clarify these criteria,we provide examples of good and bad questions..3.2 self-assessment.
workers assigned to the justification protocolare given the writing task described above (section3.1) and are also tasked with writing a 1–3 sentenceexplanation for each question.
they are asked toexplain the reasoning needed to select the correctanswer choice, mentioning what they think makesthe question they wrote challenging..3.3.iterative feedback and qualiﬁcation.
tutorial workers assigned to the crowd andexpert protocols are directed to a tutorial uponassignment.
the tutorial consists of two quizzesand writing tasks.
the quizzes have four steps.
ineach step workers are shown a passage, two ques-tion candidates and are asked to select which can-didate (i) is less ambiguous, (ii) is more difﬁcult,(iii) is more creative, or (iv) has better distractinganswer choices.
these concepts are informallydescribed in the writing task instructions, but thetutorial makes the rubric explicit, giving crowd-workers a clearer understanding of our desiderata.
we give workers immediate feedback on their per-formance during the ﬁrst quiz and not the secondso that we can use it for evaluation.
lastly, forthe tutorial writing tasks, we provide two passagesand ask workers to write two questions (with an-swer choices) for each passage.
these questionsare graded by three experts5 using a rubric with thesame metrics described in the quiz, shown in fig-ure 2. we give the qualiﬁcation to continue onto.
4following multinli (williams et al., 2018), we selectthe ten genres from oanc that are accessible to non-experts:face-to-face, telephone, 911, travel, letters, slate, verbatim,government, oup, and ﬁction..5the expert annotators are authors of this paper and dhara.
mungra.
all have research experience in nlu..1. is the question answerable and unambiguous?.
yes.
no.
yes, but the label is wrong.
2. how closely do you think someone would need toread the passage to correctly answer the question?.
wouldn’t need to read itquickly skim a few words or one sentencequickly skim a few sentencesread the whole passagemay need to read the passage more than once.
3. how creative do you think the question is?.
not creativea little creativefairly creative very creative.
4. does the example have distracting answer.
choices?
yes.
no.
figure 2: the grading rubric used to evaluate examplessubmitted during the intermediate writing rounds in theexpert and crowd protocols..the writing tasks to the top 60% of crowdwork-ers who complete the tutorial.
we only qualify theworkers who wrote answerable, unambiguous ques-tions, and we qualify enough workers to ensure thatwe would have a large pool of people in our ﬁnalwriting round..intermediate writing rounds after passingthe tutorial, workers go through three small roundsof writing tasks.
at the end of each round, wesend them feedback and qualify a smaller pool ofworkers for the next round.
we only collect 400–500 examples in these intermediate rounds.
at theend of each round, we evaluate the submitted workusing the same rubric deﬁned in the tutorial.
inthe expert protocol, three experts grade workersubmissions, evaluating at least four questions perworker.
the evaluation annotations are averagedand workers are qualiﬁed for the next round basedon their performance.
the qualifying workers aresent a message with feedback on their performanceand a bonus for qualifying.
appendix a gives de-tails on the feedback sent..evaluating the examples in each round is labor-intensive and challenging to scale (avg.
30 expert-min.
per worker).
in the crowd protocol we exper-iment with crowdsourcing these evaluations.
afterthe ﬁrst intermediate writing round in crowd, ex-perts evaluate the submitted work.
the evaluationsare used to qualify workers for the second writinground and to promote the top 20% of workers intoa feedback role.
after intermediate writing rounds.
12242 and 3, the promoted workers are tasked with eval-uating all the examples (no one evaluates their ownwork).
we collect ﬁve evaluations per exampleand use the averaged scores to send feedback andqualify workers for the subsequent round..for both crowd and expert protocols, the top80% of workers are requaliﬁed at the end of eachround.
of the 150 workers who complete the tuto-rial, 20% qualify for the ﬁnal writing round.
ourqualiﬁcation rate is partly dictated by a desire tohave a large enough pool of people in the ﬁnal writ-ing task to ensure that no dataset is skewed by onlya few people (geva et al., 2019)..cost we aim to ensure that our pay rate is at leastus $15/hr for all tasks.
the total cost per question,excluding platform fees, is $1.75 for the baselineprotocol and $2 for justification.
if we discardall the data collected in the intermediate writingrounds, the cost is $3.76 per question for expert,6and $5 for crowd..the average pay given during training to workersthat qualify for the ﬁnal writing task in expert isabout $120/worker (with an estimated 6–7 hoursspent in training).
in crowd, there is an additionalcost of $85/worker for collecting crowdsourcedevaluations.
the cost per example, after training,is $1.75 per question for both protocols, and totaltraining cost does not scale linearly with datasetsize, as one may not need twice as many writersfor double the dataset size.
more details on ourpayment and incentive structure can be found inappendix b..4 data validation.
we collect label annotations by asking crowdwork-ers to pick the correct answer choice for a question,given the context passage.
in addition to the answerchoices written by the writer, we add an invalidquestion / no answer option.
we validate the datafrom each protocol.
for crowd and expert, weonly validate the data from the ﬁnal large writingrounds.
data from all four protocols is shufﬂed andwe run a single validation task, collecting eithertwo or ten annotations per example..we use the same minimum qualiﬁcations as thewriting task (section 3), and require that workers.
6the discarded data collected during training was anno-tated by experts, and if we account for the cost of expert timeused, the cost for expert increases to $4.23/question.
thisestimate is based on the approximate hourly cost of paying aus phd student, including beneﬁts and tuition..ﬁrst pass a qualiﬁcation task.
the qualiﬁcation taskconsists of 5 multiple-choice qa examples thathave been annotated by experts.7 people who an-swer at least 3 out of 5 questions correctly receivethe qualiﬁcation to work on the validation tasks.
ofthe 200 crowdworkers who complete the qualiﬁca-tion task, 60% qualify for the main validation task.
following ho et al.
(2015), to incentivize higherquality annotations, we include expert labeled ex-amples in the validation task, constituting 10% ofall examples.
if a worker’s annotation accuracyon these labeled examples falls below 50%, weremove their qualiﬁcation (7 workers are disquali-ﬁed through this process), conversely workers wholabel these examples correctly receive a bonus..10-way validation pavlick and kwiatkowski(2019) show that annotation disagreement may notbe noise, but could be a signal of true ambiguity.
nie et al.
(2020b) recommend using high-human-agreement data for model evaluation to avoid suchambiguity.
to have enough annotations to ﬁlterthe data for high human agreement and to estimatehuman performance, we collect ten annotations for500 randomly sampled examples per protocol..cost we pay $2.50 for the qualiﬁcation task and$0.75 per pair of questions for the main validationtask.
for every 3 out of 4 expert-labeled examples aworker annotates correctly, we send a $0.50 bonus..5 datasets and analysis.
we collect around 1,500 question-answer pairsfrom each protocol design: 1,558 for baseline,1,534 for justification, 1,600 for crowd, and1,580 for expert.
we use the validation annota-tions to determine the gold-labels and to ﬁlter outexamples: if there is no majority agreement on theanswer choice, or if the majority selects invalidquestion, the example is discarded (∼ 5% of ex-amples).
for the 2-way annotated data, we takea majority vote over the two annotations plus theoriginal writer’s label.
for the 10-way annotateddata, we sample four annotations and take a ma-jority vote over those four plus the writer’s vote,reserving the remainder to compute an independentestimate of human performance..7these examples are taken from intermediate rounds 1, 2,.and 3 of the expert protocol..1225dataset.
baselinejustificationcrowdexpert.
baselinejustificationcrowdexpert.
baselinejustificationcrowdexpert.
baselinejustificationcrowdexpert.
results on the 10-way annotated subset.
high agreement (>80%) portion of 10-way annotated data.
unanimous agreement portion of 10-way annotated data.
n.human roberta ∆.
uniqa ∆.
1492143715441500.
482471472464.
436419410383.
340307277271.
----.
95.995.594.892.8.
97.797.896.898.2.
99.198.798.699.3.
88.8 (0.2)86.5 (0.6)81.8 (0.7)81.3 (0.6).
----.
87.2 (0.8)86.7 (1.0)83.5 (1.0)80.6 (1.1).
89.3 (0.8)89.5 (0.6)86.2 (0.9)84.7 (1.3).
92.1 (0.7)93.2 (0.3)88.9 (0.9)85.4 (1.1).
8.78.911.312.2.
8.48.310.613.5.
7.05.59.713.9.
93.691.488.187.7.
92.590.990.589.8.
94.093.193.692.9.
96.295.897.192.5.
----.
3.34.74.33.0.
3.74.83.25.3.
2.92.91.46.7.table 1: human and model performance on each of our datatsets.
n shows the number of examples in the dataset.
roberta shows average zero-shot performance for six robertalarge models ﬁnetuned on race, standard devi-ation is in parentheses.
uniqa shows zero-shot performance of the t5-based uniﬁedqa-v2 model.
∆ shows thedifferences in human and model performance..5.1 human performance and agreement.
for the 10-way annotated subsets of the data, wetake a majority vote over the six annotations thatare not used when determining the gold answer, andcompare the result to the gold answer to estimatehuman performance.
table 1 shows the result foreach dataset.
the expert and crowd datasetshave lower human performance numbers thanbaseline and justification.
this is also mir-rored in the inter-annotator agreement for valida-tion, where krippendorf’s α (krippendorff, 1980)is 0.67 and 0.71 for expert and crowd, comparedto 0.81 and 0.77 for baseline and justification(table 3 in appendix c).
the lower agreement maybe reﬂective of the fact that while these examplesare still clearly human solvable, they are more chal-lenging than those in baseline and justifica-tion as a result, annotators are prone to highererror rates, motivating us to look at the higher agree-ment portions of the data to determine true datasetdifﬁculty.
and while the agreement rate is lowerfor expert and crowd, more than 80% of thedata still has high human-agreement on the gold-label, where at least 4 out of 5 annotators agree onthe label.
the remaining low-agreement examplesmay have more ambiguous questions, and we fol-low nie et al.’s (2020b) recommendation and focus.
our analysis on the high-agreement portions of thedataset..5.2 zero-shot model performance.
we test two pretrained models that perform wellon other comparable qa datasets: robertalarge(liu et al., 2019b) and uniﬁedqa-v2 (khashabiet al., 2020).
we ﬁne-tune robertalarge onrace (lai et al., 2017), a large-scale multiple-choice qa dataset that is commonly used fortraining (sun et al., 2019).
we ﬁne-tune 6robertalarge models and report the average per-formance across runs.
the uniﬁedqa-v2 model isa single t5-based model that has been trained on15 qa datasets.8 we also ﬁne-tune robertalargeon cosmosqa and quail, ﬁnding that zero-shotmodel performance is best with race ﬁne-tuningbut that the trends in model accuracy across ourfour datasets are consistent (appendix d)..5.3 comparing protocols.
as shown in table 1, model accuracy on the fulldatasets is lowest for expert, followed by crowd,justification, and then baseline.
however,model accuracy alone does not tell us how much.
8the authors of uniﬁedqa kindly shared the unreleased.
v2 model with us..1226headroom is left in the datasets.
instead, we lookat the difference between the estimated human per-formance and model performance..human–model gap the trends in the human–model gap on the 10-way annotated sample areinconsistent across models.
for a more conclusiveanalysis, we focus on the higher-agreement por-tions of the data where label ambiguity is minimal.
on the high agreement section of the datasets,both models’ performance is weakest on expert.
robertalarge shows the second largest human–model gap on crowd, however for uniﬁedqajustification is the next hardest dataset.
thisdiscrepancy between the two types of iterativefeedback protocols is even more apparent in theunanimous agreement portion of the data.
on theunanimous agreement examples, both models showthe lowest performance on expert but uniﬁed-qa achieves near perfect performance on crowd.
this suggests that while the crowd protocol usednearly the same crowdsourcing pipeline as expert,the evaluations done by experts are a much morereliable metric for selecting workers to qualify andfor generating feedback, at the cost of greater dif-ﬁculty with scaling to larger worker pools.
thisis conﬁrmed by inter-annotator agreement: expertagreement on the rubric-based evaluations has akrippendorf’s α of 0.65, while agreement betweencrowdworker evaluations is 0.33..self-justiﬁcation model performance on theunanimous agreement examples of justificationis comparable to, or better than, performance onbaseline.
to estimate the quality of justiﬁcations,we manually annotate a random sample of 100 jus-tiﬁcations.
about 48% (95% ci: [38%, 58%]) areduplicates or near-duplicates of other justiﬁcations,and of this group, nearly all are trivial (e.g.
goodand deep knowledge is needed to answer this ques-tion) and over half are in non-ﬂuent english (e.g.
toread the complete passage to understand the ques-tion to answer.).
on the other hand, non-duplicatejustiﬁcations are generally of much higher quality,mentioning distractors, giving speciﬁc reasoning,and rewording phrases from the passage (e.g.
only#1 is discussed in that last paragraph.
the rest ofthe parts are from the book, not the essay.
also theanswer is paraphrased from “zero-sum” to “one’sgain is another’s loss”).
while we ﬁnd that justi-fication does not work as a stand-alone strategy,we cannot conclude that self-justiﬁcation would.
partial input.
p + a.q + a.a.baselinejustificationcrowdexpert.
69.9 (4.7)57.9 (1.3)57.7 (3.1)52.0 (1.5).
41.9 (2.9)38.3 (2.2)43.9 (2.0)42.8 (1.8).
34.9 (2.4)33.9 (6.3)35.2 (1.9)35.7 (1.4).
table 2: accuracy (std.)
of partial input baselines.
p ispassage, q is question, and a is answer choices..be equally ineffective if combined with more ag-gressive screening to exclude crowdworkers whoauthor trivial or duplicate justiﬁcations.
gadirajuet al.
(2017) also recommend using the accuracy ofa worker’s self-assessments to screen workers..cross-protocol transfer since the datasetsfrom some protocols are clearly more chal-lenging than others,it prompts the question:are these datasets also better for training mod-els?
to test cross-protocol transfer, we ﬁne-tunerobertalarge on one dataset and evaluate on theother three.
we ﬁnd that model accuracy is notsubstantively better from ﬁne-tuning on any onedataset (table 5, appendix e).
the beneﬁt of ex-pert being a more challenging evaluation datasetdoes not clearly translate to training.
however,these datasets may be too small to offer clear anddistinguishable value in this setting..annotation artifacts to test for undesirable ar-tifacts, we evaluate partial input baselines (kaushikand lipton, 2018; poliak et al., 2018).
we take arobertalarge model, pretrained on race, andﬁne-tune it using ﬁve-fold cross-validation, pro-viding only part of the example input.
we eval-uate three baselines: providing the model withthe passage and answer choices only, the questionand answer choices only, and the answer choicesalone.
results are shown in table 2. the pas-sage+answer baseline has signiﬁcantly lower per-formance on the expert dataset in comparison tothe others.
this indicates that the iterative feed-back and qualiﬁcation method using expert assess-ments not only increases overall example difﬁcultybut may also lower the prevalence of simple arti-facts that can reveal the answer.
performance ofthe question+answer and answer-only baselines iscomparably low on all four datasets..question and answer length we observe thatthe difﬁculty of the datasets is correlated with aver-age answer length (figure 3).
the hardest dataset,expert, also has the longest answer options with.
1227generic.
but our results suggest that difﬁculty maybe due more to the subtlety of the answer options,and the presence of distracting options, rather thanthe complexity or originality of the questions..order of questions we elicit two questions perpassage in all four protocols with the hypothesisthat the second question may be more difﬁcult onaggregate.
however, we ﬁnd that there is only aslight drop in model accuracy from the ﬁrst to sec-ond question on the crowd and expert datasets(1.0 and 0.7 percentage points).
and model ac-curacy on baseline remains stable, while it in-creases by 2.7 percentage points on justifica-tion.
a task design with minimal constraints, likeours, does not prompt workers to write an easierquestion followed by a more difﬁcult one, or viceversa..5.4.item response theory.
individual examples within any dataset can havedifferent levels of difﬁculty.
to better understandthe distribution of difﬁcult examples in each proto-col, we turn to item response theory (irt; bakerand kim, 1993), which has been used to estimateindividual example difﬁculty based on model re-sponses (lalor et al., 2019; mart´ınez-plumed et al.,2019).
speciﬁcally, we use the three-parameter lo-gistic (3pl) irt model, where an example is char-acterized by discrimination, difﬁculty, and guessingparameters.
discrimination deﬁnes how effectivean example is at distinguishing between weak andstrong models, difﬁculty deﬁnes the minimum abil-ity of a model needed to obtain high performance,and the guessing parameter deﬁnes the probabilityof a correct answer by random guessing.
followingvania et al.
(2021), we use 90 transformer-basedmodels ﬁne-tuned on race, with varying abilitylevels, and use their predictions on our four datasetsas responses.
for comparison, we also use modelpredictions on quail and cosmosqa.
refer toappendix f for more details..figure 4 shows the distribution of example difﬁ-culty for each protocol.
also plotted are the difﬁ-culty parameters for the intermediate rounds of datathat are collected in the iterative feedback proto-cols.9 we see that expert examples have the high-est median and 75th percentile difﬁculty scores,.
9the irt parameters for discrimination range from 0.6 to2.1, while for guessing they range from 0.03 to 0.74. however,we observe that the distributions of both parameters across thefour datasets are similar..figure 3: distribution of answer lengths.
the distri-butions for different datasets and for the correct andincorrect answer options are plotted separately..an average of 9.1 words, compared to 3.7 for base-line, 4.1 for justification, and 6.9 for crowd.
this reﬂects the tendency of the 1- and 2-wordanswers common in the baseline and justifica-tion datasets to be extracted directly from the pas-sage.
while sentence-length answers, more com-mon in expert and crowd, tend to be more ab-stractive.
figure 3 also shows that incorrect answeroptions tend to be shorter than correct ones.
thispattern holds across all datasets, suggesting a weaksurface cue that models could exploit.
using ananswer-length based heuristic alone, accuracy issimilar to the answer-only model baseline: 34.2%for baseline, 31.7% for justification, 31.5%for crowd, and 34.3% for expert..wh-words we ﬁnd that the questions in expertand crowd protocols have similar distributionsof wh-words, with many why questions and fewwho or when questions compared to the baselineand justification protocols, seemingly indicat-ing that this additional feedback prompts workersto write more complex questions..non-passage-speciﬁc questions we also ob-serve that many questions in the datasets are for-mulaic and include no passage-speciﬁc content, forinstance which of the following is true?, what isthe main point of the passage?, and which of thefollowing is not mentioned in the passage?.
wemanually annotate 200 questions from each proto-col for questions of this kind.
we ﬁnd that thereis no clear association between the dataset’s dif-ﬁculty and the frequency of such questions: 15%of questions in expert are generic, compared to4% for crowd, 10% for justification, and 3%for baseline.
we might expect that higher qual-ity examples that require reading a passage closelywould ask questions that are speciﬁc rather than.
122805101520number of words0.000.050.100.150.200.25densityexpertcrowdjustificationbaselinecorrect answersincorrect answersresults suggest that asking workers to write justi-ﬁcations is not a helpful stand-alone strategy forimproving nlu dataset difﬁculty, at least in theabsence of explicit incentives for workers to writehigh-quality justiﬁcations.
however, we ﬁnd thattraining workers using an iterative feedback andrequaliﬁcation protocol is an effective strategy forcollecting high-quality qa data.
the beneﬁt of thismethod is most evident in the high-agreement sub-set of the data where label noise is low.
we ﬁnd thatusing expert assessments to conduct this iterativeprotocol is fruitful, in contrast with crowdsourcedassessments that have much lower inter-annotatoragreement and the noisy signal from these assess-ments does not boost example difﬁculty..acknowledgements.
we thank dhara mungra for her early contribu-tions to this project, and for being one of the ex-pert graders during data collection.
we also thankdaniel khashabi for giving us access to uniﬁedqa-v2 for our experiments.
this work has beneﬁtedfrom ﬁnancial support to sb by eric and wendyschmidt (made by recommendation of the schmidtfutures program), apple, and intuit, and fromin-kind support by the nyu high-performancecomputing center and by nvidia corporation(with the donation of a titan v gpu).
ss was sup-ported by jst presto grant no.
jpmjpr20c4.
this material is based upon work supported bythe national science foundation under grant no.
1922658. any opinions, ﬁndings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reﬂectthe views of the national science foundation..ethics statement.
we are cognizant of the asymmetrical relationshipbetween requesters and workers in crowdsourcing,and we take care to be responsive employers andto pay a wage commensurate with the high-qualitywork we’re looking for.
so in additional to the eth-ical reasons for paying fair wages, our successeswith collecting high-quality nlu data offer weakevidence that others should also follow this practice.
however, the mere existence of more research onnlu crowdsourcing with positive results could ar-guably encourage more people to do crowdsourcingunder a conventional model, with low pay and littleworker recourse against employer malpractice.
theonly personal information we collect from workers.
figure 4: distribution of examples according to theirdifﬁculty parameters.
crowd/expert-{1, 2, 3} arethe three intermediate rounds of data that are not in-cluded in the ﬁnal datasets..while baseline scores the lowest.
we also notethat the greatest gain in difﬁculty for crowd ex-amples happens between rounds 1 and 2, the onlyfeedback and qualiﬁcation stage that is conductedby experts.
this offers further evidence that expertassessments are more reliable, and that crowdsourc-ing such assessments poses a signiﬁcant challenge.
while the examples in expert have higher difﬁ-culty scores than the other protocols, the scores aresigniﬁcantly lower than those for cosmosqa andquail (all four datasets show similar discrimina-tion scores to cosmosqa and quail).
the datacollection methods used for both cosmosqa andquail differ substantially from methods we tested.
rogers et al.
(2020) constrain the task design forquail and require workers to write questions ofspeciﬁc types, like those targeting temporal reason-ing.
similarly, in cosmosqa workers are encour-aged to write questions that require causal or deduc-tive commonsense reasoning.
in contrast, we avoiddictating question type in our instructions.
the irtresults here suggest that using prior knowledge toslightly constrain the task design can be effectivefor boosting example difﬁculty.
in addition to dif-fering task design, cosmosqa and quail also usequalitatively different sources for passages.
bothdatasets use blogs and personal stories, quail alsouses texts from published ﬁction and news.
explor-ing the effect of source text genre on crowdsourceddata quality is left to future work..6 conclusion.
we present a study to determine effective proto-cols for crowdsourcing difﬁcult nlu data.
werun a randomized trial to compare interventions inthe crowdsourcing pipeline and task design.
our.
1229basejustif.crowd-1crowd-2crowd-3crowdexpert-1expert-2expert-3expertcosmosqaquail10123difficultyis their mechanical turk worker ids, which wekeep secure and will not release.
however, we donot engage with issues of bias during data collec-tion and we expect that the data collected underall our protocols will, at least indirectly, reinforcestereotypes..we conﬁrmed with new york university’s irbthat crowdsourced nlp dataset construction work,including experimental work on data collectionmethods, is exempt from their oversight.
the onlypersonal information we collect from workers istheir mechanical turk worker ids, which we keepsecure and will not release..references.
george a. akerlof.
1978. the market for “lemons”:quality uncertainty and the market mechanism.
inuncertainty in economics.
academic press..frank b. baker and seock-ho kim.
1993..itemresponse theory: parameter estimation techniques.
journal ofthe american statistical association,88:707–707..max bartolo, alastair roberts, johannes welbl, sebas-tian riedel, and pontus stenetorp.
2020. beat theai: investigating adversarial human annotation forreading comprehension.
transactions of the associ-ation for computational linguistics, 8:662–678..michael s. bernstein, d. karger, r. miller, andj. brandt.
2012. analytic methods for optimizingrealtime crowdsourcing.
arxiv preprint 1204.2995..david boud.
1995. enhancing learning through self-.
assessment.
philadelphia: kogan page..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..chris callison-burch.
2009. fast, cheap, and creative:evaluating translation quality using amazon’s me-in proceedings of the 2009 con-chanical turk.
ference on empirical methods in natural languageprocessing, pages 286–295, singapore.
associationfor computational linguistics..jesse chandler, gabriele paolacci, and pam mueller.
2013. risks and rewards of crowdsourcing market-in pietro michelucci, editor, handbook ofplaces.
human computation, pages 377–392.
springer newyork..eunsol choi, he he, mohit iyyer, mark yatskar, wen-tau yih, yejin choi, percy liang, and luke zettle-moyer.
2018. quac: question answering in con-in proceedings of the 2018 conference ontext..empirical methods in natural language processing,pages 2174–2184, brussels, belgium.
associationfor computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..ryan cotterell and chris callison-burch.
2014. amulti-dialect, multi-genre corpus of informal writtenin proceedings of the ninth internationalarabic.
conference on language resources and evaluation(lrec’14), pages 241–245, reykjavik, iceland.
eu-ropean language resources association (elra)..ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in machine learning challenges work-shop, pages 177–190.
springer..florian daniel, pavel kucherbaev, cinzia cappiello,boualem benatallah, and mohammad allahbakhsh.
2018. quality control in crowdsourcing: a surveyof quality attributes, assessment techniques, and as-surance actions.
acm comput.
surv., 51(1)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..steven dow, anand kulkarni, brie bunge, trucnguyen, scott klemmer, and bj¨orn hartmann.
2011.shepherding the crowd: managing and providingin chi ’11 extendedfeedback to crowd workers.
abstracts on human factors in computing systems,chi ea ’11, page 1669–1674, new york, ny, usa.
association for computing machinery..steven dow, anand kulkarni, scott klemmer, andbj¨orn hartmann.
2012.shepherding the crowdin proceedings of the acmyields better work.
2012 conference on computer supported coopera-tive work, cscw ’12, page 1013–1022, new york,ny, usa.
association for computing machinery..ryan drapeau, l. chilton,.
jonathan bragg, anddaniel s. weld.
2016. microtalk: using argumen-tation to improve crowdsourcing accuracy.
in pro-ceedings of the aaai conference on human compu-tation and crowdsourcing (hcomp), pages 32–41.
aaai press..1230dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..ujwal gadiraju, besnik fetahu, ricardo kawase,patrick siehndel, and stefan dietze.
2017. usingworker self-assessments for competence-based pre-selection in crowdsourcing microtasks.
acm trans-actions on computer-human interaction (tochi),24(4):1–26..matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameersingh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesin findings of the associationvia contrast sets.
for computational linguistics: emnlp 2020, pages1307–1323, online.
association for computationallinguistics..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an inves-tigation of annotator bias in natural language under-standing datasets.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1161–1166, hong kong, china.
as-sociation for computational linguistics..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah a.smith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107–112, new orleans, louisiana.
associa-tion for computational linguistics..chien-ju ho, aleksandrs slivkins, siddharth suri, andincentivizingjennifer wortman vaughan.
2015.high quality crowdwork, page 419–429.
interna-tional world wide web conferences steering com-mittee, republic and canton of geneva, che..jeff howe.
2006. the rise of crowdsourcing.
wired.
magazine, 14(6):1–4..the naacl hlt 2009 workshop on active learn-ing for natural language processing, pages 27–35,boulder, colorado.
association for computationallinguistics..hai hu, kyle richardson, liang xu, lu li, sandrak¨ubler, and lawrence moss.
2020. ocnli: orig-inal chinese natural language inference.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 3512–3526, online.
as-sociation for computational linguistics..lifu huang, ronan le bras, chandra bhagavatula, andyejin choi.
2019. cosmos qa: machine readingcomprehension with contextual commonsense rea-soning.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages2391–2401, hong kong, china.
association forcomputational linguistics..nancy ide and keith suderman.
2006..integratinglinguistic resources: the american national corpusin proceedings of the fifth internationalmodel.
conference on language resources and evaluation(lrec’06), genoa, italy.
european language re-sources association (elra)..divyansh kaushik and zachary c. lipton.
2018. howmuch reading does reading comprehension require?
a critical investigation of popular benchmarks.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages5010–5015, brussels, belgium.
association forcomputational linguistics..daniel khashabi, sewon min, tushar khot, ashishsabharwal, oyvind tafjord, peter clark, and han-naneh hajishirzi.
2020. unifiedqa: crossing for-mat boundaries with a single qa system.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 1896–1907, online.
as-sociation for computational linguistics..aniket kittur, ed h. chi, and bongwon suh.
2008.crowdsourcing user studies with mechanical turk.
in proceedings of the sigchi conference on hu-man factors in computing systems, chi ’08, page453–456, new york, ny, usa.
association forcomputing machinery..aniket kittur, boris smus, susheel khamkar, androbert e. kraut.
2011. crowdforge: crowdsourc-ing complex work.
in proceedings of the 24th an-nual acm symposium on user interface softwareand technology, uist ’11, page 43–52, new york,ny, usa.
association for computing machinery..klaus krippendorff.
1980. content analysis: an intro-duction to its methodology.
beverly hills, ca: sagepublications..pei-yun hsueh, prem melville, and vikas sindhwani.
2009. data quality from crowdsourcing: a studyin proceedings ofof annotation selection criteria..guokun lai, qizhe xie, hanxiao liu, yiming yang,and eduard hovy.
2017. race: large-scale read-ining comprehension dataset from examinations..1231proceedings of the 2017 conference on empiricalmethods in natural language processing, pages785–794, copenhagen, denmark.
association forcomputational linguistics..john p. lalor, hao wu, and hong yu.
2016. build-ing an evaluation scale using item response theory.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages648–657, austin, texas.
association for computa-tional linguistics..john p. lalor, hao wu, and hong yu.
2019. learn-ing latent parameters without human response pat-terns: item response theory with artiﬁcial crowds.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4249–4259, hong kong, china.
association for computa-tional linguistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020..hector j levesque, ernest davis, and leora morgen-stern.
2012. the winograd schema challenge.
inproceedings of the thirteenth international confer-ence on principles of knowledge representationand reasoning, pages 552–561..shixia liu, changjian chen, yafeng lu, fangxinouyang, and bin wang.
2019a.
an interac-tive method to improve crowdsourced annotations.
ieee transactions on visualization and computergraphics, 25(1):235–245..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint 1907.11692..fernando mart´ınez-plumed, ricardo b.c.
prudˆencio,adolfo mart´ınez-us´o, and jos´e hern´andez-orallo.
2019.item response theory in ai: analysing ma-chine learning classiﬁers at the instance level.
artiﬁ-cial intelligence, 271:18 – 42..todor mihaylov, peter clark, tushar khot, and ashishsabharwal.
2018. can a suit of armor conduct elec-tricity?
a new dataset for open book question an-swering.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 2381–2391, brussels, belgium.
associationfor computational linguistics..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020a.
ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computational.
linguistics, pages 4885–4901, online.
associationfor computational linguistics..yixin nie, xiang zhou, and mohit bansal.
2020b.
what can we learn from collective human opinionsin proceed-on natural language inference data?
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages9131–9143, online.
association for computationallinguistics..qiang ning, hao wu, rujun han, nanyun peng, mattgardner, and dan roth.
2020. torque: a readingcomprehension dataset of temporal ordering ques-in proceedings of the 2020 conference ontions.
empirical methods in natural language process-ing (emnlp), pages 1158–1172, online.
associa-tion for computational linguistics..ellie pavlick and tom kwiatkowski.
2019..inherentdisagreements in human textual inferences.
transac-tions of the association for computational linguis-tics, 7:677–694..adam poliak, jason naradowsky, aparajita haldar,rachel rudinger, and benjamin van durme.
2018.hypothesis only baselines in natural language in-in proceedings of the seventh joint con-ference.
ference on lexical and computational semantics,pages 180–191, new orleans, louisiana.
associa-tion for computational linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..anna rogers, olga kovaleva, matthew downey, andanna rumshisky.
2020. getting closer to ai com-plete question answering: a set of prerequisite realin proceedings of the thirty-fourth aaaitasks.
conference on artiﬁcial intelligence, pages 8722–8731. aaai press..paul roit, ayal klein, daniela stepanov, jonathanmamou, julian michael, gabriel stanovsky, lukezettlemoyer, and ido dagan.
2020. controlledcrowdsourcing for high-quality qa-srl annotation.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages7008–7013, online.
association for computationallinguistics..ori shapira, ramakanth pasunuru, hadar ronen, mo-hit bansal, yael amsterdamer, and ido dagan.
2020.evaluating interactive summarization: an expansion-based framework.
arxiv preprint 2009.08380..rion snow, brendan o’connor, daniel jurafsky, andandrew ng.
2008. cheap and fast – but is it good?
evaluating non-expert annotations for natural lan-guage tasks.
in proceedings of the 2008 conference.
1232teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..rowan zellers, ari holtzman, yonatan bisk, alifarhadi, and yejin choi.
2019. hellaswag: canin pro-a machine really ﬁnish your sentence?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4791–4800, florence, italy.
association for computationallinguistics..yian zhang, alex warstadt, haau-sing li, andsamuel r. bowman.
2021. when do you need bil-lions of words of pretraining data?
in proceedingsof the 59th annual meeting of the association forcomputational linguistics.
association for compu-tational linguistics..on empirical methods in natural language process-ing, pages 254–263, honolulu, hawaii.
associationfor computational linguistics..david q. sun, hadas kotek, christopher klein,mayank gupta, william li, and jason d. williams.
improving human-labeled data through dy-2020.in proceed-namic automatic conﬂict resolution.
ings of the 28th international conference on com-putational linguistics, pages 3547–3557, barcelona,spain (online).
international committee on compu-tational linguistics..kai sun, dian yu, dong yu, and claire cardie.
2019.improving machine reading comprehension within proceedings of thegeneral reading strategies.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 2633–2643, minneapolis, min-nesota.
association for computational linguistics..alon talmor, jonathan herzig, nicholas lourie, andjonathan berant.
2019. commonsenseqa: a ques-tion answering challenge targeting commonsenseknowledge.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4149–4158, minneapolis, minnesota.
associ-ation for computational linguistics..clara vania, phu mon htut, william huang, dharajason phang,mungra, richard yuanzhe pang,haokun liu, kyunghyun cho, and samuel r. bow-man.
2021. comparing test sets with item responsein proceedings of the 59th annual meet-theory.
ing of the association for computational linguistics.
association for computational linguistics..jennifer wortman vaughan.
2018. making better useof the crowd: how crowdsourcing can advance ma-chine learning research.
journal of machine learn-ing research, 18(193):1–46..aobo wang, cong duy vu hoang, and min-yen kan.2013. perspectives on crowdsourcing annotationslanguage re-for natural language processing.
sources and evaluation, 47:9–31..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,.
1233a iterative protocol feedback.
in the expert and crowd protocols, we conductthree small intermediate rounds of data collectionto help train crowdworkers and give them feed-back on their submissions.
at the end of eachsmall round of writing, the submitted examples areevaluated either by experts or crowdworkers, as de-scribed in section 3.3. the rubric given in figure2 is used during evaluations.
after compiling theevaluations, we qualify the top 80% of workers forthe next round and send them a feedback message.
we tell workers what their difﬁculty and creativityscores are in comparison to the average.
we alsotell them what percentage of their question-answerpairs were labeled as having distracting answerchoices and what percentage were labeled ambigu-ous, with examples of any such questions.
lastly,we list the examples they wrote that received thehighest and lowest overall rubric scores..b payment and incentive structure.
the compensation for for writing two questions inthe baseline writing task is $3.50, excluding plat-form fees, we estimate it takes 12–15 minutes todo a close reading of the passage and write twochallenging questions.
for the justification pro-tocol, the compensation is $4 per task to account forthe additional time it takes to write a justiﬁcationsfor each question.
for the tutorial that workers inthe crowd and expert protocols need to com-plete, we pay $3.50, and give a bonus of $1.50 ifthey qualify onto the writing tasks.
similarly, atthe end of each intermediate writing batch, a bonusis sent to the workers that qualify for the subse-quent round: $5, $7, and $10 after the 1st, 2nd and3rd rounds respectively.
promoted workers whoare tasked with the crowdsourced evaluations inthe crowd protocol, are paid $0.50 per question.
they are also sent a bonus of $5 for each round ofevaluations they complete..c inter-annotator agreement.
table 3 shows the inter-annotator agreement duringdata validation task for each dataset.
the krippen-dorf’s α is lowest for expert, which also has thelowest human performance baseline, likely due tothe pressure to produce subtle questions..protocol.
baselinejustificationcrowdexpert.
αall.
0.810.770.710.67.α10.
0.790.740.690.64.table 3: inter-annotator agreement statistics for eachdatatset.
αall and α10 give the krippendorf’s α scoresfor all examples and the subset of 10-way annotatedexamples respectively..dataset.
race.
cosmosqa quail.
baselinejustificationcrowdexpert.
88.886.581.881.3.
74.165.965.156.8.
80.568.862.752.4.table 4: zero-shot model accuracy on our datasets,when training on the datasets named in the columns..d zero-shot model performance:.
cosmosqa and quail.
in addition to ﬁne-tuning robertalarge onrace, we also ﬁne-tune it on cosmosqa, andquail to test zero-shot model performance.
ta-ble 4 shows the zero-shot results.
we observe thatmodel performance on our datasets is substantiallyworse when ﬁne-tuning on cosmosqa or quail.
however, the pattern in model behaviour is con-sistent regardless of corpus used.
in all three con-ditions, model accuracy is highest on baseline,followed by justification, then crowd, andﬁnally expert..e cross-protocol transfer.
as discussed in section 5.3, we test cross-protocoltransfer by ﬁne-tuning robertalarge on onedataset and evaluating on the other three.
for abaseline comparison, we also ﬁne-tune the modelon each dataset using ﬁve-fold cross-validation.
re-sults are shown in table 5..base.
just.
crowd.
exp.
basejustcrowdexpert.
-84.981.680.6.
88.2-83.281.2.
87.485.3-81.7.
87.884.981.7-.
cross-val.
87.9 (2.0)85.6 (2.4)82.5 (1.9)82.8 (1.4).
table 5: cross-protocol evaluation where the row andcolumn indicate target and source datasets respectively.
cross-val shows the accuracy and std.
dev.
from ﬁve-fold cross-validation on each dataset..1234batch size of 8, learning rate of 1.0 × 10−5, andﬁnetune the models using the adam optimizer for4 epochs on the race dataset..f irt setup.
irt model we use the 3pl irt model, wherethe probability of a responder i of answering anitem j is given as:.
pj(θi) = γj +.
1 − γj1 + e−αj (θi−βj )).
where α, β, γ denote the discrimination, the dif-ﬁculty, and the guessing parameters, respectively.
following lalor et al.
(2019), we use variationalinference (vi) to estimate these parameters.
givena set of model responses m , we use the followingvariational posterior to estimate the joint probabil-ity of the parameters π(θ, α, β, γ | m ):.
q(θ, α, β, γ) =.
πθi (θi).
j (αi)πβπα.
j (βi)πγ.
j (γi),.
i(cid:89).
i=1.
j(cid:89).
j=1.
θ ) for θ, n (µα, σ2.
where πρ(·) is the density for parameter ρ. weuse the following distributions for each parameter:α) for log α, n (µβ, σ2n (µθ, σ2β)γ) for sigmoid−1(γ).
we thenfor β, and n (µγ, σ2ﬁt the posterior parameters by minimizing the kldivergence between q(θ, α, β, γ) and the true pos-terior π(θ, α, β, γ | y ).
this is equivalent to mini-mizing the evidence lower bound (elbo)..to control for different test sizes, we weightthe log likelihood of each item’s parameter bythe inverse of the item’s test size when ﬁtting theparameters.
we adapt prior used by lalor et al.
(2019) for each parameter: n (0, 1) for θ, β, andsigmoid−1(γ).
for log α, we use n (0, σ2α) wherewe set σα by searching [0.25, 0.5] by increments of0.05 and use the value yielding the highest elbo..pretrained transformer models we use 18transformer-based models:albert-xxl-v2 (lan et al., 2020), robertalarge androbertabase (liu et al., 2019b), bertlarge andbertbase (devlin et al., 2019), xlm-r (conneauet al., 2020), and 12 minibertas (zhang et al.,2021).10 we ﬁne-tune each of these models onrace, and keep ﬁve different checkpoints—at1%, 10%, 25%, and 50% of the maximum trainingepochs, plus the best checkpoint on the racevalidation set.
in total, we have 90 model responsesfor each test example.
for all the models, we use a.
10we use pretrained models distributed with huggingface.
transformers (wolf et al., 2020)..1235