adept: an adjective-dependent plausibility task.
ali emami1, ian porada1, alexandra olteanu2,kaheer suleman2, adam trischler2, and jackie chi kit cheung1.
1mila, mcgill university2microsoft research montr´eal{ali.emami, ian.porada}@mail.mcgill.ca{alexandra.olteanu, adam.trischler, kasulema}@microsoft.comjcheung@cs.mcgill.ca.
abstract.
a false contractis more likely to be re-jected than a contract is, yet a false key isless likely than a key to open doors.
whilecorrectly interpreting and assessing the ef-fects of such adjective-noun pairs (e.g., falsekey) on the plausibility of given events (e.g.,opening doors) underpins many natural lan-guage understanding tasks, doing so often re-quires a signiﬁcant degree of world knowl-edge and common-sense reasoning.
we intro-duce adept – a large-scale semantic plausi-bility task consisting of over 16 thousand sen-tences that are paired with slightly modiﬁedversions obtained by adding an adjective to anoun.
overall, we ﬁnd that while the task ap-pears easier for human judges (85% accuracy),it proves more difﬁcult for transformer-basedmodels like roberta (71% accuracy).
ourexperiments also show that neither the adjec-tive itself nor its taxonomic class sufﬁce in de-termining the correct plausibility judgement,emphasizing the importance of endowing auto-matic natural language understanding systemswith more context sensitivity and common-sense reasoning..1.introduction.
discerning the varying effects of adjectival mod-iﬁers on the reading of a sentence is critical in avariety of tasks involving natural language under-standing.
consider the following examples:.
(1).
a. a [dead] monkey turns on a light switch.
b. a [dead] leg has one foot.
c. a [dead] leaf falls from a tree in autumn..the reading of these sentences with and withoutthe modiﬁer dead is notably different.
the plausi-bility judgement of the event where a monkey turnson a light switch decreases when the adjectivalmodiﬁer dead is added, while in the 1b or 1c exam-ples, adding the same modiﬁer leads to no changeor an increase in event plausibility, respectively..this observation has important ramiﬁcations formany nlp applications like information extrac-tion (ie) and recognizing textual entailment (rte),where solutions have often relied on normativerules that group the effects of adjectives accordingto either the adjective or its taxonomic class (mc-nally and boleda, 2004; amoia and gardent, 2007;mccrae et al., 2014).
these taxonomies distinguishadjectives like false, dead, alleged (non-subsective)from others like red, large, or valid (subsective)..speciﬁcally, while the 1a example may inﬂu-ence systems to adopt the rule that adding a non-subsective adjective like dead to a noun leads to adecrease in plausibility, the other examples suggesta conﬂicting rule.
distinguishing the effects of dif-ferent adjectives (beyond just their denotation) maythus require common-sense and world knowledge.
powerful, massively pre-trained language mod-els (lms) have pushed the performance on vari-ous natural language understanding benchmarksto impressive ﬁgures; transformer architectures in-cluding bert and roberta are believed to per-form at near human-level performance on a num-ber of natural language inference (nli) tasks (liuet al., 2019), while the recently proposed deberta,which builds upon the former two architectures,performs at state-of-the-art on mnli, rte, qnliand wnli (he et al., 2020).
it is however un-clear whether the complex effects of the classesof modiﬁers exampled above are captured by thecompeting models given their sparsity in both thecorpora and existing nli benchmarks..to examine the ability of lms to capture anddistinguish the effects of adjectives on events plau-sibility, we present a challenge task formulated asa plausibility classiﬁcation problem consisting ofsentence pairs with and without inserting possibleadjectives.
we do so to understand the strengthsand weaknesses of lms that have led to state-of-the-art performance in downstream nli-tasks.
ta-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7117–7128august1–6,2021.©2021associationforcomputationallinguistics7117adept instance.
plausibility change.
(1a): a [false] key opens doors.
(1b): a [false] statement is a lie.
(1c): a [false] alarm causes danger..(2a) an [outstanding] year is made up of 365 days.
(2b) an [outstanding] coach pushes his players.
(2c) an [outstanding] professor waits for tenure..(3a) a [dead] monkey turns on a light switch.
(3b) a [dead] leg has one foot.
(3c) a [dead] leaf falls from a tree in autumn..(4a) an [old] graveyard is for settings for horror movies.
(4b) an [old] parrot lays and egg.
(4c) an [old] nun prays..inserted modiﬁer(taxonomic class).
false (ns)false (ns)false (ns).
outstanding (s)outstanding (s)outstanding (s).
dead (ns)dead (ns)dead (ns).
old (s)old (s)old (s).
less likelynecessarily truemore likely.
equally likelymore likelyless likely.
impossibleequally likelymore likely.
more likelyless likelyequally likely.
table 1: examples of adept instances, revealing the diverse effects on plausibility change due to different adjec-tival modiﬁers.
the plausibility change depends more on the context of the sentence, and less on the modiﬁer orits taxonomic class..ble 1 illustrates the task with several examples.
ourcontributions are three-fold:.
we introduce a novel plausibility task: usingautomated mechanisms to extract, ﬁlter and con-struct natural sentences, we create adept—a largehuman-labeled semantic plausibility task consist-ing of 16 thousand pairs of sentences that differonly by one adjective added to a noun, and designedto resist the statistical correlations that might un-derpin modern distributional lexical semantics.1.
we show that transformer-based models arenot yet adept at adept: our ﬁndings suggestperformance gaps between humans and large lan-guage representation models on adept, which ap-pears to be in large part due to the models’ insen-sitivity to context, indicating an important area fortheir improvement..we show that the effect of adjectival modiﬁerson event plausibility is context dependent: wequantify the degree to which plausibility judge-ments vary for the same adjective and taxonomicclass, ﬁnding that rules based only on the adjectiveor its denotation are insufﬁcient when assessing theplausibility readings of events.
for example, in ourtask, the non-subsective adjective like dead led toa decrease in events plausibility as frequently as itled to no change at all..building on prior work showing that norma-tive rules are often broken for subsective adjec-tives (pavlick and callison-burch, 2016), we inves-.
1theour.
corpus.
and.
the.
code.
ofresultsexperimentalhttps://github.com/aemami1/adept..toare.
reproduceavailable.
allat.
tigate possible effects across all types of adjectives,beyond just the taxonomical categories.
the scopeof our analysis also goes beyond entailment effects,examining the effects on plausibility, which can beseen as both complimentary and even an extensionto entailment tasks..2 background and related work.
taxonomy of adjectives: the taxonomic clas-siﬁcation of adjectives into subsective and non-subsective categories originates from the works ofparsons (1970), montague (1970), clark (1970) &kamp and keenan (1975).
canonically, subsectiveadjectives modify a noun such that the extensionof the adjective-noun pair is a subset of the exten-sion of the noun alone (e.g., a blue ﬁsh is still aﬁsh and a loose tooth is still a tooth).
in contrast,non-subsective adjectives modify a noun such thatthe extension of the adjective-noun pair is not asubset of the noun’s extension (e.g., a former presi-dent is not a president or an alleged criminal is notnecessarily a criminal).
kamp and partee (1995)further divided non-subsective adjectives in twocategories: privative and plain.
while when com-bined with nouns privative adjectives produce adisjoint set of entities from the original noun (e.g.,former president does not fall under the class ofpresidents, making former a privative adjective),plain non-subsective adjectives do not guaranteethis mutual exclusiveness (e.g., an alleged criminalmay or may not be a criminal)..this classiﬁcation scheme has been adoptedfor many nlp applications including ie andrte (amoia and gardent, 2006, 2007; mccrae.
7118et al., 2014).
for rte, inference rules were de-veloped according to whether the adjective wasnon-subsective or not.
for ie, non-subsective ad-jectives were treated as special cases for extractingopen ie relations (angeli et al., 2015).
we showthat there is also a relation between an adjectiveand the plausibility of the rest of the clause, evenfor subsective adjectives.
this has direct implica-tions for the extraction of generalizable abstractknowledge that can be extracted from a corpus..aspects of this classiﬁcation scheme have sincebeen challenged, resulting in efforts to either ex-pand on its deﬁnitions or abandon the taxonomyaltogether.
del pinal (2015) suggests that the mean-ing of certain nouns are only partially modiﬁed bynon-subsective adjectives (e.g., only the functionalfeatures are modiﬁed), while nayak et al.
(2014)tackle the categorization problem with a statisticalapproach focused on the proportion of propertiesshared by the noun and the adjective noun pair.
even more recently, inference rules relying on theoriginal taxonomy were observed not to be with-out exceptions; pavlick and callison-burch (2016)used human annotators to highlight cases where thedeletion of non-subsective adjectives from a sen-tence does not necessarily result in non-entailment.
these on-going examinations and revisions un-derpin a profound linguistic phenomenon of mutualdependence: while adjectives play a crucial rolein the correct interpretation of a sentence context,the context words are just as instrumental in de-termining the effect of an adjective; resulting in anumber of exceptions to taxonomically-based rules.
inspired by this, our work explores the broaderquestion of how dependent the effect of any ad-jective (beyond their taxonomical class) is on theinterpretation of a sentence.
for this, we frame ourexploration in terms of changes in the plausibilityof events, which we believe it can be seen as anextension to entailment..recognizing textual entailment & semanticplausibility: the rte challenges were yearlysources of textual inference examples (dagan et al.,2006) consisting of a three-way classiﬁcation taskwith the inputs as sentence pairs {t , h} withlabels for entailment, contradiction or unknown(meaning t neither contradicts nor entails h).
vari-ations of this task are also described in snli (bow-man et al., 2015) and mnli (williams et al., 2018).
the johns hopkins ordinal commonsense infer-ence (joci) task generalizes rte to the problem of.
determining relative change in semantic plausibilityon an ordinal 5-level likert scale (from impossibleto very likely) (zhang et al., 2017).
other seman-tic plausibility datasets have collected judgmentsfor the plausibility of single events (wang et al.,2018b) and the plausibility of adjectives modify-ing a meronym (mullenbach et al., 2019).
suchplausibility tasks have often been solved using ei-ther data-driven methods (huang and luo, 2017;sasaki et al., 2017) or pre-trained lms (radfordet al., 2019)..prior work has also collected human assessmentsof the plausibility of adjective-noun pairs (lapataet al., 1999; keller and lapata, 2003; zhang et al.,2019); however, this line of work speciﬁcally fo-cuses on the plausibility of bi-grams without con-text, known as selectional preference..3 the task: adept.
we develop adept, a semantic plausibility taskthat features over 16 thousand instances consistingof two sentences, where the second sentence differsfrom the ﬁrst only by the inclusion of an adjectivalmodiﬁer.
examples of these instances are in table1, where the inserted modiﬁer is bracketed..formally, given the original sentence s and themodiﬁed sentence s(cid:48), s(cid:48) is identical to s except forthe addition of an adjective a before the root nounof the original sentence.
the task is to assess theplausibility difference in the reading of s(cid:48) versusthat of s. the possible plausibility ratings are:.
1. impossible — s(cid:48) is improbable or illogical.
2. less likely — s(cid:48) is less likely than s.3. equally likely — s(cid:48) is as plausible as s is.
4. more likely — s(cid:48) is more likely than s.5. necessarily true — s(cid:48) is true by necessity,including repetitive use of phrases or wordsthat have similar meanings..4 dataset.
to construct adept, we scrape text samples fromenglish wikipedia and common crawl, extractingadjectival modiﬁer-noun pairs that occur with highfrequency.
we then curated these pairs through amulti-stage pipeline to ﬁlter out extraction errors,typos, and inappropriate words, as well as over-sample non-subsective adjectives which tend to bein the long-tail of a given corpora.
we then useexisting knowledge bases to ﬁnd relevant predi-cates for the noun in the adjective-noun pair andcompose natural sentences based on them.
to an-.
7119noun-amodextraction:clean up raw text,and use syntacticparsing to extractnoun-adjectivalmodiﬁer pairs..noun filtering:create entries correspondingto unique nouns andadjectival modiﬁers,keeping only items thatcorrespond to valid andappropriate english words..>170 million pairs.
>50k entries.
predicateextraction + filtering:using conceptnet, weextract predicates for nounsunder predeﬁned relations,creating {noun, adjective-list, predicate-list} triplesas entries and ﬁltering outentries with explicit orungrammatical predicates..sentence construction:after applying moreﬁltering on the surface textsand predicates (to removeexplicit or ungrammaticalpredicates), we generatenatural sentences and fourcorresponding variantsby injecting the extractedadjectival modiﬁers..>7k entries.
>20k entries.
labelgeneration +quality control:human annotatorslabel the data,using annotator-quality controltests as exclusioncriteria fordataset instances..>15k labelledinstances.
figure 1: the overview of the data collection process for adept..notate the data, we provide human annotators withlabelling instructions, while implementing qual-ity control measures as exclusion criteria for ﬁnaldataset instances..4.1 data collection.
we now detail the steps of our data collection pro-cess (see figure 1 for an overview).
tables 2 and 3provide examples of how each step contributes tothe creation of an adept instance..noun-amod extraction:in order to extract ad-jectival modiﬁer and noun pairs, we use twodependency-parsed corpora: english wikipedia,which we parse using the stanza pipeline (qi et al.,2020), and a subset of depcc (panchenko et al.,2018), an automatic parse of the common crawlcorpus.
after a preliminary examination of themodiﬁer-noun pairs’ quality, we kept only thosepairs that occur at least 10 times in their respectivecorpus.
this ﬁltered out many pairs that appearedanomalous or atypical (e.g., unwieldy potato).
weextracted 10 million pairs from english wikipediaand 70 million pairs from common crawl..noun ﬁltering: using these pairs, we cre-ated dictionary items consisting of nouns—thatco-occur with at least four different adjectivalmodiﬁers—along with their adjectival modiﬁers.
this threshhold (as opposed to a higher one) allowsus to both still ﬁnd rare non-subsective adjectivesto oversample at later steps, and avoid excessivelyreducing the number of extracted pairs.2 we thenﬁlter out adjectives and nouns that e.g., are explicit,have offensive connotations using preset lists andautomatic moderation tools (e.g., profanity-ﬁlter(roman inﬂianskas, 2020)).
finally, we ensuredthat both the nouns and adjectives are valid en-.
2preliminary analyses showed that non-subsective adjec-.
tives represent less than 5% of our entries..glish words.
this yielded slightly over 50 thousandnoun-adjective dictionary items..predicate extraction: for the noun in each dic-tionary item, we use conceptnet 5 (liu and singh,2004) to ﬁnd predicates under the relationships ofiscapableof, hasproperty, receivesaction, hasa,and usedfor.
we restricted the predicates to theseas they best characterize the functional featuresof a noun, which earlier studies found to be mostsensitive to change according to the attaching mod-iﬁer (del pinal, 2015).
we also store the surfacetext—the sentence the conceptnet annotator wroteto examplify a use of the predicate with the noun(e.g., for the noun “book,” under the conceptnetrelation iscapableof, the predicate include a tableof contents is found with the surface text: a bookcan include a table of contents)..predicate filtering + scaling: after applyingadditional ﬁltering to the surface texts and predi-cates (to remove explicit or ungrammatical pred-icates), we create triples containing a noun, a setof adjectival modiﬁers, and the predicates.
thisyielded over 7,000 triples.
given that an entry maycontain more than one retrieved predicate for itsnoun, we scaled the dictionary to allow for dupli-cate nouns with different predicates (up to threepredicates).3 this yielded over 20,000 entries..sentence construction: for each oftheseadjective-noun-predicate entries, we generate natu-ral sentences and four corresponding variants.
theoriginal sentence (s in section 3) is composed onlyfrom the noun and the predicate, while the fourvariants (s(cid:48) in section 3) are modiﬁed versions ofthe original sentence created by adding the adjec-tive before the root noun in the original sentence.
3threshold selected to correspond to the average numberof different predicates extracted for each noun, and avoid scalethe dictionary excessively at the cost of dataset diversity..7120noun-amod extraction:.
predicate extraction:.
sentence construction:.
amod: thirdnoun: candidatecount: 222.amod: victoriousnoun: candidatecount: 181noun: candidatepredicate: win an election (relation: iscapableof)surface text: [[a candidate]] can [[win an election]].
amod: qualiﬁednoun: candidatecount: 250.original sentence: a candidate wins an election.
variant 1: a [victorious] candidate wins an election.
variant 2: a [third] candidate wins an election.
variant 3: a [qualiﬁed] candidate wins an election.
variant 4: a [false] candidate wins an election..amod: falsenoun: candidatecount: 130.label generation:.
original sentence: a candidate wins an election.
modiﬁed sentence: a [victorious] candidate wins an election..label: necessarily true.
table 2: examples of how adept instances are created through the pipeline..prompt:.
prompt:.
prompt:.
compared with the original statement (”a clock is working correctly.”) please assess theplausibility of the following modiﬁed version: ”a broken clock is working correctly.”.
plausibility change:.
(cid:51) impossible.
(cid:55) less likely.
(cid:55) equally likely.
(cid:55) more likely.
(cid:55) necessarily true.
compared with the original statement (”a candidate wins the election.”) please assess theplausibility of the following modiﬁed version: ”a third candidate wins the election.”.
plausibility change:.
(cid:55) impossible.
(cid:55) less likely.
(cid:51) equally likely.
(cid:55) more likely.
(cid:55) necessarily true.
compared with the original statement (”a mistake angers a person.”) please assess theplausibility of the following modiﬁed version: ”a fatal mistake angers a person.”.
plausibility change:.
(cid:55) impossible.
(cid:55) less likely.
(cid:55) equally likely.
(cid:51) more likely.
(cid:55) necessarily true.
table 3: examples of how adept instances are labelled in the crowdsourcing interface..(see table 2 for examples).
to create the naturalsentences themselves, we modify the surface textby replacing modal verbs (like can or may) withthe declarative is, as modal verbs may complicatethe evaluation of what the plausibility of describedevents might be..adjective sampling: to identify non-subsectiveadjectives in the dataset entries, we use a set of 60non-subsective adjectives identiﬁed by nayak et al.
(2014).
then, to select four adjectives we ﬁrst 1)randomly select up to two non-subsective modi-ﬁers if they co-occured with the noun, and then 2)we randomly select the remaining adjectives fromthe list of subsective modiﬁers.
we over-samplenon-subsective modiﬁers as they occur sparsely inthe corpora and we want to evaluate their effectsagainst other modiﬁers.
this random samplingstrategy results in an about 1:4 non-subsective tosubsective adjective ratio (as some entries have nonon-subsective adjectives), allowing us to analyzethe effect of non-subsective modiﬁers while main-taining an element of randomness..label generation + quality control: for eachentry, annotators (from mechanical turk) label one.
randomly selected sentence variant (from the fourvariants) against its original sentence, with labelsindicating the change in plausibility due to addingthe selected adjective (table 3).
for quality control,we also add roughly 2,000 quality-check entries—including gold label instances for which there wasunanimous agreement among four annotators inearlier pilots and “attention-check” instances thatexplicitly ask annotators to select a speciﬁc label.
we ﬁlter out all instances annotated by annotatorswho failed the attention checks or whose labels dif-fered by at least two degrees from the gold labels(e.g., selected equally likely when the gold labelwas impossible) on more than 10% of their anno-tations.
we also limit the maximum number oflabelling tasks per annotator to 100 (correspondingto less than 0.5% of the data) to ensure that no onejudge signiﬁcantly affects the quality of the data.
finally, we only keep those instances for whichwe observe a majority agreement (i.e., at least twoannotators agree about the ﬁnal label).
after thisﬁnal quality-control ﬁltering steps, the ﬁnal datasetincludes 16,115 instances..7121agreement.
impossible.
less likely.
eq.
likely.
more likely.
nec.
true.
unanimous (5267)majority (10848)no agreement (3209).
label distribution.
dataset split (16115).
0.210.79–.
0.14.
0.160.84–.
0.12.
0.400.60–.
0.67.
0.150.85–.
0.07.
0.090.91–.
0.01.train set: 12892.val set: 1611.test set: 1612.table 4: dataset statistics in terms of agreement, label, and size distributions.
the stats for unanimous & majorityrepresent their prevalence among the pairs for which we observed a majority agreement (and sum up to 1)..4.2 dataset quality assessment.
table 4 overviews the dataset ﬁgures, highlightingthe labels’ distribution and agreement.
by inspect-ing how often judges agree across our plausibilitylabels, we observe higher assessment variabilityfor instances with labels further from equally likely(also the most commonly applied label).
this isparticularly true for instances with labels at theextremes of our plausibility scale (i.e., the impos-sible and necessarily true labels).
while 40% ofthe dataset instances marked as equally likely haveunanimous annotator agreement, this is the case foronly 21% of the instances marked as impossible.
we found no agreement across the 5 plausibility la-bels (§3) for about 15% of the annotated instances,which we do not include in the ﬁnal dataset..while how much judges agree on labels variesacross plausibility levels, the directionality of theassigned labels is more stable—i.e., many disagree-ments are due to judges making different but con-sistent assessments like more likely and necessarilytrue, rather than conﬂicting assessments like lessand more likely.
because of this, we also exper-iment with alternative 3-class and 4-class taskformulations (§5.3), where the impossible and nec-essarily true labels are either combined with otherlabels or are discarded..task ambiguity closely inspecting instancesmarked as impossible and necessarily true to un-derstand possible sources of disagreement amongjudges, we ﬁnd that only about a quarter (for im-possible) to a third (for necessarily true) of theseinstances appear to be clear cases where both 1)adding the adjective led to a change in plausibilityand 2) the change in plausibility made the eventimpossible or necessarily true..sometimes the described events are already im-possible or necessarily true (e.g., average in “an[average] week is made up of seven days” doesnot change the plausibility of this statement, which.
was already necessarily true).
in other cases, theadded modiﬁer changes the semantic interpretationof the event (e.g., the modiﬁer algebraic makesthe event “an [algebraic] operator pages a doctor”impossible because it alters the sense of the termoperator), or it introduces grammatical or logicalerrors (e.g., “[former] sleeping is for maintainingsanity” was likely marked as impossible for beingillogical).
there are also clear cases of false posi-tives, where the resulting events are not impossibleor necessarily true (e.g., “[romantic] jasmine buysher dress at the store” is not impossible)..these issues were particularly prevalent amonginstances annotated as impossible, where about halfof the instances appear to be false positives, un-grammatical, or nonsensical sentences.
we there-fore also experiment with a 4-class formulationthat does not include the impossible label (§5.3)..task reliability given the subjective and am-biguous nature of our task, we also sought to char-acterize to what extent the overall reliability ofour labels might be affected by it.
for this, twoauthors independently labelled 100 randomly sam-pled instances from adept, using the same anno-tation speciﬁcations provided to the crowdsourc-ing judges.
we then measured the inter-assessoragreement between the two authors cohen’s kappaκ = 0.82, which indicates substantial agreement.
we then take the instances where both authorsagreed (87%) and compare their labels with thoseprovided by the crowd-workers, obtaining a κ =0.74 that while lower is still substantial.
finally, theindividual agreement of each of the authors’ labelswith crowdsourcing judges (which includes caseswhere authors disagree) corresponded to κ = 0.77and κ = 0.64, further demonstrating the overallreliability of the labels we collected..7122model.
majority predictionnormative rulehumanhuman (no context).
bert (no context)roberta (no context)deberta (no context).
bertrobertadeberta.
3-class dev.
accuracy.
5-class dev.
accuracy.
66.470.190.0475.04.
72.072.472.1.
72.373.173.9.
66.463.685.0471.04.
69.469.168.6.
69.870.869.7.table 5: performance of various models on the adeptdevelopment set..5 methods.
5.1 neural models.
we evaluate several transformer-based models onadept.
for ﬁne-tuning, we adopt the standardpractice for sentence-pair tasks described by devlinet al.
(2015).
we concatenate the ﬁrst and secondsentence with [sep], prepend the sequence with[cls], and feed the input to the transformer model.
the representation for [cls] is fed into a softmaxlayer for a ﬁve-way classiﬁcation..bert (devlin et al., 2015) is one of the ﬁrsttransformer-based architectures, featuring a pre-trained neural language model with bidirectionalpaths and sentence representations in consecutivehidden layers..roberta (liu et al., 2019) is an improved vari-ant of bert that adds more training data withlarger batch sizes and longer training, as well asother reﬁnements like dynamic masking.
robertaperforms consistently better than bert acrossmany benchmarks (wang et al., 2018a)..deberta builds on roberta with disentan-gled attention and enhanced mask decoder trainingwith half the data used in roberta; currently thebest-performing transformer-based model on sev-eral nli-related tasks (he et al., 2020)..5.2 baseline models.
majority prediction this heuristic always pre-dicts the equally likely label, which represents 67%of the full dataset..normative rule this heuristic corresponds tothe normative treatment of non-subsective modi-ﬁers according to the taxonomy described in sec-.
tion 2, where the general expectation is that the in-sertion of a non-subsective adjective would reducethe plausibility of the modiﬁed sentence.
thus,when the inserted adjective in s(cid:48) is among the listof non-subsective modiﬁers, this baseline predictsless likely, otherwise it predicts the majority label,which is equally likely..no context baseline we run a word associa-tion baseline to evaluate to what extent contextis needed to solve the dataset.
in this baseline, thetransformer model is provided only the noun froms(cid:48) as the representation for the original sentence,and the modiﬁer a as the representation for the themodiﬁed sentence s separated by [sep] (e.g., forsentence 1a in the introduction, this corresponds tothe input: monkey [sep] dead).
this is analogousto the hypothesis-only baseline in nli (belinkovet al., 2019), where the task does not require thefull context to achieve high performance..human evaluation to estimate human perfor-mance on our task, a new annotator (not an author)independently assessed a random sample of 100validation instances from adept.
the annotatorthen evaluated each sentence using the same in-structions provided to the crowdsourcing judges,whose majority agreement determined the ﬁnal la-bel.
the human performance thus corresponds tothe percentage of instances for which the new anno-tator’s labels agree with the adept labels.
we alsoestimate human performance under a no contextsetting, where we presented this same annotator(who was now well-acquainted with the task) witha new random sample with only the noun and themodiﬁer.
the annotator then made their best guessas to what the plausibility difference was withoutknowing the context.
we ensured the new instanceswere distinct from those in the ﬁrst random sample..5.3 experiments.
we primarily test the baselines and transformermodels using two metrics.
the ﬁrst metric corre-sponds to the prediction accuracy on the full ﬁve-label classiﬁcation task (5-class accuracy).
asan alternative metric—drawing from our observa-tions in section 4.2—we use the accuracy on athree-label classiﬁcation task (3-class accuracy),where we bundle impossible and less likely into asingle label representing a decrease in plausibility,and necessarily true and more likely into a labelrepresenting an increase in plausibility..7123figure 2: confusion matrix for the best model on the5-class setting (roberta), adept development set..figure 3: confusion matrix for the best model on the3-class setting (deberta), adept development set..5.4 training.
all models are implemented and trained using hug-gingface’s transformers library (wolf et al., 2020).
we use grid-search for hyper-parameter tuning:learning rate {1e-5, 3e-5, 5e-5}, number of epochs{3, 4, 5, 8}, batch-size {8, 16, 32} with three dif-ferent random seeds.
for ﬁne-tuning, we allow forthe ﬁne-tuning of all parameters including those inthe model’s hidden layers.3.
6 results.
easy for humans, difﬁcult for transformers:model prediction accuracy is summarized in ta-ble 5, where the general trend is as follows: thetransformer-based models have a higher predictionaccuracy than the majority prediction and norma-tive rule baselines, but still fall short of humanperformance by a large margin..of the transformer models, the highest 3-classaccuracy is achieved by deberta and the highest5-class accuracy by roberta; however, the differ-ence in accuracy of all transformer models is small(and not statistically signiﬁcant p-value > 0.05),.
3we also evaluated models where we froze the parametersof all the hidden layers as a probing mechanism, but foundthat no model performed better than the majority baseline.
4this is an estimate based on a subsample of the data..figure 4: label distribution of adept according to tax-onomic class of modiﬁer..being within a 1.6% range..in the no-context ablations where models onlysee the noun phrase and modiﬁer, the transformermodels performance decreases only slightly, whichsuggests the models might be “insensitive” to con-text.
in contrast, approximated human performancedecreases signiﬁcantly in the no-context setting,dropping e.g., from 90% to 75% accuracy for 3-class predictions.
this no-context human accuracy,however, is still superior to the best performingtransformer model with context..to understand what errors the models make, weexamine the confusion matrices for the best per-forming models on both the 3-class (figure 2) and5-class formulations (figure 3).
the most commonerrors appear to happen when a change in plausibil-ity is erroneously classiﬁed as equally likely, andwhen a modiﬁer that does not change an event’splausibility is erroneously predicted to render thenew sentence as less likely.
table 6 includes exam-ple sentences along with 5-class predictions by thebest performing transformer model..the taxonomic classes just don’t cut it: fig-ure 4 shows the distribution of plausibility labelsin adept for both subsective and non-subsectivemodiﬁers.
we see that both classes of modiﬁerslead to a wide mix of changes in the plausibility ofgiven events, corroborating pavlick and callison-burch (2016)’s ﬁndings that normative rules cannotcategorically describe a modiﬁer’s behavior.
thislikely also explains the poor performance of thenormative rule baseline on both the 5- or 3-classplausibility classiﬁcation task formulations..ambiguity makes operationalizing plausibilitydifﬁcult: some of our plausibility labels prove.
7124impossibleless likelyequally likelymore likelynecessarily truepredicted labelimpossibleless likelyequally likelymore likelynecessarily truetrue label825.09%482.98%1056.52%10.06%00.00%271.68%623.85%955.90%20.12%00.00%311.92%402.48%96059.59%392.42%00.00%10.06%50.31%603.72%362.23%00.00%20.12%10.06%120.74%20.12%00.00%0200400600800less likelyequally likelymore likelypredicted labelless likelyequally likelymore likelytrue label24615.27%17310.74%30.19%1207.45%91856.98%321.99%60.37%865.34%271.68%200400600800impossibleless likelyequally likelymore likelynecessarily true0%20%40%60%80%100%percent of classnon-subsective modifierssubsective modifiersadept instance.
annotated change.
roberta prediction.
correct?.
a [professional] mathematician proves a theorem.
[questionable] evidence proves innocence.
a [western] kitchen is for storing food.
[yellow] bananas are yellow.
you use a [strong] jack to lift your car.
a [dead] leg has one foot..more likelyless likelyequally likelynecessarily truemore likelyequally likely.
more likelyless likelyequally likelyequally likelyequally likelyimpossible.
(cid:51)(cid:51)(cid:51)(cid:55)(cid:55)(cid:55).
table 6: representative examples from the development set and corresponding model predictions..ambiguous and harder to reliably assign to ourdataset instances, particularly at the extremes ofour plausibility scale (§4.2).
given that for theimpossible label many instances did not appearto correctly capture changes in plausibility thatrender the modiﬁed event impossible, we conductexploratory experiments with a 4-class task formu-lation that excludes the impossible class.
for thebest performing model (roberta), we observe anoverall improved accuracy from 70.8% to 81.2%(compared to the 5-class classiﬁcation task)..better plausibility classiﬁcation schemes andcrowdsourcing protocols might help us more ef-fectively operationalize plausibility changes.
how-ever, how to effectively separate between 1) caseswhere the modiﬁers alter the semantic interpreta-tion of a statement (and thus lead to a differentevent) or make the sentences ungrammatical versus2) cases where modiﬁers actually lead to changesin the plausibility of the original event, remains anopen question..7 conclusions.
we present a new large-scale corpus and task,adept, for assessing semantic plausibility.
ourcorpus contains over 16 thousand difﬁcult taskinstances, speciﬁcally constructed to test a sys-tem’s ability to correctly interpret and reason aboutadjective-noun pairs within a given context.
ourexperiments suggest a persistent performance gapbetween human annotators and large language rep-resentation models, with the later exhibiting a lowersensitivity to context.
finally, our task providesdeeper insight into the effects of various classes ofadjectives on event plausibility, and suggests thatrules based solely on the adjective or its denotationdo not sufﬁce in determining the correct plausibilityreadings of events..in the future, we wish to investigate how adeptcould be used to improve performance on relatednatural language inference tasks (e.g.
mnli, snli& scitail (khot et al., 2017)).
we also plan to.
develop new models on adept and transfer themto other semantic plausibility tasks..acknowledgements.
this work was supported by the natural sciencesand engineering research council of canada andby microsoft research.
jackie chi kit cheung issupported by the canada cifar ai chair program,and is also a consulting researcher for microsoftresearch..ethical considerations.
while our focus on examining what effects adjec-tives have on the plausibility of arbitrary eventsmakes ascertaining the broader impact of our workchallenging, this work is not void of possible ad-verse social impacts or unintended consequences.
first, to generate our dataset of events, we useenglish wikipedia, common crawl, and concept-net5 (based on data from e.g., games with a pur-pose or dbpedia).
such data sources are how-ever known to exhibit a range of biases (olteanuet al., 2019; baeza-yates, 2018)—which lms re-produce (solaiman et al., 2019)—being often un-clear what and whose content they represent.
whileour goal is to enable others to explore the effects ofmodiﬁers and how these effect might impact var-ious inference tasks, users of this dataset shouldacknowledge possible biases and should not use itto make deployment decisions or rule out failures.
to this end, our dataset release will be accompaniedby a datasheet (gebru et al., 2018)..depending on the context, determining changesin plausibility can also be ambiguous or even sub-jective (see §4.2).
this means that in some down-stream applications, possible plausibility inferenceerrors might, for instance, inadvertently elevate fac-tually incorrect, subjective or misleading beliefs.
if those inference errors happen more when eventsconcern certain groups or activities, they mighthave disparate effects across stakeholders.
thus,.
7125understanding the potential impact of our plausibil-ity inference task requires us to think about bothdownstream applications and possible stakehold-ers (boyarskaya et al., 2020).
for instance, oneapplication of plausibility inferences is perhaps ve-racity or credibility assessment.
it would be prob-lematic if a system would reproduce highly harmfulstereotypes by inferring that a black witness is lesslikely to be trustworthy than just a witness, or thatan old applicant is less likely to be a productive em-ployee than just an applicant.
another application(we also used as a motivating example) is infor-mation extraction where perhaps such plausibilityinferences could be used to infer which details tokeep during extraction.
errors might for instanceharmfully reinforce the belief that the prototypicalhuman is male (menegatti and rubini, 2017), iffemale is deemed as more likely to change the plau-sibility of events about e.g., doctors, scientists, orother professionals; and thus deemed a relevant (ornot) detail to surface based on it..references.
marilisa amoia and claire gardent.
2006. adjectivein proceedings of the workshopbased inference.
kraq’06: knowledge and reasoning for languageprocessing..marilisa amoia and claire gardent.
2007. a ﬁrst orderin pro-semantic approach to adjectival inference.
ceedings of the acl-pascal workshop on textualentailment and paraphrasing, pages 185–192..gabor angeli, melvin jose johnson premkumar, andchristopher d. manning.
2015. leveraging linguis-tic structure for open domain information extraction.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages344–354, beijing, china.
association for computa-tional linguistics..ricardo baeza-yates.
2018. bias on the web.
commu-.
nications of the acm, 61(6):54–61..yonatan belinkov, adam poliak, stuart shieber, ben-jamin van durme, and alexander rush.
2019. onadversarial removal of hypothesis-only bias in natu-ral language inference.
in proceedings of the eighthjoint conference on lexical and computational se-mantics (*sem 2019), pages 256–262, minneapolis,minnesota.
association for computational linguis-tics..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference..in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..margarita boyarskaya, alexandra olteanu, and katecrawford.
2020. overcoming failures of imagina-tion in ai infused system development and deploy-ment.
in navigating the broader impacts of ai re-search workshop at neurips 2020..romane clark.
1970. concerning the logic of predi-.
cate modiﬁers.
nous, pages 311–335..ido dagan, oren glickman, alﬁo gliozzo, efrat mar-morshtein, and carlo strapparava.
2006. directword sense matching for lexical substitution.
inproceedings of the 21st international conference oncomputational linguistics and 44th annual meet-ing of the association for computational linguistics,pages 449–456, sydney, australia.
association forcomputational linguistics..guillermo del pinal.
2015. dual content semantics,privative adjectives, and dynamic compositionality.
semantics and pragmatics, 8:7–1..jacob devlin, hao cheng, hao fang, saurabh gupta,li deng, xiaodong he, geoffrey zweig, and mar-garet mitchell.
2015. language models for imagecaptioning: the quirks and what works.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 2: short papers), pages 100–105,beijing, china.
association for computational lin-guistics..timnit gebru, jamie morgenstern, briana vecchione,jennifer wortman vaughan, hanna wallach, haldaum´e iii, and kate crawford.
2018. datasheetsfor datasets.
arxiv preprint arxiv:1803.09010..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2020. deberta: decoding-enhancedarxiv preprintbert with disentangled attention.
arxiv:2006.03654..wenguan huang and xudong luo.
2017. common-sense reasoning in a deeper way: by discovering re-lations between predicates.
in icaart..hans kamp and barbara partee.
1995. prototype the-ory and compositionality.
cognition, 57(2):129–191..j. a. w. kamp and edward l. keenan.
1975. twotheories about adjectives, page 123–155.
cambridgeuniversity press..frank keller and mirella lapata.
2003. using the webto obtain frequencies for unseen bigrams.
computa-tional linguistics, 29(3):459–484..7126tushar khot, ashish sabharwal, and peter clark.
2017.answering complex questions using open informa-tion extraction.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 311–316,vancouver, canada.
association for computationallinguistics..maria lapata, scott mcdonald, and frank keller.
1999.determinants of adjective-noun plausibility.
inninth conference of the european chapter of theassociation for computational linguistics, bergen,norway.
association for computational linguistics..hugo liu and push singh.
2004. conceptnet—a practi-cal commonsense reasoning tool-kit.
bt technologyjournal, 22(4):211–226..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..john philip mccrae, francesca quattri, christinaunger, and philipp cimiano.
2014. modelling thesemantics of adjectives in the ontology-lexicon in-terface.
in proceedings of the 4th workshop on cog-nitive aspects of the lexicon (cogalex), pages 198–209..louise mcnally and gemma boleda.
2004. relationaladjectives as properties of kinds.
empirical, page179..michela menegatti and monica rubini.
2017. genderin oxford research.
bias and sexism in language.
encyclopedia of communication..richard montague.
1970. english as a formal lan-guage.
in bruno visentini, editor, linguaggi nellasocieta e nella tecnica, pages 188–221.
edizioni dicommunita..james mullenbach, jonathan gordon, nanyun peng,and jonathan may.
2019. do nuclear submarineshave nuclear captains?
a challenge dataset for com-monsense reasoning over adjectives and objects.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6052–6058, hong kong, china.
association for computa-tional linguistics..neha nayak, mark kowarsky, gabor angeli, andchristopher d manning.
2014. a dictionary of non-subsective adjectives.
technical report, technicalreport cstr 2014-04, department of computerscience, stanford..alexandra olteanu, carlos castillo, fernando diaz,social data: bi-and emre kiciman.
2019.ases, methodological pitfalls, and ethical boundaries.
frontiers in big data, 2:13..alexander panchenko, eugen ruppert, stefano far-alli, simone p. ponzetto, and chris biemann.
2018. building a web-scale dependency-parsedin proceedings ofcorpus from commoncrawl.
the eleventh international conference on languageresources and evaluation (lrec-2018), miyazaki,japan.
european languages resources association(elra)..terence parsons.
1970..some problems concern-ing the logic of grammatical modiﬁers.
synthese,21(3):320–334..ellie pavlick and chris callison-burch.
2016..so-called non-subsective adjectives.
in proceedings ofthe fifth joint conference on lexical and computa-tional semantics, pages 114–119..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..roman inﬂianskas.
2020. profanity-ﬁlter..shota sasaki, sho takase, naoya inoue, naoakiokazaki, and kentaro inui.
2017. handling multi-word expressions in causality estimation.
in iwcs2017—12th international conference on computa-tional semantics—short papers..irene solaiman, miles brundage, jack clark, amandaaskell, ariel herbert-voss, jeff wu, alec rad-ford, gretchen krueger, jong wook kim, sarahkreps, et al.
2019. release strategies and the so-cial impacts of language models.
arxiv preprintarxiv:1908.09203..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018a.
glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..su wang, greg durrett, and katrin erk.
2018b.
mod-eling semantic plausibility by injecting world knowl-edge.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 2 (short papers), pages 303–308, neworleans, louisiana.
association for computationallinguistics..7127adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..hongming zhang, hantian ding, and yangqiu song.
2019. sp-10k: a large-scale evaluation set for selec-tional preference acquisition.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 722–731, florence, italy.
association for computational linguistics..sheng zhang, rachel rudinger, kevin duh, and ben-jamin van durme.
2017. ordinal common-sense in-ference.
transactions of the association for compu-tational linguistics, 5:379–395..7128