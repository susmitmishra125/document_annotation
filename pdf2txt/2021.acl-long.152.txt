contributions of transformer attention headsin multi- and cross-lingual tasksweicheng ma1*, kai zhang2*†, renze lou3†, lili wang1, and soroush vosoughi4.
1,4department of computer science, dartmouth college2department of computer science and technology, tsinghua university3department of computer science, zhejiang university city college1{first.last}.gr@dartmouth.edu2drogozhang@gmail.com3marionojump0722@gmail.com4soroush.vosoughi@dartmouth.edu.
abstract.
this paper studies the relative importance ofattention heads in transformer-based modelsto aid their interpretability in cross-lingual andmulti-lingual tasks.
prior research has foundthat only a few attention heads are importantin each mono-lingual natural language pro-cessing (nlp) task and pruning the remainingheads leads to comparable or improved per-formance of the model.
however, the impactof pruning attention heads is not yet clear incross-lingual and multi-lingual tasks.
throughextensive experiments, we show that (1) prun-ing a number of attention heads in a multi-lingual transformer-based model has, in gen-eral, positive effects on its performance incross-lingual and multi-lingual tasks and (2)the attention heads to be pruned can be rankedusing gradients and identiﬁed with a few trialexperiments.
our experiments focus on se-quence labeling tasks, with potential applica-bility on other cross-lingual and multi-lingualtasks.
for comprehensiveness, we examinetwo pre-trained multi-lingual models, namelymulti-lingual bert (mbert) and xlm-r,on three tasks across 9 languages each.
wealso discuss the validity of our ﬁndings andtheir extensibility to truly resource-scarce lan-guages and other task settings..1.introduction.
prior research on mono-lingual transformer-based(vaswani et al., 2017) models reveals that a subsetof their attention heads makes key contributionsto each task, and the models perform comparablywell (voita et al., 2019; michel et al., 2019) or evenbetter (kovaleva et al., 2019) with the remainingheads pruned 1. while multi-lingual transformer-.
∗equal contribution.
†work done when interning at the minds, machines, and.
society lab at dartmouth college..1we regard single-source machine translation as a mono-lingual task since the inputs to the models are mono-lingual..based models, e.g.
mbert (devlin et al., 2019)and xlm-r (conneau et al., 2020), are widely ap-plied in cross-lingual and multi-lingual nlp tasks2 (wang et al., 2019; keung et al., 2019; eskanderet al., 2020), no attempt has been made to extendthe ﬁndings on the aforementioned mono-lingualresearch to this context.
in this paper, we explorethe roles of attention heads in cross-lingual andmulti-lingual tasks for two reasons.
first, betterunderstanding and interpretability of transformer-based models leads to efﬁcient model designs andparameter tuning.
second, head-pruning makestransformer-based models more applicable to trulyresource-scarce languages if it does not negativelyaffect model performance signiﬁcantly..the biggest challenge we face when studyingthe roles of attention heads in cross-lingual andmulti-lingual tasks is locating the heads to prune.
existing research has shown that each attentionhead is specialized to extract a collection of linguis-tic features, e.g., the middle layers of bert mainlyextract syntactic features (vig and belinkov, 2019;hewitt and manning, 2019) and the fourth headon the ﬁfth layer of bert greatly contributes tothe coreference resolution task (clark et al., 2019).
thus, we hypothesize that important feature extrac-tors for a task should be shared across languagesand the remaining heads can be pruned.
we eval-uate two approaches used to rank attention heads,the ﬁrst of which is layer-wise relevance propaga-tion (lrp, ding et al.
(2017)).
voita et al.
(2019)interpreted the adaptation of lrp in transformer-based models on machine translation.
motivated byfeng et al.
(2018) and serrano and smith (2019),we design a second ranking method based on gra-dients since the gradients on each attention head.
2we deﬁne a cross-lingual task as a task whose test set is ina different language from its training set.
a multi-lingual taskis a task whose training set is multi-lingual and the languagesof its test set belong to the languages of the training set..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1956–1966august1–6,2021.©2021associationforcomputationallinguistics1956reﬂect its contribution to the predictions..we study the effects of pruning attention headson three sequence labeling tasks, namely part-of-speech tagging (pos), named entity recognition(ner), and slot ﬁlling (sf).
we focus on sequencelabeling tasks since they are more difﬁcult to an-notate than document- or sentence-level classiﬁca-tion datasets and require more treatment in cross-lingual and multi-lingual research.
we choose posand ner datasets in 9 languages, where english(en), chinese (zh), and arabic (ar) are candidatesource languages.
the multiatis++ corpus (xuet al., 2020) is used in the sf evaluations with enas the source language.
we do not include syntacticchunking and semantic role labeling tasks due tolack of availability of manually written and anno-tated corpora.
in these experiments, we rank at-tention heads based only on the source language(s)to ensure the extensibility of the learned knowl-edge to cross-lingual tasks and resource-poor lan-guages.
in our preliminary experiments comparingthe gradient-based method and lrp, the average f1score improvements on ner with mbert are 0.69(cross-lingual) and 0.24 (multi-lingual) for lrpand 0.81 (cross-lingual) and 0.31 (multi-lingual)for the gradient-based method, though both meth-ods rank attention heads similarly.
thus we choosethe gradient-based method to rank attention headsin all our experiments..our evaluations conﬁrm that only a subset ofattention heads in each transformer-based modelmakes key contributions to each cross-lingual ormulti-lingual task and that these heads are sharedacross languages.
performance of models gener-ally drop when the highest-ranked or randomlyselected heads are pruned, validating the head rank-ings generated by our gradient-based method.
wealso observe performance improvements on taskswith multiple source languages by pruning atten-tion heads.
our ﬁndings potentially apply to trulyresource-scarce languages since we show that themodels perform better with attention heads prunedwhen fewer training instances are available in thetarget languages..the contributions of this paper are three-fold:• we explore the roles of attention heads in multi-lingual transformer-based models and ﬁnd thatpruning certain heads leads to comparable orbetter performance in cross-lingual and multi-lingual sequence labeling tasks..• we adapt a gradient-based method to locate atten-.
lc language family.
en ie, germanicde ie, germanicnl ie, germanicar afro-asiatic, semitiche afro-asiatic, semiticzh sino-tibetanjapanesejaur ie, indicfa ie, iranian.
training sizenerpos14,98712,54312,70513,81415,80612,2641,3296,0755,2412,78520,9053,9978007,027289,7414,04318,4634,798.table 1: details of pos and ner datasets in our ex-periments.
lc refers to language code.
training sizedenotes the number of training instances..tion heads that can be pruned without exhaustiveexperiments on all possible combinations..• we show the correctness, robustness, and ex-tensibility of the ﬁndings and our head rankingmethod under a wide range of settings throughcomprehensive experiments..2 datasets.
we use human-written and manually annotateddatasets in experiments to avoid noise from ma-chine translation and automatic label projection..we choose pos and ner datasets in 9 lan-guages, namely en, zh, ar, hebrew (he),japanese (ja), persian (fa), german (de), dutch(nl), and urdu (ur).
as table 1 shows, these lan-guages fall in diverse language families and thedatasets are very different in size.
en, zh, and arare used as candidate source languages since theyare resource-rich in many nlp tasks.
our posdatasets are all from universal dependencies (ud)v2.7 3. these datasets are labeled with a commonlabel set containing 17 pos tags..for ner, we use nl, en, and de datasetsfrom conll-2002 and 2003 challenges (tjongkim sang, 2002; tjong kim sang and de meulder,2003).
additionally, we use the people’s dailydataset 4, iob2corpus 5, aqmar (mohit et al.,2012), armanperosnercorpus (poostchi et al.,2016), mk-pucit (kanwal et al., 2020), and anews-based ner dataset (mordecai and elhadad,2012) for the languages cn, ja, ar, fa, ur, and.
3http://universaldependencies.org/4http://github.com/oye93/chinese-nlp-c.orpus/tree/master/ner/people’sdaily.
5http://github.com/hironsan/iob2corpus.
1957he, respectively.
since the ner datasets are in-dividually constructed in each language, their la-bel sets do not fully agree.
as there are four netypes (per, org, loc, misc) in the three source-language datasets, we merge other ne types intothe misc class to allow cross-lingual evaluations.
we evaluate sf models on multiatis++ withen as the source language and spanish (es), por-tuguese (pt), de, french (fr), zh, ja, hindi (hi),and turkish (tr) as target languages.
there are 71slot types in the tr dataset, 75 in the hi dataset,and 84 in the other datasets.
we do not use theintent labels in our evaluations since we study onlysequence labeling tasks.
thus our results are notdirectly comparable with xu et al.
(2020)..3 methodology.
here, we introduce the gradient-based method weuse in the experiments to rank the attention heads.
feng et al.
(2018) claim that gradients measure theimportance of features to predictions.
since eachhead functions similarly as a standalone featureextractor in a transformer-based model, we usegradients to approximate the importance of thefeature set extracted by each head and rank theheads accordingly.
michel et al.
(2019) determineimportance of heads with accumulated gradients ateach head in a training epoch.
different from theirapproach, we ﬁne-tune the model on the trainingset and rank the heads using gradients on thedevelopment set to ensure that the head importancerankings are not signiﬁcantly correlated withthe training instances in one source language.
speciﬁcally, our method generates head rankingsfor each language in three steps:(1) we ﬁne-tune a transformer-based model on amono-lingual task for three epochs.
(2) we re-run the ﬁne-tuned model on the develop-ment partition of the dataset with back-propagationbut not parameter updates to obtain gradients.
(3) we sum up the absolute gradients on eachhead, layer-wise normalize the accumulated gra-dients, and scale them into the range [0, 1] globally..we show spearman’s rank correlation coefﬁ-cients (spearman’s ρ) between head rankings ofeach language pair generated by our method onpos, ner, and sf in figure 1. the highest-ranked heads largely overlap in all three tasks,while the rankings of unimportant heads vary morein mbert than xlm-r..(a) pos-mbert.
(b) pos-xlm-r.(c) ner-mbert.
(d) ner-xlm-r.(e) sf-mbert.
(f) sf-xlm-r.figure 1: spearman’s ρ of head ranking matricesbetween languages in the pos, ner, and sf tasks.
darker colors indicate higher correlations..after ranking the attention heads, we ﬁne-tunethe model, with the lowest-ranked head in thesource language pruned.
we keep increasing thenumber of heads to prune until it reaches a pre-set limit or when the performance starts to drop.
we limit the number of trials to 12 since the mod-els mostly show improved performance within 12attempts 6..4 experiments and analysis.
this section displays and explains experimental re-sults on cross-lingual and multi-lingual pos, ner,and sf tasks.
training sets in target languages arenot used to train the model under the cross-lingualsetting.
our experiments are based on the hugging-face (wolf et al., 2020) implementations of mbert.
6on average 7.52 and 6.58 heads are pruned for pos, 7.54and 7.28 heads for ner, and 6.19 and 6.31 heads for sf,respectively in mbert and xlm-r models..1958enzharfadehejanlurenzharfadehejanlur10.810.770.820.850.820.750.870.790.8110.720.710.810.830.750.820.690.770.7210.850.720.830.730.720.720.820.710.8510.780.810.720.770.760.850.810.720.7810.810.680.860.720.820.830.830.810.8110.780.820.700.750.750.730.720.680.7810.670.690.870.820.720.770.860.820.6710.820.790.690.720.760.720.700.690.821enzharfadehejanlurenzharfadehejanlur10.950.960.970.970.970.950.970.970.9510.950.940.950.970.970.960.950.960.9510.970.960.970.940.960.960.970.940.9710.970.970.950.970.960.970.950.960.9710.970.960.970.960.970.970.970.970.9710.970.970.970.950.970.940.950.960.9710.950.960.970.960.960.970.970.970.9510.960.970.950.960.960.960.970.960.961 ( 1 = + $ 5 ) $ ' ( + ( - $ 1 / 8 5 ( 1 = + $ 5 ) $ ' ( + ( - $ 1 / 8 5                                                                                                                                                                                                                                                                                                          ( 1 = + $ 5 ) $ ' ( + ( - $ 1 / 8 5 ( 1 = + $ 5 ) $ ' ( + ( - $ 1 / 8 5                                                                                                                                                                                                                                                                                                         enzhdehifresjapttrenzhdehifresjapttr10.830.800.780.820.840.830.850.620.8310.790.800.820.780.820.830.620.800.7910.750.810.830.810.850.650.780.800.7510.720.770.810.770.710.820.820.810.7210.800.870.890.670.840.780.830.770.8010.850.840.720.830.820.810.810.870.8510.890.680.850.830.850.770.890.840.8910.670.620.620.650.710.670.720.680.671enzhdehifresjapttrenzhdehifresjapttr10.950.970.940.980.950.950.960.940.9510.940.960.950.920.970.950.950.970.9410.940.980.950.950.970.940.940.960.9410.940.900.950.940.960.980.950.980.9410.960.950.970.930.950.920.950.900.9610.940.950.910.950.970.950.950.950.9410.950.950.960.950.970.940.970.950.9510.950.940.950.940.960.930.910.950.951sl.
tl.
unpruned.
pruned.
unpruned.
pruned.
mbert.
xlm-r.crling mulling crling mulling crling mulling crling mulling95.9996.1397.0995.1997.3097.6297.0293.0797.3296.0797.0995.2297.1997.3497.4393.0197.3196.0497.2095.2997.3497.6597.5092.88.
95.3195.6894.8194.9496.5896.9796.6992.1796.6495.6694.6394.7596.5897.0196.9792.2696.5395.3194.6494.9896.7097.1597.0392.16.
46.1867.0266.5089.7880.3733.6488.0356.0443.3838.1934.6447.4750.4249.7442.5029.2663.6334.7171.5568.2867.7229.2164.8056.06.
59.9956.7158.3489.1378.0145.9587.4854.7857.0541.0345.2964.3657.9444.6961.1044.0756.9047.1464.0257.8561.8844.1860.3149.76.
95.1095.6494.4894.8196.4596.8496.4791.9296.5295.6294.5594.6296.5196.7396.7892.2196.5095.1694.5294.8296.4497.0296.8792.00.
41.1066.7566.6089.4177.4830.9888.0655.4542.3536.7133.4346.5851.2649.1240.7830.0861.7325.1270.9265.2167.4522.1162.9354.79.
95.8796.0796.8594.8197.2697.5297.0492.9497.1995.9997.0795.0697.0697.3297.3092.9097.2195.1697.1595.1697.2397.5296.8792.74.
59.8855.9857.9488.8677.9144.7387.4553.2155.6338.4143.6863.5057.1443.6359.9543.8254.7746.1963.8256.8860.3344.3258.8649.31.zharfadehejanlurenarfadehejanlurenzhfadehejanlur.
en.
zh.
ar.
table 2: f-1 scores of mbert and xlm on pos.
sl and tl refer to source and target languages and crling andmulling stand for cross-lingual and multi-lingual settings, respectively.
unpruned results are produced by the fullmodels and pruned results are the best scores each model produces with up to 12 lowest-ranked heads pruned.
thehigher performance in each pair of pruned and unpruned experiments is in bold..and xlm-r. speciﬁcally, we use the pre-trainedbert-base-multilingual-cased and xlm-roberta-basemodels for their comparable model sizes.
the mod-els are ﬁne-tuned for 3 epochs with a learning rateof 5e-5 in all the experiments.
we use the ofﬁcialdataset splits and load training instances with se-quential data samplers, so the reported evaluationscores are robust to randomness..4.1 pos.
table 2 shows the evaluation scores on pos withthree source language choices.
in the majority (88out of 96 pairs) of experiments, pruning up to 12attention heads improves mbert and xlm-r per-formance.
results are comparable in the other 8experiments with and without head pruning.
aver-age f-1 score improvements are 0.91 for mbertand 1.78 for xlm-r in cross-lingual tasks, and0.15 for mbert and 0.17 for xlm-r in multi-.
lingual tasks.
these results support that pruningheads generally has positive effects on model per-formance in cross-lingual and multi-lingual tasks,and that our method correctly ranks the heads..consistent with conneau et al.
(2020), xlm-r usually outperforms mbert, with exceptionsin cross-lingual experiments where zh and jadatasets are involved.
word segmentation in zhand ja is different from the other languages wechoose, e.g.
words are not separated by whitespaces and unpaired adjacent word pieces oftenmake up a new word.
as xlm-r applies thesentencepiece tokenization method (kudo andrichardson, 2018), it is more likely to detect wrongword boundaries and make improper predictionsthan mbert in cross-lingual experiments involv-ing zh or ja datasets.
we note that the perfor-mance improvements are solid regardless of the.
1959sl.
tl.
unpruned.
pruned.
unpruned.
pruned.
mbert.
xlm-r.crling mulling crling mulling crling mulling crling mulling91.1174.2896.9883.1089.6782.3590.3899.0791.0575.6895.6682.5487.7982.6390.5699.1091.0191.0096.7482.7389.2880.9291.1199.15.
32.3343.7855.7266.4856.8737.8877.6658.6858.5536.1151.5155.5148.9447.0652.2755.9551.0031.0352.6050.0050.8538.8748.8753.51.
93.7173.3296.9779.1988.4984.3485.1799.2287.9972.8696.2378.6789.3583.2085.2899.2887.8293.5496.8778.0487.6483.1785.2499.31.
47.6438.8140.1256.4346.9242.4564.5137.3438.5836.4345.6829.0747.1449.2129.7544.6119.2941.7046.5724.4747.1541.4926.0046.47.
51.6138.9339.8158.2746.5544.1465.5640.6041.4036.9946.5733.8147.6851.6931.4646.3320.0740.4346.8725.6246.7242.1126.3445.66.
29.9741.2154.9063.7156.9633.8777.1558.2556.4034.3151.6056.2248.5246.1849.5948.9851.3325.7853.3550.8749.5236.9849.2748.48.
90.9971.7796.6282.3188.0281.4890.2199.1590.7274.8495.6382.3385.9580.1989.5698.9990.3790.5196.5582.6387.3781.7290.7399.10.
93.2470.5596.7079.1189.1884.9184.9099.2987.6572.2796.2179.0488.2082.0284.6199.2687.8693.4696.8275.7886.7779.9084.8399.26.zharfadehejanlurenarfadehejanlurenzhfadehejanlur.
en.
zh.
ar.
table 3: f-1 scores of mbert and xlm on ner.
sl and tl refer to source and target languages and crling andmulling stand for cross-lingual and multi-lingual settings, respectively.
unpruned results are produced by the fullmodels and pruned results are the best scores each model produces with up to 12 lowest-ranked heads pruned..source language selection and severe differencesof training data sizes in en, zh, and ar.
thisdemonstrates the correctness of the head rankingsour method generates and that the important atten-tion heads for a task are almost language invariant..we also examine to what extent the score im-provements are affected by the relationships be-tween source and target languages, e.g.
languagefamilies, uriel language distance scores (littellet al., 2017), and the similarity of the head rankingmatrices.
there are three non-exclusive clustersof language families (containing more than onelanguage) in our choice of languages, namely indo-european (ie), germanic, and semitic languages.
average score improvements between models withand without head pruning are 0.40 (ie), 0.16 (ger-manic), and 0.91 (semitic) for mbert and 0.19(ie), 0.18 (germanic), and 0.19 (semitic) for xlm-r. in comparison, the overall average score im-.
provements are 0.53 for mbert and 0.97 for xlm-r. despite the generally higher performance ofmodels when the source and target languages arein the same family, the score improvements bypruning heads are not necessarily associated withlanguage families.
additionally, we use spear-man’s ρ to measure the correlations between im-proved f-1 scores and uriel language distances.
the correlation scores are 0.11 (cross-lingual) and0.12 (multi-lingual) for mbert, and -0.40 (cross-lingual) and 0.23 (multi-lingual) for xlm-r. sim-ilarly, the spearman’s ρ between score improve-ments and similarities in head ranking matricesshown in figure 1 are -0.34 (cross-lingual) and0.25 (multi-lingual) for mbert, and -0.52 (cross-lingual) and -0.10 (multi-lingual) for xlm-r. thisindicate that except in the cross-lingual xlm-rmodel which faces word segmentation issues onzh or ja experiments, pruning attention heads.
1960sl.
tl.
unpruned.
pruned.
unpruned.
pruned.
mbert.
xlm-r.crling mulling crling mulling crling mulling crling mulling94.2995.3587.1693.7788.8393.7191.2484.30.
94.2594.9587.0894.1888.1793.7890.8284.31.
67.9883.5066.3977.5981.8836.6877.5452.64.
71.8466.9745.8467.1373.9668.3263.2332.21.
93.9794.8186.7293.5189.1093.6590.7683.20.
94.1194.6085.9393.9687.7193.7390.8383.41.
62.5882.8558.3276.5381.7032.3977.4245.91.
69.8360.6944.2860.4472.2768.2859.3728.11.
95.43.
95.27.
94.59.
94.87.zhdehifresjapttren.
en.
table 4: slot f-1 scores on the multiatis++ corpus.
crling and mulling refer to cross-lingual and multi-lingualsettings, respectively.
sl and tl refer to source and target languages, respectively.
english mono-lingual resultsare reported for validity check purposes..improves model performance regardless of the dis-tances between source and target languages.
thusour ﬁndings are potentially applicable to all cross-lingual and multi-lingual pos tasks..4.2 ner.
as table 3 shows, pruning attention heads gener-ally has positive effects on our cross-lingual andmulti-lingual ner models.
even in the multi-lingual ar-ur experiment where the full mbertmodel achieves an f-1 score of 99.26, the score israised to 99.31 by pruning heads.
scores are com-parable with and without head pruning in the 19cases where model performances are not improved.
this also lends support to the specialized role ofimportant attention heads and the consistency ofhead rankings across languages.
in ner exper-iments, performance drops mostly happen whenthe source and target languages are from differentfamilies.
this is likely caused by the differencebetween named entity (ne) representations acrosslanguage families.
we show in section 5.2 that thegap is largely bridged when a language from thesame family as the target language is added to thesource languages..average score improvements are comparable onmbert (0.81 under cross-lingual and 0.31 undermulti-lingual settings) and xlm-r (1.08 undercross-lingual and 0.67 under multi-lingual settings)in ner experiments.
the results indicate that theperformance improvements introduced by head-pruning are not sensitive to the pre-training corporaof models.
the correlations between f-1 scoreimprovements and uriel language distances aresmall, with spearman’s ρ of -0.05 (cross-lingual).
and -0.27 (multi-lingual) for mbert and 0.10(cross-lingual) and 0.12 (multi-lingual) for xlm-r.similarities between head ranking matrices do notgreatly affect score improvements either, the spear-man’s ρ of which are -0.08 (cross-lingual) and 0.06(multi-lingual) for mbert and 0.05 (cross-lingual)and 0.12 (multi-lingual) for xlm-r. the ﬁndingsin pos and ner experiments are consistent, sup-porting our hypothesis that important heads for atask are shared by arbitrary source-target languageselections..4.3 slot filling.
we report sf evaluation results in table 4. in 31out of 34 pairs of experiments, pruning up to 12heads results in performance improvements, whilethe scores are comparable in the other three cases.
these results agree with those in pos and nerexperiments, showing that only a subset of headsin each model makes key contributions to cross-lingual or multi-lingual tasks..we also evaluate the correlations between scorechanges and the closeness of source and targetlanguages.
in terms of uriel language dis-tances, the spearman’s ρ are 0.69 (cross-lingual)and 0.14 (multi-lingual) for mbert and -0.59(cross-lingual) and 0.14 (multi-lingual) for xlm-r. the coefﬁcients are -0.25 (cross-lingual) and-0.73 (multi-lingual) for mbert and -0.70 (cross-lingual) and -0.14 (multi-lingual) between scoreimprovements and similarities in head ranking ma-trices.
while these coefﬁcients are generally higherthan those in pos and ner evaluations, their p-values are also high (0.55 to 0.74), indicating thecorrelations between the score changes and source-.
1961ner.
max-pruning.
rand-pruning.
crling mulling crling mulling+0.26-0.43-0.38+0.36-0.74-4.21-2.40-0.12.
-2.44-2.09+0.57+0.29-2.52-0.49-2.65-0.60.
-1.74-3.17+0.88-2.76-0.86-2.50-1.48-0.15.
+0.08-2.42-0.62-0.23-0.31-2.15-1.08-0.10pos.
max-mask.
rand-mask.
crling mulling crling mulling-0.20-0.12-0.14-0.16-0.25+0.05-0.05-0.07.
+0.03-0.65-0.64-0.13-0.75-1.27-22.29-1.78.
-0.39-0.04-0.04-0.13-0.03-0.28-0.05-0.11.
-0.14-0.66-0.64-0.11-0.53-1.06-1.23-0.77.tl.
zhardenlfahejaur.
tl.
zhardenlfahejaur.
table 5: f-1 score differences from the full mbertmodel on ner (upper) and pos (lower) by prun-ing highest ranked (max-pruning) or random (rand-pruning) heads in the ranking matrices.
the source lan-guage is en.
blue and red cells indicate score dropsand improvements, respectively..target language closeness are not statistically sig-niﬁcant.
7.
5 discussions.
in this section, we perform case studies to conﬁrmthe validity of our head ranking method.
we also il-lustrate the extensibility of the knowledge we learnfrom the main experiments to a wider range of set-tings, e.g.
when the training dataset is limited insize or constructed over multiple source languages..5.1 correctness of head rankings.
we evaluate the correctness of our head rankingmethod through comparisons between results intables 2 and 3 and those produced by pruning (1)randomly sampled heads and (2) highest rankedheads.
speciﬁcally, we repeat the head-pruningexperiments with mbert on ner and pos using.
7the p-values for all the other spearman’s ρ we reportare lower than 0.01, showing that those correlation scores arestatistically signiﬁcant..en as the source language and display the score dif-ferences from the the full models in table 5. sameas in the main experiments, we pick the best scorefrom pruning 1 to 12 heads in each experiment.
arandom seed of 42 is used for sampling attentionheads to prune under the random sampling setting.
in 14 out of 16 ner experiments, pruning theheads ranked highest by our method results in no-ticeable performance drops compared to the fullmodel.
consistently, pruning the highest-rankedattention heads harms the performance of mbertin 15 out of 16 pos experiments.
though scorechanges are slightly positive for cross-lingual en-de and multi-lingual en-zh ner tasks and in thecross-lingual en-zh pos experiment, improve-ments introduced by pruning lowest-ranked headsare more signiﬁcant, as table 2 and table 3 show.
pruning random attention heads also has mainlynegative effects on the performance of mbert.
these results indicate that while pruning attentionheads potentially boosts the performance of models,reasonably choosing the heads to prune is impor-tant.
our gradient-based method properly ranks theheads by their priority to prune..5.2 multiple source languages.
training cross-lingual models on multiple sourcelanguages is a practical way to improve their per-formance, due to enlarged training data size andsupervision from source-target languages closer toeach other (wu et al., 2020; moon et al., 2019;chen et al., 2019; rahimi et al., 2019; t¨ackstr¨om,2012).
we also explore the effects of pruning atten-tion heads under the multi-source settings.
in thissection, we experiment with mbert on en, de,ar, he, and zh datasets for both ner and postasks.
these languages fall into three mutually ex-clusive language families, enabling our analysis onthe inﬂuence of training cross-lingual models withsource languages belonging to the same family asthe target language.
similar to related research, themodel is ﬁne-tuned on the concatenation of trainingdatasets in all the languages but the one on whichthe model is tested..since the head ranking matrices are not identicalacross languages, we design three heuristics to rankthe heads in the multi-source experiments.
the ﬁrstmethod merges the head ranking matrices of all thesource languages into one matrix and re-generatesthe rankings.
the second method ranks the at-tention heads after summing up the head ranking.
1962(a) en-de.
(b) en-nl.
(c) en-ar.
(d) en-he.
(e) en-zh.
(f) en-ja.
(g) en-fa.
figure 2: f-1 scores of mbert on multi-lingual ner with 10% - 90% target language training data usage.
dashedblue lines indicate scores without head pruning and solid red lines show scores with head pruning..enfl60.77md 62.63sd 63.38ec 64.63.en81.97flmd 82.99sd 82.62ec 83.49.ner.
pos.
ar35.9040.7841.5340.78.ar74.0774.6574.4175.86.de59.1661.1061.6661.71.de88.8289.1988.7489.20.he51.1955.1554.2056.26.he75.6277.0077.3078.04.zh44.1847.5947.0847.24.zh61.3161.7461.2962.33.table 6: cross-lingual ner (upper) and pos (lower)evaluation results with multiple source languages.
flindicates unpruning.
md, sd, and ec are the threeheuristics we examine..matrices.
we also examine the efﬁcacy of pruningheads based on the head rankings from a singlelanguage.
for this heuristic, we run experimentsusing the head ranking matrix from each languageand report the highest score.
we refer to the threeheuristics as md, sd, and ec, respectively..table 6 displays the results.
we note that in thener evaluations, the performance of mbert onall the languages but zh are higher than those inthe single-source experiments.
this supports ourhypothesis that supervision from languages in the.
same family as the target language helps improvemodel performance.
different from ner, the eval-uation results on pos are not much higher than thesingle-source evaluation scores, implying that syn-tactic features are more consistent across languagesthan appearances of named entities.
however, itis consistent on both tasks that pruning attentionheads brings performance boosts to all the multi-source experiments.
while the ec heuristic pro-vides the largest improvement margin in 3 out of5 experiments, it requires a lot more trial experi-ments.
md and sd perform comparably well inmost cases so they are also promising heuristicsfor ranking attention heads under the multi-sourcesetting.
the results support that pruning attentionheads is beneﬁcial to transformer-based modelsin cross-lingual tasks even if the training dataset isalready large and diverse in languages..5.3 extension to resource-poor languages.
while the languages we use in the main experi-ments are not truly resource-poor, we examine ourﬁndings when training sets in the target languagesare smaller.
we design experiments under the multi-lingual setting with subsampled training datasetsin target languages.
speciﬁcally, we randomly di-vide the training set of each target language into 10disjoint subsets and compare model performance,with and without head pruning, using 1 to 9 sub-.
19630.10.20.30.40.50.60.70.80.9target data usage68.870.672.474.276.077.8f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage76.478.279.981.783.485.2f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage51.555.158.762.365.969.5f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage70.874.077.380.683.987.2f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage87.588.789.991.192.293.4f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage72.774.977.179.381.583.8f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage75.879.883.887.891.895.8f-1prunedunpruned(a) en-de.
(b) en-nl.
(c) en-ar.
(d) en-he.
(e) en-zh.
(f) en-ja.
(g) en-fa.
(h) en-ur.
figure 3: f-1 scores of mbert on multi-lingual the pos task with 10% - 90% target language training data usage.
dashed blue lines indicate scores without head pruning and solid red lines show scores with head pruning..sets.
we do not use 0 or 10 subsets since theycorrespond to cross-lingual and fully multi-lingualsettings, respectively.
we run the evaluations onner and pos tasks.
these datasets vary greatly insize, allowing us to validate our ﬁndings on target-language datasets with as few as 80 training exam-ples.
the ur ner dataset is excluded from thiscase study since its training set is overly large.
wenote that the score differences with and withouthead pruning are, in the main experiments, con-sistent for all the choices of models and sourcelanguages.
thus, we only display the mbert per-formance with en as the source language on nerin figure 2 and that on pos in figure 3..the evaluation results are consistent with thosein our main experiments, where the model withup to 12 attention heads pruned generally outper-forms the full mbert model.
this further supportsour hypothesis that pruning lower-ranked attentionheads has positive effects on the performance oftransformer-based models in truly resource-scarcelanguages.
it is also worth noting that pruning at-tention heads often causes the mbert model toreach peak evaluation scores with less training datain the target language.
for example, in the en-janer experiments, the full model achieves the high-est f-1 score when all the 800 training instancesin the ja dataset are used while the model withheads pruned achieves a comparable score with.
20% less data.
this suggests that pruning attentionheads makes deep transformer-based models eas-ier to train with less training data and thus moreapplicable to truly resource-poor languages..6 conclusion and future work.
this paper studied the contributions of attentionheads in transformer-based models.
past researchhas shown that in mono-lingual tasks, pruning alarge number of attention heads can achieve com-parable or higher performance than the full models.
however, we were the ﬁrst to extend these ﬁnd-ings to cross-lingual and multi-lingual sequencelabeling tasks.
using a gradient-based method, weidentiﬁed the heads to prune and showed that prun-ing attention heads generally has positive effectson mbert and xlm-r performances.
additionalcase studies empirically demonstrated the valid-ity of our ﬁndings and showed further extensibil-ity of them to a wider range of task settings.
inaddition to better understanding of transformer-based models under cross- and multi-lingual set-tings, our ﬁndings can be applied to existing modelsto achieve better performance with reduced trainingdata and resource consumption.
future work couldinclude improving model interpretability in othercross-lingual and multi-lingual tasks, e.g.
xnli(conneau et al., 2018) and other passage-level clas-siﬁcation tasks..19640.10.20.30.40.50.60.70.80.9target data usage94.094.494.895.295.696.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage95.095.495.896.296.697.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage93.093.694.294.895.496.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage93.694.294.895.496.096.6f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage91.091.892.693.494.295.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage94.094.695.295.896.497.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage84.086.088.090.092.094.0f-1prunedunpruned0.10.20.30.40.50.60.70.80.9target data usage88.088.889.690.491.292.0f-1prunedunprunedreferences.
xilun chen, ahmed hassan awadallah, hany has-san, wei wang, and claire cardie.
2019. multi-source cross-lingual model transfer: learning whatto share.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 3098–3112, florence, italy.
associationfor computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel r. bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..yanzhuo ding, yang liu, huanbo luan, and maosongsun.
2017. visualizing and understanding neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1150–1159, vancouver, canada.
association for computa-tional linguistics..ramy eskander, smaranda muresan, and michaelcollins.
2020. unsupervised cross-lingual part-of-speech tagging for truly low-resource scenarios.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4820–4831, online.
association for computa-tional linguistics..difﬁcult.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 3719–3728, brussels, belgium.
associationfor computational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..saﬁa kanwal, kamran malik, khurram shahzad,faisal aslam, and zubair nawaz.
2020. urdunamed entity recognition: corpus generation anddeep learning applications.
acm trans.
asian lowresour.
lang.
inf.
process., 19(1):8:1–8:13..phillip keung, yichao lu, and vikas bhardwaj.
2019.adversarial learning with contextual embeddings forzero-resource cross-lingual classiﬁcation and ner.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1355–1360, hong kong, china.
association for computa-tional linguistics..olga kovaleva, alexey romanov, anna rogers, andanna rumshisky.
2019. revealing the dark secretsof bert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4365–4374, hong kong, china.
association forcomputational linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..patrick littell, david r. mortensen, ke lin, kather-ine kairis, carlisle turner, and lori levin.
2017.uriel and lang2vec: representing languages as typo-logical, geographical, and phylogenetic vectors.
inproceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics: volume 2, short papers, pages 8–14.
asso-ciation for computational linguistics..paul michel, omer levy, and graham neubig.
2019.in ad-are sixteen heads really better than one?
vances in neural information processing systems,volume 32, pages 14014–14024.
curran associates,inc..shi feng, eric wallace, alvin grissom ii, mohit iyyer,pedro rodriguez, and jordan boyd-graber.
2018.pathologies of neural models make interpretations.
behrang mohit, nathan schneider, rishav bhowmick,kemal oﬂazer, and noah a. smith.
2012. recall-oriented learning of named entities in arabic.
1965blackboxnlp: analyzing and interpreting neuralnetworks for nlp, pages 63–76, florence, italy.
as-sociation for computational linguistics..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..yuxuan wang, wanxiang che, jiang guo, yijia liu,and ting liu.
2019. cross-lingual bert trans-formation for zero-shot dependency parsing.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5721–5727, hong kong, china.
association for computa-tional linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..qianhui wu, zijia lin, b¨orje karlsson, jian-guanglou, and biqing huang.
2020. single-/multi-sourcecross-lingual ner via teacher-student learning onin proceedingsunlabeled data in target language.
of the 58th annual meeting of the association forcomputational linguistics, pages 6505–6514, on-line.
association for computational linguistics..weijia xu, batool haider, and saab mansour.
2020.end-to-end slot alignment and recognition for cross-in proceedings of the 2020 confer-lingual nlu.
ence on empirical methods in natural languageprocessing (emnlp), pages 5052–5063, online.
as-sociation for computational linguistics..in proceedings of the 13th confer-wikipedia.
ence of the european chapter of the associationfor computational linguistics, pages 162–173, avi-gnon, france.
association for computational lin-guistics..taesun moon, parul aswathy, jian ni, and radutowards lingua franca namedarxiv preprint.
florian.
2019.entity recognition with bert.
arxiv:1912.01389..naama mordecai and michael elhadad.
2012. hebrew.
named entity recognition..hanieh poostchi, ehsan zare borzeshi, mohammadabdous, and massimo piccardi.
2016. personer:in proceedingspersian named-entity recognition.
of coling 2016, the 26th international confer-ence on computational linguistics: technical pa-pers, pages 3381–3389, osaka, japan.
the coling2016 organizing committee..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..soﬁa serrano and noah a. smith.
2019. is attentionin proceedings of the 57th annualinterpretable?
meeting of the association for computational lin-guistics, pages 2931–2951, florence, italy.
associa-tion for computational linguistics..oscar t¨ackstr¨om.
2012. nudging the envelope ofdirect transfer methods for multilingual named en-in proceedings of the naacl-tity recognition.
hlt workshop on the induction of linguistic struc-ture, pages 55–63, montr´eal, canada.
associationfor computational linguistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..jesse vig and yonatan belinkov.
2019. analyzingthe structure of attention in a transformer languagemodel.
in proceedings of the 2019 acl workshop.
1966