adversarial learning for discourse rhetorical structure parsing.
longyin zhang1,2, fang kong1,2∗, guodong zhou1,21. institute of artiﬁcial intelligence, soochow university, china2. school of computer science and technology, soochow university, chinalyzhang9@stu.suda.edu.cn{kongfang,gdzhou}@suda.edu.cn.
abstract.
text-level discourse rhetorical structure (drs)parsing is known to be challenging due to thenotorious lack of training data.
although re-cent top-down drs parsers can better leverageglobal document context and have achieved cer-tain success, the performance is still far fromperfect.
to our knowledge, all previous drsparsers make local decisions for either bottom-up node composition or top-down split pointranking at each time step, and largely ignoredrs parsing from the global view point.
ob-viously, it is not sufﬁcient to build an entiredrs tree only through these local decisions.
inthis work, we present our insight on evaluat-ing the pros and cons of the entire drs treefor global optimization.
speciﬁcally, based onrecent well-performing top-down frameworks,we introduce a novel method to transform bothgold standard and predicted constituency treesinto tree diagrams with two color channels.
af-ter that, we learn an adversarial bot betweengold and fake tree diagrams to estimate thegenerated drs trees from a global perspective.
we perform experiments on both rst-dt andcdtb corpora and use the original parsevalfor performance evaluation.
the experimentalresults show that our parser can substantiallyimprove the performance when compared withprevious state-of-the-art parsers..1.introduction.
as the main linguistic theory on discourse rhetor-ical structure (drs), rhetorical structure theory(rst) (mann and thompson, 1988) describes anarticle as a discourse tree (dt).
as illustrated infigure 1, each leaf node of the tree corresponds toan elementary discourse unit (edu), and relevantleaf nodes are connected by relation and nuclear-ity (nucleus (n) or satellite (s)) tags toform high-layer discourse units (dus), where the.
∗corresponding author.
figure 1: an example rst-style discourse tree..nucleus is considered more important than thesatellite.
since the rst structure can welldescribe the organization of an article, it has beenplaying a central role in various down-stream taskslike summarization (xu et al., 2020), text catego-rization (ji and smith, 2017), and so on..with the release of various discourse corpora,text-level dsr parsing has been drawing more andmore attention in the last decade.
however, sincethe corpus annotation is usually time-consuming,existing drs corpora are much limited in size.
for example, the english rst-dt (carlson et al.,2001) corpus only contains 385 wsj articles, andthe chinese cdtb (li et al., 2014b) corpus onlycontains 500 newswire articles.
in this situation,previous studies usually rely on multifarious hand-engineered features (hernault et al., 2010; fengand hirst, 2014; ji and eisenstein, 2014; li et al.,2014a, 2016; braud et al., 2017).
and all these sys-tems perform drs parsing in a bottom-up fashion.
until recently, some researchers turn to top-downdrs parsing (lin et al., 2019; zhang et al., 2020;kobayashi et al., 2020) to explore the potentialcapabilities of data-driven models.
nevertheless,text-level drs parsing is still challenging and wor-thy of in-depth exploration..theoretically, in supervised learning, annotated.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3946–3957august1–6,2021.©2021associationforcomputationallinguistics3946(cid:70)[e1: in fact,] [e2: budget indicated] [e3: it saw some benefit] [e4: to staying involved in these programs,] [e5: in which renters earn frequent-flier miles] [e6: and fliers can get car-rental discounts.]
wsj_2394e1e2e3e4e5e6same-unit (nn)attribution (ns)list (nn)elaboration (ns)elaboration (ns)figure 2: local and global optimization of drs trees..data corpora can provide neural models with spe-ciﬁc learning objectives, and the corpus size limi-tation will weaken the learning of these goals.
tomitigate this problem, we researchers need (i) anefﬁcient model to better learn from the limited dataand (ii) more high-quality training objectives toenhance the model learning.
existing studies ontext-level drs parsing show that.
• compared with bottom-up drs parsers, recenttop-down frameworks can better leverage globaldocument context and have achieved promisingresults in text-level drs parsing (zhang et al.,2020; kobayashi et al., 2020)..• all previous studies produce their drs parserswith local decisions made at each time step foreither bottom-up node composition or top-downsplit point selection (figure 2 (a)), and no globaldecisions are made for the entire drs structure(figure 2 (b)).
therefore, it is difﬁcult for themto achieve global optimization.
although somestudies (braud et al., 2017; mabona et al., 2019)leverage “beam-search” to traverse the solutionspace to ﬁnd the optimal parsing route, the algo-rithms are time-consuming to some extent..considering the above-mentioned status quo, inthis work, we study a global optimization methodbased on the well-performing top-down parsers.
for model structure, we take the top-down parserof zhang et al.
(2020) as our baseline system andmake some improvements to it.
for global opti-mization, we ﬁrst utilize a novel strategy to trans-form both gold standard and predicted drs treesinto tree diagrams with two color channels.
afterthat, an lsgan-based adversarial bot is structuredbetween gold and fake tree diagrams as an exam-iner for global estimation and optimization.
exper-imental results on the rst-dt and cdtb corporashow that our approaches are effective..bottom-up and top-down frameworks..for the ﬁrst category, early studies on drs pars-ing heavily relied on hand-crafted features and lin-guistic characteristics (hernault et al., 2010; jotyet al., 2013; feng and hirst, 2014).
during thepast decade, more and more researchers turned todata-driven approaches, and some effective strate-gies were proposed to adapt to the small-scale datacorpora.
among these studies, (ji and eisenstein,2014; li et al., 2014a, 2016; mabona et al., 2019)used some trivial features as auxiliaries in theirdata-driven systems; braud et al.
(2016; 2017) har-nessed task supervision from related tasks, alter-native views on discourse structures, and cross-lingual data to alleviate the data insufﬁciency prob-lem; wang et al.
(2017) introduced a two-stageparser to ﬁrst parse a naked tree structure andthen determine rhetorical relations for differentdiscourse levels to mitigate data sparsity; yu etal.
(2018) employed both syntax information anddiscourse boundaries in their transition-based sys-tem and achieved good performance..for the second category, some researchers (linet al., 2019; liu et al., 2019; zhang et al., 2020;kobayashi et al., 2020) turned to top-down frame-works to tap the potential capabilities of data-drivenmodels.
among them, (lin et al., 2019; liu et al.,2019) have achieved certain success in sentence-level drs parsing.
nevertheless, due to the long-distance dependency over the discourse, text-leveldrs parsing remains challenging.
to alleviate thisproblem, zhang et al.
(2020) proposed a top-downarchitecture tailored for text-level drs parsing.
kobayashi et al.
(2020) used contextualized wordrepresentation and proposed to parse a documentin three granularity levels for good performance..in the past decade, gans have achieved greatprogress in nlp (wu et al., 2019; elazar and gold-berg, 2018; chen and chen, 2019; zou et al., 2020).
however, to our knowledge, there is still no re-search on adversarial learning in drs parsing sofar.
in this work, we explore to adversarially traina discriminator to estimate the quality of the entiredrs tree for global optimization.
notably, we pro-pose to transform each drs tree into a continuoustree diagram, and thus our adversarial method doesnot suffer from the “discrete data” problem..2 related work.
3 baseline top-down architecture.
in the literature, previous studies on rst-styledrs parsing mainly consist of two categories, i.e.,.
in this section, we give a brief introduction to ourbaseline system, the top-down parser of zhang et.
3947(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)(cid:1286)(cid:1308)(cid:712)(cid:1286)(cid:1561)(cid:712)(cid:258)(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)(a)(b)e1e2e3n1local optore1e2e3global optn2n1n2al.
(2020), and make some improvements to it.
theparsing process is illustrated in figure 3..hierarchical split point encoding.
for splitpoint representation1, zhang et al.
(2020) intro-duced a hierarchical rnn-cnn architecture intheir paper.
firstly, they use an attention-basedgru encoder to encode each edu, obtaining ei.
then, the obtained edu vectors are fed into an-other bigru for context modeling, as shown infigure 3. next, a cnn net with a window size of2 and a stride size of 1 is built for each windowof edus in the discourse for split point encoding.
to our knowledge, zhang et al.
(2020) produceddummy split points at both ends of a discourse.
since the dummy split points do not participatein the split point selection process, they could beredundant.
here, we try to simplify the parsingprocedure with the dummy split points discarded,as shown in figure 3. following previous work (yuet al., 2018; kobayashi et al., 2020), we also splicethe sentence- and paragraph-level boundary fea-ture vectors to the representation of split points toenhance the encoder model..top-down split point ranking.
after achiev-ing split point representations, an encoder-decoderis used to rank the split points, as shown in fig-ure 3. during encoding, the previously obtainedsplit point vectors are taken as input to the bigruencoder, obtaining h0, .
.
.
, hn−2.
during decod-ing, a uni-directional gru with an internal stack isused to control the split point ranking process.
ini-tially, the stack contains only one element, i.e., in-dexes of the boundary split points in the discourse.
notably, since we do not add dummy split points inthis parser, we allow patterns like (τ, τ ) to appearin the stack.
at the j-th step, the tuple (b, e) ispopped from the stack and we enter the concate-nated cj = (hb; he) into the decoder for dj..after that, a biafﬁne function (dozat and man-ning, 2017) is built between the encoder and de-coder outputs for split point ranking.
differentfrom (zhang et al., 2020), all split points in theinterval [b, e] are selectable in this work.
at thestep j, we calculate the attention score between hiand dj as:.
sj,i = h t.i w dj + u hi + v dj + b.
(1).
where w, u, v, b are model parameters and sj,i ∈.
1the split position between any two neighboring edus is.
called the split point..figure 3: neural architecture of the encoder-decoder..rk denotes the score of the i-th split point over dif-ferent categories (for split point ranking, k equals1).
with this attention function used, at each timestep, split position with the highest score is selectedas the split point and the original text span is splitinto two adjacent text spans.
meanwhile, newlygenerated text spans with unselected split pointsare pushed onto the stack for following steps, asshown in figure 3. in this way, a drs tree is builtafter 5 iterations with the split points (1, 0, 2, 3, 4)detected in turn..to our knowledge, zhang et al.
(2020) use threebiafﬁne classiﬁers in their parser for structure, nu-clearity and relation prediction, respectively.
con-sidering the differences between the three learn-ing objectives, using three independent classiﬁerscould weaken the “full” performance.
to alleviatethis problem, we combine nuclearity and relationtags into n-r tags and only use two classiﬁers fordrs parsing.
therefore, for n-r prediction, thecategory number k equals 41 and 46 for the rst-dt and cdtb corpus respectively..4 adversarial learning for drs parsing.
this section introduces the proposed adversariallearning method which consists of two parts: graph-ical representation of gold and fake drs trees andthe adversarial model learning process..4.1 graphical representation of drs trees.
in this study, we aim to learn from the entire drstree to optimize our model from a global perspec-tive.
usually, our computer understands drs treesin two ways: either language description or graphi-cal representation.
since tree diagrams can reﬂectthe structural features more intuitively and are easyfor machines to understand, we explore graphicalrepresentation of drs trees in this work..for gold standard trees, we propose to trans-form each tree into multi-pattern matrices which.
3948(0,4)(2,4)(0,0)(2,4)(3,4)(4,4)e0e1e2e3e4e510234he0he1he2he3he4he5hs0hs1hs2hs3hs4h0h1h2h3h4c0c1c2c3c4d0d1d2d3d4(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)(cid:1286)(cid:1308)(cid:712)(cid:1286)(cid:1561)(cid:712)(cid:258)(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)figure 4: graphical representation of drs structure for adversarial learning of text-level drs parsing..is similar to a low resolution image with two colorchannels (i.e., the structure (st) and nuclearity-relation (nr) channels).
formally, given a drstree of height m with n split points, each split pointcorresponds to a speciﬁc non-leaf node in the tree,and we construct two matrices, xst and xnr, ofsize m × (n + 2) corresponding to the two colorchannels, as shown in figure 4.
(i) for the stchannel, all the elements in the matrix xst areinitialized2 to -2. with the upper left corner of thematrix as the origin of the coordinate axis, giventhe split point j at the i-th tree layer (top-downdirection), we directly set the element at (i-1, j+1)by zero.
besides, if the left span of the split pointis an edu, then we set the element at (i, j) by -1,and the right span is processed in a similar way.
with this method, we can recursively construct thetree diagram from top to down.
additionally, someedu positions are actually shared in the matrix,and this does not affect the understanding of thesenodes.
for the example in figure 4, although e2and e3 share a same position in the st channel, thefollowing two patterns in the matrix can still revealan accurate representation of each node:.
n1 :.
(cid:21).
(cid:20) 0 −2−2 −1.
n2 :.
(cid:21).
(cid:20)−20−1 −2.
(2).
(ii) for the nr channel, we set the positions repre-senting non-leaf nodes to speciﬁc n-r labels andthe positions of leaf nodes to −1 and other non-node positions to zero..for the automatically parsed trees, we directlyuse our model outputs to build the tree diagramwith two color channels, x (cid:48)nr.
and the2we set these non-node positions to -2 in two reasons: (i)we apply a log-softmax function to the attention weights forsplit point ranking with the output ranging (−∞, 0]; (ii) wesimply set the non-node positions by -2 to distinguish themfrom the leaf nodes marked with -1..st and x (cid:48).
two matrices of size m × (n + 2) are initializedwith zero.
(i) for the st channel, as stated be-fore, a set of attention weights are assigned to theencoder outputs during pointing and a split pointis selected according to the weights.
obviously,each split point corresponds to a group of attentionweights (after log-softmax).
therefore, we directlyadd these n-dimensional attention weights of eachsplit point in the i-th tree layer (top-down direc-tion) to the i-th line of x (cid:48)st. notably, the ﬁrst andlast columns of the matrices are actually placehold-ers initialized with unlearnable scalars representingleaves or non-node positions, so we only add thesplit point attention weights to the range from 1 ton in each row.
(ii) for the nr channel, we simplyreplace these elements corresponding to split pointsin x (cid:48)st with predicted n-r labels3 and other ele-ments keep the same as xnr.
alternatively, onlythe replaced elements in the matrix x (cid:48)nr are learn-able, while other positions serve as static featuresin the image.
in this way, the model outputs arealso abstracted as a tree diagram with two colorchannels..through the above methods, we achieve graphi-cal representation for both gold standard and auto-matically predicted drs trees.
and the graphicalrepresentation can provide our model with a globalperspective, which makes the global optimization(subsection 4.2) of drs parsing possible..4.2 adversarial model learning.
for model learning, we have two goals: (i) learningof drs parsing at each time step for local optimiza-tion and (ii) learning an adversarial bot to evaluate.
3here, we need to map the attention score, sj,i ∈ rk,to a speciﬁc n-r label.
since the argmax function does notsupport gradient calculation, we give an alternative solution:lj,i = fsigmoid(wl · sj,i + bl) × k, where k is the numberof n-r labels and lj,i ∈ r1 is the learnable n-r label..3949(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)(cid:1286)(cid:1308)(cid:712)(cid:1286)(cid:1561)(cid:712)(cid:258)(cid:18221)(cid:11096)(cid:1206)(cid:6320)(cid:7519)(cid:1167)(cid:4705)(cid:7263)(cid:2255)(cid:14125)(cid:5719)(cid:7784)(cid:12017)(cid:1953)(cid:6495)(cid:6208)(cid:1791)(cid:6320)(cid:7519)(cid:7573)(cid:4649)(cid:3927)(cid:14145)(cid:17931)(cid:15996)(cid:18000)(cid:5537)(cid:452)convolution-layerxstr3r4-1-1r2r1r30-1-1-10000-1-1000-2-1-1-1-2-2parse treeimage generariongold treeadversarial botfeature extractionzfeature extractionmax poolingimage generationreshapefeaturexe1e2e3e4e5e6n-r1n-r3n-r2n-r3n-r4xnr0m=5, n=5i=3, j=3i=2, j=0the pros and cons of the entire tree for global op-timization.
for the ﬁrst goal, we use two negativelog-likelihood loss terms to optimize the parsingmodel.
for split point ranking, we use ls to maxi-mize the probability of correct split point selectionat each decoding step.
for n-r prediction, giventhe selected split point, we use lnr to maximizethe probability of correct n-r labeling for the splitpoint.
since the convergence speeds of the two lossterms are different, we add two loss weights beforethe loss terms to balance the model training as:.
ldrs = α1ls + α2lnr.
(3).
(4).
for the second goal, we explore to learn fromthe entire drs tree for global optimization.
to thatend, we produce an adversarial bot in our parserto estimate the generated drs tree diagrams, asshown in figure 4. since the composition andsources of gold and generated tree diagrams arecompletely different, we use two isomorphic fea-ture extractors to understand the two kinds of im-ages separately.
for feature extraction, based onsuch a 2d image-like representation, we performconvolution on every 3 × (n + 2) window to digout the structural details of the entire tree:(cid:37)(f )win = frelu(w(f ) · xwin + b(f ))then we perform max-pooling in each nonoverlap-ping 3 × 1 window for feature extraction, and theresulting matrices are reshaped as (cid:37) ∈ r1×d toserve as the distributed representation of the tree.
in this work, we do not just need an excellentdiscriminator expert in classiﬁcation, we need theadversarial nets to continuously give feedback toour parsing model even when the generated treesare correctly classiﬁed.
on this basis, we lever-age least squares generative adversarial network(lsgan) (mao et al., 2017) as our adversarial botwhich has proven to perform more stable and faceless problem of vanishing gradients than the orig-inal gan.
formally, our adversarial nets consistof two parts: (i) a generative net g to capture thedata distribution pz over the training data x and (ii)a discriminative net d to estimate the probabilitythat a sample comes from x rather than pz.
onthis basis, given the distributed representation ofthe gold tree x and fake tree z, we formulate theloss functions as follows:.
v (d) =.
mind.ex∼pdata(x)[(d(x) − b)2].
12ez∼pz(z)[(d(g(z)) − a)2].
+.
12.
(5).
v (g) =.
ming.12.ez∼pz(z)[(d(g(z)) − c)2].
(6).
similar to mao et al.
(2017), we set a = 0 andb = c = 1 to make g generate samples as real aspossible.
technically, the generator g consists ofthe parsing model and the feature extractor for faketrees, and the discriminator is an mlp (in: featuresize ((cid:15)), hidden: (cid:15)/2, out: 1) without the sigmoidactivation function.
therefore, when learning g,parameters of the parsing model and the feature ex-tractor for fake trees are updated.
likewise, param-eters of the discriminator and the feature extractorfor real trees are learned when tuning d..at this time, we have a traditional loss term totrain the top-down parser at each splitting step andtwo adversarial loss terms to estimate the entiredrs tree for global optimization.
it is worth men-tioning that we ﬁrst optimize the ldrs for 7 epochsto warm up the model parameters, and then the ad-versarial nets join the training process for globaloptimization of drs parsing..5 experimentation.
5.1 experimental settings.
datasets.
following our previous work (zhanget al., 2020), we utilize both the english rst dis-course treebank (rst-dt) (carlson et al., 2001)and the chinese connective-driven discourse tree-bank (cdtb) (li et al., 2014b) as the benchmarkcorpora for experimentation.
here, we give a briefintroduction to the two corpora:.
• the rst-dt corpus contains 385 news articles(347 for training and 38 for testing) from the wallstreet journal (wsj).
following previous work,we randomly select 34 documents from the train-ing corpus as the development corpus for parame-ter tuning.
and we also binarize those non-binarysubtrees in rst-dt with right-branching (sagaeand lavie, 2005) for preprocessing..• the chinese cdtb corpus is motivated by tak-ing advantages of both the english rst-dt cor-pus and the pdtb corpus (prasad et al., 2008).
the cdtb corpus annotates each paragraph asa connective-driven discourse tree (cdt).
thecorpus consists of 500 newswire articles whichare further segmented into 2336 paragraphs and10650 edus.
the corpus is divided into threeparts with 425 articles (2002 cdt trees) for train-ing, 25 articles (105 cdt trees) for validation,and 50 articles (229 cdt trees) for testing..3950metrics.
following previous studies, we mea-sure the performance of bare tree structure (s), treestructure labeled with nuclearity (n), and tree struc-ture labeled with rhetorical relation (r).
recently,the full (f) indicator is used to estimate the treestructure labeled with both nuclearity and relationcategories.
however, since current performanceson s, n and r are imbalanced, the performanceon f is much limited by relation prediction.
inother words, the full score may underestimate theperformance in span and nuclearity prediction.
inthis work, we combine nuclearity and rhetoricalrelation tags for joint n-r prediction aiming to re-duce the uncertainty of the full measure.
moreover,since rst-parseval (marcu, 2000) overestimatesthe drs parsing performance to a certain extent,(morey et al., 2017; mabona et al., 2019; zhanget al., 2020; koto et al., 2021) adopt the originalparseval to reveal the actual performance level ofdrs parsing.
following these studies, we also usethe original parseval for evaluation and report themicro-averaged f1 scores by default..hyper-parameter setting.
for word representa-tion, we employed the 300d vectors of glove (pen-nington et al., 2014) and the 1024d vectors ofelmo (peters et al., 2018) for rst-dt and the300d vectors of qiu et al.
(2018) (qiu-w2v) forcdtb, and we did not update these vectors dur-ing training.
the english pos tags were obtainedthrough the stanford corenlp toolkit (manninget al., 2014), the chinese tags were borrowed fromchinese ptb, and all the pos embeddings wereoptimized during training.
for model learning,we used the development set to ﬁne-tune the pa-rameters in table 1, and the number of parame-ter search trials was around 20. all the experi-ments based on the above-mentioned settings wereconducted on geforce rtx 2080ti gpu, and thecodes will be published at https://github.com/nlp-discourse-soochowu/gan_dp..5.2 experimental results.
comparison between different system settings.
as stated before, we explore to make possible im-provements to the top-down architecture of zhanget al.
(2020).
here, we study the effects of thesesimpliﬁcation methods based on our simpliﬁed ar-chitecture.
for clarity, we remove the adversariallearning process in each system, and the resultsare presented in table 2. for the rst-dt corpus,the ﬁrst two rows show that the top-down parser.
parameterpos embeddinguni-directional grubigrubiafﬁne-mlp-splitbiafﬁne-mlp-nrboundary feature sizedropout ratewarm up epochstraining epochsbatch size (dts)learning rate of dlearning rate of other netsα1α2.
en30512256128128300.272051e-41e-30.31.0.cn3051225664128-0.33720645e-41e-30.31.0.table 1: fine-tuned hyper-parameters..systemst2d+ ds+ tct2d+ ds+ ds&tc.
en.
cn.
s70.769.270.682.583.285.2.n58.357.757.957.357.857.3.r46.546.146.151.752.753.3.f45.244.944.448.249.045.7.table 2: results under different model settings.
“t2d”denotes our simpliﬁed architecture, which excludes thedummy split points and only uses two classiﬁers fordrs parsing; “ds” means the dummy split points areused; “tc” means three classiﬁers are used..performs worse when dummy split points are used,and the decline is obvious in tree structure parsing.
then, we further apply three classiﬁers to the sim-pliﬁed architecture, and the results (lines 1 and 3)show that the full score drops by 1.8% for lack ofcorrelation between the three learning goals.
forthe cdtb corpus, due to the differences in lan-guages and annotation strategies, the situation isquite different.
speciﬁcally, lines 4 and 5 show thatthe top-down parser performs better on all the fourindicators when using dummy split points (zhanget al., 2020).
based on the better-performing parserusing “ds”, we further report its performance withthree independent classiﬁers used, and the results(line 6) show that the full score still drops a lot(6.7%), which suggests the necessity of joint n-rprediction.
considering the above results, in thefollowing, we separately use two sets of model set-tings for different languages.
for english, we buildour ﬁnal model based on the simpliﬁed architecturewithout dummy split points.
for chinese, we buildour ﬁnal model based on the architecture of zhanget al.
(2020).
for both systems, we only use twoclassiﬁers for drs parsing..3951systemsfinal.
- advers.
bot.
final.
- advers.
bot.
en.
cn.
s71.870.784.983.2.n59.558.358.457.8.r47.046.554.552.7.f45.945.250.349.0.table 3: comparison on the adversarial bot..comparison on the adversarial bot.
here, weperform experiments to explore the effects of theadversarial learning approach, and the experimen-tal results are presented in table 3. for the rst-dtcorpus, the results show that our adversarial modelsetting can improve the performance on all the fourindicators, especially in structure and nuclearityprediction.
similarly, the results on the cdtb cor-pus show that our adversarial method still worksmuch better than the unreinforced parser in struc-ture, relation, and full detection.
the overall resultsindicate that the global optimization method we useis deﬁnitely effective, although the effectivenesshas not yet reached the level of qualitative change.
in fact, as a preliminary attempt for global opti-mization of drs parsing, this research still hasmuch room for improvement which deserves fur-ther exploration..comparison with previous studies.
in this part,we compare with seven previous state-of-the-art(sota) parsers on text-level drs parsing.
here,we brieﬂy review these studies as follows:.
• ji and eisenstein (2014), a shift-reduce parserwith an svm that is trained by their extractedlatent features.
in this paper, we compare withthe updated version of their parser (designated as“je2017-updated”) (morey et al., 2017)..• feng and hirst (2014), a two-stage greedy parserwith linear-chain crf models and some hand-engineered features..• li et al.
(2016), an attention-based hierarchicalneural model with hand-crafted features used..• braud et al.
(2016), a hierarchical bilstmmodel that leverages information from varioussequence prediction tasks..• braud et al.
(2017), a transition-based neuralmodel with both cross-lingual information andhand-crafted features used..• mabona et al.
(2019), a generative model with abeam search algorithm used for drs parsing..systemsje2017-updatedfeng and hirst (2014)li et al.
(2016)braud et al.
(2016)braud et al.
(2017)mabona et al.
(2019)zhang et al.
(2020)ours (glove)ours (elmo)zhang et al.
(2020)zhang et al.
(2020)*ours (qiu-w2v).
en.
cn.
s64.168.664.559.562.767.167.269.971.885.284.084.9.n54.255.954.047.254.557.455.557.359.557.359.058.4.r46.845.838.134.745.545.545.346.347.053.354.254.5.f46.344.636.634.345.145.044.345.045.945.747.850.3.table 4: performance comparison with previous work.
results of the ﬁrst ﬁve lines are directly borrowed from(morey et al., 2017).
“*” denotes the updated resultsbased on the strict evaluation metric we use..• zhang et al.
(2020), a top-down neural architec-ture tailored for text-level drs parsing.
differentfrom many previous studies, this parser is a pureneural parser without using any additional hand-crafted features..for the rst-dt corpus, the results are presentedin the upper part of table 4. from the results, al-though our previous top-down parser (zhang et al.,2020) can achieve good results without using hand-crafted features, the performance is still far fromperfect.
comparing our glove-based top-downparser with previous state-of-the-art parsers, ourparser performs better than most previous ones dueto its ability in leveraging global context and theadversarial learning strategy.
furthermore, compar-ing the ﬁnal parser (line 9) with previous work, ourelmo-based parser can further improve the perfor-mance on all the four indicators, and the improve-ments on structure (4.7%) and nuclearity (3.7%)are signiﬁcant.
obviously, the contextualized wordrepresentation can greatly improve the parsing per-formance, especially in such a task with small-scaledata corpora..for the cdtb corpus, we explore to employa more strict metric4 for performance evaluationand the overall results are presented in the lowerpart of table 4. in comparison with previous work,our parser achieves comparable performance in nu-clearity and relation prediction and much betterresults on the other two indicators, which provesthe usefulness of the adversarial nets we use.
in.
4we borrow the strict evaluation method from https:.
//github.com/nlp-discourse-soochowu/t2d_discourseparser for evaluation in this study, and reportthe macro-averaged f1-scores for performance..3952systemskoto et al.
(2021)ours (xlnet).
- advers.
botours (qiu-w2v)ours (xlnet).
- advers.
bot.
en.
cn.
s73.176.376.184.986.685.8.n62.365.564.458.465.064.5.r51.555.654.354.562.160.5.f50.353.852.950.355.453.7.systemswang et al.
(2017)*yu et al.
(2018)*kobayashi et al.
(2020)*ours (final).
- advers.
bot.
uas61.561.964.972.371.4.las47.848.448.557.656.5.table 5: performance comparison with lms used..table 6: evaluation on dependency trees.
“*” denotesthe results are borrowed from (kobayashi et al., 2020)..particular, compared with previous parsers, ourparser performs signiﬁcantly better on “f” due tothe joint prediction of nuclearity and relation cat-egories.
this suggests the robustness of our sim-pliﬁed parser with only two classiﬁers.
moreover,since the two top-down drs parsers in the tableshow similar results on “r”, we speculate that thechinese rhetorical relation prediction has encoun-tered a bottleneck to some extent, which requiresmore effort to be invested..performances based on the sota language mod-els.
recently, more and more researchers (shiet al., 2020; koto et al., 2021) propose to improvedrs parsing performance through powerful lan-guage models (lms) like bert (devlin et al., 2019)and xlnet (yang et al., 2019).
following thesestudies, in this work, we perform additional exper-iments on the xlnet-base models in (yang et al.,2019) and (cui et al., 2020) for the rst-dt andcdtb corpus, respectively.
for better model inte-gration, we slightly adjust the previously describedmodel architecture5, more speciﬁcally, the eduencoder.
we ﬁrst use a pre-trained lm to encodeeach entire discourse where each edu is attachedwith the [sep] and [cls] tokens and then takethe lm outputs corresponding to [cls] as ouredu representation.
moreover, we segment eachdocument according to the maximum length of 768tokens and encode these text segments one by oneto avoid the problem of memory overﬂow..for the rst-dt corpus, we report the resultsof the recent bert-based top-down parser (kotoet al., 2021) for comparison.
for the cdtb cor-pus, we compare with our previously describedsystem based on traditional word vectors, and theoverall results are shown in table 5. from theresults we ﬁnd that our parsers achieve superiorresults when using the contextualized xlnet forexperimentation, which suggests the great effec-tiveness of pre-trained lms in such a task with.
5adjusted model parameters are shown in appendix..limited corpus size.
moreover, the ablation studyon the adversarial learning strategy further demon-strates the usefulness of our proposed method.
itshould be noted that we report the performance us-ing lms in this paper never mean to advocate usingpre-trained lms or blindly pursuing performanceimprovements in drs parsing.
sometimes, the re-wards generated by the large-scale lms could bequite different from and much more effective thanthat generated by language phenomena, which mayhinder the study on the relatively shallow (com-pared with powerful lms) yet valuable discoursefeatures.
with this in mind, it is reasonable to per-form ablation study using simple word representa-tion to explore useful discourse features and reportthe performance on powerful lms for reference..5.3 analysis and discussion.
performance evaluation of dependency trees.
recently, discourse-level dependency structure hasattracted more and more attention.
here, we ex-plore whether the proposed global optimizationmethod can improve the rst dependency analy-sis to some extent.
to achieve this, we ﬁrst con-vert the predicted drs trees into dependency treesas kobayashi et al.
(2020) did and then performevaluation on the converted dependencies labeled(las) and unlabeled (uas) with rhetorical rela-tions, and the results are shown in table 6. firstly,lines 1 to 4 show that our parser can greatly out-perform previous systems in terms of both uasand las indicators.
secondly, the last two rowsshow that the global optimization of constituencytrees can simultaneously improve the dependencyperformance, which further proves the usefulnessof our proposed adversarial method..remarkable progress in drs parsing.
com-pared with chinese drs parsing where each para-graph is annotated as a dt, the english parsing with313 dts for training is much more challenging.
nevertheless, results in table 4 and table 5 showthat our parser can largely outperform previous.
3953systemsours (glove)ours (elmo)ours (xlnet).
- advers.
bot.
nn/23% ns/61% sn/16%62.964.167.466.4.
55.758.569.666.7.
43.347.856.758.8.table 7: performance on nuclearity detection..figure 5: convergence of our parsing model over differ-ent learning rates (lrs)..state-of-the-art parsers on “full”.
(i) for nuclearityprediction, we display the results of our parserson each nuclearity category to explore where theimprovement comes from, as shown in table 7.from the results, it’s obvious that the lm we useplays a big role in nuclearity prediction, and theproposed adversarial method can further improvethe performance to a certain extent.
(ii) for rela-tion prediction, the classiﬁcation problem with 18coarse-grained relation tags (rst-dt) is really achallenge.
from the results in table 4 we can ﬁndthat the progress in relation prediction is much lim-ited in recent decade for the lack of data.
and mostof previous state-of-the-art parsers employee a va-riety of hand-engineered features for good perfor-mance.
hopefully, the experimental results in ta-ble 5 show that powerful lms can free data-drivenmodels from corpus size limitation and thus ourxlnet-based parser strongly outperforms je2017-updated (morey et al., 2017) by 18.8% on “r”.
theresults of our parsers on each rhetorical relationcategory are shown in appendix..discussion on adversarial learning.
similarto previous gan work, improving the quality ofthe generated tree images is really a challenge, andthe instability of the adversarial learning processis another intractable issue.
in order for our model.
to continuously modify the generated images evenwhen they are correctly classiﬁed, we leverage aleast squares loss in our system for model learning.
to avoid the over-learning of the discriminator, wetune it with a moderate learning rate and parameterscale.
intuitively, the convergence of our modelover different learning rates is presented in fig-ure 5. from the results, as the learning rate of thediscriminator increases, the ﬂuctuation of the lossvalue becomes larger, and it is hard to reduce thegenerator loss.
in these four cases, the ﬁrst groupseems to be more stable and in line with our ex-pectations.
therefore, we set the learning rate to1e-4 in our systems for experimentation.
notably,we also tried the sigmoid cross entropy loss in thisresearch which performs much worse than the ls-gan we use.
for reference, we also present themodel convergence over different loss functions inappendix for reference..6 conclusion.
in this research, we explored a global optimizationmethod based on recent top-down frameworks.
par-ticularly, we proposed a novel strategy to transformboth gold standard and predicted drs trees intotree diagrams with two color channels.
on this ba-sis, we produced an lsgan-based adversarial botbetween gold and fake trees for global optimiza-tion.
experimental results on two popular corporashowed that our proposed adversarial approach iseffective in drs parsing and has established newstate-of-the-art results for both corpora..acknowledgements.
here, the ﬁrst author (longyin zhang) would liketo thank his ﬁancee, dr. xin tan, for her valu-able discussion on this research.
this work wassupported by the national key r&d programof china under grant no.
2020aaa0108600,projects 61876118 and 61976146 under the na-tional natural science foundation of china andthe priority academic program development ofjiangsu higher education institutions..references.
chlo´e braud, maximin coavoux, and anders søgaard.
2017. cross-lingual rst discourse parsing.
ineacl, pages 292–304, valencia, spain.
associationfor computational linguistics..chlo´e braud, barbara plank, and anders søgaard.
2016.multi-view and multi-task training of rst discourse.
3954(cid:47)(cid:53)(cid:32)(cid:19)(cid:17)(cid:19)(cid:19)(cid:19)(cid:20)(cid:47)(cid:53)(cid:32)(cid:19)(cid:17)(cid:19)(cid:19)5(cid:47)(cid:53)(cid:32)(cid:19)(cid:17)(cid:19)(cid:19)(cid:20)(cid:47)(cid:53)(cid:32)(cid:19)(cid:17)(cid:19)(cid:19)(cid:19)5losslosslosslossstepstepstepstepparsers.
in proceedings of coling 2016, the 26thinternational conference on computational linguis-tics: technical papers, pages 1903–1913, osaka,japan.
the coling 2016 organizing committee..lynn carlson, daniel marcu,.
and mary ellenokurovsky.
2001. building a discourse-tagged cor-pus in the framework of rhetorical structure theory.
in proceedings of the second sigdial workshop ondiscourse and dialogue..francine chen and yan-ying chen.
2019. adversarialdomain adaptation using artiﬁcial titles for abstrac-tive title generation.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 2197–2203, florence, italy.
asso-ciation for computational linguistics..yiming cui, wanxiang che, ting liu, bing qin, shijinwang, and guoping hu.
2020. revisiting pre-trainedmodels for chinese natural language processing.
infindings of the association for computational lin-guistics: emnlp 2020, pages 657–668, online.
as-sociation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, minnesota.
association forcomputational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in the 5th international conference on learningrepresentations, iclr2017..yanai elazar and yoav goldberg.
2018. adversarialremoval of demographic attributes from text data.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages11–21, brussels, belgium.
association for computa-tional linguistics..vanessa wei feng and graeme hirst.
2014. a linear-time bottom-up discourse parser with constraints andin proceedings of the 52nd annualpost-editing.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 511–521,baltimore, maryland.
association for computationallinguistics..hugo hernault, helmut prendinger, mitsuru ishizuka,et al.
2010. hilda: a discourse parser using sup-port vector machine classiﬁcation.
dialogue anddiscourse, 1(3)..yangfeng ji and jacob eisenstein.
2014. represen-tation learning for text-level discourse parsing.
inproceedings of the 52nd annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 13–24, baltimore, maryland.
association for computational linguistics..yangfeng ji and noah a. smith.
2017. neural dis-course structure for text categorization.
in proceed-ings of the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 996–1005, vancouver, canada.
association forcomputational linguistics..shaﬁq joty, giuseppe carenini, raymond ng, andyashar mehdad.
2013. combining intra- and multi-sentential rhetorical parsing for document-level dis-course analysis.
in proceedings of the 51st annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 486–496,soﬁa, bulgaria.
association for computational lin-guistics..naoki kobayashi, tsutomu hirao, hidetaka kamigaito,manabu okumura, and masaaki nagata.
2020. top-down rst parsing utilizing granularity levels in docu-ments.
in association for the advancement of artiﬁ-cial intelligence 2020, aaai2020..fajri koto, jey han lau, and timothy baldwin.
2021.top-down discourse parsing via sequence labelling.
in proceedings of the 16th conference of the euro-pean chapter of the association for computationallinguistics: main volume, pages 715–726, online.
association for computational linguistics..jiwei li, rumeng li, and eduard hovy.
2014a.
recur-sive deep models for discourse parsing.
in proceed-ings of the 2014 conference on empirical methods innatural language processing (emnlp), pages 2061–2069, doha, qatar.
association for computationallinguistics..qi li, tianshi li, and baobao chang.
2016. discourseparsing with attention-based hierarchical neural net-works.
in proceedings of the 2016 conference onempirical methods in natural language process-ing, pages 362–371, austin, texas.
association forcomputational linguistics..yancui li, wenhe feng, jing sun, fang kong, andguodong zhou.
2014b.
building chinese discoursecorpus with connective-driven dependency tree struc-ture.
in proceedings of emnlp 2014, pages 2105–2114..xiang lin, shaﬁq joty, prathyusha jwalapuram, andm saiful bari.
2019. a uniﬁed linear-time frame-work for sentence-level discourse parsing.
in acl,pages 4190–4200, florence, italy.
association forcomputational linguistics..linlin liu, xiang lin, shaﬁq joty, simeng han, andlidong bing.
2019. hierarchical pointer net parsing.
in emnlp-ijcnlp, pages 1006–1016, hong kong,china.
association for computational linguistics..amandla mabona, laura rimell, stephen clark, andandreas vlachos.
2019. neural generative rhetoricalstructure parsing.
in proceedings of the 2019 confer-ence on empirical methods in natural language pro-cessing and the 9th international joint conference.
3955on natural language processing (emnlp-ijcnlp),pages 2284–2295, hong kong, china.
associationfor computational linguistics..william c mann and sandra a thompson.
1988.rhetorical structure theory: toward a functional the-ory of text organization.
text-interdisciplinary jour-nal for the study of discourse, 8(3):243–281..christopher manning, mihai surdeanu, john bauer,jenny finkel, steven bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-cessing toolkit.
in proceedings of 52nd annual meet-ing of the association for computational linguis-tics: system demonstrations, pages 55–60, balti-more, maryland.
association for computational lin-guistics..x. mao, q. li, h. xie, r. y. k. lau, z. wang, and s. p.smolley.
2017. least squares generative adversarialnetworks.
in 2017 ieee international conferenceon computer vision (iccv), pages 2813–2821..daniel marcu.
2000. the theory and practice of dis-course parsing and summarization.
mit press, cam-bridge, ma, usa..mathieu morey, philippe muller, and nicholas asher.
2017. how much progress have we made on rstdiscourse parsing?
a replication study of recent re-sults on the rst-dt.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, pages 1319–1324, copenhagen,denmark.
association for computational linguis-tics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural language pro-cessing (emnlp), pages 1532–1543, doha, qatar.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), pages 2227–2237,new orleans, louisiana.
association for computa-tional linguistics..rashmi prasad, nikhil dinesh, alan lee, eleni milt-sakaki, livio robaldo, aravind joshi, and bonniewebber.
2008. the penn discourse treebank 2.0. inlrec 2008..yuanyuan qiu, hongzheng li, shen li, yingdi jiang,renfen hu, and lijiao yang.
2018. revisiting cor-relations between intrinsic and extrinsic evaluationsof word embeddings.
in ccl & nlp-nabd 2017,pages 209–221.
springer..kenji sagae and alon lavie.
2005. a classiﬁer-basedparser with linear run-time complexity.
in proceed-ings of the ninth international workshop on pars-ing technology, pages 125–132, vancouver, britishcolumbia.
association for computational linguis-tics..ke shi, zhengyuan liu, and nancy f. chen.
2020. anend-to-end document-level neural discourse parserexploiting multi-granularity representations.
corr,abs/2012.11169..yizhong wang, sujian li, and houfeng wang.
2017.a two-stage parsing method for text-level discourseanalysis.
in proceedings of the 55th annual meetingof the association for computational linguistics (vol-ume 2: short papers), pages 184–188, vancouver,canada.
association for computational linguistics..jiawei wu, xin wang, and william yang wang.
2019.self-supervised dialogue learning.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 3857–3867, florence,italy.
association for computational linguistics..jiacheng xu, zhe gan, yu cheng, and jingjing liu.
2020. discourse-aware neural extractive text sum-marization.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5021–5031, online.
association for computa-tional linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining for lan-guage understanding.
in advances in neural infor-mation processing systems, volume 32. curran asso-ciates, inc..nan yu, meishan zhang, and guohong fu.
2018.transition-based neural rst parsing with implicitsyntax features.
in proceedings of the 27th inter-national conference on computational linguistics,pages 559–570, santa fe, new mexico, usa.
asso-ciation for computational linguistics..longyin zhang, yuqing xing, fang kong, peifeng li,and guodong zhou.
2020. a top-down neural archi-tecture towards text-level parsing of discourse rhetor-in proceedings of the 58th annualical structure.
meeting of the association for computational lin-guistics, pages 6386–6395, online.
association forcomputational linguistics..wei zou, shujian huang, jun xie, xinyu dai, and jiajunchen.
2020. a reinforced generation of adversarialexamples for neural machine translation.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 3486–3497, on-line.
association for computational linguistics..appendix.
a. adversarial model learning.
here, we display the convergence of our modelswith different loss functions and model settings.
3956table 8: results on the rst-dt corpus.
“ratio” meansthe proportion of each category label in the corpus..type-ratio%elaborate-30.4joint-15.1attribution-11.7same-unit-10.9contrast-5.8explanation-3.8background-3.4temporal-3.0cause-2.9evaluation-2.2enablement-2.2comparison-1.7topic-change-1.6textual-org-1.3condition-1.2topic-comment-1.0manner-means-0.8summary-0.8.
glove47.936.377.970.334.511.323.015.43.74.154.712.57.720.042.10.033.347.8.elmo xlnet48.839.283.071.927.016.120.815.57.70.042.012.911.128.629.00.032.144.0.
60.449.486.775.942.621.727.834.618.510.566.736.740.053.362.58.344.050.0.qiu-w2v xlnet.
type-ratio%并列 / same-unit-47.8解说 / explanation-12.6因果 / cause-9.4顺承 / consequent-7.1目的 / purpose-4.6例证 / example-3.4总分 / overall-branch-3.2评价 / evaluation-3.1转折 / contrast-2.7背景 / background-1.8条件 / condition-1.0假设 / suppose-1.0递进 / progressive-0.9对比 / comparison-0.8推断 / deduce-0.5让步 / concession-0.2.
80.250.032.54.148.510.575.026.769.00.00.00.00.00.00.00.0.
88.060.755.958.158.534.573.956.375.036.416.766.70.040.00.00.0.table 9: results on the cdtb corpus..gpu memory.
for cdtb, we set the lrs of thediscriminator, lm, and other nets to 5e-4, 1e-4,and 2e-5, respectively.
we trained the lm-basedsystems for around 30 rounds and the other systemsettings remained the same as the aforementionednon-lm-based systems..applied, as shown in figure 6. comparing the ﬁrsttwo legends, since the sigmoid cross entropy losssuffers from gradient vanishing, it’s hard for ourmodel to update the generator net, and the generatorloss keeps growing up.
to avoid the over-learningof the discriminator net, we simplify the originaldiscriminator network from a 3-layer mlp to alinear function, and the results are presented infigure 6 (c).
from the results, it’s really hard totrain both generator and discriminator nets, and theadversarial learning in figure 6 (c) seems to bemeaningless for drs parsing..figure 6: figure (a) refers to our ﬁnal model based onlsgan; ﬁgure (b) refers to our model with the sigmoidcross entropy loss function used; based on ﬁgure (b),we use a simpliﬁed discriminator in ﬁgure (c)..b. results on different relation categories.
table 8 and table 9 present the performances (f1-scores) of our systems on each relation category inthe rst-dt and cdtb corpora, respectively..c. conﬁgurations of the lm-based systems.
for better model integration, we slightly tuned themodel hyper-parameters to adapt to the lm-basedsystems.
for rst-dt, we set the lrs of all thenets to 1e-4, the hidden size of bigru to 384, thehidden size of uni-directional gru to 768, andthe batch size to 1 to suit the nvidia tesla p40.
3957losslossloss(a)(b)(c)