cross-language sentence selection viadata augmentation and rationale training.
yanda chen1, chris kedzie1, suraj nair2, petra galuˇsˇc´akov´a2, rui zhang3,douglas w. oard2, kathleen mckeown11columbia university, 2university of maryland, 3penn state university.
yc3384@columbia.edu, {kedzie, kathy}@cs.columbia.edu{srnair, petra, oard}@umd.edu, rmz5227@psu.edu.
abstract.
this paper proposes an approach to cross-language sentence selection in a low-resourceit uses data augmentation and neg-setting.
ative sampling techniques on noisy parallelsentence data to directly learn a cross-lingualembedding-based query relevance model.
re-this approach performs assults show thatwell as or better than multiple state-of-the-art machine translation + monolingual re-trieval systems trained on the same paral-lel data.
moreover, when a rationale train-ing secondary objective is applied to encour-age the model to match word alignment hintsfrom a phrase-based statistical machine trans-lation model, consistentimprovements areseen across three language pairs (english-somali, english-swahili and english-tagalog)over a variety of state-of-the-art baselines..1.introduction.
sentence-level query relevance prediction is impor-tant for downstream tasks such as query-focusedsummarization and open-domain question answer-ing; accurately pinpointing sentences containinginformation that is relevant to the query is criticalto generating a responsive summary/answer (e.g.,baumel et al.
(2016, 2018)).
in this work, we fo-cus on sentence-level query relevance predictionin a cross-lingual setting, where the query and sen-tence collection are in different languages and thesentence collection is drawn from a low-resourcelanguage.
our approach enables english speakers(e.g., journalists) to ﬁnd relevant information ex-pressed in local sources (e.g., local reaction to thepandemic and vaccines in somalia)..while we can use machine translation (mt) totranslate either the query or each sentence into acommon language, and then use a monolingual in-formation retrieval (ir) system to ﬁnd relevant sen-tences, work on probabilistic structured queries.
(psq) (darwish and oard, 2003) has shown thatthe performance of such mt+ir pipelines is hin-dered by errors in mt.
as is well known, completetranslation of the sentence collection is not neces-sary.
inspired by previous work (vuli´c and moens,2015), we go a step further and propose a simplecross-lingual embedding-based model that avoidstranslation entirely and directly predicts the rele-vance of a query-sentence pair (where the queryand sentence are in different languages)..for training, we treat a sentence as relevant to aquery if there exists a translation equivalent of thequery in the sentence.
our deﬁnition of relevance ismost similar to the lexical-based relevance used ingupta et al.
(2007) and baumel et al.
(2018) but ourquery and sentence are from different languages.
we frame the task as a problem of ﬁnding sen-tences that are relevant to an input query, and thus,we need relevance judgments for query-sentencepairs.
our focus, however, is on low-resource lan-guages where we have no sentence-level relevancejudgments with which to train our query-focusedrelevance model.
we thus leverage noisy paral-lel sentence collections previously collected fromthe web.
we use a simple data augmentation andnegative sampling scheme to generate a labeleddataset of relevant and irrelevant pairs of queriesand sentences from these noisy parallel corpora.
with this synthetic training set in hand, we canlearn a supervised cross-lingual embedding space.
while our approach is competitive with pipelinesof mt-ir, it is still sensitive to noise in the parallelsentence data.
we can mitigate the negative effectsof this noise if we ﬁrst train a phrase-based statis-tical mt (smt) model on the same parallel sen-tence corpus and use the extracted word alignmentsas additional supervision.
with these alignmenthints, we demonstrate consistent and signiﬁcant im-provements over neural and statistical mt+ir (niuet al., 2018; koehn et al., 2007; heaﬁeld, 2011),.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3881–3895august1–6,2021.©2021associationforcomputationallinguistics3881three strong cross-lingual embedding-based mod-els (bivec (luong et al., 2015), sid-sgns (levyet al., 2017), muse (lample et al., 2018)), a prob-abilistic occurrence model (xu and weischedel,2000), and a multilingual pretrained model xlm-roberta (conneau et al., 2020).
we refer to thissecondary training objective as rationale training,inspired by previous work in text classiﬁcation thatsupervises attention over rationales for classiﬁca-tion decisions (jain and wallace, 2019)..to summarize, our contributions are as follows.
we (i) propose a data augmentation and negativesampling scheme to create a synthetic training setof cross-lingual query-sentence pairs with binaryrelevance judgements, and (ii) demonstrate theeffectiveness of a supervised embedding-basedcross-lingual relevance (seclr) model trainedon this data for low-resource sentence selectiontasks on text and speech.
additionally, (iii) wepropose a rationale training secondary objective tofurther improve seclr performance, which wecall seclr-rt.
finally, (iv) we conduct trainingdata ablation and hubness studies that show ourmethod’s applicability to even lower-resource set-tings and mitigation of hubness issues (dinu andbaroni, 2015; radovanovi´c et al., 2010).
theseﬁndings are validated by empirical results of exper-iments in a low-resource sentence selection task,with english queries over sentence collections oftext and speech in somali, swahili, and tagalog..2 related work.
query-focused sentence selection sentence-level query relevance prediction is important forvarious downstream nlp tasks such as query-focused summarization (baumel et al., 2016, 2018;feigenblat et al., 2017) and open-domain questionanswering (chen et al., 2017; dhingra et al., 2017;kale et al., 2018).
such applications often dependon a sentence selection system to provide attentionsignals on which sentences to focus upon to gener-ate a query-focused summary or answer a question..cross-language sentence selection a commonapproach to cross-language sentence selection isto use mt to ﬁrst translate either the query or thesentence to the same language and then performstandard monolingual ir (nie, 2010).
the risk ofthis approach is that errors in translation cascade tothe ir system..as an alternative to generating full transla-tions, psq (darwish and oard, 2003) uses word-.
alignments from smt to obtain weighted queryin otherterm counts in the passage collection.
work, xu and weischedel (2000) use a 2-state hid-den markov model (hmm) to estimate the proba-bility that a passage is relevant given the query..cross-lingual wordembeddings cross-lingual embedding methods perform cross-lingualrelevance prediction by representing query andpassage terms of different languages in a sharedsemantic space (vuli´c and moens, 2015; litschkoet al., 2019, 2018; joulin et al., 2018).
bothsupervised approaches trained on parallel sentencecorpora (levy et al., 2017; luong et al., 2015)and unsupervised approaches with no paralleldata (lample et al., 2018; artetxe et al., 2018)have been proposed to train cross-lingual wordembeddings..our approach differs from previous cross-lingualword embedding methods in two aspects.
first,the focus of previous work has mostly been onlearning a distributional word representation wheretranslation across languages is primarily shaped bysyntactic or shallow semantic similarity; it has notbeen tuned speciﬁcally for cross-language sentenceselection tasks, which is the focus of our work..second, in contrast to previous supervised ap-proaches that train embeddings directly on a par-allel corpus or bilingual dictionary, our approachtrains embeddings on an artiﬁcial labeled datasetaugmented from a parallel corpus and directly rep-resents relevance across languages.
our data aug-mentation scheme to build a relevance model isinspired by boschee et al.
(2019), but we achievesigniﬁcant performance improvement by incorpo-rating rationale information into the embeddingtraining process and provide detailed comparisonsof performance with other sentence selection ap-proaches..trained rationale previous research has shownthat models trained on classiﬁcation tasks some-times do not use the correct rationale when makingpredictions, where a rationale is a mechanism of theclassiﬁcation model that is expected to correspondto human intuitions about salient features for the de-cision function (jain and wallace, 2019).
researchhas also shown that incorporating human rationalesto guide a model’s attention distribution can poten-tially improve model performance on classiﬁcationtasks (bao et al., 2018).
trained rationales havealso been used in neural mt (nmt); incorporat-.
3882ing alignments from smt to guide nmt attentionyields improvements in translation accuracy (chenet al., 2016)..3 methods.
we ﬁrst describe our synthetic training set gener-ation process, which converts a parallel sentencecorpus for mt into cross-lingual query-sentencepairs with binary relevance judgements for train-ing our seclr model.
following that, we detailour seclr model and ﬁnish with our method forrationale training with word alignments from smt..3.1 training set generation algorithm.
relevant query/sentence generation.
assume wehave a parallel corpus of bilingual sentence pairsequivalent in meaning.
let (e, s) be one such sen-tence pair, where e is in the query language (inour case, english) and s is in the retrieval collec-tion language (in our case, low-resource languages).
for every unigram q in e that is not a stopword,we construct a positive relevant sample by viewingq as a query and s as a relevant sentence.
becausesentences e and s are (approximately) equivalentin meaning, we know that there likely exists a trans-lation equivalent of q in the sentence s and so welabel the (q, s) pair as relevant (i.e.
r = 1)..for example, one english-somali sentence pairis e=“true president gaas attend meeting copen-hagen”, s=“ma runbaa madaxweyne gaas baaq-day shirka copenhegan” (stopwords removed).
byextracting unigrams from e as queries, we gener-ate the following positive examples: (q=“true”, s,r = 1), (q=“president”, s, r = 1), (q=“gaas”, s,r = 1), ..., (q=“copenhagen”, s, r = 1)..we generate the positive half of the training setby repeating the above process for every sentencepair in the parallel corpus.
we limit model trainingto unigram queries since higher order ngrams ap-pear fewer times and treating them independentlyreduces the risk of over-ﬁtting.
however, ourmodel processes multi-word queries during evalua-tion, as described in section 3.2..irrelevant query/sentence generation.
sincelearning with only positive examples is a challeng-ing task, we opt to create negative examples, i.e.
tuples (q, s, r = 0), via negative sampling.
foreach positive sample (q, s, r = 1), we randomlyselect another sentence pair (e(cid:48), s(cid:48)) from the par-allel corpus.
we then check whether s(cid:48) is relevantto q or not.
note that both the query q and sentence.
e(cid:48) are in the same language, so checking whether qor a synonym can be found in e(cid:48) is a monolingualtask.
if we can verify that there is no direct match orsynonym equivalent of q in e(cid:48) then by transitivityit is unlikely there exists a translation equivalent ins(cid:48), making the pair (q, s(cid:48)) a negative example.
toaccount for synonymy when we check for matches,we represent q and the words in e(cid:48) with pretrainedword embeddings.
let wq, wq(cid:48) ∈ rd be the embed-dings associated with q and the words q(cid:48) ∈ e(cid:48).
wejudge the pair (q, s(cid:48)) to be irrelevant (i.e.
r = 0)if:.
maxq(cid:48)∈e(cid:48).
cos-sim(wq, wq(cid:48)) ≤ λ1.
where λ1 is a parameter.
we manually tuned therelevance threshold λ1 on a small development setof query-sentence pairs randomly generated by thealgorithm, and set λ1 = 0.4 to achieve highest la-bel accuracy on the development set.
if (q, s(cid:48)) isnot relevant we add (q, s(cid:48), r = 0) to our synthetictraining set, otherwise we re-sample (e(cid:48), s(cid:48)) untila negative sample is found.
we generate one neg-ative sample for each positive sample to create abalanced dataset..for example, if we want to generate a negativeexample for the positive example (q=“meeting”,s=“ma runbaa madaxweyne gaas baaqday shirkacopenhegan”, r = 1), we randomly select anothersentence pair (e(cid:48)=“many candidates competingelections one hopes winner”, s(cid:48)=“musharraxiintiro badan sidoo u tartamaysa doorashada wux-uuna mid kasta rajo qabaa guusha inay dhinaci-isa ahaato”) from the parallel corpus.
to checkwhether q=“meeting” is relevant to s(cid:48), by transitiv-ity it sufﬁces to check whether q=“meeting” or asynonym is present in e(cid:48), a simpler monolingualtask.
if q is irrelevant to s(cid:48), we add (q, s(cid:48), r = 0)as a negative example..3.2 cross-lingual relevance model.
we propose seclr, a model that directly makesrelevance classiﬁcation judgments for queries andsentences of different languages without mt as anintermediate step by learning a cross-lingual em-bedding space between the two languages.
notonly should translation of equivalent words in ei-ther language map to similar regions in the em-bedding space, but dot products between queryand sentence words should be correlated with theprobability of relevance.
we assume the train-ing set generation process (section 3.1) providesus with a corpus of n query-sentence pairs along.
3883with their corresponding relevance judgements, i.e.
d = {(qi, si, ri)}|ni=1.
we construct a bilingualvocabulary v = vq ∪ vs and associate with it amatrix w ∈ rd×|v| where wx = w·,x is the wordembedding associated with word x ∈ v..when the query is a unigram q (which is trueby design in our training data d), we model theprobability of relevance to a sentence s as:(cid:19).
(cid:18).
p(r = 1|q, s; w ) = σ.w(cid:124).
q ws.
maxs∈s.
where σ denotes the logistic sigmoid (σ(x) =1/ (1 + exp(−x)))..in our evaluation setting, the query is very often aphrase q = [q1, .
.
.
, q|q|].
in this case, we requireall query words to appear in a sentence in order fora sentence to be considered as relevant.
thus, wemodify our relevance model to be:.
p(r = 1|q, s; w ) = σ.
(cid:18).
minq∈q.
maxs∈s.
(cid:19).
w(cid:124).
q ws.
our only model parameter is the embedding matrixw which is initialized with pretrained monolingualword embeddings and learned via minimization ofthe cross entropy of the relevance classiﬁcationtask:.
lrel = − log p(r|q, s; w ).
3.3 guided alignment with rationale.
training.
we can improve seclr by incorporating addi-tional alignment information as a secondary train-ing objective, yielding seclr-rt.
our intuition is(cid:124)s wqthat after training, the word ˆs = arg maxs∈s wshould correspond to a translation of q. however,it is possible that ˆs simply co-occurs frequentlywith the true translation in our parallel data but itsassociation is coincidental or irrelevant outside thetraining contexts.
we use alignment informationto correct for this.
we run two smt word align-ment models, giza++ (och and ney, 2003) andberkeley aligner (haghighi et al., 2009), on theorginal parallel sentence corpus.
the two result-ing alignments are concatenated as in zbib et al.
(2019) to estimate a unidirectional probabilisticword translation matrix a ∈ [0, 1]|vq|×|vs |, suchthat a maps each word in the query language vo-cabulary to a list of document language words withdifferent probabilities, i.e.
aq,s is the probabilityof translating q to s and (cid:80).
aq,s = 1..s∈vs.
for each relevant training sample, i.e.
(q, s, r =1), we create a rationale distribution ρ ∈ [0, 1]|s|.
which is essentially a re-normalization of possiblequery translations found in s and represents ourintuitions about which words s ∈ s that q shouldbe most similar to in embedding space, i.e..ρs =.
(cid:80).
aq,ss(cid:48)∈s aq,s(cid:48).
..for s ∈ s. we similarly create a distribution underour model, α ∈ [0, 1]|s|, where(cid:124)q ws)(cid:124)q ws(cid:48)).
exp (ws(cid:48)∈s exp (wfor s ∈ s. to encourage α to match ρ, we im-pose a kullback–leibler (kl) divergence penalty,denoted as:.
αs =.
(cid:80).
lrat = kl(ρ(cid:107)α).
to our overall loss function.
the total loss for asingle positive sample then will be a weighted sumof the relevance classiﬁcation objective and the kldivergence penalty, i.e..l = lrel + λ2lrat.
where λ2 is a relative weight between the classiﬁ-cation loss and rationale similarity loss..note that we do not consider rationale loss forthe following three types of samples: negative sam-ples, positive samples where the query word is notfound in the translation matrix, and positive sam-ples where none of the translations of the query inthe matrix are present in the source sentence..4 experiments.
4.1 dataset generation from parallel corpus.
the parallel sentence data for training our pro-posed method and all baselines includes the par-allel data provided in the build collections ofboth the material1 and lorelei (christian-son et al., 2018) programs for three low resourcelanguages: somali (so), swahili (sw), and taga-log (tl) (each paired with english).
additionally,we include in our parallel corpus publicly availableresources from opus (tiedemann, 2012), and lex-icons mined from panlex (kamholz et al., 2014)and wiktionary.2 statistics of these parallel cor-pora and augmented data are shown in table 1 andtable 2, respectively.
other preprocessing detailsare in appendix a..1https://www.iarpa.gov/index.php/.
research-programs/material.
2https://dumps.wikimedia.org/.
3884en-so.
en-sw.en-tl.
# sents.
en tkn.
lr tkn..69,8181,827,8261,804,428.
251,9281,946,5561,848,184.
232,1662,553,4392,682,076.table 1: parallel corpus statistics; “en tkn.” refers tonumber of english tokens in the parallel corpus; “lrtkn.” refers to number of low-resource tokens (somali,swahili, tagalog) in the parallel corpus..lang.
pair augmented dataset size.
en-soen-swen-tl.
1,649,4842,014,8382,417,448.table 2: augmented dataset statistics; “augmenteddataset size” refers to total number of positive and neg-ative query-sentence samples in the augmented dataset..4.2 query sets and evaluation sets.
we evaluate our sentence-selection model on en-glish (en) queries over three collections in so,sw, and tl recently made available as part ofthe iarpa material program.
in contrast toour training data which is synthetic, our evalua-tion datasets are human-annotated for relevancebetween real-world multi-domain queries and doc-uments.
for each language there are three parti-tions (analysis, dev, and eval), with the formertwo being smaller collections intended for systemdevelopment, and the latter being a larger evalua-tion corpus.
in our main experiments we do notuse analysis or dev for development and so we re-port results for all three (the ground truth relevancejudgements for the tl eval collection have notbeen released yet so we do not report eval for tl).
see table 3 for evaluation statistics.
all queries aretext.
the speech documents are ﬁrst transcribedwith an asr system (ragni and gales, 2018), andthe 1-best asr output is used in the sentence selec-tion task.
examples of the evaluation datasets areshown in appendix b. we refer readers to rubino(2020) for further details about material testcollections used in this work..while our model and baselines work at thesentence-level, the material relevance judge-ments are only at the document level.
followingprevious work on evaluation of passage retrieval,we aggregate our sentence-level relevance scoresto obtain document-level scores (kaszkiel and zo-.
bel, 1997; wade and allan, 2005; fan et al., 2018;inel et al., 2018; akkalyoncu yilmaz et al., 2019).
given a document d = [s1, .
.
.
, s|d|], which is asequence of sentences, and a query q, followingliu and croft (2002) we assign a relevance scoreby:.
ˆr = maxs∈d.
p(r = 1|q, s; w ).
4.3 experiment settings.
we initialize english word embeddings withword2vec (mikolov et al., 2013), and initializeso/sw/tl word embeddings with fasttext (graveet al., 2018).
for training we use a sparseadam(kingma and ba, 2015) optimizer with learningrate 0.001. the hyperparameter λ2 in section 3.3is set to be 3 so that lrel and λ2lrat are approx-imately on the same scale during training.
moredetails on experiments are included in appendix c..4.4 baselines.
cross-lingual word embeddings.
we compareour model with three other cross-lingual embed-ding methods, bivec (luong et al., 2015), muse(lample et al., 2018), and sid-sgns (levy et al.,2017).
bivec and sid-sgns are trained using thesame parallel sentence corpus as the dataset gener-ation algorithm used to train seclr; thus, bivecand sid-sgns are trained on parallel sentenceswhile seclr is trained on query-sentence pairsderived from that corpus.
we train muse with thebilingual dictionary from wiktionary that is usedin previous work (zhang et al., 2019).
the so-en,sw-en and tl-en dictionaries have 7633, 5301,and 7088 words respectively.
given embeddingsw (cid:48) from any of these methods, we compute sen-tence level relevance scores similarly to our modelbut use the cosine similarity:.
p(r = 1|q, s; w (cid:48)) = minq∈q.
maxs∈s.
cos-sim(w(cid:48).
s, w(cid:48)q).
since these models are optimized for this compar-ison function (luong et al., 2015; lample et al.,2018; levy et al., 2017).
document aggregationscoring is handled identically to our seclr mod-els (see section 4.2)..mt+ir.
we also compare to a pipeline of nmt(niu et al., 2018) with monolingual ir and apipeline of smt 3 with monolingual ir.
both mtsystems are trained on the same parallel sentence.
3we used moses (koehn et al., 2007) and kenlm for the.
language model (heaﬁeld, 2011)..3885lang..analysis.
dev.
#q #t.#s.#q #t.#s.#q.somaliswahilitagalog.
300300300.
338316291.
142155171.
300300300.
482449460.
213217244.
13001300/.
eval.
#t.1071710435/.
#s.46424310/.
table 3: material dataset statistics: “#q” refers to the number of queries; “#t” refers to the number of textdocuments; “#s” refers to the number of speech documents.
there is no tagalog eval dataset..analysis.
eval.
analysis.
eval.
somali.
dev.
s.16.224.39.9.
12.511.2.
16.6.
11.0.
24.428.4.t.15.022.210.3.
21.119.1.
25.0.
10.7.
23.029.5.s.12.016.016.5.
13.416.8.
20.7.
12.4.
17.422.0.method.
bivecsid-sgnsmuse.
nmt+irsmt+ir.
psq.
xlm-r.t.19.625.59.9.
18.817.4.
27.0.
13.9.seclrseclr-rt.
27.835.4†.
t.4.210.21.9.
9.49.1.
11.1.
2.3.s.4.59.12.0.
8.48.3.
8.6.
2.9.t.23.938.827.8.
23.725.5.
39.0.
23.3.s.22.736.324.5.
24.928.6.
36.6.
29.0.swahili.
dev.
t.21.933.727.3.
26.827.1.
38.0.
20.0.
40.339.6.s.21.630.328.8.
26.725.2.
38.6.
29.7.
38.145.4.t.6.216.29.5.
15.315.4.
20.4.
6.2.s.4.813.68.1.
11.413.3.
13.8.
7.5.
7.713.1†.
7.411.2†.
43.848.3†.
37.948.1†.
16.022.7†.
13.117.7†.
table 4: document-level map scores for text (t) and speech (s) for somali and swahili.
† indicates signiﬁcanceat the p = 0.01 level between seclr-rt and the best baseline..analysis.
dev.
method.
t.s.t.s.bivecsid-sgnsmuse.
nmt+irsmt+ir.
36.744.627.4.
37.744.4.
41.443.926.5.
42.352.7.
39.640.926.0.
32.639.3.
26.941.716.5.
37.535.3.psq.
51.6.
55.0.
52.7.
44.7.seclr46.7seclr-rt 61.1.
45.055.5.
49.359.0.
33.945.7.table 5: document-level map scores for text (t) andspeech (s) for tagalog..data as our seclr models.
the 1-best outputfrom each mt system is then scored with indri(strohman et al., 2005) to obtain relevance scores.
details of nmt and smt systems are included inappendix c.2..psq.
to implement the psq model of darwishand oard (2003), we use the same alignment ma-trix as in rationale training (see section 3.3) ex-.
q∈vq.
cept that here we normalize the matrix such that∀s ∈ vd, (cid:80)aq,s = 1. additionally, we em-bed the psq scores into a two-state hidden markovmodel which smooths the raw psq scores witha background unigram language model (xu andweischedel, 2000).
the psq model scores eachsentence and then aggregates the scores to docu-ment level as in section 4.2..multilingual xlm-roberta.
we compare ourmodel to the cross-lingual model xlm-roberta(conneau et al., 2020), which in previous researchhas been shown to have better performance on low-resource languages than multilingual bert (de-vlin et al., 2019).
we use the hugging face imple-mentation (wolf et al., 2019) of xlm-roberta(base).
we ﬁne-tuned the model on the sameaugmented dataset of labeled query-sentence pairsas the seclr models, but we apply the xlm-roberta tokenizer before feeding examples to themodel.
we ﬁne-tuned the model for four epochs us-ing an adamw optimizer (loshchilov and hutter,2019) with learning rate 2 × 10−5.
since xlm-roberta is pretrained on somali and swahili butnot tagalog, we only compare our models to xlm-roberta on somali and swahili..38865 results and discussion.
6 training data ablation study.
we report mean average precision (map) of ourmain experiment in table 4 (so & sw) and table 5(tl).
overall, we see that seclr-rt consistentlyoutperforms the other baselines in 15 out of 16 set-tings, and in the one case where it is not the best(sw dev text), seclr is the best.
seclr-rt isstatistically signiﬁcantly better than the best base-line on all eval partitions.4 since analysis/dev arerelatively small, only three out of 12 analysis/devsettings are signiﬁcant.
the differences betweenseclr and seclr-rt can be quite large (e.g., aslarge as 70.4% relative improvement on so evaltext), suggesting that the rationale training providesa crucial learning signal to the model..bivec and muse under-perform both of ourmodel variants across all test conditions, suggest-ing that for the sentence selection task the rele-vance classiﬁcation objective is more importantthan learning monolingual distributional signals.
curiously, sid-sgns is quite competitive with se-clr, beating it on so and sw eval (both modali-ties) and tl dev speech (ﬁve out of 16 test condi-tions) and is competitive with the other baselines.
again, the rationale training proves more effectiveas sid-sgns never surpasses seclr-rt..while mt+ir is a competitive baseline, it isconsistently outperformed by psq across all testconditions, suggesting that in low-resource set-tings it is not necessary to perform full translationto achieve good sentence selection performance.
smt, psq, and seclr-rt all make use of thesame word-alignment information but only smtgenerates translations, adding additional evidenceto this claim.
psq and seclr are close in perfor-mance on analysis and dev sets with seclr ekingout a slight advantage on seven of 12 anaylsis/devset conditions..on the larger eval partitions, it becomes clearerthat psq is superior to seclr, suggesting thatthe relevance classiﬁcation objective is not as in-formative as word alignment information.
the rel-evance classiﬁcation and trained rationale objec-tives capture slightly different information it seems;seclr-rt, which uses both, out-performs psqacross all 16 test conditions..in section 5, we have shown that seclr-rt con-sistently out-performs all baselines across all lan-guages.
since this work targets cross-languagesentence selection in a low-resource setting, weperform a training data ablation study to under-stand how training data size affects effectiveness.
we performed the ablation study for our twomodels seclr and seclr-rt, and the twostrongest baseline methods psq and sid-sgns.
to simulate further the scenario of data scarcity, wesub-sampled our parallel corpus uniformly at ran-dom for 5%, 10%, 25%, 50% of the sentence pairsof the original corpus.
each sentence pair in theparallel corpus is sampled with equal probabilityregardless of sentence length.
for consistency, foreach sample size, the same sampled parallel cor-pus is used across all models.
the word alignmentprobability matrix used by psq and seclr-rt isgenerated from the same sampled corpus.
sincewe tune the vocabulary size on the dev set, forfair comparison we only report map scores on theanalysis and eval sets..we plot map scores of the four models as afunction of percentage of data sampled in figure 1.overall, we see that seclr-rt consistently out-performs other baselines across all sample sizesin 9 out of 10 settings, and in the one case whereit does not yield consistent improvement (tagaloganalysis speech), seclr-rt achieves comparableperformance to psq..in the low-resource setting when the samplesize is 5% or 10%, seclr consistently under-performs other models, conﬁrming our observa-tion that seclr is sensitive to noise and vulnera-ble to learning co-occurrences of word pairs thatare in fact irrelevant.
when the sample size is5% or 10%, psq consistently achieves better per-formance than sid-sgns and seclr (althoughstill under-performing seclr-rt), indicating thatalignment-based methods are more robust to noiseand especially useful when data is extremely scarce.
the fact that seclr-rt consistently out-performsseclr by a wide margin for small sample sizesindicates the necessity and effectiveness of incor-porating alignment-based information into seclrto improve the robustness of the model and learnmore precise alignments..4we use a two-tailed paired t-test with bonferroni correc-tion for multiple comparisons at p < 0.01 for all signiﬁcancetests..3887figure 1: ablation study results of model performances as a function of sub-sampling percentages.
note that thex-coordinate uses the log scale for better illustration of low-resource cases..7 alleviating the hubness problem.
in this section, we show that by incorporatingalignment information through rationale training,seclr-rt signiﬁcantly alleviates the hubnessproblem present in the trained cross-lingual embed-ding space produced by seclr.
previous researchon cross-lingual word embeddings has observedthat a high-dimensional representation space witha similarity-based metric often induces a hub struc-ture (dinu and baroni, 2015).
speciﬁcally, in ahigh-dimensional space (e.g., a cross-lingual wordembedding space) deﬁned with a pairwise simi-larity metric (e.g., cosine similarity), there exist afew vectors that are the nearest neighbors of manyother vectors.
such vectors are referred to as “hubs.”the hub structure is problematic in ir since the hubvectors are often wrongly predicted as relevant andsimilar in meaning to queries that are in fact irrele-vant (radovanovi´c et al., 2010)..let vq and vs be the embedding spaces for thequery and sentence collection languages respec-tively.
we deﬁne the size of the neighborhood ofembeddings around y ∈ vs as.
nk(y) = |{x ∈ vq|rx(y) ≤ k}|.
where rx(y) is the rank of y if we order vs bysimilarity to x from highest to lowest, and k is a.model.
somali swahili tagalog.
seclrseclr-rt.
29.366.78.
54.9814.73.
43.2911.73.table 6: sn10 scores of seclr and seclr-rt respec-tively on somali, swahili and tagalog..positive integer.
a large value of nk(y) indicatesthat y is similar to many x ∈ vq, and suggests thaty is a likely hub in embedding space..following radovanovi´c et al.
(2010), we usesn10 = ey∈vs [(n10(y) − µ)3/σ3] to measure theskewness of the distribution of n10, where µ and σrefer to the mean and standard deviation of n10(y)respectively.
since cosine similarity is more fre-quently used as the similarity metric in hubnessanalysis, we re-train seclr and seclr-rt byreplacing the dot product similarity metric withcosine similarity and still get performance compa-rable to table 4 and table 5..we report sn10 scores for seclr and seclr-rt respectively in table 6. we see that seclr-rt consistently has lower sn10 value comparedto seclr on all three languages, indicating thatthe extra alignment information incorporated withrationale training is helpful in reducing hubness..38885102550100percent of data used102030mapsomali analysis text5102550100percent of data used10152025mapsomali analysis speech5102550100percent of data used510mapsomali eval text5102550100percent of data used2.55.07.510.0mapsomali eval speech5102550100percent of data used20304050mapswahili analysis text5102550100percent of data used203040mapswahili analysis speech5102550100percent of data used5101520mapswahili eval text5102550100percent of data used51015mapswahili eval speech5102550100percent of data used2030405060maptagalog analysis text5102550100percent of data used20304050maptagalog analysis speechpsqsid-sgnsseclrseclr-rt8 conclusion.
in this work, we presented a supervised cross-lingual embedding-based query relevance model,seclr, for cross-language sentence selection andalso applied a rationale training objective to fur-ther increase model performance.
the resultingseclr-rt model outperforms a range of base-line methods on a cross-language sentence selec-tion task.
study of data ablation and hubness fur-ther indicate our model’s efﬁcacy in handling low-resource settings and reducing hub structures.
infuture work, we hope to apply our sentence-levelquery relevance approach to downstream nlp taskssuch as query-focused summarization and open-domain question answering..acknowledgements.
this research is based upon work supported in partby the ofﬁce of the director of national intelli-gence (odni), intelligence advanced researchprojects activity (iarpa), via contract #fa8650-17-c-9117.
the views and conclusions containedherein are those of the authors and should not be in-terpreted as necessarily representing the ofﬁcialpolicies, either expressed or implied, of odni,iarpa, or the u.s. government.
the u.s. gov-ernment is authorized to reproduce and distributereprints for governmental purposes not withstand-ing any copyright annotation therein..references.
zeynep akkalyoncu yilmaz, wei yang, haotianzhang, and jimmy lin.
2019. cross-domain mod-eling of sentence-level evidence for document re-trieval.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3490–3496, hong kong, china.
association forcomputational linguistics..mikel artetxe, gorka labaka, and eneko agirre.
2018.a robust self-learning method for fully unsuper-vised cross-lingual mappings of word embeddings.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 789–798, melbourne, aus-tralia.
association for computational linguistics..yujia bao, shiyu chang, mo yu, and regina barzilay.
2018. deriving machine attention from human ra-tionales.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 1903–1913, brussels, belgium.
associationfor computational linguistics..tal baumel, raphael cohen, and michael elhadad.
2016. topic concentration in query focused sum-marization datasets.
in proceedings of the thirtiethaaai conference on artiﬁcial intelligence, febru-ary 12-17, 2016, phoenix, arizona, usa, pages2573–2579.
aaai press..tal baumel, matan eyal, and michael elhadad.
2018.query focused abstractive summarization: incor-porating query relevance, multi-document cover-age, and summary length constraints into seq2seqmodels.
corr, abs/1801.07704..elizabeth boschee, joel barry, jayadev billa, mar-jorie freedman, thamme gowda, constantine lig-nos, chester palen-michel, michael pust, ban-riskhem kayang khonglah, srikanth madikeri,jonathan may, and scott miller.
2019. saral: alow-resource cross-lingual domain-focused in-formation retrieval system for effective rapid doc-in proceedings of the 57th annualument triage.
meeting of the association for computational lin-guistics: system demonstrations, pages 19–24, flo-rence, italy.
association for computational linguis-tics..danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-domain questions.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879, vancouver, canada.
association for computa-tional linguistics..wenhu chen, evgeny matusov, shahram khadivi, andjan-thorsten peter.
2016. guided alignment train-ing for topic-aware neural machine translation.
corr, abs/1607.01628..caitlin christianson, jason duncan, and boyan a.onyshkevych.
2018. overview of the darpalorelei program.
machine translation, 32(1-2):3–9..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..kareem darwish and douglas w. oard.
2003. proba-bilistic structured query methods.
in proceedingsof the 26th annual international acm sigir con-ference on research and development in informaionretrieval, sigir ’03, page 338–344, new york, ny,usa.
association for computing machinery..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conference.
3889of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..bhuwan dhingra, kathryn mazaitis, and william w.cohen.
2017.for ques-tion answering by search and reading.
corr,abs/1707.03904..quasar: datasets.
georgiana dinu and marco baroni.
2015. improvingzero-shot learning by mitigating the hubness prob-lem.
in 3rd international conference on learningrepresentations, iclr 2015, san diego, ca, usa,may 7-9, 2015, workshop track proceedings..yixing fan, jiafeng guo, yanyan lan, jun xu, chengx-iang zhai, and xueqi cheng.
2018. modeling di-verse relevance patterns in ad-hoc retrieval.
inthe 41st international acm sigir conference onresearch & development in information retrieval,sigir 2018, ann arbor, mi, usa, july 08-12, 2018,pages 375–384.
acm..guy feigenblat, haggai roitman, odellia boni, anddavid konopnicki.
2017. unsupervised query-focused multi-document summarization using thethecross entropy method.
40th international acm sigir conference on re-search and development in information retrieval,sigir’17, page 961–964, new york, ny, usa.
as-sociation for computing machinery..in proceedings of.
edouard grave, piotr bojanowski, prakhar gupta, ar-mand joulin, and tom´as mikolov.
2018. learningword vectors for 157 languages.
in proceedings ofthe eleventh international conference on languageresources and evaluation, lrec 2018, miyazaki,japan, may 7-12, 2018. european language re-sources association (elra)..surabhi gupta, ani nenkova, and dan jurafsky.
2007.measuring importance and query relevance intopic-focused multi-document summarization.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 193–196, prague, czech republic.
as-sociation for computational linguistics..aria haghighi, john blitzer, john denero, and danklein.
2009. better word alignments with super-vised itg models.
in proceedings of the joint con-ference of the 47th annual meeting of the acl andthe 4th international joint conference on naturallanguage processing of the afnlp, pages 923–931,suntec, singapore.
association for computationallinguistics..kenneth heaﬁeld.
2011. kenlm: faster and smallerin proceedings of thelanguage model queries.
sixth workshop on statistical machine translation,pages 187–197, edinburgh, scotland.
associationfor computational linguistics..oana.
inel, giannis haralabopoulos, dan li,christophe van gysel, zolt´an szl´avik, elenasimperl, evangelos kanoulas, and lora aroyo.
2018. studying topical relevance with evidence-in proceedings of the 27thbased crowdsourcing.
acm international conference on informationand knowledge management, cikm ’18, page1253–1262, new york, ny, usa.
association forcomputing machinery..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556, minneapolis, minnesota.
association for computational linguistics..armand joulin, piotr bojanowski, tomas mikolov,herv´e j´egou, and edouard grave.
2018. loss intranslation: learning bilingual word mapping within proceedings of the 2018a retrieval criterion.
conference on empirical methods in natural lan-guage processing, pages 2979–2984, brussels, bel-gium.
association for computational linguistics..marcin junczys-dowmunt.
2018. dual conditionalcross-entropy filtering of noisy parallel corpora.
in proceedings of the third conference on machinetranslation: shared task papers, pages 888–895,belgium, brussels.
association for computationallinguistics..s. kale, a. kulkarni, r. patil, y. haribhakta, k. bhat-tacharjee, s. mehta, s. mithran, and a. kumar.
2018.open-domain question answering using featureencoded dynamic coattention networks.
in 2018international conference on advances in comput-ing, communications and informatics (icacci),pages 1058–1062..david kamholz, jonathan pool, and susan m. colow-ick.
2014. panlex: building a resource for pan-in proceedings of thelingual lexical translation.
ninth international conference on language re-sources and evaluation, lrec 2014, reykjavik, ice-land, may 26-31, 2014, pages 3145–3150.
europeanlanguage resources association (elra)..marcin kaszkiel and justin zobel.
1997. passage re-trieval revisited.
in proceedings of the 20th annualinternational acm sigir conference on researchand development in information retrieval, sigir’97, page 178–185, new york, ny, usa.
associa-tion for computing machinery..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,.
3890richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
in proceedings of the 45th annual meeting of theassociation for computational linguistics compan-ion volume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..guillaume lample, alexis conneau, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018.word translation without parallel data.
in 6th inter-national conference on learning representations,iclr 2018, vancouver, bc, canada, april 30 - may3, 2018, conference track proceedings.
openre-view.net..omer levy, anders søgaard, and yoav goldberg.
2017.a strong baseline for learning cross-lingual wordembeddings from sentence alignments.
proceed-ings of the 15th conference of the european chap-ter of the association for computational linguistics:volume 1, long papers..robert litschko, goran glavaˇs, ivan vulic, and lauraevaluating resource-lean cross-dietz.
2019.lingual embedding models in unsupervised re-in proceedings of the 42nd internationaltrieval.
acm sigir conference on research and devel-opment in information retrieval, sigir’19, page1109–1112, new york, ny, usa.
association forcomputing machinery..robert litschko, goran glavaˇs, simone paoloponzetto, and ivan vuli´c.
2018. unsupervisedcross-lingual information retrieval using mono-lingual data only.
the 41st international acm si-gir conference on research & development in in-formation retrieval..xiaoyong liu and w. bruce croft.
2002. passageretrieval based on language models.
in proceed-ings of the eleventh international conference on in-formation and knowledge management, cikm ’02,page 375–382, new york, ny, usa.
association forcomputing machinery..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..thang luong, hieu pham, and christopher d. man-ning.
2015. bilingual word representations withmonolingual quality in mind.
in proceedings of the1st workshop on vector space modeling for naturallanguage processing, pages 151–159, denver, col-orado.
association for computational linguistics..tom´as mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-in 1st international con-tations in vector space.
ference on learning representations, iclr 2013,scottsdale, arizona, usa, may 2-4, 2013, workshoptrack proceedings..jian-yun nie.
2010. cross-language information re-synthesis lectures on human language.
trieval.
technologies, 3:1–125..xing niu, michael denkowski, and marine carpuat.
2018. bi-directional neural machine translationwith synthetic parallel data.
proceedings of the 2ndworkshop on neural machine translation and gen-eration..franz josef och and hermann ney.
2003. a system-atic comparison of various statistical alignmentmodels.
computational linguistics, 29(1):19–51..milos radovanovi´c, alexandros nanopoulos, and mir-jana ivanovi´c.
2010. on the existence of obstinateresults in vector space models.
in proceedings ofthe 33rd international acm sigir conference onresearch and development in information retrieval,sigir ’10, page 186–193, new york, ny, usa.
as-sociation for computing machinery..anton ragni and mark gales.
2018. automatic speechrecognition system development in the “wild”.
inproc.
interspeech 2018, pages 2217–2221..prajit ramachandran, peter liu, and quoc le.
2017.unsupervised pretraining for sequence to sequencelearning.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 383–391, copenhagen, denmark.
associationfor computational linguistics..carl rubino.
2020. the effect of linguistic parame-in proceedings of theters in clir performance.
workshop on cross-language search and summa-rization of text and speech (clssts2020), pages 1–6, marseille, france.
european language resourcesassociation..trevor strohman, donald metzler, howard turtle, andindri: a language model-w bruce croft.
2005.based search engine for complex queries.
in pro-ceedings of the international conference on intelli-gent analysis, volume 2, pages 2–6.
citeseer..j¨org tiedemann.
2012. parallel data, tools and in-terfaces in opus.
in proceedings of the eighth in-ternational conference on language resources andevaluation, lrec 2012, istanbul, turkey, may 23-25, 2012, pages 2214–2218.
european language re-sources association (elra)..ivan vuli´c and marie-francine moens.
2015. monolin-gual and cross-lingual information retrieval mod-els based on (bilingual) word embeddings.
in pro-ceedings of the 38th international acm sigir con-ference on research and development in informa-tion retrieval, sigir ’15, page 363–372, new york,ny, usa.
association for computing machinery..courtney wade and james allan.
2005. passage re-.
trieval and evaluation.
technical report..3891thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
corr, abs/1910.03771..jinxi xu and ralph weischedel.
2000. cross-lingualinformation retrieval using hidden markov mod-els.
in proceedings of the 2000 joint sigdat con-ference on empirical methods in natural languageprocessing and very large corpora: held in con-junction with the 38th annual meeting of the asso-ciation for computational linguistics - volume 13,emnlp ’00, page 95–103, usa.
association forcomputational linguistics..rabih zbib, lingjun zhao, damianos karakos,william hartmann,jay deyoung, zhongqianghuang, zhuolin jiang, noah rivkin, le zhang,richard schwartz, and john makhoul.
2019. neural-network lexical translation for cross-lingual irthefrom text and speech.
42nd international acm sigir conference on re-search and development in information retrieval,sigir’19, page 645–654, new york, ny, usa.
as-sociation for computing machinery..in proceedings of.
rui zhang, caitlin westerﬁeld, sungrok shim, gar-rett bingham, alexander fabbri, william hu, nehaverma, and dragomir radev.
2019.improvinglow-resource cross-lingual document retrievalby reranking with deep bilingual representations.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics..3892a extra training dataset details.
when we train seclr and seclr-rt via dataaugmentation, we randomly split the parallel cor-pus into train set (96%), validation set (3%) andtest set (1%).
we then use the dataset augmenta-tion technique introduced in section 3.1 to gen-erate positive and negative samples for each set.
augmenting the dataset upon the split corpus al-lows us to achieve more independence betweentrain/validation/test set compared to splitting thedataset augmented on the entire parallel corpus.
note that we only use the validation set for earlystopping but we do not tune hyperparameters withthe validation set..we preprocess the parallel corpus, the query col-lection and the sentence collection with the mosestoolkit (koehn et al., 2007).
the same preprocess-ing steps are used for all four languages (english,somali, swahili, tagalog).
first, we use mosespuncutation normalizer to normalize the raw text.
second, we use the moses tokenizer to tokenize thenormalized text.
finally, we remove the diacriticsin the tokenized text as a cleaning step..b examples of evaluation data.
in this section we demonstrate some examplesfrom the material dataset used for evaluation.
example queries include: “evidence”, “humanrights”, “chlorine”, “academy”, “ratify”, “consti-tution”, “carnage” and “kenya”.
on average only0.13% of the documents in the eval collection arerelevant to each query, which makes the task hard.
here are two examples from somali analysistext.
because the documents are long, here weonly include the relevant segment of a long rele-vant document.
in the ﬁrst example, the englishquery is “contravention” and the relevant segmentof a long relevant document (translated from so-mali to english by human) is “the security forcescaptured military equipment coming into the coun-try illegally.” this segment is relevant to the querybecause of the word “illegally”..here is another example where the the englishquery is “integrity”.
the relevant segment of a longrelevant document (translated from somali to en-glish by human) is “hargeisa (dawan) - ahmedmohamed diriye (nana) the member of parliamentwho is part of the somaliland house of represen-tatives has accused the opposition parties (wad-dani and ucid) of engaging in acts of nationaldestruction, that undermines the existence and.
sovereignty of the country of somaliland.” thissegment is relevant to the query because of theword “sovereignty”..since there are multiple ways to translate a wordand since mt performance is relatively poor in low-resource settings, the task is far more challengingthan a simple lexical match between queries andtranslated documents..c extra experimental details.
in this section we include extra implementation andexperiment details that are not included in the mainpaper.
information already included in the mainpaper are not repeated here for conciseness..c.1 model and training details.
we train our seclr and seclr-rt models ontesla v100 gpus.
each model is trained on asingle gpu.
we report training time of seclrand seclr-rt on somali, swahili and tagalog intable 7..somali swahili tagalog.
seclrseclr-rt.
77179.
112254.
124319.table 7: training time of seclr and seclr-rt onsomali, swahili and tagalog respectively (in minutes)..as is discussed in section 3.2, the only trainablemodel parameters of seclr and seclr-rt arethe word embedding matrices.
thus, seclr andseclr-rt have the same number of model param-eters.
we report the number of trainable parametersof both models on somali, swahili and tagalog intable 8..somali.
swahili tagalog.
# params..14.03m 22.31m 21.35m.
table 8: number of trainable model parameters ofseclr/seclr-rt on somali, swahili and tagalog.
“m” stands for million..we used mean average precision (map) as theevaluation metric in this work.
we use the fol-lowing implementation to compute map: https://trec.nist.gov/trec_eval/..c.2 mt baseline details.
for nmt we train bidirectional mt systems with a6-layer transformer architecture with model size of.
3893somali.
swahili.
tagalog.
analysis.
dev.
analysis.
dev.
analysis.
dev.
method.
t.s.t.s.t.s.t.s.t.s.t.s.with lstm 16.327.8no lstm.
14.524.4.
11.923.0.
12.017.4.
27.543.8.
27.037.9.
19.540.3.
25.138.1.
29.746.7.
29.745.0.
23.049.3.
27.133.9.table 9: document-level map scores for text (t) and speech (s) of the seclr model with and without lstm..somali.
swahili.
tagalog.
analysis.
dev.
analysis.
dev.
analysis.
dev.
embed.
init..t.s.t.s.t.s.t.s.t.s.t.s.cross-lingualmonolingual.
35.335.4.
27.528.4.
31.129.5.
23.222.0.
48.848.3.
41.148.1.
42.539.6.
41.645.4.
56.361.1.
51.155.5.
53.859.0.
45.345.7.table 10: document-level map scores for text (t) and speech (s) of the seclr-rt model with monolingual orcross-lingual (sid-sgns) word embedding initialization..512, feed-forward network size of 2048, 8 attentionheads, and residual connections.
we adopt layernormalization and label smoothing.
we tie theoutput weight matrix with the source and targetembeddings.
we use adam optimizer with a batchsize of 2048 words.
we checkpoint models every1000 updates.
training stops after 20 checkpointswithout improvement.
during inference, the beamsize is set to 5..our smt system uses the following feature func-tions: phrase translation model, distance-basedreordering model, lexicalized reordering model,5-gram language model on the target side, wordpenalty, distortion, unknown word penalty andphrase penalty..we use backtranslation in earlier versions ofmt systems.
following previous work (niu et al.,2018), we train a bidirectional nmt model thatbacktranslates source or target monolingual datawithout an auxiliary model.
this backtranslation-based model was the state-of-the-art mt modelon somali and swahili when the above paper ispublished..later, we discover that decoder pretraining withmonolingual data achieves better performance com-pared to backtranslation.
the decoder pretrainingscheme we use now is most similar to the paperby ramachandran et al.
(2017), where the authorsshow state-of-the-art results on the wmt englishto german translation task with decoder pretrain-ing..there is no wmt benchmark for somali,swahili or tagalog, but we use state-of-the-art tech-niques in our mt systems.
we have also experi-mented with the bilingual data selection method(junczys-dowmunt, 2018).
however, this tech-nique does not work well, mostly because low-resource mt systems are not good enough to doscoring..d extra experimental results.
in this section we include extra experimental resultsthat are not included in the main text due to limitedspace..d.1 seclr architecture exploration.
when we are designing the seclr model, we ex-periment with adding lstms and using the dotproduct between lstm hidden states to computepairwise similarity between the query and the sen-tence.
we report map scores of seclr withlstm in table 9. experimental results show thatadding lstms reduces model performance consis-tently across all three languages.
we conjecturethat in low-resource settings, contextualized mod-els create spurious correlations (section 3.3).
infact, the xlm-roberta baseline, which capturescontext effectively via self-attention, also under-performs our seclr model consistently..3894d.2 word embeddings initialization.
in our seclr and seclr-rt models, we initial-ize word embeddings with monolingual word em-beddings in english, somali, swahili and tagalog(mikolov et al., 2013; grave et al., 2018).
one natu-ral question is whether we can achieve performanceimprovement if we directly initialize with cross-lingual word embeddings.
because sid-sgnsout-performs both bivec and muse consistentlyby a wide margin (table 4 and table 5), in thisexperiment we initialize seclr-rt with the cross-lingual embeddings produced by sid-sgns.
theresults of monolingual and cross-lingual embed-ding initialization (sid-sgns) are shown in ta-ble 10. we see that overall monolingual initial-ization slightly out-performs cross-lingual initial-ization.
monolingual initialization yields betterperformance in eight out of 12 analysis/dev setconditions and a map improvement of 1.7 pointswhen we take the average across analysis/dev andall three languages..3895