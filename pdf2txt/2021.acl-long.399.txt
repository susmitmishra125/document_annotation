structural pre-training for dialogue comprehension.
zhuosheng zhang1,2,3, hai zhao1,2,3,∗1department of computer science and engineering, shanghai jiao tong university2key laboratory of shanghai education commission for intelligent interactionand cognitive engineering, shanghai jiao tong university, shanghai, china3moe key lab of artiﬁcial intelligence, ai institute, shanghai jiao tong universityzhangzs@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn.
abstract.
pre-trained language models (prlms) havedemonstrated superior performance due tolan-their strong ability to learn universalguage representations from self-supervisedpre-training.
however, even with the help ofthe powerful prlms, it is still challenging to ef-fectively capture task-related knowledge fromdialogue texts which are enriched by corre-lations among speaker-aware utterances.
inthis work, we present spider, structural pre-trained dialogue reader, to capture dialogueexclusive features.
to simulate the dialogue-like features, we propose two training objec-tives in addition to the original lm objectives:1) utterance order restoration, which predictsthe order of the permuted utterances in dia-logue context; 2) sentence backbone regular-ization, which regularizes the model to im-prove the factual correctness of summarizedsubject-verb-object triplets.
experimental re-sults on widely used dialogue benchmarks ver-ify the effectiveness of the newly introducedself-supervised tasks..1.introduction.
recent advances in large-scale pre-training lan-guage models (prlms) have achieved remarkablesuccesses in a variety of natural language pro-cessing (nlp) tasks (peters et al., 2018; radfordet al., 2018; devlin et al., 2019a; yang et al., 2019;clark et al., 2020; zhang et al., 2020d).
provid-ing ﬁne-grained contextualized embedding, thesepre-trained models are widely employed as en-coders for various downstream nlp tasks.
al-though the prlms demonstrate superior perfor-.
∗ corresponding author.
this paper was partially sup-ported by national key research and development pro-gram of china (no.
2017yfb0304100), key projects ofnational natural science foundation of china (u1836222and 61733011), huawei-sjtu long term ai project, cutting-edge machine reading comprehension and language model.
this work was supported by huawei noah’s ark lab..figure 1: a multi-turn dialogue example.
different col-ors indicate the utterances from different speakers..mance due to their strong representation abilityfrom self-supervised pre-training, it is still chal-lenging to effectively adapt task-related knowledgeduring the detailed task-speciﬁc training which isusually in a way of ﬁne-tuning (gururangan et al.,2020).
generally, those prlms handle the wholeinput text as a linear sequence of successive tokensand implicitly capture the contextualized represen-tations of those tokens through self-attention.
suchﬁne-tuning paradigm of exploiting prlms wouldbe suboptimal to model dialogue task which holdsexclusive text features that plain text for prlmtraining may hardly embody.
therefore, we ex-plore a fundamental way to alleviate this difﬁcultyby improving the training of prlm.
this work de-votes itself to designing the natural way of adaptingthe language modeling to the dialogue scenario mo-tivated by the natural characteristics of dialoguecontexts..as an active research topic in the nlp ﬁeld,multi-turn dialogue modeling has attracted great in-terest.
the typical task is response selection (loweet al., 2015; wu et al., 2017; zhang et al., 2018) thataims to select the appropriate response accordingto a given dialogue context containing a number ofutterances, which is the focus in this work.
how-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5134–5145august1–6,2021.©2021associationforcomputationallinguistics5134u1: how can i connect to url streaming server ?u2:  i think mplayer does itu3:  i use kino , could be ?u4:  kino is editing software .
i don't think it supports media .u5:  sorry emoji , i mean movie playeru6:  not sure about it .
if it can't try vlc and mplayer too .u7: is mplayer aptable ?u8: yes but i 'm not sure if it is in the main repo  .
u9: have you ever updated your sources.list ?u10: i have no idea..i use adept on kubuntu .
ever, selecting a coherent and informative responsefor a given dialogue context remains a challenge.
the multi-turn dialogue typically involves two ormore speakers that engage in various conversationtopics, intentions, thus the utterances are rich ininteractions, e.g., with criss-cross discourse struc-tures (li et al., 2020a; bai and zhao, 2018; qinet al., 2016, 2017).
a critical challenge is the learn-ing of rich and robust context representations andinteractive relationships of dialogue utterances, sothat the resulting model is capable of adequatelycapturing the semantics of each utterance, and therelationships among all the utterances inside thedialogue..inspired by the effectiveness for learning univer-sal language representations of prlms, there areincreasing studies that employ prlms for conver-sation modeling (mehri et al., 2019; zhang et al.,2020b; rothe et al., 2020; whang et al., 2020; hanet al., 2021).
these studies typically model theresponse selection with only the context-responsematching task and overlook many potential train-ing signals contained in dialogue data.
althoughthe prlms have learned contextualized semanticrepresentation from token-level or sentence-levelpre-training tasks like mlm, nsp, they all do notconsider dialogue related features like speaker role,continuity and consistency.
one obvious issue ofthese approaches is that the relationships betweenutterances are harder to capture using word-level se-mantics.
besides, some latent features, such as userintent and conversation topic, are under-discoveredin existing works (xu et al., 2021).
therefore, theresponse retrieved by existing dialogue systemssupervised by the conventional way still faces criti-cal challenges, including incoherence and inconsis-tency..in this work, we present spider (structural pre-trained dialogue reader), a structural languagemodeling method to capture dialogue exclusive fea-tures.
motivated to efﬁciently and explicitly modelthe coherence among utterances and the key factsin each utterance, we propose two training objec-tives in analogy to the original bert-like languagemodel (lm) training: 1) utterance order restoration(uor), which predicts the order of the permutedutterances in dialogue context; 2) sentence back-bone regularization (sbr), which regularizes themodel to improve the factual correctness of sum-marized subject-verb-object (svo) triplets.
exper-imental results on widely used benchmarks show.
that spder boosts the model performance for var-ious multi-turn dialogue comprehension tasks in-cluding response selection and dialogue reasoning..2 background and related work.
2.1 pre-trained language models.
recent works have explored various architecturechoices and training objectives for large-scalelm pre-training (zhou et al., 2020b,a; xu et al.,2020a,b; li et al., 2021, 2020b).
most of the prlmsare based on the encoder in transformer, amongwhich bidirectional encoder representations fromtransformers (bert) (devlin et al., 2019b) is oneof the most representative work.
bert uses multi-ple layers of stacked transformer encoder to obtaincontextualized representations of the language atdifferent levels.
bert has helped achieve greatperformance improvement in a broad range of nlptasks (bai and zhao, 2018; zhang et al., 2020a;luo and zhao, 2020; zhang et al., 2021).
severalsubsequent variants have been proposed to furtherenhance the capacity of prlms, such as xlnet(yang et al., 2019), roberta (liu et al., 2019),albert (lan et al., 2020), electra (clarket al., 2020).
for simplicity and convenient com-parison with public studies, we select the mostwidely used bert as the backbone in this work..there are two ways of training prlms ondialogue scenarios, including open-domain pre-training and domain-adaptive post-training.
somestudies perform training on open-domain conver-sational data like reddit for response selection orgeneration tasks (wolf et al., 2019; zhang et al.,2020c; henderson et al., 2020; bao et al., 2020),but they are limited to the original pre-trainingtasks and ignore the dialogue related features.
fordomain-adaptive post-training, prior works have in-dicated that the order information would be impor-tant in the text representation, and the well-knownnext-sentence-prediction (devlin et al., 2019b) andsentence-order-prediction (lan et al., 2020) can beviewed as special cases of order prediction.
espe-cially in the dialogue scenario, predicting the wordorder of utterance, as well as the utterance orderin the context, has shown effectiveness in the dia-logue generation task (kumar et al., 2020; gu et al.,2020b), where the order information is well recog-nized (chen et al., 2019).
however, there is little at-tention paid to dialogue comprehension tasks suchas response selection (lowe et al., 2015; wu et al.,2017; zhang et al., 2018).
the potential difﬁculty.
5135is that utterance order restoration involves muchmore ordering possibilities for utterances that mayhave a quite ﬂexible order inside dialogue text thannsp and sop which only handle the predicationof two-class ordering..our work is also profoundly related to auxiliarymulti-task learning, whose common theme is toguide the language modeling transformers withexplicit knowledge and complementing objectives(zhang et al., 2019; sun et al., 2019b; xu et al.,2020a).
a most related work is xu et al.
(2020a),which introduces four self-supervised tasks includ-ing next session prediction, utterance restoration,incoherence detection and consistency discrimina-tion.
our work differs from xu et al.
(2020a) bythree sides.
1) motivation: our method is designedfor a general-purpose in broad dialogue compre-hension tasks whose goals may be either utterance-level discourse coherence or inner-utterance factualcorrectness, instead of only motivated for down-stream context-response matching, whose goal isto measure if two sequences are related or not.
2)technique: we propose both sides of intra- andinter- utterance objectives.
in contrast, the fourobjectives proposed in xu et al.
(2020a) are naturalvariants of nsp in bert, which are all utterance-level.
3) training: we empirically evaluate domain-adaptive training and multi-task learning, instead ofonly employing multi-task learning, which requiresmany efforts of optimizing coefﬁcients in the lossfunctions, which would be time-consuming..in terms of factual backbone modeling, com-pared with the existing studies that enhance theprlms by annotating named entities or incorporat-ing external knowledge graphs (eric et al., 2017;liu et al., 2018), the svo triplets extracted inour sentence backbone predication objective (sbp)method, appear more widely in the text itself.
suchtriplets ensure the correctness of svo and enableour model to discover the salient facts from thelengthy texts, sensing the intuition of “who didwhat”..2.2 multi-turn dialogue comprehension.
multi-turn dialogue comprehension aims to teachmachines to read dialogue contexts and solve taskssuch as response selection (lowe et al., 2015; wuet al., 2017; zhang et al., 2018) and answering ques-tions (sun et al., 2019a; cui et al., 2020), whosecommon application is building intelligent human-computer interactive systems (chen et al., 2017a;.
shum et al., 2018; li et al., 2017; zhu et al., 2018b).
early studies mainly focus on the matching be-tween the dialogue context and question (huanget al., 2019; zhu et al., 2018a).
recently, inspiredby the impressive performance of prlms, the main-stream is employing prlms to handle the wholeinput texts of context and question, as a linear se-quence of successive tokens and implicitly capturethe contextualized representations of those tokensthrough self-attention (qu et al., 2019; liu et al.,2020).
such a way of modeling would be subopti-mal to capture the high-level relationships betweenutterances in the dialogue history.
in this work, weare motivated to model the structural relationshipsbetween utterances from utterance order restorationand the factual correctness inside each utterance inthe perspective of language modeling pre-traininginstead of heuristically stacking deeper model ar-chitectures..3 approach.
this section presents our proposed method spi-der (structural pre-trained dialogue reader).
first, we will present the standard dialogue com-prehension model as the backbone.
then, we willintroduce our designed language modeling objec-tives for dialogue scenarios, including utteranceorder restoration (uor) and sentence backboneregularization (sbr).
in terms of model training,we employ two strategies, i.e., 1) domain adap-tive post-training that ﬁrst trains a language modelbased on newly proposed objectives and then ﬁne-tunes the response selection task; 2) multi-task ﬁne-tuning that trains the model for downstream tasks,along with lm objectives..3.1 transformer encoder.
we ﬁrst employ a pre-trained language modelsuch as bert (devlin et al., 2019a) to obtain theinitial word representations.
the utterances andresponse are concatenated and then fed into theencoder.
given the context c and response r, weconcatenate all utterances in the context and theresponse candidate as a single consecutive token se-quence with special tokens separating them: x ={[cls]r[sep]u1[eou] .
.
.
[eou]un[sep]},where [cls] and [sep] are special tokens.
[eou] is the “end of utterance” tag designedfor multiturn context.
x is then fed into thebert encoder, which is a deep multi-layer bidi-rectional transformer, to obtain a contextualized.
5136figure 2: structural language modeling manipulations..representation h..in detail, let x = {x1, .
.
.
, xn} be the embed-ding of the sequence, which are features of encod-ing sentence words of length n. the input em-beddings are then fed into the multi-head attentionlayer to obtain the contextual representations..the embedding sequence x is processed to amulti-layer bidirectional transformer for learningcontextualized representations, which is deﬁned as.
h = ffn(multihead(k, q, v )),.
(1).
where k,q,v are packed from the input sequencerepresentation x. as the common practice, we setk = q = v in the implementation..for.
the following part, we use h ={h1, .
.
.
, hn} to denote the last-layer hidden statesof the input sequence..3.2 spider training objectives.
to simulate the dialogue-like features, we proposetwo pre-training objectives in addition to the origi-nal lm objectives: 1) utterance order restoration,which predicts the order of the permuted utterancesin dialogue context; 2) sentence backbone regular-ization, which regularizes the model to improvethe factual correctness of summarized subject-verb-object triplets.
the utterance manipulations areshown in figure 2. the following subsections de-scribe the objectives in turn..3.2.1 utterance order restoration.
coherence is an essential aspect of conversationmodeling.
in a coherent discourse, utterancesshould respect speciﬁc orders of relations and logic.
the ordering of utterances in a dialogue contextdetermines the semantic of the conversation.
there-fore, learning to order a set of disordered utterances.
in such a way that maximizes the discourse co-herence will have a critical impact in learning therepresentation of dialogue contexts..however, most previous studies focused on se-mantic relevance between context and responsecandidate.
here we introduce utterance-level posi-tion modeling, i.e., utterance order restoration toencourage the model to be aware of the semanticconnections among utterances in the context.
theidea is similar to autoencoding (ae) which aimsto reconstruct the original data from corrupted in-put (yang et al., 2019).
given permuted dialoguecontexts that comprise utterances in random orders,we maximize the expected log-likelihood of a se-quence of the original ground-truth order..the goal of the utterance order restorationis to organize randomly shufﬂed utterances ofa conversation into a coherent dialogue context.
we extract the hidden states of [eou] from has the representation of each utterance.
for-mally, given an utterance sequence denoted asc(cid:48) = [hu1; hu2; .
.
.
; huk ] with order o =[o1; o2; .
.
.
; ok], where k means the number ofmaximum positions to be predicted.
we expect anordered context c∗ = [uo∗] is the; .
.
.
; uo∗most coherent permutation of utterances..; uo∗.
k.1.
2.as predicting the permuted orders is a more chal-lenging optimization problem than nsp and soptasks due to the large searching space of permuta-tions and causes slow convergence in preliminaryexperiments, we choose to only predict the order ofthe last few permuted utterances by a permutationratio δ to control the maximum number of permu-tations: k(cid:48) = k ∗ δ. the uor training objectiveis then formed as:.
luor = −.
[ok log ˆok] ,.
(2).
k(cid:48)(cid:88).
k=1.
5137u1:  well, i'm afraid my cooking isn't to your taste.u2:  actually, i like it very much.u3:  i'm glad you say that.
let me serve you more fish.u4:  thanks.
i didn't know you were good at cooking.u5:  why not bring your wife next time?u6:  ok, i will.
she will be very glad to see  you, too.u1:  well, i'm afraid my cooking isn't to your taste.u2:  actually, i like it very much.u6:  ok, i will.
she will be very glad to see  you, too.u5:  why not bring your wife next time?u4:  thanks.
i didn't know you were good at cooking.u3:  i'm glad you say that.
let me serve you more fish.cooking isn't  →  tastei like → iti am → gladyou say → thatme serve → fishi didn’t → knowyou were → goodshe be → gladoriginal contextpermuted contextsvo tripletswhere ˆok denotes the predicted order..3.2.2 sentence backbone regularizationthe sentence backbone regularization objective ismotivated to guide the model to distinguish the in-ternal relation of the fact triplets that are extractedfrom each utterance, which would be helpful toimprove the ability to capture the key facts of theutterance as well as the correctness.
first, we applya fact extractor to conduct the dependency parsingof each sentence.
after that, we extract the sub-ject, the root verb, and the object tokens as an svotriplet corresponding to each utterance.
inspiredby bordes et al.
(2013) where the embedding ofthe tail entity should be close to the embedding ofthe head entity plus some vector that depends onthe relationship, we assume that given the dialogueinput, in the hidden representation space, the sum-mation of the subject and the verb should be closeto the object as much as possible, i.e.,.
hsubject + hverb → hobject..(3).
consequently, based on the sequence hiddenstates hi where i = 1, ..., ly, we introduce a regu-larization for the extracted facts:.
from tokens in text.
more details of the lm trainingtasks can be found from devlin et al.
(2019a).
theoverall post-training loss is the sum of the mlm,nsp, uor, and sbr loss..our full model is trained by a joint loss by com-.
bining both of the objectives above:.
l = λ1(lmlm + lnsp) + λ2luor + λ3lsbr, (5).
where λ1, λ2, λ3 are hyper-parameters..after post-training the language model on thedialogue corpus, we load the pre-trained weights asthe same way of using bert (devlin et al., 2019a),to ﬁne-tune the downstream tasks such as responseselection and dialogue reasoning as focused in thiswork (details in section 5.1)..4.2 multi-task fine-tuning.
since our objectives can well share the same inputas the downstream tasks, there is an efﬁcient wayof using multi-task ﬁne-tuning (mtf) to directlytrain the task-speciﬁc models along with our spi-der objectives.
therefore, we feed the permutedcontext to the dialogue comprehension model andcombine the three losses for training:.
lsbr =.
(1 − cos(hsubjk + hverbk , hobjk )), (4).
l = β1ldm + β2luor + β3lsbr,.
(6).
m(cid:88).
k=1.
where m is the total number of fact tuples extractedfrom the summary and k indicates the k-th triplet.
“subjk”, “verbk”, and “objk” are indexes of thek-th fact tuple’s subject, verb, and object..in our implementation, since prlms take sub-words as input while the svo extraction performsin word-level, we use the ﬁrst-token hidden state asthe representation of the original word followingthe way in devlin et al.
(2019a) for named entityrecognition..where β1, β2, β3 are hyper-parameters..in order to train a task-speciﬁc model for di-alogue comprehension, the hidden states h willbe fed into a classiﬁer with a fully connected andsoftmax layer.
we learn model g(·, ·) by minimiz-ing cross entropy loss with dataset d. let θ de-note the parameters, for binary classiﬁcation likethe response selection task, the objective functionl(d, θ) can be formulated as:.
4 use of spider objectives.
ldm = −.
[yi log(g(ci, ri))+.
n(cid:88).
i=1.
(1 − yi) log(1 − g(ci, ri))]..in this section, we introduce two training meth-ods to take the newly proposed language modelingobjectives into account, namely domain-adaptivepost-training and multi-task ﬁne-tuning, as illus-trated in figure 3..4.1 domain adaptive post-training.
similar to bert, we also adopt the masked lan-guage model (mlm) and the next sentence pre-diction (nsp) as lm-training tasks to enable ourmodel to capture lexical and syntactic information.
where n denotes the number of examples.
formultiple choice task like mutual, the loss functionis:.
ldm = −.
yi,c log(g(ci, ri,k))..n(cid:88).
c(cid:88).
i=1.
k=1.
where c is the number of choice..5138figure 3: model training ﬂow of domain-adaptive post-training (a) and multi-task ﬁne-tuning (b)..ubuntuvalid.
test.
train.
test.
train.
doubanvalid.
# context-response pairs# candidates per contextavg # turns per contextavg # words per utterance.
500k1010.1111.34.
500k1010.1111.37.
1m26.6918.56.
50k26.7518.50.
10k106.4520.74.
1m25.517.02.train.
1m210.1311.35.ecdvalid.
10k25.486.99.test.
10k105.647.11.table 1: data statistics of ubuntu, douban, and ecd datasets..5 experiments.
5.1 datasets.
we evaluated our model on two english datasets:ubuntu dialogue corpus (ubuntu) (lowe et al.,2015) and multi-turn dialogue reasoning (mu-tual) (cui et al., 2020),1 and two chinese datasets:douban conversation corpus (douban) (wu et al.,2017) and e-commerce dialogue corpus (ecd)(zhang et al., 2018)..5.1.1 ubuntu dialogue corpus.
ubuntu (lowe et al., 2015) consists of englishmulti-turn conversations about technical supportcollected from chat logs of the ubuntu forum.
thedataset contains 1 million context-response pairs,0.5 million for validation and 0.5 million for test-in training set, each context has one posi-ing.
tive response generated by human and one neg-ative response sampled randomly.
in validationand test sets, for each context, there are 9 negativeresponses and 1 positive response..1actually, mutual is a retrieval-based dialogue corpus inform, but the theme is english listening comprehension exams,thus we regard as a reading comprehension corpus in this work.
because the test set of mutual is not publicly available, weconducted the comparison with our baselines on the dev setfor convenience..5.1.2 douban conversation corpusdouban (wu et al., 2017) is different from ubuntuin the following ways.
first, it is an open domainwhere dialogues are extracted from douban group.
second, response candidates on the test set arecollected by using the last turn as the query toretrieve 10 response candidates and labeled by hu-mans.
third, there could be more than one correctresponse for a context..5.1.3 e-commerce dialogue corpusecd (zhang et al., 2018) dataset is extracted fromconversations between customer and service staffon taobao.
it contains over 5 types of conversationsbased on over 20 commodities.
there are also 1million context-response pairs in the training set,0.5 million in the validation set, and 0.5 million inthe test set..5.1.4 multi-turn dialogue reasoningmutual (cui et al., 2020) consists of 8860 manu-ally annotated dialogues based on chinese studentenglish listening comprehension exams.
for eachcontext, there is one positive response and threenegative responses.
the difference compared tothe above three datasets is that only mutual isreasoning-based.
there are more than 6 types ofreasoning abilities reﬂected in mutual..5139......domain-adaptive language modeldialogue comprehension modeldialogue corpustask training datadialogue comprehension modeltask training datamlmnspuorsbr......rs......rsuorsbr(a) domain-adaptive post training(b) multi-task fine-tuningu1u2u3u5u4hu1hu2hu3hu5hu4u1u2u3u4u5hu1hu2hu3hu4hu5u1u2u3u5u4hu1hu2hu3hu5hu4prlmoutputprlmoutputprlmoutputmodel.
ubuntu corpus.
douban conversation corpus.
e-commerce corpus.
r10@1 r10@2 r10@5 map mrr p@1 r10@1 r10@2 r10@5 r10@1 r10@2 r10@5.smnduadamioimsnmrfnsa-bert.
72.675.276.779.680.078.685.5.multi-task fine-tuningbert.
81.7+ spider 83.1.
84.786.887.489.489.988.692.8.
90.491.3.domain adaptive post-trainingbert.
85.7+ spider 86.9.
93.093.8.
96.196.296.997.497.897.698.3.
52.955.155.057.358.757.161.9.
56.959.960.162.163.261.765.9.
39.742.142.744.447.044.849.6.
23.324.325.426.929.527.631.3.
97.798.0.
58.859.8.
63.163.8.
45.345.9.
27.728.5.
98.598.7.
60.560.9.
64.765.0.
47.447.5.
29.129.6.
39.642.141.045.145.243.548.1.
46.448.7.
47.848.8.
72.478.075.778.678.878.384.7.
81.882.6.
84.983.6.
45.350.1--60.6-70.4.
61.762.6.
66.470.8.
65.470.0--77.0-87.9.
81.182.7.
84.885.3.
88.692.1--93.7-98.5.
97.097.1.
97.698.6.table 2: performance comparison on ubuntu, douban and e-commerce datasets..5.2.implementation details.
for the sake of computational efﬁciency, the max-imum number of utterances is specialized as 20.the concatenated context, response, [cls] and[sep] in one sample is truncated according tothe “longest ﬁrst” rule or padded to a certain length,which is 256 for mutual and 384 for the other threedatasets.
for the hyper-parameters, we empiricallyset λ1 = λ2 = λ3 = β1 = β2 = 1..our model is implemented using pytorch andbased on the transformer library.2 we use bert(devlin et al., 2019a) as our backbone model.
adamw (loshchilov and hutter, 2019) is used asour optimizer.
the batch size is 24 for mutual, and64 for others.
the initial learning rate is 4 × 10−6for mutual and 3 × 10−5 for others.
the ratio isset to 0.4 in our implementation by default.
werun 3 epochs for mutual and 2 epochs for othersand select the model that achieves the best result invalidation.
the training epochs are 3 for dap..our domain adaptive post-training for the cor-responding response selection tasks is based onthe three large-scale dialogue corpus includingubuntu, douban, and ecd, respectively.3 the datastatistics are in table 1. since domain adaptivepost-training is time-consuming, following previ-ous studies (gu et al., 2020a), we use bert-base-uncased, and bert-base-chinese for the english and.
2our source code is available at https://github..com/cooelf/spider..3since phrases are quite common in chinese, making itinaccurate to calculate the svo relations according to eq.
3,thus we did not use the sbr objective for the two chinesetasks in this work..chinese datasets, respectively.
because there is noappropriate domain data for the small-scale mutualdataset, we only report the multi-task ﬁne-tuning re-sults with our spider objectives, and also presentthe results with other prlms such as electra(clark et al., 2020) for general comparison..5.3 baseline models.
we include the following models for comparison:• multi-turn matching models: sequentialmatching network (smn) (wu et al., 2017), deepattention matching network (dam) (zhou et al.,2018), deep utterance aggregation (dua) (zhanget al., 2018), interaction-over-interaction (ioi) (taoet al., 2019b) have been stated in section 2.2.besides, multi-representation fusion network(mrfn) (tao et al., 2019a) matches context andresponse with multiple types of representations.
multi-hop selector network (msn) (yuan et al.,2019) utilizes a multi-hop selector to ﬁlter neces-sary utterances and matches among them..• prlms-based models: bert (devlin et al.,2019b), sa-bert (gu et al., 2020a), and elec-tra (clark et al., 2020)..5.4 evaluation metrics.
following (lowe et al., 2015; wu et al., 2017), wecalculate the proportion of true positive responseamong the top-k selected responses from the list ofn available candidates for one context, denoted asrn@k. besides, additional conventional metricsof information retrieval are employed on douban:mean average precision (map) (baeza-yates et al.,.
5140model.
mrr.
r4@1.r4@2.model.
r10@1.r10@2.r10@5.bertbase+ uor+ sbr+ spider.
bertlarge+ uor+ sbr+ spider.
electrabase.
+ uor+ sbr+ spider.
electralarge.
+ uor+ sbr+ spider.
80.080.781.381.6.
82.282.883.483.9.
86.586.987.688.2.
94.995.395.595.6.
65.366.167.467.6.
69.169.871.071.8.
76.276.677.179.2.
90.691.391.692.0.
86.086.787.187.3.
87.988.689.489.2.
91.691.892.092.3.
97.797.897.897.9.table 3: results on mutual dataset..1999), mean reciprocal rank (mrr) (voorheeset al., 1999), and precision at position 1 (p@1)..5.5 results.
tables 2-3 show the results on the four benchmarkdatasets.
we have the following observations:.
1) generally, the previous models based onmulti-turn matching networks perform worse thansimple prlms-based ones, illustrating the power ofcontextualized representations in context-sensitivedialogue modeling.
prlm can perform even bet-ter when equipped with our spider objectives,verifying the effectiveness of dialogue-aware lan-guage modeling, where inter-utterance position in-formation and inner-utterance key facts are betterexploited.
compared with sa-bert that involvesmore complex architecture and more parametersby injecting extra speaker-aware embeddings, spi-der keeps the same model size as the backbonebert, and even surpasses sa-bert on most ofthe metrics..2) in terms of the training methods, dap gen-erally works better than mtf, with the merits oftwo-step procedures including the pure lm-basedpost-training.
according to the ablation study intable 4, we see that both of the dialogue-aware lmobjectives are essentially effective and combiningthem (spider) gives the best performance, whichveriﬁes the necessity of modeling the utterance or-der and factual correctness.
we also notice thatuor shows better performance than sbr in dap,while gives relative descent in mft.
the most plau-.
spiderdapw/o uorw/o sbrw/o both.
spidermtfw/o uorw/o sbrw/o both.
86.986.286.485.7.
83.182.682.381.7.
93.893.393.593.0.
91.391.090.890.4.
98.798.698.698.5.
98.097.997.897.7.table 4: ablation study on the ubuntu dataset..sible reason would be that uor would permute theutterances in the dialogue context which helps thelanguage model learn the utterance in uor.
how-ever, in mft, the major objective is the downstreamdialogue comprehension task.
the permutation ofthe context would possibly bring some negativeeffects to the downstream task training..5.6.inﬂuence of permutation ratio.
for the uor objective, a hyper-parameter δ is setto control the maximum number of permutations(as described in section 3.2.1), which would pos-sibly inﬂuence the overall model performance.
toinvestigate the effect, we set the permutation ratiofrom [0, 20%, 40%, 60%, 80%, 100%].
the resultis depicted in figure 4, in which our model out-performs the baseline in general, showing that thepermutation indeed strengthens the baseline..5.7 comparison with different context.
length.
context length can be measured by the numberof turns and average utterance length in a conver-sation respectively.
we split test instances fromthe ubuntu dataset into several buckets and com-pare spider with uor with the bert baseline.
according to the results depicted in figure 5, weobserve that spider performs much better on con-texts with long utterances, and it also performsrobustly and is signiﬁcantly and consistently supe-rior to the baseline.
the results indicate the beneﬁtsof modeling the utterance order for dialogue com-prehension..5.8 human evaluation about factual.
correctness.
to compare the improvements of spider overthe baseline on factual correctness, we extract theerror cases of the bert baseline on mutual (102in total) and 42 (41.2%) are correctly answered.
5141our method.
baseline.
acm press new york..88.
87.
86.
1@.
01r.90.
88.
86.
84.
1@01r.0.0.
1.00.4figure 4: inﬂuence of the permutation ratio δ..0.2.
0.8.
0.6.spider.
baseline.
0-4.
4-88-12number of utterances.
12-16.
16-20.figure 5: r10@1 of spider and the baseline berton different numbers of utterances..by spider.
among the 42 solved cases, 33/42(78.6%) are entailed with svo facts in contexts,indicating the beneﬁts of factual correctness..6 conclusion.
in this paper, we focus on the task-related adapta-tion of the pre-trained language models and pro-pose spider (structural pre-trained dialoguereader), a structural language modeling methodto capture dialogue exclusive features.
to explic-itly model the coherence among utterances andthe key facts in each utterance, we introduce twonovel dialogue-aware language modeling tasks in-cluding utterance order restoration and sentencebackbone regularization objectives.
experimentson widely-used multi-turn dialogue comprehensionbenchmark datasets show the superiority over base-line methods.
our work reveals a way to makebetter use of the structure learning of the contex-tualized representations from pre-trained languagemodels and gives insights on how to adapt the lan-guage modeling training objectives in downstreamtasks..references.
ricardo baeza-yates, berthier ribeiro-neto, et al.
1999. modern information retrieval, volume 463..hongxiao bai and hai zhao.
2018. deep enhancedrepresentation for implicit discourse relation recog-nition.
in proceedings of the 27th international con-ference on computational linguistics, pages 571–583, santa fe, new mexico, usa.
association forcomputational linguistics..siqi bao, huang he, fan wang, hua wu, and haifengwang.
2020. plato: pre-trained dialogue genera-tion model with discrete latent variable.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 85–96, online.
association for computational linguistics..antoine bordes, nicolas usunier, alberto garc´ıa-dur´an,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 2787–2795..hongshen chen, xiaorui liu, dawei yin, and jiliangtang.
2017a.
a survey on dialogue systems: recentadvances and new frontiers.
in acm sigkdd ex-plorations newsletter.
19(2):25–35..kehai chen, rui wang, masao utiyama, and eiichirosumita.
2019. neural machine translation with re-in proceedings of the 57thordering embeddings.
annual meeting of the association for computa-tional linguistics, pages 1787–1799, florence, italy.
association for computational linguistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..leyang cui, yu wu, shujie liu, yue zhang, and mingzhou.
2020. mutual: a dataset for multi-turn dia-logue reasoning.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 1406–1416, online.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019a.
bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019b.
bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding..5142of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..mihail eric, lakshmi krishnan, francois charette, andchristopher d. manning.
2017. key-value retrievalnetworks for task-oriented dialogue.
in proceedingsof the 18th annual sigdial meeting on discourseand dialogue, pages 37–49, saarbr¨ucken, germany.
association for computational linguistics..jia-chen gu, tianda li, quan liu, zhen-hua ling,zhiming su, si wei, and xiaodan zhu.
2020a.
speaker-aware bert for multi-turn response selec-tion in retrieval-based chatbots.
in cikm ’20: the29th acm international conference on informationand knowledge management, virtual event, ireland,october 19-23, 2020, pages 2041–2044.
acm..xiaodong gu, kang min yoo, and jung-woo ha.
2020b.
dialogbert: discourse-aware response gen-eration via learning to recover and rank utterances.
arxiv preprint arxiv:2012.01775..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360, online.
association for computationallinguistics..janghoon han, taesuk hong, byoungjae kim,youngjoong ko, and jungyun seo.
2021.fine-grained post-training for improving retrieval-baseddialogue systems.
in proceedings of the 2021 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 1549–1558..matthew henderson, i˜nigo casanueva, nikola mrkˇsi´c,pei-hao su, tsung-hsien wen, and ivan vuli´c.
2020. convert: efﬁcient and accurate conversa-in find-tional representations from transformers.
ings of the association for computational linguis-tics: emnlp 2020, pages 2161–2174, online.
as-sociation for computational linguistics..hsin-yuan huang, eunsol choi, and wen-tau yih.
2019. flowqa: grasping ﬂow in history for con-in 7th inter-versational machine comprehension.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..pawan kumar, dhanajit brahma, harish karnick, andpiyush rai.
2020. deep attentive ranking networksfor learning to order sentences.
in the thirty-fourthaaai conference on artiﬁcial intelligence, aaai2020, the thirty-second innovative applications ofartiﬁcial intelligence conference, iaai 2020, thetenth aaai symposium on educational advances.
in artiﬁcial intelligence, eaai 2020, new york, ny,usa, february 7-12, 2020, pages 8115–8122.
aaaipress..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..feng-lin li, minghui qiu, haiqing chen, xiong-wei wang, xing gao, jun huang, juwei ren,zhongzhou zhao, weipeng zhao, lei wang, et al.
2017. alime assist: an intelligent assistant for cre-ating an innovative e-commerce experience.
in pro-ceedings of the 2017 acm on conference on infor-mation and knowledge management, pages 2495–2498..jiaqi li, ming liu, min-yen kan, zihao zheng, zekunwang, wenqiang lei, ting liu, and bing qin.
2020a.
molweni: a challenge multiparty dialogues-based machine reading comprehension dataset withdiscourse structure.
in proceedings of the 28th inter-national conference on computational linguistics,pages 2642–2652, barcelona, spain (online).
inter-national committee on computational linguistics..junlong li, zhuosheng zhang, hai zhao, xi zhou,and xiang zhou.
2020b.
task-speciﬁc objectives ofpre-trained language models for dialogue adaptation.
arxiv preprint arxiv:2009.04984..zuchao li, zhuosheng zhang, hai zhao, rui wang,kehai chen, masao utiyama, and eiichiro sumita.
2021. text compression-aided transformer encod-ing.
ieee transactions on pattern analysis and ma-chine intelligence..chuang liu, deyi xiong, yuxiang jia, hongying zan,and changjian hu.
2020. hisbert for conversa-in 2020 interna-tional reading comprehension.
tional conference on asian language processing(ialp), pages 147–152.
ieee..shuman liu, hongshen chen, zhaochun ren, yangfeng, qun liu, and dawei yin.
2018. knowledgediffusion for neural dialogue generation.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1489–1498, melbourne, australia.
as-sociation for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..5143ryan lowe, nissan pow, iulian serban, and joellepineau.
2015. the ubuntu dialogue corpus: a largedataset for research in unstructured multi-turn dia-logue systems.
in proceedings of the 16th annualmeeting of the special interest group on discourseand dialogue, pages 285–294, prague, czech re-public.
association for computational linguistics..ying luo and hai zhao.
2020. bipartite ﬂat-graph net-in pro-work for nested named entity recognition.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6408–6418, online.
association for computational lin-guistics..shikib mehri, evgeniia razumovskaia, tianchengzhao, and maxine eskenazi.
2019.pretrainingmethods for dialog context representation learning.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages3836–3845, florence, italy.
association for compu-tational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..lianhui qin, zhisong zhang, and hai zhao.
2016. astacking gated neural architecture for implicit dis-course relation classiﬁcation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 2263–2270, austin,texas.
association for computational linguistics..lianhui qin, zhisong zhang, hai zhao, zhiting hu,and eric xing.
2017.adversarial connective-exploiting networks for implicit discourse relationin proceedings of the 55th annualclassiﬁcation.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1006–1017,vancouver, canada.
association for computationallinguistics..chen qu, liu yang, minghui qiu, w. bruce croft,yongfeng zhang, and mohit iyyer.
2019. bertwith history answer embedding for conversationalin proceedings of the 42ndquestion answering.
international acm sigir conference on researchand development in information retrieval, sigir2019, paris, france, july 21-25, 2019, pages 1133–1136. acm..alec radford, karthik narasimhan, tim salimans, andilya sutskever.
2018.improving language under-standing by generative pre-training.
technical re-port..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-.
quence generation tasks.
transactions of the asso-ciation for computational linguistics, 8:264–280..heung-yeung shum, xiao-dong he, and di li.
2018.from eliza to xiaoice: challenges and opportunitieswith social chatbots.
frontiers of information tech-nology & electronic engineering, 19(1):10–26..kai sun, dian yu, jianshu chen, dong yu, yejinchoi, and claire cardie.
2019a.
dream: a chal-lenge data set and models for dialogue-based read-ing comprehension.
transactions of the associationfor computational linguistics, 7:217–231..yu sun, shuohuan wang, yukun li, shikun feng, xuyichen, han zhang, xin tian, danxiang zhu, haotian, and hua wu.
2019b.
ernie: enhanced rep-resentation through knowledge integration.
arxivpreprint arxiv:1904.09223..chongyang tao, wei wu, can xu, wenpeng hu,dongyan zhao, and rui yan.
2019a.
multi-representation fusion network for multi-turn re-sponse selection in retrieval-based chatbots.
in pro-ceedings of the twelfth acm international confer-ence on web search and data mining, wsdm 2019,melbourne, vic, australia, february 11-15, 2019,pages 267–275.
acm..chongyang tao, wei wu, can xu, wenpeng hu,dongyan zhao, and rui yan.
2019b.
one time ofinteraction may not be enough: go deep with aninteraction-over-interaction network for response se-lection in dialogues.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 1–11, florence, italy.
associationfor computational linguistics..ellen m voorhees et al.
1999. the trec-8 question an-swering track report.
in trec, volume 99, pages 77–82..taesun whang, dongyub lee, dongsuk oh, chan-hee lee, kijong han, dong-hun lee, and saebyeoklee.
2020. do response selection models reallyknow what’s next?
utterance manipulation strategiesarxiv preprintfor multi-turn response selection.
arxiv:2009.04703..thomas wolf, victor sanh, julien chaumond, andtransfertransfo: alearning approach for neural networkarxiv preprint.
clement delangue.
2019.transferbased conversational agents.
arxiv:1901.08149..yu wu, wei wu, chen xing, ming zhou, and zhou-jun li.
2017.sequential matching network: anew architecture for multi-turn response selectionin proceedings of thein retrieval-based chatbots.
55th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages496–505, vancouver, canada.
association for com-putational linguistics..5144ruijian xu, chongyang tao, daxin jiang, xueliangzhao, dongyan zhao, and rui yan.
2020a.
learn-ing an effective context-response matching modelwith self-supervised tasks for retrieval-based dia-logues.
arxiv preprint arxiv:2009.06265..yi xu, hai zhao, and zhuosheng zhang.
2021. topic-aware multi-turn dialogue modeling.
in the thirty-fifth aaai conference on artiﬁcial intelligence(aaai-21)..zenan xu, daya guo, duyu tang, qinliang su,linjun shou, ming gong, wanjun zhong, xiao-jun quan, nan duan, and daxin jiang.
2020b.
syntax-enhanced pre-trained model.
arxiv preprintarxiv:2012.14116..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forin advances in neurallanguage understanding.
information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 5754–5764..chunyuan yuan, wei zhou, mingming li, shangwenlv, fuqing zhu, jizhong han, and songlin hu.
2019.multi-hop selector network for multi-turn responseselection in retrieval-based chatbots.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 111–120, hongkong, china.
association for computational lin-guistics..shuailiang zhang, hai zhao, yuwei wu, zhuoshengzhang, xi zhou, and xiang zhou.
2020a.
dcmn+:dual co-matching network for multi-choice readingcomprehension.
in proceedings of the thirty-fourthaaai conference on artiﬁcial intelligence (aaai-2020), volume 34, pages 9563–9570..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020b.
dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020c.
dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..zhengyan zhang, xu han, zhiyuan liu, xin jiang,maosong sun, and qun liu.
2019. ernie: en-hanced language representation with informative en-in proceedings of the 57th annual meet-tities.
ing of the association for computational linguis-tics, pages 1441–1451, florence, italy.
associationfor computational linguistics..zhuosheng zhang, jiangtong li, pengfei zhu, haizhao, and gongshen liu.
2018. modeling multi-turn conversation with deep utterance aggregation.
in proceedings of the 27th international conferenceon computational linguistics, pages 3740–3752,santa fe, new mexico, usa.
association for com-putational linguistics..zhuosheng zhang, yuwei wu, hai zhao, zuchao li,shuailiang zhang, xi zhou, and xiang zhou.
2020d.
semantics-aware bert for language understanding.
in the thirty-fourth aaai conference on artiﬁcialintelligence, aaai 2020, the thirty-second inno-vative applications of artiﬁcial intelligence confer-ence, iaai 2020, the tenth aaai symposium on ed-ucational advances in artiﬁcial intelligence, eaai2020, new york, ny, usa, february 7-12, 2020,pages 9628–9635.
aaai press..zhuosheng zhang, junjie yang, and hai zhao.
2021.retrospective reader for machine reading compre-in the thirty-fifth aaai conference onhension.
artiﬁcial intelligence (aaai-21)..junru zhou, zuchao li, and hai zhao.
2020a.
parsingall: syntax and semantics, dependencies and spans.
in findings of the association for computationallinguistics: emnlp 2020, pages 4438–4449, on-line.
association for computational linguistics..junru zhou, zhuosheng zhang, hai zhao, and shuail-iang zhang.
2020b.
limit-bert : linguistics in-formed multi-task bert.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 4450–4461, online.
association for computa-tional linguistics..xiangyang zhou, lu li, daxiang dong, yi liu, yingchen, wayne xin zhao, dianhai yu, and hua wu.
2018. multi-turn response selection for chatbotswith deep attention matching network.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 1118–1127, melbourne, australia.
as-sociation for computational linguistics..chenguang zhu, michael zeng, and xuedong huang.
2018a.
sdnet: contextualized attention-baseddeep network for conversational question answering.
arxiv preprint arxiv:1812.03593..pengfei zhu, zhuosheng zhang, jiangtong li, yafanghuang, and hai zhao.
2018b.
lingke: a ﬁne-grained multi-turn chatbot for customer service.
inproceedings of the 27th international conference oncomputational linguistics: system demonstrations,pages 108–112, santa fe, new mexico.
associationfor computational linguistics..5145