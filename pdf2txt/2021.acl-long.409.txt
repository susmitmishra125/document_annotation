meta-learning with variational semantic memory for word sensedisambiguation.
yingjun duuniversity of amsterdamy.du@uva.nl.
nithin hollaamberscriptnithin.holla7@gmail.com.
xiantong zhenuniversity of amsterdamx.zhen@uva.nl.
cees g.m.
snoekuniversity of amsterdamc.g.m.snoek@uva.nl.
ekaterina shutovauniversity of amsterdame.shutova@uva.nl.
abstract.
a critical challenge faced by supervised wordsense disambiguation (wsd) is the lack oflarge annotated datasets with sufﬁcient cover-age of words in their diversity of senses.
thisinspired recent research on few-shot wsd us-ing meta-learning.
while such work has suc-cessfully applied meta-learning to learn newword senses from very few examples, its per-formance still lags behind its fully-supervisedcounterpart.
aiming to further close this gap,we propose a model of semantic memory forwsd in a meta-learning setting.
semanticmemory encapsulates prior experiences seenthroughout the lifetime of the model, whichaids better generalization in limited data set-tings.
our model is based on hierarchical vari-ational inference and incorporates an adaptivememory update rule via a hypernetwork.
weshow our model advances the state of the artin few-shot wsd, supports effective learningin extremely data scarce (e.g.
one-shot) sce-narios and produces meaning prototypes thatcapture similar senses of distinct words..1.introduction.
disambiguating word meaning in context is atthe heart of any natural language understandingtask or application, whether it is performed ex-plicitly or implicitly.
traditionally, word sensedisambiguation (wsd) has been deﬁned as thetask of explicitly labeling word usages in contextwith sense labels from a pre-deﬁned sense inven-tory.
the majority of approaches to wsd relyon (semi-)supervised learning (yuan et al., 2016;raganato et al., 2017a,b; hadiwinoto et al., 2019;huang et al., 2019; scarlini et al., 2020; bevilac-qua and navigli, 2020) and make use of trainingcorpora manually annotated for word senses.
typi-cally, these methods require a fairly large numberof annotated training examples per word.
this prob-lem is exacerbated by the dramatic imbalances in.
sense frequencies, which further increase the needfor annotation to capture a diversity of senses andto obtain sufﬁcient training data for rare senses..this motivated recent research on few-shotwsd, where the objective of the model is to learnnew, previously unseen word senses from only asmall number of examples.
holla et al.
(2020a) pre-sented a meta-learning approach to few-shot wsd,as well as a benchmark for this task.
meta-learningmakes use of an episodic training regime, where amodel is trained on a collection of diverse few-shottasks and is explicitly optimized to perform wellwhen learning from a small number of examplesper task (snell et al., 2017; finn et al., 2017; tri-antaﬁllou et al., 2020).
holla et al.
(2020a) haveshown that meta-learning can be successfully ap-plied to learn new word senses from as little as oneexample per sense.
yet, the overall model perfor-mance in settings where data is highly limited (e.g.
one- or two-shot learning) still lags behind that offully supervised models..in the meantime, machine learning researchdemonstrated the advantages of a memory com-ponent for meta-learning in limited data settings(santoro et al., 2016a; munkhdalai and yu, 2017a;munkhdalai et al., 2018; zhen et al., 2020).
thememory stores general knowledge acquired inlearning related tasks, which facilitates the acquisi-tion of new concepts and recognition of previouslyunseen classes with limited labeled data (zhenet al., 2020).
inspired by these advances, we intro-duce the ﬁrst model of semantic memory for wsdin a meta-learning setting.
in meta-learning, pro-totypes are embeddings around which other datapoints of the same class are clustered (snell et al.,2017).
our semantic memory stores prototypicalrepresentations of word senses seen during train-ing, generalizing over the contexts in which theyare used.
this rich contextual information aids inlearning new senses of previously unseen words.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5254–5268august1–6,2021.©2021associationforcomputationallinguistics5254that appear in similar contexts, from very few ex-amples..the design of our prototypical representation ofword sense takes inspiration from prototype theory(rosch, 1975), an established account of categoryrepresentation in psychology.
it stipulates that se-mantic categories are formed around prototypicalmembers, new members are added based on resem-blance to the prototypes and category membershipis a matter of degree.
in line with this account,our models learn prototypical representations ofword senses from their linguistic context.
to dothis, we employ a neural architecture for learningprobabilistic class prototypes: variational prototypenetworks, augmented with a variational semanticmemory (vsm) component (zhen et al., 2020)..unlike deterministic prototypes in prototypicalnetworks (snell et al., 2017), we model class proto-types as distributions and perform variational infer-ence of these prototypes in a hierarchical bayesianframework.
unlike deterministic memory accessin memory-based meta-learning (santoro et al.,2016b; munkhdalai and yu, 2017a), we accessmemory by monte carlo sampling from a varia-tional distribution.
speciﬁcally, we ﬁrst performvariational inference to obtain a latent memoryvariable and then perform another step of varia-tional inference to obtain the prototype distribu-tion.
furthermore, we enhance the memory updateof vanilla vsm with a novel adaptive update ruleinvolving a hypernetwork (ha et al., 2016) thatcontrols the weight of the updates.
we call ourapproach β-vsm to denote the adaptive weight βfor memory updates..we experimentally demonstrate the effectivenessof this approach for few-shot wsd, advancing thestate of the art in this task.
furthermore, we ob-serve the highest performance gains on word senseswith the least training examples, emphasizing thebeneﬁts of semantic memory for truly few-shotlearning scenarios.
our analysis of the meaningprototypes acquired in the memory suggests thatthey are able to capture related senses of distinctwords, demonstrating the generalization capabili-ties of our memory component.
we make our codepublicly available to facilitate further research.1.
2 related work.
word sense disambiguation knowledge-basedapproaches to wsd (lesk, 1986; agirre et al.,.
1https://github.com/ydu-uva/vsm_wsd.
2014; moro et al., 2014) rely on lexical resourcessuch as wordnet (miller et al., 1990) and do notrequire a corpus manually annotated with wordsenses.
alternatively, supervised learning meth-ods treat wsd as a word-level classiﬁcation taskfor ambiguous words and rely on sense-annotatedcorpora for training.
early supervised learning ap-proaches trained classiﬁers with hand-crafted fea-tures (navigli, 2009; zhong and ng, 2010) andword embeddings (rothe and sch¨utze, 2015; ia-cobacci et al., 2016) as input.
raganato et al.
(2017a) proposed a benchmark for wsd based onthe semcor corpus (miller et al., 1994) and foundthat supervised methods outperform the knowledge-based ones..neural models for supervised wsd includelstm-based (hochreiter and schmidhuber, 1997)classiﬁers (k˚ageb¨ack and salomonsson, 2016;melamud et al., 2016; raganato et al., 2017b), near-est neighbour classiﬁer with elmo embeddings(peters et al., 2018), as well as a classiﬁer basedon pretrained bert representations (hadiwinotoet al., 2019).
recently, hybrid approaches incorpo-rating information from lexical resources into neu-ral architectures have gained traction.
glossbert(huang et al., 2019) ﬁne-tunes bert with word-net sense deﬁnitions as additional input.
ewise(kumar et al., 2019) learns continuous sense em-beddings as targets, aided by dictionary deﬁnitionsand lexical knowledge bases.
scarlini et al.
(2020)present a semi-supervised approach for obtainingsense embeddings with the aid of a lexical knowl-edge base, enabling wsd with a nearest neighboralgorithm.
by further exploiting the graph structureof wordnet and integrating it with bert, ewiser(bevilacqua and navigli, 2020) achieves the cur-rent state-of-the-art performance on the benchmarkby raganato et al.
(2017a) – an f1 score of 80.1%.
unlike few-shot wsd, these works do not ﬁne-tune the models on new words during testing.
in-stead, they train on a training set and evaluate ona test set where words and senses might have beenseen during training..meta-learning meta-learning, or learning tolearn (schmidhuber, 1987; bengio et al., 1991;thrun and pratt, 1998), is a learning paradigmwhere a model is trained on a distribution of tasksso as to enable rapid learning on new tasks.
bysolving a large number of different tasks, it aimsto leverage the acquired knowledge to learn new,unseen tasks.
the training set, referred to as the.
5255meta-training set, consists of episodes, each cor-responding to a distinct task.
every episode isfurther divided into a support set containing justa handful of examples for learning the task, anda query set containing examples for task evalua-tion.
in the meta-training phase, for each episode,the model adapts to the task using the support set,and its performance on the task is evaluated onthe corresponding query set.
the initial parame-ters of the model are then adjusted based on theloss on the query set.
by repeating the process onseveral episodes/tasks, the model produces repre-sentations that enable rapid adaptation to a newtask.
the test set, referred to as the meta-test set,also consists of episodes with a support and queryset.
the meta-test set corresponds to new tasks thatwere not seen during meta-training.
during meta-testing, the meta-trained model is ﬁrst ﬁne-tunedon a small number of examples in the support setof each meta-test episode and then evaluated onthe accompanying query set.
the average perfor-mance on all such query sets measures the few-shotlearning ability of the model..metric-based meta-learning methods (kochet al., 2015; vinyals et al., 2016; sung et al., 2018;snell et al., 2017) learn a kernel function and makepredictions on the query set based on the similaritywith the support set examples.
model-based meth-ods (santoro et al., 2016b; munkhdalai and yu,2017a) employ external memory and make predic-tions based on examples retrieved from the memory.
optimization-based methods (ravi and larochelle,2017; finn et al., 2017; nichol et al., 2018; anto-niou et al., 2019) directly optimize for generaliz-ability over tasks in their training objective..meta-learning has been applied to a range oftasks in nlp, including machine translation (guet al., 2018), relation classiﬁcation (obamuyideand vlachos, 2019), text classiﬁcation (yu et al.,2018; geng et al., 2019), hypernymy detection (yuet al., 2020), and dialog generation (qian and yu,2019).
it has also been used to learn across distinctnlp tasks (dou et al., 2019; bansal et al., 2019) aswell as across different languages (nooralahzadehet al., 2020; li et al., 2020).
bansal et al.
(2020)show that meta-learning during self-supervised pre-training of language models leads to improved few-shot generalization on downstream tasks..holla et al.
(2020a) propose a framework forfew-shot word sense disambiguation, where thegoal is to disambiguate new words during meta-.
testing.
meta-training consists of episodes formedfrom multiple words whereas meta-testing has oneepisode corresponding to each of the test words.
they show that prototype-based methods – proto-typical networks (snell et al., 2017) and ﬁrst-orderprotomaml (triantaﬁllou et al., 2020) – obtainpromising results, in contrast with model-agnosticmeta-learning (maml) (finn et al., 2017)..memory-based models memory mechanisms(weston et al., 2014; graves et al., 2014; krotovand hopﬁeld, 2016) have recently drawn increas-ing attention.
in memory-augmented neural net-work (santoro et al., 2016b), given an input, thememory read and write operations are performedby a controller, using soft attention for reads andleast recently used access module for writes.
metanetwork (munkhdalai and yu, 2017b) uses twomemory modules: a key-value memory in com-bination with slow and fast weights for one-shotlearning.
an external memory was introduced toenhance recurrent neural network in munkhdalaiet al.
(2019), in which memory is conceptualized asan adaptable function and implemented as a deepneural network.
semantic memory has recentlybeen introduced by zhen et al.
(2020) for few-shotlearning to enhance prototypical representations ofobjects, where memory recall is cast as a variationalinference problem..in nlp, tang et al.
(2016) use content andlocation-based neural attention over external mem-ory for aspect-level sentiment classiﬁcation.
daset al.
(2017) use key-value memory for question an-swering on knowledge bases.
mem2seq (madottoet al., 2018) is an architecture for task-oriented di-alog that combines attention-based memory withpointer networks (vinyals et al., 2015).
geng et al.
(2020) propose dynamic memory induction net-works for few-shot text classiﬁcation, which uti-lizes dynamic routing (sabour et al., 2017) overa static memory module.
episodic memory hasbeen used in lifelong learning on language tasks, asa means to perform experience replay (d’autumeet al., 2019; han et al., 2020; holla et al., 2020b)..3 task and dataset.
we treat wsd as a word-level classiﬁcation prob-lem where ambiguous words are to be classiﬁedinto their senses given the context.
in traditionalwsd, the goal is to generalize to new contexts ofword-sense pairs.
speciﬁcally, the test set consistsof word-sense pairs that were seen during train-.
5256ing.
on the other hand, in few-shot wsd, thegoal is to generalize to new words and senses al-together.
the meta-testing phase involves furtheradapting the models (on the small support set) tonew words that were not seen during training andevaluates them on new contexts (using the queryset).
it deviates from the standard n -way, k-shotclassiﬁcation setting in few-shot learning since thewords may have a different number of senses andeach sense may have different number of examples(holla et al., 2020a), making it a more realisticfew-shot learning setup (triantaﬁllou et al., 2020)..dataset we use the few-shot wsd benchmarkprovided by holla et al.
(2020a).
it is based onthe semcor corpus (miller et al., 1994), annotatedwith senses from the new oxford american dic-tionary by yuan et al.
(2016).
the dataset con-sists of words grouped into meta-training, meta-validation and meta-test sets.
the meta-test setconsists of new words that were not part of meta-training and meta-validation sets.
there are foursetups varying in the number of sentences in the|s| = 4 corre-support set |s| = 4, 8, 16, 32.sponds to an extreme few-shot learning scenariofor most words, whereas |s| = 32 comes closerto the number of sentences per word encounteredin standard wsd setups.
for |s| = 4, 8, 16, 32,the number of unique words in the meta-training/ meta-validation / meta-test sets is 985/166/270,985/163/259, 799/146/197 and 580/85/129 respec-tively.
we use the publicly available standarddataset splits.2.
episodes the meta-training episodes were cre-ated by ﬁrst sampling a set of words and a ﬁxednumber of senses per word, followed by samplingexample sentences for these word-sense pairs.
thisstrategy allows for a combinatorially large numberof episodes.
every meta-training episode has |s|sentences in both the support and query sets, andcorresponds to the distinct task of disambiguatingbetween the sampled word-sense pairs.
the totalnumber of meta-training episodes is 10, 000. in themeta-validation and meta-test sets, each episodecorresponds to the task of disambiguating a single,previously unseen word between all its senses.
forevery meta-test episode, the model is ﬁne-tuned ona few examples in the support set and its generaliz-ability is evaluated on the query set.
in contrast to.
2https://github.com/nithin-holla/.
metawsd.
the meta-training episodes, the meta-test episodesreﬂect a natural distribution of senses in the cor-pus, including class imbalance, providing a realisticevaluation setting..4 methods.
4.1 model architectures.
we experiment with the same model architecturesas holla et al.
(2020a).
the model fθ, with param-eters θ, takes words xi as input and produces a per-word representation vector fθ(xi) for i = 1, ..., lwhere l is the length of the sentence.
sense pre-dictions are only made for ambiguous words usingthe corresponding word representation..glove+gru single-layer bi-directional gru(cho et al., 2014) network followed by a singlelinear layer, that takes glove embeddings (pen-nington et al., 2014) as input.
glove embed-dings capture all senses of a word.
we thus evalu-ate a model’s ability to disambiguate from sense-agnostic input..elmo+mlp a multi-layer perception (mlp)network that receives contextualized elmo embed-dings (peters et al., 2018) as input.
their contex-tualised nature makes elmo embeddings bettersuited to capture meaning variation than the staticones.
since elmo is not ﬁne-tuned, this model hasthe lowest number of learnable parameters..bert pretrained bertbase (devlin et al.,2019) model followed by a linear layer, fully ﬁne-tuned on the task.
bert underlies state-of-the-artapproaches to wsd..4.2 prototypical network.
(cid:80).
our few-shot learning approach builds upon pro-totypical networks (snell et al., 2017), which iswidely used for few-shot image classiﬁcation andhas been shown to be successful in wsd (hollait computes a prototype zk =et al., 2020a).
1k fθ(xk) of each word sense (where k is theknumber of examples for each word sense) throughan embedding function fθ, which is realized as theaforementioned architectures.
it computes a dis-tribution over classes for a query sample x givena distance function d(·, ·) as the softmax over itsdistances to the prototypes in the embedding space:.
p(yi = k|x) =.
exp(−d(fθ(x), zk))k(cid:48) exp(−d(fθ(x), zk(cid:48))).
(cid:80).
(1).
5257however, the resulting prototypes may not besufﬁciently representative of word senses as seman-tic categories when using a single deterministicvector, computed as the average of only a few ex-amples.
such representations lack expressivenessand may not encompass sufﬁcient intra-class vari-ance, that is needed to distinguish between differentﬁne-grained word senses.
moreover, large uncer-tainty arises in the single prototype due to the smallnumber of samples..4.3 variational prototype network.
variational prototype network (zhen et al., 2020)(vpn) is a powerful model for learning latent rep-resentations from small amounts of data, where theprototype z of each class is treated as a distribution.
given a task with a support set s and query set q,the objective of vpn takes the following form:.
lvpn =.
1|q|.
|q|(cid:88).
i=1.
(cid:104) 1lz.
lz(cid:88).
lz=1.
(cid:105)+ λdkl[q(z|s)||p(z|xi)].
− log p(yi|xi, z(lz)).
(2)where q(z|s) is the variational posterior over z,p(z|xi) is the prior, and lz is the number of montecarlo samples for z. the prior and posterior areassumed to be gaussian.
the re-parameterizationtrick (kingma and welling, 2013) is adopted toenable back-propagation with gradient descent, i.e.,z(lz) = f (s, (cid:15)(lz)), (cid:15)(lz) ∼ n (0, i), f (·, ·) =(cid:15)(lz) ∗ µz + σz, where the mean µz and diagonalcovariance σz are generated from the posterior in-ference network with s as input.
the amortizationtechnique is employed for the implementation ofvpn.
the posterior network takes the mean wordrepresentations in the support set s as input andreturns the parameters of q(z|s).
similarly, theprior network produces the parameters of p(z|xi)by taking the query word representation xi ∈ q asinput.
the conditional predictive log-likelihood isimplemented as a cross-entropy loss..4.4 β-variational semantic memory.
in order to leverage the shared common knowledgebetween different tasks to improve disambiguationin future tasks, we incorporate variational semanticmemory (vsm) as in zhen et al.
(2020).
it consistsof two main processes: memory recall, which re-trieves relevant information that ﬁts with speciﬁctasks based on the support set of the current task;.
figure 1: computational graph of variational semanticmemory for few-shot wsd.
m is the semantic memorymodule, s the support set, x and y are the query sampleand label, and z is the word sense prototype..memory update, which effectively collects new in-formation from the task and gradually consolidatesthe semantic knowledge in the memory.
we adopta similar memory mechanism and introduce an im-proved update rule for memory consolidation..memory recall the memory recall of vsm aimsto choose the related content from the memory, andis accomplished by variational inference.
it intro-duces latent memory m as an intermediate stochas-tic variable, and infers m from the addressed mem-ory m .
the approximate variational posteriorq(m|m, s) over the latent memory m is obtainedempirically by.
q(m|m, s) =.
γap(m|ma),.
(3).
|m |(cid:88).
a=1.
where.
γa =.
exp (cid:0)g(ma, s)(cid:1)i exp (cid:0)g(mi, s)(cid:1)(cid:80).
(4).
g(·) is the dot product, |m | is the number of mem-ory slots, ma is the memory content at slot a andstores the prototype of samples in each class, andwe take the mean representation of samples in s.the variational posterior over the prototype then.
becomes:.
˜q(z|m, s) ≈.
q(z|m(lm), s),.
(5).
1lm.
lm(cid:88).
lm=1.
where m(lm) is a monte carlo sample drawn fromthe distribution q(m|m, s), and lm is the numberof samples.
by incorporating the latent memorym from eq.
(3), we achieve the objective for varia-.
5258tional semantic memory as follows:.
achieved by scaling as follows:.
|q|(cid:88).
(cid:104).
lvsm =.
− eq(z|s,m).
(cid:2) log p(yi|xi, z)(cid:3).
i=1+ λzdkl.
(cid:2)q(z|s, m)||p(z|xi)(cid:3).
+ λmdkl.
γip(m|mi)||p(m|s)(cid:3)(cid:105).
|m |(cid:88).
(cid:2).
i.
(6)where p(m|s) is the introduced prior over m, λzand λm are the hyperparameters.
the overall com-putational graph of vsm is shown in figure 1.similarly, the posterior and prior over m are alsoassumed to be gaussian and obtained by usingamortized inference networks; more details are pro-vided in appendix a.1..memory update the memory update is to beable to effectively absorb new useful information toenrich memory content.
vsm employs an updaterule as follows:.
mc ← βmc + (1 − β) ¯mc,.
(7).
where mc is the memory content correspond-ing to class c, ¯mc is obtained using graph atten-tion (veliˇckovi´c et al., 2017), and β ∈ (0, 1) is ahyperparameter..adaptive memory update although vsm wasshown to be promising for few-shot image classi-ﬁcation, it can be seen from the experiments byzhen et al.
(2020) that different values of β haveconsiderable inﬂuence on the performance.
β de-termines the extent to which memory is updated ateach iteration.
in the original vsm, β is treatedas a hyperparameter obtained by cross-validation,which is time-consuming and inﬂexible in dealingwith different datasets.
to address this problem,we propose an adaptive memory update rule bylearning β from data using a lightweight hypernet-work (ha et al., 2016).
to be more speciﬁc, weobtain β by a function fβ(·) implemented as anmlp with a sigmoid activation function in the out-put layer.
the hypernetwork takes ¯mc as input andreturns the value of β:.
β = fβ( ¯mc).
(8).
moreover, to prevent the possibility of endlessgrowth of memory value, we propose to scale downthe memory value whenever (cid:107)mc(cid:107)2 > 1. this is.
mc =.
mcmax(1, (cid:107)mc(cid:107)2).
(9).
when we update memory, we feed the new ob-tained memory ¯mc into the hypernetwork fβ(·)and output adaptive β for the update.
we providea more detailed implementation of β-vsm in ap-pendix a.1..5 experiments and results.
experimental setup the size of the shared lin-ear layer and memory content of each word senseis 64, 256, and 192 for glove+gru, elmo+mlpand bert respectively.
the activation functionof the shared linear layer is tanh for glove+gruand relu for the rest.
the inference networksgφ(·) for calculating the prototype distribution andgψ(·) for calculating the memory distribution areall three-layer mlps, with the size of each hid-den layer being 64, 256, and 192 for glove+gru,elmo+mlp and bert.
the activation functionof their hidden layers is elu (clevert et al., 2016),and the output layer does not use any activationfunction.
each batch during meta-training includes16 tasks.
the hypernetwork fβ(·) is also a three-layer mlp, with the size of hidden state consis-tent with that of the memory contents.
the linearlayer activation function is relu for the hypernet-work.
for bert and |s| = {4, 8}, λz = 0.001,λm = 0.0001 and learning rate is 5e−6; |s| = 16,λz = 0.0001, λm = 0.0001 and learning rate is1e−6; |s| = 32, λz = 0.001, λm = 0.0001 andlearning rate is 1e−5.
hyperparameters for othermodels are reported in appendix a.2.
all the hy-perparameters are chosen using the meta-validationset.
the number of slots in memory is consistentwith the number of senses in the meta-training set– 2915 for |s| = 4 and 8; 2452 for |s| = 16; 1937for |s| = 32. the evaluation metric is the word-level macro f1 score, averaged over all episodesin the meta-test set.
the parameters are optimizedusing adam (kingma and ba, 2014)..we compare our methods against several base-lines and state-of-the-art approaches.
the near-est neighbor classiﬁer baseline (nearestneighbor)predicts a query example’s sense as the sense ofthe support example closest in the word embed-ding space (elmo and bert) in terms of co-sine distance.
the episodic ﬁne-tuning baseline(ef-protonet) is one where only meta-testing is.
5259embedding/encoder.
method.
|s| = 4.average macro f1 score|s| = 16|s| = 8.
|s| = 32.
-.
majoritysensebaseline.
0.247.
0.259.
0.264.
0.261.glove+gru.
elmo+mlp.
bert.
nearestneighboref-protonetprotonetprotofomamlβ-vsm (ours).
nearestneighboref-protonetprotonetprotofomamlβ-vsm (ours).
nearestneighboref-protonetprotonetprotofomamlβ-vsm (ours).
–0.522 ± 0.0080.579 ± 0.0040.577 ± 0.0110.597 ± 0.005.
0.6240.609 ± 0.0080.656 ± 0.0060.670 ± 0.0050.679 ± 0.006.
0.6810.594 ± 0.0080.696 ± 0.0110.719 ± 0.0050.728 ± 0.012.
–0.539 ± 0.0090.601 ± 0.0030.616 ± 0.0050.631 ± 0.004.
0.6410.635 ± 0.0040.688 ± 0.0040.700 ± 0.0040.709 ± 0.005.
0.7040.655 ± 0.0040.750 ± 0.0080.756 ± 0.0070.773 ± 0.005.
–0.538 ± 0.0030.633 ± 0.0080.626 ± 0.0050.652 ± 0.006.
0.6450.661 ± 0.0040.709 ± 0.0060.724 ± 0.0030.735 ± 0.004.
0.7160.682 ± 0.0050.755 ± 0.0020.744 ± 0.0070.776 ± 0.003.
–0.562 ± 0.0050.654 ± 0.0040.631 ± 0.0080.678 ± 0.007.
0.6540.683 ± 0.0030.731 ± 0.0060.737 ± 0.0070.758 ± 0.005.
0.7410.721 ± 0.0090.766 ± 0.0030.761 ± 0.0050.788 ± 0.003.table 1: model performance comparison on the meta-test words using different embedding functions..performed, starting from a randomly initializedmodel.
prototypical network (protonet) and proto-fomaml achieve the highest few-shot wsd per-formance to date on the benchmark of holla et al.
(2020a)..resultsin table 1, we show the average macrof1 scores of the models, with their mean and stan-dard deviation obtained over ﬁve independent runs.
our proposed β-vsm achieves the new state-of-the-art performance on few-shot wsd with all theembedding functions, across all the setups withvarying |s|.
for glove+gru, where the input issense-agnostic embeddings, our model improvesdisambiguation compared to protonet by 1.8% for|s| = 4 and by 2.4% for |s| = 32. with contextualembeddings as input, β-vsm with elmo+mlpalso leads to improvements compared to the pre-vious best protofomaml for all |s|.
holla et al.
(2020a) obtained state-of-the-art performance withbert, and β-vsm further advances this, resultingin a gain of 0.9 – 2.2%.
the consistent improve-ments with different embedding functions and sup-port set sizes suggest that our β-vsm is effectivefor few-shot wsd for varying number of shots andsenses as well as across model architectures..6 analysis and discussion.
to analyze the contributions of different compo-nents in our method, we perform an ablation studyby comparing protonet, vpn, vsm and β-vsmand present the macro f1 scores in table 2..role of variational prototypes vpn consis-tently outperforms protonet with all embeddingfunctions (by around 1% f1 score on average).
theresults indicate that the probabilistic prototypesprovide more informative representations of wordsenses compared to deterministic vectors.
the high-est gains were obtained in case of glove+gru(1.7% f1 score with |s| = 8), suggesting thatprobabilistic prototypes are particularly useful formodels that rely on static word embeddings, as theycapture uncertainty in contextual interpretation..role of variational semantic memory we showthe beneﬁt of vsm by comparing it with vpn.
vsm consistently surpasses vpn with all threeembedding functions.
according to our analysis,vsm makes the prototypes of different word sensesmore distinctive and distant from each other.
thesenses in memory provide more context informa-tion, enabling larger intra-class variations to be cap-tured, and thus lead to improvements upon vpn..role of adaptive β to demonstrate the effec-tiveness of the hypernetwork for adaptive β, wecompare β-vsm with vsm where β is tuned bycross-validation.
it can be seen from table 2 thatthere is consistent improvement over vsm.
thus,the learned adaptive β acquires the ability to deter-mine how much of the contents of memory needsto be updated based on the current new memory.
β-vsm enables the memory content of different wordsenses to be more representative by better absorb-ing information from data with adaptive update,resulting in improved performance..5260embedding/encoder.
glove+gru.
elmo+mlp.
bert.
method.
protonetvpnvsmβ-vsm.
protonetvpnvsmβ-vsm.
protonetvpnvsmβ-vsm.
|s| = 4.
0.579 ± 0.0040.583 ± 0.0050.587 ± 0.0040.597 ± 0.005.
0.656 ± 0.0060.661 ± 0.0050.670 ± 0.0060.679 ± 0.006.
0.696 ± 0.0110.703 ± 0.0110.717 ± 0.0130.728 ± 0.012.average macro f1 score|s| = 16|s| = 8.
0.601 ± 0.0030.618 ± 0.0050.625 ± 0.0040.631 ± 0.004.
0.688 ± 0.0040.694 ± 0.0060.707 ± 0.0060.709 ± 0.005.
0.750 ± 0.0080.761 ± 0.0070.769 ± 0.0060.773 ± 0.005.
0.633 ± 0.0080.641 ± 0.0070.645 ± 0.0060.652 ± 0.006.
0.709 ± 0.0060.718 ± 0.0040.726 ± 0.0050.735 ± 0.004.
0.755 ± 0.0020.762 ± 0.0040.770 ± 0.0050.776 ± 0.003.
|s| = 32.
0.654 ± 0.0040.668 ± 0.0050.670 ± 0.0050.678 ± 0.007.
0.731 ± 0.0060.741 ± 0.0040.750 ± 0.0040.758 ± 0.005.
0.766 ± 0.0030.779 ± 0.0020.784 ± 0.0020.788 ± 0.003.table 2: ablation study comparing the meta-test performance of the different variants of prototypical networks..(a) protonet.
(b) vpn.
(c) vsm.
(d) β-vsm.
figure 2: distribution of average macro f1 scores over number of senses for bert-based models with |s| = 16..variation of performance with the number ofsensesin order to further probe into the strengthsof β-vsm, we analyze the macro f1 scores of thedifferent models averaged over all the words in themeta-test set with a particular number of senses.
in figure 2, we show a bar plot of the scores ob-tained from bert for |s| = 16. for words witha low number of senses, the task corresponds toa higher number of effective shots and vice versa.
it can be seen that the different models performroughly the same for words with fewer senses, i.e.,2 – 4. vpn is comparable to protonet in its distri-bution of scores.
but with semantic memory, vsmimproves the performance on words with a highernumber of senses.
β-vsm further boosts the scoresfor such words on average.
the same trend is ob-served for |s| = 8 (see appendix a.3).
therefore,the improvements of β-vsm over protonet comefrom tasks with fewer shots, indicating that vsm isparticularly effective at disambiguation in low-shotscenarios..visualization of prototypes to study the distinc-tion between the prototype distributions of wordsenses obtained by β-vsm, vsm and vpn, wevisualize them using t-sne (van der maaten andhinton, 2008).
figure 3 shows prototype distribu-.
tions based on bert for the word draw.
differentcolored ellipses indicate the distribution of its dif-ferent senses obtained from the support set.
differ-ent colored points indicate the representations ofthe query examples.
β-vsm makes the prototypesof different word senses of the same word moredistinctive and distant from each other, with lessoverlap, compared to the other models.
notably,the representations of query examples are closerto their corresponding prototype distribution for β-vsm, thereby resulting in improved performance.
we also visualize the prototype distributions ofsimilar vs. dissimilar senses of multiple words infigure 4 (see appendix a.4 for example sentences).
the blue ellipse corresponds to the ‘set up’ senseof launch from the meta-test samples.
green andgray ellipses correspond to a similar sense of thewords start and establish from the memory.
wecan see that they are close to each other.
orangeand purple ellipses correspond to other senses ofthe words start and establish from the memory, andthey are well separated.
for a given query word,our model is thus able to retrieve related sensesfrom the memory and exploit them to make itsword sense distribution more representative anddistinctive..5261(a) vpn.
(b) vsm.
(c) β-vsm.
figure 3: prototype distributions of distinct senses of draw with different models..references.
eneko agirre, oier l´opez de lacalle, and aitor soroa.
2014. random walks for knowledge-based wordsense disambiguation.
computational linguistics,40(1):57–84..antreas antoniou, harrison edwards, and amosstorkey.
2019. how to train your maml.
in inter-national conference on learning representations..trapit bansal, rishikesh jha, and andrew mccallum.
2019. learning to few-shot learn across diverse nat-ural language classiﬁcation tasks.
arxiv preprintarxiv:1911.03863..trapit bansal, rishikesh jha, tsendsuren munkhdalai,and andrew mccallum.
2020.self-supervisedmeta-learning for few-shot natural language classiﬁ-cation tasks.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 522–534, online.
associationfor computational linguistics..y. bengio, s. bengio, and j. cloutier.
1991. learn-in ijcnn-91-seattleing a synaptic learning rule.
international joint conference on neural networks,volume ii, pages 969 vol.2–..michele bevilacqua and roberto navigli.
2020. break-ing through the 80% glass ceiling: raising the stateof the art in word sense disambiguation by incor-porating knowledge graph information.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 2854–2864,online.
association for computational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..djork-arn´e clevert, thomas unterthiner, and sepphochreiter.
2016. fast and accurate deep networkin 4thlearning by exponential linear units (elus)..figure 4: prototype distributions of similar sense oflaunch (blue), start (green) and establish (grey).
dis-tinct senses: start (orange) and establish (purple)..7 conclusion.
in this paper, we presented a model of variationalsemantic memory for few-shot wsd.
we use avariational prototype network to model the pro-totype of each word sense as a distribution.
toleverage the shared common knowledge betweentasks, we incorporate semantic memory into theprobabilistic model of prototypes in a hierarchicalbayesian framework.
vsm is able to acquire long-term, general knowledge that enables learning newsenses from very few examples.
furthermore, wepropose adaptive β-vsm which learns an adaptivememory update rule from data using a lightweighthypernetwork.
the consistent new state-of-the-artperformance with three different embedding func-tions shows the beneﬁt of our model in boostingfew-shot wsd..since meaning disambiguation is central tomany natural language understanding tasks, modelsbased on semantic memory are a promising direc-tion in nlp, more generally.
future work might in-vestigate the role of memory in modeling meaningvariation across domains and languages, as well asin tasks that integrate knowledge at different levelsof linguistic hierarchy..5262international conference on learning representa-tions, iclr 2016, san juan, puerto rico, may 2-4,2016, conference track proceedings..alex graves, greg wayne,.
2014. neural turing machines.
arxiv:1410.5401..and ivo danihelka.
arxiv preprint.
rajarshi das, manzil zaheer, siva reddy, and andrewmccallum.
2017. question answering on knowl-edge bases and text using universal schema andmemory networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 2: short papers), pages 358–365, vancouver, canada.
association for computa-tional linguistics..cyprien de masson d’autume, sebastian ruder, ling-peng kong, and dani yogatama.
2019. episodicmemory in lifelong language learning.
in advancesin neural information processing systems 32, pages13143–13152.
curran associates, inc..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..zi-yi dou, keyi yu, and antonios anastasopoulos.
2019.investigating meta-learning algorithms forlow-resource natural language understanding tasks.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1192–1197, hong kong, china.
association for computa-tional linguistics..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofin proceedings of the 34th in-deep networks.
ternational conference on machine learning, vol-ume 70 of proceedings of machine learning re-search, pages 1126–1135, international conventioncentre, sydney, australia.
pmlr..ruiying geng, binhua li, yongbin li, jian sun, andxiaodan zhu.
2020. dynamic memory inductionin pro-networks for few-shot text classiﬁcation.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1087–1094, online.
association for computational lin-guistics..ruiying geng, binhua li, yongbin li, xiaodan zhu,ping jian, and jian sun.
2019. induction networksin proceedings offor few-shot text classiﬁcation.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3904–3913, hong kong,china.
association for computational linguistics..jiatao gu, yong wang, yun chen, victor o. k. li,and kyunghyun cho.
2018. meta-learning for low-in proceed-resource neural machine translation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 3622–3631,brussels, belgium.
association for computationallinguistics..david ha, andrew dai, and quoc v le.
2016. hyper-.
networks.
arxiv preprint arxiv:1609.09106..christian hadiwinoto, hwee tou ng, and wee chunggan.
2019. improved word sense disambiguation us-ing pre-trained contextualized word representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5297–5306, hong kong, china.
association for computa-tional linguistics..xu han, yi dai, tianyu gao, yankai lin, zhiyuan liu,peng li, maosong sun, and jie zhou.
2020. contin-ual relation learning via episodic memory activationand reconsolidation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 6429–6440, online.
associationfor computational linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..nithin holla, pushkar mishra, helen yannakoudakis,and ekaterina shutova.
2020a.
learning to learnto disambiguate: meta-learning for few-shot wordin proceedings of the 2020sense disambiguation.
conference on empirical methods in natural lan-guage processing: findings, pages 4517–4533, on-line.
association for computational linguistics..nithin holla, pushkar mishra, helen yannakoudakis,and ekaterina shutova.
2020b.
meta-learning withsparse experience replay for lifelong language learn-ing.
arxiv preprint arxiv:2009.04891..luyao huang, chi sun, xipeng qiu, and xuanjinghuang.
2019. glossbert: bert for word sensedisambiguation with gloss knowledge.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 3509–3514, hongkong, china.
association for computational lin-guistics..ignacio iacobacci, mohammad taher pilehvar, androberto navigli.
2016. embeddings for word sensein proceed-disambiguation: an evaluation study.
ings of the 54th annual meeting of the association.
5263for computational linguistics (volume 1: long pa-pers), pages 897–907, berlin, germany.
associationfor computational linguistics..mikael k˚ageb¨ack and hans salomonsson.
2016. wordsense disambiguation using a bidirectional lstm.
in proceedings of the 5th workshop on cognitiveaspects of the lexicon (cogalex - v), pages 51–56,osaka, japan.
the coling 2016 organizing com-mittee..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..diederik p kingma and max welling.
2013. auto-arxiv preprint.
encoding variational bayes.
arxiv:1312.6114..gregory koch, richard zemel, and ruslan salakhutdi-nov. 2015. siamese neural networks for one-shot im-age recognition.
in icml deep learning workshop,volume 2. lille..dmitry krotov and john j hopﬁeld.
2016. dense as-arxiv.
sociative memory for pattern recognition.
preprint arxiv:1606.01164..sawan kumar, sharmistha jat, karan saxena, andpartha talukdar.
2019. zero-shot word sense dis-ambiguation using sense deﬁnition embeddings.
inproceedings oftheassociation for computational linguistics, pages5670–5681, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
michael lesk.
1986. automatic sense disambiguationusing machine readable dictionaries: how to tellin proceed-a pine cone from an ice cream cone.
ings of the 5th annual international conference onsystems documentation, sigdoc ’86, page 24–26,new york, ny, usa.
association for computingmachinery..zheng li, mukul kumar, william headden, bing yin,ying wei, yu zhang, and qiang yang.
2020. learnto cross-lingual transfer with meta graph learningacross heterogeneous languages.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 2290–2301, online.
association for computational lin-guistics..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(11)..andrea madotto, chien-sheng wu, and pascale fung.
2018. mem2seq: effectively incorporating knowl-edge bases into end-to-end task-oriented dialog sys-tems.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1468–1478, melbourne,australia.
association for computational linguis-tics..oren melamud, jacob goldberger, and ido dagan.
2016. context2vec: learning generic context em-in proceedingsbedding with bidirectional lstm.
of the 20th signll conference on computationalnatural language learning, pages 51–61, berlin,germany.
association for computational linguis-tics..george a. miller, richard beckwith, christiane fell-baum, derek gross, and katherine miller.
1990.wordnet: an on-line lexical database.
internationaljournal of lexicography, 3:235–244..george a. miller, martin chodorow, shari landes,claudia leacock, and robert g. thomas.
1994. us-ing a semantic concordance for sense identiﬁcation.
in human language technology: proceedings of aworkshop held at plainsboro, new jersey, march 8-11, 1994..andrea moro, alessandro raganato, and roberto nav-igli.
2014. entity linking meets word sense disam-biguation: a uniﬁed approach.
transactions of theassociation for computational linguistics, 2:231–244..tsendsuren munkhdalai, alessandro sordoni, tongwang, and adam trischler.
2019. metalearned neu-ral memory.
in advanced in neural information pro-cessing systems..tsendsuren munkhdalai and hong yu.
2017a.
metanetworks.
in proceedings of the 34th internationalconference on machine learning, volume 70 ofproceedings of machine learning research, pages2554–2563, international convention centre, syd-ney, australia.
pmlr..tsendsuren munkhdalai and hong yu.
2017b.
metanetworks.
in proceedings of the 34th internationalconference on machine learning, proceedings ofmachine learning research, pages 2554–2563, in-ternational convention centre, sydney, australia.
pmlr..tsendsuren munkhdalai, xingdi yuan, soroush mehri,and adam trischler.
2018. rapid adaptation withconditionally shifted neurons.
in international con-ference on machine learning, pages 3664–3673.
pmlr..roberto navigli.
2009. word sense disambiguation: a.survey.
acm computing surveys, 41(2):1–69..alex nichol, joshua achiam, and john schulman.
on ﬁrst-order meta-learning algorithms..2018.arxiv preprint arxiv:1803.02999..farhad nooralahzadeh, giannis bekoulis, johannesbjerva, and isabelle augenstein.
2020. zero-shotin pro-cross-lingual transfer with meta learning.
ceedings of the 2020 conference on empirical meth-ods in natural language processing (emnlp),pages 4547–4562, online.
association for compu-tational linguistics..5264abiola obamuyide and andreas vlachos.
2019.model-agnostic meta-learning for relation classiﬁca-tion with limited supervision.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5873–5879, florence,italy.
association for computational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543, doha, qatar.
asso-ciation for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..kun qian and zhou yu.
2019. domain adaptive dia-log generation via meta learning.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 2639–2649, florence,italy.
association for computational linguistics..alessandro raganato, jose camacho-collados, androberto navigli.
2017a.
word sense disambigua-tion: a uniﬁed evaluation framework and empiri-in proceedings of the 15th con-cal comparison.
ference of the european chapter of the associationfor computational linguistics: volume 1, long pa-pers, pages 99–110, valencia, spain.
association forcomputational linguistics..alessandro raganato, claudio delli bovi, and robertonavigli.
2017b.
neural sequence learning mod-in proceed-els for word sense disambiguation.
ings of the 2017 conference on empirical methodsin natural language processing, pages 1156–1167,copenhagen, denmark.
association for computa-tional linguistics..sachin ravi and hugo larochelle.
2017. optimiza-tion as a model for few-shot learning.
in 5th inter-national conference on learning representations,iclr 2017, toulon, france, april 24-26, 2017, con-ference track proceedings..eleanor rosch.
1975. cognitive representations of se-mantic categories.
journal of experimental psychol-ogy: general, 104:192–233..sascha rothe and hinrich sch¨utze.
2015. autoex-tend: extending word embeddings to embeddingsin proceedings of thefor synsets and lexemes.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 1: long papers), pages 1793–1803, beijing,china.
association for computational linguistics..sara sabour, nicholas frosst, and geoffrey e hinton.
in ad-2017. dynamic routing between capsules.
vances in neural information processing systems30, pages 3856–3866..adam santoro, sergey bartunov, matthew botvinick,daan wierstra, and timothy lillicrap.
2016a.
meta-learning with memory-augmented neural networks.
in international conference on machine learning,pages 1842–1850.
pmlr..adam santoro, sergey bartunov, matthew botvinick,daan wierstra, and timothy lillicrap.
2016b.
meta-learning with memory-augmented neural networks.
in proceedings of the 33rd international confer-ence on machine learning, volume 48 of proceed-ings of machine learning research, pages 1842–1850, new york, new york, usa.
pmlr..bianca scarlini, tommaso pasini, and roberto nav-igli.
2020. with more contexts comes better per-formance: contextualized sense embeddings forin proceed-all-round word sense disambiguation.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages3528–3539, online.
association for computationallinguistics..jurgen schmidhuber.
1987. evolutionary principles inself-referential learning.
on learning now to learn:the meta-meta-meta...-hook.
diploma thesis, tech-nische universitat munchen, germany, 14 may..jake snell, kevin swersky, and richard zemel.
2017.prototypical networks for few-shot learning.
in ad-vances in neural information processing systems30, pages 4077–4087..flood sung, yongxin yang, li zhang, tao xiang,philip hs torr, and timothy m hospedales.
2018.learning to compare: relation network for few-shotin proceedings of the ieee conferencelearning.
on computer vision and pattern recognition, pages1199–1208..duyu tang, bing qin, and ting liu.
2016. aspectlevel sentiment classiﬁcation with deep memory net-in proceedings of the 2016 conference onwork.
empirical methods in natural language processing,pages 214–224, austin, texas.
association for com-putational linguistics..sebastian thrun and lorien pratt, editors.
1998. learn-ing to learn.
kluwer academic publishers, usa..eleni triantaﬁllou, tyler zhu, vincent dumoulin, pas-cal lamblin, utku evci, kelvin xu, ross goroshin,carles gelada, kevin swersky, pierre-antoine man-zagol, and hugo larochelle.
2020. meta-dataset: adataset of datasets for learning to learn from few ex-in international conference on learningamples.
representations..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro lio, and yoshua bengio.
2017. graph attention networks.
arxiv preprintarxiv:1710.10903..5265oriol vinyals, charles blundell, timothy lillicrap, ko-ray kavukcuoglu, and daan wierstra.
2016. match-in advancesing networks for one shot learning.
in neural information processing systems 29, pages3630–3638..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems, volume 28, pages2692–2700.
curran associates, inc..jason weston, sumit chopra, and antoine bor-arxiv preprint.
des.
2014. memory networks.
arxiv:1410.3916..changlong yu, jialong han, haisong zhang, and wil-fred ng.
2020. hypernymy detection for low-in proceed-resource languages via meta learning.
ings of the 58th annual meeting of the associationfor computational linguistics, pages 3651–3656,online.
association for computational linguistics..mo yu, xiaoxiao guo, jinfeng yi, shiyu chang, salonipotdar, yu cheng, gerald tesauro, haoyu wang,and bowen zhou.
2018. diverse few-shot text clas-siﬁcation with multiple metrics.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 1206–1215, new orleans, louisiana.
association for computational linguistics..dayu yuan, julian richardson, ryan doherty, colinevans, and eric altendorf.
2016. semi-supervisedword sense disambiguation with neural models.
inproceedings of coling 2016,the 26th interna-tional conference on computational linguistics:technical papers, pages 1374–1385, osaka, japan.
the coling 2016 organizing committee..xiantong zhen, yingjun du, huan xiong, qiang qiu,cees snoek, and ling shao.
2020. learning tolearn variational semantic memory.
in proceedingsof neurips..zhi zhong and hwee tou ng.
2010. it makes sense:a wide-coverage word sense disambiguation systemfor free text.
in proceedings of the acl 2010 sys-tem demonstrations, pages 78–83, uppsala, swe-den.
association for computational linguistics..a appendix.
a.1.
implementation details.
in the meta-training phase, we implement β-vsmby end-to-end learning with stochastic neural net-works.
the inference network and hypernetworkare parameterized by a feed-forward multi-layerperceptrons (mlp).
at meta-train time, we ﬁrstextract the features of the support set via fθ(xs),where fθ is the feature extraction network and weuse permutation-invariant instance-pooling oper-ations to get the mean feature ¯f sc of samples in.
c.c , f 2.c , f 1.c , .
.
.
, f nc.
c = mc, f i>0.
c }, where f nc.
the c-th class.
then we get the memory ma byusing the support representation ¯f sc of each class.
the memory obtained ma will be fed into a smallthree-layers mlp network gψ(·) to calculate themean µm and variance σm of the memory dis-tribution m, which is then used to sample thememory m by m ∼ n (µm, diag((σm)2)).
thenew memory ¯mc is obtained by using graph at-tention.
the nodes of the graph are a set of fea-ture representations of the current task samples:∈ rd,fc = {f 0nc = |sc ∪ qc|, f 0c).
nccontains all samples including both the supportand query set from the c-th category in the currenttask.
when we update memory, we take the newobtained memory ¯mc into the hypernetwork fβ(·)as input and output the adaptive β to update thememory using equation 8. we calculate the pro-totype of the latent distribution, i.e., the mean µzand variance σz by another small three-layer mlpnetwork gφ(·, ·), whose inputs are ¯f sc and m. thenthe prototype z(lz) is sampled from the distributionz(lz) ∼ n (µz, diag((σz)2)).
by using the pro-totypical word sense of support samples and thefeature embedding of query sample xi, we obtainthe predictive value ˆyi..c = fθ(xi.
at meta-test time, we feed the support represen-tation ¯f sc into the gψ(·) to generate the memory ma.
then, using the sampled memory ma and the sup-port representation ¯f sc , we obtain the distributionof prototypical word sense z. finally, we makepredictions for the query sample by using the queryrepresentation extracted from embedding functionand the support prototype z..a.2 hyperparameters and runtimes.
we present our hyperparameters in table 3. formonte carlo sampling, we set different lz and lmfor the each embedding function and |s|, whichare chosen using the validation set.
training timediffers for different |s| and different embeddingfunctions.
here we give the training time perepoch for |s| = 16. for glove+gru, the ap-proximate training time per epoch is 20 minutes;for elmo+mlp it is 80 minutes; and for bert,it is 60 minutes.
the number of meta-learned pa-rameters for glove+gru is θ are 889, 920; forelmo+mlp it is 262, 404; and for bert it is θare 107, 867, 328. we implemented all models us-ing the pytorch framework and trained them on annvidia tesla v100..5266embedding/encoder.
|s|.
learningrate.
λz.
λm.
l‡.
lm.
glove+gru.
elmo+mlp.
bert.
481632.
481632.
481632.
1e−51e−51e−41e−4.
1e−51e−51e−41e−4.
5e−65e−61e−61e−4.
1e−31e−31e−41e−3.
1e−41e−41e−31e−3.
1e−31e−31e−41e−3.
1e−41e−41e−31e−3.
1e−41e−41e−31e−3.
1e−41e−41e−41e−4.
200200150150.
200200150150.
200200150150.
150150150150.
150150150150.
200200150100.table 3: hyperparameters used for training the models..a.3 variation of performance with the.
number of senses.
to further demonstrate that β-vsm achieves betterperformance in extremely data scarce scenarios, wealso analyze variation of macro f1 scores with thenumber of senses for bert and |s| = 8. in fig-ure 5, we observe a similar trend as with |s| = 16.β-vsm has an improved performance for wordswith many senses, which corresponds to a low-shotscenario.
for example, with 8 senses, the task isessentially one-shot..a.4 example sentences to visualize.
prototypes.
in table 4, we provide some example sentencesused to generate the plots in figure 4. these exam-ples correspond to words launch, start and estab-lish, and contain senses ‘set up’, ‘begin’ and ‘buildup’..a.5 results on the meta-validation set.
we provide the results on the on the meta-validationset in the table 5, to better facilitate reproducibility..5267(a) protonet.
(b) vpn.
(c) vsm.
(d) β-vsm.
figure 5: distribution of average macro f1 scores over number of senses for bert-based models with |s| = 8..word.
launchlaunch.
sense.
set upset up.
sentence.
the corinthian yacht club in tiburon launches its winter races nov. 5.the most infamous of all was launched by the explosion of the islandof krakatoa in 1883; it raced across the paciﬁc at 300 miles an hourdevastated the coasts of java and sumatra with waves 100 to 130 feethigh, and pounded the shore as far away as san francisco.
in several signiﬁcant cases, such as india, a decade of concentrated effortcan launch these countries into a stage in which they can carry forwardtheir own economic and social progress with little or no government-to-government assistance.
with these maps completed, the inventory phase of the plan has beenstarted.
congress starts another week tomorrow with sharply contrasting fore-casts for the two chambers.
for the convenience of guests bundle centers have been establishedthroughout the city and suburbs where the donations may be depositedbetween now and the date of the big event.
from the outset of his ﬁrst term, he established himself as one of theguiding spirits of the house of delegates..launch.
set up.
start.
start.
set up.
begin.
establish.
set up.
establish.
build up.
table 4: example sentences for different word-sense pairs used to generate the visualization in figure 4..embedding/encoder.
glove+gru.
elmo+mlp.
bert.
method.
protonetvpnvsmβ-vsm.
protonetvpnvsmβ-vsm.
protonetvpnvsmβ-vsm.
|s| = 4.
0.591 ± 0.0080.602 ± 0.0040.617 ± 0.0050.622 ± 0.005.
0.682 ± 0.0080.689 ± 0.0040.693 ± 0.0050.701 ± 0.006.
0.742 ± 0.0070.752 ± 0.0110.767 ± 0.0090.771 ± 0.008.average macro f1 score|s| = 16|s| = 8.
0.615 ± 0.0010.624 ± 0.0040.635 ± 0.0050.649 ± 0.004.
0.701 ± 0.0070.709 ± 0.0060.712 ± 0.0070.723 ± 0.005.
0.759 ± 0.0130.769 ± 0.0050.778 ± 0.0050.784 ± 0.006.
0.638 ± 0.0070.646 ± 0.0060.649 ± 0.0040.657 ± 0.005.
0.741 ± 0.0070.749 ± 0.0050.754 ± 0.0060.760 ± 0.005.
0.786 ± 0.0040.793 ± 0.0030.801 ± 0.0060.810 ± 0.004.
|s| = 32.
0.634 ± 0.0060.651 ± 0.0050.673 ± 0.0060.680 ± 0.006.
0.722 ± 0.0110.748 ± 0.0040.755 ± 0.0060.761 ± 0.004.
0.770 ± 0.0090.785 ± 0.0040.815 ± 0.0050.829 ± 0.004.table 5: average macro f1 scores of the meta-validation words..5268