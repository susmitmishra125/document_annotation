how to adapt your pretrained multilingual model to 1600 languages.
abteen ebrahimi and katharina kannuniversity of colorado boulder{abteen.ebrahimi,katharina.kann}@colorado.edu.
abstract.
pretrained multilingual models (pmms) en-able zero-shot learning via cross-lingual trans-fer, performing best for languages seen dur-ing pretraining.
while methods exist to im-prove performance for unseen languages, theyhave almost exclusively been evaluated usingamounts of raw text only available for a smallfraction of the world’s languages.
in this paper,we evaluate the performance of existing meth-ods to adapt pmms to new languages usinga resource available for over 1600 languages:the new testament.
this is challenging fortwo reasons: (1) the small corpus size, and(2) the narrow domain.
while performancedrops for all approaches, we surprisingly stillsee gains of up to 17.69% accuracy for part-of-speech tagging and 6.29 f1 for ner on aver-age over all languages as compared to xlm-r.another unexpected ﬁnding is that continuedpretraining, the simplest approach, performsbest.
finally, we perform a case study to dis-entangle the effects of domain and size and toshed light on the inﬂuence of the ﬁnetuningsource language..1.introduction.
pretrained multilingual models (pmms) are astraightforward way to enable zero-shot learningvia cross-lingual transfer, thus eliminating the needfor labeled data for the target task and language.
however, downstream performance is highest forlanguages that are well represented in the pretrain-ing data or linguistically similar to a well repre-sented language.
performance degrades as repre-sentation decreases, with languages not seen duringpretraining generally having the worst performance.
in the most extreme case, when a language’s scriptis completely unknown to the model, zero-shot per-formance is effectively random..while multiple methods have been shown toimprove the performance of transfer to underrep-.
figure 1: the number of space-separated words in thebible and wikipedia for six low-resource languagesused in our experiments; plotted on a log scale..resented languages (cf.
section 2.3), previouswork has evaluated them using unlabeled data fromsources available for a relatively small number oflanguages, such as wikipedia or common crawl,which cover 3161 and 1602 languages, respectively.
due to this low coverage, the languages that wouldmost beneﬁt from these methods are precisely thosewhich do not have the necessary amounts of mono-lingual data to implement them as-is.
to enablethe use of pmms for truly low-resource languages,where they can, e.g., assist language documentationor revitalization, it is important to understand howstate-of-the-art adaptation methods act in a settingmore broadly applicable to many languages..in this paper, we ask the following question: canwe use the bible – a resource available for roughly1600 languages – to improve a pmm’s zero-shotperformance on an unseen target language?
and,if so, what adaptation method works best?
weinvestigate the performance of xlm-r (conneau.
1https://en.wikipedia.org/wiki/list_.
of_wikipedias.
2https://commoncrawl.github.io/cc-crawl-statistics/plots/languages.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4555–4567august1–6,2021.©2021associationforcomputationallinguistics4555et al., 2020) when combined with continued pre-training (chau et al., 2020), vocabulary extension,(wang et al., 2020), and adapters (pfeiffer et al.,2020b) making the following assumptions: (1) theonly text available in a target language is the newtestament, and (2) no annotated training data existsin the target language..we present results on 2 downstream tasks – part-of-speech (pos) tagging and named entity recog-nition (ner) – on a typologically diverse set of30 languages, all of which are unseen during thepretraining of xlm-r. we ﬁnd that, surprisingly,even though we use a small corpus from a narrowdomain, most adaptation approaches improve overxlm-r’s base performance, showing that the bibleis a valuable source of data for our purposes.
wefurther observe that in our setting the simplest adap-tation method, continued pretraining, performs bestfor both tasks, achieving gains of up to 17.69% ac-curacy for pos tagging, and 6.29 f1 for ner onaverage across languages..additionally, we seek to disentangle the effectsof two aspects of our experiments on downstreamperformance: the selection of the source language,and the restricted domain of the new testament.
towards this, we conduct a case study focusingon three languages with cyrillic script: bashkir,chechen, and chuvash.
in order to understand theeffect of the choice of source language, we use amore similar language, russian, as our source oflabeled data.
to explore the effect of the new tes-tament’s domain, we conduct our pretraining exper-iments with an equivalent amount of data sampledfrom the wikipedia in each language.
we ﬁnd thatchanging the source language to russian increasesaverage baseline performance by 18.96 f1, and weachieve the highest results across all settings whenusing both wikipedia and russian data..2 related work.
2.1 background.
prior to the introduction of pmms, cross-lingualtransfer was often based on word embeddings(mikolov et al., 2013).
joulin et al.
(2018) presentmonolingual embeddings for 294 languages usingwikipedia, succeeded by grave et al.
(2018) whopresent embeddings for 157 languages trained onadditional data from common crawl.
for cross-lingual transfer, monolingual embeddings can thenbe aligned using existing parallel resources, or in acompletely unsupervised way (bojanowski et al.,.
code.
language.
script.
language family.
task.
acehneseegyptian arabicbashkirbambaracebuanochechenchuvashcopticcrimean turkishmanxancient greekswiss germanhakka chineseigboilokokinyarwanda.
acearzbakbamcebchechvcopcrhglvgrcgswhakiboilokinmag magahimhrminmltmrimyvndsoryscotattgkwarwolyor.
eastern mariminangkabaumaltesemaorierzyalow germanodiascotstatartajikwaraywolofyoruba.
austronesianlatinafro-asiaticarabiccyrillicturkiclatin, n’ko mandelatincyrilliccyrilliccopticcyrilliclatingreeklatinchineselatinlatinlatindevanagaricyrilliclatinlatinlatincyrilliclatinodialatincyrilliccyrilliclatinlatinlatin.
nernernerposaustronesiannernortheast caucasian nernerturkicposancient egyptiannerturkicposindo-europeanposindo-europeanposindo-europeannersino-tibetannerniger-congoneraustronesiannerniger-congoposindo-iranianneruralicneraustronesianbothafro-asiaticneraustronesianposuralicnerindo-europeannerindo-iraniannerindo-europeannerturkicnerindo-iranianneraustronesianbothniger-congobothniger-congo.
table 1: languages used in our experiments, none ofwhich are represented in xlm-r’s pretraining data..2017; artetxe et al., 2017; lample et al., 2017;conneau et al., 2017; artetxe et al., 2016).
al-though they use transformer based models, artetxeet al.
(2020) also transfer in a monolingual set-ting.
another method for cross-lingual transfer in-volves multilingual embeddings, where languagesare jointly learned as opposed to being aligned(ammar et al., 2016; artetxe and schwenk, 2019).
for a more in-depth look at cross-lingual word em-beddings, we refer the reader to ruder et al.
(2019).
while the above works deal with generally im-proving cross-lingual representations, task-speciﬁccross-lingual systems often show strong perfor-mance in a zero-shot setting.
for pos tagging,in a similar setting to our work, eskander et al.
(2020) achieve strong zero-shot results by using un-supervised projection (yarowsky et al., 2001) withaligned bibles.
recent work for cross-lingual nerincludes mayhew et al.
(2017) who use dictionarytranslations to create target-language training data,as well as xie et al.
(2018) who use a bilingualdictionary in addition to self-attention.
bharad-waj et al.
(2016) use phoneme conversion to aidcross-lingual ner in a zero-shot setting.
morerecently, bari et al.
(2020) propose a model onlyusing monolingual data for each language, and qiet al.
(2020) propose a language-agnostic toolkitsupporting ner for 66 languages.
in contrast tothese works, we focus on the improvements offered.
4556by adaptation methods for pretrained models forgeneral tasks..2.2 pretrained multilingual models.
pmms can be seen as the natural extension ofmultilingual embeddings to pretrained transformer-based models.
mbert was the ﬁrst pmm, cover-ing the 104 languages with the largest wikipedias.
it uses a 110k byte-pair encoding (bpe) vocabu-lary (sennrich et al., 2016) and is pretrained onboth a next sentence prediction and a masked lan-guage modeling (mlm) objective.
languages withsmaller wikipedias are upsampled and highly rep-resented languages are downsampled.
xlm is apmm trained on 15 languages.
xlm similarlytrains on wikipedia data, using a bpe vocabularywith 95k subwords and up- and downsamples lan-guages similarly to mbert.
xlm also introducestranslation language modeling (tlm), a supervisedpretraining objective, where tokens are masked asfor mlm, but parallel sentences are concatenatedsuch that the model can rely on subwords in bothlanguages for prediction.
finally, xlm-r is animproved version of xlm.
notable differencesinclude the larger vocabulary of 250k subwordscreated using sentencepiece tokenization (kudoand richardson, 2018) and the training data, whichis taken from commoncrawl and is considerablymore than for mbert and xlm.
xlm-r reliessolely on mlm for pretraining and achieves state-of-the-art results on multiple benchmarks (conneauet al., 2020).
we therefore focus solely on xlm-rin our experiments..downstream performance of pmms whilepires et al.
(2019) and wu and dredze (2019) showthe strong zero-shot performance of mbert, wuand dredze (2020) shine light on the difference inperformance between well and poorly representedlanguages after ﬁnetuning on target-task data..muller et al.
(2020) observe varying zero-shotperformance of mbert on different languagesnot present in its pretraining data.
they groupthem into ‘easy’ languages, on which mbert per-forms well without any modiﬁcation, ‘medium’ lan-guages, on which mbert performs well after addi-tional pretraining on monolingual data, and ‘hard’languages, on which mbert’s performs poorlyeven after modiﬁcation.
they additionally notethe importance of script, ﬁnding that transliteratinginto latin offers improvements for some languages.
as transliteration involves language speciﬁc tools,.
we consider it out of scope for this work, and leavefurther investigation in how to best utilize translit-eration for future work.
lauscher et al.
(2020)focus on pmm ﬁnetuning, and ﬁnd that for un-seen languages, gathering labeled data for few-shotlearning may be more effective than gathering largeamounts of unlabeled data..additionally, chau et al.
(2020), wang et al.
(2020), and pfeiffer et al.
(2020b) present the adap-tation methods whose performance we investigatehere in a setting where only the bible is available.
we give a general overview of these methods in theremainder of this section, before describing theirapplication in our experiments in section 3..2.3 adaptation methods.
continued pretraining in a monolingual set-ting, continued pretraining of a language represen-tation model on an mlm objective has shown tohelp downstream performance on tasks involvingtext from a domain distant from the pretraining cor-pora (gururangan et al., 2020).
in a multilingualsetting, it has been found that, given a target lan-guage, continued pretraining on monolingual datafrom that language can lead to improvements ondownstream tasks (chau et al., 2020; muller et al.,2020)..vocabulary extension many pretrained mod-els make use of a subword vocabulary, whichstrongly reduces the issue of out-of-vocabulary to-kens.
however, when the pretraining and target-task domains differ, important domain-speciﬁcwords may be over-fragmented, which reduces per-formance.
in the monolingual setting, zhang et al.
(2020) show that extending the vocabulary with in-domain tokens yields performance gains.
a similarresult to that of continued pretraining holds in themultilingual setting: downstream performance ofan underrepresented language beneﬁts from addi-tional tokens in the vocabulary, allowing for betterrepresentation of that language.
wang et al.
(2020)ﬁnd that extending the vocabulary of mbert withnew tokens and training on a monolingual corpusyields improvements for a target language, regard-less of whether the language was seen or unseen.
chau et al.
(2020) have similar results, and intro-duce tiered vocabulary augmentation, where newembeddings are learned with a higher learning rate.
while both approaches start with a random initial-ization, they differ in the amount of new tokensadded: wang et al.
(2020) limit new subwords to.
455730,000, while chau et al.
(2020) set a limit of 99,selecting the subwords which reduce the numberof unknown tokens while keeping the subword-to-token ratio similar to the original vocabulary..adapters adapters are layers with a small num-ber of parameters, injected into models to helptransfer learning (rebufﬁ et al., 2017).
houlsbyet al.
(2019) demonstrate the effectiveness of task-speciﬁc adapters in comparison to standard ﬁne-tuning.
pfeiffer et al.
(2020b) present invertibleadapters and mad-x, a framework utilizing themalong with language and task adapters for cross-lingual transfer.
after freezing model weights, in-vertible and language adapters for each language,including english, are trained together using mlm.
the english-speciﬁc adapters are then used alongwith a task adapter to learn from labeled englishdata.
for zero-shot transfer, the invertible and lan-guage adapters are replaced with those trained onthe target language, and the model is subsequentlyevaluated..3 experiments.
3.1 data and languages.
unlabeled data we use the johns hopkins uni-versity bible corpus (jhubc) from mccarthyet al.
(2020), which contains 1611 languages, pro-viding verse-aligned translations of both the oldand new testament.
however, the new testamentis much more widely translated: 86% of transla-tions do not include the old testament.
we there-fore limit our experiments to the new testament,which accounts to about 8000 verses in total, al-though speciﬁc languages may not have transla-tions of all verses.
for the 30 languages we con-sider, this averages to around 402k subword tokensper language.
the speciﬁc versions of the bible weuse are listed in table 5..labeled data for ner, we use the splits ofrahimi et al.
(2019), which are created from thewikiann dataset (pan et al., 2017).
for pos tag-ging, we use data taken from the universal depen-dencies project (nivre et al., 2020).
as xlm-r uti-lizes a subword vocabulary, we perform sequencelabeling by assigning labels to the last subwordtoken of each word.
for all target languages, weonly ﬁnetune on labeled data in english..guages for which a test set exists for either down-stream task and we have a bible for.
we thenﬁlter these languages by removing those presentin the pretraining data of xlm-r. see table 1 fora summary of languages, their attributes, and thedownstream task we use them for..3.2 pmm adaptation methods.
our goal is to analyze state-of-the-art pmm adap-tion approaches in a true low-resource settingwhere the only raw text data available comes fromthe new testament and no labeled data exists atall.
we now describe our implementation of thesemethods.
we focus on the base version of xlm-r(conneau et al., 2020) as our baseline pmm..continued pretraining we consider three mod-els based on continued pretraining.
in the sim-plest case, +mlm, we continue training xlm-rwith an mlm objective on the available verses ofthe new testament.
additionally, as bible trans-lations are a parallel corpus, we also consider amodel, +tlm, trained using translation languagemodeling.
finally, following the ﬁndings of lam-ple and conneau (2019), we also consider a modelusing both tlm and mlm, +{m|t}lm.
for thismodel, we alternate between batches consistingsolely of verses from the target bible and batchesconsisting of aligned verses of the target-languageand source-language bible.
for ner, we pretrain+mlm and +tlm models for 40 epochs, and pre-train +{m|t}lm models for 20 epochs.
for postagging, we follow a simlar pattern, training +mlmand +tlm for 80 epochs, and +{m|t}lm for 40epochs..vocabulary extension to extend the vocabularyof xlm-r, we implement the process of wang et al.
(2020).
we denote this as +extend.
for each tar-get language, we train a new sentencepiece (kudoand richardson, 2018) tokenizer on the bible ofthat language with a maximum vocabulary size of30,000.3 to prevent adding duplicates, we ﬁlterout any subword already present in the vocabularyof xlm-r. we then add additional pieces repre-senting these new subwords into the tokenizer ofxlm-r, and increase xlm-r’s embedding matrixaccordingly using a random initialization.
finally,we train the embeddings using mlm on the bible.
for ner, we train +extend models for 40 epochs,and for pos tagging, we train for 80 epochs..language selection to select the languages forour experiments, we ﬁrst compile lists of all lan-.
3we note that for many languages, the tokenizer cannotcreate the full 30,000 subwords due to the limited corpus size..4558adapters for adapters, we largely follow the fullmad-x framework (pfeiffer et al., 2020b), usinglanguage, invertible, and task adapters.
this isdenoted as +adapters.
to train task adapters,we download language and invertible adapters forthe source language from adapterhub (pfeifferet al., 2020a).
we train a single task adapter foreach task, and use it across all languages.
we trainlanguage and invertible adapters for each targetlanguage by training on the target bible with anmlm objective.
as before, for ner we train for40 epochs, and for pos we train for 80 epochs..3.3 hyperparameters and training details.
for ﬁnetuning, we train using 1 nvidia v100 32gbgpu, and use an additional gpu for adaptationmethods.
experiments for ner and pos takearound 1 and 2 hours respectively, totalling to 165total training hours, and 21.38 kgco2eq emitted(lacoste et al., 2019).
all experiments are run us-ing the huggingface transformers library (wolfet al., 2020).
we limit sequence lengths to 256tokens..we select initial hyperparameters for ﬁnetuningby using the english pos development set.
wethen ﬁx all hyperparameters other than the numberof epochs, which we tune using the 3 languageswhich have development sets, ancient greek, mal-tese, and wolof.
we do not use early stopping.
forour ﬁnal results, we ﬁnetune for 5 epochs with abatch size of 32, and a learning rate of 2e-5.
weuse the same hyperparameters for both tasks..for each task and adaptation approach, wesearch over {10, 20, 40, 80} epochs, and selectthe epoch which gives the highest average perfor-mance across the development languages.
we usethe same languages as above for pos.
for nerwe use 4 languages with varying baseline perfor-mances: bashkir, kinyarwanda, maltese, and scots.
we pretrain with a learning rate of 2e-5 and a batchsize of 32, except for +adapters, for which weuse a learning rate of 1e-4 (pfeiffer et al., 2020b)..4 results.
we present results for ner and pos tagging in ta-bles 2 and 3, respectively.
we additionally provideplots of the methods’ performances as compared tothe xlm-r baseline in figures 2 and 3, showingperformance trends for each model..ner we ﬁnd that methods based on our moststraightforward approach, continued pretraining.
lang.
xlm-r +mlm +tlm +{m|t}lm +extend +adapters.
acearzbakcebchechvcrhhakiboilokinmhrminmltmrindsoryscotattgkwaryor.
avg.
∆ avg..31.9549.8030.3451.6414.8446.9039.2731.3644.8856.2557.3945.7444.1348.8011.9562.7531.4976.4036.6322.9257.8752.91.
43.010.00.
38.10 37.2954.05 51.9436.10 32.2853.48 54.1216.26 15.0864.34 66.6736.56 43.7736.07 43.9550.19 48.0661.47 63.8961.21 60.8748.39 46.2942.91 46.7660.00 62.8431.93 48.2863.37 70.8825.64 24.2474.51 73.5651.52 50.2332.68 36.5963.11 70.2238.79 36.67.
47.30 49.296.294.29.
38.0653.3337.9952.9013.7259.2336.6940.3650.3966.0656.5443.4441.4358.4828.2768.5328.9574.4250.5633.9864.0035.29.
46.943.93.
37.5444.5329.9749.3114.4748.7843.0127.5343.4861.9548.9536.7842.4945.6720.8964.6322.1374.9033.7835.6565.7941.41.
42.44-0.57.
33.5636.0733.8052.5118.9354.3632.2828.6742.9652.6356.1340.7541.7035.2614.7459.2125.4465.8743.7036.8664.3233.87.
41.07-1.94.table 2: f1 score for all models on ner..(+mlm, +tlm, +{m|t}lm), perform best, with3.93 to 6.29 f1 improvement over xlm-r. both+extend and +adapters obtain a lower aver-age f1 than the xlm-r baseline, which shows thatthey are not a good choice in our setting: eitherthe size or the domain of the bible causes themto perform poorly.
focusing on the script of thetarget language (cf.
table 1), the average perfor-mance gain across all models is higher for cyrilliclanguages than for latin languages.
therefore,in relation to the source language script, perfor-mance gain is higher for target languages with amore distant script from the source.
when consid-ering approaches which introduce new parameters,+extend and +adapters, performance only in-creases for cyrillic languages and decreases forall others.
however, when considering continuedpretraining approaches, we ﬁnd a performance in-crease for all scripts..looking at figure 2, we see that the lower thebaseline f1, the larger the improvement of the adap-tion methods on downstream performance, with allmethods increasing performance for the languagefor which the baseline is weakest.
as baseline per-formance increases, the beneﬁt provided by thesemethods diminishes, and all methods underperformthe baseline for scots, the language with the high-est baseline performance.
we hypothesize that at.
4559figure 2: ner results (f1).
trendlines are created using linear regression..this point the content of the bible offers little to noextra knowledge for these languages compared tothe existing knowledge in the pretraining data..pos tagging our pos tagging results largelyfollow the same trend as those for ner, with con-tinued pretraining methods achieving the highestincrease in performance: between 15.81 and 17.61points.
also following ner and as shown in fig-ure 3, the largest performance gain can be seen forlanguages with a low baseline performance, and,as the latter increases, the beneﬁts obtained fromadaptation become smaller.
however, unlike forner, all methods show a net increase in perfor-mance, with +adapters, the lowest performingadaptation model, achieving a gain of 9.01 points.
we hypothesize that a likely reason for this is thedomain and style of the bible.
while it may betoo restrictive to signiﬁcantly boost downstreamner performance, it is still a linguistically rich re-source for pos tagging, a task that is less sensitiveto domain in general..additionally, there is a notable outlier language,coptic, on which no model performs better thanrandom choice (which corresponds to 6% accu-racy).
this is because the script of this languageis almost completely unseen to xlm-r, and prac-tically all non-whitespace subwords map to theunknown token: of the 50% of non-whitespace to-kens, 95% are unknown.
while +extend solves.
this issue, we believe that for a language with acompletely unseen script the bible is not enoughto learn representations which can be used in azero-shot setting..lang.
xlm-r +mlm +tlm +{m|t}lm +extend +adapters.
bamcopglvgrcgswmagmltmyvwolyor.
32.444.3133.1253.7947.7851.0930.1846.6237.9731.34.
60.59 60.914.024.0359.78 57.9158.07 55.4261.98 59.1460.77 57.3059.60 56.0066.63 68.4865.52 62.9748.54 49.40.avg.
∆ avg..36.860.00.
54.55 53.1617.69 16.29.
63.134.0359.0554.2158.3858.5253.9566.5559.3048.31.
52.5415.68.
53.404.3543.4352.3556.7254.5750.5058.6255.5647.71.
47.7210.86.
54.743.7050.8834.8661.7055.8144.1757.0554.5241.29.
45.8759.01.table 3: pos tagging accuracy..5 case study.
as previously stated, using the bible as the cor-pus for adaptation is limiting in two ways: the ex-tremely restricted domain as well as the small size.
to separate the effects of these two aspects, werepeat our experiments with a different set of data.
we sample sentences from the wikipedia of eachtarget language to simulate a corpus of similar sizeto the bible which is not restricted to the bible’sdomain or content.
to further minimize the effect.
4560figure 3: pos results (accuracy).
trendlines are created by ﬁtting a 2nd order, least squares polynomial..of domain, we focus solely on ner, such that thedomain of the data is precisely that of the targettask.
additionally, we seek to further investigatethe effect on the downstream performance gains ofthese adaptation methods when the source languageis more similar to the target language.
to this end,we focus our case study on three languages writtenin cyrillic: bashkir, chechen, and chuvash.
webreak up the case study into 3 settings, dependingon the data used.
in the ﬁrst setting, we changethe language of our labeled training data from en-glish to russian.
while russian is not necessarilysimilar to the target languages or mutually intelligi-ble, we consider it to be more similar than english;russian is written in the same script as the targetlanguages, and there is a greater likelihood for lexi-cal overlap and the existence of loanwords.
in thesecond setting, we pretrain using wikipedia andin the third setting we use both wikipedia data aswell as labeled russian data..to create our wikipedia training data, we ex-tract sentences with wikiextractor (attardi, 2015)and split them with moses sentencesplitter (koehnet al., 2007).
to create a comparable training setfor each language, we ﬁrst calculate the total num-ber of subword tokens found in the new testament,and sample sentences from wikipedia until we havean equivalent amount.
in the setting where we usedata from the new testament and labeled russiandata, for +tlm and +{m|t}lm we additionallysubstitute the english bible with the russian bible.
when using wikipedia, we omit results for +tlm.
and +{m|t}lm, as they rely on a parallel corpus..setting.
model.
bak.
che.
chv avg..b-e.xlm-rbest.
30.34 14.84 46.90 30.6937.99 16.26 66.67 40.30.
53.84 43.94 51.16 49.65xlm-r58.46 38.67 55.09 50.74+mlm+tlm53.58 27.78 50.96 44.10+{m|t}lm 57.99 31.64 57.14 48.9239.86 27.16 46.32 37.78+extend48.03 21.64 36.11 35.26+adapters.
+mlm+extend+adapters.
+mlm+extend+adapters.
43.12 18.18 74.82 45.3730.26 20.93 35.62 28.9441.88 41.15 71.74 51.59.
61.19 55.89 67.42 61.5036.84 44.08 35.46 38.7956.39 28.65 73.12 52.72.b-r.w-e.w-r.table 4: case study: cyrillic ner (f1).
setting de-scribes the source of data for adaptation, either the(b)ible or (w)ikipedia, as well as the language of theﬁnetuning data, (e)nglish or (r)ussian..5.1 results.
we present the results of our case study in table4. in the sections below, we refer to case studysettings as they are described in the table caption..effects of the finetuning language we ﬁndthat using russian as the source language (the “rus-sian baseline”; b-r w/ xlm-r) increases perfor-mance over the english baseline (b-e w/ xlm-r)by 18.96 f1.
interestingly, all of the adaptationmethods utilizing the bible do poorly in this set-.
4561ting (b-r), with +mlm only improving over therussian baseline by 1.09 f1, and all other methodsdecreasing performance.
we hypothesize that whenadaptation data is limited in domain, as the sourcelanguage approaches the target language in similar-ity, the language adaptation is mainly done in theﬁnetuning step, and any performance gain from theunlabeled data is minimized.
this is supported bythe previous ner results, where we ﬁnd that, whenusing english as the source language, the adapta-tion methods lead to higher average performancegain over the baseline for cyrillic languages, i.e.,the more distant languages, as opposed to latin lan-guages.
the adaptation methods show a larger im-provement when switching to wikipedia data (w-r), with +mlm improving performance by 11.85f1 over the russian baseline.
finally, the perfor-mance of +extend when using russian labeleddata is similar on average regardless of the adap-tation data (b-r, w-r), but noticeably improvesover the setting which uses wikipedia and englishlabeled data..effects of the domain used for adaptationfixing english as the source language and chang-ing the pretraining domain from the bible towikipedia (w-e) yields strong improvements, with+adapters improving over the english base-line by 20.9 f1 and +mlm improving by 14.68f1.
however, we note that, while the average of+adapters is higher than that of +mlm, this isdue to higher performance on only a single lan-guage.
when compared to the best performingpretraining methods that use the bible (b-e), thesemethods improve by 11.29 f1 and 5.30 f1 respec-tively.
when using both wikipedia and russiandata, we see the highest overall performance, and+mlm increases over the english baseline by 30.81f1 and the russian baseline by 11.85 f1..6 limitations.
one limitation of this work – and other workswhich involve a high number of languages – is taskselection.
while part-of-speech tagging and namedentity recognition4 are important, they are bothlow-level tasks largely based on sentence structure,with no requirement for higher levels of reasoning,unlike tasks such as question answering or naturallanguage inference.
while xtreme (hu et al.,.
4we also note that the wikiann labels are computer gen-erated and may suffer from lower recall when compared tohand-annotated datasets..2020) is a great, diverse benchmark covering thesehigher level tasks, the number of languages is stilllimited to only 40 languages, all of which havewikipedia data available.
extending these bench-marks to truly low resource languages by introduc-ing datasets for these tasks will further motivateresearch on these languages, and provide a morecomprehensive evaluation for their progress..additionally, while the bible is currently avail-able in some form for 1611 languages, the availabletext for certain languages may be different in termsof quantity and quality from the bible text we use inour experiments.
therefore, although we make nolanguage-speciﬁc assumptions, our ﬁndings maynot fully generalize to all 1611 languages due tothese factors.
furthermore, this work focuses onanalyzing the effects of adaptation methods foronly a single multilingual transformer model.
al-though we make no model-speciﬁc assumptions inour methods, the set of unseen languages differsfrom model to model.
moreover, although we showimprovements for the two tasks, we do not claim tohave state-of-the-art results.
in a low-resource set-ting, the best performance is often achieved throughtask-speciﬁc models.
similarly, translation-basedapproaches, as well as few-shot learning may offeradditional beneﬁts over a zero-shot setting.
wealso do not perform an extensive analysis of the tar-get languages, or an analysis of the selected sourcelanguage for ﬁnetuning.
a better linguistic under-standing of the languages in question would allowfor a better selection of source language, as well asthe ability to leverage linguistic features potentiallyleading to better results..finally, by using a pmm, we inherit all of thatmodel’s biases.
the biases captured by word em-beddings are well known, and recent work hasshown that contextual models are not free of biaseseither (caliskan et al., 2017; kurita et al., 2019).
the use of the bible, and religious texts in general,may further introduce additional biases.
last, weacknowledge the environmental impact from thetraining of models on the scale of xlm-r (strubellet al., 2019)..7 conclusion.
in this work, we evaluate the performance of contin-ued pretraining, vocabulary extension, and adaptersfor unseen languages of xlm-r in a realistic low-resource setting.
using only the new testament,we show that continued pretraining is the best per-.
4562forming adaptation approach, leading to gains of6.29 f1 on ner and 17.69% accuracy on pos tag-ging.
we therefore conclude that the bible can bea valuable resource for adapting pmms to unseenlanguages, especially when no other data exists.
furthermore, we conduct a case study on threelanguages written in cyrillic script.
changing thesource language to one more similar to the targetlanguage reduces the effect of adaptation, but theperformance of the adaptation methods relative toeach other is preserved.
changing the domain ofthe adaptation data to one more similar to the tar-get task while keeping its size constant improvesperformance..acknowledgments.
we would like to thank the acl reviewers fortheir constructive and insightful feedback as wellas yoshinari fujinuma, st´ephane aroca-ouellette,and other members of the cu boulder’s nalagroup for their advice and help..references.
waleed ammar, george mulcaire, yulia tsvetkov,guillaume lample, chris dyer, and noah a. smith.
2016. massively multilingual word embeddings.
arxiv, abs/1602.01925..m. artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in acl..m. artetxe and holger schwenk.
2019. massively mul-tilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
transactions of the as-sociation for computational linguistics, 7:597–610..mikel artetxe, gorka labaka, and eneko agirre.
2016.learning principled bilingual mappings of word em-beddings while preserving monolingual invariance.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages2289–2294, austin, texas.
association for compu-tational linguistics..mikel artetxe, gorka labaka, and eneko agirre.
2017.learning bilingual word embeddings with (almost)no bilingual data.
in proceedings of the 55th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 451–462,vancouver, canada.
association for computationallinguistics..giusepppe attardi.
2015. wikiextractor.
https://.
github.com/attardi/wikiextractor..m saiful bari, shaﬁq r. joty, and prathyusha jwalapu-ram.
2020. zero-resource cross-lingual named en-tity recognition.
in aaai..akash bharadwaj, david mortensen, chris dyer, andjaime carbonell.
2016. phonologically aware neuralmodel for named entity recognition in low resourcein proceedings of the 2016 con-transfer settings.
ference on empirical methods in natural languageprocessing, pages 1462–1472, austin, texas.
asso-ciation for computational linguistics..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..aylin caliskan,.
and arvindjoanna j. bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356(6334):183–186..ethan c. chau, lucy h. lin, and noah a. smith.
2020.parsing with multilingual bert, a small corpus, and asmall treebank.
arxiv, abs/2009.14124..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, e. grave, myle ott, luke zettlemoyer, andveselin stoyanov.
2020. unsupervised cross-lingualrepresentation learning at scale.
in acl..alexis conneau, guillaume lample, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2017.arxivword translation without parallel data.
preprint arxiv:1710.04087..ramy eskander, smaranda muresan, and michaelcollins.
2020. unsupervised cross-lingual part-of-speech tagging for truly low-resource scenarios.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4820–4831, online.
association for computa-tional linguistics..edouard grave, piotr bojanowski, prakhar gupta, ar-mand joulin, and tomas mikolov.
2018. learningin proceedingsword vectors for 157 languages.
of the international conference on language re-sources and evaluation (lrec 2018)..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:adaptlanguage models to domains and tasks.
arxiv, abs/2004.10964..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in proceedings of the 36th international conferenceon machine learning, volume 97 of proceedingsof machine learning research, pages 2790–2799,long beach, california, usa.
pmlr..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-taskbenchmark for evaluating cross-lingual generaliza-tion.
corr, abs/2003.11080..4563armand joulin, piotr bojanowski, tomas mikolov,herv´e j´egou, and edouard grave.
2018. loss intranslation: learning bilingual word mapping with aretrieval criterion.
in proceedings of the 2018 con-ference on empirical methods in natural languageprocessing..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems, volume 26, pages 3111–3119.
curran as-sociates, inc..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..keita kurita, nidhi vyas, ayush pareek, alan w black,and yulia tsvetkov.
2019. measuring bias in contex-tualized word representations..alexandre lacoste, alexandra luccioni, victorschmidt, and thomas dandres.
2019. quantifyingthe carbon emissions of machine learning.
arxivpreprint arxiv:1910.09700..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
in neurips..guillaume lample, alexis conneau, ludovic denoyer,and marc’aurelio ranzato.
2017. unsupervised ma-chine translation using monolingual corpora only.
arxiv preprint arxiv:1711.00043..anne lauscher, vinit ravishankar, ivan vuli´c, andgoran glavaˇs.
2020. from zero to hero: on the lim-itations of zero-shot language transfer with multilin-gual transformers.
pages 4483–4499..stephen mayhew, chen-tse tsai, and dan roth.
2017.cheap translation for cross-lingual named entityrecognition.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 2536–2545, copenhagen, denmark.
as-sociation for computational linguistics..arya d. mccarthy, rachel wicks, dylan lewis, aaronmueller, winston wu, oliver adams, garrett nico-lai, matt post, and david yarowsky.
2020. thejohns hopkins university bible corpus:1600+in proceed-tongues for typological exploration.
ings of the 12th language resources and evaluationconference, pages 2884–2892, marseille, france.
european language resources association..b. muller, antonis anastasopoulos, benoˆıt sagot, anddjam´e seddah.
2020. when being unseen frommbert is just the beginning: handling new lan-guages with multilingual language models.
arxiv,abs/2010.12858..joakim nivre, marie-catherine de marneffe, f. gin-ter, jan hajivc, christopher d. manning, sampopyysalo, sebastian schuster, francis tyers, andd. zeman.
2020. universal dependencies v2: an ev-ergrowing multilingual treebank collection.
arxiv,abs/2004.10643..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..jonas pfeiffer, andreas r¨uckl´e, clifton poth, aish-ivan vuli´c, sebastian ruder,warya kamath,kyunghyun cho,and iryna gurevych.
2020a.
adapterhub: a framework for adapting transform-ers.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 46–54..jonas pfeiffer, ivan vuli´c, iryna gurevych, and se-bastian ruder.
2020b.
mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
inemnlp..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..4564sylvestre-alvise rebufﬁ, hakan bilen, and andreavedaldi.
2017. learning multiple visual domainswith residual adapters.
in advances in neural infor-mation processing systems, volume 30, pages 506–516. curran associates, inc..david yarowsky, grace ngai, and richard wicen-inducing multilingual text analysistowski.
2001.tools via robust projection across aligned corpora.
inproceedings of the first international conference onhuman language technology research..rong zhang, revanth gangi reddy, md arafat sul-tan, vittorio castelli, anthony ferritto, radu flo-rian, efsun sarioglu kayi, salim roukos, avi sil,and todd ward.
2020. multi-stage pre-training forlow-resource domain adaptation.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 5461–5468, online.
association for computational lin-guistics..sebastian ruder, ivan vuli´c, and anders søgaard.
2019. a survey of cross-lingual word embeddingmodels.
journal of artiﬁcial intelligence research,65:569–631..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..emma strubell, ananya ganesh, and andrew mccal-lum.
2019. energy and policy considerations fordeep learning in nlp..zihan wang, karthikeyan k, stephen mayhew, anddan roth.
2020. extending multilingual bert tolow-resource languages.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 2649–2656, online.
association for computa-tional linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..shijie wu and mark dredze.
2020. are allguages created equal in multilingual bert?
repl4nlp@acl..lan-in.
jiateng xie, zhilin yang, graham neubig, noah a.smith, and jaime carbonell.
2018. neural cross-lingual named entity recognition with minimal re-sources.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 369–379, brussels, belgium.
association forcomputational linguistics..4565a appendix.
in table 5, we provide the number of subwordscreated by the xlm-r tokenizer from the newtestament of each target language, in addition tothe speciﬁc version of the bible we use, as foundin the jhu bible corpus.
in table 7 and 6 weprovide the relative performance of all adaptationmethods as compared to baseline performance..lang.
bible version.
bible size (thousands).
ace-x-bible-ace-v1arz-x-bible-arz-v1bak-bakibtbam-x-bible-bam-v1ceb-x-bible-bugna2009-v1che-x-bible-che-v1chv-chvibtcop-x-bible-bohairic-v1crh-crhibteng-x-bible-kingjames-v1glv-x-bible-glv-v1grc-x-bible-textusreceptusvar1-v1gsw-x-bible-alemannisch-v1hak-x-bible-hak-v1ibo-x-bible-ibo-v1ilo-x-bible-ilo-v1kin-x-bible-bird-youversion-v1mag-magssimhr-x-bible-mhr-v1min-x-bible-min-v1mlt-x-bible-mlt-v1mri-x-bible-mri-v1myv-x-bible-myv-v1nds-x-bible-nds-v1ory-x-bible-ory-v1rus-x-bible-kulakov-v1sco-x-bible-sco-v1tat-ttribttgk-tgkibtwar-x-bible-war-v1wol-x-bible-wol-v1yor-x-bible-yor-v1.
acearzbakbamcebchechvcopcrhengglvgrcgswhakiboilokinmagmhrminmltmrimyvndsoryrusscotattgkwarwolyor.
avg..536k828k453k429k384k523k519k259k347k461k196k322k351k598k458k378k344k388k398k505k389k411k463k333k386k283k30k438k233k401k383k450k.
402k.
table 5: size of the new testaments of each language,along with the speciﬁc bible version.
size is calcu-lated in subword units using the base xlm-roberta to-kenizer..4566lang.
xlm-r ∆mlm ∆tlm ∆{m|t}lm ∆extend ∆adapters.
bamcopglvgrcgswmagmltmyvwolyor.
32.444.3133.1253.7947.7851.0930.1846.6237.9731.34.
28.15-0.2926.664.2814.209.6829.4220.0127.5517.20.
28.47-0.2824.791.6311.366.2125.8221.8625.0018.06.avg..36.86.
17.69.
16.29.
30.69-0.2825.930.4210.607.4323.7719.9321.3316.97.
15.68.
20.960.0410.31-1.448.943.4820.3212.0017.5916.37.
10.86.
22.30-0.6117.76-18.9313.924.7213.9910.4316.559.95.
9.01.table 6: accuracy deltas for pos tagging compared to baseline.
lang.
xlm-r ∆mlm ∆tlm ∆{m|t}lm ∆extend ∆adapters.
acearzbakcebchechvcrhhakiboilokinmhrminmltmrindsoryscotattgkwaryor.
31.9549.8030.3451.6414.8446.9039.2731.3644.8856.2557.3945.7444.1348.8011.9562.7531.4976.4036.6322.9257.8752.91.
6.155.344.252.145.761.941.842.481.420.2417.4419.77-2.714.504.7112.595.313.185.227.643.823.482.650.55-1.222.6311.2014.0419.9836.330.628.13-5.85-7.25-1.89-2.8414.8913.609.7613.6712.355.24-14.12 -16.24.
6.113.537.651.26-1.1212.33-2.589.005.519.81-0.85-2.30-2.709.6816.325.78-2.54-1.9813.9311.066.13-17.62.
5.59-5.27-0.37-2.33-0.371.883.74-3.83-1.405.70-8.44-8.96-1.64-3.138.941.88-9.36-1.50-2.8512.737.92-11.50.
1.61-13.733.460.874.097.46-6.99-2.69-1.92-3.62-1.26-4.99-2.43-13.542.79-3.54-6.05-10.537.0713.946.45-19.04.avg..43.01.
4.29.
6.29.
3.93.
-0.57.
-1.94.table 7: f1 deltas for ner compared to baseline.
4567