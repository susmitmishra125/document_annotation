improving the faithfulness of attention-based explanations withtask-speciﬁc information for text classiﬁcation.
george chrysostomou nikolaos aletrasdepartment of computer science, university of shefﬁeldunited kingdom{gchrysostomou1, n.aletras}@sheffield.ac.uk.
abstract.
neural network architectures in natural lan-guage processing often use attention mech-anisms to produce probability distributionsover input token representations.
attentionhas empirically been demonstrated to im-prove performance in various tasks, whileits weights have been extensively used asexplanations for model predictions.
re-cent studies (jain and wallace, 2019; ser-rano and smith, 2019; wiegreffe and pin-ter, 2019) have showed that it cannot gener-ally be considered as a faithful explanation(jacovi and goldberg, 2020) across encodersand tasks.
in this paper, we seek to im-prove the faithfulness of attention-based expla-nations for text classiﬁcation.
we achieve thisby proposing a new family of task-scaling(tasc) mechanisms thatlearn task-speciﬁcnon-contextualised information to scale theoriginal attention weights.
evaluation tests forexplanation faithfulness, show that the threeproposed variants of tasc improve attention-based explanations across two attention mech-anisms, ﬁve encoders and ﬁve text classiﬁca-tion datasets without sacriﬁcing predictive per-formance.
finally, we demonstrate that tascconsistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.1.
1.introduction.
natural language processing (nlp) approachesfor text classiﬁcation are often underpinned bylarge neural network models (cho et al., 2014; de-vlin et al., 2019).
despite the high accuracy andefﬁciency of these models in dealing with largeamounts of data, an important problem is their in-creased complexity that makes them opaque andhard to interpret by humans which usually treat.
1code is available at:gchrysostomou/tasc.git.
https://github.com/.
them as black boxes (zhang et al., 2018; linzenet al., 2019)..attention mechanisms (bahdanau et al., 2015)produce a probability distribution over the inputto compute a vector representation of the entiretoken sequence as the weighted sum of its con-stituent vectors.
a common practice is to provideexplanations for a given prediction and qualitativemodel analysis by assigning importance to inputtokens using scores provided by attention mecha-nisms (chen et al., 2017; wang et al., 2016; jainet al., 2020; sun and lu, 2020) as a mean towardsmodel interpretability (lipton, 2016; miller, 2019).
a faithful explanation is one that accurately rep-resents the true reasoning behind a model’s pre-diction (jacovi and goldberg, 2020).
a series ofrecent studies illustrate that explanations obtainedby attention weights do not always provide faith-ful explanations (serrano and smith, 2019) whiledifferent text encoders can affect attention inter-pretability, e.g.
results can differ when using arecurrent or non-recurrent encoder (wiegreffe andpinter, 2019)..a limitation of attention as an indicator of inputimportance is that it refers to the word in contextdue to information mixing in the model (tutekand snajder, 2020).
motivated by this, we aimto improve the effectiveness of neural models inproviding more faithful attention-based explana-tions for text classiﬁcation, by introducing non-contextualised information in the model.
our con-tributions are as follows:.
• we introduce three task-scaling (tasc) mech-anisms (§4), a family of encoder-independentcomponents thatlearn task-speciﬁc non-contextualised importance scores for eachword in the vocabulary to scale the originalattention weights which can be easily portedto any neural architecture;.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages477–488august1–6,2021.©2021associationforcomputationallinguistics477• we show that tasc variants offer more ro-bust, consistent and faithful attention-basedexplanations compared to using vanilla atten-tion in a set of standard interpretability bench-marks, without sacriﬁcing predictive perfor-mance (§6);.
• we demonstrate that attention-based explana-tions with tasc consistently outperform expla-nations obtained from two gradient-based anda word-erasure explanation approaches (§7)..2 related work.
2.1 model interpretability.
explanations for neural networks can be obtainedby identifying which parts of the input are impor-tant for a given prediction.
one way is to usesparse linear meta-models that are easier to inter-pret (ribeiro et al., 2016; lundberg and lee, 2017;nguyen, 2018).
another way is to calculate thedifference in a model’s prediction between keepingand omitting an input token (robnik- ˇsikonja andkononenko, 2008; li et al., 2016b; nguyen, 2018).
input importance is also measured using the gra-dients computed with respect to the input (kinder-mans et al., 2016; li et al., 2016a; arras et al., 2016;sundararajan et al., 2017).
chen and ji (2020) pro-pose learning a variational word mask to improvemodel interpretability.
finally, extracting a shortsnippet from the original input text (rationale) andusing it to make a prediction has been recently pro-posed (lei et al., 2016; bastings et al., 2019; tre-viso and martins, 2020; jain et al., 2020; chalkidiset al., 2021)..nguyen (2018) and atanasova et al.
(2020)compare explanations produced by different ap-proaches, showing that in most cases gradient-based approaches outperform sparse linear meta-models..2.2 attention as explanation.
attention weights have been extensively used tointerpret model predictions in nlp; i.e.
(cho et al.,2014; xu et al., 2015; barbieri et al., 2018; ghaeiniet al., 2018).
however, the hypothesis that atten-tion should be used as explanation had not beenexplicitly studied until recently..jain and wallace (2019) ﬁrst explored the effec-tiveness of attention explanations.
they show thatadversary attention distributions can yield equiva-lent predictions with the original attention distribu-tion, suggesting that attention weights do not offer.
robust explanations.
in contrast to jain and wallace(2019), wiegreffe and pinter (2019) and vashishthet al.
(2019) demonstrate that attention weights canin certain cases provide robust explanations.
pruthiet al.
(2020) also investigate the ability of attentionweights to provide plausible explanations.
theytest this through manipulating the attention mech-anism by penalising words a priori known to berelevant to the task, showing that the predictive per-formance remain relatively unaffected.
sen et al.
(2020) assess the plausibility of attention weightsby correlating them with manually annotated expla-nation heat-maps, where plausibility refers to howconvincing an explanation is to humans (jacovi andgoldberg, 2020).
however, jacovi and goldberg(2020) and grimsley et al.
(2020) suggest cautionwith interpreting the results of these experiments asthey do not test the faithfulness of explanations (e.g.
an explanation can be non-plausible but faithful orvice-versa)..serrano and smith (2019) test the faithfulnessof attention-based explanations by removing to-kens to observe how fast a decision ﬂip happens.
results show that gradient attention-based rank-ings (i.e.
combining an attention weight withits gradient) better predict word importance formodel predictions, compared to just using the at-tention weights.
tutek and snajder (2020) proposea method to improve the faithfulness of attentionexplanations when using recurrent encoders by in-troducing a word-level objective to sequence classi-ﬁcation tasks.
focusing also on recurrent-encoders,mohankumar et al.
(2020) introduce a modiﬁcationto recurrent encoders to reduce repetitive informa-tion across different words in the input to improvefaithfulness of explanations..to the best of our knowledge, no previouswork has attempted to improve the faithfulnessof attention-based explanations across differentencoders for text classiﬁcation by inducing task-speciﬁc information to the attention weights..3 neural text classiﬁcation models.
in a typical neural model with attention for textclassiﬁcation; one-hot-encoded tokens xi p r|v|are ﬁrst mapped to embeddings ei p rd, wherei p r1, ..., ts denotes the position in the sequence,t the sequence length, |v | the vocabulary size andd the dimensionality of the embeddings.
the em-beddings ei are then passed to an encoder to pro-duce hidden representations hi “ encpeiq, where.
478hi p rn, with n the size of the hidden representa-tion.
a vector representation c for the entire textsequence x1, ..., xt is subsequently obtained as thesum of hi weighted by attention scores αi:.
c “.
ci,.
ci “ hiαi,.
c p rn.
(1).
ÿ.i.vector c is ﬁnally passed to the output, a fully-connected linear layer followed by a softmax acti-vation function..3.1 encoders.
to obtain representations hi, we consider thefollowing recurrent, non-recurrent and trans-former (vaswani et al., 2017) encoders, encp.q, asin (jain and wallace, 2019; wiegreffe and pinter,2019): (i) bidirectional long short-term mem-ory (lstm; hochreiter and schmidhuber (1997));(ii) bidirectional gated recurrent unit (gru; choet al.
(2014)); (iii) convolutional neural network(cnn; lecun et al.
(1999)); (iv) multi-layer per-ceptron (mlp); (v) bert2 (devlin et al., 2019)..encoder each token representation hi contains in-formation from the whole sequence so the attentionweights actually refer to the input word in contextand not individually (tutek and snajder, 2020)..inspired by the simple and highly interpretablebag-of-words models, which assign a single weightfor each word type (word in a vocabulary), wehypothesise that by scaling each input word’s con-textualised representation ci (see eq.
1) by itsattention score and and a non-contextualised wordtype scalar score, we can improve attention-basedexplanations.
the intuition is that by having a lesscontextualised sequence representation c we canreduce information mixing for attention..for.
that purpose, we introduce the non-contextualised word type score sxi in eq.
1 to en-rich the text representation c, such that:.
c “.
hiαisxi,.
c p rn.
(5).
ÿ.i.we compute sxi by proposing three task-scaling(tasc) mechanisms.3.
3.2 attention mechanisms.
4.1 linear tasc (lin-tasc).
attention scores (ai) are computed by passing therepresentations (hi) obtained from the encoder tothe attention mechanism which usually consists ofa similarity function φ followed by softmax:.
ai “.
ř.exppφphi, qqqtk“1 exppφpq, hkqq.
(2).
where q p rn is a trainable self-attention vectorsimilar to yang et al.
(2016)..following jain and wallace (2019), we considertwo self-attention similarity functions: (i) additiveattention (tanh; bahdanau et al.
(2015)):.
φphi, qq “ qt tanhpw hiq.
(3).
where w is a trainable model parameter; and (ii)scaled dot-product (dot; vaswani et al.
(2017)):.
φphi, qq “.
hti q?
n.(4).
4 task-scaling (tasc) mechanisms.
attention indicates how well inputs around a po-sition i correspond to the output (bahdanau et al.,2015).
for example, in a bidirectional recurrent.
we ﬁrst introduce linear tasc (lin-tasc), the sim-plest method in the family of tasc mechanismsthat estimates a scalar weight for each word in thevocabulary by introducing a new vector u p r|v|.
given the input sequence x “ rx1, .
.
.
, xts repre-senting one-hot-encodings of the tokens, we per-form a look up on u to obtain the scalar weightsof words in the sequence.
u is randomly initialisedand updated partially at each training iteration, be-cause naturally each input sequence contains onlya small subset of the vocabulary words..we then obtain a task-scaled embedding ˆei fora token i in the input by multiplying the originaltoken embedding with its word type weight ui:.
ˆei “ uiei.
(6).
the intuition is that the embedding vector eiwas trained on general corpora and is a non-contextualised “generic” representation of input xi.
as such the score ui will scale ei to the task.
wesubsequently compute context-independent scoressxi for each token in the sequence, by summing allelements of its corresponding task-scaled embed-d ˆei in a similar way that tokending ˆei; sxi “embeddings are averaged in the top-layers of a.ř.
2we use bert to obtain hi with an attention mechanism.
3number of parameters for each proposed mechanism in.
on top for consistency with the other encoders.
appendix b..479neural architecture.
we opted to sum-up and notaverage, because we want to retain large and smallvalues from the task-scaled embedding vector ˆei(atanasova et al., 2020).4.as the attention scores pertain to the word incontext (tutek and snajder, 2020), we also expectthe score sxi to pertain to the word without thecontextualised information.
that way, we comple-ment attention which results into a richer sequencerepresentation c..4.2 feature-wise tasc (feat-tasc).
lin-tasc assigns equal weighting to all the dimen-sions of the word embedding ei (see eq.
6), butsome of them might be more important than others.
inspired by the retain mechanism (choi et al.,2016), feature-wise tasc (feat-tasc) learns dif-ferent weights for each embedding dimension toidentify the most important of them.
comparedto lin-tasc where ei is scaled uniformly acrossall vector dimensions, with feat-tasc each dimen-sion is scaled independently.
to achieve this, weintroduce a learnable matrix u p r|v|ˆd.
similarto lin-tasc, given the input sequence x, we per-form a look up on u to obtain us “ ru1, .
.
.
, uts.
u is randomly initialised and updated partially ateach training iteration.
to obtain sxi, we perform adot product between ui and embedding vector ei;sxi “ ui ¨ ei..4.3 convolutional tasc (conv-tasc).
lin-tasc and feat-tasc weigh the original wordembedding ei but do not consider any interactionsbetween embedding dimensions.
conv-tasc ad-dresses this limitation by extending lin-tasc.5 weapply a cnn6 with n channels over the scaled em-bedding ˆei from lin-tasc, keeping a single strideand a 1-dimensional kernel.
this way, we ensurethat input words remain context-independent.
wethen sum over the ﬁltered scaled embedding ˆefi , toobtain the scores sxi; sxi “.
d ˆef.
ř.i .4.
4we also tried max and mean-pooling or using the uidirectly instead of si in early experimentation resulting inlower results..5we only apply conv-tasc over lin-tasc to keep themechanism relatively lightweight.
note that feat-tasc learnsan extra matrix of equal size to the embedding matrix..6see cnn conﬁgurations in appendix a..5 evaluating attention-based.
interpretability.
jacovi and goldberg (2020) propose that an appro-priate measure of faithfulness of an explanationcan be obtained through erasure (the most relevantparts of the input–according to the explanation–are removed).
we therefore follow this evalua-tion approach similar to serrano and smith (2019),atanasova et al.
(2020) and nguyen (2018).7.
5.1 attention-based importance metrics.
we opt using the following three input importancemetrics by serrano and smith (2019):8.
• α: importance rank corresponding to nor-.
malised attention scores..• ∇α: provides a ranking by computing thegradient of the predicted label ˆy with respectto each attention score αi in descending order,such that ∇αi “ b ˆybαi.
..• α∇α: scales the attention scores αi with.
their corresponding gradients ∇αi..5.2 faithfulness metrics.
decision flip - most informative token: theaverage percentage of decision ﬂips (i.e.
changesin model prediction) occurred in the test set byremoving the token with highest importance..decision flip - fraction of tokens: the aver-age fraction of tokens required to be removed tocause a decision ﬂip in the test set..note that we conduct all experiments at the inputlevel (i.e.
by removing the token from the input se-quence instead of only removing its correspondingattention weight) as we consider the scores fromimportance metrics to pertain to the correspondinginput token following related work (arras et al.,2016, 2017; nguyen, 2018; vashishth et al., 2019;grimsley et al., 2020; atanasova et al., 2020)..6 experiments and results.
6.1 data.
we use ﬁve datasets for text classiﬁcation follow-ing jain and wallace (2019): (i) sst (socher et al.,2013); (ii) imdb (maas et al., 2011); (iii) adr.
7note that jacovi and goldberg (2020) argue that a human.
evaluation is not an appropriate method to test faithfulness..8serrano and smith (2019) show that gradient-based at-tention ranking metrics (∇α, α∇α) are better in providingfaithful explanations compared to just using attention (α)..480dataset av.
|w |.
|v|.
sstadrimdbagmimic.
2022185342,180.
13,6866,71612,14714,57316,277.splitstrain/dev/test6,920 / 872 / 1,82114,452 / 2,551 / 4,25117,212 / 4,304 / 4,36360,895 / 7,145 / 3,9604,654 / 822 / 1,369.table 1: dataset statistics including average words perinstance, vocabulary size and splits..tweets (sarker et al., 2015); (iv) ag news;9 and(v) mimic anemia (johnson et al., 2016).
seetable 1 for detailed data statistics..6.2 predictive performance.
a prerequisite of interpretability is to obtain robustexplanations without sacriﬁcing predictive perfor-mance (lipton, 2016).
table 2 shows the macro f1-scores of all models across datasets, encoders andattention mechanisms using the three tasc variants(lin-tasc, feat-tasc and conv-tasc described insection 4) and without tasc (no-tasc).10.in general, all tasc models obtain comparableperformance and in some cases outperform no-tasc across datasets and attention mechanisms.
however, our main aim is not to improve predictiveperformance but the faithfulness of attention-basedexplanations, which we illustrate below..6.3 decision flip: most informative token.
table 3 and figure 1 present the mean average per-centage of decision ﬂips (higher is better) acrossattention mechanisms, encoders and datasets byremoving the most informative token for tasc vari-ants and no-tasc for all attention-based impor-tance metrics (see section 5)..in table 3, we observe that tasc variants areeffective in identifying the single most importanttoken, outperforming no-tasc in 12 out of 18 casesacross attention-based importance metrics.
thissuggests that the attention mechanisms beneﬁt fromthe non-contextualised information encapsulatedin tasc when allocating importance to the inputtokens.
models using tanh without tasc appear toproduce on average a higher percentage of decisionﬂips compared to those using the dot mechanism.
using either of the tasc variants improves both.
9https://di.unipi.it/˜gulli/ag_corpus_.
of_news_articles.html.
10for model hyper-parameters and prepossessing steps see.
appendix a..11lower predictive performance is observed with bert inmimic, as bert accepts a maximum of 512 word pieces asinput.
see appendix a..adr.
sst.
data enc() no-tasc lin-tasc feat-tasc conv-tascdot tanh dot tanh dot tanh dot tanh.91.77.77.79.76.77.75.75.74.75.93.89.89.88.89.94.92.92.92.92.83.88.88.86.88.bert .91 .90 .89 .88 .85 .88 .91lstm .76 .75 .79 .79 .79 .80 .78gru .76 .77 .79 .78 .80 .79 .77mlp .76 .76 .78 .78 .79 .78 .79cnn .76 .74 .80 .78 .80 .80 .78bert .80 .79 .78 .77 .79 .76 .78lstm .74 .73 .75 .75 .74 .75 .73gru .74 .73 .76 .75 .74 .76 .74mlp .74 .68 .75 .74 .75 .74 .75cnn .73 .69 .75 .74 .74 .75 .76bert .93 .93 .93 .92 .92 .92 .93lstm .89 .89 .88 .88 .88 .89 .89gru .89 .90 .88 .88 .89 .89 .89mlp .88 .88 .88 .88 .88 .88 .89cnn .88 .88 .88 .88 .88 .88 .88bert .94 .94 .94 .94 .94 .94 .94lstm .92 .93 .92 .92 .92 .92 .92gru .92 .92 .92 .92 .92 .92 .92mlp .92 .92 .92 .92 .91 .91 .92cnn .92 .92 .92 .92 .92 .92 .92bert11 .82 .84 .82 .83 .83 .83 .83lstm .87 .89 .87 .87 .88 .88 .88gru .87 .89 .87 .88 .88 .88 .88mlp .87 .87 .87 .86 .86 .86 .87cnn .88 .89 .88 .87 .87 .87 .88.ag.
imdb.
mimic.
table 2: f1-macro average scores (3 runs) acrossdatasets, encoders and attention mechanisms for mod-els with and without tasc (no-tasc).
underlined andbold values indicate comparable and better predictiveperformance by using tasc respectively.
standard de-viations do not exceed 0.01.att.
no-tasc lin-tasc feat-tasc conv-tasctanh5.4 (0.6)6.5 (0.8)7.3 (0.9)dot4.5 (0.8)4.8 (0.9)4.3 (0.8)10.2 (1.2) 11.2 (1.4) 10.4 (1.3)tanh10.9 (1.6) 12.2 (1.8) 11.1 (1.6)dot14.0 (1.2) 13.5 (1.1) 12.2 (1.0)tanh11.8 (1.4) 12.6 (1.5) 11.3 (1.4)dot.
8.45.48.26.911.78.2.α.
∇α.
α∇α.
table 3: mean average percentage of decision ﬂipsacross attention mechanisms occurred by removing themost informative token, using the three tasc variantsand no-tasc (higher is better).
bold and underlinedvalues denote best performing method row-wise andoverall (for each attention mechanism).
relative im-provement over no-tasc in parenthesis (ą1 tasc isbetter than no-tasc)..mechanisms, with dot mechanism beneﬁting themost, making it comparable to tanh.
for example,dot moves from 8.2% with no-tasc to 11.8% withlin-tasc, which is closer to 14.0% achieved bylin-tasc with tanh (for α∇α)..the ﬁrst row of figure 1 presents a compari-son across encoders.
tasc variants achieve im-proved performance over no-tasc across all en-coder variants with ∇α and α∇α.
all tasc vari-ants yield comparable results with the exception.
481(a) α.
(b) ∇α.
(c) α∇α.
figure 1: mean average percentage of decision ﬂips occurred by removing the most informative token, using thethree tasc variants and no-tasc across encoders (ﬁrst row) and datasets (second row), where lower is better..of conv-tasc with bert.
results further suggestthat non-recurrent encoders (mlp, cnn) withouttasc outperform recurrent encoders (lstm, gru)and bert which has the poorest performance.
wehypothesise that this is due to the attention modulebecoming more important without feature contex-tualisation which is similar to ﬁndings of serranoand smith (2019) and wiegreffe and pinter (2019).
however, we observe that using any of the tascvariants across encoders results into improvementswith lstm and gru becoming comparable tomlp and cnn.
for example, bert without tascimproves from 5.7% to 8.0% (relative improvement1.4x) and 9.3% (relative improvement 1.6x) usinglin-tasc and feat-tasc respectively (for α∇α)..6.4 decision flip: fraction of tokens.
providing one token (i.e., the most informative) asan explanation is not always a realistic approachin our second experi-to assessing faithfulness.
ment, we test tasc by measuring the fraction ofimportant tokens required to be removed to causea decision ﬂip (change model’s prediction).
table4 and figure 2 show the mean average fraction oftokens required to be removed to cause a decisionﬂip (lower is better) across attention mechanisms,encoders and datasets for all importance metrics..att.
no-tasc lin-tasc feat-tasc conv-tasctanhdottanhdottanhdot.
.39 (0.9).52 (0.9).21 (0.6).22 (0.5).17 (0.5).21 (0.5).
.43 (1.0).56 (0.9).26 (0.7).26 (0.6).24 (0.7).26 (0.6).
.42 (0.9).53 (0.9).19 (0.5).22 (0.5).18 (0.5).21 (0.5).
.44.60.36.42.32.41.α.
∇α.
α∇α.
observing results in the second row of figure1, we see that tasc variants outperform no-tascin all datasets when using ∇α and α∇α.
thishighlights the robustness of tasc as improvementsin general, lin-are irrespective of the dataset.
tasc and feat-tasc perform equally well, howeverlin-tasc has the smaller number of parametersamongst the three variants.
similar to the ﬁndingsof serrano and smith (2019) best results overall,irrespective of the use of tasc, are obtained usingα∇α to rank importance..table 4: mean average fraction of informative tokensrequired to cause a decision ﬂip across attention mech-anisms, using the three tasc variants and no-tasc(lower is better).
bold and underlined values denotebest performing method row-wise and overall (for eachattention mechanism).
relative improvement over no-tasc in parenthesis (ă1 tasc is better than no-tasc)..in table 4, we see that attention-based expla-nations from models trained with any of the tascmechanisms require on average a lower fractionof tokens to cause a decision ﬂip compared to no-.
482(a) α.
(b) ∇α.
(c) α∇α.
figure 2: mean average fraction of tokens required to cause a decision ﬂip, using the three tasc variants andno-tasc across encoders (ﬁrst row) and datasets (second row), where lower is better..tasc (in 17 out of 18 cases).
overall lin-tascachieves higher or comparable relative improve-ments over conv-tasc and feat-tasc in 5 out of 6times..we present an across encoders comparison inthe ﬁrst row of figure 2. all three tasc variantsobtain comparable performance with the excep-tion of conv-tasc with bert.
we hypothesise thatwith bert, conv-tasc fails to capture interactionsbetween embedding dimensions due to perhapshigher contextualisation of bert embeddings (i.e.
contain more duplicate information).
similarly tothe previous experiment results suggest that non-recurrent encoders (mlp and cnn) without tascoutperform the remainder of encoders, with berthaving the worst performance.
this strengthensour hypothesis that attention becomes more im-portant to a model with reduced contextualisation.
when using tasc, performance across all encodersbecomes comparable with the exception of bert.
for example, gru improves from .43 with no-tasc to .16 with lin-tasc, .17 with feat-tasc and.18 with conv-tasc (for α∇α)..the second row of figure 2 presents resultsacross datasets.
all three tasc mechanims manageto outperform vanilla attention.
lin-tasc and feat-tasc perform comparably, with the ﬁrst having aslight edge obtaining highest relative improvementsin 3 out of 5 datasets with α∇α.
for example in.
adr, no-tasc requires on average .77 of all to-kens to be removed for a decision ﬂip to occurcompared to .34 obtained by lin-tasc (for α∇α).
the beneﬁts of tasc become evident when consid-ering longer sequences.
for example in mimic,lin-tasc requires on average 44 tokens to cause adecision ﬂip compared to 220 for no-tasc..6.5 robustness analysis.
we also perform a detailed comparison betweenthe best performing tasc variant (lin-tasc) andvanilla attention (no-tasc) across all test instances.
figure 3 shows box-plots with the median frac-tion of tokens required to be removed for causinga decision ﬂip when ranking tokens by all threeimportance metrics.
for brevity we present resultsfor four cases..we notice that the median fraction of tokensrequired to cause a decision ﬂip for lin-tasc us-ing α is higher compared to no-tasc in certaincases.
however, lin-tasc results in consistentlylower medians (with substantially reduced vari-ances) compared to no-tasc using ∇α and α∇αwhich are more effective importance metrics.
thisis particularly visible in adr using bert, wherethe 25% and 75% percentiles are much closer tothe median values, compared to no-tasc.
reducedvariances suggest that the explanation faithfulnessacross instances remains consistent..483rics across all encoders and datasets.12 we ob-serve that using α∇α with tasc to rank wordimportance requires a lower fraction of tokens tocause a decision ﬂip on average compared to wo,x∇x and ig without tasc.
we outperform theother explanation approaches in 40 out of 50 cases,whilst obtaining comparable performance in other5 cases.
this demonstrates the efﬁcacy of tascin providing more faithful attention-based explana-tions than strong baselines without tasc (nguyen,2018; atanasova et al., 2020).
the improvementsare particularly evident using bert as an encoder.
in imdb, wo with tanh requires on average .23of the tokens to be removed for a decision ﬂip com-pared to just .07 for α∇α with tasc..we also observe that the attention-based impor-tance metric (α∇α) with tasc is a more robust ex-planation technique than non-attention based ones,obtaining lower variance in the fraction of tokensrequired to cause a decision ﬂip across encoders.
for example α∇α with tasc and tanh requires afraction of tokens in the range of .01-.05 comparedto ig which requires .02-.43 in mimic, showingthe consistency of our proposed approach..finally we observe that tasc consistently im-proves non-attention based explanation approaches(wo, x∇x and ig) requiring a lower fraction of to-kens to be removed compared to non-tasc acrossencoders, datasets and attention mechanisms in themajority of cases (see full results in appendix e)..8 qualitative analysis.
we ﬁnally examine qualitatively what type of infor-mation the parameter u from lin-tasc learns.
sim-ilar to a bag-of-words model, our initial hypothesisis that u will assign high scores to the words thatare most relevant to the task.
figure 4 illustrates the5 highest and lowest scored words from the imdband adr datasets with a lstm encoder and dotattention and cnn encoder and tanh attention re-spectively.
for brevity we include two examples,however observations hold similar throughout otherconﬁgurations (e.g.
encoders, datasets) and whenincreasing the number of top-k words..we ﬁrst observe in 4a, that indeed words ex-pressing sentiment are assigned with high scores(e.g.
excellent, waste, perfect), either positive ornegative.
however, a positive or negative sign does.
12we do not compare with lime (ribeiro et al., 2016)because wo and the gradient-based approaches outperform it(nguyen, 2018; atanasova et al., 2020)..figure 3: box-plots of fractions of tokens removedacross all test instances and importance metrics.
de-notes attention without tasc;denotes attention withlin-tasc (lower and narrower is better)..7 comparing tasc with non-attention.
input importance metrics.
we ﬁnally compare explanations provided by usinglin-tasc and α∇α to three standard non-attentioninput importance metrics without tasc which arestrong baselines for explainability (nguyen, 2018;atanasova et al., 2020)..word omission (wo) (robnik- ˇsikonja andkononenko, 2008; nguyen, 2018): ranking in-put words by computing the difference betweenthe probabilities of the predicted class when includ-ing a word i and omitting it: woi “ ppˆy|xq ´ppˆy|xzxiq.
inputxgrad (x∇x) (kindermans et al., 2016;atanasova et al., 2020): ranking words by mul-tiplying the gradient of the input by the input withrespect to the predicted class: ∇xi “ b ˆybxi.
integrated gradients (ig) (sundararajan et al.,2017): ranking words by computing the integralof the gradients taken along a straight path froma baseline input to the original input, where thebaseline is the zero embedding vector..comparison results table 5 shows the resultson decision ﬂip (fraction of tokens removed) com-paring the best performing attention-based impor-tance metric (α∇α) with lin-tasc to non-tascmodels with wo, x∇x and ig importance met-.
484tanh.
dot.
sst.
adr.
non-tasc tasc non-tasc tascdata enc() wo x∇x ig α∇α wo x∇x ig α∇α.64 .51 .22.62 .49 .55.23 .19 .19.24 .20 .19.25 .23 .19.22 .19 .18.19 .18 .18.26 .24 .18.20 .18 .19.25 .20 .19.90 .87 .50.91 .89 .31.88 .87 .34.81 .80 .32.80 .80 .38.84 .84 .35.43 .39 .40.63 .57 .31.74 .74 .36.78 .78 .37.72 .49 .20.69 .43 .07.09 .07 .05.12 .07 .04.15 .08 .05.12 .07 .04.07 .06 .05.05 .05 .05.07 .06 .05.09 .07 .05.76 .60 .60.78 .56 .50.52 .35 .46.51 .30 .38.40 .30 .22.36 .31 .20.25 .23 .19.24 .25 .19.35 .25 .21.38 .28 .20.57 .26 .05.67 .43 .03.40 .30 .01.32 .12 .01.18 .08 .01.24 .23 .01.04 .03 .02.03 .22 .01.09 .02 .02.15 .02 .01.bert .29lstm .25gru .24mlp .36cnn .30bert .83lstm .82gru .84mlp .71cnn .80bert .23lstm .18gru .18mlp .16cnn .21bert .62lstm .53gru .45mlp .53cnn .55bert .24lstm .35gru .20mlp .40cnn .26.
.32.21.24.22.22.81.87.79.49.77.24.26.27.18.27.56.47.54.44.53.21.28.36.13.43.ag.
imdb.
mimic.
table 5: average fraction of tokens required to cause adecision ﬂip using the best performing attention-basedranking (α∇α) with tasc, word omission without tasc(wo), inputxgrad without tasc (∇x) and integratedgradients without tasc (ig)..not correspond to supporting the positive or nega-tive class respectively.
for example withdrawal inadr can be considered relevant to positive class,yet it is negatively scored.
also sick can be con-sidered a withdrawal symptom which is relevant tothe negative class, yet it is positively scored.
wespeculate that this happens due to the complex non-linear relationships between the input words andthe target classes learned by the model..(a) imdb - lstm - dot.
(b) adr - gru - tanh.
figure 4: highest and lowest scored 5 words fromlearnable parameter u with lstm encoder and dotmechanism for the imdb dataset..9 conclusion.
acknowledgments.
we introduced tasc, a family of three encoder-induce context-independent mechanisms thatindependent task-speciﬁc information to attention.
we conducted an extensive series of experimentsshowing the superiority of tasc over vanilla atten-tion on improving faithfulness of attention-basedinterpretability without sacriﬁcing predictive per-formance.
finally, we showed that attention-basedexplanations with tasc outperform other inter-pretability techniques.
for future work, we willexplore the effectiveness of tasc in sequence-to-sequence tasks similar to vashishth et al.
(2019)..we would like to thank the anonymous review-ers for their constructive and detailed commentsthat helped to improve the paper.
nikolaos ale-tras is supported by epsrc grant ep/v055712/1,part of the european commission chist-eraprogramme, call 2019 xai: explainable machinelearning-based artiﬁcial intelligence..references.
leila arras, franziska horn, gr´egoire montavon,klaus-robert m¨uller, and wojciech samek.
2016.explaining predictions of non-linear classiﬁers in.
485nlp.
in proceedings of the 1st workshop on rep-resentation learning for nlp, pages 1–7..leila arras, gr´egoire montavon, klaus-robert m¨uller,and wojciech samek.
2017. explaining recurrentneural network predictions in sentiment analysis.
inproceedings of the 8th workshop on computationalapproaches to subjectivity, sentiment and socialmedia analysis, pages 159–168..pepa atanasova, jakob grue simonsen, christina li-oma, and isabelle augenstein.
2020. a diagnosticstudy of explainability techniques for text classiﬁ-in proceedings of the 2020 conference oncation.
empirical methods in natural language process-ing (emnlp), pages 3256–3274, online.
associa-tion for computational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, conference track proceedings..francesco barbieri, luis espinosa-anke,.
josecamacho-collados, steven schockaert, and hora-interpretable emoji predictioncio saggion.
2018.in proceedingsvia label-wise attention lstms.
of the 2018 conference on empirical methods innatural language processing, pages 4766–4771..jasmijn bastings, wilker aziz, and ivan titov.
2019.interpretable neural predictions with differentiablebinary variables.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2963–2977, florence, italy.
associa-tion for computational linguistics..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..ilias chalkidis, manos fergadiotis, dimitrios tsarapat-sanis, nikolaos aletras, ion androutsopoulos, andprodromos malakasiotis.
2021. paragraph-level ra-tionale extraction through regularization: a casestudy on european court of human rights cases.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 226–241, online.
association for computa-tional linguistics..hanjie chen and yangfeng ji.
2020. learning varia-tional word masks to improve the interpretability ofin proceedings of the 2020neural text classiﬁers.
conference on empirical methods in natural lan-guage processing (emnlp), pages 4236–4251, on-line.
association for computational linguistics..peng chen, zhongqian sun, lidong bing, and weiyang.
2017. recurrent attention network on mem-ory for aspect sentiment analysis.
in proceedings ofthe 2017 conference on empirical methods in natu-ral language processing, pages 452–461..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734..edward choi, mohammad taha bahadori, jimeng sun,joshua kulas, andy schuetz, and walter stewart.
2016. retain: an interpretable predictive model forhealthcare using reverse time attention mechanism.
in d. d. lee, m. sugiyama, u. v. luxburg, i. guyon,and r. garnett, editors, advances in neural informa-tion processing systems 29, pages 3504–3512..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..reza ghaeini, xiaoli fern, and prasad tadepalli.
2018.interpreting recurrent and attention-based neuralmodels: a case study on natural language infer-in proceedings of the 2018 conference onence.
empirical methods in natural language processing,pages 4952–4957, brussels, belgium.
associationfor computational linguistics..christopher grimsley, elijah mayﬁeld, and juliar.s.
bursten.
2020. why attention is not expla-nation: surgical intervention and causal reasoningin proceedings of the 12thabout neural models.
language resources and evaluation conference,pages 1780–1790, marseille, france.
european lan-guage resources association..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural computation, 9:1735–80..alon jacovi and yoav goldberg.
2020. towards faith-fully interpretable nlp systems: how should we de-ﬁne and evaluate faithfulness?
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4198–4205, online.
as-sociation for computational linguistics..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556..sarthak jain, sarah wiegreffe, yuval pinter, and by-ron c. wallace.
2020. learning to faithfully rational-ize by construction.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4459–4473, online.
associationfor computational linguistics..486alistair ew johnson, tom j pollard, lu shen,h lehman li-wei, mengling feng, moham-mad ghassemi, benjamin moody, peter szolovits,leo anthony celi, and roger g mark.
2016. mimic-iii, a freely accessible critical care database.
scien-tiﬁc data, 3:160035..armand joulin, edouard grave, piotr bojanowski,matthijs douze, herv´e j´egou, and tomas mikolov.
2016. fasttext.zip: compressing text classiﬁcationmodels.
corr, abs/1612.03651..pieter-jan kindermans, kristof sch¨utt, klaus-robertm¨uller, and sven d¨ahne.
2016.investigatingthe inﬂuence of noise and distractors on the in-arxiv preprintterpretation of neural networks.
arxiv:1611.07270..yann lecun, patrick haffner, l´eon bottou, andyoshua bengio.
1999. object recognition within shape, contour andgradient-based learning.
grouping in computer vision, pages 319–345..tao lei, regina barzilay, and tommi jaakkola.
2016.rationalizing neural predictions.
in proceedings ofthe 2016 conference on empirical methods in nat-ural language processing, pages 107–117, austin,texas.
association for computational linguistics..jiwei li, xinlei chen, eduard hovy, and dan juraf-sky.
2016a.
visualizing and understanding neuralin proceedings of the 2016 con-models in nlp.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 681–691..jiwei li, will monroe, and dan jurafsky.
2016b.
un-derstanding neural networks through representationerasure.
arxiv preprint arxiv:1612.08220..tal linzen, grzegorz chrupała, yonatan belinkov, anddieuwke hupkes, editors.
2019. proceedings of the2019 acl workshop blackboxnlp: analyzing andinterpreting neural networks for nlp..zachary c lipton.
2016. the mythos of model inter-in machine learning: work-pretability.
int.
conf.
shop on human interpretability in machine learn-ing..scott m lundberg and su-in lee.
2017. a uniﬁedin ad-approach to interpreting model predictions.
vances in neural information processing systems,volume 30. curran associates, inc..andrew l maas, raymond e daly, peter t pham, danhuang, andrew y ng, and christopher potts.
2011.learning word vectors for sentiment analysis.
inproceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies-volume 1, pages 142–150..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their composition-in c. j. c. burges, l. bottou, m. welling,ality..z. ghahramani, and k. q. weinberger, editors, ad-vances in neural information processing systems26, pages 3111–3119..tim miller.
2019. explanation in artiﬁcial intelligence:insights from the social sciences.
artiﬁcial intelli-gence, 267:1–38..akash kumar mohankumar, preksha nema, sharannarasimhan, mitesh m. khapra, balaji vasan srini-vasan, and balaraman ravindran.
2020. towardstransparent and explainable attention models.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4206–4216, online.
association for computational lin-guistics..dong nguyen.
2018. comparing automatic and hu-man evaluation of local explanations for text clas-in proceedings of the 2018 conferencesiﬁcation.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 1069–1078, new orleans, louisiana.
association for com-putational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..danish pruthi, mansi gupta, bhuwan dhingra, gra-ham neubig, and zachary c. lipton.
2020. learn-ing to deceive with attention-based explanations.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4782–4793, online.
association for computational lin-guistics..radim ˇreh˚uˇrek and petr sojka.
2010. software frame-work for topic modelling with large corpora.
inproceedings of the lrec 2010 workshop on newchallenges for nlp frameworks, pages 45–50..marco ribeiro, sameer singh, and carlos guestrin.
2016.
“why should i trust you?”: explaining the pre-dictions of any classiﬁer.
in proceedings of the 2016conference of the north american chapter of theassociation for computational linguistics: demon-strations, pages 97–101, san diego, california.
as-sociation for computational linguistics..marko robnik- ˇsikonja and igor kononenko.
2008.explaining classiﬁcations for individual instances.
ieee transactions on knowledge and data engi-neering, 20(5):589–600..abeed sarker, rachel ginn, azadeh nikfarjam, kareno’connor, karen smith, swetha jayaraman, te-jaswi upadhaya, and graciela gonzalez.
2015. uti-lizing social media data for pharmacovigilance: a re-view.
journal of biomedical informatics, 54:202–212..487yequan wang, minlie huang, xiaoyan zhu, andli zhao.
2016. attention-based lstm for aspect-level sentiment classiﬁcation.
in proceedings of the2016 conference on empirical methods in naturallanguage processing, pages 606–615..sarah wiegreffe and yuval pinter.
2019. attention isnot not explanation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 11–20..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-in proceedings of the 32nd internationaltion.
conference on machine learning, volume 37 ofproceedings of machine learning research, pages2048–2057..zichao yang, diyi yang, chris dyer, xiaodong he,alex smola, and eduard hovy.
2016. hierarchicalattention networks for document classiﬁcation.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1480–1489, san diego, california.
associa-tion for computational linguistics..zhongheng zhang, marcus w beck, david a win-kler, bin huang, wilbert sibanda, hemant goyal,et al.
2018. opening the black box of neuralnetworks: methods for interpreting neural networkmodels in clinical applications.
annals of transla-tional medicine, 6(11)..cansu sen, thomas hartvigsen, biao yin, xiangnankong, and elke rundensteiner.
2020. human at-tention maps for text classiﬁcation: do humans andneural networks focus on the same words?
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4596–4608, online.
association for computational lin-guistics..soﬁa serrano and noah a. smith.
2019. is attentionin proceedings of the 57th annualinterpretable?
meeting of the association for computational lin-guistics, pages 2931–2951..richard socher, alex perelygin, jean wu, jasonchuang, christopher d. manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642, seattle, washington, usa.
asso-ciation for computational linguistics..chi sun, xipeng qiu, yige xu, and xuanjing huang.
2019. how to ﬁne-tune bert for text classiﬁcation?
in china national conference on chinese computa-tional linguistics, pages 194–206.
springer..xiaobing sun and wei lu.
2020. understanding at-tention for text classiﬁcation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 3418–3428, online.
as-sociation for computational linguistics..mukund sundararajan, ankur taly, and qiqi yan.
2017.in pro-axiomatic attribution for deep networks.
ceedings of the 34th international conference onmachine learning - volume 70, icml’17, page3319–3328.
jmlr.org..marcos treviso and andr´e f. t. martins.
2020. the ex-planation game: towards prediction explainabilityin proceedings ofthrough sparse communication.
the third blackboxnlp workshop on analyzing andinterpreting neural networks for nlp, pages 107–118, online.
association for computational linguis-tics..martin tutek and jan snajder.
2020. staying true toyour word: (how) can attention become explanation?
in proceedings of the 5th workshop on representa-tion learning for nlp, pages 131–142, online.
as-sociation for computational linguistics..shikhar vashishth, shyam upadhyay, gaurav singhtomar, and manaal faruqui.
2019. attention in-arxiv preprintterpretability across nlp tasks.
arxiv:1909.11218..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..488