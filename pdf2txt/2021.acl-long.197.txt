self-guided contrastive learning for bert sentence representations.
taeuk kim†∗, kang min yoo‡, and sang-goo lee††dept.
of computer science and engineering, seoul national university, seoul, korea‡naver ai lab, seongnam, korea{taeuk,sglee}@europa.snu.ac.kr, kangmin.yoo@navercorp.com.
abstract.
although bert and its variants have reshapedthe nlp landscape,it still remains unclearhow best to derive sentence embeddings fromsuch pre-trained transformers.
in this work,we propose a contrastive learning method thatutilizes self-guidance for improving the qual-ity of bert sentence representations.
ourmethod ﬁne-tunes bert in a self-supervisedfashion, does not rely on data augmentation,and enables the usual [cls] token embed-dings to function as sentence vectors.
more-over, we redesign the contrastive learning ob-jective (nt-xent) and apply it to sentence rep-resentation learning.
we demonstrate with ex-tensive experiments that our approach is moreeffective than competitive baselines on diversesentence-related tasks.
we also show it is efﬁ-cient at inference and robust to domain shifts..1.introduction.
pre-trained transformer (vaswani et al., 2017) lan-guage models such as bert (devlin et al., 2019)and roberta (liu et al., 2019) have been integralto achieving recent improvements in natural lan-guage understanding.
however, it is not straightfor-ward to directly utilize these models for sentence-level tasks, as they are basically pre-trained to focuson predicting (sub)word tokens given context.
themost typical way of converting the models into sen-tence encoders is to ﬁne-tune them with supervisionfrom a downstream task.
in the process, as initiallyproposed by devlin et al.
(2019), a pre-deﬁned to-ken’s (a.k.a.
[cls]) embedding from the last layerof the encoder is deemed as the representation of aninput sequence.
this simple but effective methodis possible because, during supervised ﬁne-tuning,the [cls] embedding functions as the only com-munication gate between the pre-trained encoder.
*this work has been mainly conducted when tk was a.research intern at naver ai lab..figure 1: bert(-base)’s layer-wise performance withdifferent pooling methods on the sts-b test set.
weobserve that the performance can be dramatically var-ied according to the selected layer and pooling strategy.
our self-guided training (sg / sg-opt) assures muchimproved results compared to those of the baselines..and a task-speciﬁc layer, encouraging the [cls]vector to capture the holistic information..on the other hand,.
in cases where labeleddatasets are unavailable, it is unclear what the beststrategy is for deriving sentence embeddings frombert.1 in practice, previous studies (reimers andgurevych, 2019; li et al., 2020; hu et al., 2020)reported that na¨ıvely (i.e., without any processing)leveraging the [cls] embedding as a sentencerepresentation, as is the case of supervised ﬁne-tuning, results in disappointing outcomes.
cur-rently, the most common rule of thumb for buildingbert sentence embeddings without supervision isto apply mean pooling on the last layer(s) of bert..1in this paper, the term bert has two meanings: nar-rowly, the bert model itself, and more broadly, pre-trainedtransformer encoders that share the same spirit with bert..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2528–2540august1–6,2021.©2021associationforcomputationallinguistics2528yet, this approach can be still sub-optimal.
in apreliminary experiment, we constructed sentenceembeddings by employing various combinations ofdifferent bert layers and pooling methods, andtested them on the semantic textual similarity(sts) benchmark dataset (cer et al., 2017).2 wediscovered that bert(-base)’s performance, mea-sured in spearman correlation (× 100), can rangefrom as low as 16.71 ([cls], the 10th layer) to63.19 (max pooling, the 2nd layer) depending onthe selected layer and pooling method (see fig-ure 1).
this result suggests that the current prac-tice of building bert sentence vectors is not solidenough, and that there is room to bring out more ofbert’s expressiveness..in this work, we propose a contrastive learningmethod that makes use of a newly proposed self-guidance mechanism to tackle the aforementionedproblem.
the core idea is to recycle intermediatebert hidden representations as positive samplesto which the ﬁnal sentence embedding should beclose.
as our method does not require data augmen-tation, which is essential in most recent contrastivelearning frameworks, it is much simpler and easierto use than existing methods (fang and xie, 2020;xie et al., 2020).
moreover, we customize the nt-xent loss (chen et al., 2020), a contrastive learningobjective widely used in computer vision, for bettersentence representation learning with bert.
wedemonstrate that our approach outperforms com-petitive baselines designed for building bert sen-tence vectors (li et al., 2020; wang and kuo, 2020)in various environments.
with comprehensive anal-yses, we also show that our method is more compu-tationally efﬁcient than the baselines at inferencein addition to being more robust to domain shifts..2 related work.
contrastive representation learning.
con-trastive learning has been long considered as ef-fective in constructing meaningful representations.
for instance, mikolov et al.
(2013) propose to learnword embeddings by framing words nearby a tar-get word as positive samples while others as neg-ative.
logeswaran and lee (2018) generalize theapproach of mikolov et al.
(2013) for sentence rep-resentation learning.
more recently, several stud-ies (fang and xie, 2020; giorgi et al., 2020; wuet al., 2020) suggest to utilize contrastive learning.
2in the experiment, we employ the settings identical withones used in chapter 4. refer to chapter 4 for more details..for training transformer models, similar to our ap-proach.
however, they generally require data aug-mentation techniques, e.g., back-translation (sen-nrich et al., 2016), or prior knowledge on trainingdata such as order information, while our methoddoes not.
furthermore, we focus on revising bertfor computing better sentence embeddings ratherthan training a language model from scratch..on the other hand, contrastive learning has beenalso receiving much attention from the computer vi-sion community (chen et al.
(2020); chen and he(2020); he et al.
(2020), inter alia).
we improvethe framework of chen et al.
(2020) by optimizingits learning objective for pre-trained transformer-based sentence representation learning.
for ex-tensive surveys on contrastive learning, refer tole-khac et al.
(2020) and jaiswal et al.
(2020)..fine-tuning bert with supervision.
it is notalways trivial to ﬁne-tune pre-trained transformermodels of gigantic size with success, especiallywhen the number of target domain data is limited(mosbach et al., 2020).
to mitigate this training in-stability problem, several approaches (aghajanyanet al., 2020; jiang et al., 2020; zhu et al., 2020)have been recently proposed.
in particular, gunelet al.
(2021) propose to exploit contrastive learningas an auxiliary training objective during ﬁne-tuningbert with supervision from target tasks.
in con-trast, we deal with the problem of adjusting bertwhen such supervision is not available..sentence embeddingsfrom bert.
sincebert and its variants are originally designed tobe ﬁne-tuned on each downstream task to attaintheir optimal performance, it remains ambiguoushow best to extract general sentence representationsfrom them, which are broadly applicable acrossdiverse sentence-related tasks.
following con-neau et al.
(2017), reimers and gurevych (2019)(sbert) propose to compute sentence embeddingsby conducting mean pooling on the last layer ofbert and then ﬁne-tuning the pooled vectors onthe natural language inference (nli) datasets (bow-man et al., 2015; williams et al., 2018).
meanwhile,some other studies concentrate on more effectivelyleveraging the knowledge embedded in bert toconstruct sentence embeddings without supervi-sion.
speciﬁcally, wang and kuo (2020) proposea pooling method based on linear algebraic algo-rithms to draw sentence vectors from bert’s inter-mediate layers.
li et al.
(2020) suggest to learn a.
2529mapping from the average of the embeddings ob-tained from the last two layers of bert to a spher-ical gaussian distribution using a ﬂow model, andto leverage the redistributed embeddings in placeof the original bert representations.
we followthe setting of li et al.
(2020) in that we only utilizeplain text during training, however, unlike all theothers that rely on a certain pooling method evenafter training, we directly reﬁne bert so that thetypical [cls] vector can function as a sentenceembedding.
note also that there exists concurrentwork (carlsson et al., 2021; gao et al., 2021; wanget al., 2021) whose motivation is analogous to ours,attempting to improve bert sentence embeddingsin an unsupervised fashion..3 method.
as bert mostly requires some type of adaptationto be properly applied to a task of interest, it mightnot be desirable to derive sentence embeddingsdirectly from bert without ﬁne-tuning.
whilereimers and gurevych (2019) attempt to alleviatethis problem with typical supervised ﬁne-tuning,we restrict ourselves to revising bert in an un-supervised manner, meaning that our method onlydemands a bunch of raw sentences for training..among possible unsupervised learning strate-gies, we concentrate on contrastive learning whichcan inherently motivate bert to be aware of sim-ilarities between different sentence embeddings.
considering that sentence vectors are widely usedin computing the similarity of two sentences, theinductive bias introduced by contrastive learningcan be helpful for bert to work well on such tasks.
the problem is that sentence-level contrastive learn-ing usually requires data augmentation (fang andxie, 2020) or prior knowledge on training data, e.g.,order information (logeswaran and lee, 2018), tomake plausible positive/negative samples.
we at-tempt to circumvent these constraints by utilizingthe hidden representations of bert, which are read-ily accessible, as samples in the embedding space..3.1 contrastive learning with self-guidance.
we aim at developing a contrastive learning methodthat is free from external procedure such as dataaugmentation.
a possible solution is to leverage(virtual) adversarial training (miyato et al., 2018)in the embedding space.
however, there is no as-surance that the semantics of a sentence embeddingwould remain unchanged when it is added with a.figure 2: self-guided contrastive learning framework.
we clone bert into two copies at the beginning oftraining.
bertt (except layer 0) is then ﬁne-tuned tooptimize the sentence vector ci while bertf is ﬁxed..random noise.
as an alternative, we propose to uti-lize the hidden representations from bert’s inter-mediate layers, which are conceptually guaranteedto represent corresponding sentences, as pivots thatbert sentence vectors should be close to or beaway from.
we call our method as self-guided con-trastive learning since we exploit internal trainingsignals made by bert itself to ﬁne-tune it..we describe our training framework in figure2. first, we clone bert into two copies, bertf(ﬁxed) and bertt (tuned) respectively.
bertfis ﬁxed during training to provide a training sig-nal while bertt is ﬁne-tuned to construct bettersentence embeddings.
the reason why we differen-tiate bertf from bertt is that we want to pre-vent the training signal computed by bertf frombeing degenerated as the training procedure contin-ues, which often happens when bertf = bertt .
this design decision also reﬂects our philosophythat our goal is to dynamically conﬂate the knowl-edge stored in bert’s different layers to producesentence embeddings, rather than introducing newinformation via extra training.
note that in oursetting, the [cls] vector from the last layer ofbertt , i.e., ci, is regarded as the ﬁnal sentenceembedding we aim to optimize/utilize during/afterﬁne-tuning..second, given b sentences in a mini-batch,say s1, s2, · · · , sb, we feed each sentence si intobertf and compute token-level hidden represen-tations hi,k ∈ rlen(si)×d:.
[hi,0; hi,1; · · · ; hi,k; · · · ; hi,l] = bertf (si),.
2530𝑝copysampler𝜎projection head 𝑓(initialize)...layer1layer0𝐁𝐄𝐑𝐓𝑭...layerℓlayer1layer0𝒉!
[cls]𝒄!the cat sat on the mat𝑠"𝒉!,#𝒉!,$𝒉!,%𝒄"𝒉"𝑓𝒉!𝑓𝒄!pooling𝑝𝑝𝐁𝐄𝐑𝐓𝑻𝐁𝐄𝐑𝐓𝑻$𝑭the bat and …𝑠%…𝑓𝒉"𝑓𝒄"nt-xentloss 𝐿layerℓwhere 0 ≤ k ≤ l (0: the non-contextualized layer),l is the number of hidden layers in bert, len(si)is the length of the tokenized sentence, and d isthe size of bert’s hidden representations.
then,we apply a pooling function p to hi,k for derivingdiverse sentence-level views hi,k ∈ rd from alllayers, i.e., hi,k = p(hi,k).
finally, we choose theﬁnal view to be utilized by applying a samplingfunction σ:.
hi = σ({hi,k|0 ≤ k ≤ l})..as we have no speciﬁc constraints in deﬁning pand σ, we employ max pooling as p and a uni-form sampler as σ for simplicity, unless otherwisestated.
this simple choice for the sampler impliesthat each hi,k has the same importance, which ispersuasive considering it is known that differentbert layers are specialized at capturing disparatelinguistic concepts (jawahar et al., 2019).3.third, we compute our sentence embedding ci.
for si as follows:.
ci = bertt (si)[cls],.
where bert(·)[cls] corresponds to the [cls]vector obtained from the last layer of bert.
next,we collect the set of the computed vectors intox = {x|x ∈ {ci} ∪ {hi}}, and for all xm ∈ x,we compute the nt-xent loss (chen et al., 2020):.
lbase.
m = − log (φ(xm, µ(xm))/z),where φ(u, v) = exp(g(f (u), f (v))/τ ).
and z = (cid:80)2b.
n=1,n(cid:54)=m φ(xm, xn)..note that τ is a temperature hyperparameter, fis a projection head consisting of mlp layers,4g(u, v) = u · v/(cid:107)u(cid:107)(cid:107)v(cid:107) is the cosine similarityfunction, and µ(·) is the matching function deﬁnedas follows,.
µ(x) =.
(cid:40).
hici.
if x is equal to ci.
if x is equal to hi..lastly, we sum all lbase.
m divided by 2b, and adda regularizer lreg = (cid:107)bertf − bertt (cid:107)22 to pre-vent bertt from being too distant from bertf .5.
3we can also potentially make use of another samplerfunctions to inject our bias or prior knowledge on target tasks.
4we employ a two-layered mlp whose hidden size is 4096.each linear layer in the mlp is followed by a gelu function.
5to be speciﬁc, lreg is the square of the l2 norm of thedifference between bertf and bertt .
as shown in figure2, we also freeze the 0th layer of bertt for stable learning..figure 3: four factors of the original nt-xent loss.
green and yellow arrows represent the force of attrac-tion and repulsion, respectively.
best viewed in color..as a result, the ﬁnal loss lbase is:.
lbase =.
m + λ · lreg,lbase.
12b.
2b(cid:88).
m=1.
where the coefﬁcient λ is a hyperparameter..to summarize, our method reﬁnes bert so thatthe sentence embedding ci has a higher similaritywith hi, which is another representation for thesentence si, in the subspace projected by f whilebeing relatively dissimilar with cj,j(cid:54)=i and hj,j(cid:54)=i.
after training is completed, we remove all the com-ponents except bertt and simply use ci as theﬁnal sentence representation..3.2 learning objective optimization.
in section 3.1, we relied on a simple variation ofthe general nt-xent loss, which is composed offour factors.
given sentence si and sj without lossof generality, the factors are as follows (figure 3):.
(1) ci →← hi (or cj →← hj): the main com-ponent that mirrors our core motivation that abert sentence vector (ci) should be consis-tent with intermediate views (hi) from bert.
(2) ci ←→ cj: a factor that forces sentence em-beddings (ci, cj) to be distant from each other.
(3) ci ←→ hj (or cj ←→ hi): an element thatmakes ci being inconsistent with views forother sentences (hj)..(4) hi ←→ hj: a factor that causes a discrepancybetween views of different sentences (hi, hj)..even though all the four factors play a certain role,some components may be useless or even cause anegative inﬂuence on our goal.
for instance, chenand he (2020) have recently reported that in imagerepresentation learning, only (1) is vital while oth-ers are nonessential.
likewise, we customize the.
2531𝒄!𝒄"𝒉!𝒉"(1)(2)(3)(4)training loss with three major modiﬁcations so thatit can be more well-suited for our purpose..first, as our aim is to improve ci with the aid ofhi, we re-deﬁne our loss focusing more on ci ratherthan considering ci and hi as equivalent entities:lopt1i = − log (φ(ci, hi)/ ˆz),j=1,j(cid:54)=i φ(ci, cj) + (cid:80)bwhere ˆz = (cid:80)bj=1 φ(ci, hj).
in other words, hi only functions as points that ci isencouraged to be close to or away from, and is notdeemed as targets to be optimized.
this revisionnaturally results in removing (4).
furthermore, wediscover that (2) is also insigniﬁcant for improvingperformance, and thus derive lopt2i = − log(φ(ci, hi)/ (cid:80)blopt2.
ij=1 φ(ci, hj))..:.
lastly, we diversify signals from (1) and (3) by.
allowing multiple views {hi,k} to guide ci:lopt3.
φ(ci,hi,k)(cid:80)l.i,k = − log.
φ(ci,hi,k)+(cid:80)b.m=1,m(cid:54)=i.
n=0 φ(ci,hm,n).
..we expect with this reﬁnement that the learning ob-jective can provide more precise and fruitful train-ing signals by considering additional (and freelyavailable) samples being provided with.
the ﬁnalform of our optimized loss is:.
lopt =.
1b(l + 1).
b(cid:88).
l(cid:88).
i=1.
k=0.
lopt3i,k + λ · lreg..in section 5.1, we show the decisions made in thissection contribute to improvements in performance..4 experiments.
4.1 general conﬁgurations.
in terms of pre-trained encoders, we leveragebert (devlin et al., 2019) for english datasetsand mbert, which is a multilingual variant ofbert, for multilingual datasets.
we also employroberta (liu et al., 2019) and sbert (reimersand gurevych, 2019) in some cases to evaluatethe generalizability of tested methods.
we use thesufﬁxes ‘-base’ and ‘-large’ to distinguish smalland large models.
every trainable model’s per-formance is reported as the average of 8 separateruns to reduce randomness.
hyperparameters areoptimized on the sts-b validation set using bert-base and utilized across different models.
see table8 in appendix a.1 for details.
our implementationis based on the huggingface’s transformers(wolf et al., 2019) and sbert (reimers andgurevych, 2019) library, and publicly available athttps://github.com/galsang/sg-bert..4.2 semantic textual similarity tasks.
we ﬁrst evaluate our method and baselines on se-mantic textual similarity (sts) tasks.
given twosentences, we derive their similarity score by com-puting the cosine similarity of their embeddings..datasets and metrics.
following the literature,we evaluate models on 7 datasets in total, that is,sts-b (cer et al., 2017), sick-r (marelli et al.,2014), and sts12-16 (agirre et al., 2012, 2013,2014, 2015, 2016).
these datasets contain pairs oftwo sentences, whose similarity scores are labeledfrom 0 to 5. the relevance between gold annota-tions and the scores predicted by sentence vectorsis measured in spearman correlation (× 100)..baselines and model speciﬁcation.
we ﬁrstprepare two non-bert approaches as baselines,i.e., glove (pennington et al., 2014) mean embed-dings and universal sentence encoder (use; ceret al.
(2018)).
in addition, various methods forbert sentence embeddings that do not requiresupervision are also introduced as baselines:.
• cls token embedding: it regards the [cls]vector from the last layer of bert as a sentencerepresentation..• mean pooling: this method conducts mean pool-ing on the last layer of bert and use the outputas a sentence embedding..• wk pooling: this follows the method of wangand kuo (2020), which exploits qr decomposi-tion and extra techniques to derive meaningfulsentence vectors from bert..• flow: this is bert-ﬂow proposed by li et al.
(2020), which is a ﬂow-based model that mapsthe vectors made by taking mean pooling on thelast two layers of bert to a gaussian space.6• contrastive (bt): following fang and xie(2020), we revise bert with contrastive learning.
however, this method relies on back-translationto obtain positive samples, unlike ours.
detailsabout this baseline are speciﬁed in appendix a.2..we make use of plain sentences from sts-b toﬁne-tune bert using our approach, identical withflow.7 we name the bert instances trained withour self-guided method as contrastive (sg) and.
6we restrictively utilize this model, as we ﬁnd it difﬁcultto exactly reproduce the model’s result with its ofﬁcial code.
7for training, li et al.
(2020) utilize the concatenation ofthe sts-b training, validation, and test set (without gold anno-tations).
we also follow the same setting for a fair comparison..2532models.
pooling.
sts-b.
sick-r.sts12.
sts13.
sts14.
sts15.
sts16.
avg..non-bert baselinesglove†use†.
bert-base+ no tuning+ no tuning+ no tuning+ flow+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
bert-large+ no tuning+ no tuning+ no tuning+ flow+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
sbert-base+ no tuning+ no tuning+ no tuning+ flow‡+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
sbert-large+ no tuning+ no tuning+ no tuning+ flow‡+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
mean-.
58.0274.92.
53.7676.69.
55.1464.49.
70.6667.80.
59.7364.61.
68.2576.83.
63.6673.18.
61.3271.22.clsmeanwkmean-2clsclscls.
clsmeanwkmean-2clsclscls.
clsmeanwkmean-2clsclscls.
clsmeanwkmean-2clsclscls.
20.3047.2916.0771.35±0.2763.27±1.4875.08±0.7377.23±0.43.
26.7547.0035.7572.72±0.3663.84±1.0575.22±0.5776.16±0.42.
73.6676.9878.3881.0374.67±0.3081.05±0.3481.46±0.27.
76.0179.1961.8781.1876.71±1.2282.35±0.1582.05±0.39.
42.4258.2241.5464.95±0.1666.91±1.2968.19±0.3668.16±0.50.
43.4453.8538.3963.77±0.1866.53±2.6269.63±0.9570.20±0.65.
69.7172.9174.3174.9770.31±0.4575.78±0.5576.64 ±0.42.
70.9973.7567.0674.5271.56±1.3476.44±0.4176.44±0.29.
21.5430.8716.0164.32±0.1754.26±1.8463.60±0.9866.84±0.73.
27.4427.6712.6562.82±0.1752.04±1.7564.37±0.7267.02±0.72.
70.1570.9769.7568.9571.19±0.3773.76±0.7675.16±0.56.
69.0572.2749.9570.1969.95±3.5774.84±0.5774.58±0.59.
32.1159.8921.8069.72±0.2564.03±2.3576.48±0.6980.13±0.51.
30.7655.7926.4171.24±0.2262.59±1.8477.59±1.0179.42±0.80.
71.1776.5376.9278.4872.41±0.6080.08±0.4581.27±0.37.
71.3478.4653.0280.2772.66±1.1682.89±0.4183.79±0.14.
21.2847.7315.9663.67±0.0654.28±1.8767.57±0.5771.23±0.40.
22.5944.4923.7465.39±0.1554.25±1.4568.27±0.4070.38±0.65.
68.8973.1972.3277.6269.90±0.4375.58±0.5776.31±0.38.
69.5074.9046.5578.8570.38±2.1077.27±0.3576.98±0.19.
37.8960.2933.5977.77±0.1568.19±0.9579.42±0.4981.56±0.28.
29.9851.6729.3478.98±0.2171.07±1.1180.08±0.2881.72±0.32.
75.5379.0981.1781.9577.16±0.4883.52±0.4384.71±0.26.
76.6680.9962.4782.9777.80±3.2484.44±0.2384.57±0.27.
44.2463.7334.0769.59±0.2867.50±0.9674.85±0.5477.17±0.22.
42.7461.8834.4273.23±0.2466.71±1.0874.53±0.4376.35±0.22.
70.1674.3076.2578.9471.63±0.5579.10±0.5180.33±0.19.
70.0876.2560.3280.5771.41±1.7379.54±0.4979.87±0.42.
31.4052.5725.5868.77±0.0762.63±1.2872.17±0.4474.62±0.25.
31.9648.9128.6770.07±0.8162.43±1.0772.81±0.3174.46±0.35.
71.3274.8575.5977.4272.47±0.3778.41±0.3379.41±0.17.
71.9576.5457.3278.3672.92±1.5379.68±0.3779.76±0.33.
table 1: experimental results on sts tasks.
results for trained models are averaged over 8 runs (±: the standarddeviation).
the best ﬁgure in each (model-wise) part is in bold and the best in each column is underlined.
ourmethod with self-guidance (sg, sg-opt) generally outperforms competitive baselines.
we borrow scores fromprevious work if we could not reproduce them.
†: from reimers and gurevych (2019).
‡: from li et al.
(2020)..contrastive (sg-opt), which utilize lbase andlopt in section 3 respectively..results.
we report the performance of differentapproaches on sts tasks in table 1 and table 11(appendix a.6).
from the results, we conﬁrm thefact that our methods (sg and sg-opt) mostlyoutperform other baselines in a variety of experi-mental settings.
as reported in earlier studies, thena¨ıve [cls] embedding and mean pooling areturned out to be inferior to sophisticated methods.
to our surprise, wk pooling’s performance is evenlower than that of mean pooling in most cases, andthe only exception is when wk pooling is appliedto sbert-base.
flow shows its strength outper-forming the simple strategies.
nevertheless, itsperformance is shown to be worse than that of ourmethods (although some exceptions exist in thecase of sbert-large).
note that contrastive learn-ing becomes much more competitive when it iscombined with our self-guidance algorithm ratherthan back-translation.
it is also worth mentioning.
models.
spanish.
baseline (agirre et al., 2014)umcc-dlsi-run2 (rank #1).
mbert+ cls+ mean pooling+ wk pooling+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
80.69.
12.6081.1479.7878.0482.0982.74.table 2: semeval-2014 task 10 spanish task..that the optimized version of our method (sg-opt)generally shows better performance than the basicone (sg), proving the efﬁcacy of learning objec-tive optimization (section 3.2).
to conclude, wedemonstrate that our self-guided contrastive learn-ing is effective in improving the quality of bertsentence embeddings when tested on sts tasks..4.3 multilingual sts tasks.
we expand our experiments to multilingual settingsby utilizing mbert and cross-lingual zero-shottransfer.
speciﬁcally, we reﬁne mbert using only.
2533models.
arabic(track 1).
spanish(track 3).
english(track 5).
baselinescosine baseline (cer et al., 2017)encu (rank #1, tian et al.
(2017)).
mbert+ cls+ mean pooling+ wk pooling+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
60.4574.40.
30.5751.0950.3854.2457.0958.52.
71.1785.59.
29.3854.5655.8768.1678.9380.19.
72.7885.18.
24.9754.8654.8773.8978.2478.03.table 3: results on semeval-2017 task 1: track 1(arabic), track 3 (spanish), and track 5 (english)..models.
mr cr subj mpqa sst2 trec mrpc avg..bert-base81.46 86.71 95.37+ mean+ wk80.64 85.53 95.27+ sg-opt 82.47 87.42 95.40.
87.90 85.83 90.3088.63 85.03 94.0388.92 86.20 91.60.
73.36 85.8571.71 85.8374.21 86.60.bert-large+ mean84.38 89.01 95.6086.69 89.20 90.9082.68 87.92 95.32 87.25 87.81 91.18+ wk+ sg-opt 86.03 90.18 95.82 87.08 90.73 94.65.
72.79 86.9470.13 86.0473.31 88.26.sbert-base82.80 89.03 94.07+ mean82.96 89.33 95.13+ wk+ sg-opt 83.34 89.45 94.68.
89.79 88.08 86.9390.56 88.10 91.9889.78 88.57 87.30.
75.11 86.5476.66 87.8275.26 86.91.table 4: experimental results on senteval..english data and test it on datasets written in otherlanguages.
as in section 4.2, we use the englishsts-b for training.
we consider two datasets forevaluation: (1) semeval-2014 task 10 (spanish;agirre et al.
(2014)) and (2) semeval-2017 task 1(arabic, spanish, and english; cer et al.
(2017)).
performance is measured in pearson correlation (×100) for a fair comparison with previous work..from table 2, we see that mbert with meanpooling already outperforms the best system (at thetime of the competition was held) on semeval-2014 and that our method further boosts themodel’s performance.
in contrast, in the case ofsemeval-2017 (table 3), mbert with mean pool-ing even fails to beat the strong cosine baseline.8however, mbert becomes capable of outperform-ing (in english/spanish) or being comparable with(arabic) the baseline by adopting our algorithm.
we observe that while cross-lingual transfer us-ing mbert looks promising for the languagesanalogous to english (e.g., spanish), its effective-ness may shrink on distant languages (e.g., arabic).
compared against the best system which is trainedon task-speciﬁc data, mbert shows reasonableperformance considering that it is never exposed toany labeled sts datasets.
in summary, we demon-strate that mbert ﬁne-tuned with our method hasa potential to be used as a simple but effective toolfor multilingual (especially european) sts tasks..4.4 senteval and supervised fine-tuning.
we also evaluate bert sentence vectors using thesenteval (conneau and kiela, 2018) toolkit.
givensentence embeddings, senteval trains linear classi-ﬁers on top of them and estimates the quality of thevectors via their performance (accuracy) on down-.
8the cosine baseline computes its score as the cosinesimilarity of binary sentence vectors with each dimensionrepresenting whether an individual word appears in a sentence..stream tasks.
among available tasks, we employ 7:mr, cr, subj, mpqa, sst2, trec, mrpc.9.
in table 4, we compare our method (sg-opt)with two baselines.10 we ﬁnd that our methodis helpful over usual mean pooling in improvingthe performance of bert-like models on senteval.
sg-opt also outperforms wk pooling on bert-base/large while being comparable on sbert-base.
from the results, we conjecture that self-guidedcontrastive learning and sbert training suggesta similar inductive bias in a sense, as the bene-ﬁt we earn by revising sbert with our methodis relatively lower than the gain we obtain whenﬁne-tuning bert.
meanwhile, it seems that wkpooling provides an orthogonal contribution that iseffective in the focused case, i.e., sbert-base..in addition, we examine how our algorithm im-pacts on supervised ﬁne-tuning of bert, althoughit is not the main concern of this work.
brieﬂy re-porting, we identify that the original bert(-base)and one tuned with sg-opt show comparable per-formance on the glue (wang et al., 2019) valida-tion set, implying that our method does not inﬂu-ence much on bert’s supervised ﬁne-tuning.
werefer readers to appendix a.4 for more details..5 analysis.
we here further investigate the working mechanismof our method with supplementary experiments.
all the experiments conducted in this section followthe conﬁgurations stipulated in section 4.1 and 4.2..9refer to conneau and kiela (2018) for each task’s spec.
10we focus on reporting our own results as we discoveredthat the toolkit’s outcomes can be ﬂuctuating depending onits conﬁguration (we list our settings in appendix a.3).
wealso restrict ourselves to evaluating sg-opt for simplicity, assg-opt consistently showed better performance than othercontrastive methods in previous experiments..2534sts tasks (avg.).
layer.
elapsed time.
training (sec.).
inference (sec.).
models.
bert-base+ sg-opt (lopt3)+ lopt2+ lopt1+ sg (lbase).
bert-base + sg-opt (τ = 0.01, λ = 0.1)+ τ = 0.1+ τ = 0.001+ λ = 0.0+ λ = 1.0- projection head (f ).
table 5: ablation study..74.6273.14 (-1.48)72.61 (-2.01)72.17 (-2.45).
74.6270.39 (-4.23)74.16 (-0.46)73.76 (-0.86)73.18 (-1.44)72.78 (-1.84).
bert-base-+ mean pooling-+ wk pooling155.37 (≈ 2.6 min.)
+ flow+ contrastive (sg-opt) 455.02 (≈ 7.5 min.).
13.94197.03 (≈ 3.3 min.)
28.4910.51.table 6: computational efﬁciency tested on sts-b..that no matter which test set is utilized (sts-b orall the seven sts tasks), our method clearly out-performs flow in every case, showing its relativerobustness to domain shifts.
sg-opt only loses1.83 (on the sts-b test set) and 1.63 (on averagewhen applied to all the sts tasks) points respec-tively when trained with nli rather than sts-b,while flow suffers from the considerable losses of12.16 and 4.19 for each case.
note, however, thatfollow-up experiments in more diverse conditionsmight be desired as future work, as the nli datasetinherently shares some similarities with sts tasks..5.3 computational efﬁciency.
in this part, we compare the computational efﬁ-ciency of our method to that of other baselines.
foreach algorithm, we measure the time elapsed dur-ing training (if required) and inference when testedon sts-b.
all methods are run on the same ma-chine (an intel xeon cpu e5-2620 v4 @ 2.10ghzand a titan xp gpu) using batch size 16. theexperimental results speciﬁed in table 6 show thatalthough our method demands a moderate amountof time (< 8 min.)
for training, it is the most ef-ﬁcient at inference, since our method is free fromany post-processing such as pooling once trainingis completed..5.4 representation visualization.
we visualize a few variants of bert sentence repre-sentations to grasp an intuition on why our methodis effective in improving performance.
speciﬁcally,we sample 20 positive pairs (red, whose similarityscores are 5) and 20 negative pairs (blue, whosescores are 0) from the sts-b validation set.
thenwe compute their vectors and draw them on the 2dspace with the aid of t-sne.
in figure 5, we con-ﬁrm that our sg-opt encourages bert sentenceembeddings to be more well-aligned with their pos-itive pairs while still being relatively far from theirnegative pairs.
we also visualize embeddings fromsbert (figure 6 in appendix a.5), and identifythat our approach and the supervised ﬁne-tuning.
figure 4: domain robustness study.
the yellow barsindicate the performance gaps each method has accord-ing to which data it is trained with: in-domain (sts-b)or out-of-domain (nli).
our method (sg-opt) clearlyshows its relative robustness compared to flow..5.1 ablation study.
we conduct an ablation study to justify the deci-sions made in optimizing our algorithm.
to thisend, we evaluate each possible variant on the testsets of sts tasks.
from table 5, we conﬁrm thatall our modiﬁcations to the nt-xent loss contributeto improvements in performance.
moreover, weshow that correct choices for hyperparameters areimportant for achieving the optimal performance,and that the projection head (f ) plays a signiﬁcantrole as in chen et al.
(2020)..5.2 robustness to domain shifts.
although our method in principle can accept anysentences in training, its performance might be var-ied with the training data it employs (especially de-pending on whether the training and test data sharethe same domain).
to explore this issue, we ap-ply sg-opt on bert-base by leveraging the mixof nli datasets (bowman et al., 2015; williamset al., 2018) instead of sts-b, and observe thedifference.
from figure 4, we conﬁrm the fact.
2535models.
pooling.
sts-b.
sick-r.sts12.
sts13.
sts14.
sts15.
sts16.
avg..bert-base+ contrastive (bt)+ contrastive (sg-opt)+ contrastive (bt + sg-opt).
clsclscls.
63.27±1.48 66.91±1.29 54.26±1.84 64.03±2.35 54.28±1.87 68.19±0.95 67.50±0.96 62.63±1.2877.23±0.43 68.16±0.50 66.84±0.73 80.13±0.51 71.23±0.40 81.56±0.28 77.17±0.22 74.62±0.2577.99±0.23 68.75±0.79 68.49±0.38 80.00±0.78 71.34±0.40 81.71±0.29 77.43±0.46 75.10±0.15.
table 7: ensemble of the techniques for contrastive learning: back-translation (bt) and self-guidance (sg-opt)..ting (indeed, it worked pretty well for most cases),a speciﬁc subset of the layers or another poolingmethod might bring better performance in a partic-ular environment, as we observed in section 4.4that we could achieve higher numbers by employ-ing mean pooling and excluding lower layers inthe case of senteval (refer to appendix a.3 fordetails).
therefore, in future work, it is encouragedto develop a systematic way of making more opti-mized design choices in specifying our method byconsidering the characteristics of target tasks..second, we expect that the effectiveness of con-trastive learning in revising bert can be improvedfurther by properly combining different techniquesdeveloped for it.
as an initial attempt towards thisdirection, we conduct an extra experiment wherewe test the ensemble of back-translation and ourself-guidance algorithm by inserting the originalsentence into bertt and its back-translation intobertf when running our framework.
in table7, we show that the fusion of the two techniquesgenerally results in better performance, sheddingsome light on our future research direction..7 conclusion.
in this paper, we have proposed a contrastive learn-ing method with self-guidance for improving bertsentence embeddings.
through extensive experi-ments, we have demonstrated that our method canenjoy the beneﬁt of contrastive learning without re-lying on external procedures such as data augmen-tation or back-translation, succeeding in generatinghigher-quality sentence representations comparedto competitive baselines.
furthermore, our methodis efﬁcient at inference because it does not requireany post-processing once its training is completed,and is relatively robust to domain shifts..acknowledgments.
we would like to thank anonymous reviewers fortheir fruitful feedback.
we are also grateful to jung-woo ha, sang-woo lee, gyuwan kim, and othermembers in naver ai lab in addition to reinaldkim amplayo for their insightful comments..figure 5: sentence representation visualization.
(top)embeddings from the original bert.
(bottom) embed-dings from the bert instance ﬁne-tuned with sg-opt.
red numbers correspond to positive sentence pairs andblue to negative pairs..used in sbert provide a similar effect, making theresulting embeddings more suitable for calculatingcorrect similarities between them..6 discussion.
in this section, we discuss a few weaknesses ofour method in its current form and look into somepossible avenues for future work..first, while deﬁning the proposed method insection 3, we have made decisions on some partswithout much consideration about their optimal-ity, prioritizing simplicity instead.
for instance,although we proposed utilizing all the intermediatelayers of bert and max pooling in a normal set-.
25361122334455667788991010111112121313141415151616171718181919202021212222232324242525262627272828292930303131323233333434353536363737383839394040bert-base ([cls])1122334455667788991010111112121313141415151616171718181919202021212222232324242525262627272828292930303131323233333434353536363737383839394040bert-base + contrastive (sg-opt)references.
armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2020. better ﬁne-tuning by reducing representa-tional collapse.
arxiv preprint arxiv:2008.03156..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, i˜nigo lopez-gazpio, montse maritxalar, radamihalcea, german rigau, larraitz uria, and janycewiebe.
2015. semeval-2015 task 2: semantic tex-tual similarity, english, spanish and pilot on inter-pretability.
in semeval..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycewiebe.
2014. semeval-2014 task 10: multilingualsemantic textual similarity.
in semeval..eneko agirre, carmen banea, daniel cer, mona diab,aitor gonzalez-agirre, rada mihalcea, germanrigau, and janyce wiebe.
2016. semeval-2016task 1: semantic textual similarity, monolingual andcross-lingual evaluation.
in semeval..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representa-tions.
in lrec..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in emnlp..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl..hongchao fang and pengtao xie.
2020. cert: con-trastive self-supervised learning for language under-standing.
arxiv preprint arxiv:2005.12766..tianyu gao, xingcheng yao, and danqi chen.
2021.simcse: simple contrastive learning of sentence em-beddings.
arxiv preprint arxiv:2104.08821..john m giorgi, osvald nitski, gary d bader, andbo wang.
2020. declutr: deep contrastive learn-ing for unsupervised textual representations.
arxivpreprint arxiv:2006.03659..eneko agirre, daniel cer, mona diab, and aitorgonzalez-agirre.
2012. semeval-2012 task 6: apilot on semantic textual similarity.
in semeval..beliz gunel, jingfei du, alexis conneau, and veselinstoyanov.
2021. supervised contrastive learning forpre-trained language model ﬁne-tuning.
in iclr..eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo.
2013.
*sem 2013 sharedtask: semantic textual similarity.
in *sem..samuel bowman, gabor angeli, christopher potts, andchristopher d manning.
2015. a large annotatedcorpus for learning natural language inference.
inemnlp..fredrik carlsson, evangelia gogoulou, erik ylip¨a¨a,amaru cuba gyllensten, and magnus sahlgren.
2021. semantic re-tuning with contrastive tension.
in iclr..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity multilingual andcrosslingual focused evaluation.
in semeval..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for unsu-pervised visual representation learning.
in cvpr..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual general-isation.
in icml..ashish jaiswal, ashwin ramesh babu, moham-anda survey on con-arxiv preprint.
mad zaki zadeh, debapriya banerjee,fillia makedon.
2020.trastive self-supervised learning.
arxiv:2011.00362..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structureof language?
in acl..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,arxivet al.
2018. universal sentence encoder.
preprint arxiv:1803.11175..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledregularized optimization.
in acl..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inicml..xinlei chen and kaiming he.
2020. exploring sim-ple siamese representation learning.
arxiv preprintarxiv:2011.10566..phuc h le-khac, graham healy, and alan f smeaton.
2020. contrastive representation learning: a frame-work and review.
ieee access..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020. on the sentenceembeddings from pre-trained language models.
inemnlp..2537bin wang and c-c jay kuo.
2020. sbert-wk: a sen-tence embedding method by dissecting bert-basedword models.
arxiv preprint arxiv:2002.06652..kexin wang, nils reimers, and iryna gurevych.
2021.tsdae: using transformer-based sequential denois-ing auto-encoder for unsupervised sentence embed-ding learning.
arxiv preprint arxiv:2104.06979..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in naacl..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan fun-towicz, et al.
2019. huggingface’s transformers:state-of-the-art natural language processing.
arxivpreprint arxiv:1910.03771..zhuofeng wu, sinong wang, jiatao gu, madiankhabsa, fei sun, and hao ma.
2020. clear: con-trastive learning for sentence representation.
arxivpreprint arxiv:2012.15466..qizhe xie, zihang dai, eduard hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in neurips..chen zhu, yu cheng, zhe gan, siqi sun, tom gold-stein, and jingjing liu.
2020. freelb: enhanced ad-versarial training for natural language understanding.
in iclr..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..lajanugen logeswaran and honglak lee.
2018. anefﬁcient framework for learning sentence represen-tations.
in iclr..marco marelli, stefano menini, marco baroni, luisabentivogli, raffaella bernardi, and roberto zampar-elli.
2014. a sick cure for the evaluation of com-positional distributional semantic models.
in lrec..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
neurips..takeru miyato, shin-ichi maeda, masanori koyama,and shin ishii.
2018. virtual adversarial training:a regularization method for supervised and semi-supervised learning.
ieee transactions on patternanalysis and machine intelligence..marius mosbach, maksym andriushchenko, and diet-rich klakow.
2020. on the stability of ﬁne-tuningbert: misconceptions, explanations, and strong base-lines.
arxiv preprint arxiv:2006.04884..nathan ng, kyra yee, alexei baevski, myle ott,michael auli, and sergey edunov.
2019. facebookfair’s wmt19 news translation task submission.
inproceedings of the fourth conference on machinetranslation (volume 2: shared task papers, day 1)..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in emnlp..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in emnlp-ijcnlp..rico sennrich, barry haddow, and alexandra birch.
2016. improving neural machine translation modelswith monolingual data.
in acl..junfeng tian, zhiheng zhou, man lan, and yuanbinwu.
2017. ecnu at semeval-2017 task 1: lever-age kernel-based traditional nlp features and neu-ral networks to build a universal model for multilin-gual and cross-lingual semantic textual similarity.
inproceedings of the 11th international workshop onsemantic evaluation (semeval-2017)..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in neurips..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis plat-form for natural language understanding.
in iclr..2538a appendices.
a.1 hyperparameters.
hyperparameters.
values.
random seedevaluation stepepochbatch size (b)optimizerlearning rateearly stopping enduranceτλ.
1, 2, 3, 4, 1234, 2345, 3456, 789050116adamw (β1, β2=(0.9, 0.9))0.00005100.010.1.table 8: hyperparameters for experiments..a.2 speciﬁcation on contrastive (bt).
this baseline is identical with our contrastive(sg) model, except that it utilizes back-translationto generate positive samples.
to be speciﬁc, en-glish sentences in the training set are traslated intogerman sentences using the wmt’19 english-german translator provided by ng et al.
(2019),and then the translated german sentences are back-translated into english with the aid of the wmt’19german-english model also offered by ng et al.
(2019).
we utilize beam search during decodingwith the beam size 100, which is relatively large,since we want generated sentences to be more di-verse while grammatically correct at the same time.
note that the contrastive (bt) model is trained withthe nt-xent loss (chen et al., 2020), unlike cert(fang and xie, 2020) which leverages the mocotraining objective (he et al., 2020)..a.3 senteval conﬁgurations.
hyperparameters.
values.
random seedk-foldclassiﬁer (hidden dimension)optimizerbatch sizetenacityepoch.
1, 2, 3, 4, 1234, 2345, 3456, 78901050adam6454.table 9: senteval hyperparameters..in table 9, we stipulate the hyperparameters ofthe senteval toolkit used in our experiment.
ad-ditionally, we specify some minor modiﬁcationsapplied on our contrastive method (sg-opt).
first,we use the portion of the concatenation of snli(bowman et al., 2015) and mnli (williams et al.,2018) datasets as the training data instead of sts-b.
second, we do not leverage the ﬁrst several layersof plms when making positive samples, similar to.
wang and kuo (2020), and utilize mean poolinginstead of max pooling..a.4 glue experiments.
models.
qnli.
sst2.
cola.
mrpc.
rte.
bert-base 90.97±0.49 91.08±0.73 56.63±3.82 87.09±1.87 62.50±2.77+ sg-opt 91.28±0.28 91.68±0.41 56.36±3.98 86.96±1.11 62.75±3.91.
table 10: experimental results on a portion of theglue validation set..we here investigate the impact of our methodon typical supervised ﬁne-tuning of bert models.
concretely, we compare the original bert withone ﬁne-tuned using our sg-opt method on theglue (wang et al., 2019) benchmark.
note thatwe use the ﬁrst 10% of the glue validation setas the real validation set and the last 90% as thetest set, as the benchmark does not ofﬁcially pro-vide its test data.
we report experimental resultstested on 5 sub-tasks in table 10. the results showthat our method brings performance improvementsfor 3 tasks (qnli, sst2, and rte).
however, itseems that sg-opt does not inﬂuence much onsupervised ﬁne-tuning results, considering that theabsolute performance gap between the two modelsis not signiﬁcant.
we leave more analysis on thispart as future work..a.5 representation visualization (sbert).
figure 6: visualization of sentence vectors computedby sbert-base..a.6 roberta’s performance on sts tasks.
in table 11, we additionally report the performanceof sentence embeddings extracted from robertausing different methods.
our methods, sg and sg-opt, demonstrate their competitive performance.
25391122334455667788991010111112121313141415151616171718181919202021212222232324242525262627272828292930303131323233333434353536363737383839394040sbert-basemodels.
pooling.
sts-b.
sick-r.sts12.
sts13.
sts14.
sts15.
sts16.
avg..roberta-base+ no tuning+ no tuning+ no tuning+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
roberta-large+ no tuning+ no tuning+ no tuning+ contrastive (bt)+ contrastive (sg)+ contrastive (sg-opt).
clsmeanwkclsclscls.
clsmeanwkclsclscls.
45.4154.5335.75.
61.8962.0354.69.
16.6732.1120.31.
45.5756.3336.51.
30.3645.2232.41.
55.0861.3448.12.
56.9861.9846.32.
44.5753.3639.16.
79.93±1.08 71.97±1.00 62.34±2.41 78.60±1.74 68.65±1.48 79.31±0.65 77.49±1.29 74.04±1.1678.38±0.43 69.74±1.00 62.85±0.88 78.37±1.55 68.28±0.89 80.42±0.65 77.69±0.76 73.67±0.6277.60±0.30 68.42±0.71 62.57±1.12 78.96±0.67 69.24±0.44 79.99±0.44 77.17±0.24 73.42±0.31.
12.5247.0730.29.
40.6358.3828.25.
19.2533.6323.17.
22.9757.2230.92.
14.9345.6723.36.
33.4163.0040.07.
38.0161.1843.32.
25.9652.3131.34.
77.05±1.22 67.83±1.34 57.60±3.57 72.14±1.16 62.25±2.10 71.49±3.24 71.75±1.73 68.59±1.5376.15±0.54 66.07±0.82 64.77±2.52 71.96±1.53 64.54±1.04 78.06±0.52 75.14±0.94 70.95±1.1378.14±0.72 67.97±1.09 64.29±1.54 76.36±1.47 68.48±1.58 80.10±1.05 76.60±0.98 73.13±1.20.
table 11: performance of roberta on sts tasks when combined with different sentence embedding methods.
we could not report the performance of li et al.
(2020) (flow) as their ofﬁcial code do not support roberta..overall.
note that contrastive learning with back-translation (bt) also shows its remarkable perfor-mance in the case of roberta-base..2540