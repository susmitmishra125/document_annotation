a mutual information maximization approach forthe spurious solution problem in weakly supervised question answeringzhihong shao1, lifeng shang2, qun liu2, minlie huang1∗1the coai group, dcst, tsinghua university, institute for artiﬁcial intelligence;1state key lab of intelligent technology and systems;1beijing national research center for information science and technology;1tsinghua university, beijing 100084, china2huawei noah’s ark labszh19@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn{shang.lifeng, qun.liu}@huawei.com.
abstract.
weakly supervised question answering usuallyhas only the ﬁnal answers as supervision sig-nals while the correct solutions to derive theanswers are not provided.
this setting givesrise to the spurious solution problem:theremay exist many spurious solutions that coinci-dentally derive the correct answer, but trainingon such solutions can hurt model performance(e.g., producing wrong solutions or answers).
for example, for discrete reasoning tasks as ondrop, there may exist many equations to de-rive a numeric answer, and typically only oneof them is correct.
previous learning meth-ods mostly ﬁlter out spurious solutions withheuristics or using model conﬁdence, but donot explicitly exploit the semantic correlationsbetween a question and its solution.
in this pa-per, to alleviate the spurious solution problem,we propose to explicitly exploit such seman-tic correlations by maximizing the mutual in-formation between question-answer pairs andpredicted solutions.
extensive experimentson four question answering datasets show thatour method signiﬁcantly outperforms previouslearning methods in terms of task performanceand is more effective in training models to pro-duce correct solutions..1.introduction.
weakly supervised question answering is a com-mon setting of question answering (qa) whereonly ﬁnal answers are provided as supervision sig-nals while the correct solutions to derive them arenot.
this setting simpliﬁes data collection, but ex-poses model learning to the spurious solution prob-lem: there may exist many spurious ways to derivethe correct answer, and training a model with spu-rious solutions can hurt model performance (e.g.,misleading the model to produce unreasonable so-lutions or wrong answers).
as shown in fig 1,.
∗*corresponding author: minlie huang..figure 1: examples from three weakly supervised qatasks, i.e., multi-mention reading comprehension, dis-crete reasoning, and semantic parsing.
spans in darkgray and green denote semantic correlations between aquestion and its solution, while spans in orange are spu-rious information and should not be used in a solution..for multi-mention reading comprehension, manymentions of an answer in the document(s) are irrel-evant to the question; for discrete reasoning tasksor text2sql tasks, an answer can be produced bythe equations or sql queries that do not correctlymatch the question in logic..some previous works heuristically selected onepossible solution per question for training, e.g.,the ﬁrst answer span in the document (joshi et al.,2017; tay et al., 2018; talmor and berant, 2019);some treated all possible solutions equally andmaximized the sum of their likelihood (maximummarginal likelihood, or mml) (swayamdipta et al.,2018; clark and gardner, 2018; lee et al., 2019);many others selected solutions according to modelconﬁdence (liang et al., 2018; min et al., 2019),.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4111–4124august1–6,2021.©2021associationforcomputationallinguistics4111multi-mention reading comprehensionquestion:in the television series ‘thunderbirds’, what is lady penelope’s surname?answer:creighton warddocument(s):born on 24 december 2039, lady penelope is the 26-year old daughter of aristocrat lord hugh creighton wardand his wife, amelia.
the early years of her life were spent at creighton wardmansion.
… lady penelopecreighton wardis a fictional character introduced in the british mid-1960s supermarionationtelevision series thunderbirds, … perce is the gardener for the 2000 acre creighton wardestate and a friend of parker.
… possible solution(s):``creighton ward’’ across the document(s), only the third one is correctdiscrete reasoning over paragraphsquestion:how many years afterthe battle of powder riverdid powervillemontana become the first establishment in the county?answer:2paragraph:… from september 1-15, 1865, the powder river expedition (1865) battled native americans in the powder river battles (1865) near the future site of broadus.
on march 17, ①1876, the battle of powder river occurredin the south-central part of the county, aboutsouthwest of broadus.
in june ②1876six companies of the 7th cavalry regiment (united states) led by major marcus reno marched along the powder river … on november 1, ③1878, powderville, montana became the first establishment in the county, … on april 5, 1879, the mizpah creek incidents …possible solution(s):③1878 -①1876✓③1878 -②1876✗semantic parsingquestion: give me the kickofftime of the game that was airedon cbsagainstthe st. louis cardinals.answer: 1:00table header:| week | date | opponent| result | kickoff[a]| game site | tv| attendance | …possible solution(s):select (kickoff[a]) where tv=cbs and opponent=st.
louis cardinals✓select (kickoff[a]) where opponent=st.
louis cardinals✗i.e., the likelihood of the solutions being derivedby the model.
a drawback of these methods is thatthey do not explicitly consider the mutual semanticcorrelations between a question and its solutionwhen selecting solutions for training..intuitively speaking, a question often containsvital clues about how to derive the answer, and awrong solution together with its context often failsto align well with the question.
take the discretereasoning case in fig 1 as an example.
to answerthe question, we need to know the start year of thebattle of powder river, which is answered by theﬁrst 1876; the second 1876 is irrelevant as it is theyear of an event that happened during the battle..to exploit the semantic correlations between aquestion and its solution, we propose to maximizethe mutual information between question-answerpairs and model-predicted solutions.
as demon-strated by min et al.
(2019), for many qa tasks, itis feasible to precompute a modestly-sized, task-speciﬁc set of possible solutions containing thecorrect one.
therefore, we focus on handling thespurious solution problem under this circumstance.
speciﬁcally, we pair a task-speciﬁc model witha question reconstructor and repeat the followingtraining cycle (fig 2): (1) sample a solution fromthe solution set according to model conﬁdence,train the question reconstructor to reconstruct thequestion from that solution, and then (2) train thetask-speciﬁc model on the most likely solutionaccording to the question reconstructor.
duringtraining, the question reconstructor guides the task-speciﬁc model to predict those solutions consistentwith the questions.
for the question reconstructor,we devise an effective and uniﬁed way to encodesolutions in different tasks, so that solutions withsubtle differences (e.g., different spans with thesame surface form) can be easily discriminated..our contributions are as follows: (1) we proposea mutual information maximization approach forthe spurious solution problem in weakly supervisedqa, which exploits the semantic correlations be-tween a question and its solution; (2) we conductedextensive experiments on four qa datasets.
our ap-proach signiﬁcantly outperforms strong baselinesin terms of task performance and is more effectivein training models to produce correct solutions..2 related work.
question answering has raised prevalent attentionand has achieved great progress these years.
a lot.
of challenging datasets have been constructed toadvance models’ reasoning abilities, such as (1)reading comprehension datasets with extractive an-swer spans (joshi et al., 2017; dhingra et al., 2017),with free-form answers (kocisk´y et al., 2018), formulti-hop reasoning (yang et al., 2018), or for dis-crete reasoning over paragraphs (dua et al., 2019),and (2) datasets for semantic parsing (pasupat andliang, 2015; zhong et al., 2017; yu et al., 2018).
under the weakly supervised setting, the speciﬁcsolutions to derive the ﬁnal answers (e.g., the cor-rect location of an answer text, or the correct logicexecuting an answer) are not provided.
this settingis worth exploration as it simpliﬁes annotation andmakes it easier to collect large-scale corpora.
how-ever, this setting introduces the spurious solutionproblem, and thus complicates model learning..most existing approaches for this learning chal-lenge include heuristically selecting one possiblesolution per question for training (joshi et al., 2017;tay et al., 2018; talmor and berant, 2019), trainingon all possible solutions with mml (swayamdiptaet al., 2018; clark and gardner, 2018; lee et al.,2019; wang et al., 2019), reinforcement learning(liang et al., 2017, 2018), and hard em (min et al.,2019; chen et al., 2020).
all these approacheseither use heuristics to select possibly reasonablesolutions, rely on model architectures to bias to-wards correct solutions, or use model conﬁdenceto ﬁlter out spurious solutions in a soft or hardway.
they do not explicitly exploit the semanticcorrelations between a question and its solution..most relevantly, cheng and lapata (2018) fo-they modeled sqlcused on text2sql tasks;queries as the latent variables for question gen-eration, and maximized the evidence lower boundof log likelihood of questions.
a few works treatedsolution prediction and question generation as dualtasks and introduced dual learning losses to reg-ularize learning under the fully-supervised or thesemi-supervised setting (tang et al., 2017; caoet al., 2019; ye et al., 2019).
in dual learning,a model generates intermediate outputs (e.g., thetask-speciﬁc model predicts solutions from a ques-tion) while the dual model gives feedback signals(e.g., the question reconstructor computes the like-lihood of the question conditioned on predictedsolutions).
this method is featured in three aspects.
first, both models need training on fully-annotateddata so that they can produce reasonable intermedi-ate outputs.
second, the intermediate outputs can.
4112introduce noise during learning as they are sam-pled from models but not restricted to solutionswith correct answer or valid questions.
third, thismethod typically updates both models with rein-forcement learning while the rewards provided by adual model can be unstable or of high variance.
bycontrast, we focus on the spurious solution prob-lem under the weakly supervised setting and pro-pose a mutual information maximization approach.
solutions used for training are restricted to thosewith correct answer.
what’s more, though a task-speciﬁc model and a question reconstructor interactwith each other, they do not use the likelihood fromeach other as rewards, which can stabilize learning..3 method.
3.1 task deﬁnition.
for a qa task, each instance is a tuple (cid:104)d, q, a(cid:105),where q denotes a question, a is the answer, and d isreference information such as documents for read-ing comprehension, or table headers for semanticparsing.
a solution z is a task-speciﬁc derivationof the answer, e.g., a particular span in a document,an equation, or a sql query (as shown in fig 1).
let f (·) be the task-speciﬁc function that maps asolution to its execution result, e.g., by returning aparticular span, solving an equation, or executinga sql query.
our goal is to train a task-speciﬁcmodel pθ(z|d, q) that takes (cid:104)d, q(cid:105) as input and pre-dicts a solution z satisfying f (z) = a..under the weakly supervised setting, only theanswer a is provided for training while the ground-truth solution ¯z is not.
we denote the set of possiblesolutions as z = {z|f (z) = a}.
in cases wherethe search space of solution is large, we can usuallyapproximate z so that it contains the ground-truthsolution ¯z with a high probability (min et al., 2019;wang et al., 2019).
note that z is task-speciﬁc,which will be instantiated in section 4..during training, we pair the task-speciﬁc modelpθ(z|d, q) with a question reconstructor pφ(q|d, z)and maximize the mutual information between (cid:104)q,a(cid:105) and z. during test, given (cid:104)d, q(cid:105), we use the task-speciﬁc model to predict a solution and return theexecution result..3.2 learning method.
given an instance (cid:104)d, q, a(cid:105), the solution set z usu-ally contains only one solution that best ﬁts theinstance while the rest are spurious.
we propose toexploit the semantic correlations between a ques-.
figure 2: illustration of the learning method..tion and its solution to alleviate the spurious solu-tion problem via mutual information maximization.
our objective is to obtain the optimal task-speciﬁc model θ∗ that maximizes the followingconditional mutual information:.
θ∗ = arg max.
iθ((cid:104)q, a(cid:105); z|d).
= arg max.
h((cid:104)q, a(cid:105)|d) − hθ((cid:104)q, a(cid:105)|d, z).
= arg max.
−hθ((cid:104)q, a(cid:105)|d, z).
(1).
= arg max.
ep (d,q,a)epθ (z|d,q,a) log pθ(q, a|d, z).
θ.θ.θ.θ.where iθ((cid:104)q, a(cid:105); z|d) denotes conditional mu-information between (cid:104)q, a(cid:105) and z overtualp (d, q, a)pθ(z|d, q, a).
h(·|·) is conditional en-tropy of random variable(s).
p (d, q, a) is the prob-ability of an instance from the training distribution.
pθ(z|d, q, a) is the posterior prediction probabil-ity of z (∈ z) which is the prediction probabilitypθ(z|d, q) normalized over z:.
pθ(z|d, q, a) =.
pθ (z|d,q).
pθ (z(cid:48) |d,q).
(cid:48).
z.
∈z.
(cid:40).
(cid:80).
0.z ∈ z.z /∈ z.
(2).
note that computing pθ(q, a|d, z) is intractable.
we therefore introduce a question reconstruc-tor pφ(q|d, z) and approximate pθ(q, a|d, z) withi(f (z) = a)pφ(q|d, z) where i(·) denotes indica-tor function.
eq.
1 now becomes:.
θ∗ = arg max.
l1 + l2.
θ.l1 = ep (d,q,a)epθ (z|d,q,a) log pφ(q|d, z).
(3).
l2 = ep (d,q,a)epθ (z|d,q,a) log.
pθ(q, a|d, z)pφ(q|d, z).
to optimize eq.
3 is to repeat the following trainingcycle, which is analogous to the em algorithm:.
1. minimize l2 w.r.t.
the question reconstructorφ to draw pφ(q|d, z) close to pθ(q, a|d, z), bysampling a solution z(cid:48) ∈ z according to itsposterior prediction probability pθ(z|d, q, a)(see eq.
2) and maximizing log pφ(q|d, z(cid:48))..4113a case of discrete reasoning over paragraphsquestionq=“how many years afterthe battle of powder riverdid powervillemontana become the first establishment in the county?”answera=“2”paragraphd=“… on march 17, ①1876, the battle of powder river occurredin the south-central part of the county ... in june ②1876six companies of … on november 1, ③1878, powderville, montana became the first establishment in the county…”solution setz = {z1= ③1878 -①1876,z2= ③1878 -②1876}0.450.5500.20.40.6category 1category 2question reconstructor %sample     &!maximize log*"(,|.,&!
)-0.6-0.300.20.40.60.8category 1category 2task-specific model 1scoremaximize log*#(&!
!|.,,)scoretraining cycle1.2.log*"(,|.,&)*#(&|.,,,2)&$&%&$&%&!
!=245627&∈(*"(,|.,&)to just feed the concatenation of d and the surfaceform of z to the bart encoder; otherwise, differ-ent spans with the same surface form can no longerbe discriminated as their contextual semantics arelost.
to effectively encode d and z, we devise auniﬁed solution encoding as in fig 3 which is ap-plicable to solutions of various types.
speciﬁcally,we leave most of the surface form of z unchanged,except that we replace any span from referenceinformation with a placeholder (cid:104)span(cid:105).
the rep-resentation of (cid:104)span(cid:105) is computed by forcing it toonly attend to the contextual representation(s) ofthe referred span.
to obtain disentangled and ro-bust representations of reference information anda solution, we keep reference information and thesolution (except for the token (cid:104)span(cid:105)) from attend-ing to each other.
intuitively speaking, semanticsof reference information should not be affected bya solution, and the representations of a solutionshould largely determined by its internal logic..3.4 solution set.
while our learning method and question reconstruc-tor are task-agnostic, solutions are usually task-speciﬁc.
precomputing solution sets needs formaldeﬁnitions of solutions which deﬁne the searchspace of solutions.
a possible search method is toexhaustively enumerate all solutions that producethe correct answer.
we will introduce the deﬁni-tions of solutions for different tasks in section 4..4 experiments.
datasets.
# examples.
|z|.
train dev.
test avg median.
quasar-twebquestions 3,778.multi-mention reading comprehension8.137,012 3,000 3,00052.12,032-discrete reasoning over paragraphs69,669 7,740 9,535semantic parsing.
5.1.drop.
wikisql.
56,355 8,421 15,878 315.4.
436.
2.
4.table 1: statistics of the datasets we used.
statistics ofthe size of solution set |z| are computed on train sets..following min et al.
(2019), we conducted exper-iments on three qa tasks, namely multi-mentionreading comprehension, discrete reasoning overparagraphs, and semantic parsing.
this section in-troduces baselines, the deﬁnitions of solutions indifferent tasks, how the solution set can be precom-puted, and our experimental results.
statistics ofthe datasets we used are presented in table 1..figure 3: solution encoding.
(a) for bart encoderinputs, (cid:104)s(cid:105) and (cid:104)/s(cid:105) denote start and end of input se-(cid:104)sol(cid:105) denotes start of solution.
quence, respectively.
(cid:104)span(cid:105) is the placeholder of the referred span in ref-erence information (e.g., the second ab in this ﬁgure.
(b) for attention mask, gray circles block attention.
(cid:104)span(cid:105) retrieves the contextual representation(s) of thereferred span by only attending to the referred span.
ref-erence information and the solution (except for the to-ken (cid:104)span(cid:105)) are kept from attending to each other..2. maximize l1 w.r.t.
the task-speciﬁc modelθ. l1 can be seen as a reinforcement learn-ing objective with log pφ(q|d, z) being thereward function.
during training, the re-ward function is dynamically changing andmay be of high variance.
as we can com-pute the reward for all z ∈ z, we thereforeadopt a greedy but more stable update method,i.e., to maximize log pθ(z(cid:48)(cid:48)|d, q) where z(cid:48)(cid:48) =arg maxz∈z log pφ(q|d, z) is the best solu-tion according to the question reconstructor..we illustrate the above training cycle in fig 2..3.3 question reconstructor.
the question reconstructor pφ(q|d, z) takes refer-ence information d and a solution z as input, andreconstructs the question q. we use bartbase, apre-trained seq2seq model, as the question recon-structor so that semantic correlations between ques-tions and solutions can be better captured..a solution typically consists of task-speciﬁc op-eration token(s) (e.g., count for discrete reason-ing or semantic parsing), literal(s) (e.g., numericconstants for discrete reasoning or semantic pars-ing), or span(s) from a question or reference infor-mation (e.g., for most qa tasks).
it is problematic.
4114bart encoder<s>ab<sol>op1<span>op2</s>refers toreference infomationsolution(a) bart encoder inputs(b) bart encoder attention mask<s>abc<sol>op1<span>op2</s><s>a<sol>op1<span>op2</s>queriestargetscabdbcabdabdfor convenience, we denote reference informa-tion as d = [d1, d2, ..., d|d|] and denote a questionas q = [q1, q2, ..., q|q|] where di and qj are a tokenof d and q respectively.
a span from referenceinformation and a question span is represented as(s, e)d and (s, e)q respectively, where s and e arestart and end index of the span respectively..4.1 baselines.
z∈z pθ(z|d, q)..first only (joshi et al., 2017) which trains areading comprehension model by maximizinglog pθ(z|d, q) where z is the ﬁrst answer span in d.mml (min et al., 2019) which maximizeslog (cid:80)hardem (min et al., 2019) which maximizeslog maxz∈zpθ(z|d, q).
hardem-thres (chen et al., 2020): a variant ofhardem that optimizes only on conﬁdent solu-tions, i.e., to maximize maxz∈zi(pθ(z|d, q) >γ) log pθ(z|d, q) where γ is an exponentially de-caying threshold.
γ is initialized such that a modelis trained on no less than half of training data at theﬁrst epoch.
we halve γ after each epoch.
vae (cheng and lapata, 2018): a method thatviews a solution as the latent variable for questiongeneration and adopts the training objective of vari-ational auto-encoder (vae) (kingma and welling,2014) to regularize the task-speciﬁc model.
theoverall training objective is given by:.
θ∗, φ∗ = arg maxθ,φl(θ, φ) = lmle(θ) + λlvae(θ, φ).
l(θ, φ).
=.
(cid:88).
z∈b.
log pθ(z|d, q) + λepθ (z|d,q) log.
pφ(q|d, z)pθ(z|d, q).
where θ denotes a task-speciﬁc model and φ isour question reconstructor.
lmle(θ) is the totallog likelihood of the set of model-predicted so-lutions (denoted by b) which derive the correctanswer.
lvae(θ, φ) is the evidence lower bound ofthe log likelihood of questions.
λ is the coefﬁcientof lvae(θ, φ).
this method needs pre-training bothθ and φ before optimizing the overall objectivel(θ, φ).
notably, model θ optimizes on lvae(θ, φ)via reinforcement learning.
we tried stabilizingtraining by reducing the variance of rewards andsetting a small λ..4.2 multi-mention reading comprehension.
multi-mention reading comprehension is a naturalfeature of many qa tasks.
given a document dand a question q, a task-speciﬁc model is required.
to locate the answer text a which is usually men-tioned many times in the document(s).
a solutionis deﬁned as a document span.
the solution set zis computed by ﬁnding exact match of a:.
z = {z = (s, e)d|[ds, ..., de] = a}.
we experimented on two open domain qadatasets,i.e., quasar-t (dhingra et al., 2017)and webquestions (berant et al., 2013).
forquasar-t, we retrieved 50 reference sentences fromclueweb09 for each question; for webquestions,we used the 2016-12-21 dump of wikipedia asthe knowledge source and retrieved 50 referenceparagraphs for each question using a lucene indexsystem.
we used the same bertbase (devlin et al.,2019) reading comprehension model and data pre-processing from (min et al., 2019)..quasar-t.webquestionstest.
testem f1.
devem f1em36.0 43.9 35.6 42.8 16.740.1 47.4 39.1 46.5 18.441.5 49.1 40.7 47.7 18.0hardem-thres 42.8 50.2 41.9 49.4 19.044.7‡ 52.6‡ 44.0‡ 51.5‡ 20.4‡.
first onlymmlhardem.
ours.
f122.625.024.225.327.2‡.
table 2: evaluation on multi-mention reading compre-hension datasets.
numbers marked with ‡ are signiﬁ-cantly better than the others (t-test, p-value < 0.05)..results: our method outperforms all baselines onboth datasets (table 2).
the improvements can beattributed to the effectiveness of solution encod-ing, as solutions for this task are typically differentspans with the same surface form, e.g., in qusart-t,all z ∈ z share the same surface form..4.3 discrete reasoning over paragraphs.
some reading comprehension tasks pose the chal-lenge of comprehensive analysis of texts by requir-ing discrete reasoning (e.g., arithmetic calculation,sorting, and counting) (dua et al., 2019).
in thistask, given a paragraph d and a question q, an an-swer a can be one of the four types: numeric value,a paragraph span or a question span, a sequenceof paragraph spans, and a date from the paragraph.
the deﬁnitions of z depend on answer types (table4).
these solutions can be searched by followingchen et al.
(2020).
note that some solutions in-volve numbers in d. we treated those numbers asspans while reconstructing q from z..we experimented on drop (dua et al., 2019).
as the original test set is hidden, for convenience of.
4115overall test number (61.97%) span (31.47%) spans (4.99%) date (1.57%)emf1em58.99‡ 62.30‡ 55.38mmlhardem 68.52‡ 71.88‡ 68.40hardem-thres 69.06 72.35‡ 69.0532.34‡ 36.28‡ 51.6569.35 72.92 69.96.em39.29 66.0144.79 69.6339.50 66.380.008.8942.86 70.42.em f142.57 49.0549.32 56.8752.67 58.750.004.1148.67 57.47.f175.5179.2579.7910.0179.32.em69.9673.5074.610.3773.38.f155.5868.7069.3952.3570.27.vaeours.
f1.
table 3: evaluation on drop.
we used the public development set of drop as our test set.
we also provideperformance breakdown of different question types on our test set.
results on the overall test set marked with ‡are signiﬁcantly worse than the best one (t-test, p-value < 0.05)..arithmetic.
sorting.
counting.
numeric answers.
z =n1[, o1, n2[, o2, n3]],s.t.
o1, o2 ∈ {+, −},.
n1, n2, n3 ∈ nd ∪ s.z =o{nk}k≥1,s.t.
o ∈ {max, min}, nk ∈ ndz =|{(sk, ek)d}k≥1|.
non-numeric answers.
span(s).
z = {(sk, ek)t}k≥1, s.t.
t ∈ {d, q}.
sorting.
z =o{kv(cid:104)(sk, ek)d, nk(cid:105)}k≥1,s.t.
o ∈ {argmax, argmin}, nk ∈ nd.
table 4: deﬁnitions of solutions for numeric answersand non-numeric answers.
nd is the set of num-bers in d, and s is a set of pre-deﬁned numbers.
for arithmetic solutions for numeric answers, z =n1[, o1, n2[, o2, n3]] denotes equations with no morethan three operands.
for solutions of sorting type fornon-numeric answers, kv(cid:104)·, ·(cid:105) is a key-value pair wherethe key is a span in d and the value is its associated num-ber from d. argmax (argmin) returns the key with thelargest (smallest) value..analysis, we used the public development set as ourtest set, and split the public train set into 90%/10%for training and development.
we used neural sym-bolic reader (nerd) (chen et al., 2020) as the task-speciﬁc model.
nerd is a seq2seq model whichencodes a question and a paragraph, and decodes asolution (e.g., count (paragraph span(s1, e1), para-graph span(s2, e2)) where paragraph span(si, ei)means a paragraph span starting at si and endingat ei).
we used the precomputed solution sets pro-vided by chen et al.
(2020)1. data preprocessing.
1our implementation of nerd has four major differencesfrom that of (chen et al., 2020).
(1) instead of choosingbertlarge as encoder, we chose the discriminator of electrabase(clark et al., 2020) which is of a smaller size.
(2) we did notuse moving averages of trained parameters.
(3) we did notuse the full public train set for training but used 10% of it fordevelopment.
(4) for some questions, it is hard to guaranteethat a precomputed solution set covers the ground-truth solu-tion.
for example, the question how many touchdowns did.
was also kept the same.
results: as shown in table 3, our method signif-icantly outperforms all baselines in terms of f1score on our test set..we also compared our method with the base-line vae which uses a question reconstructor φto adjust the task-speciﬁc model θ via maximiz-ing a variational lower bound of log p (q|d) as theregularization term lvae(θ, φ).
to pre-train thetask-speciﬁc model for this method, we simply ob-tained the best task-speciﬁc model trained withhardem-thres.
vae optimizes the task-speciﬁcmodel on lvae(θ, φ) with reinforcement learningwhere pφ(q|d, z) is used as learning signals for thetask-speciﬁc model.
despite our efforts to stabi-lize training, the f1 score still dropped to 36.28after optimizing the overall objective l(θ, φ) for1,000 steps.
by contrast, our method does not usepφ(q|d, z) to compute learning signals for the task-speciﬁc model but rather uses it to select solutionsto train the task-speciﬁc model, which makes abetter use of the question reconstructor..4.4 semantic parsing.
text2sql is a popular semantic parsing task.
given a question q and a table header d =[h1, ..., hl] where hl is a multi-token column, aparser is required to parse q into a sql query zand return the execution results.
under the weaklysupervised setting, only the ﬁnal answer is providedwhile the sql query is not.
following min et al.
(2019), z is approximated as a set of non-nestedsql queries with no more than three conditions:.
z = {z = (zsel, zagg, {zcondzsel ∈ {h1, ..., hl}, zcondzagg ∈ {none, sum, mean, max, min, count}}.
}3k=1)|f (z) = a,.
∈ {none} ∪ c,.
k.k.brady throw?
needs counting, but the related mentions arenot known.
(chen et al., 2020) partly solved this problem byadding model-predicted solutions (with correct answer) intothe initial solution sets as learning proceeds.
in this paper, wekept the initial solution sets unchanged during training, so thatdifferent qa tasks share the same experimental setting..4116where zagg is an aggregating operator and zsel isthe operated column (a span of d).
c = {(h, o, v)}is the set of all possible conditions, where h is acolumn, o ∈ {=, <, >}, and v is a question span.
we experimented on wikisql (zhong et al.,2017) under the weakly supervised setting2.
wechose sqlova (hwang et al., 2019) as the task-speciﬁc model which is a competitive text2sqlparser on wikisql.
hyperparameters were keptthe same as in (hwang et al., 2019).
we used thesolution sets provided by min et al.
(2019).
results: all models in table 5 do not applyexecution-guided decoding during inference.
ourmethod achieves new state-of-the-art results underthe weakly supervised setting.
though without su-pervision of ground-truth solutions, our executionaccuracy (i.e., accuracy of execution results) onthe test set is close to that of the fully supervisedsqlova.
notably, grappa focused on represen-tation learning and used a stronger task-speciﬁcmodel while we focus on the learning method andoutperform grappa with a weaker model..5 ablation study.
5.1 performance on test data with different.
size of solution set.
fig 4 shows the performance on test data with dif-ferent size of solution set3.
our method consis-tently outperforms hardem-thres and by a largemargin when test examples have a large solutionset..5.2 effect of |z| at training.
the more complex a question is, the larger the setof possible solutions tends to be, the more likely amodel will suffer from the spurious solution prob-lem.
we therefore investigated whether our learn-ing method can deal with extremely noisy solutionsets.
speciﬁcally, we extracted a hard train set fromthe original train set of wikisql.
the hard trainset consists of 10k training data with the largestz. the average size of z on the hard train set is1,554.6, much larger than that of the original trainset (315.4).
we then compared models trained onthe original train set and the hard train set usingdifferent learning methods..2wikisql has annotated ground-truth sql queries.
we.
only used them for evaluation but not for training..3in this experiment, |z| is only seen as a property of anexample.
evaluated solutions are predicted by the task-speciﬁcmodel but not from z..model.
execution accuracydev.
test.
fully-supervised setting.
sqlova (hwang et al., 2019) 87.289.1hydranet (lyu et al., 2020).
weakly-supervised setting.
merl (agarwal et al., 2019) 74.985.9grappa (yu et al., 2021)70.6mml(min et al., 2019)84.5‡hardem85.2†hardem-thres85.9ours.
86.289.2.
74.884.770.584.1‡84.1‡85.6.table 5: evaluation on wikisql.
accuracy that is sig-niﬁcantly lower than the highest one is marked with †for p-value < 0.1, and ‡ for p-value < 0.05 (t-test)..figure 4: performance on test examples with differentsize of z on drop..figure 5: logical form accuracy (left) and executionaccuracy (right) on dev set and test set of wikisql.
amethod marked with ori.
train or hard train meansthe evaluated model is trained on the original train setor a hard subset of training data, respectively.
the hardtrain set consists of 10k training data with the largestsolution set; the average size of solution set is 1,554.6..as shown in fig 5, models trained with ourmethod consistently outperform baselines in termsof logical form accuracy (i.e., accuracy of predictedsolutions) and execution accuracy.
when using thehard train set, the logical form accuracy of modelstrained with hardem or hardem-thres drop to be-low 14%.
compared with hardem, hardem-thresis better when trained on the original train set but isworse when trained on the hard train set.
these in-dicate that model conﬁdence can be unreliable andthus insufﬁcient to ﬁlter out spurious solutions.
bycontrast, our method explicitly exploits the seman-tic correlations between a question and a solution,thus much more resistant to spurious solutions..4117020406080100626568717477[0,3)[3,5)[5,7)[7,9)[9,+∞)% of dataf1 score|z|% of datahardem-thresours02550751006568717477[0,3)[3,5)[5,7)[7,9)[9,+∞)% of dataf1 score|z|% of datahardem-thresourstraining epochs.
102bartbase w/ hardem 65.1 60.8 59.7 58.6 61.0sqlova w/ hardem 61.3 62.2 61.8 61.8 61.779.7 82.8 79.8 81.2 87.4.sqlova w/ ours.
4.
8.
6.table 6: accuracy on the sql selection task.
thehard train set was used for training.
bartbase w/ har-dem and sqlova w/ hardem are a bartbase parserand sqlova, respectively; both were trained with har-dem.
sqlova w/ ours is sqlova trained with the pro-posed mutual information maximization approach (us-ing bartbase question reconstructor)..5.3 effect of the question reconstructor.
as we used bartbase as the question resconstruc-tor, we investigated how our question reconstructorcontributes to performance improvements..we ﬁrst investigated whether bartbase itself isless affected by the spurious solution problem thanthe task-speciﬁc models.
speciﬁcally, we viewedtext2sql as a sequence generation task and ﬁne-tuned a bartbase on the hard train set of wikisqlwith hardem.
the input of bart shares the sameformat as that of sqlova, which is the concate-nation of a question and a table header.
the out-put of bart is a sql query.
without constraintson decoding, bart might not produce valid sqlqueries.
we therefore evaluated models on a sqlselection task instead: for each question in the de-velopment set of wikisql, a model picks out thecorrect sql from at most 10 candidates by select-ing the one with the highest prediction probability.
as shown in table 6, when trained with hardem,both bartbase parser and sqlova perform sim-ilarly, and underperform our method by a largemargin.
this indicates that using bartbase as atask-speciﬁc model can not avoid the spurious so-lution problem.
it is our mutual information maxi-mization objective that makes a difference..drop.
dev.
test.
wikisql (hard train set)testdev.
em f1 em f1 lf.
acc exe.
acc lf.
acc exe.
acc.
t-scratch 61.5 66.3 69.0 72.4t-dae 61.5 66.3 69.4 72.7bartbase 61.5 66.4 69.3 72.9.
24.749.445.8.
67.968.969.1.
24.948.545.6.
67.568.468.4.table 7: results with different question reconstructors.
lf.
acc and exe.
acc are logical form accuracy andexecution accuracy, respectively.
t-scratch is a trans-former without pre-training.
t-dae is a transformerpre-trained as a denoising auto-encoder of questions..we further investigated the effect of the choiceof question reconstructor.
we compared bartbasewith two alternatives: (1) t-scratch: a three-layertransformer (vaswani et al., 2017) without pre-.
training and (2) t-dae: a three-layer transformerpre-trained as a denoising auto-encoder of ques-tions on the train set; the text inﬁlling pre-trainingtask for bart was used.
as shown in table 7, ourmethod with either of the three question reconstruc-tors outperforms or is at least competitive with base-lines, which veriﬁes the effectiveness of our mu-tual information maximization objective.
what’smore, using t-dae is competitive with bartbase,indicating that our training objective is compatiblewith other choices of question reconstructor besidesbart, and that using a denoising auto-encoder toinitialize the question reconstructor may be beneﬁ-cial to exploit the semantic correlations between aquestion and its solution..6 evaluation of solution prediction.
as solutions with correct answer can be spurious,we further analyzed the quality of predicted solu-tions.
we randomly sampled 50 test examples fromdrop for which our method produced the correctanswer, and found that our method also producedthe correct solution for 92% of them..to investigate the effect of different learningmethods on models’ ability to produce correct so-lutions, we manually analyzed another 50 test sam-ples for which hardem, hardem-thres, and ourmethod produced the correct answer with differentsolutions.
the percentage of samples for which ourmethod produced the correct solution is 58%, muchhigher than that of hardem (10%) and hardem-thres (30%).
for experimental details, please re-fer to the appendix..7 case study.
fig 6 compares nerd predictions on four types ofquestions from drop when using different learn-ing methods.
an observation is that nerd usingour method shows more comprehensive understand-ing of questions, e.g., in the arithmetic case, nerdusing our method is aware of the two key elementsin the question including the year when mission-aries arrived in ayutthaya and the year when theseminary of saint joseph was built, while nerd us-ing hardem-thres misses the ﬁrst element.
what’smore, nerd using our method is more precise in lo-cating relevant information, e.g., in the ﬁrst sortingcase, nerd with our method locates the second ap-pearance of 2 whose contextual semantics matchesthe question, while nerd using hardem-thres lo-cates the ﬁrst appearance of 2 which is irrelevant..4118figure 6: nerd predictions on four types of questions from drop when using different learning methods.
spansin dark gray and green denote semantic correlations between a question and its solution, while spans in orange arespurious information and should not be used in a solution..these two observations can be attributed to ourmutual information maximization objective whichbiases a task-speciﬁc model towards those solutionsthat align well with the questions..however, we also observed that when there aremultiple mentions of relevant information of thesame type, nerd trained with hardem-thres or ourmethod has difﬁculty in recalling them all, e.g., inthe second sorting case, the correct solution shouldlocate all four mentions of sebastian janikowski’sﬁeld goals while nerd using either method lo-cates only two of them.
we conjecture that thisis because the solution sets provided by chen et al.
(2020) are noisy.
for example, all precomputedsolutions of sorting type for numeric answers in-volve up to two numbers from reference informa-tion, which makes it hard for a model to learn tosort more than two numbers..8 conclusion.
to alleviate the spurious solution problem inweakly supervised qa, we propose to explicitly.
exploit the semantic correlations between a ques-tion and its solution via mutual information maxi-mization.
during training, we pair a task-speciﬁcmodel with a question reconstructor which guidesthe task-speciﬁc model to predict solutions that areconsistent with the questions.
experiments on fourqa datasets demonstrate the effectiveness of ourlearning method.
as shown by automatic and man-ual analyses, models trained with our method aremore resistant to spurious solutions during training,and are more precise in locating information that isrelevant to the questions during inference, leadingto higher accuracy of both answers and solutions..9 acknowledgements.
this work was partly supported by the nsfcprojects (key project with no.
61936010 and reg-ular project with no.
61876096).
this work wasalso supported by the guoqiang institute of ts-inghua university, with grant no.
2019gqg1 and2020gqg0005..4119span(s)question:which team attempted a 2-point conversion?answer:ramsparagraph: hoping to rebound from their road loss to the patriots, the ①ramswent home for a week 9 nfc west duel with the arizona cardinals … in the second quarter, the cardinals responded with a vengeance as safety antrel rolle returned an interception 40 yards for a touchdown, kicker neil rackers got a 36-yard field goal, rb tim hightower got a 30-yard td run, and former ②ramsqb kurt warner completed a 56-yard td pass to wr jerhemeurban.
in the third quarter, arizona increased its lead as warner completed a 7-yard td pass to wr anquan boldin.
in the fourth quarter, the ③ramstried to come back as bulger completed a 3-yard td pass to wr torryholt (with a failed 2-point conversion).
however, the cardinals flew away as rackers nailed a 30-yard field goal.
during the game, the ④ramsinducted former head coach dick vermeil (who helped the franchise win super bowl xxxiv) onto the ⑤ramsring of honor.model prediction:ours:③rams✓hardem-thres:⑤rams✗arithmeticquestion:how many years after the missionaries arrived in ayutthayadid the build the seminary of saint joseph?answer:2 or 1paragraph:in 1664, a group of missionariesled by franoispallu, bishop of heliopolis, also of the paris foreign missions society, joined lambert in the capital city of ayutthayaafter 24 months overland travel and started missionary work.
in 1665-66they builta seminary in ayutthaya with the approval of king narai, the seminary of saint joseph.
in 1669, louis laneau, bishop of motella, also a member of the paris foreign missions society, …model prediction:ours:1666 -1664✓hardem-thres:1666 -1665✗sortingquestion:how many yards was the shortesttouchdown pass?answer:2paragraph:the giants played their week ①2home opener against the green bay packers … the giants responded with a 26-yard scoring strike by eli manning to plaxico burress.
the giants got a lawrence tynesfield goal and a 10-7 half time lead.
in the second half, the packers drove 51 yards to start the second half.
favre capped offthe scoring drive with a ②2-yard passto bubba franks for a 14-10 lead the packers would not relinquish… model prediction:ours:min{②2}✓hardem-thres:①2✗----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------question:how many yards was sebastian janikowski'slongestfield goal?answer:49paragraph:… the seahawks immediately trailed on a scoring rally by the raiders with kicker sebastian janikowskinailing a 31-yard field goal.
this was followed in the second quarter by qb jason campbell's 30-yard td pass to fb marcel reece.
then in the third quarter janikowskimade a 36-yard field goal.
then hemade a 22-yard field goalin the fourth quarter to put the raiders up 16-0 ... with kicker olindomare hitting a 47-yard field goal.
however, they continued to trail as janikowskimade a 49-yard field goal…model prediction:ours:max{49, 36}incompletehardem-thres:max{49, 31}incompletecountingquestion:how many passeddid houshmandzadehcatch?answer:2paragraph: … in the third quarter, cincinnati tried to rally as qb carson palmer completed an 18-yard td passto wr t. j. houshmandzadeh...cincinnati tried to come back as palmer completed a 10-yard td passto houshmandzadeh(with a failed 2-point conversion), but dallas pulled away with romo completing a 15-yard td pass to wr patrick crayton.model prediction:ours:|{18-yard td pass, 10-yard}|✓hardem-thres:2✗references.
rishabh agarwal, chen liang, dale schuurmans, andmohammad norouzi.
2019. learning to generalizein pro-from sparse and underspeciﬁed rewards.
ceedings of the 36th international conference onmachine learning, icml 2019, 9-15 june 2019,long beach, california, usa, volume 97 of pro-ceedings of machine learning research, pages 130–140. pmlr..jonathan berant, andrew chou, roy frostig, andpercy liang.
2013. semantic parsing on freebasefrom question-answer pairs.
in proceedings of the2013 conference on empirical methods in naturallanguage processing, emnlp 2013, 18-21 octo-ber 2013, grand hyatt seattle, seattle, washington,usa, a meeting of sigdat, a special interest groupof the acl, pages 1533–1544.
acl..ruisheng cao, su zhu, chen liu, jieyu li, and kai yu.
2019. semantic parsing with dual learning.
in pro-ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 51–64.
association for computationallinguistics..xinyun chen, chen liang, adams wei yu, dennyzhou, dawn song, and quoc v. le.
2020. neu-ral symbolic reader: scalable integration of dis-tributed and symbolic representations for readingcomprehension.
in 8th international conference onlearning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..jianpeng cheng and mirella lapata.
2018. weakly-supervised neural semantic parsing with a generativeranker.
in proceedings of the 22nd conference oncomputational natural language learning, conll2018, brussels, belgium, october 31 - november1, 2018, pages 356–367.
association for computa-tional linguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics, acl2018, melbourne, australia, july 15-20, 2018, vol-ume 1: long papers, pages 845–855.
associationfor computational linguistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human language.
technologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..bhuwan dhingra, kathryn mazaitis, and william w.cohen.
2017. quasar: datasets for question answer-ing by search and reading.
corr, abs/1707.03904..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019,volume 1 (long and short papers), pages 2368–2378. association for computational linguistics..wonseok hwang, jinyeung yim, seunghyun park, andminjoon seo.
2019. a comprehensive explorationon wikisql with table-aware word contextualization.
corr, abs/1902.01069..mandar joshi, eunsol choi, daniel s. weld, and lukezettlemoyer.
2017. triviaqa: a large scale distantlysupervised challenge dataset for reading comprehen-sion.
in proceedings of the 55th annual meeting ofthe association for computational linguistics, acl2017, vancouver, canada, july 30 - august 4, vol-ume 1: long papers, pages 1601–1611.
associationfor computational linguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..diederik p. kingma and max welling.
2014. auto-in 2nd internationalencoding variational bayes.
conference on learning representations,iclr2014, banff, ab, canada, april 14-16, 2014, con-ference track proceedings..tom´as kocisk´y, jonathan schwarz, phil blunsom,chris dyer, karl moritz hermann, g´abor melis, andedward grefenstette.
2018. the narrativeqa read-ing comprehension challenge.
trans.
assoc.
com-put.
linguistics, 6:317–328..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th conference of the association for computa-tional linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages6086–6096.
association for computational linguis-tics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,.
4120and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 7871–7880.
association for computationallinguistics..the 57th conference of the association for compu-tational linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages4911–4921.
association for computational linguis-tics..chen liang, jonathan berant, quoc v. le, kenneth d.forbus, and ni lao.
2017. neural symbolic ma-chines: learning semantic parsers on freebase within proceedings of the 55th an-weak supervision.
nual meeting of the association for computationallinguistics, acl 2017, vancouver, canada, july 30 -august 4, volume 1: long papers, pages 23–33.
as-sociation for computational linguistics..chen liang, mohammad norouzi, jonathan berant,quoc v. le, and ni lao.
2018. memory augmentedpolicy optimization for program synthesis and se-mantic parsing.
in advances in neural informationprocessing systems 31: annual conference on neu-ral information processing systems 2018, neurips2018, december 3-8, 2018, montr´eal, canada,pages 10015–10027..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..qin lyu, kaushik chakrabarti, shobhit hathi, sou-vik kundu, jianwen zhang, and zheng chen.
2020.corr,hybrid ranking network for text-to-sql.
abs/2008.04759..sewon min, danqi chen, hannaneh hajishirzi, andluke zettlemoyer.
2019. a discrete hard em ap-proach for weakly supervised question answering.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 2851–2864. association for computational linguistics..panupong pasupat and percy liang.
2015. composi-tional semantic parsing on semi-structured tables.
inproceedings of the 53rd annual meeting of the asso-ciation for computational linguistics and the 7th in-ternational joint conference on natural languageprocessing of the asian federation of natural lan-guage processing, acl 2015, july 26-31, 2015, bei-jing, china, volume 1: long papers, pages 1470–1480. the association for computer linguistics..swabha swayamdipta, ankur p. parikh, and tomkwiatkowski.
2018. multi-mention learning forreading comprehension with neural cascades.
in 6thinternational conference on learning representa-tions, iclr 2018, vancouver, bc, canada, april 30- may 3, 2018, conference track proceedings.
open-review.net..duyu tang, nan duan, tao qin, and ming zhou.
2017.question answering and question generation as dualtasks.
corr, abs/1706.02027..yi tay, anh tuan luu, siu cheung hui, and jiansu.
2018. densely connected attention propaga-in advances intion for reading comprehension.
neural information processing systems 31: annualconference on neural information processing sys-tems 2018, neurips 2018, december 3-8, 2018,montr´eal, canada, pages 4911–4922..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..bailin wang, ivan titov, and mirella lapata.
2019.learning semantic parsers from denotations withlatent structured alignments and abstract programs.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 3772–3783. association for computational linguistics..zhilin yang, peng qi, saizheng zhang, yoshua ben-gio, william w. cohen, ruslan salakhutdinov, andchristopher d. manning.
2018. hotpotqa: a datasetfor diverse, explainable multi-hop question answer-in proceedings of the 2018 conference oning.
empirical methods in natural language process-ing, brussels, belgium, october 31 - november 4,2018, pages 2369–2380.
association for computa-tional linguistics..hai ye, wenjie li, and lu wang.
2019. jointly learn-ing semantic parser and natural language generatorvia dual information maximization.
in proceedingsof the 57th conference of the association for compu-tational linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages2090–2101.
association for computational linguis-tics..tao yu, chien-sheng wu, xi victoria lin, bailin wang,yi chern tan, xinyi yang, dragomir radev, richardgra{pp}a:socher, and caiming xiong.
2021.grammar-augmented pre-training for table semanticin international conference on learningparsing.
representations..alon talmor and jonathan berant.
2019. multiqa: anempirical investigation of generalization and trans-in proceedings offer in reading comprehension..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma,irene li,qingning yao, shanelle roman, zilin zhang, and.
4121dragomir r. radev.
2018.spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, brussels,belgium, october 31 - november 4, 2018, pages3911–3921.
association for computational linguis-tics..victor zhong, caiming xiong, and richard socher.
seq2sql: generating structured queries2017.from natural language using reinforcement learning.
corr, abs/1709.00103..a implementation details.
a.1 learning methods.
hardem: we followed min et al.
(2019) to applyannealing to hardem on reading comprehensiontasks: at the training step t, a model optimizesmml objective with a probability of min(t/τ, 0.8)and optimizes hardem objective otherwise.
τ waschosen from {10k, 20k, 30k, 40k, 50k} basedon model performance on the development set.
hardem-thres: we set the conﬁdence thresholdas γ = 0.5n where n was initialized as follows: weﬁrst computed the prediction probability of eachsolution with a task-speciﬁc model, and then set nto a value such that the model was trained on noless than half of training data at the ﬁrst epoch.
wehalved γ after each epoch.
vae(cheng and lapata, 2018): a method thatviews a solution as the latent variable for ques-tion generation and adopts the training objective ofvariational auto-encoder (vae) to regularize thetask-speciﬁc model.
the overall training objectiveis given by:.
θ∗, φ∗ = arg maxθ,φl(θ, φ) = lmle(θ) + λlvae(θ, φ).
l(θ, φ).
=.
(cid:88).
z∈b.
log pθ(z|d, q) + λepθ (z|d,q) log.
pφ(q|d, z)pθ(z|d, q).
where lmle(θ) is the total log likelihood of the setof model-predicted solutions (denoted by b) withcorrect answer.
lvae(θ, φ) is the evidence lowerbound of the log likelihood of questions.
λ is thecoefﬁcient of lvae(θ, φ).
the optimization pro-cess is divided into three stages: (1) the 1st stagepre-trains a task-speciﬁc model θ with hardem-thres on solution sets4; (2) the 2nd stage pairs thetask-speciﬁc model with our question reconstruc-tor φ to optimize l(θ, φ) for one epoch, exceptthat lvae(θ, φ) is used to pre-train φ and is keptfrom back-propagating to θ; (3) the 3rd stage opti-mizes l(θ, φ) while allowing lvae(θ, φ) to back-propagate to θ. the gradient of lvae(θ, φ) w.r.t.
θis given by:.
(cid:53)θlvae(θ, φ) = epθ (z|d,q)r (cid:53)θ log pθ(z|d, q).
r = log.
pφ(q|d, z)pθ(z|d, q).
where r is the reward function.
to stabilize train-ing, we use the average reward of 5 sampled so-.
4cheng and lapata (2018) pre-trained the task-speciﬁcmodel θ by maximizing lmle(θ).
we enhanced their methodby pre-training θ with hardem-thres..4122lutions as a baseline b and re-deﬁne the rewardfunction as r(cid:48) = r − b. λ is set to 0.1..in section 4.3, we report performance of the bestmodel in the 3rd stage.
at the 2nd stage, as thetask-speciﬁc model optimized on both correct solu-tions and spurious solutions equally, the f1 scoredropped from 72.35 to 67.93 at the end of this stage,indicating that correct training solutions is vitalfor generalization.
at the 3rd stage, model learn-ing was further regularized with lvae(θ, φ) whichwas optimized via reinforcement learning.
despiteour efforts to stabilize training, the f1 score stilldropped to 36.28 after training for 1,000 steps atthe 3rd stage..a.2 experimental settings.
for all experiments, we used previously proposedtask-speciﬁc models and optimized them with theiroriginal optimizer.
we chose the best task-speciﬁcmodel according to its performance on the devel-opment set.
as for our learning method, we usedbartbase as the question reconstructor.
adamwoptimizer (loshchilov and hutter, 2019) was usedto update the question reconstructor with learningrate set to 5e-5..a.2.1 multi-mention readingcomprehension.
we adopted the reading comprehension model, datapreprocessing, and training conﬁgurations frommin et al.
(2019).
task-speciﬁc model: the model is based on un-cased version of bertbase, which takes as input theconcatenation of a question and a paragraph, andoutputs the probability distribution of the start andend position of the answer span.
to deal with multi-paragraph reading comprehension, it also trains aparagraph selector; during inference, it outputs aspan from the paragraph ranked 1st.
data preprocessing: documents are split to seg-ments up to 300 tokens.
for quasar-t, as re-trieved sentences are short, we concatenated allsentences into one document in decreasing orderof retrieval score (i.e., relevance with the question);for webquestions, we concatenated 5 retrievedparagraphs into one document, resulting in 10 ref-erence documents per question.
training: batch size is 20. bertadam optimizerwas used to update the reading comprehensionmodel with learning rate set to 5e-5.
the numberof training epochs is 10..a.2.2 discrete reasoning over paragraphs.
we used nerd (chen et al., 2020) for discrete rea-soning.
the major differences with its originalimplementation have been discussed in section 4.3.task-speciﬁc model: chen et al.
(2020) have de-signed a domain-speciﬁc language for discrete rea-soning on drop.
the deﬁnitions of solutions fordiscrete reasoning introduced in section 4.3 arealso expressed in this language except that we usedifferent symbols (e.g., the minus sign “-” in ourdeﬁnitions has the same meaning as the symbol“diff” in their paper).
nerd is a seq2seq modelwhich tasks as input the concatenation of a ques-tion and a paragraph, and generates the solution asa sequence.
the answer is obtained by executingthe solution.
data preprocessing: the input of the task-speciﬁcmodel is truncated to up to 512 words.
we used thesolution sets provided by chen et al.
(2020), whichcover 93.2% of examples in the train set.
training: batch size is 32. adam optimizer(kingma and ba, 2015) was used to update nerdwith learning rate set to 5e-5.
the number of train-ing epochs is 20..a.2.3 semantic parsing.
following min et al.
(2019), we used sqlova(hwang et al., 2019) on wikisql.
task-speciﬁc model: sqlova encodes the con-catenation of a question and a table header withuncased bertbase, and outputs a sql query viaslot ﬁlling with an nl2sql (natural language tosql) layer.
data preprocessing: data preprocessing was keptthe same as in (min et al., 2019).
we also used thesolution sets provided by min et al.
(2019) whichcover 98.8% of examples in the train set.
training: following min et al.
(2019), we set thebatch size to 10. following hwang et al.
(2019),adam optimizer was used to update sqlova withlearning rate of bertbase and nl2sql layer set to1e-5 and 1e-3, respectively.
the number of trainingepochs is 15 and 20 when using the original trainset and the hard train set of wikisql, respectively..a.3 computing infrastructure.
we conducted experiments on 24gb quadro rtx6000 gpus.
most experiments used 1 gpu exceptthat experiments on drop used 4 gpus in parallel..4123b details of ablation study.
b.1 sql selection task.
we deﬁned a sql selection task on the develop-ment set of wikisql.
speciﬁcally, for each ques-tion, we randomly sampled min(10, |z|) solutioncandidates from the solution set z without replace-ment while ensuring the ground-truth solution wasone of the candidates.
a model was required topick out the ground-truth solution by selecting thecandidate with the highest prediction probability.
in section 5.3, we only show model accuracy inthe ﬁrst 10 training epochs because for bartbasew/ hardem, sqlova w/ hardem, and sqlova w/ours, model conﬁdence (computed as the averagelog likelihood of selected sqls) showed a down-ward trend after the 2nd, 4th, and ≥ 10th epoch,respectively..b.2 choice of question reconstructor.
we investigated how the choice of the question re-constructor affects results.
one alternative choiceis a transformer pre-trained as a denoising auto-encoder of questions on the train set.
this questionreconstructor is the same as bartbase except thatthe number of encoder layers and the number of de-coder layers are 3 respectively.
we pre-trained thequestion reconstructor for one epoch to reconstructoriginal questions from corrupted ones.
for 50%of the time, the input question is the original ques-tion; otherwise, we followed lewis et al.
(2020) tocorrupt the original question by randomly maskinga number of text spans with span lengths drawnfrom a poisson distribution (λ = 3).
batch size is 4.adamw optimizer was used with learning rate setto 5e-5..4124