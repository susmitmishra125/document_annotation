directed acyclic graph network for conversational emotion recognition.
weizhou shen, siyue wu, yunyi yang, xiaojun quan∗school of computer science and engineering, sun yat-sen university, china{shenwzh3, wusy39, yangyy37}@mail2.sysu.edu.cnquanxj3@mail.sysu.edu.cn.
abstract.
the modeling of conversational context playsa vital role in emotion recognition from con-versation (erc).
in this paper, we put for-ward a novel idea of encoding the utteranceswith a directed acyclic graph (dag) to bettermodel the intrinsic structure within a conversa-tion, and design a directed acyclic neural net-work, namely dag-erc1, to implement thisidea.
in an attempt to combine the strengths ofconventional graph-based neural models andrecurrence-based neural models, dag-ercprovides a more intuitive way to model the in-formation ﬂow between long-distance conver-sation background and nearby context.
exten-sive experiments are conducted on four ercbenchmarks with state-of-the-art models em-ployed as baselines for comparison.
the empir-ical results demonstrate the superiority of thisnew model and conﬁrm the motivation of thedirected acyclic graph architecture for erc..1.introduction.
utterance-level emotion recognition in conversa-tion (erc) is an emerging task that aims to identifythe emotion of each utterance in a conversation.
this task has been recently concerned by a con-siderable number of nlp researchers due to itspotential applications in several areas, such as opin-ion mining in social media (chatterjee et al., 2019)and building an emotional and empathetic dialogsystem (majumder et al., 2020)..the emotion of a query utterance is likely to beinﬂuenced by many factors such as the utterancesspoken by the same speaker and the surroundingconversation context.
indeed, how to model theconversational context lies at the heart of this task(poria et al., 2019a).
empirical evidence also shows.
∗ corresponding author.
1the code is available at https://github.com/.
shenwzh3/dag-erc.
figure 1: conversation as a directed acyclic graph,with brown directed edges representing the informationpropagation between speakers and blue ones represent-ing the information propagation inside a same speaker..that a good representation of conversation contextsigniﬁcantly contributes to the model performance,especially when the content of query utterance istoo short to be identiﬁed alone (ghosal et al., 2019)..numerous efforts have been devoted to the mod-eling of conversation context.
basically, they canbe divided into two categories: graph-based meth-ods (zhang et al., 2019a; ghosal et al., 2019; zhonget al., 2019; ishiwatari et al., 2020; shen et al.,2020) and recurrence-based methods (hazarikaet al., 2018a; hazarika et al., 2018b; majumderet al., 2019; ghosal et al., 2020).
for the graph-based methods, they concurrently gather informa-tion of the surrounding utterances within a certainwindow, while neglecting the distant utterancesand the sequential information.
for the recurrence-based methods, they consider the distant utterancesand sequential information by encoding the utter-ances temporally.
however, they tend to update thequery utterance’s state with only relatively limitedinformation from the nearest utterances, makingthem difﬁcult to get a satisfying performance..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1551–1560august1–6,2021.©2021associationforcomputationallinguistics1551according to the above analysis, an intuitivelybetter way to solve erc is to allow the advantagesof graph-based methods and recurrence-based mod-els to complement each other.
this can be achievedby regarding each conversation as a directed acyclicgraph (dag).
as illustrated in figure 1, each ut-terance in a conversation only receives informationfrom some previous utterances and cannot propa-gate information backward to itself and its prede-cessors through any path.
this characteristic indi-cates that a conversation can be regarded as a dag.
moreover, by the information ﬂow from predeces-sors to successors through edges, dag can gatherinformation for a query utterance from both theneighboring utterances and the remote utterances,which acts like a combination of graph structureand recurrence structure.
thus, we speculate thatdag is a more appropriate and reasonable waythan graph-based structure and recurrence-basedstructure to model the conversation context in erc..in this paper, we propose a method to model theconversation context in the form of dag.
firstly,rather than simply connecting each utterance with aﬁxed number of its surrounding utterances to builda graph, we propose a new way to build a dagfrom the conversation with constraints on speakeridentity and positional relations.
secondly, inspiredby dagnn (thost and chen, 2021), we proposea directed acyclic graph neural network for erc,namely dag-erc.
unlike the traditional graphneural networks such as gcn (kipf and welling,2016) and gat (veliˇckovi´c et al., 2017) that ag-gregate information from the previous layer, dag-erc can recurrently gather information of prede-cessors for every utterance in a single layer, whichenables the model to encode the remote contextwithout having to stack too many layers.
besides,in order to be more applicable to the erc task, ourdag-erc has two improvements over dagnn:(1) a relation-aware feature transformation to gatherinformation based on speaker identity and (2) a con-textual information unit to enhance the informationof historical context.
we conduct extensive exper-iments on four erc benchmarks and the resultsshow that the proposed dag-erc achieves compa-rable performance with the state-of-the-art models.
furthermore, several studies are conducted to ex-plore the effect of the proposed dag structure andthe modules of dag-erc..the contributions of this paper are threefold.
first, we are the ﬁrst to consider a conversation.
as a directed acyclic graph in the erc task.
sec-ond, we propose a method to build a dag from aconversation with constraints based on the speakeridentity and positional relations.
third, we proposea directed acyclic graph neural network for erc,which takes dagnn as its backbone and has twomain improvements designed speciﬁcally for erc..2 related work.
2.1 emotion recognition in conversation.
recently, several erc datasets with textual datahave been released (busso et al., 2008; schulleret al., 2012; zahiri and choi, 2017; li et al., 2017;chen et al., 2018; poria et al., 2019b), arousingthe widespread interest of nlp researchers.
in thefollowing paragraphs, we divide the related worksinto two categories according to the methods theyuse to model the conversation context.
graph-based models dialoggcn (ghosal et al.,2019) treats each dialog as a graph in which eachutterance is connected with the surrounding utter-ances.
rgat (ishiwatari et al., 2020) adds posi-tional encodings to dialoggcn.
congcn (zhanget al., 2019a) regards both speakers and utterancesas graph nodes and makes the whole erc dataseta single graph.
ket (zhong et al., 2019) uses hier-archical transformers (vaswani et al., 2017) withexternal knowledge.
dialogxl (shen et al., 2020)improves xlnet (yang et al., 2019) with enhancedmemory and dialog-aware self-attention.2recurrence-based models in this category, icon(hazarika et al., 2018a) and cmn (hazarika et al.,2018b) both utilize gated recurrent unit (gru) andmemory networks.
higru (jiao et al., 2019) con-tains two grus, one for utterance encoder andthe other for conversation encoder.
dialogrnn(majumder et al., 2019) is a recurrence-basedmethod that models dialog dynamics with severalrnns.
cosmic (ghosal et al., 2020) is the latestmodel, which adopts a network structure very closeto dialogrnn and adds external commonsenseknowledge to improve performance..2.2 directed acyclic graph neural network.
directed acyclic graph is a special type of graphstructure that can be seen in multiple areas, forexample, the parsing results of source code (alla-manis et al., 2018) and logical formulas (crouse.
2we regard ket and dialogxl as graph-based modelsbecause they both adopt transformer in which self-attentioncan be viewed as a fully-connected graph in some sense..1552et al., 2019).
a number of neural networks that em-ploy dag architecture have been proposed, suchas tree-lstm (tai et al., 2015), dag-rnn(shuaiet al., 2016), d-vae (zhang et al., 2019b), anddagnn (thost and chen, 2021).
dagnn is dif-ferent from the previous dag models in the modelstructure.
speciﬁcally, dagnn allows multiplelayers to be stacked, while the others have onlyone single layer.
besides, instead of merely carry-ing out naive sum or element-wise product on thepredecessors’ representations, dagnn conductsinformation aggregation using graph attention..3 methodology.
3.1 problem deﬁnition.
in erc, a conversation is deﬁned as a sequence ofutterances {u1, u2, ..., un }, where n is the num-ber of utterances.
each utterance ui consists of nitokens, namely ui = {wi1, wi2, ..., wini}.
a dis-crete value yi ∈ s is used to denote the emotionlabel of ui, where s is the set of emotion labels.
the speaker identity is denoted by a function p(·).
for example, p(ui) ∈ p denotes the speaker of uiand p is the collection of all speaker roles in anerc dataset.
the objective of this task is to predictthe emotion label yt for a given query utterance utbased on dialog context {u1, u2, ..., un } and thecorresponding speaker identity..3.2 building a dag from a conversation.
we design a directed acyclic graph (dag) to modelthe information propagation in a conversation.
adag is denoted by g = (v, e, r).
in this paper,the nodes in the dag are the utterances in the con-versation, i.e., v = {u1, u2, ..., un }, and the edge(i, j, rij) ∈ e represents the information propa-gated from ui to uj, where rij ∈ r is the relationtype of the edge.
the set of relation types of edges,r = {0, 1}, contains two types of relation: 1 forthat the two connected utterances are spoken by thesame speaker, and 0 for otherwise..we impose three constraints to decide when anutterance would propagate information to another,i.e., when two utterances are connected in the dag:direction: ∀j > i, (j, i, rji) /∈ e. a previous ut-terance can pass message to a future utterance, buta future utterance cannot pass message backwards.
remote information: ∃τ < i, p(uτ ) = p(ui), (τ, i, rτ i) ∈ e and ∀j < τ, (j, i, rji) /∈ e. for eachutterance ui except the ﬁrst one, there is a previousutterance uτ that is spoken by the same speaker as.
algorithm 1 building a dag from a conversationinput: the dialog {u1, u2, ..., un }, speaker iden-.
tity p(·), hyper-parameter ω.output: g = (v, e, r).
1: v ← {u1, u2, ..., un }2: e ← ∅3: r ← {0, 1}4: for all i ∈ {2, 3, ..., n } do5:.
c ← 0τ ← i − 1while τ > 0 and c < ω doif p(uτ ) = p(ui) thene ← e ∪ {(τ, i, 1)}c ← c + 1.e ← e ∪ {(τ, i, 0)}.
else.
end ifτ ← τ − 1.
6:.
7:.
8:.
9:.
10:.
11:.
12:.
13:.
14:.
end while.
15:16: end for17: return g = (v, e, r).
ui.
the information generated before uτ is calledremote information, which is relatively less impor-tant.
we assume that when the speaker speaks uτ ,she/he has been aware of the remote informationbefore uτ .
that means, uτ has included the remoteinformation and it will be responsible for propagat-ing the remote information to ui.
local information: ∀l, τ < l < i, (l, i, rli) ∈ e.usually, the information of the local context is im-portant.
consider uτ and ui deﬁned in the secondconstraint.
we assume that every utterance ul inbetween uτ and ui contains local information, andthey will propagate the local information to ui..the ﬁrst constraint ensures the conversation tobe a dag, and the second and third constraintsindicate that uτ is the cut-off point of remote andlocal information.
we regard uτ as the ω-th latestutterance spoken by p(ui) before ui, where ω isa hyper-parameter.
then for each utterance ul inbetween uτ and ui, we make a directed edge fromul to ui.
we show the above process of building adag in algorithm 1..an example of the dag is shown in figure 2.in general, our dag has two main advancementscompared to the graph structures developed in pre-vious works (ghosal et al., 2019; ishiwatari et al.,2020): first, our dag doesn’t have edges fromfuture utterances to previous utterances, which we.
1553where f (·) is the information processing function,aggregate(·) is the information aggregation func-tion to gather information from neighboring nodes,and ni denotes the neighbours of the i-th node..recurrence-based models (rnn) allow infor-mation to propagate temporally at the same layer,while the i-th node only receives information fromthe (i−1)-th node:.
h l.i = f (h l.i−1, h l−1.
i.
)..(2).
directed acyclic graph models (dagnn) worklike a combination of gnn and rnn.
they aggre-gate information for each node in temporal order,and allow all nodes to gather information fromneighbors and update their states at the same layer:.
h l.i = f (aggregate({h l.j|j ∈ ni}), h l−1.
)..i.
(3).
the strength of applying dagnn to erc isrelatively apparent: by allowing information topropagate temporally at the same layer, dagnncan get access to distant utterances and model theinformation ﬂow throughout the whole conversa-tion, which is hardly possible for gnn.
besides,dagnn gathers information from several neigh-boring utterances, which sounds more appealingthan rnn as the latter only receives informationfrom the (i−1)-th utterance..3.3.3 dag-erc layers.
our proposed dag-erc is primarily inspired bydagnn (thost and chen, 2021), with novel im-provements specially made for emotion recognitionin conversation.
at each layer l of dag-erc, dueto the temporal information ﬂow, the hidden stateof utterances should be computed recurrently fromthe ﬁrst utterance to the last one..for each utterance ui, the attention weights be-tween ui and its predecessors are calculated byusing ui’s hidden state at the (l − 1)-th layer to at-tend to the predecessors’ hidden states at l-th layer:.
ij = softmaxj∈ni(w lαl.
α[h l.j(cid:107)h l−1i.
]).
(4).
where w lthe concatenation operation..α are trainable parameters and (cid:107) denotes.
the information aggregation operation in dag-erc is different from that in dagnn.
instead ofmerely gathering information according to the at-tention weights, inspired by r-gcn (schlichtkrullet al., 2018), we apply a relation-aware feature.
figure 2: an example dag built from a three-partyconversation, with ω = 1. the three speakers’ utter-ances are colored by red, blue and green, respectively.
solid lines represent the edges of local information, anddash lines denote the edges of remote information..argue is more reasonable and realistic, as the emo-tion of a query utterance should not be inﬂuencedby the future utterances in practice.
second, ourdag seeks a more meaningful uτ for each utter-ance, rather than simply connecting each utterancewith a ﬁxed number of surrounding utterances..3.3 directed acyclic graph neural network.
in this section, we introduce the proposed directedacyclic graph neural network for erc (dag-erc).
the framework is shown in figure 3..3.3.1 utterance feature extractiondag-erc regards each utterance as a graph node,the feature of which can be extracted by a pre-trained transformer-based language model.
fol-lowing the convention, the pre-trained languagemodel is ﬁrstly ﬁne-tuned on each erc dataset,and its parameters are then frozen while trainingdag-erc.
following ghosal et al.
(2020), weemploy roberta-large (liu et al., 2019), whichhas the same architecture as bert-large (devlinet al., 2018), as our feature extractor.
more speciﬁ-cally, for each utterance ui, we prepend a specialtoken [cls] to its tokens, making the input a formof {[cls], wi1, wi2, ..., wini}.
then, we use the[cls]’s pooled embedding at the last layer as thefeature representation of ui..3.3.2 gnn, rnn and dagnnbefore introducing the dag-erc layers in de-tail, we ﬁrst brieﬂy describe graph-based mod-els, recurrence-based models and directed acyclicgraph models to help understand their differences.
for each node at each layer, graph-based models(gnn) aggregate the information of its neighboringnodes at the previous layer as follows:.
h l.i = f (aggregate({h l−1.
j.
|j ∈ ni}), h l−1.
), (1).
i.
1554figure 3: the framework of directed acyclic graph neural network for erc (dag-erc)..transformation to make full use of the relationaltype of edges:.
m l.i =.
αijw l.rij h lj,.
(5).
(cid:88).
j∈ni.
where w lfor the relation-aware transformation..rij ∈ {w l.0, w l.1} are trainable parameters.
after the aggregated information m l.i is calcu-lated, we make it interact with ui’s hidden state atthe previous layer h l−1to obtain the ﬁnal hiddenstate of ui at the current layer.
in dagnn, the ﬁnalhidden state is obtained by allowing m li to controlinformation propagation of h l−1to the l-th layerwith a gated recurrent unit (gru):.
i.i.
(cid:101)h l.i = grul.
h (h l−1i., m l.i ),.
(6).
where h l−1and output of the gru, respectively..i , and (cid:101)h l., m l.i.i are the input, hidden state.
we refer to the process in equation 6 as nodalinformation unit, because it focuses on the nodeinformation propagating from the past layer to thecurrent layer.
nodal information unit may be suit-able for the tasks that dagnn is originally de-signed to solve.
however, we ﬁnd that only usingnodal information unit is not enough for erc, es-pecially when the query utterance ui’s emotionshould be derived from its context.
the reason isthat in dagnn, the information of context m li isonly used to control the propagation of ui’s hiddenstate, and under this circumstance, the informationof context is not fully leveraged.
therefore, we de-sign another gru called contextual informationunit to model the information ﬂow of historical.
context through a single layer.
in the contextualinformation unit, the roles of h i−1and m li in gruare reversed, i.e., h i−1controls the propagation ofm li :.
i.i.cl.
i = grul.
m (m l.i , h l−1i.
)..(7).
the representation of ui at the l-th layer is the.
sum of (cid:101)h l.i and cli:.
h l.i = (cid:101)h l.i + cli..(8).
3.3.4 training and prediction.
we take the concatenation of ui’s hidden statesat all dag-erc layers as the ﬁnal representationof ui, and pass it through a feed-forward neuralnetwork to get the predicted emotion:.
l=0 h lhi =(cid:107)li ,zi = relu(wh hi + bh ),pi = softmax(wzzi + bz),(cid:98)yi = argmaxk∈s(pi[k])..(9).
(10).
(11).
(12).
for the training of dag-erc, we employ thestandard cross-entropy loss as objective function:.
l(θ) = −.
logpi,t[yi,t],.
(13).
m(cid:88).
ni(cid:88).
i=1.
t=1.
where m is the number of training conversations,ni is the number of utterances in the i-th conver-sation, yi,t is the ground truth label, and θ is thecollection of trainable parameters of dag-erc..1555dataset.
# conversationsval.
train.
# uterrancesval.
train.
iemocapmelddailydialogemorynlp.
120.
103811118713.
114100099.test31280100085.
5810.
9989871709934.
110980691344.test1623261077401328.table 1: the statistics of four datasets..4 experimental settings.
4.1.implementation details.
we conduct hyper-parameter search for our pro-posed dag-erc on each dataset by hold-out vali-dation with a validation set.
the hyper-parametersto search include learning rate, batch size, dropoutrate, and the number of dag-erc layers.
for theω that is described in 3.2, we let ω = 1 for theoverall performance comparison by default, but wereport the results with ω varying from 1 to 3 in 5.2.for other hyper-parameters, the sizes of all hiddenvectors are equal to 300, and the feature size for theroberta extractor is 1024. each training and test-ing process is run on a single rtx 2080 ti gpu.
each training process contains 60 epochs and itcosts at most 50 seconds per epoch.
the reportedresults of our implemented models are all based onthe average score of 5 random runs on the test set..4.2 datasets.
we evaluate dag-erc on four erc datasets.
thestatistics of them are shown in table 1.iemocap (busso et al., 2008): a multimodalerc dataset.
each conversation in iemocapcomes from the performance based on script bytwo actors.
models are evaluated on the sampleswith 6 types of emotion, namely neutral, happiness,sadness, anger, frustrated, and excited.
since thisdataset has no validation set, we follow shen et al.
(2020) to use the last 20 dialogues in the trainingset for validation.
meld (poria et al., 2019b): a multimodal ercdataset collected from the tv show friends.
thereare 7 emotion labels including neutral, happiness,surprise, sadness, anger, disgust, and fear.
dailydialog (li et al., 2017): human-written di-alogs collected from communications of englishlearners.
7 emotion labels are included: neutral,happiness, surprise, sadness, anger, disgust, andfear.
since it has no speaker information, we con-sider utterance turns as speaker turns by default.
emorynlp (zahiri and choi, 2017): tv showscripts collected from friends, but varies from.
meld in the choice of scenes and emotion labels.
the emotion labels of this dataset include neutral,sad, mad, scared, powerful, peaceful, and joyful..we utilize only the textual modality of the abovedatasets for the experiments.
for evaluation met-rics, we follow ishiwatari et al.
(2020) and shenet al.
(2020) and choose micro-averaged f1 exclud-ing the majority class (neutral) for dailydialog andweighted-average f1 for the other datasets..4.3 compared methods.
we compared our model with the following base-lines in our experiments:recurrence-based methods: dialoguernn (ma-jumder et al., 2019), dialogrnn-roberta(ghosal et al., 2020), and cosmic without ex-ternal knowledge3 (ghosal et al., 2020).
graph-based methods: dialogurgcn (ghosalet al., 2019), ket (zhong et al., 2019), dialogxl(shen et al., 2020) and rgat (ishiwatari et al.,2020).
feature extractor: roberta (liu et al., 2019).
previous models with our extracted features:dialoguegcn-roberta, rgat-roberta anddagnn (thost and chen, 2021)4.ours: dag-erc..5 results and analysis.
5.1 overall performance.
the overall results of all the compared methods onthe four datasets are reported in table 2. we cannote from the results that our proposed dag-ercachieves competitive performances across the fourdatasets and reaches a new state of the art on theiemocap, dailydialog and emorynlp datasets.
as shown in the table, when the feature ex-tracting method is the same, graph-based modelsgenerally outperform recurrence-based models oniemocap, dailydialog, and emorynlp.
this phe-nomenon indicates that recurrence-based modelscannot encode the context as effectively as graph-based models, especially for the more importantlocal context.
what’s more, we see a signiﬁcantimprovement of dag-erc over the graph-based.
3in this paper, we compare our dag-erc with cosmicwithout external knowledge, rather than the complete cos-mic, in order to make a clearer comparison on the modelarchitecture, even though our dag-erc outperforms the com-plete cosmic on iemocap, dailydialog and emorynlp.
4dagnn is not originally designed for erc, so we apply.
our dag building method and the extracted feature for it..1556iemocap meld dailydialog emorynlp.
modeldialoguernn+roberta.
cosmicketdialogxldialoguegcn+roberta.
rgat.
+roberta.
robertadagnndag-erc.
62.7564.7663.0559.5665.9464.1864.9165.2266.3663.3864.6168.03.
57.0363.6164.2858.1862.4158.1063.0260.9162.8062.8863.1263.65.
-57.3256.1653.3754.93-57.5254.3159.0258.0858.3659.33.
-37.4437.1033.9534.73-38.1034.4237.8937.7837.8939.02.table 2: overall performance on the four datasets..models on iemocap, which demonstrates dag-erc’s superior ability to capture remote informa-tion given that the dialogs in iemocap are muchlonger (almost 70 utterances per dialog)..on meld, however, we observe that neithergraph-based models nor our dag-erc outper-forms the recurrence-based models.
after goingthrough the data, we ﬁnd that due to the data collec-tion method (collected from tv shows), sometimestwo consecutive utterances in meld are not coher-ent.
under this circumstance, graph-based models’advantage in encoding context is not that important.
besides, the graph-based models see consider-able improvements when implemented with thepowerful feature extractor roberta.
in spite ofthis, our dag-erc consistently outperforms theseimproved graph-based models and dagnn, con-ﬁrming the superiority of the dag structure andthe effectiveness of the improvements we make tobuild dag-erc upon dagnn..5.2 variants of dag structure.
in this section, we investigate how the structure ofdag would affect our dag-erc’s performanceby applying different dag structures to dag-erc.
in addition to our proposed structure, we furtherdeﬁne three kinds of dag structure: (1) sequence,in which utterances are connected one by one; (2)dag with single local information, in which eachutterance only receives local information from itsnearest neighbor, and the remote information re-mains the same as our dag; (3) common dag, inwhich each utterance is connected with κ previousutterances.
note that if there are only two speakerstaking turns to speak in a dialog, then our dag isequivalent to common dag with κ = 2ω, mak-ing the comparison less meaningful.
therefore, weconduct the experiment on emorynlp, where thereare usually multiple speakers in one dialog, and the.
dagsequencesingle local informationcommon κ = 2common κ = 4common κ = 6ours ω = 1ours ω = 2ours ω = 3.
# preds0.921.661.783.284.502.694.465.65.f1 score37.5738.2238.3038.3438.4839.0238.9038.94.table 3: different dags applied to dag-erc..speakers speak in arbitrary order.
the test perfor-mances are reported in table 3, together with theaverage number of each utterance’s predecessors.
several instructive observations can be madefrom the experimental results.
firstly, the per-formance of dag-erc drops signiﬁcantly whenequipped with the sequence structure.
secondly,our proposed dag structure has the highest perfor-mance among the dag structures.
considering ourdag with ω = 2 and common dag with κ = 6,with very close numbers of predecessors, our dagstill outperforms the common dag by a certainmargin.
this indicates that the constraints basedon speaker identity and positional relation are ef-fective inductive biases, and the structure of ourdag is more suitable for the erc task than rigidlyconnecting each utterance with a ﬁxed number ofpredecessors.
finally, we ﬁnd that increasing thevalue of ω may not contribute to the performanceof our dag, and ω = 1 tends to be enough..5.3 ablation study.
to study the impact of the modules in dag-erc,we evaluate dag-erc by removing relation-awarefeature transformation, the nodal information unit,and the contextual information unit individually.
the results are shown in table 4..as shown in the table, removing the relation-aware feature transformation causes a sharp per-formance drop on iemocap and dailydialog,while a slight drop on meld and emorynlp.
note that there are only two speakers per dialog.
methoddag-ercw/o rel-transw/o (cid:101)hw/o c.iemocap68.0364.12 (↓3.91)66.19 (↓1.84)66.32 (↓1.71).
meld63.6563.29 (↓0.36)63.17 (↓0.48)63.36 (↓0.29).
dailydialog59.3357.12 (↓2.21)58.05 (↓1.28)58.90 (↓0.43).
emorynlp39.0238.87 (↓0.15)38.54 (↓0.48)38.50 (↓0.52).
table 4: results of ablation study on the four datasets,with rel-trans, (cid:101)h, and c denoting relation-aware fea-ture transformation, nodal information unit, and con-textual information unit, respectively..1557dataset.
iemocapmelddailydialogemorynlp.
emotional shift# samples accuracy57.98%59.02%57.26%37.29%.
5761003670673.w/o emotional shift# samples accuracy74.25%69.45%59.25%42.10%.
1002861454361.table 5: test accuracy of dag-erc on samples withemotional shift and without it..5.5 error study.
after going through the prediction results on thefour datasets, we ﬁnd that our dag-erc fails todistinguish between similar emotions very well,such as frustrated vs anger, happiness vs excited,scared vs mad, and joyful vs peaceful.
this kind ofmistake is also reported by ghosal et al.
(2019).
be-sides, we ﬁnd that dag-erc tends to misclassifysamples of other emotions to neutral on meld,dailydialog and emorynlp due to the majorityproportion of neutral samples in these datasets..we also look closely into the emotional shiftissue, which means the emotions of two consecu-tive utterances from the same speaker are different.
existing erc models generally work poorly inemotional shift.
as shown in table 5, our dag-erc also fails to perform better on the sampleswith emotional shift than that without it, thoughthe performance is still better than previous mod-els.
for example, the accuracy of dag-erc in thecase of emotional shift is 57.98% on the iemo-cap dataset, which is higher than 52.5% achievedby dialoguernn (majumder et al., 2019) and 55%achieved by dialogxl (shen et al., 2020)..6 conclusion.
in this paper, we presented a new idea of mod-eling conversation context with a directed acyclicgraph (dag) and proposed a directed acyclic graphneural network, namely dag-erc, for emotionrecognition in conversation (erc).
extensive ex-periments were conducted and the results showthat the proposed dag-erc achieves compara-ble performance with the baselines.
moreover, bycomprehensive evaluations and ablation study, weconﬁrmed the superiority of our dag-erc and theimpact of its modules.
several conclusions can bedrawn from the empirical results.
first, the dagstructures built from conversations do affect the per-formance of dag-erc, and with the constraintson speaker identity and positional relation, the pro-posed dag structure outperforms its variants.
sec-.
figure 4: test results of rgat-roberta, dagnn,and dag-erc on the iemocap dataset by differentnumbers of network layers..in iemocap and dailydialog, and there are usu-ally more than two speakers in dialogs of meldand emorynlp.
therefore, we can infer that therelation of whether two utterances have the samespeaker is sufﬁcient for two-speaker dialogs, whilefalls short in the multi-speaker setting..moreover, we ﬁnd that on each dataset, the per-formance drop caused by ablating nodal informa-tion unit is similar to contextual information unit,and all these drops are not that critical.
this im-plies that either the nodal information unit or con-textual information unit is effective for the erctask, while combining the two of them can yieldfurther performance improvement..5.4 number of dag-erc layers.
according to the model structure introduced insection 3.3.2, the only way for gnns to receiveinformation from a remote utterance is to stackmany gnn layers.
however, it is well knownthat stacking too many gnn layers might causeperformance degradation due to over-smoothing(kipf and welling, 2016).
we investigate whetherthe same phenomenon would happen when stack-ing many dag-erc layers.
we conduct an ex-periment on iemocap and plot the test resultby different numbers of layers in figure 4, withrgat-roberta and dagnn as baselines.
asillustrated in the ﬁgure, rgat suffers a signiﬁcantperformance degradation after the number of lay-ers exceeds 6. while for dagnn and dag-erc,with the number of layers changes, both of theirperformances ﬂuctuate in a relatively narrow range,indicating that over-smoothing tends not to happenin the directed acyclic graph networks..1558123456789101112# layers636465666768f1 scorergat-robertadagnndag-ercond, the widely utilized graph relation type ofwhether two utterances have the same speaker isinsufﬁcient for multi-speaker conversations.
third,the directed acyclic graph network does not sufferover-smoothing as easily as gnns when the num-ber of layers increases.
finally, many of the errorsmisjudged by dag-erc can be accounted for bysimilar emotions, neutral samples and emotionalshift.
these reasons have been partly mentionedin previous works but have yet to be solved, whichare worth further investigation in future work..acknowledgments.
we thank the anonymous reviewers.
this paperwas supported by the program for guangdong in-troducing innovative and entrepreneurial teams(no.2017zt07x355)..references.
miltiadis allamanis, earl t barr, premkumar devanbu,and charles sutton.
2018. a survey of machinelearning for big code and naturalness.
acm com-puting surveys (csur), 51(4):1–37..carlos busso, murtaza bulut, chi-chun lee, abekazemzadeh, emily mower, samuel kim, jean-nette n chang, sungbok lee, and shrikanth siemocap: interactive emotionalnarayanan.
2008.language re-dyadic motion capture database.
sources and evaluation, 42(4):335..ankush chatterjee, kedhar nath narahari, meghanajoshi, and puneet agrawal.
2019. semeval-2019task 3: emocontext contextual emotion detection intext.
in proceedings of the 13th international work-shop on semantic evaluation, pages 39–48..sheng-yeh chen, chao-chun hsu, chuan-chun kuo,ting-hao, huang, and lun-wei ku.
2018. emo-tionlines: an emotion corpus of multi-party conver-in 11th international conference on lan-sations.
guage resources and evaluation, lrec 2018, pages1597–1601..maxwell crouse, ibrahim abdelaziz, cristina cornelio,veronika thost, lingfei wu, kenneth forbus, andachille fokoue.
2019. improving graph neural net-work representations of logical formulae with sub-graph pooling.
arxiv preprint arxiv:1911.06904..jacob devlin, ming-wei chang, kenton lee, andkristina n. toutanova.
2018. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..deepanway ghosal, navonil majumder, alexandergelbukh, rada mihalcea, and soujanya poria.
2020.cosmic: commonsense knowledge for emotionin proceedings ofidentiﬁcation in conversations.
the 2020 conference on empirical methods in nat-ural language processing: findings, pages 2470–2481..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander gelbukh.
2019.dialoguegcn: a graph convolutional neural networkfor emotion recognition in conversation.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 154–164..devamanyu hazarika, soujanya poria, rada mihal-cea, erik cambria, and roger zimmermann.
2018a.
icon:interactive conversational memory networkin proceedingsfor multimodal emotion detection.
of the 2018 conference on empirical methods innatural language processing, pages 2594–2604..devamanyu hazarika, soujanya poria, amir zadeh,erik cambria, louis-philippe morency, and rogerzimmermann.
2018b.
conversational memory net-work for emotion recognition in dyadic dialoguein proceedings of the 2018 conference ofvideos.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), volume 1, pages2122–2132..taichi ishiwatari, yuki yasuda, taro miyazaki, andjun goto.
2020. relation-aware graph attention net-works with relational position encodings for emo-tion recognition in conversations.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 7360–7370..irwin king,.
wenxiang jiao, haiqin yang,.
andmichael r lyu.
2019. higru: hierarchical gated re-current units for utterance-level emotion recognition.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 397–406..thomas n kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
arxiv preprint arxiv:1609.02907..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manuallylabelled multi-turn dialogue dataset.
in proceedingsof the eighth international joint conference on nat-ural language processing (volume 1: long papers),volume 1, pages 986–995..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..1559ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, pages 5998–6008..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro lio, and yoshua bengio.
2017. graph attention networks.
arxiv preprintarxiv:1710.10903..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..sayyed m. zahiri and jinho d. choi.
2017. emo-tion detection on tv show transcripts with sequence-based convolutional neural networks.
in aaai work-shops, pages 44–52..dong zhang, liangqing wu, changlong sun,shoushan li, qiaoming zhu, and guodong zhou.
2019a.
modeling both context- and speaker-sensitive dependence for emotion detection inin proceedings ofmulti-speaker conversations.
the twenty-eighth international joint conferenceon artiﬁcial intelligence, ijcai-19, pages 5415–5421. international joint conferences on artiﬁcialintelligence organization..muhan zhang, shali jiang, zhicheng cui, roman gar-nett, and yixin chen.
2019b.
d-vae: a variationalin ad-autoencoder for directed acyclic graphs.
vances in neural information processing systems,pages 1588–1600..peixiang zhong, di wang, and chunyan miao.
2019.knowledge-enriched transformer for emotion detec-in proceedings oftion in textual conversations.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 165–176..navonil majumder, pengfei hong, shanshan peng,jiankun lu, deepanway ghosal, alexander gel-bukh, rada mihalcea, and soujanya poria.
2020.mime: mimicking emotions for empathetic re-sponse generation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 8968–8979..navonil majumder, soujanya poria, devamanyu haz-arika, rada mihalcea, alexander gelbukh, and erikcambria.
2019. dialoguernn: an attentive rnn foremotion detection in conversations.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 33, pages 6818–6825..soujanya poria, devamanyu hazarika, navonil ma-jumder, rada mihalcea, gautam naik, and erikcambria.
2019b.
meld: a multimodal multi-partydataset for emotion recognition in conversations.
inacl 2019 : the 57th annual meeting of the associa-tion for computational linguistics, pages 527–536..soujanya poria, navonil majumder, rada mihalcea,and eduard hovy.
2019a.
emotion recognition inconversation: research challenges, datasets, and re-cent advances.
ieee access, 7:100943–100953..michael sejr schlichtkrull, thomas n. kipf, peterivan titov, andbloem, rianne van den berg,max welling.
2018. modeling relational data withgraph convolutional networks.
in 15th internationalconference on extended semantic web conference,eswc 2018, pages 593–607..bj¨orn schuller, michel valster, florian eyben, roddycowie, and maja pantic.
2012. avec 2012: the con-tinuous audio/visual emotion challenge.
in proceed-ings of the 14th acm international conference onmultimodal interaction, pages 449–456..weizhou shen, junqing chen, xiaojun quan, andzhixian xie.
2020. dialogxl: all-in-one xlnet formulti-party conversation emotion recognition.
arxivpreprint arxiv:2012.08695..bing shuai, zhen zuo, bing wang, and gang wang.
2016. dag-recurrent neural networks for scene la-in proceedings of the ieee conferencebeling.
on computer vision and pattern recognition, pages3620–3629..kai sheng tai, richard socher, and christopher dmanning.
2015. improved semantic representationsfrom tree-structured long short-term memory net-in proceedings of the 53rd annual meet-works.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing (volume 1: long papers),pages 1556–1566..veronika thost and jie chen.
2021. directed acyclicgraph neural networks.
in international conferenceon learning representations..1560