rejuvenating low-frequency words:making the most of parallel data in non-autoregressive translation.
liang ding∗the university of sydneyldin3097@sydnye.edu.au.
longyue wang∗tencent ai labvinnylywang@tencent.com.
xuebo liuuniversity of macaunlp2ct.xuebo@gmail.com.
derek f. wonguniversity of macauderekfw@um.edu.com.
dacheng taojd explore academy, jd.comdacheng.tao@gmail.com.
zhaopeng tutencent ai labzptu@tencent.com.
abstract.
knowledge distillation (kd) is commonlyused to construct synthetic data for trainingnon-autoregressive translation (nat) models.
however, there exists a discrepancy on low-frequency words between the distilled and theoriginal data, leading to more errors on pre-dicting low-frequency words.
to alleviate theproblem, we directly expose the raw data intonat by leveraging pretraining.
by analyz-ing directed alignments, we found that kdmakes low-frequency source words alignedwith targets more deterministically but fails toalign sufﬁcient low-frequency words from tar-get to source.
accordingly, we propose reversekd to rejuvenate more alignments for low-frequency target words.
to make the most ofauthentic and synthetic data, we combine thesecomplementary approaches as a new train-ing strategy for further boosting nat perfor-mance.
we conduct experiments on ﬁve trans-lation benchmarks over two advanced architec-tures.
results demonstrate that the proposedapproach can signiﬁcantly and universally im-prove translation quality by reducing transla-tion errors on low-frequency words.
encour-agingly, our approach achieves 28.2 and 33.9bleu points on the wmt14 english-germanand wmt16 romanian-english datasets, re-spectively.
our code, data, and trained mod-els are available at https://github.com/longyuewangdcu/rlfw-nat..1.introduction.
recent years have seen a surge of interest in non-autoregressive translation (nat, gu et al., 2018),which can improve the decoding efﬁciency by pre-dicting all tokens independently and simultane-ously.
the non-autoregressive factorization breaksconditional dependencies among output tokens,.
∗ liang ding and longyue wang contributed equally tothis work.
work was done when liang ding and xuebo liuwere interning at tencent ai lab..which prevents a model from properly capturingthe highly multimodal distribution of target trans-lations.
as a result, the translation quality of natmodels often lags behind that of autoregressivetranslation (at, vaswani et al., 2017) models.
tobalance the trade-off between decoding speed andtranslation quality, knowledge distillation (kd) iswidely used to construct a new training data fornat models (gu et al., 2018).
speciﬁcally, targetsentences in the distilled training data are gener-ated by an at teacher, which makes nat easilyacquire more deterministic knowledge and achievesigniﬁcant improvement (zhou et al., 2020)..previous studies have shown that distillation maylose some important information in the originaltraining data, leading to more errors on predict-ing low-frequency words.
to alleviate this prob-lem, ding et al.
(2021b) proposed to augment natmodels the ability to learn lost knowledge fromthe original data.
however, their approach relieson external resources (e.g.
word alignment) andhuman-crafted priors, which limits the applicabil-ity of the method to a broader range of tasks andlanguages.
accordingly, we turn to directly exposethe raw data into nat by leveraging pretrainingwithout intensive modiﬁcation to model architec-tures (§2.2).
furthermore, we analyze bilinguallinks in the distilled data from two alignment di-rections (i.e.
source-to-target and target-to-source).
we found that kd makes low-frequency sourcewords aligned with targets more deterministicallybut fails to align low-frequency words from tar-get to source due to information loss.
inspired bythis ﬁnding, we propose reverse kd to recall morealignments for low-frequency target words (§2.3).
we then concatenate two kinds of distilled data tomaintain advantages of deterministic knowledgeand low-frequency information.
to make the mostof authentic and synthetic data, we combine threecomplementary approaches (i.e.
raw pretraining,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3431–3441august1–6,2021.©2021associationforcomputationallinguistics3431bidirectional distillation training and kd ﬁnetun-ing) as a new training strategy for further boostingnat performance (§2.4)..we validated our approach on ﬁve translationbenchmarks (wmt14 en-de, wmt16 ro-en,wmt17 zh-en, wat17 ja-en and wmt19 en-de) over two advanced architectures (mask pre-dict, ghazvininejad et al., 2019; levenshtein trans-former, gu et al., 2019).
experimental resultsshow that the proposed method consistently im-prove translation performance over the standardnat models across languages and advanced natarchitectures.
extensive analyses conﬁrm that theperformance improvement indeed comes from thebetter lexical translation accuracy especially onlow-frequency tokens..contributions our main contributions are:.
• we show the effectiveness of rejuvenating low-frequency information by pretraining nat mod-els from raw data..• we provide a quantitative analysis of bilinguallinks to demonstrate the necessity to improvelow-frequency alignment by leveraging both kdand reverse kd..• we introduce a simple and effective trainingrecipe to accomplish this goal, which is robustlyapplicable to several model structures and lan-guage pairs..2 rejuvenating low-frequency words.
2.1 preliminaries.
translation given.
non-autoregressiveasource sentence x, an at model generateseach target word yt conditioned on previouslygenerated ones y<t, leading to high latency on thedecoding stage.
in contrast, nat models break thisautoregressive factorization by producing targetwords in parallel.
accordingly, the probability ofgenerating y is computed as:.
p(y|x) =.
p(yt|x; θ).
(1).
t(cid:89).
t=1.
where t is the length of the target sequence, andit is usually predicted by a separate conditionaldistribution.
the parameters θ are trained to max-imize the likelihood of a set of training examplesaccording to l(θ) = arg maxθ log p(y|x; θ).
typ-ically, most nat models are implemented upon theframework of transformer (vaswani et al., 2017)..knowledge distillation gu et al.
(2018) pointedout that nat models suffer from the multimodalityproblem, where the conditional independence as-sumption prevents a model from properly capturingthe highly multimodal distribution of target transla-tions.
thus, the sequence-level knowledge distilla-tion is introduced to reduce the modes of trainingdata by replacing their original target-side sampleswith sentences generated by an at teacher (guet al., 2018; zhou et al., 2020; ren et al., 2020).
formally, the original parallel data raw and thedistilled data.
−→kd can be deﬁned as follows:.
raw = {(xi, yi)}ni=1−→kd = {(xi, fs(cid:55)→t(xi))|xi ∈ raws}ni=1.
(2).
(3).
where fs(cid:55)→t represents an at-based translationmodel trained on raw data for translating text fromthe source to the target language.
n is the to-tal number of sentence pairs in training data.
asshown in figure 1 (a), well-performed nat models−→kd data instead of raw.
are generally trained on.
2.2 pretraining with raw data.
motivation gao et al.
(2018) showed that morethan 90% of words are lower than 10e-4 frequencyin wmt14 en-de dataset.
this token imbalanceproblem biases translation models towards over-ﬁtting to frequent observations while neglectingthose low-frequency observations (gong et al.,2018; nguyen and chiang, 2018; gu et al., 2020).
thus, the at teacher fs(cid:55)→t tends to generate morehigh-frequency tokens and less low-frequency to-kens during constructing distilled data.
−→kd..on the one hand, kd can reduce the modes intraining data (i.e.
multiple lexical choices for asource word), which lowers the intrinsic uncer-tainty (ott et al., 2018) and learning difﬁculty fornat (zhou et al., 2020; ren et al., 2020), makingit easily acquire more deterministic knowledge.
onthe other hand, kd aggravates the imbalance ofhigh-frequency and low-frequency words in train-ing data and lost some important information origi-nated in raw data.
ding et al.
(2021b) revealed theside effect of distilled training data, which causelexical choice errors for low-frequency words innat models.
accordingly, they introduced anextra bilingual data-dependent prior objective toaugments nat models the ability to learn the lostknowledge from raw data.
we use their ﬁndings asour departure point, but rejuvenate low-frequency.
3432(a) traditional training.
(b) raw pretraining.
(c) bidirectional distillation training.
figure 1: an illustration of different strategies for training nat models.
“distill” and “reverse distill” indicatesequence-level knowledge distillation with forward and backward at teachers, respectively.
the data block intransparent color means source- or target-side data are synthetically generated.
best view in color..data.
s (cid:55)→ t lfw links.
t (cid:55)→ s lfw links.
r.p.f1.
r.p.f1.
raw 66.4−→73.4kd←−kd61.2.
81.989.279.4.
73.380.569.1.
72.369.982.9.
80.679.183.1.
76.274.283.0.data sentenceraws 海克曼 和 奥德海姆 提出 ... 模型rawt hackman and oldham propose ... model−→kdt heckman and oddheim propose ... model←−kds 哈克曼 和 奥尔德姆 提出 ... 模式.
table 1: evaluation on aligned links between source-and target-side low-frequency words (lfw).
a di-rected line indicates aligning bilingual words from thesource to the target side (s (cid:55)→ t) or in an opposite way(t (cid:55)→ s).
r, p and f1 are recall, precision and f1-score..table 2: an example in different kinds of data.
“raw”←−means the original data while “kd” indicatesyntactic data distilled by kd and reverse kd, respec-tively.
the subscript “s” or “t” is short for source- ortarget-side.
the low-frequency words are highlightedwith colors and italics are incorrect translations..−→kd” and “.
words in a more simple and direct way: directlyexposing raw data into nat via pretraining..our approach many studies have shown thatpretraining could transfer the knowledge and datadistribution, especially for rare categories, henceimproving the model robustness (hendrycks et al.,2019; mathis et al., 2021).
here we want to trans-fer the distribution of lost information, e.g.
low-frequency words.
as illustrated in figure 1(b), wepropose to ﬁrst pretrain nat models on raw data−→kd data.
theand then continuously train them onraw data maintain the original distribution espe-cially on low-frequency words.
although it is difﬁ-cult for nat to learn high-mode data, the pretrain-ing can acquire general knowledge from authenticdata, which may help better and faster learning fur-ther tasks.
thus, we early stop pretraining when themodel can achieve 90% of the best performance ofraw data in terms of bleu score (platanios et al.,2019)1. in order to keep the merits of low-modes,.
1in preliminary experiments, we tried another simple strat-egy: early-stop at ﬁxed step according to the size of trainingdata (e.g.
training 70k en-de and early stop at 20k / 30k/ 40k, respectively).
we found that both strategies achieve.
we further train the pretrained model on distilled−→datakd.
as it is easy for nat to learn deter-ministic knowledge, we ﬁnetune the model for therest steps.
for fair comparison, the total trainingsteps of the proposed method are same as the tradi-tional one.
in general, we expect that this trainingrecipe can provide a good trade-off between rawand distilled data (i.e.
high-modes and complete vs.low-modes and incomplete)..2.3 bidirectional distillation training.
analyzing bilingual links in data kd simpli-ﬁes the training data by replacing low-frequencytarget words with high-frequency ones (zhou et al.,2020).
this is able to facilitate easier aligningsource words to target ones, resulting in high bilin-gual coverage (jiao et al., 2020).
due to theinformation loss, we argue that kd makes low-frequency target words have fewer opportunities toalign with source ones.
to verify this, we proposea method to quantitatively analyze bilingual linksfrom two directions, where low-frequency words.
similar performance..3433are aligned from source to target (s (cid:55)→ t) or in anopposite direction (t (cid:55)→ s)..the method can be applied to different typesof data.
here we take s (cid:55)→ t links in raw dataas an example to illustrate the algorithm.
giventhe wmt14 en-de parallel corpus, we employ anunsupervised word alignment method2 (och andney, 2003) to produce a word alignment, and thenwe extract aligned links whose source words arelow-frequency (called s (cid:55)→ t lfw links).
second,we randomly select a number of samples from theparallel corpus.
for better comparison, the sub-set should contains the same i in equation (2) asthat of other type of datasets (e.g.
i in equation−→(3) forkd).
finally, we calculate recall, preci-sion, f1 scores based on low-frequency bilinguallinks for the subset.
recall (r) represents howmany low-frequency source words can be alignedto targets.
precision (p) means how many alignedlow-frequency links are correct according to humanevaluation.
f1 is the harmonic mean between pre-cision and recall.
similarly, we can analyze t (cid:55)→ slfw links by considering low-frequency targets.
table 1 shows the results on low-frequency links.
−→kd can recall more s (cid:55)→ tcompared with raw,lfw links (73.4 vs. 66.4) with more accurate align-ment (89.2 vs. 73.3).
this demonstrates the effec-tiveness of kd for nat models from the bilingualalignment perspective.
however, in the t (cid:55)→ s di-rection, there are fewer lfw links (69.9 vs. 72.3)with worse alignment quality (79.1 vs. 80.6) in−→kd than those in raw.
this conﬁrms our claimthat kd harms nat models due to the loss of low-frequency target words.
inspired by these ﬁndings,it is natural to assume that reverse kd exhibits com-plementary properties.
accordingly, we conduct←−the same analysis method onkd data, and foundbetter t (cid:55)→ s links but worse s (cid:55)→ t links comparedwith raw.
take the zh-en sentence pair in ta-−→ble 2 for example,kd retains the source side low-frequency chinese words “海克曼” (raws) butgenerates the high-frequency english words “heck-−→man” instead of the golden “hackman” (kdt).
on←−the other hand,kd preserves the low-frequency en-glish words “hackman” (rawt) but produces thehigh-frequency chinese words “哈克曼” (.
←−kds)..our approach based on analysis results, wepropose to train nat models on bidirectional distil-.
lation by concatenating two kinds of distilled data.
the reverse distillation is to replace the source sen-tences in the original training data with syntheticones generated by a backward at teacher.3 ac-←−kd can be formulated as:cording to equation 3,.
←−kd = {(yi, ft(cid:55)→s(yi))|yi ∈ rawt}ni=1.
(4).
where ft(cid:55)→s represents an at-based translationmodel trained on raw data for translating text fromthe target to the source language..−→kd and.
−→kd and.
figure 1(c) illustrates the training strategy.
first,we employ both fs(cid:55)→t and ft(cid:55)→s at models to gen-←−eratekd data, respectively.
consideringcomplementarity of two distilled data, we com-←−binekd as a new training data for train-ing nat models.
we expect that 1) distilled datacan maintain advantages of low-modes; 2) bidi-rectinoal distillation can recall more lfw links ontwo directions with better alignment quality, lead-ing to the overall improvements.
besides, nguyenet al.
(2020) claimed that combining different dis-tilled data (generated by various models trainedwith different seeds) improves data diversiﬁcationfor nmt, and we leave this for future work..2.4 combining both of them:.
low-frequency rejuvenation (lfr).
we have proposed two parallel approaches to re-juvenate low-frequency knowledge from authentic(§2.2) and synthetic (§2.3) data, respectively.
intu-itively, we combine both of them to further improvethe model performance..−→kd +.
−→kd +.
←−kd →.
from data view, two presented training strategies←−−→are: raw →kdkd (raw pretraining) and(bidirectional distillation training).
consideringthe effectiveness of pretraining (mathis et al., 2021)and clean ﬁnetuning (wu et al., 2019), we introduce−→a combined pipeline: raw →kd asout best training strategy.
there are many possibleways to implement the general idea of combiningtwo approaches.
the aim of this paper is not toexplore the whole space but simply to show thatone fairly straightforward implementation workswell and the idea is reasonable.
nonetheless, wecompare possible strategies of combination twoapproaches as well as demonstrate their comple-mentarity in §3.3.
while in main experiments (in§3.2), we valid the combination strategy, namelylow-frequency rejuvenation (lfr)..2the fastalign (dyer et al., 2013) was employed to build.
word alignments for the training datasets..3this is different from back-translation (edunov et al.,2018), which is an alternative to leverage monolingual data..3434model.
iteration speed.
en-de.
ro-en.
bleu alf bleu alf.
at models.
transformer-base (ro-en teacher)transformer-big (en-de teacher).
n/an/a.
1.0× 27.30.8× 29.2.
70.5 34.173.0 n/a.
73.6n/a.
existing nat models.
nat (gu et al., 2018)iterative nat (lee et al., 2018)disco (kasai et al., 2020)mask-predict (ghazvininejad et al., 2019)levenshtein (gu et al., 2019).
1.010.04.810.02.5.
2.4× 19.22.0× 21.63.2× 26.81.5× 27.03.5× 27.3.n/a.
n/a.
31.430.233.333.333.3.our nat models.
mask-predict (ghazvininejad et al., 2019).
+low-frequency rejuvenation.
levenshtein (gu et al., 2019).
+low-frequency rejuvenation.
10.0.
1.5×.
2.5.
3.5×.
27.027.8†27.428.2†.
68.4 33.372.3 33.9†69.2 33.272.8 33.8†.
70.972.4.
71.172.7.table 3: comparison with previous work on wmt14 en-de and wmt16 ro-en.
“iteration” indicates the numberof iterative reﬁnement while “speed” shows the speed-up ratio of decoding.
“alf” is the translation accuracy onlow-frequency words.
“†” indicates statistically signiﬁcant difference (p < 0.05) from corresponding baselines..3 experiment.
3.1 setup.
data main experiments are conducted on fourwidely-used translation datasets: wmt14 english-german (en-de, vaswani et al.
2017), wmt16romanian-english (ro-en, gu et al.
2018),wmt17 chinese-english (zh-en, hassan et al.
2018), and wat17 japanese-english (ja-en, mor-ishita et al.
2017), which consist of 4.5m, 0.6m,20m, and 2m sentence pairs, respectively.
we usethe same validation and test datasets with previousworks for fair comparison.
to prove the univer-sality of our approach, we further experiment ondifferent data volumes, which are sampled fromwmt19 en-de.4 the small and medium corporarespectively consist of 1.0m and 4.5m sentencepairs, and large one is the whole dataset whichcontains 36m sentence pairs.
we preprocess alldata via bpe (sennrich et al., 2016) with 32kmerge operations.
we use tokenized bleu (pa-pineni et al., 2002) as the evaluation metric, andsign-test (collins et al., 2005) for statistical sig-niﬁcance test.
the translation accuracy of low-frequency words is measured by aolc (ding et al.,2021b), where word alignments are established.
based on the widely-used automatic alignment toolgiza++ (och and ney, 2003)..models we validated our research hypotheses ontwo state-of-the-art nat models:.
• mask-predict (maskt, ghazvininejad et al.
2019) that uses the conditional mask lm (de-vlin et al., 2019) to iteratively generate the targetsequence from the masked input.
we followedits optimal settings to keep the iteration numberas 10 and length beam as 5..• levenshtein transformer (levt, gu et al.
2019)that introduces three steps: deletion, placeholderand token prediction.
the decoding iterationsadaptively depends on certain conditions..we closely followed previous works to applysequence-level knowledge distillation to nat (kimand rush, 2016).
speciﬁcally, we train both baseand big transformer as the at teachers.
for bigmodel, we adopt large batch strategy (i.e.
458ktokens/batch) to optimize the performance.
mostnat tasks employ transformer-big as their strongteacher except for ro-en and small en-de, whichare distilled by transformer-base..4http://www.statmt.org/wmt19/.
translation-task.html.
training traditionally, nat models are usuallytrained for 300k steps on regular batch size (i.e..3435model.
zh-en.
ja-en.
model.
law med..it.
kor.
sub..bleu alf bleu alf.
at.
41.5.
27.5.
8.6.
15.4.at.
25.3.maskt 24.2+lfr 25.1†levt24.4+lfr 25.1†.
66.2.
61.564.8.
62.765.3.
29.8.
28.929.6†29.129.7.
70.8.
66.968.9.
66.869.2.table 4: performance on other language pairs, includ-ing wmt17 zh-en and wat17 ja-en.
“†” indicatesstatistically signiﬁcant difference (p < 0.05) from cor-responding baselines..128k tokens/batch).
in this work, we empiricallyadopt large batch strategy (i.e.
480k tokens/batch)to reduce the training steps for nat (i.e.
70k).
ac-cordingly, the learning rate warms up to 1 × 10−7for 10k steps, and then decays for 60k steps withthe cosine schedule (ro-en models only need 4kand 21k, respectively).
for regularization, we tunethe dropout rate from [0.1, 0.2, 0.3] based on val-idation performance in each direction, and applyweight decay with 0.01 and label smoothing with (cid:15)= 0.1. we use adam optimizer (kingma and ba,2015) to train our models.
we followed the com-mon practices (ghazvininejad et al., 2019; kasaiet al., 2020) to evaluate the performance on an en-semble of top 5 checkpoints to avoid stochasticity.
note that the total training steps of the proposedapproach (in §2.2∼2.4) are identical with thoseof the standard training (in §2.1).
taking the best−→training strategy (raw →kd) forexample, we empirically set the training step foreach stage is 20k, 20k and 30k, respectively.
andro-en models respectively need 8k, 8k and 9ksteps in corresponding training stage..←−kd →.
−→kd +.
3.2 results.
comparison with previous work table 3 liststhe results of previous competitive nat mod-els (gu et al., 2018; lee et al., 2018; kasai et al.,2020; gu et al., 2019; ghazvininejad et al., 2019)on the wmt16 ro-en and wmt14 en-de bench-mark.
we implemented our approach on top of twoadvanced nat models (i.e.
mask-predict and lev-enshtein transformer).
compared with standardnat models, our training strategy signiﬁcantlyand consistently improves translation performance(bleu↑) across different language pairs and natmodels.
besides, the improvements on translation.
maskt 37.3+lfr 38.1†levt37.5+lfr 38.5†.
30.8.
28.228.8.
28.429.4†.
24.625.4†24.725.9†.
7.38.9†7.58.4†.
11.214.3†12.414.5†.
table 5: performance on domain shift setting.
mod-els are trained on wmt14 en-de news domain butevaluated on out-of-domain test sets, including law,medicine, it, koran and subtitle.
“†” indicates statis-tically signiﬁcant difference (p < 0.05) from corre-sponding baselines..performance are mainly due to a increase of trans-lation accuracy on low-frequency words (alf↑),which reconﬁrms our claims.
for instance, ourmethod signiﬁcantly improves the standard mask-predict model by +0.8 bleu score with a substan-tial +3.6 increase in alf score.
encouragingly, ourapproach push the existing nat models to achievenew sota performances (i.e.
28.2 and 33.9 bleuon en-de and ro-en, respectively)..it is worth noting that our data-level approachesneither modify model architecture nor add extratraining loss, thus do not increase any latency(“speed”), maintaining the intrinsic advantages ofnon-autoregressive generation.
we must admit thatour strategy indeed increase the amount of comput-ing resources due to that we should train ft(cid:55)→s at←−kd data.
teachers for building.
results on other language pairs table 4 liststhe results of nat models on zh-en and ja-en lan-guage pairs, which belong to different languageindo-european, sino-tibetan andfamilies (i.e.
japonic).
compared with baselines, our methodsigniﬁcantly and incrementally improves the trans-lation quality in all cases.
for zh-en, lfr achieveson average +0.8 bleu improvement over the tra-ditional training, along with increasing on average+3.0% accuracy on low-frequency word transla-tion.
for long-distance language pair ja-en, ourmethod still improves the nat model by on aver-age +0.7 bleu point with on average +2.2 alf.
furthermore, nat models with the proposed train-ing strategy perform closely to their at teachers(i.e.
0.2 ∆bleu).
this shows the effectiveness anduniversality of our method across language pairs..3436bleu.
model.
bleu alf.
model.
1.0m 4.5m 36.0m.
at.
25.5.
37.6.
40.2.maskt 23.7+lfr 24.3†.
35.436.2†.
36.837.7†.
table 6: performance on different scale of training data.
the small and medium datasets are sampled from thelarge wmt19 en-de dataset, and evaluations are con-ducted on the same testset.
“†” indicates statisticallysigniﬁcant difference (p < 0.05) from correspondingbaselines..results on domain shift scenario the lexicalchoice must be informed by linguistic knowledgeof how the translation model’s input data maps ontowords in the target domain.
since low-frequencywords get lost in traditional nat models, the prob-lem of lexical choice is more severe under domainshift scenario (i.e.
models are trained on one do-main but tested on other domains).
thus, we con-duct evaluation on wmt14 en-de models overﬁve out-of-domain test sets (m¨uller et al., 2020),including law, medicine, it, koran and movie sub-title domains.
as shown in table 5, standard natmodels suffer large performance drops in terms ofbleu score (i.e.
on average -2.9 bleu over atmodel).
by observing these outputs, we found alarge amount of translation errors on low-frequencywords, most of which are domain-speciﬁc termi-nologies.
in contrast, our approach improves trans-lation quality (i.e.
on average -1.4 bleu over atmodel) by rejuvenating low-frequency words toa certain extent, showing that lfr increases thedomain robustness of nat models..results on different data scales to conﬁrmthe effectiveness of our method across differentdata sizes, we further experiment on three en-dedatasets at different scale.
the small- and medium-scale training data are randomly sampled fromwm19 en-de corpus, containing about 1.0m and4.5m sentence pairs, respectively.
the large-scaleone is collected from wmt19, which consists of36m sentence pairs.
we report the bleu scores onsame testset newstest2019 for fair comparison.
we employs base model to train the small-scale atteacher, and big model with large batch strategy(i.e.
458k tokens/batch) to build the at teachersfor medium- and large-scale.
as seen in table 6,our simple training recipe boost performances for.
mask-predict+raw data prior+low-frequency.
+combination.
27.027.827.8.
28.1.
68.472.472.3.
72.9.table 7: complementary to other work.
“combination”indicates combining “+raw data prior” proposed byding et al.
(2021b) with our “+low-frequency”.
ex-periments are conducted on wmt14 en-de..nat models across different size of datasets, espe-cially on large scale (+0.9), showing the robustnessand effectiveness of our approach..complementary to related work ding et al.
(2021b) is relevant to our work, which introducedan extra bilingual data-dependent prior objectiveto augment nat models the ability to learn low-frequency words in raw data.
our method is com-plementary to theirs due to that we only changedata and training strategies (model-agnostic).
asshown in table 7, two approaches yield comparableperformance in terms of bleu and alf.
besides,combination can further improve bleu as well asalf scores (i.e.
+0.3 and +0.6).
this illustratesthe complementarity of model-level and data-levelapproaches on rejuvenating low-frequency knowl-dege for nat models..3.3 analysis.
we conducted extensive analyses to better under-stand our approach.
all results are reported on themask-predict models..accuracy of lexical choice to understandwhere the performance gains come from, we con-duct ﬁne-grained analysis on lexical choice.
wedivide “all” tokens into three categories based ontheir frequency, including “high”, “medium” and“low”.
following ding et al.
(2021b), we mea-sure the accuracy of lexical choice on differentfrequency of words.
table 8 shows the results.
takeaway: the majority of improvements on trans-lation accuracy is from the low-frequency words,conﬁrming our hypothesis..low-frequency words in output we expectto recall more low-frequency words in translationoutput.
as shown in table 9, we calculate the ra-tio of low-frequency words in generated sentences.
as seen, kd biases the nat model towards gen-.
3437model.
en-de.
zh-en.
ja-en.
all high med.
low all high med.
low all high med.
low.
maskt (raw)maskt (kd).
74.376.3.
75.982.4.
74.678.3.
+raw-pretrain77.7+bi-distillation 77.9.
83.1 78.483.1 78.5.
72.568.4.
71.972.3.
68.572.7.
71.581.4.
73.4 81.673.7 81.7.
68.375.2.
75.375.3.
65.161.5.
64.164.8.
73.175.3.
76.176.5.
75.582.8.
83.483.5.
74.776.3.
76.776.7.
69.166.9.
68.368.9.table 8: analysis on different frequency words in terms of accuracy of lexical choice.
we split “all” words into“high”, “medium” and “low” categories.
shades of cell color represent differences between ours and kd..model.
en-de zh-en ja-en.
model.
all high med.
low.
maskt (raw)maskt (kd).
+raw-pretrain+bi-distillation.
10.3%7.6%.
9.3%9.7%.
6.7% 9.4%4.2% 6.9%.
5.6% 8.4%6.8% 8.7%.
table 9: ratio of low-frequency target words in output..training on raw data.
at-teacherat-student.
79.376.8.
84.780.2.
80.277.4.
73.072.8.training on distilled data78.678.7.
77.378.1.
82.583.2.at-student.
+lft.
70.972.5.
# strategy.
bleu alf.
1 raw−→kd2.
3 raw+4 raw→.
5 raw+6 raw→7 raw→.
−→kd−→kd←−kd+←−kd+←−kd+.
24.125.4.
25.625.9.
69.366.4.
67.768.2.
67.968.369.5.
−→kd−→kd−→kd→.
25.725.7−→kd 26.3.table 10: performances of different strategies.
themodels are trained and tested on wmt14 en-de.
“a+b” means concatenate a and b while “a→b” in-dicates pretraining on a and then ﬁnetuning on b..erating high-frequency tokens (low freq.↓) whileour method can not only correct this bias (on av-erage +18% and +26% relative changes for +raw-pretrain and +bi-distillation), but also enhancetranslation (bleu↑ in table 4).
takeaway: ourmethod generates translations that contain morelow-frequency words..effects of variant training strategies as dis-cussed in §2.4, we carefully investigate alternativetraining approaches in table 10. we make the to-tal training step identical to that of vanilla natmodels, and report both bleu and alf scores.
as seen, all variant strategies perform better thanthe standard kd method in terms both bleu and.
table 11: analysis on at models in term of the accu-racy of lexical choice on wmt14 en-de.
we split “all”words into “high”, “medium” and “low” categories..alf scores, conﬁrming the necessity of our work.
takeaway: 1) pretraining is more effective thancombination on utilizing data manipulation strate-gies; 2) raw data and bidirectional distilled dataare complementary to each other; 3) it is indispens-able to ﬁnetune models on.
−→kd in the last stage..our approach works for at models al-though our work is designed for nat models,we also investigated whether our lft methodworks for general cases, e.g.
autoregressive models.
we used transformer-big as the teacher model.
for fair comparison, we leverage the transformer-base as the student model, which shares the samemodel capacity with nat student (i.e.
maskt).
the result lists in table 11. as seen, at mod-els also suffer from the problem of low-frequencywords when using knowledge distillation, and ourapproach also works for them.
takeaway: ourmethod works well for general cases through reju-venating more low-frequency words..4 related work.
low-frequency words beneﬁting from contin-uous representation learned from the training data,nmt models have shown the promising perfor-mance.
however, koehn and knowles (2017) point.
3438that low-frequency words translation is still oneof the key challenges for nmt according to thezipf’s law (zipf, 1949).
for at models, arthuret al.
(2016) address this problem by integratinga count-based lexicon, and nguyen and chiang(2018) propose an additional lexical model, whichis jointly trained with the at model.
recently, guet al.
(2020) adaptively re-weight the rare wordsduring training.
the lexical choice problem is moreserious for nat models, since 1) the lexical choiceerrors (low-resource words in particular) of at dis-tillation will propagate to nat models; and 2) natlacks target-side dependencies thus misses neces-sary target-side context.
in this work, we alleviatethis problem by solving the ﬁrst challenge..data manipulation our work is related to previ-ous studies on manipulating training data for nmt.
bogoychev and sennrich (2019) show that forward-and backward-translations (ft/ bt) could bothboost the model performances, where ft playsthe role of domain adaptation and bt makes thetranslation ﬂuent.
fadaee and monz (2018) samplethe monolingual data with more difﬁcult words (e.g.
rare words) to perform bt, achieving signiﬁcant im-provements compared with randomly sampled bt.
nguyen et al.
(2020) diversify the data by applyingft and bt multiply times.
however, different fromat, the prerequisite of training a well-performednat model is to perform kd.
we compared withrelated works in table 10 and found that our ap-proach consistently outperforms them.
note that allthe ablation studies focus on exploiting the paralleldata without augmenting additional data..non-autoregressive translation a variety ofapproaches have been exploited to bridge the per-formance gap between nat and at models.
someresearchers proposed new model architectures (leeet al., 2018; ghazvininejad et al., 2019; gu et al.,2019; kasai et al., 2020), aided with additional sig-nals (wang et al., 2019; ran et al., 2019; ding et al.,2020),introduced sequential information (weiet al., 2019; shao et al., 2019; guo et al., 2020;hao et al., 2021), and explored advanced trainingobjectives (ghazvininejad et al., 2020; du et al.,2021).
our work is close to the research line ontraining methods.
ding et al.
(2021b) revealedthe low-frequency word problem in distilled train-ing data, and introduced an extra kullback-leiblerdivergence term derived by comparing the lexicalchoice of nat model and that embedded in the raw.
data.
ding et al.
(2021a) propose a simple and ef-fective training strategy, which progressively feedsdifferent granularity of data into nat models byleveraging curriculum learning..5 conclusion.
in this study, we propose simple and effective train-ing strategies to rejuvenate the low-frequency infor-mation in the raw data.
experiments show that ourapproach consistently and signiﬁcantly improvestranslation performance across language pairs andmodel architectures.
notably, domain shift is anextreme scenario to diagnose low-frequency trans-lation, and our method signiﬁcant improves them.
extensive analyses reveal that our method improvesthe accuracy of lexical choices for low-frequencysource words, recalling more low-frequency wordsin translations as well, which conﬁrms our claim..acknowledgments.
we are grateful to the anonymous reviewers andthe area chair for their insightful comments andsuggestions.
xuebo liu and derek f. wongwere supported in part by the science and tech-nology development fund, macau sar (grantno.
0101/2019/a2), and the multi-year researchgrant from the university of macau (grant no.
myrg2020-00054-fst)..references.
philip arthur, graham neubig, and satoshi nakamura.
incorporating discrete translation lexicons.
2016.into neural machine translation.
in emnlp..nikolay bogoychev and rico sennrich.
2019. domain,translationese and noise in synthetic data for neuralmachine translation.
arxiv..michael collins, philipp koehn, and ivona kuˇcerov´a.
2005. clause restructuring for statistical machinetranslation.
in acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl..liang ding, longyue wang, xuebo liu, derek fwong, dacheng tao, and zhaopeng tu.
2021a.
progressive multi-granularity training for non-autoregressive translation.
in acl..liang ding, longyue wang, xuebo liu, derek f.wong, dacheng tao, and zhaopeng tu.
2021b.
un-derstanding and improving lexical choice in non-autoregressive translation.
in iclr..3439liang ding, longyue wang, di wu, dacheng tao, andzhaopeng tu.
2020. context-aware cross-attentionfor non-autoregressive translation.
in coling..dan hendrycks, kimin lee, and mantas mazeika.
2019. using pre-training can improve model robust-ness and uncertainty.
in icml..cunxiao du, zhaopeng tu, and jing jiang.
2021.order-agnostic cross entropy for non-autoregressivemachine translation.
in icml..chris dyer, victor chahuneau, and noah a smith.
2013. a simple, fast, and effective reparameteriza-tion of ibm model 2. in naacl..sergey edunov, myle ott, michael auli, and davidgrangier.
2018. understanding back-translation atscale.
in emnlp..marzieh fadaee and christof monz.
2018. back-translation sampling by targeting difﬁcult words inneural machine translation.
in emnlp..jun gao, di he, xu tan, tao qin, liwei wang,and tieyan liu.
2018. representation degenera-tion problem in training natural language generationmodels.
in iclr..marjan ghazvininejad, vladimir karpukhin, lukezettlemoyer, and omer levy.
2020. aligned crossentropy for non-autoregressive machine translation.
in icml..marjan ghazvininejad, omer levy, yinhan liu, andluke zettlemoyer.
2019. mask-predict: parallel de-coding of conditional masked language models.
inemnlp..chengyue gong, di he, xu tan, tao qin, liwei wang,and tie-yan liu.
2018. frage: frequency-agnosticword representation.
neurips..jiatao gu, james bradbury, caiming xiong, victor okli, and richard socher.
2018. non-autoregressiveneural machine translation.
in iclr..jiatao gu, changhan wang, and junbo zhao.
2019..levenshtein transformer.
in neurips..shuhao gu, jinchao zhang, fandong meng, yangfeng, wanying xie, jie zhou, and dong yu.
2020.token-level adaptive training for neural machinetranslation.
in emnlp..junliang guo, xu tan, linli xu, tao qin, enhongchen, and tie-yan liu.
2020. fine-tuning by cur-riculum learning for non-autoregressive neural ma-chine translation.
in aaai..yongchang hao, shilin he, wenxiang jiao, zhaopengtu, michael lyu, and xing wang.
2021. multi-tasklearning with shared encoder for non-autoregressivemachine translation.
in naacl..hany hassan, anthony aue, chang chen, vishalchowdhary,jonathan clark, christian feder-mann, xuedong huang, marcin junczys-dowmunt,william lewis, mu li, et al.
2018. achieving hu-man parity on automatic chinese to english newstranslation.
arxiv..wenxiang jiao, xing wang, shilin he, irwin king,michael r. lyu, and zhaopeng tu.
2020. data reju-venation: exploiting inactive training examples forneural machine translation.
in emnlp..jungo kasai, james cross, marjan ghazvininejad, andjiatao gu.
2020. parallel machine translation withdisentangled context transformer.
in arxiv..yoon kim and alexander m rush.
2016. sequence-.
level knowledge distillation.
in emnlp..diederik p. kingma and jimmy ba.
2015. adam: a.method for stochastic optimization.
in iclr..philipp koehn and rebecca knowles.
2017. six chal-lenges for neural machine translation.
in wmt..jason lee, elman mansimov, and kyunghyun cho.
deterministic non-autoregressive neuralin.
2018.sequence modeling by iterative reﬁnement.
emnlp..alexander mathis, thomas biasi, steffen schneider,mert yuksekgonul, byron rogers, matthias bethge,and mackenzie w mathis.
2021. pretraining boostsinout-of-domain robustness for pose estimation.
wacv..makoto morishita, jun suzuki, and masaaki nagata.
2017. ntt neural machine translation systems at wat2017. in ijcnlp..mathias m¨uller, annette rios, and rico sennrich.
2020. domain robustness in neural machine trans-lation.
in amta..toan nguyen and david chiang.
2018. improving lexi-cal choice in neural machine translation.
in naacl..xuan-phi nguyen, joty shaﬁq, kui wu, and ai ti aw.
2020. data diversiﬁcation: a simple strategy forneural machine translation.
in neurips..franz josef och and hermann ney.
2003. a systematiccomparison of various statistical alignment models.
computational linguistics..myle ott, michael auli, david grangier,.
andmarc’aurelio ranzato.
2018. analyzing uncer-tainty in neural machine translation.
in icml..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl..emmanouil antonios platanios, otilia stretcu, grahamneubig, barnabas poczos, and tom mitchell.
2019.competence-based curriculum learning for neuralmachine translation.
in naacl..qiu ran, yankai lin, peng li, and jie zhou.
2019.guiding non-autoregressive neural machine transla-tion decoding with reordering information.
arxiv..3440yi ren, jinglin liu, xu tan, zhou zhao, shengzhao, and tie-yan liu.
2020. a study of non-autoregressive model for sequence generation.
inacl..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in acl..chenze shao, jinchao zhang, yang feng, fandongmeng, and jie zhou.
2019. minimizing the bag-of-ngrams difference for non-autoregressive neural ma-chine translation.
in aaai..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in neurips..yiren wang, fei tian, di he, tao qin, chengxiangzhai, and tie-yan liu.
2019. non-autoregressivemachine translation with auxiliary regularization.
inaaai..bingzhen wei, mingxuan wang, hao zhou, junyanglin, and xu sun.
2019. imitation learning for non-autoregressive neural machine translation.
in acl..lijun wu, yiren wang, yingce xia, qin tao, jian-huang lai, and tie-yan liu.
2019. exploiting mono-lingual data at scale for neural machine translation.
in emnlp..chunting zhou, graham neubig, and jiatao gu.
2020. understanding knowledge distillation in non-autoregressive machine translation.
in iclr..george k. zipf.
1949. human behavior and the princi-.
ple of least effort..3441