kaggledbqa: realistic evaluation of text-to-sql parsers.
chia-hsuan lee♢ oleksandr polozov♠ matthew richardson♠♢university of washington ♠microsoft research, redmondchiahlee@uw.edu{polozov,mattri}@microsoft.com.
abstract.
the goal of database question answering is toenable natural language querying of real-liferelational databases in diverse application do-mains.
recently, large-scale datasets such asspider and wikisql facilitated novel modelingtechniques for text-to-sql parsing, improvingzero-shot generalization to unseen databases.
in this work, we examine the challenges thatstill prevent these techniques from practical de-ployment.
first, we present kaggledbqa, anew cross-domain evaluation dataset of realweb databases, with domain-speciﬁc data types,original formatting, and unrestricted questions.
second, we re-examine the choice of evalua-tion tasks for text-to-sql parsers as appliedin real-life settings.
finally, we augment ourin-domain evaluation task with database do-cumentation, a naturally occurring source ofimplicit domain knowledge.
we show thatkaggledbqa presents a challenge to state-of-the-art zero-shot parsers but a more realisticevaluation setting and creative use of associateddatabase documentation boosts their accuracyby over 13.2%, doubling their performance..1.introduction.
text-to-sql parsing is a form of database ques-tion answering (dbqa) that answers a user’snatural-language (nl) question by converting itinto a sql query over a given relational database.
itcan facilitate nl-based interfaces for arbitrary end-user applications, thereby removing the need fordomain-speciﬁc ux or learning query languages.
as such, dbqa attracted signiﬁcant attention inacademia and industry, with development of super-vised datasets (yu et al., 2018), large-scale mod-els (wang et al., 2020b; zeng et al., 2020), andnovel modeling techniques (yu et al., 2020; denget al., 2020)..the key challenge of text-to-sql parsing is zero-shot generalization to unseen domains, i.e.
to new.
database schemas and diﬀerently distributed nlquestions.
large-scale annotated datasets like spi-der (yu et al., 2018) and wikisql (zhong et al.,2017) evaluate cross-domain generalization of text-to-sql parsers by restricting overlap between trainand test domains.
such challenging benchmarksfacilitate rapid progress in dbqa.
state-of-the-art(sota) accuracy on spider rose from 12.4% to70.5% in just two years since its release, demonstrat-ing the value of well-chosen evaluation settings..despite impressive progress in dbqa, deploy-ment of sota parsers is still challenging.
theyoften lack robustness necessary to deploy on real-life application domains.
while many challengesunderlie the gap between sota dbqa and its real-life deployment, we identify three speciﬁc discrep-ancies..first, spider and wikisql datasets normalizeand preprocess database schemas or rely on aca-demic example databases that originate with human-readable schemas (suhr et al., 2020).
in contrast, in-dustrial databases feature abbreviated and obscurenaming of table, columns, and data values, often ac-crued from legacy development or migrations.
fig-ure 1 shows a characteristic example.
after deploy-ment, text-to-sql parsers struggle with schemalinking to domain-speciﬁc entities because they donot match the distribution seen in their pre-training(e.g.
bert) or supervised training (e.g.
spider)..second, the nl questions of spider and wik-isql have high column mention percentage (denget al., 2020), which makes their language unrealistic.
this can be an artifact of rule-generated nl tem-plates (as in wikisql) or annotation uis that primethe annotators toward the schema (as in spider).
ei-ther way, real-world deployment of a text-to-sqlparser optimized on spider faces a distribution shiftin nl, which reduces its realistic performance..finally, the standard evaluation setting of cross-domain text-to-sql parsing assumes no in-domain.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2261–2273august1–6,2021.©2021associationforcomputationallinguistics2261database: student math score.
table finrev_fed_17: ⁄ state_code school_district yr_data t_fed_rev c14.
new york cityschool districtfairfax co schs.
17.
17.
2061297.c15.
⋮956851 439209 ⋮.
126916.
21035.
36886.
⋮.
column descriptions: t_fed_rev total federal revenue through the state to each school district.
federal revenue through the state-title 1 (no child left behind act)federal revenue through the state - child nutrition a.
33.
47.c14c15.
table finrev_fed_17_key: ⁄ state_code state.
1⋯5051.
#_records137⋯.
alabama⋯wisconsin 42548wyoming.
example question: which school district received the most of federal revenue through state in wisconsin?
example sql:.
select t1.school_districtfrom finrev_fed_17 as t1 join finrev_fed_key_17 as t2on t1.state_code = t2.state_code where t2.state = "wisconsin"order by t1.t_fed_rev desc limit 1.figure 1: two table excerpts from the student math score database in kaggledbqa and an example question-sqlpair.
the column names are abbreviated (e.g.
t_fed_rev) or obscure (e.g.
c14, c25) but documentation (e.g.
column descriptions) alleviates this.
source: https://kaggle.com/loganhenslee/studentmathscores..supervision.
this simpliﬁes parser evaluation andraises the challenge level for zero-shot generaliza-tion.
however, it does not leverage knowledgesources commonly present in real-world applica-tions, both explicit (annotated in-domain examples)and implicit (e.g.
database documentation, sqlqueries in the application codebase, or data dis-tributions).
a well-chosen alternative evaluationsetting would facilitate development of dbqa tech-nologies that match their real-world evaluation..kaggledbqa we introduce kaggledbqa, anew dataset and evaluation setting for text-to-sqlparsers to bridge the gap between sota dbqaresearch and its real-life deployment.1 it systemati-cally addresses three aforementioned challenges:.
• to test database generalization, it includes real-world databases from kaggle,2 a platform fordata science competitions and dataset distribu-tion.
they feature abbreviated and obscure col-umn names, domain-speciﬁc categorical values,and minimal preprocessing (section 3.1)..• to test question generalization, we collected un-restricted nl questions over the databases inkaggledbqa.
importantly, the annotators werenot presented with original column names, andgiven no task priming (section 3.2).
out of 400collected questions, one-third were out of scopefor sota text-to-sql parsers.
the remaining.
1available at https://aka.ms/kaggledbqa.
2https://www.kaggle.com.
272 questions, while expressible, can only besolved to 13.56% accuracy (section 4)..• finally, we augment kaggledbqa withdatabase documentation, common metadatafor real-world databases and a rich sourcedatabaseof implicit domain knowledge.
documentation includes column and tabledescriptions, categorical value descriptions(known as data dictionaries), sql examples,and more (section 3.3).
we present a techniqueto augment sota parsers with column andvalue descriptions, which signiﬁcantly improvestheir out-of-domain accuracy (section 4)..figure 1 shows a representative example fromthe dataset.
aligning “federal revenue” andt_fed_rev is hard without domain knowledge..in addition to more realistic data and questions,we argue that evaluation of real-world text-to-sqlperformance should assume few-shot access to ∼10in-domain question-sql examples rather than mea-suring zero-shot performance.
in practical terms,few-shot evaluation assumes up to 1-2 hours of ef-fort by a target database administrator or applica-tion developer, and translates to signiﬁcant perfor-mance beneﬁts.
in a few-shot evaluation setting,augmenting a sota text-to-sql parser (rat-sqlby wang et al.
(2020b)) with database documenta-tion almost doubled its performance from 13.56%to 26.77%.
see section 4..22622 related work.
text-to-sql semantic parsing semantic pars-ing has been studied extensively for decades (liang,2016).
key in-domain datasets such as geo-query (zelle and mooney, 1996) and atis (dahlet al., 1994) acted as initial catalyst for the ﬁeldby providing an evaluation measure and a trainingset for learned models.
applying a system to a do-main with a diﬀerent distribution of questions orparses required out-of-domain data or domain trans-fer techniques.
recently, cross-domain datasetswikisql (zhong et al., 2017) and spider (yu et al.,2018) proposed a zero-shot evaluation methodol-ogy that required out-of-domain generalization tounseen database domains.
this inspired rapid devel-opment of domain-conditioned parsers that work“out of the box” such as rat-sql (wang et al.,2020b) and irnet (guo et al., 2019).
we use thesame exact match accuracy metric as these works.
recent work (zhong et al., 2020) has proposed eval-uating sql prediction via semantic accuracy bycomputing denotation accuracy on automaticallygenerated databases instead..in this paper, we propose afew-shot learningfew-shot evaluation to inspire future research ofpractical text-to-sql parsers.
like zero-shot, few-shot has access to many out-of-domain examples,but it also has access to a small number of in-domain examples as well.
few-shot learning hasbeen applied to text classiﬁcation in (mukherjeeand awadallah, 2020), and has also been appliedto semantic parsing.
common techniques includemeta-learning (huang et al., 2018; wang et al.,2020a; li et al., 2021; sun et al., 2020) and ad-versarial learning (li et al., 2020)..generalization and practical usability recentwork has begun to question whether existingdatasets are constructed in a way that will lead tomodels that generalize well to new domains.
suhret al.
(2020) identiﬁed a number of challenges withtext-to-sql datasets, one of which is an artiﬁciallyhigh overlap between words in a question and wordsin the tables.
this issue appears in spider and is abyproduct of the fact that question authors view thedatabase schema as they write their question.
thespider-realistic (deng et al., 2020) dataset aims toreduce this by explicitly rewriting the questions toavoid overlapping terms.
other works has studiedthe problem of the gap between academic datasetsand their practical usability (de vries et al., 2020;.
radhakrishnan et al., 2020; zhang et al., 2020), in-cluding highlighting the need for data to be real.
our goal was to create an evaluation dataset andmetric that minimizes this gap; our dataset is con-structed from real data found on kaggle that hasbeen used for competitions or other analyses..another direction of generalization being ex-plored is compositionality.
keysers et al.
(2020)used rules to generate a large-scale semantic parsingdataset that speciﬁcally tests models for compos-ability..leveraging other resources for learning ras-togi et al.
(2020) provide nl descriptions for slotsand intents to help dialogue state tracking.
lo-geswaran et al.
(2019) use descriptions to facilitatezero-shot learning for entity linking.
weller et al.
(2020) use descriptions to develop a system that canperform zero-shot learning on new tasks.
we fol-low by including documentation on each includedreal-world database.
notably, this documentationwas written for human consumption of the databaserather than prepared for kaggledbqa, and thusis a natural source of domain knowledge.
it pro-vides similar beneﬁts to codebase documentationand comments, which improve source code encod-ing for ai-assisted software engineering tasks (pan-thaplackel et al., 2020; wei et al., 2019)..3 kaggledbqa: a real world dataset.
the goal of the kaggledbqa evaluation datasetis to more closely reﬂect the data and questions atext-to-sql parser might encounter in a real-worldsetting.
as such, it expands upon contemporarycross-domain text-to-sql datasets in three key as-pects: (i) its databases are pulled from real-worlddata sources and not normalized; (ii) its questionsare authored in environments that mimic naturalquestion answering; (iii) its evaluation assumesthe type of system augmentation and tuning thatcould be expected from domain experts that executetext-to-sql parser deployment.
we describe eachof these components in turn in this section..3.1 database collection.
we chose to obtain databases from kaggle, a pop-ular platform for hosting data science competi-tions and sharing datasets and code.
their hosteddatasets are by deﬁnition “real” as they are usedby members of the site for research.
competi-tion hosts upload their data unnormalized, and the.
2263table 1: comparison of text-to-sql datasets.
we follow the data ﬁltering rules of suhr et al.
(2020) and denget al.
(2020), which reduces the eﬀective number of examples from the original datasets to make them consistent.
%where measures the percentage of examples where all where/having columns in the sql query are explicitlymentioned in the nl question.
%val compares all the values in the sql queries; %select compares all theselect columns; %non select compares all columns except the select columns.
kaggledbqa has lowcolumn mention percentage and contains databases with multiple tables..dataset.
# examples.
# db # table/db % where % val % select % non-select.
atisgeoqueryrestaurantsacademicimdbyelpscholaradvising.
spider trainspider dev.
kaggledbqa.
2755253917911168396281.
70001034.
272.
11111111.
8.
14020.
2573171781015.
5.264.05.
2.25.
0.03.80.05.21.64.20.04.0.
40.839.2.
8.7.
95.6100.0100.0100.0100.0100.0100.0100.0.
89.0191.
73.5.
0.032.90.015.17.15.70.76.1.
52.448.2.
24.6.
0.09.10.01.70.84.10.23.9.
41.633.1.
6.8.data content and formatting matches its domain-speciﬁc usage (see figure 1 for an example).
toconstruct kaggledbqa, we randomly selected 8kaggle datasets that satisﬁed the following criteria:(a) contained a sqlite database; (b) licensed undera republishing-permissive license; (c) had associ-ated documentation that described the meaning ofthe tables and columns..3.2 questions.
for each database, we asked ﬁve annotators to writeten domain-speciﬁc questions that they think some-one might be interested in and that can be answeredusing the database.
we use ﬁve annotators perdatabase to help guarantee diversity of questions.
each annotated two databases, for a total of 20 an-notators and 400 questions..the annotators are not required to possess sqlknowledge so their questions are more reﬂective ofnatural user interests.
importantly, to discourageusers from using the same terms from the databaseschema in their questions, we replace the originalcolumn names with the column descriptions.
whenannotating the questions, the annotators are shown aparagraph description of the database, table names,column descriptions and ten sampled rows for eachtable.
we do not provide any constraints or tem-plates other than asking them to avoid using exactphrases from the column headings in the questions.
appendix a.2.3 shows the full guidelines..separately, each question is annotated with itssql equivalent by independent sql experts.
theyare given full access to all of the data content and.
database schema.
one-third of the questions wereyes/no, percentage, temporal, or unexpressible insql and were not considered in our evaluationof sota models (see appendix a.2.2 for details),leaving 272 questions in total..3.3 database documentation.
each database has associated plain-text documenta-tion that can assist text-to-sql parsing.
it is com-monly found as internal documentation for databaseadministrators or external documentation accom-panying a dataset release.
the contents vary butoften contain an overview of the database domain,descriptions of tables and columns, sample queries,original sources, and more..while all of these types of information couldbe leveraged to assist with domain transfer, in thiswork we focus on the column descriptions.
theyhelp address the schema linking problem of text-to-sql parsing, i.e.
aligning entity references inthe question with database columns (wang et al.,2020b).
for example, “federal revenue” in fig-ure 1 must be aligned to the column t_fed_reveven though its abbreviated name makes alignmentnon-obvious..we manually extract the column descriptionsfrom the database documentation and provide themapping from column to description as part ofkaggledbqa.
the descriptions are free text andsometimes contain additional information such asdeﬁning the values in an categorical column.
suchinformation could help with the value-linking prob-lem (mapping a value in the question to the column.
2264table 2: average partial match % of columns descrip-tions across examples.
we check whether 1- to 3-gramsin the question are part of any column descriptions..type of n-gram.
1.
2.
% cols matched in golden sql# cols matched in golden sql# cols matched not in the sql.
56.271.064.69.
21.470.371.29.
3.
4.800.070.13.that likely contains it).
we leave the entire descrip-tion as a single ﬁeld and leave it to future work toexplore these uses further.
in addition to columndescriptions, we also include the original unstruc-tured documentation which can be used for futureresearch on automatically extracting descriptionsor leveraging other domain knowledge..3.4 few-shot evaluation setting.
the current cross-domain datasets spider (yu et al.,2018) and wikisql (zhong et al., 2017) evaluatemodels in a zero-shot setting, meaning the modelis trained on one set of domains and evaluated on acompletely disjoint set.
this evaluation encouragesthe development of systems that work well "outof the box" and has spurred great development incross-domain text-to-sql systems that are able togeneralize to new domains.
however, we believethe zero-shot setting is overly-restrictive comparedto how text-to-sql systems are likely to be actuallyused in practice..we postulate that it is more realistic to assumea setting where an application author spends 1-2hours authoring examples and adapting existingdatabase documentation.
this time investment is asmall fraction of the time required to prepare an ap-plication itself and so we believe application authorswould devote the time if it resulted in increasedtext-to-sql accuracy.
in informal experiments, wehave found sql annotators can author 10-20 exam-ples in an hour.
thus, the kaggledbqa evaluationsetting is few-shot: 30% of the questions for eachdomain (6-15 depending on the domain) are des-ignated as in-domain and may be used as part oftraining for that domain, along with documentation.
the remaining 70% are used for evaluation..we report accuracy in both the few-shot as wellas the standard zero-shot (cross-domain) setting inthis paper, but consider the few-shot setting to bethe primary evaluation setting for kaggledbqa.
evaluation is conducted on the same 70% portionregardless of setting, to ensure comparable results..figure 2: comparisons of text-to-sql datasets in termsof sql structure hardness.
kaggledbqa has morecomplex sql query structure than the spider dev set..3.5 dataset statistics and comparison.
we compare kaggledbqa with previous bench-mark datasets using key metrics in table 1.kaggledbqa has the lowest value mention per-centage among all datasets, and also exhibits a lowoverlap between question terms and column namessimilar to that in all of the datasets besides spi-der, making it more in line with what would beexpected in a real-world setting where the peopleasking questions are not familiar with the actualdatabase schema and terminology.
this is likely aresult of replacing column names with descriptionsin the question annotation task..we also analyze the overlap between questionterms and column descriptions in table 2. becausethe descriptions are signiﬁcantly longer than col-umn names, we require only that they share an n-gram in common (ignoring stop-words) rather thanrequiring exact match as was done for column men-tion percent.
unigram overlap is reasonably high(56% of correct columns match the question) butalso results in many false-positive matches withother columns.
increasing n-gram size decreasesfalse-positives but also rapidly decreases the correctcolumn match percent.
thus, column descriptionsmay help guide the model, but are not as strong ofa signal as found in spider which suﬀers from highexact column name match overlap.
this was ourintention in asking our annotators to avoid usingthe descriptions verbatim when writing questions..to measure the complexity of sql inkaggledbqa, we adopt the hardness criteria ofspider and report the numbers in figure 2. thequeries are on average more complex than spider’s,with signiﬁcantly more hard and extra-hard ones..2265hardness criteria % in dataset01020304050easymediumhard extrakaggledbqaspider dev 4 experiments.
4.1 baseline resultswe ﬁrst evaluate kaggledbqa using models thatwere developed for the spider dataset..editsql (zhang et al., 2019): editsql (withbert) is the highest-performing model on the spi-der dataset that also provides an open-source im-plementation along with a downloadable trainedmodel.3 the model was built for edit-based multi-turn parsing tasks, but can also be used as a single-turn parser for spider or kaggledbqa.
it employsa sequence-to-sequence model with a question-table co-attention encoder for schema encoding..rat-sql (wang et al., 2020b): rat-sql (v3+ bert) is the model with highest accuracy onthe spider leaderboard that also provides an open-source implementation.4,5 it adds string matchingto the encoder through the use of relation-awareself-attention and adopts a tree-based decoder toensure the correctness of the generated sql..throughout this paper, we use the same exact-match accuracy metric introduced by the spiderdataset.
although our primary evaluation settingis few-shot, we ﬁrst examine the traditional zero-shot setting to present an unbiased comparison withprevious results.
table 3 compares the performanceof these two models (both trained on spider).
ascan be seen, the performance of both models issigniﬁcantly lower on kaggledbqa.
this echoesthe ﬁndings of suhr et al.
(2020) who found thata model trained on spider did not generalize wellto other datasets.
also, kaggledbqa has muchfewer column mentions and much more complexsql than spider (see table 1 and figure 2)..for all further experiments on kaggledbqa thatemulate real-world evaluation, we choose rat-sql as the best performing parser..4.2 rat-sql on kaggledbqa.
4.2.1 moving to the few-shot settingto apply rat-sql to kaggledbqa’s few-shotsetting, for each domain we create a model byﬁne-tuning on its 30% in-domain data.
see ap-pendix a.3 for implementation details.
this ﬁne-.
3https://github.com/ryanzhumich/.
editsql.
4as of one month before paper authoring.
current sotasystems are also based on rat-sql and add less than 5%accuracy, thus will likely behave similarly..5https://github.com/microsoft/rat-sql.
table 3: zero-shot testing results of various open-sourcemodels on kaggledbqa and on the test set of spider.
all numbers are the exact match accuracy evaluated bythe spider oﬃcial scripts.
the spider results are fromthe oﬃcial leaderboard.
the kaggledbqa results arethe average of three diﬀerent runs..modelsrat-sql (wang et al., 2020b)editsql (zhang et al., 2019).
spider kaggledbqa.
65.6053.40.
13.5611.73.tuning is always performed as the last step beforeevaluation..as table 4 shows, ﬁne-tuning on a small amountof in-domain data dramatically increases overallaccuracy from 13.56% to 17.96% (rows (a) and (e)),although the few-shot setting is our primary setting,we also present results in the zero-shot setting tocompare to previous work (table 4 rows (e)-(h)).
however, in the remainder of the paper we will befocusing on the few-shot setting..4.2.2 leveraging database documentationthe database schemas in kaggledbqa are ob-scure, making the task diﬃcult without leveragingthe database documentation.
we consider only thecolumn descriptions, but other portions of the do-cumentation may prove useful in future work.
thebest approach for incorporating column descrip-tions into a text-to-sql model is model-speciﬁc.
rat-sql makes use of relations between questiontokens and schema terms to assist with schema-linking.
we extend the same functionality to col-umn descriptions by appending the column descrip-tions to the column names (separated by a period)and recomputing matching relations.
the concate-nated column name is also presented to the trans-former encoder for schema encoding..simply adding these descriptions results in mis-match between the training set (spider) whichdoes not have descriptions, and the evaluation set(kaggledbqa) which does.
to alleviate it, we ﬁrstaugment the schemas in spider with artiﬁcial de-scriptions.
for column 𝑐 of table 𝑡, the descriptionfor 𝑐 is “the 𝑐 of the 𝑡”.
we then retrain rat-sqlon spider with these artiﬁcial descriptions..since the artiﬁcial descriptions simply restateinformation from the schema, the model may notlearn to leverage them for any further informationabout schema linking and simply treat them as noise.
therefore, we also evaluate rat-sql adapted tothe general domain of kaggledbqa so that it (a).
2266table 4: exact match accuracy and standard error on kaggledbqa, mean of three runs with diﬀerent random seeds..models.
(a) rat-sql(b) w. desc(c) w. adaptation(d) w. desc + adaptation.
nuclear.
crime.
28.7822.7228.7836.35.
35.1829.6244.4444.44.
11.7612.7416.6621.56.baseball.
fires whatcd soccer.
avg.
14.8111.1116.0422.22.
30.6633.3337.3341.33.
10.6819.0416.6627.38.
8.338.3313.8713.87.
17.96 ± 0.5%17.55 ± 0.6%22.82 ± 0.1%26.77 ± 0.4%.
models.
nuclear.
crime.
pesticide mathscore.
baseball.
fires whatcd soccer.
avg.
(e) rat-sql(f) w. desc(g) w. adaptation(h) w. desc + adaptation.
22.7224.2425.7530.29.
25.9220.3738.8825.92.
8.827.8412.7417.64.
12.349.877.4016.04.
17.3313.3320.0025.33.
4.767.149.5211.9.
16.6616.6616.6616.66.
13.56 ± 0.1%12.43 ± 0.1%16.80 ± 0.8%18.41 ± 0.4%.
with ﬁne-tuningpesticide mathscore.
without ﬁne-tuning.
3.503.508.767.01.
0.000.003.503.50.experiences useful descriptions and (b) adapts tothe language distribution of kaggledbqa.
weevaluate the beneﬁts of this adaptation using leave-one-out: for each domain in kaggledbqa, we ﬁne-tune the model on all other domains except for thetarget (with the same ﬁne-tuning parameters as forfew-shot learning).
adapting in this way is pre-dictive of the performance of a novel domain withsimilar characteristics..as with the other few-shot results, the modelis then ﬁne-tuned on the few examples of targetdomain data.
adaptation and ﬁne-tuning are twoseparate training processes.
adaptation is meantto adapt to the real-world distribution.
fine-tuningis meant to adjust for in-domain knowledge.
themost eﬀective setting for a target database in ourexperiments is to conduct adaptation ﬁrst, followedby ﬁne-tuning..table 4 (row (d)) shows the results.
using col-umn descriptions in the context of adaptation in-creases model accuracy from 17.96% to 26.77%.
ablations show that adaptation and descriptionseach contribute approximately half of this gain (row(c)).
descriptions provide no beneﬁt without adap-tation (row (b)), likely due to the train-test mismatchbetween artiﬁcial descriptions and real ones.
with-.
table 5: exact match accuracy and standard erroron schema-normalized kaggledbqa, average of threeruns with diﬀerent random seeds..models.
with ﬁne-tuning.
(a) rat-sql(b) w. desc(c) w. normalization(e) w. adaptation(f) w. desc + adaptation(g) w. normalization + adaptation(h) w. desc + normalization + adaptation.
avg.
17.96 ± 0.5%17.55 ± 0.6%23.09 ± 0.9%22.82 ± 0.1%26.77 ± 0.4%25.60 ± 0.9%27.83 ± 0.7%.
out any artiﬁcial descriptions, accuracy drops evenfurther so they are critical to leveraging in-domainknowledge.
overall, incorporating in-domain data(i.e.
a few-shot setting and database documenta-tion) nearly doubles model accuracy from 13.56%to 26.77% on kaggledbqa..4.3 column normalization.
one of the major challenges in kaggledbqa is thatcolumn names are often obscure or abbreviated.
anatural question is whether this creates diﬃculty be-cause the model struggles to understand the mean-ing of a column or because it leads to a low overlapbetween question and column terms.
in an attemptto tease these factors apart, we created a normalizedversion of kaggledbqa by replacing the obscurecolumn names with normalized column names suchas one might ﬁnd in the spider dataset.
this wasdone manually using column descriptions to helpclarify each column and without introducing any ex-tra knowledge into the column names except for theexpansion of abbreviations (e.g.
t_fed_rev →total federal revenue)..in table 5 we give the results of evaluation onthe normalized kaggledbqa, following the samesetup as table 4. normalization provides a signif-icant boost in performance (row (c) vs. row (a)).
the trend is similar to table 4. without adaptation,models with descriptions are not better than thosewithout (row (b) vs. row (a), row (d) vs. row (c)).
after adaptation, the train-test mismatch is partlymitigated and the performance improves (row (f)vs. row (e), row (h) vs. row (g)).
normalizationand descriptions provide complementary knowl-edge augmentation, jointly improving accuracy by5% (row (h) vs. row (e)), more than either alone..normalization helps clarify the obscure columnnames of kaggledbqa.
however, the other chal-.
2267questionno desc..desc..questionno desc.
desc..questionno desc.
desc..table 6: examples where description-augmented (“desc.”) models solve a question that unaugmented models (“nodesc.”) do not.
both models are adapted and ﬁne-tuned.
both omit values, as per the oﬃcial spider metric..database uswildfires column descriptionsstat_cause_codestat_cause_descrfire_size.
code for the (statistical) cause of the ﬁredescription of the (statistical) cause of the ﬁre.
estimate of acres within the ﬁnal perimeter of the ﬁre.
what’s the most common cause of the ﬁre (code) in the database?
select fires.stat_cause_descr from fires group by fires.
stat_cause_descr order by count(*)desc limit 1select fires.stat_cause_code from fires group by fires.
stat_cause_code order by count(*)desc limit 1what is the total area that has been burned until now?
select sum(*)from firesselect sum(fires.fire_size)from fires.
database pesticideorigincountry.
column descriptionscode indicating sample origin (1=u.s.
2=imported 3=unknown)country of origin if the sample was imported.
how many samples come from other countries?
select sampledata15.country from sampledata15select count(*)from sampledata15 where sampledata15.origin = ’⬚’.
table 7: distribution of error types in each domain over 10 randomly-selected erroneous examples..error types.
entity-column matchingincorrect final columnmissing constraintincorrect constraintunderstanding errorambiguous columnsequivalent.
0354001.nuclear crime.
pesticide mathscore baseball.
fires whatcd soccer.
%.
2232120.
2532022.
3312400.
0456010.
2450110.
0427200.
0222300.
15.00%33.75%32.50%31.25%13.75%7.50%3.75%.
lenges such as low column mention percentage andin-domain schema conventions still leave signiﬁ-cant room for improvement.
we provide the fullexperimental results on normalized tables in theappendix..4.4 error analysis.
table 6 shows examples of improvements dueto descriptions.
first, column descriptions helpthe parser correctly identify columns to select.
for instance, it chooses stat_cause_code overstat_cause_descr when asked for “the most com-mon cause of the ﬁre (code)”.
second, they clarifynecessary constraints.
for instance, when asked“how many samples come from other countries?”,the parser chooses the correct origin column ratherthan superﬁcially-matching country in the clausewhere sampledata15.origin = "2"..table 7 shows a distribution of error types inkaggledbqa using 10 randomly-selected erro-neous predictions for each domain.
the error cat-egories mostly follow suhr et al.
(2020), modulo(a) removing unobserved categories, (b) separat-.
ing semantically equivalent predictions into theirown “equivalent” category, and (c) categorizingsigniﬁcant structural errors as “understanding er-rors”.
we also provide more characteristics of eachdatabase in table 8 in an attempt to understand thediﬀerence in performance across databases.
ourmodel performs worst on the databases with themost columns (pesticide, baseball and soccer).
the only database with lower accuracy is math-score which has multiple tables and a relativelysmall ﬁne-tuning set..the most common error types and their exam-ples are summarized in table 9.
(i) the most com-mon type is “incorrect final column” (33.75%),illustrating the diﬃculty of schema linking inkaggledbqa even with documentation and ﬁne-tuning.
(ii) 32.5% of the errors are in “missing con-straints”.
in kaggledbqa questions, users some-times use implications instead of directly mention-ing the desired constraint, e.g.
“in preparation” forstatus = "under construction".
(iii) 31.25%of the errors are in “incorrect constraint”, e.g.
failing to parse “highest” into the top-1 result in.
2268table 8: statistics of each database in kaggledbqa..nuclear crime.
pesticide mathscore baseball.
fires whatcd soccer.
#tables#columns#fine-tuning examples#test examples.
1151022.
16918.
2341634.
315919.
5441227.
1191225.
2101328.
237612.table 9: the most common error types of our best model and their representative examples..question what is the latitudinal band that is most likely to experience wildﬁres in the usa?
predicted.
select stat_cause_descr from fires group by stat_cause_descr order bycount(*)desc limit 1select latitude from fires group by latitude order by count(*)desc limit 1.gold.
33.75%: incorrect final column.
questionpredictedgold.
how many nuclear power plants are in preparation to be used in japan?
select count(*)from nuclear_power_plants where country = ’⬚’select count(*)from nuclear_power_plants where country = "japan"and status.
= "under construction".
32.5%: missing constraint.
question which state gets the highest revenue?
predicted.
31.25%: incorrect constraint.
select ndecoreexcel_math_grade8.state from finrev_fed_17 joinndecoreexcel_math_grade8 group by ndecoreexcel_math_grade8.state orderby sum(finrev_fed_17.t_fed_rev)ascselect t2.state from finrev_fed_key_17 as t2 join finrev_fed_17 as t1 ont1.state_code = t2.state_code group by t2.state order by sum(t_fed_rev)desc limit 1.
15%: entity-column matching.
question which type of crime happens the most in salford?
predicted.
select type from greatermanchestercrime where location liketype order by count(*)desc limit 1select type from greatermanchestercrime where lsoa like "%salford%"groupby type order by count(*)desc limit 1.group by.
’⬚’.
gold.
gold.
13.75%: understanding error.
questionpredicted.
gold.
how many downloads of ep and album respectively?
select sum(totalsnatched), sum(totalsnatched)from torrents wherereleasetype = ’⬚’select sum(totalsnatched)from torrents where releasetype = "ep"unionselect sum(totalsnatched)from torrents where releasetype = "album".
(iv) 15% of the errors are indescending order.
“entity-column matching”, e.g.
aligning “salford”to location rather than lsoa.
this illustrates thediﬃculty of value linking, partly mitigated by valuedescriptions for categorical columns in the databasedocumentation..5 conclusion & future work.
kaggledbqa provides two resources to facili-tate real-world applications of text-to-sql pars-ing.
first, it encourages an evaluation regimethat bridges the gap between academic and indus-trial settings, leveraging in-domain knowledge andmore realistic database distribution.
we encour-age adopting this regime for established text-to-sql benchmarks.
second, it is a new dataset ofmore realistic databases and questions, present-.
ing a challenge to state-of-the-art parsers.
de-spite the addition of domain knowledge in the formof database documentation, our baselines reachonly 26.77% accuracy, struggling to generalize toharder questions.
we hope that better use of docu-mentation and new modeling and domain adapta-tion techniques will help further advance state ofthe art.
the kaggledbqa dataset is available athttps://aka.ms/kaggledbqa..ethical considerations.
dataset collection the data collection processwas pre-approved by irb.
each annotator agreedto a consent form before having access to the label-ing task.
each annotator was rewarded with a $20e-gift card for the approximately one hour of theirtime.
the authors of this paper acted as the sql an-.
2269notators and incurred no additional compensation.
the databases collected for kaggledbqa were in-dividually reviewed to ensure they were properlylicensed for re-distribution.
for other details ofdataset construction, please refer to section 3..aside from email addresses, no personal infor-mation of annotators was collected during our study.
email addresses were not shared and were promptlydeleted after compensation had been provided.
theassociation between annotator and annotation wasdeleted before any analysis or distribution was con-ducted..language distribution kaggledbqa only in-cludes question annotations and databases in en-glish, thus evaluating multi-lingual text-to-sqlmodels on it will require translation.
the set of an-notators included both native and second-languagespeakers of english, all ﬂuent..usage of dbqa technology our goal withkaggledbqa is to encourage the development ofdbqa that will work in real-world settings.
theactual deployment of a text-to-sql parser must beconducted with appropriate safeguards in place toensure users understand that the answers may beincorrect, especially if those answers are to be usedin decision making..referencesdeborah a. dahl, madeleine bates, michael brown,william fisher, kate hunicke-smith, david pallett,christine pao, alexander rudnicky, and elizabethshriberg.
1994. expanding the scope of the atistask: the atis-3 corpus.
in human language tech-nology: proceedings of a workshop held at plains-boro, new jersey, march 8-11, 1994..xiang deng, ahmed hassan awadallah, christophermeek, oleksandr polozov, huan sun, and matthewrichardson.
2020. structure-grounded pretrainingfor text-to-sql.
arxiv preprint arxiv:2010.12773..jiaqi guo, zecheng zhan, yan gao, yan xiao, jian-guang lou, ting liu, and dongmei zhang.
2019. to-wards complex text-to-sql in cross-domain databasewith intermediate representation.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4524–4535, florence,italy.
association for computational linguistics..volume 2 (short papers), pages 732–738, new or-leans, louisiana.
association for computational lin-guistics..daniel keysers, nathanael schärli, nathan scales,hylke buisman, daniel furrer, sergii kashubin,nikola momchev, danila sinopalnikov, lukaszstaﬁniak, tibor tihon, dmitry tsarkov, xiao wang,marc van zee, and olivier bousquet.
2020. measur-ing compositional generalization: a comprehensivemethod on realistic data.
in 8th international confer-ence on learning representations, iclr 2020, addisababa, ethiopia, april 26-30, 2020. openreview.net..zechang li, yuxuan lai, yansong feng, and dongyanzhao.
2020. domain adaptation for semantic pars-ing.
in proceedings of the twenty-ninth interna-tional joint conference on artiﬁcial intelligence, ij-cai 2020, pages 3723–3729.
ijcai.org..zhuang li, lizhen qu, shuo huang, and gholamrezahaﬀari.
2021. few-shot semantic parsing for newpredicates.
arxiv preprint arxiv:2101.10708..percy liang.
2016. learning executable semanticparsers for natural language understanding.
com-munications of the acm, 59(9):68–76..lajanugen logeswaran, ming-wei chang, kenton lee,kristina toutanova, jacob devlin, and honglak lee.
2019. zero-shot entity linking by reading entity de-scriptions.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 3449–3460, florence, italy.
association forcomputational linguistics..subhabrata mukherjee and ahmed awadallah.
2020.uncertainty-aware self-training for few-shot text clas-siﬁcation.
advances in neural information process-ing systems, 33..sheena panthaplackel, milos gligoric, raymond jmooney, and junyi jessy li.
2020. associating nat-ural language comment and source code entities.
inproceedings of the aaai conference on artiﬁcialintelligence, volume 34, pages 8592–8599..karthik radhakrishnan, arvind srikantan, and xi vic-toria lin.
2020. colloql: robust cross-domaintext-to-sql over search queries.
arxiv preprintarxiv:2010.09927..abhinav rastogi, xiaoxue zang, srinivas sunkara,raghav gupta, and pranav khaitan.
2020. towardsscalable multi-domain conversational agents: theschema-guided dialogue dataset.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8689–8696..po-sen huang, chenglong wang, rishabh singh, wen-tau yih, and xiaodong he.
2018. natural languageto structured query generation via meta-learning.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,.
alane suhr, ming-wei chang, peter shaw, and kentonlee.
2020. exploring unexplored generalization chal-lenges for cross-database semantic parsing.
in pro-ceedings of the 58th annual meeting of the associa-tion for computational linguistics, pages 8372–8388,online.
association for computational linguistics..2270system demonstrations, pages 204–214, online.
as-sociation for computational linguistics..rui zhang, tao yu, heyang er, sungrok shim, eric xue,xi victoria lin, tianze shi, caiming xiong, richardsocher, and dragomir radev.
2019. editing-basedsql query generation for cross-domain context-dependent questions.
in proceedings of the 2019conference on empirical methods in natural lan-guage processing and the 9th international joint con-ference on natural language processing (emnlp-ijcnlp), pages 5338–5349, hong kong, china.
as-sociation for computational linguistics..yusen zhang, xiangyu dong, shuaichen chang, tao yu,peng shi, and rui zhang.
2020. did you ask a goodquestion?
a cross-domain question intention classi-ﬁcation benchmark for text-to-sql.
arxiv preprintarxiv:2010.12634..ruiqi zhong, tao yu, and dan klein.
2020. semanticevaluation for text-to-sql with distilled test suite.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 396–411..victor zhong, caiming xiong, and richard socher.
2017. seq2sql: generating structured queries fromnatural language using reinforcement learning.
arxivpreprint arxiv:1709.00103..yibo sun, duyu tang, nan duan, yeyun gong, xi-aocheng feng, bing qin, and daxin jiang.
2020. neu-ral semantic parsing in low-resource settings withback-translation and meta-learning.
in proceedingsof the aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8960–8967..harm de vries, dzmitry bahdanau, and christophermanning.
2020. towards ecologically valid re-search on language user interfaces.
arxiv preprintarxiv:2007.14435..bailin wang, mirella lapata, and ivan titov.
2020a.
meta-learning for domain generalization in semanticparsing.
arxiv preprint arxiv:2010.11988..bailin wang, richard shin, xiaodong liu, oleksandrpolozov, and matthew richardson.
2020b.
rat-sql: relation-aware schema encoding and linkingfor text-to-sql parsers.
in proceedings of the 58thannual meeting of the association for computationallinguistics, pages 7567–7578, online.
associationfor computational linguistics..bolin wei, ge li, xin xia, zhiyi fu, and zhi jin.
2019.code generation as a dual task of code summarization.
in advances in neural information processing sys-tems 32: annual conference on neural informationprocessing systems 2019, neurips 2019, december8-14, 2019, vancouver, bc, canada, pages 6559–6569..orion weller, nicholas lourie, matt gardner, andmatthew peters.
2020. learning from task descrip-in proceedings of the 2020 conference ontions.
empirical methods in natural language processing(emnlp), pages 1361–1375, online.
association forcomputational linguistics..tao yu, chien-sheng wu, xi victoria lin, bailinwang, yi chern tan, xinyi yang, dragomir radev,richard socher, and caiming xiong.
2020. grappa:grammar-augmented pre-training for table semanticparsing.
arxiv preprint arxiv:2009.13845..tao yu, rui zhang, kai yang, michihiro yasunaga,dongxu wang, zifan li, james ma, irene li, qingn-ing yao, shanelle roman, zilin zhang, and dragomirradev.
2018. spider: a large-scale human-labeleddataset for complex and cross-domain semantic pars-ing and text-to-sql task.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3911–3921, brussels, bel-gium.
association for computational linguistics..john m zelle and raymond j mooney.
1996. learningto parse database queries using inductive logic pro-gramming.
in proceedings of the national conferenceon artiﬁcial intelligence, pages 1050–1055..jichuan zeng, xi victoria lin, steven c.h.
hoi, richardsocher, caiming xiong, michael lyu, and irwin king.
2020. photon: a robust cross-domain text-to-sqlsystem.
in proceedings of the 58th annual meet-ing of the association for computational linguistics:.
2271table 10: evaluation results on kaggledbqa using 100% of the evaluation data.
all numbers are the exact matchaccuracy evaluated by the spider oﬃcial scripts.
here we report the average score of three runs with diﬀerentrandom seeds..models.
nuclear.
crime.
pesticide mathscore.
baseball.
fires whatcd soccer.
avg.
ratsqlw. descw. adaptationw. desc + adaptation.
22.9121.8720.8329.16.
23.4520.9833.3325.88.
8.009.9912.6618.00.
0.000.003.573.57.
11.1111.1111.1116.23.
25.2218.0124.3230.62.
4.766.508.9310.53.
11.1111.1112.9612.96.
13.3212.4415.9618.37.table 11: the original user question distribution.
this reﬂects the natural information need from users..has there been a recent surge in violent crime in manchester?
what percentage of august crime detections resulted in prosecution of a suspect?.
question types.
example.
#.
51.yes/nopercentage.
time-related.
sql-unexpressible.
46.divide the day into 3 slots (6am to 4pm, 4pm to 11pm, 11pm to 6am),which has the highest amount of crime conducted per hour?
31 which states had the highest percentage change in average scores.
over the last few years?.
sql-expressible.
272 which lsoa has had the most instances of bicycle theft this month?.
a appendix.
a.1 evaluation on full testing datawe show the zero shot testing and out-of-domainadaptation results in table 10. in contrast to table 4,they are evaluated using the full set of testing data..a.2 details of dataset construction.
a.2.1 example page of user instructionsfor each user, we show two diﬀerent htmlﬁles that contain diﬀerent instructions of the task,database overview, table name(s), column descrip-tions, ten sampled rows of the database content..a.2.2 question typesquestion annotators were allowed to write any typeof question without restriction.
while this repre-sents a natural distribution of questions one mightexpect to encounter in a realistic setting, some typesdo not appear in the spider training set and thuspose particular diﬃculty with current text-to-sqlsystems.
we remove these from the oﬃcial evalua-tion but still include them in the dataset for futurework on these types of questions.
table 11 summa-rizes the distribution over these types of questions..a.2.3 sql annotation guidelineswe also establish few guidelines and follow themthroughout the annotation process:.
of sexual oﬀenses crime events?
→ selectlocation from greatermanchestercrimewhere type = "violence and sexualoffences"group by location order byif it is free-formcount(*)desc limit 1).
text use "like" operator with a term fromthe question (e.g., what were the closing oddsfor a draw in matches with vfb stuttgart?
→ select draw_closing from betfrontwhere match like "%vfb stuttgart%")..2. sometimes id columns are paired with theirname realizations (e.g., state_code and state).
we choose to return id whenever users do notexplicitly ask for the name realizations..3. duplicate rows can sometimes yield an incorrectresult.
however, it is not possible for models toknow in advance unless they encode databasecontent.
so we use the distinct operator whennecessary to return the correct answer or it isexplicitly asked for by the user (e.g., what aretitles for each unique entry?)..
implementation details.
a.3for all our experiments we use the rat-sql of-ﬁcial implementation and the pre-trained bert-large from google.
6 we follow the original set-tings to get the pre-ﬁne-tuned/pre-adapted models..1. if the referred column is categorical, use "="operator with the value from the database (e.g.,where is the area with the largest number.
6we use the bert-large, uncased (whole word mask-from https://storage.googleapis..ing) modelcom/bert_models/2019_05_30/wwm_uncased_l-24_h-1024_a-16.zip.
2272table 12: exact match accuracy and standard error on schema-normalized kaggledbqa, average of three runswith diﬀerent random seeds..models.
(a) rat-sql(b) w. desc(c) w. adaptation(d) w. desc + adaptation.
nuclear.
crime.
25.7525.7530.3033.33.
44.4440.7346.2949.99.
23.5219.6019.6028.43.baseball.
fires whatcd soccer.
avg.
19.7420.9819.7422.21.
33.3328.0041.3337.33.
22.6125.0021.4226.18.
8.338.3313.8816.44.
23.09 ± 0.9%21.48 ± 1.0%25.60 ± 0.9%27.86 ± 0.7%.
models.
nuclear.
crime.
pesticide mathscore.
baseball.
fires whatcd soccer.
avg.
(e) rat-sql(f) w. desc(g) w. adaptation(h) w. desc + adaptation.
30.2924.2325.7534.84.
35.1825.9240.7337.03.
15.6813.7221.5623.52.
12.340.0814.8118.51.
22.6613.3325.3324.00.
5.950.0710.6916.66.
25.0013.8725.0021.96.
19.04 ± 0.6%13.35 ± 0.9%22.23 ± 0.7%23.16 ± 0.5%.
with ﬁne-tuningpesticide mathscore.
without ﬁne-tuning.
7.013.5012.278.76.
0.050.0014.028.76.for adaptation and ﬁne-tuning, we decrease thelearning rate of bert parameters by 50 times to6e-8 to avoid overﬁtting.
we keep the learning rateof non-bert parameters the same at 7.44e-4.
wealso increase the dropout rate of the transformersfrom 0.1 to 0.3 to provide further regularization..2273