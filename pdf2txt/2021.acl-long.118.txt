challenges in information-seeking qa:unanswerable questions and paragraph retrieval.
akari asaiuniversity of washingtonakari@cs.washington.edu.
eunsol choi∗the university of texas at austineunsol@cs.utexas.edu.
abstract.
recent pretrained language models “solved”many reading comprehension benchmarks,where questions are written with the accessto the evidence document.
however, datasetscontaining information-seeking queries whereevidence documents are provided after thequeries are written independently remainchallenging.
we analyze why answeringinformation-seeking queries is more challeng-ing and where their prevalent unanswerabili-ties arise, on natural questions and tydi qa.
our controlled experiments suggest two head-rooms – paragraph selection and answerabil-ity prediction, i.e.
whether the paired evidencedocument contains the answer to the query ornot.
when provided with a gold paragraph andknowing when to abstain from answering, ex-isting models easily outperform a human an-notator.
however, predicting answerability it-self remains challenging.
we manually an-notate 800 unanswerable examples across sixlanguages on what makes them challenging toanswer.
with this new data, we conduct per-category answerability prediction, revealing is-sues in the current dataset collection as well astask formulation.
together, our study pointsto avenues for future research in information-seeking question answering, both for datasetcreation and model development.1.
1.introduction.
addressing the information needs of users by an-swering their questions can serve a variety of prac-tical applications.
to answer such information-seeking queries – where users pose a question be-cause they do not know the answer – in an un-constrained setting is challenging for annotatorsas they have to exhaustively search over the web..1our code and annotated data is publicly avail-https://github.com/akariasai/.
ableatunanswerable_qa..to reduce annotator burden, the task has been sim-pliﬁed as reading comprehension: annotators aretasked with ﬁnding an answer in a single document.
recent pretrained language models surpassed es-timated human performance (liu et al., 2019; de-vlin et al., 2019) in many reading comprehensiondatasets such as squad (rajpurkar et al., 2016)and coqa (reddy et al., 2019), where questionsare posed with an answer in mind.
however, thosestate-of-the-art models have difﬁculty answeringinformation-seeking questions (kwiatkowski et al.,2019; choi et al., 2018)..in this work, we investigate what makesinformation-seeking question answering (qa)more challenging, focusing on the natural ques-tions (nq; kwiatkowski et al., 2019) and tydiqa (clark et al., 2020) datasets.
our experimen-tal results from four different models over six lan-guages on nq and tydi qa show that most oftheir headroom can be explained by two subprob-lems: selecting a paragraph that is relevant to aquestion and deciding whether the paragraph con-tains an answer.
the datasets are annotated at thedocument level, with dozens of paragraphs, andﬁnding the correct paragraph is nontrivial.
whenprovided with a gold paragraph and an answer type(i.e., if the question is answerable or not), the per-formance improves signiﬁcantly (up to 10% f1 innq), surpassing that of a single human annotator.
after identifying the importance of answerabil-ity prediction, in section 4, we compare a ques-tion only baseline, state-of-the-art qa models, andhuman agreement on this task.
for comparison,we also evaluate unanswerability prediction in areading comprehension dataset including unanswer-able questions (rajpurkar et al., 2018).
while alldatasets contain a large proportion of unanswerablequestions (33-59%), they differ in how easily mod-els can detect them.
this motivates us to furtherinvestigate the source of unanswerability..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1492–1504august1–6,2021.©2021associationforcomputationallinguistics1492to this end, we quantify the sources of unanswer-ability by annotating unanswerable questions fromnq and tydi qa; we ﬁrst classify unanswerablequestions into six categories and then further an-notate answers and alternative knowledge sourceswhen we can ﬁnd the answers to the unanswer-able questions.
despite the difﬁculty of annotatingquestions from the web and crowdsourcing bilin-gual speakers, we annotated 800 examples acrosssix typologically diverse languages.
our analysisshows that why questions are unanswerable dif-fers based on the dataset or language.
we conductper-category answerability prediction on those an-notated data, and found unanswerable questionsfrom some categories are particularly hard to beidentiﬁed.
we provide a detailed analysis for al-ternative sources of an answer beyond wikipedia.
grounded in our analysis, we suggest avenues forfuture research, both for dataset creation and modeldevelopment based on the analysis..our contributions are summarized as follows:.
• we provide in-depth analysis on information-seeking qa datasets, namely on natural ques-tions and tydi qa to identify the remainingheadrooms..• we show that answerability prediction andparagraph retrieval remain challenging evenfor state-of-the-art models through controlledexperiments using four different models..• we manually annotate reasons for unan-swerability for 800 examples across six lan-guages, and suggest potential improvementsfor dataset collections and task design..2 background and datasets.
we ﬁrst deﬁne the terminology used in this paper.
in this work, we focus on a reading comprehensionsetting, where reference documents (context) aregiven and thus retrieval is unnecessary, unlike openretrieval qa (chen et al., 2021)..information-seeking qa datasets contain ques-tions written by a human who wants to know theanswer but doesn’t know it yet.
in particular, nqis a collection of english google search enginequeries (anonymized) and tydi qa is a collectionof questions authored by native speakers of 11 lan-guages.
the answers are annotated post hoc byanother annotator, who selects a paragraph withsufﬁcient information to answer (long answer).
al-ternatively, the annotator can select “unanswerable”.
data.
% answerable.
%short un-ans.
long only.
nqtydi qasquad 2.0.
14.75.4-.
35.234.866.6.
50.159.933.4.avg.
# of p.131.341.11.0.table 1: answer type and paragraph number statisticsin three datasets’ train portions.
“avg.
# o p” de-notes the average number of the paragraphs includedin the reference context per question.
about half of thequestions are unanswerable (un-ans); the rest consistof questions with only paragraph-level answers (longonly) and additional span-level answers (short)..if there is no answer on the page, or if the infor-mation required to answer the question is spreadacross more than one paragraph.
if they have identi-ﬁed the long answer, then the annotators are taskedto choose the short answer, a span or set of spanswithin the chosen paragraph, if there is any.
ques-tions are collected independently from existing doc-uments, so those datasets tend to have limited lexi-cal overlap between questions and context, which isa common artifact in prior reading comprehensiondatasets (sugawara et al., 2018)..reading comprehension datasets such assquad (rajpurkar et al., 2016), by contrast, havebeen created by asking annotators to write ques-tion and answer pairs based on a single providedparagraph.
squad 2.0 (rajpurkar et al., 2018)includes unanswerable questions that are writtenby annotators who try to write confusing questionsbased on the single paragraph..as shown in table 1, while unanswerable ques-tions are very common in nq, tydi qa andsquad 2.0, there are some major differences be-tween the ﬁrst two datasets and the last: first, nqand tydi qa unanswerable questions arise natu-rally, while squad 2.0 unanswerable questionsare artiﬁcially created by annotators (e.g.
chang-ing an entity name).
prior work (kwiatkowskiet al., 2019) suggests that those questions can beidentiﬁed as such with little reasoning.
second,while nq or tydi qa models have to select theevidence paragraph (long answer) from dozens ofparagraphs, squad 2.0 provides a single refer-ence paragraph.
that lengthy context provided innq and tydi qa requires systems to select andfocus on relevant information to answer.
as ofjanuary 2021, the best models on nq or tydi qalag behind humans, while several models surpasshuman performance on squad and squad 2.0.2.
2https://rajpurkar.github.io/.
1493in the following sections, we focus on information-seeking qa datasets, investigating how to improvethe answer coverage of those questions that arecurrently labeled as unanswerable through severalcontrolled experiments and manual analysis..3 qa performances with gold answer.
type and gold paragraph.
we quantify how the two aforementioned sub-problems in information-seeking qa – decidinganswer type, also referred to as answer calibra-tions (kamath et al., 2020) or answerability pre-diction, and ﬁnding a paragraph containing theanswer – affect the ﬁnal qa performance.
weconduct oracle analysis on existing models giventwo pieces of key information: gold paragraphand gold type.
in the gold paragraph setting,we provide the long answer to limit the answerspace.
in the gold type setting, a model outputsthe ﬁnal answer following the gold answer typeti ∈ {short, long only, unanswerable},which correspond to the questions with short an-swers,3 questions with long answers only, and ques-tions without any answers, respectively.
this liftsthe burden of answer calibration from the model..3.1 comparison systems.
qa models.
for nq, we use rikinet (liu et al.,2020)4 and etc (ainslie et al., 2020).
these sys-tems are within 3% of the best-performing systemson the long answer and short answer predictiontasks as of january 2021. we use the originalmbert (devlin et al., 2019) baseline for tydiqa.
rikinet uses an answer type predictor whosepredicted scores are used as biases to the predictedlong and short answers.
etc and mbert jointlypredict short answer spans and answer types, fol-lowing alberti et al.
(2019)..human.
the nq authors provide upper-boundperformance by estimating the performance of asingle annotator (single), and one of the aggregatesof 25 annotators (super).
super-annotator perfor-mance is considered as an nq upper bound.
seecomplete distinction in kwiatkowski et al.
(2019)..squad-explorer/.
answer is also provided..we appreciate their help..3the short answer is found inside the long answer, so long.
4we contacted authors of rikinet for the prediction ﬁles..long answerp.r.f1.
short answerp.r.f1.
rikinetw/gold t.etcw/gold tw/gold pw/gold t&p.
74.385.2.
79.784.6--.
76.385.2.
72.284.6--.
75.285.2.
75.884.6--.
61.464.6.
67.562.567.968.9.
57.364.6.
49.962.557.767.6.
59.364.6.
57.462.562.468.3.human- single- super.
80.490.0.
67.684.6.
73.487.2.
63.479.1.
52.672.6.
57.575.7.table 2: oracle analysis on the dev set for nq.
“goldt” denotes gold type, and “gold p” denotes “goldparagraph”..long answerp.r.f1.
short answerp.r.f1.
mbertw/ gold t.annotator.
64.378.5.
84.4.
66.478.5.
74.5.
65.278.5.
79.9.
58.960.8.
70.8.
50.660.8.
62.4.
54.360.8.
70.1.table 3: oracle analysis on the dev set of tydi qa.
“gold t” denotes gold type..3.2 evaluation metrics.
the ﬁnal metric of nq is based on precision, recalland f1 among the examples where more than oneannotators select non-null answers and a modelpredicts a non-null answer (kwiatkowski et al.,2019), to prevent a model always outputting unan-swerable for achieving high scores..tydi qa evaluation is based on recall, precisionand byte-level f1 scores among the examples withanswer annotations.
the ﬁnal score is calculatedby taking a macro-average score of the results on11 target languages..3.3 results.
table 2 presents oracle analysis on nq.
havingaccess to gold answer type and gold paragraph is al-most equally crucial for short answer performanceon nq.
for long answers, we observe that the mod-els rank the paragraphs correctly but struggle todecide when to abstain from answering.
when thegold type is given, etc reaches 84.6 f1 for the longanswer task, which is only 2.6 points behind theupper bound, and signiﬁcantly outperforms singleannotator performance.
provided both gold para-graph and answer type (“gold t&p”), the model’sshort answer f1 score reaches 10% above that ofa single annotator, while slightly behind super hu-man performance.
for short answers, providinggold paragraph can improve etc’s performance.
1494by 5 points, gaining mostly in recall.
having thegold answer type information also signiﬁcantly im-proves recall at a small cost of precision..table 3 shows that a similar pattern holds intydi qa: answerability prediction is a remainingchallenge for tydi qa model.5 given the goldtype information, the long answer f1 score is only1.4 points below the human performance.
these re-sults suggest that our models performed well whenselecting plausible answers and would beneﬁt fromimproved answerability prediction..4 answerability prediction.
we ﬁrst quantitatively analyze how easy it is toestimate answerability from the question alone,and then we test the state-of-the-art models’ per-formance to see how well our complex modelsgiven question and the gold context perform onthis task.
we conduct the same experiments onsquad 2.0, to highlight the unique challenges ofthe information-seeking queries..each example consists of a question qi, alist of paragraphs of an evidence document di,and a list of answer annotations ai, whichtype ti ∈are aggregated into an answer{short, long, unanswerable}..4.1 models.
majority baseline.
we outputthe most fre-quent label for each dataset (i.e., short for nq,unanswerable for tydi qa and squad 2.0)..question only model (q only).
this modeltakes a question and classify it into one of three(i.e., short,long,unanswerable)classessolely based on the question input.
in particular,we use a bert-based classiﬁer: encode each inputquestion with bert, and use the [cls] token asthe summary representation to classify.
experimen-tal details can be found in the appendix..qa models.
we convert the state-of-the-art qamodels’ ﬁnal predictions into answer type predic-tions.
when a qa system outputs any short/longanswers, we map them to short / long type; oth-erwise we map them to unanswerable.
weuse etc for nq, and mbert baseline for tydiqa as in section 3.3. for squad 2.0, we useretro-reader (zhang et al., 2021).6 the evaluation.
5we do not experiment with gold p setting for tydi qa,.
as it’s included in the original paper (clark et al., 2020)..6we contacted authors of retro-reader for the prediction.
ﬁle.
we appreciate their help..model.
majorityq onlyqa model.
human- binary- aggregate.
nq (etc).
tydi.
3-way.
2-way.
3-way.
2-way.
50.965.572.0.
71.079.6.
58.972.782.5.
78.985.6.
58.269.874.2.
88.193.3.
58.270.279.4.
86.994.0.squad2-way.
50.063.094.1.
--.
answer.
table 4:classiﬁcation accuracy:long, short, none for three-way classiﬁcation andanswerable,unanswerable for two-way classiﬁcation..type.
script of nq and tydi qa calibrates the answertype for each question by thresholding long andshort answers respectively to optimize the f1 score.
we use the ﬁnal predictions after this calibrationprocess..human.
we compare the models’ performancewith two types of human performance: binary andaggregate.
“binary” evaluation computes pair-wiseagreements among all combinations of 5 annotatorsfor nq and 3 annotators for tydi qa.
“aggregate”evaluation compares each annotator’s label to themajority label selected by the annotators.
thisinﬂates human performance modestly as each an-notator’s own label contributes to the consensuslabel..4.2 results.
the results in table 4 indicate the different charac-teristics of the naturally occurring and artiﬁciallyannotated unanswerable questions.
question onlymodels yield over 70% accuracy in nq and tydiqa, showing there are clues in the question alone,as suggested in liu et al.
(2020).
while modelsoften outperform binary agreement score betweentwo annotators, the answer type prediction com-ponent of etc performs on par with the q onlymodel, suggesting that answerability calibrationhappens mainly at the f1 optimization processing..which unanswerable questions can be easilyidentiﬁed?
we randomly sample 50 nq exam-ples which both q only and etc successfully an-swered.
32% of them are obviously too vague orare not valid questions (e.g., “bye and bye going tosee the king by blind willie johnson”, “history of1st world war in bangla language”).
13% of theminclude keywords that are likely to make the ques-tions unanswerable (e.g., “which of the followingwould result in an snp?”).
14% of the questionsrequire complex reasoning, in particular, listing en-.
1495tities or ﬁnding a maximum / best one (e.g., “top 10air defense systems in the world”), which are oftenannotated as unanswerable in nq due to the difﬁ-culty of ﬁnding a single paragraph answering thequestions.
models, including the q only models,seem to easily recognize such questions..comparison with squad 2.0.in squad 2.0,somewhat surprisingly, the question only baselineachieved only 63% accuracy.
we hypothesizethat crowdworkers successfully generated unan-swerable questions that largely resemble answer-able questions, which prevents the question onlymodel from exploiting artifacts in question sur-face forms.
however, when the context was pro-vided, the qa model achieves almost 95% accu-racy, indicating that detecting unanswerability be-comes substantially easier when the correct contextis given.
yatskar (2019) ﬁnds the unanswerablequestions in squad 2.0 focus on simulating ques-tioner confusion (e.g., adding made-up entities, in-troducing contradicting facts, topic error), whichthe current state-of-the-art models can recognizewhen the short reference context is given.
by de-sign, these questions are clearly unanswerable, un-like information-seeking queries which can be par-tially answerable.
thus, identifying unanswerableinformation-seeking queries poses additional chal-lenges beyond matching questions and contexts..5 annotating unanswerability.
in this section, we conduct an in-depth analysisto answer the following questions: (i) where theunanswerability in information-seeking qa arises,(ii) whether we can answer those unanswerablequestions when we have access to more knowledgesources beyond a single provided wikipedia article,and (iii) what kinds of questions remain unanswer-able when these steps are taken.
to this end, weannotate 800 unanswerable questions from nq andtydi qa across six languages.
then, we conductper-category performance analysis to determine thetypes of questions for which our models fail topredict answerability..5.1 categories of unanswerable questions.
we ﬁrst deﬁne the categories of the unanswerablequestions.
retrieval miss includes questions thatare valid and answerable, but paired with a doc-ument which does not contain a single paragraphwhich can answer the question.
we subdivide this.
category into three categories based on the questiontypes: factoid, non-factoid, and multi-evidencequestions.
factoid questions are unanswerable dueto the failure of retrieving articles with answersavailable on the web.
these questions fall into twocategories: where the wikipedia documents includ-ing answers are not retrieved by google search, orwhere wikipedia does not contain articles answer-ing the questions so alternative knowledge sources(e.g., non-wikipedia articles) are necessary.
wealso ﬁnd a small number of examples whose an-swers cannot be found on the web even when weexhaustively searched dozens of web-pages.7 non-factoid questions cover complex queries whoseanswers are often longer than a single sentenceand no single paragraphs fully address the ques-tions.
lastly, multi-evidence questions requirereasoning over multiple facts such as multi-hopquestions (yang et al., 2018; dua et al., 2019).
aquestion is assigned this category only when theauthors need to combine information scattered intwo or more paragraphs or articles.
theoretically,the boundaries among the categories can overlap(i.e., there could be one paragraph that conciselyanswers the query, which we fail to retrieve), butin practice, we achieved a reasonable annotationagreement..invalid qa includes invalid questions, falsepremise and invalid answers.
invalid questionsare ill-deﬁned queries, where we can only vaguelyguess the questioner’s intent.
nq authors found14% of nq questions are marked as bad questions;here, we focus on the unanswerable subset of theoriginal data.
we regard queries with too much am-biguity or subjectivity to determine single answersas invalid questions (e.g., where is turkey com-modity largely produced in our country).
falsepremise (kim et al., 2021) are questions based onincorrect presuppositions.
for example, the ques-tion in table 5 is valid, but no harry potter moviewas released in 2008, as its sixth movie releasewas pushed back from 2008 to 2009 to boosterits release schedule.
invalid answers are annota-tion errors, where the annotator missed an answerexisting in the provided evidence document..5.2 manual study setting.
we randomly sampled and intensively annotated atotal of 450 unanswerable questions from the nq.
7such cases were more common in low resource lan-.
guages..1496type.
sub-type.
query.
wiki page title answer.
retrieval miss.
factoid question (fact).
non-factoid question (non-f).
multi-evidence question (multi).
when is this is us season 2 released ondvdwhat is the difference between a bernesemountain dog and a swiss mountain doghow many states in india have at leastone international border.
invalid qa.
invalid questions (q.).
the judds love can build a bridge album love.
this is us (sea-son 2)bernese moun-tain dogborder.
canbuild a bridge(album)harry(ﬁlm series)independenceday(1996ﬁlm).
potter.
september11, 2018-.
-.
vivica a.fox.
false premise (false).
invalid answers (ans.).
what harry potter movie came out in2008who played will smith’s girlfriend inindependence day.
table 5: types of unanswerable questions and their examples in nq..development set, and 350 unanswerable questionsacross ﬁve languages from the tydi qa develop-ment set.
here, we sample questions where annota-tors unanimously agreed that no answer exists.
seetable 6 for the statistics.
for nq, the authors ofthis paper annotated 100 examples and adjudicatedthe annotations to clarify common confusions.
theremaining 350 questions were annotated individu-ally.
before the adjudication, the annotators agreedon roughly 70% of the questions.
after this adju-dication process, the agreements on new samplesreached over 90%..for tydi qa, we recruit ﬁve native speakersto annotate examples in bengali, japanese, ko-rean, russian, and telugu.
we provide detailedinstructions given the adjudication process, andclosely communicate with each annotator whenthey experienced difﬁculty deciding among multi-ple categories.
similar to nq annotation, annota-tors searched the answers using google search, inboth the target language and english, referring toany web pages (not limited to wikipedia) and re-annotated the answer, while classifying questionsinto the categories described earlier..5.3 results.
causes of unanswerability.
table 6 summarizesour manual analysis.
we found different patternsof unanswerability in the two datasets.
invalid an-swers were relatively rare in both, which showsthey are high quality.
we observe that invalidanswers are more common for questions whereannotators need to skim through large referencedocuments.
in nq, where the questions are natu-rally collected from user queries, ill-deﬁned querieswere prevalent (such queries account for 14% ofthe whole nq data, but 38% of the unanswerable.
% retrieval miss.
n fact non-f multi.
% invalidfalse.
ans..nq 450.bnjakorute.
501001005050.
25.
6861575074.
20.
011862.q..38.
4214814.
6.
41520320.
3.
64000.
8.
1871412.table 6: the manual classiﬁcation results based on theunanswerable question categories (table 5) on n exam-ples per row.
the bottom ﬁve rows represent tydi qabengali, japanese, korean, russian and telugu, respec-tively in order..subset).
in tydi qa, document retrieval was a ma-jor issue across all ﬁve languages (50-74%), and asigniﬁcantly larger proportion of re-annotated an-swers were found in other wikipedia pages (50%in tydi qa v.s.
21.8% in nq), indicating that theretrieval system used for document selection mademore mistakes.
document retrieval is a crucial partof qa, not just for modeling but also for datasetconstruction.
we observe more complex and chal-lenging questions in some tydi qa languages;20% of the unanswerable questions in korean and32% of the unanswerable questions in russian re-quire multiple paragraphs to answer, as opposed to6% in nq..alternative knowledge sources.
table 7 showsthe breakdown of the newly annotated answersources for the “retrieval miss (factoid)” questions.
as mentioned above, in tydi qa new answers arefound in other wikipedia pages (66.7% of retrievalmiss in japanese subset, 55.6% in korean subsetand 34.8% in russian), while in nq, the majorityof the answers are from non-wikipedia websites,which indicates that using wikipedia as the single.
1497dataset.
total.
ib / tab.
number (%)diff.
wiki.
nqbnjakorute.
1194060542323.
3 (2.5)9 (22.5)10 (16.7)13 (24.1)10 (43.4)4 (17.4).
26 (21.8)27 (67.5)40 (66.7)30 (55.6)8 (34.8)5 (21.8).
non-wiki.
119 (75.6)4 (10.0)10 (16.7)11 (20.3)5 (21.7)14 (60.9).
table 7: the knowledge sources for retrieval miss ques-tions in nq and tydi bengali, japanese, korean, rus-sian and telugu annotation.
the bottom ﬁve rows repre-sent tydi qa bengali, japanese, korean, russian andtelugu, respectively.
“ib / tab” denotes infobox or tablein the same wikipedia pages, “diff.
wiki” denotes dif-ferent wikipedia pages, and “non-wiki” denotes non-wikipedia webpages..knowledge source hurts the coverage of answerabil-ity.
table 8 shows retrieval miss (factoid) questionsin tydi japanese, korean and russian subsets.
inthe ﬁrst example, the retrieved document is abouta voice actor who has acted on a character namedvincent.
yet, japanese wikipedia has an articleabout vince lombardi, and we could ﬁnd the cor-rect answer “57” there.
the second group showstwo examples where we cannot have wikipediaarticles with sufﬁcient information to answer butcan ﬁnd non-wikipedia articles on the web.
forexample, we cannot ﬁnd useful korean wikipediaarticles for a question about pokemon, but a non-wikipedia pokemon fandom page clearly answersthis question.
this is also prevalent in nq.
we pro-vide a list of the alternative web articles sampledfrom the retrieval misses (factoid) cases of nq intable 11 in the appendix..for the tydi qa dataset, answers were some-times found in tables or infoboxes of providedwikipedia documents.
this is because tydi qaremoves non-paragraph elements (e.g., table, list,infobox) to focus on the modeling challenges ofmultilingual text (clark et al., 2020).
wikidata alsoprovides an alternative source of information, cov-ering roughly 15% of queries.
these results showthe potential of searching heterogeneous knowl-edge sources (chen et al., 2020b; oguz et al., 2020)to increase answer coverage.
alternatively, asaiet al.
(2021) show that searching documents inanother language signiﬁcantly increases the an-swer coverage of the questions particularly in low-resource languages.
lastly, a non-negligible num-ber of telugu and bengali questions cannot be an-swered even after an extensive search over multiple.
documents due to the lack of information on theweb.
a bengali question asks “who is the fatherof famous space researcher abdus sattar khan (abangladeshi scientist)?”, and our annotator couldnot ﬁnd any supporting documents for this ques-tion..limitations of the current task designs.
ta-ble 9 shows non-factoid or multi-evidence ques-tions from tydi qa, which are marked as unan-swerable partially due to the task formulation – an-swers have to be extracted from a single paragraphbased on the information provided in the evidencedocument.
on the ﬁrst three examples of non-factoid questions, we have found that to completelyanswer the questions, we need to combine evidencefrom multiple paragraphs and to write descriptiveanswers.
the second group shows several exam-ples for multi-evidence questions.
although theyare not typical compositional questions in multi-hop qa datasets (yang et al., 2018), it requirescomparison across several entities..5.4 per-category performance.
how challenging is it to detect unanswerablity fromdifferent causes?
table 10 shows the per-categoryperformance of answerability prediction using themodels from section 4. both q only and qa mod-els show the lowest error rate on invalid questionson nq, suggesting that those questions can be eas-ily predicted as unanswerable, even from the ques-tion surface only.
unsurprisingly, all models strug-gle on the invalid answer category.
we found thatin some of those cases, our model ﬁnds the cor-rect answers but is penalized.
detecting factoidquestions’ unanswerability is harder when refer-ence documents are incorrect but look relevant dueto some lexical overlap to the questions.
for ex-ample, given a question “who sang the song angelof my life” and the paired document saying “mylife is a song by billy joel that ﬁrst appeared onhis 1978”, which is about a different song, our qamodel extracts billy joel as the answer with a highconﬁdence score.
this shows that even the state-of-the-art models can be fooled by lexical overlap..5.5 discussion.
we summarize directions for future work from themanual analysis.
first, going beyond wikipediaas the only source of information is effective to in-crease the answer coverage.
many of the unanswer-able questions in nq or tydi qa can be answered.
1498sub-type.
example.
query.
original wiki title.
new article.
answer.
different wikipedia ヴ ィ ン セ ン ト ・ ト マ ス ・ ロ ン バル デ ィ は 何 歳 で 死 去 し た ？ (atwhat age did vincent thomas lom-bardy die?)
skol~ko marok bylo vypuweno vsssr v 1938?
(how many stampswere produced in the ussr in 1938?).
not wikipedia.
포켓몬스터에서 가장 큰 포켓몬은무엇인가?
(what’s the largest poke-mon in pokemonster?)
日本で平均的に車が買い替えられる頻度は？ (how often do people buya new car on average in japan?).
森 川 智 之(toshiyukimorikawa).
ヴ ィ ン ス ・ ロ ンバ ル デ ィ (vincelombardi).
poqtovye markisssr (postagestampstheussr).
of.
하 야 시 바 라 메구미 (hayashibaramegumi)モ ー タ リ ゼ ーション (effects ofthe car on societies).
znaki poqtovo(cid:26)oplaty sssr(1938) (signs ofthe postage of theussr (1938))toppokémon.
10 largest.
2017年 度 乗 用 車市場動向調査 (fy2017 private carmarket survey).
57.
97.onix.
7.0.table 8: examples of retrieval miss (factoid) questions in tydi japanese, korean and russian subsets.
englishtranslations annotated by native speakers are written in the parentheses..sub-type.
non-factoidquestion.
multi-evidencequestion.
example.
query공리주의는 영국에 어떤 영향을 미쳤는가?
(how did utilitarian-ism affect uk?)
poqemu nado pod(cid:25)igat~ absent?
(why should you lit absintheon ﬁre?)
スペースシャトルと宇宙船の違いは何？ (what is the differ-ence between a space shuttle and a spaceship?)
닥터 후 시리즈 중 가장 높은 시청률을 기록한 시리즈는 무엇인가?
(which doctor who series scored the highest view rate?).
進化論裁判はアメリカ以外で起きたことはある？ (has anylegal case about creation and evolution in public education everhappened outside of the us?).
wiki page title제러미 벤담 (jeremy bentham).
absent (absinthe).
宇宙船 (space ship).
코드 블루 -닥터헬기긴급구명-(code blue (tv series)).
進化論 (darwinism).
table 9: examples of non-factoid and multi-evidence questions in tydi japanese, korean and russian subsets..category q only.
qa # ex.
q only.
nq.
24.122.918.57.214.348.6.
33.916.927.58.542.847.2.tydi (mbert)# ex..21.217.417.020.614.332.0.
212235329725.
11287291651435.factnon-fmultiq.falseans..table 10: per-category answerablity prediction errorrates.
the categories correspond to the six categoriesin table 5 and ‘# ex’ column represents the number ofexamples in each category..if we use non-wikipedia web pages (e.g., imdb)or structured knowledge bases (e.g., wikidata).
al-ternative web pages where we have found answershave diverse formats and writing styles.
searchingthose documents to answer information-seekingqa may introduce additional modeling challengessuch as domain adaptation or generalization.
to ourknowledge, there is no existing large-scale dataset.
addressing this topic.
although there are severalnew reading comprehension datasets focusing onreasoning across multiple modalities (talmor et al.,2021; hannan et al., 2020), limited prior work inte-grate heterogeneous knowledge sources for open-domain or information-seeking qa (oguz et al.,2020; chen et al., 2021)..invalid or ambiguous queries are common ininformation-seeking qa, where questions are of-ten under-speciﬁed.
we observed there are manyambiguous questions included in nq data.
consis-tent with the ﬁndings of min et al.
(2020), we havefound that many of the ambiguous questions or ill-posed questions can be ﬁxed by small edits, and wesuggest asking annotators to edit those questionsor asking them a follow-up clariﬁcation instead ofsimply marking and leaving the questions as is inthe future information-seeking qa dataset creation..lastly, we argue that the common task formula-tion, extracting a span or a paragraph from a single.
1499document, limits answer coverage.
to further im-prove, models should be allowed to generate theanswer based on the evidence document (lewiset al., 2020), instead of limiting to selecting a sin-gle span in the document.
evaluating the correct-ness of free-form answers is more challenging, andrequires further research (chen et al., 2020a)..while all the individual pieces might be revealedin independent studies (min et al., 2020; oguz et al.,2020), our study quantiﬁes how much each factoraccounts for reducing answer coverage..artifacts in datasets.
recent work (gururanganet al., 2018; kaushik and lipton, 2018; sugawaraet al., 2018; chen and durrett, 2019) exhibitedthat models can capture annotation bias in crowd-sourced data effectively, achieving high perfor-mance when only provided with a partial input.
although nq and tydi qa attempt to avoid suchtypical artifacts of qa data by annotating questionsindependently from the existing documents (clarket al., 2020), we found artifacts in question surfaceforms can let models easily predict answerabilitywith a partial input (i.e., question only)..6 related work.
7 conclusion.
analyzing unanswerable questions.
there isprior work that seeks to understand unanswerabil-ity in reading comprehension datasets.
yatskar(2019) analyzes unanswerable questions in squad2.0 and two conversational reading comprehensiondatasets, namely coqa and quac, while we fo-cus on information-seeking qa datasets to under-stand the potential dataset collection improvementsand quantify the modeling challenges of the state-of-the-art qa models.
ravichander et al.
(2019)compare unanswerable factors between nq anda qa dataset on privacy policies.
this work pri-marily focuses on a privacy qa, which leads todifferences of the categorizations of the unanswer-able questions.
we search alternative knowledgesources as well as the answers to understand howwe could improve answer coverage from datasetcreation perspective and connect the annotation re-sults with answerability prediction experiments formodeling improvements..answer calibrations.
answerability predictioncan bring practical values, when errors are expen-sive but abstaining from it is less so (kamath et al.,2020).
while predicting answerability has beenstudied in squad 2.0 (zhang et al., 2021; huet al., 2019), the unanswerability in squad 2.0has different characteristics from unanswerabilityin information-seeking qa as we discussed above.
to handle unanswerable questions in information-seeking qa, models either adopt threshold basedanswerable veriﬁcation (devlin et al., 2019), or in-troduce an extra layer to classify unanswerablityand training the model jointly (zhang et al., 2020;yang et al., 2019).
kamath et al.
(2020) observesthe difﬁculty of answer calibrations, especially un-der domain shift..we provide the ﬁrstin-depth analysis oninformation-seeking qa datasets to inspect whereunanswerability arises and quantify the remainingmodeling challenges.
our controlled experimentsidentiﬁes two remaining headrooms, answerabilityprediction and paragraph selection.
observing alarge percentage of questions are unanswerable, weprovide manual analysis studying why questionsare unanswerable and make suggestions to improveanswer coverage: (1) going beyond wikipedia tex-tual information as the only source of information,(2) addressing ambiguous queries instead of sim-ply marking and leaving the questions as is, (3)enable accessing multiple documents and introduc-ing abstractive answers for non-factoid questions.
together, our work shed light on future work forinformation-seeking qa, both for modeling anddataset design..legal and ethical considerations.
all of the manual annotations conducted by the au-thors of the papers and our collaborators.
the nqand tydi qa data is publicly available and furtheranalysis built upon on them is indeed encouraged.
this work would encourage future dataset creationand model development for information-seekingqa towards building a qa model that could workwell on users’ actual queries..acknowledgments.
we thank jon clark, michael collins, kentonlee, tom kwiatkowski, jennimaria palomaki, se-won min, colin lockard, david wadden, yizhongwang for helpful feedback and discussion.
wethank vitaly nikolaev for helping with the russiandata annotation, trina chatterjee for help with ben-gali data annotation, and for aditya kusupati for.
1500telegu data annotation.
we also thank the authorsof rikinet, retro-reader and etc for their coop-eration on analyzing their system outputs.
we aregrateful for the feedback and suggestions from theanonymous reviewers.
this research was supportedby gifts from google and the nakajima foundationfellowship..references.
joshua ainslie, santiago ontañón, chris alberti, philippham, anirudh reddy ravula, and sumit sanghai.
2020. etc: encoding long and structured data intransformers.
in emnlp..chris alberti, kenton lee, and michael collins.
2019.arxiv.
a bert baseline for the natural questions.
preprint arxiv:1901.08634..akari asai, jungo kasai, jonathan h. clark, kentonlee, eunsol choi, and hannaneh hajishirzi.
2021.xor qa: cross-lingual open-retrieval question an-swering.
in naacl-hlt..anthony chen, gabriel stanovsky, s. singh, and mattgardner.
2020a.
mocha: a dataset for trainingand evaluating generative reading comprehensionmetrics.
in emnlp..jifan chen and greg durrett.
2019. understandingdataset design choices for multi-hop reasoning.
innaacl-hlt..wenhu chen, ming-wei chang, eva schlinger,william yang wang, and william w. cohen.
2021.open question answering over tables and text.
iniclr..wenhu chen, hanwen zha, zhiyu chen, wenhanxiong, hong wang, and william yang wang.
2020b.
hybridqa: a dataset of multi-hop question answer-in findings ofing over tabular and textual data.
emnlp..eunsol choi, he he, mohit iyyer, mark yatskar, wen-tau yih, yejin choi, percy liang, and luke zettle-moyer.
2018. quac: question answering in context.
in acl..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
tacl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl-hlt..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in naacl-hlt..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel r. bowman, andnoah a. smith.
2018. annotation artifacts in nat-ural language inference data.
in naacl-hlt..darryl hannan, akshay jain, and mohit bansal.
2020.manymodalqa: modality disambiguation and qaover diverse inputs.
in aaai..1501minghao hu, furu wei, yuxing peng, zhen huang,nan yang, and dongsheng li.
2019. read+ verify:machine reading comprehension with unanswerablequestions.
in aaai..abhilasha ravichander, alan w black, shomir wilson,thomas norton, and norman sadeh.
2019. ques-tion answering for privacy policies: combining com-putational and legal perspectives.
in emnlp..amita kamath, robin jia, and percy liang.
2020. se-inlective question answering under domain shift.
acl..siva reddy, danqi chen, and christopher d. manning.
2019. coqa: a conversational question answeringchallenge.
tacl..saku sugawara, kentaro inui, satoshi sekine, andakiko aizawa.
2018. what makes reading compre-hension questions easier?
in emnlp..alon talmor, ori yoran, amnon catav, dan lahav,yizhong wang, akari asai, gabriel ilharco, han-naneh hajishirzi, and jonathan berant.
2021. mul-timodalqa: complex question answering over text,tables and images.
in iclr..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in emnlp (system demonstrations)..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in neurips..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018. hotpotqa: a dataset fordiverse, explainable multi-hop question answering.
in emnlp..mark yatskar.
2019. a qualitative comparison ofcoqa, squad 2.0 and quac.
in naacl-hlt..zhuosheng zhang, yuwei wu, junru zhou, sufengduan, and hai zhao.
2020. sg-net: syntax-guidedmachine reading comprehension.
in aaai..zhuosheng zhang, junjie yang, and hai zhao.
2021.retrospective reader for machine reading compre-hension.
in aaai..divyansh kaushik and zachary c. lipton.
2018. howmuch reading does reading comprehension require?
a critical investigation of popular benchmarks.
inemnlp..najoung kim, ellie pavlick, burcu karagol ayan,and d. ramachandran.
2021. which linguist in-vented the lightbulb?
presupposition veriﬁcation forquestion-answering.
arxiv, abs/2101.00391..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris alberti,danielle epstein, illia polosukhin, matthew kelcey,jacob devlin, kenton lee, kristina n. toutanova,llion jones, ming-wei chang, andrew dai, jakobuszkoreit, quoc le, and slav petrov.
2019. natu-ral questions: a benchmark for question answeringresearch.
tacl..patrick lewis, ethan perez, aleksandara piktus,f. petroni, v. karpukhin, naman goyal, heinrichkuttler, m. lewis, wen tau yih, tim rocktäschel,sebastian riedel, and douwe kiela.
2020. retrieval-augmented generation for knowledge-intensive nlptasks.
in neurips..dayiheng liu, yeyun gong, jie fu, yu yan, jiushengchen, daxin jiang, jiancheng lv, and nan duan.
2020. rikinet: reading wikipedia pages for natu-ral question answering.
in acl..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv, abs/1907.11692..sewon min, julian michael, hannaneh hajishirzi, andluke zettlemoyer.
2020. ambigqa: answering am-biguous open-domain questions.
in emnlp..barlas oguz, xilun chen, vladimir karpukhin,stan peshterliev, dmytro okhonko, michaelschlichtkrull, sonal gupta, yashar mehdad, andscott yih.
2020.uniﬁed open-domain ques-tion answering with structured and unstructuredknowledge.
arxiv preprint arxiv:2012.14610..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-tions for squad.
in acl..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100, 000+ questions formachine comprehension of text.
in emnlp..15025e-5.
we set the maximum total input sequencelength to 128. we train our model with a singlegeforce rtx 2080 with 12 gb memory forthree epochs, which roughly takes around 15minutes, 30 minutes and 45 minutes for eachepochs on squad 2.0, tydi and nq, respectively.
the hyperparameters are manually searched byauthors, and we use the same hyperparametersacross datasets that perform best on nq q-onlyexperiments..c additional annotation results.
c.1 examples of alternative web pages for.
nq retrieval miss (factoid).
table 11 shows several examples of alternative webpages where we could ﬁnd answers to originallyunanswerable questions.
although those additionalknowledge sources are highly useful, they are di-verse (from a fandom site to a shopping web site),and all have different formats and writing styles..c.2 examples of retrieval misses without any.
alternative knowledge sources.
table 12 shows the examples where we cannotﬁnd any alternative knowledge sources on the web.
those questions often ask some entities who arenot widely known but are closely related to cer-tain culture or community (e.g., a japanese athlete,geography of an indian village)..a annotation instruction.
the authors annotated examples in the followingprocess..• (step 1) translate the query, if not in english..• (step 2) decide whether the query is valid →if not, mark as (5) annotation error (questionis ambiguous or unclear), if not, go to step 3..• (step 3) if the query is valid, look at the linkeddocument.
if the answer is in the document,write down the answer in the “answer” col-umn of the spreadsheet, mark it as (4) invalidqa.
the corner case here is if the answer isin the infobox, according to tydi deﬁnitionit won’t work.
so in this case mark as (1)retrieval error (factoid question) and labelas "type of missing information: no descrip-tion in paragraphs, but can be answered basedon infobox or table".
if you cannot ﬁnd theanswer in the document, go to step 4..• (step 4) if the answer is not in the document,google question to ﬁnd an answer.
- if there’sa factoid answer found, mark it as (1) re-trieval error (factoid question) and copy-pastethe answer.
mark the source of the answer –whether from other wikipedia page, or in en-glish wikipedia, or in the web.
if the answeris non factoid and can be found, mark it as(2) retrieval error (non-factoid question), andcopy paste a link where the answer.
mark thesource of the answer – whether from anotherwikipedia page, or in the web.
- if the ques-tion is very complex and basically you can’tﬁnd an answer, mark it as (3) retrieval error(complex question)..b experimental details of question only.
baseline.
our implementations are all based on pytorch.
to implement our classiﬁca-in particular,tion based and span-based model, we usepytorch-transformersal.,2020).8we use bert-base-uncasedmodelsquad,nqforbert-base-multilingual-uncasedfor tydi as initial pre-trained models.
the trainingbatch size is set to 8, the learning rate is set to.
(wolf.
and.
et.
8https://github.com/huggingface/.
transformers.
1503query.
new article.
paragraph.
when is fairy tail ep278 coming outwhere wasamericanstory cult ﬁlmed.
thehorror.
what are the maintypes of meat eatenin the ukaround the world in80 days book pages.
fairy tail wiki at: https://fairytail.fandom.com/american horror story “cult”fillming locations at: https://hollywoodfilminglocations.
com/a roundup of the most popularmeats eaten in the uk at: https://newyorkstreetfood.com/around the world in 80 days pa-perback – november 6, 2018 at:https://www.amazon.com/.
information japan air date october 7,2018the horror show “american horror storycult” starring sarah paulson &ryan mur-phy was ﬁlmed on location throughoutsouthern california and michigan.
beef (33% out of 94% consider beef astheir top choice): beef is the most pre-ferred choice among british peoplepublisher : createspace independent pub-lishing platform (november 6, 2018) lan-guage : english paperback : 130 pages.
answer.
october 7,2018southerncalifor-niamichiganbeef.
and.
130 pages.
table 11: examples of the alternative websites we could ﬁnd answers to the retrieval miss (factoid) questions fromnatural questions..dataset (language).
query.
tydi (telugu)tydi (telugu)tydi (japanese).
nq (english)nq (english).
what is the main agricultural crop in onuru village (a village in india)?
as of 2002, what is the biggest construction in tenali town (a city of india)?
what is yuta shitara (a japanese long-distance runner.
)’s best record for 10000meters?
how many blocks does hassan whiteside have in his careerwho migrated to the sahara savanna in present-day southeastern nigeria.
table 12: examples of questions we cannot ﬁnd any web resources including answers..1504