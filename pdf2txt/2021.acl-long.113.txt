agggen: ordering and aggregating while generating.
xinnuo xu†, ondˇrej duˇsek‡, verena rieser† and ioannis konstas††the interaction lab, macs, heriot-watt university, edinburgh, uk‡charles university, faculty of mathematics and physics, prague, czechiaxx6, v.t.rieser, i.konstas@hw.ac.ukodusek@ufal.mff.cuni.cz.
abstract.
we present agggen (pronounced ‘again’), adata-to-text model which re-introduces two ex-plicit sentence planning stages into neural data-to-text systems: input ordering and input ag-gregation.
in contrast to previous work us-ing sentence planning, our model is still end-to-end: agggen performs sentence planningat the same time as generating text by learn-ing latent alignments (via semantic facts) be-tween input representation and target text.
ex-periments on the webnlg and e2e challengedata show that by using fact-based alignmentsour approach is more interpretable, expressive,robust to noise, and easier to control, whileretaining the advantages of end-to-end sys-tems in terms of ﬂuency.
our code is avail-able at https://github.com/xinnuoxu/agggen..1.introduction.
recent neural data-to-text systems generate text“end-to-end” (e2e) by learning an implicit mappingbetween input representations (e.g.
rdf triples)and target texts.
while this can lead to increasedﬂuency, e2e methods often produce repetitions,hallucination and/or omission of important con-tent for data-to-text (duˇsek et al., 2020) as wellas other natural language generation (nlg) tasks(cao et al., 2018; rohrbach et al., 2018).
tradi-tional nlg systems, on the other hand, tightlycontrol which content gets generated, as well as itsordering and aggregation.
this process is calledsentence planning (reiter and dale, 2000; duboueand mckeown, 2001, 2002; konstas and lapata,2013; gatt and krahmer, 2018).
figure 1 showstwo different ways to arrange and combine the rep-resentations in the input, resulting in widely differ-ent generated target texts..in this work, we combine advances of bothparadigms into a single system by reintroducing.
figure 1: two different sentence plans with their cor-responding generated target texts from our model onthe webnlg dataset.
planning and generation is per-formed jointly.
the dashed line denotes aggregation..sentence planning into neural architectures.
wecall our system agggen (pronounced ‘again’).
agggen jointly learns to generate and plan at thesame time.
crucially, our sentence plans are in-terpretable latent states using semantic facts1 (ob-tained via semantic role labelling (srl)) thatalign the target text with parts of the input repre-sentation.
in contrast, the plan used in other neuralplan-based approaches is usually limited in termsof its interpretability, control, and expressivity.
forexample, in (moryossef et al., 2019b; zhao et al.,2020) the sentence plan is created independently,incurring error propagation; wiseman et al.
(2018)use latent segmentation that limits interpretability;shao et al.
(2019) sample from a latent variable, notallowing for explicit control; and shen et al.
(2020)aggregate multiple input representations which lim-its expressiveness..agggen explicitly models the two planningprocesses (ordering and aggregation), but can di-rectly inﬂuence the resulting plan and generated.
1each fact roughly captures “who did what to whom”..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1419–1434august1–6,2021.©2021associationforcomputationallinguistics1419william anders  dateofretirement  1969-09-01apollo 8   commander   frank bormanwilliam anders  member_of   apollo 8apollo 8  backup_pilot  buzz aldrinapollo 8   operator   nasainput dbpedia tripleswilliam anders served as a crew member on apollo 8 operated by nasa.
the backup pilotwas buzz aldrin.
frank borman was also an apollo 8 commander.
william anders retiredon september 1st, 1969.william anders retired on 1969-09-01. he was a crew member of nasa 's apollo 8. frankborman was also a commander with buzz aldrin as the backup pilot.operatorbackup_pilotcommanderdateofretirementdateofretirementoperatorbackup_pilotcommanderdateofretirementcommandermember_ofbackup_pilotoperatormember_ofmember_ofwilliam anders, who retired onseptember 1st, 1969, was a crewmember on apollo 8 and servedunder commander frank borman.apollo 8 was operated by nasawith buzz aldrin as backup pilot.human-authored textsentence plan 1:generated target text 1:sentence plan 2:generated target text 2:target text, using a separate inference algorithmbased on dynamic programming.
crucially, this en-ables us to directly evaluate and inspect the model’splanning and alignment performance by comparingto manually aligned reference texts..we demonstrate this for two data-to-text gener-ation tasks: the e2e nlg (novikova et al., 2017)and the webnlg challenge (gardent et al., 2017a).
we work with a triple-based semantic representa-tion where a triple consists of a subject, a predicateand an object.2 for instance, in the last triple infigure 1, apollo 8, operator and nasa are the sub-ject, predicate and object respectively.
our contri-butions are as follows:.
• we present a novel interpretable architecturefor jointly learning to plan and generate based onmodelling ordering and aggregation by aligningfacts in the target text to input representations withan hmm and transformer encoder-decoder..• we show that our method generates outputwith higher factual correctness than vanilla encoder-decoder models without semantic information..• we also introduce an intrinsic evaluationframework for inspecting sentence planning with arigorous human evaluation procedure to assess fac-tual correctness in terms of alignment, aggregationand ordering performance..2 related work.
factual correctness is one of the main issues fordata-to-text generation: how to generate text ac-cording to the facts speciﬁed in the input tripleswithout adding, deleting or replacing information?
the prevailing sequence-to-sequence (seq2seq)architectures typically address this issue via rerank-ing (wen et al., 2015a; duˇsek and jurˇc´ıˇcek, 2016;juraska et al., 2018) or some sophisticated trainingtechniques (nie et al., 2019; kedzie and mckeown,2019; qader et al., 2019).
for applications wherestructured inputs are present, neural graph encoders(marcheggiani and perez-beltrachini, 2018; raoet al., 2019; gao et al., 2020) or decoding of ex-plicit graph references (logan et al., 2019) are ap-plied for higher accuracy.
recently, large-scalepretraining has achieved sota results on webnlgby ﬁne-tuning t5 (kale and rastogi, 2020)..several works aim to improve accuracy and con-trollability by dividing the end-to-end architec-ture into sentence planning and surface realisation..2note that e2e nlg data and other input semantic repre-.
sentations can be converted into triples, see section 4.1..castro ferreira et al.
(2019) feature a pipeline withmultiple planning stages and elder et al.
(2019)introduce a symbolic intermediate representationin multi-stage neural generation.
moryossef et al.
(2019b,a) use pattern matching to approximate therequired planning annotation (entity mentions, theirorder and sentence splits).
zhao et al.
(2020) use aplanning stage in a graph-based model – the graphis ﬁrst reordered into a plan; the decoder conditionson both the input graph encoder and the linearizedplan.
similarly, fan et al.
(2019) use a pipeline ap-proach for story generation via srl-based sketches.
however, all of these pipeline-based approacheseither require additional manual annotation or de-pend on a parser for the intermediate steps..other works, in contrast, learn planning and re-alisation jointly.
for example, su et al.
(2018) in-troduce a hierarchical decoding model generatingdifferent parts of speech at different levels, whileﬁlling in slots between previously generated to-kens.
puduppully et al.
(2019) include a jointlytrained content selection and ordering module thatis applied before the main text generation step.themodel is trained by maximizing the log-likelihoodof the gold content plan and the gold output text.
liand rush (2020) utilize posterior regularization ina structured variational framework to induce whichinput items are being described by each token of thegenerated text.
wiseman et al.
(2018) aim for bettersemantic control by using a hidden semi-markovmodel (hsmm) for splitting target sentences intoshort phrases corresponding to “templates”, whichare then concatenated to produce the outputs.
how-ever it trades the controllability for ﬂuency.
simi-larly, shen et al.
(2020) explicitly segment targettext into fragment units, while aligning them withtheir corresponding input.
shao et al.
(2019) use ahierarchical variational model to aggregate inputitems into a sequence of local latent variables andrealize sentences conditioned on the aggregations.
the aggregation strategy is controlled by samplingfrom a global latent variable..in contrast to these previous works, we achieveinput ordering and aggregation, input-output align-ment and text generation control via interpretablestates, while preserving ﬂuency..3.joint planning and generation.
we jointly learn to generate and plan by aligningfacts in the target text with parts of the input repre-sentation.
we model this alignment using a hidden.
1420markov model (hmm) that follows a hierarchicalstructure comprising two sets of latent states, corre-sponding to ordering and aggregation.
the modelis trained end-to-end and all intermediate steps arelearned in a uniﬁed framework..3.1 model overview.
let x = {x1, x2, .
.
.
, xj } be a collection of j in-put triples and y their natural language description(human written target text).
we ﬁrst segment yinto a sequence of t facts y1:t = y1, y2, .
.
.
, yt ,where each fact roughly captures “who did what towhom” in one event.
we follow the approach ofxu et al.
(2020), where facts correspond to predi-cates and their arguments as identiﬁed by srl (seeappendix b for more details).
for example:.
t , .
.
.
, yntt.each fact yt consists of a sequence of tokensy1t , y2.
unlike the text itself, the plan-ning information, i.e.
input aggregation and order-ing, is not directly observable due to the absenceof labelled datasets.
agggen therefore utilisesan hmm probabilistic model which assumes thatthere is an underlying hidden process that can bemodeled by a ﬁrst-order markov chain.
at eachtime step, a latent variable (in our case input triples)is responsible for emitting an observed variable (inour case a fact text segment).
the hmm speciﬁesa joint distribution on the observations and the la-tent variables.
here, a latent state zt emits a factyt, representing the group of input triples that isverbalized in yt.
we write the joint likelihood as:.
figure 2: the structure of our model.
zt, zt−1, yt,and yt−1 represent the basic hmm structure, where zt,zt−1 are latent states and yt, yt−1 are observations.
in-side the dashed frames is the corresponding structurefor each latent state zt, which is a sequence of latentvariables oltrepresenting the predicates that emit thetobservation.
for example, at time step t − 1 two in-put triples (‘member of’ and ‘operator’) are verbalizedin the observed fact yt−1, whose predicates are repre-sented as latent variables o1(t−1).
t1–4 rep-resent transitions introduced in section 3.2..(t−1) and o2.
let ol.
t ∈ q = {1, .
.
.
, k} be a set of possiblelatent variables, then klt is the size of the searchspace for zt.
if olt maps to unique triples, the searchspace becomes intractable for a large value of k.to make the problem tractable, we decrease kby representing triples by their predicate.
q thusstands for the collection of all predicates appearingin the corpus.
to reduce the search space for ztfurther, we limit lt < l, where l = 3.
3.p (z1:t , y1:t | x) = p (z1:t | x) p (y1:t | z1:t , x).
=.
p (z1 | x).
p (zt | zt−1, x).
p (yt | zt, x).
..(cid:34).
t(cid:89).
t=2.
(cid:35) (cid:34) t(cid:89).
t=1.
(cid:35).
transition distribution.
the transition distribu-tion between latent variables (t1 in figure 2) isa k × k matrix of probabilities, where each rowsums to 1. we deﬁne this matrix as.
i.e., it is a product of the probabilities of each la-tent state transition (transition distribution) and theprobability of the observations given their respec-tive latent state (emission distribution)..3.2 parameterization.
latent state.
a latent state zt represents the in-put triples that are verbalized in the observed factyt.
it is not guaranteed that one fact always ver-balizes only one triple (see bottom example infigure 1).
thus, we represent state zt as a sequencet , .
.
.
, oltof latent variables o1, where lt is the num-tber of triples verbalized in yt.
figure 2 shows thestructure of the model..(cid:16).
p.t | o(l−1)ol.
t., x.
(cid:17).
= softmax (ab (cid:12) m (q)) (1).
where (cid:12) denotesthe hadamard product.
a ∈ rk×m and b ∈ rm×k are matricesof predicate embeddings with dimension m.q = {q1, q2, .
.
.
, qj } is the set of predicates of theinput triples x, and each qj ∈ q is the predicate ofthe triple xj.
m (q) is a k × k masking matrix,where mij = 1 if i ∈ q and j ∈ q, otherwise.
3by aligning the triples to facts using a rule-based aligner(see section 5), we found that the chance of aggregating morethan three triples to a fact is under 0.01% in the training set ofboth webnlg and e2e datasets..1421william anders, who retired in 1969, was a crew member on apollo 8.fact-1fact-2t1t2t3t4qhe was a crew member of nasa 's apollo 8member_ofoperatormember_of operatordateofretirement member_ofoperatorbackup_pilotcommanderpredicates of input triplesmij = 0. we apply row-wise softmax over theresulting matrix to obtain probabilities..the probability of generating the latent state zt(t2 in figure 2) can be written as the joint distribu-tion of the latent variables o1.
assuminga ﬁrst-order markov chain, we get:(cid:16).
t , .
.
.
, oltt.(cid:17).
p (zt | x) = p.t , o1o0.
t , o2.
= p (cid:0)o0.
t | x(cid:1).
p.t | o(l−1)ol.
t., x.
(cid:35).
(cid:17).
,.
| x.t , .
.
.
, oltt(cid:34) lt(cid:89).
(cid:16).
l=1.
where o0.
t is a marked start-state..on top of the generation probability of the la-tent states p (zt | x) and p (zt−1 | x), we deﬁnethe transition distribution between two latent states(t3 in figure 2) as:.
p (zt | zt−1, x) =p.
(cid:16).
· p.· p.(cid:16).
(cid:17).
(t−1), .
.
.
, olt−1o0(t−1) | x(cid:17)(cid:16).
t | olt−1o1.
(t−1), x.t , .
.
.
, olto0t.(cid:17).
| x.,.
where olt−1(t−1) denotes the last latent variable in la-tent state zt−1, while ot1 denotes the ﬁrst latentvariable (other than the start-state) in latent statezt.
we use two sets of parameters {ain, bin} and{aout, bout} to describe the transition distributionbetween latent variables within and across latentstates, respectively..emission distribution.
the emission distribu-tion p (yt | zt, x) (t4 in figure 2) describes thegeneration of fact yt conditioned on latent state ztand input triples x. we deﬁne the probability ofgenerating a fact as the product over token-levelprobabilities,.
p (yt | zt, x) = p(y1.
t | zt, x).
p(yi.
t | y1:(i−1).
t., zt, x)..nt(cid:89).
i=2.
the ﬁrst and last token of a fact are marked fact-start and fact-end tokens.
we adopt transformer(vaswani et al., 2017) as the model’s encoder anddecoder..each triple is linearized into a list of tokens fol-lowing the order: subject, predicate, and object.
in order to represent individual triples, we insertspecial [sep] tokens at the end of each triple.
aspecial [cls] token is inserted before all inputtriples, representing the beginning of the entire in-put.
an example where the encoder produces acontextual embedding for the tokens of two inputtriples is shown in figure 6 in appendix e..at time step t, the decoder generates fact yttoken-by-token autoregressively, conditioned on.
both the contextually-encoded input and the latentstate zt.
to guarantee that the generation of ytconditions only on the input triples whose predicateis in zt, we mask out the contextual embeddings oftokens from other unrelated triples for the encoder-decoder attention in all transformer layers..autoregressive decoding.
autoregressive hid-den markov model (ar-hmm) introduces ex-tra links into hmm to capture long-term corre-lations between observed variables, i.e., output to-kens.
following wiseman et al.
(2018), we usear-hmm for decoding, therefore allowing the in-terdependence between tokens to generate moreﬂuent and natural text descriptions.
each tokendistribution depends on all the previously gener-ated tokens, i.e., we deﬁne the token-level proba-1:(t−1), y1:(i−1)t | y1:ntbilities as p(yi, zt, x) instead oft | y1:(i−1)p(yi, zt, x).
during training, at each timestep t, we teacher-force the generation of the fact ytby feeding the ground-truth history, y1:(t−1), to theword-level transformer decoder.
however, sinceonly yt depends on the current hidden state zt, weonly calculate the loss over yt..t.t.3.3 learning.
we apply the backward algorithm (rabiner, 1989)to learn the parameters introduced in section 3.2,where we maximize p(y | x), i.e., the marginal like-lihood of the observed facts y given input triplesx, over all the latent states z and o on the entiredataset using dynamic programming.
followingmurphy (2012), and given that the latent state attime t is c, we deﬁne a conditional likelihood offuture evidence as:.
βt (c) (cid:44) p (yt+1:t | zt = c, x) ,.
(2).
where c denotes a group of predicates that areassociated with the emission of y. the size of cranges from 1 to l and each component is from thecollection of predicates q (see section 3.2).
then,the backward recurrences are:.
βt−1.
(cid:0)c (cid:48)(cid:1) = p (cid:0)yt:t | zt−1 = c (cid:48), x(cid:1).
βt (c) p (yt | zt = c, x) p (cid:0)zt = c | zt−1 = c (cid:48), x(cid:1).
(cid:88).
=.
c.with the base case βt (c) = 1. the marginalprobability of y over latent z is then obtained asp (y | x) = (cid:80)c β0 (c) p (z1 = c|x).
in equation 2, the size of the search space forc is (cid:80)lα=1 kα, where k = |q|, i.e., the numberof unique predicates appearing in the dataset.
the.
1422search5 based on the transition distribution intro-duced in equation 1. speciﬁcally, we use a tran-sition distribution between latent variables withinlatent states, calculated with predicate embeddingsain and bin (see section 3.2).
to guarantee that thegenerated sequence does not suffer from omissionand duplication of predicates, we constantly updatethe masking matrix m (q) by removing generatedpredicates from the set q. the planning processstops when q is empty.
planning: input aggregation.
the goal is to ﬁndthe top-n most likely aggregations for each resultof the input ordering step.
to implement thisprocess efﬁciently, we introduce a binary state foreach predicate in the sequence: 0 indicates “wait”and 1 indicates “emit” (green squares in figure 3).
then we list all possible combinations6 of the bi-nary states for the input ordering result.
for eachcombination, the aggregation algorithm proceedsleft-to-right over the predicates and groups thoselabelled as “emit” with all immediately precedingpredicates labelled as “wait”.
in turn, we rank allthe combinations with the transition distributionintroduced in equation 1. in contrast to the inputordering step, we use the transition distributionbetween latent variables across latent states, cal-culated with predicate embeddings aout and bout.
that is, we do not take into account transitions be-tween two consecutive predicates if they belong tothe same group.
instead, we only consider consec-utive predicates across two connected groups, i.e.,the last predicate of the previous group with theﬁrst predicate of the following group.
text generation.
the ﬁnal step generates a textdescription conditioned on the input triples and theplanning result (obtained from the input aggrega-tion step).
we use beam search and the planning-conditioned generation process described in sec-tion 3.2 (“emission distribution”)..3.5 controllability over sentence plans.
while the jointly learnt model is capable of fullyautomatic generation including the planning step(see section 3.4), the discrete latent space allowsdirect access to manually control the planning com-ponent, which is useful in settings which require.
5we use beam search since viterbi decoding aims at gettingz∗ = arg maxz(z1:t |y1:t ), but y1:t is not available at thisstage..6we assume that each fact is comprised of l triples atmost.
to match this assumption, we discard combinationscontaining a group that aggregates more than l predicates..figure 3: the inference process (section 3.4).
problem can still be intractable due to high k, de-spite the simpliﬁcations explained in section 3.2(cf.
predicates).
to tackle this issue and reduce thesearch space of c, we: (1) only explore permuta-tions of c that include predicates appearing on theinput; (2) introduce a heuristic based on the overlapof tokens between a triple and a fact—if a certainfact mentions most tokens appearing in the predi-cate and object of a triple we hard-align it to thistriple.4 as a result, we discard the permutationsthat do not include the aligned predicates..3.4.inference.
after the joint learning process, the model is ableto plan, i.e., order and aggregate the input triplesin the most likely way, and then generate a text de-scription following the planning results.
therefore,the joint prediction of (ˆy, ˆz) is deﬁned as:.
(ˆy, ˆz) = arg max.
p (cid:0)y(cid:48), z(cid:48) | x(cid:1).
(y(cid:48),z(cid:48)),z(cid:48)∈{˜z(i)}.
= arg max.
(y(cid:48),z(cid:48)),z(cid:48)∈{˜z(i)}.
p(y(cid:48) | z(cid:48), x)p(z(cid:48) | x),.
(3).
where {˜z(i)} denotes a set of planning results, ˆyis the text description, and ˆz is the planning resultthat ˆy is generated from..the entire inference process (see figure 3) in-cludes three steps: input ordering, input aggrega-tion, and text generation.
the ﬁrst two steps areresponsible for the generation of {˜z(i)} togetherwith their probabilities {p(˜z(i) | x)}, while the laststep is for the text generation p(y(cid:48) | ˜z(i), x)..planning: input ordering.
the aim is to ﬁnd thetop-k most likely orderings of predicates appearingin the input triples.
in order to make the searchprocess more efﬁcient, we apply left-to-right beam-.
4this heuristic is using the rule-based aligner introduced insection 5 with a threshold to rule out alignments in which thetriples are not covered over 50%, since our model emphasisesmore on precision.
thus, not all triples are aligned to a fact..142301001fact1fact2input orderingpredicates of input triplesinput aggregationtext generationsentence planningincreased human supervision and is a unique fea-ture of our architecture.
the plans (latent variables)can be controlled in two ways: (1) hyperparam-eter.
our code offers a hyperparameter that canbe tuned to control the level of aggregation: noaggregation, aggregate one, two triples, etc.
themodel can predict the most likely plan based onthe input triples and the hyperparameter and gener-ate a corresponding text description; (2) the modelcan directly adopt human-written plans, e.g.
us-ing the notation [eattype][near customer-rating],which translates to: ﬁrst generate ‘eattype’ as anindependent fact and then aggregate the predicates‘near’ and ‘customer-rating’ in the following factand generate their joint description..4 experiments.
4.1 datasets.
we tested our approach on two widely used data-to-text tasks: the e2e nlg (novikova et al., 2017)and webnlg7 (gardent et al., 2017a).
comparedto e2e, webnlg is smaller, but contains morepredicates and has a larger vocabulary.
statisticswith examples can be found in appendix c. we fol-lowed the original training-development-test datasplit for both datasets..4.2 evaluation metrics.
generation evaluation focuses on evaluating thegenerated text with respect to its similarity tohuman-authored reference sentences.
to compareto previous work, we adopt their associated metricsto evaluate each task.
the e2e task is evaluatedusing bleu (papineni et al., 2002), nist (dod-dington, 2002), rouge-l (lin, 2004), meteor(lavie and agarwal, 2007), and cider (vedantamet al., 2015).
webnlg is evaluated in terms ofbleu, meteor, and ter (snover et al., 2006).
factual correctness evaluation tests if the gener-ated text corresponds to the input triples (wen et al.,2015b; reed et al., 2018; duˇsek et al., 2020).
weevaluated on the e2e test set using automatic sloterror rate (ser),8 i.e., an estimation of the occur-rence of the input attributes (predicates) and theirvalues in the outputs, implemented by duˇsek et al..7since we propose exploring sentence planning and in-creasing the controllability of the generation model and do notaim for a zero-shot setup, we only focus on the seen categoryin webnlg..8ser is based on regular expression matching.
since onlythe format of e2e data allows such patterns for evaluation, weonly evaluate factual correctness on the e2e task..modelt5(cid:7)planenc(cid:7)adapt(cid:7)tilb-pipe(cid:7)transformeragggenagggen−odagggen−ag.
bleu ter meteor.
64.7064.4260.5944.3458.4758.7455.3052.17.
—0.330.370.480.370.400.440.50.
0.460.450.440.380.420.430.430.44.table 1: generation evaluation results on thewebnlg (seen).
the models labelled with (cid:7) are fromprevious work.
the rest are our implementations..(2020).
ser counts predicates that were added,missed or replaced with a wrong object.
intrinsic planning evaluation examines plan-ning performance in section 6..4.3 baseline model and training detailsto evaluate the contributions of the planning com-ponent, we choose the vanilla transformer model(vaswani et al., 2017) as our baseline, trained onpairs of linearized input triples and target texts.
inaddition, we choose two types of previous worksfor comparison: (1) best-performing models re-ported on the webnlg 2017 (seen) and e2edataset, i.e.
t5 (kale and rastogi, 2020), planenc(zhao et al., 2020), adapt (gardent et al., 2017b),and tgen (duˇsek and jurˇc´ıˇcek, 2016); (2) modelswith explicit planning, i.e.
tilb-pipe (gardentet al., 2017b), ntemp+ar (wiseman et al., 2018)and shen et al.
(2020)..to make our hmm-based approach convergefaster, we initialized its encoder and decoder withthe baseline model parameters and ﬁne-tuned themduring training of the transition distributions.
en-coder and decoder parameters were chosen basedon validation results of the baseline model for eachtask (see appendix d for details)..5 experiment results.
5.1 generation evaluation results.
table 1 shows the generation results on thewebnlg seen category (gardent et al., 2017b).
our model outperforms tilb-pipe and trans-former, but performs worse than t5, planenc andadapt.
however, unlike these three models, ourapproach does not rely on large-scale pretrain-ing, extra annotation, or heavy pre-processing us-ing external resources.
table 2 shows the resultswhen training and testing on the original e2e set.
agggen outperforms ntemp+ar and is compa-rable with shen et al.
(2020), but performs slightly.
1424bleu.
nist met.
r-l.add miss wrong.
ser.
modeltgen(cid:7)ntemp+ar(cid:7)shen et al.
(2020)(cid:7)transformeragggenagggen−odagggen−ag.
66.4159.8065.1068.2364.1458.9044.00.
8.55657.5600—8.67658.35097.91006.0890.
45.0738.7545.5044.3145.1343.2143.75.
69.1765.0168.2069.88¯66.6262.1258.24.cider.
2.22531.95002.24102.21532.19531.96560.8202.
00.14——00.3000.3201.6508.74.
04.11——04.6701.6602.9900.45.
00.03——00.2000.7103.0100.92.
04.27——05.1602.7007.6510.11.table 2: evaluation of generation (middle) and factual correctness (right) trained/tested on the original e2e data(section 5 for metrics description).
models with (cid:7) are from previous work, the rest are our implementations..modeltgen(cid:7)transformeragggenagggen−odagggen−ag.
bleu nist met r-l.cider.
6.0225.756.
36.9735.92.
55.52 1.76239.231.66838.5755.451.84441.06 6.207 37.91 55.131.65351.5338.240.93649.9430.44.
36.5637.99.
5.9514.636.table 3: evaluation of generation trained on the orig-inal e2e data, while tested on the cleaned e2e data.
note that, the clean test set has more diverse mrs andfewer references per mr, which leads to lower scores– see also the paper introducing the cleaned e2e data(table 2 and 3 in duˇsek et al.
(2019))..worse than both seq2seq models in terms of word-overlap metrics..however, the results in table 3 demonstrate thatour model does outperform the baselines on mostsurface metrics if trained on the noisy original e2etraining set and tested on clean e2e data (duˇseket al., 2019).
this suggests that the previous perfor-mance drop was due to text references in the origi-nal dataset that did not verbalize all triples or addedinformation not present in the triples that may havedown-voted the fact-correct generations.9 this alsoshows that agggen produces correct outputs evenwhen trained on a noisy dataset.
since constructinghigh-quality data-to-text training sets is expensiveand labor-intensive, this robustness towards noiseis important..5.2 factual correctness results.
the results for factual correctness evaluated usingser on the original e2e test set are shown in ta-ble 2. the ser of agggen is the best amongall models.
especially, the high “miss” scoresfor tgen and transformer demonstrate the highchance of information omission in vanilla seq2seq-in contrast, agggen showsbased generators.
much better coverage over the input triples whilekeeping a low level of hallucination (low “add”.
9we also trained and tested models on the cleaned e2edata.
the full results (including the factual correctness evalu-ation) are shown in table 8 in appendix f: there is a similartrend as in results in table 3, compared to transformer..and “wrong” scores)..5.3 ablation variants.
to explore the effect of input planning on textgeneration, we introduced two model variants:agggen−od, where we replaced the input order-ing with randomly shufﬂing the input triples beforeinput aggregation, and agggen−ag, where theinput ordering result was passed directly to thetext generation and the text decoder generated afact for each input triple individually..the generation evaluation results on bothdatasets (table 1 and table 2) show that agggenoutperforms agggen−od and agggen−ag sub-stantially, which means both input ordering andinput aggregation are critical.
table 2 shows thatthe factual correctness results for the ablative vari-ants are much worse than full agggen, indi-cating that planning is essential for factual cor-rectness.
an exception is the lower number ofmissed slots in agggen−ag.
this is expectedsince agggen−ag generates a textual fact foreach triple individually, which decreases the pos-sibility of omissions at the cost of much lower ﬂu-ency.
this strategy also leads to a steep increase inadded information..additionally, agggen−ag performs evenworse on the e2e dataset than on the webnlgset.
this result is also expected, since input ag-gregation is more pronounced in the e2e datasetwith a higher number of facts and input triples persentence (cf.
appendix c)..5.4 qualitative error analysiswe manually examined a sample of 100 outputs(50 from each dataset) with respect to their factualcorrectness and ﬂuency.
for factual correctness,we follow the deﬁnition of ser and check whetherthere are hallucinations, substitutions or omissionsin generated texts.
for ﬂuency, we check whetherthe generated texts suffer from grammar mistakes,redundancy, or contain unﬁnished sentences.
fig-.
14256.1 human-annotated alignments.
we asked crowd workers on amazon mechanicalturk to align input triples to their fact-based textsnippets to derive a “reference plan” for each tar-get text.11 each worker was given a set of inputtriples and a corresponding reference text descrip-tion, segmented into a sequence of facts.
the work-ers were then asked to select the triples that areverbalised in each fact.12 we sampled 100 inputsfrom the webnlg13 test set for annotation.
eachinput was paired with three reference target textsfrom webnlg.
to guarantee the correctness ofthe annotation, three different workers annotatedeach input-reference pair.
we only consider thealignments where all three annotators agree.
usingfleiss kappa (fleiss, 1971) over the facts alignedby each judge to each triple, we obtained an aver-age agreement of 0.767 for the 300 input-referencepairs, which is considered high agreement..6.2 study of sentence planning.
we now check the agreement between the model-generated and reference plans based on the top-1input aggregation result (see section 3.4).
weintroduce two metrics:.
• normalized mutual information (nmi) (strehland ghosh, 2002) to evaluate aggregation.
werepresent each plan as a set of clusters of triples,where a cluster contains the triples sharing the samefact verbalization.
using nmi we measure mutualinformation between two clusters, normalized intothe 0-1 range, where 0 and 1 denote no mutualinformation and perfect correlation, respectively.
• kendall’s tau (τ ) (kendall, 1945) is a rankingbased measure which we use to evaluate both or-dering and aggregation.
we represent each planas a ranking of the input triples, where the rankof each triple is the position of its associated factverbalization in the target text.
τ measures rankcorrelation, ranging from -1 (strong disagreement)to 1 (strong agreement)..in the crowdsourced annotation (section 6.1),each set of input triples contains three referencetexts with annotated plans.
we ﬁst evaluate the cor-respondence among these three reference plans by.
11the evaluation requires human annotations, since anchor-based automatic alignments are not accurate enough (86%) forthe referred plan annotation.
see table 5 (“rb”) for details.
12the annotation guidelines and an example annotation task.
are shown in figure 7 in appendix g..13we chose webnlg over e2e for its domain and predicate.
diversity..figure 4: examples of input and system-generated tar-get text for e2e (top) and webnlg (bottom).
the se-quences in square brackets are the sentence plans..ure 4 shows two examples of generated texts fromtransformer and agggen (more examples, includ-ing target texts generated by agggen−od andagggen−ag, are shown in table 6 and table 7in appendix a).
we observe that, in general, theseq2seq transformer model tends to compressmore triples into one ﬂuent fact, whereas agggenaggregates triples in more but smaller groups, andgenerates a shorter/simpler fact for each group.
therefore, the texts generated by transformer aremore compressed, while agggen’s generationsare longer with more sentences.
however, the plan-ning ensures that all input triples will still be men-tioned.
thus, agggen generates texts with higherfactual correctness without trading off ﬂuency.10.
6.intrinsic evaluation of planning.
we now directly inspect the performance of theplanning component by taking advantage of thereadability of srl-aligned facts.
in particular, weinvestigate: (1) sentence planning performance.
we study the agreement between model’s plan-ning and reference planning for the same set ofinput triples; (2) alignment performance – we useagggen as an aligner and examine its ability toalign segmented facts to the corresponding inputtriples.
since both studies require ground-truthtriple-to-fact alignments, which are not part of thewebnlg and e2e data, we ﬁrst introduce a humanannotation process in section 6.1..10the number of ﬂuent generations for transformer andagggen among the examined 100 examples are 96 and 95 re-spectively.
the numbers for agggen−od and agggen−agare 86 and 74, which indicates that both input ordering andinput aggregation are critical for generating ﬂuent texts..1426the cricketers is a chinese restaurant near all bar one in the city centre .it is children friendly and has an average customer rating .william anders birthplacebritish hong kongapollo 8 backup_pilotbuzz aldrinapollo 8 crewmembersfrank bormanapollo 8 operatornasawilliam anders was born in british hong kong and served as a crew member onapollo 8. frank borman was a crewman aboard the nasa operated apollo 8 mission.the backup pilot was buzz aldrin.
william anders retired on september 1st , 1969 .william anders (born in british hong kong) was a crew member of nasa's apol8 alongside frank borman.
william anders retired on september 1st, 1969 .
[birthplace] [crew_member] [operator crewmembers] [backup_pilot] [retirement][eattype pricerange] [food customerrating] [familyfriendly area near]the cricketers areacity centrethe cricketers customerratingaveragethe cricketers eattyperestaurantthecricketersfamilyfriendlyyesthe cricketers foodchinese the cricketers nearall bar onethe cricketers pricerangehighinputstransagggenthe cricketers is a chinese restaurant with a high price range.
it has an averagecustomerratingandischildrenfriendlynearallbaroneinthecitycentre.william anders retirement1969-09-01william anders crew_memberapollo 8inputstransagggenhumanagggen.
nmimax nmiavg k-taumax k-tauavg0.24880.93400.20640.7101.
0.84150.6416.
0.75870.6247.table 4: planning evaluation results.
nmi and k-taucalculated between human-written references (bottom),and between references and our system agggen (top)..rb (%)vtb (%).
precision86.2089.73.recall100.0084.16.f192.5986.85.table 5: alignment evaluation results.
alignmentaccuracy for the viterbi algorithm (vtb) and the rule-based aligner (rb)..calculating nmi and τ between one plan and theremaining two.
in the top row of table 4, the highaverage and maximum nmi indicate that the refer-ence texts’ authors tend to aggregate input triplesin similar ways.
on the other hand, the low aver-age τ shows that they are likely to order the aggre-gated groups differently.
then, for each set of inputtriples, we measure nmi and τ of the top-1 inputaggregation result (model’s plan) against each ofthe corresponding reference plans and compute av-erage and maximum values (bottom row in table 4).
compared to the strong agreement among refer-ence plans on the input aggregation, the agreementbetween model’s and reference plans is slightlyweaker.
our model has slightly lower agreement onaggregation (nmi), but if we consider aggregationand ordering jointly (τ ), the agreement between ourmodel’s plans and reference plans is comparable tothe agreement among reference plans..6.3 study of alignment.
in this study, we use the hmm model as an alignerand assess its ability to align input triples with theirfact verbalizations on the human-annotated set.
given the sequence of observed variables, a trainedhmm-based model is able to ﬁnd the most likelysequence of hidden states z∗ = arg max(z1:t |y1:t )using viterbi decoding.
similarly, given a set ofinput triples and a factoid segmented text, we useviterbi with our model to align each fact with thecorresponding input triple(s).
we then evaluate theaccuracy of the model-produced alignments againstthe crowdsourced alignments..z.the alignment evaluation results are shown intable 5. we compare the viterbi (vtb) alignmentswith the ones calculated by a rule-based aligner(rb) that aligns each triple to the fact with thegreatest word overlap.
the precision of the viterbialigner is higher than the rule-based aligner.
how-.
ever, the viterbi aligner tends to miss triples, whichleads to a lower recall.
since hmms are locallyoptimal, the model cannot guarantee to annotateinput triples once and only once..7 conclusion and future workwe show that explicit sentence planning, i.e., inputordering and aggregation, helps substantially toproduce output which is both semantically correctas well as naturally sounding.
crucially, this alsoenables us to directly evaluate and inspect both themodel’s planning and alignment performance bycomparing to manually aligned reference texts.
oursystem outperforms vanilla seq2seq models whenconsidering semantic accuracy and word-overlapbased metrics.
experiment results also show thatagggen is robust to noisy training data.
we planto extend this work in three directions:other generation models.
we plan to plug othertext generators, e.g.
pre-training based approaches(lewis et al., 2020; kale and rastogi, 2020), intoagggen to enhance their interpretability and con-trollability via sentence planning and generation.
zero/few-shot scenarios.
kale and rastogi(2020)’s work on low-resource nlg uses a pre-trained language model with a schema-guided rep-resentation and hand-written templates to guide therepresentation in unseen domains and slots.
thesetechniques can be plugged into agggen, which al-lows us to examine the effectiveness of the explicitsentence planning in zero/few-shot scenarios.
including content selection.
in this work, weconcentrate on the problem of faithful surface re-alization based on e2e and webnlg data, whichboth operate under the assumption that all inputpredicates have to be realized in the output.
incontrast, more challenging tasks such as rotowire(wiseman et al., 2017), include content selectionbefore sentence planning.
in the future, we plan toinclude a content selection step to further extendagggen’s usability.
acknowledgmentsthis research received funding from the epsrcproject aisec (ep/t026952/1), charles universityproject primus/19/sci/10, a royal society re-search grant (rgs/r1/201482), a carnegie trustincentive grant (rig009861).
this research alsoreceived funding from apple to support research atheriot-watt university and charles university.
wethank alessandro suglia, jindˇrich helcl, and hen-rique ferrolho for their suggestions.
we thank theanonymous reviewers for their helpful comments..1427references.
ziqiang cao, furu wei, wenjie li, and sujian li.
2018.faithful to the original: fact aware neural abstrac-in aaai, new orleans, la,tive summarization.
usa.
arxiv: 1711.04434..thiago castro ferreira, chris van der lee, emiel vanmiltenburg, and emiel krahmer.
2019. neural data-to-text generation: a comparison between pipelinein 2019 conferenceand end-to-end architectures.
on empirical methods in natural language pro-cessing (emnlp) and 9th international joint con-ference on natural language processing (ijcnlp),hong kong..george doddington.
2002..automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
in proceedings of the secondinternational conference on human language tech-nology research, pages 138–145..natural language generation: the e2e nlg challenge.
computer speech & language, 59:123 – 156..henry elder, jennifer foster, james barry, and alexan-der o’connor.
2019. designing a symbolic inter-mediate representation for neural surface realization.
in proceedings of the workshop on methods for op-timizing and evaluating neural language genera-tion, pages 65–73, minneapolis, minnesota.
associ-ation for computational linguistics..angela fan, mike lewis, and yann dauphin.
2019.strategies for structuring story generation.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2650–2660, florence, italy.
association for computa-tional linguistics..joseph l fleiss.
1971. measuring nominal scale agree-ment among many raters.
psychological bulletin,76(5):378..pablo duboue and kathleen mckeown.
2002. contentplanner construction via evolutionary algorithmsin proceed-and a corpus-based ﬁtness function.
ings of the international natural language genera-tion conference, pages 89–96, harriman, new york,usa.
association for computational linguistics..hanning gao, lingfei wu, po hu, and fanglixu.
2020. rdf-to-text generation with graph-augmented structural neural encoders.
in proceed-ings of the twenty-ninth international joint con-ference on artiﬁcial intelligence, pages 3030–3036,yokohama, japan..pablo a. duboue and kathleen r. mckeown.
2001.empirically estimating order constraints for contentplanning in generation.
in proceedings of the 39thannual meeting of the association for computa-tional linguistics, pages 172–179, toulouse, france.
association for computational linguistics..ondˇrej duˇsek, david m. howcroft, and verena rieser.
2019. semantic noise matters for neural natural lan-guage generation.
in proc.
of the 12th internationalconference on natural language generation, pages421–426, tokyo, japan.
association for computa-tional linguistics..ondˇrej duˇsek and filip jurˇc´ıˇcek.
2016. sequence-to-sequence generation for spoken dialogue via deepin proceedings of thesyntax trees and strings.
54th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages45–51, berlin, germany.
association for computa-tional linguistics..ondˇrej duˇsek, jekaterina novikova, and verena rieser.
2020. evaluating the state-of-the-art of end-to-endnatural language generation: the e2e nlg chal-lenge.
computer speech & language, 59:123–156..ondˇrej duˇsek and filip jurˇc´ıˇcek.
2016. sequence-to-sequence generation for spoken dialogue via deepin proceedings of thesyntax trees and strings.
54th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages45–51, berlin.
association for computational lin-guistics..ondˇrej duˇsek, jekaterina novikova, and verena rieser.
2020. evaluating the state-of-the-art of end-to-end.
claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017a.
creating train-in proceed-ing corpora for nlg micro-planners.
ings of the 55th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 179–188, vancouver, canada.
associa-tion for computational linguistics..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017b.
the webnlginchallenge: generating text from rdf data.
proceedings of the 10th international conference onnatural language generation, pages 124–133, san-tiago de compostela, spain.
association for compu-tational linguistics..albert gatt and emiel krahmer.
2018. survey of thestate of the art in natural language generation: coretasks, applications and evaluation.
journal of artiﬁ-cial intelligence research, 61:65–170..luheng he, kenton lee, omer levy, and luke zettle-moyer.
2018. jointly predicting predicates and argu-ments in neural semantic role labeling.
in proceed-ings of the 56th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 364–369, melbourne, australia..juraj juraska, panagiotis karagiannis, kevin k. bow-den, and marilyn a. walker.
2018. a deep en-semble model with slot alignment for sequence-to-sequence natural language generation.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long papers), pages 152–162, new orleans,la, usa..1428mihir kale and abhinav rastogi.
2020. text-to-textpre-training for data-to-text tasks.
in proceedings ofthe 13th international conference on natural lan-guage generation, pages 97–102, dublin, ireland.
association for computational linguistics..in proceed-structured data to text generation.
ings of the 11th international conference on natu-ral language generation, pages 1–9, tilburg uni-versity, the netherlands.
association for computa-tional linguistics..chris kedzie and kathleen mckeown.
2019. a goodsample is hard to find: noise injection samplingand self-training for neural language generationmodels.
in inlg, tokyo, japan..amit moryossef,.
ido dagan, and yoav goldberg.
improving quality and efﬁciency in plan-in inlg,.
2019a.
based neural data-to-text generation.
tokyo, japan..maurice g kendall.
1945. the treatment of ties in rank-.
ing problems.
biometrika, pages 239–251..diederik kingma and jimmy ba.
2015. adam: ain proceed-method for stochastic optimization.
ings of the 3rd international conference on learn-ing representations, san diego, ca, usa.
arxiv:1412.6980..ioannis konstas and mirella lapata.
2013..induc-ing document plans for concept-to-text generation.
in proceedings of the 2013 conference on empiri-cal methods in natural language processing, pages1503–1514, seattle, washington, usa.
associationfor computational linguistics..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of the second workshop on statistical machinetranslation, pages 228–231..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..xiang lisa li and alexander rush.
2020. posteriorin proceedingscontrol of blackbox generation.
of the 58th annual meeting of the association forcomputational linguistics, pages 2731–2743, on-line.
association for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..robert logan, nelson f. liu, matthew e. peters, mattgardner, and sameer singh.
2019. barack’s wifehillary: using knowledge graphs for fact-awarelanguage modeling.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 5962–5971, florence, italy.
asso-ciation for computational linguistics..amit moryossef, yoav goldberg, and ido dagan.
2019b.
step-by-step: separating planning fromrealization in neural data-to-text generation.
innaacl, minneapolis, mn, usa..kevin p murphy.
2012. machine learning: a proba-.
bilistic perspective.
mit press..feng nie, jin-ge yao, jinpeng wang, rong pan, andchin-yew lin.
2019. a simple recipe towardsreducing hallucination in neural surface realisa-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 2673–2679, florence, italy..jekaterina novikova, ondˇrej duˇsek, and verena rieser.
2017. the e2e dataset: new challenges for end-in proceedings of the 18th an-to-end generation.
nual sigdial meeting on discourse and dialogue,pages 201–206, saarbr¨ucken, germany.
associationfor computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ratish puduppully, li dong, and mirella lapata.
2019.data-to-text generation with content selection andplanning.
in proceedings of the 33rd aaai confer-ence on artiﬁcial intelligence, honolulu, hawaii..raheel qader, francois portet, and cyril labbe.
2019.semi-supervised neural text generation by jointlearning of natural language generation and nat-in inlg,ural language understanding models.
tokyo, japan..lawrence r rabiner.
1989. a tutorial on hiddenmarkov models and selected applications in speechrecognition.
proceedings of the ieee, 77(2):257–286..jinfeng rao, kartikeya upasani, anusha balakrish-nan, michael white, anuj kumar, and rajen subba.
2019. a tree-to-sequence model for neural nlgin task-oriented dialog.
in inlg, tokyo, japan..diego marcheggiani and laura perez-beltrachini.
2018. deep graph convolutional encoders for.
lena reed, shereen oraby, and marilyn walker.
2018.can neural generators for dialogue learn sentence.
1429tsung-hsien wen, milica gasic, dongho kim, nikolamrksic, pei-hao su, david vandyke, and steveyoung.
2015a.
stochastic language generationin dialogue using recurrent neural networks withconvolutional sentence reranking.
in proceedingsof the 16th annual meeting of the special interestgroup on discourse and dialogue, pages 275–284,prague, czech republic.
association for computa-tional linguistics..tsung-hsien wen, milica gaˇsi´c, nikola mrkˇsi´c, pei-hao su, david vandyke, and steve young.
2015b.
semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages1711–1721, lisbon, portugal.
association for com-putational linguistics..sam wiseman, stuart shieber, and alexander rush.
2017. challenges in data-to-document generation.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2253–2263, copenhagen, denmark.
association forcomputational linguistics..sam wiseman, stuart shieber, and alexander rush.
2018. learning neural templates for text genera-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 3174–3187, brussels, belgium.
associationfor computational linguistics..xinnuo xu, ondˇrej duˇsek, jingyi li, verena rieser,fact-based contentand ioannis konstas.
2020.weighting for evaluating abstractive summarisation.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages5071–5081..chao zhao, marilyn walker, and snigdha chaturvedi.
2020. bridging the structural gap between encod-ing and decoding for data-to-text generation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 2481–2491, online.
association for computational lin-guistics..in proceed-planning and discourse structuring?
ings of the 11th international conference on natu-ral language generation, pages 284–295, tilburguniversity, the netherlands.
association for com-putational linguistics..ehud reiter and robert dale.
2000. building naturallanguage generation systems.
cambridge universitypress..anna rohrbach, lisa anne hendricks, kaylee burns,trevor darrell, and kate saenko.
2018. object hal-lucination in image captioning.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4035–4045, brus-sels, belgium..zhihong shao, minlie huang, jiangtao wen, wenfeixu, and xiaoyan zhu.
2019. long and diverse textgeneration with planning-based hierarchical varia-in proceedings of the 2019 confer-tional model.
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3257–3268, hong kong, china.
as-sociation for computational linguistics..xiaoyu shen, ernie chang, hui su, jie zhou, and di-etrich klakow.
2020. neural data-to-text generationvia jointly learning the segmentation and correspon-dence.
in proceedings of the 58th annual meetingof the association for computational linguistics..matthew snover, bonnie dorr, richard schwartz, lin-nea micciulla, and john makhoul.
2006. a study oftranslation edit rate with targeted human annotation.
in proceedings of the 7th conference of the associa-tion for machine translation in the americas, pages223–231, cambridge, ma, usa..alexander strehl and joydeep ghosh.
2002. clusterensembles—a knowledge reuse framework for com-bining multiple partitions.
journal of machine learn-ing research, 3(dec):583–617..shang-yu su, kai-ling lo, yi-ting yeh, and yun-nung chen.
2018. natural language generation byhierarchical decoding with linguistic patterns.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 2 (short papers), pages 61–66, new orleans,louisiana.
association for computational linguis-tics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..ramakrishna vedantam, c lawrence zitnick, and deviparikh.
2015. cider: consensus-based image de-in proceedings of the ieeescription evaluation.
conference on computer vision and pattern recogni-tion, pages 4566–4575..1430a examples of input and system-generated target text.
table 6: examples of input and system-generated target text for e2e nlg dataset..input.
transf.
agggen.
agggen−ag.
agggen−od.
input.
transf.
agggen.
agggen−ag.
agggen−od.
input.
transf.
agggen.
agggen−ag.
agggen−od.
input.
transf.
agggen.
agggen−ag.
agggen−od.
the cricketers [customerrating] highthe cricketers [eattype] restaurantthe cricketers [near] express by holiday innpredicted text: the cricketers is a highly rated restaurant near express by holiday inn.
sentence planing: [eattype] [customerrating near]predicted text: the cricketers is a highly rated restaurant located near express by holiday inn.
sentence planing: [eattype] [near] [customerrating]predicted text: the cricketers is a restaurant located near express by holiday inn .
it has a high customerrating.
sentence planing: [eattype near] [customerrating]predicted text: the cricketers is a restaurant near express by holiday inn .
it has a high customer rating.
blue spice [eattype] pubblue spice [familyfriendly] yesblue spice [food] englishblue spice [near] rainbow vegetarian caf´epredicted text: the blue spice is a family friendly pub near the rainbow vegetarian caf´e.
it serves englishfood in the higher price range.
sentence planing: [eattype familyfriendly] [food near]predicted text: blue spice is a family friendly pub serving english food near the rainbow vegetarian caf´e.
sentence planing: [eattype] [familyfriendly] [food] [near]predicted text: blue spice is a mid - priced pub that is family friendly and serves british food .
it is locatednear the rainbow vegetarian caf´e.
sentence planing: [near familyfriendly eattype] [food]predicted text: blue spice is a family friendly pub near the rainbow vegetarian caf´e .
it serves english food.
the cricketers [area] riversidethe cricketers [customerrating] 5 out of 5the cricketers [eattype] restaurantthe cricketers [familyfriendly] yesthe cricketers [food] chinesethe cricketers [near] all bar onepredicted text: the cricketers is a family friendly chinese restaurant near all bar one in the riverside area .
ithas a customer rating of 5 out of 5.sentence planing: [eattype familyfriendly] [food area near] [customerrating]predicted text: the cricketers is a family friendly chinese restaurant located in the riverside area near all barone .
it has a customer rating of 5 out of 5.sentence planing: [eattype] [food] [area] [customerrating] [familyfriendly] [near]predicted text: the cricketers is a restaurant that serves chinese food.
it is located in the riverside area andhas a customer rating of 5 out of 5 and is children friendly.
the coffee shop is near all bar one.
sentence planing: [customerrating] [familyfriendly eattype] [food near area]predicted text: the cricketers is a 5 out of 5 rated , family friendly chinese restaurant located in riversidenear all bar one.
the cricketers [area] city centrethe cricketers [customerrating] averagethe cricketers [eattype] restaurantthe cricketers [familyfriendly] yesthe cricketers [food] chinesethe cricketers [near] all bar onethe cricketers [pricerange] highpredicted text: the cricketers is a chinese restaurant near all bar one in the city centre .
it is children friendlyand has an average customer rating.
sentence planing: [eattype pricerange] [food customerrating] [familyfriendly area near]predicted text: the cricketers is a chinese restaurant with a high price range .
it has an average customerrating and is children friendly near all bar one in the city centre.
sentence planing: [eattype] [pricerange] [food] [customerrating] [familyfriendly] [area] [near]predicted text: the cricketers is a chinese restaurant that is children friendly, has a high price range, acustomer rating of 3 out of 5, is located near the portland arms and is in the city centre.
it has an averagecustomer rating.
is also a children-friendly .
you can ﬁnd it is called the cricketers.
sentence planing: [area] [familyfriendly pricerange] [eattype food near] [customerrating]predicted text: located in the city centre, the cricketers is a child friendly chinese restaurant with a highprice range.
it is near all bar one.
its customer rating is average..1431input.
transf.
agggen.
agggen−ag.
agggen−od.
input.
transf.
agggen.
agggen−ag.
agggen−od.
input.
transf.
agggen.
agggen−ag.
agggen−od.
table 7: examples of input and system-generated target text for webnlg dataset..11th mississippi infantry monument [category] contributing property11th mississippi infantry monument [country] united states11th mississippi infantry monument [established] 200011th mississippi infantry monument [location] seminary ridge11th mississippi infantry monument [municipality] gettysburg, pennsylvaniapredicted text: the 11th mississippi infantry monument is located at seminary ridge, gettysburg, pennsylva-nia, united states.
it was established in 2000 and falls under the category of contributing property.
sentence planing: [location country established] [category] [municipality]predicted text: the 11th mississippi infantry monument, established in 2000, is located at seminary ridge,gettysburg, pennsylvania, united states.
it is categorized as a contributing property and is in the municipalityof gettysburg pennsylvania.
sentence planing: [country] [established] [category] [municipality] [location]predicted text: the 11th mississippi infantry monument is located in gettysburg, pennsylvania, united states.
the monument was established in 2000 and falls under the category of contributing property.
it can be foundin the municipality of gettysburg in pennsylvania and is in the seminary ridge.
sentence planing: [municipality] [category] [country established] [location]predicted text: the 11th mississippi infantry monument is in the municipality of gettysburg, pennsylvania.
it is categorized as a contributing property.
the monument was established in the year 2000 in the unitedstates and is located at seminary ridge.
apollo 8 [backup pilot] buzz aldrinapollo 8 [crewmembers] frank bormanapollo 8 [operator] nasawilliam anders [birthplace] british hong kongwilliam anders [dateofretirement] 1969-09-01william anders [was a crew member of] apollo 8predicted text: william anders (born in british hong kong) was a crew member of nasa’s apollo 8 alongsidefrank borman.
william anders retired on september 1st, 1969.sentence planing: [birthplace] [was a crew member of] [operator crewmembers] [backup pilot] [date-ofretirement]predicted text: william anders was born in british hong kong and served as a crew member on apollo 8.frank borman was a crewman aboard the nasa operated apollo 8 mission.
the backup pilot was buzz aldrin.
william anders retired on september 1st, 1969.sentence planing: [birthplace] [was a crew member of] [operator] [crewmembers] [backup pilot] [da-teofretirement]predicted text: william anders (born in british hong kong) was a crew member of apollo 8 operated bynasa, where buzz aldrin was a backup pilot.
frank borman was also a crewman aboard apollo 8. buzz aldrin’sbackup pilot was buzz aldrin.
william anders retired on september 1st, 1969.sentence planing: [backup pilot] [was a crew member of] []birthplace] [dateofretirement operator][crewmembers]predicted text: buzz aldrin was a crew member of apollo 8 with buzz aldrin as backup pilot and frankborman as buzz aldrin.
william anders was a crewman aboard apollo 8. british hong kong born williamanders retired.
he was a backup pilot on the apollo 8 mission was operated by nasa.
frank borman was also acrewman.
school of business and social sciences at the aarhus university [afﬁliation] european university associationschool of business and social sciences at the aarhus university [city] aarhusschool of business and social sciences at the aarhus university [country] denmarkschool of business and social sciences at the aarhus university [dean] thomas pallesenschool of business and social sciences at the aarhus university [established] 1928school of business and social sciences at the aarhus university [latinname] universitas aarhusiensisschool of business and social sciences at the aarhus university [numberofstudents] 16000predicted text: the school of business and social sciences at the aarhus university in aarhus, denmark wasestablished in 1928. it has 737 academic staff and 16,000 students.
its dean is thomas pallesen and it isafﬁliated with the european university association.
sentence planing: [country established city] [afﬁliation latinname numberofstudents] [dean]predicted text: the school of business and social sciences at the aarhus university in aarhus, denmark wasestablished in 1928. it has 737 academic staff and 16,000 students.
its latin name is ”universitas aarhusiensis”and it is afﬁliated with the european university association.
its dean is thomas pallesen.
sentence planing: [country] [established] [city] [afﬁliation] [latinname] [numberofstudents] [dean]predicted text: the school of business and social sciences at the aarhus university in denmark is afﬁliatedwith the european university association in brussels.
the school was established in 1928 and is located inthe city of aarhus.
it is afﬁliated to the european univerrsity in the school the latin name of ”universitasaarhusiensis”.
and there are 16000 students.
its dean is thomas pallesen”.
sentence planing: [latinname numberofstudents] [country established] [dean afﬁliation] [city]predicted text: the school of business and social sciences at the aarhus university in aarhus, denmark has16,000 students and its latin name is ”universitas aarhusiensis”.
it was established in 1928 in the country ofdenmark and it is afﬁliated with the european university association.
the current dean is thomas pallesen andit has strong connections with the school also afﬁliated to its dean of business.
and it also home to the school..1432b factoid sentence segmentation.
in order to align meaningful parts of the human-written target text to semantic triples, we ﬁrst segmentthe target sentences into sequences of facts using srl, following xu et al.
(2020).
the aim is to breakdown sentences into sub-sentences (facts) that verbalize as few input triples as possible; the originalsentence can still be fully recovered by concatenating all its sub-sentences.
each fact is represented by asegment of the original text that roughly captures “who did what to whom” in one event.
we ﬁrst parsethe sentences into srl propositions using the implementation of he et al.
(2018).14 we consider eachpredicate-argument structure as a separate fact, where the predicate stands for the event and its argumentsare mapped to actors, recipients, time, place, etc.
(see figure 5).
the sentence segmentation consists oftwo consecutive steps:.
(1) tree construction, where we construct a hierarchical tree structure for all the facts of one sentence,by choosing the fact with the largest coverage as the root and recursively building sub-trees by replacingarguments with their corresponding sub-facts (arg1 in fact1 is replaced by fact2)..(2) argument grouping, where each predicate (fact in tree) with its leaf-arguments corresponds to asub-sentence.
for example, in figure 5, leaf-argument “was” and “a crew member on apollo 8” of fact1are grouped as one sub-sentence..figure 5: semantic role labeling based tree meaning representation and factoid sentence segmentation for text“william anders, who retired on september 1st, 1969, was a crew member on apollo 8.”.
c datasets.
webnlg.
the corpus contains 21k instances (input-text pairs) from 9 different domains (e.g., as-tronauts, sports teams).
the number of input triples ranges from 1 to 7, with an average of 2.9. theaverage number of facts that each text contains is 2.4 (see appendix b).
the corpus contains 272 distinctpredicates.
the vocabulary size for input and output side is 2.6k and 5k respectively..e2e nlg.
the corpus contains 50k instances from the restaurant domain.
we automatically convertthe original attribute-value pairs to triples: for each instance, we take the restaurant name as the subjectand use it along with the remaining attribute-value pairs as corresponding predicates and objects.
thenumber of triples in each input ranges from 1 to 7 with an average of 4.4. the average number of factsthat each text contains is 2.6. the corpus contains 9 distinct predicates.
the vocabulary size for inputs andoutputs is 120 and 2.4k respectively.
we also tested our approach on an updated cleaned release (duˇseket al., 2019)..d hyperparameters.
webnlg.
both encoder and decoder are a 2-layer 4-head transformer, with hidden dimension of256. the size of token embeddings and predicate embeddings is 256 and 128, respectively.
the adamoptimizer (kingma and ba, 2015) is used to update parameters.
for both the baseline model and thepre-train of the hmm-based model, the learning rate is 0.1. during the training of the hmm-based model,.
14the code can be found in https://allennlp.org with 86.49 test f1 on the ontonotes 5.0 dataset..1433william anders, who retired on september 1st, 1969, a crew member on apollo 8.fact1-was: [arg1: william anders, who retired on september 1st, 1969], [v: was] [arg2: a crew member on apollo 8]fact2-retired: [arg0: william anders], [r-arg0: who] [v: retired] [argm-tmp: on september 1st, 1969], was a crew member on apollo 8fact1-wasarg1varg2fact2-retiredarg0r-arg0vargm-tmpwasa crew member on apollo 8william anderswhoretiredon september 1st, 1969srl representationtree mrwilliam anders, who retired on september 1st, 1969was a crew member on apollo 8sentence segmentation:the learning rate for the encoder-decoder ﬁne-tuning and the training of the transition distributions is setas 0.002 and 0.01, respectively..e2e.
both encoder and decoder are a transformer with hidden dimension of 128. the size of tokenembeddings and predicate embeddings is 128 and 32, respectively.
the rest hyper-parameters are samewith webnlg..e parameterization: emission distribution.
figure 6: the transformer encoder takes linearized triples and produces contextual embeddings we assume that,at time step t, the transformer decoder is generating fact yt conditioned on zt.
the number of latent variables ltis 1. in other words, zt = ot1.
if the value of ot1 is the predicate of the ﬁrst triple (solid borders), then the secondtriple (dashed borders) is masked out for the encoder-decoder attention during decoding..f full experiment results on e2e.
modeltgen(cid:7)transformeragggenagggen−odagggen−agtgen(cid:7)transformeragggenagggen−odagggen−ag.
train test bleu.
nist met.
r-l.add miss wrong.
ser.
lanigiro.naelc.naelc.naelc.39.2338.5741.0638.2430.4440.7338.6239.8838.2826.92.
6.02175.75556.20685.95094.63556.17116.08046.17046.00274.2877.
36.9735.9237.9136.5637.9937.7636.0337.3536.9436.60.
55.5255.4555.1351.5349.9456.0954.8254.0351.5547.95.cider.
1.76231.66761.84431.65250.93591.85181.75441.81931.63970.9205.
00.4002.1302.0402.9408.7100.0703.1501.1001.7405.99.
03.5905.7103.3803.6701.6000.7204.5601.8502.7401.54.
00.0700.5100.6402.1800.8700.0801.3201.2500.6202.31.
04.0508.3506.0608.8011.2400.8709.0204.2105.1109.98.table 8: evaluation of generation (middle) and factual correctness (right) on the e2e nlg data (see section 5for metrics decription)..g annotation interface.
figure 7: an example of the fact-triple alignment task (highlights correspond to facts)..1434apollooperator[cls]nasa[sep]apollobackuppilottransformer encodercontextual embeddingsinput triplestransformer decoderbuzzaldrin[sep]