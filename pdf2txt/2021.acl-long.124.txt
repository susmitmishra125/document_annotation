improving formality style transfer with context-aware rule injection.
zonghai yaouniversity of massachusetts amherstzonghaiyao@cs.umass.edu.
hong yuuniversity of massachusetts lowellhong yu@uml.edu.
abstract.
models pre-trained on large-scale regular textcorpora often do not work well for user-generated data where the language styles dif-fer signiﬁcantly from the mainstream text.
here we present context-aware rule injec-tion (cari), an innovative method for formal-ity style transfer (fst).
cari injects multi-ple rules into an end-to-end bert-based en-coder and decoder model.
it learns to selectoptimal rules based on context.
the intrin-sic evaluation showed that cari achieved thenew highest performance on the fst bench-mark dataset.
our extrinsic evaluation showedthat cari can greatly improve the regular pre-trained models’ performance on several tweetsentiment analysis tasks..1.introduction.
many user-generated data deviate from standardlanguage in vocabulary, grammar, and languagestyle.
for example, abbreviations, phonetic sub-stitutions, hashtags, acronyms, internet language,ellipsis, and spelling errors, etc are common intweets (ghani et al., 2019; muller et al., 2019; hanet al., 2013; liu et al., 2020).
such irregularityleads to a signiﬁcant challenge in applying existinglanguage models pre-trained on large-scale corporadominated with regular vocabulary and grammar.
one solution is using formality style transfer (fst)(rao and tetreault, 2018), which aims to transferthe input text’s style from the informal domain tothe formal domain.
this may improve the down-stream nlp applications such as information ex-traction, text classiﬁcation and question answering.
a common challenge for fst is low resource(wu et al., 2020; malmi et al., 2020; wang et al.,2020).
therefore, approaches that integrate exter-nal knowledge, such as rules, have been developed.
however, existing work (rao and tetreault, 2018;wang et al., 2019) deploy context-insensitive rule.
injection methods (ciri).
as shown in figure 1,when we try to use ciri-based fst as the prepro-cessing for user-generated data in the sentimentclassiﬁcation task, according to the rule detectionsystem, ”extro” has two suggested changes ”extra”or ”extrovert” and ”intro” corresponds to either ”in-troduction” or ”introvert.” the existing ciri-basedfst models would arbitrarily choose rules follow-ing ﬁrst come ﬁrst served (fcfs).
as such, theinput ”always, always they think i an extro, but ima big intro actually” could be translated wrongly as”they always think i am an extra, but actually, i ama big introduction.” this leads to the wrong senti-ment classiﬁcation since the fst result completelydestroys the original input’s semantic meaning..in this work, we propose context-aware ruleinjection (cari), an end-to-end bert-based en-coder and decoder model that is able to learn toselect optimal rules based on context.
as shownin figure 1, cari chooses rules based on context.
with cari-based fst, pre-trained models can per-form better on the downstream natural languageprocessing (nlp) tasks.
in this case, cari outputsthe correctly translated text ”they always think iam an extrovert, but actually, i am a big introvert,”which helps the bert-based classiﬁcation modelhave the correct sentiment classiﬁcation..in this study, we performed both intrinsic andextrinsic evaluation of existing fst models andcompared them with the cari model.
the intrin-sic evaluation results showed that cari improvedthe state-of-the-art results from 72.7 and 77.2 to74.31 and 78.05, respectively, on two domains of afst benchmark dataset.
for the extrinsic evalua-tion, we introduced several tweet sentiment analy-sis tasks.
considering that tweet data is typical in-formal user-generated data, and regular pre-trainedmodels are usually pre-trained on formal englishcorpora, using fst as a preprocessing step of tweetdata is expected to improve the performance of reg-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1561–1570august1–6,2021.©2021associationforcomputationallinguistics1561figure 1: an example of using context-insensitive rule injection (ciri) and context-aware rule injection(cari) fst models.
ciri models are not context aware and therefore select rules arbitrarily and in this case,apply the rules first come first serve (fcfs).
the errors introduced (”extra” and ”introduction”) in the cirimodel impact the downstream nlp tasks, and in this case leading to the incorrect sentiment classiﬁcation.
incari, rules are associated with context and through training, cari can learn to choose the right rules accordingto the context.
this leads to improved fst thereby improves the downstream sentiment classiﬁcation tasks..ular pre-trained models on tweet downstream tasks.
we regard measuring such improvement as the ex-trinsic evaluation.
the extrinsic evaluation resultsshowed that using cari model as the prepocess-ing step improved the performance for both bertand roberta on several downstream tweet senti-ment classiﬁcation tasks.
our contributions are asfollows:.
1. we propose a new method, cari, to integraterules for pre-trained language models.
cariis context-aware and can be trained end-to-endwith the downstream nlp applications..2. we have achieved new state-of-the-art resultsfor fst on the benchmark gyafc dataset.
3. we are the ﬁrst to evaluate fst methodswith extrinsic evaluation and we show thatcari outperformed existing rule-based fstapproaches for sentiment classiﬁcation..2 related work.
rule-based formality style transferin thepast few years, style-transfer generation has at-tracted increasing attention in nlp research.
earlywork transfers between modern english and theshakespeare style with a phrase-based machinetranslation system (xu et al., 2012).
recently, styletransfer has been more recognized as a control-lable text generation problem (hu et al., 2017),where the style may be designated as sentiment(fu et al., 2018), tense (hu et al., 2017), or evengeneral syntax (bao et al., 2019; chen et al., 2019).
formality style transfer has been mostly driven bythe grammarly’s yahoo answers formality cor-pus (gyafc) (rao and tetreault, 2018).
since it isa parallel corpus, fst usually takes a seq2seq-likeapproach (niu et al., 2018; xu et al., 2019).
exist-ing research attempts to integrate the rules into the.
1562user-generated input: always, always they think i an extro, but im a big intro actuallyrule detection systemextro→ extra    extro→ extrovert   im    → i amintro → introductionintro → introvertcontext-insensitive rule injection (ciri) context-aware rule injection (cari) always, always they think i an extro, but im a big intro actuallyalways, always they think i an extra, but im a big intro actuallyalways, always they think i an extra, but i am a big intro actuallyalways, always they think i an extra, but i am a big introduction actuallyinfo1: i an extra , butinfo2: i an extrovert , butinfo3: , but i am a biginfo4: a big introduction actuallyinfo5: a big introvert actuallyinfo1 + info2 + info3 + info4 + info5encoder-decodermodeluser-generated input + ciri outputuser-generated input + cari outputinput + ciri output:always, always they think i an extro, but im a big intro actually [sep]  always, always they think i an extra, but i am a big introduction actuallyciri fst result:they always think i am an extra, but actually, i am a big introduction (incorrect fst result which fails the downstream tasks)input + cari output:always, always they think i an extro, but im a big intro actually [sep]  i an extra , but [sep] i an extrovert , but [sep] , but i am a big [sep] a big introduction actually [sep] a big introvert actuallycari fst result:they always think i am an extrovert, but actually, i am a big introvert (correct fst result which helps the downstream tasks)cari fst resultciri fst result(context window size = 2)encodeencodedecodedecodebert-basedtweet classificationmodeldownstream nlp tasks(e.g.
tweet classification)ciri outputcari outputuser-generated inputciri fst resultcari fst resultinputinputoutputoutputincorrect classificationcorrect classificationincorrect classificationinputoutputmodel because the gyafc is low resource.
how-ever, rule matching and selection are context insen-sitive in previous methods (wang et al., 2019).
thispaper focuses on developing methods for context-aware rule selection..evaluating style transfer previous work onstyle transfer (xu et al., 2012; jhamtani et al., 2017;niu et al., 2017; sennrich et al., 2016a) has re-purposed the machine translation metric bleu (pa-pineni et al., 2002) and the paraphrase metric pinc(chen and dolan, 2011) for evaluation.
xu et al.
(2012) introduced three evaluation metrics basedon cosine similarity, language model and logisticregression.
they also introduced human judgmentsfor adequacy, ﬂuency and style (xu et al., 2012;niu et al., 2017).
rao and tetreault (2018) evalu-ated formality, ﬂuency and meaning on the gyafcdataset.
recent work on the gyafc dataset (wanget al., 2019; zhang et al., 2020) mostly used bleuas the evaluation metrics for fst.
however, allaforementioned work focused on intrinsic evalu-ations.
our work has in addition evaluated fstextrinsically for downstream nlp applications..lexical normalisation lexical normalisation(han and baldwin, 2011; baldwin et al., 2015)is the task of translating non-canonical words intocanonical ones.
like fst, lexical normalisationcan also be used to preprocess user-generated data.
the monoise model (van der goot and van noord,2017) is a state-of-the-art model based on feature-based random forest.
the model ranks candidatesprovided by modules such as a spelling checker(aspell), a n-gram based language model and wordembeddings trained on millions of tweets.
unlikefst, monoise and other lexical normalisation mod-els can not change data’s language style.
in thisstudy, we explore the importance of language styletransfer for user-generated data by comparing theresults of monoise and fst models on tweets nlpdownstream tasks..improving language models’ performance foruser-generated data user-generated data oftendeviate from standard language.
in addition tothe formality style transfer, there are some otherways to solve this problem (eisenstein, 2013).
fine-tuning on downstream tasks with a user-generateddataset is most straightforward, but this is not easyfor many supervised tasks without a large amountof accurately labeled data.
another method is toﬁne-tune pre-trained models on the target domain.
corpora (gururangan et al., 2020).
however, it alsorequires sizable training data, which could be re-source expensive (sohoni et al., 2019; dai et al.,2019; yao et al., 2020)..3 approach.
for the downstream nlp tasks where input isuser-generated data, we ﬁrst used the fst modelfor preprocessing, and then ﬁne-tuned the pre-trained models (bert and roberta) with boththe original data dori and the fst data df st ,which were concatenated with a special token[sep ], forming an input like (dori[sep ]df st ).
for the formality style transfer task, we use thebert-initialized encoder paired with the bert-initialized decoder (rothe et al., 2020) as theseq2seq model.
all weights were initialized froma public bert-base checkpoint (devlin et al.,2019).
the only variable that was initialized ran-domly is the encoder-decoder attention.
here, wedescribe cari and several baseline methods ofinjecting rules into the seq2seq model..3.1 no rule (nr).
first we ﬁne-tuned the bert model with onlythe original user-generated input.
given an infor-mal input xi and formal output yi, we ﬁne-tunedthe model with {(xi, yi)}mi=0, where m is the num-ber of data..3.2 context insensitive methods.
for baseline models, we experimented with twostate-of-the-art methods for injecting rules.
we fol-lowed rao and tetreault (2018) to create a set ofrules to convert original data xi to prepossesseddata x(cid:48)i by rules, and then ﬁne-tune the model withparallel data {(x(cid:48)i=0.
this is called rulebase (rb) method.
the prepossessed data, how-ever, serves as a markov blanket, i.e., the systemis unaware of the original data, provided that onlythe prepossessed one is given.
therefore, the ruledetection system could easily make mistakes andintroduce noise..i, yi)}m.wang et al.
(2019) improved the rb by con-catenating the original text xi with the text pro-cessed by rules x(cid:48)i with a special token [sep ] inbetween, forming a input like (xi [sep ] x(cid:48)i).
inthis way, the model can make use of a rule detec-tion system but also recognize its errors during theﬁne-tuning.
this is called rule concatenation(rcat) method.
however, both rb and rcat.
1563methods are context insensitive, the rules were se-lected arbitrarily.
in figure 1 ciri part, ”extra”and ”introduction” were incorrectly selected.
thisgreatly limits the performance of the rule-basedmethods..3.3 context-aware rule injection (cari).
as shown in figure 1, the input of cari con-sists of the original sentence xi and supplementaryinformation.
suppose that ri is an exhaustive listof the rules that are successfully matched on xi.
we make ri = {(ti,j, ci,j, ai,j)}nj=0, where n is thetotal number of matched rules in ri.
here, ti,j andci,j are the corresponding matched text and con-text in the original sentence, respectively, for everymatched rule in ri, and ai,j are the correspondingalternative texts for every matched rule in ri.
eachsupplementary information is composed of one al-ternative text ai,j and its corresponding context ci,j.
we connect all the supplementary information withthe special token [sep ] and then connect it afterthe original input.
in this way, we form an input like(xi[sep ] ai,1, ci,1 [sep ]... [sep ] ai,j, ci,j).
finally, the concatenated sequence and the corre-sponding formal reference yi serve as a parallel textpair to ﬁne-tune the seq2seq model.
like rcat,cari can also use rule detection system and recog-nize its errors during the ﬁne-tuning.
furthermore,since we keep all rules in the input, cari is able todynamically identify which rule to use, maximizingthe use of the rule detection system..4 experimental setup.
4.1 datasets.
for the intrinsic evaluation, we used the gyafcdataset.1 it consists of handcrafted informal-formalsentence pairs in two domains, namely, entertain-ment & music (e&m) and family & relationship(f&r).
table 1 shows the statistics of the training,validation, and test sets for the gyafc dataset.
inthe validation and test sets of gyafc, each sen-tence has four references.
for better exploring thedata requirements of different methods to combinerules, we followed zhang et al.
(2020) and used theback translation method (sennrich et al., 2016b) toobtain additional 100,000 data for training.
for ruledetection system, we used the grammarbot api,2,and grammarly3 to help us create a set of rules..1https://github.com/raosudha89/gyafc-corpus2https://www.grammarbot.io/3https://www.grammarly.com/.
fst gyafc dataset.
entertainment & music.
family & relationship.
train52,59551,967.valid2,8772,788.affect in tweets ei-oc.
anger.
fear.
joy.
sadness.
irony-a.
irony-b.
irony detection.
train1,7012,2521,6161,533.train30673067.valid388389290397.valid767767.test1,4161,322.test1,0029861,105975.test784784.table 1: the data statistics for gyafc dataset of for-mality style transfer task and tweet nlp downstreamclassiﬁcation datasets..for the extrinsic evaluation, we used twodatasets for sentiment classiﬁcation: semeval-2018 task 1: affect in tweets ei-oc (mohammadet al., 2018), and task 3: irony detection in englishtweets (van hee et al., 2018).
table 1 shows thestatistics of the training, validation, and test set forthe two datasets.
we normalized two tweet nlpclassiﬁcation datasets by translating word tokensof user mentions and web/url links into special to-kens @user and httpurl, respectively, andconverting emotion icon tokens into correspondingstrings..4.2 fine-tuning models.
we employed the transformers library (wolfet al., 2019) to independently ﬁne-tune the bert-based encoder and decoder model for each methodin 20,000 steps (intrinsic evaluation), and ﬁne-tunethe bert-based and roberta-based classiﬁcationmodels for each tweet sentiment analysis task in10,000 steps (extrinsic evaluation).
we used theadam algorithm (kingma and ba, 2014) to trainour model with a batch size 32. we set the learn-ing rate to 1e-5 and stop training if validation lossincreases in two successive epoch.
we computedthe task performance every 1,000 steps on the val-idation set.
finally, we selected the best modelcheckpoint to compute the performance score onthe test set.
we repeated this ﬁne-tuning processthree times with different random seeds and re-ported each ﬁnal test result as an average over thetest scores from the three runs.
during inference,we use beam search with a beam size of 4 and beam.
1564figure 2: the performance (bleu) of different rule injection methods with different training size.
the resultsshow that: 1) cari achieved the best results in both e&m and f&r domains.
2) rb, rcat and cari achievedoptimal performance on less training size compared with nr, indicating the advantages of integrating rules tomitigate the low resource challenge.
3) compared with rb and rcat, cari required slightly larger training sizedue to its context-aware learning model..width of 6 to generate sentences.
the whole exper-iment is carried out on 1 titanx gpu.
each fstmodel ﬁnished training within 12 hours..4.3.intrinsic evaluation baselines.
we used two state-of-the-art models, which werealso relevant to our methods, as the strong intrinsicbaseline models..rulegpt like rcat, wang et al.
(2019) aimedto solve the problem of information loss and noisecaused by directly using rules as normalization inpreprocessing.
they put forward the gpt (radfordet al., 2019) based methods to concatenate the orig-inal input sentence and the sentence preprocessedby the rule detection system.
like the ciri meth-ods (rb, rcat), their methods could not make fulluse of rules since they were also context-insensitivewhen selecting rules..bt + m-task + f-dis zhang et al.
(2020) usedthree data augmentation methods, back translation(sennrich et al., 2016b), formality discrimination,and multi-task transfer to solve the low-resourceproblem.
in our experiments, we also use the backtranslation method to obtain additional data be-cause we want to verify the impact on the amountof training data required when using different meth-ods to combine rules..4.4 extrinsic evaluation baselines.
bert (devlin et al., 2018) and roberta (liuet al., 2019) are two typical regular language mod-els pre-trained on large-scale regular formal textcorpora, like bookscorpus (zhu et al., 2015) andenglish wikipedia.
the user-generated data, suchas tweets, deviate from the formal text in vocab-ulary, grammar, and language style.
as a result,regular language models often perform poorly onuser-generated data.
fst aims to generate a formalsentence given an informal one, while keeping itssemantic meaning.
a good fst result is expectedto make regular language models perform betteron user-generated data.
for the extrinsic evalu-ation, we chose bert and roberta as the ba-sic model.
we introduced several tweet sentimentanalysis tasks to explore the fst models’ abilityto transfer the user-generated data from the infor-mal domain to the formal domain.
ideally, fst re-sults for tweet data can improve the performance ofbert and roberta on tweet sentiment analysistasks.
we regard measuring such improvement asthe extrinsic evaluations.
besides, tweet data havemuch unique information, like emoji, hashtags,ellipsis, etc., which are not available in the gyafcdataset.
so in the extrinsic evaluation result anal-ysis, although the ﬁnal scores of fst-bert andfst-roberta were good, we paid more attentionto the improvement of their performance beforeand after using fst, rather than the scores..we used two different kinds of state-of-the-art.
156530k40k50k60k70k80k90k100k110k120k130k140k150ktraining size0.660.680.70.720.74bleuentertainment & music (e&m)nrrbrcatcari30k40k50k60k70k80k90k100k110k120k130k140k150ktraining size0.640.660.680.700.720.740.760.78bleufamily & relationship (f&r)nrrbrcatcariirony detection (evaluation metrics: f1).
irony-airony-b.
ucdcc72.450.7.bert monoise rcat cari72.571.850.948.6.
72.248.8.
72.250.2.roberta monoise rcat cari73.753.8.
72.651.2.
73.153.3.
72.651.affect in tweets ei-oc (evaluation metrics: pearson r).
joyangersadfear.
seernet7270.671.763.7.bert monoise rcat cari70.469.17271.668.366.869.266.9.
68.671.766.466.8.
69.771.967.467.1.roberta monoise rcat cari73.572.270.171.4.
72.972.369.170.5.
71.571.76869.4.
71.87268.269.8.table 2: the extrinsic evaluation results on tweet sentiment analysis tasks.
through observation, we can ﬁnd that1) compared with the previous state-of-the-art results, the results of using bert and roberta directly were oftenvery poor.
2) monoise can not effectively improve the results of bert and roberta, while fst method can.
3)compared with rcat, cari can better improve the results of bert and roberta on user-generated data..model.
no editrulegptbt + m-task + f-disnrrbrcatcari.
e&m f&rbleu bleu51.6750.28.
72.7.
72.63.
71.94.
72.01.
73.01.
74.31.
77.26.
77.01.
75.65.
75.67.
77.37.
78.05.table 3: the comparison of our approaches to the state-of-the-art results on the gyafc test set..methods as our extrinsic evaluation baselines..seernet and ucdcc we used the best resultsin the semeval-2018 workshop as the ﬁrst compar-ison method.
for the task affect in tweets ei-o,the baseline is seernet (duppada et al., 2018), andfor the task irony detection in english tweets, thebaseline is ucdcc (ghosh and veale, 2018)..monoise monoise (van der goot and van no-ord, 2017) is the state-of-the-art model for the lex-ical normalization (baldwin et al., 2015), whichaimed to translate non-canonical words into canon-ical ones.
like the fst model, monoise can alsobe used as the prepossessing step in tweet classiﬁ-cation tasks to normalize tweet input.
so we usedmonoise as another comparison method..5 experimental results.
5.1.intrinsic evaluation.
figure 2 showed the validation performance onboth the e&m and the f&r domain.
compared to.
the nr, the rb did not signiﬁcantly improve.
aswe discussed above, even though the rule detectionsystem will bring some useful information, it willalso make mistakes and introduce noise.
rb has noaccess to the original data, so it cannot distinguishhelpful information from noise and mistakes.
onthe contrary, both rcat and cari have accessto the original data, so their results improved a lotcompared with rb.
cari had a better result com-pared to the rcat.
this is because rcat is con-text insensitive while cari is context-aware whenselecting rules to modify the original input.
there-fore, cari is able to learn to select optimal rulesbased on context, while rcat may miss usingmany correct rules with its pipeline prepossessingstep for rules..figure 2 also showed the relationship betweenthe different methods and the different training size.
compared with the nr method, the three methodswhich use rules can reach their best performancewith smaller training size.
this result showed thepositive effect of adding rules in the low-resourcesituation of the gyafc dataset.
moreover, cariused larger training set to reach its best perfor-mance than rb and rcat, since it needed moredata to learn how to dynamically identify whichrule to use..in table 4, we explored how large the contextwindow size was appropriate for the cari methodon gyafc dataset.
the results showed that forboth domains when the window size reaches two(taking two tokens each from the text before andafter), seq2seq model can well match all rules withthe corresponding position in the original input and.
1566context window size for cari5074.5e&m 68.1.
374.6.
274.2.
474.3.
172.5.f&r.
70.5.
74.3.
76.9.
77.5.
76.8.
77.3.table 4: cari performance (bleu) by different con-text window size.
when the context window size reach2, the model can make good use of the rules’ informa-tion..select the correct one to use..5.2 extrinsic evaluation.
table 2 showed the effectiveness of using thecari as the preprocessing step for user-generateddata on applying regular pre-trained models (bertand roberta) on the downstream nlp tasks..compared with the previous state-of-the-art re-sults (ucdcc and seernet), the results of usingbert and roberta directly were often very poor,since bert and roberta were only pre-trainedon regular text corpora.
tweet data has the very dif-ferent vocabulary, grammar, and language stylefrom the regular text corpora, so it is hard forbert and roberta to have good performancewith small amount of ﬁne-tuning data..the results of rcat and cari showed that fstcan help bert and roberta improve their per-formance on tweet data, because they can transfertweets into more formal text while keeping theoriginal intention as much as possible.
cari per-formed better than rcat, which was also in linewith the results of intrinsic evaluation.
this resultalso showed the rationality of our extrinsic evalua-tion metrics..comparing the results of monoise with bertand roberta, the input prepossessed by monoisecan not help the pre-trained model to improve ef-fectively.
we think that this is because the lexi-cal normalization models represented by monoiseonly translate non-canonical words on tweet datainto canonical ones.
therefore, monoise can ba-sically solve the problem of different vocabularybetween regular text corpora and user-generateddata, but it can not effectively solve the problemof different grammar and language style.
as a re-sult, for bert and roberta, even though thereis no out-of-vocabulary (oov) problem in the in-put data processed by monoise, they still can notaccurately understand the meaning of the input..this result conﬁrmed the previous view thatlexical normalization on tweets is a lossy trans-.
lation task (owoputi et al., 2013; nguyen et al.,2020).
on the contrary, the positive results of thefst methods also showed that fst is more suit-able as the downstream task prepossessing step ofuser-generated data.
because fst models needto transfer the informal language style to a formalone while keeping its semantic meaning, whichmakes a good fst model can ideally handle allthe problems from vocabulary, grammar, and lan-guage style.
this can help most language modelspre-trained on the regular corpus, like bert androberta, perform better on user-generated data..5.3 manual analysis.
the prior evaluation results reveal the rela-tive performance differences between approaches.
here, we identify trends per and between ap-proaches.
we sample 50 informal sentences totalfrom the datasets and then analyze the outputs fromeach model.
we present several representative re-sults in table 5..examples 1 and 2 showed that, for bert androberta, fst models are more suitable for pre-processing user-generated data than lexical normal-ization models.
in example 1, both methods caneffectively deal with the problem at the vocabularylevel (”2” to ”to,” ”ur” to ”your,” and ”u” to ”you”).
however, in example 2, fst can further transformsource data into a more familiar language stylefor bert and roberta, which is not available inthe current lexical normalization methods such asmonoise..example 3 showed the importance of injectingrules into the fst models.
the word ”idiodic” is amisspelling of ”idiotic,” which is an oov.
there-fore, without the help of rules, the model can notunderstand the source data’s meanings and pro-duced the wrong ﬁnal output ”i do not understandyour question.”.
example 4 showed the importance of context forrule selection.
the word ”concern” provides therequired context to understand that ”exo” refers toan ”extra” ticket.
so the cari-based model canchoose the right one (”exo” to ”extra”)..examples 5 and 6 showed the shortcomings ofcari.
in example 5, the rule detection system didnot provide the information that the ”ﬁdy center”should be ”50 cent (american rapper)”, so caridelivered the wrong result.
even though carihelps mitigate the data low resource challenge, itfaces the challenge on its own.
cari depends.
1567example 2: source:.
example 1: source:.
monoise:fst:.
explain 2 ur parents that u really want 2 act !!!
explain to your parents that you really want to act !
explain to your parents that you want to act .
my observation skills???
wow, very dumb......monoise: my observation skills ?
wow, very dumb .
verymy observation skills are very bad .
fst:hell no your idiodic for asking .
i do not understand your question .
absolutely not and i feel you are idiotic for asking .
got exo to share, concert in hk !
u interested ?
have you got exo to share, concert in hong kong .
are you interested ?
i got extra to share , concert in hong kong .
are you interested ?
ﬁdy cent he is ﬁne and musclar50 cent is ﬁne and muscular .
ﬁdy cent is ﬁne and muscular .
if my pet bird gets too ﬂappy, my pet kitty cat might eatyif my pet bird gets too ﬂappy, my pet kitty cat might eat itif my pet bird gets too ﬂappy, my pet kitty cat might eat me.
example 3: source:.
nr:cari:example 4: source:rcat:cari:example 5: source:target:cari:example 6: source:target:cari:.
table 5: sample model outputs.
example 1 shows that both monoise and fst models can handle some simplestmodiﬁcations.
example 2 shows that fst can transform the language style of user-generated data, while monoisecan not.
example 3 shows that nr-based fst can not understand the source because of oov noises in the data,while cari-based fst can understand with rules.
example 4 shows the importance of context for rule selection.
the word ”concern” provides the required context to understand that ”exo” refers to an ”extra” ticket.
in example5, the rule detection system did not provide the information that the ”ﬁdy center” should be ”50 cent (americanrapper)”, so cari makes the wrong result.
in example 6, cari mistakenly selected the rule ”eat me.”.
on the quality of the rules, and in this case, norule exists that links ”ﬁdy” to ”50.” in example 6,cari mistakenly selected the rule ”eat me,” butnot ”eat it.” this example also demonstrates thedata sparsity that cari faces.
here ”eat me” ismore commonly used than ”eat it.”.
6 conclusions.
in this work, we proposed the context-awarerule injection(cari), an innovative method forformality style transfer (fst) by injecting multiplerules into an end-to-end bert-based encoder anddecoder model.
the intrinsic evaluation showedour cari method achieved the highest perfor-mance with previous metrics on the fst bench-mark dataset.
besides, we were the ﬁrst to evaluatefst methods with extrinsic evaluation and specif-ically on the sentiment classiﬁcation tasks.
theextrinsic evaluation results showed that using thecari-based fst as the preprocessing step outper-formed existing rule-based fst approaches.
ourresults showed the rationality of adding such exten-sive evaluation..acknowledgments.
the authors are grateful to hadi amiri (univer-sity of massachusetts, lowell) for his expert helpin processing twitter data, and to umass bionlpgroup for lots of meaningful discussions..this work was supported in part by the centerfor intelligent information retrieval.
any opinions,ﬁndings and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reﬂect those of the sponsor..references.
timothy baldwin, marie-catherine de marneffe,bo han, young-bum kim, alan ritter, and wei xu.
2015. shared tasks of the 2015 workshop on noisyuser-generated text: twitter lexical normalizationand named entity recognition.
in proceedings of theworkshop on noisy user-generated text, pages 126–135..yu bao, hao zhou, shujian huang, lei li, lilimou, olga vechtomova, xinyu dai, and jiajunchen.
2019. generating sentences from disentan-gled syntactic and semantic spaces.
arxiv preprintarxiv:1907.05789..1568david chen and william b dolan.
2011. collectinginhighly parallel data for paraphrase evaluation.
proceedings of the 49th annual meeting of the asso-ciation for computational linguistics: human lan-guage technologies, pages 190–200..mingda chen, qingming tang, sam wiseman, andkevin gimpel.
2019. controllable paraphrase gen-eration with a syntactic exemplar.
arxiv preprintarxiv:1906.00565..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc v le, and ruslan salakhutdinov.
2019. transformer-xl: attentive language mod-els beyond a ﬁxed-length context.
arxiv preprintarxiv:1901.02860..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..venkatesh duppada, royal jain, and sushant hiray.
2018. seernet at semeval-2018 task 1: domainarxiv preprintadaptation for affectarxiv:1804.06137..in tweets..jacob eisenstein.
2013. what to do about bad languageon the internet.
in proceedings of the 2013 confer-ence of the north american chapter of the associa-tion for computational linguistics: human languagetechnologies, pages 359–369..zhenxin fu, xiaoye tan, nanyun peng, dongyan zhao,and rui yan.
2018. style transfer in text: explo-ration and evaluation.
in proceedings of the aaaiconference on artiﬁcial intelligence, volume 32..norjihan abdul ghani, suraya hamid,.
ibrahimabaker targio hashem, and ejaz ahmed.
2019. so-cial media big data analytics: a survey.
computersin human behavior, 101:417–428..aniruddha ghosh and tony veale.
2018..ironymag-net at semeval-2018 task 3: a siamese network forirony detection in social media.
in proceedings ofthe 12th international workshop on semantic eval-uation, pages 570–575..rob van der goot and gertjan van noord.
2017.monoise: modeling noise using a modular normal-ization system.
arxiv preprint arxiv:1710.03476..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:.
adapt language models to domains and tasks.
arxivpreprint arxiv:2004.10964..bo han and timothy baldwin.
2011. lexical normali-sation of short text messages: makn sens a# twitter.
in proceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies, pages 368–378..bo han, paul cook, and timothy baldwin.
2013. lex-acmical normalization for social media text.
transactions on intelligent systems and technology(tist), 4(1):1–27..zhiting hu, zichao yang, xiaodan liang, ruslansalakhutdinov, and eric p xing.
2017. toward con-in international con-trolled generation of text.
ference on machine learning, pages 1587–1596.
pmlr..harsh jhamtani, varun gangal, eduard hovy, and ericnyberg.
2017. shakespearizing modern languageusing copy-enriched sequence-to-sequence models.
arxiv preprint arxiv:1707.01161..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..yinan liu, wei shen, zonghai yao, jianyong wang,zhenglu yang, and xiaojie yuan.
2020. named en-tity location prediction combining twitter and web.
ieee transactions on knowledge and data engi-neering..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..eric malmi, aliaksei severyn, and sascha rothe.
2020.unsupervised text style transfer with padded maskedlanguage models.
arxiv preprint arxiv:2010.01054..saif mohammad, felipe bravo-marquez, mohammadsalameh, and svetlana kiritchenko.
2018. semeval-2018 task 1: affect in tweets.
in proceedings of the12th international workshop on semantic evaluation,pages 1–17..benjamin muller, benoˆıt sagot, and djam´e seddah.
2019. enhancing bert for lexical normalization.
inproceedings of the 5th workshop on noisy user-generated text (w-nut 2019), pages 297–306..dat quoc nguyen, thanh vu, and anh tuan nguyen.
2020. bertweet: a pre-trained language model forenglish tweets.
arxiv preprint arxiv:2005.10200..xing niu, marianna martindale, and marine carpuat.
2017. a study of style in machine translation: con-trolling the formality of machine translation output.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2814–2819..1569yunli wang, yu wu, lili mou, zhoujun li, and wen-han chao.
2019. harnessing pre-trained neural net-works with rules for formality style transfer.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3573–3578, hong kong, china.
association for computa-tional linguistics..yunli wang, yu wu, lili mou, zhoujun li, and wen-han chao.
2020.formality style transfer withshared latent space.
in proceedings of the 28th inter-national conference on computational linguistics,pages 2236–2249..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, et al.
2019. huggingface’s transformers: state-of-the-art natural language processing.
arxiv, pagesarxiv–1910..yu wu, yunli wang, and shujie liu.
2020. a datasetfor low-resource stylized sequence-to-sequence gen-eration.
in thirty-fourth aaai conference on artiﬁ-cial intelligence..ruochen xu, tao ge, and furu wei.
2019. formalitystyle transfer with hybrid textual annotations.
arxivpreprint arxiv:1903.06353..wei xu, alan ritter, william b dolan, ralph grish-man, and colin cherry.
2012. paraphrasing for style.
in proceedings of coling 2012, pages 2899–2914..zonghai yao, liangliang cao, and huapu pan.
2020.zero-shot entity linking with efﬁcient long range se-quence modeling.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing: findings, pages 2517–2522..yi zhang, tao ge, and xu sun.
2020. parallel data aug-mentation for formality style transfer.
arxiv preprintarxiv:2005.07522..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee inter-national conference on computer vision, pages 19–27..xing niu, sudha rao, and marine carpuat.
2018.multi-task neural models for translating betweenstyles within and across languages.
arxiv preprintarxiv:1806.04357..olutobi owoputi, brendan o’connor, chris dyer,kevin gimpel, nathan schneider, and noah aimproved part-of-speech tagging forsmith.
2013.online conversational text with word clusters.
inproceedings of the 2013 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 380–390..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
techni-cal report, openai..sudha rao and joel tetreault.
2018. dear sir ormadam, may i introduce the gyafc dataset: cor-pus, benchmarks and metrics for formality styletransfer.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), pages 129–140,new orleans, louisiana.
association for computa-tional linguistics..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-quence generation tasks.
transactions of the asso-ciation for computational linguistics, 8:264–280..rico sennrich, barry haddow, and alexandra birch.
2016a.
controlling politeness in neural machinein proceedings oftranslation via side constraints.
the 2016 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 35–40..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..nimit sharad sohoni, christopher richard aberger,megan leszczynski, jian zhang, and christopherr´e.
2019. low-memory neural network training: atechnical report.
arxiv preprint arxiv:1904.10631..cynthia van hee, els lefever, and v´eronique hoste.
2018. semeval-2018 task 3: irony detection in en-in proceedings of the 12th interna-glish tweets.
tional workshop on semantic evaluation, pages 39–50..1570