intrinsic bias metrics do not correlate with application bias.
seraphina goldfarb-tarrant∗†.
rebecca marchant∗†.
ricardo mu ˜noz s´anchez∗†.
mugdha pandya∗†.
adam lopez‡†.
†university of edinburgh, ‡rasa technologies gmbhs.tarrant@ed.ac.uk{rebecca.marchant31, ricardoms.math, pandya.mugdha4}@gmail.coma.lopez@rasa.com.
abstract.
natural language processing (nlp) systemslearn harmful societal biases that cause themto amplify inequality as they are deployed inmore and more situations.
to guide effortsat debiasing these systems, the nlp commu-nity relies on a variety of metrics that quan-tify bias in models.
some of these metrics areintrinsic, measuring bias in word embeddingspaces, and some are extrinsic, measuring biasin downstream tasks that the word embeddingsenable.
do these intrinsic and extrinsic met-rics correlate with each other?
we compareintrinsic and extrinsic metrics across hundredsof trained models covering different tasks andexperimental conditions.
our results show noreliable correlation between these metrics thatholds in all scenarios across tasks and lan-guages.
we urge researchers working on de-biasing to focus on extrinsic measures of bias,and to make using these measures more feasi-ble via creation of new challenge sets and an-notated test data.
to aid this effort, we releasecode, a new intrinsic metric, and an annotatedtest set focused on gender bias in hate speech.1.
1.introduction.
awareness of bias in natural language processing(nlp) systems has rapidly increased as more andmore systems are discovered to perpetuate societalunfairness at massive scales.
this awareness hasprompted a surge of research into measuring andmitigating bias, but this research suffers from lackof consistent metrics that discover and measurebias.
instead, work on bias is “rife with unstatedassumptions” (blodgett et al., 2020) and relies onmetrics that are easy to measure rather than metricsthat meaningfully detect bias in applications..∗ equal contribution.
correspondence to s.tarrant@.
ed.ac.uk.
1https://tinyurl.com/serif-embed.
(a) intrinsic metrics summarize biases in the geometryof embeddings.
for example, in this embedding space,male words are closer to words about career and aboutmath & science, whereas female words are closer towords about family..(b) extrinsic bias metrics summarize disparities in appli-cation performance across populations, such as rates offalse negatives between different gender groups.
for ex-ample, a coreference system may make more errors inan anti-stereotypical career coreferent (red arc) than in apro-stereotypical one (green arc)..figure 1: the relationship between intrinsic bias met-rics (a) and extrinsic bias metrics (b) has been assumed,but not conﬁrmed..a recent comprehensive survey of bias in nlp(blodgett et al., 2020) found that one third of all re-search papers focused on bias in word embeddings.
this makes embeddings the most common topicin studies of bias — over twice as common as anyother topic related to bias in nlp.
as is visualisedin figure 1a, bias in embedding spaces is mea-sured with intrinsic metrics, most commonly withthe word embedding association test (weat)(caliskan et al., 2017), which relates bias to thegeometry of the embedding space.
once embed-dings are incorporated into an application, bias canbe measured via extrinsic metrics (figure 1b) that.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1926–1940august1–6,2021.©2021associationforcomputationallinguistics1926test whether the application performs differently onlanguage related to different populations.
hence,research on debiasing embeddings relies cruciallyon a hypothesis that doing so will remove or re-duce bias in downstream applications.
however,we are aware of no prior research that conﬁrms thishypothesis..this untested assumption leaves nlp bias re-search in a precarious position.
research into thesemantics of word embeddings has already shownthat intrinsic metrics (e.g.
using analogies and se-mantic similarity, as in hill et al., 2015) do notcorrelate well with extrinsic metrics (faruqui et al.,2016).
research into the bias of word embeddingslacks the same type of systematic study, and thusas a ﬁeld we are exposed to three large risks: 1)making misleading claims about the fairness of oursystems, 2) concentrating our efforts on the wrongproblem, and most importantly, 3) feeling a falsesense of security that we are making more progresson the problem than we are.
our bias research canbe rigorous and innovative, but unless we under-stand the limitations of metrics we use to evaluateit, it might have no impact..in this paper, we ask: does the commonly usedintrinsic metric for embeddings (weat) corre-late with extrinsic metrics of application bias?
to answer this question, we analyse the relation-ship between intrinsic and extrinsic bias.
our studyconsiders two languages (english and spanish),two common embedding algorithms (word2vec andfasttext) and two downstream tasks (coreferenceresolution and hatespeech detection)..while we ﬁnd a moderately high correlation be-tween these metrics in a handful of conditions, weﬁnd no correlation or even negative correlation inmost conditions.
therefore, we recommend thatthe ethical scientist or engineer does not rely onintrinsic metrics when attempting to mitigate bias,but instead focuses on the harms of speciﬁc appli-cations and test for bias directly..as additional contributions to these ﬁndings, werelease new weat metrics for spanish, and a newgender-annotated test set for hatespeech detectionfor english, both of which we created in the courseof this research..2 bias metrics.
in all of our experiments, we compute correlationsbetween commonly-used metrics, both intrinsicand extrinsic..2.1.intrinsic bias metrics.
intrinsic bias metrics are applied directly to wordembeddings, formulating bias in terms of geomet-ric relationships between concepts such as male,female, career, or family.
each concept is in turnrepresented by curated wordlists.
for example, theconcept male is represented by words like brother,father, grandfather, etc.
while the concept math &science is represented by words like programmer,engineer, etc..the most commonly used metric is weat(caliskan et al., 2017).2, which measures the differ-ence in mean cosine similarity between two targetconcepts x and y ; and two attribute concepts aand b. this difference represents the imbalancein associations between concepts.
using (cid:126)w to rep-resent the embedding of word w, we have a teststatistic:.
s(x, y, a, b) =.
s(x, a, b) −.
s(y, a, b).
(cid:88).
x∈x.
(cid:88).
y∈y.
where.
s(w, a, b) = meana∈a.
cos( (cid:126)w, (cid:126)a) − meanb∈b.
cos( (cid:126)w,(cid:126)b).
this is normalised by the standard deviation to getthe effect size which we use in our experiments..weat was initially developed as an indicatorof bias, to show that the implicit association test(iat) from the ﬁeld of psychology (greenwaldet al., 1998) can be replicated via word embeddingsmeasurements.
there are thus 10 original testschosen to replicate the tests presented to humansubjects in iat.
the tests measure different kindsof biased associations, such as african-americannames vs. white names with pleasant vs. unpleas-ant terms, and female terms vs. male terms withcareer vs. family words..weat was later repurposed as a predictor ofbias in embedding spaces, via a somewhat muddylogical journey.
it has since been translated into 6other languages (xweat; lauscher and glavas,2019), and extended to operate on full sentences(may et al., 2019) and on contextual language mod-els (kurita et al., 2019).
when weat is used as ametric, papers report the effect size of the subset oftests relevant to the task at hand, each separately..there are known issues with weat, such as sen-sitivity to corpus word frequency, and sensitivity.
2we count 34 papers from *cl and fat* conferencessince january 2020 that use weat or seat (may et al., 2019)in their methodology..1927to target and attribute wordlists, as found by sedocand ungar (2019) and ethayarajh et al.
(2019).
thelatter proposes an alternative more theoretically ro-bust metric, relational inner product association(ripa), which uses the principal component of agender subspace (determined via the method ofbolukbasi et al.
(2016)) to directly measure how”gendered” a word is.
we have chosen to use themost common version of weat for this ﬁrst empir-ical study, since it is most widely used.
it would beinteresting to test ripa in the same way, if it wereextended to more types of bias and more languages.
but we note that all intrinsic metrics are sensitiveto chosen wordlists, so this must be done carefully,especially across languages, a topic we will returnto in section 4.3..2.2 extrinsic bias metrics.
extrinsic bias metrics measure bias in applications,via some variant of performance disparity, or per-formance gap between groups.
for instance, aspeech recognition system is unfair if it has highererror rates for african-american dialects (tatman,2017), meaning that systems perform less well forthose speakers.
a hiring classiﬁcation system isunfair if it has more false negatives for womenthan for men, meaning that more qualiﬁed womenare accidentally rejected than are qualiﬁed men.3there are two commonly used metrics to quantifythis possible performance disparity: predictive par-ity (hutchinson and mitchell, 2019), which mea-sures the difference in precision for a privileged andnon-privileged group, and equality of opportunity(hardt et al., 2016), which measures the differencein recall between those groups (see appendix afor formal deﬁnitions)..the metric that best identiﬁes bias in a systemvaries based on the task.
for different applications,false negatives may be more harmful, for othersfalse positives may be.
for our ﬁrst task of coref-erence (figure 1b), false negatives — where thesystem fails to identify anti-stereotypical corefer-ence chains (e.g.
women as farmers or as ceos) —are more harmful to the underprivileged class thanfalse positives.
for our second task, hate speechdetection (figure 2), both can be harmful, for dif-ferent reasons.
false positives for one group cansystematically censor certain content, as has beenfound for hate speech detection applied to african-american vernacular english (aave) (sap et al.,.
3https://tinyurl.com/y6c6clzu.
figure 2: examples from twitter hatespeech detection:correct (a), false positive (b), and false negative (c).
this shows both kinds of problematic performance gap.
b) censors harmless text and c) lets a targeted toxiccomment slip through..2019; davidson et al., 2019).
false negatives per-mit abuse of minority populations that are targetsof hate speech.
we examine performance gaps inboth precision and in recall for broad coverage..3 methodology.
each of our experiments measures the correlationbetween a speciﬁc instance of weat and a spe-ciﬁc extrinsic bias metric.
in each experiment, wetrain an embedding, measure the bias according toweat, and measure the bias in the downstreamtask that uses that embedding.
we then modifythe embeddings by applying an algorithm to eitherdebias them, or — by inverting the algorithm’sbehavior — to overbias them.
again we measureweat on the modiﬁed embedding and also thedownstream bias in the target task.
when we havedone this multiple times until we reach a stoppingcondition (detailed below), we compute the corre-lation between the two metrics (via pearson corre-lation and analysis with scatterplots)..rather than draw conclusions from a single ex-periment, we attempt to draw more robust conclu-sions by running many experiments, which varyalong several dimensions.
we consider two com-mon embedding algorithms, two tasks, and twolanguages.
a full table of experiment conditionscan be found in table 1..3.1 debiasing and overbiasing.
we need to measure the relationship between intrin-sic and extrinsic metrics as bias changes, we mustgenerate many datapoints for each experiment.
pre-vious work on bias in embeddings studies methodsto reduce embedding bias.
to generate enoughdata points, we take the novel approach of bothdecreasing and increasing bias in the embeddings.
we measure the baseline bias level, via weat, foreach embedding trained normally on the originalcorpus.
we then adjust the bias up or down, re-measure weat, and measure the change in thedownstream task..1928task.
data.
bias type.
intrinsic metrics.
english coreference ontonotes/winobiasenglish hate speechspanish hate speechspanish hate speech.
twittertwittertwitter.
gendergendergendermigrants xweat migrants (new).
weat 6, 7, 8weat 6, 7, 8xweat 7+8 (new).
table 1: tasks used in our experiments.
each experiment consists of a task, an embedding method (either word2vecor fasttext), an intrinsic metric (one experiment for each listed), and an extrinsic metric (either predictive parity orequality of opportunity).
we run an experiment for all possible combinations.
to produce data points for eachexperiment, we use preprocessing and post-processing methods to debias and overbias the input word embeddings..we choose two methods from previous work thatare capable of both debiasing and overbiasing: theﬁrst is a preprocessing method that operates on thetraining data before training, the second is a post-processing method that operates on the embeddingspace once it has been trained.
this is importantsince both kinds of methods may be used in prac-tice: a large company with proprietary data willtrain embeddings from scratch, and thus may use apreprocessing method; whereas a small companymay rely on publicly available pretrained embed-dings, and thus use a post-processing method.
4.for preprocessing, we use dataset balancing(dixon et al., 2018), which consists of sub-sampling the training data to be more equal withrespect to some attributes.
for instance, if we areadjusting gender bias, we identify pro-stereotypicalsentences5 such as ‘she was a talented house-keeper’ vs. anti-stereotypical sentences, such as‘he was a talented housekeeper’ or ‘she was atalented analyst’.
we sub-sample and reduce thefrequency of the pro-stereotypical collocations todebias, and sub-sample the anti-stereotypical con-ditions to overbias..as a postprocessing method for already trainedembeddings, we use the attract-repel (mrksicet al., 2017) algorithm.
this algorithm was de-.
4there are additional embedding based debiasing methodsused in practice, based on identifying and removing a gendersubspace during training or as postprocessing (bolukbasi et al.,2016; zhao et al., 2018b).
however, these methods do notchange a word’s nearest neighbour clusters (gonen and gold-berg, 2019), and so we would expect these debiasing methodsto show superﬁcial bias changes in weat without changingdownstream bias.
both methods that we select modify theunderlying word distribution and move many words in relationto each other.
we veriﬁed this with tsne visualisation as infigure 1a following gonen and goldberg (2019) and ﬁnd thatour bias modiﬁcation methods do change word clusters..5stereotypes as deﬁned by zhao et al.
(2018a) and bycaliskan et al.
(2017), who use the u.s. bureau of laborstatistics and the implicit association test, respectively..veloped to use dictionary wordlists (synonyms,antonyms) to reﬁne semantic spaces.
it aims tomove similar words (synonyms) close to eachother and dissimilar words (antonyms) farther fromeach other, while keeping a regularisation term topreserve original semantics as much as possible.
lauscher et al.
(2020) used an approach inspiredby attract-repel for debiasing, though with con-straints implemented somewhat differently.
weuse the same pro- and anti-stereotypical wordlistsas in dataset-balancing.
for debiasing, we usethe algorithm to increase distance between pro-stereotypical combinations (she, housekeeper) anddecrease distance between anti-stereotypical com-binations (she, analyst or he, housekeeper).
foroverbiasing we do the reverse.6.
as the stopping condition for preprocessing, weconstrain the sub-sampling so that it does not sub-stantially change the dataset size, by limiting itto removing less than ﬁve percent of the originaldata.
for postprocessing we limit the algorithm tomaximum 5 iterations..3.2 embedding algorithms.
we use two common word embedding algorithms:fasttext (bojanowski et al., 2017) and skip-gramword2vec (mikolov et al., 2013) embeddings.
word embeddings in fasttext are composed fromembeddings of both the word and its subwordsin the form of character n-grams.
lauscher andglavas (2019) suggest that this difference maycause bias to be acquired and encoded differentlyin fasttext and word2vec (we discuss this in moredetail in section 5)..despite recent widespread interest in contextualembeddings (e.g.
bert; devlin et al., 2019), ourexperiments use these simpler contextless embed-.
6wordlists used for bias-modiﬁcation and conﬁgs for.
attract-repel are included in the codebase..1929dings because they are widely available in manytoolkits and used in many real-world applications.
their design simpliﬁes our experiments, whereascontextual embeddings would add signiﬁcant com-plexity.
however, we know that bias is still a prob-lem for large contextual embeddings (zhao et al.,2019, 2020; gehman et al., 2020; sheng et al.,2019), so our work remains important.
if intrin-sic and extrinsic measures do not correlate withsimple embeddings, this result is unlikely to bechanged by adding more architectural layers andconﬁgurable hyperparameters..3.3 downstream tasks.
we use three tasks that appear often in bias lit-erature: coreference resolution for english, hatespeech detection for english, and hate speech detec-tion for spanish.
to make the scenarios as realisticas possible, we use a common, easy-to-implementand high performing architecture for each task: theend-to-end coreference system of lee et al.
(2017)and the the cnn of kim (2014), which has beenused in high-scoring systems in recent hate speechdetection shared tasks (basile et al., 2019).
foreach task, we feed pretrained embeddings to themodel, frozen, and then train the model using thestandard hyperparameters published for each modeland task..3.4 languages.
we experiment on both english and spanish.
itis important to take a language with pervasivegender-marking (spanish) into account, as previouswork has shown that grammatical gender-markinghas a strong effect on gender bias in embeddings(mccurdy and serbetci, 2017; gonen et al., 2019;zhou et al., 2019).
we use spanish only for hatespeech detection, because gender marking makes achallenge-set style coreference evaluation trivial toresolve and not a candidate for detection of genderbias.7.
4 experiments.
4.1 datasets.
to train embeddings, we use domain-matched datafor each downstream task.
for coreference wetrain on wikipedia data, and for hatespeech detec-tion we train on english tweets or spanish tweets,.
7this fact is the premise behind the work of stanovskyet al.
(2019) who use the explicit marking in translation toreveal bias..consistent with the task.8 our english corefer-ence system is trained on ontonotes (weischedelet al., 2017) and evaluated on winobias (zhao et al.,2018a), a winograd-schema style challenge set de-signed to measure gender bias in coreference res-olution.
english hate speech detection uses theabusive tweets dataset of founta et al.
(2018), andis evaluated on the test set of ten thousand tweets,which we have hand labelled as targeted male, fe-male, and neutral (we release this labelled testset for future work).
spanish hate speech detec-tion uses the data from the shared task of basileet al.
(2019), which contains labels for commentsdirected at women and directed at migrants..4.2 weat & bias modiﬁcation wordlists.
both weat and bias modiﬁcation methods dependon seed wordlists.9 these wordlists are closelyrelated to each other, and we match them by type ofbias, such that we measure weat tests for genderbias with embeddings modiﬁed via gender biaswordlists (themselves derived from weat lists, asdetailed below) and weat tests for migrant biaswith embeddings modiﬁed for migrant bias..weat wordlists are standardised, and for en-glish we use the three weat test wordlists (num-bers 6,7,8) for gender.10.
to generate bias modiﬁcation wordlists we fol-low the approach of lauscher et al.
(2020) and usea pretrained set of embeddings (from spacy.io) toexpand the set of weat words to their 100 uniquenearest neighbours.
for all experiments, we takethe union of all weat terms, expand them, anduse this expanded set for both dataset balancingand for attract-repel.11 for gender bias in corefer-ence and hate speech, we use terms that are malevs. female and are career, math, science, vs. fam-ily, art.
for gender bias and migrant bias in span-ish hate speech, we compare male/female iden-tity or migrant/non-migrant identity with pleasant-unpleasant term expansions.12.
8details of datasets & preprocessing are in appendix c.9weat uses wordlists to measure relationships betweenwords in the space, and bias modiﬁcation depends on identi-fying words to sub or supersample (for databalancing), or toadjust (for attract-repel).
many other debiasing methods thatwe did not use (e.g bolukbasi et al.
(2016)) also use wordlists.
10all weat wordlists are in appendix b. we make a smallsubstitution of general gender words instead of proper namesin weat 6, as proper names by design do not appear in ourcoreference task..11final word sets are 200-400 words, due to signiﬁcantoverlap in nearest neighbors & manual removal of odd terms.
12we did additionally experiment with using the exact.
19304.3 new spanish weat.
we substantially modiﬁed spanish weat (akaxweat for non-english weats) and added en-tirely new terms.
the reason for this is that theoriginal xweat was translated from english veryliterally, which causes two problems..the ﬁrst problem with xweat is that many ofthe terms do not make sense in a spanish speakingcommunity — names included in the original, likeamy, are names in spanish and thus were untrans-lated, but are uncommon and have upper class con-notations not intended in the original test.
anotherexample is ﬁrearms translated as arma de fuego,which while technically a correct literal translation,is not commonly used to describe weapons.13.
the second problem with xweat is that nounson the wordlists for both abstract math and scienceconcepts as well as abstract art concepts are almostentirely grammatically female.
for instance, cien-cia (science), geometr´ıa (geometry) are grammati-cally female, as are escultura (sculpture) and nov-ela (novel).
it is well established that for languageswith grammatical gender, words that share a gram-matical gender have embeddings that are closertogether than words that do not (gonen et al., 2019;mccurdy and serbetci, 2017).
so, when weatin english was translated into xweat in spanish(glavas et al., 2019), the terms were imbalancedwith regard to grammatical gender, which makesthe results misleading.
we balance the lists, of-ten replacing abstract nouns with correspondingadjectives which can take male or female form,e.g.
cient´ıﬁco and cient´ıﬁca (scientiﬁc, male andfemale), such that we can use both versions to ac-count for the effect of grammatical gender..finally, we needed a metric to examine biasagainst migrants.
metrics for intrinsic bias mustbe targeted to the type of harm expected in thedownstream application, and there is not an out-of-the-box weat test for this.
so we create a newweat test for bias against migrants in spanish.
following the setup of tests for racial bias in orig-inal weat — based on american racial biasesin english — we have lists of names associatedwith migrants vs. non-migrants, and compare themwith lists of pleasant and unpleasant terms.
thenames are based on work of salamanca and pereira.
weat terms for debiasing, and found the trends to be similarbut of smaller magnitude, so we settled on expanded lists as amore realistic scenario..13the standard would be armas.
arma de fuego is also com-posed of three words, and so will not appear in any vocabulary..(2013), who studied ranking names as lower vs.upper class; class status is closely correlated withwhether a person is a migrant.
we select a subsetof names in which the majority in the study agreeon the class.
pleasant and unpleasant terms exist inweat and xweat, but we again modify them tobalance grammatical gender..5 results.
figure 3 displays data for all tasks: one scatterplotper triple of experimental variables: an intrinsicmetric, an extrinsic metric, an embedding algo-rithm.
if we want to be able to broadly use weatmetrics for any given bias research, these graphsshould each show a clear and a positive correlation.
none of them do.
there are no trends in correlationbetween the metrics that hold in all cases regard-less of experimental detail, for any of the tasks.
wehave additionally examined whether there are cor-relations within one bias modiﬁcation method (preor postprocessing) in case a difference in the waythese methods modify embeddings causes differ-ences in trends.
in most cases this breakout tells thesame story.
the select cases where positive (andnegative) correlations are present are discussed be-low.
further breakout graphs and combinations areincluded in appendix d..coreference (en): gender the coreference task(figure 3, rows 1-3) doesn’t display a clear cor-relation in all cases, and yet it has the clearestrelationship of all three tasks, with a signiﬁcantmoderate positive correlation for both predictiveparity (precision) and equality of opportunity (re-call) for word2vec (columns 3 & 4).
the overalltrends are muddied by the data for fasttext, whichdoes not have a signiﬁcant correlation under anyconditions.
both are expected: that coreferencewould display the strongest trends, and that fast-text would display more unpredictable or weakertrends.
the winobias coreference task is as directlymatched to the weat tests as it is possible to be- since both use common career words to measurebias.
so the relationship between the two metrics isclearest here: moving female terms closer to certaincareer terms most directly helps a system resolveanti-stereotypical coreference chains.
however, westill only see a correlation in wod2vec, not fast-text.
fasttext may behave less predictably becauseof its use of subwords; when subwords are used,.
1931fasttext embeddings.
word2vec embeddings.
precision.
recall.
precision.
recall.
6taew.7taew.8taew.6taew.7taew.8taew.rednegtaew.tnargimtaew.ecnereferochsilgne.noitceted.hceeps.etah.hsilgne.noitcetedhceeps.etahhsinaps.figure 3: experimental results, showing one scatterplot per experiment.
an experiment consists of a task (outerrow label), an embedding (outer column label), an intrinsic metric (inner row label), and an extrinsic metric (innercolumn label).
each point in a scatterplot is the intrinsic (y-axis) and extrinsic (x-axis) measure of bias for a singlerun, where word embeddings for each run have been debiased or overbiased using pre- or post-processing..1932word representations are more interconnected.14we can debias with regard to a speciﬁc word, butthat word’s embedding will still be inﬂuenced byall other words that share its character ngrams.
itis difﬁcult to predict how changing the composi-tion of a training corpus will affect all words thatcontain a certain ngram (e.g.
ch) in them.
forthis reason, fasttext may be initially more resistantto encoding biases than word2vec, as was foundin lauscher and glavas (2019), but may also bemore complex to debias.
this has implications forextending this work to contextual models, whichalways use some form of subword unit..hatespeech (en): gender hatespeech (en) hasfewer and more restricted correlations than corefer-ence, as can be seen in figure 3, rows 4-6. theseplots show no relationship at all between intrinsicand extrinsic metrics.
when data is broken outby bias modiﬁcation method (see figure 4b in ap-pendix d), it becomes clear that there is a moderatepositive correlation for postprocessing for recall,and the aggregate appears this way because thereis a moderate negative correlation for preprocess-ing.
this holds for both embedding algorithms,though both positive and negative correlations arestronger for fasttext.
precision displays no corre-lation.
note that the absolute variance in recall ismuch smaller than for precision, but this is still sig-niﬁcant for each embedding algorithm individuallyand for both grouped together..of interest for future bias research is that thebaseline level of bias (premodiﬁcation, from rawtwitter data) in english hatespeech differs by em-bedding type, but only for precision.
initial mod-els (with unmodiﬁed embeddings) using fasttexthave 10 additional points of precision for male-targeted hatespeech than for female-targeted.
how-ever initial models using word2vec have the oppo-site bias and have 4 fewer points of precision formale-targeted than female targeted hatespeech.
forrecall, the two embedding algorithms are equiva-lent, with 6 fewer points for male-targeted hate-speech.
in fact, in the recall metric there is an earlyindication of unreliability of the relationship weare examining between weat and extrinsic bias,because there is a spread of different weat resultsthat map to nearly the same difference in recall..14for example, the representation of the word childish is bydesign also made up of the representations for child and ish,but also all unigrams, bigrams, and trigrams it contains (c, ch,chi, etc)..hatespeech (es): gender and migrant forhatespeech in spanish, we examine two kinds ofbias separately — gender bias and bias against mi-grants, in figure 3, rows 7,8. neither gender biasnor migrant bias show positive correlations in anyexperimental conditions..gender bias in our models is in an absolutesense never present, since in spanish hatespeechtargeted against women is easier to identify thanagainst others (with f1 in the high 80s).15 butthere are no overall trends when this is bias is mod-iﬁed to be more or less extreme, and there are nopositive correlations in any conditions.
there isa moderate negative correlation for precision onlywhen looking at fasttext embeddings..migrant bias similarly has no trends save invery restricted conditions broken out by bias modi-ﬁcation type.
in contrast to the gender case, hate-speech against migrants is clearly challenging toidentify, with much lower f1 in the low 60s.
thereis a positive correlation between migrant bias andperformance gap for recall with preprocessing infasttext only.
this ﬁts the expectation that fasttextmay be more sensitive to preprocessing than post-processing due to subwords, as discussed above,though in the gender bias case with negative corre-lation it is equally sensitive to both, so it is hard todraw conclusions.
given the smaller number of dat-apoints for spanish (discussed below) this is likelyjust noise.
to confuse the situation further, the onlytrends in precision are present in word2vec, and arenegative correlations..note that all graphs for spanish display centralclusters, because it was more difﬁcult to get aneven spread of bias measures, and because spanishhas fewer data points than english.
this is for anumber of reasons that compound and underscorethe difﬁculty of expanding supposedly language-agnostic techniques beyond english, even to highresourced languages like spanish.
we have onlyone weat test for each type of bias, since wemade our own that carefully balanced grammaticalgender, after rectifying the issues with the existingtranslated versions (see section 4.3).
bias mod-iﬁcation is also more difﬁcult - the richer agree-ment system in spanish means that there are moresurface forms of what would be one word in en-glish.
in addition, the language model used fornearest neighbour expansion of wordlists (see sec-.
15this is perhaps due to examples in the training data havingclearer markers such as speciﬁc anti-female slurs, but is itselfan interesting question..1933tion 4.2) produces predominantly formal registerwords from news or scientiﬁc articles, due to aless varied makeup of its training data than the en-glish model.
this makes them less well suited todebiasing twitter data speciﬁcally, and there wereno readily available models that had more casualregister.
for bias against migrants, there is the ad-ditional challenge that wordlists are predominantlybased on proper names, which are much rarer intwitter (which tends to use @ mentions instead)than in other media..6 discussion.
the broad result of this research is that changes inweat do not correlate with changes in applicationbias, and therefore that weat should not be usedto measure progress in debiasing algorithms.
wehave found that even when we maximally targetbias modiﬁcation of an embedding, we cannot pro-duce a reliable change in bias direction downstream.
there was no pattern or correlation between tasks,for the same task in different languages, or evenin most cases within one task.
and we have cho-sen one of the simplest possible setups, with full-word embeddings and a single type of bias at atime.
real world scenarios can easily be morecomplicated and involve multiple types of bias orsubword embeddings.
our ﬁndings also indicatethat additional complexity may muddy the relation-ship further.
for example, fasttext behaved lesspredictably than word2vec across experiments, sug-gesting that if we were to expand to larger modelsthat are fully reliant on subwords the patterns maybecome even less clear..the implication of this ﬁnding is that an nlpscientist or engineer has limited options when in-vestigating and mitigating bias.
they must a) ﬁndthe speciﬁc set of wordlists, embedding algorithms,downstream tasks, and bias modiﬁcation methodsthat are together predictive of bias for the giventask, language, and model or b) implement full sys-tems to test application bias directly, even if theirwork focuses on embeddings..while the latter may seem onerous, it may not bemore so than exhaustively searching for a conﬁgu-ration where intrinsic bias metrics are predictive.
this underscores the importance of making gooddownstream bias measures available, as either ap-proach will require these.
more datasets that arecollected need to be annotated with subgroup de-mographic and identity information — there are.
very few available.
more research needs to focuson creating good challenge sets to measure applica-tion bias.
additional research on more broad usageof unsupervised methods (zhao and chang, 2020)would also be valuable, though those also wouldbeneﬁt from subgroup identity annotation to maketheir results more interpretable..it is only when more of these things are readilyavailable that we can see the true measure of theefﬁcacy of our debiasing efforts..we do note a limitation of this study in that alldownstream tasks are discriminative classiﬁcationtasks.
bias in classiﬁcation is more straightfor-ward to measure, with well established metrics, butcovers allocational harms (performance disparity),whereas the inclusion of generative models couldbetter cover representational harms (misleading orharmful representations/portrayals) (blodgett et al.,2020; crawford, 2017).
concurrent research oncausal mediation analysis for bias has shown thatthe embedding layer in open-domain generationhas the strongest effect on gender bias (as com-pared to other layers of the network) (vig et al.,2020).
further work could investigate whether gen-eration tasks have display the same or differentrelationship to intrinsic metrics..7 conclusion.
we have examined the relationship of the intrinsicbias metric weat to the extrinsic bias metrics ofequality of opportunity and predictive parity, formultiple tasks and languages, and determined thatpositive correlations between them exist only invery restricted settings.
in many cases there is ei-ther negative correlation or none at all.
while intrin-sic metrics such as weat remain good descriptivemetrics for computational social science, and forexamining bias in human texts, we advise that thenlp community not rely on them for measuringmodel bias.
we instead advise that they focus oncareful consideration of downstream applicationsand the creation of datasets and challenge sets thatenable measurement at this stage..acknowledgements.
we thank andreas grivas, kate mccurdy, yevgenmatusevych, elizabeth nielsen, ramon sanabria,ida szubert, sabine weber, bj¨orn ross, agostinacalabrese, and eddie ungless for comments onearlier drafts of this paper..1934references.
valerio basile, c. bosco, e. fersini, debora nozza,v. patti, f. pardo, p. rosso, and m. sanguinetti.
2019.semeval-2019 task 5: multilingual detection of hatespeech against immigrants and women in twitter.
insemeval@naacl-hlt..su lin blodgett, solon barocas, hal daum´e iii, andhanna wallach.
2020. language (technology) ispower: a critical survey of “bias” in nlp.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5454–5476, online.
association for computational lin-guistics..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..tolga bolukbasi, kai-wei chang, james y. zou,venkatesh saligrama, and adam tauman kalai.
2016. man is to computer programmer as womanis to homemaker?
debiasing word embeddings.
innips..aylin caliskan,.
and arvindjoanna j bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356:183–186..kate crawford.
2017. the trouble with bias.
(keynote.
at neurips)..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech andabusive language detection datasets.
in proceedingsof the third workshop on abusive language online,pages 25–35, florence, italy.
association for com-putational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..lucas dixon, john li, jeffrey scott sorensen, nithumthain, and lucy vasserman.
2018. measuring andmitigating unintended bias in text classiﬁcation.
inaies ’18..kawin ethayarajh, david duvenaud, and graeme hirst.
2019. understanding undesirable word embeddingin proceedings of the 57th annualassociations.
meeting of the association for computational lin-guistics, pages 1696–1705, florence, italy.
associa-tion for computational linguistics..manaal faruqui, yulia tsvetkov, pushpendre rastogi,and chris dyer.
2016. problems with evaluation.
of word embeddings using word similarity tasks.
in proceedings of the 1st workshop on evaluatingvector-space representations for nlp, pages 30–35, berlin, germany.
association for computationallinguistics..antigoni founta, constantinos djouvas, despoinachatzakou, ilias leontiadis, jeremy blackburn, gi-anluca stringhini, athena vakali, michael siriv-ianos, and nicolas kourtellis.
2018. large scalecrowdsourcing and characterization of twitter abu-sive behavior.
in icwsm..samuel gehman, suchin gururangan, maarten sap,yejin choi, and noah a. smith.
2020. realtoxici-typrompts: evaluating neural toxic degeneration inlanguage models.
in emnlp..goran glavas, robert litschko, sebastian ruder, andivan vulic.
2019. how to (properly) evaluate cross-lingual word embeddings: on strong baselines, com-parative analyses, and some misconceptions.
inacl..hila gonen and yoav goldberg.
2019. lipstick on apig: debiasing methods cover up systematic genderbiases in word embeddings but do not remove them.
in naacl-hlt..hila gonen, yova kementchedjhieva, and yoav gold-berg.
2019. how does grammatical gender affectnoun representations in gender-marking languages?
in proceedings of the 23rd conference on computa-tional natural language learning (conll), pages463–471, hong kong, china.
association for com-putational linguistics..a. greenwald, d. mcghee, and j. l. schwartz.
1998.measuring individual differences in implicit cogni-tion: the implicit association test.
journal of person-ality and social psychology, 74 6:1464–80..moritz hardt, eric price, and nathan srebro.
2016.equality of opportunity in supervised learning.
innips..felix hill, roi reichart, and a. korhonen.
2015.simlex-999: evaluating semantic models with (gen-uine) similarity estimation.
computational linguis-tics, 41:665–695..ben hutchinson and margaret mitchell.
2019.
50 yearsof test (un)fairness: lessons for machine learning.
in fat* ’19..yoon kim.
2014. convolutional neural networks for.
sentence classiﬁcation.
in emnlp..keita kurita, n. vyas, ayush pareek, a. black, and yu-lia tsvetkov.
2019. quantifying social biases in con-textual word representations.
in 1st acl workshopon gender bias for natural language processing..anne lauscher and goran glavas.
2019..arewe consistently biased?
multidimensional analy-sis of biases in distributional word vectors.
in*sem@naacl-hlt..1935jesse vig, sebastian gehrmann, yonatan belinkov,sharon qian, daniel nevo, yaron singer, and stu-art shieber.
2020. investigating gender bias in lan-guage models using causal mediation analysis.
inadvances in neural information processing systems,volume 33, pages 12388–12401.
curran associates,inc..r. weischedel, e. hovy, m. marcus, and marthapalmer.
2017. ontonotes : a large training corpusfor enhanced processing..jieyu zhao and kai-wei chang.
2020. logan: lo-cal group bias detection by clustering.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1968–1977, online.
association for computationallinguistics..jieyu zhao, subhabrata mukherjee, saghar hosseini,kai-wei chang, and ahmed hassan awadallah.
2020. gender bias in multilingual embeddings andcross-lingual transfer.
in acl..jieyu zhao, tianlu wang, mark yatskar, ryan cot-terell, vicente ordonez, and kai-wei chang.
2019.gender bias in contextualized word embeddings.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 629–634,minneapolis, minnesota.
association for computa-tional linguistics..jieyu zhao, tianlu wang, mark yatskar, vicente or-donez, and kai-wei chang.
2018a.
gender biasin coreference resolution: evaluation and debiasingmethods.
in naacl-hlt..jieyu zhao, yichao zhou, z. li, w. wang, and kai-wei chang.
2018b.
learning gender-neutral wordembeddings.
in emnlp..pei zhou, weijia shi, jieyu zhao, kuan-hao huang,muhao chen, ryan cotterell, and kai-wei chang.
2019. examining gender bias in languages withgrammatical gender.
in emnlp/ijcnlp..anne lauscher, goran glavas, simone paolo ponzetto,and ivan vulic.
2020. a general framework for im-plicit and explicit debiasing of distributional wordvector spaces.
in aaai..kenton lee, luheng he, m. lewis, and luke zettle-moyer.
2017. end-to-end neural coreference resolu-tion.
in emnlp..chandler may, alex wang, shikha bordia, samuel r.bowman, and rachel rudinger.
2019. on measur-ing social biases in sentence encoders.
in naacl-hlt..k. mccurdy and oguz serbetci.
2017. grammaticalgender associations outweigh topical gender bias incrosslinguistic word embeddings.
winlp..tomas mikolov, kai chen, gregory s. corrado, andjeffrey dean.
2013. efﬁcient estimation of word rep-resentations in vector space.
corr, abs/1301.3781..nikola mrksic, ivan vulic, diarmuid ´o s´eaghdha, iraleviant, roi reichart, milica gasic, anna korho-nen, and steve j. young.
2017. semantic special-ization of distributional word vector spaces usingmonolingual and cross-lingual constraints.
transac-tions of the association for computational linguis-tics, 5:309–324..gast˜a salamanca and lidia pereira.
2013. presti-gio y estigmatizaci ˜a“n de 60 nombrespropios en 40 sujetos de nivel educa-cional superior.
universum (talca), 28:35 –57..maarten sap, d. card, saadia gabriel, yejin choi, andnoah a. smith.
2019. the risk of racial bias in hatespeech detection.
in acl..jo˜ao sedoc and lyle ungar.
2019. the role of pro-tected class word lists in bias identiﬁcation of con-textualized word representations.
in proceedings ofthe first workshop on gender bias in natural lan-guage processing, pages 55–61, florence, italy.
as-sociation for computational linguistics..emily sheng, kai-wei chang, premkumar natarajan,and nanyun peng.
2019. the woman worked asa babysitter: on biases in language generation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3407–3412, hong kong, china.
association for computa-tional linguistics..gabriel stanovsky, noah a. smith, and luke zettle-moyer.
2019. evaluating gender bias in machinetranslation.
in acl..rachael tatman.
2017. gender and dialect bias inyoutube’s automatic captions.
in ethnlp@eacl..1936a bias metric deﬁnitions & formulas.
performance gap metrics measure difference inperformance across different demographic splits ofthe data, and are in our case (and most commonly)applied to classiﬁcation tasks..where a is a demographic variable (race,gender, etc), y is the true label, and ˆy is thepredicted label, a fair system will satisfy:.
p ( ˆy = 1|a = x, y = 1) = p ( ˆy = 1|a = y, y = 1).
where x and y are demographic values usually ofan privileged and a underprivileged group.
thisexpresses that the probability of a given test sam-ple being correctly identiﬁed as a true positiveshould be equal regardless of group, and is knownas equality of opportunity (hardt et al., 2016).
a fair system will also satisfy:.
p ( ˆy = 1|a = x, y = 0) = p ( ˆy = 1|a = y, y = 0).
which expresses that that probability of a giventest sample being incorrectly identiﬁed as positiveis equal regardless of group.
this is knownas predictive parity and when combined withequality of opportunity is known as equalizedodds..these are easily measured in most nlp sys-tems.
the former is captured by measuringrecall gap, where if x is the privileged groupand y the underprivileged, unfairness is capturedby recallx − recally, where any positivevalue is unfair.
is captured byp recisionx − p recisiony, again where positivevalues are unfair..the latter.
b weat formula and wordlists.
b.1 english weat lists.
all are tests for gender bias..b.1.1 weat 6weat 6 was modiﬁed to use the general genderterms of 7,8 rather than proper names, because theco-reference task contains no names.
male male, man, boy, brother, he, him, his, sonfemaledaughtercareercorporation, salary, ofﬁce, business, career.
executive, management, professional,.
female, woman, girl, sister, she, her, hers,.
family home, parents, children, family, cousins,marriage, wedding, relatives.
the following.
the original weat 6 usesmale and female names as the gender terms:male: john, paul, mike, kevin, steve, greg, jeff,billfemale: amy, joan, lisa, sarah, diana, kate, ann,donna..b.1.2 weat 7.female, woman, girl, sister, she, her, hers,.
male male, man, boy, brother, he, him, his, sonfemaledaughtermath math, algebra, geometry, calculus, equa-tions, computation, numbers, additionart poetry, art, dance, literature, novel, sym-phony, drama, sculpture.
b.1.3 weat 8.sister, mother, aunt, grandmother, daugh-.
male brother, father, uncle, grandfather, son, he,his, himfemaleter, she, hers, herscienceeinstein, nasa, experiment, astronomyart poetry, art, shakespeare, dance, literature,novel, symphony, drama.
science, technology, physics, chemistry,.
b.2 changes to english list.
we modify weat 6 to use the gender terms forweat 7/8 as the terms for 6, but otherwise leaveterms as is..weat 6 (career/family vs. male/female) usesproper names as gender terms, whereas the othertwo tests use more standard gender terms (she, her,he, him, mother, father).
this is an artifact of repli-cating iat, which introduces a confound in theircomparability – if the weat tests have differentpatterns of correlation, we don’t know whether thisis because of the difference in the way gender biaspatterns for career/family vs. for arts/science orwhether it patterns differently because of propernames vs. gender terms.
this is exacerbated inour case where proper names are treated even moredifferently than usual both in twitter (where @men-tions stand in for proper names) and in the wino-bias metric that we use (where professions are usedinstead of proper names precisely because namescontain gender information and the challenge setintends to be ambiguous)..1937b.3 spanish weat lists:.
c training data and preprocessing.
recall that we created these ourselves, the gendertest with reference to both the original gender fo-cused weat 6,7,8 of caliskan et al.
(2017) and thetranslation of lauscher and glavas (2019), signiﬁ-cantly modiﬁed and extended to balance grammati-cal gender across sets of words.
the migrant testwas created with reference to the tests for racismthat use african-american vs. european-americannames paired with pleasant vs. unpleasant terms inweat 3, 4, 5, using the lists of european spanishvs. migrant spanish names identiﬁed by salamancaand pereira (2013)..b.3.1 gender.
male: masculino, hombre, ni˜no, hermano, ´el,hijo, hermano, padre, pap´a, t´ıo, abuelo.
female:hija, hermana, madre, mam´a, t´ıa, abuela.
femenino, mujer, ni˜na, hermana, ella,.
cient´ıﬁco, f´ısico, qu´ımico, astr´onomo,science:tecnol´ogico, bi´ologo, cient´ıﬁca, f´ısica, qu´ımica,astr´onoma, tecnol´ogica, bi´ologaart: arquitecto, escultor, pintor, escritor, po-eta, bailar´ın, actor, fot´ografo, arquitecta, escul-tora, pintora, escritora, poetisa, bailarina, actora,fot´ografa.
b.3.2 migrants.
european-spanish names: agustina, martina,josefa, antonia, sof´ıa, isidora, crist´obal, se-basti´an, agust´ın, alonso, joaqu´ın, le´on, ignacio,julieta, matildemigrant-spanish names: shirley, yamileth,sharon, britney, maryori, melody, nayareth,yaritza, byron, brian, jason, malcon, justin,jeremy, jordan, brayan, yeison, yeremi, bairon,yastin.
caricia, libertad, salud, amor,pleasant terms:paz, animar, amistad, cielo, lealtad, placer, dia-mante, gentil, honestidad, suerte, arcoiris, diploma,regalo, honor, milagro, amanecer, familia, alegr´ıa,felicidad, risa, para´ıso, vacaci´on, paz, maravilloso,maravillosa.
abuso, choque, suciedad, as-unpleasant terms:esinato, enfermedad, accidente, muerte, sufrim-iento, veneno, hedor, apestar, ataque, asalto, desas-tre, odio, contaminaci´on, tragedia, divorcio, c´arcel,pobreza, fea, feo, c´ancer, matar, v´omito, bomba,maldad, podrido, podrida, agon´ıa, terrible, horri-ble, guerra, repugnante.
this details the data for training embeddings.
fordata used in training the ﬁnal models, see relevantpapers cited in section 4.1..c.1 wikipedia.
wikipedia data is downloaded from the latestwikipedia article dump, tokenized with nltk(https://www.nltk.org/), and all words appear-ing less than 10 times are replaced with <unk>.
the ﬁnal dataset has 439,935,872 words..c.2 twitter.
twitter data is from 2019 and is downloadedfrom the internet archive https://archive.org/details/twitterstream.
retweets are removed,and data is lowercased, tokenized with nltktweettokenizer, and hashtags and @mentions arereplaced with <hash> and <mention> respec-tively.
all words appearing less than 10 timesare replaced with <unk>.
english twitter datasize is 3,641,306 tweets with 38,376,060 words.
spanish twitter data size is 10,683,846 tweets with142,715,339 words..d further results graphs.
below are breakouts of graphs by bias modiﬁcationmethod, as well as full graphs with metric scalesand legends..figure 4 breaks out all tasks by bias modiﬁca-tion method (pre- vs. post-processing).
the maininteresting thing to note here is for hatespeech inenglish.
based on the spread of data points, it iseasy to see that there is overall more effect on preci-sion gap when embeddings are modiﬁed, whereasrecall performance gap occupies a narrower bandover a wide spread of weat metrics.
yet recallis the only metric which has a positive correlationwith weat, and then only in the postprocessingcondition.
for spanish it is also visible that it ismuch more difﬁcult to modify bias for spanishwhen preprocessing vs. when postprocessing..figure 5 shows one graph for each task and biastype combination, in full, in order to view the effectof not controlling for experimental variable.
it alsoshows the scale for the spread of data points..finally, for interest, we also include figure 6,which displays the correlation broken out by typeof winobias test (which differ in difﬁculty becausetype 1 is semantic and type 2 is syntactic)..1938(a) coreference (en) results broken out by bias modiﬁcation method (pre- vs. post-processing)..(b) hatespeech (en) results broken out by bias modiﬁcation method (pre- vs. post-processing)..(c) hatespeech (es) results for gender bias metrics broken out by bias modiﬁcation method..figure 4: bias modiﬁcation method breakout by pre vs. post-processing for gender bias for each task for bothprecision and recall..1939(a) coreference results (en), gender bias.
(b) hatespeech detection results (en), gender bias.
(c) hatespeech detection results (es), gender bias.
(d) hatespeech detection results (es), migrant bias.
figure 5: scatterplots showing all data points for each of the 4 tasks: gender bias in co-reference (en), gender biasin hatespeech detection (en), gender bias in hatespeech detection (es), and migrant bias in hatespeech detection (es).
in each plot, the x-axis represents weat, and the y-axis shows performance gap between groups (male-female,female-other, migrant-other).
original embeddings (before modiﬁcation) shown in black.
there is no correlationthat holds independently of experimental conditions (embedding type, bias modiﬁcation method, weat test)..figure 6: coreference (en) results broken out by type of winobias challenge, type 1 is more difﬁcult as there areonly semantic cues to correct coreference, type 2 has also syntactic cues..1940