improving pretrained cross-lingual language models viaself-labeled word alignment.
zewen chi†∗, li dong‡, bo zheng‡∗, shaohan huang‡xian-ling mao†, heyan huang†, furu wei‡†beijing institute of technology‡microsoft research{czw,maoxl,hhy63}@bit.edu.cn{lidong1,v-zhebo,shaohanh,fuwei}@microsoft.com.
abstract.
the cross-lingual language models are typi-cally pretrained with masked language model-ing on multilingual text or parallel sentences.
in this paper, we introduce denoising wordalignment as a new cross-lingual pre-trainingtask.
speciﬁcally, the model ﬁrst self-labelsword alignments for parallel sentences.
thenwe randomly mask tokens in a bitext pair.
given a masked token,the model uses apointer network to predict the aligned tokenin the other language.
we alternately per-form the above two steps in an expectation-maximization manner.
experimental resultsshow that our method improves cross-lingualtransferability on various datasets, especiallyon the token-level tasks, such as question an-swering, and structured prediction.
more-over,the model can serve as a pretrainedword aligner, which achieves reasonably lowerror rates on the alignment benchmarks.
thecode and pretrained parameters are available atgithub.com/czwin32768/xlm-align..1.introduction.
despite the current advances in nlp, most applica-tions and resources are still english-centric, mak-ing non-english users hard to access.
therefore, itis essential to build cross-lingual transferable mod-els that can learn from the training data in high-resource languages and generalize on low-resourcelanguages.
recently, pretrained cross-lingual lan-guage models have shown their effectiveness forcross-lingual transfer.
by pre-training on monolin-gual text and parallel sentences, the models providesigniﬁcant improvements on a wide range of cross-lingual end tasks (conneau and lample, 2019; con-neau et al., 2020; liu et al., 2020; chi et al., 2021b).
cross-lingual language model pre-training is typ-ically achieved by learning various pretext tasks on.
∗contribution during internship at microsoft research..monolingual and parallel corpora.
by simply learn-ing masked language modeling (mlm; devlin et al.
2019) on monolingual text of multiple languages,the models surprisingly achieve competitive resultson cross-lingual tasks (wu and dredze, 2019; ket al., 2020).
besides, several pretext tasks areproposed to utilize parallel corpora to learn bettersentence-level cross-lingual representations (con-neau and lample, 2019; chi et al., 2021b; hu et al.,2020a).
for example, the translation language mod-eling (tlm; conneau and lample 2019) task per-forms mlm on the concatenated parallel sentences,which implicitly enhances cross-lingual transfer-ability.
however, most pretext tasks either learnalignment at the sentence level or implicitly en-courage cross-lingual alignment, leaving explicitﬁne-grained alignment task not fully explored..in this paper, we introduce a new cross-lingualpre-training task, named as denoising word align-ment.
rather than relying on external word alignerstrained on parallel corpora (cao et al., 2020; zhaoet al., 2020; wu and dredze, 2020), we utilizeself-labeled alignments in our task.
during pre-training, we alternately self-label word alignmentsand conduct the denoising word alignment task inan expectation-maximization manner.
speciﬁcally,the model ﬁrst self-labels word alignments for atranslation pair.
then we randomly mask tokensin the bitext sentence, which is used as the per-turbed input for denosing word alignment.
foreach masked token, the model learns a pointer net-work to predict the self-labeled alignments in theother language.
we repeat the above two steps toiteratively boost the bitext alignment knowledgefor cross-lingual pre-training..we conduct extensive experiments on a widerange of cross-lingual understanding tasks.
experi-mental results show that our model outperforms thebaseline models on various datasets, particularlyon the token-level tasks such as question answer-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3418–3430august1–6,2021.©2021associationforcomputationallinguistics3418ing and structured prediction.
moreover, our modelcan also serve as a multilingual word aligner, whichachieves reasonable low error rates on the bitextalignment benchmarks..our contributions are summarized as follows:.
• we present a cross-lingual pre-trainingparadigm that alternately self-labels and pre-dicts word alignments..• we introduce a pre-training task, denoisingword alignment, which predicts word align-ments from perturbed translation pairs..• we propose a word alignment algorithm thatformulates the word alignment problem asoptimal transport..• we demonstrate that our explicit alignmentobjective is effective for cross-lingual transfer..2 related work.
cross-lingual lm pre-training pretrained withmasked language modeling (mlm; devlin et al.
2019) on monolingual text, multilingual bert(mbert; devlin et al.
2019) and xlm-r (con-neau et al., 2020) produce promising resultson cross-lingual transfer benchmarks (hu et al.,2020b).
mt5 (xue et al., 2020) learns a multilin-gual version of t5 (raffel et al., 2020) with text-to-text tasks.
in addition to monolingual text, severalmethods utilize parallel corpora to improve cross-lingual transferability.
xlm (conneau and lample,2019) presents the translation language modeling(tlm) task that performs mlm on concatenatedtranslation pairs.
alm (yang et al., 2020) intro-duces code-switched sequences into cross-linguallm pre-training.
unicoder (huang et al., 2019) em-ploys three cross-lingual tasks to learn mappingsamong languages.
from an information-theoreticperspective, infoxlm (chi et al., 2021b) proposesthe cross-lingual contrastive learning task to alignsentence-level representations.
additionally, am-ber (hu et al., 2020a) introduces an alignmentobjective that minimizes the distance between theforward and backward attention matrices.
more re-cently, ernie-m (ouyang et al., 2020) presents theback-translation masked language modeling taskthat generates pseudo parallel sentence pairs forlearning tlm, which provides better utilizationof monolingual corpus.
veco (luo et al., 2020)pretrains a uniﬁed cross-lingual language modelfor both nlu and nlg.
mt6 (chi et al., 2021a).
improves the multilingual text-to-text transformerwith translation pairs..notably, word-aligned bert models (cao et al.,2020; zhao et al., 2020) ﬁnetune mbert by anexplicit alignment objective that minimizes the dis-tance between aligned tokens.
wu and dredze(2020) exploit contrastive learning to improve theexplicit alignment objectives.
however, wu anddredze (2020) show that these explicit alignmentobjectives do not improve cross-lingual representa-tions under a more extensive evaluation.
moreover,these models are restricted to stay close to theiroriginal pretrained values, which is not applicablefor large-scale pre-training.
on the contrary, wedemonstrate that employing our explicit alignmentobjective in large-scale pre-training can provideconsistent improvements over baseline models..word alignment the ibm models (brown et al.,1993) are statistical models for modeling the trans-lation process that can extract word alignments be-tween sentence pairs.
a large number of wordalignment models are based on the ibm mod-els (och and ney, 2003; mermer and sarac¸lar,2011; dyer et al., 2013; ¨ostling and tiedemann,2016).
recent studies have shown that word align-ments can be extracted from neural machine trans-lation models (ghader and monz, 2017; koehn andknowles, 2017; li et al., 2019) or from pretrainedcross-lingual lms (jalili sabet et al., 2020; nagataet al., 2020)..3 method.
figure 1 illustrates an overview of our methodfor pre-training our cross-lingual lm, which iscalled xlm-align.
xlm-align is pretrainedin an expectation-maximization manner with twoalternating steps, which are word alignment self-labeling and denoising word alignment.
we ﬁrstformulate word alignment as an optimal transportproblem, and self-label word alignments of the in-put translation pair on-the-ﬂy.
then, we update themodel parameters with the denoising word align-ment task, where the model uses a pointer net-work (vinyals et al., 2015) to predict the alignedtokens from the perturbed translation pair..3.1 word alignment self-labeling.
the goal of word alignment self-labeling isto estimate the word alignments of the inputtranslation pair on-the-ﬂy, given the currentxlm-align model.
given a source sentence.
3419figure 1: an overview of our method.
xlm-align is pretrained in an expectation-maximization manner withtwo alternating steps.
(a) word alignment self-labeling: we formulate word alignment as an optimal transportproblem, and self-labels word alignments of the input translation pair on-the-ﬂy; (b) denoising word alignment:we update the model parameters with the denoising word alignment task, where the model uses a pointer networkto predict the aligned tokens from the perturbed translation pair..+.
s = s1 .
.
.
si .
.
.
sn and a target sentence t =t1 .
.
.
tj .
.
.
tm, we model the word alignment be-tween s and t as a doubly stochastic matrixa ∈ rn×msuch that the rows and the columnsall sum to 1, where aij stands for the probabilityof the alignment between si and tj.
the rows andthe columns of a represent probability distribu-tions of the forward alignment and the backwardalignment, respectively.
to measure the similaritybetween two tokens from s and t , we deﬁne ametric function fsim by using cross-lingual repre-sentations produced by xlm-align:fsim(si, tj) = − log max((cid:15), h(cid:62).
(1).
i hj).
where (cid:15) is a constant to avoid negative values in thelog function, and hi is the hidden vector of the i-thtoken by encoding the concatenated sequence of sand t with xlm-align.
empirically, the metricfunction produces a high similarity score if the twoinput tokens are semantically similar..the word alignment problem is formulated asﬁnding a that maximizes the sentence similaritybetween s and t :n(cid:88).
m(cid:88).
aijfsim(si, tj).
(2).
maxa.i=1.
j=1.
we can ﬁnd that eq.
(2) is identical to the regular-ized optimal transport problem (peyr´e et al., 2019),.
if we add an entropic regularization to a:.
maxa.n(cid:88).
m(cid:88).
i=1.
j=1.
aijfsim(si, tj) − µaij log aij.
(3).
eq.
(3) has a unique solution a∗ such that.
a∗ = diag(u)kdiag(v)kij = efsim(si,tj )/µ.
(4).
(5).
+, v ∈ rm.
+ , k ∈ rn×m.
where u ∈ rn+ .
accordingto sinkhorn’s algorithm (peyr´e et al., 2019), thevariables u and v can be calculated by the follow-ing iterations:.
ut+1 =.
1nkvt , vt+1 =.
1mk(cid:62)ut+1.
(6).
where vt can be initialized by vt=0 = 1m..with the solved stochastic matrix a∗, we can−→a by apply-.
produce the forward word alignmentsing argmax over rows:.
−→a = {(i, j) | j = arg max.
a∗.
ik}.
k.(7).
←−a can besimilarly, the backward word alignmentscomputed by applying argmax over columns.
toobtain high-precision alignment labels, we adopt aniterative alignment ﬁltering operation.
we initialize.
3420(a) word alignment self-labelingxlm-align encoder你好世界。helloworld.你好<-> hello世界<->world。<-> .
self-labeled word alignmentsxlm-align encoderalignment probability你好[m]。[m]world.你好世界。helloworld.alignment as optimal transporttranslationpairnoisy translation pair (random masks)pointer networkquerykeysdwa loss(b) denoising word alignmenta∗.
ij ←.
ij, ∃k (i, k) ∈ a ∨ (k, j) ∈ a.ldwa =.
ce(ai, a(i)).
(15).
(cid:88).
i∈p.
(9).
where ce(·, ·) stands for the cross-entropy loss,and a(i) is the self-labeled aligned position of thei-th token..the alignment labels a as ∅.
in each iteration, wefollow the procedure of itermax (jalili sabet et al.,←−a by eq.
(7).
2020) that ﬁrst computesthen, the alignment labels are updated by:.
−→a and.
a ← a ∪ (.
−→a ∩.
←−a ).
(8).
finally, a∗ is updated by:.
.
.
0,αa∗a∗ij,.
(i, j) ∈ a.others.
where α is a discount factor.
after several itera-tions, we obtain the ﬁnal self-labeled word align-ments a..3.2 denoising word alignment.
after self-labeling word alignments, we update themodel parameters with the denoising word align-ment (dwa) task.
the goal of dwa is to predictthe word alignments from the perturbed version ofthe input translation pair..consider the perturbed version of the input trans-lation pair (s ∗, t ∗) constructed by randomly re-placing the tokens with masks.
we ﬁrst encodethe translation pair into hidden vectors h∗ with thexlm-align encoder:.
1 .
.
.
h∗h∗.
n+m = encoder([s ∗, t ∗]).
(10).
where [s ∗, t ∗] is the concatenated sequence of s ∗and t ∗ with the length of n + m. then, we builda pointer network upon the xlm-align encoderthat predicts the word alignments.
speciﬁcally, forthe i-th source token, we use h∗i as the query vectorand h∗n+m as the key vectors.
given thequery and key vectors, the forward alignment prob-ability ai is computed by the scaled dot-productattention (vaswani et al., 2017):.
n+1, .
.
.
, h∗.
ai = softmax(.
q(cid:62)i k√dh.
).
qi = linear(h∗i )n+1 .
.
.
h∗k = linear([h∗.
n+m]).
(11).
(12).
(13).
and masked positions as queries.
formally, weuse the following query positions in the pointernetwork:.
p = {i|(i, ·) ∈ a ∨ (·, i) ∈ a} ∩ m (14).
where m is the set of masked positions.
thetraining objective is to minimize the cross-entropybetween the alignment probabilities and the self-labeled word alignments:.
algorithm 1 pre-training xlm-aligninput: multilingual corpus dm, parallel corpus.
dp, learning rate τ.output: xlm-align parameters θ.
1: initialize θ with cold-start pre-training2: while not converged do3:.
x ∼ dm, (s, t ) ∼ dpa ← fself-labeling(s, t ; θ)g ← ∇θlmlm(x ) + ∇θltlm(s, t ) +.
4:.
5:.
∇θldwa(s, t , a)θ ← θ − τ g.6:.
3.3 pre-training xlm-align.
we illustrate the pre-training procedure of xlm-align in algorithm 1. in addition to dwa, wealso include mlm and tlm for pre-training xlm-align, which implicitly encourage the cross-lingual alignment.
the overall loss function isdeﬁned as:.
lmlm(x ) + ltlm(s, t ) + ldwa(s, t , a).
in each iteration, we ﬁrst sample monolingual textx , and parallel text (s, t ).
then, we self-labelword alignments and update the model parametersby learning pretext tasks.
notice that the model pa-rameters are initialized by a cold-start pre-trainingto avoid producing low-quality alignment labels.
the cold-start pre-training can be accomplished byusing a pretrained lm as the model initialization..where dh is the dimension of the hidden vectors.
similarly, the backward alignment probability canbe computed by above equations if we use target to-kens as the query vectors and h∗n as key vec-tors.
notice that we only consider the self-labeled.
1 .
.
.
h∗.
4 experiments.
4.1 pre-training.
following previous cross-lingual pretrained mod-els (conneau and lample, 2019; conneau et al.,.
3421model.
metrics.
mbert*xlm*mt5basexlm-rbasexlm-align.
f1.
70.370.1-75.676.0.structured predictionpos.
ner.
question answeringmlqa.
xquad.
tydiqa.
sentence classiﬁcationxnli.
paws-x.
avg.
f1.
62.261.256.661.863.7.f1 / em.
f1 / em.
f1 / em.
acc..64.5 / 49.459.8 / 44.367.0 / 49.071.9 / 56.474.7 / 59.0.
61.4 / 44.248.5 / 32.664.6 / 45.065.1 / 47.268.1 / 49.8.
59.7 / 43.943.6 / 29.158.1 / 42.855.4 / 38.362.1 / 44.8.
65.469.175.475.076.2.acc..81.980.987.484.986.8.
63.158.6-66.468.9.table 1: evaluation results on xtreme structured prediction, question answering, and sentence classiﬁcationtasks.
we adopt the cross-lingual transfer setting, where models are only ﬁne-tuned on the english training databut evaluated on all target languages.
results with “*” are taken from (hu et al., 2020b).
results of xlm-alignand xlm-rbase are averaged over ﬁve runs..2020; chi et al., 2021b), we use raw sentencesfrom the wikipedia dump and ccnet (wenzeket al., 2019) for mlm, including 94 languages.
for tlm and dwa, we use parallel corporafrom multiun (ziemski et al., 2016), iit bom-bay (kunchukuttan et al., 2018), opus (tiede-mann, 2012), and wikimatrix (schwenk et al.,2019), including 14 english-centric language pairs.
we pretrain a transformer with 12 layers andthe hidden size of 768, where the parameters areinitialized with xlm-r (conneau et al., 2020).
the model is optimized with the adam opti-mizer (kingma and ba, 2015) for 150k steps withbatch size of 2, 048. notice that tlm and dwashare the same forward procedure for encoding theperturbed sentence pair.
the pre-training of xlm-align takes about six days with two nvidia dgx-2 stations.
more details of the training data and thehyperparameters are in supplementary document..4.2 xtreme benchmark.
xtreme is a multilingual benchmark for eval-uating cross-lingual generalization.
we evaluateour model on 7 cross-lingual downstream tasks in-cluded by xtreme, which can be grouped into3 categories: (1) structured prediction: part-of-speech tagging on the universal dependenciesv2.5 (zeman et al., 2019), and named entity recog-nition on the wikiann (pan et al., 2017; rahimiet al., 2019) dataset; (2) question answering: cross-lingual question answering on mlqa (lewis et al.,2020) and xquad (artetxe et al., 2020), and goldpassage of typologically diverse question answer-ing (tydiqa-goldp; clark et al.
2020); (3) sen-tence classiﬁcation: cross-lingual natural languageinference (xnli; conneau et al.
2018), and cross-lingual paraphrase adversaries from word scram-bling (paws-x; yang et al.
2019)..baselines we use the following pretrained cross-lingual lms as baselines.
(1) multilingual bert(mbert; devlin et al.
2019) is pretrained withmasked language modeling (mlm) and next sen-tence prediction on wikipedia of 104 languages;(2) xlm (conneau and lample, 2019) is jointlypretrained with mlm on 100 languages and trans-lation language modeling (tlm) on 14 languagepairs; (3) mt5 (xue et al., 2020) is the multilingualversion of t5 pretrained with text-to-text tasks;(4) xlm-r (conneau et al., 2020) is pretrainedwith mlm on large-scale cc-100 dataset with longtraining steps..fine-tuning following hu et al.
(2020b), weadopt the zero-shot transfer setting for evaluation,where the models are only ﬁne-tuned on englishtraining data but evaluated on all target languages.
besides, we only use one model for evaluation onall target languages, rather than selecting differentmodels for each language.
the detailed ﬁne-tuninghyperparameters can be found in supplementarydocument..resultsin table 1, we present the evaluation re-sults on xtreme structured prediction, questionanswering, and sentence classiﬁcation tasks.
it canbe observed that our xlm-align obtains the bestaverage score over all the baseline models, improv-ing the previous score from 66.4 to 68.9. it demon-strates that our model learns more transferable rep-resentations for the cross-lingual tasks, which isbeneﬁcial for building more accessible multilin-gual nlp applications.
it is worth mentioning thatour method brings noticeable improvements on thequestion answering and the structured predictiontasks.
compared with xlm-rbase, xlm-alignprovides 6.7% and 1.9% f1 improvements on ty-diqa and ner.
the improvements show that the.
3422alignment method.
pretrainedmodel.
alignment error rate ↓.
en-de.
en-fr.
en-hi.
en-ro.
fast align (dyer et al., 2013)simalign - argmax (jalili sabet et al., 2020)simalign - itermax (jalili sabet et al., 2020)simalign - itermax (reimplementation)ours - optimal transport (section 3.1).
-xlm-rxlm-rxlm-rxlm-r.32.1419.
20.
20.1517.74.simalign (reimplementation)ours - optimal transport (section 3.1).
xlm-align 18.93xlm-align 16.63.
19.467.
9.
10.057.54.
10.336.61.
59.9039.
39.
38.7237.79.
33.8433.98.
-29.
28.
27.4127.49.
27.0926.97.avg.
-24.
24.
24.0822.64.
22.5521.05.table 2: evaluation results for word alignment on four english-centric language pairs.
we report the alignment er-ror rate scores (lower is better).
for both simalign (jalili sabet et al., 2020) and our optimal-transport alignmentmethod, we use the hidden vectors from the 8-th layer produced by xlm-rbase or xlm-align.
“(reimplementa-tion)” is our reimplementation of simalign-itermax..2003) as the evaluation metrics..results we ﬁrst explore whether our word align-ment self-labeling method is effective for generat-ing high-quality alignment labels.
thus, we com-pare our method with (1) fast align (dyeret al., 2013), a widely-used implementation(2)of ibm model 2 (och and ney, 2003);simalign (jalili sabet et al., 2020), state-of-the-art unsupervised word alignment method.
for a faircomparison, we use the same pretrained lm andhidden layer as in simalign to produce sentencerepresentations.
in speciﬁc, we take the hiddenvectors from the 8-th layer of xlm-rbase or xlm-align, and obtain the alignments following theprocedure as described in section 3.1. since theproduced alignments are subword-level, we con-vert the alignments into word-level by the followingrule that “if two subwords are aligned, the wordsthey belong to are also aligned”..as shown in table 2, we report the aerscores on the four language pairs.
it can be ob-served that our optimal-transport method outper-forms fast align and simalign, demonstrat-ing that our method can produce high-quality align-ment labels, which is helpful for the dwa task.
moreover, our method consistently outperformssimalign when using hidden vectors from bothxlm-rbase and xlm-align..then, we compare our xlm-align with xlm-rbase on the word alignment task.
empirically, alower aer indicates that the model learns bet-ter cross-lingual representations.
from table 2,xlm-align obtains the best aer results overall the four language pairs, reducing the aver-aged aer from 22.64 to 21.05. besides, un-.
figure 2: evaluation results on word alignment acrossdifferent layers.
we illustrate the averaged aer scoreson the test sets of four language pairs.
the results of theﬁrst two layers are not included due to the high aer..pretrained xlm-align beneﬁts from the explicitword alignment objective, particularly on the struc-tured prediction and question answering tasks thatrequire token-level cross-lingual transfer.
in termsof sentence classiﬁcation tasks, xlm-align alsoconsistently outperforms xlm-rbase..4.3 word alignment.
word alignment is the task of ﬁnding correspondingword pairs in a parallel sentence.
we conduct eval-uations with golden alignments of four languagepairs from europarl1, wpt20032, and wpt20053,containing 1,244 annotated sentence pairs in total.
we use alignment error rate (aer; och and ney.
1www-i6.informatik.rwth-aachen.de/.
goldalignment/.
2web.eecs.umich.edu/˜mihalcea/wpt/3web.eecs.umich.edu/˜mihalcea/wpt05/.
34233456789101112layer1520253035alignment error ratexlm-rxlm-alignmodels.
xnli pos ner mlqa avg.
layer.
xnli pos ner mlqa avg.
xlm-r*xlm-align−dwa−tlm.
74.675.275.174.4.
75.775.675.276.0.
61.662.662.060.4.
65.766.765.866.0.
69.470.069.569.2.layer-8layer-10layer-12.
75.175.275.2.
75.375.675.8.
61.962.662.3.
66.766.767.0.
69.870.070.1.table 3: ablation studies on the components of xlm-align.
xlm-r* stands for continue-training xlm-rbase with mlm for fair comparisons.
results are aver-aged over ﬁve runs..der both simalign and our optimal-transportmethod, xlm-align provides consistent reduc-tion of aer, demonstrating the effectiveness ofour method for learning ﬁne-grained cross-lingualrepresentations..we also compare xlm-align with xlm-rbaseusing the hidden vectors from the 3-th layer to the12-th layer.
we illustrate the averaged aer scoresin figure 2. notice that the results on the ﬁrst twolayers are not presented in the ﬁgure because of thehigh aer.
it can be observed that xlm-alignconsistently improves the results over xlm-rbaseacross these layers.
moreover, it shows a parabolictrend across the layers of xlm-rbase, which isconsistent with the results in (jalili sabet et al.,2020).
in contrast to xlm-rbase, xlm-alignalleviates this trend and greatly reduces aer in thelast few layers.
we believe this property of xlm-align brings better cross-lingual transferabilityon the end tasks..5 analysis.
in this section, we conduct comprehensive ablationstudies for a better understanding of our xlm-align.
to reduce the computational cost, we re-duce the batch size to 256, and pretrain modelswith 50k steps in the following experiments..5.1 ablation studies.
we perform ablation studies to understand the com-ponents of xlm-align, by removing the de-noising word alignment loss (−dwa), the tlmloss (−tlm), or removing both (xlm-r*), whichis identical to continue-training xlm-rbase withmlm.
we evaluate the models on xnli, pos,ner, and mlqa, and present the results in ta-ble 3. comparing −tlm with −dwa, we ﬁndthat dwa is more effective for pos and mlqa,while tlm performs better on xnli and ner.
comparing −tlm with xlm-r*, it shows thatdirectly learning dwa slightly harms the perfor-.
table 4: results of xlm-align with different lay-ers used for word alignment self-labeling during pre-training.
results are averaged over ﬁve runs..layer.
xnli pos ner mlqa avg.
layer-8layer-10layer-12.
75.475.175.2.
75.375.675.8.
61.762.562.3.
66.266.367.0.
69.769.970.1.table 5: results of xlm-align with different layersused for denoising word alignment during pre-training.
results are averaged over ﬁve runs..mance.
however, jointly learning dwa with tlmprovides remarkable improvements over −dwa,especially on the question answering and the struc-ture prediction tasks that requires token-level cross-lingual transfer.
this indicates that tlm poten-tially improves the quality of self-labeled wordalignments, making dwa more effective for cross-lingual transfer..5.2 word alignment self-labeling layer.
it has been shown that the word alignment perfor-mance has a parabolic trend across the layers ofmbert and xlm-r (jalili sabet et al., 2020).
itindicates that the middle layers produce higher-quality word alignments than the bottom and thetop layers.
to explore which layer produces bet-ter alignment labels for pre-training, we pretrainthree variants of xlm-align, where we use thehidden vectors from three different layers for wordalignment self-labeling.
we use the 8-th, 10-th,and 12-th layers for word alignment self-labelingduring the pre-training.
we present the evaluationresults in table 4. surprisingly, although layer-8 produces higher-quality alignment labels at thebeginning of the pre-training, using the alignmentlabels from the 12-th layer learns a more trans-ferable xlm-align model for cross-lingual endtasks..5.3 denoising word alignment layer.
beyond the self-labeling layer, we also investigatewhich layer is better for learning the denoisingword alignment task.
recent studies have shown.
3424filtering xnli pos ner mlqa avg.
position.
xnli pos ner mlqa avg.
enabledisable.
75.274.2.
75.675.3.
62.661.6.
66.765.3.
70.069.1.table 6: effects of alignment ﬁltering in word align-ment self-labeling.
results are averaged over ﬁve runs..that it is beneﬁcial to learn sentence-level cross-lingual alignment at a middle layer (chi et al.,2021b).
therefore, we pretrain xlm-align mod-els by using three different layers for dwa, thatis, using the hidden vectors of middle layers as theinput of the pointer network.
we compare the eval-uation results of the three models in table 5. it canbe found that learning dwa at layer-8 improvesxnli while learning dwa at higher layers pro-duces better performance on the other three tasks.
it suggests that, compared with sentence-level pre-text tasks that prefers middle layers, the dwa taskshould be applied at top layers..5.4 effects of alignment filtering.
although our self-labeling method produces high-quality alignment labels, the alignment ﬁlteringoperation can potentially make some of the tokensunaligned, which reduces the example efﬁciency.
thus, we explore whether the alignment ﬁltering isbeneﬁcial for pre-training xlm-align.
to thisend, we pretrain an xlm-align model withoutalignment ﬁltering.
in speciﬁc, we use the union setof the forward and backward alignments as the self-labeled alignments so that all tokens are aligned atleast once.
the forward and backward alignmentsare obtained by applying the argmax function overrows and columns of a∗, respectively.
empiri-cally, the alignment ﬁltering operation generateshigh-precision yet fewer labels, while removingthe ﬁltering promises more labels but introduceslow-conﬁdent labels.
in table 6, we compare theresults of the models with or without alignmentﬁltering.
it can be observed that the alignment ﬁl-tering operation improves the performance on theend tasks.
this demonstrates that it is necessary touse high-precision labels for learning the denoisingword alignment task.
on the contrary, using per-turbed alignment labels in pre-training harms theperformance on the end tasks..5.5 effects of dwa query positions.
in the denoising word alignment task, we alwaysuse the hidden vectors of the masked positions.
maskedunmaskedall-alignedno-query.
75.275.575.375.1.
75.675.575.975.2.
62.662.061.662.0.
66.766.566.765.8.
70.069.869.969.5.table 7: effects of the query positions in the pointernetwork for denoising word alignment.
results are av-eraged over ﬁve runs..as the query vectors in the pointer network.
toexplore the impact of the dwa query positions,we compare three different query positions in ta-ble 7: (1) masked: only using the masked tokensas queries; (2) unmasked: randomly using 15% ofthe unmasked tokens as queries; (3) all-aligned:for each self-labeled aligned pair, randomly usingone of the two tokens as a query.
also, we in-clude the no-query baseline that does not use anyqueries, which is identical to removing dwa.
itcan be observed that using all the three query posi-tions improves the performance over the no-querybaseline.
moreover, using the masked positions asqueries achieves better results than the other twopositions, demonstrating the effectiveness of themasked query positions..6 discussion.
in this paper, we introduce denoising word align-ment as a new cross-lingual pre-training task.
byalternately self-labeling and predicting word align-ments, our xlm-align model learns transferablecross-lingual representations.
experimental resultsshow that our method improves the cross-lingualtransferability on a wide range of tasks, particularlyon the token-level tasks such as question answeringand structured prediction..despite the effectiveness for learning cross-lingual transferable representations, our methodalso has the limitation that requires a cold-startpre-training to prevent the model from producinglow-quality alignment labels.
in our experiments,we also try to pretrain xlm-align from scratch,i.e., without cold-start pre-training.
however, thedwa task does not work very well due to the low-quality of self-labeled alignments.
thus, we recom-mend continue-training xlm-align on the basisof other pretrained cross-lingual language models.
for future work, we would like to research on re-moving this restriction so that the model can learnword alignments from scratch..34257 ethical considerations.
despite the current advances in nlp, most nlp re-search works and applications are english-centric,making none-english users hard to access to nlp-related services.
our method aims to pretraincross-lingual language models that transfer super-vision signals from high-resource languages to low-resource languages, which makes the nlp servicesand applications more accessible for low-resource-language speakers.
furthermore, our method canbuild multilingual models that serve on differentlanguages at the same time, reducing the computa-tional resources for building multilingual modelsseparately for each language..acknowledgements.
heyan huang is the corresponding author.
thework is supported by national key r&d plan(no.
2018yfb1005100), national natural sciencefoundation of china (no.
61751201, 61602197and 61772076), natural science fund of beijing(no.
z181100008918002), and the funds of bei-jing advanced innovation center for language re-sources (no.
tyz19005)..references.
mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..peter f. brown, stephen a. della pietra, vincent j.della pietra, and robert l. mercer.
1993. the math-ematics of statistical machine translation: parameterestimation.
computational linguistics, 19(2):263–311..steven cao, nikita kitaev, and dan klein.
2020. mul-tilingual alignment of contextual word representa-tions.
in international conference on learning rep-resentations..zewen chi, li dong, shuming ma, shaohan huangxian-ling mao, heyan huang, and furu wei.
2021a.
mt6: multilingual pretrained text-to-texttransformer with translation pairs.
arxiv preprintarxiv:2104.08692..zewen chi, li dong, furu wei, nan yang, sak-sham singhal, wenhui wang, xia song, xian-lingmao, heyan huang, and ming zhou.
2021b.
in-foxlm: an information-theoretic framework forin pro-cross-lingual language model pre-training.
ceedings of the 2021 conference of the north amer-ican chapter of the association for computational.
linguistics: human language technologies, pages3576–3588, online.
association for computationallinguistics..jonathan h. clark, eunsol choi, michael collins, dangarrette, tom kwiatkowski, vitaly nikolaev, andjennimaria palomaki.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
transactions of theassociation for computational linguistics, 8:454–470..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems, pages7057–7067.
curran associates, inc..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..chris dyer, victor chahuneau, and noah a smith.
2013. a simple, fast, and effective reparameteriza-in proceedings of the 2013tion of ibm model 2.conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 644–648..hamidreza ghader and christof monz.
2017. whatdoes attention in neural machine translation pay at-in proceedings of the eighth interna-tention to?
tional joint conference on natural language pro-cessing (volume 1: long papers), pages 30–39,taipei, taiwan.
asian federation of natural lan-guage processing..junjie hu, melvin johnson, orhan firat, aditya sid-dhant, and graham neubig.
2020a.
explicit align-ment objectives for multilingual bidirectional en-coders.
arxiv preprint arxiv:2010.07972..3426junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020b.
xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual general-ization.
arxiv preprint arxiv:2003.11080..fuli luo, wei wang, jiahao liu, yijia liu, bin bi,songfang huang, fei huang, and luo si.
2020.veco: variable encoder-decoder pre-training forcross-lingual understanding and generation.
arxivpreprint arxiv:2010.16046..haoyang huang, yaobo liang, nan duan, ming gong,linjun shou, daxin jiang, and ming zhou.
2019.unicoder: a universal language encoder by pre-in pro-training with multiple cross-lingual tasks.
ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing, pages 2485–2494, hong kong, china.
association for computational linguistics..masoud jalili sabet, philipp dufter, franc¸ois yvon,and hinrich sch¨utze.
2020. simalign: high qual-ity word alignments without parallel training data us-ing static and contextualized embeddings.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 1627–1643, online.
as-sociation for computational linguistics..karthikeyan k, zihan wang, stephen mayhew, anddan roth.
2020. cross-lingual ability of multilin-gual bert: an empirical study.
in international con-ference on learning representations..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in 3rd interna-tional conference on learning representations, sandiego, ca..philipp koehn and rebecca knowles.
2017. six chal-in proceed-lenges for neural machine translation.
ings of the first workshop on neural machine trans-lation, pages 28–39, vancouver.
association forcomputational linguistics..anoop kunchukuttan, pratik mehta, and pushpak bhat-tacharyya.
2018. the iit bombay english-hindiparallel corpus.
in proceedings of the eleventh in-ternational conference on language resources andevaluation, miyazaki, japan.
european languageresources association..patrick lewis, barlas oguz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7315–7330, online.
association for computational lin-guistics..xintong li, guanlin li, lemao liu, max meng, andshuming shi.
2019. on the word alignment fromin proceedings of theneural machine translation.
57th annual meeting of the association for compu-tational linguistics, pages 1293–1303..cos¸kun mermer and murat sarac¸lar.
2011. bayesianword alignment for statistical machine translation.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 182–187..masaaki nagata, katsuki chousa,.
and masaakinishino.
2020.a supervised word alignmentmethod based on cross-language span prediction us-ing multilingual bert.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 555–565, online.
association for computational linguistics..franz josef och and hermann ney.
2003. a systematiccomparison of various statistical alignment models.
computational linguistics, 29(1):19–51..robert ¨ostling and j¨org tiedemann.
2016. efﬁcientword alignment with markov chain monte carlo.
the prague bulletin of mathematical linguistics,106(1):125–146..xuan ouyang, shuohuan wang, chao pang, yu sun,hao tian, hua wu, and haifeng wang.
2020. ernie-m: enhanced multilingual representation by align-ing cross-lingual semantics with monolingual cor-pora.
arxiv preprint arxiv:2012.15674..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..gabriel peyr´e, marco cuturi, et al.
2019. computa-tional optimal transport: with applications to datafoundations and trends® in machinescience.
learning, 11(5-6):355–607..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..afshin rahimi, yuan li, and trevor cohn.
2019. mas-in proceed-sively multilingual transfer for ner.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 151–164, flo-rence, italy.
association for computational linguis-tics..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
arxivpreprint arxiv:2001.08210..holger schwenk, vishrav chaudhary, shuo sun,hongyu gong, and francisco guzm´an.
2019. wiki-matrix: mining 135m parallel sentences in 1620arxiv preprintlanguage pairs from wikipedia.
arxiv:1907.05791..3427j¨org tiedemann.
2012. parallel data, tools and inter-in proceedings of the eighth in-faces in opus.
ternational conference on language resources andevaluation, pages 2214–2218, istanbul, turkey.
eu-ropean language resources association..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention isin advances in neural informationall you need.
processing systems, pages 5998–6008.
curran as-sociates, inc..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems, volume 28, pages2692–2700..guillaume wenzek, marie-anne lachaux, alexis con-neau, vishrav chaudhary, francisco guzman, ar-mand joulin, and edouard grave.
2019. ccnet: ex-tracting high quality monolingual datasets from webcrawl data.
arxiv preprint arxiv:1911.00359..shijie wu and mark dredze.
2019. beto, bentz, becas:the surprising cross-lingual effectiveness of bert.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, pages 833–844, hong kong,china.
association for computational linguistics..shijie wu and mark dredze.
2020. do explicit align-ments robustly improve multilingual encoders?
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4471–4482, online.
association for computa-tional linguistics..linting xue, noah constant, adam roberts, mi-hir kale, rami al-rfou, aditya siddhant, adityabarua, and colin raffel.
2020. mt5: a mas-sively multilingual pre-trained text-to-texttrans-former.
arxiv preprint arxiv:2010.11934..jian yang, shuming ma, dongdong zhang, shuangzhiwu, zhoujun li, and ming zhou.
2020. alternatinglanguage modeling for cross-lingual pre-training.
inthirty-fourth aaai conference on artiﬁcial intelli-gence..yinfei yang, yuan zhang, chris tar, and jasonpaws-x: a cross-lingual ad-baldridge.
2019.versarial dataset for paraphrase identiﬁcation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3687–3692, hong kong, china.
association for computa-tional linguistics..daniel zeman,.
joakim nivre, mitchell abrams,and et al.
2019.universal dependencies 2.5.lindat/clariah-cz digital library at the insti-tute of formal and applied linguistics ( ´ufal), fac-ulty of mathematics and physics, charles univer-sity..wei zhao, steffen eger, johannes bjerva, and is-inducing language-arxiv.
abelle augenstein.
2020.agnostic multilingualpreprint arxiv:2008.09112..representations..michał ziemski, marcin junczys-dowmunt, and brunopouliquen.
2016. the united nations parallel corpusv1.
0. in lrec, pages 3530–3534..a pre-training data.
we use raw sentences from the wikipedia dumpand ccnet4 as monolingual corpora.
the ccnetcorpus we use is reconstructed following (conneauet al., 2020) to reproduce the cc-100 corpus.
theresulting corpus contains 94 languages.
table 8and table 9 report the language codes and datasize of ccnet and wikipedia dump.
notice thatseveral languages share the same iso languagecodes, e.g., zh represents both simpliﬁed chineseand traditional chinese.
besides, table 10 showsthe statistics of our parallel corpora..code.
size (gb).
code.
size (gb).
code.
size (gb).
afamarasazbabebgbncackbcscydadeeleneoeseteufaﬁfrgaglguhehi.
0.20.416.10.10.80.20.57.05.53.00.614.90.46.999.013.1731.60.585.61.41.019.05.989.90.21.50.34.45.0.hrhuhyidisitjakakkkmknkokylaloltlvmkmlmnmrmsmtmynenlnnnoor.
1.49.50.717.20.547.286.81.00.60.20.340.00.50.30.22.31.30.61.30.40.50.70.20.40.625.90.45.50.3.paplpsptrorusasdsiskslsqsrsvswtatetgthtltrttugukuruzviyizh.
0.828.60.439.411.0253.30.20.21.313.66.23.07.260.40.37.92.30.733.01.256.40.60.213.43.00.174.50.396.8.table 8: the statistics of ccnet used for pre-training..b hyperparameters for pre-training.
as shown in table 11, we present the hyperpa-rameters for pre-training xlm-align.
we usethe same vocabulary with xlm-r (conneau et al.,2020)..4https://github.com/facebookresearch/.
cc_net.
3428code.
size (gb).
code.
size (gb).
code.
size (gb).
afamarasazbabebgbncackbcscydadeeleneoeseteufaﬁfrgaglguhehi.
0.120.011.290.040.240.130.310.620.411.100.000.810.060.335.430.7312.580.253.380.230.240.660.684.000.030.270.091.110.38.hrhuhyidisitjakakkkmknkokylaloltlvmkmlmnmrmsmtmynenlnnnoor.
0.280.800.600.520.052.702.650.370.290.120.250.560.100.050.010.190.120.340.280.050.100.200.010.150.061.380.130.540.04.paplpsptrorusasdsiskslsqsrsvswtatetgthtltrttugukuruzviyizh.
0.101.550.041.500.425.630.040.020.090.210.210.110.741.700.030.460.450.040.520.040.430.090.032.430.130.060.760.021.08.table 9: the statistics of wikipedia dump used for pre-training..iso code.
size (gb).
iso code.
size (gb).
en-aren-bgen-deen-elen-esen-fren-hi.
5.880.494.212.287.097.630.62.en-ruen-swen-then-tren-uren-vien-zh.
7.720.060.470.340.390.864.02.table 10: parallel data used for pre-training..c hyperparameters for fine-tuning.
in table 12, we present the hyperparameters forﬁne-tuning xlm-rbase and xlm-align on thextreme end tasks.
for each task, the hyperpa-rameters are searched on the joint validation set ofall languages..d detailed results on xtreme.
we present the detailed results of xlm-align onxtreme in table 13-19..hyperparameters.
layershidden sizeffn inner hidden sizeattention headstraining stepsbatch sizeadam (cid:15)adam βlearning ratelearning rate schedulewarmup stepsgradient clippingweight decayself-labeling layerentropic regularization µsinkhorn iterationsalignment ﬁltering iterationsalignment ﬁltering α.value.
127683,07212150k2,0481e-6(0.9, 0.98)2e-4linear10,0001.00.01101.0220.9.table 11: hyperparameters used for pre-training xlm-align..3429pos.
ner.
xquad.
mlqa.
tydiqa.
xnli.
paws-x.
batch sizelearning ratelr schedulewarmupweight decayepochs.
{8,16,32}{1,2,3}e-5linear10%010.
8{5,...,9}e-6linear10%010.
32{2,3,4}e-5linear10%04.
32{2,3,4}e-5linear10%0{2,3,4}.
32{2,3}e-5linear.
32{5,...,8}e-6linear10% 12,500 steps010.
0{5,10,15,20}.
32{1,2}e-5linear10%010.table 12: hyperparameters used for ﬁne-tuning xlm-rbase and xlm-align on the xtreme end tasks..model.
af.
ar.
bg.
de.
el.
en.
es.
et.
eu.
fa.
fr.
he.
hi.
hu.
id.
it.
xlm-align 88.5 69.1 88.8 88.8 85.8 95.9 88.5 84.9 68.3 70.9 84.8 88.1 79.6 71.6 83.3 72.3 89.4.model.
ja.
kk.
ko.
mr.nl.
pt.
ru.
ta.
te.
th.
tr.
ur.
vi.
yo.
zh avg.
xlm-align 51.1 75.3 53.8 80.3 89.3 87.6 88.9 62.3 85.9 60.2 90.1 74.8 63.3 55.9 24.2 67.9 76.0.ﬁ.tl.
table 13: results on part-of-speech tagging..model.
ar.
he.
vi.
id.
jv.
ms.tl.
eu.
ml.
ta.
te.
af.
nl.
en.
de.
el.
bn.
hi.
mr.ur.
xlm-align 57.7 54.3 72.5 49.7 56.9 68.3 72.0 53.1 68.6 58.0 54.6 76.3 82.1 84.2 77.9 76.4 73.1 69.2 64.9 65.8.model.
fa.
fr.
it.
pt.
es.
bg.
ru.
ja.
ka.
ko.
th.
sw.yo my.
zh.
kk.
tr.
et.
ﬁ.hu avg.
xlm-align 53.2 79.0 79.4 78.8 73.8 78.9 66.2 23.0 70.6 56.6 2.2 69.3 43.8 56.5 28.3 49.2 77.5 73.3 77.0 77.0 63.7.table 14: results on wikiann named entity recognition..model.
en.
es.
de.
el.
ru.
tr.
ar.
vi.
th.
zh.
hi.
avg.
xlm-align 85.7 / 74.6 70.3 / 52.5 76.6 / 60.3 75.5 / 56.8 79.4 / 60.8 71.8 / 54.7 75.4 / 59.4 72.1 / 61.0 70.9 / 55.5 76.7 / 56.9 67.3 / 56.8 74.7 / 59.0.table 15: results on xquad question answering..model.
en.
es.
de.
ar.
hi.
vi.
zh.
avg.
xlm-align.
81.5 / 68.3.
70.3 / 52.2.
64.5 / 49.8.
60.7 / 41.2.
65.2 / 47.5.
69.8 / 48.9.
64.4 / 40.4.
68.1 / 49.8.table 16: results on mlqa question answering..model.
en.
ar.
bn.
ﬁ.id.
ko.
ru.
sw.te.
avg.
xlm-align 69.4 / 56.2.
68.7 / 49.4.
56.0 / 38.9.
64.2 / 47.2.
73.9 / 57.9.
53.0 / 40.4.
62.3 / 38.0.
60.1 / 42.8.
51.0 / 31.9.
62.1 / 44.8.table 17: results on tydiqa question answering..model.
en.
fr.
es.
de.
el.
bg.
ru.
tr.
ar.
vi.
th.
zh.
hi.
sw.ur.
avg.
xlm-align.
86.7.
80.6.
81.0.
78.8.
77.4.
78.8.
77.4.
75.2.
73.9.
76.9.
73.8.
77.0.
71.9.
67.1.
66.6.
76.2.table 18: results on xnli natural language inference..model.
en.
fr.
de.
es.
ja.
ko.
zh.
avg.
xlm-align.
95.1.
89.3.
90.5.
90.7.
79.1.
79.5.
83.2.
86.8.table 19: results on paws-x cross-lingual paraphrase adversaries..3430