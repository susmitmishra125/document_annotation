topic-aware evidence reasoning and stance-aware aggregation for factveriﬁcation.
jiasheng si† deyu zhou†∗ tongzhe li† xingyu shi† yulan he§† school of computer science and engineering, key laboratory of computer networkand information integration, ministry of education, southeast university, china§ department of computer science, university of warwick, uk{jasenchn, d.zhou, 220184611, xyu-shi}@seu.edu.cn,yulan.he@warwick.ac.uk.
abstract.
fact veriﬁcation is a challenging task that re-quires simultaneously reasoning and aggregat-ing over multiple retrieved pieces of evidenceto evaluate the truthfulness of a claim.
ex-isting approaches typically (i) explore the se-mantic interaction between the claim and ev-idence at different granularity levels but failto capture their topical consistency during thereasoning process, which we believe is cru-cial for veriﬁcation; (ii) aggregate multiplepieces of evidence equally without consideringtheir implicit stances to the claim, thereby in-troducing spurious information.
to alleviatethe above issues, we propose a novel topic-aware evidence reasoning and stance-aware ag-gregation model for more accurate fact veri-ﬁcation, with the following four key proper-ties: 1) checking topical consistency betweenthe claim and evidence; 2) maintaining topicalcoherence among multiple pieces of evidence;3) ensuring semantic similarity between theglobal topic information and the semantic rep-resentation of evidence; 4) aggregating evi-dence based on their implicit stances to theclaim.
extensive experiments conducted onthe two benchmark datasets demonstrate thesuperiority of the proposed model over sev-eral state-of-the-art approaches for fact veriﬁ-cation.
the source code can be obtained fromhttps://github.com/jasenchn/tarsa..1.introduction.
the internet breaks the physical distance barrieramong individuals to allow them to share data andinformation online.
however, it can also be usedby people with malicious purposes to disseminatemisinformation or fake news.
such misinformationmay cause ethnics conﬂicts, ﬁnancial losses and po-litical unrest, which has become one of the greatestthreats to the public (zafarani et al., 2019; zhou.
∗corresponding author.
et al., 2019b).
moreover, as shown in vosoughiet al.
(2018), compared with truth, misinformationdiffuses signiﬁcantly farther, faster, and deeper inall genres.
therefore, there is an urgent need forquickly identifying the misinformation spread onthe web.
to solve this problem, we focus on thefact veriﬁcation task (thorne et al., 2018), whichaims to automatically evaluate the veracity of agiven claim based on the textual evidence retrievedfrom external sources..recent approaches for fact veriﬁcation are domi-nated by natural language inference models (angeliand manning, 2014) or textual entailment recogni-tion models (ma et al., 2019), where the truthful-ness of a claim is veriﬁed via reasoning and aggre-gating over multiple pieces of retrieved evidence.
in general, existing models follow an architecturewith two main sub-modules: the semantic inter-action module and the entailment-based aggrega-tion module (hanselowski et al., 2018a; nie et al.,2019a; soleimani et al., 2020; liu et al., 2020).
the semantic interaction module attempts to graspthe rich semantic-level interactions among multi-ple pieces of evidence at the sentence-level (maet al., 2019; zhou et al., 2019a; subramanian andlee, 2020) or the semantic roles-level (zhong et al.,2020).
the entailment-based aggregation moduleaims to ﬁlter out irrelevant information to capturethe salient information related to the claim by ag-gregating the semantic information coherently..however, the aforementioned approaches typ-ically learn the representation of each evidence-claim pair from the semantic perspective suchas obtaining the semantic representation of eachevidence-claim pair through pre-trained languagemodels (devlin et al., 2019) or graph-based mod-els (velickovic et al., 2018), which largely over-looked the topical consistency between claim andevidence.
for example in figure 1, given the claim“a high school student named cole withrow was.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1612–1622august1–6,2021.©2021associationforcomputationallinguistics1612figure 1: an example of fact veriﬁcation.
the bold italic words are topic words extracted by latent dirichletallocation (lda).
the red solid line denotes the topical consistency between the claim and evidence.
the blackdotted line denotes the implicit stance of evidence towards the claim.
the blue solid line denotes the topicalcoherence among evidence..charged for leaving an unloaded shotgun in hisvehicle while parking at school” and the retrievedevidence sentences (i.e., e1-e4), we would ex-pect a fact checking model to automatically ﬁlterevidence which is topically-unrelated to the claimsuch as e3 and e4 and only relies on the evidencewhich is topically-consistent with the claim suchas e1 and e2 for veracity assessment of the claim.
in addition, we also expect the topical coherence ofmultiple pieces of supporting evidence such as e1and e2.
furthermore, in previous approaches, thelearned representations of multiple pieces of evi-dence are aggregated via element-wise max poolingor simple dot-product attention, which inevitablyfails to capture the implicit stances of evidence to-ward the claim (e.g., e1 and e2 support the claimimplicitly, e3 and e4 are unrelated to the claim)and leads to the combination of irrelevant informa-tion with relevant one..to address these problems, in this paper, we pro-pose a novel neural structure reasoning model forfact veriﬁcation, named tarsa (topic-aware ev-idence reasoning and stance-aware aggregationmodel).
a coherence-based topic attention is de-veloped to model the topical consistency between aclaim and each piece of evidence and the topical co-herence among evidence built on the sentence-leveltopical representations.
in addition, a semantic-topic co-attention is created to measure the coher-ence between the global topical information and thesemantic representation of the claim and evidence.
moreover, the capsule network is incorporated tomodel the implicit stances of evidence toward theclaim by the dynamic routing mechanism..the main contributions are listed as follows:.
• we propose a novel topic-aware evidencereasoning and stance-aware aggregation ap-proach, which is, to our best knowledge, theﬁrst attempt of jointly exploiting semantic in-teraction and topical consistency to learn la-tent evidence representation for fact veriﬁca-tion..• we incorporate the capsule network structureinto our proposed model to capture the im-plicit stance relations between the claim andthe evidence..• we conduct extensive experiments on the twobenchmark datasets to demonstrate the effec-tiveness of tarsa for fact veriﬁcation..2 related work.
in general, fact veriﬁcation is a task to assess theauthenticity of a claim backed by a validated cor-pus of documents, which can be divided into twostages: fact extraction and claim veriﬁcation (zhouand zafarani, 2020).
fact extraction can be furthersplit into the document retrieval phase and the ev-idence selection phase to shrink the search spaceof evidence (thorne et al., 2018).
in the documentretrieval phase, researchers typically reuse the topperforming approaches in the fever1.0 challengeto extract the documents with high relevance fora given claim (hanselowski et al., 2018b; yonedaet al., 2018; nie et al., 2019a).
in the evidenceselection phase, to select relevant sentences, re-searchers generally train the classiﬁcation modelsor rank models based on the similarity between theclaim and each sentence from the retrieved doc-uments (chen et al., 2017; stammbach and neu-.
1613claima high school student named cole withrow was charged for leaving an unloaded shotgun in his vehicle while parking at school.e1 (gold)family friend kim boykin said withrow, an eagle scout and honors student, accidentally left his gun in the car afterskeet shooting over the weekend.e2 (gold)others in the princeton high community agree that withrow's punishment is too harsh, especially after charges weren't filed when a loaded gun was found in an assistant principal's car two years ago.e3 (non-gold)“please know that with student and personnel issues, we carefully balance all factors to arrive at a fair and just outcome.” she said in a statement.verdict: supportse4 (non-gold)he locks his vehicle, goes inside and tries to do the right thing.
mann, 2019; soleimani et al., 2020; wadden et al.,2020; zhong et al., 2020; zhou et al., 2019a)..many fact veriﬁcation approaches focus on theclaim veriﬁcation stage, which can be addressed bynatural language inference methods (parikh et al.,2016; ghaeini et al., 2018; luken et al., 2018).
typ-ically, these approaches contain the representationlearning process and evidence aggregation process.
hanselowski et al.
(2018b) and nie et al.
(2019a)concatenate all pieces of evidence as input and usethe max pooling to aggregate the information forclaim veriﬁcation via the enhanced sequential in-ference model (esim) (chen et al., 2017).
in asimilar vein, yin and roth (2018) incorporate theidentiﬁcation of evidence to further improve claimveriﬁcation using esim with different granularitylevels.
ma et al.
(2019) leverage the co-attentionmechanism between claim and evidence to gener-ate claim-speciﬁc evidence representations whichare used to infer the claim..beneﬁting from the development of pre-trainedlanguage models, zhou et al.
(2019a) are the ﬁrstto learn evidence representations by bert (de-vlin et al., 2019), which are subsequently used in aconstructed evidence graph for claim inference byaggregating all claim-evidence pairs.
zhong et al.
(2020) further establish a semantic-based graph forrepresentation and aggregation with xlnet (yanget al., 2019).
liu et al.
(2020) incorporate two setsof kernels into a sentence-level graph to learn amore ﬁne-grained evidence representations.
sub-ramanian and lee (2020) further incorporate evi-dence set retrieval and hierarchical attention sumblock to improve the performance of claim veriﬁ-cation..different from all previous approaches, our workfor the ﬁrst time handles the fact veriﬁcation taskby considering the topical consistency and the se-mantic interactions between claim and evidence.
moreover, we employ the capsule network to modelthe implicit stance relations of evidence toward theclaim..3 method.
in this section, we present an overview of the archi-tecture of the proposed framework tarsa for factveriﬁcation.
as shown in figure 2, our approachconsists of three main layers: 1) the representa-tion layer to embed claim and evidence into threetypes of representations by a semantic encoder anda topic encoder; 2) the coherence layer to incorpo-.
rate the topic information into our model by twoattention components; 3) the aggregation layer tomodel the implicit stances of evidence toward claimusing the capsule network..3.1 representation layer.
this section describes how tarsa extracts se-mantic representations, sentence-level topic repre-sentations, and global topic information through asemantic encoder and a topic encoder separately..semantic encoder the semantic encoder intarsa is a vanilla transformer (vaswani et al.,2017) with the extra hop attention (zhao et al.,2020).
for each claim c paired with n pieces of re-trieved evidence sentences e = {e1, e2, · · · , en },tarsa constructs the evidence graph by treat-ing each evidence-claim pair xi = (ei, c) as anode (i.e., xi = (cid:2)[cls]; ei; [sep ]; c; [sep ](cid:3))and build a fully-connected evidence graph g. wealso add a self-loop to every node to perform mes-sage propagation from itself..speciﬁcally, we ﬁrst apply the vanilla trans-former on each node to generate the claim-dependent evidence representation using the inputxi,.
hi = t ransf ormer(xi).
(1).
where i denotes the i-th node in g. we treat theﬁrst token representation hi,0 as the local contextof node i..then the extra hop attention takes the [cls]token in each node as a “hub token”, which is toattend on hub tokens of all other connected nodesto learn the global context.
one layer of extra hopattention can be viewed as a single-hop messagepropagation among all the nodes along the edges,.
ˆhi,0 =.
(cid:88).
sof tmaxj(.
j;ei,j =1.
i,0 · ˆkj,0ˆqt√dk.)
· ˆνj,0.
(2).
where ei,j = 1 denotes that there is an edge be-tween the node i and the node j, ˆqi,0 denotes thequery vector of the [cls] token of node i, ˆkj,0 andˆνj,0 denote the key vector and the value vector ofthe [cls] token of node j, respectively, anddkdenotes the scaling factor..√.
the local context and the global context are con-catenated to learn the semantic representation ofall the nodes:.
˜hi,0 = linear([hi,0; ˆhi,0]),˜hi,τ = hi,τ ; ∀τ (cid:54)= 0..(3).
1614figure 2: the overview of the architecture of our topic-aware evidence reasoning and stance-aware aggregationmodel (tarsa).
by stacking l layers of the transformer withthe extra hop attention which takes the semanticrepresentation of the previous layer as input, welearn the semantic representation of evidence h =[˜h1, ˜h2, · · · , ˜hn ] ∈ rn ×d from the graph g..topic encoder we extract topics in the fol-lowing two forms via latent dirichlet allocation(lda) (blei et al., 2003):sentence-level topic representation: given a claimc and n pieces of the retrieved evidence e, weextract latent topic distribution t ∈ rk for eachsentence as the sentence-level topic representation,where k is the number of topics.
more concretely,we denote tc ∈ rk for claim c and tei ∈ rkfor evidence ei.
each scalar value tk denotes thecontribution of topic k in representing the claim orevidence.
global topic information: we extract global topicinformation p = [p1, p2, · · · , pk] ∈ rk×v fromthe topic-word distribution by treating each sen-tence (i.e., claim or evidence) in corpus d as adocument, where v denotes the vocabulary size..3.2 coherence layer.
this section describes how to incorporate the topicinformation into our model with two attention com-ponents..coherence-based topic attention based onthe observation as illustrated in figure 1, we as-.
sume that given a claim, the sentences used asevidence should be topically coherent with eachother and the claim should be topically consistentwith the relevant evidence.
therefore, two kinds oftopical relationship are considered: 1) topical co-herence among multiple pieces of evidence (t cee);2) topical consistency between the claim and eachevidence (t cce)..speciﬁcally, to incorporate the topical coherenceamong multiple pieces of evidence into our model,we disregard the order of evidence and treat eachevidence independently.
then we utilize the multi-head attention (vaswani et al., 2017) without posi-tion embedding to generate the new topic represen-tation of evidence ˆte based on the sentence-leveltopic representation te ∈ rn ×k of the retrievedevidence for a given claim..ˆte = multihead(te).
(4).
moreover, we utilize the co-attention mecha-nism (chen and li, 2020) to weigh each evidencebased on the topic consistency between the claimand the evidence.
given the sentence-level topicrepresentation tc for claim and te for the corre-sponding evidence, the co-attention attends to theclaim and the evidence simultaneously.
we ﬁrstcompute the proximity matrix f ∈ rn ,.
f = tanh(tcwltt.
e ),.
(5).
1615multi-head attentioneasupportsrefutesnei0.80.10.10.70.10.20.20.10.7dzw1x2xnxct1et2etnet1p2pkp1h2hnhfeheeafehees1u2unu1|1u1|2u2|2u3|2u1|nu2|nu3|nu2|1u3|1uwhere wl ∈ rk×k is the learnable weight matrix.
the proximity matrix can be viewed as a trans-formation from the claim attention space to theevidence attention space.
then we can predict theinteraction attention by treating f as the feature,.
h e = tanh (cid:0)wett.
e + (wctt.
c )f (cid:1),.
(6).
where we, wc ∈ rl×k are the learnable weightmatrices.
finally we can generate a topic similarityscore between the claim and each evidence usingthe softmax function,.
αe = sof tmax(wh e),.
(7).
where w ∈ r1×l is the learnable weight, αe ∈ rnis the attention score of each piece of evidencefor the claim.
eventually, the topic representationa ∈ rn ×k can be computed as follows,.
a = αe (cid:12) ˆte,.
(8).
where (cid:12) is the dot product operation..semantic-topic co-attention we weigh eachpiece of evidence ei to indicate the importance ofthe evidence and infer the claim based on the coher-ence between the semantic representation and theglobal topic information via the co-attention mecha-nism, which is similar to the coherence-based topicattention in section 3.2. more concretely, taking hand p as input, we compute the proximity matrixf ∈ rk×n to transform the topic attention spaceto the semantic attention space by eq.
(5).
as aresult, the attention weights βe ∈ rn of evidencecan be obtained by eq.
(6) and (7).
eventually, thesemantic representation s ∈ rn ×d can be updatedvia s = βe (cid:12) h..formally, let uj|i be the predicted vector from.
the evidence capsule ui to the class capsule oj,.
uj|i = wj,iui.
(9).
where wj,i ∈ rdo×de denotes the transformationmatrix from the evidence capsule ui to the classcapsule oj.
each class capsule aggregates all of theevidence capsules by a weighted summation overall corresponding predicted vectors:.
oj = g(.
γjiuj|i), ˆpji = |ui|,.
(10).
n(cid:88).
i=1.
where g is a non-linear squashing function whichlimits the length of oj to [0, 1], γji is the couplingcoefﬁcient that determines the probability that theevidence capsule ui should be coupled with theclass capsule oj.
the coupling coefﬁcient is cal-culated by the unsupervised and iterative dynamicrouting algorithm on original logits bji, which issummarized in algorithm 1. we can easily classifythe claim by choosing the class capsule with thelargest ρj via the capsule loss (sabour et al., 2017).
moreover, the cross entropy loss is applied on theevidence capsules to identify whether the evidenceis the ground truth evidence..algorithm 1 dynamic routing algorithmprocedure: routing(uj|i, ˆpji)1: initialize the logit of coupling coefﬁcient bij == 0;2: for each iteration do3:.
for all evidence capsule ui and class oj:γji = ˆpji · leaky sof tmax(bji).
4:5:.
update all the class capsules via eq.
(10);for all evidence capsule ui and the class oj:.
bji = bji + uj|i · oj.
6: end for7: return o ∈ rm ×do , ρj = |oj|j=1:m.3.3 aggregation layer.
4 experimental setting.
to model the implicit stances of evidence towardclaim, we incorporate the capsule network (sabouret al., 2017) into our model.
as illustrated in fig-ure 2, we concatenate both the semantic representa-tion s and the topical representation a to form thelow-level evidence capsules ui = [ai; si]|ni=1 ∈j=1 ∈ rdo denote the high-level classrde.
let oj|mcapsules, where m denotes the number of classes.
the capsule network models the relationship be-tween the evidence capsules and the class capsulesby the dynamic routing mechanism (yang et al.,2018), which can be viewed as the implicit stancesof each evidence toward three classes..this section describes the datasets, evaluation met-rics, baselines, and implementation details in ourexperiments..datasets we conduct experiments on two pub-lic fact checking datasets: (1) fever (thorneet al., 2018) is a large-scale dataset consisting of185,455 claims along with 5,416,537 wikipediapages from the june 2017 wikipedia dump.
theground truth evidence and the label (i.e., “sup-ports”, “refutes” and “not enough info(nei)”) are also available except in the test set.
(2) ukp snopes (hanselowski et al., 2019) is a.
1616datasetsfeverukp snopes.
train.
dev145,449 19,998 19,9985824,659.
583.
25,7532,258.test vocabulary size.
table 1: statistics on fever and ukp snopes.
mixed-domain dataset along with 16,508 snopespages.
to maintain the consistency of two datasets,we merge the verdicts {false, mostly false}, {true,mostly true}, {mixture, unproven, undetermined}as “refutes”,“supports” and “nei”, respec-tively.
and we omit all other labels (i.e., legent,outdated, and miscaptioned) as these instances aredifﬁcult to distinguish.
table 1 presents the statis-tics of the two datasets..evaluation metrics the ofﬁcial evaluation met-rics1 for the fever dataset are label accuracy(la) and fever score (f-score).
la measuresthe accuracy of the predicted label ˆyi matchingthe ground truth label yi without considering theretrieved evidence.
the fever score labels a pre-diction as correct if the predicted label ˆyi is correctand the retrieved evidence matches at least onegold-standard evidence, which is a better indicatorto reﬂect the inference capability of the model.
weuse precision, recall, and macro f1 on ukp snopesto evaluate the performance..baselines the following approaches are em-ployed as the baselines,including three topperforming models on fever1.0 shared task(ukp athene (hanselowski et al., 2018b), uclmrg (yoneda et al., 2018) and unc nlp (nieet al., 2019a)), han (ma et al., 2019), bert-basedmodels (sr-mrs (nie et al., 2019b), bert con-cat (soleimani et al., 2020) and hesm (subra-manian and lee, 2020)), and graph-based mod-els (gear (zhou et al., 2019a), transformer-xh (zhao et al., 2020), kgat (liu et al., 2020)and dream (zhong et al., 2020))..implementation details we describe our imple-mentation details in this section..document retrieval takes a claim along witha collection of documents as the input, then re-turns n most relevant documents.
for the feverdataset, following hanselowski et al.
(2018a), weadopt the entity linking method since the title of awikipedia page can be viewed as an entity and canbe linked easily with the extracted entities from.
1https://github.com/shefﬁeldnlp/fever-scorer.
the claim.
for the ukp snopes dataset, follow-ing hanselowski et al.
(2019), we adopt the tf-idfmethod where the tf-idf similarity between claimand concatenation of all sentences of each snopespage is computed, and then the 5 highest rankeddocuments are taken as retrieved documents..evidence selection retrieves the related sentencesfrom retrieved documents in ranking setting.
forthe fever dataset, we follow the previous methodfrom zhao et al.
(2020).
taking the concatenationof claim and each sentence as input, the [cls] to-ken representation is learned through bert whichis then used to learn a ranking score through a lin-ear layer.
the hinge loss is used to optimize thebert model.
for the ukp snopes dataset, weadopt the tf-idf method from hanselowski et al.
(2019), which achieves the best precision..claim veriﬁcation.
during the training phase,each claim is paired with 5 pieces of evidence, weset the batch size to 1 and the accumulate step to 8,the layer l is 3, the head number is 5, the l is 100,the number of class capsules m is 3, the dimensionof class capsules do is 10, the topic number kranges from 25 to 100. in our implementation, themaximum length of each claim-evidence pair is130 for both datasets..5 experimental results.
in this section, we evaluate our tarsa model indifferent aspects.
firstly, we compare the overallperformance between our model and the baselines.
then we conduct an ablation study to explore theeffectiveness of the topic information and the cap-sule network structure.
finally, we also explore theadvantages of our model in single-hop and multi-hop reasoning scenarios..5.1 overall performance.
table 2 and table 3 report the overall performanceof our model against the baselines for the feverdataset and the ukp snopes dataset 2. as shown intable 2, our model signiﬁcantly outperforms bert-based models on both development and test sets.
however, compared with the graph-based models,.
2note that we did not compare hesm, sr-mes anddream with our model on the ukp snopes dataset for thefollowing reasons.
hesm requires hyperlinks to constructthe evidence set, which are not available in ukp snopes; sr-mrs concatenates query and context as the input to bert,which is similar to the bert concat model; the composi-tion of a claim in the ukp snopes is more complicated thanfever, which is more difﬁcult for deram to construct agraph at the semantic level..1617fever.
models.
devla f-score64.7468.49ukp athene65.4169.66ucl mrg66.4969.72unc nlp57.1072.00han71.3873.51bert(base)72.4274.59bert(large)68.9073.30bert pair68.8973.67bert concat70.18sr-mrs75.1273.44hesm(albert base) 75.7770.6974.84gear75.8878.02kgat(bert base)75.8677.91kgat(bert large)-79.16dream74.9878.05transformer-xhour tarsa77.9681.24.testla f-score61.5865.4662.5267.6264.2168.21--68.5070.6769.6671.8665.1869.7565.6471.0167.2672.5670.0673.2567.1071.6069.4072.8170.2473.6176.8570.6069.0772.3970.7073.97.table 2: overall performance on the fever dataset(%)..models.
random baselinemajority votebertembbert concatgearkgattransformer-xhours tarsa.
ukp snopesprecision recall macro f10.3330.1980.4770.4740.3370.4400.5290.540.
0.3330.1700.4930.4850.3680.4930.5320.611.
0.3330.2490.4850.4780.3520.4650.5310.573.table 3: overall performance on the ukp snopesdataset..tarsa outperforms previous systems, gear andkgat, except dream for la on the test set.
one possible reason is that dream constructsan evidence graph based on the semantic roles ofclaim and evidence, which leverages an explicitgraph-level semantic structure built from semanticroles extracted by semantic role labeling (shi andlin, 2019) in a ﬁne-grained setting.
nevertheless,tarsa shows superior performance than dreamon the fever score, which is a more desirable in-dicator to demonstrate the reasoning capability ofthe model.
as shown in table 3, tarsa performsthe best compared with all previous approaches onthe ukp snopes dataset..5.2 effect of topic number.
table 4 shows the results of our tarsa modelwith different number of topics on the development.
#topic.
255075100.fever (%)la81.2480.3080.6280.30.f-score77.9677.1377.3877.13.ukp snopes.
p.0.5600.6110.5630.592.r.0.5390.5400.5640.533.macro f10.5490.5730.5640.561.table 4: evaluation of tarsa with different numberof topics on fever and ukp snopes..set of fever and ukp snopes.
it can be observedthat the optimal topic number is 25 for fever and50 for ukp snopes.
one possible reason is thatukp snopes is retrieved from multiple domainswhich includes more diverse categories than thoseof fever..5.3 ablation study.
to further illustrate the effectiveness of the topicinformation and the capsule-level aggregation mod-eling, we perform an ablation study on the devel-opment set of fever..effect of topic information: we ﬁrst explorehow the model performance is impacted by theremoval of various topic components.
the ﬁrstsix rows in table 5 present the label accuracy(la) and the fever score on the developmentset of fever after removing various components,where sti denotes the semantic-topic informationin section 3.2, t cee denotes the topical coher-ence among multiple pieces of evidence, t cce de-notes the topical consistency between the claimand each piece of evidence.
as expected, laand the fever score decrease consistently witha gradual removal of various components, whichdemonstrates the effectiveness of incorporatingtopic information in three aspects.
we ﬁnd thatafter all modules are removed, the performanceof tarsa is still nearly 2% higher than our basemodel, transformer-xh, due to the use of the cap-sule network in tarsa..effect of capsule-level aggregation: we ex-plore the effectiveness of the capsule-level aggrega-tion by comparing it with four different aggregationmethods.
the last four rows in table 5 show theresults of aggregation analysis in the developmentset on fever.
the max pooling, sum, and meanaggregation consider the learned representationsof evidence as a single matrix, then apply a linearlayer to classify the input claim as supports,refutes, or nei.
the attention-based aggrega-.
1618models.
our tarsa-sti-t c ee-t cce-t cee - t cce-t cee - t cce - sti.
aggregation.
max poolingsummeanattention-based.
la81.2480.6280.5180.3580.0679.93.
79.3679.6079.2879.52.f-score77.9677.3877.3177.1676.8876.80.
76.3376.5776.1976.45.table 5: ablation analysis in the development set offever..single-hop.
multi-hop.
models.
la89.93bert concat81.56gearkgat90.99transformer-xh 89.23our tarsa91.30.f-score84.2376.6285.2283.5085.48.la92.7489.2193.7393.3994.82.f-score89.9286.6690.9390.7192.03.table 6: fact veriﬁcation accuracy on claims that re-quire single and multiple pieces of evidence..tion method is used in zhou et al.
(2019a), wherethe dot-product attention is computed between theclaim and each evidence to weigh them differently.
finally, our tarsa model aggregates the infor-mation of all pieces of evidence using the capsulenetwork, which connects the evidence capsules tothe class capsules in a clustered way.
from the re-sults, our model outperforms all other aggregationmethods..5.4 performance on different scenarios.
table 6 presents the performance of our model onsingle-hop and multi-hop reasoning scenarios onthe fever dataset compared with several base-lines.
the single-hop mainly focuses on the denois-ing ability of the model with the retrieved evidence,which selects the salient evidence for inference.
the multi-hop mainly emphasizes the relatednessof different pieces of evidence for the joint reason-ing, which is a more complex task..we build the training and testing sets for bothsingle-hop and multi-hop scenarios based on thenumber of gold-standard evidence of a claim.
ifmore than one gold-standard evidence is required,then the claim would require multi-hop reasoning.
the instances with the nei label are removed be-cause there is no gold-standard evidence matchingthis label.
the single-hop reasoning set contains.
claim.
evidence.
example: refutes.
during an interview with the washing-ton post, president obama stated thatamericans would be better off undermartial law.
e1: in a statement appearing in the wash-ington post, united states president bar-rack hussein obama said americanswould be better living under martial law.
e2: the washington post, a long timedemocratic mouth piece and obama sup-porter, downplayed the statement by sug-gesting it was made in jest and that pres-ident obama had been joking around”with the reporter at the time the state-ment was made.
e3: a washington insider, speaking un-der conditions of anonymity, reveals thatobama made additional inﬂammatorycomments not reported by the washing-ton post.
e4: americans have had their chance toaspire to be better, to rise to the occasion,but time and again they fail.
e5: would tighter restrictions really besuch an imposition?.
table 7: example of retrieved evidence ranked by thetopical consistency between the claim and each pieceof evidence.
the topic words are marked in blue..78,838 and 9,682 instances for training and testing,respectively, while the multi-hop reasoning set con-tains 30,972 and 3,650 instances for training andtesting, respectively.
as table 6 shows, tarsaoutperforms all other baselines on la by at least0.31% in the single-hop scenario and 1.09% inthe multi-hop scenario, respectively, which showsa consistent improvement in both scenarios.
inaddition, tarsa is more effective on the multi-hop scenario as the capsule-level aggregation helpsbetter aggregate the information of all pieces ofevidence..5.5 case study.
table 7 illustrates an example from the ukpsnopes dataset which is correctly detected as re-futes, where the topic words extracted by ldaare marked in blue.
from the table we can observe:1) the top two pieces of evidence (i.e., e1 and e2)have higher topical overlap with the claim and alsowith each other; 2) the lower two pieces of evi-dence (i.e., e4 and e5) seem less important becausethey are less topically relevant to the claim; 3) fore3, it is difﬁcult to judge its relevance from eitherthe topical or the semantic perspective, which isambiguous for the identiﬁcation of the truthfulness.
1619of the claim..5.6 error analysis.
we randomly select 100 incorrectly predicted in-stances from fever and ukp snopes datasets andcategorize the main errors.
the ﬁrst type of errorsis caused by the quality of topics extracted by lda.
this is because the average length of sentences inboth datasets is much shorter after removing thelow- and high-frequency tokens, which poses achallenge for lda to extract high quality topicsto match the topical consistency between a claimand each evidence.
the second type of errors isdue to the failure of detecting multiple entity men-tions referring to the same entity.
for example, theclaim describes “go ask alice was the real life di-ary of a teenager girl”, where evidence describesthat “this book is a work of ﬁction”.
the modelfail to understand the relationship between diaryand ﬁction..6 conclusion.
we have presented a novel topic-aware evidencereasoning and stance-aware aggregation model forfact veriﬁcation.
our model jointly exploits thetopical consistency and the semantic interactionto learn evidence representations at the sentencelevel.
moreover, we have proposed the use of thecapsule network to model the implicit stances ofevidence toward a claim for a better aggregation ofinformation encoded in evidence.
the results ontwo public datasets demonstrate the effectivenessof our model.
in the future, we plan to explore aniterative reasoning mechanism for more efﬁcientevidence aggregation for fact checking..acknowledgements.
we would like to thank anonymous reviewersfor their valuable comments and helpful sugges-tions.
this work was funded by the nationalkey research and development program of china(2016yfc1306704), the national natural sciencefoundation of china (61772132), and the epsrc(grant no.
ep/t017112/1, ep/v048597/1).
yh issupported by a turing ai fellowship funded bythe uk research and innovation (ukri) (grant no.
ep/v020579/1)..references.
gabor angeli and christopher d. manning.
2014. nat-uralli: natural logic inference for common sense rea-.
soning.
in proceedings of the 2014 conference onempirical methods in natural language processing,pages 534–545, doha, qatar..david m. blei, andrew y. ng, and michael i. jordan.
2003. latent dirichlet allocation.
the journel of ma-chine learning research, 3:993–1022..hsin-yu chen and cheng-te li.
2020. henin: learn-ing heterogeneous neural interaction networks forexplainable cyberbullying detection on social media.
in proceedings of the 2020 conference on empiri-cal methods in natural language processing, pages2543–2552, online..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstm forin proceedings of thenatural language inference.
55th annual meeting of the association for compu-tational linguistics, pages 1657–1668, vancouver,canada..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4171–4186, minneapolis, mn..reza ghaeini, sadid a. hasan, vivek v. datla, joeyliu, kathy lee, ashequl qadir, yuan ling, aadityaprakash, xiaoli z. fern, and oladimeji farri.
2018.dr-bilstm: dependent reading bidirectional lstmin proceedings offor natural language inference.
the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 1460–1469,louisiana, la..andreas hanselowski, avinesh p. v. s., benjaminschiller, felix caspelherr, debanjan chaudhuri,christian m. meyer, and iryna gurevych.
2018a.
aretrospective analysis of the fake news challengestance-detection task.
in proceedings of the 27th in-ternational conference amiron computational lin-guistics, pages 1859–1874, santa fe, nm..andreas hanselowski, christian stab, claudia schulz,zile li, and iryna gurevych.
2019. a richly anno-tated corpus for different tasks in automated fact-checking.
in proceedings of the 23rd conference oncomputational natural language learning, pages493–503, hong kong, china..andreas hanselowski, hao zhang, zile li, daniilsorokin, benjamin schiller, claudia schulz, andukp-athene: multi-iryna gurevych.
2018b.
sentence textual entailment for claim veriﬁcation.
corr, abs/1809.01479..zhenghao liu, chenyan xiong, maosong sun, andzhiyuan liu.
2020. fine-grained fact veriﬁcationwith kernel graph attention network.
in proceedingsof the 58th annual meeting of the association for.
1620computational linguistics, pages 7342–7351, on-line..jackson luken, nanjiang jiang, and marie-catherinede marneffe.
2018. qed: a fact veriﬁcation systemfor the fever shared task.
in proceedings of thefirst workshop on fact extraction and veriﬁcation(fever), pages 156–160, brussels, belgium..jing ma, wei gao, shaﬁq r. joty, and kam-fai wong.
2019. sentence-level evidence embedding for claimveriﬁcation with hierarchical attention networks.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2561–2571, florence, italy..yixin nie, haonan chen, and mohit bansal.
2019a.
combining fact extraction and veriﬁcation with neu-ral semantic matching networks.
in proceedings ofthe thirty-third aaai conference on artiﬁcial intel-ligence, pages 6859–6866, honolulu, hi..yixin nie, songhe wang, and mohit bansal.
2019b.
revealing the importance of semantic retrieval forin proceedings of themachine reading at scale.
2019 conference on empirical methods in naturallanguage processing and the 9th international jointconference on natural language processing, pages2553–2566, hong kong, china..ankur p. parikh, oscar t¨ackstr¨om, dipanjan das, andjakob uszkoreit.
2016. a decomposable attentionin proceed-model for natural language inference.
ings of the 2016 conference on empirical methodsin natural language processing, pages 2249–2255,austin, tx..sara sabour, nicholas frosst, and geoffrey e. hinton.
in ad-2017. dynamic routing between capsules.
vances in neural information processing systems30, pages 3856–3866, long beach, ca..peng shi and jimmy lin.
2019. simple bert mod-els for relation extraction and semantic role labeling.
corr, abs/1904.05255..amir soleimani, christof monz, and marcel worring.
2020. bert for evidence retrieval and claim veri-ﬁcation.
in proceedings of 42nd european confer-ence on information retrieval, pages 359–366, lis-bon, portugal..dominik stammbach and guenter neumann.
2019.team domlin: exploiting evidence enhancement forin proceedings of the sec-the fever shared task.
ond workshop on fact extraction and veriﬁcation(fever), pages 105–109, hong kong, china..shyam subramanian and kyumin lee.
2020. hierar-chical evidence set modeling for automated fact ex-traction and veriﬁcation.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing, pages 7798–7809, online..james.
andreas vlachos,.
and arpit mittal..christosthorne,2018.christodoulopoulos,fever: a large-scale dataset for fact extraction andveriﬁcation.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 809–819, new orleans, la..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008, long beach,ca..petar velickovic, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
2018. graph attention networks.
in proceedings of6th international conference on learning represen-tations, vancouver, canada..soroush vosoughi, deb roy, and sinan aral.
2018.the spread of true and false news online.
science,359(6380):1146–1151..david wadden, shanchuan lin, kyle lo, lucy luwang, madeleine van zuylen, arman cohan, andhannaneh hajishirzi.
2020. fact or ﬁction: verify-in proceedings of the 2020ing scientiﬁc claims.
conference on empirical methods in natural lan-guage processing, pages 7534–7550, online..min yang, wei zhao, jianbo ye, zeyang lei, zhouzhao, and soufei zhang.
2018.investigating cap-sule networks with dynamic routing for text classi-ﬁcation.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 3110–3119, brussels, belgium..zhilin yang, zihang dai, yiming yang, jaime g. car-bonell, ruslan salakhutdinov, and quoc v. le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems 32, pages 5754–5764,vancouver, canada..wenpeng yin and dan roth.
2018. twowingos: a two-wing optimization strategy for evidential claim veri-ﬁcation.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 105–114, brussels, belgium..takuma yoneda, jeff mitchell, johannes welbl, pon-tus stenetorp, and sebastian riedel.
2018. ucl ma-chine reading group: four factor framework for factin proceedings of the first work-ﬁnding (hexaf).
shop on fact extraction and veriﬁcation (fever),pages 97–102, brussels, belgium..reza zafarani, xinyi zhou, kai shu, and huan liu.
fake news research: theories, detection2019.in proceedings ofstrategies, and open problems.
the 25th acm sigkdd international conference onknowledge discovery & data mining, pages 3207–3208, anchorage, ak..1621chen zhao, chenyan xiong, corby rosset, xiasong, paul n. bennett, and saurabh tiwary.
2020.transformer-xh: multi-evidence reasoning with ex-in proceedings of 8th interna-tra hop attention.
tional conference on learning representations, ad-dis ababa, ethiopia..wanjun zhong, jingjing xu, duyu tang, zenan xu,nan duan, ming zhou, jiahai wang, and jian yin.
2020. reasoning over semantic-level graph for factchecking.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6170–6180, online..jie zhou, xu han, cheng yang, zhiyuan liu, lifengwang, changcheng li, and maosong sun.
2019a.
gear: graph-based evidence aggregating and rea-in proceedings of thesoning for fact veriﬁcation.
57th annual meeting of the association for compu-tational linguistics, pages 892–901, florence, italy..xinyi zhou and reza zafarani.
2020. a survey offake news: fundamental theories, detection meth-ods, and opportunities.
acm computing surveys,53(5):109:1–109:40..xinyi zhou, reza zafarani, kai shu, and huan liu.
theories, de-fake news: fundamental2019b.
in proceedingstection strategies and challenges.
of the twelfth acm international conference onweb search and data mining, pages 836–837, mel-bourne, australia..1622