mate-kd: masked adversarial text, a companion to knowledgedistillation.
ahmad rashid1∗, vasileios lioutas2∗†, mehdi rezagholizadeh11huawei noah’s ark lab, 2university of british columbiaahmad.rashid@huawei.com, contact@vlioutas.com,mehdi.rezagholizadeh@huawei.com.
abstract.
the advent of large pre-trained language mod-els has given rise to rapid progress in the ﬁeldof natural language processing (nlp).
whilethe performance of these models on standardbenchmarks has scaled with size, compres-sion techniques such as knowledge distilla-tion have been key in making them practi-cal.
we present mate-kd, a novel text-based adversarial training algorithm which im-proves the performance of knowledge distilla-tion.
mate-kd ﬁrst trains a masked languagemodel-based generator to perturb text by max-imizing the divergence between teacher andstudent logits.
then using knowledge distilla-tion a student is trained on both the originaland the perturbed training samples.
we evalu-ate our algorithm, using bert-based models,on the glue benchmark and demonstrate thatmate-kd outperforms competitive adversar-ial learning and data augmentation baselines.
on the glue test set our 6 layer robertabased model outperforms bertlarge..1.introduction.
et.
(vaswani.
al., 2017).
transformersandtransformer-based pre-trained language models(plms) (devlin et al., 2019) are ubiquitous inapplications of nlp.
they are highly parallelizableand their performance scales well with an increasein model parameters and data.
increasing modelparameters depends on the availability of computa-tional resources and plms are typically trained onunlabeled data which is cheaper to obtain..recently, the trillion parameter mark has beenbreached for plms (fedus et al., 2021) amid seri-ous environmental concerns (strubell et al., 2019).
however, without a change in our current training.
∗ equal contribution† work done during an internship at huawei noah’s ark.
lab..paradigm , training larger models may be unavoid-able (li et al., 2020).
in order to deploy thesemodels for practical applications such as for vir-tual personal assistants, recommendation systems,e-commerce platforms etc.
model compression isnecessary..knowledge distillation (kd) (buciluˇa et al.,2006; hinton et al., 2015) is a simple, yet pow-erful knowledge transfer algorithm which is usedfor neural model compression (jiao et al., 2019;sanh et al., 2019), ensembling (hinton et al., 2015)inand multi-task learning (clark et al., 2019).
nlp, kd for compression has received renewedinterest in the last few years.
it is one of the mostwidely researched algorithms for the compressionof transformer-based plms (rogers et al., 2020).
one key feature which makes kd attractive isthat it only requires access to the teacher’s output orlogits and not the weights themselves.
therefore, ifa trillion parameter model resides on the cloud, anapi level access to the teacher’s output is sufﬁcientfor kd.
consequently, the algorithm is architectureagnostic, i.e., it can work for any deep learningmodel and the student can be a different modelfrom the teacher..recent works on kd for transfer learning withplms extend the algorithm in two main direc-tions.
the ﬁrst is towards “model” distillation (sunet al., 2019; wang et al., 2020; jiao et al., 2019)i.e.
distilling the intermediate weights such as theattention weights or the intermediate layer outputof transformers.
the second direction is towardscurriculum-based or progressive kd (sun et al.,2020; mirzadeh et al., 2019; jafari et al., 2021)where the student learns one layer at a time or froman intermediary teacher, known as a teacher as-sistant.
while these works have shown accuracygains over standard kd, they have come at the costof architectural assumptions, least of them a com-mon architecture between student and teacher, and.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1062–1071august1–6,2021.©2021associationforcomputationallinguistics1062greater access to teacher parameters and interme-diate outputs.
another issue is that the decisionto distill one teacher layer and to skip another isarbitrary.
still the teacher typically demonstratesbetter generalization.
we are interested in kd for model compressionand study the use of adversarial training (good-fellow et al., 2014) to improve student accuracyusing just the logits of the teacher as in standardkd.
speciﬁcally, our work makes the followingcontributions:.
• we present a text-based adversarial algorithm,mate-kd, which increases the accuracy ofthe student model using kd..• our algorithm only requires access to theteacher’s logits and thus keeps the teacher andstudent architecture independent..• we evaluate our algorithm on the glue(wang et al., 2018) benchmark and demon-strate improvement over competitive base-lines..• on the glue test set, we achieve a score of.
80.9, which is higher than bertlarge.
• we also demonstrate improvement on out-of-.
domain (ood) evaluation..2 related work.
2.1 knowledge distillation.
we can summarize the knowledge distillation loss,l, as following:.
lce = hce.
(cid:0)y, s(x))(cid:1)(cid:16).
lkd = t 2dkl.
σ(.
zt(x)t.), σ(.
zs(x)t.(cid:17).
).
(1).
l = (1 − λ)lce + λlkd.
where hce represents the cross entropy betweenthe true label y and the student network predictions(x) for a given input x, dkl is the kl diver-gence between the teacher and student predictionssoftened using the temperature parameter t , z(x)is the network output before the softmax layer (log-its), and σ(.)
indicates the softmax function.
theterm λ in the above equation is a hyper-parameterwhich controls the amount of contribution from thecross entropy and kd loss..patient kd (sun et al., 2019) introduces an ad-ditional loss to kd which distills the intermediate.
layer information onto the student network.
due toa difference in the number of student and teacherlayers they propose either skipping alternate lay-ers or distilling only the last few layers.
tiny-bert (jiao et al., 2019) applies embedding distil-lation and intermediate layer distillation which in-cludes hidden state distillation and attention weightdistillation.
although it achieves strong results onthe glue benchmark, this approach is infeasiblefor very large teachers.
minilm (wang et al., 2020)proposed an interesting alternative whereby theydistill the key, query and value matrices of the ﬁnallayer of the teacher..2.2 adversarial training.
adversarial examples are small perturbations totraining samples indistinguishable to humans butenough to produce misclassiﬁcations by a trainedneural network.
goodfellow et al.
(2014) showedthat adding these examples to the training set canmake a neural network model robust to perturba-tions.
miyato et al.
(2016) adapt adversarial train-ing to text classiﬁcation and improve performanceon a few supervised and semi-supervised text clas-siﬁcation tasks..training has.
in nlp, adversarial.
surpris-ingly been shown to improve generalization aswell (cheng et al., 2019; zhu et al., 2019).
chenget al.
(2019) study machine translation and proposemaking the model robust to both source and targetperturbations, generated by swapping the embed-ding of a word with that of its synonym.
theymodel small perturbations by considering wordswaps which cause the smallest increase in the lossgradient.
they achieve a higher bleu score onchinese-english and english-german translationcompared to the baseline..zhu et al.
(2019) propose a novel adversarialtraining algorithm, freelb, to make gradient-basedadversarial training efﬁcient by updating both em-bedding perturbations and model parameters simul-taneously during the backward pass of training.
they show improvements on multiple languagemodels on the glue benchmark.
embeddingperturbations are attractive because they producestronger adversaries (zhu et al., 2019) and keep thesystem end-to-end differentiable as the embeddingsare continuous.
the salient features of adversar-ial training for nlp are a) a minimax formulationwhere adversarial examples are generated to max-imize a loss function and the model is trained to.
1063minimize the loss function and b) a way of keepingthe perturbations small such as a norm-bound onthe gradient (zhu et al., 2019) or replacing wordsby their synonyms (cheng et al., 2019)..if these algorithms are adapted to kd one keychallenge is the embedding mismatch between theteacher and student.
even if the embedding size isthe same, the student embedding needs to be frozento match the teacher embedding and freezing em-beddings typically leads to lower performance.
ifwe adapt adversarial training to kd, one key advan-tage is that access to the teacher distribution relaxesthe requirement of generating label preserving per-turbations.
these considerations have promptedus to design an adversarial algorithm where weperturb the actual text instead of the embedding.
rashid et al.
(2020) also propose a text-based adver-sarial algorithm for the problem of zero-shot kd(where the teacher’s training data is unavailable),but their generator instead of perturbing text gen-erates new samples and requires additional lossesand pre-training to work well..2.3 data augmentation.
one of the ﬁrst works on bert compression (tanget al., 2019) used kd and proposed data augmenta-tion using heuristics such as part-of-speech guidedword replacement.
they demonstrated improve-ment on three glue tasks.
one limitation of thisapproach is that the heuristics are task speciﬁc.
jiaoet al.
(2019) present an ablation study in their workwhereby they demonstrate a strong contribution ofdata augmentation to their kd algorithm perfor-mance.
they augment the data by randomly select-ing a few words of a training sentence and replac-ing them with words with the closest embeddingunder cosine distance.
our adversarial learning al-gorithm can be interpreted as a data augmentationalgorithm, but instead of a heuristic approach wepropose a principled end-to-end differentiable aug-mentation method based on adversarial learning..khashabi et al.
(2020) presented a data augmen-tation technique for question answering wherebythey took seed questions and asked humans to per-turb only a few tokens to generate new ones.
thehuman annotators could modify the label if needed.
they demonstrated improved generalization and ro-bustness with the augmented data.
we will demon-strate that our algorithm is built on similar prin-ciples but does not require humans in the loop.
instead of human annotators to modify the labels.
we use the teacher..3 methodology.
we propose an algorithm that involves co-trainingand deploy an adversarial text generator while train-ing a student network using kd.
figure 1 gives anillustration of our architecture..figure 1: illustration of the maximization and mini-mization steps of mate-kd.
3.1 generator.
the text generator is simply a pre-trained maskedlanguage model which is trained to perturb trainingsamples adversarially.
we can frame our techniquein a minimax regime such that in the maximizationstep of each iteration, we feed the generator with atraining sample with few of the tokens replaced bymasks.
we ﬁx the rest of the sentence and replacethe masked tokens with the generator output toconstruct a pseudo training sample x (cid:48).
this pseudosample is fed to both the teacher and the studentmodels and the generator is trained to maximizethe divergence between the teacher and the student.
we present an example of the masked generationprocess in figure 2. the student is trained duringthe minimization step..1064x (cid:48) = gφ(x m)forward.
= argmax(cid:0)σgumbel(zφ(x m)(cid:1).
where.
(4).
(5).
figure 2: this ﬁgure illustrates how a training samplewill be randomly masked and then fed to the text gen-erator gφ to get the pseudo training sample..σgumbel(zi) =.
exp.
(cid:17).
(cid:16)(cid:0) log(zi) + gi.
(cid:1)/τ(cid:16)(cid:0) log(zj) + gj.
(cid:17).
(cid:1)/τ.
σk.
j=1 exp.
3.2 maximization step.
the generator is trained to generate pseudo samplesby maximizing the following loss function:.
maxφ.lg(φ) =.
(cid:16).
dkl.
t (cid:0)gφ(x m)(cid:1), sθ.
(cid:0)gφ(x m)(cid:1)(cid:17).
,.
(2).
where dkl is the kl divergence, gφ(.)
is thetext generator network with parameters φ, t (·) andsθ(·) are the teacher and student networks respec-tively, and x m is a randomly masked version ofthe input x = [x1, x2, ..., xn] with n tokens..∀xi ∈ x = [x1, ..., xi, ..., xn] ∼ d,xmi = mask(xi ∈ x, pi).
p∼unif(0,1).
(cid:40).
=.
xi,< mask >, o.w..pi ≥ ρ.
(3).
where unif(0, 1) represents the uniform distribu-tion, and the mask( · ) function masks the tokens ofinputs sampled from the data distribution d withthe probability of ρ. the term ρ can be treated asa hyper-parameter in our technique.
in summary,for each training sample, we randomly mask sometokens according to the samples derived from theuniform distribution and the threshold value of ρ.then in the forward pass, the masked sample,x m, is fed to the generator to obtain the outputpseudo text based on the generator predictions ofthe mask tokens.
the generator needs to output aone-hot representation but using an argmax insidethe generator would lead to non-differentiability.
instead we apply the gumbel-softmax (jang et al.,2016), which, is an approximation to samplingfrom the argmax.
using the straight through es-timator (bengio et al., 2013) we can still applyargmax in the forward pass and can obtain text, x (cid:48)from the network outputs:.
gi ∼ gumbel(0, 1) and zφ(.)
returns the logits pro-duced by the generator for a given input.
τ is thetemperature in equation 5..in the backward pass, the generator simply ap-plies the gradients from the gumbel-softmax with-out the argmax :.
gφ(x m)backward.
= σgumbel(zφ(x m)).
(6).
3.3 minimization step.
in the minimization step, the student network istrained to minimize the gap between the teacherand student predictions and match the hard labelsfrom the training data by minimizing the followingloss equation:.
minθ.
13.lmate-kd(θ) =.
lce(θ) +.
lkd(θ) +.
ladv (θ).
13.
13.
(7).
where.
ladv (θ) = dkl.
(cid:16).
(cid:17)t (x (cid:48)), sθ(x (cid:48)).
(8).
in equation 7, the terms lkd and lce are thesame as equation 1, lkd(θ) and ladv (θ) areused to match the student with the teacher, andlce(θ) is used for the student to follow the ground-truth labels y..bear in mind that our lmate-kd(θ) loss is dif-ferent from the regular kd loss in two aspects:ﬁrst, it has the additional adversarial loss, ladvto minimize the gap between the predictions of thestudent and the teacher with respect to the gener-ated masked adversarial text samples, x (cid:48), in themaximization step; second, we do not have theweight term λ form kd in our technique any more(i.e.
we consider equal weights for the three lossterms in lmate-kd)..10653.4 rationale behind the masked.
adversarial text generation for kd.
the rationale behind generating partially maskedadversarial texts instead of generating adversarialtexts from scratch (that is equivalent to masking theinput of the text generator entirely) is three-fold:.
1. partial masking is able to generate more real-istic sentences compared to generating themfrom scratch when trained only to increaseteacher and student divergence.
we present afew generated sentences in section 4.6.
2. generating text from scratch increases thechance of generating ood data.
feedingood data to the kd algorithm leads to match-ing the teacher and student functions acrossinput domains that the teacher is not trainedon..3. by masking and changing only a few tokensof the original text, we constrain the amountof perturbation as is required for adversarialtraining..in our mate-kd technique, we can tweak the ρto control our divergence from the data distributionand ﬁnd the sweet spot which gives rise to max-imum improvement for kd.
we also present anablation on the effect of this parameter on down-stream performance in section 4.5..4 experiments.
we evaluated mate-kd on all nine datasets ofthe general language understanding evaluation(glue) (wang et al., 2018) benchmark which in-clude classiﬁcation and regression.
these datasetscan be broadly divided into 3 families of prob-lems.
single set tasks which include linguisticacceptability (cola) and sentiment analysis (sst-2).
similarity and paraphrasing tasks which includeparaphrasing (mrpc and qqp) and a regressiontask (sts-b).
inference tasks which include natu-ral language inference (mnli, wnli, rte) andquestion answering (qnli)..4.1 experimental setup.
we evaluate our algorithm on two different setups.
on the ﬁrst the teacher model is robertalarge(liu et al., 2019) and the student is initialized withthe weights of distillroberta (sanh et al., 2019).
robertalarge consists of 24 layers with a hid-den dimension of 1024 and 16 attention heads and.
a total of 355 million parameters.
we use the pre-trained model from huggingface (wolf et al., 2019).
the student consists of 6 layers, 768 hidden dimen-sion, 8 attention heads and 82 million parameters.
both models have a vocabulary size of 50,265 ex-tracted using the byte pair encoding (bpe) (sen-nrich et al., 2016) tokenization method..on our second setup,.
the teacher model isbertbase (devlin et al., 2019) and the studentmodel is initialized with the weights of distilbertwhich consists of 6 layers with a hidden dimen-sion of 768 and 8 attention heads.
the pre-trainedmodels are taken from the authors’ release.
theteacher and the student are 110m and 66m param-eters respectively with a vocabulary size of 30,522extracted using bpe..hyper-parameters we ﬁne-tuned the robertastudent model and picked the best checkpoint thatgave the highest score on the dev set of glue.
these hyper-parameters were ﬁxed for the gluetest submissions as well as the bert experiments.
we used the adamw (loshchilov and hutter,2017) optimizer with the default values.
in addition,we used a linear decay learning rate scheduler withno warmup steps.
we set the masking probabilityp to be 0.3. additionally, we set the value ng to10 and ns to 100. the learning rate, number ofepochs, and other hyper-parameters are presentedon table 8 of appendix a..hardware details we trained all models usinga single nvidia v100 gpu.
we used mixed-precision training (micikevicius et al., 2018) toexpedite the training procedure.
all experimentswere run using the pytorch1 framework..4.2 results.
table 1 presents the results of mate-kd on theglue dev set.
even though the datasets have dif-ferent evaluation metrics, we present the average ofall scores as well, which is used to rank the submis-sions to glue.
our ﬁrst baseline is the ﬁne-tuneddistilroberta and then we compare with kd,freelb, freelb plus kd, and tinybert (jiaoet al., 2019) data augmentation plus kd..we observe that freelb (zhu et al., 2019) signif-icantly improves the ﬁne-tuned student by around1.2 points on average.
however, when we applyboth freelb + kd, we do not see any further im-provement whereas applying kd alone improves.
1https://pytorch.org/.
1066method.
cola sst-2 mrpc sts-b qqp mnli qnli rte.
score.
robertalarge (teacher).
distilroberta (student)student + freelbstudent + freelb + kdstudent + kdstudent + tinybert aug + kd.
student + mate-kd (ours).
68.1.
56.658.158.160.961.3.
65.9.
96.4.
92.793.193.292.593.3.
94.1.
91.9.
89.590.190.590.290.4.
91.9.
92.3.
87.288.888.689.088.6.
90.4.
91.5.
90.890.991.291.691.7.
91.9.
90.2.
84.184.083.784.184.4.
85.8.
94.6.
91.391.090.891.391.6.
92.5.
86.3.
65.767.868.271.172.5.
75.0.
85.28.
78.7880.0180.0680.7781.12.
82.64.table 1: dev set results using distilroberta as the student on the glue benchmark.
the score for the wnlitask is 56.3 for all models..the score by about 2 points.
this is so becausefreelb relies on the model (student) output ratherthan the teacher output to generate adversarial per-turbation and therefore cannot beneﬁt from kd.
aspreviously discussed, freelb relies on embeddingperturbation and in order to generate the teacheroutput on the perturbed student, both the embed-dings need to be tied together, which is infeasibledue to the size and training requirements..we also compared against the data augmentationalgorithm of tinybert.
we ran their code to gen-erate the augmented data ofﬂine.
although theyaugment the data about 20 times depending on theglue task, we observed poor results if we use allthis data to ﬁne-tune with kd.
we only generated1x augmented data and saw an average improve-ment of 0.35 score over kd.
mate-kd achievesthe best result among the student models on allglue tasks and achieves an average improvementof 1.87 over just kd.
we also generated the samenumber of adversarial samples as the training data.
we present the results on the test set of glue ontable 2. we list the number of parameters for eachmodel.
the results of bertbase, bertlarge(devlin et al., 2019), tinybert and mobilebert(sun et al., 2020) are taken from the glue leader-board2.
the kd models have robertalarge, ﬁne-tuned without ensembling as the teacher..tinybert and mobilebert are the currentstate-of-the-art 6 layer transformer models on theglue leaderboard.
we include them in this com-parison although their teacher is bertbase as op-posed to robertalarge.
we make the case that onereason we can train with a larger and more power-ful teacher is that we only require the logits of theteacher while training.
most of the works in theliterature proposing intermediate layer distillation(jiao et al., 2019; sun et al., 2020, 2019) are trained.
2https://gluebenchmark.com/leaderboard.
on 12 layer bert teachers.
as plms get biggerin size, feasible approaches to kd will involve al-gorithms which rely on only minimal access toteachers..we apply a standard trick to boost the perfor-mance of sts-b and rte, i.e., we initialize thesemodels with the trained checkpoint of mnli (liuet al., 2019).
this was not done for the dev results.
the wnli score is the same for all the modelsand although, not displayed on the table, is partof the average score.
we make a few observationsfrom this table.
firstly, using kd a student witha powerful teacher can overcome a signiﬁcant dif-ference in parameters between competitive models.
secondly, our algorithm signiﬁcantly improves kdwith an average 2 point increase on the unseenglue testset.
our model is able to achieve state-of-the-art results for a 6 layer transformer modelon the glue leaderboard..we also evaluate our algorithm using bertbaseas teacher and distilbert as student on gluebenchmark.
wnli results are the same for all andthey are used to calculate the average.
we com-pare against the teacher, student, and kd plus tiny-bert augmentation.
here, remarkably mate-kdcan beat the teacher performance on average.
onthe two largest datasets in glue, qqp and mnli,we beat and match the teacher performance respec-tively..we observe that mate-kd outperforms its com-petitors when both the teacher is twice the size andfour times the size of the student.
this may bebecause the algorithm generates adversarial exam-ples based on the teacher’s distribution.
a welldesigned adversarial algorithm can help us probeparts of the teacher’s distribution not spanned bythe training data leading to better generalization..1067model (param.).
cola sst-2.
mrpc.
sts-b.
qqp.
mnli-m/mm qnli rte.
score.
tinybert (66m)bertbase (110m)mobilebert (66m)distilrob.
+ kd (82m)bertlarge (340m).
mate-kd (82m).
51.152.151.154.360.5.
56.0.
93.193.592.693.194.9.
94.9.
87.3/82.688.9/84.888.8/84.586.0/80.889.3/85.4.
85.0/83.787.1/85.886.2/84.885.7/84.987.6/86.5.
71.6/89.171.2/89.270.5/88.371.9/89.572.1/89.3.
84.6/83.284.6/83.484.3/83.483.6/82.986.7/85.9.
91.7/88.7.
88.3/87.7.
72.6/89.7.
85.5/84.8.
90.490.591.690.892.7.
92.1.
70.066.470.474.170.1.
75.0.
78.178.378.578.980.5.
80.9.table 2: leaderboard test results of experiments on glue tasks.
the score for the wnli task is 65.1 for allmodels..method.
bertbase (teacher).
distilbert (student)student + tinybert aug. + kd.
student + mate-kd (ours).
cola sst-2 mrpc sts-b qqp mnli qnli rte.
score.
59.5.
51.355.2.
60.4.
93.1.
91.391.9.
92.2.
86.7.
87.587.0.
88.0.
88.4.
86.987.8.
88.5.
91.0.
88.589.5.
91.4.
84.6.
82.182.1.
84.5.
91.5.
89.289.7.
91.2.
68.2.
59.968.6.
70.0.
79.9.
77.078.7.
80.3.table 3: dev results on the glue benchmark using distilbert as the student model.
wnli results are 56.3 forall models..4.3 ood evaluation.
it has been shown that strong nlu models tendto learn spurious surface level patterns from thedataset (poliak et al., 2018; gururangan et al.,2018) and may perform poorly on carefully con-structed ood datasets.
in table 4 we present theevaluation of mate-kd (roberta-based) trainedon mnli and qqp on the hans (mccoy et al.,2019) and the paws (zhang et al., 2019) evalua-tion sets respectively..model.
hans.
paws.
distilrobertamate-kd.
58.966.6.
36.538.3.table 4: model performance on ood evaluation setshans and paws for mnli and qqp respectively.
we use the same model checkpoint as the onepresented in table 1 and compare against dis-tilroberta.
we observe that mate-kd im-proves the baseline performance on both evaluationdatasets.
the performance increase on hans islarger.
we can conclude that the algorithm improve-ments are not due to learning spurious correlationsand biases in the dataset..4.4 ablation study.
table 5 presents the contribution of the generatorand adversarial learning to mate-kd.
we ﬁrstpresent the result of mate-kd on all the gluedatasets (except wnli) and compare against the.
effect of removing the adversarial training and thenthe generator altogether.
when we remove the ad-versarial training, we essentially remove the maxi-mization step and do not train the generator.
thegenerator in this setting is a pre-trained maskedlanguage model.
in the minimization step, we stillgenerate pseudo samples and apply all losses.
thesetting where we remove the generator is akin to asimple kd..we observe that the generator improves kd byan average of 1.3 and the adversarial training in-creases the score further by 0.6..4.5 sensitivity analysis.
our algorithm does not require the loss interpo-lation weight of kd but instead relies on one ad-ditional parameter, ρ, which is the probability ofmasking a given token.
we present the effect ofchanging ρ in table 7 on mnli and rte dev set re-sults ﬁxing all other hyper-parameters.
we selectedmnli and rte because they are part of naturallanguage inference, which is one of the hardesttasks on glue.
moreover, in the roberta exper-iments we see the largest drop in student scoresfor these two datasets.
we can observe that formnli the best result is for 30% followed by 20%and for rte the best choice is 40% followed by30%.
this corresponds to the heuristic based dataaugmentation works where they typically modifytokens with a 30% to 40% probability.
we set thisparameter to 30% for all the experiments and didnot tune this for each dataset or each architecture..1068model.
cola sst-2 mrpc sts-b qqp mnli qnli rte.
score.
mate-kd- adv train- generator.
65.964.760.9.
94.193.192.5.
91.990.090.2.
90.490.389.0.
91.991.891.6.
85.885.384.1.
92.592.891.3.
75.074.071.1.
82.6482.0380.77.table 5: the ablation of mate-kd on four datasets from the glue benchmark.
we present the result of mate-kd, a version of the algorithm without training the generator and a version of the algorithm without the generator.
results are on the dev set..originalthe new insomnia is a surprisingly faithfulremake of its chilly predecessor, andbeautifully shot, delicately scored andpowered by a set of heartfelt performancesa perfectly pleasant if slightly pokey comedythat appeals to megood news to anyone who’s fallen underthe sweet, melancholy spell of thisunique director’s previous ﬁlms.
generatedsinister new insomnia shows a surprisingly terribleremake of its hilarious predecessor, andbeautifully sublime, delicately scored,powered by great dozens of heartfelt performancesa 10 pleasant if slightly pokey comedyfederal appeals punished megood news for anyone who’s fallen underthe sweet, melancholy spell of thisunique director’s previous mistakes.
table 6: examples of original and adversarially generated samples during training for the sst-2 dataset.
task.
p hyperparameter10% 20% 30% 40% 50 %.
mnlirte.
85.474.0.
85.574.8.
85.875.0.
84.775.4.
84.674.6.table 7: ρ value sensitivity analysis on two gluetasks..4.6 generated samples.
we present a few selected samples that our genera-tor produced during training for the sst-2 dataseton table 6. sst-2 is a binary sentiment analysisdataset.
the data consist of movie reviews and isboth at the phrase and sentence level..we observe that we only modify a few tokens inthe generated text.
however, one of three thingshappens if the text is semantically plausible.
eitherthe generated sentence keeps the same sentimentas in examples 2 and 3, or it changes the sentimentas in examples 1 and 4 or the text has ambiguoussentiment as in example 5. we can use all of thesefor training since we do not rely on the originallabel but obtain the teacher’s output..5 discussion and future work.
we have presented mate-kd, a novel text-basedadversarial training algorithm which improves thestudent model in kd by generating adversarial ex-amples while accessing the logits of the teacher.
only.
this approach is architecture agnostic andcan be easily adapted to other applications of kdsuch as model ensembling and multi-task learning.
we demonstrate the need for an adversarial train-ing algorithm for kd based on text rather than em-bedding perturbation.
moreover, we demonstratethe importance of masking for our algorithm..one key theme that we have presented in thiswork is that as plms inevitably increase in sizeand number of parameters, techniques that relyon access to the various layers and intermediateparameters of the teacher will be more difﬁcultto train.
in contrast, algorithms which are well-motivated and require minimal access to the teachermay learn from more powerful teachers and wouldbe more useful.
an example of such an algorithmis the kd algorithm itself..future work will consider a) using label informa-tion and a measure of semantic quality to ﬁlter thegenerated sentences b) exploring the application ofour algorithm to continuous data such as speechand images and c) exploring other applications ofkd..acknowledgement.
we thank mindspore 3, a new deep learning com-puting framework, for the partial support of thiswork.
3https://www.mindspore.cn/.
1069impact statement.
our research primarily deals with deploying highquality nlp applications to a wide audience aroundthe globe.
we contend that these technologies cansimplify many of our mundane tasks and free upour time to pursue more pleasurable work..references.
yoshua bengio, nicholas l´eonard,.
and aaroncourville.
2013. estimating or propagating gradi-ents through stochastic neurons for conditional com-putation.
arxiv preprint arxiv:1308.3432..cristian buciluˇa, rich caruana,.
and alexandruniculescu-mizil.
2006. model compression.
in pro-ceedings of the 12th acm sigkdd internationalconference on knowledge discovery and data min-ing, pages 535–541..yong cheng, lu jiang, and wolfgang macherey.
2019.robust neural machine translation with doubly ad-versarial inputs.
arxiv preprint arxiv:1906.02443..kevin clark, minh-thang luong, urvashi khandel-wal, christopher d manning, and quoc v le.
2019. bam!
born-again multi-task networks forarxiv preprintnaturalarxiv:1907.04829..language understanding..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..william fedus, barret zoph, and noam shazeer.
2021.switch transformers: scaling to trillion parametermodels with simple and efﬁcient sparsity.
arxivpreprint arxiv:2101.03961..ian j goodfellow, jonathon shlens, and christianszegedy.
2014. explaining and harnessing adversar-ial examples.
arxiv preprint arxiv:1412.6572..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah asmith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107–112..geoffrey hinton, oriol vinyals, and jeff dean.
2015.distilling the knowledge in a neural network.
arxivpreprint arxiv:1503.02531..aref jafari, mehdi rezagholizadeh, pranav sharma,and ali ghodsi.
2021. annealing knowledge distil-lation.
in proceedings of the 16th conference of theeuropean chapter of the association for computa-tional linguistics: main volume, pages 2493–2504..eric jang, shixiang gu, and ben poole.
2016. categor-ical reparameterization with gumbel-softmax.
arxivpreprint arxiv:1611.01144..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2019. tinybert: distilling bert for natural languageunderstanding.
arxiv preprint arxiv:1909.10351..daniel khashabi, tushar khot, and ashish sabharwal.
2020. more bang for your buck: natural perturba-tion for robust question answering.
in proceedingsof the 2020 conference on empirical methods innatural language processing (emnlp), pages 163–170..zhuohan li, eric wallace, sheng shen, kevin lin,kurt keutzer, dan klein, and joseph e gonzalez.
2020. train large, then compress: rethinking modelsize for efﬁcient training and inference of transform-ers.
arxiv preprint arxiv:2002.11794..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrain-ing approach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2017. decou-pled weight decay regularization.
arxiv preprintarxiv:1711.05101..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448..paulius micikevicius, sharan narang, jonah alben,gregory diamos, erich elsen, david garcia, borisginsburg, michael houston, oleksii kuchaiev,ganesh venkatesh, and hao wu.
2018. mixed pre-in international conference oncision training.
learning representations..seyed-iman mirzadeh, mehrdad farajtabar, angli, nir levine, akihiro matsukawa, and has-improved knowledgesan ghasemzadeh.
2019.arxiv preprintdistillation via teacher assistant.
arxiv:1902.03393..takeru miyato, andrew m dai, and ian good-fellow.
2016. adversarial training methods forsemi-supervised text classiﬁcation.
arxiv preprintarxiv:1605.07725..adam poliak, jason naradowsky, aparajita haldar,rachel rudinger, and benjamin van durme.
2018..1070thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2019.huggingface’s transformers: state-of-the-art naturallanguage processing.
arxiv, abs/1910.03771..yuan zhang, jason baldridge, and luheng he.
2019.paws: paraphrase adversaries from word scrambling.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 1298–1308..chen zhu, yu cheng, zhe gan, siqi sun, thomasgoldstein, and jingjing liu.
2019.freelb: en-hanced adversarial training for language understand-ing.
arxiv preprint arxiv:1909.11764..a training details.
we present the details of the learning rate, num-ber of epochs, and the batch size we use for eachtraining set of glue for both the bert and theroberta settings..colasst-2mrpcsts-bqqpmnliqnlirtewnli.
batch size832832323232168.lr2e-52e-53e-52e-52e-52e-52e-57e-67e-5.
epochs50501001003030505050.table 8: hyper-parameter values fordatasets.
lr is the learning rate..the glue.
hypothesis only baselines in natural language in-in proceedings of the seventh joint con-ference.
ference on lexical and computational semantics,pages 180–191..ahmad rashid, vasileios lioutas, abbas ghaddar, andmehdi rezagholizadeh.
2020. towards zero-shotknowledge distillation for natural language process-ing.
arxiv preprint arxiv:2012.15495..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
arxiv preprint arxiv:2002.12327..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725..emma strubell, ananya ganesh, and andrew mc-energy and policy considera-arxiv preprint.
callum.
2019.tions for deep learning in nlp.
arxiv:1906.02243..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
arxiv preprint arxiv:1908.09355..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020. mobilebert:a compact task-agnostic bert for resource-limited de-vices.
arxiv preprint arxiv:2004.02984..raphael tang, yao lu, linqing liu, lili mou, olgavechtomova, and jimmy lin.
2019. distilling task-speciﬁc knowledge from bert into simple neural net-works.
arxiv preprint arxiv:1903.12136..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355..wenhui wang, furu wei, li dong, hangbo bao,nan yang, and ming zhou.
2020. minilm: deepself-attention distillation for task-agnostic compres-arxiv preprintsion of pre-trained transformers.
arxiv:2002.10957..1071