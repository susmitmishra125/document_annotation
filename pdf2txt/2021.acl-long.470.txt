long-span summarization via local attention and content selection.
potsawee manakul and mark j. f. galesdepartment of engineering, university of cambridgepm574@cam.ac.uk, mjfg@eng.cam.ac.uk.
abstract.
transformer-based models haveachievedstate-of-the-art results in a wide range of nat-ural language processing (nlp) tasks includ-ing document summarization.
typically thesesystems are trained by ﬁne-tuning a large pre-trained modeltask.
one is-to the targetsue with these transformer-based models isthat they do not scale well in terms of mem-ory and compute requirements as the inputlength grows.
thus, for long document sum-marization, it can be challenging to train orin this work, we ex-ﬁne-tune these models.
ploit large pre-trained transformer-based mod-els and address long-span dependencies in ab-stractive summarization using two methods:local self-attention; and explicit content se-lection.
these approaches are compared ona range of network conﬁgurations.
experi-ments are carried out on standard long-spansummarization tasks, including spotify pod-cast, arxiv, and pubmed datasets.
we demon-strate that by combining these methods, wecan achieve state-of-the-art results on all threetasks in the rouge scores.
moreover, with-out a large-scale gpu card, our approach canachieve comparable or better results than exist-ing approaches.1.
1.introduction.
transformer-based models (vaswani et al., 2017)are ubiquitously state-of-art across many naturallanguage processing (nlp) tasks, including sum-marization.
to achieve the best results, the com-munity has trained ever larger transformer modelson larger amount of data, and/or more task-speciﬁcoptimization objectives (devlin et al., 2019; raf-fel et al., 2020; lewis et al., 2020; brown et al.,2020).
in long document summarization, the input.
1our code is available at https://github.com/.
potsawee/longsum0..sequences could be more than an order of mag-nitude longer than the limits of these transformermodels.
although the limits can be extended, train-ing large transformer models on long sequences isexpensive and may not be possible on a standardgpu card because of the self-attention mechanismthat grows quadratically with sequence length..to tackle the quadratic characteristic, recentworks have modiﬁed self-attention mechanism andproposed variants of the transformer such that thequadratic complexity is reduced (tay et al., 2020b;kitaev et al., 2020; child et al., 2019; beltagyet al., 2020; ainslie et al., 2020; zaheer et al.,2020).
however, pre-trained weights of the mod-iﬁed models are not readily available.
in contrast,standard models such as bert (devlin et al., 2019)or bart (lewis et al., 2020) have been trained onvarious target tasks, including text summarization(liu and lapata, 2019b).
this allows practition-ers to achieve good performance with less trainingtime.
thus, we are interested in exploiting pre-trained models for long-span summarization tasks.
we study a range of design conﬁgurations empir-ically and theoretically in regards to memory andcompute requirements as well as their performance.
we propose that long-span dependencies can behandled by two complementary methods.
firstly,inspired by modiﬁed self-attention transformers,we exploit standard transformer models by con-straining attention mechanism to be local, allow-ing longer input spans during training.
secondly,because abstractive summarization systems per-form content selection implicitly (nallapati et al.,2016; lebanoff et al., 2020), to reduce memoryand compute requirements an alternative methodis to perform content selection explicitly beforethe abstractive stage.
we study content selectionduring two phases: training time and test time.
attraining time, we investigate methods to select datafor training ﬁxed-span abstractive models.
at test.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6026–6041august1–6,2021.©2021associationforcomputationallinguistics6026time, we extend existing model-based selectionmethods, and we propose a multitask content selec-tion method that ranks sentences through extractivelabelling based module (cheng and lapata, 2016)and attention based module (see et al., 2017).
ulti-mately, we explore the combined approach, consist-ing of local self-attention transformer and contentselection for long-document summarization..we conduct our experiments using a numberof design conﬁgurations on the spotify open-domain podcast summarization dataset (cliftonet al., 2020).
this dataset is challenging not onlybecause of its long-span nature, but also becausetranscribed spoken utterances typically have lowerinformation density (li et al., 2019; manakul et al.,2020).
furthermore, we carry out experiments onarxiv and pubmed datasets (cohan et al., 2018) tofurther demonstrate and verify the effectiveness ofour approach as well as making comparisons to ex-isting approaches.
we highlight the strengths andweaknesses of our approach in different resourcesand tasks.
the main contributions of this paper are:.
• on local self-attention, we show how to ex-ploit a standard transformer model for long-span summarization, and we show good de-sign considerations based on empirical results..• on content selection, we demonstrate the bestselection method at training time, and wepropose a multitask content selection (mcs)method outperforming baselines at test time..• our work has set new state-of-the-art re-sults on spotify podcast, arxiv and pubmeddatasets in the rouge scores.
furthermore,with a small-scale gpu card, our approachachieves comparable or superior performanceto previous state-of-the-art systems..2 related work.
efﬁcient transformers.
pre-trained transformermodels have shown success and become the start-ing point for various nlp problems such as bert(devlin et al., 2019) in contextual representation,gpt2 in text generation (radford et al., 2019),or bart in seq2seq tasks (lewis et al., 2020).
however, the memory and time requirements fortransformer models grow quadratically with the se-quence length, and for long-span tasks this quicklyleads to gpu running out of memory in training.
to mitigate the quadratic nature, a wide range ofmodiﬁed architectures have recently been proposed.
(tay et al., 2021).
they reduce the quadratic com-plexity of the full self-attention mechanism by us-ing ﬁxed attention patterns (parmar et al., 2018;dai et al., 2019; child et al., 2019; qiu et al., 2020;ainslie et al., 2020; zaheer et al., 2020; beltagyet al., 2020), learnable patterns (kitaev et al., 2020;tay et al., 2020a), low-rank matrix approximation(wang et al., 2020), or kernel method (choroman-ski et al., 2021).
alternatively, it has been shownthat some attention heads are redundant and canbe pruned to reduce model size (voita et al., 2019;michel et al., 2019).
knowledge distillation re-duces memory and compute by compressing alarge model to a smaller one (hinton et al., 2015;sanh et al., 2019).
in contrast, we focus on thedependencies of long input and target sequencesin encoder-decoder architectures, and we exploitpublicly available transformer models with summa-rization weights to long-span summarization tasks..long-span summarization.
efﬁcient transformerarchitectures have been applied to summarize longdocuments such as bigbird (zaheer et al., 2020),and longformer-encoder-decoder (led) (beltagyet al., 2020), which has recently been revised par-allel to this work.2 hierarchical transformer archi-tectures have been applied to multi-document sum-marization (liu and lapata, 2019a), and extractivenews and table-to-text summarization (zhang et al.,2019; narayan et al., 2020).
hierarchical attentionrnn system has been applied to summarize longarticles (cohan et al., 2018)..alternatively, earlier methods show that goodcontent selection helps abstractive news sum-marization systems (chen and bansal, 2018;gehrmann et al., 2018; hsu et al., 2018).
hy-brid systems that select sentences and generate anabstractive summary have been proposed such asextractive system + tlm for scientiﬁc articles (pi-lault et al., 2020), simple selection + bart forpodcasts (manakul and gales, 2020; song et al.,2020), and guided summarization by bert-basedkeyword/sentence extraction + bart for news andscientiﬁc articles (he et al., 2020; dou et al., 2021).
other work includes dividing the source and tar-get into multiple smaller pairs to train abstractivesummarizers (gidiotis and tsoumakas, 2020).
ex-tractive methods with and without redundancy re-duction techniques for long-span summarizationhave been studied (xiao and carenini, 2019, 2020)..2on the self-attention aspect, we believe this system is.
the most comparable to ours (see comparisons in sec.
6.2)..6027figure 1: overview of the combined architecture where we highlight different aspects of this work.
n0 is theoriginal document length, n is the input length to the generation system, and m is the summary length..3 experimental setup.
3.1 datasetspotify podcast.3 the dataset consists of asrtranscripts with human descriptions as summaries(clifton et al., 2020).
we follow the dataprocessing at trec2020 (jones et al., 2020)in removing bad transcript-summary pairs fromresultinga total of 105,360+1,027 episodes,in train/valid/test splits of 60,415/2,189/1,027episodes the same as manakul and gales (2020)..arxiv and pubmed.
popular long document sum-marization datasets consist of academic articleswith abstracts as summaries (cohan et al., 2018)and train/valid/test splits of 203,037/6,436/6,440for arxiv and 119,924/6,633/6,658 for pubmed..dataset.
#doc.
input.
90th% target.
podcastarxivpubmed.
106k216k133k.
5,7278,5843,865.
11,67716,1087,234.
61.1367260.table 1: length statistics (mean & 90th%-ile)..(a) full.
(b) local (w =9).
figure 2: self-attention pattern..hierarchical rnn.
the content selection modelis based on a hierarchical encoder-decoder architec-ture that has been shown effective on meeting andlong document summarization (cohan et al., 2018;zhao et al., 2019; li et al., 2019).
the model con-sists of word-level and sentence-level grus (choet al., 2014).
we add a linear layer on top of thesentence-level gru to perform extractive labelling.
the sentence-level attention mechanism and extrac-tive labelling modules form our multitask contentselection (mcs).
more details in section 5.2..we provide the full details about our implemen-tation, model parameters, hyperparameters, opti-mizer, and training conﬁgurations in appendix b..3.2 models.
4 longer span via local self-attention.
bart and lobart.
we use the publicly releasedbart model (lewis et al., 2020) ﬁne-tuned on cn-ndm (hermann et al., 2015).4 following the lo-cal window attention in sparse transformer (childet al., 2019) and longformer (beltagy et al., 2020),we modify the self-attention mechanism in the en-coder to local self-attention (see figure 2), and werefer to this local self-attention bart as lobart.
it has the same architecture as bart, e.g.
the num-ber of parameters, except that we extend positionalembedding beyond 1,024 by copying bart’s posi-tional embedding with ﬂipping to allow a smoothertransition.
see details in appendix b.1..3https://podcastsdataset.byspotify.com4https://huggingface.co/facebook/.
bart-large-cnn.
it has been known that memory and compute com-plexity of transformers is quadratic with the se-quence length.
however, in encoder-decoder archi-tectures, the exact dependencies on input length n ,target length m , and batch size b are less under-stood.
this is particularly important in long-spanseq2seq tasks because large memory or computerequirement could make training impractical.
thus,this work studies these dependencies, and showsthe trade-off between the size of input span and thesize of attention span in local self-attention..4.1 memory analysis and lobart design.
firstly,through a regression analysis for anencoder-decoder architecture such as bart, the.
6028oraclepad-randmultitask cslocal self-attntransformersummaryreferencelen n0len nlen mdesign for large n (sec.4)train-time cs (sec.5.1)test-time cs (sec.5.2)combination (sec.6)traintestmemory required in training is:.
6n 2).
3n + cb.
2m + cb.
5m 2 + cb.
4m n + cb.
1 + b(cbcbthe term cb1 depends on only the model size andoptimizer, and it is constant (theoretical calculationprovided in appendix a).
the remaining termsare activation memory associated with the activa-tion outputs cached for backpropagation, and theygrow with n , m , and b. table 2 shows system-independent5 regression results for the memory intraining bart.
it is apparent that as n grows the6n 2, which is associated withdominant term is cbthe encoder self-attention.
thus, this motivates usto modify self-attention only on the encoder side..term cb1.
2m cbcb.
3n cb.
gib 6.05 0.23 0.84.
4m n cb0.21.
5m 20.02.
6n 2cb1.53.table 2: bart’s memory proﬁle (n =1024, m =144)..by introducing local self-attention of width w , thememory in training lobart becomes:.
1 + b(clcl.
2m + cl.
3n + cl.
4m n + cl.
5m 2 + cl.
6n w ).
6 ≈ 1.72cb.
for large n , the memory is now dominated bycl6n w .
the coefﬁcient cl6, suggestingthat w should be at most 0.58n to reduce memory.
we provide more details about the exact theoreticalcalculation for model and optimizer memory aswell as time complexity in appendix a..the memory for training bart/lobart in fig-ure 3 enables us to choose an operating point.
ad-ditionally, other complementary techniques for re-ducing memory in training include: (i) gradient-checkpoint where a subset of intermediate values inthe computation graph are cached, and the rest arere-computed during backpropagation (chen et al.,2016), but this requires changes to optimizationand leads to longer training time; (ii) half/mixed-precision training (micikevicius et al., 2018) thatwould almost halve y-axis in figure 3, but thisrequires changes to the model precision and mayresult in lower performance; (iii) model parallelismwith micro-batching (huang et al., 2019), but thismethod requires multiple accelerators..4.2 bart and lobart.
we study the characteristics of the full self-attention in bart by deﬁning the mean attention.
5system-independent across hardware and machines; al-beit implementation-dependent.
this analysis is based onwidely used pytorch and huggingface implementation..figure 3: operating points for b=1 and m =144.
(1)section 4 studies local attention to reduce quadraticcomplexity to linear.
as w decreases, the gradient oflinear complexity decreases.
(2) section 5 studies con-tent selection to move an operating point to the left..distance in a particular layer and head as follows:.
d =.
n(cid:88).
n(cid:88).
.
.
i=1.
j=1.
1n..
αi,j × |i − j|.
.
(1).
where αi,j is the attention weight of position iattending to position j ((cid:80)nj=1 αi,j = 1).
thismeasure corresponds to the average distance ofself-attention.
if the attention weight is uniform,du = n 2−13n .
for n = 1024, du = 341. infigure 4, our results show that most layers have ashorter mean distance than du , supporting that theinformation is more localized.
the mean distancesof differently initialized bart models computedon the podcast data also show that the attentionmechanism is learned during pre-training stage asthere is little variation after the pre-training stage.
as illustrated in figure 4, the average attention dis-tance d of the bart model is around 250-350tokens.
this suggests the window size w shouldbe designed to be above 700, allowing half localattention window w/2 be greater than 250-350to effectively match bart and to exploit transferlearning more efﬁciently..subsequently, we train different conﬁgurationsof bart/lobart models up to our gpu memorylimit of 32gib.
the results in table 3 show that:(i) expanding the model to accommodate longer in-put spans improve over the baseline bart(1k) asopposed to manakul and gales (2020) that trainedlonger-span models by freezing bottom layers anddid not show any improvement over their baseline;(ii) although lobart(8k) with w =512 can pro-cess longer input spans than lobart(4k) withw =1024, it performs worse and we suggest thatthis is because lobart(8k)’s window is too small,.
60290100020003000400050006000700080009000input length (n)010203040memory usage (gib)32gib16gib12gibbart (full attn)lobart (local attn, w=1024)lobart (local attn, w=512)lobart (local attn, w=256)lobart (local attn, w=128)5.1 training-time content selection.
during training, ground-truth targets are available.
we categorize selection methods in this phase intotwo types: ground-truth based (model-free), whichis also referred to as oracle; and model-based.
ground-truth based methods cannot be used attest time, while model-based methods can beapplied at both phases.
although model-basedmethods do not rely on ground-truth targets, theyhave the advantage of matching in training andtest phases.
existing oracle methods includeusing rouge-2 recall (liu et al., 2018) orthe average of rouge-1,2,l recall(pilaultet al., 2020).
we discuss model-based methodsin section 5.2, where we propose the mcsthe subscript (i, j) denote themethod.
position of the j-th word in the i-th input sentence,input x = {x1, ..., xi, ..., xn1} =the full[x1,1, x1,2, x1,j1].
, ..., xn1,1, xn1,jn1, ..., xi,1, xi,ji(cid:124)(cid:125)(cid:125)(cid:123)(cid:122)(cid:125)sent 1.
(cid:123)(cid:122)sent icontent selection re-ranks, truncates, and sorts xto get xcs for training bart/lobart as follows:.
(cid:123)(cid:122)sent n1.
let.
(cid:124).
(cid:124).
¯x = {xr1, xr2, xr3, ..., xrr}xcs = sortorig(truncaten( ¯x)).
(2).
(3).
where ri is the index of the sentence of rank i, thetruncaten operation ﬁlters ¯x such that the totalof number of words is less than n , and sortorigretains the original sentence order.
the followingranking methods are considered:.
• truncation (trc): rk = k..• model-based: given the score f of model φ,rk = {i ∈ n1 : fφ(i|x) is ranked k-th}.
• oracle (orc): given the ground-truth sum-.
mary y and similarity measure d,rk = {i ∈ n1 : d(xi, y) is ranked k-th}.
in this work, we use rouge-2 recall as the sim-ilarity measure d. for the orc method, ﬁrst, weretain only sentences with positive d, leading tor ≤ n1.
we found that the number of sentenceswith positive d is low at 21.3% of the total numberof sentences in average on podcast data.
this cor-responds to 56% of training instances being shorterthan bart input span of 1024.6 this no-paddingoracle method (orcno-pad) is highly aggressive,potentially preventing the downstream summarizer.
6we refer to this percentage as %agorcno-pad (the per-centage of inputs aggressively extracted by the oracle method)..figure 4: the average mean distance across multi-heads for each layer.
the average mean distance ofthe random weight model is slightly lower than du assome inputs are shorter than 1,024..e.g.
<700, to utilize transfer learning efﬁcientlyand its effective receptive ﬁeld is also smaller..system.
w gib.
r1.
r2.
rl.
bart(1k).
full.
8.9.
26.43.
9.22.
18.35.lobart(2k)lobart(2k)lobart(2k)lobart(2k)bart(2k).
lobart(4k)lobart(4k)lobart(4k)lobart(4k).
lobart(8k)lobart(8k)lobart(8k).
1282565121024full.
1282565121024.
128256512.
9.610.211.614.214.5.
12.814.116.722.0.
19.321.127.1.
25.8825.9326.3526.4426.63.
26.4226.6626.7527.02.
26.4526.7226.90.
8.898.808.989.269.41.
9.029.229.549.57.
9.049.309.47.
17.8717.8218.1918.2518.65.
18.1218.3318.5418.78.
18.2318.3618.50.table 3: bart & lobart memory requirement intraining and performance.
(nk) denotes maximum in-put length of n × 1024..5 longer span via content selection.
some input sequences still exceed lobart’slonger ﬁxed-span limit.
further extending theinput span would lead to a small local attentionspan, a diminishing improvement, or gpu runningout of memory.
alternatively, it has been shownthat a better content selection improves abstractivesummarization in news (chen and bansal, 2018;gehrmann et al., 2018; hsu et al., 2018), multi doc-uments (liu and lapata, 2019a; liu et al., 2018),and scientiﬁc articles (pilault et al., 2020).
thus,we propose to tackle the excess length by contentselection.
here, we distinguish between two phasesof content selection: training time and test time..60300246810layer id (bottom=0, top=11)150200250300350400450500average attention distanceaverage attention distance over all heads (mean±std)random weightsd_ubart-large (no-finetune)bart-cnnbart-podcastfrom learning complex abstraction.
hence, wepropose variants of oracle methods to extend theorcno-pad-selected input to the max input span n :.
• orcpad-lead: pad by leading unselected sen-tences and keep the original sentence order..• orcpad-rand: pad by random unselected sen-tences and keep the original sentence order..figure 5: the impact of training-time content selectionmethods on bart(1k) performance..in figure 5, since any oracle method is consid-ered cheating at test time, the best performanceis obtained by mcs (in blue), and the upperbound performance is obtained by optimal oraclemethod (in green).
the results show that althoughorcno-pad yields the highest upper bound, the ab-stractive model in fact does not learn how to per-form abstraction.
for instance, with trc or mcsat test time, orcno-pad yields the lowest perfor-mance level.
the best way to ﬁne-tune the abstrac-tive model shown in figure 5 is using orcpad-rand.
compared to orcpad-lead, orcpad-rand is better as itintroduces more diversity to the abstractive model.
compared to the model-based method, orcpad-randis also computationally less expensive..in addition, table 5 shows that when there isno content selection at test time (i.e.
trc ap-plied), lobart(4k) and lobart(8k) beneﬁt fromorcpad-rand, whereas bart(1k) does not.
this isbecause in the 1k setting, content selection is moreaggressive; as a result, the large mismatch betweentraining and test leads to a poor result.
thus, wesuggest that the best content selection during train-ing is orcpad-rand given that content selection willbe used at test time, or model’s input span is long..5.2 multitask content selection (mcs).
to process long input sequences entirely, we con-sider rnn, whose memory requirement grows lin-.
early with the sequence length, and hierarchicalarchitectures which have been shown effective forlong seq2seq tasks (cohan et al., 2018; li et al.,2019).
in this work, the hierarchical rnn modeldescribed in section 3.2 has memory requirementgiven the target length of 144 during training of0.83+b(3.96×10−5+3.33×10−5n2)n1,7 wheren1 is #sentences, and n2 is the maximum numberof words in a sentence, and b is batch size.
bysetting n1=1000 and n2=50, only 2% of podcastdata exceeds this limit, while taking gpu memoryto only 2.53gib for b=1.
thus, this shows thatthis model can cover long sequences..previous model-based methods treat content se-lection as extractive labelling and create labelsheuristically (pilault et al., 2020), or using encoder-decoder attention mechanism (manakul and gales,2020).
to utilize both of these in one framework,we propose a multitask content selection (mcs)method where we train the hierarchical encoder-decoder with attention mechanism and a classiﬁ-cation layer on top of the encoder (described insection 3.2).
first, the model is trained on seq2seqabstractive summarization objective:.
lseq2seq = −.
log p (ym|y<m, x).
(4).
m(cid:88).
m=1.
second, we create binary labels as follows: forsentence i, the label zi is 1 if d(xi, y) > 0; else ziis 0, and d is the rouge-2 recall measure.
theextractive labelling task objective is:llabel = − (cid:80)n1.
i=1 (zi log ˆzi + (1 − zi) log(1 − ˆzi)) (5).
ˆzi = sigmoid(wt.
clshi + bcls).
(6).
where hi is the sentence-level encoder output as-sociated with sentence i, and wcls, bcls are theparameters of the classiﬁcation layer.
thus, themcs training loss is deﬁned as follows:.
lmcs = γllabel + (1 − γ)lseq2seq.
(7).
at inference stage, there are two modes: (i) stan-dard abstractive summary generation, e.g.
via beamsearch decoding; (ii) ranking input sentences vialabelling score and seq2seq attention score.
thelatter is how we use mcs during inference.8 forsentence i, the scores are:scorei,(label) = ˆzi, scorei,(seq2seq) = (cid:80)m.m=1 αs.
m,i.
(8).
7obtained by least-squares regression with 20 samples.
8in practice, we run beam search decoding of width 4,.and we obtain the attention score from the top beam..6031trcmcsorc-pad-leadorc-pad-randorc-no-pad22.024.026.028.030.032.034.0rouge-1 (f1)27.8828.1429.9930.3932.3926.8227.2426.3427.2825.2626.4326.3224.7825.5422.71abstractive generation performance of downsteam barttesttime: oracle (upperbound)testtime: mcs (currentbest)testtime: truncate (baseline)where αsm,i is the sentence-level attention weightat decoder step m over input sentence i. sincethe scores are on different scales, rather than usingthe scores deﬁned in eq.
8, we simply rank thescores, and then normalize the score ranks into therange 0.0 to 1.0. let nscore denote the normalizedranking score, the mcs inference score is:.
fφ(i|x) = nscorei,(label) + nscorei,(seq2seq).
(9).
in our preliminary experiments, we vary theamount of selected sentences from the limit ofbart/lobart to a few sentences, and we foundthat more aggressive selection at test time degradesthe performance.
therefore, our mcs selects inputsentences up to the limit of bart/lobart..by setting γ=0.0, our method is comparable tothe attention-based method in manakul and gales(2020).
by setting γ=1.0, our method is similarto the extractive models in hsu et al.
(2018); pi-lault et al.
(2020).
in table 4, we show that whencoupled with bart, mcs yields better summariza-tion performance than both attn-only and ext-onlybaselines.
mcs also achieves higher recall rate ofsentences with d(xi, y) > 0 than the two baselines..system.
%recall.
r1.
r2.
rl.
attn (lseq2seq)ext (llabel).
38.8535.26.
26.9026.39.
9.708.90.
18.7818.03.mcs (lmcs).
40.50.
27.28.
9.82.
19.00.table 4: the impact of test-time content selection meth-ods on bart(1k) trained using orcpad-rand.
optimalγ=0.2 is tuned between 0.0-1.0 on the validation set..6 combined approach.
6.1 spotify podcast results.
in table 5, a performance gain is obtained in allsettings by adding mcs.
by comparing differentconﬁgurations with mcs, it can be seen that thegain from mcs in lobart(8k) system is the low-est.
this is because the average length is 5,727,meaning that many podcasts inputs to lobart(8k)do not beneﬁt from content selection..cued-ﬁlt, the best single-model system in man-akul and gales (2020), uses an attention-based con-tent selection at both training and test time, andit is combined with ﬁne-tuned vanilla bart.
ourapproach outperforms cued-ﬁlt by improved con-tent selection at both training time and test time as.
demonstrated by bart(1k)-orc+mcs.
addition-ally, local self-attention allows training on longersequences, and our lobart(4k)-orc+mcs sys-tem has yielded the best results.
lastly, eventhough lobart(8k) requires more resource totrain, it does not perform as well as lobart(4k)due to its smaller attention window, and it also hasa lower improvement when adding mcs..systemcued-ﬁlt∗.
bart(1k)bart(1k)bart(1k)bart(1k).
cs-trn cs-tst r1.
r2.
rl.
(cid:51).
(cid:51).
26.96 9.75 18.90.
(cid:55).
(cid:55)26.43 9.22 18.35(cid:55)mcs 26.82 9.39 18.57orc25.54 9.00 17.83orc mcs 27.28 9.82 19.00.
(cid:55).
27.02 9.57 18.78lobart(4k)mcs 27.53 9.95 19.08lobart(4k)(cid:55)lobart(4k) orc27.36 10.04 19.33lobart(4k) orc mcs 27.81 10.30 19.61.
26.90 9.47 18.50lobart(8k)mcs 27.02 9.52 18.62lobart(8k)(cid:55)lobart(8k) orc27.16 9.84 19.08lobart(8k) orc mcs 27.49 9.98 19.25.
(cid:55)(cid:55).
(cid:55)(cid:55).
(cid:55).
(cid:55).
table 5: podcast results.
the impact of training-timeorcpad-rand and test-time mcs.
∗cued systems werethe top systems by human evaluation at spotify chal-lenge 2020; cued systems use bart with a model-based (trained on lseq2seq) content selection in bothtraining and test stages..6.2 arxiv and pubmed results.
to verify the effectiveness of our systems, were-train bart(1k) and lobart(4k) on arxivand pubmed datasets.
our training is differentfrom ext+tlm (pilault et al., 2020) where theirabstractive models are trained using inputs ex-tracted from top two sentences in rouge recallfor each target sentence without padding, similarto orcno-pad.
although in 1k setting, orcno-padyields %agorcno-pad (deﬁned in section 5.1) ofonly 2.8% on arxiv (12% on pubmed), in 4k set-ting this is 39% on arxiv (71% on pubmed).
basedon the best conﬁgurations on podcast data, wetrain bart(1k) and lobart(4k) using trc ororcpad-rand content selection, and we train the hi-erarchical model on arxiv/pubmed for mcs..arxiv.
in table 6, both bart(1k)+mcs andlobart(4k)+mcs outperform all existing sys-tems.
to better understand the advantages of ourapproach, the following systems are compared:.
6032type system.
krow.suoiverp.abs discourse-aware (cohan et al., 2018)ext+tlm (pilault et al., 2020)mixextsum-lg+rd(xiao and carenini, 2020)extabspegasus (zhang et al., 2020)abs dancer (gidiotis and tsoumakas, 2020)abs bigbird(3k) (zaheer et al., 2020)absabsmix ctrlsum(bart+bert) (he et al., 2020).
led(4k) (beltagy et al., 2020)led(16k) (beltagy et al., 2020).
row.k absmixabsmix.
siht.†bart(1k)‡bart(1k)+mcs‡lobart(4k)‡lobart(4k)+mcs.
arxivr2.
11.0514.6917.7916.9517.6019.0217.9419.6218.02.
17.2519.7718.7220.55.rl.
r1.
pubmedr2.
31.8038.0339.0938.8340.5641.7739.7641.8342.14.
39.7642.2541.2443.31.
38.9342.1345.3045.9746.3446.32---.
45.0646.4947.4748.06.
15.3716.2720.4220.1519.9720.65---.
18.2719.4520.4720.96.rl.
35.2139.2140.9541.3442.4242.33---.
40.8442.0443.0243.56.r1.
35.8041.6244.0144.2145.0146.6344.4046.6346.91.
44.9647.6846.5948.79.table 6: results on arxiv and pubmed.
†denotes trc applied, and ‡denotes orcpad-rand applied at training time..ctrlsum versus our bart(1k) baseline; ledand bigbird versus our lobart(4k) system..ctrlsum extends bart by conditioning it withextracted keywords v using a bert-based model,e.g.
p(y|x, v).
their bert-based model usessliding window allowing it to extract v in longsequences, but their bart is still limited to theﬁrst 1,024 tokens.
as a result, it performs betterthan bart(1k), but worse than bart(1k)+mcs.
lobart(4k) has a similar architecture toled(4k) without the global attention pattern forspecial tokens.
instead, our lobart(4k) beneﬁtsfrom knowledge transferred from cnndm and theorcpad-rand training-time content selection, whichyields a larger gain when mcs is applied, i.e.
thesystem trained with truncated data has a smallergain when mcs is applied.
transfer learning com-parison and additional results on the impact oforcpad-rand are provided in appendix c..compared to bigbird, lobart(4k) has a longerinput span, e.g.
3,072 vs. 4,096. however, bigbirdbeneﬁts from utilizing more recent summarizationspeciﬁc pre-training pegasus (zhang et al., 2020)which is better than our transfer learning.
bigbirdincorporates a global attention pattern similar toled, and it also has a random attention pattern.
hence, lobart without mcs performs worse..ultimately, we show that adding mcs to eitherbart(1k) or lobart(4k) yields a signiﬁcant im-provement, resulting in state-of-the-art results inboth settings.
moreover, although the gain fromadding mcs is comparable to the gain observedin extending led(4k) to led(16k), the contentselection method adds less training cost..pubmed.
similarly, lobart(4k)+mcs achievesstate-of-the-art results shown in table 6. in con-trast to the arxiv results, bart(1k)+mcs does notoutperform lobart(4k) nor bigbird, and the gainfrom mcs is not as high in both 1k and 4k settings..6.3 local attention v.s.
mcs..local attention yields better performance onpubmed, while mcs yields better performanceon arxiv.
to understand this discrepancy, a ﬁne-grained analysis is conducted..(a) arxiv (len:avg=8,584, 90th%=16,108).
(b) pubmed (len:avg=3,865, 90th%=7,234).
figure 6: rouge-1 score relative to that of bart(1k)system evaluated on different partitions by length..60330200040006000800010000120001400016000average input length in each partition-1.00.01.02.03.04.05.0improvement in rouge-1bart(1k)bart(1k)+mcslobart(4k)lobart(4k)+mcs0100020003000400050006000700080009000average input length in each partition0.01.02.03.04.0improvement in rouge-1bart(1k)bart(1k)+mcslobart(4k)lobart(4k)+mcsin figure 6, we partition the test sets by inputlengths, and we evaluate the performance improve-ment in each partition with respect to the bart(1k)baseline.9 the results illustrate that as the inputlength n increases:.
• the improvement of systems with mcs in-.
creases and subsequently plateaus out..• the improvement of systems without mcsdecreases once the input exceeds the lengthlimit but then plateaus, suggesting that ﬁxed-span systems without content selection per-form worse once the maximum ﬁxed-spanis reached.
for instance, below 4,000 inputwords, lobart(4k) without mcs performsbetter than bart(1k)+mcs on both datasets..therefore, our mcs method is more effective onarxiv compared to pubmed because the averagelength of pubmed documents is more than twiceshorter than the average length of arxiv documents..7 conclusion.
we study two methods for long-span summariza-tion tasks.
first, on local self-attention transform-ers, we present the design considerations for localself-attention bart, and we investigate the feasibil-ity and performance of different network conﬁgura-tions.
second, on content selection, we distinguishbetween training time and test time methods, andwe provide a good practice for both phases.
attraining time, we show that the oracle method withrandom sentences padded (orcpad-rand) yields thebest results.
at test time, we propose multitaskcontent selection (mcs) that shows an improve-ment over baselines.
we demonstrate that contentselection is essential, in particular for longer docu-ments such as the articles in the arxiv dataset.
ourbart(1k)+mcs outperforms the current best sys-tems on podcast and arxiv datasets, and this systemdoes not require a large-scale accelerator in train-ing.
ultimately, by combining local self-attentiontechnique with mcs, our lobart(4k)+mcs sys-tem has set new state-of-the-art results in terms ofrouge scores in all three long-span summariza-tion tasks.
future work will focus on training ourlobart+mcs system in an end-to-end fashion..9for arxiv/pubmed, each test set consists of over 6,000instances, while podcast test set has only 1,027 instances.
thesame analysis is conducted on podcast, but the results arenoisy due to the smaller size of its test set (see appendix c)..acknowledgments.
this paper reports on research supported by altainstitute, cambridge assessment english, univer-sity of cambridge, and cambridge international &st john’s college scholarship.
thanks to yitinglu, qingyun dou, xixin wu, raf czlonka, andkate knill for interesting discussions and comput-ing resource support.
thanks to the anonymousreviewers for their helpful comments..references.
joshua ainslie, santiago ontanon, chris alberti, va-clav cvicek, zachary fisher, philip pham, anirudhravula, sumit sanghai, qifan wang, and li yang.
2020. etc: encoding long and structured inputsin proceedings of the 2020 con-in transformers.
ference on empirical methods in natural languageprocessing (emnlp), pages 268–284, online.
asso-ciation for computational linguistics..iz beltagy, matthew e peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv preprint arxiv:2004.05150..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers.
in advances in neural information processingsystems 33: annual conference on neural informa-tion processing systems 2020, neurips 2020, de-cember 6-12, 2020, virtual..tianqi chen, bing xu, chiyuan zhang, and carlosguestrin.
2016. training deep nets with sublinearmemory cost.
arxiv preprint arxiv:1604.06174..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
in proceedings of the 56th annual meet-ing of the association for computational linguis-tics (volume 1: long papers), pages 675–686, mel-bourne, australia.
association for computationallinguistics..jianpeng cheng and mirella lapata.
2016. neural sum-marization by extracting sentences and words.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 484–494, berlin, germany.
as-sociation for computational linguistics..6034rewon child, scott gray, alec radford,.
andgenerating long se-ilya sutskever.
2019.quences with sparse transformers.
arxiv preprintarxiv:1904.10509..the north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4830–4842, online.
association forcomputational linguistics..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734, doha, qatar.
association for computationallinguistics..krzysztof marcin choromanski, valerii likhosherstov,david dohan, xingyou song, andreea gane, tamassarlos, peter hawkins, jared quincy davis, afrozmohiuddin, lukasz kaiser, david benjamin be-langer, lucy j colwell, and adrian weller.
2021.in interna-rethinking attention with performers.
tional conference on learning representations..ann clifton, sravana reddy, yongze yu, aasish pappu,rezvaneh rezapour, hamed bonab, maria eske-vich, gareth jones, jussi karlgren, ben carterette,and rosie jones.
2020.
100,000 podcasts: a spo-in proceedingsken english document corpus.
of the 28th international conference on compu-tational linguistics, pages 5903–5917, barcelona,spain (online).
international committee on compu-tational linguistics..arman cohan, franck dernoncourt, doo soon kim,trung bui, seokhwan kim, walter chang, and na-zli goharian.
2018. a discourse-aware attentionmodel for abstractive summarization of long docu-in proceedings of the 2018 conference ofments.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 615–621,new orleans, louisiana.
association for computa-tional linguistics..zihang dai, zhilin yang, yiming yang, jaime car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988, florence, italy.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..zi-yi dou, pengfei liu, hiroaki hayashi, zhengbaojiang, and graham neubig.
2021. gsum: a gen-eral framework for guided neural abstractive summa-rization.
in proceedings of the 2021 conference of.
sebastian gehrmann, yuntian deng, and alexanderrush.
2018. bottom-up abstractive summarization.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 4098–4109, brussels, belgium.
associationfor computational linguistics..alexios gidiotis and grigorios tsoumakas.
2020. adivide-and-conquer approach to the summarizationof long documents.
ieee/acm transactions on au-dio, speech, and language processing, 28:3029–3040..junxian he, wojciech kry´sci´nski, bryan mccann,nazneen rajani, and caiming xiong.
2020. ctrl-sum: towards generic controllable text summariza-tion.
arxiv preprint arxiv:2012.04281..karl moritz hermann, tom´as kocisk´y, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines toread and comprehend.
in advances in neural infor-mation processing systems 28: annual conferenceon neural information processing systems 2015,december 7-12, 2015, montreal, quebec, canada,pages 1693–1701..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..wan-ting hsu, chieh-kai lin, ming-ying lee, keruimin, jing tang, and min sun.
2018. a uniﬁedmodel for extractive and abstractive summarizationin proceedings of theusing inconsistency loss.
56th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 132–141, melbourne, australia.
associationfor computational linguistics..yanping huang, youlong cheng, ankur bapna, orhanfirat, dehao chen, mia xu chen, hyoukjoonglee, jiquan ngiam, quoc v. le, yonghui wu, andzhifeng chen.
2019. gpipe: efﬁcient training of gi-ant neural networks using pipeline parallelism.
inadvances in neural information processing systems32: annual conference on neural information pro-cessing systems 2019, neurips 2019, december 8-14, 2019, vancouver, bc, canada, pages 103–112..rosie jones, ben carterette, ann clifton, maria es-kevich, gareth j. f. jones, jussi karlgren, aasishpappu, sravana reddy, and yongze yu.
2020. trec2020 podcasts track overview.
in the 29th text re-trieval conference (trec) notebook..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,.
6035iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in 8thinternational conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..logan lebanoff, franck dernoncourt, doo soon kim,walter chang, and fei liu.
2020. a cascade ap-proach to neural abstractive summarization with con-tent selection and fusion.
in proceedings of the 1stconference of the asia-paciﬁc chapter of the associ-ation for computational linguistics and the 10th in-ternational joint conference on natural languageprocessing, pages 529–535, suzhou, china.
associ-ation for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..manling li, lingyu zhang, heng ji, and richard j.radke.
2019. keep meeting summaries on topic:abstractive multi-modal meeting summarization.
inproceedings oftheassociation for computational linguistics, pages2190–2196, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..peter j. liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser, and noamshazeer.
2018. generating wikipedia by summariz-ing long sequences.
in 6th international conferenceon learning representations, iclr 2018, vancou-ver, bc, canada, april 30 - may 3, 2018, confer-ence track proceedings.
openreview.net..yang liu and mirella lapata.
2019a.
hierarchicaltransformers for multi-document summarization.
inproceedings oftheassociation for computational linguistics, pages5070–5081, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
yang liu and mirella lapata.
2019b.
text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..potsawee manakul.
2020.cued speech at trec 2020 podcastsum-marisation track.
arxiv preprint arxiv:2012.02535..and mark gales..potsawee manakul, mark j.f.
gales, and linlin wang.
2020. abstractive spoken document summariza-tion using hierarchical model with multi-stage at-tention diversity optimization.
in proc.
interspeech2020, pages 4248–4252..paul michel, omer levy, and graham neubig.
2019.in ad-are sixteen heads really better than one?
vances in neural information processing systems32: annual conference on neural information pro-cessing systems 2019, neurips 2019, december 8-14, 2019, vancouver, bc, canada, pages 14014–14024..paulius micikevicius, sharan narang, jonah alben,gregory f. diamos, erich elsen, david garc´ıa,boris ginsburg, michael houston, oleksii kuchaiev,ganesh venkatesh, and hao wu.
2018. mixed pre-cision training.
in 6th international conference onlearning representations, iclr 2018, vancouver,bc, canada, april 30 - may 3, 2018, conferencetrack proceedings.
openreview.net..ramesh nallapati, bowen zhou, cicero dos santos,c¸ a˘glar gulc¸ehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290, berlin, germany.
association for computational linguistics..shashi narayan, joshua maynez, jakub adamek,daniele pighin, blaz bratanic, and ryan mcdon-ald.
2020. stepwise extractive summarization andplanning with structured transformers.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4143–4159, online.
association for computationallinguistics..image transformer..niki parmar, ashish vaswani, jakob uszkoreit, lukaszkaiser, noam shazeer, alexander ku, and dustinin proceedingstran.
2018.of the 35th international conference on machinelearning, icml 2018, stockholmsm¨assan, stock-holm, sweden, july 10-15, 2018, volume 80 ofproceedings of machine learning research, pages4052–4061.
pmlr..jonathan pilault, raymond li, sandeep subramanian,and chris pal.
2020. on extractive and abstractiveneural document summarization with transformerlanguage models.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 9308–9319, online.
as-sociation for computational linguistics..jiezhong qiu, hao ma, omer levy, wen-tau yih,sinong wang, and jie tang.
2020. blockwise self-attention for long document understanding.
in find-ings of the association for computational linguis-.
6036tics: emnlp 2020, pages 2555–2565, online.
as-sociation for computational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
journal of machine learning research..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..kaiqiang song, chen li, xiaoyang wang, dong yu,and fei liu.
2020. automatic summarization ofarxiv preprintopen-domain podcast episodes.
arxiv:2011.04132..yi tay, dara bahri, liu yang, donald metzler, andda-cheng juan.
2020a.
sparse sinkhorn attention.
in proceedings of the 37th international conferenceon machine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 9438–9447.
pmlr..yi tay, mostafa dehghani, samira abnar, yikangshen, dara bahri, philip pham, jinfeng rao, liuyang, sebastian ruder, and donald metzler.
2021.long range arena : a benchmark for efﬁcient trans-in international conference on learningformers.
representations..yi tay, mostafa dehghani, dara bahri, and donaldmetzler.
2020b.
efﬁcient transformers: a survey.
arxiv preprint arxiv:2009.06732..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th annual meeting of the association for com-putational linguistics, pages 5797–5808, florence,italy.
association for computational linguistics..sinong wang, belinda li, madian khabsa, hanself-arxiv preprint.
fang, and hao ma.
2020.attention with linear complexity.
arxiv:2006.04768..linformer:.
thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..wen xiao and giuseppe carenini.
2019. extractivesummarization of long documents by combiningin proceedings of theglobal and local context.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3011–3021, hong kong,china.
association for computational linguistics..wen xiao and giuseppe carenini.
2020. systemati-cally exploring redundancy reduction in summariz-ing long documents.
in proceedings of the 1st con-ference of the asia-paciﬁc chapter of the associa-tion for computational linguistics and the 10th in-ternational joint conference on natural languageprocessing, pages 516–528, suzhou, china.
associ-ation for computational linguistics..manzil zaheer, guru guruganesh, kumar avinavadubey, joshua ainslie, chris alberti, santiago on-tanon, philip pham, anirudh ravula, qifan wang,li yang, et al.
2020. big bird: transformers forlonger sequences.
advances in neural informationprocessing systems, 33..jingqing zhang, yao zhao, mohammad saleh, and pe-ter liu.
2020. pegasus: pre-training with extractedgap-sentences for abstractive summarization.
in in-ternational conference on machine learning, pages11328–11339.
pmlr..xingxing zhang, furu wei, and ming zhou.
2019. hi-bert: document level pre-training of hierarchicalbidirectional transformers for document summariza-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 5059–5069, florence, italy.
association forcomputational linguistics..zhou zhao, haojie pan, changjie fan, yan liu, linlinli, and min yang.
2019. abstractive meeting sum-marization via hierarchical adaptive segmental net-work learning.
in the world wide web conference,www 2019, san francisco, ca, usa, may 13-17,2019, pages 3455–3461.
acm..6037a detailed memory & time analysis.
a.2 lobart memory.
our memory analysis is system-independent, al-beit implementation-dependent.
we carry out theexperiments using pytorch version 1.2.0. we usepytorch_memlab10 to compute gpu memoryduring forward and backward passes.
our nota-input length n , target length m , localtion is:self-attention width w , and batch size b..a.1 bart memory.
we collect 30 samples, spanning n ∈ [64, 3000]and m ∈ [36, 576] using batch size of 1. ourleast-squared regression of the memory equationmemory = cb5m 2+2m +cb6n 2) yields r2 = 1, rmse = 0.026, and thecb2 = 1.594 × 10−3,1 = 6.054, cbcoefﬁcients are: cbcb3 = 8.192 × 10−4, cb5 =1.077 × 10−6, cb.
4 = 1.418 × 10−6, cb.
6 = 1.456 × 10−6..4m n +cb.
1+b(cb.
3n +cb.
model and optimizerthe constant term cb1 = 6.054 gib is independentof batch size, system, or implementation (giventhe same ﬂoating-point precision).
this term com-prises model and optimizer memory as follows (in32-bit ﬂoating point, 1 variable takes 4 bytes):.
1. model parameter: bart has 406,290,432 pa-rameters, yielding 406290432 × 4 = 1.625 ×109bytes = 1.51 gib..2. model gradient: each parameter has one cor-responding gradient variable, e.g.
.grad inpytorch.
thus, this also occupies 1.51 gib..3. optimizer: adam optimizer (kingma and ba,2015) stores ﬁrst moment and second momentfor each and every model parameters, hence,taking 3.02 gib..2, ..., cb.
activationthe terms corresponding to cb6 are associatedwith activation buffers cached for computing gradi-ents in backpropagation.
these terms grow linearly6n 2b growswith batch size.
the dominant term cbquadratically with the input length n , motivatingencoder’s local self-attention design..chen et al.
(2016) proposes a method to savethe activation memory by only caching buffers ofa subset of layers, and re-computing the rest dy-namically during backpropagation.
this results inrepeated computations and more training time..10https://github.com/stonesjtu/.
pytorch_memlab.
2m + cl.
1 + b(cl.
1 = 6.104, cl.
3 = 1.032 × 10−3, cl.
we collect 36 samples, spanning n ∈ [512, 4096],m ∈ [100, 400], and w ∈ [32, 512] using batchsize of 1. our least-squared regression of the mem-ory equation memory = cl3n +cl5m 2 + cl4m n + cl6n w ) yields rmse = 0.010,and the coefﬁcients are: cl2 = 1.443 ×10−3, cl4 = 1.487 × 10−6,6 = 2.503×10−6.
the model5 = 1.277×10−6, clcland optimizer memory is similar to the analysis forbart.
the activation memory is now dominatedby cl6. thus, we high-light that once w > 0.58n , lobart no longerreduces memory.
note that we also tried incorpo-rating the terms n 2 and w in the least-squaredregression analysis, but their resulting coefﬁcientsare small, making both terms negligible.
this isexpected as quadratic self-attention is replaced bylocal attention of width w , and the width w onlydetermines the receptive ﬁeld of each and everyposition in n , resulting in the n w term..6n w × b, where cl.
6 = 1.72cb.
a.3 time: bart & lobart.
unlike memory, time requirement is both systemand implementation dependent.
in this analysis, weshow the results on our infrastructure consisting ofa 32 gib v100 gpu and 32-core intel xeon 4215rcpu (3.20ghz).
we compute the time requiredfor 50 forward and backward passes in 12 settingsfor each model conﬁguration.
similar to the mem-ory analysis, we perform least-squared regressionwhere the results are shown in figure 7. it canbe seen that although lobart reduces memoryrequirement, when it comes to time requirement,lobart is only comparable to bart.
this is dueto the implementation of local self-attention thatinvolves more processes such as chunking..figure 7: quadratic time complexity of bart and lin-ear time complexity of lobart for m =144 and b=1..6038100020003000400050006000700080009000input length (n)0.00.51.01.52.02.53.0second/batchbart (full attn)lobart (local attn, w=1024)lobart (local attn, w=512)lobart (local attn, w=256)lobart (local attn, w=128)b implementation details.
b.1 models.
bart & lobart.
we use publicly released bart-large.11 for lo-bart, our local self-attention is based on hugging-face’s implementation (wolf et al., 2020).12 thenumber of parameters in bart is 406m..the positional embedding of lobart beyond1,024 is created by copying bart’s positional em-bedding with ﬂipping to allow a smoother transitionas shown in figure 8, and the number of parametersin lobart(nk) is 406m + 50,264×(n-1)×1,024..figure 8: lobart positional embedding is initializedby copying and ﬂipping bart’s positional embedding..hierarchical rnn.
the encoder consists of word-level and sentence-level bidirectional grus.
the word-level grutakes embedding vector ei,j of word i in sentence j,and outputs forward representation h(f)i,j and back-ward representation h(b)i,j .
the sentence-level grutakes concatenated vector [h(f)1,j ], and outputssentence representation hj.
the decoder consistsof a unidirectional gru.
each of the encoder grushas 2 layers with a dropout layer (p=0.1), and thedecoder gru has 1 layer.
there are word-leveland sentence-level attention mechanisms connect-ing the encoder and decoder.
the classiﬁcationhead is a single-layer feedforward layer.
the di-mension of embedding space is 256, and the hiddensize is 512. the number of parameters is 52m..nj ,j;h(b).
b.2 training & inference hyperparameters.
we process data using the same byte-pair-encodingtokenizer as the bart-large tokenizer, and we usenltk tokenizer for sentence splitting.
we use 32-bit precision training.
we stop training when theloss on the validation set stop improving for 3 times.
for example, the training steps are approximately:.
11https://huggingface.co/facebook/.
bart-large-cnn.
180k for podcast; 240k for arxiv; 160k for pubmed.
we report the validation performance when trainingis stopped in table 10. adam optimizer is used forall experiments with learning rate:.
lr = 0.002 × min(step−0.5, step × warmup−1.5).
parameter.
podcast.
arxiv/pubmed.
max.
tgt len mdropoutbatch sizegradient accum.
warmupvalid step.
1440.11210,00020,000.
4000.11220,00020,000.losscompute (bart)compute (lobart).
cross entropy1×gtx titan x (12gib)1×v100 (32gib).
table 7: bart/lobart training hyperparameters..parameter.
podcast.
arxiv/pubmed.
6401204000.12120,00020,000.max.
src #sentmax.
src #words-in-sentmax.
tgt len mdropoutbatch sizegradient accum.
warmupvalid steploss∗compute.
1000501440.12120,00020,000.lseq2seq & lext1×gtx titan x (12gib).
table 8: rnn training hyperparameters.
∗both lossfunctions are cross entropy based..parameter.
beam widthlength penaltymin lengthmax length∗no repeat trigram size.
value.
42.056144 & 4003.table 9: inference hyperparameters.
∗144 for podcast,and 400 for arxiv/pubmed..b.3 evaluation.
our rouge (lin, 2004) scoring tool is pyrouge,which is a wrapper for perl script.13.
12https://huggingface.co/transformers/.
13https://pypi.org/project/pyrouge/.
603901000200030004000positionpostional embedding valueoriginalfilppedsystem.
attn-width.
cs-train.
podcast.
arxiv pubmed.
bart(1k)lobart(4k)∗lobart(4k).
full10241024.truncatetruncateorcpad-rand.
2.7672.6802.647.
2.1791.8781.721.
1.8671.5301.474.table 10: performance measured by the average cross-entropy on validation set.
∗best system on the test set..system.
cs-train cs-test.
bart(1k)bart(1k)bart(1k)bart(1k).
lobart(4k)lobart(4k)lobart(4k)lobart(4k).
(cid:55)(cid:55)orcorc.
(cid:55)(cid:55)orcorc.
(cid:55)mcs(cid:55)mcs.
(cid:55)mcs(cid:55)mcs.
arxivr2.
17.2518.7915.6219.77.
18.8820.1118.7220.55.rl.
r1.
pubmedr2.
39.7640.8337.1542.25.
41.5042.5841.2443.31.
45.0646.4643.2046.49.
47.4047.7647.4748.06.
18.2719.5417.0219.45.
20.4320.7620.4720.96.rl.
40.8441.9139.1942.04.
42.9543.2743.0243.56.r1.
44.9646.1142.0347.68.
46.9048.0546.5948.79.system.
initialization.
r1.
r2.
rl.
bart(1k).
14.61randombart-large25.82bart-large-cnndm 26.43.
0.829.079.22.
11.5417.9918.35.system.
led(4k).
cs-train.
truncate.
initialization∗bart-large.
r1.
r2.
rl.
44.40.
17.94.
39.76.lobart(4k).
bart-large46.17truncatebart-large-cnndm 46.90truncate45.25orcpad-rand bart-largeorcpad-rand bart-large-cnndm 46.59.
17.9618.8817.4018.72.
40.7441.5039.9641.24.table 11: extended results on arxiv and pubmed (in table 6).
orc is orcpad-rand training-time content selection..table 12: podcast results.
the impact of transfer learning.
truncation is applied at both training and test stages..table 13: arxiv results.
the impact of transfer learning on initializing lobart.
at test time, there is no contentselection.
∗to our understanding, led-large was initialized from bart-large as described in beltagy et al.
(2020)..c additional results.
losses on validation setsin table 10, we show the standard cross entropylosses on validation sets of our bart/lobart..bart and lobart on arxiv/pubmedin table 11, we provide conﬁgurations in additionto table 6. these results (as well as podcast resultsin table 5) show that: in all settings, applying mcsat test time yields a performance gain; and withorc applied at training, a larger gain is observed..transfer learning from cnn/dailymailin table 12, we show the impact of transfer learn-ing on ﬁne-tuning bart to podcast.
in table 13,led(4k) should be very close to lobart(4k)-trc-bart-large, we believe that the performancedifference is due to the stochastic nature of training..nevertheless, our experiments are carried out us-ing the same training setting, e.g.
hyperparameters,optimizer, etc.
thus, based on the results, we be-lieve that there is an observable improvement dueto transfer learning from cnndm..fine-grained analysis on podcast test set.
figure 9: rouge-1 score relative to that of bart(1k)on spotify podcast (len:avg=5,727, 90th%=11,677)..6040reference summary: this week, irwin and i discuss the iconic designs of the rolex submariner and theporsche 911. remaining subjectively unchanged through the years.
we talk about the subtle changes overthe years for this special car and watch and what are the similarities from their history.
if you love carsand watches you’ll enjoy the show.
grail watch and grail porsche: irwin: 1959 rolex submariner 5512 x1967 porsche 911s anthony: 2020 moser streamliner chronograph x 1988 porsche 911 959..lobart(4k)+mcs: in this episode we talk to @grandobsessionsirwin about his passion for porscheand vintage rolex.
we talk about the submariner and 911 submariners, the design evolution over thedecades and the future of porsche in the world of watchmaking.
we also chat about the evolution of therolex submariner from the 1950s to the present day..table 14: spotify podcast test set sample #31..reference summary: we present data from our investigation of the anomalous orange - colored afterglowthat was seen in the gammev chameleon afterglow search ( chase ) .
these data includes information aboutthe broad band color of the observed glow , the relationship between the glow and the temperature ofthe apparatus , and other data taken prior to and during the science operations of chase .
while differingin several details , the generic properties of the afterglow from chase are similar to luminescence seenin some vacuum compounds .
contamination from this , or similar , luminescent signatures will likelyimpact the design of implementation of future experiments involving single photon detectors and highintensity light sources in a cryogenic environment ..lobart(4k)+mcs: the gammev chameleon afterglow search ( chase ) experiment at the fermilabtevatron reported the discovery of an anomalous afterglows in its apparatus after shining a high - powerpulsed laser into the bore of a cryogenic vacuum chamber immersed in a magnetic ﬁeld .
we present all ofour data that pertains materially to the characterization of the “ orange glow “ signal .
we do not claim anyspeciﬁc explanation of the source or cause of the orange glow , though the dependence upon temperaturesuggests strongly that the effect is due to some chemical or material property that is excited by the inputlaser .
the data and discussion presented here may be useful for the design of future experiments that usehigh intensity light sources in conjunction with single photon detectors in cryogenic environments ..table 15: arxiv test set sample #315..reference summary: the survey of how canadian intensive care units ( icus ) prevent and diagnosevenous thromboembolism ( vte ) presented in this issue of critical care illustrates considerable variability .
lack of optimal patient care reﬂects how vte is rated in icus .
the discussion should no longer focus on theincidence of thrombosis , but rather on its prevention .
unfractionated heparin remains the most commonlyused agent to prevent vte , despite the recognized efﬁcacy and safety of low - molecular - weight heparins( lmwhs ) in the icu setting .
in addition , too few icu directors consider the use of mechanical prophylacticmeasures , such as graded elastic stockings and venous foot pump .
the present situation calls for largerandomized controlled trials in either medical or surgical icu patients , and for new education programmesin order to modify the care of icu patients with regard to vte ..lobart(4k)+mcs: deep vein thrombosis ( dvt ) remains an underestimated problem in intensive careunit ( icu ) patients , despite the ﬁndings of many randomized controlled trials performed in the ﬁeld ofdvt prophylaxis after surgery during the past few decades .
the canadian survey reported in the presentissue of critical care provides a useful snapshot of daily clinical practice in canada with regard to dvtprevention in icu patients .
it strongly suggests that studies dedicated to this topic should be performedin order to develop useful recommendations .
furthermore , a great effort should be made to educatephysicians regarding both dvt screening and pharmacological aspects ..table 16: pubmed test set sample #3150..6041