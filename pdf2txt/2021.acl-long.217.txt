text2event: controllable sequence-to-structure generationfor end-to-end event extractionyaojie lu1,3, hongyu lin1, jin xu4,∗, xianpei han1,2,∗, jialong tang1,3,annan li1,3, le sun1,2, meng liao4, shaoyi chen41chinese information processing laboratory 2state key laboratory of computer scienceinstitute of software, chinese academy of sciences, beijing, china3university of chinese academy of sciences, beijing, china4data quality team, wechat, tencent inc., china{yaojie2017,jialong2019,liannan2019}@iscas.ac.cn{hongyu,xianpei,sunle}@iscas.ac.cn{jinxxu,maricoliao,shaoyichen}@tencent.com.
abstract.
event extraction is challenging due to the com-plex structure of event records and the semanticgap between text and event.
traditional meth-ods usually extract event records by decompos-ing the complex structure prediction task intomultiple subtasks.
in this paper, we proposetext2event, a sequence-to-structure gener-ation paradigm that can directly extract eventsfrom the text in an end-to-end manner.
specif-ically, we design a sequence-to-structure net-work for uniﬁed event extraction, a constraineddecoding algorithm for event knowledge injec-tion during inference, and a curriculum learningalgorithm for efﬁcient model learning.
exper-imental results show that, by uniformly mod-eling all tasks in a single model and univer-sally predicting different labels, our methodcan achieve competitive performance usingonly record-level annotations in both super-vised learning and transfer learning settings..1.introduction.
event extraction is an essential task for naturallanguage understanding, aiming to transform thetext into event records (doddington et al., 2004;ahn, 2006).
for example, in figure 1, mapping“the man returned to los angeles from mexicofollowing his capture tuesday by bounty hunters.”into two event records {type: transport, trigger:returned, arg1 role: artifact, arg1: the man,arg2 role: destination, arg2: los angeles, ...} and {type: arrest-jail, trigger: capture, arg1role: person, arg1: the man, arg2 role: agent,arg2: bounty hunters, ... }..event extraction is challenging due to the com-plex structure of event records and the semanticgap between text and event.
first, an event recordcontains event type, trigger, and arguments, which.
∗corresponding authors..figure 1: the framework of text2event.
here,text2event takes raw text as input and generatesa transport event and an arrest-jail event..form a table-like structure.
and different eventtypes have different structures.
for example, infigure 1, transport and arrest-jail have entirelydifferent structures.
second, an event can be ex-pressed using very different utterances, such asdiversiﬁed trigger words and heterogeneous syntac-tic structures.
for example, both “the dismission ofthe man” and “the man departed his job” expressthe same event record {type: end-position, arg1role: person, arg1: the man}..currently, most event extraction methods em-ploy the decomposition strategy (chen et al., 2015;nguyen and nguyen, 2019; wadden et al., 2019;zhang et al., 2019b; du and cardie, 2020; li et al.,2020; paolini et al., 2021), i.e., decomposing theprediction of complex event structures into mul-tiple separated subtasks (mostly including entityrecognition, trigger detection, argument classiﬁca-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2795–2806august1–6,2021.©2021associationforcomputationallinguistics2795the manreturned to los angeles from mexico following his capture tuesday by bounty hunters.controllablegenerationevent schemaarrest-jail•person•crime•agent•timetransport•destination •origin•artifact•vehicle•timeevent typearrest-jailtriggercapturepersonthe mantimetuesdayagentbounty huntersevent typetransporttriggerreturnedartifactthe mandestinationlos angelesoriginmexicosequence-to-structurenetworkconstrainttion), and then compose the components of differ-ent subtasks for predicting the whole event struc-ture (e.g., pipeline modeling, joint modeling orjoint inference).
the main drawbacks of thesedecomposition-based methods are: (1) they needmassive and ﬁne-grained annotations for differentsubtasks, often resulting in the data inefﬁciencyproblem.
for example, they need different ﬁne-grained annotations for transport trigger detection,for person entity recognition, for transport.artifactargument classiﬁcation, etc.
(2) it is very challeng-ing to design the optimal composition architectureof different subtasks manually.
for instance, thepipeline models often lead to error propagation.
and the joint models need to heuristically predeﬁnethe information sharing and decision dependencebetween trigger detection, argument classiﬁcation,and entity recognition, often resulting in subopti-mal and inﬂexible architectures..in this paper, we propose a sequence-to-structure generation paradigm for event extraction– text2event, which can directly extract eventsfrom the text in an end-to-end manner.
speciﬁ-cally, instead of decomposing event structure pre-diction into different subtasks and predicting la-bels, we uniformly model the whole event extrac-tion process in a neural network-based sequence-to-structure architecture, and all triggers, arguments,and their labels are universally generated as naturallanguage words.
for example, we generate a subse-quence “attack ﬁre” for trigger extraction, whereboth “attack” and “ﬁre” are treated as natural lan-guage words.
compared with previous methods,our method is more data-efﬁcient: it can be learnedusing only coarse parallel text-record annotations,i.e., pairs of (cid:104)sentence, event records(cid:105), rather thanﬁne-grained token-level annotations.
besides, theuniform architecture makes it easy to model, learnand exploit the interactions between different un-derlying predictions, and the knowledge can beseamlessly shared and transferred between differ-ent components..furthermore, we design two algorithms for effec-tive sequence-to-structure event extraction.
first,we propose a constrained decoding algorithm,which can guide the generation process using eventschemas.
in this way, the event knowledge canbe injected and exploited during inference on-the-ﬂy.
second, we design a curriculum learning al-gorithm, which starts with current pre-trained lan-guage models (plms), then trains them on simple.
event substructure generation tasks such as triggergeneration and independent argument generation,ﬁnally trains the model on the full event structuregeneration task..we conducted experiments1 on ace and eredatasets, and the results veriﬁed the effectivenessof text2event in both supervised learning andtransfer learning settings.
in summary, the contri-butions are as follows:.
1. we propose a new paradigm for event ex-traction -– sequence-to-structure generation,which can directly extract events from the textin an end-to-end manner.
by uniformly model-ing all tasks in a single model and universallypredicting different labels, our method is ef-fective, data-efﬁcient, and easy to implement..2. we design an effective sequence-to-structurearchitecture, which is enhanced with a con-strained decoding algorithm for event knowl-edge injection during inference and a curricu-lum learning algorithm for efﬁcient modellearning..3. many information extraction tasks can be for-mulated as structure prediction tasks.
oursequence-to-structure method can motivatethe learning of other information extractionmodels..2 text2event: end-to-end event.
extraction as controllable generation.
given the token sequence x = x1, ..., x|x| of theinput text, text2event directly generate theevent structures e = e1, ..., e|e| via an encoder-decoder architecture.
for example, in figure 1,text2event take the raw text as input and out-put two event records including {type: transport,trigger: returned, arg1 role: artifact, arg1: theman, ...} and {type: arrest-jail, trigger: capture,..., arg2 role: agent, arg2: bounty hunters, ...}..for end-to-end event extraction, text2eventﬁrst encodes input text, then generates the lin-earized structure using the constrained decodingalgorithm.
in the following, we ﬁrst introduce howto reformulate event extraction as structure gener-ation via structure linearization, then describe thesequence-to-structure model and the constraineddecoding algorithm..1our.
source.
codes.
are.
openly.
available.
at.
https://github.com/luyaojie/text2event.
2796((transport returned(artifact the man)(destination los angeles)(origin mexico)).
(arrest-jail capture.
(person the man)(time tuesday)(agent bounty hunters)).
(a) record format..(b) tree format..(c) linearized format..figure 2: examples of three event representations.
the red solid line indicates the event-role relation; the bluedotted line indicates the label-span relation where the head is a label and the tail is a text span.
for example,“transport-returned” is a label-span relation edge, which head is “transport” and tail is “returned”..2.1 event extraction as structure generation.
this section describes how to linearize event struc-ture so that events can be generated in an end-to-end manner.
speciﬁcally, the linearized event rep-resentations should: (1) be able to express multipleevent records in a text as one expression; (2) beeasy to reversibly converted to event records in adeterministic way; (3) be similar to the token se-quence of general text generation tasks so that textgeneration models can be leveraged and transferredeasily..concretely,.
the process of converting fromrecord format to linearized format is shown in fig-ure 2. we ﬁrst convert event records (figure 2a)into a labeled tree (figure 2b) by: 1) ﬁrst labelingthe root of the tree with the type of event (root -transport, root - arrest-jail), 2) then connectingmultiple event argument role types with event types(transport - artifact, transport - origin, etc.
), and3) ﬁnally linking the text spans from the raw textto the corresponding nodes as leaves (transport -returned, transport - origin - mexico, transport- artifact - the man, etc.).
given the convertedevent tree, we linearize it into a token sequence(figure 2c) via depth-ﬁrst traversal (vinyals et al.,2015), where “(” and “)” are structure indicatorsused to represent the semantic structure of linearexpressions.
the traversal order of the same depthis the order in which the text spans appear in thetext, e.g., ﬁrst “return” then “capture” in figure 2b.
noted that each linearized form has a virtual root –root.
for a sentence that contains multiple eventrecords, each event links to root directly.
for asentence that doesn’t express any event, its treeformat will be linearized as “()”..2.2 sequence-to-structure network.
based on the above linearization strategy,text2event generates the event structure via.
a transformer-based encoder-decoder architecture(vaswani et al., 2017).
given the token sequencex = x1, ..., x|x| as input, text2event outputsthe linearized event representation y = y1, ..., y|y|.
to this end, text2event ﬁrst computes the hid-den vector representation h = h1, ..., h|x| of theinput via a multi-layer transformer encoder:.
h = encoder(x1, ..., x|x|).
(1).
where each layer of encoder(·) is a transformerblock with the multi-head attention mechanism..after the input token sequence is encoded, thedecoder predicts the output structure token-by-token with the sequential input tokens’ hidden vec-tors.
at the step i of generation, the self-attentiondecoder predicts the i-th token yi in the linearizedform and decoder state hd.
i as:i = decoder([h; hd.
yi, hd.
1, ..., hd.
i−1], yi−1).
(2).
where each layer of decoder(·) is a transformerblock that contains self-attention with decoder statehd.
i and cross-attention with encoder state h.the generated output structured sequence startsfrom the start token “(cid:104)bos(cid:105)” and ends with the endtoken “(cid:104)eos(cid:105)”.
the conditional probability of thewhole output sequence p(y|x) is progressively com-bined by the probability of each step p(yi|y<i, x):.
|y|(cid:89).
p(y|x) =.
p(yi|y<i, x).
(3).
iwhere y<i = y1...yi−1, and p(yi|y<i, x) is theprobability over the target vocabulary v normal-ized by softmax(·) ..because all tokens in linearized event represen-tations are also natural language words, we adoptthe pre-trained language model t5 (raffel et al.,2020) as our transformer-based encoder-decoder ar-chitecture.
in this way, the general text generationknowledge can be directly reused..2797the manreturned to los angeles from mexico following his capture tuesday by bounty hunters.event typetransporttriggerreturnedartifactthe mandestinationlos angelesoriginmexicoevent typearrest-jailtriggercapturepersonthe mantimetuesdayagentbounty hunterstransportreturnedartifactorigindestinationarrest-jailcapturetimepersonrootthe manmexicolos angelestuesdaythe man…2.3 constrained decoding.
given the hidden sequence h, the sequence-to-structure network needs to generate the linearizedevent representations token-by-token.
one straight-forward solution is to use a greedy decoding al-gorithm, which selects the token with the highestpredicted probability p(yi|y<i, x) at each decod-ing step i. unfortunately, this greedy decodingalgorithm cannot guarantee the generation of validevent structures.
in other words, it could end upwith invalid event types, mismatch of argument-type, and incomplete structure.
furthermore, thegreedy decoding algorithm ignores the useful eventschema knowledge, which can be used to guidethe decoding effectively.
for example, we can con-strain the model to only generate event type tokensin the type position..to exploit the event schema knowledge, we pro-pose to employ a trie-based constrained decodingalgorithm (chen et al., 2020a; cao et al., 2021) forevent generation.
during constrained decoding, theevent schema knowledge is injected as the promptof the decoder and ensures the generation of validevent structures..concretely, unlike the greedy decoding algo-rithm that selects the token from the whole tar-get vocabulary v at each step, our trie-based con-strained decoding method dynamically chooses andprunes a candidate vocabulary v (cid:48) based on the cur-rent generated state.
a complete linearized formdecoding process can be represented by executing atrie tree search, as shown in figure 3a.
speciﬁcally,each generation step of text2event has threekinds of candidate vocabulary v (cid:48):.
• event schema: label names of event types t.and argument roles r;.
• mention strings: event trigger word and argu-ment mention s, which is the text span in theraw input;.
• structure indicator: “(” and “)” which areused to combine event schemas and mentionstrings..r s.).
).
(.
........(cid:104)bos(cid:105).
(.
).
(cid:104)eos(cid:105).
(.
).
(.
).
t.s.(cid:104)eos(cid:105).
(a) the trie of event structure..(b) the trie of event type t ..figure 3: the preﬁx tree (trie) of the constrained de-coding algorithm for controllable structure generation.
t and r indicate the label name of event type and ar-gument role.
s indicates the text span in the raw text,which is the event trigger or argument mention of theextracted event..t , argument role name r and text span s, thedecoding process can be considered as executingsearch on a subtree of the trie tree.
for example,in figure 3b, the candidate vocabulary v (cid:48) for “(transfer” is {“ownership”, “money”}..finally, the decoder’s output will be transformedto event records and used as ﬁnal extraction results..3 learning.
section describes how to learn thethistext2event neural network in an end-to-end manner.
our method can be learned usingonly the coarse parallel text-record annotations,i.e., pairs of (cid:104)sentence, event records(cid:105), with noneed for ﬁne-grained token-level annotationused in traditional methods.
given a trainingdataset d = {(x1, y1), ...(x|d|, y|d|)} where eachinstance is a (cid:104)sentence, event records(cid:105) pair, thelearning objective is the negative log-likelihoodfunction as:.
l = −.
log p(y|x, θ).
(4).
(cid:88).
(x,y)∈d.
where θ is model parameters..the decoding starts from the root “(cid:104)bos(cid:105)” andends at the terminator “(cid:104)eos(cid:105)”.
at the generationstep i, the candidate vocabulary v (cid:48) is the childrennodes of the last generated node.
for instance, atthe generation step with the generated string “(cid:104)bos(cid:105)(”, the candidate vocabulary v (cid:48) is {“(”, “)”} infigure 3a.
when generating the event type name.
unfortunately, unlike general text-to-text gener-ation models, the learning of sequence-to-structuregeneration models is more challenging: 1) there isan output gap between the event generation modeland the text-to-text generation model.
comparedwith natural word sequences, the linearized eventstructure contains many non-semantic indicators.
2798attackdie∙∙∙transferownershipmoney∙∙∙startpositionorg(such as “(” and “)”, and they don’t follow the syntaxconstraints of natural language sentences.
2) thenon-semantic indicators “(” and “)” appear veryfrequently but contain little semantic information,which will mislead the learning process..to address the above challenges, we employ acurriculum learning (bengio et al., 2009; xu et al.,2020) strategy.
speciﬁcally, we ﬁrst train plmsusing simple event substructure generation tasksso that they would not overﬁt in non-semantic in-dicators; then we train the model on the full eventstructure generation task..substructure learning.
because event represen-tations often have complex structures and their to-ken sequences are different from natural languageword sequences, it is challenging to train them withthe full sequence generation task directly.
there-fore, we ﬁrst train text2event on simple eventsubstructures..speciﬁcally, we learn our model by starting fromgenerating only “(label, span)” substructures, in-cluding “(type, trigger words)” and “(role, argu-ment words)” substructures.
for example, we willextract substructure tasks in figure 2c in this stageas: (transport returned) (artifactthe man) (arrest-jail capture), etc.
we construct a (cid:104)sentence, substructures(cid:105) pair foreach extracted substructures, then train our modelusing the loss in equation 4..full structure learning.
after the substructurelearning stage, we further train our model for thefull structure generation task using the loss in equa-tion 4. we found the curriculum learning strategyuses data annotation more efﬁciently and makesthe learning process more smooth..4 experiments.
this section evaluates the proposed text2eventmodel by conducting experiments in both super-vised learning and transfer learning settings..4.1 experimental settings.
datasets.
we conducted experiments on theevent extraction benchmark – ace2005 (walkeret al., 2006), which has 599 english annotated doc-uments and 33 event types.
we used the same splitand preprocessing step as the previous work (zhanget al., 2019b; wadden et al., 2019; du and cardie,2020), and we denote it as ace05-en..dataset.
split.
#sents.
#events.
#roles.
ace05-en.
ace05-en+.
ere-en.
traindevtest.
traindevtest.
traindevtest.
17,172923832.
19,216901676.
14,7361,2091,163.
4,202450403.
4,419468424.
6,208525551.
4,859605576.
6,607759689.
8,924730822.table 1: dataset statistics..in addition to ace05-en, we also conducted ex-periments on two other benchmarks: ace05-en+and ere-en, using the same split and preprocess-ing step in the previous work (lin et al., 2020).
compared to ace05-en, ace05-en+ and ere-en further consider pronoun roles and multi-tokenevent triggers.
ere-en contains 38 event cate-gories and 458 documents..statistics of all datasets are shown in table 1.for evaluation, we used the same criteria in pre-vious work (zhang et al., 2019b; wadden et al.,2019; lin et al., 2020).
since text2event isa text generation model, we reconstructed the off-set of predicted trigger mentions by ﬁnding thematched utterance in the input sequence one byone.
for argument mentions, we found the nearestmatched utterance to the predicted trigger mentionas the predicted offset..baselines.
currently, event extraction supervi-sion can be conducted at two different levels:1) token-level annotation, which labels each to-ken in a sentence with event labels, e.g., “the/odismission/b-end-position of/o ..”; 2) parallel text-record annotation, which only gives (cid:104)sentence,event(cid:105) pairs but without expensive token-level an-notations, e.g., (cid:104)the dismission of ..., {type: end-position, trigger: dismission, ...}(cid:105).
furthermore,some previous works also leverage golden entityannotation for model training, which marks all en-tity mentions with their golden types, to facilitateevent extraction.
introducing more supervisionknowledge will beneﬁt the event extraction but ismore label-intensive.
the proposed text2eventonly uses parallel text-record annotation, whichmakes it more practical in a real-world application.
to verify text2event, we compare our.
method with the following groups of baselines:.
1. baselines using token annotation: tanl is the.
2799models.
trig-c.arg-c.p.r.f1.
p.r.f1.
plm.
models using token annotation + entity annotation.
joint3ee (nguyen and nguyen, 2019)dygie++ (wadden et al., 2019)gail (zhang et al., 2019b)oneiew/o global (lin et al., 2020)oneie (lin et al., 2020).
68.0-74.8--.
71.8-69.4--.
69.869.772.073.574.7.
52.1-61.6--.
52.1-45.7--.
52.148.852.453.956.8.
-bert-largeelmobert-largebert-large.
models using token annotation.
eeqa (du and cardie, 2020)mqaee (li et al., 2020).
71.1-.
73.7-.
72.471.7.
56.8-.
50.2-.
53.353.4.
2×bert-base3×bert-large.
generation-based baselines using token annotation.
tanl (paolini et al., 2021)multi-task tanl (paolini et al., 2021).
--.
--.
68.468.5.
--.
--.
47.648.5.t5-baset5-base.
our model using parallel text-record annotation.
text2eventtext2event.
67.569.6.
71.274.4.
69.271.9.
46.752.5.
53.455.2.
49.853.8.t5-baset5-large.
table 2: experiment results on ace05-en.
trig-c indicates trigger identiﬁcation and classiﬁcation.
arg-c indicatesargument identiﬁcation and classiﬁcation.
plm represents the pre-trained language models used by each model..sota sequence generation-based method that mod-els event extraction as a trigger-argument pipelinemanner (paolini et al., 2021); multi-task tanl ex-tends tanl by transferring structure knowledgefrom other tasks; eeqa (du and cardie, 2020)and mqaee (li et al., 2020) are qa-based modelswhich use machine reading comprehension modelfor trigger detection and argument extraction..2. baselines using both token annotation andentity annotation: joint3ee is a joint entity, trigger,argument extraction model based on the shared hid-den representations (nguyen and nguyen, 2019);dygie++ is a bert-based model which capturesboth within-sentence and cross-sentence context(wadden et al., 2019); gail is an inverse rein-forcement learning-based joint entity and event ex-traction model (zhang et al., 2019b); oneie is anend-to-end ie system which employs global featureand beam search to extract globally optimal eventstructures (lin et al., 2020)..implementations.
we optimized our model us-ing label smoothing (szegedy et al., 2016; m¨ulleret al., 2019) and adamw (loshchilov and hutter,2019) with learning rate=5e-5 for t5-large, 1e-4for t5-base.
for curriculum learning, the epoch ofsubstructure learning is 5, and full structure learn-.
ing is 30. we conducted each experiment on asingle nvidia geforce rtx 3090 24gb.
due togpu memory limitation, we used different batchsizes for different models: 8 for t5-large and 16 fort5-base; and truncated the max length of raw textto 256 and linearized form to 128 during training.
we added the task name as the preﬁx for the t5default setup..4.2 results in supervised learning setting.
table 2 presents the performance of all base-lines and text2event on ace05-en.
andtable 3 shows the performance of sota andtext2event on ace05-en+ and ere-en.
wecan see that:.
1) by uniformly modeling all.
tasks in asingle model and predicting labels universally,text2event can achieve competitive perfor-mance with weaker supervision and simpler ar-chitecture.
our method, only using the weak paral-lel text-record annotations, surpasses most of thebaselines using token and entity annotations andachieves competitive performance with sota.
fur-thermore, using the simple encoder-decoder archi-tecture, text2event outperforms most of thecounterparts with complicated architectures..2800datasets.
trig-c.arg-c.trig-c.arg-c.settings.
p.r.f1.
p.r.f1.
p.r.f1.
p.r.f1.
sota (token + entity annotation).
oneie (token + entity annotation).
ace05-en+ere-en∗.
-56.9.
-58.7.
72.857.8.
-51.9.
-47.8.
54.849.8.text2event (parallel text-record annotation).
ace05-en+ 71.259.2.ere-en.
72.559.6.
71.859.4.
54.049.4.
54.847.2.
54.448.3.table 3: experiment results on ace05-en+ and ere-en.
sota indicates the state-of-the-art system – oneie.
* the result of sota for ere-en is reproduced by theofﬁcial release code because of the slightly differentdataset statistic result on ere-en..non-transfertransfergain.
non-transfertransfergain.
non-transfertransfergain.
78.178.9.
62.361.7.
50.957.1.
37.940.0.eeqa (token annotation).
69.979.5.
67.361.7.
36.533.9.
37.441.2.
69.369.2-0.1.
68.669.5+0.9.
69.072.7+3.7.
text2event (parallel text-record annotation).
79.482.1.
61.165.3.
58.458.8.
40.945.4.
43.547.0+3.5.
36.937.2+0.3.
48.051.2+3.2.
2) by directly generating event structure from thetext, text2event can signiﬁcantly outperformsequence generation-based methods.
our methodimproves arg-c f1 by 4.6% and 2.7% over thesota generation baseline and its extended multi-task tanl.
compared with sequence generation,structure generation can be effectively guided us-ing event schema knowledge during inference, andthere is no need to generate irrelevant information.
3) by uniformly modeling and sharing infor-mation between different tasks and labels, thesequence-to-structure framework can achieve ro-bust performance.
from table 2 and table 3, wecan see that the performance of oneie decreases onthe harder dataset ace05-en+, which has morepronoun roles and multi-token triggers.
by contrast,the performance of text2event remains nearlythe same on ace05-en.
we believe this may bebecause the proposed sequence-to-structure modelis a universal model that doesn’t specialize in labelsand can better share information between differentlabels..4.3 results in transfer learning setting.
text2event is a universal model, thereforecan facilitate the knowledge transfer between dif-ferent labels.
to verify the transfer ability oftext2event, we conducted experiments in thetransfer learning setting, and the results are shownin table 4. speciﬁcally, we ﬁrst randomly split thesentences which length larger than 8 in ace05-en+ into two equal-sized subsets src and tgt: srconly retains the annotations of the top 10 frequentevent types, and tgt only retains the annotations ofthe remaining 23 event types.
for both src and tgt,we use 80% of the dataset for model training and.
table 4: experiment results on the tgt subset of ace05-en+ in the transfer learning setting..20% for evaluation.
for transfer learning, we ﬁrstpre-trained an event extraction model on the srcdataset, then ﬁne-tuned the pre-trained model forextracting the new event types in tgt.
from table 4,we can see that:.
1) data-efﬁcient text2event can make bet-ter use of supervision signals.
even training ontgt from scratch, the proposed method also outper-forms strong baselines.
we believe that this maybecause baselines using token and entity annota-tion require massive ﬁne-grained data for modellearning.
different from baselines, text2eventuniformly models all subtasks, thus the knowledgecan be seamlessly transferred, which is more data-efﬁcient..2) text2event can effectively transfer knowl-edge between different labels.
compared with thenon-transfer setting, which is directly trained on tgttraining set, the transfer setting of text2eventcan achieve signiﬁcant f1 improvements of 3.7 and3.2 on trig-c and arg-c, respectively.
by contrast,the other two baselines cannot obtain signiﬁcantf1 improvements of both trig-c and arg-c viatransfer learning.
note that the information of en-tity annotation is shared across src and tgt.
asa result, oneie can leverage such information tobetter argument prediction even with worse triggerprediction.
however, even without using entity an-notation, the proposed method can still achieve asimilar improvement in the transfer learning setting.
this is because the labels are provided universallyin text2event, so the parameters are not label-speciﬁc..2801trig-c f1.
1% 5% 25% 100%.
5 related work.
text2event + cl 24.617.9text2event13.2w/o cd0.0w/o es.
text2event + cltext2eventw/o cdw/o es.
8.63.72.30.0.
52.852.146.824.3.
33.630.927.37.0.
65.565.064.331.6.
44.044.744.48.2.
71.469.668.655.5.
53.352.652.328.9.arg-c f1.
1% 5% 25% 100%.
table 5: experiment results of variants trained withdifferent-sized training set on the development set oface05-en..4.4 detailed analysis.
this section analyzes the effects of event schemaknowledge, constrained decoding, and curriculumlearning algorithm in text2event.
we designedfour ablated variants based on t5-base:.
• “text2event” is the base model that is di-rectly trained with the full structure learning..• “+ cl” indicates training text2event withthe proposed curriculum learning algorithm..• “w/o cd” discards the constrained decodingduring inference and generates event struc-tures as an unconstrained generation model..• “w/o es” replaces the names of event typesand roles with meaningless symbols, whichis used to verify the effect of event schemaknowledge..table 5 shows the results on the development setof ace05-en using different training data sizes.
we can see that: 1) constrained decoding can ef-fectively guide the generation with event schemas,especially in low-resource settings.
comparingto “w/o cd”, constrained decoding improves theperformance of text2event, especially in low-resource scenarios, e.g., using 1%, 5% training set.
2) curriculum learning is useful for model learning.
substructure learning improves 4.7% trig-c f1and 5.8% arg-c f1 on average.
3) it is crucial toencode and generate event labels as words, ratherthan meaningless symbols.
because by encodinglabels as natural language words, our method caneffectively transfer knowledge from pre-trained lan-guage models..our work is a synthesis of two research directions:event extraction and structure prediction via neuralgeneration model..event extraction has received widespread atten-tion in recent years, and mainstream methods usu-ally use different strategies to obtain a completeevent structure.
these methods can be dividedinto: 1) pipeline classiﬁcation (ahn, 2006; ji andgrishman, 2008; liao and grishman, 2010; honget al., 2011, 2018; huang and riloff, 2012; chenet al., 2015; sha et al., 2016; lin et al., 2018;yang et al., 2019; wang et al., 2019; ma et al.,2020; zhang et al., 2020c), 2) multi-task joint mod-els (mcclosky et al., 2011; li et al., 2013, 2014;yang and mitchell, 2016; nguyen et al., 2016; liuet al., 2018; zhang et al., 2019a; zheng et al.,2019), 3) semantic structure grounding (huanget al., 2016, 2018; zhang et al., 2020a), and 4)question-answering (chen et al., 2020b; du andcardie, 2020; li et al., 2020; liu et al., 2020)..compared with previous methods, we model allsubtasks of event extraction in a uniform sequence-to-structure framework, which leads to better de-cision interactions and information sharing.
theneural encoder-decoder generation architecture(sutskever et al., 2014; bahdanau et al., 2015) hasshown its strong structure prediction ability andhas been widely used in many nlp tasks, such asmachine translation (kalchbrenner and blunsom,2013), semantic parsing (dong and lapata, 2016;song et al., 2020), entity extraction (strakov´a et al.,2019), relation extraction (zeng et al., 2018; zhanget al., 2020b), and aspect term extraction (ma et al.,2019).
like text2event in this paper, tanl(paolini et al., 2021) and grit (du et al., 2021)also employ neural generation models for eventextraction, but they focus on sequence generation,rather than structure generation.
different fromprevious works that extract text span via labeling(strakov´a et al., 2019) or copy/pointer mechanism(zeng et al., 2018; du et al., 2021), text2eventdirectly generate event schemas and text spans toform event records via constrained decoding (caoet al., 2021; chen et al., 2020a), which allowstext2event to handle various event types andtransfer to new types easily..6 conclusions.
in this paper, we propose text2event, asequence-to-structure generation paradigm for.
2802event extraction.
text2event directly learnsfrom parallel text-record annotation and uniformlymodels all subtasks of event extraction in asequence-to-structure framework.
concretely, wepropose an effective sequence-to-structure networkfor event extraction, which is further enhanced bya constrained decoding algorithm for event knowl-edge injection during inference and a curriculumlearning algorithm for efﬁcient model learning.
ex-perimental results in supervised learning and trans-fer learning settings show that text2event canachieve competitive performance with the previoussota using only coarse text-record annotation..for future work, we plan to adapt our method toother information extraction tasks, such as n-aryrelation extraction..acknowledgments.
we sincerely thank the reviewers for their insight-ful comments and valuable suggestions.
thiswork is supported by the national natural sciencefoundation of china under grants no.
u1936207and 61772505, beijing academy of artiﬁcialintelligence (baai2019qn0502), and in partby the youth innovation promotion associationcas(2018141)..references.
david ahn.
2006. the stages of event extraction.
inproceedings of the workshop on annotating and rea-soning about time and events, pages 1–8, sydney,australia.
association for computational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in the third interna-tional conference on learning representations..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inproceedings of the 26th international conference onmachine learning, pages 41–48, montreal.
omni-press..nicola de cao, gautier izacard, sebastian riedel, andfabio petroni.
2021. autoregressive entity retrieval.
in international conference on learning representa-tions..pinzhen chen, nikolay bogoychev, kenneth heaﬁeld,and faheem kirefu.
2020a.
parallel sentence miningby constrained decoding.
in proceedings of the 58thannual meeting of the association for computationallinguistics, pages 1672–1678, online.
associationfor computational linguistics..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 1: long papers), pages 167–176,beijing, china.
association for computational lin-guistics..yunmo chen, tongfei chen, seth ebner, aaron stevenwhite, and benjamin van durme.
2020b.
readingthe manual: event extraction as deﬁnition compre-hension.
in proceedings of the fourth workshop onstructured prediction for nlp, pages 74–83, online.
association for computational linguistics..george doddington, alexis mitchell, mark przybocki,lance ramshaw, stephanie strassel, and ralphweischedel.
2004. the automatic content extrac-tion (ace) program – tasks, data, and evaluation.
inproceedings of the fourth international conferenceon language resources and evaluation (lrec’04),lisbon, portugal.
european language resources as-sociation (elra)..li dong and mirella lapata.
2016. language to logicalform with neural attention.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages33–43, berlin, germany.
association for computa-tional linguistics..xinya du and claire cardie.
2020. event extraction byanswering (almost) natural questions.
in proceedingsof the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 671–683,online.
association for computational linguistics..xinya du, alexander rush, and claire cardie.
2021. grit: generative role-ﬁller transformers fordocument-level event entity extraction.
in proceed-ings of the 16th conference of the european chap-ter of the association for computational linguistics:main volume, pages 634–644, online.
associationfor computational linguistics..yu hong, jianfeng zhang, bin ma, jianmin yao,guodong zhou, and qiaoming zhu.
2011. usingcross-entity inference to improve event extraction.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 1127–1136, portland,oregon, usa.
association for computational lin-guistics..yu hong, wenxuan zhou, jingli zhang, guodong zhou,and qiaoming zhu.
2018. self-regulation: employ-ing a generative adversarial network to improve eventdetection.
in proceedings of the 56th annual meetingof the association for computational linguistics (vol-ume 1: long papers), pages 515–526, melbourne,australia.
association for computational linguistics..lifu huang, taylor cassidy, xiaocheng feng, hengji, clare r. voss, jiawei han, and avirup sil.
2016..2803liberal event extraction and event schema induction.
in proceedings of the 54th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 258–268, berlin, germany.
association for computational linguistics..lifu huang, heng ji, kyunghyun cho, ido dagan, se-bastian riedel, and clare voss.
2018. zero-shottransfer learning for event extraction.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 2160–2170, melbourne, australia.
associationfor computational linguistics..ruihong huang and ellen riloff.
2012. modeling tex-tual cohesion for event extraction.
in proceedings ofthe twenty-sixth aaai conference on artiﬁcial intel-ligence, aaai’12, page 1664–1670.
aaai press..heng ji and ralph grishman.
2008. reﬁning eventextraction through cross-document inference.
in pro-ceedings of acl-08: hlt, pages 254–262.
associa-tion for computational linguistics..nal kalchbrenner and phil blunsom.
2013. recurrentin proceedings ofcontinuous translation models.
the 2013 conference on empirical methods in natu-ral language processing, pages 1700–1709, seattle,washington, usa.
association for computationallinguistics..fayuan li, weihua peng, yuguang chen, quan wang,lu pan, yajuan lyu, and yong zhu.
2020. eventextraction as multi-turn question answering.
in find-ings of the association for computational linguistics:emnlp 2020, pages 829–838, online.
associationfor computational linguistics..qi li, heng ji, yu hong, and sujian li.
2014. con-structing information networks using one singlemodel.
in proceedings of the 2014 conference onempirical methods in natural language processing(emnlp), pages 1846–1851, doha, qatar.
associa-tion for computational linguistics..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-tures.
in proceedings of the 51st annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 73–82, soﬁa, bulgaria.
association for computational linguistics..shasha liao and ralph grishman.
2010. using doc-ument level cross-event inference to improve eventextraction.
in proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 789–797, uppsala, sweden.
association forcomputational linguistics..hongyu lin, yaojie lu, xianpei han, and le sun.
2018.nugget proposal networks for chinese event detec-tion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1565–1574, melbourne,australia.
association for computational linguistics..ying lin, heng ji, fei huang, and lingfei wu.
2020.a joint neural model for information extraction withglobal features.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 7999–8009, online.
association forcomputational linguistics..jian liu, yubo chen, kang liu, wei bi, and xiaojiangliu.
2020. event extraction as machine reading com-prehension.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 1641–1651, online.
associationfor computational linguistics..xiao liu, zhunchen luo, and heyan huang.
2018.jointly multiple events extraction via attention-basedgraph information aggregation.
in proceedings of the2018 conference on empirical methods in naturallanguage processing, pages 1247–1256, brussels,belgium.
association for computational linguistics..ilya loshchilov and frank hutter.
2019. decoupledweight decay regularization.
in seventh internationalconference on learning representations..dehong ma, sujian li, fangzhao wu, xing xie,and houfeng wang.
2019. exploring sequence-to-sequence learning in aspect term extraction.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 3538–3547, florence, italy.
association for computationallinguistics..jie ma, shuai wang, rishita anubhai, miguel balles-teros, and yaser al-onaizan.
2020. resource-enhanced neural model for event argument extraction.
in findings of the association for computational lin-guistics: emnlp 2020, pages 3554–3559, online.
association for computational linguistics..david mcclosky, mihai surdeanu, and christophermanning.
2011. event extraction as dependencyparsing.
in proceedings of the 49th annual meet-ing of the association for computational linguistics:human language technologies, pages 1626–1635,portland, oregon, usa.
association for computa-tional linguistics..rafael m¨uller, simon kornblith, and geoffrey e hin-ton.
2019. when does label smoothing help?
inadvances in neural information processing systems,volume 32, pages 4694–4703.
curran associates,inc..thien huu nguyen, kyunghyun cho, and ralph grish-man.
2016. joint event extraction via recurrent neuralnetworks.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 300–309, san diego, california.
association for computational linguistics..trung minh nguyen and thien huu nguyen.
2019.one for all: neural joint modeling of entities andin the thirty-third aaai conference onevents..2804artiﬁcial intelligence, aaai ’2019.
association forthe advancement of artiﬁcial intelligence..giovanni paolini, ben athiwaratkun, jason krone,jie ma, alessandro achille, rishita anubhai, ci-cero nogueira dos santos, bing xiang, and stefanosoatto.
2021. structured prediction as translationbetween augmented natural languages.
in the ninthinternational conference on learning representa-tions..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploring thelimits of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..lei sha,.
jing liu, chin-yew lin, sujian li,baobao chang, and zhifang sui.
2016. rbpb:regularization-based pattern balancing method forevent extraction.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 1224–1234,berlin, germany.
association for computational lin-guistics..linfeng song, ante wang, jinsong su, yue zhang, kunxu, yubin ge, and dong yu.
2020. structural infor-mation preserving for graph-to-text generation.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7987–7998, online.
association for computational lin-guistics..jana strakov´a, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through lineariza-tion.
in proceedings of the 57th annual meeting ofthe association for computational linguistics, pages5326–5331, florence, italy.
association for compu-tational linguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014. se-quence to sequence learning with neural networks.
inadvances in neural information processing systems,volume 27, pages 3104–3112.
curran associates,inc..c. szegedy, v. vanhoucke, s. ioffe, j. shlens, andz. wojna.
2016. rethinking the inception architec-ture for computer vision.
in 2016 ieee conferenceon computer vision and pattern recognition (cvpr),pages 2818–2826..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilingualtraining corpus..xiaozhi wang, ziqi wang, xu han, zhiyuan liu, juanzili, peng li, maosong sun, jie zhou, and xiang ren.
2019. hmeae: hierarchical modular event argu-ment extraction.
in proceedings of the 2019 confer-ence on empirical methods in natural language pro-cessing and the 9th international joint conferenceon natural language processing (emnlp-ijcnlp),pages 5777–5783, hong kong, china.
associationfor computational linguistics..benfeng xu, licheng zhang, zhendong mao, quanwang, hongtao xie, and yongdong zhang.
2020.curriculum learning for natural language understand-ing.
in proceedings of the 58th annual meeting ofthe association for computational linguistics, pages6095–6104, online.
association for computationallinguistics..bishan yang and tom m. mitchell.
2016. joint extrac-tion of events and entities within a document context.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 289–299, san diego, california.
associationfor computational linguistics..sen yang, dawei feng, linbo qiao, zhigang kan, anddongsheng li.
2019. exploring pre-trained languagemodels for event extraction and generation.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5284–5294, florence, italy.
association for computationallinguistics..xiangrong zeng, daojian zeng, shizhu he, kang liu,and jun zhao.
2018. extracting relational facts byan end-to-end neural model with copy mechanism.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume 1:long papers), pages 506–514, melbourne, australia.
association for computational linguistics..hongming zhang, haoyu wang, and dan roth.
2020a.
unsupervised label-aware event trigger and argumentclassiﬁcation..oriol vinyals, ł ukasz kaiser, terry koo, slav petrov,ilya sutskever, and geoffrey hinton.
2015. gram-mar as a foreign language.
in advances in neuralinformation processing systems, volume 28, pages2773–2781.
curran associates, inc..junchi zhang, yanxia qin, yue zhang, mengchi liu,and donghong ji.
2019a.
extracting entities andevents as a single task using a transition-based neuralmodel.
in proceedings of the twenty-eighth inter-national joint conference on artiﬁcial intelligence,.
2805ijcai-19, pages 5422–5428.
international joint con-ferences on artiﬁcial intelligence organization..ranran haoran zhang, qianying liu, aysa xuemo fan,heng ji, daojian zeng, fei cheng, daisuke kawa-hara, and sadao kurohashi.
2020b.
minimize ex-posure bias of seq2seq models in joint entity andrelation extraction.
in findings of the associationfor computational linguistics: emnlp 2020, pages236–246, online.
association for computational lin-guistics..tongtao zhang, heng ji, and avirup sil.
2019b.
jointentity and event extraction with generative adversar-ial imitation learning.
data intelligence, 1(2):99–120..zhisong zhang, xiang kong, zhengzhong liu, xuezhema, and eduard hovy.
2020c.
a two-step approachfor implicit event argument detection.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 7479–7485, online.
association for computational linguistics..shun zheng, wei cao, wei xu, and jiang bian.
2019.doc2edag: an end-to-end document-level frame-work for chinese ﬁnancial event extraction.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 337–346, hongkong, china.
association for computational linguis-tics..2806