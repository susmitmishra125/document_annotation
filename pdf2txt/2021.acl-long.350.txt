syntax-augmented multilingual bert for cross-lingual transfer.
wasi uddin ahmad†∗, haoran li‡, kai-wei chang†, yashar mehdad‡†university of california, los angeles, ‡facebook ai†{wasiahmad,kwchang}@cs.ucla.edu, ‡{aimeeli,mehdad}@fb.com.
abstract.
in recent years, we have seen a colossal effortin pre-training multilingual text encoders us-ing large-scale corpora in many languages tofacilitate cross-lingual transfer learning.
how-ever, due to typological differences across lan-guages, the cross-lingual transfer is challeng-ing.
nevertheless, language syntax, e.g., syn-tactic dependencies, can bridge the typologi-cal gap.
previous works have shown that pre-trained multilingual encoders, such as mbert(devlin et al., 2019), capture language syn-tax, helping cross-lingual transfer.
this workshows that explicitly providing language syn-tax and training mbert using an auxiliaryobjective to encode the universal dependencytree structure helps cross-lingual transfer.
weperform rigorous experiments on four nlptasks, including text classiﬁcation, question an-swering, named entity recognition, and task-oriented semantic parsing.
the experiment re-sults show that syntax-augmented mbert im-proves cross-lingual transfer on popular bench-marks, such as paws-x and mlqa, by 1.4and 1.6 points on average across all languages.
in the generalized transfer setting, the perfor-mance boosted signiﬁcantly, with 3.9 and 3.1points on average in paws-x and mlqa..1.introduction.
cross-lingual transfer reduces the requirement oflabeled data to perform natural language process-ing (nlp) in a target language, and thus has theability to avail nlp applications in low-resourcelanguages.
however, transferring across languagesis challenging because of linguistic differences atlevels of morphology, syntax, and semantics.
forexample, word order difference is one of the cru-cial factors that impact cross-lingual transfer (ah-mad et al., 2019).
the two sentences in englishand hindi, as shown in figure 1 have the same.
∗work done during internship at facebook ai..figure 1: two parallel sentences in english and hindifrom xnli (conneau et al., 2018) dataset.
the wordshighlighted with the same color have the same mean-ing.
although the sentences have a different word or-der, their syntactic dependency structure is similar..meaning but a different word order (while englishhas an svo (subject-verb-object) order,hindi follows sov).
however, the sentences havea similar dependency structure, and the constituentwords have similar part-of-speech tags.
presum-ably, language syntax can help to bridge the typo-logical differences across languages..in recent years, we have seen a colossal effortto pre-train transformer encoder (vaswani et al.,2017) on large-scale unlabeled text data in oneor many languages.
multilingual encoders, suchas mbert (devlin et al., 2019) or xlm-r (con-neau et al., 2020) map text sequences into a sharedmultilingual space by jointly pre-training in manylanguages.
this allows us to transfer the multilin-gual encoders across languages and have foundeffective for many nlp applications, includingtext classiﬁcation (bowman et al., 2015; conneau.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4538–4554august1–6,2021.©2021associationforcomputationallinguistics4538englishspanish.
english.
spanish.
mbert.
q.c.a.mbert + syn..how many members of the senate are elected?
cu´antos miembros del senado son elegidos?
the chamber of deputies has 630 elected members, while the senate has 315elected members.
.
.
.
la c´amara de los diputados est´a formada por 630 miembros, mientras que hay315 senadores m´as los senadores vitalicios.
.
.
.
[q:english-c:english] 315 ((cid:51)); [q:spanish-c:spanish] 630 ((cid:55))[q:spanish-c:english] 315 ((cid:51)); [q:english-c:spanish] 630 ((cid:55))[q:english-c:english] 315 ((cid:51)); [q:spanish-c:spanish] 315 ((cid:51))[q:spanish-c:english] 315 ((cid:51)); [q:english-c:spanish] 315 ((cid:51)).
figure 2: a parallel qa example in english (en) and spanish (es) from mlqa (lewis et al., 2020) with predictionsfrom mbert and our proposed syntax-augmented mbert.
in “q:x-c:y”, x and y indicates question and contextlanguages, respectively.
based on our analysis of the highlighted tokens’ attention weights, we conjecture thatmbert answers 630 as the token is followed by “miembros”, while 315 is followed by “senadores” in spanish..et al., 2018), question answering (rajpurkar et al.,2016; lewis et al., 2020), named entity recogni-tion (pires et al., 2019; wu and dredze, 2019), andmore.
since the introduction of mbert, severalworks (wu and dredze, 2019; pires et al., 2019; ket al., 2020) attempted to reason their success incross-lingual transfer.
in particular, wu and dredze(2019) showed that mbert captures language syn-tax that makes it effective for cross-lingual transfer.
a few recent works (hewitt and manning, 2019;jawahar et al., 2019; chi et al., 2020) suggest thatbert learns compositional features; mimickinga tree-like structure that agrees with the universaldependencies taxonomy..however, ﬁne-tuning for the downstream taskin a source language may not require mbert toretain structural features or learn to encode syn-tax.
we argue that encouraging mbert to learnthe correlation between syntax structure and targetlabels can beneﬁt cross-lingual transfer.
to supportour argument, we show an example of question an-swering (qa) in figure 2. in the example, mbertpredicts incorrect answers given the spanish lan-guage context that can be corrected by exploitingsyntactic clues.
utilizing syntax structure can alsobeneﬁt generalized cross-lingual transfer (lewiset al., 2020) where the input text sequences belongto different languages.
for example, answering anenglish question based on a spanish passage orpredicting text similarity given the two sentencesas shown in figure 1. in such a setting, syntacticclues may help to align sentences..in this work, we propose to augment mbertwith universal language syntax while ﬁne-tuningon downstream tasks.
we use a graph attention.
network (gat) (veliˇckovi´c et al., 2018) to learnstructured representations of the input sequencesthat are incorporated into the self-attention mech-anism.
we adopt an auxiliary objective to traingat such that it embeds the dependency structureof the input sequence accurately.
we perform anevaluation on zero-shot cross-lingual transfer fortext classiﬁcation, question answering, named en-tity recognition, and task-oriented semantic parsing.
experiment results show that augmenting mbertwith syntax improves cross-lingual transfer, suchas in paws-x and mlqa, by 1.4 and 1.6 pointson average across all the target languages.
syntax-augmented mbert achieves remarkable gain inthe generalized cross-lingual transfer; in paws-xand mlqa, performance is boosted by 3.9 and 3.1points on average across all language pairs.
fur-thermore, we discuss challenges and limitations inmodeling universal language syntax.
we releasethe code to help future works.1.
2 syntax-augmented multilingual bert.
multilingual bert (mbert) (devlin et al., 2019)enables cross-lingual learning as it embeds text se-quences into a shared multilingual space.
mbertis ﬁne-tuned on downstream tasks, e.g., text classi-ﬁcation using monolingual data and then directlyemployed to perform on the target languages.
thisrefers to zero-shot cross-lingual transfer.
our mainidea is to augment mbert with language syntaxfor zero-shot cross-lingual transfer.
we employgraph attention network (gat) (veliˇckovi´c et al.,2018) to learn syntax representations and fuse theminto the self-attention mechanism of mbert..1https://github.com/wasiahmad/syntax-mbert.
4539in this section, we ﬁrst brieﬂy review the trans-former encoder that bases mbert (§ 2.1), andthen describe the graph attention network (gat)that learns syntax representations from dependencystructure of text sequences (§ 2.2).
finally, wedescribe how language syntax is explicitly incorpo-rated into the transformer encoder (§ 2.3)..2.1 transformer encoder.
transformer encoder (vaswani et al., 2017) is com-posed of an embedding layer and stacked encoderlayers.
each encoder layer consists of two sub-layers, a multi-head attention layer followed bya fully connected feed-forward layer.
we detailthe process of encoding an input token sequence(w1, .
.
.
, wn) into a sequence of vector representa-tions h = [h1, .
.
.
, hn] as follows..embedding layeris parameterized by two em-bedding matrices — the token embedding ma-trix we ∈ ru ×dmodel and the position embed-ding matrix wp ∈ ru ×dmodel (where u is thevocabulary size and dmodel is the encoder outputdimension).
an input text sequence enters intothe model as two sequences: the token sequence(w1, .
.
.
, wn) and the corresponding absolute po-sition sequence (p1, .
.
.
, pn).
the output of theembedding layer is a sequence of vectors {xi}ni=1where xi = wiwe + piwp.
the vectors are packedinto matrix h 0 = [x1, .
.
.
, xn] ∈ rn×dmodel andfed to an l-layer encoder..multi-head attention allows to jointly attendto information from different representation sub-spaces, known as attention heads.
multi-head at-tention layer composed of h attention heads withthe same parameterization structure.
at each atten-tion head, the output from the previous layer h l−1is ﬁrst linearly projected into queries, keys, andvalues as follows.
q = h l−1w q, k = h l−1w kllwhere the parameters w ql ∈ rdmodel×dk and, w klw vl ∈ rdmodel×dv are unique per attention head.
then scaled dot-product attention is performed tocompute the output vectors {oi}n., v = h l−1w vl.i=1 ∈ rn×dv ..,.
attention(q, k, v, m, dk)(cid:18) qkt + m√.
= softmax.
(cid:19).
v,.
dk.
(1).
where m ∈ rn×n is the masking matrix that deter-mines whether a pair of input positions can attend.
figure 3: a simpliﬁed illustration of the multi-headself-attention in the graph attention network whereineach head attention is allowed between words within δdistance from each other in the dependency graph.
forexample, as shown, in one of the attention heads, theword “likes” is only allowed to attend its adjacent (δ=1)words “dog” and “play”..each other.
in classic multi-head attention, m is azero matrix (all positions can attend each other)..the output vectors from all the attention headsare concatenated and projected into dmodel dimen-sion using the parameter matrix wo ∈ rhdv×dmodel.
finally the vectors are passed through a feed-forward network to output h l ∈ rn×dmodel..2.2 graph attention network.
we embed the syntax structure of the input tokensequences using their universal dependency parse.
a dependency parse is a directed graph where thenodes represent words, and the edges representdependencies (the dependency relation betweenthe head and dependent words).
we use a graphattention network (gat) (veliˇckovi´c et al., 2018)to embed the dependency tree structure of the inputsequence.
we illustrate gat in figure 3..given the input sequence, the words (wi) andtheir part-of-speech tags (posi) are embedded intovectors using two parameter matrices: the tokenembedding matrix we and the part-of-tag embed-ding wpos.
the input sequence is then encodedinto an input matrix g0 = [g1, .
.
.
, gn], wheregi = wiwe + posiwpos ∈ rdmodel.
note that to-ken embedding matrix we is shared between gatand the transformer encoder.
then g0 is fed intoan lg-layer gat where each layer generates wordrepresentations by attending their adjacent words..4540gat uses the multi-head attention mechanism andperform a dependency-aware self-attention as.
o = attention(t , t , v, m, dg).
(2).
namely setting the query and key matrices to be thesame t ∈ rn×dg respectively and the mask m by.
(cid:40).
mij =.
0,−∞,.
dij ≤ δotherwise.
(3).
where d is the distance matrix and dij indicatesthe shortest path distance between word i and j inthe dependency graph structure..typically in gat, δ is set to 1; allowing atten-tion between adjacent words only.
however, in ourstudy, we ﬁnd setting δ to [2, 4] helpful for thedownstream tasks.
finally, the vector representa-tions from all the attention heads (as in eq.
(2))are concatenated to form the output representationsgl ∈ rn×kdg , where k is the number of attentionheads employed.
the goal of the gat encoderis to encode the dependency structure into vectorrepresentations.
therefore, we design gat to belight-weight; consisting of much less parametersin comparison to transformer encoder.
note that,gat does not employ positional representationsand only consists of multi-head attention; there isno feed-forward sublayer and residual connections..dependency tree over wordpieces and specialsymbols mbert tokenizes the input sequenceinto subword units, also known as wordpieces.
therefore, we modify the dependency structure oflinguistic tokens to accommodate wordpieces.
weintroduce additional dependencies between the ﬁrstsubword (head) and the rest of the subwords (de-pendents) of a linguistic token.
more speciﬁcally,we introduce new edges from the head subword tothe dependent subwords.
the inputs to mbert usespecial symbols: [cls] and [sep].
we add an edgefrom the [cls] token to the root of the dependencytree and the [sep] tokens..2.3 syntax-augmented transformer encoder.
we want the transformer encoder to consider syn-tax structure while performing the self-attention be-tween input sequence elements.
we use the syntaxrepresentations produced by gat (outputs from thelast layer, denoting as g) to bias the self-attention.
o = attention(q + ggql , k + ggkl , v, m, dk),where gql ∈ rdkdg ×dk are new parametersthat learn representations to bias the self-attention..l , gk.
we consider the addition terms (ggql ) assyntax-bias that provide syntactic clues to guidethe self-attention.
the high-level intuition behindthe syntax bias is to attend tokens with a speciﬁcpart-of-speech tag sequence or dependencies.2.
l , ggk.
syntax-heads mbert employs h (=12) atten-tion heads and the syntax representations can beinfused into one or more of these heads, and werefer them as syntax-heads.
in our experiments, weobserved that instilling structural information intomany attention heads degenerates the performance.
for the downstream tasks, we consider one or twosyntax-heads that gives the best performance.3.
syntax-layersrefers to the encoder layers thatare infused by syntax representations from gat.
mbert has a 12-layer encoder and our study ﬁndsconsidering all of the layers as syntax-layers bene-ﬁcial for cross-lingual transfer..2.4 fine-tuning.
we jointly ﬁne-tune mbert and gat on down-stream tasks in the source language (english in thiswork) following the standard procedure.
however,the task-speciﬁc training may not guide gat toencode the tree structure.
therefore, we adopt anauxiliary objective that supervises gat to learnrepresentations which can be used to decode thetree structure.
more speciﬁcally, we use gat’s out-put representations g = [g1, .
.
.
, gn] to predict thetree distance between all pairs of words (gi, gj) andthe tree depth ||gi|| of each word wi in the inputsequence.
following hewitt and manning (2019),we apply a linear transformation θ1 ∈ rm×kdg tocompute squared distances as follows..dθ1(gi, gj)2 = (θ1(gi − gj))t (θ1(gi − gj)).
the parameter matrix θ1 is learnt by minimizing:.
minθ1.
(cid:88).
(cid:88).
1n2.
s.i,j.
|dist(wi, wj)2 − dθ(gi, gj)2|,.
where s denotes all the text sequences in the train-ing corpus.
similarly, we train another parame-ter matrix θ2 to compute squared vector norms,dθ2(gi) = (θ2gi)t (θ2gi) that characterize the tree.
2in example shown in figure 2, token dependencies: [en:root → has → has → members → 315], and [es: root → for-mada → hay → senadores → 315] or corresponding part-of-speech tag sequence [verb → verb → noun → num])may help mbert to predict the correct answer..3this aligns with the ﬁndings of hewitt and manning(2019) as they showed 64 or 128 dimension of the contextualrepresentations are sufﬁcient to capture the syntax structure..4541taskdatasetclassiﬁcationxnliclassiﬁcationpaws-xqamlqaqaxquadnerwikiannnerconllmtopsemantic parsingmatis++ semantic parsing.
|dev|2.5k2k.
|test||train|5k392k2k49k34k 4.5k-11k87k119034k87k1k-10k20k10k1.5k-5k15k 2k-3k2.2k 2.8k-4.4k893490.
15.7k4.5k.
|lang|13771015459.metricaccuracyaccuracyf1 / exact matchf1 / exact matchf1f1exact matchexact match.
table 1: statistics of the evaluation datasets.
|train|, |dev| and |test| are the numbers of examples in the training,dev and test sets, respectively.
for train set, the number is for the source language, english, while for dev and testset, the number is for each target language.
|lang| is the number of target languages we consider for each task..depth of the words.
we train gat’s parametersand θ1, θ2 by minimizing the loss: l = ltask +α(ldist + ldepth), where α is weight for the treestructure prediction loss..pre-training gat unlike mbert’s parameters,gat’s parameters are trained from scratch duringtask-speciﬁc ﬁne-tuning.
for low-resource tasks,gat may not learn to encode the syntax structureaccurately.
therefore, we utilize the universal de-pendency parses (nivre et al., 2019) to pre-traingat on the source and target languages.
note that,the pre-training objective for gat is to predict thetree distances and depths as described above..3 experiment setup.
to study syntax-augmented mbert’s performancein a broader context, we perform an evaluation onfour nlp applications: text classiﬁcation, namedentity recognition, question answering, and task-oriented semantic parsing.
our evaluation focuseson assessing the usefulness of utilizing universalsyntax in the zero-shot cross-lingual transfer..3.1 evaluation tasks.
text classiﬁcation we conduct experiments ontwo widely used cross-lingual text classiﬁcationtasks: (i) natural language inference and (ii) para-phrase detection.
we use the xnli (conneau et al.,2018) and paws-x (yang et al., 2019) datasetsfor the tasks, respectively.
in both tasks, a pair ofsentences is given as input to mbert.
we combinethe dependency tree structure of the two sentencesby adding two edges from the [cls] token to theroots of the dependency trees..named entity recognition is a structure predic-tion task that requires to identify the named enti-ties mentioned in the input sentence.
we use the.
wikiann dataset (pan et al., 2017) and a subset oftwo tasks from conll-2002 (tjong kim sang,2002) and conll-2003 ner (tjong kim sangand de meulder, 2003).
we collect the conlldatasets from xglue (liang et al., 2020).
in bothdatasets, there are 4 types of named entities: per-son, location, organization, and miscellaneous.4.
question answering we evaluate on two cross-lingual question answering benchmarks, mlqa(lewis et al., 2020), and xquad (artetxe et al.,2020).
we use the squad dataset (rajpurkar et al.,2016) for training and validation.
in the qa task,the inputs are a question and a context passage thatconsists of many sentences.
we formulate qa as amulti-sentence reading comprehension task; jointlytrain the models to predict the answer sentence andextract the answer span from it.
we concatenatethe question and each sentence from the contextpassage and use the [cls] token representationto score the candidate sentences.
we adopt theconﬁdence method from clark and gardner (2018)and pick the highest-scored sentence to extract theanswer span during inference.
we provide moredetails of the qa models in appendix..task-oriented semantic parsing the fourthevaluation task is cross-lingual task-oriented se-mantic parsing.
in this task, the input is a userutterance and the goal is to predict the intent of theutterance and ﬁll the corresponding slots.
we con-duct experiments on two recently proposed bench-marks: (i) mtop (li et al., 2021) and (ii) matis++(xu et al., 2020).
we jointly train the bert modelsas suggested in chen et al.
(2019)..we summarize the evaluation task benchmark.
datasets and evaluation metrics in table 1..4miscellaneous entity type covers named entities that do.
not belong to the other three types.
4542ko.
ja.
nl.
pt avg.
tr.
fr.
el.
ar.
hi.
vi.
ru.
es.
ur.
de.
zh.
en.
---.
---.
---.
---.
---.
---.
---.
85.785.785.9.
87.4 87.088.4 87.689.1 88.2.modelbgclassiﬁcation - xnli (conneau et al., 2018)[1]80.8 64.3 68.0 70.0 65.3 73.5 73.4 58.9 67.8 60.9 57.2 69.3 67.8mbert 81.8 63.8 68.0 70.7 65.4 73.8 72.4 59.3 68.4 60.7 56.7 68.6 67.8+ syn.
81.6 65.4 69.3 70.7 66.5 74.1 73.2 60.5 68.8 62.4 58.7 69.9 69.3classiﬁcation - paws-x (yang et al., 2019)-[1]94.0-mbert 93.9-+ syn.
94.0ner - wikiann (pan et al., 2017)[1]85.2 41.1 77.0 78.0 72.5 77.4 79.6 65.0 64.0 71.8 36.9 71.8-mbert 83.6 38.8 77.0 76.0 70.4 74.7 78.9 63.4 63.5 70.9 37.7 73.5-+ syn.
84.4 40.0 77.0 77.0 71.5 76.1 79.3 64.2 63.8 71.4 37.3 72.7-ner - conll (tjong kim sang, 2002; tjong kim sang and de meulder, 2003)-75.4[2]90.6--74.5mbert 90.7--73.6-90.6+ syn.
qa - mlqa (lewis et al., 2020)-[3]77.7 45.7-mbert 80.5 47.2+ syn.
80.4 48.9-qa - xquad (artetxe et al., 2020)70.6 62.6 75.5[1]83.5 61.568.9 60.2 71.1mbert 84.2 54.871.4 61.3 72.8+ syn.
84.0 55.5semantic parsing - mtop (li et al., 2021)mbert 81.0--+ syn.
81.3semantic parsing - matis++ (xu et al., 2020)-mbert 86.0-+ syn.
86.2.
59.2 71.3 55.455.7 68.6 48.954.6 68.4 49.8.
69.5 58.064.0 57.267.6 56.1.
57.1 57.556.5 56.659.3 60.1.
43.7 36.9 16.244.5 38.9 18.7.
40.2 38.8 9.843.0 41.2 11.5.
43.847.546.7.
69.268.369.1.
57.959.060.8.
64.363.965.9.
28.130.0.
38.140.1.
7.88.0.
1.31.5.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
---.
--.
--.
--.
--.
--.
--.
--.
--.
--.
--.
--.
--.
---.
---.
---.
---.
--.
--.
77.0 69.6 73.078.0 73.6 73.180.7 76.3 75.8.
59.659.359.3.
81.8 80.8 69.581.9 78.7 68.581.9 79.0 69.0.
67.567.568.5.
82.082.984.3.
78.277.878.0.
57.758.760.3.
66.763.464.2.
39.641.4.
---.
---.
---.
---.
---.
--.
---.
---.
---.
---.
---.
--.
77.977.678.5.
---.
---.
---.
---.
--.
--.
28.227.3.
38.2 32.937.3 33.6.table 2: cross-lingual transfer results for all the evaluation tasks (on test set) across 17 languages.
we report f1score for the question answering (qa) datasets (for other datasets, see table 1).
we train and evaluate mbert onthe same pre-processed datasets and considers its performance as the baseline (denoted by “mbert” rows in thetable) for syntax-augmented mbert (denoted by “+ syn.” rows in the table).
bold-faced values indicate that thesyntax-augmented mbert is statistically signiﬁcantly better (by paired bootstrap test, p < 0.05) than the baseline.
we include results from published works ([1]: hu et al.
(2020), [2]: liang et al.
(2020), and [3]: lewis et al.
(2020)) as a reference.
except for the qa datasets, all our results are averaged over three different seeds..3.2.implementation details.
we collect the universal part-of-speech tags and thedependency parse of sentences by pre-processingthe datasets using udpipe.5 we ﬁne-tune mberton the pre-processed datasets and consider it asthe baseline for our proposed syntax-augmentedmbert.
we extend the xtreme framework(hu et al., 2020) that is developed based ontransformers api (wolf et al., 2020).
we usethe same hyper-parameter setting for mbert mod-els, as suggested in xtreme.
for the graph at-.
5https://ufal.mff.cuni.cz/udpipe/2.
tention network (gat), we set lg = 4, k = 4,and dg = 64 (resulting in ∼0.5 million parame-ters).
we tune δ6 (shown in eq.
(3)) and α (weightof the tree structure prediction loss) in the range[1, 2, 4, 8] and [0.5 − 1.0], respectively.
we detailthe hyper-parameters in the appendix..6we observed that the value of δ depends on the down-stream task and the source language.
for example, a larger δvalue is beneﬁcial for tasks taking a pair of text sequences asinputs, while a smaller δ value results in better performancesfor tasks taking single text input.
experiments on paws-xusing each target language as the source language indicatethat δ should be set to a larger value for source language withlonger text sequences (e.g., arabic) and vice versa..4543s1/s2endeesfrjakozh.
en-0.51.00.95.23.15.8.de0.7-2.11.75.32.85.5.es1.62.0-1.95.64.36.3.fr1.42.11.7-5.13.96.0.
(a) paws-x.
ja4.75.14.65.0-6.46.1.ko2.53.53.02.75.9-4.5.zh5.45.96.65.45.15.1-.
q/cenesdedehivizh.
en-4.13.51.81.05.63.8.es-0.2-2.82.41.84.53.3.hi0.95.32.9-0.1-4.20.9.vi0.67.34.06.2-0.6-5.4.zh1.17.65.04.41.05.5-.
de0.33.5-1.10.55.54.4.ar0.45.44.0-0.26.92.4.
(b) mlqa.
table 3: the performance difference between syntax-augmented mbert and mbert in the generalized cross-lingual transfer setting.
the rows and columns indicate (a) language of the ﬁrst and second sentences in thecandidate pairs and (b) context and question languages.
the gray cells have a value greater than or equal to theaverage performance difference, which is 3.9 and 3.1 for (a) and (b)..4 experiment results.
we aim to address the following questions..1. does augmenting mbert with syntax im-prove (generalized) cross-lingual transfer?
2. does incorporating syntax beneﬁt speciﬁc lan-.
guages or language families?.
3. which nlp tasks or types of tasks get more.
beneﬁts from utilizing syntax?.
4.1 cross-lingual transfer.
experiment results to compare mbert and syntax-augmented mbert are presented in table 2. over-all, the incorporation of language syntax in mbertimproves cross-lingual transfer for the downstreamtasks, in many languages by a signiﬁcant margin(p < 0.05, t-test).
the average performancesacross all languages on xnli, paws-x, mlqa,and mtop benchmarks improve signiﬁcantly (by atleast 1 point).
on the other benchmarks: wikiann,conll, xquad, and matis++, the average per-formance improvements are 0.5, 0.2, 0.8, and 0.7points, respectively.
note that the performancegains in the source language (english) for all thedatasets except wikiann is ≤ 0.3. this indicatesthat cross-lingual transfer gains are not due to im-proving the downstream tasks, but instead, lan-guage syntax helps to transfer across languages..4.2 generalized cross-lingual transfer.
in the generalized cross-lingual transfer setting(lewis et al., 2020), the input text sequences forthe downstream tasks (e.g., text classiﬁcation, qa)may come from different languages.
as shown infigure 2, given the context passage in english, amultilingual qa model should answer the questionwritten in spanish.
due to the parallel nature of.
the existing benchmark datasets: xnli, paws-x,mlqa, and xquad, we evaluate mbert and its’syntax-augmented variant on the generalized cross-lingual transfer setting.
the results for paws-xand mlqa are presented in table 3 (results for theother datasets are provided in appendix)..in both text classiﬁcation and qa benchmarks,we observe signiﬁcant improvements for most lan-guage pairs.
in the paws-x text classiﬁcation task,language pairs with different typologies (e.g., en-ja,en-zh) have the most gains.
when chinese (zh) orjapanese (ja) is in the language pairs, the perfor-mance is boosted by at least 4.5%.
the dataset char-acteristics explain this; the task requires modelingstructure, context, and word order information.
onthe other hand, in the xnli task, the performancegain pattern is scattered, and this is perhaps syntaxplays a less signiﬁcant role in the xnli task.
thelargest improvements result when the languagesof the premise and hypothesis sentences belong to{bulgarian, chinese} and {french, arabic}..in both qa datasets, syntax-augmented mbertboosts performance when the question and contextlanguages are typologically different except thehindi language.
surprisingly, we observe a largeperformance gain when questions in spanish andgerman are answered based on the english context.
based on our manual analysis on mlqa, we sus-pect that although questions in spanish and germanare translated from english questions (by human),the context passages are from wikipedia that oftenare not exact translation of the corresponding en-glish passage.
take the context passages in figure2 as an example.
we anticipate that syntactic clueshelp a qa model in identifying the correct answerspan when there are more than one semanticallyequivalent and plausible answer choices..4544figure 4: performance improvements for xnli, wikiann, mlqa, and matis++ across languages.the languagesin x-axis are grouped by language families: ie.germanic (nl, de), ie.romance (pt, fr, es), ie.slavic (ru, bg),ie.greek (el), ie.indic (hi, ur), afro-asiatic (ar, vi), altaic (tr), sino-tibetan (zh), korean (ko), and japanese (ja)..4.3 analysis & discussion.
we discuss and analyze our ﬁndings on the follow-ing points based on the empirical results..impact on languages we study if ﬁne-tuningsyntax-augmented mbert on english (source lan-guage) impacts speciﬁc target languages or fami-lies of languages.
we show the performance gainson the target languages grouped by their familiesin four downstream tasks in figure 4. there isno observable trend in the overall performance im-provements across tasks.
however, the xnli curveweakly indicates that when target languages aretypologically different from the source language,there is an increase in the transfer performance(comparing left half to the right half of the curve)..impact of pre-training gat before ﬁne-tuningsyntax-augmented mbert, we pre-train gat onthe 17 target languages (discussed in § 2.4).
in ourexperiments, we observe such pre-training boostssemantic parsing performance, while there is a littlegain on the classiﬁcation and qa tasks.
we also ob-serve that pre-training gat diminishes the gain ofﬁne-tuning with the auxiliary objective (predictingthe tree structure).
we hypothesize that pre-trainingor ﬁne-tuning gat using auxiliary objective helpswhen there is limited training data.
for example,semantic parsing benchmarks have a small numberof training examples, while xnli has many.
asa result, the improvement due to pre-training orﬁne-tuning gat in the semantic parsing tasks issigniﬁcant, and in the xnli task, it is marginal..discussion to foster research in this direction,we discuss additional experiment ﬁndings..• a natural question is, instead of using gat, whywe do not modify attention heads in mbert toembed the dependency structure (as shown in eq.
3).
we observed a consistent performance drop.
across all the tasks if we intervene in self-attention(blocking pair-wise attention).
we anticipate fusinggat encoded syntax representations helps as itadds bias to the self-attention.
for future works,we suggest exploring ways of adding structure bias,e.g., scaling attention weights based on dependencystructure (bugliarello and okazaki, 2020)..• among the evaluation datasets, wikiann consistsof sentence fragments, and the semantic parsingbenchmarks consist of user utterances that are typi-cally short in length.
sorting and analyzing the per-formance improvements based on sequence lengthssuggests that the utilization of dependency struc-ture has limited scope for shorter text sequences.
however, part-of-speech tags help to identify spanboundaries improving the slot ﬁlling tasks..4.4 limitations and challenges.
in this work, we assume we have access to an off-the-shelf universal parser, e.g., udpipe (strakaand strakov´a, 2017) or stanza (qi et al., 2020)to collect part-of-speech tags and the dependencystructure of the input sequences.
relying on sucha parser has a limitation that it may not support allthe languages available in benchmark datasets, e.g.,we do not consider thai and swahili languages inthe benchmark datasets..there are a couple of challenges in utilizing theuniversal parsers.
first, universal parsers tokenizethe input sequence into words and provide part-of-speech tags and dependencies for them.
thetokenized words may not be a part of the input.7 asa result, tasks requiring extracting text spans (e.g.,qa) need additional mapping from input tokens towords.
second, the parser’s output word sequenceis tokenized into wordpieces that often results in.
7for example, in the german sentence “wir gehen zumkino” (we are going to the cinema), the token “zum” is decom-posed into words “zu” and “dem”..4545nldeptfresrubgelhiurarvitrzhkoja101234xnliwikiannmlqamatis++inconsistent wordpieces resulting in degeneratedperformance in the downstream tasks.8.
5 related work.
encoding syntax for language transfer uni-versal language syntax, e.g., part-of-speech (pos)tags, dependency parse structure, and relationsare shown to be helpful for cross-lingual trans-fer (kozhevnikov and titov, 2013; praˇz´ak andkonop´ık, 2017; wu et al., 2017; subburathinamet al., 2019; liu et al., 2019; zhang et al., 2019;xie et al., 2020; ahmad et al., 2021).
many ofthese prior works utilized graph neural networks(gnn) to encode the dependency graph structureof the input sequences.
in this work, we utilizegraph attention networks (gat) (veliˇckovi´c et al.,2018), a variant of gnn that employs the multi-head attention mechanism..syntax-aware multi-head attention a largebody of prior works investigated the advantagesof incorporating language syntax to enhance theself-attention mechanism (vaswani et al., 2017).
existing techniques can be broadly divided into twotypes.
the ﬁrst type of approach relies on an exter-nal parser (or human annotation) to get a sentence’sdependency structure during inference.
this typeof approaches embed the dependency structure intocontextual representations (wu et al., 2017; chenet al., 2017; wang et al., 2019a,b; zhang et al.,2019, 2020; bugliarello and okazaki, 2020; sachanet al., 2021; ahmad et al., 2021).
our proposedmethod falls under this category; however, unlikeprior works, our study investigates if fusing the uni-versal dependency structure into the self-attentionof existing multilingual encoders help cross-lingualtransfer.
graph attention networks (gats) that usemulti-head attention has also been adopted for nlptasks (huang and carley, 2019) also fall into thiscategory.
the second category of approaches doesnot require the syntax structure of the input textduring inference.
these approaches are trained topredict the dependency parse via supervised learn-ing (strubell et al., 2018; deguchi et al., 2019)..6 conclusion.
in this work, we propose incorporating universallanguage syntax into multilingual bert (mbert).
8this happen for languages, such as arabic as parsers nor-malize the input that lead to inconsistent characters betweeninput text and the output tokenized text..by infusing structured representations into its multi-head attention mechanism.
we employ a modiﬁedgraph attention network to encode the syntax struc-ture of the input sequences.
the results endorsethe effectiveness of our proposed approach in thecross-lingual transfer.
we discuss limitations andchallenges to drive future works..acknowledgments.
we thank yuqing tang for his insightful commentson our paper and anonymous reviewers for theirhelpful feedback.
we also thank ucla-nlp groupfor helpful discussions and comments..broader impact.
in today’s world, the number of speakers for somelanguages is in billions, while it is only a few thou-sands for many languages.
as a result, a few lan-guages offer large-scale annotated resources, whilefor many languages, there are limited or no labeleddata.
due to this disparity, natural language pro-cessing (nlp) is extremely challenging in the low-resourced languages.
in recent years, cross-lingualtransfer learning has achieved signiﬁcant improve-ments, enabling us to avail nlp applications to awide range of languages that people use across theworld.
however, one of the challenges in cross-lingual transfer is to learn the linguistic similarityand differences between languages and their cor-relation with the target nlp applications.
moderntransferable models are pre-trained on unlabeledhumongous corpora such that they can learn lan-guage syntax and semantic and encode them intouniversal representations.
such pre-trained modelscan beneﬁt from explicit incorporation of univer-sal language syntax during ﬁne-tuning for differentdownstream applications.
this work presents athorough study to analyze the pros and cons ofutilizing universal dependencies (ud) frameworkthat consists of grammar annotations across manyhuman languages.
our work can broadly impactthe development of cross-lingual transfer solutionsand making them accessible to people across theglobe.
in this work, we discuss the limitations andchallenges in utilizing universal parsers to beneﬁtthe pre-trained models.
among the negative as-pects of our work is the lack of explanation thatwhy some languages get more beneﬁts over othersdue to universal syntax knowledge incorporation..4546references.
wasi ahmad, zhisong zhang, xuezhe ma, eduardhovy, kai-wei chang, and nanyun peng.
2019. ondifﬁculties of cross-lingual transfer with order differ-ences: a case study on dependency parsing.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 2440–2452,minneapolis, minnesota.
association for computa-tional linguistics..wasi uddin ahmad, nanyun peng, and kai-weichang.
2021. gate: graph attention transformerencoder for cross-lingual relation and event extrac-tion.
in proceedings of the thirty-fifth aaai con-ference on artiﬁcial intelligence..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 4623–4637, online.
asso-ciation for computational linguistics..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..emanuele bugliarello and naoaki okazaki.
2020. en-hancing machine translation with dependency-awarein proceedings of the 58th annualself-attention.
meeting of the association for computational lin-guistics, pages 1618–1627, online.
association forcomputational linguistics..huadong chen, shujian huang, david chiang, and ji-ajun chen.
2017.improved neural machine trans-lation with a syntax-aware encoder and decoder.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1936–1945, vancouver,canada.
association for computational linguistics..qian chen, zhu zhuo, and wen wang.
2019. bertfor joint intent classiﬁcation and slot ﬁlling.
arxivpreprint arxiv:1902.10909..ethan a. chi, john hewitt, and christopher d. man-ning.
2020.finding universal grammatical rela-tions in multilingual bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5564–5577, online.
as-sociation for computational linguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 845–855, melbourne,.
australia.
association for computational linguis-tics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..hiroyuki deguchi, akihiro tamura, and takashi ni-nomiya.
2019. dependency-based self-attention forin proceedings of the interna-transformer nmt.
tional conference on recent advances in naturallanguage processing (ranlp 2019), pages 239–246, varna, bulgaria.
incoma ltd..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gen-in proceedings of the 37th interna-eralization.
tional conference on machine learning, volume119, pages 4411–4421.
pmlr..binxuan huang and kathleen carley.
2019. syntax-level sentiment classiﬁcation withaware aspectin proceedings of thegraph attention networks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5469–5477, hong kong,china.
association for computational linguistics..4547ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structurein proceedings of the 57th annualof language?
meeting of the association for computational lin-guistics, pages 3651–3657, florence, italy.
associa-tion for computational linguistics..karthikeyan k, zihan wang, stephen mayhew, anddan roth.
2020. cross-lingual ability of multilin-gual bert: an empirical study.
in international con-ference on learning representations..mikhail kozhevnikov and ivan titov.
2013. cross-lingual transfer of semantic role labeling models.
in proceedings of the 51st annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1190–1200, soﬁa, bulgaria.
association for computational linguistics..patrick lewis, barlas oguz, ruty rinott, sebastianriedel, and holger schwenk.
2020. mlqa: evalu-ating cross-lingual extractive question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7315–7330, online.
association for computational lin-guistics..haoran li, abhinav arora, shuohui chen, anchitgupta, sonal gupta, and yashar mehdad.
2021.mtop: a comprehensive multilingual task-orientedsemantic parsing benchmark.
in proceedings of the16th conference of the european chapter of theassociation for computational linguistics: mainvolume, pages 2950–2962, online.
association forcomputational linguistics..yaobo liang, nan duan, yeyun gong, ning wu, fen-fei guo, weizhen qi, ming gong, linjun shou,daxin jiang, guihong cao, xiaodong fan, ruofeizhang, rahul agrawal, edward cui, sining wei,taroon bharti, ying qiao, jiun-hung chen, winniewu, shuguang liu, fan yang, daniel campos, ran-gan majumder, and ming zhou.
2020. xglue: anew benchmark datasetfor cross-lingual pre-training,understanding and generation.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 6008–6018,online.
association for computational linguistics..jian liu, yubo chen, kang liu, and jun zhao.
2019.neural cross-lingual event detection with minimalparallel resources.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 738–748, hong kong, china.
as-sociation for computational linguistics..joakim nivre, mitchell abrams,.
ˇzeljko agi´c, larsahrenberg, gabriel˙e aleksandraviˇci¯ut˙e, lene an-tonsen, katya aplonova, maria jesus aranz-abe, gashaw arutie, masayuki asahara, lumaateyah, mohammed attia, aitziber atutxa, lies-beth augustinus, elena badmaeva, miguel balles-teros, esha banerjee, sebastian bank, verginica.
barbu mititelu, victoria basmov, john bauer, san-dra bellato, kepa bengoetxea, yevgeni berzak,irshad ahmad bhat, riyaz ahmad bhat, ericabiagetti, eckhard bick, agn˙e bielinskien˙e, ro-gier blokland, victoria bobicev, lo¨ıc boizou,emanuel borges v¨olker, carl b¨orstell, cristinabosco, gosse bouma, sam bowman, adrianeboyd, kristina brokait˙e, aljoscha burchardt, mariecandito, bernard caron, gauthier caron, g¨uls¸encebiro˘glu eryi˘git, flavio massimiliano cecchini,ˇc´epl¨o, savasgiuseppe g. a. celano, slavom´ırcetin, fabricio chalub, jinho choi, yongseok cho,jayeol chun, silvie cinkov´a, aur´elie collomb,c¸ a˘grı c¸ ¨oltekin, miriam connor, marine courtin,elizabeth davidson, marie-catherine de marneffe,valeria de paiva, arantza diaz de ilarraza, carlydickerson, bamba dione, peter dirix, kaja do-brovoljc, timothy dozat, kira droganova, puneetdwivedi, hanne eckhoff, marhaba eli, ali elkahky,binyam ephrem, tomaˇz erjavec, aline etienne,rich´ard farkas, hector fernandez alcalde, jenniferfoster, cl´audia freitas, kazunori fujita, katar´ınagajdoˇsov´a, daniel galbraith, marcos garcia, moag¨ardenfors, sebastian garza, kim gerdes, filipginter, iakes goenaga, koldo gojenola, memduhg¨okırmak, yoav goldberg, xavier g´omez guino-vart, berta gonz´alez saavedra, matias grioni, nor-munds gr¯uz¯ıtis, bruno guillaume, c´eline guillot-barbance, nizar habash, jan hajiˇc, jan hajiˇc jr.,linh h`a m˜y, na-rae han, kim harris, daghaug, johannes heinecke, felix hennig, barborahladk´a, jaroslava hlav´aˇcov´a, florinel hociung, pet-ter hohle, jena hwang, takumi ikeda, radu ion,elena irimia, o. l´aj´ıd´e ishola, tom´aˇs jel´ınek, an-ders johannsen, fredrik jørgensen, h¨uner kas¸ıkara,andre kaasen, sylvain kahane, hiroshi kanayama,jenna kanerva, boris katz, tolga kayadelen, jes-sica kenney, v´aclava kettnerov´a, jesse kirchner,arne k¨ohn, kamil kopacewicz, natalia kotsyba,jolanta kovalevskait˙e, simon krek, sookyoungkwak, veronika laippala, lorenzo lambertino, lu-cia lam, tatiana lando, septina dian larasati,john lee, phng lˆe h`ˆong,alexei lavrentiev,alessandro lenci, saran lertpradit, herman le-ung, cheuk ying li, josie li, keying li, kyung-tae lim, yuan li, nikola ljubeˇsi´c, olga logi-nova, olga lyashevskaya, teresa lynn, vivienmacketanz, aibek makazhanov, michael mandl,christopher manning, ruli manurung, c˘at˘alinam˘ar˘anduc, david mareˇcek, katrin marheinecke,h´ector mart´ınez alonso, andr´e martins,janmaˇsek, yuji matsumoto, ryan mcdonald, sarahmcguinness, gustavo mendonc¸a, niko miekka,margarita misirpashayeva, anna missil¨a, c˘at˘alinmititelu, yusuke miyao, simonetta montemagni,amir more, laura moreno romero, keiko sophiemori, tomohiko morioka, shinsuke mori, shigekimoro, bjartur mortensen, bohdan moskalevskyi,kadri muischnek, yugo murawaki, kaili m¨u¨urisep,pinkey nainwani, juan ignacio navarro hor˜niacek,anna nedoluzhko, gunta neˇspore-b¯erzkalne, lngnguy˜ˆen thi., huy`ˆen nguy˜ˆen thi.
minh, yoshi-hiro nikaido, vitaly nikolaev, rattima nitisaroj,.
4548hanna nurmi, stina ojala, ad´edayo.
ol´u`okun, maiomura, petya osenova, robert ¨ostling, lilja øvre-lid, niko partanen, elena pascual, marco passarotti,agnieszka patejuk, guilherme paulino-passos, an-gelika peljak-łapi´nska, siyao peng, cenel-augustoperez, guy perrier, daria petrova, slav petrov,jussi piitulainen, tommi a pirinen, emily pitler,barbara plank, thierry poibeau, martin popel,lauma pretkalnin¸a, sophie pr´evost, prokopis proko-pidis, adam przepi´orkowski, tiina puolakainen,sampo pyysalo, andriela r¨a¨abis, alexandre rade-maker, loganathan ramasamy, taraka rama, car-los ramisch, vinit ravishankar, livy real, sivareddy, georg rehm, michael rießler, erikarimkut˙e, larissa rinaldi, laura rituma, luisarocha, mykhailo romanenko, rudolf rosa, da-vide rovati, valentin ros, ca, olga rudina, jackrueter, shoval sadde, benoˆıt sagot, shadi saleh,alessio salomoni, tanja samardˇzi´c, stephanie sam-son, manuela sanguinetti, dage s¨arg, baiba saul¯ıte,yanin sawanakunanon, nathan schneider, sebastianschuster, djam´e seddah, wolfgang seeker, mojganseraji, mo shen, atsuko shimada, hiroyuki shirasu,muh shohibussirri, dmitry sichinava, natalia sil-veira, maria simi, radu simionescu, katalin simk´o,m´aria ˇsimkov´a, kiril simov, aaron smith, isabelasoares-bastos, carolyn spadine, antonio stella,milan straka, jana strnadov´a, alane suhr, umutsulubacak, shingo suzuki, zsolt sz´ant´o, dimataji, yuta takahashi, fabio tamburini, takaakitanaka, isabelle tellier, guillaume thomas, li-isi torga, trond trosterud, anna trukhina, reuttsarfaty, francis tyers, sumire uematsu, zdeˇnkaureˇsov´a, larraitz uria, hans uszkoreit, sowmyavajjala, daniel van niekerk, gertjan van no-ord, viktor varga, eric villemonte de la clerg-erie, veronika vincze, lars wallin, abigail walsh,jonathan north washington,jing xian wang,maximilan wendt, seyi williams, mats wir´en,christian wittern, tsegay woldemariam, tak-sumwong, alina wr´oblewska, mary yako, naoki ya-mazaki, chunxiao yan, koichi yasuoka, marat m.yavrumyan, zhuoran yu, zdenˇek ˇzabokrtsk´y, amirzeldes, daniel zeman, manying zhang,andhanzhi zhu.
2019. universal dependencies 2.4.lindat/clariah-cz digital library at the insti-tute of formal and applied linguistics ( ´ufal), fac-ulty of mathematics and physics, charles univer-sity..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1946–1958, vancouver,canada.
association for computational linguistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..ondˇrej praˇz´ak and miloslav konop´ık.
2017. cross-lingual srl based upon universal dependencies.
inproceedings of the international conference recentadvances in natural language processing, ranlp2017, pages 592–600, varna, bulgaria.
incomaltd..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..devendra sachan, yuhao zhang, peng qi, andwilliam l. hamilton.
2021. do syntax trees helppre-trained transformers extract information?
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 2647–2661, online.
association for computational linguistics..milan straka and jana strakov´a.
2017. tokenizing,pos tagging, lemmatizing and parsing ud 2.0 withudpipe.
in proceedings of the conll 2017 sharedtask: multilingual parsing from raw text to univer-sal dependencies, pages 88–99, vancouver, canada.
association for computational linguistics..patrick verga, daniel andor,emma strubell,david weiss,and andrew mccallum.
2018.linguistically-informed self-attention for semanticin proceedings of the 2018 confer-role labeling.
ence on empirical methods in natural languageprocessing, pages 5027–5038, brussels, belgium.
association for computational linguistics..ananya subburathinam, di lu, heng ji, jonathanmay, shih-fu chang, avirup sil, and clare voss.
cross-lingual structure transfer for rela-2019.in proceedings of thetion and event extraction.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language process-ing (emnlp-ijcnlp), pages 313–325, hong kong,china.
association for computational linguistics..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
in.
4549weijia xu, batool haider, and saab mansour.
2020.end-to-end slot alignment and recognition for cross-in proceedings of the 2020 confer-lingual nlu.
ence on empirical methods in natural languageprocessing (emnlp), pages 5052–5063, online.
as-sociation for computational linguistics..yinfei yang, yuan zhang, chris tar, and jasonpaws-x: a cross-lingual ad-baldridge.
2019.versarial dataset for paraphrase identiﬁcation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3687–3692, hong kong, china.
association for computa-tional linguistics..yue zhang, rui wang, and luo si.
2019. syntax-enhanced self-attention-based semantic role label-in proceedings of the 2019 conference oning.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages616–626, hong kong, china.
association for com-putational linguistics..zhuosheng zhang, yuwei wu, junru zhou, sufengduan, hai zhao, and rui wang.
2020. sg-net:syntax-guided machine reading comprehension.
inproceedings of the thirty-fourth aaai conferenceon artiﬁcial intelligence..proceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
2018. graph attention networks.
in internationalconference on learning representations..chengyi wang, shuangzhi wu, and shujie liu.
source dependency-aware transformerarxiv preprint.
2019a.
with supervised self-attention.
arxiv:1909.02273..xing wang, zhaopeng tu, longyue wang, and shum-self-attention with structuraling shi.
2019b.
in proceedings of theposition representations.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1403–1409, hong kong,china.
association for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..shuangzhi wu, ming zhou, and dongdong zhang.
improved neural machine translation with2017.in proceedings of the twenty-sixthsource syntax.
international joint conference on artiﬁcial intelli-gence, ijcai-17, pages 4179–4185..zhiwen xie, runjie zhu, kunsong zhao, jin liu,guangyou zhou, and jimmy xiangji huang.
2020.a contextual alignment enhanced cross graph atten-tion network for cross-lingual entity alignment.
inproceedings of the 28th international conferenceon computational linguistics, pages 5918–5928,barcelona, spain (online).
international committeeon computational linguistics..4550supplementary material: appendices.
syntax-augmented mbert and mbert on thegeneralized cross-lingual transfer on xnli andxquad is presented in table 12 and 13..different source languagesin our study, weprimarily use english as the source language astraining examples used in all the benchmarks are inenglish.
however, authors of many of these bench-marks released translated-train examples in the tar-get languages.
this allows us to train mbertand syntax-augmented mbert in different lan-guages (as source) and examine how it impactscross-lingual transfer.
we perform experiments onpaws-x task and present the results in figure 5.we observe the largest transfer performance im-provements when english and german are used asthe source language.
the improvements are rela-tively smaller when japanese, korean, and chineselanguages are used as the source language.
we sus-pect that the dependency parser may not accuratelyparse translated sentences, and as a result, we donot see an explainable trend in the improvements..a model implementations.
we follow the standard way to model text classiﬁca-tion, named entity recognition, and task-oriented se-mantic parsing using mbert.
however, since ourproposed model uses the input sentences’ depen-dency structure, we frame question answering (qa)as multi-sentence reading comprehension.
the in-put context is split into a list of sentences and trainthe mbert model to predict the answer sentenceand extract the answer span from the selected sen-tence following clark and gardner (2018).
we con-catenate the question and each sentence from thecontext passage and use the [cls] token represen-tation to score the candidate sentences.
we adoptthe shared-normalization approach from the “con-ﬁdence method” as suggested in clark and gard-ner (2018) and pick the highest-scored sentence toextract the answer span during inference.
our ap-proach of utilizing syntax can be extended to applyto passages directly.
to combine all the sentences’dependency structure in the passage, we can addedges from the [cls] token to the roots of all thesentences’ dependency tree.
however, would thatapproach work in practice requires empirical study,and we leave this as future work..b hyper-prameter details.
we present the hyper-parameter details in table 4..c additional experiment results.
cross-lingual transfer we provide the exactmatch (em) and f1 accuracy of the mlqa datasetin table 5.intent classiﬁcation accuracy, slotf1, and exact match (em) accuracy for the task-oriented semantic parsing is reported in table zero-shot cross-lingual transfer results for the evaluationtasks in table 6. we highlight the cross-lingualtransfer gap for mbert and syntax-augmentedmbert on the evaluation tasks in table 7..generalized cross-lingual transferin gener-alized cross-lingual transfer, we assume the taskinputs are a pair of text that belong to two differentlanguages, e.g., answering spanish question basedon an english context (lewis et al., 2020).
wepresent the generalized cross-lingual transfer per-formance of syntax-augmented mbert on xnli,mlqa, and xquad in table 8, 9, 10, and 11, re-spectively.
the performance differences between.
45514 (tuned on [2, 4, 8])4 (tuned on [1, 2, 4, 8])64 (tuned on [32, 64, 128])4 (classiﬁcation, qa), 1 (ner, semantic parsing).
graph attention network (gat)# layers (lg)# heads (k)dgδsyntax-augmented mbert# syntax-layers# syntax-headsα.
# epochs.
12 (tuned in the range 1 – 12)1 (xnli, paws-x, wikiann, conll, matis++), 2 (mlqa, xquad, mtop)0.0 (xnli), 0.2 (matis++), 0.5 (paws-x, wikiann, mlqa, xquad), 1.0(conll), 2.0 (mtop)3 (qa), 5 (classiﬁcation), 10 (ner, semantic parsing).
table 4: details of the hyper-parameters used during ﬁne-tuning syntax-augmented mbert..en.
modelslewis et al.
77.7/65.2 64.3/46.6 57.9/44.3 45.7/29.8 43.8/29.7 57.1/38.6 57.5/37.3 57.7/41.6mbert (ours) 80.5/67.2 63.9/44.1 59.0/43.3 47.2/28.6 47.5/32.1 56.5/35.2 57.8/33.0 58.9/40.580.4/67.3 65.9/47.1 60.8/45.1 48.9/30.3 46.7/32.4 59.3/38.0 60.1/35.5 60.3/42.2mbert+syn..avg.
de.
zh.
es.
vi.
hi.
ar.
table 5: zero-shot cross-lingual transfer performance (f1/em) of mbert on mlqa dataset..mtop (li et al., 2021).
matis++ (xu et al., 2020).
mbert (ours)90.063.462.160.230.7----61.3.
81.038.840.228.19.8----39.6.
95.563.868.758.241.2----65.5.mbert+syn.
90.264.162.959.431.4----61.6.
95.667.873.163.244.2----68.8.
81.341.243.030.011.5----41.4.mbert (ours)94.970.274.068.249.460.766.316.924.358.3.
86.036.943.738.116.228.238.21.37.832.9.
97.392.994.189.780.483.694.871.387.688.0.mbert+syn.
94.974.177.269.454.761.366.718.621.259.8.
86.238.944.540.118.727.337.31.58.033.6.
97.390.490.889.580.481.992.768.786.086.4.enfresdehijapttrzhavg..table 6: zero-shot cross-lingual task-oriented semantic parsing results.
the values for each model indicates intentaccuracy, slot f1, and exact match, respectively..modelmbert (ours)mbert+syn..xnli paws-x wikiann conll mlqa xquad mtop matis++15.514.2.
17.216.9.
16.115.7.
12.811.3.
23.222.1.
59.759.2.
25.423.5.
51.849.9.table 7: the cross-lingual transfer gap of mbert and syntax-augmented mbert on the evaluation tasks.
thetransfer gap is the difference between performance on the english test set and the other languages’ average perfor-mance.
a transfer gap of 0 indicates perfect cross-lingual transfer.
for the qa datasets, we use f1 scores..s1/s2endeesfrjakozh.
en-86.887.28665.669.371.4.de85.6-8281.864.567.368.1.es87.282-84.864.667.968.8.fr86.38285.5-64.267.768.9.ja66.16563.764.2-6967.5.ko6867.766.166.667.3-65.9.zh70.568.668.367.96866.4-.
table 8: generalized cross-lingual transfer performance of syntax-augmented mbert on paws-x.
the row andcolumn indicates the language of the input sentence pairs..4552p/henfresderuelbgartrhiurvizh.
en-72.572.571.169.36368.463.76061.159.965.966.8.fr70.1-68.665.764.559.86359.255.25555.160.258.9.es70.169-65.165.56164.659.954.954.754.159.358.4.de66.263.963-62.557.561.256.653.954.753.556.356.1.ru64.863.263.763.1-56.96455.851.953.750.755.554.8.el57.356.557.756.155.9-5753.551.75249.953.250.9.bg61.860.260.86062.756.9-54.252.552.34952.753.4.ar59.158.659.1585755.957.4-53.253.752.954.254.5.tr53.852.552.652.85150.65150.1-5048.647.848.8.hi53.552.151.553.251.249.852.550.950.4-54.649.749.4.ur51.249.948.550.648.247.848.149.748.453.2-46.847.2.vi64.462.461.560.358.856.658.855.853.35450.7-61.5.zh65.261.360.960.558.554.759.155.553.753.752.662.3-.
table 9: generalized cross-lingual transfer performance of syntax-augmented mbert on xnli.
the row andcolumn indicates the language of premise and hypothesis..q/cenesdearhivizh.
en80.469.369.247.041.656.258.5.es67.665.962.944.136.549.752.0.de63.158.160.841.335.847.549.0.ar53.347.849.548.932.440.641.0.hi55.146.250.635.346.739.838.8.vi64.056.056.238.935.159.353.0.zh59.952.252.340.833.847.960.1.table 10: f1 score for generalized cross-lingual transfer of syntax-augmented on mlqa.
columns show contextlanguage, rows show question language..q/cenesderuelarhitrvizh.
en84.267.166.663.748.647.039.138.753.554.1.es74.471.359.961.646.747.436.735.249.048.8.de70.758.069.258.042.941.337.133.944.245.9.ru65.155.656.069.941.742.333.931.143.945.6.el59.449.050.749.163.437.628.826.938.236.6.ar52.645.143.342.833.955.629.225.134.337.6.hi52.941.945.645.937.237.756.225.936.538.3.tr53.141.941.941.530.429.129.749.233.735.5.vi63.948.950.651.036.933.931.226.264.248.7.zh51.739.541.740.428.131.724.221.637.558.0.table 11: f1 score for generalized cross-lingual transfer for xquad.
columns show context language, rows showquestion language..4553p/henfresderuelbgartrhiurvizh.
en--0.5-0.40.30.80.71.10.80.60.91.00.81.7.q/cenesderuelarhitrvizh.
fr0.9-1.51.72.22.23.52.72.41.51.93.12.8.es0.21.1-1.21.41.82.61.82.01.51.72.02.3.de0.61.61.5-1.41.12.32.11.91.41.42.02.2.ru0.51.30.50.7-1.10.11.31.10.90.41.41.5.el0.61.20.90.71.5-1.41.21.50.60.61.41.3.bg0.61.61.51.41.21.8-1.81.51.40.61.82.2.ar0.92.31.92.22.21.82.3-1.61.51.22.41.9.tr1.21.81.91.72.01.71.72.0-1.31.31.41.6.hi0.81.61.71.01.61.42.11.51.7-1.61.21.4.ur1.10.91.00.81.71.41.51.61.51.0-1.31.6.vi1.22.01.71.21.81.42.91.81.71.10.7-3.0.zh1.31.41.61.21.81.32.61.61.41.21.02.1-.
en-0.34.44.41.8-0.20.61.13.34.54.0.es-1.01.63.90.20.8-0.11.64.35.14.4.de-0.33.72.31.61.32.4-1.54.06.95.4.ru0.94.33.1-0.34.21.42.03.84.83.7.el2.24.42.71.20.21.14.25.35.75.3.ar2.45.64.34.74.50.82.25.96.55.hi1.66.12.4-0.60.4-0.3-1.22.43.31.6.tr1.34.35.43.53.15.20.20.63.73.8.vi1.77.44.52.24.13.62.24.13.53.6.zh0.04.53.44.13.00.92.74.75.3-1.2.table 12: cross-lingual transfer performance difference between syntax-augmented mbert and mbert on thexnli dataset in the generalized setting.
the row and column indicates the language of premise and hypothesis.
the gray cell have a value ≥ 1.5 (average difference)..table 13: f1 score difference for generalized crosslingual transfer for xquad.
columns show context language,rows show question language.
the gray cells have a value ≥ 3.1 (average difference)..figure 5: zero-shot cross-lingual transfer performance difference between syntax-augmented mbert and mbertfor paws-x task using different languages as source..4554endeesfrjakozhsource languagesendeesfrjakozhtarget languages0.20.0-0.10.4-0.40.70.20.10.8-0.1-0.1-0.80.50.90.71.10.70.9-1.30.90.80.60.7-0.4-0.5-1.40.10.22.71.80.80.7-0.30.00.32.71.40.01.70.1-0.4-0.22.70.70.31.0-1.20.60.30.80.00.81.62.4