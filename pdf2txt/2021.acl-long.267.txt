g-transformer for document-level machine translation.
guangsheng bao1,2, yue zhang∗,1,2, zhiyang teng1,2, boxing chen3 and weihua luo31 school of engineering, westlake university2 institute of advanced technology, westlake institute for advanced study3 damo academy, alibaba group inc.{baoguangsheng, zhangyue, tengzhiyang}@westlake.edu.cn{boxing.cbx, weihua.luowh}@alibaba-inc.com.
abstract.
document-level mt models are still far fromsatisfactory.
existing work extend translationunit from single sentence to multiple sentences.
however, study shows that when we further en-large the translation unit to a whole document,supervised training of transformer can fail.
inthis paper, we ﬁnd such failure is not caused byoverﬁtting, but by sticking around local min-ima during training.
our analysis shows thatthe increased complexity of target-to-source at-tention is a reason for the failure.
as a solution,we propose g-transformer, introducing local-ity assumption as an inductive bias into trans-former, reducing the hypothesis space of theattention from target to source.
experimentsshow that g-transformer converges faster andmore stably than transformer, achieving newstate-of-the-art bleu scores for both non-pretraining and pre-training settings on threebenchmark datasets..(a) sentence-by-sentence translation.
(b) multi-sentence translation.
(c) g-transformer (doc-by-doc translation).
figure 1: overview of model structures for document-level machine translation..1.introduction.
document-level machine translation (mt) has re-ceived increasing research attention (gong et al.,2011; hardmeier et al., 2013; garcia et al., 2015;miculicich et al., 2018a; maruf et al., 2019; liuet al., 2020).
it is a more practically useful taskcompared to sentence-level mt because typical in-puts in mt applications are text documents ratherthan individual sentences.
a salient difference be-tween document-level mt and sentence-level mtis that for the former, much larger inter-sententialcontext should be considered when translatingeach sentence, which include discourse structuressuch as anaphora, lexical cohesion, etc.
studiesshow that human translators consider such contextswhen conducting document translation (hardmeier,2014; l¨aubli et al., 2018).
despite that neural mod-els achieve competitive performances on sentence-.
∗* corresponding author..level mt, the performance of document-level mtis still far from satisfactory..existing methods can be mainly classiﬁed intotwo categories.
the ﬁrst category translates a doc-ument sentence by sentence using a sequence-to-sequence neural model (zhang et al., 2018; mi-culicich et al., 2018b; maruf et al., 2019; zhenget al., 2020).
document-level context is integratedinto sentence-translation by introducing additionalcontext encoder.
the structure of such a modelis shown in figure 1(a).
these methods sufferfrom two limitations.
first, the context needs to beencoded separately for translating each sentence,which adds to the runtime complexity.
second,more importantly, information exchange cannot bemade between the current sentence and its docu-ment context in the same encoding module..the second category extends the translationunit from a single sentence to multiple sentences(tiedemann and scherrer, 2017; agrawal et al.,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3442–3455august1–6,2021.©2021associationforcomputationallinguistics3442context encodersource encodertarget decodersourcetranslationcontextsource encodertarget decodersource1translation1source2…translation2…source1translation1source2…translation2………①②③①②③2018; zhang et al., 2020) and the whole document(junczys-dowmunt, 2019; liu et al., 2020).
re-cently, it has been shown that when the translationunit increases from one sentence to four sentences,the performance improves (zhang et al., 2020;scherrer et al., 2019).
however, when the wholedocument is encoded as a single unit for sequenceto sequence translation, direct supervised traininghas been shown to fail (liu et al., 2020).
as asolution, either large-scale pre-training (liu et al.,2020) or data augmentation (junczys-dowmunt,2019) has been used as a solution, leading to im-proved performance.
these methods are shown infigure 1(b).
one limitation of such methods is thatthey require much more training time due to thenecessity of data augmentation..intuitively, encoding the whole input documentas a single unit allows the best integration of con-text information when translating the current sen-tence.
however, little work has been done investi-gating the underlying reason why it is difﬁcult totrain such a document-level nmt model.
one re-mote clue is that as the input sequence grows larger,the input becomes more sparse (pouget-abadieet al., 2014; koehn and knowles, 2017).
to gainmore understanding, we make dedicated experi-ments on the inﬂuence of input length, data scaleand model size for transformer (section 3), ﬁnd-ing that a transformer model can fail to convergewhen training with long sequences, small datasets,or big model size.
we further ﬁnd that for the failedcases, the model gets stuck at local minima duringtraining.
in such situation, the attention weightsfrom the decoder to the encoder are ﬂat, with largeentropy values.
this can be because that largerinput sequences increase the challenge for focusingon a local span to translate when generating eachtarget word.
in other words, the hypothesis spacefor target-to-source attention is increased..given the above observations, we investigate anovel extension of transformer, by restricting self-attention and target-to-source attention to a localcontext using a guidance mechanism.
as shown infigure 1(c), while we still encode the input docu-ment as a single unit, group tags 1(cid:13) 2(cid:13) 3(cid:13) are as-signed to sentences to differentiate their positions.
target-to-source attention is guided by matchingthe tag of target sentence to the tags of source sen-tences when translating each sentence, so that thehypothesis space of attention is reduced.
intuitively,the group tags serve as a constraint on attention,.
which is useful for differentiating the current sen-tence and its context sentences.
our model, namedg-transformer, can be thus viewed as a combina-tion of the method in figure 1(a) and figure 1(b),which fully separate and fully integrates a sentencebeing translated with its document level context,respectively..we evaluate our model on three commonlyused document-level mt datasets for english-german translation, covering domains of tedtalks, news, and europarl from small to large.
experiments show that g-transformer convergesfaster and more stably than transformer on dif-ferent settings, obtaining the state-of-the-art re-sults under both non-pretraining and pre-trainingsettings.
to our knowledge, we are the ﬁrstto realize a truly document-by-document transla-tion model.
we release our code and model athttps://github.com/baoguangsheng/g-transformer..2 experimental settings.
we evaluate transformer and g-transformer onthe widely adopted benchmark datasets (marufet al., 2019), including three domains for english-german (en-de) translation..ted.
the corpus is transcriptions of ted talksfrom iwslt 2017. each talk is used as a document,aligned at the sentence level.
tst2016-2017 is usedfor testing, and the rest for development..news.
this corpus uses news commentaryv11 for training, which is document-delimited andsentence-aligned.
newstest2015 is used for devel-opment, and newstest2016 for testing..europarl.
the corpus is extracted from eu-roparl v7, where sentences are segmented andaligned using additional information.
the train,dev and test sets are randomly split from the cor-pus..the detailed statistics of these corpora are shownin table 1. we pre-process the documents by split-ting them into instances with up-to 512 tokens, tak-ing a sentence as one instance if its length exceeds512 tokens.
we tokenize and truecase the sentenceswith moses (koehn et al., 2007) tools, applyingbpe (sennrich et al., 2016) with 30000 mergingoperations..we consider three standard model conﬁgura-.
tions..base model.
following the standard trans-former base model (vaswani et al., 2017), we use 6layers, 8 heads, 512 dimension outputs, and 2048.
3443language dataset.
#sentencestrain/dev/test0.21m/9k/2.3k0.24m/2k/3k.
#documentstrain/dev/test1.7k/92/226k/80/1541.67m/3.6k/5.1k 118k/239/359.
#instances avg #sents/inst avg #tokens/insttrain/dev/test436/428/429380/355/321320/326/323.
train/dev/test18.3/18.5/18.312.8/12.6/11.310.3/10.4/10.3.
train/dev/test11k/483/12318.5k/172/263162k/346/498.
tednewseuroparl.
en-de.
table 1: en-de datasets for evaluation..(a) input length (base modelwith ﬁltered data.).
(b) data scale (base modelwith 512 tokens input.).
figure 2: transformer on various input length and datascale..(a) failed model.
(b) successful model.
figure 3: loss curve of the models and the local min-ima..dimension hidden vectors..big model.
we follow the standard transformerbig model (vaswani et al., 2017), using 6 layers, 16heads, 1024 dimension outputs, and 4096 dimen-sion hidden vectors..large model.
we use the same settings ofbart large model (lewis et al., 2020), which in-volves 12 layers, 16 heads, 1024 dimension outputs,and 4096 dimension hidden vectors..we use s-bleu and d-bleu (liu et al., 2020)as the metrics.
the detailed descriptions are inappendix a..3 transformer and long inputs.
we empirically study transformer (see appendixb) on the datasets.
we run each experiment ﬁvetimes using different random seeds, reporting theaverage score for comparison..3.1 failure reproduction.
input length.
we use the base model and ﬁxeddataset for this comparison.
we split both the train-ing and testing documents from europarl datasetinto instances with input length of 64, 128, 256,512, and 1024 tokens, respectively.
for fair com-parison, we remove the training documents with alength of less than 768 tokens, which may favoursmall input length.
the results are shown in fig-ure 2a.
when the input length increases from 256tokens to 512 tokens, the bleu score drops dra-matically from 30.5 to 2.3, indicating failed train-ing with 512 and 1024 tokens.
it demonstrates thedifﬁculty when dealing with long inputs of trans-.
former..data scale.
we use the base model and a ﬁxedinput length of 512 tokens.
for each setting, werandomly sample a training dataset of the expectedsize from the full dataset of europarl.
the resultsare shown in figure 2b.
the performance increasessharply when the data scale increases from 20k to40k.
when data scale is equal or less than 20k, thebleu scores are under 3, which is unreasonablylow, indicating that with a ﬁxed model size andinput length, the smaller dataset can also causethe failure of the training process.
for data scalemore than 40k, the bleu scores show a widedynamic range, suggesting that the training processis unstable..model size.
we test transformer with differentmodel sizes, using the full dataset of europarl and aﬁxed input length of 512 tokens.
transformer-basecan be trained successfully, giving a reasonablebleu score.
however, the training of the big andlarge models failed, resulting in very low bleuscores under 3. it demonstrates that the increasedmodel size can also cause the failure with a ﬁxedinput length and data scale..the results conﬁrm the intuition that the per-formance will drop with longer inputs, smallerdatasets, or bigger models.
however, the bleuscores show a strong discontinuity with the changeof input length, data scale, or model size, fallinginto two discrete clusters.
one is successfullytrained cases with d-bleu scores above 10, andthe other is failed cases with d-bleu scores under3..3444-505101520253035d-bleutokens641282565121024-505101520253035d-bleuinstances1.25k2.5k5k10k20k40k80k160k246810120k2k4k6k8k10k12klossstepstrainvalid246810120k10k20k30k40k50k60klossstepstrainvalid(a) failed model.
(b) successful model.
figure 4: cross-attention distribution of transformershows that the failed model sticks at the local minima..(a) encoder.
(b) decoder.
figure 5: for the successful model, the attention distri-bution shrinks to narrow range (low entropy) and thenexpands to wider range (high entropy)..3.2 failure analysis.
training convergence.
looking into the failedmodels, we ﬁnd that they have a similar pattern onloss curves.
as an example of the model trainedon 20k instances shown in figure 3a, although thetraining loss continually decreases during trainingprocess, the validation loss sticks at the level of 7,reaching a minimum value at around 9k trainingsteps.
in comparison, the successfully trained mod-els share another pattern.
taking the model trainedon 40k instances as an example, the loss curvesdemonstrate two stages, which is shown in figure3b.
in the ﬁrst stage, the validation loss similarto the failed cases has a converging trend to thelevel of 7. in the second stage, after 13k trainingsteps, the validation loss falls suddenly, indicatingthat the model may escape successfully from localminima.
from the two stages of the learning curve,we conclude that the real problem, contradictingour ﬁrst intuition, is not about overﬁtting, but aboutlocal minima..attention distribution.
we further look intothe attention distribution of the failed models, ob-serving that the attentions from target to source arewidely spread over all tokens.
as figure 4a shows,the distribution entropy is high for about 8.14 bitson validation.
in contrast, as shown in figure 4b,the successfully trained model has a much lowerattention entropy of about 6.0 bits on validation.
furthermore, we can see that before 13k training.
source: <s> the commission shares ... of the european union institu-.
tional framework .
</s>.
<s> commission participation is expressly.
1.provided for ... of all its preparatory bodies .
</s>.
<s> only in excep-.
2.tional circumstances ... be excluded from these meetings .
</s>.
....3.target: <s> die kommission teilt die ansicht ... des institutionellen.
rahmens der europischen union ist .
</s>.
<s> die geschftsordnung.
1.des rates ... der kommission damit ausdrcklich vor .
</s>.
<s> die.
kommission kann nur ... wobei fallweise zu entscheiden ist .
</s>.
....2.
3.figure 6: example of english-german translation withgroup alignments..steps, the entropy sticks at a plateau, conﬁrmingwith the observation of the local minima in figure3b.
it indicates that the early stage of the trainingprocess for transformer is difﬁcult..figure 5 shows the self-attention distributionsof the successfully trained models.
the attentionentropy of both the encoder and the decoder dropsfast at the beginning, leading to a shrinkage ofthe attention range.
but then the attention entropygradually increases, indicating an expansion of theattention range.
such back-and-forth oscillationof the attention range may also result in unstabletraining and slow down the training process..3.3 conclusion.
the above experiments show that training failureon transformer can be caused by local minima.
additionally, the oscillation of attention range maymake it worse.
during training process, the atten-tion module needs to identify relevant tokens fromwhole sequence to attend to.
assuming that thesequence length is n , the complexity of the at-tention distribution increases when n grows fromsentence-level to document-level..we propose to use locality properties (rizzi,2013; hardmeier, 2014; jawahar et al., 2019) ofboth the language itself and the translation task asa constraint in transformer, regulating the hypoth-esis space of the self-attention and target-to-sourceattention, using a simple group tag method..4 g-transformer.
an example of g-transformer is shown in fig-ure 6, where the input document contains morethan 3 sentences.
as can be seen from the ﬁgure,g-transformer extends transformer by augment-ing the input and output with group tags (bao andzhang, 2021).
in particular, each token is assigneda group tag, indicating its sentential index.
while.
34457.67.888.28.40k2k4k6k8k10k12kentropy (bit)stepstrainvalid567890k10k20k30k40k50k60kentropy (bit)stepstrainvalid4567890k10k20k30k40k50k60kentropy (bit)stepstrainvalid4.655.45.86.26.670k10k20k30k40k50k60kentropy (bit)stepstrainvalidsource group tags can be assigned deterministically,target tags are assigned dynamically according towhether a generated sentence is complete.
start-ing from 1, target words copy group tags from itspredecessor unless the previous token is </s>, inwhich case the tag increases by 1. the tags serve asa locality constraint, encouraging target-to-sourceattention to concentrate on the current source sen-tence being translated..formally, for a source document x and a targetdocument y , the probability model of transformercan be written as.
ˆy = arg max.
p (y |x),.
y.
(1).
and g-transformer extends it by having.
ˆy = argy maxy,gy.
p (y, gy |x, gx ),.
(2).
where gx and gy denotes the two sequences ofgroup tags.
gx = {gi = k if wi ∈ sentxgy = {gj = k if wj ∈ senty.
k else 0}||x|i=1,k else 0}||y |j=1,.
(3).
where sentk represents the k-th sentence of xor y .
for the example shown in figure 6,gx = {1, ..., 1, 2, ..., 2, 3, ..., 3, 4, ...} and gy ={1, ..., 1, 2, ..., 2, 3, ..., 3, 4, ...}..group tags inﬂuence the auto-regressive transla-tion process by interfering with the attention mech-anism, which we show in the next section.
in g-transformer, we use the group-tag sequence gxand gy for representing the alignment between xand y , and for generating the localized contextualrepresentation of x and y ..4.1 group attention.
an attention module can be seen as a function map-ping a query and a set of key-value pairs to an out-put (vaswani et al., 2017).
the query, key, value,and output are all vectors.
the output is computedby summing the values with corresponding atten-tion weights, which are calculated by matchingthe query and the keys.
formally, given a set ofqueries, keys, and values, we pack them into matrixq, k, and v , respectively.
we compute the matrixoutputs.
attention(q, k, v ) = softmax.
(4).
(cid:18) qk t√dk.
(cid:19).
v,.
where dk is the dimensions of the key vector..attention allows a model to focus on differentpositions.
further, multi-head attention (mha).
allows a model to gather information from differentrepresentation subspaces.
mha(q, k, v ) = concat(head1, ..., headh)w o,, v w v.headi = attention(qw q.i , kw k.i.i ),.
(5).
where the projections of w o, w qiare parameter matrices.., w ki., and w vi.we update eq 4 using group-tags, naming itgroup attention (groupattn).
in addition to inputsq, k, and v , two sequences of group-tag inputsare involved, where gq corresponds to q and gkcorresponds to k. we have.
groupattn(args) = softmax.
+ m (gq, gk ).
v,.
args = (q, k, v, gq, gk ),.
(cid:18) qk t√dk.
(cid:19).
(6).
where function m (·) works as an attention mask,excluding all tokens outside the sentence.
speciﬁ-cally, m (·) gives a big negative number γ to makesoftmax close to 0 for the tokens with a differentgroup tag compared to current token.
m (gq, gk ) = min(1, abs(gqi t.k − iqgt.
k )) ∗ γ,.
(7).
where ik and iq are constant vectors with value1 on all dimensions, that ik has dimensions equalto the length of gk and iq has dimensions equalto the length of gq.
the constant value γ cantypically be −1e8..similar to eq 5, we use group multi-head atten-.
args = (q, k, v, gq, gk ),groupmha(args) = concat(head1, ..., headh)w o,.
(8).
tion.
where.
headi = groupattn(qw q.i , kw k.i., v w v.i , gq, gk ),.
and the projections of w o, w qiare parameter matrices..(9), and w vi., w ki.encoder.
for each layer a group multi-head at-tention module is used for self-attention, assigningthe same group-tag sequence for the key and thevalue that gq = gk = gx ..decoder.
we use one group multi-head attentionmodule for self-attention and another group multi-head attention module for cross-attention.
similarto the encoder, we assign the same group-tag se-quence to the key and value of the self-attention,that gq = gk = gy , but use different group-tagsequences for cross-attention that gq = gy andgk = gx ..3446method.
ted.
newss-bleu d-bleu s-bleu d-bleu s-bleu d-bleu.
europarl.
sentnmt (vaswani et al., 2017)han (miculicich et al., 2018b)san (maruf et al., 2019)hybrid context (zheng et al., 2020)flat-transformer (ma et al., 2020)transformer on sent (baseline)transformer on doc (baseline)g-transformer random initialized (ours)g-transformer ﬁne-tuned on sent transformer (ours).
flat-transformer+bert (ma et al., 2020)g-transformer+bert (ours)transformer on sent ﬁne-tuned on bart (baseline)transformer on doc ﬁne-tuned on bart (baseline)g-transformer ﬁne-tuned on bart (ours).
23.1024.5824.4225.1024.8724.82-23.5325.12.
26.6126.8127.78-28.06.
------0.7625.84*27.17*.
---28.2930.03*.
22.4025.0324.8424.9123.5525.19-23.5525.52.
24.5226.1429.90-30.34*.
------0.6025.23*27.11*.
---30.4931.71*.
29.4028.6029.7530.4030.0931.37-32.18*32.39*.
31.9932.4631.87-32.74*.
------33.1033.87*34.08*.
---34.0034.31*.
fine-tuning on pre-trained model.
table 2: case-sensitive bleu scores on en-de translation.
“*” indicates statistically signiﬁcant at p < 0.01compared to the transformer baselines..complexity.
consider a document with m sen-tences and n tokens, where each sentence con-tains n/m tokens on average.
the complexities ofboth the self-attention and cross-attention in trans-former are o(n 2).
in contrast, the complexityof group attention in g-transformer is o(n 2/m )given the fact that the attention is restricted to alocal sentence.
theoretically, since the averagelength n/m of sentences tends to be constant, thetime and memory complexities of group attentionare approximately o(n ), making training and in-ference on very long inputs feasible..4.2 combined attention.
we use only group attention on lower layers forlocal sentence representation, and combined atten-tion on top layers for integrating local and globalcontext information.
we use the standard multi-head attention in eq 5 for global context, naming itglobal multi-head attention (globalmha).
groupmulti-head attention in eq 8 and global multi-headattention are combined using a gate-sum module(zhang et al., 2016; tu et al., 2017).
layers for integrating local and global context.
bythis design, on lower layers, the sentences are iso-lated from each other, while on top layers, the cross-sentence interactions are enabled.
our experimentsshow that the top 2 layers with global attentionare sufﬁcient for document-level nmt, and morelayers neither help nor harm the performance..4.3.inference.
during decoding, we generate group-tag sequencegy according to the predicted token, starting with1 at the ﬁrst <s> and increasing 1 after each </s>.
we use beam search and apply the maximum lengthconstraint on each sentence.
we generate the wholedocument from start to end in one beam searchprocess, using a default beam size of 5..5 g-transformer results.
we compare g-transformer with transformer base-lines and previous document-level nmt modelson both non-pretraining and pre-training settings.
the detailed descriptions about these training set-tings are in appendix c.1.
we make statisticalsigniﬁcance test according to collins et al.
(2005)..hl = groupmha(q, k, v, gq, gk ),hg = globalmha(q, k, v ),.
g = sigmoid([hl, hg]w + b),h = hl (cid:12) g + hg (cid:12) (1 − g),.
where w and b are linear projection parameters,and (cid:12) denotes element-wise multiplication..previous study (jawahar et al., 2019) shows thatthe lower layers of transformer catch more localsyntactic relations, while the higher layers repre-sent longer distance relations.
based on these ﬁnd-ings, we use combined attention only on the top.
(10).
5.1 results on non-pretraining settings.
as shown in table 2, the sentence-level trans-former outperforms previous document-level mod-els on news and europarl.
compared to thisstrong baseline, our randomly initialized modelof g-transformer improves the s-bleu by 0.81point on the large dataset europarl.
the resultson the small datasets ted and news are worse,indicating overﬁtting with long inputs.
when g-transformer is trained by ﬁne-tuning the sentence-.
3447level transformer, the performance improves onthe three datasets by 0.3, 0.33, and 1.02 s-bleupoints, respectively..different from the baseline of document-leveltransformer, g-transformer can be successfullytrained on small ted and news.
on europarl,g-transformer outperforms transformer by 0.77d-bleu point, and g-transformer ﬁne-tuned onsentence-level transformer enlarges the gap to 0.98d-bleu point..g-transformer outperforms previous document-level mt models on news and europarl with asigniﬁcant margin.
compared to the best recentmodel hyrbid-context, g-transformer improvesthe s-bleu on europarl by 1.99. these resultssuggest that in contrast to previous short-contextmodels, sequence-to-sequence model taking thewhole document as input is a promising direction..5.2 results on pre-training settings.
there is relatively little existing work aboutdocument-level mt using pre-training.
althoughflat-transformer+bert gives a state-of-the-artscores on ted and europarl, the score on news isworse than previous non-pretraining model han(miculicich et al., 2018b).
g-transformer+bertimproves the scores by margin of 0.20, 1.62, and0.47 s-bleu points on ted, news, and europarl,respectively.
it shows that with a better contextualrepresentation, we can further improve document-level mt on pretraining settings..we further build much stronger transformerbaselines by ﬁne-tuning on mbart25 (liu et al.,2020).
taking advantage of sequence-to-sequencepre-training, the sentence-level transformer givesmuch better s-bleus of 27.78, 29.90, and31.87, respectively.
g-transformer ﬁne-tunedon mbart25 improves the performance by 0.28,0.44, and 0.87 s-bleu, respectively.
comparedto the document-level transformer baseline, g-transformer gives 1.74, 1.22, and 0.31 higherit demonstratesd-bleu points, respectively.
that even with well-trained sequence-to-sequencemodel, the locality bias can still enhance the per-formance..5.3 convergence.
we evaluate g-transformer ad transformer on var-ious input length, data scale, and model size tobetter understand that to what extent it has solvedthe convergence problem of transformer..(a) input length.
(b) data scale.
figure 7: g-transformer compared with transformer..(a) cross-attention.
(b) encoder self-attention.
figure 8: comparison on the development of cross-attention and encoder self-attention..input length.
the results are shown in figure7a.
unlike transformer, which fails to train onlong input, g-transformer shows stable scores forinputs containing 512 and 1024 tokens, suggestingthat with the help of locality bias, a long input doesnot impact the performance obviously..data scale.
as shown in figure 7b, overall g-transformer has a smooth curve of performance onthe data scale from 1.25k to 160k.
the variancesof the scores are much lower than transformer,indicating stable training of g-transformer.
addi-tionally, g-transformer outperforms transformerby a large margin on all the settings..model size.
unlike transformer, which failsto train on big and large model settings, g-transformer shows stable scores on different modelsizes.
as shown in appendix c.2, although per-formance on small datasets ted and news dropslargely for big and large model, the performanceon large dataset europarl only decreases by 0.10d-bleu points for the big model and 0.66 for thelarge model..loss.
looking into the training process of theabove experiments, we see that both the trainingand validation losses of g-transformer convergemuch faster than transformer, using almost halftime to reach the same level of loss.
furthermore,the validation loss of g-transformer converges tomuch lower values.
these observations demon-strate that g-transformer converges faster and bet-ter..attention distribution.
beneﬁting from theseparate group attention and global attention, g-transformer avoids the oscillation of attention.
3448-5515253545d-bleutokens641282565121024transformerg-transformer-5515253545d-bleuinstances1.25k2.5k5k10k20k40k80k160ktransformerg-transformer34567890k10k20k30k40k50k60kentropy (bit)stepstransformergroup attentionglobal attention34567890k10k20k30k40k50k60kentropy (bit)stepstransformergroup attentionglobal attentionmethodg-transformer (fnt.)
- target-side context- source-side context.
ted news25.5225.1225.4125.0524.5824.56.europarl drop.
32.3932.1631.39.
--0.14-0.70.table 3: impact of source-side and target-side contextreporting in s-bleu.
here, fnt.
denotes the model ﬁne-tuned on sentence-level transformer..methodcadec (voita et al., 2019b)lstm-tran (zhang et al., 2020)sent (voita et al., 2019b)concat (voita et al., 2019b)g-transformer.
deixis81.691.050.083.589.9.el.inﬂ.
72.282.253.076.284.8.el.vp80.078.228.476.682.4.table 4: impact on discourse by the source-side con-text, in accuracy of correctly identifying the discoursephenomena.
here, el.
means ellipsis.
lstm-tran de-notes lstm-transformer..range, which happens to transformer.
as shownin figure 8a, transformer sticks at the plateau areafor about 13k training steps, but g-transformershows a quick and monotonic convergence, reach-ing the stable level using about 1/4 of the time thattransformer takes.
through figure 8b, we can ﬁndthat g-transformer also has a smooth and stablecurve for the convergence of self-attention distribu-tion.
these observations imply that the potentialconﬂict of local sentence and document context canbe mitigated by g-transformer..5.4 discussion of g-transformer.
document context.
we study the contribution ofthe source-side and target-side context by remov-ing the cross-sentential attention in eq 10 from theencoder and the decoder gradually.
the resultsare shown in table 3. we take the g-transformerﬁne-tuned on the sentence-level transformer asour starting point.
when we disable the target-side context, the performance decreases by 0.14s-bleu point on average, which indicates that thetarget-side context does impact translation perfor-mance signiﬁcantly.
when we further remove thesource-side context, the performance decrease by0.49, 0.83, and 0.77 s-bleu point on ted, news,and europarl, respectively, which indicates that thesource-side context is relatively more important fordocument-level mt..to further understand the impact of the source-side context, we conduct an experiment on auto-matic evaluation on discourse phenomena whichrely on source context.
we use the human labeledevaluation set (voita et al., 2019b) on english-.
methodg-transformer (rnd.)
- word-dropout- language locality- translation locality.
ted news europarl drop25.8425.4922.470.76.
--0.37-1.78-14.68.
33.8733.7033.6333.10.
25.2324.6522.410.60.table 5: contribution of locality bias and word-dropoutreporting in d-bleu.
here, rnd.
denotes the modeltrained using randomly initialized parameters..method.
ted news.
europarl drop.
g-transformer (rnd.).
combined attentiononly group attentiononly global attention.
25.8425.6225.00.
25.2325.1424.54.
33.8733.1232.87.
--0.35-0.84.table 6: separate effect of group and global attentionreporting in d-bleu.
here, rnd.
denotes the modeltrained using randomly initialized parameters..russion (en-ru) for deixis and ellipsis.
we fol-low the transformer concat baseline (voita et al.,2019b) and use both 6m sentence pairs and 1.5mdocument pairs from opensubtitles2018 (lisonet al., 2018) to train our model.
the results areshown in table 4. g-transformer outperformstransformer baseline concat (voita et al., 2019b)with a large margin on three discourse features,indicating a better leverage of the source-side con-text.
when compared to previous model lstm-t,g-transformer achieves a better ellipsis on bothinﬂ.
and vp.
however, the score on deixis is stilllower, which indicates a potential direction that wecan investigate in further study..word-dropout.
as shown in table 5, word-dropout (appendix c.1) contributes about 0.37 d-bleu on average.
its contribution to ted andnews is obvious in 0.35 and 0.58 d-bleu, respec-tively.
however, for large dataset europarl, thecontribution drops to 0.17, suggesting that with suf-ﬁcient data, word-dropout may not be necessary..locality bias.
in g-transformer, we introducelocality bias to the language modeling of sourceand target, and locality bias to the translation be-tween source and target.
we try to understand thesebiases by removing them from g-transformer.
when all the biases removed, the model down-grades to a document-level transformer.
the re-sults are shown in table 5. relatively speaking,the contribution of language locality bias is about1.78 d-bleu on average.
while the translationlocality bias contributes for about 14.68 d-bleuon average, showing critical impact on the modelconvergence on small datasets.
these results sug-gest that the locality bias may be the key to train.
3449whole-document mt models, especially when thedata is insufﬁcient..combined attention..in g-transformer, weenable only the top k layers with combined atten-tion.
on europarl7, g-transformer gives 33.75,33.87, and 33.84 d-bleu with top 1, 2, and 3layers with combined attention, respectively, show-ing that k = 2 is sufﬁcient.
furthermore, westudy the effect of group and global attention sep-arately.
as shown in table 6, when we replacethe combined attention on top 2 layers with groupattention, the performance drops by 0.22, 0.09, and0.75 d-bleu on ted, news, and europarl, respec-tively.
when we replace the combined attentionwith global attention, the performance decrease isenlarged to 0.84, 0.69, and 1.00 d-bleu, respec-tively.
these results demonstrate the necessity ofcombined attention for integrating local and globalcontext information..6 related work.
the unit of translation has evolved from word(brown et al., 1993; vogel et al., 1996) to phrase(koehn et al., 2003; chiang, 2005, 2007) and fur-ther to sentence (kalchbrenner and blunsom, 2013;sutskever et al., 2014; bahdanau et al., 2014) inthe mt literature.
the trend shows that larger unitsof translation, when represented properly, can leadto improved translation quality..a line of document-level mt extends translationunit to multiple sentences (tiedemann and scher-rer, 2017; agrawal et al., 2018; zhang et al., 2020;ma et al., 2020).
however, these approaches arelimited within a short context of maximum foursentences.
recent studies extend the translationunit to whole document (junczys-dowmunt, 2019;liu et al., 2020), using large augmented datasetor pretrained models.
liu et al.
(2020) showsthat transformer trained directly on document-level dataset can fail, resulting in unreasonablylow bleu scores.
following these studies, wealso model translation on the whole document.
wesolve the training challenge using a novel localitybias with group tags..another line of work make document-level ma-chine translation sentence by sentence, using addi-tional components to represent the context (marufand haffari, 2018; zheng et al., 2020; zhang et al.,2018; miculicich et al., 2018b; maruf et al., 2019;yang et al., 2019).
different from these approaches,g-transformer uses a generic design for both.
source and context, translating whole document inone beam search instead of sentence-by-sentence.
some methods use a two-pass strategy, generatingsentence translation ﬁrst, integrating context infor-mation through a post-editing model (voita et al.,2019a; yu et al., 2020).
in contrast, g-transformeruses a single model, which reduces the complexityfor both training and inference..the locality bias we introduce to g-transformeris different from the ones in longformer (beltagyet al., 2020) and reformer (kitaev et al., 2020) inthe sense that we discuss locality in the contextof representing the alignment between source sen-tences and target sentences in document-level mt.
speciﬁcally, longformer introduces locality onlyto self-attention, while g-transformer also intro-duces locality to cross-attention, which is shown tobe the key for the success of g-transformer.
re-former, basically same as transformer, searchesfor attention targets in the whole sequence, whileg-transformer mainly restricts the attention insidea local sentence.
in addition, the motivations aredifferent.
while longformer and reformer focuson the time and memory complexities, we focuson attention patterns in cases where a translationmodel fails to converge during training..7 conclusion.
we investigated the main reasons for transformertraining failure in document-level mt, ﬁnding thattarget-to-source attention is a key factor.
accord-ing to the observation, we designed a simple ex-tension of the standard transformer architecture,using group tags for attention guiding.
experimentsshow that the resulting g-transformer convergesfast and stably on small and large data, giving thestate-of-the-art results compared to existing modelsunder both pre-training and random initializationsettings..acknowledgments.
we would like to thank the anonymous review-ers for their valuable feedback.
we thank west-lake university high-performance computing cen-ter for supporting on gpu resources.
this workis supported by grants from alibaba group inc.and sichuan lan-bridge information technologyco.,ltd..3450references.
ruchit rajeshkumar agrawal, marco turchi, and mat-teo negri.
2018. contextual handling in neural ma-chine translation: look behind, ahead and on bothsides.
in 21st annual conference of the europeanassociation for machine translation, pages 11–20..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..guangsheng bao and yue zhang.
2021. contextu-in thealized rewriting for text summarization.
thirty-fifth aaai conference on artiﬁcial intelli-gence, aaai 2021..iz beltagy, matthew e. peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv:2004.05150..samuel r. bowman, luke vilnis, oriol vinyals, an-drew dai, rafal jozefowicz, and samy bengio.
2016. generating sentences from a continuousin proceedings of the 20th signll con-space.
ference on computational natural language learn-ing, pages 10–21, berlin, germany.
association forcomputational linguistics..peter f. brown, stephen a. della pietra, vincent j.della pietra, and robert l. mercer.
1993. the math-ematics of statistical machine translation: parameterestimation.
computational linguistics, 19(2):263–311..david chiang.
2005. a hierarchical phrase-basedin pro-model for statistical machine translation.
ceedings of the 43rd annual meeting of the as-sociation for computational linguistics (acl’05),pages 263–270, ann arbor, michigan.
associationfor computational linguistics..david chiang.
2007. hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201–228..michael collins, philipp koehn, and ivona kuˇcerov´a.
2005. clause restructuring for statistical machinetranslation.
in proceedings of the 43rd annual meet-ing of the association for computational linguistics(acl05), pages 531–540..eva mart´ınez garcia, cristina espa˜na-bonet, and llu´ısm`arquez.
2015. document-level machine transla-in proceedings oftion with word vector models.
the 18th annual conference of the european asso-ciation for machine translation, pages 59–66, an-talya, turkey..zhengxian gong, min zhang, and guodong zhou.
2011. cache-based document-level statistical ma-chine translation.
in proceedings of the 2011 con-ference on empirical methods in natural languageprocessing, pages 909–919, edinburgh, scotland,uk.
association for computational linguistics..christian hardmeier.
2014. discourse in statistical ma-chine translation.
ph.d. thesis, acta universitatisupsaliensis..christian hardmeier, sara stymne, j¨org tiedemann,and joakim nivre.
2013. docent: a document-leveldecoder for phrase-based statistical machine trans-in proceedings of the 51st annual meet-lation.
ing of the association for computational linguistics:system demonstrations, pages 193–198, soﬁa, bul-garia.
association for computational linguistics..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structurein proceedings of the 57th annualof language?
meeting of the association for computational lin-guistics, pages 3651–3657, florence, italy.
associa-tion for computational linguistics..marcin junczys-dowmunt.
2019. microsoft translatorat wmt 2019: towards large-scale document-levelin proceedings of theneural machine translation.
fourth conference on machine translation (volume2: shared task papers, day 1), pages 225–233, flo-rence, italy.
association for computational linguis-tics..nal kalchbrenner and phil blunsom.
2013. recurrentin proceedings ofcontinuous translation models.
the 2013 conference on empirical methods in natu-ral language processing, pages 1700–1709, seattle,washington, usa.
association for computationallinguistics..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in inter-national conference on learning representations..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..philipp koehn and rebecca knowles.
2017. six chal-in proceed-lenges for neural machine translation.
ings of the first workshop on neural machine trans-lation, pages 28–39, vancouver.
association forcomputational linguistics..philipp koehn, franz j. och, and daniel marcu.
2003.statistical phrase-based translation.
in proceedingsof the 2003 human language technology confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 127–133..samuel l¨aubli, rico sennrich, and martin volk.
2018.has machine translation achieved human parity?
ain proceed-case for document-level evaluation.
ings of the 2018 conference on empirical methods.
3451in natural language processing, pages 4791–4796,brussels, belgium.
association for computationallinguistics..on empirical methods in natural language process-ing, pages 2947–2954, brussels, belgium.
associa-tion for computational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..pierre lison, j¨org tiedemann, and milen kouylekov.
2018. opensubtitles2018: statistical rescoring ofsentence alignments in large, noisy parallel corpora.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
transac-tions of the association for computational linguis-tics, 8:726–742..shuming ma, dongdong zhang, and ming zhou.
2020.a simple and effective uniﬁed encoder for document-in proceedings of thelevel machine translation.
58th annual meeting of the association for compu-tational linguistics, pages 3505–3511, online.
as-sociation for computational linguistics..sameen maruf and gholamreza haffari.
2018. docu-ment context neural machine translation with mem-in proceedings of the 56th annualory networks.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1275–1284, melbourne, australia.
association for compu-tational linguistics..sameen maruf, andr´e f. t. martins, and gholamrezahaffari.
2019. selective attention for context-awarein proceedings of theneural machine translation.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 3092–3102, minneapolis, min-nesota.
association for computational linguistics..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018a.
document-level neu-ral machine translation with hierarchical attentionin proceedings of the 2018 conferencenetworks.
on empirical methods in natural language process-ing, pages 2947–2954, brussels, belgium.
associa-tion for computational linguistics..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018b.
document-level neu-ral machine translation with hierarchical attentionin proceedings of the 2018 conferencenetworks..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..jean pouget-abadie, dzmitry bahdanau, bart vanmerri¨enboer, kyunghyun cho, and yoshua bengio.
2014. overcoming the curse of sentence length forneural machine translation using automatic segmen-tation.
in proceedings of ssst-8, eighth workshopon syntax, semantics and structure in statisticaltranslation, pages 78–85, doha, qatar.
associationfor computational linguistics..luigi rizzi.
2013. locality.
lingua, 130:169–186..yves scherrer, j¨org tiedemann, and sharid lo´aiciga.
analysing concatenation approaches to2019.document-level nmt in two different domains.
inproceedings of the fourth workshop on discourse inmachine translation (discomt 2019), pages 51–61,hong kong, china.
association for computationallinguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the third workshop on discourse in machinetranslation, pages 82–92, copenhagen, denmark.
association for computational linguistics..zhaopeng tu, yang liu, zhengdong lu, xiaohua liu,and hang li.
2017. context gates for neural ma-chine translation.
transactions of the associationfor computational linguistics, 5:87–99..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, pages 6000–6010..stephan vogel, hermann ney, and christoph tillmann.
1996. hmm-based word alignment in statisticaltranslation.
in coling 1996 volume 2: the 16thinternational conference on computational linguis-tics..3452elena voita, rico sennrich, and ivan titov.
2019a.
context-aware monolingual repair for neural ma-chine translation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 877–886, hong kong, china.
as-sociation for computational linguistics..elena voita, rico sennrich, and ivan titov.
2019b.
when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 1198–1212, flo-rence, italy.
association for computational linguis-tics..zhengxin yang,.
jinchao zhang, fandong meng,shuhao gu, yang feng, and jie zhou.
2019. en-hancing context modeling with a query-guided cap-sule network for document-level translation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 1527–1537, hong kong, china.
association for computa-tional linguistics..lei yu, laurent sartran, wojciech stokowiec, wangling, lingpeng kong, phil blunsom, and chrisdyer.
2020. better document-level machine trans-lation with bayes’ rule.
transactions of the associ-ation for computational linguistics, 8:346–360..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 533–542, brussels, bel-gium.
association for computational linguistics..meishan zhang, yue zhang, and duy-tin vo.
2016.gated neural networks for targeted sentiment anal-ysis.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, volume 30..pei zhang, boxing chen, niyu ge, and kai fan.
2020.long-short term masking transformer: a simplebut effective baseline for document-level neural ma-chine translation.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1081–1087, online.
as-sociation for computational linguistics..zaixiang zheng, xiang yue, shujian huang, jiajunchen, and alexandra birch.
2020. towards mak-ing the most of context in neural machine trans-in proceedings of the twenty-ninth inter-lation.
national joint conference on artiﬁcial intelligence,ijcai-20, pages 3983–3989..3453a evaluation metrics.
following liu et al.
(2020), we use sentence-levelbleu score (s-bleu) as the major metric for ourevaluation.
however, when document-level trans-former is compared, we use document-level bleuscore (d-bleu) since the sentence-to-sentencealignment is not available..s-bleu.
to calculate sentence-level bleuscore on document translations, we ﬁrst split thetranslations into sentences, mapping to the corre-sponding source sentences.
then we calculate thebleu score on pairs of translation and referenceof the same source sentence..d-bleu.
when the alignments between transla-tion and source sentences are not available, we cal-culate the bleu score on document-level, match-ing n-grams in the whole document..b transformer.
b.1 model.
transformer (vaswani et al., 2017) has an encoder-decoder structure, using multi-head attention andfeed-forward network as basic modules.
in this pa-per, we mainly concern about the attention module.
attention.
an attention module works as a func-tion, mapping a query and a set of key-value pairsto an output, that the query, keys, values, and out-put are all vectors.
the output is computed as aweighted sum of the values, where the weight as-signed to each value is computed by a matchingfunction of the query with the corresponding key.
formally, for matrix inputs of query q, key k, andvalue v ,.
attention(q, k, v ) = softmax.
(11).
(cid:18) qk t√dk.
(cid:19).
v,.
where dk is the dimensions of the key vector..multi-head attention.
build upon single-headattention module, multi-head attention allows themodel to attend to different positions of a sequence,gathering information from different representationsubspaces by heads..multihead(q, k, v ) = concat(head1, ..., headh)w o,.
(12).
where.
headi = attention(qw q.i , kw k.i., v w v.i ),.
(13).
that the projections of w o, w qiare parameter matrices.., w ki., and w vi.encoder.
the encoder consists of a stack of nidentical layers.
each layer has a multi-head self-attention, stacked with a feed-forward network.
aresidual connection is applied to each of them..decoder.
similar as the encoder, the decoderalso consists of a stack of n identical layers.
foreach layer, a multi-head self-attention is used torepresent the target itself, and a multi-head cross-attention is used to attend to the encoder outputs.
the same structure of feed-forward network andresidual connection as the encoder is used..b.2 training settings.
we build our experiments based on transformerimplemented by fairseq (ott et al., 2019).
weuse shared dictionary between source and target,and use a shared embedding table between the en-coder and the decoder.
we use the default settingproposed by transformer (vaswani et al., 2017),which uses adam optimizer with β1 = 0.9 andβ2 = 0.98, a learning rate of 5e − 4, and an inverse-square schedule with warmup steps of 4000. weapply label-smoothing of 0.1 and dropout of 0.3 onall settings.
to study the impact of input length,data scale, and model size, we take the learning rateand other settings as controlled variables that areﬁxed for all experiments.
we determine the num-ber of updates/steps automatically by early stop onvalidation set.
we train base and big models on4 gpus of navidia 2080ti, and large model on 4gpus of v100..c g-transformer.
c.1 training settings.
we generate the corresponding group tag sequencedynamically in the model according to the spe-cial sentence-mark tokens <s> and </s>.
tak-ing a document “<s> there is no public transport.
</s> <s> local people struggle to commute.
</s>” as an example, a group-tag sequenceg = {1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2} isgenerated according to eq 3, where 1 starts onthe ﬁrst <s> and ends on the ﬁrst </s>, 2 the sec-ond, and so on.
the model can be trained eitherrandomly initialized or ﬁne-tuned..randomly initialized.
we use the same set-tings as transformer to train g-transformer, usinglabel-smoothing of 0.1, dropout of 0.3, adam op-timizer, and a learning rate of 5e − 4 with 4000warmup steps.
to encourage inferencing the trans-lation from the context, we apply a word-dropout.
3454method.
ted.
newss-bleu d-bleu s-bleu d-bleu s-bleu d-bleu.
europarl.
g-transformer random initialized (base)g-transformer random initialized (big)g-transformer random initialized (large).
23.5323.296.23.
25.8425.488.95.
23.5522.2213.68.
25.2323.8215.33.
32.1832.0431.51.
33.8733.7733.21.table 7: g-transformer on different model size..(bowman et al., 2016) with a probability of 0.3 onboth the source and the target inputs..fine-tuned on sentence-level transformer.
we use the parameters of an existing sentence-level transformer to initialize g-transformer.
wecopy the parameters of the multi-head attention intransformer to the group multi-head attention ing-transformer, leaving the global multi-head at-tention and the gates randomly initialized.
for theglobal multi-head attention and the gates, we use alearning rate of 5e − 4, while for other components,we use a smaller learning rate of 1e − 4. all the pa-rameters are jointly trained using adam optimizerwith 4000 warmup steps.
we apply a word-dropoutwith a probability of 0.1 on both the source and thetarget inputs..fine-tuned on mbart25.
similar as the ﬁne-tuning on sentence-level transformer, we also copyparameters from mbart25 (liu et al., 2020) tog-transformer, leaving the global multi-head at-tention and the gates randomly initialized.
we fol-lowing the settings (liu et al., 2020) to train themodel, using adam optimizer with a learning rateof 3e − 5 and 2500 warmup steps.
here, we donot apply word-dropout, which empirically showsa damage to the performance..c.2 results on model size.
as shown in table 7, g-transformer has a rela-tively stable performance on different model size.
when increasing the model size from base to big,the performance drops for about 0.24, 1.33, and0.14 s-bleu points, respectively.
further to largemodel, the performance drops further for about17.06, 8.54, and 0.53 s-bleu points, respectively.
although the performance drop on small dataset islarge since overﬁtting on larger model, the drop onlarge dataset europarl is relatively small, indicatinga stable training on different model size..3455