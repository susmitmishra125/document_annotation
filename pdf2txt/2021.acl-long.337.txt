hierarchy-aware label semantics matching network for hierarchicaltext classiﬁcation.
haibin chen, qianli ma*, zhenxi lin, jiangyue yanschool of computer science and engineering,south china university of technology, guangzhou, chinahaibin chen@foxmail.comqianlima@scut.edu.cn∗.
abstract.
htc are to model the large-scale, imbalanced, andstructured label hierarchy (mao et al., 2019)..hierarchical text classiﬁcation is an importantyet challenging task due to the complex struc-ture of the label hierarchy.
existing methodsignore the semantic relationship between textand labels, so they cannot make full use ofthe hierarchical information.
to this end, weformulate the text-label semantics relationshipas a semantic matching problem and thus pro-pose a hierarchy-aware label semantics match-ing network (himatch).
first, we project textsemantics and label semantics into a joint em-bedding space.
we then introduce a joint em-bedding loss and a matching learning loss tomodel the matching relationship between thetext semantics and the label semantics.
ourmodel captures the text-label semantics match-ing relationship among coarse-grained labelsand ﬁne-grained labels in a hierarchy-awaremanner.
the experimental results on vari-ous benchmark datasets verify that our modelachieves state-of-the-art results..1.introduction.
hierarchical text classiﬁcation (htc) is widelyused in natural language processing (nlp), suchas news categorization (lewis et al., 2004) and sci-entiﬁc paper classiﬁcation (kowsari et al., 2017).
htc is a particular multi-label text classiﬁcationproblem, which introduces hierarchies to organizelabel structure.
as depicted in figure 1, htc mod-els predict multiple labels in a given label hierarchy,which generally construct one or multiple pathsfrom coarse-grained labels to ﬁne-grained labels ina top-down manner (aixin sun and ee-peng lim,2001).
generally speaking, ﬁne-grained labels arethe most appropriate labels for describing the inputtext.
coarse-grained labels are generally the parentnodes of coarse- or ﬁne-grained labels, expressinga more general concept.
the key challenges of.
∗*corresponding author.
figure 1: an hierarchical text classiﬁcation exampletagged with labels economics and debt from coarse-grained label to ﬁne-grained label..existing work in htc has introduced variousmethods to use hierarchical information in a holis-tic way.
to capture the holistic label correlationfeatures, some researchers proposed a hierarchy-aware global model to exploit the prior probabil-ity of label dependencies through graph convolu-tion networks (gcn) and treelstm (zhou et al.,2020).
some researchers also introduced more la-bel correlation features such as label semantic sim-ilarity and label co-occurrence (lu et al., 2020).
they followed the traditional way to transformhtc into multiple binary classiﬁers for every label(f¨urnkranz et al., 2008).
however, they ignoredthe interaction between text semantics and label se-mantics (f¨urnkranz et al., 2008; wang et al., 2019),which is highly useful for classiﬁcation (chen et al.,2020).
hence, their models may not be sufﬁcientto model complex label dependencies and providecomparable text-label classiﬁcation scores (wanget al., 2019)..a natural strategy for modeling the interactionbetween text semantics and label semantics is to in-troduce a text-label joint embedding by label atten-tion (xiao et al., 2019) or autoencoders (yeh et al.,2017).
label attention-based methods adopted a.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4370–4379august1–6,2021.©2021associationforcomputationallinguistics4370rooteconomicsdebtrevenuesociety coarse-grained labelsfine-grained labelsinput text: "global debt is set to reach $200 trillion ..."label hierarchy self-attention mechanism to identify label-speciﬁcinformation (xiao et al., 2019).
autoencoder-basedmethods extended the vanilla canonical correlatedautoencoder (yeh et al., 2017) to a ranking-basedautoencoder architecture to produce comparabletext-label scores (wang et al., 2019).
however,these methods assume all the labels are indepen-dent without fully considering the correlation be-tween coarse-grained labels and ﬁne-grained labels,which cannot be simply transferred to htc models(zhou et al., 2020)..in this paper, we formulate the interaction be-tween text and label as a semantic matching prob-lem and propose a hierarchy-aware label seman-tics matching network (himatch).
the principalidea is that the text representations should be se-mantically similar to the target label representa-tions (especially ﬁne-grained labels), while theyshould be semantically far away from the incor-rect label representations.
first, we adopt a textencoder and a label encoder (shown in figure 2)to extract textual semantics and label semantics,respectively.
second, inspired by the methods oflearning common embeddings (wang et al., 2019),we project both textual semantics and label seman-tics into a text-label joint embedding space wherecorrelations between text and labels are exploited.
in this joint embedding space, we introduce a jointembedding loss between text semantics and targetlabel semantics to learn a text-label joint embed-ding.
after that, we apply a matching learningloss to capture text-label matching relationships ina hierarchy-aware manner.
in this way, the ﬁne-grained labels are semantically closest to the textsemantics, followed by the coarse-grained labels,while the incorrect labels should be semanticallyfar away from the text semantics.
hence, we pro-pose a hierarchy-aware matching learning methodto capture different matching relationships throughdifferent penalty margins on semantic distances.
fi-nally, we employ the textual representations guidedby the joint embedding loss and matching learningloss to perform the hierarchical text classiﬁcation..the major contributions of this paper are:.
1. by considering the text-label semantics match-ing relationship, we are the ﬁrst to formulate htcas a semantic matching problem rather than merelymultiple binary classiﬁcation tasks..2. we propose a hierarchy-aware label semanticsmatching network (himatch), in which we intro-duce a joint embedding loss and a matching learn-.
ing loss to learn the text-label semantics matchingrelationship in a hierarchy-aware manner..3. extensive experiments (with/without bert)on various datasets show that our model achievesstate-of-the-art results..2 related work.
2.1 hierarchical text classiﬁcation.
hierarchical text classiﬁcation is a particular multi-label text classiﬁcation problem, where the classi-ﬁcation results are assigned to one or more nodesof a taxonomic hierarchy.
existing state-of-the-artmethods focus on encoding hierarchy constraintin a global view such as directed graph and treestructure.
zhou et al.
(2020) proposed a hierarchy-aware global model to exploit the prior probabilityof label dependencies.
lu et al.
(2020) introducedthree kinds of label knowledge graphs, i.e., tax-onomy graph, semantic similarity graph, and co-occurrence graph to beneﬁt hierarchical text clas-siﬁcation.
they regarded hierarchical text clas-siﬁcation as multiple binary classiﬁcation tasks(f¨urnkranz et al., 2008).
the limitation is that thesemodels did not consider the interaction of label se-mantics and text semantics.
therefore, they failedto capture complex label dependencies and can notprovide comparable text-label classiﬁcation scores(wang et al., 2019), which leads to restricted per-formance (chen et al., 2020).
hence, it is crucialto exploit the relationship between text and labelsemantics, and help the model distinguish targetlabels from incorrect labels in a comparable andhierarchy-aware manner.
we perform matchinglearning in a joint embedding of text and label tosolve these problems in this work..2.2 exploit joint embedding of text and.
label.
to determine the correlation between text and label,researchers proposed various methods to exploita text-label joint embedding such as (xiao et al.,2019) or autoencoder (yeh et al., 2017).
in theﬁeld of multi-label text classiﬁcation, xiao et al.
(2019) proposed a label-speciﬁc attention net-work (lsan) to learn a text-label joint embeddingby label semantic and document semantic.
wanget al.
(2019) extended vanilla canonical correlatedautoencoder (yeh et al., 2017) to a ranking-basedautoencoder architecture to produce comparablelabel scores.
however, they did not fully con-sider label semantics and holistic label correlation.
4371figure 2: the overall architecture of the proposed model.
firstly, the text encoder and label encoder extract thetext semantics and label semantics, respectively.
then text semantics and label semantics are projected into a jointembedding space.
joint embedding loss encourages the text semantics to be similar to the target label semantics.
by introducing matching learning loss, ﬁne-grained labels semantics (debt) is semantically closest to the textsemantics, followed by coarse-grained labels (economics), while other incorrect labels semantics is semanticallyfar away from text semantics (revenue, society).
the relative order is d1 < d2 < d3 < d4, where d represents themetric distances in joint embedding..among ﬁne-grained labels, coarse-grained labels,and incorrect labels.
in addition, we can not simplytransfer these multi-label classiﬁcation methods tohtc due to the constraint of hierarchy (zhou et al.,2020)..3 proposed method.
in this section, we will describe the details aboutour hierarchy-aware label semantics matchingnetwork.
figure 2 shows the overall architectureof our proposed model..3.1 text encoder.
in the htc task, given the input sequence xseq ={x1, ..., xn}, the model will predict the label y ={y1, ..., yk} where n is the number of words andk is the number of label sets.
the label witha probability higher than a ﬁxed threshold (0.5)will be regarded as the prediction result.
the se-quence of token embeddings is ﬁrstly fed into abidirectional gru layer to extract contextual fea-ture h = {h1, ..., hn}.
then, cnn layers withtop-k max-pooling are adopted for generating keyn-gram features t ∈ rk×dcnn where dcnn indicatesthe output dimension of the cnn layer..following the previous work (zhou et al., 2020),we further introduce a hierarchy-aware text featurepropagation module to encode label hierarchy in-formation.
we deﬁne a hierarchy label structure.
(cid:16).
←−e ,.
−→e.(cid:17).
vt,.
as a directed graph g =.
, where vt←−eindicates the set of hierarchy structure nodes.
are built from the top-down hierarchy paths repre-senting the prior statistical probability from parent−→e are built from thenodes to children nodes.
bottom-up hierarchy paths representing the con-nection relationship from children nodes to parentnodes.
the feature size of graph adjacency matrix← e and → e is ∈ rk×k, where k is the num-ber of label sets.
text feature propagation moduleﬁrstly projects text features t to node inputs vt by alinear transformation wproj ∈ rk×dcnn×dt, wheredt represents the hierarchy structure node dimen-sion from text feature.
then a graph convolutionnetwork (gcn) is adopted to explicitly combinetext semantics with prior hierarchical information←−e and.
−→e :.
(cid:16)←−.
st = σ.e · vt · wg1 +.
−→e · vt · wg2.
(cid:17).
(1).
where σ isthe activation function relu.
wg1, wg2 ∈ rdt×dt are the weight matrix of gcn.
st is the text representation aware of prior hierar-chy paths..3.2 label encoder.
in the htc task, the hierarchical label structure can(cid:17)be regarded as a directed graph g =,.
←−e ,.
−→e.(cid:16).
vl,.
4372global debt is set to reach...input texttext encoderclassification layerclassification losseconomics(coarse -grained target  label)debt(fine-grained  target label)revenue(incorrect  sibling label)society(other incorrect label)matching learning lossjoint embedding losscnn+poolingbi-grulabel  encodergraph convolutiontext representationslabel representationsclassification learningmlprooteconomicsdebtrevenuesociety mlplabel settext representationslabel representationsfeature propagation text-label joint embeddingd1d2d3d4d1<d2<d3<d4minimize(text, target labels)maximize(text, incorrect labels)joint embedding learninghierarchy-aware matching learningcoarse-grained  labelsfine-grained  labelswhere vl indicates the set of hierarchy structurenodes with label representation.
the graph g in−→elabel encoder shares the same structurewith the graph in text encoder.
given the totallabel set y = {y1, ..., yk} as input, we create labelembeddings vl ∈ rdl by averaging of pre-trainedlabel embeddings ﬁrst.
then gcn could be utilizedas label encoder:.
←−e and.
(cid:16)←−.
sl = σ.e · vl · wg3 +.
−→e · vl · wg4.
(cid:17).
(2).
where σ isthe activation function relu.
wg3, wg4 ∈ rdl×dl are the weight matrix of gcn.
sl is the label representation aware of prior hierar-chy paths.
it must be noted that the weight matrixand input representation of the label encoder aredifferent with those in the text encoder..3.3 label semantics matching.
3.3.1.joint embedding learning.
in this section, we will introduce the methods oflearning a text-label joint embedding and hierarchy-aware matching relationship.
for joint embeddinglearning, ﬁrstly, we project text semantics st andlabel semantics sl into a common latent space asfollows:.
φt = ffnt (st) ,φl = ffnl (sl).
(3).
(4).
where ffnt and ffnl are independent two-layerfeedforward neural networks.
φt, φl ∈ rdϕ rep-resent text semantics and label semantics in jointembedding space, respectively.
dϕ indicates thedimension of joint embedding..in order to align the two independent seman-tic representations in the latent space, we employthe mean squared loss between text semantics andtarget labels semantics:.
ljoint =.
(cid:88).
(cid:13)(cid:13)φt − φpl.(cid:13)2(cid:13)2.
(5).
p∈p (y).
where p (y) is target label sets.
ljoint aims tominimize the common embedding loss betweeninput text and target labels..3.3.2 hierarchy-aware matching learning.
based on the text-label joint embedding loss, themodel only captures the correlations between textsemantics and target labels semantics, while corre-lations among different granular labels are ignored..figure 3: illustration of hierarchy-aware margin.
tar-get labels are colored yellow.
each colored line repre-sent the matching operation between text and differentlabels.
the two vertical axes for semantic matchingdistance and penalty margin are on the right.
the se-mantic matching distance can be sorted by the order ofd1 (ﬁne-grained target labels) < d2 (coarse-grained tar-get labels) < d3 (incorrect sibling labels) < d4 (otherincorrect labels).
we introduce penalty margins γ tomodel the relative matching relationships..in the htc task, it is expected that the matching re-lationship between text semantics and ﬁne-grainedlabels should be the closest, followed by coarse-grained labels.
text semantics and incorrect labelssemantics should not be related..insight of these, we propose a hierarchy-awarematching loss lmatch to incorporate the correla-tions among text semantics and different labels se-mantics.
lmatch aims to penalize the small seman-tic distance between text semantics and incorrectlabels semantics with a margin γ:lmatch = max (cid:0)0, d (cid:0)φt, φp.
(cid:1) − d (φt, φn.
l.l ) + γ(cid:1)(6).
where φpl represents target labels semantics andφnl represents incorrect labels semantics.
we usel2-normalized euclidean distance for metric d andγ is a margin constant for margin-based triplet loss.
we take the average of all the losses between everylabel pairs as the margin loss..hierarchy-aware margin due to the largelabel sets in the htc task, it is time-consuming tocalculate every label’s matching loss.
therefore,we propose hierarchy-aware sampling to alleviatethe problem.
speciﬁcally, we sample all parent la-bels (coarse-grained labels), one sibling label, andone random incorrect label for every ﬁne-grainedlabel to obtain its negative label sets n ∈ n (y).
itis also unreasonable to assign the same margin fordifferent label pairs since the label semantics sim-ilarity is quite different in a large structured labelhierarchy.
our basic idea is that the semantics re-lationship should be closer if two labels are closer.
4373economics (coarse-grained  target label)debt (fine-grained  target label)revenue (incorrect sibling  label)society (other incorrect label)large semantic  distancesmall semantic distancerootd4d3d1text: "global debt  is set to..."matchingmatchingmatchingmatchingd2large penalty  marginsmall penalty  marginγ4γ3γ1γ2in the hierarchical structure.
firstly, the text se-mantics should match ﬁne-grained labels the most,which is exploited in joint embedding learning.
then we regard the pair with the smallest semanticdistance (d1) as a positive pair and regard other text-label matching pairs as negative pairs.
as depictedin the schema ﬁgure 3, compared with the posi-tive pair, the semantics matching distance betweentext and coarse-grained target labels (d2) should belarger.
the incorrect sibling labels have a certainsemantic relationship with the target labels.
hence,the semantics matching distance between text andthe incorrect sibling labels of ﬁne-grained labels(d3) should be further larger, while the semanticsmatching distance between text and other incorrectlabels (d4) should be the largest.
we introducehierarchy-aware penalty margins γ1, γ2, γ3, γ4 tomodel the comparable relationship.
the penaltymargin is smaller if we expect the semantic match-ing distance to be smaller.
we neglect γ1 becausethe matching relationships between text semanticsand ﬁne-grained labels are exploited in joint em-bedding learning.
γ2, γ3, γ4 are penalty marginscompared with the matching relationships betweentext semantics and ﬁne-grained labels semantics.
we introduce two hyperparameters α, β to measuredifferent matching relationships of γ:.
γ2 = αγ;.
γ3 = βγ;.
γ4 = γ.
(7).
where 0 < α < β < 1. the proposed loss capturesthe relative semantics similarity rankings amongtarget labels and incorrect labels in a hierarchy-aware manner..3.4 classiﬁcation learning and objective.
function.
we ﬁnd that it is easier to overﬁt for classiﬁcationlearning if we perform classiﬁcation learning inthe text-label joint embedding directly.
hence, weuse the text semantics representation st guided byjoint embedding loss and matching learning lossto perform classiﬁcation learning.
st is fed into afully connected layer to get the label probability ˆyfor prediction..the overall objective function includes a cross-entropy category loss, joint embedding loss andhierarchy-aware matching loss:.
l = lcls(y, ˆy) + λ1ljoint + λ2lmatch.
(8).
where y and ˆy are the ground-truth label and outputprobability, respectively.
λ1, λ2 are the hyperpa-rameters for balancing the joint embedding loss and.
dataset.
|l| depth avg(|li|) t rain.
v al.
t est.
rcv1-v2wos.
103141eurlex-57k 4271.
425.
3.2425.
208333007045000.
231675186000.
78126593976000.table 1: statistics of three datasets for hierarchical|l|: number of tar-multi-label text classiﬁcation.
get classes.
depth: maximum level of hierarchy.
avg(|li|): average number of classes per sample.
t rain/v al/t est: size of train/validation/test set..matching learning loss.
we minimize the abovefunction by gradient descent during training..4 experiment.
4.1 experiment setup.
datasets to evaluate the effectiveness of ourmodel, we conduct experiments on three widely-studied datasets for hierarchical multi-label textclassiﬁcation.
statistics of these datasets are listedin table 1. rcv1-v2 (lewis et al., 2004) is anews categorization corpora, and wos (kowsariet al., 2017) includes abstracts of published papersfrom web of science.
eurlex57k is a large hi-erarchical multi-label text classiﬁcation (lmtc)dataset that contains 57k english eu legislativedocuments, and is tagged with about 4.3k labelsfrom the european vocabulary (chalkidis et al.,2019).
the label sets are split into zero-shot labels,few-shot labels, and frequent labels.
few-shot la-bels are labels whose frequencies in the trainingset are less than or equal to 50. frequent labelsare labels whose frequencies in the training set aremore than 50. the label setting is the same as pre-vious work (lu et al., 2020).
in eurlex57k, thecorpora are only tagged with ﬁne-grained labels,and the parent labels of ﬁne-grained labels are nottagged as the target labels..evaluation metric on rcv1-v2 and wosdatasets, we measure the experimental results bymicro-f1 and macro-f1.
micro-f1 takes the over-all precision and recall of all the instances intoaccount, while macro-f1 equals the average f1-score of labels.
we report the results of two rank-ing metrics on large hierarchical multi-label textclassiﬁcation dataset eurlex-57k, including re-call@5 and ndcg@5. the ranking metrics arepreferable for eurlex-57k since it does not in-troduce a signiﬁcant bias towards frequent labels(lu et al., 2020)..implementation details we initialize the wordembeddings with 300d pre-trained glove vectors.
4374(pennington et al., 2014).
then we use a one-layerbigru with hidden dimension 100 and used 100ﬁlters with kernel size [2,3,4] to setup the cnns.
the dimension of the text propagation feature andgraph convolution weight matrix are both 300. thehidden size of joint embedding is 200. the match-ing margin γ is set to 0.2 on rcv1-v2 and wosdatasets, and set to 0.5 on eurlex-57k dataset.
we set the value of hierarchy-aware penalty hyper-parameters α, β to 0.01 and 0.5, respectively.
theloss balancing factor λ1, λ2 are set to 1. for faircomparisons with previous work (lu et al., 2020;chalkidis et al., 2019) on eurlex-57k dataset,ﬁrstly, we do not use cnn layer and text featurepropagation module.
secondly, to adapt to the zero-shot settings, the prediction is generated by the dotproduct similarity between text semantics and labelsemantics.
our model is optimized by adam witha learning rate of 1e-4..for pretrained language model bert (devlinet al., 2018), we use the top-level representationhcls of bert’s special cls token to performclassiﬁcation.
to combine our model with bert,we replace the text encoder of himatch with bert,and the label representations are initiated by pre-trained bert embedding.
the batch size is set to16, and the learning rate is 2e-5..comparison models on rcv1-v2 and wosdatasets, we compare our model with three typesof strong baselines: 1) text classiﬁcation baselines:textrcnn (lai et al., 2015), textrcnn with labelattention (textrcnn-la) (zhou et al., 2020), andsgm (yang et al., 2018).
2) hierarchy-aware mod-els: he-agcrcnn (peng et al., 2019), hmcn(mao et al., 2019), htrans (banerjee et al., 2019),hilap-rl (mao et al., 2019) which introduced re-inforcement learning to simulate the assignmentprocess, hiagm (zhou et al., 2020) which ex-ploited the prior probability of label dependeciesthrough graph convolution network and treel-stm.
3) pretrained language model: a more power-ful pretrained language model bert (devlin et al.,2018) than tradition text classiﬁcation models whenﬁne-tuned on downstream tasks..on eurlex-57k dataset, we compare ourmodel with strong baselines with/without zero-shot settings such as bigru-att, bigru-lwan(chalkidis et al., 2019) which introduced label-wise attention.
the models starting with “zero”make predictions by calculating similarity scoresbetween text and label semantics for zero-shot set-.
tings.
agru-kamg (lu et al., 2020) is a state-of-the-art model which introduced various labelknowledge..4.2 experiment results.
models.
micro macro.
baselines.
textrcnn (zhou et al., 2020).
81.57 59.25textrcnn-la (zhou et al., 2020) 81.88 59.8577.30 47.49.sgm (zhou et al., 2020).
hierarchy-aware modelshe-agcrcnn (peng et al., 2019) 77.80 51.3080.80 54.6080.51 58.4983.30 60.1083.96 63.3584.73 64.11.hmcn (mao et al., 2019)htrans (banerjee et al., 2019)hilap-rl (mao et al., 2019)hiagm (zhou et al., 2020)himatch.
pretrained language models.
bert (devlin et al., 2018)bert+himatch.
86.26 67.3586.33 68.66.table 2: the experimental results comparing to otherstate-of-the-art models on rcv1-v2 dataset..models.
micro macro.
baselines.
77.94 69.65textrnn (zhou et al., 2020)textcnn (zhou et al., 2020)82.00 76.18textrcnn (zhou et al., 2020) 83.55 76.99hierarchy-aware models.
hiagm (zhou et al., 2020)himatch.
85.82 80.2886.20 80.53.pretrained language models.
bert (devlin et al., 2018)bert+himatch.
86.26 80.5886.70 81.06.table 3: the experimental results comparing to otherstate-of-the-art models on web-of-science dataset..table 2, 3 and 4 report the performance of ourapproaches against other methods.
hiagm is aneffective baseline on rcv1-v2 and wos due tothe introduction of holistic label information.
how-ever, they ignored the semantic relationship be-tween text and labels.
our model achieves thebest results by capturing the matching relationshipsamong text and labels in a hierarchy-aware manner,which achieves stronger performances especiallyon macro-f1.
the improvements show that ourmodel can make better use of structural informa-tion to help imbalanced htc classiﬁcation..the pretrained language model bert is an ef-fective method when ﬁne-tuned on downstreamtasks.
compared with the results regarding htc.
4375frequent.
few.
zero.
overall.
r@5 ndcg@5 r@5 ndcg@5 r@5 ndcg@5 r@5 ndcg@50.740bigru-att (chalkidis et al., 2019)0.755bigru-lwan (chalkidis et al., 2019)zero-cnn-lwan (chalkidis et al., 2019)0.683zero-bigru-lwan (chalkidis et al., 2019) 0.7160.7310.769.agru-kamg (lu et al., 2020)himatch.
0.0510.0290.3210.4380.5280.399.
0.0270.0190.2640.3450.4140.372.
0.5960.6610.4940.5600.5630.697.
0.6750.6920.6170.6480.6610.705.
0.7890.7960.7170.7520.7660.807.
0.8130.8190.7450.7800.7950.830.
0.5800.6180.4540.5100.5180.648.table 4: the experimental results comparing to other state-of-the-art models on eurlex-57k dataset..as multiple binary classiﬁers, our results show thatthe full use of structured label hierarchy can bringgreat improvements to bert model on rcv1-v2and wos datasets..on eurlex57k dataset, our model achievesthe best results on different matrics except for zero-shot labels.
the largest improvements come fromfew-shot labels.
agru-kamg achieves the bestresults on zero-shot labels by fusing various knowl-edge such as label semantics similarities and labelco-occurrence.
however, our model performs se-mantics matching among seen labels based on train-ing corpora, which is not designed for a speciﬁczero-shot learning task..4.3 analysis.
4.3.1 ablation studyin this section, we investigate to study the inde-pendent effect of each component in our proposedmodel.
firstly, we validate the inﬂuence of twoproposed losses, and the hierarchy-aware sampling.
the results are reported in table 5. the resultsshow that f1 will decrease with removing jointembedding loss or matching learning loss.
jointembedding loss has a great inﬂuence since labelsemantics matching relies on the joint embedding.
besides, in the hierarchy-aware margin subsection,we perform hierarchy-aware sampling by samplingcoarse-grained labels, incorrect sibling labels, andother incorrect labels as negative label sets.
whenwe remove hierarchy-aware sampling and replaceit with random sampling, the results will decrease,which shows the effectiveness of hierarchy-awaresampling..4.3.2 hyperparameters studyto study the inﬂuence of the hyperparameters γ, α,and β, we conduct seven experiments on rcv1-v2 dataset.
the results are reported in table 6.the ﬁrst experiment is the best hyperparameters ofour model.
then we ﬁne-tune the matching learn-ing margin γ in experiments two and three.
we.
micro macroablation models81.57 59.25textrcnn84.73 64.11himatch- w/o joint embedding loss84.49 62.57- w/o matching learning loss84.46 63.58- w/o hierarchy-aware sampling 84.67 63.45.table 5: ablation study on rcv1-v2 dataset..n o..γ.α.micro macro.
βhimatch0.5.
(cid:172).
(cid:173)(cid:174).
(cid:175)(cid:176)(cid:177)(cid:178).
0.2.
0.01.
84.73.
64.11.fine-tuning γ.
0.50.5.
0.010.01fine-tuning α, β0.510.010.5.
0.0110.010.5.
84.5184.69.
84.5284.3784.4984.47.
0.022.
0.20.20.20.2.
63.2663.55.
63.3563.4563.2064.02.table 6: hyperparameter study on rcv1-v2 dataset..ﬁnd that a proper margin γ = 0.2 is beneﬁcialfor matching learning compared with a large orsmall margin.
furthermore, we validate the effec-tiveness of the hierarchy-aware margin.
in exper-iment four, the performance will decrease if weviolate the hierarchical structure by setting a largepenalty margin for coarse-grained labels, and set-ting a small penalty margin for incorrect siblinglabels.
in experiment ﬁve, the performance hasa relatively larger decrease if we set α = 1 andβ = 1, which ignores hierarchical structure com-pletely.
we speculate that the penalty margin thatviolates the hierarchical structure will affect theresults, since the semantics relationship should becloser if the labels are closer in the hierarchicalstructure.
moreover, we validate the effectivenessof different penalty margins among different gran-ular labels.
in experiments six and seven, the re-sults will degrade if we ignore the relationshipsbetween coarse-grained target labels and incorrectsibling labels, by setting the same margin for α and.
4376figure 4: figure a) is a part of the hierarchical label structure.
figure b) is the t-sne visualization of textrepresentations and label representations of the labels in figure a) by introducing joint embedding loss.
figure c)is the t-sne visualization with both joint embedding loss and matching learning loss..such as labels gwelf, e61 are far away from c17,c171, c172.
besides, the text representations ofsemantically similar labels (c171 and c172) arefar away relatively compared with figure b).
thet-sne visualization shows that our model can cap-ture the semantics relationship among texts, coarse-grained labels, ﬁne-grained labels and unrelatedlabels..4.3.4 performance study on label.
granularity.
we analyze the performance with different la-bel granularity based on their hierarchical levels.
we compute level-based micro-f1 and macro-f1scores of the rcv1-v2 dataset on textrcnn, hi-agm, and our model in figure 5. on rcv1-v2dataset, both the second and third hierarchical lev-els contain ﬁne-grained labels (leaf nodes).
thesecond level has the largest number of labels andcontains confusing labels with similar concepts, soits micro-f1 is relatively low.
both the second andthird levels contain some long-tailed labels, so theirmacro-f1 are relatively low.
figure 5 shows thatour model achieves a better performance than othermodels on all levels, especially among deep levels.
the results demonstrate that our model has a betterability to capture the hierarchical label semantic,especially on ﬁne-grained labels with a complexhierarchical structure..4.3.5 computational complexity.
in this part, we compare the computational com-plexity between hiagm and our model.
for timecomplexity, the training time of himatch is 1.11times that of hiagm with batch size 64. for spacecomplexity during training, himatch has 37.4m pa-rameters, while hiagm has 27.8m.
the increasemainly comes from the label encoder with large.
figure 5: performance study on label granularity basedon hierarchical levels..β. therefore, it is necessary to set a small penaltymargin for coarse-grained target labels, and a largerpenalty margin for incorrect sibling labels..4.3.3 t-sne visualization of joint.
embedding.
we plot the t-sne projection of the text repre-sentations and label representations in the jointembedding in figure 4. figure a) is a part of the hi-erarchical label structure in rcv1-v2.
label c171and c172 are ﬁne-grained labels, and label c17 iscoarse-grained label of c171 and c172.
gwelfand e61 are other labels with different semanticswith c17, c171 and c172.
in figure b), by intro-ducing joint embedding loss, we can see that thetext representations are close to their correspond-ing label representations.
furthermore, the textrepresentations of labels c171 and c172 are closeto the label representation of their coarse-grainedlabel c17.
however, the text representations ofdifferent labels may overlap, since the matchingrelationships among different labels are ignored.
infigure c), by introducing both joint embedding lossand matching learning loss, the text representationsof different labels are more separable.
other unre-lated text representations and label representations.
4377a) label hierarchy.
.
.c17: funding/captialc172: bonds/debt issuesb) joint embedding lossxlabel representationstext representationsc171: share captialgwelf: welfare/social  servicese61: housing  startsgwelfe61c171c17c172gwelfe61c171c172c) joint embedding loss and matching  learning losslabel sets.
however, during testing, the time andspace complexity of himatch is the same as hi-agm.
the reason is that only the classiﬁcationresults are needed, and we can remove the jointembedding.
himatch achieves new state-of-the-artresults, and we believe that the increase of compu-tational complexity is acceptable..5 conclusion.
here we present a novel hierarchical text classiﬁca-tion model called himatch that can capture seman-tic relationships among texts and labels at differentabstraction levels.
instead of treating htc as mul-tiple binary classiﬁcation tasks, we consider thetext-label semantics matching relationship and for-mulate it as a semantic matching problem.
we learna joint semantic embedding between text and labels.
finally, we propose a hierarchy-aware matchingstrategy to model different matching relationshipsamong coarse-grained labels, ﬁne-grained labelsand incorrect labels.
in future work, we plan to ex-tend our model to the zero-shot learning scenario..acknowledgments.
61502174, and 61872148),.
we thank the anonymous reviewers for theirhelpful feedbacks.
the work described in thispaper was partially funded by the nationalnatural science foundation of china (grantno.
the natu-ral science foundation of guangdong province(grant no.
2017a030313355, 2019a1515010768and 2021a1515011496),the guangzhou sci-ence and technology planning project (grantno.
201704030051, and 201902010020), thekey r&d program of guangdong province (no.
2018b010107002) and the fundamental researchfunds for the central universities..references.
aixin sun and ee-peng lim.
2001. hierarchicalin proceedingstext classiﬁcation and evaluation.
2001 ieee international conference on data min-ing, pages 521–528..siddhartha banerjee, cem akkaya, francisco perez-sorrosal, and kostas tsioutsiouliklis.
2019. hier-archical transfer learning for multi-label text classi-in proceedings of the 57th annual meet-ﬁcation.
ing of the association for computational linguis-tics, pages 6295–6300, florence, italy.
associationfor computational linguistics..ilias chalkidis, emmanouil fergadiotis, prodromosand ion androutsopoulos.
2019.malakasiotis,large-scale multi-label text classiﬁcation on eu leg-in proceedings of the 57th annual meet-islation.
ing of the association for computational linguis-tics, pages 6314–6322, florence, italy.
associationfor computational linguistics..boli chen, xin huang, lin xiao, zixin cai, and lipingjing.
2020. hyperbolic interaction model for hierar-chical multi-label classiﬁcation.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 7496–7503..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..johannes f¨urnkranz, eyke h¨ullermeier, eneldoloza menc´ıa, and klaus brinker.
2008. multilabelclassiﬁcation via calibrated label ranking.
mach.
learn., 73(2):133–153..k. kowsari, d. e. brown, m. heidarysafa, k. jafarimeimandi, m. s. gerber, and l. e. barnes.
2017.hdltex: hierarchical deep learning for text classiﬁ-cation.
in 2017 16th ieee international conferenceon machine learning and applications (icmla),pages 364–371..siwei lai, liheng xu, kang liu, and jun zhao.
2015.recurrent convolutional neural networks for textin proceedings of the aaai confer-classiﬁcation.
ence on artiﬁcial intelligence, volume 29..david d. lewis, yiming yang, tony g. rose, and fanli.
2004. rcv1: a new benchmark collection fortext categorization research.
5:361–397..jueqing lu, lan du, ming liu, and joanna dip-nall.
2020. multi-label few/zero-shot learning withknowledge aggregated from multiple label graphs.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 2935–2943, online.
association for computa-tional linguistics..yuning mao, jingjing tian, jiawei han, and xiangren.
2019. hierarchical text classiﬁcation with re-in proceedings of theinforced label assignment.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language process-ing (emnlp-ijcnlp), pages 445–455, hong kong,china.
association for computational linguistics..hao peng, jianxin li, qiran gong, senzhang wang,lifang he, bo li, lihong wang, and philip s. yu.
2019. hierarchical taxonomy-aware and attentionalgraph capsule rcnns for large-scale multi-label textclassiﬁcation.
corr, abs/1906.04898..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conference.
4378on empirical methods in natural language process-ing (emnlp), pages 1532–1543..bingyu wang, li chen, wei sun, kechen qin, kefengli, and hui zhou.
2019. ranking-based autoen-coder for extreme multi-label classiﬁcation.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 2820–2830,minneapolis, minnesota.
association for computa-tional linguistics..lin xiao, xin huang, boli chen, and liping jing.
2019. label-speciﬁc document representation forin proceedings ofmulti-label text classiﬁcation.
the 2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language process-ing (emnlp-ijcnlp), pages 466–475, hong kong,china.
association for computational linguistics..pengcheng yang, xu sun, wei li, shuming ma, weiwu, and houfeng wang.
2018. sgm: sequenceingeneration model for multi-label classiﬁcation.
proceedings of the 27th international conference oncomputational linguistics, pages 3915–3926, santafe, new mexico, usa.
association for computa-tional linguistics..chih-kuan yeh, wei-chieh wu, wei-jen ko, and yu-chiang frank wang.
2017. learning deep latentspace for multi-label classiﬁcation.
in proceedingsof the aaai conference on artiﬁcial intelligence,volume 31..jie zhou, chunping ma, dingkun long, guangwei xu,ning ding, haoyu zhang, pengjun xie, and gong-shen liu.
2020. hierarchy-aware global model forhierarchical text classiﬁcation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 1106–1117, online.
as-sociation for computational linguistics..4379