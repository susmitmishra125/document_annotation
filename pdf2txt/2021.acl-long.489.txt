fine-grained information extraction from biomedical literature basedon knowledge-enriched abstract meaning representation.
zixuan zhang1, nikolaus parulian1, heng ji1,ahmed s. elsayed2, skatje myers2, martha palmer21university of illinois at urbana-champaign2university of colorado boulder{zixuan11, nnp2, hengji}@illinois.edu{ahmed.s.elsayed, skatje.myers, martha.palmer}@colorado.edu.
abstract.
biomedical information extraction from sci-entiﬁc literature presents two unique and non-trivial challenges.
first, compared with gen-eral natural language texts, sentences from sci-entiﬁc papers usually possess wider contextsbetween knowledge elements.
moreover, com-prehending the ﬁne-grained scientiﬁc entitiesand events urgently requires domain-speciﬁcbackground knowledge.
in this paper, we pro-pose a novel biomedical information extrac-tion (ie) model to tackle these two challengesand extract scientiﬁc entities and events fromenglish research papers.
we perform abstractmeaning representation (amr) to compressthe wide context to uncover a clear semanticstructure for each complex sentence.
besides,we constructthe sentence-level knowledgegraph from an external knowledge base anduse it to enrich the amr graph to improve themodel’s understanding of complex scientiﬁcconcepts.
we use an edge-conditioned graphattention network to encode the knowledge-enriched amr graph for biomedical ie tasks.
experiments on the genia 2011 dataset showthat the amr and external knowledge havecontributed 1.8% and 3.0% absolute f-scoregains respectively.
in order to evaluate the im-pact of our approach on real-world problemsthat involve topic-speciﬁc ﬁne-grained knowl-edge elements, we have also created a newontology and annotated corpus for entity andevent extraction for the covid-19 scientiﬁcliterature, which can serve as a new benchmarkfor the biomedical ie community.1.
entities, relations, and key events.
it is an essen-tial task for accelerating practical applications ofthe results and achievements from scientiﬁc re-search.
for example, practical progress on combat-ing covid-19 depends highly on efﬁcient trans-mission, assessment and extension of cutting-edgescientiﬁc research discovery (wang et al., 2020a;lybarger et al., 2020; möller et al., 2020).
in thisscenario, a powerful biomedical ie system will beable to create a dynamic knowledge base from thesurging number of relevant papers, making it moreefﬁcient to get access to the latest knowledge anduse it for scientiﬁc discovery, as well as diagnosisand treatment of patients..ie from biomedical scientiﬁc papers presentstwo unique and non-trivial challenges.
first, theauthors of scientiﬁc papers tend to compose longsentences, where the event triggers and entity men-tions are usually located far away from each otherwithin the sentence.
as shown in table 1, we cansee that compared to the ace05 dataset in newsdomain, the average distance between triggers andentities is much longer in biomedical scientiﬁc pa-pers.
therefore, it is more difﬁcult for ie models tocapture the global context with only ﬂat sequentialsentence encoders such as biobert (lee et al.,2020) and scibert (beltagy et al., 2019)..dataset.
average distance maximal distance.
ace05-e.0.212 sentence.
genia-2011.
0.330 sentence.
56 words.
77 words.
1.introduction.
the task of biomedical information extraction (ie)aims to extract structured knowledge from biomed-ical literature, which is usually represented by aninformation network composed of scientiﬁc named.
1data and source code are publicly available at https:.
//github.com/zhangzx-uiuc/knowledge-amr..table 1: comparison of the average and maximum dis-tance between each event-argument pair in news do-main (ace-05 dataset) and scientiﬁc papers (genia-2011 dataset) with the same sentence tokenizer..moreover, comprehending sentences from scien-tiﬁc papers urgently requires external knowledge,because there are a number of domain-speciﬁc un-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6261–6270august1–6,2021.©2021associationforcomputationallinguistics6261sentences with distant trigger and entity pairs, weuse amr parsing to compress each sentence and tobetter capture global interactions between tokens.
for example, as shown in figure 1, the positiveregulation event trigger “changes” is located faraway from its arguments ctf, otf-1, otf-2 inthe original sentence.
however, in the amr graph,such trigger-entity pairs are linked within two hops.
therefore, it will be much easier for the model toidentify such kinds of events with the guidance ofamr parsing..in addition, to make better use of the externalknowledge, we extract a global knowledge graphfrom the comparative toxicogenomics database(ctdb) that covers all biomedical entities in thecorpus.
for each sentence, we select a minimalconnected subgraph as the sentence-level kg.
weuse this sentence kg to enrich amr nodes andedges to give the model additional prior domainknowledge, especially the biomedical and chemi-cal interactions between different genes and pro-teins.
these ﬁne-grained relations are importantfor biomedical event extraction.
for example, as infigure 1, the incorporation of the external kg canindicate that mono mac 6 can result in leukemia,which will affect the expression of ctf, otf-1,and oft-2 proteins.
with this external knowledge,it will be much easier for the model to identifysuch proteins as the arguments of a positive regu-lation event.
we encode the knowledge-enrichedamr graph using an edge-conditioned graph atten-tion network (gat) that is able to incorporate ﬁne-grained edge features before conducting ie tasks.
we evaluate our model on the existing benchmarkgenia-2011 dataset where our model greatly out-performs our baseline model by 4.8%.
in additionto the existing genia-2011 benchmark, we alsoaim to evaluate the effectiveness of our frameworkon topic-speciﬁc literature.
we develop a new on-tology for entities and events with a large corpusfrom covid-19 research papers, which is speciﬁ-cally annotated by medical professionals and canserve as a new benchmark for the biomedical iecommunity..the major contributions of this paper are sum-.
marized as follows..• we are the ﬁrst to enrich the amr graph withthe external knowledge and use a graph neuralnetwork to incorporate the ﬁne-grained edgefeatures..• we evaluate our model and create a new state-.
figure 1: an illustrating example with ie annotationsand the kg-enriched amr graph from genia cor-pus.
note that we only include part of the kg-enrichedamr graph for conciseness..explained common expressions, acronyms, and ab-breviations that are difﬁcult for the model to un-derstand.
for instance, as shown in figure 1, it isnearly impossible for a typical end-to-end model,which only takes in the sentence as input, to getclear understanding of ctf, otf-1, and otf-2without background knowledge.
moreover, thecomplex biomedical and chemical interactions be-tween multifarious chemicals, genes, and proteinsare even harder to understand in addition to theentities themselves..to tackle these two challenges, we propose anovel framework for biomedical ie that integratesabstract meaning representation (amr) (ba-narescu et al., 2013) and external knowledgegraphs.
amr is a semantic representation lan-guage that converts the meaning of each input sen-tence into a rooted, directed, labeled, acyclic graphstructure.
amr semantic representation includespropbank (palmer et al., 2005) frames, non-coresemantic roles, coreference, entity typing and link-ing, modality, and negation.
the nodes in amr areconcepts instead of words, and the edge types aremuch more ﬁne-grained compared with traditionalsemantic languages like dependency parsing and se-mantic role labeling.
we train a transformer-basedamr semantic parser (fernandez astudillo et al.,2020) on biomedical scientiﬁc texts and use it inour biomedical ie model.
to better handle long.
6262we identified a cell-type-specific differential response: creb, ctf, otf-1,oft-2, and nf-kappa bgenes were strongly induced1 to 4 hours afterinfluenza a virus infection in the monocytic cell linemono mac 6, while infreshly prepared human monocytes no significant changeswere detected.proteinproteinproteinpositiveregulationpositiveregulationprotein“ctf”protein“otf-1”protein“otf-2”positiveregulation:“changes”positiveregulation:“induced”themethemethemethemethemethemeinduced-01changeand“ctf”“otf-1”“oft-2”afterinfect-01“monomac6”leukemiaresultsaffectsaffectsaffectskg-enrichedamrgraphoutputinformationnetworkkgnodesamrnodesof-the-art for biomedical event extraction onthe genia-2011 corpus..• we develop a new dataset from covid-19 re-lated research papers based on a new ontologythat contains 25 ﬁne-grained entity types and14 event types..2 approach.
2.1 overview.
as shown in figure 2, our proposed biomedicalinformation extraction framework mainly consistsof four steps.
first, we extract a global knowl-edge graph (kg) that contains all the entities fromthe corpus, and select out a sentence-level knowl-edge subgraph for the input sentence.
then, weperform amr parsing and construct the sentence-level amr graph, and use the sentence knowledgesubgraph to enrich the amr graph by adding ad-ditional nodes and edges.
after that, given thecontextualized word embeddings, we ﬁrst identifyentity and trigger spans, and then conduct messagepassing on the knowledge enriched amr graphbased on an edge-conditioned gat.
finally, we usefeed-forward neural networks based classiﬁers fortrigger and argument labeling..2.2 knowledge graph construction.
global knowledge graph we use the compar-ative toxicogenomics database (ctdb)2 whichcontains ﬁne-grained biomedical and chemical in-teractions between chemicals, genes, and diseases.
we construct a global knowledge graph that in-volves all entities from the corpus with their pair-wise chemical interactions.
we extract these entitypairs with their biomedical interactions as triples,e.g., in figure 1, (mono mac 6, results, leuku-mia) indicates that mono mac 6 cell can resultin the disease of leukemia.
we merge all the ex-tracted triples and form a global knowledge graphgg = (v g, eg).
our extracted global kg consistsof 39,436 nodes and 590,235 edges..sentence-level knowledge graph given an in-put sentence, we aim to generate a sentence-levelkg by selecting out a subgraph from the globalkg, which contains the external knowledge be-tween all entities within the sentence.
given aninput sentence s, we use scispacy3 to obtain allthe related biomedical entities, including genes,.
chemicals, cells, and proteins.
we then link eachentity mention from the sentence to the nodes inglobal kg gg = (v g, eg).
to select the sentencesubgraph from the global kg, given the set of en-tity mentions e = {ε1, · · · , ε|e|} (where each εiis a word span), we select the connected subgraphthat covers all entity mentions in e with the mini-mal number of nodes as the sentence kg.
note thatsuch a sentence kg construction procedure can beaccomplished in linear time complexity in termsof the number of nodes |v g|.
this can be doneby ﬁrst traversing all the nodes in the global kgusing depth-ﬁrst search and obtaining all connectedsubgraphs of gg in linear time.
after that, we se-lect the set of subgraphs that can cover e and thenchoose the one gs = (v s, es) with the minimalnumber of nodes as the sentence kg..2.3 kg-enriched amr parsing.
amr parsing after obtaining the sentence kg,we fuse it with the amr graph as an externalknowledge enrichment procedure.
given an in-put sentence s = {w1, w2, · · · , wn }, we ﬁrst per-form amr parsing and obtain a sentence-levelamr graph ga = (v a, ea) with an alignmentbetween amr nodes and the spans in the origi-nal sentence.
we employ the transformer-basedamr parser4 (fernandez astudillo et al., 2020)pretrained on the biomedical amr corpus5 re-leased from the amr ofﬁcial website.
each nodei ) ∈ v a represents an amr con-i = (mavacept or predicate, and we use (mai ) to denotethe corresponding span for such an amr node.
for amr edges, we use eai,j to denote the speciﬁcrelation type between nodes vaj in amr an-notations (e.g., arg-x, :time, :location, etc.).
werandomly initialize the edge embeddings as a look-up embedding matrix eamr, which is optimized inend-to-end training..i and va.i , na.
i , na.
enrich amr with sentence kg given a pairof amr graph ga and sentence kg gs, we fusethem into an enriched amr graph g = (v, e)as the external reference for the subsequent infor-mation extraction tasks.
in general, there are threei ∈ v scases for fusing each sentence’s kg nodes vsinto the amr graph.
first, if vsi represents an en-tity within the sentence, and there is also an amr.
4https://github.com/ibm/.
transition-amr-parser.
2http://ctdbase.org/3https://allenai.github.io/scispacy/.
5https://amr.isi.edu/download/2018-01-25/.
amr-release-bio-v3.0.txt.
6263figure 2: overview of our proposed framework for biomedical information extraction..j with the same span, we then match vs.node vai tovaj and add all kg edges linked to vsi into the amrgraph.
second, if vsi represents an entity within thesentence, but there is not any amr node vaj witha matched span, we then add a new node (as wellas all related edges) into the amr graph.
third,if vsi is an additional kg node that does not rep-resent any entity in the sentence, we directly addthis node into the amr graph with all related kgedges.
after we match and link all the sentence kgnodes towards the amr graph, we obtain the fusedgraph g = (v, e).
note that such a graph fusionprocedure could result in multiple edges between apair of nodes.
we keep all these edges with theirembeddings for the subsequent message passingprocedure.
the illustration for the graph fusionprocedure is shown in figure 2..2.4 node identiﬁcation and message passing.
contextualized encoder given an input sen-tence s, we use the bert model pretrained onbiomedical scientiﬁc texts (lee et al., 2020) toobtain the contextualized word representations{x1, x2, · · · , xn }.
if one word is split into mul-tiple pieces by the bert tokenizer, we take theaverage of the representation vectors for all piecesas the ﬁnal word representation..node identiﬁcation after encoding the inputsentence using bert, we ﬁrst identify the entityand trigger spans as the candidate nodes.
similarto (wadden et al., 2019), given the contextualizedword representations, we ﬁrst enumerate all possi-ble spans up to a ﬁxed length k, and calculate eachspan representation according to the concatenationof the left and right endpoints and a trainable fea-.
ture vector characterizing the span length6.
specif-ically, given each span si = [start(i), end(i)], thespan representation vector is:.
si = (cid:2)xstart(i), xend(i), z(si)(cid:3) ,.
(1).
where z(si) denotes a trainable feature vector thatis only determined by the span length.
we useseparate binary classiﬁers for each speciﬁc entityand trigger type to handle the spans with multiplelabels.
each binary classiﬁer is a feed-forwardneural network with relu activation in the hiddenlayer, which is trained with binary cross-entropyloss jointly with the whole model.
in the diagnosticsetting of using gold-standard entity mentions, weonly employ span enumeration for event triggeridentiﬁcation, and use the gold-standard entity setfor the following event extraction steps..edge-conditioned gat to fully exploit the in-formation of external knowledge and amr seman-tic structure, similar to (zhang and ji, 2021), weuse an l-layer graph attention network to let themodel aggregate neighbor information from thefused graph g = (v, e).
we use hli to denote thenode feature for vi ∈ v in layer l, and ei,j to repre-sent the edge feature vector for ei,j ∈ e. to updatethe node feature from l to l + 1, we ﬁrst calculatethe attention score for each neighbor j ∈ ni basedon the concatenation of node features hlj andedge features ei,j..i, hl.
αl.
i,j =.
(cid:80).
(cid:16).
(cid:16).
f l[whlσexp (cid:0)σ (cid:0)f l[whl.
exp.
k∈ni.
i : weei,j : whlj].
(cid:17)(cid:17).
i : weei,k : whl.
k](cid:1)(cid:1) ,.
6we use different maximum span length k for entity and.
trigger spans..6264werestronglyinducednosignificantchanges……,ctfotf-1oft-2bertembeddingsnodeidentificationglobalkgsentencekgamrgraphenrichedamrgraphieresultsmessagepassing……………………corpusentitylinkingexternalkbsentenceamrparsinglookupfusewhere w, we are trainable parameters, and f l andσ(·) are a single layer feed-forward neural networkand leakyrelu activation function respectively.
then we obtain the neighborhood information h∗iby the weighted sum of all neighbor features:.
h∗.
i =.
i,jw∗hlαlk,.
(cid:88).
k∈ni.
where w∗ is a trainable parameter.
the updatednode feature is calculated by a combination of theoriginal node feature and its neighborhood informa-tion, where γ controls the level of message passingbetween neighbors..hl+1i = hl.
i + γ · h∗i.
(2).
note that our edge-conditioned gat structure issimilar to (huang et al., 2020).
the main differenceis that (huang et al., 2020) only uses edge featuresfor calculating the attention score αli,j, while weuse the concatenation of the feature vectors of eachedge and its involved pair of nodes.
such a methodcan better characterize differing importance levelsfor neighbor nodes, and thus yield better modelperformance.
we select the last layer hli as theﬁnal representation for each entity or trigger..message passing given the knowledge enrichedamr graph g = (v, e) and representation vectorsof extracted trigger and entity spans, we initializethe feature vectors for nodes and edges as follows.
for each kg node vsi which does not belong to anyamr node, we initialize its feature vectors vsi us-ing kg embeddings pre-trained on the global kgusing transe (bordes et al., 2013).
for each origi-i , nanal amr node vai = (mai ), we ﬁrst calculateits span representation vai according to eq.
(1), andthen use a linear transformation wavai + ba toinitialize the node feature vector h0i .
for edge fea-tures, we use pre-trained transe embeddings forkg edges, and use the trainable embedding matrixeamr for amr relations.
we use our proposededge-conditioned gat to conduct message passingand get the feature vectors from the ﬁnal layer asthe updated node representations.
we obtain theﬁnal representation vectors for the trigger and en-tity nodes and denote them as {τ1, · · · , τ|t |} and{ε1, · · · , ε|e|} respectively..entity set e with the representations εi, we use lito denote the loss for binary classiﬁers for eventtrigger and entity extraction in the node identiﬁ-cation step.
for event argument role labeling, weconcatenate candidate trigger-entity pairs or trigger-trigger pairs (for nested events) and feed them intotwo separate ffns (with softmax activation func-tion in the output layer) for role type classiﬁca-tion, where we have ytti,j = ffntt ([τi : τj]) orytei,j = ffnte ([τi : εj]).
the overall training ob-jective is deﬁned in a multi-task setting, whichincludes the cross-entropy loss for trigger and ar-gument classiﬁcation, as well as the binary classiﬁ-cation loss li .
(cid:88).
(cid:88).
i,j log ˆyttytt.
i,j −.
i,j log ˆyteyte.
i,j.
(3).
l = li −.
i,j.
i,j.
3 experiments.
3.1 experimental setup.
data similarly to the recent work (li et al., 2019;huang et al., 2020; ramponi et al., 2020), wealso conduct experiments on the bionlp genia2011 (kim et al., 2011) dataset consisting of bothabstracts and main body texts from biomedical sci-entiﬁc papers.
similarly to previous work (li et al.,2019; huang et al., 2020; ramponi et al., 2020),we only focus on extracting the core events, whichinvolves protein entities, 9 ﬁne-grained event types,and 2 event argument types.
we do not incorpo-rate event ontology or training data from the newerversions of the bionlp genia shared tasks (e.g.,genia 2013) to ensure fair comparisons with pre-vious models.
the statistics of this dataset areshown in table 2. the original genia dataset.
data split.
train set dev set.
test set.
# documents.
# sentences.
# proteins.
# events.
908.
8,620.
11,625.
10,310.
259.
2,846.
4,690.
3,250.
231.
3,348.
5,301.
4,487.table 2: genia 2011 dataset statistics..is annotated in paragraphs.
following (li et al.,2019), we focus on sentence-level event extractionand only keep events and argument roles withineach sentence (around 94% of the events)..2.5 biomedical event extraction.
model training given the event trigger set twith the event trigger representations τi, and the.
implementation details for pretrained kg em-beddings, we use 600-dim embedding vectorspre-trained on the global knowledge graph using.
6265transe.
we use a two-layer edge-conditioned gatand the feature dimensions are 2048 for nodes and256 for edges.
speciﬁcally, the ffns consist of twolayers with a dropout rate of 0.4, where the num-bers of hidden units are 150 for entity extractionand 600 for event extraction.
we train our modelwith adam (kingma and ba, 2015) on nvidiatesla v100 gpus for 80 epochs (approximatelytakes 4 minutes for 1 training epoch) with learn-ing rate 1e-5 for bert parameters and 5e-3 forother parameters.
we select the model checkpointwith optimal f1-score on the development set toevaluation on the test set from the ofﬁcial website..3.2 baselines and ablation variants.
we consider the most recent models on biomedicalevent extraction: kb-tree-lstm (li et al., 2019),geanet (huang et al., 2020), beesl (ramponiet al., 2020), and deepeventmine (trieu et al.,2020) for comparison in our experiments, and wereport the precision, recall, and f1 score from thegenia 2011 online test set evaluation service7.
in addition to the previous models, we also con-duct ablation studies to evaluate the contributionsof different parts in our model.
we adopt the modelvariants bert-flat and bert-amr, where bert-flat only uses the bert representations withoutany help from amr and kg, and bert-amr de-notes the model with an edge-conditioned gatto encode the amr graph without incorporatingexternal knowledge..3.3 overall performance.
we report the performance of our model and com-pare it with the most recent biomedical ie modelskb-tree-lstm (li et al., 2019), geanet (huanget al., 2020), beesl (ramponi et al., 2020), anddeepeventmine (trieu et al., 2020) in table 3. ingeneral, our kg enriched amr model can achieveslightly higher performance compared with thestate-of-the-art model deepeventmine.
besides,our model greatly outperforms all other previousmodels for biomedical event extraction.
to fur-ther measure the impact of each individual partin our model, we also introduce two model vari-ants for the ablation study.
we can see that com-pared with simply ﬁnetuning a ﬂat bert model,the amr parsing contributes a 1.84% absolute gainon f1-score, while the incorporation of external.
7http://bionlp-st.dbcls.jp/ge/2011/.
eval-test/.
knowledge graph contributes 2.95%.
we also re-port the overall development set f1 scores withoutusing gold-standard entities, and compare the per-formance with beesl in table 4. we can discoverthat our model performs signiﬁcantly better thanthe beesl model, which proves that our modelcan better handle practical scenarios without gold-standard entities..model.
string matching.
tree-lstm (li et al., 2019)geanet (huang et al., 2020)beesl (ramponi et al., 2020)deepem (trieu et al., 2020).
bert-flatbert-amrbert-amr-kg (ours).
prec.
43.92.
67.0164.6169.7271.71.
64.6868.3972.74.rec.
21.82.
52.1456.1153.0056.20.
52.9853.5855.62.f1.
29.16.
58.6560.0660.2263.02.
58.2560.0963.04.table 3: overall test f-score (%) of biomedical extrac-tion on genia 2011 dataset..model.
f1-score.
beesl (w/o gold-standard entities)ours (w/o gold-standard entities).
59.5160.16.table 4: overall dev f-score (%) of biomedical ex-traction on genia 2011 dataset without using gold-standard entities..3.4 case study on covid-19 dataset.
covid-19 datasetin order to evaluate the im-pact of our approach on real-world problems, be-sides the genia dataset, we also develop a newdataset speciﬁcally labeled by medical profession-als from research papers related to covid-19.
weselect out 186 full-text articles with 12,916 sen-tences from pubmed and pmc.
three experiencedannotators who are biomedical domain expertshave participated in the annotation, and the cohen’skappa scores for pairwise agreement between theannotators are 0.79, 0.84, and 0.74 respectively.
the pre-deﬁned entity and event type distributionsin this dataset are shown in table 6..results we evaluate our proposed model by re-moving the event argument labeling procedure toaccommodate a scenario limited to entity and eventtrigger labeling, that is, we remove the argumentrole classiﬁers ffntt and ffnte while the overalltraining loss in eq.
(3) only contains the ﬁrst twoterms for span identiﬁcation and event trigger clas-siﬁcation.
as shown in table 5, our model achieves78.05% overall f1 score with 83.60% f1 on entityextraction task and 72.37% f1 on event extraction..6266the entity extraction performance on the coviddataset is lower than typical coarse-grained entityextraction model performance for bert-like mod-els on other datasets (e.g., our model can get around86% f1 score for entity extraction on genia-2011development set).
this is probably because our pro-posed covid-19 dataset is challenging with moreﬁnd-grained biomedical entity and event types..model.
bert-amr-kg (entities)bert-amr-kg (events).
bert-amr-kg (overall).
prec.
83.8972.47.
78.11.rec.
83.3272.27.
78.00.f1.
83.6072.37.
78.05.table 5: f-scores (%) on covid-19 test dataset forentity and event extraction..entities.
# labels.
events.
# labels.
biologicalprocessresearchactivitylabortestresulttherapeuticproceduresymptomorsigninfectioncontrolpharmacologicalactionepidemicdiagnosticprocedurediseasetransmissionlaboratorytechniquesbiologicalprocessenvironmentalexposuregeneticevolution.
6,7375,1774,6373,8192,5851,9371,5671,1801,01480752726224983.diseasemedicaldrugpatientschemicalhumancountrygene/proteinsars-cov-2organizationanatomicalstructuremedicalexperturbanareaorganismcoronavirusanimalfluidsandsecretionssars-covcellularcomponentgenemers-covprotectiveequipviralparticlecelllineantigenspecies.
6,2313,9013,4302,7192,1461,6101,5011,4521,1821,1301,105794666567460406341289272185184146663228.table 6: our new covid-19 ontology with 24 ﬁne-grained entity types and 15 biomedical event types..3.5 qualitative analysis.
we select two typical examples in table 7 to showhow kg enriched amr parsing helps to improvethe performance of biomedical ie..in the ﬁrst example, we can see that the ﬂatmodel fails to identify caii as an entity of the bindevent, which is probably due to the long distance be-tween the trigger bind and the argument caii (themodel successfully detects the other two argumentsv-erba and c-erba because they are much nearer).
with the help of amr parsing, the model success-fully links caii to the bind event since in the amrgraph, the three entities c-erba, v-erba, and caiiare located within the same number of hops fromthe bind trigger.
but the model still cannot rec-ognize caii as the theme of transcription.
this.
is probably because the model is not clear whatwhose refers to in the sentence.
however, with thehelp of external knowledge, the model knows inadvance that v-erba could inhibit the transcriptionof caii, thus it is able to identify caii as the themeof the transcription event..in the second example, the ﬂat model is confusedabout which entity belongs to which event betweentwo binding events in the same sentence.
here, theamr parsing provides a clear tree structure andguides the model to correctly link the event-entitypairs (i.e., heterodimers with rar beta, bindingwith vdr).
however, the bert-amr model stillfails to identify heterodimers as the theme of stim-ulated.
with the further help of the external kg,the model knows in advance that ra can stimulatethe generation of rar beta heterodimers, and thusit is able to correctly identify a positive regulationbetween these two triggers..3.6 remaining challenges.
we compare the predictions from our model withthe gold-standard annotations on the developmentset and discover the following typical remainingerror cases..non-verb event triggers most of the biomed-ical events are triggered by verbs (bind, express,etc.)
or their noun forms (binding, expression, etc.).
however, there are also events triggered by adjec-tives (e.g., subsequent), proper nouns (e.g., mrna,sirna), and even prepositions (e.g., from) and con-junctions (e.g., rather than).
our model missesa lot of these non-verb event triggers due to theinsufﬁcient training examples..misleading verb preﬁx we also ﬁnd that thepreﬁx of a verb can sometimes be misleading forevent trigger classiﬁcation, especially for nega-tive regulation events.
many negative regulationevents are triggered by words with certain stylesof preﬁx (in- or de-), e.g., inactivation, inactivated,decrease, degradation, etc., representing some neg-ative interactions.
as a result, the model mistakenlylabels many other words with the same preﬁxesas negative regulation event triggers.
for exam-ple, in the sentence: dephosphorylation of 4e-bp1was also observed ..., the word dephosphorylationshould not be classiﬁed as a negative regulationevent although it has a de- preﬁx.
because de-phosphorylation denotes an inverse chemical pro-cess of phosphorylation rather than negative regu-lation between different events or proteins.
this is.
6267sentence: here, we show that v-erba and c-erba bind directly to sequences within the promoter of the erythrocyte-speciﬁc carbonic anhydrase ii (caii), a gene whose transcription is efﬁciently suppressed by v-erba..sentence: concomitant stimulation by vitd3 inhibited the ra-stimulated formation of rar beta/rxr heterodimers,favoring vdr/rxr binding to the rare..table 7: examples from development set showing how kg enriched amr graph improves the model performance..probably because the bert tokenizer breaks thesewords into pieces de, phosphorylation, encouragingbert models to learn misleading patterns..4 related work.
biomedical information extraction a numberof previous studies contribute to biomedical eventextraction with various techniques, such as depen-dency parsing (mcclosky et al., 2011; li et al.,2019), external knowledge base (li et al., 2019;huang et al., 2020), joint inference of triggers andarguments (poon and vanderwende, 2010; ram-poni et al., 2020), abstract meaning representa-tion (rao et al., 2017), search based neural mod-els (espinosa et al., 2019), and multi-turn questionanswering (wang et al., 2020b).
recently, to han-dle the nested biomedical events, beesl (ram-poni et al., 2020) models biomedical event extrac-tion as a uniﬁed sequence labeling problem forend-to-end training.
deepeventmine (trieu et al.,2020) proposes to use a neural network based clas-siﬁer to decide the structure of complex nestedevents.
our model is also in an end-to-end train-ing pipeline, but additionally utilizes ﬁne-grainedamr semantic parsing and external knowledge toimprove the performance..utilization of external knowledgein terms ofutilization of external knowledge, (li et al., 2019)proposes a knowledge-driven tree-lstm frame-work to capture dependency structures and en-tity properties from an external knowledge base.
more recently, geanet (huang et al., 2020) in-.
troduces a graph edge conditioned attention net-work (geanet) that incorporates domain knowl-edge from the uniﬁed medical language system(umls) into the ie framework.
the main differ-ence of our model is that we use ﬁne-grained amrparsing to compress the wide context, and manageto use an external kg to enrich the amr to bet-ter incorporate domain knowledge.
incorporatingexternal knowledge is also widely used in othertasks such as relation extraction (chan and roth,2010; cheng and roth, 2013), and qa for domain-speciﬁc (science) questions (pan et al., 2019)..biomedical benchmarks for covid-19 (loet al., 2020) releases a dataset containing open-access biomedical papers related to covid-19.
alot of research has been done based on this dataset,including information retrieval (wise et al., 2020),entity recognition (wang et al., 2020b), distantsupervision on ﬁne-grained biomedical name en-tity recognition to support automatic informationretrieval indexing or evidence mining (wang et al.,2020c), and end-to-end question answering (qa)system for covid-19 with domain adaptive syn-thetic qa training (reddy et al., 2020).
ourcovid-19 dataset will further advance the ﬁeld indeveloping effective ie techniques speciﬁcally forthe covid-19 domain..5 conclusions and future work.
in this paper, we propose a novel biomedical infor-mation extraction framework to effectively tackletwo unique challenges for scientiﬁc domain ie:.
6268show-01suppress-01andkgnodesamrnodesc-erbacaiibind-01sequencev-erbav-erbatranscribe-01inhibitamrgraphenrichedbyexternalkgprotein“c-erba”protein“v-erba”protein“caii”transcription:“transcription”negativeregulation:“suppressed”binding:“bind”causethemethemeprotein“v-erba”bert-flatmodelpredictionsprotein“c-erba”protein“v-erba”protein“caii”transcription:“transcription”negativeregulation:“suppressed”themebinding:“bind”causethemethemethemeprotein“v-erba”bert-amrmodelpredictionsprotein“c-erba”protein“v-erba”protein“caii”transcription:“transcription”negativeregulation:“suppressed”themebinding:“bind”causethemethemethemeprotein“v-erba”bert-amr-kgmodelpredictionstheme（correct）kgnodesamrnodesinhibit-01favor-01resultsamrgraphenrichedbyexternalkgform-01bind-01stimulate-01heterodimerrararerar-betavdrprotein“rarbeta”protein“vdr”positiveregulation:“stimulated”binding:“heterodimers”themebert-flatmodelpredictionsbinding:“binding”themethemeprotein“rarbeta”protein“vdr”positiveregulation:“stimulated”binding:“heterodimers”themebert-amrmodelpredictionsbinding:“binding”themeprotein“rarbeta”protein“vdr”positiveregulation:“stimulated”binding:“heterodimers”themethemebert-amr-kgmodelpredictionsbinding:“binding”theme（correct）complex sentence structure and unexplained con-cepts.
we utilize amr parsing to compress widecontexts, and incorporate external knowledge intothe amr.
our proposed model produces signiﬁ-cant performance gains compared with most state-of-the-art methods.
in the future, we intend to ex-ploit tables and ﬁgures in the scientiﬁc literature formultimedia representation.
we also plan to furtherincorporate coreference graphs among sentencesto further enrich contexts.
we will also continueexploring the use of richer information from anexternal knowledge base to further improve themodel’s performance..acknowledgement.
this research is based upon work supported by themolecule maker lab institute: an ai researchinstitutes program supported by nsf under awardno.
2019897, nsf no.
2034562, u.s. darpakairos program no.
fa8750-19-2-1004, theofﬁce of the director of national intelligence(odni), intelligence advanced research projectsactivity (iarpa), via contract no.
fa8650-17-c-9116, and air force no.
fa8650-17-c-7715.
any opinions, ﬁndings and conclusions or recom-mendations expressed in this document are thoseof the authors and should not be interpreted as rep-resenting the ofﬁcial policies, either expressed orimplied, of the u.s. government.
the u.s. gov-ernment is authorized to reproduce and distributereprints for government purposes notwithstandingany copyright notation here on..references.
laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifﬁtt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse, law-id@acl 2013, august 8-9, 2013,soﬁa, bulgaria, pages 178–186..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁc text.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 3613–3618. association for computational linguistics..antoine bordes, nicolas usunier, alberto garcía-jason weston, and oksana yakhnenko..durán,.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,lake tahoe, nevada, united states, pages 2787–2795..yee seng chan and dan roth.
2010..exploitingbackground knowledge for relation extraction.
inproceedings of the 23rd international conferenceon computational linguistics (coling 2010), pages152–160..xiao cheng and dan roth.
2013. relational inferencein proceedings of the 2013 con-for wikiﬁcation.
ference on empirical methods in natural languageprocessing, pages 1787–1796, seattle, washington,usa.
association for computational linguistics..kurt junshean espinosa, makoto miwa, and sophiaananiadou.
2019. a search-based neural model forbiomedical nested and overlapping event detection.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 3677–3684..ramón fernandez astudillo, miguel ballesteros,tahira naseem, austin blodgett, and radu flo-rian.
2020. transition-based parsing with stack-in findings of the association fortransformers.
computational linguistics: emnlp 2020, pages1001–1007, online.
association for computationallinguistics..kung-hsiang huang, mu yang, and nanyun peng.
2020. biomedical event extraction with hierarchi-in findings of the associa-cal knowledge graphs.
tion for computational linguistics: emnlp 2020,pages 1277–1285, online.
association for compu-tational linguistics..jin-dong kim, yue wang, toshihisa takagi, and aki-nori yonezawa.
2011. overview of genia event taskin proceedings ofin bionlp shared task 2011.bionlp shared task 2011 workshop, pages 7–15,portland, oregon, usa.
association for computa-tional linguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020. biobert: a pre-trainedbiomedicalforbiomedical text mining.
bioinform., 36(4):1234–1240..language representation model.
6269diya li, lifu huang, heng ji, and jiawei han.
2019.biomedical event extraction based on knowledge-driven tree-lstm.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1421–1430, minneapolis, minnesota.
association for computational linguistics..k lo, y chandrasekhar, r reas, j yang, d eide,k funk, r kinney, z liu, w merrill, p mooney,et al.
2020. cord-19: the covid-19 open researchdataset.
arxiv..kevin lybarger, mari ostendorf, matthew thompson,and meliha yetisgen.
2020. extracting covid-19 di-agnoses and symptoms from clinical text: a newannotated corpus and neural event extraction frame-work..david mcclosky, mihai surdeanu, and christopher d.manning.
2011. event extraction as dependencyin the 49th annual meeting of the asso-parsing.
ciation for computational linguistics: human lan-guage technologies, proceedings of the conference,19-24 june, 2011, portland, oregon, usa, pages1626–1635..timo möller, anthony reina, raghavan jayakumar,and malte pietsch.
2020. covid-qa: a questionin proceedingsanswering dataset for covid-19.
of the 1st workshop on nlp for covid-19 at acl2020, online.
association for computational lin-guistics..martha palmer, dan gildea, and paul kingsbury.
2005.the proposition bank: a corpus annotated with se-mantic roles.
computational linguistics journal,31(1)..revanth gangi reddy, bhavani iyer, md arafat sultan,rong zhang, avi sil, vittorio castelli, radu flo-rian, and salim roukos.
2020. end-to-end qa oncovid-19: domain adaptation with synthetic train-ing.
arxiv preprint arxiv:2012.01414..hai-long trieu, thy thy tran, anh-khoa duongnguyen, anh nguyen, makoto miwa, and sophiaananiadou.
2020. deepeventmine: end-to-end neu-ral nested event extraction from biomedical texts.
bioinformatics, 36(19):4910–4917..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..qingyun wang, manling li, xuan wang, nikolausparulian, guangxing han, jiawei ma, jingxuantu, ying lin, haoran zhang, weili liu, aabhaschauhan, yingjun guan, bangzheng li, ruisongli, xiangchen song, heng ji, jiawei han, shih-fuchang, james pustejovsky, david liem, ahmed el-sayed, martha palmer, jasmine rah, cynthia schnei-der, and boyan onyshkevych.
2020a.
covid-19 lit-erature knowledge graph construction and drug re-purposing report generation.
in arxiv:2007.00576..xing david wang, leon weber, and ulf leser.
2020b.
biomedical event extraction as multi-turn questionanswering.
in proceedings of the 11th internationalworkshop on health text mining and informationanalysis, pages 88–96, online.
association for com-putational linguistics..xiaoman pan, kai sun, dian yu, jianshu chen, hengji, claire cardie, and dong yu.
2019.improvingquestion answering with external knowledge.
arxivpreprint arxiv:1902.00993..xuan wang, weili liu, aabhas chauhan, yingjunguan, and jiawei han.
2020c.
automatic textual ev-idence mining in covid-19 literature.
arxiv preprintarxiv:2004.12563..hoifung poon and lucy vanderwende.
2010..jointinference for knowledge extraction from biomedi-in human language technologies:cal literature.
the 2010 annual conference of the north ameri-can chapter of the association for computationallinguistics, pages 813–821, los angeles, california.
association for computational linguistics..alan ramponi, rob van der goot, rosario lombardo,and barbara plank.
2020. biomedical event extrac-in proceedings of thetion as sequence labeling.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 5357–5367,online.
association for computational linguistics..sudha rao, daniel marcu, kevin knight, and haldaumé iii.
2017. biomedical event extraction us-in bionlping abstract meaning representation.
2017, pages 126–135, vancouver, canada,.
associa-tion for computational linguistics..colby wise, vassilis n ioannidis, miguel romerocalvo, xiang song, george price, ninad kulkarni,ryan brand, parminder bhatia, and george karypis.
2020. covid-19 knowledge graph: accelerating in-formation retrieval and discovery for scientiﬁc liter-ature.
arxiv preprint arxiv:2007.12731..zixuan zhang and heng ji.
2021. abstract meaningrepresentation guided graph encoding and decodingfor joint information extraction.
in proceedings ofthe 2021 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 39–49, on-line.
association for computational linguistics..6270a appendices.
a.1.
implementation details.
for pretrained kg embeddings, we use 600-dimembedding vectors pre-trained on the global knowl-edge graph using transe.
we use a two-layer edge-conditioned gat and the feature dimensions are2048 for nodes and 256 for edges.
speciﬁcally, theffns consist of two layers with a dropout rate of0.4, where the numbers of hidden units are 150 forentity extraction and 600 for event extraction.
wetrain our model with adam (kingma and ba, 2015)on nvidia tesla v100 gpus for 80 epochs (ap-proximately takes 4 minutes for 1 training epoch)with learning rate 1e-5 for bert parameters and5e-3 for other parameters.
we select the modelcheckpoint with optimal f1-score on the devel-opment set to evaluation on the test set from theofﬁcial website.
the detailed hyper-parameter set-tings are shown in table 8..hyper-parameters.
values.
number of model parameters (except bert).
3.25m.
kg embedding dimensions.
num of features for each nodenum of features for amr relationnum of gat layersmessage passsing level γ.num of layers for ffnnsffnn hidden dimensions for entity extractionffnn hidden dimensions for event extractiondropout rateactivation function.
learning rate for bert paramslearning rate for other paramsbatch size.
600.
2,04825620.003.
21506000.4relu.
1e-51e-316.table 8: detailed settings for model hyper-parameters..