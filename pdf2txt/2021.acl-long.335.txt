are pre-trained convolutions better than pre-trained transformers?.
yi taygoogle researchmountain view, californiayitay@google.com.
mostafa dehghanigoogle research, brain teamamsterdam, netherlandsdehghani@google.com.
jai guptagoogle researchmountain view, californiajaigupta@google.com.
vamsi aribandi∗google researchmountain view, californiaaribandi@google.com.
dara bahrigoogle researchmountain view, californiadbahri@google.com.
zhen qingoogle researchmountain view, californiazhenqin@google.com.
donald metzlergoogle researchmountain view, californiametzler@google.com.
abstract.
in the era of pre-trained language models,transformers are the de facto choice of modelarchitectures.
while recentresearch hasshown promise in entirely convolutional, orcnn, architectures, they have not been ex-plored using the pre-train-ﬁne-tune paradigm.
in the context of language models, are con-volutional models competitive to transform-ers when pre-trained?
this paper investigatesthis research question and presents several in-teresting ﬁndings.
across an extensive set ofexperiments on 8 datasets/tasks, we ﬁnd thatcnn-based pre-trained models are competi-tive and outperform their transformer counter-part in certain scenarios, albeit with caveats.
overall,the ﬁndings outlined in this papersuggest that conﬂating pre-training and archi-tectural advances is misguided and that bothadvances should be considered independently.
we believe our research paves the way for ahealthy amount of optimism in alternative ar-chitectures..1.introduction.
in the modern era of pre-training, there appearsto be an unbreakable tie between transformer ar-chitectures (vaswani et al., 2017) and pre-trainedlanguage models.
models such as bert (devlinet al., 2018), roberta (liu et al., 2019), and t5(raffel et al., 2019) have all adopted transformersas their underlying architecture.
as a matter of fact,there are barely any recent pre-trained models notbased on transformers..while the contextual representation learning hasa rich history (pennington et al., 2014; dai and le,.
∗google ai resident.
2015; chidambaram et al., 2018; liu et al., 2020;qiu et al., 2020), modern pre-trained language mod-eling started with models like elmo (peters et al.,2018) and cove (mccann et al., 2017) which arebased on recurrent (e.g.
lstm (hochreiter andschmidhuber, 1997)) architectures.
although theywere successful, research using these architecturesdwindled as transformers stole the hearts of thenlp community, having, possibly implicitly, beenperceived as a unequivocal advancement over itspredecessors..recent work demonstrates the promise of en-tirely convolution-based models (wu et al., 2019;gehring et al., 2017) and questions the necessity ofself-attentive architectures like transformers.
forexample, in (wu et al., 2019), the proposed convo-lutional seq2seq models outperform transformerson a series of canonical benchmarks such as ma-chine translation and language modeling.
fromthese ﬁndings emerge a rather natural line of ques-tioning - should we consider pre-trained modelsbeyond transformers?.
despite early success, the relevance of convo-lutional models in the era of pre-trained languagemodels remains an open question.
to the best ofour knowledge, convolutional architectures havenot yet been rigorously evaluated under the pre-train-ﬁne-tune paradigm.
this is the primary pur-pose of this work.
concretely, this paper seeks toempirically validate whether pre-trained convolu-tions are competitive with pre-trained transformersacross a range of tasks..the interaction between pre-training schemesand model architectures is an under-studied topic.
are only transformers able to capitalize on the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4349–4359august1–6,2021.©2021associationforcomputationallinguistics4349beneﬁts of pre-training?
if we use a different ar-chitectural inductive bias, would there also be asubstantial gain unlocked by pre-training?
are pre-trained convolutions better in particular scenarios?
this paper investigates these questions..there are a number of obvious beneﬁts ofconvolution-based models.
firstly, convolutionsdo not suffer from the quadratic memory complex-ity of self-attention - a problem signiﬁcant enoughthat it spawned the creation of the entirely new cat-egory of “efﬁcient” transformer architectures (tayet al., 2020b, 2021).
secondly, convolutions oper-ate locally and do not rely on positional encodingsas an order signal to the model.
that said, convo-lutions also come with a slew of downsides.
forexample, being unable to access global informationmeans such models are unable to perform a form ofcross-attention across multiple sequences.
we diveinto the details of this more in subsequent sections.
in this paper, we present a pre-trained convolu-tional sequence-to-sequence, or seq2seq, model.
we train our convolutional model using span-basedsequence-to-sequence denoising objectives similarto those employed in t5 (raffel et al., 2019).
weevaluate a variety of convolutional variants (e.g., di-lated, lightweight, dynamic (wu et al., 2019), etc.)
under both raw (no pre-training) and pre-train-ﬁne-tune paradigms.
our goal is to understand the truecompetitiveness of convolutional architectures inthe era of pre-training..we show that pre-trained convolutions are com-petitive against pre-trained transformers via aset of experiments on a potpourri of nlp tasks,like toxicity detection, sentiment classiﬁcation,news classiﬁcation, query understanding and se-mantic parsing/compositional generalization (kimand linzen, 2020).
moreover, we ﬁnd that pre-trained convolutions can outperform, in terms ofmodel quality and training speed, state-of-the-artpre-trained transformers (raffel et al., 2019) incertain scenarios.
however, to provide a balancedperspective, we also describe scenarios where pre-trained convolutions do not perform well and maybe deemed unsuitable..contributions overall, the main contributionsof this paper can be summarized as follows:.
• we perform a comprehensive empirical evalu-ation of convolutional seq2seq models underthe pre-train-ﬁne-tune paradigm.
to the bestof our knowledge, the competitiveness and.
relevance of pre-trained convolutions still re-mains an open question..• we make several.
important observations.
speciﬁcally, we ﬁnd that (1) pre-training helpsconvolutional models just as much as it helpstransformers, and (2) pre-trained convolu-tions are competitive alternatives in certainscenarios in terms of model quality and train-ing speed..• we conduct extensive experiments across 8datasets spanning a diverse range of tasks anddomains.
on 7 out of 8 tasks, we ﬁnd thatpre-trained convolutions outperform a recentstate-of-the-art transformer (t5 (raffel et al.,2019)) with and without pre-training.
we ex-amine the speed and operation count (flops)of convolutions versus transformers and ﬁndthat convolutions are not only faster but alsoscale better to longer sequence lengths..2 related work.
pre-training on a large corpus has become the pri-mary method of learning universal language rep-resentations to solve different downstream nlptasks.
the ﬁrst generation of pre-trained mod-els aimed at learning embedding for words, likeskip-gram (mikolov et al., 2013) and glove (pen-nington et al., 2014), and quickly developed tolearning contextualized representation for words,like elmo (peters et al., 2018), gpt (radfordet al., 2018), and bert (devlin et al., 2018).
this,however, is not the only axis in which pre-trainedmodels have evolved..different objective functions and various tasks,both supervised and unsupervised, have been ex-plored for pre-training.
for instance, cove (mc-cann et al., 2017) uses machine translation as thepre-training task, elmo (peters et al., 2018) andgpt (radford et al., 2018) use language modelingobjectives, bert (devlin et al., 2018) uses maskedlanguage modeling, t5 (raffel et al., 2019) andmass (song et al., 2019) use seq2seq maskedlanguage modeling, and xlnet (yang et al., 2019)utilizes permuted language modeling.
in additionto this, bart (lewis et al., 2019) uses a denois-ing autoencoder setup during pre-training, wherethe model takes a partially corrupted input and istrained to recover the original, undistorted input.
some models use a contrastive learning setup dur-ing pertaining, like replaced token detection, used.
4350by electra (clark et al., 2020), and sentence or-der prediction, used by albert (lan et al., 2019)and structbert (wang et al., 2019)..another axis where pre-trained models in nlpexplored different ideas is model architecture.
elmo (peters et al., 2018) and cove (mccannet al., 2017) used lstms as the base model.
later,transformers (vaswani et al., 2017) became thede facto architecture of pre-trained nlp models.
bert (devlin et al., 2018), xlnet (yang et al.,2019) and roberta (liu et al., 2019) use thetransformer encoder, while gpt (radford et al.,2018), gpt-2 (radford et al.
), and gpt-3 (brownet al., 2020) use the transformer decoder as thebackbone.
some pre-trained models are also arebased on the encoder-decoder transformer archi-tecture, like t5 (raffel et al., 2019), mass (songet al., 2019), and bart (lewis et al., 2019).
in thispaper, we investigate another model architecturevariation by studying the power of convolutionalneural network as the backbone of pre-trained mod-els for nlp..convolutions have always been an interestingchoice for sequence modeling and nlp applica-tions (kim, 2014; bai et al., 2018; kalchbrenneret al., 2016).
convolutions are lightweight and fastand have many interesting use-cases, notably forlightweight classiﬁcation.
in the era when lstmswere the workhorses of nlp applications, convolu-tions were positioned nicely on the pareto frontierof the compute-performance curve.
they are fastand lightweight, and unlike transformers, they donot suffer from quadratic complexity.
our workis also well-aligned with the resurgence of interestin convolutions where (wu et al., 2019) showedthat convolutions can outperform self-attention onseveral sequence transduction tasks.
moreover,the necessity of the self-attention inductive biasin transformers have been also a subject of recentinterest.
synthesizer models (tay et al., 2020a)showed that transformers can still do pretty wellwithout token-token dot product self-attention anda random attention matrix can perform competi-tively on certain tasks..3 pre-trained convolution models.
this section describes the pre-trained convolutionmodel.
for most of our experiments, we adoptdepthwise separable convolutions (kaiser et al.,2017; sifre and mallat, 2014; chollet, 2017) whichhave shown to be fast and efﬁcient variants of the.
standard convolution..3.1 lightweight depthwise convolution.
this section introduces lightweight depthwiseconvolutions (wu et al., 2019) which forms thebackbone of our pre-trained convolution model..3.1.1 depthwise convolutionsdepthwise convolutions convolve independentlyover every channel.
given an input tensor xof dimensions n × d, the depthwise convolution,d(x, wc,:, i, c) is deﬁned as:.
oi,c =.
wc,j · xi+j−(cid:100) k+1.
2 (cid:101)), c.(1).
k(cid:88).
j−1.
where w ∈ rd×k are the learnable parametersof the layer.
oi,c is the output at position i andchannel c. the overall output is a tensor of n × dof identical shape as the input..3.1.2 lightweight convolutionsl(.)
are depthwise separable convolutions with (1)softmax-normalized kernels and (2) shared outputchannels and weight tying.
speciﬁcally, this iswritten as:.
ol.
i,c =.
k(cid:88).
j−1.
softmax(wˆc,j) · xi+j−(cid:100) k+1.
2 (cid:101)), ˆc (2).
where ˆc = chd .
in short, parameters are sharedevery dh output channels.
when h = 1, this isequivalent to sharing all the weights of all channels..3.1.3 dynamic convolutionsdynamic convolutions dy (.)
are a new form oflightweight convolutions introduced by (wu et al.,2019).
the key idea is to learn position-speciﬁckernels for performing lightweight convolutions.
this can be written as:.
dy = l(x, f (xi)h,:, i, c),.
(3).
where f (.)
is a linear transformation with param-eters w q ∈ rh×k×d that learns a position depen-dent kernel..3.2 span-based seq2seq pre-training.
we adopt span-based sequence-to-sequence pre-training as per (raffel et al., 2019).
speciﬁcally,given an input sequence, we randomly mask spansof lengths l and replace them with a special sen-tinel token.
the pre-training task is then to generatethe masked tokens as targets.
for example: inputs:the happy cat sat [mask].
and outputs: on the mat..43513.2.1 convolutional seq2seq architecture.
we implement a seq2seq (sutskever et al., 2014)architecture similar to (wu et al., 2019).
the keydifference when compared with transformer archi-tectures is that we replace the multi-headed self-attention with convolutional blocks.
instead ofquery-key-value transforms, we use gated linearunit projections following (wu et al., 2019).
eachconvolution block be written as:.
x 1 = w i x (cid:12) sigmoid(w sx),x 2 = convblock(x 1),x 3 = w o(x 2),.
where w i , w s, w o are trainable parameters.
weexperiment with simple lightweight convolutions,dynamic convolutions and dilated convolutionsin our experiments.
following (wu et al., 2019;gehring et al., 2017), the encoder-decoder atten-tion remains untouched.
the convention followsthe backbone transformer model in which we wrapeach submodule with layer normalization and resid-ual connectors.
hence, each conv block is writtenas:.
xa = layernorm(conv(x)) + x,xb = layernorm(ffn(xa) + xa,.
where conv is any of the convolution models thatwe explore in our experiments.
ffn(.)
is a twolayer feed-forward network with relu activationsin the middle..3.2.2 optimization.
the model optimizes the token-wise cross-entropyloss and is trained with teacher forcing..l =.
l(cid:88).
n(cid:88).
t=1.
i=1.
log(πt.
i) + (1 − yt.
i) log(1 − πt.
i),.
where πtt and yttime step t..i is the prediction of class i at time stepi is the ground truth label of the class i at.
4 research questions and discussion.
before we delve into our experiments, we establisha set of research questions and agenda we hope thiswork aims to bring clarity to..• rq2: are convolutional models, pre-trainedor otherwise, competitive with transformermodels?
when do they perform well?.
• rq3: what are the beneﬁts (if any) of us-ing pre-trained convolution models over pre-trained transformers?
are convolutions fasteralternatives to self-attention based transform-ers?.
• rq4: what are the failure modes, caveats andreasons to not use pre-trained convolutions?.
• rq5: are certain convolution variants better.
than others?.
5 experiments and analysis.
this section presents our analysis and results..5.1 datasets.
our evaluation is based on the following datasetsand tasks..• toxicity detection - we use the civil com-ments (borkan et al., 2019) and wiki toxicsubtypes dataset (wulczyn et al., 2017).
given a piece of short text (originating fromsocial media or wikipedia), the goal is to de-termine if the content is toxic, i.e., a binaryclassiﬁcation task.
for this task, we evaluateon both accuracy and f1 score..• sentiment classiﬁcation - this is a binaryclassiﬁcation task that determines the polarityof documents, sentences and/or tweets.
weuse the imdb reviews dataset (maas et al.,2011), stanford sentiment treebank (sst-2) (socher et al., 2013) dataset, along withtwitter sentiment140 (s140) (go et al., 2009)dataset..• news classiﬁcation - this is a task of topiccategorization for news articles.
we use theagnews dataset (zhang et al., 2015).
this isa four-way classiﬁcation task..• question classiﬁcation we use the trecﬁne-grained question classiﬁcation dataset (liand roth, 2002).
this task involves classi-fying questions into 46 ﬁne-grained questioncategories..• rq1: do convolutions beneﬁt from pre-.
training as much as transformers?.
• semantic parsing / compositional gener-alization compositional generalization is the.
4352ability of models to generalize composition-ally outside of the training distribution.
tobe speciﬁc, it needs be able to handle unseencombinations at test time.
for this task, we usethe cogs dataset (kim and linzen, 2020), atask of generating semantic representation ofa given english sentence.
for example, a catsmiled → cat(x1) and smile.agent(x2, x1)..all of the datasets, with the exception of the re-cent cogs dataset (kim and linzen, 2020), aretensorﬂow datasets1..for each dataset, we evaluate all models withand without pre-training (details in subsequent sec-tions).
table 1 reports the statistics of the datasetsused in this paper..dataset / taskcivil commentswiki toxicityimdbsst-2s140trecagnewscogs.
# train3,820,210561,80825,00067,0001,600,0004,500120,00024,000.
# test205,781234,56425,0001,8003595007,6003000.
# class22222464n/a.
table 1: statistics of datasets used in our experiments.
datasets are diverse in terms of domains, tasks andamount of labeled data..5.2 experimental setup.
this section describes our experimental setup..5.2.1 models.
our models are largely based on sequence to se-quence models, a paradigm that has demonstratedgreat success made evident by models such asbart (lewis et al., 2019) and t5(raffel et al.,2019).
we implement our models in mesh ten-sorﬂow (mtf) (shazeer et al., 2018), a libraryfor distributed and efﬁcient parallel model train-ing that has similar api to tensorﬂow.
we trainmodels that are of base size, which corresponds to12 layers each in the encoder and decoder, alongwith 3072 dimensions for the feed-forward layers,a model dimension of 768 and a total of 12 heads.
our transformer models are largely based on t5(raffel et al., 2019), which is considered the cur-rent state-of-the-art transformer model for nlptasks and hence serves as a strong baseline.
for theconvolution models, our lightweight convolution.
1https://www.tensorflow.org/datasets/.
catalog/overview..and dynamic convolution models have a windowsize2 of 7 across all layers, the number of uniquedepth ﬁlters is 2. for dilated models, we use a ﬁltersize of [4, 4, 7, 7, 15, 15, 15, 15, 31, 31, 31] for our12 layer convolution model..5.2.2 pre-training.
we pre-train both our convolutional and trans-former models for 524k steps with a batch sizeof 128. given the input sequence length of 512,this corresponds to 65536 tokens per batch.
forpre-training, we use the colossal cleaned com-moncrawl corpus (c4) (raffel et al., 2019) datasetwhich has demonstrated impressive results ondownstream tasks.
we use the span based seq2seqobjective as the pre-training objective as mentionedin earlier sections.
the span size is set to 3 anda corruption rate of 15% is adopted.
we use theadafactor optimizer (shazeer and stern, 2018) withan inverse square root learning rate scheduler.
eachpre-training run is performed using 16 tpu-v3chips and takes approximately 12 hours to com-plete for models of base size..5.2.3 downstream fine-tuning.
we ﬁne-tune the pre-trained models using thefollowing set of hyperparameters: we use aconstant learning rate which is tuned amongst{0.001, 0.0005, 0.0001}.
the batch size is gener-ally set to 64 but occasionally set to 32 for smallerdatasets.
intuitively, sequence length is task de-pendent but generally approximately the 90th per-centile for each task.
we ﬁne-tune for a maximumof 100k steps and report peak validation perfor-mance.
fine-tuning uses the same adafactor opti-mizer as during training.
we perform ﬁne-tuningon similar hardware, i.e., typically 16 tpuv3 chipsare used per ﬁne-tuning job..5.3 experimental results.
this section describes our experimental setup andresults..5.4 results on toxicity detection.
table 2 reports results on toxicity detection.
onboth toxicity detection datasets the pre-trained andno-pre-training (raw) setup, the best models are thedilated convolution models and the dynamic con-volution models.
in fact, all convolutional models.
2we believe that tuning the hyperparameters of the convo-lution models can result in even better performance.
however,we decided to keep these hyperparameters simple for the start..4353outperform transformers on both civilcommentsand wikitoxic.
before pre-training, convolutionsoutperform transformers by approximately 1.5 ab-solute percentage points.
the gap narrows after pre-training where transformers see a better gain (e.g.,+5.1% against +4.3%) from pre-training over con-volutions on the civilcomments dataset.
however,the converse is true on wikitoxic - the only case ofperformance degradation after pre-training.
over-all, on this task, convolutions are competitive totransformers and outperform them..5.5 results on sentiment classiﬁcation.
results on sentiment classiﬁcation (imdb, sst-2and s140) can be found in table 2. on the imdb re-views dataset, the best non-pre-trained model is thelightweight convolution model, outperforming thetransformer model.
the best pre-trained model isthe transformer model.
however, all convolutionalmodels come in close with less than a percentagepoint gap difference with pre-trained transformers.
on the sst-2 and s140 tasks, we observe that thebest models are convolution-based, regardless ofwhether the model is pre-trained or not..5.6 results on question classiﬁcation.
the best non-pre-trained model is the lightweightconvolution model.
for pre-trained models, con-volutional models also outperform the pre-trainedtransformer.
on this task, while most models ben-eﬁt signiﬁcantly from pre-training, transformersseem to beneﬁt slightly more from pre-training..5.7 results on news classiﬁcation.
results on news classiﬁcation seems to follow sim-ilar trends as other benchmarks.
convolutionalmodels outperform transformers both in non-pre-trained and pre-trained setups.
the highest gainfrom pre-training is obtained from the dilated con-volution model..5.8 results on compositional generalization.
challenge and semantic parsing.
we conduct additional experiments on semanticparsing and compositional generalization.
the taskis framed as a sequence generation task.
we use therecently proposed (kim and linzen, 2020) dataset.
on the in-distribution test set, transformers andconvolutions have identical performance (95%).
on the generalization or out of distribution set,transformers perform at 77.5% while convolutions.
come in at 76.9. while convolutions do not ex-actly outperform transformers, they come in closeenough to be considered competitive..5.9 summary of results.
on the seven tasks across a broad range of do-mains we ﬁnd that (1) non-pre-trained convolutionsare competitive and frequently outperform non-pre-trained transformers, (2) pre-trained convolutionsoutperform pre-trained transformers on six out ofseven tasks.
this answers rq2..we also ﬁnd that convolutions are able to ben-in a similar fashion toeﬁt from pre-training,self-attention-based models.
hence, the beneﬁtsachieved by pre-training are not exclusive to trans-former models.
this answers rq1..amongst the pre-trained convolutional models,we ﬁnd that dilated convolutions and dynamic con-volutions are generally better than lightweight con-volutions, thus answering rq5..finally, we observe that relative performance(i.e., rankings) do change with pre-training.
thisdeﬁnitely shows that there is some kind of effectfrom composing architectures with pre-training.
the direct implication of this effect is that a modelthat performs well (relatively) without pre-trainingwill not necessarily perform the best when pre-trained (and vice versa).
hence, aside from conﬂat-ing architectures with pre-training schemes, we doalso need to take note that different architecturesmay behave differently under pre-training..6 discussion and analysis.
this section expands on the results via a detailedanalysis and discussion.
we discuss the pros/consof pretrained convolutions,the impact of pre-training on performance and also recommendationsto the broader community..6.1 when do we expect pre-trained.
convolutions to fail?.
in our experimental section, we observed the po-tential upsides of convolutional models over well-established pre-trained transformers and observethat we are able to get quality improvements incertain cases.
however, it might be good to furtherunderstand the drawbacks of convolutions..one obvious weakness of pre-trained convolu-tions are their lack of cross-attention inductivebias that comes for free with self-attention in thetransformer encoder.
for this reason, it is not a.
4354civilcomment wikitoxicf1accno pre-training.
imdbacc.
acc.
f1.
sst-2acc.
77.2278.5879.9478.49.
81.1681.4781.6781.83.
85.0985.8286.5084.71.
86.5687.5887.7887.71.
91.9391.0592.2990.06.
91.4693.6193.8493.76.
84.8185.8885.8485.69.
95.4594.6594.9195.66with pre-training95.1296.4896.2196.53.
94.1693.6093.9293.35.
78.4481.6579.0182.80.
92.0992.2092.0991.59.gain from pre-training.
s140acc.
58.8460.6455.6260.84.
61.6561.6562.8562.45.trecacc.
78.0082.2079.6080.20.
93.6093.6094.2092.40.newsacc.
84.2587.2281.2485.13.
93.5493.6393.2693.93.
+5.1% +1.7% -0.6% -0.4% +11.0% +17.4% +4.7% +20.0% +11.0%+3.7% +2.1% +2.8% +1.9% +9.0% +13.0% +1.7% +14.0% +7.3%+2.1% +1.5% +1.7% +1.4% +9.4% +17.0% +13.0% +18.0% +14.8%+4.3% +3.5% +4.1% +1.0% +8.9% +10.6% +2.6% +15.2% +10.4%.
model.
trans.
lightdilat.
dyna..trans.
lightdilat.
dyna..trans.
lightdilat.
dyn..table 2: comparison of pre-trained convolutions and pre-trained transformers on toxicity detection, sentimentclassiﬁcation, question classiﬁcation and news classiﬁcation.
all models have approximately 230m parametersand are 12 layered seq2seq architectures.
our ﬁndings show that convolutions (1) also beneﬁt from pretraining and(2) are consistently competitive to transformer models with and without pretraining..good idea to use pre-trained convolutions for tasksthat requires modeling the relationship betweentwo or more sequences.
to verify this, we run ex-periments on squad and multinli and ﬁnd thatconvolutions do not come close to transformersjust because of this missing inductive bias.
thisshould be clearly distinguished when examiningand evaluating models, as how the early snlileaderboard3 distinguished between models thatused cross-attention and models that did not..our initial evaluations on benchmarks likesquad/mnli (rajpurkar et al., 2016; williamset al., 2017) showed that pre-trained convolutionsare indeed signiﬁcantly lackluster.
for exam-ple, convolutions only achieve ≈ 75% accuracyon multinli, while transformers easily achieve≈ 84% accuracy.
likewise, while transformersachieve about ≈ 90% f1 on squad, convolutionscome in around ≈ 70%.
this is entirely expectedbecause there is no way the premise/question caninteract with the hypothesis/context.
(rq4).
how-ever, our experiments show that this was onlybecause they lack this cross-attention property.
when we augment convolutions with a single layerof cross attention at the encoder, we ﬁnd thatpre-trained convolutions come close (a delta of.
3https://nlp.stanford.edu/projects/.
snli/.
(≈ 1%)) to pre-trained transformers on datasetssuch as multinli (williams et al., 2017), achievingabout ≈ 83% accuracy..that said, we leave it to the practitioner to decidewhether the cross-attention inductive bias is actu-ally important for the problem at hand.
we also liketo emphasize that the pattern of concatenating sen-tence pairs is not necessary practical when scalingup since this requires inference on every permuta-tion of sentence pairs.
for this reason, dual encodersetups that do fast embedding space look-ups aremore practical and feasible in practice (guo et al.,2020).
given the strong performance of convolu-tions in a series of encoding tasks, we can expectpre-trained convolutions to do well in a dual en-coder setup..6.2 what are the beneﬁts of pre-trainedconvolutions over transformers?.
we observed a reasonable quality improvementfrom using convolutions over transformers.
thissection discusses the additional beneﬁt..6.2.1 convolutions are faster and scale better.
to long sequences.
figure 1 reports training speed of convolution(lightconvs) versus transformers on a sequenceto sequence task.
the input lengths are variedfrom {64, 128, 256, 512, 1024, 2048, 4096}.
we.
4355figure 1: effect of sequence length on processingspeed (examples per second) on a seq2seq masked lan-guage modeling task.
results are benchmarked on 16tpuv3 chips on c4 pre-training.
results are in logscale..show that convolutions are not only consistentlyfaster (even at shorter sequences) but scale bet-ter than transformers.
convolution scales linearlywhile transformers are not able to scale to longersequences..6.2.2 convolutions are flops efﬁcientwe measure the number of flops of convolutionsversus transformers as we increase the sequencelength.
figure 2 shows the phenomenon whilevarying sequence length.
in general, across allsequence lengths, convolutions are more efﬁcientin the number of ﬂoating point operations..moreover, we ﬁnd that the flop efﬁciency of con-volutions scales better across sequence lengths..6.3 are we suggesting to completely replace.
transformers with convolution?.
while transformers have dominated the researchlandscape in nlp, this paper suggests that thereare commonly overlooked beneﬁts to convolutionssuch as model quality, speed, flops and scalabil-ity.
moreover, it is previously unknown to whetherconvolutions beneﬁt from pre-training.
in this pa-per, we showed that they are competitive on sometasks and also beneﬁt from pre-training in simi-lar fashion to transformer models.
however, onthe ﬂip side, we also highlighted that they are un-able to handle tasks that require cross-attention orwhen there is a need to model > 1 sentence ordocuments within the same sequence.
we believethat practitioners have good options and it mightbe worthwhile to explore architectures outside thewell-established transformer models..6.4 on not conﬂating pre-training with.
architectural advances.
three otherin this paper, we showed that(convolutional-based)(e.g.,lightweight, dymamic and dilated) also ben-eﬁt from pre-training to the same extent astransformer models..architectures.
in the current research landscape, pre-traininghas always be tightly coupled and associated withtransformers architectures.
as a result, the successof bert, transformers and large language modelsseem to be pretty conﬂated.
while it is true that,to this date, the only model that large-scale pre-training has been applied to are transformer mod-els, we believe there might be potential in otherarchitectures..based on our empirical ﬁndings, we believethere is still signiﬁcant room for the improvingthe understanding of the compositional effects ofarchitecture and pre-training.
hence, we believethat the impact of this work extends beyond show-ing the competitiveness of convolution models innlp.
more concretely, the take home message isthat there should be a healthy level of optimism inexploring architectural alternatives..figure 2: effect of sequence length on number offlops (einsum ops) on a seq2seq masked languagemodeling task.
results are benchmarked on 16 tpuv3chips on c4 pre-training.
results are in log scale..7 conclusion.
the overall ﬁndings that convolutions are fasterboth in wall clock time and in flops answers rq3..in this paper, we conducted an extensive study ofthe viability and feasibility of pre-trained convolu-.
4356tions.
our experimental results show that convo-lutions can outperform transformers in both pre-train and non-pre-trained setups.
our extensiveexperiments across 8 datasets spanning a diverserange of tasks, show that convolutions are ableto beneﬁt from pre-training to the same (or some-times greater) extent than transformers.
whilepre-trained transformers are the de-facto choice ofarchitecture, our results show that they might notbe the best in certain scenarios.
additionally, wediscussed the caveats, trade-offs pertaining withruntime, scalability, number of flops and modelquality.
finally, we discussed the situations or datatypes that convolutions are not well equipped tohandle and make an empirically informed recom-mendation for practitioners..references.
shaojie bai, j zico kolter, and vladlen koltun.
2018. an empirical evaluation of generic convolu-tional and recurrent networks for sequence modeling.
arxiv preprint arxiv:1803.01271..daniel borkan, lucas dixon, jeffrey sorensen, nithumthain, and lucy vasserman.
2019. nuanced metricsfor measuring unintended bias with real data for textclassiﬁcation.
corr, abs/1903.04561..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..muthuraman chidambaram, yinfei yang, daniel cer,steve yuan, yun-hsuan sung, brian strope, and raykurzweil.
2018. learning cross-lingual sentencerepresentations via a multi-task dual-encoder model.
arxiv preprint arxiv:1810.12836..franc¸ois chollet.
2017. xception: deep learning within proceedingsdepthwise separable convolutions.
of the ieee conference on computer vision and pat-tern recognition, pages 1251–1258..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2020. electra: pre-trainingtext encoders as discriminators rather than genera-tors.
arxiv preprint arxiv:2003.10555..jonas gehring, michael auli, david grangier, de-nis yarats, and yann n dauphin.
2017. convolu-tional sequence to sequence learning.
arxiv preprintarxiv:1705.03122..alec go, richa bhayani, and lei huang.
2009. twit-ter sentiment classiﬁcation using distant supervision.
cs224n project report, stanford, 1(12):2009..ruiqi guo, philip sun, erik lindgren, quan geng,david simcha, felix chern, and sanjiv kumar.
2020.accelerating large-scale inference with anisotropicvector quantization.
in international conference onmachine learning..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..lukasz kaiser, aidan n gomez, and francois chol-depthwise separable convolutionsarxiv preprint.
let.
2017.for neural machine translation.
arxiv:1706.03059..nal kalchbrenner, lasse espeholt, karen simonyan,aaron van den oord, alex graves, and koraykavukcuoglu.
2016. neural machine translation inlinear time.
arxiv preprint arxiv:1610.10099..najoung kim and tal linzen.
2020. cogs: a compo-sitional generalization challenge based on semanticinterpretation.
arxiv preprint arxiv:2010.05465..yoon kim.
2014..convolutional neural networksin proceedings of thefor sentence classiﬁcation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1746–1751,doha, qatar.
association for computational lin-guistics..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learn-arxiv preprinting of language representations.
arxiv:1909.11942..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..xin li and dan roth.
2002. learning question clas-in coling 2002: the 19th international.
siﬁers.
conference on computational linguistics..andrew m dai and quoc v le.
2015..supervised sequence learning.
arxiv:1511.01432..semi-arxiv preprint.
qi liu, matt j kusner, and phil blunsom.
2020. asurvey on contextual embeddings.
arxiv preprintarxiv:2003.07278..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..4357andrew l maas, raymond e daly, peter t pham, danhuang, andrew y ng, and christopher potts.
2011.learning word vectors for sentiment analysis.
inproceedings of the 49th annual meeting of the as-sociation for computational linguistics: human lan-guage technologies-volume 1, pages 142–150.
asso-ciation for computational linguistics..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimenttreebank.
in proceedings of the 2013 conference onempirical methods in natural language processing,pages 1631–1642..bryan mccann, james bradbury, caiming xiong, andrichard socher.
2017. learned in translation: con-textualized word vectors.
in advances in neural in-formation processing systems, pages 6294–6305..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to sequencepre-training for language generation.
arxiv preprintarxiv:1905.02450..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013. distributed represen-tations of words and phrases and their composition-ality.
arxiv preprint arxiv:1310.4546..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
arxiv preprint arxiv:1802.05365..xipeng qiu, tianxiang sun, yige xu, yunfan shao,ning dai, and xuanjing huang.
2020. pre-trainedmodels for natural language processing: a survey.
science china technological sciences, pages 1–26..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
language mod-els are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016.squad: 100,000+ questionsfor machine comprehension of text.
arxiv preprintarxiv:1606.05250..noam shazeer, youlong cheng, niki parmar, dustintran, ashish vaswani, penporn koanantakool, peterhawkins, hyoukjoong lee, mingsheng hong, cliffyoung, et al.
2018. mesh-tensorﬂow: deep learningfor supercomputers.
in advances in neural informa-tion processing systems, pages 10414–10423..noam shazeer and mitchell stern.
2018. adafactor:adaptive learning rates with sublinear memory cost.
arxiv preprint arxiv:1804.04235..laurent sifre and st´ephane mallat.
2014.motion scattering for image classiﬁcation..rigid-.
ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..yi tay, dara bahri, donald metzler, da-cheng juan,zhe zhao, and che zheng.
2020a.
synthesizer: re-thinking self-attention in transformer models.
arxivpreprint arxiv:2005.00743..yi tay, mostafa dehghani, samira abnar, yikangshen, dara bahri, philip pham, jinfeng rao, liuyang, sebastian ruder, and donald metzler.
2021.long range arena : a benchmark for efﬁcient trans-in international conference on learningformers.
representations..yi tay, mostafa dehghani, dara bahri, and donaldmetzler.
2020b.
efﬁcient transformers: a survey.
arxiv preprint arxiv:2009.06732..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..wei wang, bin bi, ming yan, chen wu, zuyi bao,jiangnan xia, liwei peng, and luo si.
2019. struct-incorporating language structures into pre-bert:arxivtraining for deep language understanding.
preprint arxiv:1908.04577..adina williams, nikita nangia, and samuel r bow-man.
2017. a broad-coverage challenge corpus forarxivsentence understanding through inference.
preprint arxiv:1704.05426..felix wu, angela fan, alexei baevski, yann ndauphin, and michael auli.
2019. pay less attentionwith lightweight and dynamic convolutions.
arxivpreprint arxiv:1901.10430..ellery wulczyn, nithum thain, and lucas dixon.
2017.ex machina: personal attacks seen at scale.
in pro-ceedings of the 26th international conference onworld wide web, www ’17, pages 1391–1399, re-public and canton of geneva, che.
internationalworld wide web conferences steering committee..zhilin yang, zihang dai, yiming yang, jaime car-bonell, ruslan salakhutdinov, and quoc v le.
2019. xlnet: generalized autoregressive pretrain-arxiv preprinting for language understanding.
arxiv:1906.08237..4358xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation..4359