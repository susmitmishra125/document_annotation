cori: collective relation integration with data augmentationfor open information extraction.
zhengbao jiang1∗,.
jialong han2, bunyamin sisman2, xin luna dong2.
language technologies institute, carnegie mellon university1amazon2zhengbaj@cs.cmu.edu{jialongh,bunyamis,lunadong}@amazon.com.
abstract.
integrating extracted knowledge from the webto knowledge graphs (kgs) can facilitate taskslike question answering.
we study relation in-tegration that aims to align free-text relationsin subject-relation-object extractions to rela-tions in a target kg.
to address the challengethat free-text relations are ambiguous, previ-ous methods exploit neighbor entities and rela-tions for additional context.
however, the pre-dictions are made independently, which can bemutually inconsistent.
we propose a two-stagecollective relation integration (cori) model,where the ﬁrst stage independently makes can-didate predictions, and the second stage em-ploys a collective model that accesses all can-didate predictions to make globally coherentpredictions.
we further improve the collec-tive model with augmented data from the por-tion of the target kg that is otherwise unused.
experiment results on two datasets show thatcori can signiﬁcantly outperform the base-lines, improving auc from .677 to .748 andfrom .716 to .780, respectively..1.introduction.
with its large volume, the web has been a majorresource for knowledge extraction.
open infor-mation extraction (open ie; sekine 2006; bankoet al.
2007) is a prominent approach that harvestssubject-relation-object extractions in free text with-out assuming a predeﬁned set of relations.
one wayto empower downstream applications like questionanswering is to integrate those free-text extractionsinto a knowledge graph (kg), e.g., freebase.
rela-tion integration is the ﬁrst step to integrate those ex-tractions, where their free-text relations (i.e., sourcerelations) are normalized to relations in the targetkg (i.e., target relations).
only after relation in-tegration can entity linking proceed to resolve the.
∗this work was performed while at amazon..figure 1: a motivating example.
trained on paralleldata, a local model may suffer from sparse context fora new entity pair nell-marie at inference, wrongly dis-ambiguating “parent” to father instead of mother..free-text subjects and objects to their canonical en-tities in the target kg.
local approaches.
relation integration has beenstudied by the natural language processing (nlp)community.
with exact matching in literal formbetween entity names in the source graph and tar-get kg, previous methods obtain parallel data, i.e.,common entity pairs, between the two graphs asin fig.
1. features of the entity pairs (e.g., malia-barack) in the source graph and their relations inthe target kg (e.g., father) are used to train mod-els to predict target relations for future extractions.
a common challenge is the ambiguity of source re-lations, e.g., “parent” may correspond to fatheror mother in different contexts.
previous methodsexploited contextual features including embeddingsof seen entities (e.g., “malia”; riedel et al.
2013),middle relations between (e.g., “parent”; riedelet al.
2013; toutanova et al.
2015; verga et al.
2017,2016; weston et al.
2013), and neighbor relationsaround the entity pair (e.g., “gender”; zhang et al.
2019)..assuming rich contexts to address the ambiguitychallenge, previous methods may fall short underthe evolving and incomplete nature of the source.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4706–4716august1–6,2021.©2021associationforcomputationallinguistics4706nellbillyfatherburtonmarieparentparentnellbillyfatherburtonmariefatherfathermothersource graphpredicted target graphinferencetrainunmatched kgparallel datasource relationtarget relationsource entitytarget entityparentmotherfathermotherfathergendergenderbarackmichellemalemaliamalemaliamichellebarackparent↓methods.
middle no entity neighbor collectiverelation inferencerelation param..(riedel et al., 2013) (cid:88)(cid:88)(verga et al., 2017)(zhang et al., 2019) (cid:88)(cid:88)cori (ours).
(cid:88)(cid:88)(cid:88).
(cid:88)(cid:88).
(cid:88).
table 1: comparisons between cori and baselines..graph.
for example, in the lower part of fig.
1,emerging entities may come from new extractionswith sparse contextual information.
for the pairnell-marie, a conventional model learned on theparallel data may have neither seen entities norneighborhood information (e.g., “gender”) to de-pend on, thus failing to disambiguate “parent” andwrongly predicting father.
due to the local na-ture of previous approaches, i.e., predictions fordifferent entity pairs are made independently ofeach other, the model is unaware that “nell” hastwo fathers in the ﬁnal predictions.
such predic-tions are incoherent in common sense that a personis more likely to have one father and one mother,which is indicated by the graph structure aroundmalia in the target kg part of the parallel data..1.1 our collective approach.
to alleviate the incoherent prediction issue of lo-cal approaches, we propose collective relationintegration (cori) that exploits the dependency ofpredictions between adjacent entity pairs to enforceglobal coherence..speciﬁcally, we follow two stages, i.e., candi-date generation and collective inference.
in can-didate generation, we simply use a local model tomake independent predictions as candidates, e.g.,father for all the three pairs in the lower partof fig.
1. in collective inference, we employ acollective model that is aware of the common sub-structures of the target graph, e.g., malia.
thecollective model makes predictions by not onlytaking as input all contextual features to the lo-cal model but also the candidate predictions of thecurrent and all neighbor pairs.
for the pair nell-marie, the collective model will have access tothe candidate prediction father of nell-burton,which helps ﬂip its ﬁnal prediction to the correctmother.
tab.
1 summarizes cori and representa-tive previous work from four aspects.
to the bestof our knowledge, cori is the ﬁrst to collectivelyperform relation integration rather than locally..being responsible to make globally consistent.
predictions, the collective model needs to be trainedto encode common structures of the target kg,e.g., malia having only one father/mother in theparallel data of fig.
1. to this end, we train thecollective model in a stacked manner (wolpert,1992).
we ﬁrst train the ﬁrst-stage local modelon the parallel data, then train the second-stagecollective model by conditioning on the candidatepredictions of neighbor entity pairs from the ﬁrststage (e.g., father for malia-barrack) to makeglobally consistent predictions (e.g., mother formalia-michelle).
parallel data augmentation.
the parallel datamay be bounded by the low recall of exact namematching or the limited extractions generated byopen ie systems.
we observe that, even withoutcounterpart extractions, the unmatched part of thetarget graph (as in fig.
1) may also have rich com-mon structures to guide the training of the collec-tive model.
to this end, we propose augmentingthe parallel data by sampling subgraphs from theunmatched kg and creating pseudo parallel databy synthesizing their extractions, so the collectivemodel can beneﬁt from additional training datacharacterizing the desired global coherence..to summarize, our contributions are three-fold:(1) we propose cori, a two-stage framework thatimproves state-of-the-art methods by making col-lective predictions with global coherence.
(2) wepropose using the unmatched target kg to aug-ment the training data.
(3) experimental resultson two datasets demonstrate the superiority of ourapproaches, improving auc from .677 to .748 andfrom .716 to .780, respectively..2 preliminaries.
in this section, we ﬁrst formulate the task of rela-tion integration, then describe local methods byexemplifying with the state-of-the-art approachopenki (zhang et al., 2019)..2.1 relation integration.
we treat subject-relation-object extractions fromopen ie systems as a source graph k(e, r) ={(s, r, o) | s, o ∈ e, r ∈ r}, where e denotes ex-tracted textual entities, e.g., “barack obama”, andr denotes extracted source relations, e.g., “parent”.
we denote by (s, o) a source entity pair.
for (s, o),ks,o = {r | (s, r, o) ∈ k} denotes all source re-lations between them.
similarly, kr = {s, o |(s, r, o) ∈ k} denotes all entity pairs with relation.
4707r in between.
we use the union kr = (cid:83)to refer to all extracted entity pairs..r∈r kr.
deﬁnition 1 (relation integration).
given asource graph k and a target kg k(cid:48)(e (cid:48), r(cid:48)) withtarget entities e (cid:48) and target relations r(cid:48), the taskof relation integration is to predict all applicabletarget relations for each extracted entity pair in kr:.
γ ⊆ kr × r(cid:48),.
where (s, r(cid:48), o) ∈ γ is an integrated extractionindicating that a target relation r(cid:48) holds for (s, o).
to train relation integration models, all methods.
employ parallel data formalized as follow:.
deﬁnition 2 (parallel data).
parallel data arecommon entity pairs shared between kr and k(cid:48)r(cid:48)and their ground truth target relations in k(cid:48): t ={(cid:104)(s, o), k(cid:48)r(cid:48)}.
for exam-ple, (cid:104)(malia, barack), {father}(cid:105) is an instanceof parallel data in fig.
1..s,o(cid:105) | (s, o) ∈ kr ∩ k(cid:48).
to obtain parallel data, a widely used approachis to ﬁnd entities shared by e and e (cid:48) by exact namematching, then generate common entity pairs andtheir ground truth..2.2 local approaches.
previous local methods score potential integratedextractions by assuming their independence:.
p (γ | k) =.
(cid:89).
pθ(s, r(cid:48), o | k),.
(1).
(s,r(cid:48),o)∈kr×r(cid:48).
where θ is the parameters of the local model.
onerepresentative local model achieving state-of-the-art performance is openki (zhang et al., 2019).
itencodes the neighborhood of (s, o) in k by group-ing and averaging embeddings of source relationsin three parts.
let ks,· be the set of source rela-tions between s and neighbor entities other than o,and similarly for k·,o.
openki represents (s, o) byconcatenating the three averaged embeddings intoa local representation tl:.
tl = [a(ks,o); a(ks,·); a(k·,o)],.
(2).
where l stands for local, and a(.)
takes a set of rela-tions and outputs the average of their embeddings.
then each integrated extraction is scored by:.
pθ(s, r(cid:48), o | k) = σ(mlpl(tl))r(cid:48),.
(3).
figure 2: input of both stages on the nell-marie case.
solid edges are features for nell-marie.
additionaledges in the lower part are predicted candidate targetrelations γl..{(cid:104)(s, o), k(cid:48)s,o(cid:105)}, the loss function per training ex-ample trades between maximizing the probabilitiesof positive target relations and minimizing those ofnegative target relations:.
(cid:80).
r(cid:48)∈k(cid:48).
s,o.
log pθ(s, r(cid:48), o | k).
l(cid:0)(s, o), k(cid:48).
(cid:1) = −.
s,o.
γ (cid:80).
r(cid:48)∈r(cid:48)\k(cid:48).
+.
s,o|r(cid:48) \ k(cid:48).
s,o|.
s,o|log pθ(s, r(cid:48), o | k).
|k(cid:48).
,.
(4).
where γ is a hyperparameter to account for theimbanlance between positive and negative relations,because the latter often outnumber the former.
theﬁnal loss is the sum of all examples..3 collective relation integration.
as discussed in § 1, the drawback of local methodsis that predictions of different entity pairs are inde-pendently made.
neglecting their dependency maylead to predictions inconsistent with each other..to address the issue, we propose a collectiveapproach cori, which achieves collective relationintegration via two stages: candidate generationand collective inference.
in this section, we demon-strate the input and output of the two stages, as wellas our current implementations..3.1 candidate generation.
as mentioned in § 1.1, candidate generation’s re-sponsibility is to provide candidate predictions tothe collective inference stage.
formally, candidatepredictions γl (l means local) are generated by ex-ecuting a local model on the source graph k:.
γl = argmax.
pθ(γ | k)..(5).
γ.where mlpl is a multi-layer perceptron and σthe sigmoid function.
given a parallel data t =.
the candidate predictions in γl may be partiallywrong, but the other correct ones can help adjust.
4708nellbillyfatherparentfatherfathermarienellparentbillyburtonfathermarieinput to the first stageinput to the second stagesource relationtarget relationentityparentparentfatherburtonwrong predictions of their adjacent entity pairs inthe collective inference stage, under the guidanceof the collective model..for example, in the upper part of fig.
2, we havea source graph k with three entity pairs.
the inputto candidate generation is the entire k. after ap-plying the local model (openki in our case), wehave three additional edges as the output γl in thelower part of fig.
2. note that the candidate pre-diction father for nell-marie (denoted by blackoutline) is incorrect due to insufﬁcient informationin its neighborhood in k, i.e., both the relations inbetween of and around the entity pair (denoted bysolid edges) are ambiguous “parent”s..fortunately, the entity pair nell-burton is rel-atively easy for the local model to predict asfather because it can leverage the neighbor rela-tion “father” between billy-burton.
such correctcandidate predictions are included in γl, providedto the collective inference stage as additional sig-nals for later correction of the wrong predictionssuch as father for nell-marie..3.2 collective inference.
collective inference’s responsibility is to encodethe structures of the target graph and use such in-formation to reﬁne the candidate predictions γl byenforcing coherence among them.
to this end, acollective model pβ (with parameters β) takes boththe source graph k and the candidate predictionsγl as input, and outputs the ﬁnal predictions γ:.
p (γ | k) = pβ(γ | k, γl)..(6).
in the nell-marie case of fig.
2, when makingthe ﬁnal prediction, its own candidate predictionsand those of the neighbor entity pairs (solid edgesin γl of the lower part in fig.
2) are used to leveragethe dependency among them.
we concatenate theembeddings of candidate predictions to the localrepresentation tl obtained in the ﬁrst stage, andrepresent each entity pair as follow:.
tc = [tl; a(γl.
s,o); a(γl.
s,·); a(γl.
·,o)],.
(7).
where c means collective.
γls,o includes candidatetarget relations between s and o, and similarly forγls,· and γl·,o.
then we use another multi-layerperceptron mlpc to convert tc to probabilities.
pβ(s, r(cid:48), o | k, γl) = σ(mlpc(tc))r(cid:48),.
(8).
and minimize the loss function for pβ similar tothat of the local model pθ in eq.
4..algorithm 1: training collective model..result: collective model β.t (1), .
.
.
, t (t ) ← split training data t into t folds;for fold i = 1, .
.
.
, t do.
θ(i) ← train local model on data folds1, .
.
.
, i − 1, i + 1, .
.
.
, t ;i ← local predictions on t (i) using θ(i);γl.
endγl ← ∪iγli;β ← train collective model on t with input k and γl;.
3.3 training collective modelaccording to eq.
6, we need γl as features to trainthe collective model pβ.
this is to ensure that pβcaptures the dependencies among target relations.
one may ask why we do not directly use groundtruth k(cid:48) instead of predictions γl.
at test time, wecan only use target relations predicted by pθ as in-put to pβ because the ground truth target relationsof neighbor entity pairs might not be available.
ifwe train pβ using the ground truth, there will bea discrepancy between training and testing, poten-tially hurting the performance..speciﬁcally, we split the training set t into tfolds.
we generate γl by rotating and unioninga temporary local model’s predictions on a held-out fold, where the temporary model is trained onthe other folds.
then we train pβ on the paralleldata t with γl.
in this manner, we can use thefull dataset to optimize the collective model whileavoiding generating candidates on the training dataof the local model, which leads to overﬁtting.
thedetailed training procedure is given in alg.
1..4 data augmentation w/ unmatched kg.
as in def.
2, the volume of parallel data is limitedby the number of shared entity pairs kr ∩ k(cid:48)r(cid:48) ofthe two graphs.
in fig.
1, the unmatched part of thetarget kg, containing entity pairs without extrac-tion counterparts (i.e., k(cid:48)r(cid:48) \ kr) and their targetrelations, can also indicate common substructuresof the target kg, and guide the training of the col-lective model.
to this end, we propose leveragingunmatched kg to generate pseudo parallel data toaugment the limited training data.
synthesizing pseudo extractions.
to leveragethe unmatched kg, we need to synthesize pseudoextractions for the target entities and relations toadd to k as features.
since we do not use entity-speciﬁc parameters, we only synthesize source re-lations like “parent”, and keep the target entities.
4709algorithm 2: our augmentation approach..result: collective model β with data augmentation.
(1) synthesizing pseudo extractions kpkp ← ∅; t p ← ∅;for (s(cid:48), r(cid:48), o(cid:48)) ∈ k(cid:48), where (s(cid:48), o(cid:48)) ∈ k(cid:48).
r(cid:48) \ kr do.
s ← s(cid:48) and o ← o(cid:48);sample r ∼ p (r|r(cid:48));kp ← kp ∪ {(s, r, o)};.
end.
(2) pseudo data selectionfor entity pair ∈ kr ∩ k(cid:48).
r(cid:48) do.
s ← its top k similar entity pairs in kpt p ← t p ∪ {(cid:104)(s, o), k(cid:48).
s,o(cid:105) | (s, o) ∈ s};.
r ∩ k(cid:48).
r(cid:48) ;.
endβ ← train on t ∪ t p with alg.
1;.
as “tokens”.
for each entity pair from the paralleldata t , we use bm25 (robertson and zaragoza,2009) to retrieve its top k most similar entity pairsfrom t p, and add them to the selected pseudo par-allel data t p for training, as detailed in alg.
2..figure 3: illustration of parallel data augmentation.
weﬁrst generate pseudo extractions for the unmatched kg,then select a subset of entity pairs that are similar to theparallel data (with black outline) to augment training..unchanged, as illustrated in fig.
3. speciﬁcally, foreach subject-relation-object tuple (s(cid:48), r(cid:48), o(cid:48)) in theunmatched kg, we keep s(cid:48) and o(cid:48) unchanged, andsynthesize source relations r by sampling from:.
p (r | r(cid:48)) =.
|kr ∩ k(cid:48)|k(cid:48)r(cid:48)|.
r(cid:48)|.
,.
5 experimental settings.
(9).
5.1 datasets and evaluation.
i.e., the conditional probability of observing r givenr(cid:48) based on co-occurrences in the parallel data.
|kr ∩ k(cid:48)r(cid:48)| is the number of entity pairs with both rand r(cid:48) in between, and |k(cid:48)r(cid:48)| is the number of entitypairs with r(cid:48) in between.
in this way, we obtain apseudo extraction (s, r, o), as detailed in alg.
2pseudo data selection.
we regard all pseudo ex-tractions as a graph kp.
similar to def.
2, we deﬁnepseudo parallel data as below..r(cid:48)}..r ∩ k(cid:48).
deﬁnition 3 (pseudo parallel data).
pseudo par-allel data t p includes common entity pairs betweenpseudo extractions kp and the target kg k(cid:48), asso-ciated with their ground truth target relations, i.e.,s,o(cid:105)} | (s, o) ∈ kpt p = {(cid:104)(s, o), k(cid:48)to make use of pseudo parallel data t p, themost straightforward way is to use them togetherwith parallel data t to train the collective modelpβ.
however, not all substructures in the targetgraph k(cid:48) are useful for pβ.
for example, whenk(cid:48) has other domains irrelevant to the source ex-traction graph, substructures in those domains maydistract pβ from concentrating on the domains ofthe source graph.
to mitigate this issue, we onlyuse a subset of t p similar to t , as shown by theblack-outlined parts in fig.
3. speciﬁcally, we rep-resent each entity pair (s, o) as a virtual documents,· ∪ k(cid:48)with surrounding target relations k(cid:48)·,o.
s,o ∪ k(cid:48).
we use the reverb dataset (fader et al., 2011) asthe source graph, and freebase1 and wikidata2 asthe target kgs, respectively.
we follow the samename matching approach in zhang et al.
(2019)to obtain parallel data.
to simulate real scenarioswhere models are trained on limited labeled databut applied to a large testing set, we use 20% ofentity pairs in the parallel data for training and theother 80% for testing, and there is no overlap.
wealso compare the performance under other ratios in§ 6.3. dataset statistics are listed in tab.
2..datasets.
#train.
#test.
|r|.
reverb + freebasereverb + wikidata.
12,3448,447.
49,62933,849.
97,196182,407.table 2: dataset statistics.
we follow zhang et al.
(2019) to use the most frequent 250 target relations..we evaluate by ranking all integrated extractionsbased on their probabilities, and report area underthe curve (auc).
considering real scenarios wherewe want to integrate as many extractions as possi-ble while keeping a high precision, we also reportrecall and f1 when precision is 0.8, 0.9, or 0.95..1https://developers.google.com/.
freebase.
2https://www.wikidata.org.
4710parallel dataparentmotherfathermotherfathergendergenderbarackmichellemalemaliamalemaliamichellebarackparentalignedaugmentationparentparentextractionskgunmatched kgpseudo extractionspseudo parallel data5.2 compared methods.
we compare the following methods in experiments.
relation translation is a simple method thatmaps source relations to target relations with con-ditional probability p (r(cid:48) | r) similar to eq.
9. foran entity pair (s, o), the predicted target relationsare {arg maxr(cid:48) p (r(cid:48)|r) | r ∈ ks,o}.
universal schema (e-model) (riedel et al., 2013)learns entity and relation embeddings through ma-trix factorization, which cannot generalize to un-seen entities.
it is a local model that scores eachintegrated extraction independently.
rowless universal schema (verga et al., 2017) isa local model which improves over the e-model byeliminating entity-speciﬁc parameters, thus gener-alizing to unseen entities.
openki (zhang et al., 2019) is a local model thataddresses the ambiguity of source relations by us-ing neighbor relations for more context.
cori is our collective two-stage relation integra-tion model trained with alg.
1.cori + da is our model where the training datais augmented by pseudo parallel data with alg.
2.to verify the necessity of retrieval-based pseudodata selection, we also compare with a random dabaseline where we select k random entity pairs.
cori + kge is another approach to exploit the un-matched kg with kg embeddings (kge) trainedon the entire target kg in an unsupervised manner.
we initialize the embeddings of target relations av-eraged by a(.)
in eq.
7 with transe (bordes et al.,2013) embeddings trained on the target graph..5.3.implementation details.
we uniformly use 32-dimension embeddings forall relations, and adamw (loshchilov and hutter,2019) optimizer with learning rate 0.01 and epsilon10-8. the ratio γ in eq.
4 is set to 10. we sample atmost 30 neighbor source relations to handle entitypairs with too many neighbor relations.
we uset = 5 folds in alg.
1 to train our collective model.
we retrieve top k = 5 entity pairs in pseudo dataselection, adding about 20k and 12k entity pairsto the two datasets in tab.
2, respectively.
we usebm25 (robertson and zaragoza, 2009) implemen-tation in elasticsearch3 in pseudo data selection.
we use the kge released by openke.4 our modelis trained with 32 cpu cores and a single 2080tigpu, and it takes 1-2 hours to converge..3https://www.elastic.co/4https://github.com/thunlp/openke.
(a) reverb + freebase.
(b) reverb + wikidata.
figure 4: precision-recall curves of best three methods..6 experimental results.
we aim to answer the following questions: (1) iscori superior to local models?
(2) is cori robustw.r.t.
varying size of training and testing data?
(3)is unmatched kg useful for cori?
is our paralleldata augmentation approach the best choice?.
6.1 main results.
in tab.
3, we show results comparing all methodson both datasets.
our observations are as follows.
collective inference is beneﬁcial.
among thebaselines, openki generally performs best becauseit leverages neighbor relations besides middle rela-tions between entity pairs, without relying on entityparameters.
even without data augmentation, corioutperforms openki by a large margin, improvingauc from .677 to .708 and from .716 to .746 onthe two datasets, respectively, which demonstratesthe effectiveness of collective inference.
data augmentation further improves the per-formance.
by comparing cori with cori + da(retrieval), we observe that data augmentation fur-ther improves auc from .708 to .748 and from .746to .780, respectively, which indicates that using un-matched kg can effectively augment the training ofthe collective model.
we plot the precision-recallcurves of the best three approaches in fig.
4. itdemonstrates the superiority of our methods acrossthe whole spectrum.
generalization on unseen entities is necessary.
among the baselines, the e-model uses entity-speciﬁc parameters, hindering it from generalizingto unseen entities and making it less competitive..6.2 effectiveness of pseudo data selection.
as shown in tab.
3, both kge, random, andretrieval-based data augmentation approaches per-form better than cori (without da), indicatingthe effectiveness of using the unmatched kg.
ourretrieval-based da outperforms the random coun-.
4711coricori + dacoricori + dadatasets.
metrics.
translatione-modelrowlessopenki.
cori.
+ kge+ da (random)+ da (retrieval).
reverb + freebase.
reverb + wikidata.
auc.
.571.205.593.677.
.708.711.734.748.prec = 0.8rec f1.
.590 .679.014 .027.473 .594.553 .654.
.590 .679.597 .684.616 .696.636 .708.prec = 0.9rec f1.
.100 .180.010 .020.372 .526.449 .599.
.494 .638.514 .654.518 .658.539 .674.prec = 0.95rec.
f1.
auc.
prec = 0.8rec f1.
prec = 0.9rec f1.
prec = 0.95rec.
f1.
.067 .125.005 .010.186 .310.314 .472.
.381 .544.418 .581.395 .558.421 .583.
.604.214.647.716.
.746.763.774.780.
.595 .683.
.088 .160.
.042 .080.
-.
-.
.511 .624.605 .689.
.641 .712.662 .725.678 .734.685 .738.
-.
-.
.381 .536.511 .652.
.558 .689.596 .717.606 .724.613 .729.
-.
-.
.266 .416.407 .570.
.461 .621.520 .672.521 .673.529 .680.table 3: main experimental results.
the best results are in bold, and the best external baselines are underlined.
cori outperforms the best baseline openki by a large margin, and parallel data augmentation (da) further im-proves its performance.
“-” indicates that the precision was not achieved..terpart, which conﬁrms the superiority of similarity-based data augmentation in choosing substructuresthat cover domains relevant to the original paralleldata.
our da approach outperforms kge, demon-strating the necessity of selectively using the un-used kg to avoid discrepancies with the paralleldata.
different numbers of pseudo data entity pairs.
in fig.
5, we compare the performance of da w.r.t.
different numbers of retrieved entity pairs k. weobserve that k=5 yields better performance thank=1.
however, further increasing k hurts the per-formance, which is probably due to pseudo entitypairs with lower similarity to the parallel data caus-ing a domain shift.
this validates the necessity ofselectively using pseudo parallel data..6.3.impacts of data size on cori.
due to its collective nature, one may wonder aboutcori’s performance w.r.t.
other training and testingdata sizes.
we analyze these factors in this section.
our observations are similar on both datasets, sowe only report the results on reverb + freebase.
varying size of training data.
in fig.
6a, wecompare cori (without da) with openki by vary-ing the portion of the parallel data for training from20% (used in our main results in tab.
3) to 80%.
we observe that using more training data improvesthe performance, as shown by the increasing trendsw.r.t.
all metrics.
our method outperforms openkiin all settings, demonstrating that our method iseffective in both high- and low-resource settings.
varying % of accessible neighbor entity pairs.
our collective framework is special in its collectiveinference stage, where the collective model reﬁnesthe candidate prediction of an entity pair by con-sidering its neighbor entity pairs’ candidates.
we.
hypothesize that the more neighbor entity pairs thecollective model has access to, the better perfor-mance it should achieve.
for example, if we usea portion of 50%, candidate predictions for onlyhalf of the neighbor entity pairs rather than the en-tire γl will be used in eq.
7. we vary the portionfrom 25% to 100% (used in our main experimentsin tab.
3).
as shown in fig.
6b, even accessing25% can make cori outperform openki.
as thepercentage increases, cori continues to improve,while openki remains the same because it is local,i.e., not using candidate predictions..6.4 case study.
in fig.
7, we show two cases from reverb + free-base where cori corrects the mistakes of openkiin the collective inference stage.
in the ﬁrst case,the source relation “is in” between “iowa” and“mahaska county” is extracted but in the wrongdirection.
openki just straightforwardly predictscontainedby based on the surface form, butfails to leverage the neighbor relations to infer thatiowa is a larger geographical area.
with the col-lective model, cori is able to use the other twocandidate predictions of containedby to ﬂip thewrong prediction to contains..in the second case, a prediction is needed be-tween “bily joel” and “columbia”.
here the sourcerelation “was in” and the object entity “columbia”are both ambiguous, which can refer to geograph-ical containment with a place or membership toa company.
openki makes no prediction due tothe ambiguity, while cori makes the right predic-tion music label by collectively working on theother entity pairs, where all predictions coherentlyindicate that “columbia” is a music company..4712(a) varying size of training data..(b) varying % of accessible neighbor pairs.
figure 5: performance of dataaugmentation with different num-bers of retrieved pairs k..figure 6: cori (bars without ﬁlling) vs. openki (solid bars) on reverb + free-base.
cori consistently outperforms openki by a large margin.
larger improve-ments are achieved when candidates of more neighbor entity pairs are accessed..middle relations, but eliminate entity parameters.
zhang et al.
(2019) moves one step further by ex-plicitly considering neighbor relations, leveragingmore context from the local link structure.
someworks (weston et al., 2013; angeli et al., 2015) di-rectly minimize the distance between embeddingsof relations sharing the same entity pairs.
yu et al.
(2017) further leverage compositional representa-tions of entity names instead of using free parame-ters to deal with unseen entities at test time..there are also works on open ie canonicaliza-tion that cluster source relations.
some use entitypairs as clustering signals (yates and etzioni, 2009;nakashole et al., 2012; gal´arraga et al., 2014),while others use lexical features or side informa-tion (min et al., 2012; vashishth et al., 2018).
how-ever, the clusters are not ﬁnally aligned to relationsin target kgs, different from our problem..the two-stage collective inference frameworkhas been explored in other problems like entitylinking (cucerzan, 2007; guo et al., 2013; shenet al., 2012), where candidate entities are gener-ated for each mention independently, and collec-tively ranked based on their compatibility in thesecond stage.
in machine translation, an effectiveapproach to leverage monolingual corpus in the tar-get language is to back-translate it to the source lan-guage to augment the limited parallel corpus (sen-nrich et al., 2016).
the above works inspired usto use collective inference for relation integrationand leverage the unmatched kg for data augmen-tation.
another approach to perform collectiveinference is to solve learning problem with con-straints, such as integer linear programming (rothand yih, 2004), posterior regularization (ganchevet al., 2010), and conditional random ﬁelds (laf-ferty et al., 2001).
comparing to our approach,these methods usually involve heavy computation,or are hard to optimize.
examining the perfor-.
figure 7: two cases from reverb + freebase withpredictions in this font.
the wrong predictions ofopenki is corrected by our collective model..7 related work.
relation integration has been studied by both thedatabase (db) and the nlp communities.
thedb community formulates it as schema matchingthat aligns the schemas of two tables, e.g., match-ing columns of an is in table to those of anothersubarea of table (rahm and bernstein, 2001;cafarella et al., 2008; kimmig et al., 2017).
suchtable-level alignment is valid since all rows in anis in table should have the same semantics, i.e.,being geographical containment or not.
however,in open ie, predictions should be made at the en-tity pair level because of the ambiguous nature ofsource relations.
putting all extracted “is in” entitypairs into one table to conduct schema matchingis problematic from the ﬁrst step since the entitypairs may have different ground truths..the nlp community, on the other hand, investi-gates the problem at the entity pair level.
besidesmanually designed rules (soderland et al., 2013),most works leverage the link structure between en-tities and relations.
universal schema (riedel et al.,2013) learns embeddings of entities and middle re-lations between entity pairs through decomposingtheir co-occurrence matrix.
however, the entityembeddings make it not generalize to unseen enti-ties.
other methods (toutanova et al., 2015; vergaet al., 2016, 2017; gupta et al., 2019) also exploit.
47130.30.40.50.60.70.815102050aucr@0.8f@0.8r@0.9f@0.9r@0.95f@0.95k=0.30.40.50.60.70.8aucr@0.8f@0.8r@0.9f@0.9r@0.95f@0.9520%40%60%80%portion of training split:0.30.40.50.60.70.8aucr@0.8f@0.8r@0.9f@0.9r@0.95f@0.9525%50%75%100%accessible neighbor pairs:iowamahaska countycouncil bluffscedar rapidsis a city ofis a town inis incontainedbycontainedbycontainedbyàcontainscolumbiabilyjoelcouncil bluffscedar rapidssigned upsigned tois inartistmusic_label∅àmusic_labelusawas in∅mance of these methods is an interesting futuredirection.
besides, we also adopted ideas of se-lecting samples from out-domain data similar toin-domain samples (xu et al., 2020; du et al., 2020)to select our pseudo parallel data..8 conclusion.
in this paper, we proposed cori, a collective infer-ence approach to relation integration.
to the best ofour knowledge, this is the ﬁrst work exploring thisidea.
we devised a two-stage framework, wherethe candidate generation stage employs existing lo-cal models to make candidate predictions, and thecollective inference stage reﬁnes the candidate pre-dictions by enforcing global coherence.
observingthat the target kg is rich in substructures indicatingthe desired global coherence, we further proposedexploiting the unmatched kg by selectively synthe-sizing pseudo parallel data to augment the trainingof our collective model.
our solution signiﬁcantlyoutperforms all baselines on two datasets, indicat-ing the effectiveness of our approaches..acknowledgments.
we would like to thank prashant shiralkar, haowei, colin lockard, binxuan huang, and all thereviewers for their insightful comments and sug-gestions..references.
gabor angeli, melvin jose johnson premkumar, andchristopher d. manning.
2015. leveraging linguis-tic structure for open domain information extraction.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing of the asian federation of naturallanguage processing, acl 2015, july 26-31, 2015,beijing, china, volume 1: long papers, pages 344–354. the association for computer linguistics..michele banko, michael j. cafarella, stephen soder-land, matthew broadhead, and oren etzioni.
2007.open information extraction from the web.
in ijcai2007, proceedings of the 20th international jointconference on artiﬁcial intelligence, hyderabad, in-dia, january 6-12, 2007, pages 2670–2676..antoine bordes, nicolas usunier, alberto garc´ıa-dur´an,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems 26: 27th annual conference onneural information processing systems 2013. pro-ceedings of a meeting held december 5-8, 2013,.lake tahoe, nevada, united states, pages 2787–2795..michael j. cafarella, alon y. halevy, daisy zhe wang,eugene wu, and yang zhang.
2008. webtables: ex-ploring the power of tables on the web.
proc.
vldbendow., 1(1):538–549..silviu cucerzan.
2007. large-scale named entity dis-ambiguation based on wikipedia data.
in proceed-ings of the 2007 joint conference on empirical meth-ods in natural language processing and computa-tional natural language learning (emnlp-conll),pages 708–716..jingfei du, edouard grave, beliz gunel, vishravchaudhary, onur celebi, michael auli, ves stoy-anov, and alexis conneau.
2020. self-training im-proves pre-training for natural language understand-ing.
corr, abs/2010.02194..anthony fader, stephen soderland, and oren etzioni.
2011. identifying relations for open information ex-traction.
in proceedings of the 2011 conference onempirical methods in natural language processing,emnlp 2011, 27-31 july 2011, john mcintyre con-ference centre, edinburgh, uk, a meeting of sig-dat, a special interest group of the acl, pages1535–1545.
acl..luis gal´arraga, geremy heitz, kevin murphy, andfabian m. suchanek.
2014. canonicalizing openknowledge bases.
in proceedings of the 23rd acminternational conference on conference on infor-mation and knowledge management, cikm 2014,shanghai, china, november 3-7, 2014, pages 1679–1688. acm..kuzman ganchev, jo˜ao grac¸a, jennifer gillenwater,and ben taskar.
2010. posterior regularization forstructured latent variable models.
j. mach.
learn.
res., 11:2001–2049..stephen guo, ming-wei chang, and emre kiciman.
2013. to link or not to link?
a study on end-to-end tweet entity linking.
in proceedings of the 2013conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 1020–1030..swapnil gupta, sreyash kenkre, and partha talukdar.
2019. care: open knowledge graph embeddings.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 378–388.
association for computational linguistics..angelika kimmig, alex memory, ren´ee j. miller, andlise getoor.
2017. a collective, probabilistic ap-proach to schema mapping.
in 33rd ieee interna-tional conference on data engineering, icde 2017,san diego, ca, usa, april 19-22, 2017, pages 921–932. ieee computer society..4714john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning (icml2001), williams college, williamstown, ma, usa,june 28 - july 1, 2001, pages 282–289.
morgankaufmann..ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..bonan min, shuming shi, ralph grishman, and chin-yew lin.
2012. ensemble semantics for large-scaleunsupervised relation extraction.
in proceedings ofthe 2012 joint conference on empirical methodsin natural language processing and computationalnatural language learning, emnlp-conll 2012,july 12-14, 2012, jeju island, korea, pages 1027–1037. acl..ndapandula nakashole, gerhard weikum,.
andfabian m. suchanek.
2012. patty: a taxonomyof relational patterns with semantic types.
inproceedings ofthe 2012 joint conference onempirical methods in natural language processingand computational natural language learning,emnlp-conll 2012, july 12-14, 2012, jeju island,korea, pages 1135–1145.
acl..erhard rahm and philip a. bernstein.
2001. a sur-vey of approaches to automatic schema matching.
vldb j., 10(4):334–350..sebastian riedel, limin yao, andrew mccallum, andbenjamin m. marlin.
2013. relation extractionwith matrix factorization and universal schemas.
in human language technologies: conference ofthe north american chapter of the association ofcomputational linguistics, proceedings, june 9-14,2013, westin peachtree plaza hotel, atlanta, geor-gia, usa, pages 74–84.
the association for compu-tational linguistics..stephen e. robertson and hugo zaragoza.
2009. theprobabilistic relevance framework: bm25 and be-yond.
found.
trends inf.
retr., 3(4):333–389..dan roth and wen-tau yih.
2004. a linear program-ming formulation for global inference in natural lan-in proceedings of the eighth confer-guage tasks.
ence on computational natural language learning,conll 2004, held in cooperation with hlt-naacl2004, boston, massachusetts, usa, may 6-7, 2004,pages 1–8.
acl..satoshi sekine.
2006. on-demand information extrac-tion.
in acl 2006, 21st international conference oncomputational linguistics and 44th annual meet-ing of the association for computational linguistics,proceedings of the conference, sydney, australia,17-21 july 2006. the association for computer lin-guistics..rico sennrich, barry haddow, and alexandra birch.
improving neural machine translation mod-2016.in proceedings of theels with monolingual data.
54th annual meeting of the association for compu-tational linguistics, acl 2016, august 7-12, 2016,berlin, germany, volume 1: long papers.
the asso-ciation for computer linguistics..wei shen, jianyong wang, ping luo, and min wang.
2012. linden: linking named entities with knowl-edge base via semantic knowledge.
in proceedingsof the 21st international conference on world wideweb, pages 449–458..stephen soderland, john gilmer, robert bart, orenetzioni, and daniel s. weld.
2013. open informa-tion extraction to kbp relations in 3 hours.
in pro-ceedings of the sixth text analysis conference, tac2013, gaithersburg, maryland, usa, november 18-19, 2013. nist..kristina toutanova, danqi chen, patrick pantel, hoi-fung poon, pallavi choudhury, and michael gamon.
2015. representing text for joint embedding of textin proceedings of the 2015and knowledge bases.
conference on empirical methods in natural lan-guage processing, emnlp 2015, lisbon, portugal,september 17-21, 2015, pages 1499–1509.
the as-sociation for computational linguistics..shikhar vashishth, prince jain, and partha p. talukdar.
2018. cesi: canonicalizing open knowledge basesusing embeddings and side information.
in proceed-ings of the 2018 world wide web conference onworld wide web, www 2018, lyon, france, april23-27, 2018, pages 1317–1327.
acm..patrick verga, david belanger, emma strubell, ben-jamin roth, and andrew mccallum.
2016. multi-lingual relation extraction using compositional uni-in naacl hlt 2016, the 2016versal schema.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, san diego california, usa,june 12-17, 2016, pages 886–896.
the associationfor computational linguistics..patrick verga, arvind neelakantan, and andrew mc-callum.
2017. generalizing to unseen entities andentity pairs with row-less universal schema.
in pro-ceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics, eacl 2017, valencia, spain, april 3-7,2017, volume 1: long papers, pages 613–622.
as-sociation for computational linguistics..jason weston, antoine bordes, oksana yakhnenko,and nicolas usunier.
2013. connecting languageand knowledge bases with embedding models for re-lation extraction.
in proceedings of the 2013 con-ference on empirical methods in natural languageprocessing, emnlp 2013, 18-21 october 2013,grand hyatt seattle, seattle, washington, usa, ameeting of sigdat, a special interest group of theacl, pages 1366–1371.
acl..4715david h. wolpert.
1992. stacked generalization.
neu-.
ral networks, 5(2):241–259..frank f. xu, zhengbao jiang, pengcheng yin, bogdanvasilescu, and graham neubig.
2020. incorporatingexternal knowledge through pre-training for naturallanguage to code generation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, acl 2020, online, july 5-10,2020, pages 6045–6052.
association for computa-tional linguistics..alexander yates and oren etzioni.
2009. unsuper-vised methods for determining object and relationsynonyms on the web.
j. artif.
intell.
res., 34:255–296..dian yu, lifu huang, and heng ji.
2017. open re-lation extraction and grounding.
in proceedings ofthe eighth international joint conference on natu-ral language processing, ijcnlp 2017, taipei, tai-wan, november 27 - december 1, 2017 - volume 1:long papers, pages 854–864.
asian federation ofnatural language processing..dongxu zhang, subhabrata mukherjee, colin lockard,xin luna dong, and andrew mccallum.
2019.integrating open information extractionopenki:and knowledge bases with relation inference.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages762–772.
association for computational linguis-tics..4716