bass: boosting abstractive summarization with uniﬁed semantic graph.
wenhao wu1∗, wei li2, xinyan xiao2, jiachen liu2,ziqiang cao3, sujian li1†, hua wu2,haifeng wang21key laboratory of computational linguistics, moe, peking university2baidu inc., beijing, china3institute of artiﬁcial intelligence, soochow university, china{waynewu,lisujian}@pku.edu.cn{liwei85,xiaoxinyan,liujiachen,wu hua,wanghaifeng}@baidu.com{zqcao}@suda.edu.cn.
abstract.
abstractive summarization for long-documentor multi-document remains challenging forthe seq2seq architecture, as seq2seq is notgood at analyzing long-distance relations intext.
in this paper, we present bass, a novelframework for boosting abstractive summa-rization based on a uniﬁed semantic graph,which aggregates co-referent phrases distribut-ing across a long range of context and con-veys rich relations between phrases.
further,a graph-based encoder-decoder model is pro-posed to improve both the document repre-sentation and summary generation process byleveraging the graph structure.
speciﬁcally,several graph augmentation methods are de-signed to encode both the explicit and im-plicit relations in the text while the graph-propagation attention mechanism is developedin the decoder to select salient content intothe summary.
empirical results show that theproposed architecture brings substantial im-provements for both long-document and multi-document summarization tasks..1.introduction.
nowadays, the sequence-to-sequence (seq2seq)based summarization models have gained unprece-dented popularity (rush et al., 2015; see et al.,2017; lewis et al., 2020).
however, complex sum-marization scenarios such as long-document ormulti-document summarization (mds), still bringgreat challenges to seq2seq models (cohan et al.,2018; liu et al., 2018).
in a long document nu-merous details and salient content may distributeevenly (sharma et al., 2019) while multiple doc-uments may contain repeated, redundant or con-tradictory information (radev, 2000).
these prob-lems make seq2seq models struggle with contentselection and organization which mainly depend.
∗work is done during an internship at baidu inc.† corresponding author..figure 1: illustration of a uniﬁed semantic graph and itsconstruction procedure for a document containing threesentences.
in graph construction, underlined tokensrepresent phrases., co-referent phrases are representedin the same color.
in the uniﬁed semantic graph,nodes of different colors indicate different types, ac-cording to section 3.1..on the long source sequence (shao et al., 2017).
thus, how to exploit deep semantic structure inthe complex text input is a key to further promotesummarization performance..compared with sequence, graph can aggregaterelevant disjoint context by uniformly representingthem as nodes and their relations as edges.
thisgreatly beneﬁts global structure learning and long-distance relation modeling.
several previous workshave attempted to leverage sentence-relation graphto improve long sequence summarization, wherenodes are sentences and edges are similarity or dis-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6052–6067august1–6,2021.©2021associationforcomputationallinguistics6052explanationcopnomod:of (cid:81)(cid:82)(cid:80)(cid:82)(cid:71)(cid:29)(cid:83)(cid:82)(cid:86)(cid:86)graph construction  albert einstein, a german physicist, published the theory of   relativity.
he won the physics nobel prize in 1921.  the great prize was for his explanation of the photoelectric effect.
1912apposdobjobjnsubjthe uniﬁed semantic graphthe photoelectric eﬀectthe physics nobel prizewonthe theory of relativitya german physicistalbert einsteinpublishedwas fornsubjoblnsubjhuman written summary   albert einstein received the physics nobel prize in 1912 for his   discovery of the law of the photoelectric effect(cid:1) course relations between sentences (li et al., 2020).
however, the sentence-relation graph is not ﬂexi-ble for ﬁne-grained (such as entities) informationaggregation and relation modeling.
some otherworks also proposed to construct local knowledgegraph by openie to improve seq2seq models (fanet al., 2019; huang et al., 2020).
however, theopenie-based graph only contains sparse relationsbetween partially extracted phrases, which cannotreﬂect the global structure and rich relations of theoverall sequence..for better modeling the long-distance relationsand global structure of a long sequence, we proposeto apply a phrase-level uniﬁed semantic graph tofacilitate content selection and organization.
basedon ﬁne-grained phrases extracted from dependencyparsing, our graph is suitable for information ag-gregation with the help of coreference resolutionthat substantially compresses the input and beneﬁtscontent selection.
furthermore, relations betweenphrases play an important role in organizing thesalient content when generating summaries.
forexample, in figure 1 the phrases “albert einstein”,“the great prize” and “explanation of the of the pho-toelectric” which distribute in different sentencesare easily aggregated through their semantic rela-tions to compose the ﬁnal summary sentence..we further propose a graph-based encoder-decoder model based on the uniﬁed semantic graph.
the graph-encoder effectively encodes long se-quences by explicitly modeling the relations be-tween phrases and capturing the global structurebased on the semantic graph.
besides, severalgraph augmentation methods are also applied dur-ing graph encoding to tap the potential semanticrelations.
for the decoding procedure, the graphdecoder incorporates the graph structure by graphpropagate attention to guide the summary genera-tion process, which can help select salient contentand organize them into a coherent summary..we conduct extensive experiments on boththe long-document summarization dataset big-patent and mds dataset wikisum to validatethe effectiveness of our model.
experiment re-sults demonstrate that our graph-based model sig-niﬁcantly improves the performance of both long-document and multi-document summarization overseveral strong baselines.
our main contributionsare summarized as follows:.
• we present the uniﬁed semantic graph whichaggregates co-referent phrases distributed.
in context for better modeling the long-distance relations and global structure in long-document summarization and mds..• we propose a graph-based encoder-decodermodel to improve both the document represen-tation and summary generation process of theseq2seq architecture by leveraging the graphstructure..• automatic and human evaluation on bothlong-document summarization and mds out-perform several strong baselines and validatethe effectiveness of our graph-based model..2 related works.
2.1 abstractive summarization.
abstractive summarization aims to generate a ﬂu-ent and concise summary for the given input doc-ument (rush et al., 2015).
most works applyseq2seq architecture to implicitly learn the sum-marization procedure (see et al., 2017; gehrmannet al., 2018; paulus et al., 2017; celikyilmaz et al.,2018).
more recently, signiﬁcant improvementshave been achieved by applying pre-trained lan-guage models as encoder (liu and lapata, 2019b;rothe et al., 2020) or pre-training the generationprocess leveraging a large-scale of unlabeled cor-pus (dong et al., 2019; lewis et al., 2020; qi et al.,2020; zhang et al., 2020a).
in mds, most of theprevious models apply extractive methods (erkanand radev, 2004; cho et al., 2019).
due to the lackof large-scale datasets, some attempts on abstrac-tive methods transfer single document summariza-tion (sds) models to mds (lebanoff et al., 2018;yang et al., 2019) or unsupervised methods basedon auto-encoder (chu and liu, 2019; braˇzinskaset al., 2020; amplayo and lapata, 2020).
after therelease of several large mds datasets (liu et al.,2018; fabbri et al., 2019), some supervised ab-stractive models for mds appear (liu and lapata,2019a; li et al., 2020).
their works also empha-size the importance of modeling cross-documentrelations in mds..2.2 structure enhanced summarization.
explicit structures play an important role in re-cent deep learning-based extractive and abstractivesummarization methods (li et al., 2018a,b; liuet al., 2019a).
different structures beneﬁt sum-marization models from different aspects.
con-stituency parsing greatly beneﬁts content selection.
6053input length#nodes#edges.
800140154.
1600291332.
2400467568.
3000579703.table 1:illustration of how the average number ofnodes and edges in the graph changes when the inputsequence becomes longer on wikisum..and compression for extractive models.
cao et al.
(2015) propose to extract salient sentences basedon their constituency parsing trees.
xu and dur-rett (2019) and desai et al.
(2020) jointly selectand compress salient content based on syntax struc-ture and syntax rules.
dependency parsing helpssummarization models in semantic understanding.
jin et al.
(2020) incorporate semantic dependencygraphs of input sentences to help the summariza-tion models generate sentences with better seman-tic relevance .
besides sentence-level structures,document-level structures also attract a lot of atten-tion.
fernandes et al.
(2019) build a simple graphconsisting of sentences, tokens and pos for sum-mary generation.
by incorporating rst trees, xuet al.
(2020) propose a discourse-aware model toextract sentences.
similarly, structures from se-mantic analysis also help.
liu et al.
(2015) andliao et al.
(2018) propose to guide summarizationwith abstract meaning representation (amr) fora better comprehension of the input context.
(liand zhuge, 2019) propose semantic link networksbased mds but without graph neural networks.
recently, the local knowledge graph by openie at-tracts great attention.
leveraging openie extractedtuples, fan et al.
(2019) compress and reduce re-dundancy in multi-document inputs in mds.
theirwork mainly focus on the efﬁciency in processinglong sequences.
huang et al.
(2020) utilize openie-based graph for boosting the faithfulness of thegenerated summaries.
compared with their work,our phrase-level semantic graph focus on modelinglong-distance relations and semantic structures..3 uniﬁed semantic graph.
in this section, we introduce the deﬁnition and con-struction of the uniﬁed semantic graph..for example, in figure 1 the node “albert einstein”is merged from phases “albert einstein” and “his”which indicate the same person by coreference res-olution.
deﬁned as a heterogeneous graph g, everynode v ∈ v and every edge eij ∈ e in our graphbelongs to a type of phrase and dependency parsingrelation, respectively.
determined by the type ofphrases merged from, nodes are categorized intothree different types: noun phrase (n), verb phrase(v), other phrase (o).
we neglect dependency re-lations in edges as they mainly indicate sentencesyntax.
instead, the meta-paths (sun et al., 2011) inthe uniﬁed semantic graph convey various seman-tic relations.
notice that most o such as adjectivephrases, adverb phrases function as modiﬁers, andthe meta-path o-n indicates modiﬁcation relation.
the meta-path n-n between noun phrases repre-sents appositive relation or appositional relation.
furthermore, two-hop meta-path represents morecomplex semantic relations in graph.
for example,n-v-n like [albert einstein]-[won]-[the physicsnobel prize] indicates svo (subject–verb–object)relation.
it is essential to effectively model thetwo-hop meta-path for complex semantic relationmodeling..3.2 graph construction.
to construct the semantic graph, we extract phrasesand their relations from sentences by ﬁrst mergingtokens into phrases and then merging co-referentphrases into nodes.
we employ corenlp (man-ning et al., 2014) to obtain coreference chains ofthe input sequence and the dependency parsing treeof each sentence.
based on the dependency parsingtree, we merge consecutive tokens that form a com-plete semantic unit into a phrase.
afterwards, wemerge the same phrases from different positionsand phrases in the same coreference chain to formthe nodes in the semantic graph..the ﬁnal statistics of the uniﬁed semantic graphon wikisum are illustrated in table 1, which indi-cates that the scale of the graph expands moderatelywith the inputs.
this also demonstrates how theuniﬁed semantic graph compresses long-text infor-mation..3.1 graph deﬁnition.
4 summarization model.
the uniﬁed semantic graph is a heterogeneousgraph deﬁned as g = (v, e), where v and e arethe set of nodes and edges.
every node in v repre-sents a concept merged from co-referent phrases..in this section, we introduce our graph-based ab-stractive summarization model, which mainly con-sists of a graph encoder and a graph decoder, asin the encoding stage, ourshown in figure 2..6054figure 2: illustration of our graph-based summarization model.
the graph node representation is initialized frommerging token representations in two-level.
the graph encoder models the augmented graph structure.
the decoderattends to both token and node representations and utilizes graph structure by graph-propagation attention..model takes a document or the concatenation ofa set of documents as text input (represented asx = {xk}), and encodes it by a text encoder to ob-tain a sequence of local token representations.
thegraph encoder further takes the uniﬁed semanticgraph as graph input (represented as g = (v, e)in section 3.1), and explicitly model the semanticrelations in graph to obtain global graph representa-tions.
based on several novel graph-augmentationmethods, the graph encoder also effectively tapsthe implicit semantic relations across the text input.
in the decoding stage, the graph decoder leveragesthe graph structure to guide the summary genera-tion process by a novel graph-propagate attention,which facilitates salient content selection and or-ganization for generating more informative andcoherent summaries..4.1 text encoder.
to better represent local features in sequence, weapply the pre-trained language model roberta(liu et al., 2019b) as our text encoder.
as the max-imum positional embedding length of robertais 512, we extend the positional embedding lengthand randomly initialize the extended part.
to bespeciﬁc, in every layer, the representation of ev-ery node is only updated by it’s neighbors by selfattention..4.2 graph encoder.
after we obtain token representations by the textencoder, we further model the graph structure toobtain node representations.
we initialize node rep-resentations in the graph based on token representa-tions and the token-to-node alignment informationfrom graph construction.
after initialization, weapply graph encoding layers to model the explicitsemantic relations features and additionally applyseveral graph augmentation methods to learn theimplicit structure conveyed by the graph.
node initializationsimilar to graph construc-tion in section 3.2, we initialize graph represen-tations following the two-level merging, tokenmerging and phrase merging.
the token mergingcompresses and abstracts local token features intohigher-level phrase representations.
the phrasemerging aggregates co-referent phrases in a widecontext, which captures long-distance and cross-document relations.
to be simple, these two merg-ing steps are implemented by average pooling.
graph encoding layerfollowing previousworks in graph-to-sequence learning (koncel-kedziorski et al., 2019; yao et al., 2020), we applytransformer layers for graph modeling by applyingthe graph adjacent matrix as self-attention mask.
graph augmentationfollowing previousworks (bastings et al., 2017; koncel-kedziorskiet al., 2019), we add reverse edges and self-loopedges in graph as the original directed edges are.
6055albert einstein was a theoretical physicist .
he was born in germany(cid:335)text encoderalbert einsteinwas born ina theoretical physicistwas germanygraph parser<bos>german physicist albert einstein… context attentiongraph encodergraph-prop attentionmasked self-attentiongraph fusionfeed forwardadd & normgraph decodergraph structuretext inputgraph inputalberteinsteinawastheoreticalphysicisthewasbornalbert einsteinwasa theoretical  physicisthewas born iningermanysummary outputtwo-level mergingpartial summarygermanygerman physicist albert einstein was …snot enough for learning backward information.
forbetter utilizing the properties of the united semanticgraph, we further propose two novel graph augmen-tation methods.
supernode as the graph becomes larger, noisesintroduced by imperfect graph construction also in-crease, which may cause disconnected sub-graphs.
to strengthen the robustness of graph modelingand learn better global representations, we add aspecial supernode connected with every other nodein the graph to increase the connectivity.
shortcut edgesindicated by previous works,graph neural networks are weak at modeling multi-hop relations (abu-el-haija et al., 2019).
how-ever, as mentioned in section 3.1, the meta-pathsof length two represent rich semantic structuresthat require further modeling the two-hop relationsbetween nodes.
as illustrated in figure 2, in a n-v-n meta-path [albert einstein]-[was]-[a theoreticalphysicist], the relations [albert einstein]-[was] and[was]-[a theoretical physicist] are obviously lessimportant than the two-hop relation [albert ein-stein]- [a theoretical physicist].
therefore we addshortcut edges between every node and its two-hop relation neighbors, represented as blue edgesin figure 2. we have also attempted other com-plex methods such as mixhop (abu-el-haija et al.,2019), but we ﬁnd shortcut edges are more efﬁcientand effective.
the effectiveness of these graphaugmentation methods has also been validated insection 6.2..4.3 graph decoding layer.
token and node representations beneﬁt summarygeneration in different aspects.
token representa-tions are better at capturing local features whilegraph representations provide global and abstractfeatures.
for leveraging both representations, weapply a stack of transformer-based graph decodinglayers as the decoder which attends to both repre-sentations and fuse them for generating summaries.
let yl−1denotes the representation of t-th sum-tmary token output by (l − 1)-th graph decodinglayer.
for the graph attention, we apply multi-headattention using yl−1as query and node representa-ttions v = {vj} as keys and values:.
αt,j =.
(yl−1.
t wq)(vjwk)tdhead.
√.
(1).
where wq, wk ∈ rd×d are parameter weights,αt,j denote the salient score for node j to yl−1..t.t.t = w t.we then calculate the global graph vector gtas weighted sum over values of nodes: gt =(cid:80)j sof tmax(αt,j)(vjwv ) where wv ∈ rd×dis a learnable parameter.
we also obtain contextu-alized text vector ct similar to the procedure aboveby calculating multi-head attention between yl−1and token representations.
afterwards, we use agraph fusion layer which is a feed-forward neuralnetwork to fuse the concatenation of the two fea-d ([gt, ct]), where wd ∈ r2d×d istures: dlthe linear transformation parameter and dlt is thehybrid representation of tokens and graph.
afterlayer-norm and feed-forward layer, the l-th graphdecoding layer output ylt is used as the input ofthe next layer and also used for generating the tthtoken in the ﬁnal layer.
graph-propagate attention when applyingmulti-head attention to graph, it only attends tonode representations linearly, neglecting the graphstructure.
inspired by klicpera et al.
(2019), we pro-pose the graph-propagate attention to leverage thegraph structure to guide the summary generationprocess.
by further utilizing semantic structure, thedecoder is more efﬁcient in selecting and organiz-ing salient content.
without extra parameters, thegraph-propagation attention can be convenientlyapplied to the conventional multi-head attention forstructure-aware learning..graph-propagate attention consists of two steps:salient score prediction and score propagation.
inthe ﬁrst step, we predict the salient score for everynode linearly.
we apply the output of multi-head at-tention αt ∈ r|v|×c in equation 1 as salient scores,where |v| is the number of nodes in the graph and cis the number of attention heads.
c is regarded asc digits or channels of the salient score for everynode.
we then make the salient score structure-aware through score propagation.
though pager-ank can propagate salient scores over the entiregraph, it leads to over-smoothed scores, as in everysummary decoding step only parts of the contentare salient.
therefore, for each node we only propa-gate its salient score p times in the graph, aggregat-ing at most p-hop relations.
let β0t = αt denotesthe initial salient score predicted in previous step,the salient score after p-th propagation is:.
βpt = ω ˆaβp−1.
t + (1 − ω)β0t.(2).
where ˆa = ad−1 is a degree-normalized adjacentmatrix of the graph1, and ω ∈ (0, 1] is the teleport.
1adjacent matrix a contains self-loop and reverse edges..6056probability which deﬁnes the salient score has theprobability ω to propagate towards the neighbornodes and 1 − ω to restart from initial.
the graph-propagation procedure can also be formulated as:.
βpt = (ωp ˆap + (1 − ω)(.
ωi ˆai))αt.
(3).
p−1(cid:88).
i=0.
after p steps of salient score propagation, the graphvector is then calculated by weighted sum of nodevalues:.
r-1model38.22lead36.12lexrank40.56transs2s40.77t-dmcaht41.53berts2s41.49robertas2s 42.0542.99graphsum43.65bass(2400)44.33bass(3000).
r-216.8511.6725.3525.6026.5225.7327.0027.8328.5528.38.r-l26.8922.5234.7334.9035.7635.5936.5637.3637.8537.87.bs--25.43-25.62-29.1329.6931.9131.71.
(cid:88).
(cid:48).
g.t =.
sof tmax(βp.
t,j)(vjw v ).
(4).
j.table 2: evaluation results on the test set of wikisum.
rouge-1, rouge-2, rouge-l and bertscore are ab-breviated as r-1,r-2,r-l and bs, respectively..where for the convenience of expression, theconcatenation of multi-head is omitted.
the outputof fusing g(cid:48)t and ct is then applied to generate thetth summary token as mentioned before..5 experiment setup.
in this section, we describe the datasets of our ex-periments and various implementation details..5.1 summarization datasets.
we evaluate our model on a sds dataset and anmds dataset, namely bigpatent (sharma et al.,2019) and wikisum (liu et al., 2018).
bigpatent is a large-scale patent documentsummarization dataset with an average input of3572.8 words and a reference with average lengthof 116.5 words.
bigpatent is a highly abstrac-tive summarization dataset with salient contentevenly distributed in the input.
we follow the stan-dard splits of sharma et al.
(2019) for training,validation, and testing (1,207,222/67,068/67,072).
wikisum is a large-scale mds dataset.
follow-ing liu and lapata (2019a), we treat the generationof lead wikipedia sections as an mds task.
tobe speciﬁc, we directly utilize the preprocessedresults from liu and lapata (2019a), which splitsource documents into multiple paragraphs andrank the paragraphs based on their titles to se-lect top-40 paragraphs as source input.
the av-erage length of each paragraph and the target sum-mary are 70.1 tokens and 139.4 tokens, respectively.
we concatenate all the paragraphs as the input se-quence.
we use the standard splits of liu and la-pata (2019a) for training, validation, and testing(1,579,360/38,144/38,205)..5.2.implementation details.
we train all the abstractive models by max like-lihood estimation with label smoothing (labelsmoothing factor 0.1).
as we ﬁne-tune the pre-trained language model roberta as text encoder,we apply two different adam optimizers (kingmaand ba, 2015) with β1 = 0.9 and β2 = 0.998to train the pre-trained part and other parts of themodel (liu and lapata, 2019b).
the learning rateand warmup steps are 2e-3 and 20,000 for the pre-trained part and 0.1 and 10,000 for other parts.
asnoticed from experiments, when the learning rateis high, graph-based models suffer from unstabletraining caused by the gradient explosion in the textencoder.
gradient clipping with a very small max-imum gradient norm (0.2 in our work) solves thisproblem.
all the models are trained for 300,000steps on bigpatent and wikisum with 8 gpus(nvidia tesla v100).
we apply dropout (withthe probability of 0.1) before all linear layers.
inour model, the number of graph-encoder layers andgraph-decoder layers are set as 2 and 6, respec-tively.
the hidden size of both graph encodingand graph decoding layers is 768 in alignment withroberta, and the feed-forward size is 2048 for pa-rameter efﬁciency.
for graph-propagation attention,the parameter ω is 0.9, and the propagation stepsp is 2. during decoding, we apply beam searchwith beam size 5 and length penalty with factor 0.9.trigram blocking is used to reduce repetitions..6 results.
6.1 automatic evaluation.
we evaluate the quality of generated summaries us-ing rouge f1(lin, 2004) and bertscore (zhang.
6057r-1model31.27lead43.56oracle35.99lexrank28.74seq2seqpointer30.5933.14pointer+cov37.12fastabs36.41tlmtranss2s34.93robertas2s 43.6245.83bart43.55pegasus-base45.04bass.
r-28.7516.9111.147.8710.0111.6311.8711.389.8618.6219.5320.4320.32.r-l26.1836.5229.6024.6625.6528.5532.4530.8829.9237.86--39.21.bs--------9.4218.18--20.13.table 3: evaluation results on the test set of big-patent where the length input of bass is 1024..et al., 2020b).
for rouge, we report unigram andbigram overlap between system summaries and ref-erence summaries (rouge-1, rouge-2).
we re-port sentence-level rouge-l for the bigpatentdataset and summary-level rouge-l for the wik-isum for a fair comparison with previous works.
we also report bertscore 2 f1, a better metricat evaluating semantic similarity between systemsummaries and reference summaries.
results on mds table 2 summarizes the evalua-tion results on the wikisum dataset.
we compareour model with several strong abstractive and ex-tractive baselines.
as listed in the top block, leadand lexrank (erkan and radev, 2004) are two clas-sic extractive methods.
the second block showsthe results of several different abstractive meth-ods.
transs2s is the transformer-based encoder-decoder model.
by replacing the transformer en-coder in transs2s with bert (devlin et al., 2019)or roberta and training with two optimizers (liuand lapata, 2019b), we obtain two strong base-lines berts2s and robertas2s.
t-dmca isthe best model presented by liu et al.
(2018) forsummarizing long sequence.
ht is the best modelpresented by liu and lapata (2019a) with the hi-erarchical transformer encoder and a ﬂat trans-former decoder.
graphsum, presented by li et al.
(2020), leverages paragraph-level explicit graph bythe graph encoder and decoder, which gives the cur-rent best performance on wikisum.
we report the.
2we apply roberta-large l17 no-idf version as the metricmodel and rescale with baseline setting according to sugges-tions on https://github.com/tiiiger/bert score..modelfull modelw/o structure+w/o merging.
r-142.2941.8641.56.r-227.1927.0626.61.r-l36.4636.4335.93.bs30.6229.8429.15.table 4: graph structure analysis on wikisum test setwhere the input length is 800. w/o structure and +w/omerging refer to remove relations between phrases andfurther remove phrase merging in graph construction,respectively..modelfull modelw/o shortcutw/o supernodew/o graph-propw/o graph.
r-143.4042.5042.9342.8442.05.r-228.5027.9728.0828.1427.00.r-l37.7137.2337.4237.4236.56.bs31.6431.1031.1531.3329.13.table 5: ablation study on wikisum test set where theinput length is 1600. graph-prop is the abbreviation ofgraph-propagation..best results of graphsum with roberta and theinput length is about 2400 tokens.
the last blockreports the results of our model bass with theinput lengths of 2400 and 3000. compared withall the baselines, our model bass achieves greatimprovements on all the four metrics.
the resultsdemonstrates the effectiveness of our phrase-levelsemantic graph comparing with other robertabased models, robertas2s (without graph) andgraphsum (sentence-relation graph).
furthermore,the phrase-level semantic graph improves the se-mantic relevance of the generated summaries andreferences, as the bertscore improvements ofbass is obvious.
results on sds table 3 shows our experimentresults along with other sds baselines.
similarto wikisum, we also report lexrank, transs2s,and robertas2s.
besides, we report the perfor-mance of several other baselines.
oracle is theupper-bound of current extrative models.
seq2seqis based on lstm encoder-decoder with atten-tion mechanism (bahdanau et al., 2015).
pointerand pointer+cov are pointer-generation (see et al.,2017) with and without coverage mechanism, re-spectively.
fastabs (chen and bansal, 2018) isan abstractive method by jointly training sentenceextraction and compression.
tlm (pilault et al.,2020) is a recent long-document summarizationmethod based on language model.
we also reportthe performances of recent pretrianing-based sota.
6058text generation models bart (large) and peagua-sus (base) on bigpatent, which both contain aparameter size of 406m .
the last block shows theresults of our model, which contains a parametersize of 201m .
the results show that bass consis-tently outperforms robertas2s, and comparablewith current large sota models with only half ofthe parameter size.
this further demonstrates theeffectiveness of our graph-augmented model onlong-document summarization..6.2 model analysis.
for a thorough understanding of bass, we con-duct several experiments on the wikisum test set,including the effects of the graph structure and in-put length.
we also validate the effectiveness of thegraph-augmentation methods in graph encoder andthe graph-propagation attention in graph decoderby ablation studies.
graph structure analysisto analyze how theuniﬁed semantic graph beneﬁts summarizationlearning, we conduct ablation studies on the graphstructures.
illustrated in table 4, after removingexplicit relations between phrases by fully connect-ing all the nodes, the r-1 metric drops obviouslywhich indicates the relations between phrases im-prove the informativeness of generated summaries.
after further removing phrase merging, we observea performance decrease in all the metrics, whichindicates the long-distance relations beneﬁt boththe informativeness and ﬂuency of summary.
ablation study the experimental results of re-moving supernode and shortcut edges from theuniﬁed semantic graph prove the effectiveness ofgraph augmentation methods in the graph encoder.
experimental results without the gaph-propagationattention conﬁrms that the structure of the uni-ﬁed semantic graph is also beneﬁcial for decoding.
overall, the performance of the model drops themost when removing shortcut edges which indi-cates the rich potential information is beneﬁcialfor summarization.
finally, after removing all thegraph-relevant components, performance dramati-cally drops on all the metrics.
length comparison according to liu et al.
(2018), input length affects the summarization per-formance seriously for seq2seq models as most ofthem are not efﬁcient at handling longer sequences.
the basic transs2s achieves its best performanceat the input length of 800, while longer input hurtsperformance.
several previous models achieve bet-.
figure 3: comparison of ht, graphsum (gsum in ﬁg-ure), bass under various length of input tokens..ter performance when utilizing longer sequences.
as illustrated in figure 3, the performance of htremains stable when the input length is longer than800. leveraging the power of sentence-level graph,graphsum achieves the best performance at 2,400but its performance begins to decrease when the in-put length reaches 3000. unlike previous methods,rouge-1 of bass signiﬁcantly increased in 3000indicates that the uniﬁed semantic graph beneﬁtssalient information selection even though the inputlength is extreme.
abastractiveness analysis we also study the ab-stractiveness of bass and other summarizationsystems on wikisum.
we calculate the averagenovel n-grams to the source input, which reﬂectsthe abstractiveness of a summarization system (seeet al., 2017).
illustrated in figure 4, bass gener-ates more abstract summaries comparing to recentmodels, graphsum, ht, and weaker than rober-tas2s.
summarized from observation, we drawto a conclusion that robertas2s usually gener-ates context irrelevant contents due to the strongpretrained roberta encoder but a randomly ini-tialized decoder that relays on the long-text inputpoorly.
graph-based decoders of bass and graph-sum alleviate this phenomenon..6.3 human evaluation.
in addition to the above automatic evaluations, wealso conduct human evaluations to assess the per-formance of systems.
because the patent datasetbigpatent contains lots of terminologies andrequires professional background knowledge forannotators, we select wikisum as the dataset forevaluations.
as wikipedia entries can be summa-rized in many different aspects, annotators willnaturally favor systems with longer outputs.
thuswe ﬁrst ﬁlter instances that the summaries of dif-ferent systems are signiﬁcantly different in lengths.
60590.8k1.6k2.4k3k41.041.542.042.543.043.544.0r-1htgsumbass0.8k1.6k2.4k3k26.527.027.528.028.5r-20.8k1.6k2.4k3k35.035.536.036.537.037.538.0r-lmodel.
1.transs2s 0.320.390.310.64.r.b.
g.s.
bass.
20.140.220.380.16.
30.090.260.200.14.
4.rating0.45 −0.21∗0.48∗0.130.58∗0.111.180.06.table 6: ranking results of system summaries by hu-man evaluation.
1 is the best and 4 is the worst.
thelarger rating denotes better summary quality.
r.b.
andg.s.
are the abbreviations of robertas2s and graph-sum.
* indicates the overall ratings of the correspond-ing model are signiﬁcantly (by welchs t-test with p<0.01) outperformed by bass..on both long-document summarization and mdsshow that our model outperforms several strongbaselines, which demonstrates the effectiveness ofour graph-based model and the superiority of theuniﬁed semantic graph for long-input abstractivesummarization.
though remarkable achievementshave been made by neural network-based summa-rization systems, they still do not actually under-stand languages and semantics.
incorporating lan-guage structures in deep neural networks as priorknowledge is a straightforward and effective wayto help summarization systems, as proved by thiswork and previous works..acknowledgments.
this work was partially supported by national keyr&d program of china (no.
2020yfb1406701)and national natural science foundation of china(no.
61876009)..references.
sami abu-el-haija, bryan perozzi, amol kapoor,nazanin alipourfard, kristina lerman, hrayr haru-tyunyan, greg ver steeg, and aram galstyan.
2019.mixhop: higher-order graph convolutional architec-in pro-tures via sparsiﬁed neighborhood mixing.
ceedings of the 36th international conference onmachine learning, volume 97 of proceedings of ma-chine learning research, pages 21–29, long beach,california, usa.
pmlr..reinald kim amplayo and mirella lapata.
2020. un-supervised opinion summarization with noising anddenoising.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 1934–1945, online.
association for computa-tional linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointly.
figure 4: illustration of novel n-grams in generatedsummaries form different systems..and then randomly select 100 instances.
we invite2 annotators to assess the summaries of differentmodels independently..annotators evaluate the overall quality of sum-maries by ranking them taking into account thefollowing criterias: (1) informativeness: whetherthe summary conveys important and faithful factsof the input?
(2) fluency: whether the summaryis ﬂuent, grammatical, and coherent?
(3) succinct-ness: whether the summary is concise and dose notdescribe too many details?
summaries with thesame quality get the same order.
all systems getscore 2,1,-1,2 for ranking 1,2,3,4 respectively.
therating of each system is averaged by the scores ofall test instances..the results of our system and the other threestrong baselines are shown in table 6. the per-centage of rankings and the overall scores are bothreported.
summarized from the results, our modelbass is able to generate higher quality summaries.
some examples are also shown in the appendix.
speciﬁcally, bass generates ﬂuent and concisesummaries containing more salient content com-pared with other systems.
the human evaluationresults further validate the effectiveness of our se-mantic graph-based model..7 conclusion and future work.
in this paper, we propose to leverage the uniﬁed se-mantic graph to improve the performance of neuralabstractive models for long-document summariza-tion and mds.
we further present a graph-basedencoder-decoder model to improve both the docu-ment representation and summary generation pro-cess by leveraging the graph structure.
experiments.
6060123n-gram01020304050607080% novel n-gramhttransformers2sgraphsumbassrobertas2sreferencein 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..jasmijn bastings,.
ivan titov, wilker aziz, diegomarcheggiani, and khalil sima’an.
2017. graphconvolutional encoders for syntax-aware neural ma-chine translation.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, pages 1957–1967, copenhagen, den-mark.
association for computational linguistics..arthur braˇzinskas, mirella lapata, and ivan titov.
unsupervised opinion summarization as2020.in proceedings of thecopycat-review generation.
58th annual meeting of the association for compu-tational linguistics, pages 5151–5169, online.
as-sociation for computational linguistics..ziqiang cao, furu wei, li dong, sujian li, andming zhou.
2015. ranking with recursive neu-ral networks and its application to multi-documentthe twenty-summarization.
ninth aaai conference on artiﬁcial intelligence,aaai’15, page 2153–2159.
aaai press..in proceedings of.
asli celikyilmaz, antoine bosselut, xiaodong he, andyejin choi.
2018. deep communicating agents forin proceedings of theabstractive summarization.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 1662–1675, new orleans, louisiana.
association for computational linguistics..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
corr, abs/1805.11080..sangwoo cho, chen li, dong yu, hassan foroosh,and fei liu.
2019. multi-document summariza-tion with determinantal point processes and con-in proceedings of thetextualized representations.
2nd workshop on new frontiers in summarization,pages 98–103, hong kong, china.
association forcomputational linguistics..eric chu and peter j. liu.
2019. meansum: a neu-ral model for unsupervised multi-document abstrac-tive summarization.
in proceedings of the 36th in-ternational conference on machine learning, icml2019, 9-15 june 2019, long beach, california, usa,volume 97 of proceedings of machine learning re-search, pages 1223–1232.
pmlr..arman cohan, franck dernoncourt, doo soon kim,trung bui, seokhwan kim, walter chang, and na-zli goharian.
2018. a discourse-aware attentionmodel for abstractive summarization of long docu-in proceedings of the 2018 conference ofments.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 615–621,new orleans, louisiana.
association for computa-tional linguistics..shrey desai, jiacheng xu, and greg durrett.
2020.compressive summarization with plausibility andsalience modeling.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 6259–6274, online.
as-sociation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed languagemodel pre-training for natural language understand-in advances in neural infor-ing and generation.
mation processing systems 32: annual conferenceon neural information processing systems 2019,neurips 2019, december 8-14, 2019, vancouver,bc, canada, pages 13042–13054..g¨unes erkan and dragomir r radev.
2004. lexrank:graph-based lexical centrality as salience in textsummarization.
journal of artiﬁcial intelligence re-search, 22:457–479..alexander fabbri, irene li, tianwei she, suyi li, anddragomir radev.
2019. multi-news: a large-scalemulti-document summarization dataset and abstrac-tive hierarchical model.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 1074–1084, florence, italy.
association for computational linguistics..angela fan, claire gardent, chlo´e braud, and an-toine bordes.
2019. using local knowledge graphconstruction to scale seq2seq models to multi-document inputs.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 4186–4196, hong kong, china.
as-sociation for computational linguistics..patrick fernandes, miltiadis allamanis, and marcbrockschmidt.
2019. structured neural summariza-tion.
in 7th international conference on learningrepresentations, iclr 2019, new orleans, la, usa,may 6-9, 2019. openreview.net..sebastian gehrmann, yuntian deng, and alexanderrush.
2018. bottom-up abstractive summarization.
the 2018 conference on em-in proceedings ofpirical methods in natural language processing,pages 4098–4109, brussels, belgium.
associationfor computational linguistics..6061luyang huang, lingfei wu, and lu wang.
2020.knowledge graph-augmented abstractive summa-rization with semantic-driven cloze reward.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5094–5107, online.
association for computational lin-guistics..hanqi jin, tianming wang, and xiaojun wan.
2020.semsum: semantic dependency guided neural ab-in aaai, pages 8026–stractive summarization.
8033..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..johannes klicpera, aleksandar bojchevski,.
andstephan g¨unnemann.
2019. predict then propagate:graph neural networks meet personalized pagerank.
in 7th international conference on learning repre-sentations, iclr 2019, new orleans, la, usa, may6-9, 2019. openreview.net..rik koncel-kedziorski, dhanush bekal, yi luan,mirella lapata, and hannaneh hajishirzi.
2019.text generation from knowledge graphs within proceedings of the 2019graph transformers.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 2284–2293, minneapolis, minnesota.
association for computational linguistics..logan lebanoff, kaiqiang song, and fei liu.
2018.adapting the neural encoder-decoder frameworkfrom single to multi-document summarization.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages4131–4141, brussels, belgium.
association forcomputational linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..wei li, xinyan xiao, jiachen liu, hua wu, haifengwang, and junping du.
2020. leveraging graphto improve abstractive multi-document summariza-in proceedings of the 58th annual meetingtion.
of the association for computational linguistics,pages 6232–6243, online.
association for compu-tational linguistics..wei li, xinyan xiao, yajuan lyu, and yuanzhuo wang.
2018a.
improving neural abstractive document sum-marization with explicit information selection mod-in proceedings of the 2018 conference oneling..empirical methods in natural language process-ing, brussels, belgium, october 31 - november 4,2018, pages 1787–1796.
association for computa-tional linguistics..wei li, xinyan xiao, yajuan lyu, and yuanzhuo wang.
2018b.
improving neural abstractive document sum-in pro-marization with structural regularization.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, brussels, bel-gium, october 31 - november 4, 2018, pages 4078–4087. association for computational linguistics..wei li and hai zhuge.
2019. abstractive multi-document summarization based on semantic link net-work.
ieee transactions on knowledge and dataengineering..kexin liao, logan lebanoff, and fei liu.
2018. ab-stract meaning representation for multi-documentin proceedings of the 27th inter-summarization.
national conference on computational linguistics,pages 1178–1190, santa fe, new mexico, usa.
as-sociation for computational linguistics..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..fei liu, jeffrey flanigan, sam thomson, normansadeh, and noah a. smith.
2015. toward abstrac-tive summarization using semantic representations.
in proceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1077–1086, denver, colorado.
associationfor computational linguistics..peter j. liu, mohammad saleh, etienne pot, bengoodrich, ryan sepassi, lukasz kaiser, and noamshazeer.
2018. generating wikipedia by summariz-ing long sequences.
in 6th international conferenceon learning representations, iclr 2018, vancou-ver, bc, canada, april 30 - may 3, 2018, confer-ence track proceedings.
openreview.net..yang liu and mirella lapata.
2019a.
hierarchicaltransformers for multi-document summarization.
intheproceedings ofassociation for computational linguistics, pages5070–5081, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
yang liu and mirella lapata.
2019b.
text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3730–3740, hong kong,china.
association for computational linguistics..yang liu, ivan titov, and mirella lapata.
2019a.
sin-ingle document summarization as tree induction.
proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,.
6062naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages1745–1755.
association for computational linguis-tics..louis shao, stephan gouws, denny britz, annagoldie, brian strope, and ray kurzweil.
2017. gen-erating long and diverse responses with neural con-versation models.
corr, abs/1701.03185..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..christopher manning, mihai surdeanu, john bauer,jenny finkel, steven bethard, and david mcclosky.
2014. the stanford corenlp natural language pro-in proceedings of 52nd annualcessing toolkit.
meeting of the association for computational lin-guistics: system demonstrations, pages 55–60, bal-timore, maryland.
association for computationallinguistics..romain paulus, caiming xiong, and richard socher.
2017. a deep reinforced model for abstractive sum-marization.
corr, abs/1705.04304..jonathan pilault, raymond li, sandeep subramanian,and chris pal.
2020. on extractive and abstractiveneural document summarization with transformerlanguage models.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 9308–9319, online.
as-sociation for computational linguistics..weizhen qi, yu yan, yeyun gong, dayiheng liu,nan duan, jiusheng chen, ruofei zhang, and mingzhou.
2020. prophetnet: predicting future n-gramfor sequence-to-sequencepre-training.
in findingsof the association for computational linguistics:emnlp 2020, pages 2401–2410, online.
associa-tion for computational linguistics..dragomir radev.
2000. a common theory of infor-mation fusion from multiple text sources step one:cross-document structure.
in 1st sigdial workshopon discourse and dialogue, pages 74–83, hongkong, china.
association for computational lin-guistics..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-quence generation tasks.
transactions of the asso-ciation for computational linguistics, 8:264–280..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-in proceedings of the 2015tence summarization.
conference on empirical methods in natural lan-guage processing, pages 379–389, lisbon, portugal.
association for computational linguistics..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..eva sharma, chen li, and lu wang.
2019. big-patent: a large-scale dataset for abstractive andcoherent summarization.
in proceedings of the 57thannual meeting of the association for computa-tional linguistics, pages 2204–2213, florence, italy.
association for computational linguistics..yizhou sun, jiawei han, xifeng yan, philip s yu,and tianyi wu.
2011. pathsim: meta path-basedtop-k similarity search in heterogeneous informationnetworks.
proceedings of the vldb endowment,4(11):992–1003..jiacheng xu and greg durrett.
2019. neural extractivetext summarization with syntactic compression.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3292–3303, hong kong, china.
association for computa-tional linguistics..jiacheng xu, zhe gan, yu cheng, and jingjing liu.
2020. discourse-aware neural extractive text sum-marization.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5021–5031, online.
association for computa-tional linguistics..wenmian yang, weijia jia, wenyuan gao, xiaojiezhou, and yutao luo.
2019. interactive variance at-tention based online spoiler detection for time-synccomments.
in proceedings of the 28th acm inter-national conference on information and knowledgemanagement, cikm ’19, page 1241–1250, newyork, ny, usa.
association for computing machin-ery..shaowei yao, tianming wang, and xiaojun wan.
2020. heterogeneous graph transformer for graph-to-sequence learning.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 7145–7154.
association for computationallinguistics..jingqing zhang, yao zhao, mohammad saleh, andpeter j. liu.
2020a.
pegasus: pre-training withextracted gap-sentences for abstractive summariza-tion.
in proceedings of the 37th international con-ference on machine learning, icml 2020, 13-18july 2020, virtual event, volume 119 of proceedingsof machine learning research, pages 11328–11339.
pmlr..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020b.
bertscore:evaluating text generation with bert.
in 8th inter-national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..6063a graph construction.
given a document set with n documents d ={d1, ...dn} and each document di ∈ d containski sentences.
algorithm 1 gives the details of con-structing the uniﬁed semantic graph based on de-pendency parsing..we apply corenlp for both coreference res-olution and dependency parsing.
we ﬁrst extractcoreference chains from every document and mergecoreference chains with overlap phrases.
we mem-orize all the coreference chains in set c, whereeach chain c = {p1, ..., pkc} ∈ c contains a set ofco-referent phrases.
we then parse every sentencein every document into a dependency parsing treets.
afterwords we reﬁnes the tree by following.
algorithm 1: construct uniﬁed seman-tic graphsinput: documents set d = {d1, ..., dn}, document.
di ∈ d, di = {s1, ..., ski }.
output: the uniﬁed semantic graph g.(cid:46) coreference resolution.
12 c ← ∅3 foreach d ∈ d do4.
56 end78 t ← ∅9 foreach d ∈ d do10.foreach s ∈ d do.
cd ← corefernce resolusion(d)c ← corefernce merge(c, cd).
(cid:46) dependency parsing.
11.
12.
13.
14.
15.
16.ts ← dependency parse(s)ts ← identify node types(ts)ts ← remove punctuation(ts)ts ← merge coref phrase(ts, c)ts ← merge nodes(ts)t ← t (cid:83){ts}.
(cid:46) initialize graph.
end.
1718 end1920 g = (v, e), v ← ∅, e ← ∅21 foreach tree t = (vt , et ) ∈ t do22.v ← v (cid:83){vt }e ← e (cid:83){et }.
2324 end2526 foreach corefernce chain c ∈ c do2728 end29 g ← (v, e)30 return g.(v, e) ← merge phrase(c, v, e).
(cid:46) merge co-referent nodes.
operations:.
• identify node types:.
after depen-dency parsing, each node in the tree isattached with a pos tag.
we associate everynode with its pos tag for future mergingoperations..• prune punctuation: we remove all the.
punctuation nodes and their edges..• merge corefe phrase: since a corefer-ence chain contains a set of phrases but a de-pendency parsing tree is based on tokens, weﬁrst obtain phrases in coreference chains forthe future convenience in merging coreferentphrases.
for every phrase pi in a co-referencechain c, we merge the corresponding tokens ofpi to form the target phrase pi in the tree.
themerging operation is carried out by removingedges between the nodes and represent thetokens as a uniﬁed node..• node merge: after obtaining phrases incoreference chains, we merge other tokennodes into concise phrases.
this procedureis carried out by traveling every dependencygraph in depth-ﬁrst, and merge the tokens intoa phrase if they satisfy the merging conditions.
overall, we merge consecutive tokens thatform a complete semantic unit into a phrases..after we extract all the phrases, we merge all thesame phrases and phrases in the same coreferencechain by merge phrase and return the ﬁnalsemantic graph..b case study.
we select several cases from human evaluation anddemonstrate them to show the overall quality of sys-tems.
in each table, there are four blocks presentthe input article (article), the reference summary(reference summary), the output summary of astrong baseline graphsum (baseline) and the out-put summary of our model bass (bass), sepa-rately.
the original input article is the concate-nation of several document paragraphs by the “||”symbol containing 1600 tokens in maximum.
weonly show the salient part of the input article dueto the paragraph constraints.
spans in highlightindicate the salient contents.
spans in red indi-cate the unfaithful content, irrelevant content orrepeats a system generated.
the case in table 7describes an american ice hockey player “colleencoyne”.
the important fact, “won a gold medalat the 1998 winter olympics”, is well capturedby bass, however, the baseline model only men-tions she “was a member” neglecting the substan-tial achievement.
the case in table 8 introducesthe play “colleen coyne” which based on the four.
6064novels of “leonardo padura” is difﬁcult to sum-marize, as the relation between “colleen coyne”,“leonardo padura” and the name list of the four nov-els cross different documents and a long-span.
thebaseline model confuses with the name of stars andfails to list the names of four books.
the referencesummary in table 9 is not informative enough togive a precise description of what is “cetaceanintelligence”.
though bass does not introducethe deﬁnition of “cetacean”, it clearly describesthe categories of “cetacean intelligence” which ismore essential to the topic.
in table 10, bass andbaseline generate summaries with similar content,but bass provides more details such as, “right-handed”, distributed in different documents.
inthe case describing broadcast, in table 11, whilethe baseline generates irrelevant titles of editors,bass describes essential characters of the maga-zine.
though all the models apply trigram-blockto avoid repeats, table 12 shows that sometimesthe baseline still generates repeated n-grams whilethis seldomly happens on bass..6065article: colleen coyne is a graduate of the university of new hampshire.
an ice hockey player , she.
represented the united states , as a defenseman, on 6 nat ....|| ... colleen coyne was one of the.
trailblazing women who won gold at the 1998 olympic winter games .
...||...history and heroes:george.
nagobads, mike ilitch’s pizza & colleen coyne of the 1998 u.s. women’s olympic team ....reference summary: colleen m. coyne (born september 19, 1971 ) is an american ice hockey player.
she won a gold medal at the 1998 winter olympics..baseline: colleen coyne (born november 3, 1974) is an american former ice hockey player.
she was amember of the united states women ’s national ice hockey team at the 1998 winter olympics..bass:she won a gold meda at the 1998 winter olympics..colleen.
coyne.
an.
is.
1998 winter olympics..american.
ice.
hockey.
player..table 7: colleen coyne.
article: “havana quartet” , based on the popular book series by cuban novelist.
leonardo padura ,.
|| “havana quartet”follows a hard-drinking, romantic cuban police detective mario conde...follows hard-drinking, romantic cuban police detective mario conde... || known cuban playwrighteduardo machado -lrb- starz ’s magic city -rrb-, who lives in the u.s., is the writer on the project,the popular four-book series of detective novels havana blue, havana gold, havana redbased.
on.
by another cuban writer.
that.
is well known internationally, novelist.
and havana blackleonardo padura ..reference summary: havana quartet is an upcoming american television drama series starringantonio banderas.
the series is based on four detective novels by cuban author leonardo padura ,.
havana blue , havana gold , havana red and havana black ..baseline: havana quartet is an upcoming american television drama series based on the novel series ofthe same name by cuban novelist leonardo padura the series stars antonio banderas, eduardo machadoand eduardo machad..bass: havana quartet -lrb- spanish : la gazeta de havana -rrb- is an upcoming american televisionseries baed on leonardo padura ’s novels havana blue , havana gold , havana red and havana black..table 8: havana quartetarticle:in the past three decades, new research has revealed that dolphin brains are not onlylarge but are extremely complex in their organization... dolphin brains are larger than expectedfor their body size and contain numerous features associated with sophisticated perceptualcognitive abilities, echolocation, communication, cooperation, and dynamic social networks ,and.
cultural transmission of learned behaviors , and self-awareness , to name a few..reference summary: cetacean intelligence refers to the cognitive capabilities of the cetacea order ofmammals.
this order includes whales, porpoises, and dolphins..baseline: cetacean intelligence ( cetacean intelligence ) refers to the ability of cetaceans to communi-cate with other animals.
cetacean intelligence is an object of fascination among scientists..bass:cetacean intelligence refers to the ability of cetaceans to communicate with other animals.
cetacans have a wide range of social and cognitive abilities, including echolocation, communication,.
cooperation and dynamic social networks, cultural transmission of learned behaviors,and.
self-awareness ..table 9: cetacean intelligence.
60661921 ,the sixth and ﬁnal child born to raymond and pearl.
article:|| james tolbert hearn was born on april 11 ,gia ,st. louis cardinals , new york giants , and philadelphia phillies from 1947 through 1959 .||.
in atlanta.
( adams.
,.
geor-) hearn ||.
jim hearn a || right-handed pitcher who helped propel the new york giants to victory ....reference summary: james tolbert hearn ( april 11 , 1921 – june 10 , 1998 ) was an americanright-handed pitcher in major league baseball for 13 seasons ( 1947 – 59 ) .
a right-hander , he stood 6feet 3 inches ( 1.91 m ) tall and weighed 205 pounds ( 93 kg ) ..baseline: james tolbert hearn ( april 11 , 1921 – june 10 , 1998 ) was an american professional baseballpitcher .
he played 13 seasons in major league baseball ( mlb ) for the st. louis cardinals , new yorkgiants , and philadelphia phillies ..bass:james tolbert hearn ( april 11, 1921 – june 10, 1998 ) was an american right-handedpitcher in major league baseball who played for the st.louis cardinals, new york giants ,and philadelphia phillies from 1947 through 1959 ..table 10: james tolbert hearnarticle: broadcast ( magazine ) content s.v.p.
chief content ofﬁcer derek t. dingle s.v.p.
/ executiveeditor-at-large alfred a. edmond jr. .
v.p.
director of ...|| broadcast , a weekly magazine for the uk tele-vision and radio industry , ...|| broadcast is the ideal supplement for those working in the uk broadcastingindustry , offering up-to-date industry news from the most experienced and authoritative voices inbroadcasting ||as a broadcast subscriber , you will beneﬁt from the uk tv industry’s most knowledgeablesource of industry news and analysis on the successful people , programmes and companies ..reference summary: broadcast is a weekly magazine for the united kingdom television and radioindustry .
it covers a wide range of news and issues affecting the professional broadcast market in theuk .
broadcast has regular weekly sections covering news ....baseline: broadcast is a british monthly magazine covering the uk television industry .
it is publishedby content .v.p.
.
the magazine was established in 1931 and is edited by alfred a. edmond , jr. .
v.p..bass:broadcast is a monthly magazine covering the uk television industry .
ﬁrst published in 1931 .
on the successful people , programmes and companies ..the magazine wasit is the uk ’s most authoritative voices of industry news and analysis.
table 11: broadcastarticle:dams building ( sault ste.
marie , michigan ) npgallery allows you to search the national registerinformation system a database of over 90,000 historic buildings ,... ( added 2010 - - # 10000218 )also known as central savings bank building 418 ashmun st. , sault ste.
marie || for those of you whoare interested in working with data in a gis environment....reference summary: the adams building , also known as the central savings bank building , wasbuilt as a commercial and ofﬁce building located at 418 ashmun street in sault ste.
marie , michigan .
....it was listed on the national register of historic places in 2010 ..baseline: the adams building , also known as the central savings bank building building , is a buildinglocated at 418 ashmun street in sault ste.
marie , michigan .
it was listed on the national register ofhistoric places in 2010 ..bass:the adams building , also known as the central savings bank building , is a commercial buildinglocated at 418 ashmun street in sault ste.
marie , michigan .
it was listed on the national register ofhistoric places in 2010 ..table 12: the adams building.
6067