modeling fine-grained entity types with box embeddings.
yasumasa onoe♠, michael boratko♢, andrew mccallum♢♣, greg durrett♠♠the university of texas at austin♢university of massachusetts amherst♣google research{yasumasa,gdurrett}@cs.utexas.edu{mboratko,mccallum}@cs.umass.edumccallum@google.com.
abstract.
neural entity typing models typically repre-sent fine-grained entity types as vectors in ahigh-dimensional space, but such spaces arenot well-suited to modeling these types’ com-plex interdependencies.
we study the abilityof box embeddings, which embed concepts asd-dimensional hyperrectangles, to capture hi-erarchies of types even when these relation-ships are not defined explicitly in the ontol-ogy.
our model represents both types and en-tity mentions as boxes.
each mention and itscontext are fed into a bert-based model toembed that mention in our box space; essen-tially, this model leverages typological cluespresent in the surface text to hypothesize atype representation for the mention.
box con-tainment can then be used to derive both theposterior probability of a mention exhibiting agiven type and the conditional probability rela-tions between types themselves.
we compareour approach with a vector-based typing modeland observe state-of-the-art performance onseveral entity typing benchmarks.
in addi-tion to competitive typing performance, ourbox-based model shows better performance inprediction consistency (predicting a supertypeand a subtype together) and confidence (i.e.,calibration), demonstrating that the box-basedmodel captures the latent type hierarchies bet-ter than the vector-based model does.1.
1.introduction.
the development of named entity recognition andentity typing has been characterized by a growth inthe size and complexity of type sets: from 4 (tjongkim sang and de meulder, 2003) to 17 (hovyet al., 2006) to hundreds (weischedel and brun-stein, 2005; ling and weld, 2012) or thousands(choi et al., 2018).
these types follow some kind.
1the code is available at https://github.com/.
yasumasaonoe/box4types..of hierarchical structure (weischedel and brunstein,2005; ling and weld, 2012; gillick et al., 2014;murty et al., 2018), so effective models for thesetasks frequently engage with this hierarchy explic-itly.
prior systems incorporate this structure viahierarchical losses (murty et al., 2018; xu andbarbosa, 2018; chen et al., 2020) or by embed-ding types into a high-dimensional euclidean orhyperbolic space (yogatama et al., 2015; l´opez andstrube, 2020).
however, the former approach re-quires prior knowledge of the type hierarchy, whichis unsuitable for a recent class of large type setswhere the hierarchy is not explicit (choi et al.,2018; onoe and durrett, 2020a).
the latter ap-proaches, while leveraging the inductive bias ofhyperbolic space to represent trees, lack a proba-bilistic interpretation of the embedding and do notnaturally capture all of the complex type relation-ships beyond strict containment..in this paper, we describe an approach that rep-resents entity types with box embeddings in a high-dimensional space (vilnis et al., 2018).
we build anentity typing model that jointly embeds each entitymention and entity types into the same box space todetermine the relation between them.
volumes ofboxes correspond to probabilities and taking inter-sections of boxes corresponds to computing jointdistributions, which allows us to model mention-type relations (what types does this mention ex-hibit?)
and type-type relations (what is the typehierarchy?).
concretely, we can compute the condi-tional probability of a type given the entity mentionwith straightforward volume calculations, allowingus to construct a probabilistic type classificationmodel..compared to embedding types as points in eu-clidean space (ren et al., 2016a), the box spaceis expressive and suitable for representing entitytypes due to its geometric properties.
boxes cannest, overlap, or be completely disjoint to capture.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2051–2064august1–6,2021.©2021associationforcomputationallinguistics2051figure 1: a mention (suzanne collins) and three entity types are embedded into a vector space (left) and a boxspace (right).
the box space can more richly represent hierarchical interactions between types and uncertaintyabout the properties of the mention..subtype, correlation, or disjunction relations, prop-erties which are not explicitly manifested in eu-clidean space.
the nature of the box computationalso allows these complex relations to be repre-sented in a lower-dimensional space than neededby vector-based models..in our experiments, we focus on comparingour box-based model against a vector-based base-line.
we evaluate on four entity typing bench-marks: ultra-fine entity typing (choi et al., 2018),ontonotes (gillick et al., 2014), bbn (weischedeland brunstein, 2005), and figer (ling and weld,2012).
to understand the behavior of box embed-dings, we further analyze the model outputs interms of consistency (predicting coherent super-types and subtypes together), robustness (sensitiv-ity against label noise), and calibration (i.e., modelconfidence).
lastly, we compare entity representa-tions obtained by the box-based and vector-basedmodels.
our box-based model outperforms thevector-based model on two benchmarks, ultra-fineentity typing and ontonotes, achieving state-of-the-art-performance.
in our other experiments, thebox-based model also performs better at predict-ing supertypes and subtypes consistently and beingrobust against label noise, indicating that our ap-proach is capable of capturing the latent hierarchi-cal structure in entity types..2 motivation.
when predicting class labels like entity types thatexhibit a hierarchical structure, we naturally wantour model’s output layer to be sensitive to this struc-ture.
previous work (ren et al., 2016a; shimaokaet al., 2017; choi et al., 2018; onoe and durrett,2019, inter alia) has fundamentally treated types asvectors, as shown in the left half of figure 1. as isstandard in multiclass or multi-label classification,the output layer of these models typically involvestaking a dot product between a mention embedding.
and each possible type.
a type could be more gen-eral and predicted on more examples by havinghigher norm,2 but it is hard for these representa-tions to capture that a coarse type like personwill have many mutually orthogonal subtypes..by contrast, box embeddings naturally representthese kinds of hierarchies as shown in the righthalf of figure 1. a box that is completely con-tained in another box is a strict subtype of thatbox: any entity exhibiting the inner type will ex-hibit the outer one as well.
overlapping boxes likepolitician and author represent types thatare not related in the type hierarchy but which arenot mutually exclusive.
the geometric structureof boxes enables complex interactions with only amoderate number of dimensions (dasgupta et al.,2020).
vilnis et al.
(2018) also define a probabilitymeasure over the box space, endowing it with prob-abilistic semantics.
if the boxes are restricted to aunit hypercube, for example, the volumes of typeboxes represent priors on types and intersectionscapture joint probabilities, which can then be usedto derive conditional probabilities..critically, box embeddings have previously beentrained explicitly to reproduce a given hierarchysuch as wordnet.
a central question of this workis whether box embeddings can be extended tomodel the hierarchies and type relationships thatare implicit in entity typing data: we do not as-sume access to explicit knowledge of a hierar-chy during training.
while some datasets such asontonotes have orderly ontologies, recent work onentity typing has often focused on noisy type setsfrom crowdworkers (choi et al., 2018) or derivedfrom wikipedia (onoe and durrett, 2020a).
weshow that box embeddings can learn these struc-tures organically; in fact, they are not restrictedto only tree structures, but enable a natural venn-diagram style of representation for concepts, as.
2we do not actually observe this in our vector-based model..2052… the hunger games, the first of 3 best selling books by suzanne collins.
figure 2: box-based entity typing model.
the mention and context (left) are embedded into the box space andprobabilities for each type are computed with a soft volume computation..with politician and author in figure 1..3 type modeling with boxes.
3.1 background: box embeddings.
our box embeddings represent entity types asn-dimensional hyperrectangles.
a box x ischaracterized by two points (xm, xm ), wherexm, xm ∈ rd are the minimum and the maximumcorners of the box x and xm,i ≤ xm,i for eachcoordinate i ∈ {1, ..., d}.
the volume of the boxx is computed as vol(x) = ∏︁i(xm,i − xm,i).
if we normalize the volume of the box spaceto be 1, we can interpret the volume of eachbox as the marginal probability of a mentionfurther-exhibiting the given entity type.
the intersection volume between twomore,boxes, x and y,is defined as vol(x ∩ y) =∏︁i max (min(xm,i, ym,i) − max(xm,i, ym,i), 0)and can be seen as the joint probability of entitytypes x and y. thus, we can obtain the conditionalprobability p (y | x) = vol(x∩y)vol(x) ..soft boxes computing conditional probabilitiesbased on hard intersection poses some practical dif-ficulties in the context of machine learning: sparsegradients caused by disjoint or completely con-tained boxes prevent gradient-based optimizationmethods from working effectively.
to ensure thatgradients always flow for disjoint boxes, li et al.
(2019) relax the hard edges of the boxes usinggaussian convolution.
we follow the more recentapproach of dasgupta et al.
(2020), who further im-prove training of box embeddings using max andmin gumbel distributions (i.e., gumbel boxes) torepresent the min and max coordinates of a box..3.2 box-based multi-label type classifier.
model is an arbitrary number of predicted types{t0, t1, ...} ∈ t , where tk is an entity type be-longing to a type inventory t .
because we do notassume an explicit type hierarchy, we treat entitytyping as a multi-label classification problem, or|t | independent binary classification problems foreach mention..section 3.3 will describe how to use a bert-based model to predict a mention and contextbox3 x from (m, s).
for now, we assume x isgiven and we are computing the probability ofthat mention exhibiting the kth entity type, withtype box yk.
each type tk ∈ t has a dedicatedbox yk, which is parameterized by a center vec-y ∈ rd.
thetor ckminimum and maximum corners of a box yk arey − softplus(okcomputed as yky)) andykm = σ(cky)) respectively, so thatparameters c ∈ rd and o ∈ rd yield a valid boxwith nonzero volume..y ∈ rd and an offset vector ok.m = σ(cky + softplus(ok.the conditional probability of the type tk given.
pθ(tk | m, s) =.
the mention and context (m, s) is calculated asvol(zk)vol(x)where zk is the intersection between x and yk ((2)and (3) in figure 2).
our final type predictionsare based on thresholding these probabilities; i.e.,predict the type if p > 0.5..vol(x ∩ yk)vol(x).
=.
,.
as mentioned in section 3.1, we use the gumbelbox approach of dasgupta et al.
(2020), in whichthe box coordinates are interpreted as the locationparameter of a gumbel max (resp.
min) distributionwith variance β. in this approach, the intersection.
let s denote a sequence of context words and mdenote an entity mention span in s. given the in-put tuple (m, s), the output of the entity typing.
3we could represent mentions as points instead of boxes;however, representing them as boxes enables the size of amention box to naturally reflect epistemic uncertainty about amention’s types given limited information..2053offset center embed mention and context into box space with bertcompute soft volume of intersection(1)(2)compute conditional probability by dividing volumes(3)0.70.2… the hunger games, the first of 3 best selling books by suzanne collins.mention and contextbert encoder==box coordinates becomexmβ + e.zkm = β ln.
(︃.
e.)︃.
ykmβ.,.
zkm = −β ln.
(︄.
e− xm.
β + e−.
)︄.
ykmβ.
..following dasgupta et al.
(2020), we approximatethe expected volume of a gumbel box using a soft-plus function:.
vol(x) ≈.
softplus.
∏︂.
i.
(︃ xm,i − xm,iβ.
)︃.
− 2γ.
,.
where i is an index of each coordinate and γ ≈0.5772 is the euler–mascheroni constant,4 andsoftplus(x) = 1t log(1 + exp(xt)), with t as aninverse temperature value..3.3 mention and context encoder.
we format the context words s and the mentionspan m as x = [cls] m [sep] s [sep] andchunk into wordpiece tokens (wu et al., 2016).
us-ing pre-trained bert5 (devlin et al., 2019), we en-code the whole sequence into a single vector by tak-ing the hidden vector at the [cls] token.
a high-way layer (srivastava et al., 2015) projects downthe hidden vector h[cls] ∈ rℓ to the r2d space,where ℓ is the hidden dimension of the encoder(bert), and d is the dimension of the box space.
this highway layer transforms representations in avector space to the box space without impeding thegradient flow.
we further split the hidden vectorh¯ ∈ r2d into two vectors: the center point of thebox cx ∈ rd and the offset from the maximumand minimum corners ox ∈ rd.
the minimum andmaximum corners of the mention and context boxare computed as xm = σ(cx − softplus(ox))and xm = σ(cx + softplus(ox)), where σ isan element-wise sigmoid function, and softplusis an element-wise softplus function as defined insection 3.2 ((1) in figure 2).
the output of thesoftplus is guaranteed to be positive, guaranteeingthat the boxes have volume greater than zero..3.4 learning.
the goal of training is to find a set of parametersθ that minimizes the sum of binary cross-entropylosses over all types over all examples in our train-.
4from dasgupta et al.
(2020), the euler-mascheroni con-stant appears due to the interpretation of xm,i, xm,i as thelocation parameters of gumbel distributions..5we use bert-large uncased (whole word masking) in.
our experiments..ing dataset d:.
l = −.
∑︂.
∑︂.
gold · log pθ(tk | m, s)tk.
k.(m,s,t)∈d+ (1 − tk.
gold) · log(1 − pθ(tk | m, s)),.
where tkgold ∈ {0, 1} is the gold label for the typetk.
we optimize this objective using gradient-basedoptimization algorithms such as adam (kingmaand ba, 2015).6.
4 experimental setup.
our focus here is to shed light on the differencebetween type hierarchies learned by the box-basedmodel and the vector-based model.
to this end, wefirst evaluate those two models on standard entitytyping datasets.
then, we test models’ consistency,robustness, and calibration, and evaluate the pre-dicted types as entity representations on a down-stream task (coreference resolution).
see appendixa for hyperparameters..4.1 baseline.
our chief comparison is between box-based andvector-based modeling of entity types.
as our mainbaseline for all experiments, we use a vector-basedversion of our entity typing model.
we use thesame mention and context encoder followed by ahighway layer, but this baseline has vector-basedtype embeddings (i.e., a |t | × d′ matrix), and typepredictions are given by a dot product betweenthe type embeddings and the mention and contextrepresentation followed by element-wise logisticregression.
this model is identical to that of onoeand durrett (2020b) except for the additional high-way layer..4.2 evaluation and datasets.
entity typing we evaluate our approach on theultra-fine entity typing (ufet) dataset (choiet al., 2018) with the standard splits (2k for eachof train, dev, and test).
in addition to the manuallyannotated training examples, we use the denoiseddistantly annotated training examples from onoeand durrett (2019).7 this dataset contains 10,331entity types, and each type is marked as one ofthe three classes: coarse, fine, and ultra-fine.
note.
6with large type sets, most types are highly skewed to-wards the negative class (>99% negative for many fine-grained types).
while past work such as choi et al.
(2018) hasused modified training objectives to handle this class imbal-ance, we did not find any modification to be necessary..7this consists of 727k training examples derived from the.
distantly labeled ufet data..2054that this classification does not provide explicithierarchies in the types, and all classes are treatedequally during training..additionally, we test our box-based model onthree other entity typing benchmarks that have rel-atively simpler entity type inventories with knownhierarchies, namely ontonotes (gillick et al.,2014), bbn (weischedel and brunstein, 2005) ,and figer (ling and weld, 2012).
see appendixb for more details on these datasets..consistency a model that captures hierarchicalstructure should be aware of the relationships be-tween supertypes and subtypes.
when a modelpredicts a subtype, we want it to predict the corre-sponding supertype together, even when this is notexplicitly enforced as a constraint or consistentlydemonstrated in the data, such as in the ufetdataset.
that is, when a model predicts artist,person should also be predicted.
to check thisability, we analyze the model predictions on theufet dev set.
we select 30 subtypes from theufet type inventory and annotate correspondingsupertypes for them in cases where these relation-ships are clear, based on their cooccurrence in theufet training set and human intuition.
based onthe 30 pairs, we compute accuracy of predictingsupertypes and subtypes together.
table 10 in ap-pendix c lists the 30 pairs..robustness entity typing datasets with verylarge ontologies like ufet are noisy; does ourbox-based model’s notion of hierarchy do a betterjob of handling intrinsic noise in a dataset?
to testthis in a controlled fashion, we synthetically createnoisy labels by randomly dropping the gold labelswith probability 13 .8 we derive two noisy trainingsets from the ufet training set: 1) adding noiseto the coarse types and 2) adding noise to fine &ultra-fine types.
we train on these noised datasetsand evaluate on the standard ufet dev set..calibration desai and durrett (2020) study cali-bration of pre-trained transformers such as bertand roberta (liu et al., 2019) on natural languageinference, paraphrase detection, and commonsensereasoning.
in a similar manner, we investigate ifour box-based entity typing model is calibrated: dothe probabilities assigned to types by the modelmatch the empirical likelihoods of those types?
since models may naturally have different scales.
8if this causes the gold type set to be empty, we retain the.
original gold type(s); however, this case is rare..model.
boxvector.
p.r.f1.
52.8 38.8 44.853.0 36.3 43.1.
47.1 24.2 32.0choi et al.
(2018)50.3 29.2 36.9label gcn (xiong et al., 2019)51.5 33.0 40.2elmo (onoe and durrett, 2019)bert-base (onoe and durrett, 2019) 51.6 33.0 40.2.table 1: macro-averaged p/r/f1 on the test set for theultra-fine entity typing task of choi et al.
(2018)..for their logits depending on how long they aretrained, we post-hoc calibrate each of our modelsusing temperature scaling (guo et al., 2017) and ashift parameter.
we report the total error (e.g., thesum of the errors between the mean confidence andthe empirical accuracy) on the ufet dev set andthe ontonotes dev set..entity representations we are interested in theusefulness of the trained entity typing models ina downstream task.
following onoe and durrett(2020b), we evaluate entity representation givenby the box-based and vector-based models on thecoreference arc prediction (cap) task (chen et al.,2019) derived from preco (chen et al., 2018).
thistask is a binary classification problem, requiring tojudge if two mention spans (either in one sentenceor two sentences) are the same entity or not.
asin onoe and durrett (2020b), we obtain type pre-dictions (a vector of probabilities associated withtypes) for each span and use it as an entity repre-sentation.
the final prediction of coreference for apair of mentions is given by the cosine similaritybetween the entity type probability vectors with athreshold 0.5. the original data split provides 8kexamples for each of the training, dev, and test sets.
we report accuracy on the cap test set..5 results and discussion.
5.1 entity typing.
here we report entity typing performance on ultra-fine entity typing (ufet), ontonotes, figer,and bbn.
for each dataset, we select the bestmodel from 5 runs with different random seedsbased on the development performance..ufet table 1 shows the macro-precision, re-call, and f1 scores on the ufet test set.
our box-based model outperforms the vector-based modeland state-of-the-art systems in terms of macro-.
2055total.
coarse.
fine.
ultra-fine.
model.
p.r.f1.
p.r.f1.
p.r.f1.
p.r.f1.
boxvector.
52.9 39.1 45.053.3 36.7 43.5.
71.2 82.5 76.471.7 79.9 75.6.
50.9 55.2 53.051.9 48.5 50.2.
45.4 24.5 31.943.7 22.7 29.8.
48.1 23.2 31.3choi et al.
(2018)49.3 28.1 35.8label gcn (xiong et al., 2019)elmo (onoe and durrett, 2019)50.7 33.1 40.1hy xlarge (l´opez and strube, 2020) 43.4 34.2 38.2.
60.3 61.6 61.066.2 68.8 67.566.9 80.7 73.261.4 73.9 67.1.
40.4 38.4 39.443.9 40.7 42.241.7 46.2 43.835.7 46.6 40.4.
42.814.68.842.4 14.2 21.345.6 17.4 25.236.5 19.9 25.7.table 2: macro-averaged p/r/f1 on the dev set for the entity typing task of choi et al.
(2018) comparing varioussystems.
our box-based model outperforms models from past work as well as our vector-based baseline..f1.9 compared to the vector-based model, the box-based model improves primarily in macro-recallcompared to macro-precision.
choi et al.
(2018)is a lstm-based model using glove (penningtonet al., 2014).
on top of this model, xiong et al.
(2019) add a graph convolution layer to model typedependencies.
onoe and durrett (2019) use elmo(peters et al., 2018) and apply denoising to fix labelinconsistency in the distantly annotated data..note that past work on this dataset has usedbert-base (onoe and durrett, 2019).
work onother datasets has used elmo and observed thatbert-based models have surprisingly underper-formed (lin and ji, 2019).
some of the gain fromour vector-based model can be attributed to ouruse of bert-large; however, our box model stillachieves stronger performance than the correspond-ing vector-based version which uses the same pre-trained model..table 2 breaks down the performance into thecoarse, fine, and ultra-fine classes.
our box-basedmodel consistently outperforms the vector-basedmodel in macro-recall and f1 across the threeclasses.
the largest gap in macro-recall is in thefine class, leading to the largest gap in macro-f1within the three classes..we also list the numbers from prior work intable 2. hy xlarge (l´opez and strube, 2020),a hyperbolic model designed to learn hierarchicalstructure in entity types, exceeds the performanceof the models with similar sizes such as choi et al.
(2018) and xiong et al.
(2019) especially in macro-recall.
in the ultra-fine class, both our box-basedmodel and hy xlarge achieve higher macro-f1compared to their vector-based counterparts..one possible reason for the higher recall of our.
9we omit the test number of l´opez and strube (2020),since they report results broken down into coarse, fine, andultra-fine types instead of an aggregated f1 value.
however,based on the development results, their approach substantiallyunderperforms the past work of onoe and durrett (2019) re-gardless..model is a stronger ability to model dependenciesbetween types.
instead of failing to predict a highlycorrelated type, the model may be more likely topredict a complete, coherent set of types..other datasets table 3 compares macro-f1 andmicro-f1 on the ontonotes, bbn, and figertest sets.10 on ontonotes, our box-based modelachieves better performance than the vector-basedmodel.
zhang et al.
(2018) use document-levelinformation, chen et al.
(2020) apply a hierarchi-cal ranking loss that assumes prior knowledge oftype hierarchies, and lin and ji (2019) propose anelmo-based model with an attention layer overmention spans and train their model on the aug-mented data from choi et al.
(2018).
among themodels trained only on the original ontonotestraining set, the box-based model achieves the high-est macro-f1 and micro-f1..the state-of-the-art system on bbn, the sys-tem of chen et al.
(2020) in the “undefined” set-ting, uses explicit knowledge of the type hierar-chy.
this is particularly relevant on the bbndataset, where the training data is noisy and fea-tures training points with obviously conflicting la-bels like person and organization, whichappear systematically in the data.
to simulate con-straints like the ones they use, we use three simplerules to modify our models’ prediction: (1) drop-ping person if organization exists, (2) drop-ping location if gpe exists, and (3) replacingfacility by fac, since both versions of this tagappear in the training set but only fac in the devand test set.
our box-based model and the vector-based model perform similarly and both achieveresults comparable with recent systems..on figer, our box-based model shows lowerperformance compared to the vector-based model,though both are approaching comparable results.
10note that our hyperparameters are optimized for macro.
f1 on ontonotes..2056ontonotes.
bbn.
figer.
model.
ma-f1 mi-f1 ma-f1 mi-f1 ma-f1 mi-f1.
boxvector.
zhang et al.
(2018)chen et al.
(2020) (exclusive)chen et al.
(2020) (undefined)lin and ji (2019).
77.376.2.
72.172.473.082.9†.
70.968.9.
66.567.268.177.3†.
78.7*78.3*.
75.763.279.779.3.
78.0*78.0*.
75.161.080.578.1.
79.481.6.
78.782.680.583.0.
75.077.0.
75.580.878.179.8.table 3: macro-averaged f1 and micro-averaged f1 on the test set for theentity typing task of ontonotes, bbn, figer.
†: not directly comparablesince large-scale augmented data is used.
*: we fix the predictions usingsimple rules post-hoc..bbn.
figer.
model dev ma-f1 dev ma-f1.
boxvector.
92.492.3.
94.394.7.table 4: macro-averaged f1on the dev set of bbn andfiger.
these dev sets aredrawn from the same distribu-tions as their training sets..with state-of-the-art systems.
we notice thatsome of the test examples have inconsistent la-bels (e.g., /organization/sports team ispresent, but its supertype /organization ismissing), penalizing models that predict the super-type correctly.
in addition, figer, like bbn, hassystematic shifts between training and test distri-butions.
we hypothesize that our model’s hyperpa-rameters (tuned on ontonotes only) are suboptimal.
the high dev performance shown in table 4 impliesthat our model optimized on held-out training ex-amples may not capture these specific shifts as wellas other models whose inductive biases are bettersuited to this unusually mislabeled data..5.2 consistency.
one factor we can investigate is whether our modelis able to predict type relations in a sensible, con-sistent fashion independent of the ground truth fora particular example.
for this evaluation, we in-vestigate our model’s predictions on the ufet devset.
we count the number of occurrences for eachsubtype in 30 supertype/subtype pairs (see table 10in appendix c).
then, for each subtype, we counthow many times its corresponding supertype is alsopredicted.
although these supertype-subtype rela-tions are not strictly defined in the training data,we believe they should nevertheless be exhibitedby models’ predictions.
accuracy is given by theratio between those counts, indicating how oftenthe supertype was correctly picked up..table 5 lists the total and per-supertype accu-racy on the supertype/subtype pairs.
we reportthe number of subtypes grouped by their super-types to show their frequency (the “count” columnin table 5).
our box-based model achieves bet-ter accuracy compared to the vector-based modelon all supertypes.
the gaps are particularly largeon place and organization.
note that some.
of the ufet training examples have inconsistentlabels (e.g., a subtype team can be a supertypeorganization or group), and this ambiguitypotentially confuses a model during training.
evenin those tricky cases, the box-based model showsreasonable performance.
the geometry of the boxspace itself gives some evidence as to why thisconsistency would arise (see section 5.6 for visual-ization of box edges)..5.3 robustness.
table 6 analyzes models’ sensitivity to the labelnoise.
we list the ufet dev performance by mod-els trained on the noised ufet training set.
whenthe coarse types are noised (i.e., omitting some su-pertypes), the vector-based model loses 4.8 pointsof macro-f1 while our box-based model only loses1.5 points.
a similar trend can be seen when thefine and ultra-fine types are noised (i.e., omittingsome subtypes).
in both cases, the vector-basedmodel shows lower recall compared to the samemodel trained on the clean data, while our box-based model is more robust.
we also note that thevector-based model tends to overfit to the trainingdata quickly.
we hypothesize that the use of boxesworks as a form of regularization, since movingboxes may be harder than moving points in a space,thus being less impacted by noisy labels..5.4 calibration.
following nguyen and o’connor (2015), we splitmodel confidence (output probability) for each typ-ing decision of each example into 10 bins (e.g., 0-0.1, 0.1-0.2 etc.).
for each bin, we compute meanconfidence and empirical accuracy.
we show thetotal calibration error (lower is better) as well asthe scaling and shifting constants in table 7. as theresults on ufet and ontonotes show, both box-based and vector-based entity typing models can be.
2057box.
vector.
model scale / shift total error.
model.
test acc..supertype.
count acc.
count acc..ufet.
personlocationplaceorganization.
98247049496.
99.786.195.984.6.
74545029407.
98.684.468.977.8.total.
1,997.
92.7.
1,631.
89.0.table 5: consistency: accuracy evaluated on the 30 su-pertype & subtypes pairs.
the “count” column showsthe number of subtypes found in the predictions.
theaccuracy is the frequency of predicting the correspond-ing supertype when the subtype is exhibited..training data model.
p.r.f1 ∆ in f1.
noised coarse.
51.0 37.9 43.5boxvector 51.5 31.0 38.7.noised fine.
53.0 37.2 43.7& ultra-fine vector 58.6 30.6 40.2.box.
-1.5-4.8.
-1.3-3.3.table 6: entity typing results of the ufet dev set.
models are trained on the noised ufet training set.
the “∆ in f1” column shows the performance dropfrom the model trained on the original ufet trainingset (not noised)..reasonably well calibrated after applying tempera-ture scaling and shifting.
however, the box-basedmodel achieves slightly lower total error..5.5 entity representation for coreference.
this experiment evaluates if model outputs are im-mediately useful in a downstream task.
for thistask, we use the box-based and vector-based en-tity typing models trained on the ufet trainingset (i.e., we do not train models on the cap train-ing set).
table 8 shows the test accuracy on thecap data.
our box-based model achieves slightlyhigher accuracy than the vector-based model, indi-cating that “out-of-the-box” entity representationsobtained by the box-based model contains moreuseful features for the cap task.11.
5.6 box edges.
to analyze how semantically related type boxes arelocated relative to one another in the box space, weplot the edges of the person and actor boxesalong the 109 dimensions one by one.
figure 3shows how those two boxes overlap each other inthe high-dimensional box space.
the upper plot.
11our results are not directly comparable to those of onoeand durrett (2020b); we train on the training set of ufetdataset, and they train on examples from the train, dev, andtest sets..boxvector.
0.5 / -1.10.2 / -1.1.
0.11190.3279.ontonotes.
boxvector.
0.9 / -0.30.7 / -0.4.
0.13580.1568.table 7: total calibration erroron ufet and ontronotes.
wescale and shift logits post-hoc..boxvectorrandom.
78.177.350.0.table 8: accuracyon the cap test set(chen et al., 2019).
this is a binaryclassification task..in figure 3 compares the person box and theactor box learned on the ufet data.
we cansee that the edges of person contain the edges ofactor in many dimensions but not all, meaningthat the person box overlaps with the actor boxbut doesn’t contain it perfectly as we might expect.
however, we can additionally investigatewhether the actor box is effectively contained inthe person for parts of the space actually used bythe mention boxes.
the lower plot in figure 3 com-pares the person box and the minimum boundingbox of the intersections between the actor andthe mention and context boxes obtained using theufet dev examples where the actor type is pre-dicted.
this minimum bounding box approximatesthe effective region within the actor box.
nowthe edges of actor are contained in the edges ofperson in the most of dimensions, indicating thatthe person box almost contains this “effective”actor box..6 related work.
embeddings embedding concepts/words into ahigh-dimensional vector space (hinton, 1986) hasa long history and has been an essential part ofneural networks for language (bengio et al., 2003;collobert et al., 2011).
there is similarly a longhistory of rethinking the semantics of these em-bedding spaces, such as treating words as regionsusing sparse count-based vectors (erk, 2009a,b) ordense distributed vectors (vilnis and mccallum,2015).
order embeddings (vendrov et al., 2016) ortheir probabilistic version (poe) (lai and hocken-maier, 2017) are one technique suited for hierarchi-cal modeling.
however, oe can only handle binaryentailment decisions, and poe cannot model nega-tive correlations between types, a critical limitationin its use as a probabilistic model; these shortcom-ings directly led to the development of box embed-dings.
hyperbolic embeddings (nickel and kiela,.
2058(a).
(b)figure 3: edges of (a) the person box vs the actor box and (b) the person box vs the minimum boundingbox of the intersections between mention & context boxes and the actor box..2017; l´opez and strube, 2020) can also modelhierarchical relationships as can hyperbolic entail-ment cones (ganea et al., 2018); however, theseapproaches lack a probabilistic interpretation..recent work on knowledge base completion(abboud et al., 2020) and reasoning over knowl-edge graphs (ren et al., 2020) embeds relationsor queries using box embeddings, but entities arestill represented as vectors.
in contrast, our modelembed both entity mentions and types as boxes..entity typing entity typing and named entityrecognition (tjong kim sang and de meulder,2003) are old problems in nlp.
recent work has fo-cused chiefly on predicted fine-grained entity types(ling and weld, 2012; gillick et al., 2014; choiet al., 2018), as these convey significantly more in-formation for downstream tasks.
as a result, thereis a challenge of scaling to large type inventories,which has inspired work on type embeddings (renet al., 2016a,b)..entity typing information has been used acrossa range of nlp tasks, including models for entitylinking and coreference (durrett and klein, 2014).
typing has been shown to be useful for cross-domain entity linking specifically (gupta et al.,2017; onoe and durrett, 2020a).
it has also re-cently been applied to coreference resolution (onoeand durrett, 2020b; khosla and rose, 2020) andtext generation (dong et al., 2020), suggesting thatit can be a useful intermediate layer even in pre-trained neural models..7 conclusion.
in this paper, we investigated a box-based modelfor fine-grained entity typing.
by representing en-tity types in a box embedding space and project-ing entity mentions into the same space, we cannaturally capture the hierarchy of and correlationsbetween entity types.
our experiments showed sev-eral benefits of box embeddings over the equivalentvector-based model, including typing performance,calibration, and robustness to noise..acknowledgments.
thanks to the members of the ut taur lab,pengxiang cheng, and eunsol choi for helpful dis-cussion; tongfei chen and ying lin for providingthe details of experiments.
this work was also par-tially supported by nsf grant iis-1814522, nsfgrant shf-1762299, and based on research in partsupported by the air force research laboratory(afrl), darpa, for the kairos program un-der agreement number fa8750-19-2-1003, as wellas university of southern california subcontractno.
123875727 under office of naval researchprime contract no.
n660011924032.
the u.s. gov-ernment is authorized to reproduce and distributereprints for governmental purposes notwithstand-ing any copyright notation thereon.
the views andconclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of afrl, darpa, orthe u.s. government..2059references.
̇ismailand tommaso salvatori..̇ilkan ceylan, thomasralph abboud,lukasiewicz,2020.boxe: a box embedding model for knowledgein proceedings of advances inbase completion.
neural information processing systems (neurips)..yoshua bengio, r´ejean ducharme, pascal vincent, andchristian janvin.
2003. a neural probabilistic lan-guage model.
3:1137–1155..lukas biewald.
2020..weights and biases.
wandb.com..experiment tracking withsoftware available from.
hong chen, zhenhua fan, hao lu, alan yuille, andshu rong.
2018. preco: a large-scale dataset inpreschool vocabulary for coreference resolution.
inproceedings of the conference on empirical meth-ods in natural language processing (emnlp)..mingda chen, zewei chu, yang chen, karl stratos,and kevin gimpel.
2019. enteval: a holistic eval-uation benchmark for entity representations.
in pro-ceedings of the conference on empirical methods innatural language processing (emnlp)..tongfei chen, yunmo chen, and benjamin van durme.
2020. hierarchical entity typing via multi-levelin proceedings of the annuallearning to rank.
meeting of the association for computational lin-guistics (acl)..eunsol choi, omer levy, yejin choi, and luke zettle-moyer.
2018. ultra-fine entity typing.
in proceed-ings of the annual meeting of the association forcomputational linguistics (acl)..ronan collobert, jason weston, l´eon bottou, michaelkarlen, koray kavukcuoglu, and pavel p. kuksa.
2011. natural language processing (almost) fromjournal of machine learning research,scratch.
12:2493–2537..shib sankar dasgupta, michael boratko, dongxuzhang, luke vilnis, xiang lorraine li, and andrewmccallum.
2020. improving local identifiability inin proceedings ofprobabilistic box embeddings.
advances in neural information processing systems(neurips)..shrey desai and greg durrett.
2020. calibration ofpre-trained transformers.
proceedings of the con-ference on empirical methods in natural languageprocessing (emnlp)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-in proceedings of the conference ofderstanding.
the north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl-hlt)..xiangyu dong, wenhao yu, chenguang zhu, andinjecting entity typesarxiv,.
meng jiang.
2020.into entity-guided text generation.
abs/2009.13401..greg durrett and dan klein.
2014. a joint model forentity analysis: coreference, typing, and linking.
transactions of the association for computationallinguistics (tacl), 2:477–490..katrin erk.
2009a.
representing words as regionsin proceedings of the confer-in vector space.
ence on computational natural language learning(conll)..katrin erk.
2009b.
supporting inferences in semanticin proceed-space: representing words as regions.
ings of the international conference on computa-tional semantics (iwcs)..octavian-eugen ganea, gary b´ecigneul, and thomashofmann.
2018. hyperbolic entailment cones forlearning hierarchical embeddings.
in proceedingsof the international conference on machine learn-ing (icml)..dan gillick, nevena lazic, kuzman ganchev, jessekirchner, and david huynh.
2014.context-dependent fine-grained entity type tagging.
corr, abs/1412.1820..chuan guo, geoff pleiss, yu sun, and kilian q. wein-berger.
2017. on calibration of modern neural net-works.
in proceedings of the international confer-ence on machine learning (icml)..nitish gupta, sameer singh, and dan roth.
2017. en-tity linking via joint encoding of types, descrip-in proceedings of the confer-tions, and context.
ence on empirical methods in natural languageprocessing (emnlp)..geoffrey e hinton.
1986. learning distributed repre-sentations of concepts.
in proceedings of the eighthannual conference of the cognitive science society..eduard hovy, mitchell marcus, martha palmer, lanceramshaw, and ralph weischedel.
2006. ontonotes:in proceedings of the confer-the 90% solution.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies (naacl-hlt)..sopan khosla and carolyn rose.
2020. using typeinformation to improve entity coreference resolu-tion.
in proceedings of the first workshop on com-putational approaches to discourse..diederik p. kingma and jimmy ba.
2015. adam:in inter-a method for stochastic optimization.
national conference on learning representations(iclr)..alice lai and julia hockenmaier.
2017. learning topredict denotational probabilities for modeling en-in proceedings of the conference of thetailment..2060european chapter of the association for computa-tional linguistics (eacl)..lisha li, kevin g. jamieson, giulia desalvo, afshinrostamizadeh, and ameet talwalkar.
2017. hy-perband: bandit-based configuration evaluationfor hyperparameter optimization.
in internationalconference on learning representations (iclr)..xiang li, luke vilnis, dongxu zhang, michael bo-ratko, , and andrew mccallum.
2019. smoothingthe geometry of probabilistic box embeddings.
ininternational conference on learning representa-tions (iclr)..ying lin and heng ji.
2019. an attentive fine-grained entity typing model with latent type rep-in proceedings of the conference onresentation.
empirical methods in natural language processing(emnlp)..xiao ling and daniel s. weld.
2012. fine-grained en-tity recognition.
in proceedings of the aaai con-ference on artificial intelligence..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrain-ing approach.
arxiv, abs/1907.11692..federico l´opez and michael strube.
2020. a fully hy-perbolic neural model for hierarchical multi-classin findings of the association forclassification.
computational linguistics: emnlp..shikhar murty, patrick verga, luke vilnis,.
irenaradovanovic, and andrew mccallum.
2018. hierar-chical losses and new resources for fine-grainedin proceedings of theentity typing and linking.
annual meeting of the association for computa-tional linguistics (acl)..khanh nguyen and brendan o’connor.
2015. poste-rior calibration and exploratory analysis for nat-ural language processing models.
in proceedingsof the conference on empirical methods in naturallanguage processing (emnlp)..maximilian nickel and douwe kiela.
2017. poincar´eembeddings for learning hierarchical representa-tions.
in proceedings of advances in neural infor-mation processing systems (neurips)..yasumasa onoe and greg durrett.
2019. learningto denoise distantly-labeled data for entity typ-ing.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..yasumasa onoe and greg durrett.
2020b..inter-pretable entity representations through large-scaletyping.
in findings of the association for computa-tional linguistics: emnlp..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordin proceedings of the conferencerepresentation.
on empirical methods in natural language process-ing (emnlp)..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the conference ofresentations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl-hlt)..hongyu ren, weihua hu, and jure leskovec.
2020.query2box: reasoning over knowledge graphs inin inter-vector space using box embeddings.
national conference on learning representations(iclr)..xiang ren, wenqi he, meng qu, lifu huang, hengji, and jiawei han.
2016a.
afet: automatic fine-grained entity typing by hierarchical partial-labelin proceedings of the conference onembedding.
empirical methods in natural language processing(emnlp)..xiang ren, wenqi he, meng qu, clare r. voss, hengji, and jiawei han.
2016b.
label noise reduction inentity typing by heterogeneous partial-label em-bedding.
in proceedings of the 22nd acm sigkddinternational conference on knowledge discoveryand data mining..sonse shimaoka, pontus stenetorp, kentaro inui, andsebastian riedel.
2017. neural architectures forin pro-fine-grained entity type classification.
ceedings of the conference of the european chap-ter of the association for computational linguistics(eacl)..rupesh kumar srivastava, klaus greff, and j¨urgenschmidhuber.
2015. highway networks.
arxiv,abs/1505.00387..erik f. tjong kim sang and fien de meulder.
2003.introduction to the conll-2003 sharedtask: language-independent named entity recogni-tion.
in proceedings of the conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies(naacl-hlt)..yasumasa onoe and greg durrett.
2020a..fine-grained entity typing for domain independent en-tity linking.
in proceedings of the aaai conferenceon artificial intelligence..ivan vendrov, ryan kiros, sanja fidler, and raquel ur-tasun.
2016. order-embeddings of images and lan-in international conference on learningguagen.
representations (iclr)..2061conference on lexical and computational seman-tics (*sem)..luke vilnis, xiang li, shikhar murty, and andrew mc-callum.
2018. probabilistic embedding of knowl-in pro-edge graphs with box lattice measures.
ceedings of the annual meeting of the associationfor computational linguistics (acl)..luke vilnis and andrew mccallum.
2015. wordrepresentations via gaussian embedding.
in inter-national conference on learning representations(iclr)..ralph weischedel and ada brunstein.
2005. bbn pro-noun coreference and entity type corpus.
linguisticdata consortium..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan fun-towicz, joe davison, sam shleifer, patrick vonplaten, clara ma, yacine jernite, julien plu, can-wen xu, teven le scao, sylvain gugger, mariamadrame, quentin lhoest, and alexander m. rush.
2020. transformers: state-of-the-art natural lan-guage processing.
in proceedings of the conferenceon empirical methods in natural language process-ing (emnlp)..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, lukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, gregory s. corrado, macduffhughes, and jeffrey dean.
2016. google’s neu-ral machine translation system: bridging the gapbetween human and machine translation.
arxiv,abs/1609.08144..wenhan xiong, jiawei wu, deren lei, mo yu, shiyuchang, xiaoxiao guo, and william yang wang.
2019. imposing label-relational inductive bias forextremely fine-grained entity typing.
in proceed-ings of the conference of the north american chap-ter of the association for computational linguistics:human language technologies (naacl-hlt)..peng xu and denilson barbosa.
2018. neural fine-grained entity type classification with hierarchy-in proceedings of the conference ofaware loss.
the north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl-hlt)..dani yogatama, daniel gillick, and nevena lazic.
2015. embedding methods for fine grained entityin proceedings of the annualtype classification.
meeting of the association for computational lin-guistics (acl)..sheng zhang, kevin duh, and benjamin van durme.
2018.fine-grained entity typing through in-creased discourse context and adaptive classifica-tion thresholds.
in proceedings of the seventh joint.
2062appendix a: hyperparameter search.
we use bayesian hyperparameter tuning and thehyperband stopping criteria (li et al., 2017) imple-mented in the weights & biases software (biewald,2020).
we use adam (kingma and ba, 2015) for allexperiments.
we perform hyperparameter searchon ontonotes due to its fast convergence.
thisfinds a lower dimension for the box-based modelcompared to the vector-based model (109-d × 2vs 307-d), resulting fewer parameters in the box-based model.
when we train the box-based modelon the ufet dataset, we sample 1,000 negatives(i.e., wrong types) to speed up convergence; this isnot effective in the vector-based model, so we donot do this there..we use the same hyperparameters for the otherthree datasets.
we train all models using nvidiav100 gpu with batch size 128. we implement ourmodels using huggingface’s transformers library(wolf et al., 2020)..table 9 shows hyperparameters of the box-basedand vector-based models as well as their rangesto search.
for adam, we use β1 = 0.9 and β2 =0.999 for training..model hyperparameter.
range.
selected.
box.
vector.
batch sizelr (bert)lr (other)box dimensiongumbel temp.
softplus temp.†.
{16, 32, 64, 128}-[0.0001, 0.01][50, 250][0.0001, 0.01]*[0.1, 10]*.
batch sizelr (bert)lr (other)vector dimension.
{16, 32, 64, 128}-[0.0001, 0.01][100, 500].
1282e-50.003721090.000361.2471.
1282e-50.00539307.appendix b: entity typing benchmarks.
et.
al.,.
2014).
(gillick.
hasontonotes89 types with a 3-level hierarchy (e.g.,/location/geography/mountain).
we use the same splits (250k train / 2k dev / 9ktest) provided by (shimaoka et al., 2017).
figer(ling and weld, 2012), derived from wikipedia,uses 113 types with a 2-level hierarchy (e.g.,/person/musician).
we use the same splits(2m train / 10k dev / 563 test) as (shimaoka et al.,2017).
bbn (weischedel and brunstein, 2005)is based on the one million word penn treebank.
corpus from wall street journal articles.
we usethe same splits (84k train / 2k dev / 14k test) asren et al.
(2016b); chen et al.
(2020)..appendix c: supertype/subtype pairs.
table 10 shows the supertype/subtype pairs wemanually annotated for our consistency test..supertype.
subtype.
politicianathleteleaderofficialspokespersonmusicianactorprofessionalmalefemalecountrycityarearegionpositionspacedistrictterritorystructurebuildingcompanyinstitutiongovernmentagencyteam.
personpersonpersonpersonpersonpersonpersonpersonpersonpersonlocationlocationlocationlocationlocationlocationlocationlocationplaceplaceorganizationorganizationorganizationorganizationorganizationorganization administrationorganizationorganizationorganizationorganization.
militaryassociationsocial groupcommittee.
table 10: 30 supertype and subtype pairs used for theconsistency test..similar to figure 3, we plot the semantically unre-lated type boxes food and building in figure 4.these boxes are largely misaligned as expected,and the minimum bounding box of the intersec-tions between the building and the mention andcontext boxes is also off from the food box..appendix e: reliability plot.
figure 5 visualizes the alignment between confi-dence and empirical accuracy on the ufet andontonotes dev sets..table 9: hyperparameters and their ranges.
*: we usea log uniform distribution.
†: pytorch implementationof a softplus function takes inverse β..appendix d: box edges.
2063(a).
(b).
figure 4: edges of (a) the food box vs the building box and (b) the food box vs the minimum bounding boxof the intersections between mention & context boxes and the building box..(a) ufet.
(b) ontonotes.
figure 5: reliability plots on (a) ufet and (b) ontonotes..2064