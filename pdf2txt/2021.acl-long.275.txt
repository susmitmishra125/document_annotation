nested named entity recognition viaexplicitly excluding the inﬂuence of the best path.
yiran wang1∗, hiroyuki shindo2, yuji matsumoto3, taro watanabe21national institute of information and communications technology (nict), kyoto, japan2nara institute of science and technology (naist), nara, japan3riken center for advanced intelligence project (aip), tokyo, japanyiran.wang@nict.go.jp, shindo@is.naist.jp,yuji.matsumoto@riken.jp, taro@is.naist.jp.
abstract.
this paper presents a novel method for nestednamed entity recognition.
as a layeredmethod, our method extends the prior second-best path recognition method by explicitly ex-cluding the inﬂuence of the best path.
ourmethod maintains a set of hidden states at eachtime step and selectively leverages them tobuild a different potential function for recogni-tion at each level.
in addition, we demonstratethat recognizing innermost entities ﬁrst resultsin better performance than the conventionaloutermost entities ﬁrst scheme.
we provideextensive experimental results on ace2004,ace2005, and genia datasets to show theeffectiveness and efﬁciency of our proposedmethod..1.introduction.
named entity recognition (ner), as a key tech-nique in natural language processing, aims at de-tecting entities and assigning semantic categorylabels to them.
early research (huang et al., 2015;ma and hovy, 2016; lample et al., 2016) proposedto employ deep learning methods and obtainedsigniﬁcant performance improvements.
however,most of them assume that the entities are not nestedwithin other entities, so-called ﬂat ner.
inherently,these methods do not work satisfactorily whennested entities exist.
figure 1 displays an exampleof the nested ner task..recently, a large number of papers proposednovel methods (fisher and vlachos, 2019; wanget al., 2020) for the nested ner task.
among them,layered methods solve this task through multi-levelsequential labeling, in which entities are dividedinto several levels, where the term level indicatesthe depth of entity nesting, and sequential labelingis performed repeatedly.
as a special case of lay-ered method, shibuya and hovy (2020) force the.
∗ this work was done when the ﬁrst author was at naist..figure 1: an example of nested ner..next level entities to locate on the second-best pathof the current level search space.
hence, their algo-rithm can repeatedly detect inner entities throughapplying a conventional conditional random ﬁeld(crf) (lafferty et al., 2001) and then exclude theobtained best paths from the search space.
to accel-erate computation, they also designed an algorithmto efﬁciently compute the partition function withthe best path excluded.
moreover, because theysearch the outermost entities ﬁrst, performing thesecond-best path search only on the spans of ex-tracted entities is sufﬁcient, since inner entities canonly exist within outer entities..however, we claim that the target path at thenext level is neither necessary nor likely to be thesecond-best path at the current level.
instead, thosepaths sharing many overlapping labels with the cur-rent best path are likely to be the second-best path.
besides, shibuya and hovy (2020) reuse the samepotential function at all higher levels.
thus, eventhough they exclude the best path, the inﬂuence ofthe best path is still preserved, since the emissionscores of labels on the best path are used in the nextlevel recognition.
moreover, these best path labelsare treated as the target labels at the current level.
however, if they are not on the best path of the nextlevel, they will be treated as non-target labels atthe next level, hence these adversarial optimizationgoals eventually hurt performance..in this paper, we use a different potential func-tion at each level to solve this issue.
we proposeto achieve this by introducing an encoder that pro-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3547–3557august1–6,2021.©2021associationforcomputationallinguistics3547formerhogwartsheadmasterdumbledorealbusroleroleorgroleperperduces a set of hidden states at each time step.
ateach level, we select some hidden states for en-tity recognition, then, remove these hidden stateswhich have interaction with the best path labelsbefore moving to the next level.
in this way, theemission scores of these best path labels are com-pletely different, so we can explicitly exclude theinﬂuence of the best path.
furthermore, we alsopropose three different selection strategies for fullyleveraging information among hidden states..besides, shibuya and hovy (2020) proposed torecognize entities from outermost to inner.
weempirically demonstrate that extracting the inner-most entities ﬁrst results in better performance.
this may due to the fact that some long entities donot contain any inner entity, so using outermost-ﬁrst encoding mixes these entities with other shortentities at the same levels, therefore leading en-coder representations to be dislocated.
in this paper,we convert entities to the iobes encoding scheme(ramshaw and marcus, 1995), and solve nestedner through applying crf level by level..our contributions are considered as fourfold,(a) we design a novel nested ner algorithm toexplicitly exclude the inﬂuence of the best paththrough using a different potential function at eachlevel, (b) we propose three different selection strate-gies for fully utilizing information among hiddenstates, (c) we empirically demonstrate that recog-nizing entities from innermost to outer results inbetter performance, (d) and we provide extensiveexperimental results to demonstrate the effective-ness and efﬁciency of our proposed method on theace2004, ace2005, and genia datasets..2 proposed method.
named entities recognition task aims to recognizeentities in a given sequence {xt}nt=1.
for nestedner some shorter entities may be nested withinlonger entities, while for ﬂat ner there is no suchcase.
existing algorithms solve ﬂat ner by ap-plying a sequential labeling method, which assignseach token a label yt ∈ y to determine the spanand category of each entity and non-entity simul-taneously.
to solve nested ner, we follow theprevious layered method and extend this sequen-tial labeling method with a multi-level encodingscheme.
in this encoding scheme, entities are di-vided into several levels according to their depths,we apply the sequential labeling method level bylevel to recognize all entities..2.1 encoding schemes.
shibuya and hovy (2020) proposed to recognizethe outermost entities ﬁrst and recursively detectthe nested inner entities.
however, we ﬁnd that de-tecting from the innermost entities results in betterperformance.
we take the sentence in figure 1 asan example to illustrate the details of these two en-coding schemes.
the results of the outermost-ﬁrstencoding scheme look as follows..i-per.
i-per.
b-per.
i-per e-per(level 1)(level 2) b-role i-role e-role b-per e-perb-role e-role(level 3)s-roles-org(level 4)oo(level 5)oo(level 6).
oooo.oooo.oooo.labels b-, i-, e- indicate the current wordis the beginning, the intermediate, and the end ofan entity, respectively.
label s- means this isa single word entity, and label o stands for non-entity word.
for example, the outermost entity“former hogwarts headmaster albus dumbledore”appears at the ﬁrst level, while innermost entities“hogwarts” and “headmaster” appear at the fourthlevel.
since there exists no deeper nested entity,the remaining levels contain only label o..in contrast, the innermost-ﬁrst encoding schemeconverts the same example to the following labelsequences..oo.s-org(level 1)b-role e-role(level 2)(level 3) b-role i-role e-rolei-per(level 4)o(level 5)o(level 6).
b-peroo.i-peroo.s-role b-per e-peroo.oo.i-per e-per.
oo.oo.in this encoding scheme,.
innermost entities“hogwarts”, “headmaster”, and “albus dumble-dore” appear at the ﬁrst level.
note that theinnermost-ﬁrst encoding scheme is not the sim-ple reverse of the outermost-ﬁrst encoding scheme.
for example, the entity “former hogwarts head-master” and the entity “albus dumbledore” appearat the same level in the outermost-ﬁrst scheme butthey appear at different levels in the innermost-ﬁrstscheme..2.2.inﬂuence of the best path.
although the second-best path searching algorithmis proposed as the main contribution of shibuyaand hovy (2020), we claim that forcing the targetpath at the next level to be the second-best path at.
3548figure 2: the architecture of our model.
the dotted lines mean these components are shared across levels..the current level is not optimal.
as the innermost-the best pathﬁrst encoding example above,at level 3 is b-role,i-role,e-role,o,o.
therefore the second-best path is more likelyto be one of those paths that share as manyas possible labels with the best path, e.g.,b-role,i-role,e-role,o,s-org, ratherthan the actual target label sequence at level 4,i.e., b-per,i-per,i-per,i-per,e-per,which does not overlap with the best path at all.
in addition, shibuya and hovy (2020) reuse thesame potential function at all higher levels.
thisindicates that, for instance, at level 3 and time step1, their model encourages the dot product of thehidden state and the label embedding h(cid:62)1 vb-roleto be larger than h(cid:62)1 vb-per, while at level 4, the re-maining inﬂuence of the best path reversely forcesh(cid:62)1 vb-per to be larger than h(cid:62)1 vb-role.
theseadversarial optimization goals eventually hurt per-formance and result in sub-optimal performance..1 vb-role > h3(cid:62).
level 3 and encouraging h4(cid:62).
therefore, the crux of the matter is to introducedifferent emission scores for different levels.
forexample, encouraging h3(cid:62)1 vb-per1 vb-per >ath4(cid:62)1 vb-role at level 4 will not lead to adversar-ial optimization directions anymore, where h31 andh41 are two distinctive hidden states to be used atlevels 3 and 4, respectively..t}m.to achieve this goal, we introduce a novel en-coder which outputs m hidden states {hll=1,where m is the number of levels, as an alternativeto the conventional encoder which can only outputa single hidden state ht ∈ rdh at each time step.
to make a distinction between our m hidden statesand the conventional single hidden state, we use theterm chunk from now on to refer to these hiddent ∈ rdh/m.
we restrict chunk dimension tostates hl.
be dh/m, so the total number of parameters remainunchanged..2.3 chunk selection.
as we mentioned above, our algorithm maintains achunk set for each time step, through selecting andremoving chunks, to exclude the inﬂuence of thebest path.
naturally, how to select chunk becomesthe next detail to be ﬁnalized..for clarity, we use notation hl.
t to denote thechunk set at level l, and use hl to refer to all ofthese chunk sets at level m across time steps, i.e.,{hlt=1.
because we remove one and only onechunk at each time step, |hlt| + l = m + 1 alwaysholds..t}n.an intuitive idea is to follow the original chunkorder and simply to select the l-th chunk for level l.at level l, no matter to which label, the emissionscore is calculated by using hlt. in this way, thisnaive potential function can be deﬁned as follow,.
φ (yl.
t−1, yl.
t, hl.
t) = ayl.
t−1,ylt.+ hl(cid:62).
t vyl.
t.(1).
t−1,yltt−1 to label yl.
where a ∈ r|y|×|y| is the transition matrix, y isthe label set, aylindicates the transition score∈ rdh/m is thefrom label ylembedding of label ylt. in this case, the l-th chunkhlt ∈ hlt is just the chunk which have an interactionwith target label, thus should be removed from hlt..t, and vyl.
t.hl+1.
t = hl.
t \ {hlt}.
(2).
one concern of the naive potential function isthat it implicitly assumes the outputs of the encoderare automatically arranged in the level order insteadof other particular syntactic or semantic order, e.g.,the encoder may encodes all loc related informa-tion at the ﬁrst hd/m dimensions while remaining.
3549formerhogwartsheadmasteralbusdumbledoreembembembembembbilstmos-orgs-roleb-pere-perob-rolee-roleoox lcrfcrfalgorithm 1: training.
:ﬁrst level chunk sets h1:target label sequences y1, · · · , ym.
inputinputoutput :negative log-likelihood ll ← 0for l = 1 to m do.
l ← l − log p (y l | hl)for t = 1 to n dot ← hl.
hl+1.
t \ {arg max.
h(cid:62)vyl.
t.}.
h∈hlt.end.
end.
algorithm 2: decoding.
:ﬁrst level chunk sets h1.
inputoutput :recognized entity set ee ← ∅for l = 1 to m doˆyl ← arg max.
p (y(cid:48) | hl).
y(cid:48)∈y n.for t = 1 to n dot ← hl.
hl+1.
t \ {arg max.
h(cid:62)v ˆyl.
t.}.
h∈hlt.ende ← e (cid:83) label-to-entity (ˆyl).
end.
org relevant information to the ﬁnal hd/m dimen-sion.
for instance, at level 3 time step 1, naive po-tential function forces h3(cid:62)1 vb-role > h3(cid:62)1 vb-per.
but if there exists another chunk, say h51, whichis more similar to vb-per, then directly selecting1 vb-role > h5(cid:62)1 and forcing h3(cid:62)h51 vb-per is morereasonable.
because it makes training harder thanthe former one, due to h5(cid:62)1 vb-per.
in other words, this selection strategy leads tohσ1(cid:62)t , wheretσl is the index of selected chunk at level l, but fornaive potential function, the inequation above doesnot always hold.
from this aspect, our method canalso be considered as selecting the best path in thesecond-best search space..1 vb-per > h3(cid:62).
> .
.
.
> hσm(cid:62).
> hσ2(cid:62)t.vym.
vy1.
vy2.
t.t.t.therefore,.
instead of following the originalchunk orders, we propose to let each label yj selectthe most similar chunk to it to obtain an emissionscore.
we denote this deﬁnition as max potentialfunction,.
φ (yl.
t−1, yl.
t, hl.
t) = ayl.
t−1,ylt.+ maxh∈hlt.h(cid:62)vyl.
t.(3).
in this case, we update chunk sets by removingthese chunks which are selected by the target labels..the chunk set is updated in the same way as equa-tion 4. we refer to this potential function deﬁnitionas logsumexp in the rest of this paper..2.4 embedding layer.
following previous work (shibuya and hovy,2020), we convert words to word embeddingswt ∈ rdw and employ a character-level bidirec-tional lstm to obtain character-based word em-beddings ct ∈ rdc.
the concatenation of them isfed into the encoding layer as the token representa-tion xt = [wt, ct] ∈ rdx..2.5 encoding layer.
we employ a three-layered bidirectional lstm toencode sentences and leverage contextual informa-tion,.
t=1).
{ht}n.t=1 = lstm ({xt}nwhere ht ∈ rdh is the hidden state.
in contrastto the encoders of previous work, which can onlyoutput single hidden states at each time step, wesplit ht into m chunks,.
(6).
[h1.
t , .
.
.
, hm.
t ] = ht.
(7).
hl+1.
t = hl.
t \ {arg max.
h(cid:62)vyl.
t.}.
(4).
h∈hlt.where hjchunk set, i.e., h1.
t ∈ rdh/m, and use them as the ﬁrst levelj=1, to start recognition..t = {hj.
t }m.furthermore, since the log-sum-exp operationis a well known differentiable approximation ofthe max operation, we also introduce it as the thirdpotential function,.
φ (yl.
t−1, yl.
t, hl.
t) = ayl.
+ log.
t−1,ylt.exp h(cid:62)vyl.
t.(cid:88).
h∈hlt.(5).
2.6 decoding layer.
t, hl.
t−1, yl.
at each level, we run a shared conventionalcrf with its corresponding potential functionφ (ylt) and update the chunk sets untilﬁnishing all m levels.
on the training stage, weremove chunks according to the selections of thetarget labels, while on the decoding stage, it de-pends on the selections of the predicted labels..35502.7 training and decoding.
3.2 hyper-parameters settings.
following the deﬁnition of crf, the conditionalprobabilistic function of a given label sequence att}nl-th level, i.e., yl = {yl.
t=1, can be deﬁned as,.
p (y l | hl) =.
φ (yl.
t−1, yl.
t, hlt).
1z(hl).
exp.
n(cid:88).
t=1.
z(hl) =.
(cid:88).
exp.
φ (y(cid:48)l.t−1, y(cid:48)l.t , hlt).
n(cid:88).
t=1.
y(cid:48)∈y n.(8).
(9).
where z(hl) is the sum of all paths’ scores and iscommonly known as the partition function..we optimize our model by minimizing the sum.
of the negative log-likelihoods of all levels..l = −.
log p (y l | hl).
(10).
m(cid:88).
l=1.
on the decoding stage, we iteratively apply theviterbi algorithm (forney, 1973) at each level tosearch the most probable label sequences..ˆyl = arg max.
p (y(cid:48) | hl).
(11).
y(cid:48)∈y n.the pseudocodes of the training and the decod-ing algorithms with max or logsumexp potentialfunction can be found in algorithms 1 and 2, re-spectively..3 experiments.
3.1 datasets.
we conduct experiments on three nested named en-tity recognition datasets in english, i.e., ace2004(doddington et al., 2004), ace2005 (walker et al.,2006) and genia (kim et al., 2003).
we divide allthese datasets into tran/dev/test split by followingshibuya and hovy (2020) and wang et al.
(2020).
the dataset statistics can be found in table 1..dataset.
ace2004ace2005genia.
sentences.
mentions.
|y| m.6,198 / 742 / 8097,285 / 968 / 1,05815,022 / 1,669 / 1,855.
22,195 / 2,514 / 3,03424,700 / 3,218 / 3,02947,006 / 4,461 / 5,596.
292921.
664.table 1: sizes of the dataset shown in the train/dev/testsplit.
|y| is the size of the label set, m is the maximaldepth of entity nesting..for word embeddings initialization, we utilize 100-dimensional pre-trained glove (pennington et al.,2014) for the ace2004 and the ace2005 datasets,and use 200-dimensional biomedical domain wordembeddings1 (chiu et al., 2016) for the geniadataset.
moreover, we randomly initialize 30-dimensional vectors for character embeddings.
thehidden state dimension of character-level lstmdc is 100, i.e., 50 in each direction, thus the di-mension of token representation dx is 200. weapply dropout (srivastava et al., 2014) on tokenrepresentations before feeding it into the encoder.
the hidden state dimension of the three-layeredlstm is 600 for ace2004 and ace2005, i.e., 300in each direction, and 400 for genia.
choosing adifferent dimension is because the maximal depthof entity nesting m is different.
we apply layernormalization (ba et al., 2016) and dropout with0.5 ratio after each bidirectional lstm layer..different from shibuya and hovy (2020), weuse only one crf instead of employing differentcrfs for different entity types.
besides, our crfis also shared across levels, which means we learnand decode entities at all levels with the same crf.
our model is optimized by using stochastic gra-dient descent (sgd), with a decaying learning rateητ = η0/(1 + γ · τ ), where τ is the index of thecurrent epoch.
for ace2004, ace2005, and ge-nia, the initial learning rates η0 are 0.2, 0.2, and0.1, and the decay rates γ are 0.01, 0.02, and 0.02respectively.
we set the weight decay rate, the mo-mentum, the batch size, and the number of epochsto be 10−8, 0.5, 32, and 100 respectively, espe-cially we use batch size 64 on the genia dataset.
we clip the gradient exceeding 5..besides, we also conduct experiments to evalu-ate the performance of our model with contextualword representations.
bert (devlin et al., 2019)and flair (akbik et al., 2018) are the most com-monly used contextual word representations in pre-vious work, and have also been proved that theycan substantially improve the model performance.
in these settings, contextual word representationsare concatenated with word and character repre-sentations to form the token representations, i.e.,xt = [wt, ct, et], where et is the contextual wordrepresentation and it is not ﬁne-tuned in any of ourexperiments..1https://github.com/cambridgeltl/.
bionlp-2016.
3551methods.
ju et al.
(2018)wang et al.
(2018)wang and lu (2018)luo and zhao (2020)lin et al.
(2019)strakov´a et al.
(2019)shibuya and hovy (2020)wang et al.
(2020)our method (naive)our method (max)our method (logsumexp).
strakov´a et al.
(2019) [b]shibuya and hovy (2020) [b]wang et al.
(2020) [b]our method (naive)[b]our method (max)[b]our method (logsumexp)[b].
strakov´a et al.
(2019) [b+f]84.51shibuya and hovy (2020) [b+f]85.94wang et al.
(2020) [b+f]87.01our method (naive)[b+f]86.56our method (max)[b+f]86.96our method (logsumexp)[b+f] 86.74.ace2004r.f1.
p.ace2005r.f1.
geniar.f1.
74.978.0.
71.872.4.
73.375.1.
78.9279.9380.8381.1281.9081.24.
84.7185.2386.0886.1986.2786.42.
75.3375.1078.8677.7178.0578.96.
83.9684.7286.4885.2885.0985.71.
84.2985.6986.5585.6585.4586.11.
77.0877.4479.8379.38 (0.31)79.92 (0.10)80.08 (0.22).
84.3384.9786.2885.73 (0.24)85.68 (0.09)86.06 (0.10).
84.4085.8286.7886.11 (0.24)86.19 (0.17)86.42 (0.31).
p.74.274.576.875.076.276.3578.2779.2779.4580.6879.49.
82.5883.3083.9584.2385.2883.95.
83.4883.8384.9084.1784.7084.81.
70.371.572.375.273.674.3975.4479.3777.2277.0377.65.
84.2984.6985.3984.1784.1584.67.
85.2184.8786.0884.8884.7685.06.
72.273.074.575.174.975.3676.8379.3278.32 (0.26)78.81 (0.04)78.55 (0.12).
83.4283.9984.6684.20 (0.30)84.71 (0.09)84.30 (0.13).
84.3384.3485.4984.52 (0.21)84.73 (0.21)84.93 (0.24).
p.78.578.077.077.475.879.6078.7077.9178.8378.8078.58.
79.9277.4679.4578.8379.2078.83.
80.1177.8179.9879.2879.5179.20.
71.370.273.374.673.973.5375.7477.2075.3275.7176.21.
76.5576.6578.9478.0778.1678.27.
76.6076.9478.5178.3178.2578.67.
74.773.975.176.074.876.4477.1977.5577.03 (0.13)77.22 (0.10)77.37 (0.15).
78.2077.0579.1978.45 (0.32)78.67 (0.18)78.54 (0.02).
78.3177.3679.2478.79 (0.17)78.87 (0.04)78.93 (0.26).
table 2: experimental results on the ace2004, ace2005 and genia datasets.
labels [b] and [f] stand forbert and flair contextual word representations respectively.
bold and underlined numbers indicates the best andthe second-best results respectively.
naive, max, and logsumexp refer to the three potential function deﬁnitions, i.e.,equations 1, 3, and 5, respectively.
these numbers in parentheses are standard deviations..bert is a transformer-based (vaswani et al.,2017) pre-trained contextual word represen-tation.
in our experiments, for the ace2004and ace2005 datasets we use the general do-main checkpoint bert-large-uncased,and for the genia dataset we use the biomed-ical domain checkpoint biobert largev1.1 2 (lee et al., 2019).
we average allbert subword embeddings in the last fourlayers to build 1024-dimensional vectors..flair is a character-level bilstm-based pre-trained contextual word representation.
weconcatenate these vectors obtained from thenews-forward and news-backwardcheckpoints for ace2004 and ace2005,the pubmed-forward andand usepubmed-backwardforgenia, to build 4096-dimensional vectors..checkpoints.
3.3 evaluation.
experiments are all evaluated by precision, recall,and f1.
all of our experiments were run 4 times.
2https://github.com/naver/.
biobert-pretrained.
with different random seeds and averaged scoresare reported in the following tables..our model 3 is implemented with pytorch(paszke et al., 2019) and we run experiments ongeforce gtx 1080ti with 11 gb memory..3.4 experimental results.
table 2 shows the performance of previous workand our model on the ace2004, ace2005, andgenia datasets.
our model substantially outper-forms most of the previous work, especially whencomparing with our baseline shibuya and hovy(2020).
when using only word embeddings andcharacter-based word embeddings our method ex-ceeds theirs by 2.64 f1 score, and also achievescomparable results with the recent competitivemethod (wang et al., 2020).
in the case of utiliz-ing bert and further employing flair, our methodconsistently outperforms shibuya and hovy (2020)by 1.09 and 0.60 by f1 scores, respectively..on the ace2005 dataset, our method improvesthe f1 scores by 1.98, 0.72, and 0.59 respectively,comparing with shibuya and hovy (2020).
al-though our model performance is inferior to wang.
3https://github.com/speedcell4/nersted.
3552et al.
(2020) at general, our max potential functionmethod is slightly superior to them by 0.05 in f1score when employing bert..furthermore, on the biomedical domain datasetgenia, our method constantly outperformsshibuya and hovy (2020) by 0.18, 1.62, and1.57 in f1 score, respectively.
although the lowscores of shibuya and hovy (2020) are due totheir usage of the general domain checkpointbert-large-uncased, instead of our biomed-ical domain checkpoint, our model is still superiorto strakov´a et al.
(2019) by 0.47 and 0.62 in f1scores, who used the same checkpoint as us..as for these three potential functions, we noticethe max and logsumexp potential functions gener-ally works better than the naive potential function.
these results demonstrate that the chunk selectionstrategy of the max and logsumexp can leverageinformation from all remaining chunks and con-strains hidden states of lstm to be more semanti-cally ordered.
when we use bert and flair, theadvantage of the max and the logsumexp potentialfunction is less obvious compared with the casewhen we only use word embeddings and character-based word embeddings, especially on the geniadataset.
we hypothesize that bert and flair canprovide rich contextual information, then select-ing chunks in the original order is sufﬁcient, thusour dynamic selecting mechanism can only slightlyimprove the model performance..3.5.inﬂuence of the encoding scheme.
we also conduct experiments on the ace2004dataset to measure the inﬂuence of the outermost-ﬁrst and innermost-ﬁrst encoding schemes.
asshown in table 3,the innermost-ﬁrst encod-ing scheme consistently works better than theoutermost-ﬁrst encoding scheme with all potentialfunctions.
we hypothesize that outermost entitiesdo not necessarily contain inner entities especiallyfor longer ones, and that putting those diversely.
encoding scheme.
φ.p.r.f1.
outermost first.
innermost first.
naivemaxlogsumexp.
naivemaxlogsumexp.
79.0879.0779.05.
81.1281.9081.24.
76.5775.1176.39.
77.7178.0578.96.
77.80 (0.26)77.04 (0.20)77.70 (0.32).
79.38 (0.31)79.92 (0.10)80.08 (0.22).
nested outermost entities at the same level woulddislocate the encoding representation.
furthermore,even if we use the outermost-ﬁrst encoding scheme,our method is superior to shibuya and hovy (2020),which further demonstrates the effectiveness of ex-cluding the inﬂuence of the best path..3.6 time complexity and speed.
the time complexity of encoder is o (n), and be-cause we employ the same tree reduction accelera-tion trick4 as rush (2020), the time complexity ofcrf is reduced to o (log n), therefore the overalltime complexity is o (n + m · log n)..even our model outperforms slightly worse thanwang et al.
(2020), the training and inference speedof our model is much faster than them, as shown intable 4, since we do not need to stack the decod-ing component to 16 layers.
especially, when weincrease the batch size to 64, the decoding speed ismore than two times faster than their model..method.
batch size.
training decoding.
wang et al.
(2020).
our method.
163264.
163264.
1,937.163,632.646,298.85.
3,626.534,652.055,113.85.
4,106.037,219.5710,584.80.
3,761.036,893.0311,652.92.table 4: speed comparison on the ace2005 dataset.
numbers indicate how many words can be processedper second on average..3.7 level-wise performance.
we display the performance on the datasetace2005 at each level, as in table 5. the maxpotential function at the ﬁrst three levels achievesconstantly higher precision scores than the naiveand logsumexp potential functions, while at thesame time obtains the lowest recall scores.
the log-sumexp potential function on the contrary achievesthe highest recall scores but fails to obtain satis-factory precision scores.
because most entities arelocated at the ﬁrst two levels, the max and logsum-exp achieves the best overall precision and recallscores, respectively..3.8 chunk distribution.
we analyze the chunk distribution on the test splitof the dataset ace2005 by plotting the heat maps.
table 3: inﬂuence of the two encoding schemes and thethree potential functions..4https://github.com/speedcell4/.
torchlatent.
3553figure 3: chunk distributions of the naive, max, and logsumexp potential functions, respectively.
each row displaysthe chunk selection preferences with respect to levels, syntactic and semantic labels, respectively..level.
naive.
max.
p.r.p.r.logsumexp.
p.r.123456.
80.8373.9160.09100.000.000.00.
80.1268.6748.8016.670.000.00.
82.1474.7665.2637.500.000.00.
79.5170.7649.1010.420.000.00.
80.9873.8560.1766.670.000.00.
80.1270.7653.0114.580.000.00.overall.
79.45.
77.22.
80.68.
77.03.
79.49.
77.65.table 5: precision and recall scores at each level witheach potential functions..in figure 3, in which these numbers indicate thepercentages of each chunk being selected by a par-ticular level or label.
for example, the 35 at theupper-right corner means when using logsumexppotential function, 35% of predictions at the ﬁrstlevel are made by choosing the sixth chunk, whilethe 78 at the lower-left corner shows 78% of weaare related to the ﬁrst chunk with naive.
to make iteasier to compare with the naive, we arranged thechunk orders of max and logsumexp, without los-ing generality, to make the level-chunk distributionmainly concentrate on the diagonal..the naive potential function simply selects thel-th chunk at l-th level, therefore the heat map is.
just diagonal.
at the ﬁrst level, the logsumexp po-tential function also prefers to select the sixth andthe fourth chunks rather than the ﬁrst chunk, wehypothesis this is due to most of b- and s- labelsare located on the ﬁrst level, and this can be con-ﬁrmed according to the syntactic-chunk heat mapof logsumexp where 78% b- and 70% s- labels goto the sixth and fourth chunks.
similarly, max alsohas a high probability to select the second chunk.
generally, the chunk distribution of logsumexpis more smooth than max.
besides, we ﬁnd label oalmost uniformly select chunks, in both the syntac-tic and semantic heat maps, while other meaningfullabels have their distinguished preferences..syntactic labels s- and b- mainly represent thebeginning of an entity, while i- and e- standsfor the continuation and ending of an entity.
inthe syntactic-chunk heat map of naive, they areindiscriminately distributed to the ﬁrst chunk, be-cause most of the entities are located on the ﬁrstlevel.
however, max and logsumexp utilize differ-ent chunks to represents these different syntacticcategories..likewise, the semantic label gpe, when usinglogsumexp, also has a 61% probability to select thesixth chunks other than concentrating on the ﬁrst.
3554      123456level100000000100000000100000000100000000100000000100            5920672742922151416322301962021727258211010121828222223164314            446493351630191513613243411154111720272148151426298889121943      osbiesyntactic131617181818841600008116300013622320081153000           14171718171784000161820001532281301542452371520           16171816181500070030000193783213734864017219220123456chunk(naive)ofacgpelocorgpervehweasemantic13161718181855242000088120000564400007522300059301010048371500078220000123456chunk(max)        141717181717431300133076620973920101426579209234717701216351600183171150914123456chunk(logsumexp)        16171816181521554452012501926121504752218413024524113254332070466211300211453chunk as naive.
these observations further demon-strate our dynamic chunk selection strategies arecapable of learning more meaningful representa-tions..4 related work.
existing ner algorithms commonly employ vari-ous neural networks to leverage more morpholog-ical and contextual information to improve per-formance.
for example, to handle the out-of-vocabulary issue through introducing morpholog-ical features, huang et al.
(2015) proposed to em-ploy manual spelling feature, while ma and hovy(2016) and lample et al.
(2016) suggested introduc-ing cnn and lstm to build word representationsfrom character-level.
zhang et al.
(2018) and chenet al.
(2019) introduced global representation toenhance encoder capability of encoding contextualinformation..layered model as a layered model, ju et al.
(2018) dynamically update span-level representa-tions for next layer recognition according to rec-ognized inner entities.
fisher and vlachos (2019)proposed a merge and label method to enhance thisidea further.
recently, shibuya and hovy (2020)designed a novel algorithm to efﬁciently learn anddecode the second-best path on the span of detectedentities.
luo and zhao (2020) build two differentgraphs, one is the original token sequence, and theother is the tokens in recognized entities, to modelthe interaction among them.
wang et al.
(2020) pro-posed to learn the l-gram representations at layer lthrough applying a decoder component to reducea sentence layer by layer and to directly classifythese l-gram spans..region-based model lin et al.
(2019) proposedan anchor-region network to recognize nested en-tities through detecting anchor words and entityboundaries ﬁrst, and then classify each detectedspan.
exhaustive models simply enumerate all pos-sible spans and utilize a maximum entropy tagger(byrne, 2007) and neural networks (xu et al., 2017;sohrab and miwa, 2018; zheng et al., 2019) forclassiﬁcation.
luan et al.
(2019) additionally aimsto consider the relationship among entities and pro-posed a novel method to jointly learn both entitiesand relations..hypergraph-based model lu and roth (2015)proposed a hyper-graph structure, in which edgesare connected to multiple nodes to represents.
nested entities.
muis and lu (2017) and wangand lu (2018) resolved spurious structures andambiguous issue of hyper-graph structure.
andkatiyar and cardie (2018) proposed another kindof hyper-graph structure..parsing-based model finkel and manning(2009) indicated all these nested entities are locatedin some non-terminal nodes of the constituencyparses of the original sentences, thus they proposedto use a crf-based constituency parser to obtainthem.
however, the cubic time complexity limitsits applicability.
wang et al.
(2018) instead pro-posed to use a transition-based constituency parserto incrementally build constituency forest, its lin-ear time complexity ensures it can handle longersentences..5 conclusion.
in this paper, we proposed a simple and effec-tive method for nested named entity recognitionby explicitly excluding the inﬂuence of the bestpath through selecting and removing chunks ateach level to build different potential functions.
we also proposed three different selection strate-gies to leverage information from all remainingchunks.
besides, we found the innermost-ﬁrst en-coding scheme works better than the conventionaloutermost-ﬁrst encoding scheme.
extensive exper-imental results demonstrate the effectiveness andefﬁciency of our method.
however, one of thedemerits of our method is the number of chunks,i.e., the maximal depth of entity nesting, must bechosen in advance as a hyper-parameter.
we willextend it to arbitrary depths as future work..acknowledgements.
this work was partly supported by jst crestgrant number jpmjcr1513.
the authors wouldlike to thank the anonymous reviewers for theirinstructive comments..references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649, santa fe, new mexico, usa.
associ-ation for computational linguistics..jimmy lei ba, jamie ryan kiros, and geoffrey e hin-arxiv preprint.
ton.
2016. layer normalization.
arxiv:1607.06450..3555k. byrne.
2007. nested named entity recognition inhistorical archive text.
in international conferenceon semantic computing (icsc 2007), pages 589–596..hui chen, zijia lin, guiguang ding, jianguang lou,yusen zhang, and borje karlsson.
2019. grn:gated relation network to enhance convolutionalneural network for named entity recognition.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 33, pages 6236–6243..billy chiu, gamal crichton, anna korhonen, andsampo pyysalo.
2016. how to train good word em-in proceedings ofbeddings for biomedical nlp.
the 15th workshop on biomedical natural languageprocessing, pages 166–174, berlin, germany.
asso-ciation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..george doddington, alexis mitchell, mark przybocki,lance ramshaw, stephanie strassel, and ralphweischedel.
2004. the automatic content extraction(ace) program – tasks, data, and evaluation.
inproceedings of the fourth international conferenceon language resources and evaluation (lrec’04),lisbon, portugal.
european language resources as-sociation (elra)..jenny rose finkel and christopher d. manning.
2009.nested named entity recognition.
in proceedings ofthe 2009 conference on empirical methods in nat-ural language processing, pages 141–150, singa-pore.
association for computational linguistics..joseph fisher and andreas vlachos.
2019. merge andlabel: a novel neural network architecture for nestedin proceedings of the 57th annual meet-ner.
ing of the association for computational linguis-tics, pages 5840–5850, florence, italy.
associationfor computational linguistics..g david forney.
1973. the viterbi algorithm.
proceed-.
ings of the ieee, 61(3):268–278..zhiheng huang, wei xu, and kai yu.
2015. bidi-rectional lstm-crf models for sequence tagging.
corr, abs/1508.01991..meizhi ju, makoto miwa, and sophia ananiadou.
2018. a neural layered model for nested named en-in proceedings of the 2018 con-tity recognition.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1446–1459, new orleans, louisiana.
associationfor computational linguistics..arzoo katiyar and claire cardie.
2018. nested namedin proceedings of theentity recognition revisited.
2018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers), pages 861–871, new orleans, louisiana.
as-sociation for computational linguistics..j.-d. kim, t. ohta, y. tateisi, and j. tsujii.
2003. ge-nia corpus—a semantically annotated corpus forbio-textmining.
bioinformatics, 19:i180–i182..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning, icml’01, page 282–289, san francisco, ca, usa.
mor-gan kaufmann publishers inc..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2019. biobert: a pre-trainedbiomedicalforbiomedical text mining.
corr, abs/1901.08746..language representation model.
hongyu lin, yaojie lu, xianpei han, and le sun.
2019. sequence-to-nuggets: nested entity mentionin proceed-detection via anchor-region networks.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 5182–5192,florence, italy.
association for computational lin-guistics..wei lu and dan roth.
2015..joint mention extrac-tion and classiﬁcation with mention hypergraphs.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages857–867, lisbon, portugal.
association for compu-tational linguistics..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-in proceedings of the 2019namic span graphs.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3036–3046, minneapolis, minnesota.
association for computational linguistics..ying luo and hai zhao.
2020. bipartite ﬂat-graph net-in pro-work for nested named entity recognition.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6408–6418, online.
association for computational lin-guistics..3556xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..jana strakov´a, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through lineariza-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 5326–5331, florence, italy.
association forcomputational linguistics..aldrian obaja muis and wei lu.
2017. labeling gapsbetween words: recognizing overlapping mentionswith mention separators.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, pages 2608–2618, copenhagen,denmark.
association for computational linguis-tics..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas kopf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019.py-torch: an imperative style, high-performance deeplearning library.
in h. wallach, h. larochelle,a. beygelzimer, f. d'alch´e-buc, e. fox, and r. gar-nett, editors, advances in neural information pro-cessing systems 32, pages 8026–8037.
curran asso-ciates, inc..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..lance ramshaw and mitch marcus.
1995. text chunk-in third.
ing using transformation-based learning.
workshop on very large corpora..alexander rush.
2020. torch-struct: deep structuredin proceedings of the 58th an-prediction library.
nual meeting of the association for computationallinguistics: system demonstrations, pages 335–342, online.
association for computational linguis-tics..takashi shibuya and eduard hovy.
2020. nestednamed entity recognition via second-best sequencelearning and decoding.
transactions of the associa-tion for computational linguistics, 8:605–620..mohammad golam sohrab and makoto miwa.
2018.deep exhaustive model for nested named entityrecognition.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 2843–2849, brussels, belgium.
associa-tion for computational linguistics..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
journal of machine learning re-search, 15(56):1929–1958..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allin i. guyon, u. v. luxburg, s. bengio,you need.
h. wallach, r. fergus, s. vishwanathan, and r. gar-nett, editors, advances in neural information pro-cessing systems 30, pages 5998–6008.
curran asso-ciates, inc..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2006. ace 2005 multilin-gual training corpus.
linguistic data consortium,philadelphia, 57:45..bailin wang and wei lu.
2018. neural segmental hy-pergraphs for overlapping mention recognition.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages204–214, brussels, belgium.
association for com-putational linguistics..bailin wang, wei lu, yu wang, and hongxia jin.
2018.a neural transition-based model for nested mentionrecognition.
in proceedings of the 2018 conferenceon empirical methods in natural language process-ing, pages 1011–1017, brussels, belgium.
associa-tion for computational linguistics..jue wang, lidan shou, ke chen, and gang chen.
2020.pyramid: a layered model for nested named en-tity recognition.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5918–5928, online.
association forcomputational linguistics..mingbin xu, hui jiang, and sedtawut watcharawit-tayakul.
2017. a local detection approach for namedin pro-entity recognition and mention detection.
ceedings of the 55th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1237–1247, vancouver, canada.
as-sociation for computational linguistics..yue zhang, qi liu, and linfeng song.
2018. sentence-state lstm for text representation.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 317–327, melbourne, australia.
associationfor computational linguistics..changmeng zheng, yi cai, jingyun xu, ho-fung le-ung, and guandong xu.
2019. a boundary-awareneural model for nested named entity recognition.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 357–366, hong kong, china.
association for computa-tional linguistics..3557