robust knowledge graph completion with stacked convolutions and astudent re-ranking network.
justin lovelace1 denis newman-grifﬁs2.
shikhar vashishth3∗.
jill fain lehman4 carolyn penstein ros´e11language technologies institute, carnegie mellon university, usa2department of biomedical informatics, university of pittsburgh, usa3microsoft research,4human-computer interaction institute, carnegie mellon university, usa{jlovelac, jfl, cpa3}@cs.cmu.edu, dnewmangriffis@pitt.edut-svashishth@microsoft.com.
abstract.
knowledge graph (kg) completion researchusually focuses on densely connected bench-mark datasets that are not representative ofreal kgs.
we curate two kg datasets thatinclude biomedical and encyclopedic knowl-edge and use an existing commonsense kgdataset to explore kg completion in the morerealistic setting where dense connectivity isnot guaranteed.
we develop a deep convolu-tional network that utilizes textual entity rep-resentations and demonstrate that our modeloutperforms recent kg completion methodsin this challenging setting.
we ﬁnd that ourmodel’s performance improvements stem pri-marily from its robustness to sparsity.
we thendistill the knowledge from the convolutionalnetwork into a student network that re-rankspromising candidate entities.
this re-rankingstage leads to further improvements in perfor-mance and demonstrates the effectiveness ofentity re-ranking for kg completion.1.
1.introduction.
knowledge graphs (kgs) have been shown to beuseful for a wide range of nlp tasks, such as ques-tion answering (bordes et al., 2014a,b), dialog sys-tems (ma et al., 2015), relation extraction (mintzet al., 2009; vashishth et al., 2018), and recom-mender systems (zhang et al., 2016).
however,because scaling the collection of facts to providecoverage for all the true relations that hold betweenentities is difﬁcult, most existing kgs are incom-plete (dong et al., 2014), limiting their utility fordownstream applications.
because of this problem,kg completion (kgc) has come to be a widelystudied task (yang et al., 2015; trouillon et al.,2016; shang et al., 2018; dettmers et al., 2018;.
∗ work performed while at carnegie mellon university.
1https://github.com/justinlovelace/.
robust-kg-completion.
sun et al., 2019; balazevic et al., 2019; malaviyaet al., 2020; vashishth et al., 2020a)..the increased interest in kgc has led to thecuration of a number of benchmark datasets such asfb15k (bordes et al., 2013), wn18 (bordes et al.,2013), fb15k-237 (toutanova and chen, 2015),and yago3-10 (rebele et al., 2016) that have beenthe focus of most of the work in this area.
however,these benchmark datasets are often curated in sucha way as to produce densely connected networksthat simplify the task and are not representativeof real kgs.
for instance, fb15k includes onlyentities with at least 100 links in freebase, whileyago3-10 is limited to only include entities inyago3 (rebele et al., 2016) that have at least 10relations..real kgs are not as uniformly dense as thesebenchmark datasets and have many sparsely con-nected entities (pujara et al., 2017).
this can posea challenge to typical kgc methods that learn en-tity representations solely from the knowledge thatalready exists in the graph..textual entity identiﬁers can be used to developentity embeddings that are more robust to sparsity(malaviya et al., 2020).
it has also been shownthat textual triplet representations can be used withbert for triplet classiﬁcation (yao et al., 2019).
such an approach can be extended to the morecommon ranking paradigm through the exhaustiveevaluation of candidate triples, but that does notscale to large kg datasets..in our work, we found that existing neural kgcmodels lack the complexity to effectively ﬁt thetraining data when used with the pre-trained tex-tual embeddings that are necessary for representingsparsely connected entities.
we develop an ex-pressive deep convolutional model that utilizes tex-tual entity representations more effectively and im-proves sparse kgc.
we also develop a student re-ranking model that is trained using knowledge dis-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1016–1029august1–6,2021.©2021associationforcomputationallinguistics1016tilled from our original ranking model and demon-strate that the re-ranking procedure is particularlyeffective for sparsely connected entities.
throughthese innovations, we develop a kgc pipeline thatis more robust to the realities of real kgs.
ourcontributions can be summarized as follows.
• we develop a deep convolutional architecture thatutilizes textual embeddings more effectively thanexisting neural kgc models and signiﬁcantlyimproves performance for sparse kgc..• we develop a re-ranking procedure that distillsknowledge from our ranking model into a stu-dent network that re-ranks promising candidateentities..• we curate two sparse kg datasets containingbiomedical and encyclopedic knowledge to studykgc in the setting where dense connectivity isnot guaranteed.
we release the encyclopedicdataset and the code to derive the biomedicaldataset to encourage future work..2 related work.
knowledge graph completion:kgc mod-els typically learn entity and relation embeddingsbased on known facts (nickel et al., 2011; bordeset al., 2013; yang et al., 2015) and use the learnedembeddings to score potential candidate triples.
re-cent work includes both non-neural (nickel et al.,2016; trouillon et al., 2016; liu et al., 2017; sunet al., 2019) and neural (socher et al., 2013; donget al., 2014; dettmers et al., 2018; vashishth et al.,2020b) approaches for embedding kgs.
however,most of them only demonstrate their efﬁcacy onartiﬁcially dense benchmark datasets.
pujara et al.
(2017) show that the performance of such methodsvaries drastically with sparse, unreliable data.
wecompare our proposed method against the existingapproaches in a realistic setting where the kg isnot uniformly dense..prior work has effectively utilized entity namesor descriptions to aid kgc (socher et al., 2013;ruobing xie, 2016; xiao et al., 2016).
in morerecent work, malaviya et al.
(2020) explore theproblem of kgc using commonsense kgs, whichare much sparser than standard benchmark datasets.
they adapt an existing kgc model to utilize bert(devlin et al., 2019) embeddings.
in this paper,we develop a deep convoluational architecture thatis more effective than adapting existing shallowmodels which we ﬁnd to be underpowerered forlarge kg datasets..yao et al.
(2019) developed a triplet classiﬁca-tion model by directly ﬁne-tuning bert with tex-tual entity representations and reported strong clas-siﬁcation results.
they also adapted their tripletclassiﬁcation model to the ranking paradigm byexhaustively evaluating all possible triples for agiven query, (e1, r, ?).
however, the ranking per-formance was not competitive2, and such an ap-proach is not scalable to large kg datasets likethose explored in this work.
exhaustively applyingbert to compute all rankings for the test set forour largest dataset would take over two months.
inour re-ranking setting, we reduce the number oftriples that need to be evaluated by over 7700×, re-ducing the evaluation time to less than 15 minutes.
bert as a knowledge base: recent work(petroni et al., 2019; jiang et al., 2020; rogerset al., 2020) has utilized the masked-language-modeling (mlm) objective to probe the knowl-edge contained within pre-trained models usingﬁll-in-the-blank prompts (e.g.
“dante was bornin [mask]”).
this body of work has found thatpre-trained language models such as bert capturesome of the relational knowledge contained withintheir pre-training corpora.
this motivates us to uti-lize these models to develop entity representationsthat are well-suited for kgc..re-ranking: wang et al.
(2011) introduced cas-cade re-ranking for document retrieval.
this ap-proach applies inexpensive models to develop aninitial ranking and utilizes expensive models toimprove the ranking of the top-k candidates.
re-ranking has since been successfully applied acrossmany retrieval tasks (matsubara et al., 2020; peiet al., 2019; nogueira and cho, 2019).
despitere-ranking’s widespread success, recent kgc workutilizes a single ranking model.
we develop anentity re-ranking procedure and demonstrate theeffectiveness of the re-ranking paradigm for kgc.
knowledge distillation: knowledge distilla-tion is a popular technique that is often used formodel compression where a large, high-capacityteacher is used to train a simpler student network(hinton et al., 2015).
however, knowledge distilla-tion has since been shown to be useful for improv-ing model performance beyond the original settingof model compression.
li et al.
(2017) demon-strated that knowledge distillation improved imageclassiﬁcation performance in a setting with noisy.
2their reported hits@10 for fb15k-237 was .420 which.
is lower than all of the models evaluated in this work..1017dataset.
# nodes.
# rels.
# train.
# valid.
# test.
fb15k-237snomed ct corecn-100kfb15k-237-sparse.
14,45177,31678,08814,451.
23714034237.
272,115502,224100,00018,506.
17,53571,7781,20017,535.
20,466143,4861,20020,466.table 1: dataset statistics.
figure 1: in-degrees of entities in the training kgs (in-cluding inverse relations).
labels.
the incompleteness of kgs leads to noisytraining labels which motivates us to use knowl-edge distillation to train a student re-ranking modelthat is more robust to the label noise..3 datasets.
we examine kgc in the realistic setting where kgshave many sparsely connected entities.
we utilize acommonsense kg dataset that has been used in pastwork and curate two additional sparse kg datasetscontaining biomedical and encyclopedic knowl-edge.
we release the encyclopedic dataset andthe code to derive the biomedical dataset to encour-age future work in this challenging setting.
thesummary statistics for all datasets are presented intable 1 and we visualize the connectivity of thedatasets in figure 1..3.1 snomed ct core.
for constructing snomed ct core, we use theknowledge graph deﬁned by snomed ct (don-nelly, 2006), which is contained within the uniﬁedmedical language system (umls) (bodenreider,2004).
snomed ct is well-maintained and isone of the most comprehensive knowledge basescontained within the umls (jim´enez-ruiz et al.,2011; jiang and chute, 2009).
we ﬁrst extract theumls3 concepts found in the core problem listsubset of the snomed ct knowledge base.
thissubset is intended to contain the concepts mostuseful for documenting clinical information.
we.
3we work with the 2020aa release of the umls..then expand the graph to include all concepts thatare directly linked to those in the core problemlist subset according to the relations deﬁned bythe snomed ct kg.
our ﬁnal kg consists ofthis set of concepts and the snomed ct relationsconnecting them.
importantly, we do not ﬁlter outrare entities from the kg, as is commonly doneduring the curation of benchmark datasets..to avoid leaking data from inverse, or otherwiseinformative, relations, we divide the facts into train-ing, validation, and testing sets based on unorderedtuples of entities {e1, e2} so that all relations be-tween any two entities are conﬁned to a single split.
unlike some other kg datasets that ﬁlter out in-verse relations, we divide our dataset in such a waythat this is not necessary; our dataset already in-cludes inverse relations, and they do not need tobe manually added for training and evaluation as isstandard practice (dettmers et al., 2018; malaviyaet al., 2020)..because we represent entities using textual de-scriptions in this work, we also mine the enti-ties’ preferred concept names (e.g.
“traumatichematoma of left kidney”) from the umls..3.2 fb15k-237-sparse.
the fb15k-237 (toutanova and chen, 2015)dataset contains encyclopedic knowledge about theworld, e.g.
(barack obama, placeofbirth, hon-olulu).
although the dataset is very densely con-nected, that density is artiﬁcial.
fb15k (bordeset al., 2013), the precursor to fb15k-237, was cu-rated to only include entities with at least 100 linksin freebase (bollacker et al., 2008)..the dense connectivity of fb15k-237 does al-low us to to ablate the effect of this density.
weutilize the fb15k-237 dataset and also develop anew dataset, denoted fb15k-237-sparse, by ran-domly downsampling the facts in the training set offb15k-237 to match the average in-degree of theconceptnet-100k dataset.
we use this to directlyevaluate the effect of increased sparsity..for the fb15k-237 dataset, we use the textualidentiﬁers released by ruobing xie (2016).
theyreleased both entity names (e.g.
“jason frederickkidd”) as well as brief textual descriptions (e.g.
“jason frederick kidd is a retired american profes-sional basketball player.
.
.
”) for most entities.
weutilize the textual descriptions when available..1018figure 2: we utilize bert to precompute entity embeddings.
we then stack the precomputed entity embeddingwith a learned relation embedding and project them to a two-dimensional spatial feature map, upon which weapply a sequence of two-dimensional convolutions.
the ﬁnal feature map is then average pooled and projected toa query vector, which is used to rank candidate entities.
we extract promising candidates and train a re-rankingmodel utilizing knowledge distilled from the original ranking model.
the ﬁnal candidate ranking is generated byensembling the ranking and re-ranking models..3.3 conceptnet-100k.
conceptnet (speer and havasi, 2013) is a kg thatcontains commonsense knowledge about the worldsuch as the fact (go to dentist, motivatedby, pre-vent tooth decay).
we utilize conceptnet-100k(cn-100k) (li et al., 2016) which consists of theopen mind common sense entries in the con-ceptnet dataset.
this kg is much sparser thanbenchmark datasets like fb15k-237, which makesit well-suited for our purpose.
we use the train-ing, validation, and testing splits of malaviya et al.
(2020) to allow for direct comparison.
we also usethe textual descriptions released by malaviya et al.
(2020) to represent the kg entities..4 methods.
we provide an overview of our model architecturein figure 2. we ﬁrst extract feature representationsfrom bert (devlin et al., 2019) to develop textualentity embeddings.
motivated by our observationthat existing neural kg architectures are under-powered in our setting, we develop a deep con-volutional network utilizing architectural innova-tions from deep convolutional vision models.
ourmodel’s design improves its ability to ﬁt complexrelationships in the training data which leads todownstream performance improvements..finally, we distill our ranking model’s knowl-edge into a student re-ranking network that adjuststhe rankings of promising candidates.
in doing so,we demonstrate the effectiveness of the re-ranking.
paradigm for kgc and develop a kgc pipelinewith greater robustness to the sparsity of real kgs..4.1 entity ranking.
we follow the standard formulation for kgc.
we represent a kg as a set of entity-relation-entity facts (e1, r, e2).
given an incomplete fact,(e1, r, ?
), our model computes a score for all candi-date entities ei that exist in the graph.
an effectivekgc model should assign greater scores to correctentities than incorrect ones.
we follow recent work(dettmers et al., 2018; malaviya et al., 2020) andconsider both forward and inverse relations (e.g.
treats and treated by) in this work.
for the datasetsthat do not already include inverse relations, weintroduce an inverse fact, (e2, r−1, e1), for everyfact, (e1, r, e2), in the dataset..4.1.1 textual entity representationswe utilize bert (devlin et al., 2019) to developentity embeddings that are invariant to the connec-tivity of the kg.
we follow the work of malaviyaet al.
(2020) and adapt bert to each kg’s namingstyle by ﬁne-tuning bert using the mlm objec-tive with the set of entity identiﬁers in the kg..for cn-100k and fb15k-237, we utilize thebert-base uncased model.
for snomed ctcore kg, we utilize pubmedbert (gu et al.,2020) which is better suited for the biomedicalterminology in the umls..we apply bert to the textual entity identiﬁersand mean-pool across the token representations.
1019heart attack lung infectiontuberculosis......entity names...entity embeddings relation embeddingsprecompute1x1cnn3x3cnn1x1cnnbottleneck convolutions xntreated_bytuberculosiscandidate rankingisoniazidethambutolrifampim...pooling + projection1d cnnx  êre...knowledge distillationrifampimethambutolisoniazid...[cls] [treated_by] turberculosis[sep] [treated_by] isoniazid [sep]ensembletop-k candidatesinitial rankingfinal rankingbert bert from all bert layers to obtain a summary featurevector for the concept name.
we ﬁx these embed-dings during training because we must computescores for a large number of potential candidateentities for each training example.
this makes ﬁne-tuning bert prohibitively expensive..4.1.2 deep convolutional architecture.
inspired by the success of deep convolutional mod-els in computer vision (krizhevsky et al., 2012;simonyan and zisserman, 2015; he et al., 2016;huang et al., 2019, 2017), we develop a knowl-edge base completion model based on the seminalresnet architecture (he et al., 2016) that is sufﬁ-ciently expressive to model complex interactionsbetween the bert feature space and the relationembeddings..given an incomplete triple (ei, rj, ?
), we be-gin by stacking the precomputed entity embeddinge ∈ r1×d with the learned relation embedding ofthe same dimension r ∈ r1×d to produce a featurevector of length d with two channels q ∈ r2×d.
wethen apply a one-dimensional convolution with akernel of width 1 along the length of the feature vec-tor to project each position i to a two-dimensionalspatial feature map xi ∈ rf ×f where the con-volution has f × f ﬁlters.
thus the convolutionproduces a two-dimensional spatial feature mapx ∈ rf ×f ×d with d channels, representing theincomplete query triple (ei, rj, ?)..
the spatial feature map, x ∈ rf ×f ×d, is anal-ogous to a square image with a side length of fand d channels, allowing for the straightforwardapplication of deep convolutional models such asresnet.
we apply a sequence of 3n bottleneckblocks to the spatial feature map where n is a hy-perparameter that controls the depth of the network.
a bottleneck block consists of three consecutiveconvolutions: a 1 × 1 convolution, a 3 × 3 convo-lution, and then another 1 × 1 convolution.
theﬁrst 1 × 1 convolution reduces the feature map di-mensionality by a factor of 4 and then the second1 × 1 convolution restores the feature map dimen-sionality.
this design reduces the dimensionalityof the expensive 3 × 3 convolutions and allows usto increase the depth of our model without dramat-ically increasing its parameterization.
we doublethe feature dimensionality of the bottleneck blocksafter n and 2n blocks so the dimensionality ofthe ﬁnal feature map produced by the sequence ofconvolutions is 4d..we add residual connections to each bottleneck.
block which improves training for deep networks(he et al., 2016).
if we let f(x) represent theapplication of the bottleneck convolutions, then theoutput of the bottleneck block is y = f(x) + x.we apply batch normalization followed by a relunonlinearity (nair and hinton, 2010) before eachconvolutional layer (he et al., 2016) ..we utilize circular padding (wang et al., 2018;vashishth et al., 2020a) with the 3 × 3 convolutionsto maintain the spatial size of the feature map anduse a stride of 1 for all convolutions.
for the bottle-neck blocks that double the dimensionality of thefeature map, we utilize a projection shortcut for theresidual connection (he et al., 2016)..4.1.3 entity scoringgiven an incomplete fact (ei, rj, ?
), our convolu-tional architecture produces a feature map ˆx ∈rf ×f ×4d.
we average pool this feature representa-tion over the spatial dimension which produces asummary feature vector ˆx ∈ r4d.
we then apply afully connected layer followed by a prelu nonlin-earity (he et al., 2015) to project the feature vectorback to the original embedding dimensionality d.we denote this ﬁnal vector ˆe and compute scoresfor candidate entities using the dot product withcandidate entity embeddings.
the scores can beefﬁciently computed for all entities simultaneouslyusing a matrix-vector product with the embeddingmatrix y = ˆeet where e ∈ rm×d stores theembeddings for all m entities in the kg..4.1.4 training.
adopting the terminology used by rufﬁnelli et al.
(2020), we utilize a 1vsall training strategy withthe binary cross-entropy loss function.
we treatevery fact in our dataset, (ei, rj, ek), as a trainingsample where (ei, rj, ?)
is the input to the model.
we compute scores for all entities as described pre-viously and apply a sigmoid operator to induce aprobability for each entity.
we treat all entitiesother than ek as negative candidates and then com-pute the binary cross-entropy loss..we train our model using the adam optimizer(kingma and ba, 2015) with decoupled weight de-cay regularization (loshchilov and hutter, 2019)and label smoothing.
we train our models fora maximum of 200 epochs and terminate train-ing early if the validation mean reciprocal rank(mrr) has not improved for 20 epochs.
we trainedall of the models used in this work using a singlenvidia geforce gtx 1080 ti..10204.2 entity re-ranking.
4.2.1 re-ranking network.
we use our convolutional network to extract thetop-k entities for every unique training query andthen train a re-ranking network to rank these enti-ties.
we design our student re-ranking network as atriplet classiﬁcation model that utilizes the full can-didate fact, (ei, rj, ek), instead of an incompletefact, (ei, rj, ?).
this allows the network to modelinteractions between all elements of the triple.
there-ranking setting also enables us to directly ﬁne-tune bert which often improves performance (pe-ters et al., 2019)..we introduce relation tokens4 for each re-lation in the knowledge graph and constructthe textual input by prepending the head andtail entities with the relation token and thenconcatenating the two sequences.
thus thetriple (“head name”, ri, “tail name”) would berepresented as “[cls] [rel i] head name[sep] [rel i] tail name [sep]”.
weuse a learned linear combination of the [cls]embedding from each layer as the ﬁnal feature rep-resentation for the prediction..4.2.2 knowledge distillation.
a sufﬁciently performant ranking model can pro-vide an informative prior that can be used tosmooth the noisy training labels and improve ourre-ranking model.
for each training query i, wenormalize the logits produced by our teacher rank-ing model, ft (xi), for the k candidate triples,ft (xi)0:k, as.
sik:(i+1)k = softmax(ft (xi)0:k/t ).
where t is the temperature (hinton et al., 2015)..our training objective for our student model,fs(xi), is a weighted average of the binary crossentropy loss, lbce, using the teacher’s normalizedlogits, s, and the noisy training labels, y..lkd(yi, xi) = λlbce(si, fs(xi)).
+ (λ − 1)lbce(yi, fs(xi))= lbce((λ − 1)yi + λsi, fs(xi)).
4we use relation tokens instead of free-text relation repre-sentations because the relation identiﬁers for our datasets arenot all well-formed using natural language, and the differentstyles would introduce a confounding factor that would com-plicate our evaluation.
utilizing appropriate free-text relationidentiﬁers may improve performance, but we leave that tofuture work..we select λ ∈ {.25, .5, .75, 1} to optimize the bal-ance between the two objectives using validationperformance..4.2.3 trainingfor our experiments, we extract the top k = 10candidates produced by our ranking model for ev-ery query in the training set.
we train our studentnetwork using the adam optimizer (kingma andba, 2015) with decoupled weight decay regulariza-tion (loshchilov and hutter, 2019).
we ﬁne-tunebert for a maximum of 10 epochs and terminatetraining early if the mean reciprocal rank (mrr)on validation data has not improved for 3 epochs..4.2.4 student-teacher ensemblefor every query, we apply our re-ranking networkto the top k = 10 triples and compute the ﬁnal rank-ing using an ensemble of the teacher and studentnetworks.
the ﬁnal ranking are computed with.
ˆsik:(i+1)k = α(softmax(fs(xik:(i+1)k))).
+ (1 − α)(softmax(ft (xi)0:k))).
where 0 ≤ α ≤ 1 controls the impact of the studentre-ranker.
the cost of computing ˆsik:(i+1)k is negli-gible, so we sweep over [0, 1] in increments of .01and select the α that achieves the best validationmrr..5 experiments.
5.1 baselines.
we utilize the same representative selection of kgmodels from malaviya et al.
(2020) as baselines:distmult (yang et al., 2015), complex (trouillonet al., 2016) conve (dettmers et al., 2018), andconvtranse (shang et al., 2018).
this is not anexhaustive selection of all recent kg methods, buta recent replication study by rufﬁnelli et al.
(2020)found that the baselines that we use are competitivewith the state-of-the-art and often outperform morerecent models when trained appropriately..we develop additional baselines by adaptingthe shallow convolutional kgc models to usebert embeddings to evaluate the beneﬁts of uti-lizing our proposed convolutional architecture in-stead of simply repurposing existing kgc mod-els.
we refer to these models as bert-conveand bert-convtranse.
malaviya et al.
(2020)used bert embeddings in conjunction with con-vtranse for commonsense kgc, but their modelwas prohibitively large to reproduce.
we refer to.
1021distmult [♣]complex [♣]conve [♣]convtranse [♣].
bert-convebert-convtransebert-large-convtranse [♣].
bert-deepconv.
bert-resnet.
+ re-ranking.
+ knowledge distillation (kd)+ ranking ensemble (re)+ kd and re.
distmult [♠]complex [♠]conve [♠]convtranse [♦].
bert-convebert-convtranse.
bert-deepconv.
bert-resnet.
+ re-ranking.
+ knowledge distillation (kd)+ ranking ensemble (re)+ kd and re.
snomed ct core.
cn-100k.
mr mrr.
h@1 h@3 h@10 mr mrr.
h@1 h@3 h@10.
5146390337393585.
414514−.
265.
265265265264264.
−−−−.
193211.
190.
186187187186186.
.293.302.271.290.
.383.373−.
.479.492∗.562†.566†.576†.577†.
.343.348.339.33.
.305.296.
.327.346∗.304.310.354†.353†.
.226.224.191.213.
.277.273−.
.374.
.389.482.487.503.501.
−−−.24.
.224.218.
.246.
.262.212.220.270.269.
.318.332.303.321.
.430.417−.
.532.
.544.608.614.619.623.
−−−.37.
.330.321.
.354.
.379.329.334.387.386.
.426.456.429.442.
.591.568−.
.685.
.691.691.691.691.691.
.531.536.521.51.
.465.449.
.488.
.514.514.514.514.514.
−−−−.
260276−.
161.
169170169169169.
3061333322632285.
408390.
422.
413413413413413.
.090.114.209.187.
.453.458.523.
.540.550∗.377.528.555.569†.
.136.132.156.153.
.190.188.
.188.191∗.190.197†.199†.198†.
.045.074.140.079.
.332.340.410.
.418.
.426.216.402.438.452.
.092.091.106.103.
.128.127.
.127.
.128.128.135.137.136.
.098.125.229.239.
.521.520.585.
.610.
.628.437.603.623.647.
.146.143.165.161.
.200.199.
.197.
.201.200.209.210.211.fb15k-237.
fb15k-237-sparse.
mr mrr.
h@1 h@3 h@10 mr mrr.
h@1 h@3 h@10.
.174.190.340.390.
.691.675.735.
.772.
.769.769.769.769.769.
.223.216.258.255.
.315.310.
.314.
.317.317.317.317.317.table 2: comparison of kgc results across all datasets.
we indicate statistical signiﬁcance for: (1) improvementsof deep convolutional bert models over both shallow convolutional bert models with an underline (p < 0.005);(2) improvements of bert-resnet over bert-deepconv with a ∗ (p < 0.05); (3) improvements of the re-rankingconﬁgurations over the original rankings with a † (p < 0.005).
[♣] indicates that cn-100k results are frommalaviya et al.
(2020).
[♠] indicates that fb15k-237 results are from rufﬁnelli et al.
(2020).
[♦] indicates thatfb15k-237 results are from shang et al.
(2018).
dashes indicate that the metric was not reported by the prior work..their model as bert-large-convtranse and com-pare directly against their reported results..we also develop a deep convolutional baseline,termed bert-deepconv, to evaluate the effect ofthe architectural innovations used in our model.
bert-deepconv transforms the input embeddingsto a spatial feature map like our proposed model,but it then applies a stack of 3 × 3 convolutionsinstead of a sequence of bottleneck blocks withresidual connections.
we select hyperparameters(detailed in the appendix) for all of our bertbaselines so that they have a comparable numberof trainable parameters to our proposed model.
wediscuss the size of these models in detail in in sec-tion 6.4..to evaluate the impact of our re-ranking stage,we ablate the use of knowledge distillation and en-sembling.
thus we conduct experiments where our.
re-ranker uses only knowledge distillation, usesonly ensembling, and uses neither.
this means thatin the most naive setting, we train the re-rankerusing the hard training labels and re-rank the can-didates using only the re-ranker..5.2 evaluation.
we report standard ranking metrics: mean rank(mr), mean reciprocal rank (mrr), hits at 1(h@1), hits at 3 (h@3), and hits at 10 (h@10).
we follow past work and use the ﬁltered setting(bordes et al., 2013), removing all positive entitiesother than the target entity before calculating thetarget entity’s rank..we utilize paired bootstrap signiﬁcance testing(berg-kirkpatrick et al., 2012) with the mrr to val-idate the statistical signiﬁcance of improvements.
to account for the large number of comparisons.
1022being performed, we apply the holm–bonferronimethod (holm, 1979) to correct for multiple hy-pothesis testing.
we deﬁne families for the threeprimary hypotheses that we tested with our exper-iments.
they are as follows: (1) the deep con-volutional bert models outperform the shallowconvolutional bert models.
(2) bert-resnetimproves upon our bert-deepconv baseline.
(3)the re-ranking procedure improves the originalrankings..this selection has the beneﬁt of allowing for amore granular analysis of each conclusion whilesigniﬁcantly reducing the number of hypotheses.
the ﬁrst family includes all pairwise comparisonsbetween the two deep convolutional models andthe two shallow convolutional models.
the secondfamily involves all comparisons between bert-resnet and bert-deepconv.
the third familyincludes comparisons between all re-ranking con-ﬁgurations and the original rankings.
we note thatthe p-value for each family bounds the strict condi-tion that we report any spurious ﬁnding within thefamily..6 results and discussion.
6.1 ranking performance.
we report results across all of our datasets in table2. our ranking model, bert-resnet, outperformsthe previously published models and our baselinesacross all of the sparse datasets.
we ﬁnd that for allsparse datasets, the models that use free text entityrepresentations outperform the models that learnthe entity embeddings during training.
amongthe models utilizing textual information, the deepconvolutional methods generally outperform theadaptations of existing neural kg models.
bert-resnet outperforms bert-deepconv across alldatasets, demonstrating that the architectural inno-vations do improve downstream performance..on the full fb15k-237 dataset, our proposedmodel is able to achieve competitive results com-pared to strong baselines.
however, the focus ofthis work is not to achieve state-of-the-art perfor-mance on densely connected benchmark datasetssuch as fb15k-237.
these results do, however,allow us to observe the outsized impact of sparsityon models that do not utilize textual information..6.2 re-ranking performance.
re-ranking entities without knowledge distillationor ensembling leads to poor results, degrading the.
mrr across most datasets.
we note that the per-formance of our re-ranking model could be limitedby our use of a pointwise loss function.
further ex-ploration of pairwise or listwise learning learning-to-rank methods is a promising direction for futureexploration that could lead to further improvementsguo et al.
(2020)..the inclusion of either knowledge distillationor ensembling improves performance.
ensemblingis particularly important, achieving a statisticallysigniﬁcant improvement over the initial rankingsacross most datasets.
our ﬁnal setting using bothknowledge distillation and ensembling is the onlysetting to achieve a statistically signiﬁcant improve-ment across all four datasets, although using bothdoes not consistently improve performance overensembling alone..a plausible explanation for this is that knowl-edge distillation improves performance by reduc-ing the divergence between the re-ranker and theteacher, but ensembling can already achieve a sim-ilar effect by simply increasing the weight of theteacher in the ﬁnal prediction.
we observe thatthe weight of the teacher is reduced across all fourdatasets when knowledge distillation is used whichwould be consistent with this explanation.
knowl-edge distillation has also been shown to be use-ful in situations with noisy labels (li et al., 2017)which may explain why it was particularly effectivefor our sparsest dataset, cn-100k, where trainingwith the hard labels led to particularly poor perfor-mance..6.3 effect of re-ranking.
we bin test examples by the in-degree of the tailnodes and compute the mrr within these binsfor our model before and after re-ranking.
wereport this breakdown for the snomed ct coredataset in figure 3. our re-ranking stage improvesperformance uniformly across all levels of sparsity,but it is particularly useful for entities that are rarelyseen during training.
this is also consistent withthe comparatively smaller topline improvement forthe densely connected fb15k-237 dataset..6.4 model capacity.
we report the number of trainable parameters forthe models that use textual representations alongwith the train and test set mrr for snomed ctcore in table 3. we observe a monotonic rela-tionship between training and testing performanceand note that the shallow models fail to achieve.
1023the generalizability of our improvements acrossbiomedical, commonsense, and encyclopedic kgs..acknowledgments.
this work was supported by the national sciencefoundation grant iis 1917955 and the national li-brary medicine of the national institutes of healthunder award number t15 lm007059..ivana balazevic, carl allen, and timothy hospedales.
2019. tucker: tensor factorization for knowledgegraph completion.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 5185–5194, hong kong, china.
as-sociation for computational linguistics..taylor berg-kirkpatrick, david burkett, and danklein.
2012. an empirical investigation of statis-in proceedings of thetical signiﬁcance in nlp.
2012 joint conference on empirical methods in nat-ural language processing and computational nat-ural language learning, pages 995–1005, jeju is-land, korea.
association for computational linguis-tics..olivier bodenreider.
2004. the uniﬁed medical lan-guage system (umls): integrating biomedical ter-minology.
nucleic acids research, 32:d267–d270..kurt bollacker, colin evans, praveen paritosh, timsturge, and jamie taylor.
2008. freebase: a collab-oratively created graph database for structuring hu-man knowledge.
in proceedings of the 2008 acmsigmod international conference on managementof data, sigmod ’08, page 1247–1250, new york,ny, usa.
association for computing machinery..antoine bordes, sumit chopra, and jason weston.
2014a.
question answering with subgraph embed-in proceedings of the 2014 conference ondings.
empirical methods in natural language processing(emnlp), pages 615–620.
association for compu-tational linguistics..antoine bordes, nicolas usunier, alberto garcia-jason weston, and oksana yakhnenko.
duran,2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems, pages 2787–2795..antoine bordes, jason weston, and nicolas usunier.
2014b.
open question answering with weakly super-vised embedding models.
in machine learning andknowledge discovery in databases, pages 165–180,berlin, heidelberg.
springer berlin heidelberg..tim dettmers, minervini pasquale, stenetorp pon-tus, and sebastian riedel.
2018. convolutional 2d.
figure 3: effect of re-ranking on performance forsnomed ct core across varying levels of sparsity..references.
model.
trainableparams.
snomed ct coretrain/test mrr.
bert-convebert-convtransebert-deepconvbert-resnet.
34m37m38m33m.
.460 / .383.449 /.373.696 /.479.715 / .492.table 3: comparison of trainable parameters for kgcmodels that utilize textual entity representations..our model’s test performance on the training set.
this demonstrates that the shallow models lack thecomplexity to adequately ﬁt the training data.
asimilar trend held for all datasets except for fb15k-237-sparse whose smaller size reduces the risk ofunderﬁtting.
this explains the smaller performanceimprovement for that dataset..malaviya et al.
(2020) scaled up bert-large-convtranse to use over 524m trainable parame-ters, and their model did outperform our smallerbert-convtranse baseline.
however, their modelstill fails to match the performance of either of ourdeep convolutional models despite using over 15×the number of trainable parameters..7 conclusion.
kgs often include many sparsely connected en-tities where the use of textual entity embeddingsis necessary for strong performance.
we developa deep convolutional network that is better-suitedfor this setting than existing neural models devel-oped on artiﬁcially dense benchmark kgs.
wealso introduce a re-ranking procedure to distill theknowledge from our convolutional model into astudent re-ranking network and demonstrate thatour procedure is particularly effective at improvingthe ranking of sparse candidates.
we utilize theseinnovations to develop a kgc pipeline with greaterrobustness to the realities of kgs and demonstrate.
1024in proceedings ofknowledge graph embeddings.
the 32th aaai conference on artiﬁcial intelligence,pages 1811–1818..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..xin dong, evgeniy gabrilovich, geremy heitz, wilkohorn, ni lao, kevin murphy, thomas strohmann,shaohua sun, and wei zhang.
2014. knowledgevault: a web-scale approach to probabilistic knowl-in proceedings of the 20th acmedge fusion.
sigkdd international conference on knowledgediscovery and data mining, kdd ’14, pages 601–610, new york, ny, usa.
acm..kevin donnelly.
2006. snomed-ct: the advancedterminology and coding system for ehealth.
studiesin health technology and informatics, 121:279..yu gu, robert tinn, hao cheng, michael lu-cas, naoto usuyama, xiaodong liu, tristan nau-mann, jianfeng gao, and hoifung poon.
2020.domain-speciﬁc language model pretraining forarxiv,biomedical natural language processing.
abs/2007.15779..jiafeng guo, yixing fan, liang pang, liu yang,qingyao ai, hamed zamani, chen wu, w. brucecroft, and xueqi cheng.
2020. a deep look intoneural ranking models for information retrieval.
in-formation processing management, 57(6):102067..k. he, x. zhang, s. ren, and j. sun.
2015. delvingdeep into rectiﬁers: surpassing human-level perfor-mance on imagenet classiﬁcation.
in 2015 ieee in-ternational conference on computer vision (iccv),pages 1026–1034..k. he, x. zhang, s. ren, and j. sun.
2016. deep resid-in 2016 ieeeual learning for image recognition.
conference on computer vision and pattern recog-nition (cvpr), pages 770–778..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. identity mappings in deep residual net-works.
2016 european conference on computer vi-sion (eccv)..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..gao huang, zhuang liu, laurens van der maaten, andkilian q weinberger.
2017. densely connected con-in proceedings of the ieeevolutional networks.
conference on computer vision and pattern recog-nition..gao huang, zhuang liu, geoff pleiss, laurens vander maaten, and kilian weinberger.
2019. con-volutional networks with dense connectivity.
ieeetransactions on pattern analysis and machine intel-ligence..guoqian jiang and christopher g. chute.
2009. audit-ing the semantic completeness of snomed ct us-ing formal concept analysis.
journal of the ameri-can medical informatics association, 16(1):89–102..zhengbao jiang, frank f. xu, jun araki, and grahamneubig.
2020. how can we know what languagemodels know?
transactions of the association forcomputational linguistics, 8:423–438..ernesto jim´enez-ruiz, bernardo cuenca grau, ianhorrocks, and rafael berlanga.
2011. logic-basedassessment of the compatibility of umls ontologysources.
journal of biomedical semantics, 2(1):s2..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..alex krizhevsky, ilya sutskever, and geoffrey e. hin-imagenet classiﬁcation with deep con-ton.
2012.in proceedings of thevolutional neural networks.
25th international conference on neural informa-tion processing systems - volume 1, nips’12, page1097–1105, red hook, ny, usa.
curran associatesinc..xiang li, aynaz taheri, lifu tu, and kevin gimpel.
2016. commonsense knowledge base completion.
in proceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1445–1455, berlin, germany.
association for computational linguistics..y. li, j. yang, y. song, l. cao, j. luo, and l. li.
2017.learning from noisy labels with distillation.
in 2017ieee international conference on computer vision(iccv), pages 1928–1936..hanxiao liu, yuexin wu, and yiming yang.
2017.inference for multi-relational embed-analogicalin proceedings of the 34th internationaldings.
conference on machine learning, volume 70 ofproceedings of machine learning research, pages2168–2178, international convention centre, syd-ney, australia.
pmlr..sture holm.
1979. a simple sequentially rejective mul-tiple test procedure.
scandinavian journal of statis-tics, 6(2):65–70..ilya loshchilov and frank hutter.
2019. decoupledin international con-.
weight decay regularization.
ference on learning representations..1025y. ma, p. a. crook, r. sarikaya, and e. fosler-lussier.
2015. knowledge graph inference for spoken dialogsystems.
in 2015 ieee international conference onacoustics, speech and signal processing (icassp),pages 5346–5350..chaitanya malaviya, chandra bhagavatula, antoinebosselut, and yejin choi.
2020. commonsenseknowledge base completion with structural and se-mantic context.
proceedings of the 34th aaai con-ference on artiﬁcial intelligence..yoshitomo matsubara, thuy vu, and alessandro mos-chitti.
2020. reranking for efﬁcient transformer-based answer selection, page 1577–1580.
associ-ation for computing machinery, new york, ny,usa..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation ex-in proceedings oftraction without labeled data.
the joint conference of the 47th annual meeting ofthe acl and the 4th international joint conferenceon natural language processing of the afnlp,pages 1003–1011, suntec, singapore.
associationfor computational linguistics..vinod nair and geoffrey e. hinton.
2010. rectiﬁedlinear units improve restricted boltzmann machines.
in proceedings of the 27th international conferenceon international conference on machine learning,icml’10, pages 807–814, usa.
omnipress..maximilian nickel, lorenzo rosasco, and tomasopoggio.
2016. holographic embeddings of knowl-edge graphs.
in proceedings of the thirtieth aaaiconference on artiﬁcialintelligence, aaai’16,pages 1955–1961.
aaai press..maximilian nickel, volker tresp, and hans-peterkriegel.
2011. a three-way model for collectivelearning on multi-relational data.
in proceedings ofthe 28th international conference on internationalconference on machine learning, icml’11, page809–816, madison, wi, usa.
omnipress..rodrigo nogueira and kyunghyun cho.
2019. pas-arxiv preprint.
sage re-ranking with bert.
arxiv:1901.04085..changhua pei, yi zhang, yongfeng zhang, fei sun,xiao lin, hanxiao sun, jian wu, peng jiang, jun-feng ge, wenwu ou, and dan pei.
2019. person-alized re-ranking for recommendation.
in proceed-ings of the 13th acm conference on recommendersystems, recsys ’19, page 3–11, new york, ny,usa.
association for computing machinery..matthew e. peters, sebastian ruder, and noah a.smith.
2019. to tune or not to tune?
adapting pre-trained representations to diverse tasks.
in proceed-ings of the 4th workshop on representation learn-ing for nlp (repl4nlp-2019), pages 7–14, flo-rence, italy.
association for computational linguis-tics..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of the 2019 confer-edge bases?
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2463–2473, hong kong, china.
as-sociation for computational linguistics..jay pujara, eriq augustine, and lise getoor.
2017.sparsity and noise: where knowledge graph embed-in proceedings of the 2017 con-dings fall short.
ference on empirical methods in natural languageprocessing, pages 1751–1756, copenhagen, den-mark.
association for computational linguistics..thomas rebele, fabian suchanek, johannes hoffart,joanna biega, erdal kuzey, and gerhard weikum.
2016. yago: a multilingual knowledge base fromwikipedia, wordnet, and geonames.
in internationalsemantic web conference, pages 177–185.
springer..anna rogers, o. kovaleva, and anna rumshisky.
2020.a primer in bertology: what we know about howbert works.
arxiv, abs/2002.12327..daniel rufﬁnelli, samuel broscheit, and rainergemulla.
2020. you can teach an old dog newtricks!
on training knowledge graph embeddings.
in international conference on learning represen-tations..jia jia huanbo luan maosong sun ruobing xie,zhiyuan liu.
2016. representation learning ofknowledge graphs with entity descriptions.
in the30th aaai conference on artiﬁcial intelligence..chao shang, yun tang, jing huang, jinbo bi, xi-aodong he, and bowen zhou.
2018. end-to-endstructure-aware convolutional networks for knowl-edge base completion.
corr, abs/1811.04441..karen simonyan and andrew zisserman.
2015. verydeep convolutional networks for large-scale imagein 3rd international conference onrecognition.
learning representations, iclr 2015, san diego,ca, usa, may 7-9, 2015, conference track proceed-ings..richard socher, danqi chen, christopher d. manning,and andrew y. ng.
2013. reasoning with neuraltensor networks for knowledge base completion.
inproceedings of the 26th international conference onneural information processing systems - volume 1,nips’13, pages 926–934, usa.
curran associatesinc..robyn speer and catherine havasi.
2013. conceptnet5: a large semantic network for relational knowl-edge, pages 161–176.
springer berlin heidelberg,berlin, heidelberg..zhiqing sun, zhi-hong deng, jian-yun nie, and jiantang.
2019. rotate: knowledge graph embeddingby relational rotation in complex space.
in interna-tional conference on learning representations..1026j. tompson, r. goroshin, a. jain, y. lecun, andc. bregler.
2015. efﬁcient object localization us-ing convolutional networks.
in 2015 ieee confer-ence on computer vision and pattern recognition(cvpr), pages 648–656..kristina toutanova and danqi chen.
2015. observedversus latent features for knowledge base and textin proceedings of the 3rd workshop oninference.
continuous vector space models and their composi-tionality, pages 57–66, beijing, china.
associationfor computational linguistics..th´eo trouillon, johannes welbl, sebastian riedel, ´ericgaussier, and guillaume bouchard.
2016. complexembeddings for simple link prediction.
in proceed-ings of the 33rd international conference on inter-national conference on machine learning - volume48, icml’16, pages 2071–2080.
jmlr.org..shikhar vashishth, rishabh joshi, sai suman prayaga,chiranjib bhattacharyya, and partha talukdar.
2018.reside: improving distantly-supervised neural rela-tion extraction using side information.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1257–1266.
association for computational linguistics..shikhar vashishth, soumya sanyal, vikram nitin,in-nilesh agrawal, and partha talukdar.
2020a.
teracte:improving convolution-based knowledgegraph embeddings by increasing feature interactions.
in proceedings of the 34th aaai conference on ar-tiﬁcial intelligence, pages 3009–3016.
aaai press..shikhar vashishth, soumya sanyal, vikram nitin, andpartha talukdar.
2020b.
composition-based multi-relational graph convolutional networks.
in interna-tional conference on learning representations..lidan wang, jimmy lin, and donald metzler.
2011. acascade ranking model for efﬁcient ranked retrieval.
in proceedings of the 34th international acm sigirconference on research and development in infor-mation retrieval, sigir ’11, page 105–114, newyork, ny, usa.
association for computing machin-ery..tsun-hsuan wang, hung-jui huang, juan-ting lin,chan-wei hu, kuo-hao zeng, and min sun.
2018.omnidirectional cnn for visual place recognitionand navigation.
arxiv preprint arxiv:1803.04228..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..han xiao, minlie huang, and xiaoyan zhu.
2016. ssp:semantic space projection for knowledge graph em-bedding with text descriptions..bishan yang, scott wen-tau yih, xiaodong he, jian-feng gao, and li deng.
2015. embedding entitiesand relations for learning and inference in knowl-edge bases.
in proceedings of the international con-ference on learning representations (iclr) 2015..liang yao, chengsheng mao, and yuan luo.
2019.kg-bert: bert for knowledge graph completion.
corr, abs/1909.03193..fuzheng zhang, nicholas jing yuan, defu lian, xingxie, and wei-ying ma.
2016. collaborative knowl-edge base embedding for recommender systems.
in proceedings of the 22nd acm sigkdd inter-national conference on knowledge discovery anddata mining, kdd ’16, pages 353–362, new york,ny, usa.
acm..a implementation details.
a.1 bert mlm pre-training.
we utilize the huggingface transformers library(wolf et al., 2020) to work with pre-trained lan-guage models.
we ﬁne-tune the pre-trained lan-guage model with the masked-language-modelingobjective upon the set of textual entity identiﬁersfor the knowledge graph.
we train the model for 3epochs with a batch size of 32 using a learning rateof 3e-5.
we use a warmup proportion of 0.1 of thetotal training steps for each dataset.
we use a maxsequence length of 64 during this pre-training ex-cept when using the textual descriptions associatedwith fb15k-237 where we use a max sequencelength of 256. we utilize these dataset-speciﬁclanguage models for both generating the entity em-beddings and for initializing the re-ranking model..a.2 ranking.
a.2.1 training procedure.
we train all of the ranking models implemented inthis work for a maximum of 200 epochs and termi-nate training early if the validation mrr has notimproved for 20 epochs.
for evaluation, we reloadthe model weights from the epoch that achieved thebest validation mrr and evaluate it on the test set..a.2.2 bert-resnet implementationsfor our bert-resnet model, we set f = 5 wheref is the hyperparameter that controls the size ofthe spatial feature map produced by the initial 1dconvolution.
thus our initial 1d convolution hasf × f = 25 ﬁlters.
we set n = 2 where n is.
1027the hyperparameter that controls the depth of theconvolutional network.
this means that our bert-resnet model consists of 3n = 6 sequential bot-tleneck blocks..we trained the models using a batch size of 64with a 1vsall strategy (rufﬁnelli et al., 2020) withthe binary cross entropy loss function.
we use theadam optimizer (kingma and ba, 2015) with de-coupled weight decay regularization (loshchilovand hutter, 2019) and train the model with a learn-ing rate of 1e-3.
we use label smoothing with avalue of 0.1, clip gradients to a max value of 1,and regularize the model using weight decay witha weight of 1e-4.
we apply dropout with drop prob-ability 0.2 after the embedding layer and apply 2ddropout (tompson et al., 2015) with the same dropprobability before the 2d convolutions.
we applydropout with probability 0.3 after the pooling andfully connected layer.
we manually tuned the hy-perparameters for this model based on validationperformance..a.2.3 baseline implementations.
for our baseline implementations of distmult,complex, conve, and convtranse, we adapt theimplementations released by dettmers et al.
(2018)and malaviya et al.
(2020).
we utilize the hyper-parameters reported in the original papers and con-duct a grid search to tune the embedding dimensionfrom [100, 200, 300] and the initial learning ratefrom [5e-3, 1e-3, 5e-4, 1e-4] for each dataset.
wetrain the models with a batch size of 128 using the1vsall strategy with the cross entropy loss functionbecause the replication study by rufﬁnelli et al.
(2020) found that this training strategy generallyled to better performance than other training strate-gies.
for the grid search, we train each model for amaximum of 50 epochs and then select the hyperpa-rameters with the best validation performance andretrain the model with our aforementioned trainingprocedure..for our implementation of bert-conve andbert-convtranse, we adapt the baseline conveand convtranse to use bert embeddings in thesame manner as our model.
the convolution forbert-conve has 32 channels and the convolutionfor bert-convtranse has 64 channels.
these val-ues were selected to produce models with a compa-rable number of trainable parameters to our model.
we then project the ﬁnal feature vector down tothe embedding dimensionality and rank candidatesidentically to our model..we trained both models with a batch size of64 using 1vsall strategy (rufﬁnelli et al., 2020)with the binary cross entropy loss function usingthe adam optimizer (kingma and ba, 2015) withdecoupled weight decay regularization (loshchilovand hutter, 2019).
we train the models with alearning rate of 1e-4, use label smoothing withvalue 0.1, clip gradients to a max value of 1, andregularize the model using weight decay with aweight of 0.0001. we apply dropout with dropprobability 0.2 after the embedding layer and afterthe convolution.
we apply dropout with probability0.3 after the fully connected layer..for our baseline bert-deepconv model, we usethe same hyperparamters as bert-resnet for theinitial 1-d convolution and then apply a sequenceof three 3 × 3 convolutions with circular padding.
the second convolution doubles the number ofchannels so the dimensionality of the ﬁnal featuremap produced by the sequence of convolutions is2d.
we then mean pool and project the feature mapto the embedding dimensionality identically to ourproposed model.
we selected these hyperparame-ters so that this baseline has a similar number oftrainable parameters to our proposed model.
allother implementation details are identical to ourbert-resnet model (e.g.
use of pre-activations,application of dropout, training hyperparameters,etc.)..
a.3 re-ranking.
we ﬁne-tune bert with a learning rate of 3e−5using the adam optimizer (kingma and ba,2015) with decoupled weight decay regularization(loshchilov and hutter, 2019).
we truncate thetextual triple representation to a max length of 32tokens and ﬁne-tune bert with a batch size of 128for a maximum of 10 epochs.
training is termi-nated early if the validation mrr does not improvefor 3 epochs.
we set the weight decay parameter to0.01 and clip gradients to a max value of 1 duringtraining.
we apply dropout with probability 0.3to the ﬁnal feature representation before the pre-diction and otherwise use the default parametersprovided by the huggingface transformers library(wolf et al., 2020).
we set λ = 0.5 for snomedct core, λ = 1.0 for cn-100k, and λ = 0.75for fb15k-237 and fb15k-237-sparse.
we set thetemparature as t = 1 for all models..1028fb15k-237.
mrr.
h@1.h@3.h@10.bert-convebert-convtranse.
.308.301.
.228.224.
.334.326.bert-deepconv.
.332.
.251.
.360.bert-resnet.
.351.
.269.
.384.
.467.449.
.490.
.514.mr.189208.
186.
185.table 6: validation ranking results for fb15k-237..fb15k-237-sparse.
mr.mrr.
h@1.h@3.h@10.distmultcomplexconveconvtranse.
bert-convebert-convtranse.
.136.134.158.154.
.192.192.
.093.092.107.103.
.128.129.
.146.144.166.163.
.202.204.bert-deepconv.
.193.
.131.
.203.bert-resnet.
.194.
.131.
.204.
3034331122472275.
412390.
419.
412.
.227.220.261.257.
.321.318.
.320.
.321.b evaluation metrics.
we provide a mathematical formulation for ourevaluation metrics.
if we denote the set of all factsin the test set as t , then the mean rank (mr) issimply computed as.
the mean reciprocal rank (mrr) is computed as.
mr =.
rank(xi).
1|t |.
(cid:88).
xi∈t.
mrr =.
1|t |.
(cid:88).
xi∈t.
1rank(xi).
the hits at k (h@k) is calculated as.
h@k =.
i[rank(xi) ≤ k].
1|t |.
(cid:88).
xi∈t.
where i[p ] is 1 if the condition p is true and is0 otherwise.
when computing rank(xi), we ﬁrstﬁlter out all positive samples other than the tar-get entity xi.
this is commonly referred to as theﬁltered setting..snomed ct core.
mr.mrr.
h@1.h@3.h@10.distmultcomplexconveconvtranse.
bert-convebert-convtranse.
.294.303.271.293.
.384.374.
.226.225.191.216.
.278.274.
.319.335.303.323.
.431.417.bert-deepconv.
.481.
.376.
.534.bert-resnet.
.493.
.389.
.546.
.427.457.429.446.
.593.569.
.687.
.694.c supplementary tables.
table 7: validation ranking results for fb15k-237-sparse..5039385036183484.
386487.
250.
249.mr.283323.
261.
269.table 4: validation ranking results for snomed ctcore..bert-resnet.
+ re-ranking + kd + te.
.648.668.
.488.511.
.758.780.cn-100k.
mrr.
h@1.h@3.h@10.bert-resnet.
+ re-ranking + kd + te.
.664.678.
.523.539.
.748.761.bert-convebert-convtranse.
.370.381.
.253.267.
.423.430.bert-deepconv.
.463.
.342.
.526.bert-resnet.
.463.
.341.
.53.
.606608.
.705.
.700.table 5: validation ranking results for cn-100k..bert-resnet.
+ re-ranking + kd + te.
.698.822.
.561.724.
.787.901.snomed ct core.
mr mrr.
h@1.h@3.cn-100k.
mr mrr.
h@1.h@3.fb15k-237.
mr mrr.
h@1.h@3.fb15k-237-sparse.
mr mrr.
h@1.h@3.
22.
32.
33.
33.bert-resnet.
+ re-ranking + kd + te.
.567.589.
.407.427.
.634.667.table 8: validation re-ranking results.
we report met-rics for the subset of queries where the retrieved entityis already in the top 10 entities because the re-rankingprocedure leaves other rankings unchanged..1029