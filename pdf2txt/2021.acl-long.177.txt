qasr: qcri aljazeera speech resourcea large scale annotated arabic speech corpus.
hamdy mubarak,1 amir hussein,2 shammur absar chowdhury,1 and ahmed ali11 qatar computing research institute, hbku, doha, qatar2 kanari ai, california, usainfo@arabicspeech.org, https://arabicspeech.org/.
abstract.
we introduce the largest transcribed arabicspeech corpus, qasr1, collected from thebroadcast domain.
this multi-dialect speechdataset contains 2, 000 hours of speech sam-pled at 16khz crawled from aljazeera newschannel.
the dataset is released with lightlysupervised transcriptions, aligned with theaudio segments.
unlike previous datasets,qasr contains linguistically motivated seg-mentation, punctuation, speaker informationamong others.
qasr is suitable for train-ing and evaluating speech recognition sys-tems, acoustics- and/or linguistics- based ara-bic dialect identiﬁcation, punctuation restora-tion, speaker identiﬁcation, speaker linking,and potentially other nlp modules for spokendata.
in addition to qasr transcription, we re-lease a dataset of 130m words to aid in design-ing and training a better language model.
weshow that end-to-end automatic speech recog-nition trained on qasr reports a competi-tive word error rate compared to the previousmgb-2 corpus.
we report baseline results fordownstream natural language processing taskssuch as named entity recognition using speechtranscript.
we also report the ﬁrst baseline forarabic punctuation restoration.
we make thecorpus available for the research community..1.introduction.
research on automatic speech recognition (asr)has attracted a lot of attention in recent years (chiuet al., 2018; watanabe et al., 2018).
such successhas brought remarkable improvements in reachinghuman-level performance (xiong et al., 2016; saonet al., 2017; hussein et al., 2021).
this has beenachieved by the development of large spoken cor-pora: supervised (panayotov et al., 2015; ardilaet al., 2019); semi-supervised (bell et al., 2015; ali1qasr (cid:81)(cid:229)(cid:148)(cid:16)(cid:175) in arabic means “palace”.
the acronym.
stands for: qcri aljazeera speech resource..et al., 2016); and more recently unsupervised (valkand alum¨ae, 2020; wang et al., 2021) transcrip-tion.
this work enables to either reduce word errorrate (wer) considerably or extract metadata fromspeech: dialect-identiﬁcation (shon et al., 2020);speaker-identiﬁcation (shon et al., 2019); and code-switching (chowdhury et al., 2020b, 2021)..natural language processing (nlp), on theother hand values large amount of textual infor-mation for designing experiments.
nlp researchfor arabic has achieved a milestone in the last fewyears in morphological disambiguation, named en-tity recognition (ner) and diacritization (pashaet al., 2014; abdelali et al., 2016; mubarak et al.,2019).
the nlp stack for modern standard ara-bic (msa) has reached very high performance inmany tasks.
with the rise of dialectal arabic (da)content online, more resources and models havebeen built to study da textual dialect identiﬁcation(abdul-mageed et al., 2020; samih et al., 2017)..our objective is to release the ﬁrst arabic speechand nlp corpus to study spoken msa and da.
this is to enable empirical evaluation of learningmore than the word sequence from the speech.
inour view, existing speech and nlp corpora aremissing the link between the two different modali-ties.
speech poses unique challenges such as dis-ﬂuency (pravin and palanivelan, 2021), overlapspeech (tripathi et al., 2020; chowdhury et al.,2019), hesitation (wottawa et al., 2020; chowd-hury et al., 2017), and code-switching (du et al.,2021; chowdhury et al., 2021).
these challengesare often overlooked when it comes to nlp tasks,since they are not present in typical text data..in this paper, we create and release2 the largestcorpus for transcribed arabic speech.
it comprisesof 2, 000 hours of speech data with lightly super-vised transcriptions.
our contributions are: (i).
2data can be obtained from:.
https://arabicspeech.org/qasr.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2274–2285august1–6,2021.©2021associationforcomputationallinguistics2274aligning the transcription with the correspondingaudio segments including punctuation for build-ing asr systems; (ii) providing semi-supervisedspeaker identiﬁcation and speaker linking per audiosegments; (iii) releasing baseline results for acous-tic and linguistic arabic dialect identiﬁcation andpunctuation restoration; (iv) adding a new layer ofannotation in the publicly available mgb-2 testset,for evaluating ner for speech transcription; (v)sharing code-switching data between arabic andforeign languages for speech and text; and ﬁnally,(vi) releasing more than 130m words for languagemodel (lm)..we believe that providing the research com-munity with access to multi-dialectal speech dataalong with the corresponding nlp features will fos-ter open research in several areas, such as the anal-ysis of speech and nlp processing jointly.
here,we build models and share the baseline results forall of the aforementioned tasks..1.1 related work.
the callhome task within the nist benchmarkevaluations framework (pallett, 2003), released oneof the ﬁrst transcribed arabic dialect dataset.
overyears, nist evaluations provided with more dialec-tal - mainly in egyptian and levantine dialects, aspart of language recognition evaluation campaign.
projects such as gale and transtac (oliveet al., 2011) program, released more than 251 hoursof arabic data, including the ﬁrst spoken iraqi di-alect among others.
these datasets exposed theresearch community to the challenges of spokendialectal arabic and motivated to design competi-tion to handle dialect identiﬁcation, dialectal asramong others (see ali et al.
(2021) for details)..the following datasets are released from themulti-genre broadcast mgb challenge: (i) mgb-2 (ali et al., 2016) – this dataset is the ﬁrst mile-stone towards designing the ﬁrst large scale contin-uous speech recognition for arabic language.
thecorpus contains a total of 1, 200 hours of speechwith lightly supervised transcriptions and is col-lected from aljazeera arabic news channel spanover many years.
(ii) mgb-3 (ali et al., 2017) –focused on only egyptian arabic broadcast data(iii) mgb-5 (ali et al.,comprises of 16 hours.
2019) – consists of 13 hours of moroccan arabicspeech data.
in addition, the commonvoice3 ara-.
table 1: comparison between mgb-2 vs qasr..mgb-21, 200.qasr2, 000.msa, glf, lev, nor, egyinﬂuenced bysilence andsegment length.
linguistically andacousticallymotivatedlightlysupervised.
hoursdialects.
segmentation.
transcription.
punctuationcode-switchingpossible turn-endingspeaker names.
speaker gender.
speaker country.
ner.
lightlysupervised–––.
–.
–.
–.
(+ normalised names)(2000 speakers)covers ≈82% datamanually annotated.
in testset.
in testset.
manually annotated.
bic dataset, from the commonvoice project, pro-vides 49 hours of modern standard arabic (msa)speech data.4.
unlike mgb-2, qasr dataset is the largestmulti-dialectal corpus with linguistically motivatedsegmentation.
the dataset includes multi-layer in-formation that aids both speech and nlp researchcommunity.
qasr is the ﬁrst speech corpora toprovide resources for benchmarking ner, punc-tuation restoration systems.
for close comparisonbetween mgb-2 vs qasr, see table 1..2 corpus creation.
2.1 data collection.
we obtained aljazeera arabic news channel’sarchive (henceforth aj), spanning over 11 yearsfrom 2004 until 2015. it contains more than 4, 000episodes from 19 different programs.
these pro-grams cover different domains like politics, society,economy, sports, science, etc.
for each episode, wehave the following: (i) audio sampled at 16khz;(ii) manual transcription, the textual transcriptionscontained no timing information.
the quality ofthe transcription varied signiﬁcantly; the most chal-lenging were conversational programs in whichoverlapping speech and dialectal usage was morefrequent; and ﬁnally (iii) some metadata..for better evaluation of the qasr corpus, wereused the publicly available mgb-2 (ali et al.,2016) testset as it has been manually revised, com-ing from the same channel, thus making this testsetideal to evaluate the qasr corpus.
it is worth not-ing that we ensure that the mgb-2 dev/test sets.
3https://commonvoice.mozilla.org/en/.
datasets.
4reported on june 2021..2275itemhoursepisodessegmentswordsspeakersmalesfemalesvarietycountries.
genre.
description1017. average episode duration = 34 min8, 01469, 644. unique words = 15, 75411187 (78%)13 (11%)msa: 78%:, dialectal arabic: 22%top 5 countries are: (based on dialectal segments)eg: 18%, sy: 11%, ps: 11%, dz: 8%, sd: 7%top 5 topics are: politics: 69%, society: 9%,economy: 8%, culture/art: 4%, health: 3%.
table 2: description of the updated mgb-2 testset.
itemhoursepisodessegments.
count notes2, 0413, 545 average episode duration = 32 min.
1.6m .
average segment duration = 4 sec.
84% of segments are [2-6] sec.
average segment len = 9 words80% of segments have [5-11] words.
wordsspeakersmalesfemales.
14.3m unique words = 360k27, 977 unique speakers = 11, 092.
1, 17168.
1.2m segments (69%)99k segments (6%).
table 3: qasr corpus statistics.
are not included in qasr corpus, so they can beused to report progress on the arabic asr chal-lenge.
we have also enriched the mgb-2 testsetwith manually annotated speaker information likecountry5, gender of the speakers, along with nerinformation and used it to evaluate our baselines..moreover, we apply topic classiﬁcation and di-alect identiﬁcation.
our models achieved an over-all accuracy of 96% and 88% respectively, whichhave been measured on internal testsets also createdfrom aljazeera news articles.
more details can befound in asad demo paper (hassan et al., 2021).
table 2 gives a rough estimate about distributionsin the updated mgb-2 testset..2.2 metadata information.
most of the recorded programs have the followingmetadata: program name, episode title and date,speaker names and topics of the episode.
majorityof metadata information appear in the beginningof the ﬁle.
however, some of them are embeddedinside the episode transcription.
figure 1 shows asample input ﬁle from aljazeera.
one of the mainchallenges is the inconsistency in speaker names,e.g.
barack obama appeared in 9 different forms(barack obama, barack obama/the us president,.
5we use iso 3166 for country codes.
//en.wikipedia.org/wiki/list_of_iso_3166_country_codes.
https:.
barack obama/president of usa, barck obama(typo), etc.).
the list of guest speakers and episodetopics are not comprehensive, with many spellingmistakes in the majority of metadata ﬁeld namesand attributes.
to overcome these challenges, weapplied several iterations of automatic parsing andextraction followed by manual veriﬁcation and stan-dardization..figure 1: sample input text ﬁle from aljazeera.
outputsegments are underlined using different colors..sample output ﬁle from qasr is shown infigure 2. it contains speaker names as they ap-pear in the current episode and their correspond-ing standardized forms across all ﬁles, which canbe useful for tasks such as speaker identiﬁcationand speaker linking across the entire corpus.
foreach speaker, we provide gender information andwhether the speaker’s name refers to a unique per-son (e.g.
barack obama) or not (e.g.
one of theprotesters, or an audio reporter).
figure 2 has in-formation on the anchor speaker and two guests asthey appear in the metadata ﬁle, in addition to otherspeakers that were missed in the original transcrip-tion.
it is worth noting that we provide gender andcountry for common arabic speakers (who haveat least 20 segments in the entire corpus).
on theother hand, we ignore metadata for foreign speak-ers because dubbing their speeches can be done byany voice-over.
we provide gender information for2, 000 speakers and this covers 82% of all segmentsin the whole corpus..speech and text are aligned (see details in sec-tion 2.3) and split into short segments (see section.
2276figure 2: sample output text ﬁle from qasr (xml)..figure 3: alignment of aljazeera transcription & asr.
2.5).
for each segment, we provide: words (ele-ment), timing information (starttime and endtime)in addition to speaker id (who), average wordduration (awd) in seconds, grapheme match er-ror rate (gwer), and word match error rate(wmer).
for details about word and graphemematch, refer to (bell et al., 2015; ali et al., 2016).
figure 2 shows information for segment1 that ap-pears in figure 1..2.3 speech to text alignment.
the main concept of this method is to run an arabicspeech recognition system over the entire episode(khurana and ali) and use the recognized wordsequences and their locations in time for automaticalignment (braunschweiler et al., 2010)..for alignment, aljazeera and asr transcrip-tions are then converted into two long sequencesof words.
aligning the sequences was challengingfor many reasons; code-switching between msaand dialects; human transcription was not verba-tim, e.g.
some spoken words were dropped due torepetition or correction; spelling and grammar mis-takes; usage of foreign languages mainly englishand french; and many overlapped speeches..we used smith–waterman algorithm (smithet al., 1981), which performs local sequence align-ment to determine similar regions between twostrings.
we modiﬁed the algorithm to accept anapproximate match between the given transcriptionand the recognized word sequence.
if the lev-enshtein distance between two words ≤ half thelength (number of characters) in the given transcrip-tion, this is considered as an approximate match..figure 3 shows a sample alignment, where eachword is assigned to a speaker after parsing al-jazeera text and aligned, if possible, to a word fromasr transcription along with its timing informa-.
tion.
relaxation is applied in case of approximatematch.
time information of the missing words(highlighted in red in aj column) is estimated by in-terpolation from the matched word before and after.
(cid:44)(cid:73)(cid:46) (cid:28)(cid:46)(cid:130)(cid:29)(cid:46)in this example, we consider words (cid:233)(cid:74)(cid:46)(cid:28)(cid:46)(cid:130)(cid:29)(cid:46)(because-of, because-of-it) as approximate match..2.4 matching asr accuracy.
figure 4 shows the matching accuracy between theasr and the given transcription at the segmentlevel.
we applied two levels of matching to dealwith these challenges: exact match (where bothtranscription and asr output are identical), andapproximate match (where there is a forgiving editdistance between words in the transcription andasr output).
exact match (100% in the x-axis)would have led to less than 27% of the segments,while approximate match allows to consider moresegments..figure 4: matching accuracy between asr and al-jazeera transcription.
2.5 segmentation.
after aligning the given transcription with the asrwords for the whole episode, we want to segmentthe text into shorter segments.
unlike mgb-2, weconsidered many factors that we believe lead tobetter and logical segmentation, namely:.
2277• surface: we tried to make segments in the rangeof [3-10] words.
we consider punctuation6 asend of segments if they appear in this range, andwe increase the window to 5 words to capture anyof them in the neighbouring words.
typically,transcribers insert punctuation marks to indicateend of logical segments (sentences or phrases)..• dialog: when a speaker changes in the tran-scribed text, we consider this as a valid end of seg-ment.
by doing this, we assign only one speakerto each segment..• acoustics: if there is a silence duration of atleast 150msec between words, we consider thisas a signal to potentially end the current segment.
we consider the proceeding linguistic rules toconﬁrm the validity of this end..• linguistics: for linguistically motivated seg-mentation, we want to avoid ending segmentsin wrong places (e.g.
in the middle of namedentities (ne), noun phrases (np) or adjec-tive phrases (ap)).
to do so, from the 130mwords in the lm data, we extracted the mostfrequent 10k words that were not followed byany punctuation in 90% of the cases, then werevised them manually7.
we call this list ”no-(cid:9)(cid:175)(cid:44)(cid:248)(cid:10) (cid:88) (cid:13)(cid:241)(cid:75)(cid:10) (cid:44)(cid:250)(cid:10)stop-list”.
examples are:.
(cid:232)(cid:65)(cid:109)(cid:46)(cid:26)(cid:16)(cid:39)(cid:65)(cid:75)(cid:46).
(in, leads-to, towards).
additionally, we used thepublicly available arabic nlp tools (farasa)8for ner and part of speech (pos) tagging tolabel each word in the transcriptions.
we putmarks to avoid ending segments in the middleof nes, nps or aps.
these are some exam-ples from mgb-2 that have segmentation errors(cid:14)and words appearing erroneously in differentsegments: (cid:128)(cid:65)(cid:9)(cid:74)(cid:203)(cid:64)(cid:47) (cid:200)(cid:65)(cid:211)(cid:64) (people’s (segi) /hopes(cid:16)(cid:233)(cid:74)(cid:10)(cid:107)(cid:46) (cid:80)(cid:65) (cid:9)(cid:103)(cid:47) (cid:250)(cid:10)(cid:171)(cid:65)(cid:130)(cid:211) (external /endeavors)(segi+1)),(cid:13)(cid:66)(cid:64)(cid:47) (cid:16)(cid:232)(cid:89)(cid:106)(cid:16)(cid:74)(cid:214)(cid:207)(cid:64)(cid:16)(cid:72)(cid:65)(cid:75)(cid:10)(cid:66)(cid:241)(cid:203)(cid:64) (united states.
and/of america).
if the surface or acoustics mod-ules suggest end of segment, while contradictingthese linguistics rules, this suggestion is ignored.
details of qasr corpus after alignment and seg-mentation are presented in table 3..(cid:16)(cid:233)(cid:74)(cid:10)(cid:186)(cid:75)(cid:10)(cid:81)(cid:211).
2.6.intrasentential code-switching.
we discuss here the presence of intrasententialcode-switching in qasr.
we noticed in addition.
6common punctuation marks are: period, comma, ques-tion mark, exclamation mark, semicolon, colon and ellipsis..7the ﬁnal list has 2,200 words.
8farasa.qcri.org.
ca word/utt.
cmi range1.30 < cm i ≤ 15%1.615 < cm i ≤ 30%30 < cm i ≤ 45%1.945 < cm i ≤ 100% 2.3.
9.57.05.53.8.
#.
1, 4583, 806790178.table 4: details of code-switching level in qasr datausing cmi range.
word/utt.
represents the averageword count per utterance, ca is the mean number ofcode alternation points in utterances, #.
presents thenumber of utterances for the cmi range..to the intrasentential dialectal code switching (dis-cussed in section 3.4), the dataset also includes≈ 6k segments, where alternation between arabicand english/french languages are seen..to quantify the amount of code-switchingpresent in this data, we calculate both the utteranceand corpus level code-mixing index (cmi), moti-vated by chowdhury et al.
(2020b); gamb¨ack anddas (2016).
based on the range of utterance-levelcmi values, we group our dataset, as shown intable 4. as for the corpus-level cmi, we observean average of 30.5 cmi-value, calculated based onthe average of utterance-level9 cmi consideringthe code-switching segments in qasr dataset..furthermore, from utterance-level analysis, wenotice that the majority of the code-switched seg-ments falls under 15 < cm i ≤ 30% with anaverage of 2 alteration points per segment (e.g.
ar→ en → ar).
even though the code-switching oc-curs in only 0.4% of the full dataset, we notice thatwe have very short ≈ 968 segments (ranging cmivalue > 30%) with frequent alternating language.
code, such as: ”(cid:248)(cid:10) (cid:89)(cid:9)(cid:74)(cid:171) duplex (cid:64)(cid:241)(cid:107)(cid:46).
(cid:16)(cid:233)(cid:9)(cid:74)(cid:28)(cid:10)(cid:9)(cid:74)(cid:109)(cid:46)(cid:26)(cid:39)(cid:46) building”..in the future, these segments could be used to fur-ther explore the effect of such code-switching inthe performance of speech and nlp models jointly..3 downstream tasks.
3.1 automatic speech recognition.
in this section, we study qasr dataset for the asrtask.
we adopt the end-to-end transformer (e2e-t) architecture from hussein et al.
(2021) as ourbaseline for qasr dataset.
we ﬁrst augment thespeech data with the speed perturbation with speedfactors of 0.9, 1.0 and 1.1 (ko et al., 2015).
then,we extract 83-dimensional feature frames consist-ing of 80-dimensional log mel-spectrogram andpitch features (ghahremani et al., 2014) and apply.
9excluding switches between the utterances..2278best e2e-t-mgb-2.
baseline e2e-t-qasr.
dev wer /[s, d, i]15.0[10.0, 3.9, 1.1]15.1[7.0, 7.4, 0.7].
test wer /[s, d, i]14.3[9.5, 3.7, 1.1]14.7[7.1, 7.0, 0.6].
table 5: wer% performance with the insertion (i%),substitution (s%) and deletion (d%) rates for the trans-former asr (e2e-t) pretrained on qasr and mgb-2..cepstral mean and variance normalization.
further-more, we augment these features using the specaug-ment approach (park et al., 2019).
we use espnet(watanabe et al., 2018) to train the e2e-t modelon mgb-2 and qasr datasets.
each model wastrained for 30 epochs using 4 nvidia tesla v100gpus, each with 16 gb memory, which lastedtwo weeks.
results of the baseline model on bothdevelopment and testsets are shown in table 5..it can be seen that the best e2e-t-mgb-2achieves slightly better wer with a difference of0.3% on average.
this is expected since adoptede2e-t architecture was carefully tuned on mgb-2dataset.
however, the e2e-t-qasr achieves lowersubstitution and insertion rates with an absolute dif-ference of 2.7% and 0.5% on average respectively.
it can also be noticed that almost half of the e2e-t-qasr errors are due to deletions.
to investigatethese results further, we visualize the distributionof segmentation duration of the mgb-2 train, theqasr train and the testsets as shown in figure 5.we consider the range within 3 standard deviationsof each distribution as the effective segmentationduration that contains 99% of the segments, andthe rest 1% of the segments are considered as out-liers.
from figure 5, it can be seen that qasrdistribution is following the bell curve similar tothe testset which was segmented by an expert tran-scriber.
on the other hand, the mgb-2 distributionis right-skewed with segment duration outliers thatgo beyond 50 seconds.
in addition, one can ob-serve that the effective segmentation duration ofthe testset is 9 seconds, which is larger than qasreffective segmentation duration, which is only 7seconds.
on the other hand, the mgb-2 effectivesegmentation duration covers a much larger rangeof over 30 seconds.
the difference in the segmentduration affects the statistical properties of the dataand causes a shift in the data distribution.
we thinkthat this is the main reason why the baseline e2e-t-qasr achieves worse results than best e2e-t-mgb-2.
to validate our assumption, we analyzethe e2e-t-qasr transcription and found that the.
deletion errors mainly appeared with segments thatare larger than 7 seconds.
we illustrate our ﬁnd-ings with two transcription examples in buckwalter(bw) format shown in figure 6: short segment of6 seconds, and long segment of 10 seconds.
dele-tions are highlighted in red, substitutions in yellow,and correct in green.
it can be seen from the shortexample that e2e-t-qasr achieves better resultswith a potential for code-switching.
on the otherhand, the long example conﬁrms our assumptionabout the shift in segments duration distributionbetween qasr and the testset..3.2 automatic punctuation restoration.
cl.
(cid:44).
.
(cid:63).
o.qasr.
dev.
test.
fisher.
428k (3.2%).
2k (3.0%).
1k (2.5%).
70k (11.8%).
154k (1.2%)87k (0.7%).
1k (2.5%)623 (0.9%).
1k (1.8%)349 (0.5%).
362k (6.3%)56k (1.3%).
12m (95.0%).
68k (93.6%).
63k (95.1%).
2m (80.6%).
table 6: distribution of punctuation classes in qasr(arabic) along with 348 hours of fisher (english) cor-pus as a reference.
o representing – no punctuation,comma ((cid:44)), fstop (.
), ques ((cid:63))..in this section, we explore qasr for the auto-matic punctuation restoration task.
to prepare thetraining data, we ﬁrst segment the utterances fromthe same speaker with a maximum window of 120tokens.
we then remove utterances with ≤ 6 wordsand no punctuation in the segment.
we pre-processthe lexical utterances, removing diacritics, brack-ets, among others.
for the task, we only keep thetop 3 punctuation classes (‘(cid:44)’, ‘(cid:63)’ and ‘.’) and restare mapped to class ‘o’ representing no punctua-tion.
the distribution of punctuation in qasr arehighly imbalanced (as shown in table 6), which isexpected of a spoken corpus.
however, in compar-ison to the fisher corpus (cieri et al., 2004) andother language datasets (see (li and lin, 2020)),the distribution is more skewed.
this is becausein arabic, punctuation marks are rarely used, e.g.,segment1 in figure 1, can be logically divided intotwo segments separated by a full stop..we adapt a simple transformer-bilstm archi-tecture (alam et al., 2020) as our baseline modelusing lexical information.
given an input token se-quence (x1, x2..., xm), we extract the subwords(s1, s2..., sn) using wordpiece tokenizer.
thesesubwords are fed into the pre-trained bert model,which outputs a vector of d dimension for each timestep.
these d vectors are then passed to a bilstm.
2279(a).
(b).
(c).
figure 5: duration distributions of the speech segments for: (a) mgb-2, (b) qasr, and (c) test dataset..figure 6: e2e-t-qasr and e2e-t-mgb-2 transcription on short segment and long segments of 6 and 10 secondsrespectively.
each example includes text in arabic, buckwalter (bw) and english translation..devprf1testprf1.
o97.0%98.8%97.9%o98.1%98.4%98.3%.
52.7%38.8%44.7%.
comma fstop ques78.8% 61.3%50.4% 59.7%61.5% 60.5%comma fstop ques70.7% 52.6%51.7% 57.3%59.7% 54.9%.
44.9%48.6%46.7%.
table 7: reported precision (p), recall (r) and f-measure (f1) on test and dev set using punctuationrestoration model trained on qasr dataset..layer, consisting of h hidden units.
the choice ofusing bilstm is to make effective use of both past←−−→h ) and future (h ) contexts for prediction.
the(←−−→h output at each time step ish +concatenatedthen fed to a fully-connected layer with four outputneurons, which correspond to 3 punctuation marksand the ‘o’ token..during the training, special tokens identifyingstart- and end-of the sentence are added to the in-.
put subword sequence.10 for this task, we usedarabert (antoun et al., 2020): pre-trained onnewspaper articles, containing 3 transformer self at-tention layers with each hidden layer of 768. thesetoken embeddings are then passed onto a bilstmwith hidden dimension of 768. the baseline modelis trained using adam optimizer with a learningrate of 1e − 5 and 32 batch size for 10 epochs..despite the fact that arabic has a skewed distri-bution in punctuation, the baseline results reportedin table 7 for the 3 punctuation and ‘o’ labels showthat the prediction results of the full stop and thequestion mark are better than the comma.
thisagain reconﬁrms that in arabic, the use of commais highly debatable (mubarak et al., 2015; mubarakand darwish, 2014) and can easily be substituted bythe full stop or other punctuation.
in the future, wewill explore better architectures with informationfrom different modalities, such as acoustics..10the maximum length of the subwords is set to 256. incases, if the sequence exceeds the maximum length, it is thendivided into two separate sequences..2280 short segment ref-arabic انأ لخبص نم ةكرشلا يللا انأ اهيف علطب ىلع ةكرش ةيناث part time  لثم translation   after finishing from the company i am in i go into another part time for example ref-bw >na bxls mn al$rkp ally >na fyha btle ely $rkp vanyp part time mvla e2e-t-qasr >na bxls mn al$rkp ally >na fyha btle ely $rkp vanyp part time mvla e2e-t-mgb2 >na bxls mn al$rkp ally >na fyha btle ely $rkp vanyp bartheid mvla long segment ref-arabic ذقلا ناك لاملاو معدلا يف بصعلا نكل ينعي ةريقف لود تناك يبونجلا نميلاو ايبويثأ نأو اميس لا وه هلان معدتناك ايبويثأ مامت يفا translation   the support he received, especially since ethiopia and south yemen were poor countries i mean but the insistence in support and money was gaddafi who was completely ethiopia was ref-bw dem nalh hw la syma w>n >vywbya walymn aljnwby kant dwl fqyrp yeny lkn  alesb fy  aldem walmal kan alq*afy tmam >vywbya kant  e2e-t-qasr ***  ***  *** la syma w>n >vywbya walymn aljnwby kant dwl fqyrp yeny lkn  alesb fy aldem walmal ***       ***        ***      ***    kant e2e-t-mgb2 dem malh hw la syma w>n >vywbya walymn aljnwby kant dwl fqyrp yeny lkn  alesb fy aldem walmal  kant t*hb  tmam  >vywbya  kant speakers– male– femalesegments.
countries.
40 (anchor (a): 20, guest (g): 20)33 (a: 14, g:19)7 (a:6, g:1)4, 000 (100 / speaker)11 unique countries (dz, eg, iq,lb, ly, ma, ps, sa, sy, tn, ye).
table 8: qasr subset used for speaker veriﬁcation (sv)and arabic dialect identiﬁcation (adi) tasks..3.3 speaker veriﬁcation.
one of the biggest challenges in broadcast domainis its speech diversity.
the anchor speaker voiceis often clear and planned.
however, the spokenstyle11 of different program guests can present var-ious challenges.
here, we showcase how qasrcould be used to evaluate existing speaker modelsbased on the speakers’ role in each episode.
in thefuture, the dataset can also be used to study turn-taking and speaker dynamics, given the interactionbetween speakers in qasr..setsanchorguestmixedvoxceleb1-tst.
eer9.27.57.96.8.total pairs40k (75% male)40k (100% male)40k (75% male)38k.
table 9: reported eer on veriﬁcation trial pairs for an-chors, guest and their combination.
in addition, eerreported on voxceleb1 ofﬁcial test veriﬁcation pairs(english) as reference.
50% of the total pairs are posi-tive, i.e.
from same speaker..we adapt one of the widely-known architec-tures used to model an end-to-end text-independentspeaker recognition (sr) system.
for the study,we use a pre-trained model, with four temporal con-volution neural networks followed by a global (sta-tistical) pooling layer and then two fully connectedlayers.
the input to the model is mfccs features(with 40 coefﬁcient) computed with a 25msec win-dow and 10ms frame-rate from the 16khz audio.
the model is trained on voxceleb1 (nagrani et al.,2017) development set (containing 1, 211 speakersand ≈ 147k utterances).
more details can be foundin shon et al.
(2018); chowdhury et al.
(2020a)..for speaker veriﬁcation, we use veriﬁedsame/different-speaker pairs of speech segmentsas input.
we extract the length normalized embed-.
dings from the last layer of the sr model and thencomputed the cosine similarity between pairs..for our evaluation, we constructed these veriﬁ-cation pair trials by randomly picking up 40k ut-terance pairs from: (i) speakers of the same gender;(ii) similar utterance lengths; and (iii) a balanceddistribution between positive and negative targets12.
for this, we use the most frequent 20 anchor and20 guest speakers data subset described in table 8.we then compare the equal error rate (eer) ofthe model, reported in table 9, using the designedveriﬁcation pairs based on a particular job role, ortheir combination.
in addition, we also report theresults on voxceleb1 ofﬁcial veriﬁcation testset asa reference..from the results, we observe that the sr modeleffectively distinguishes between the positive andnegative pairs with ≈ 70% (a) - 72% (g) accuracy.
comparing the eer, we notice that it is harder todifferentiate between anchors than guests.
this canbe due to the fact that anchors are using the sameacoustic conditions, and the current models arelearning recording conditions (chowdhury et al.,2020a) as well as speaker information..3.4 arabic dialect identiﬁcation.
to understand the dialectal nature of qasr dataset,we analyze the acoustic and lexical representationsfor 100 segments from each speaker13..to obtain the dialect labels, we run the pre-identiﬁcation models for bothtrained dialectspeech and text modality.
we address the dialectidentiﬁcation as multi-stage classiﬁcation: firstly,we predict the labels of the segments - msa vsda - and, secondly, if the label is da, we furtherpropagate the labels to detect the country of theselected speaker (i.e ﬁne-grained dialect classiﬁca-tion).
for country level evaluation, we manuallyannotate each speaker’s country label (see table 8).
for lexical modality, we use the pre-trainedqadi (abdelali et al., 2020), and for the acousticmodality, we use adi-514 (shon et al., 2017; aliet al., 2019) – as msa vs da classiﬁer – alongwith adi-1715 (shon et al., 2020) for ﬁne-grainedlabels..12the ofﬁcial veriﬁcation pairs are included as a part of.
13we used the same speaker set as the sv task.
14https://github.com/swshon/dialectid_.
qasr..e2e.
11the style can vary based on language ﬂuency, speech rate,.
use of different dialects among other factors..15https://github.com/swshon/arabic-dialect-identification.
2281we observe that in both the modalities, 50%of the anchors speak msa in 70% of the time inspeech and 90% of the time in text.
as for the other50%, we notice that using the dialect identiﬁcationmodules, we can detect only 20% of the speaker’snationality correctly.
the aforementioned obser-vations are pre-anticipated, as anchors are profes-sionally trained to speak mostly in msa, making itharder for the model to predict the correct countrylabel.
this also explains why the large portion ofthe data is msa..as for guest speakers, we notice that the lexi-cal classiﬁer detected that 30% of the speakers usemsa, while 70% of the speakers were detected asda.
as for the acoustic models, we notice that allspeakers use dialects more than 70% of the time.
comparing the accuracy of identifying the correctdialects based on annotated country labels, we no-tice that both the text and acoustic models performcomparatively better in identify the guest speakers’country – 64% from text and 65% from acoustic.
our hypothesis for such increase in performanceis that guest speakers, unlike the anchors, mostlyspeak using their dialects, making it easier for themodel to infer their country..when comparing the decision from both modali-ties, we notice that there is an agreement of 67.5%(65% for anchor and 70% for guest speakers) formsa/da classiﬁcation.
most of the classiﬁcationerrors in speech and text dialect identiﬁcation mod-els are due to confusion between dialects spokenin neighboring countries; e.g.
syria and lebanonin the levantine region; tunisia and algeria in thenorth african region..3.5 named entity recognition (ner).
ner is essential for a variety of nlp applicationssuch as information extraction and summarization.
there are many researches on arabic ner fornews articles, e.g.
anercorp (benajiba and rosso,2008) and microblogs (darwish, 2013).
however,we are not aware of any studies or datasets for nerin arabic news transcription, which can be usefulfor applications like video search.
we manually an-notate and revised the mgb-2 testset for basic netypes, namely person (per), location (loc), or-ganization (org) and others (oth/misc) follow-ing the guidelines in (benajiba and rosso, 2008).
the testset (70k words) along with ner annota-tion is available as part of qasr.
from the anno-tation, we observed nes are 7% of the corpus andtheir distribution is as follows: per= 32%, loc=.
typeperlocorgoverall.
anercorp.
p87.092.381.488.7.r77.787.866.080.3.f182.190.072.984.3.qasrr51.288.119.167.5.f156.487.220.869.8.p62.886.422.872.2.table 10: ner results: precision (p), recall (r) andf1 on two testsets..46%, org= 18% and oth= 5%16..we test the publicly available arabic farasaner on our new testset and compare performancewith the standard news testset (anercorp).
re-sults are listed in table 10. as shown, testing neron transcribed speech has lower f1 by 15% com-pared to testing on a standard news testset (from84.3% to 69.8%).
we anticipate that characteris-tics of speech transcription described in section2.3 affected ner negatively17.
we keep enhancingner for speech transcription for future work..4 conclusion.
in this paper, we introduce a 2, 000 hours tran-scribed arabic speech corpus, qasr.
we reportresults for automatic speech recognition, arabic di-alect identiﬁcation, speaker veriﬁcation, and punc-tuation restoration to showcase the importance andusability of the dataset.
qasr is also the ﬁrst ara-bic speech-nlp corpus to study spoken modernstandard arabic and dialectal arabic.
we report forthe ﬁrst time named entity recognition in arabicnews transcription.
the 11, 092 unique speakerspresent in qasr can be used to study turn-takingand speaker dynamics in the broadcast domain.
the corpus can also be useful for unsupervisedmethods to select speaker for text to speech (galle-gos et al., 2020).
the qasr is publicly availablefor the research community..acknowledgements.
this work was made possible with the collabora-tion between qatar computing research institute,hbku and aljazeera media network.
this data ishosted on arabicspeech portal18, which is a com-munity based effort that runs for the beneﬁt of ara-bic speech science and technologies..16anercorp contains 150k words.
nes are 11%.
distri-(cid:13)bution: per= 39%, loc= 30%, org= 21%, misc= 10%.
17disﬂuency example: (cid:63) (cid:9)(cid:224)(cid:241)(cid:106)(cid:16)(cid:74)(cid:130)(cid:16)(cid:29) (cid:66)(cid:64) (cid:73)(cid:46) (cid:74)(cid:10)(cid:163).
(cid:16)(cid:73)(cid:130)(cid:28)(cid:10)(cid:203).
(cid:13)(cid:64) (cid:213)(cid:16)(cid:230)(cid:9)(cid:75).
(cid:9)(cid:230)(cid:170)(cid:75)(cid:10).
(cid:13)(cid:64) (cid:250)(cid:10).
(ok you are isn’t it i mean are you not ashamed?).
18https://arabicspeech.org/.
2282ethical concern and social impact.
user privacy.
qasr dataset only includes programs that havebeen broadcast by the aljazeera news media.
noadditional identity of the guest is revealed in thedata, which was made anonymous in the originalprogram.
however, in the future, if any concernis raised for a particular content, we will complyto legitimate concerns by removing the affectedcontent from the corpus..biases in qasr.
any biases found in the dataset are unintentional,and we do not intend to do harm to any groupor individual.
the bias in our data, for exampletowards a particular gender is unintentional and is atrue representation of the programs.
we do addressthese concerns by collecting examples from bothparties before any general suggestion..as for the assigned annotation label, we followa well-deﬁned schema and available informationto perceive a ﬁnal label.
for e.g.
gender label –male/female is perceived from the data and mightnot be a true representative of the speakers’ choice..potential misuse.
we request the research community to be awarethat our dataset can be used to misuse quotes forthe speakers for political or other gain.
if suchmisuse is noticed, human moderation is encouragedin order to ensure this does not occur..references.
ahmed abdelali, kareem darwish, nadir durrani, andhamdy mubarak.
2016. farasa: a fast and furiousin proceedings of the 2016segmenter for arabic.
conference of the north american chapter of the as-sociation for computational linguistics: demonstra-tions, pages 11–16..ahmed abdelali, hamdy mubarak, younes samih,sabit hassan, and kareem darwish.
2020. arabicarxiv preprintdialect identiﬁcation in the wild.
arxiv:2005.06557..muhammad abdul-mageed, chiyu zhang, houdabouamor, and nizar habash.
2020. nadi 2020: theﬁrst nuanced arabic dialect identiﬁcation sharedtask.
arxiv preprint arxiv:2010.11334..tanvirul alam, akib khan, and firoj alam.
2020.punctuation restoration using transformer modelsin proceed-for resource-rich and-poor languages.
ings of the sixth workshop on noisy user-generatedtext (w-nut 2020), pages 132–142..ahmed ali, peter bell, james glass, yacine messaoui,hamdy mubarak, steve renals, and yifan zhang.
2016. the mgb-2 challenge: arabic multi-dialectbroadcast media recognition.
in 2016 ieee spokenlanguage technology workshop (slt), pages 279–284. ieee..ahmed ali, shammur chowdhury, mohamed aﬁfy,wassim el-hajj, hazem hajj, mourad abbas, amirhussein, nada ghneim, mohammad abushariah,and assal alqudah.
2021. connecting arabs: bridg-ing the gap in dialectal speech recognition.
commu-nications of the acm, 64(4):124–129..ahmed ali, suwon shon, younes samih, hamdymubarak, ahmed abdelali, james glass, steve re-nals, and khalid choukri.
2019. the mgb-5 chal-lenge: recognition and dialect identiﬁcation of di-in ieee automatic speechalectal arabic speech.
recognition and understanding workshop (asru),pages 1026–1033.
ieee..ahmed ali, stephan vogel, and steve renals.
2017.speech recognition challenge in the wild: arabicin ieee automatic speech recognitionmgb-3.
and understanding workshop (asru), pages 316–322. ieee..wissam antoun, fady baly, and hazem hajj.
2020.arabert: transformer-based model for arabic lan-guage understanding.
in lrec 2020 workshop lan-guage resources and evaluation conference 11–16may 2020, page 9..rosana ardila, megan branson, kelly davis, michaelhenretty, michael kohler, josh meyer, reubenmorais, lindsay saunders, francis m tyers, andgregor weber.
2019. common voice: a massively-arxiv preprintmultilingualarxiv:1912.06670..speech corpus..peter bell, mark jf gales, thomas hain, jonathan kil-gour, pierre lanchantin, xunying liu, andrew mc-parland, steve renals, oscar saz, mirjam wester,et al.
2015. the mgb challenge: evaluating multi-in 2015 ieeegenre broadcast media recognition.
workshop on automatic speech recognition and un-derstanding (asru), pages 687–693.
ieee..yassine benajiba and paolo rosso.
2008. arabicnamed entity recognition using conditional randomﬁelds.
in proc.
of workshop on hlt & nlp withinthe arabic world, lrec, volume 8, pages 143–153.
citeseer..norbert braunschweiler, mark jf gales, and sabinebuchholz.
2010. lightly supervised recognition forautomatic alignment of large coherent speech record-ings.
in eleventh annual conference of the interna-tional speech communication association..chung-cheng chiu, tara n sainath, yonghui wu, ro-hit prabhavalkar, patrick nguyen, zhifeng chen,anjuli kannan, ron j weiss, kanishka rao, eka-terina gonina, et al.
2018. state-of-the-art speechinrecognition with sequence-to-sequence models.
2018 ieee international conference on acoustics,speech and signal processing (icassp), pages4774–4778.
ieee..2283shammur a chowdhury, ahmed ali, suwon shon, andjames glass.
2020a.
what does an end-to-end di-alect identiﬁcation model learn about non-dialectalinformation?
proc.
interspeech 2020, pages 462–466..shammur a chowdhury, younes samih, mohamed el-desouki, and ahmed ali.
2020b.
effects of dialec-tal code-switching on speech modules: a study us-ing egyptian arabic broadcast speech.
proc.
inter-speech..shammur absar chowdhury, amir hussein, ahmedabdelali, and ahmed ali.
2021. towards one modelto rule all: multilingual strategy for dialectal code-switching arabic asr.
arxiv:2105.14779..shammur absar chowdhury, evgeny stepanov,morena danieli, and giuseppe riccardi.
2017.functions of silences towards information ﬂowthein spoken conversation.
workshop on speech-centric natural languageprocessing, pages 1–9..in proceedings of.
shammur absar chowdhury, evgeny a stepanov,morena danieli, and giuseppe riccardi.
2019. au-tomatic classiﬁcation of speech overlaps: featurerepresentation and algorithms.
computer speech &language, 55:145–167..christopher cieri, david miller, and kevin walker.
2004. the ﬁsher corpus: a resource for the nextgenerations of speech-to-text.
in lrec, volume 4,pages 69–71..kareem darwish.
2013. named entity recognition us-ing cross-lingual resources: arabic as an example.
in proceedings of the 51st annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1558–1567..chenpeng du, hao li, yizhou lu, lan wang, andyanmin qian.
2021. data augmentation for end-to-end code-switching speech recognition.
in2021 ieee spoken language technology workshop(slt), pages 194–200.
ieee..pilar oplustil gallegos, jennifer williams, joannarownicka, and simon king.
2020. an unsupervisedmethod to select a speaker subset from large multi-speaker speech synthesis datasets.
proc.
interspeech2020, pages 1758–1762..bj¨orn gamb¨ack and amitava das.
2016. comparingthe level of code-switching in corpora.
in lrec.
pegah ghahremani, bagher babaali, daniel povey,korbinian riedhammer, jan trmal, and sanjeevkhudanpur.
2014. a pitch extraction algorithmtuned for automatic speech recognition.
in ieee in-ternational conference on acoustics, speech and sig-nal processing (icassp), pages 2494–2498.
ieee.
sabit hassan, hamdy mubarak, ahmed abdelali, andkareem darwish.
2021. asad: arabic social me-dia analytics and understanding.
in proceedings ofthe 16th conference of the european chapter of theassociation for computational linguistics: systemdemonstrations, pages 113–118..amir hussein, shinji watanabe, and ahmed ali.
2021. arabic speech recognition by end-to-end,.
modular systems and human.
arxiv:2101.08454..arxiv preprint.
sameer khurana and ahmed ali.
qcri advancedtranscription system (qats) for the arabic multi-dialect broadcast media recognition: mgb-2 chal-lenge.
training, 1200(2214):370k..tom ko, vijayaditya peddinti, daniel povey, and san-jeev khudanpur.
2015. audio augmentation forspeech recognition.
in sixteenth annual conferenceof the international speech communication associ-ation..xinxing li and edward lin.
2020. a 43 languagemultilingual punctuation prediction neural networkmodel.
proc.
interspeech 2020, pages 1067–1071..hamdy mubarak, ahmed abdelali, hassan sajjad,younes samih, and kareem darwish.
2019. highlyeffective arabic diacritization using sequence to se-quence modeling.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 2390–2395..hamdy mubarak and kareem darwish.
2014. au-tomatic correction of arabic text: a cascadedin proceedings of the emnlp 2014approach.
workshop on arabic natural language processing(anlp), pages 132–136..hamdy mubarak, kareem darwish, and ahmed abde-lali.
2015. qcri@ qalb-2015 shared task: correctionof arabic text for native and non-native speakers’ er-rors.
in proceedings of the second workshop on ara-bic natural language processing, pages 150–154..arsha nagrani, joon son chung, and andrew zisser-man.
2017. voxceleb: a large-scale speaker identiﬁ-cation dataset.
arxiv preprint arxiv:1706.08612..handbook of natural.
joseph olive, caitlin christianson, and john mccary.
language process-2011.ing and machine translation: darpa global au-tonomous language exploitation.
springer science& business media..david s pallett.
2003. a look at nist’s benchmark asrtests: past, present, and future.
in 2003 ieee work-shop on automatic speech recognition and under-standing (ieee cat.
no.
03ex721), pages 483–488.
ieee..vassil panayotov, guoguo chen, daniel povey, andsanjeev khudanpur.
2015. librispeech: an asr cor-pus based on public domain audio books.
in 2015ieee international conference on acoustics, speechand signal processing (icassp), pages 5206–5210.
ieee..daniel s park, william chan, yu zhang, chung-chengchiu, barret zoph, ekin d cubuk, and quoc v le.
2019. specaugment: a simple data augmentationmethod for automatic speech recognition.
proc.
in-terspeech, pages 2613–2617..arfath pasha, mohamed al-badrashiny, mona t diab,ahmed el kholy, ramy eskander, nizar habash,manoj pooleery, owen rambow, and ryan roth..2284shinji watanabe, takaaki hori, shigeki karita, tomokijiro nishitoba, yuya unno, nelson-hayashi,enrique yalta soplin, jahn heymann, matthewwiesner, nanxin chen, et al.
2018. espnet: end-to-end speech processing toolkit.
proc.
interspeech,pages 2207–2211..jane wottawa, marie tahon, apolline marin, and nico-las audibert.
2020. towards interactive annotationin lrecfor hesitation in conversational speech.
2020..wayne xiong, jasha droppo, xuedong huang, frankseide, mike seltzer, andreas stolcke, dong yu, andgeoffrey zweig.
2016. achieving human parity inconversational speech recognition.
arxiv preprintarxiv:1610.05256..2014. madamira: a fast, comprehensive tool formorphological analysis and disambiguation of ara-bic.
in lrec, volume 14, pages 1094–1101.
cite-seer..sheena christabel pravin and m palanivelan.
2021. ahybrid deep ensemble for speech disﬂuency classi-ﬁcation.
circuits, systems, and signal processing,pages 1–28..younes samih, mohammed attia, mohamed eldes-ouki, ahmed abdelali, hamdy mubarak, laurakallmeyer, and kareem darwish.
2017. a neuralarchitecture for dialectal arabic segmentation.
inproceedings of the third arabic natural languageprocessing workshop, pages 46–54..george saon, gakuto kurata, tom sercu, kartik au-dhkhasi, samuel thomas, dimitrios dimitriadis,xiaodong cui, bhuvana ramabhadran, michaelpicheny, lynn-li lim, et al.
2017. english conversa-tional telephone speech recognition by humans andmachines.
arxiv preprint arxiv:1703.02136..suwon shon, ahmed ali, and james glass.
2017. mit-qcri arabic dialect identiﬁcation system for thein 20172017 multi-genre broadcast challenge.
ieee automatic speech recognition and under-standing workshop (asru), pages 374–380.
ieee..suwon shon, ahmed ali, younes samih, hamdyadi17: amubarak, and james glass.
2020.ﬁne-grained arabic dialectidentiﬁcation dataset.
in icassp 2020-2020 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 8244–8248.
ieee..suwon shon, najim dehak, douglas reynolds, andjames glass.
2019. mce 2018: the 1st multi-targetspeaker detection and identiﬁcation challenge evalu-ation.
arxiv preprint arxiv:1904.04240..suwon shon, hao tang, and james glass.
2018.text-speakerframe-levelrecognition and analysisindependentspeakerin 2018 ieee spokenof end-to-end model.
language technology workshop (slt), pages1007–1013.
ieee..embeddings.
for.
temple f smith, michael s waterman, et al.
1981.identiﬁcation of common molecular subsequences.
journal of molecular biology, 147(1):195–197..anshuman tripathi, han lu, and hasim sak.
2020.end-to-end multi-talker overlapping speech recogni-tion.
in icassp 2020-2020 ieee international con-ference on acoustics, speech and signal processing(icassp), pages 6129–6133.
ieee..j¨orgen valk and tanel alum¨ae.
2020. voxlingua107:a dataset for spoken language recognition.
arxivpreprint arxiv:2011.12998..changhan wang, morgane rivi`ere, ann lee, annewu, chaitanya talnikar, daniel haziza, marywilliamson, juan pino, and emmanuel dupoux.
voxpopuli: a large-scale multilingual2021.speech corpus for representation learning, semi-arxivsupervised learning and interpretation.
preprint arxiv:2101.00390..2285