modeling language usage and listener engagement in podcasts.
sravana reddyspotifysravana@spotify.com.
mariya lazarovaspotifymimiglazarova@gmail.com.
yongze yuspotifyyongzey@spotify.com.
rosie jonesspotifyrjones@spotify.com.
abstract.
while there is an abundance of popular writ-ing targeted to podcast creators on how tospeak in ways that engage their listeners, therehas been little data-driven analysis of pod-casts that relates linguistic style with listenerengagement.
in this paper, we investigatehow various factors – vocabulary diversity, dis-tinctiveness, emotion, and syntax, among oth-ers – correlate with engagement, based onanalysis of the creators’ written descriptionsand transcripts of the audio.
we build mod-els with different textual representations, andshow that the identiﬁed features are highly pre-dictive of engagement.
our analysis tests pop-ular wisdom about stylistic elements in high-engagement podcasts, corroborating some as-pects, and adding new perspectives on others..1.introduction.
what makes a particular podcast broadly engaging?
as a media form, podcasting is new enough thatsuch questions are only beginning to be understood(jones et al., 2021).
websites exist with adviceon podcast production, including language-relatedtips such as reducing ﬁller words and disﬂuencies,or incorporating emotion, but there has been littlequantitative research into how aspects of languageusage contribute to overall listener engagement..this paper investigates the linguistic factors thatcorrelate with engagement, leveraging the writtendescriptions of the parent show and episode as wellas the transcript of the audio.
our metric of en-gagement is stream rate, which we deﬁne as theproportion of ﬁrst-time listeners – of those whohave begun streaming the episode – who listen forat least ﬁve minutes.
notably, stream rate is dif-ferent from the metric of popularity as given bythe raw number of streams; the latter is inevitablyinﬂuenced by factors unrelated to the content, suchas the host or publisher reputation, publicity, expo-.
sure in recommendations and search engines, andtime of publication, whereas a listener’s decisionto continue listening for as long as ﬁve minutes islikely to be inﬂuenced by the content..we perform a series of descriptive tests to ex-amine differences in language usage between highand low engagement podcasts, and build predictivemodels.
our tests show that while much of theconventional wisdom on engaging podcasting style(such as to use positive language) bears out in thedata, other assumptions (such as to speak slowly)are contradicted and deserve a closer look.
we ﬁndthat stylistic features tend to be more correlatedwith engagement for podcasts with low absolutenumbers of streams than for the most popular pod-casts, suggesting that listeners may be less sensitiveto style in podcasts made by well-known creators.
we also identify those linguistic factors that corre-late with our engagement metric across the popular-ity spectrum, and those that are limited to podcastswithin a certain popularity range..our predictive models prove that stylistic fac-tors alone play a signiﬁcant role in determining ifa podcast has high or low engagement, achievingan accuracy of 72% in distinguishing between veryhigh engagement (top 25% of podcasts by streamrate in the corpus) and very low engagement (bot-tom 25%) examples.
we also show that the overalltextual information in podcasts is highly predictiveof engagement in this experiment, with an accu-racy as high as 81%.
to understand how style inpodcasts compares to other spoken media, we ap-ply our analysis to a corpus of ted talks.
finally,we manually examine the highest engagement pod-casts in our dataset to characterize their content..2 related work.
content-basedpodcast recommendationsyang et al.
(2019) model transcripts with a topic.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages632–643august1–6,2021.©2021associationforcomputationallinguistics632model, and the audio with a representation theytrained to predict the non-textual attributes ofseriousness and energy.
they ﬁnd that combiningthese representations improves over the purelytopic based model on popularity prediction.
thiswork indicates that stylistic attributes are importantfactors, and raises the question of whether stylisticfeatures derived from the text are valuable aswell.
tsagkias et al.
(2010) develop a frameworkcontaining a set of attributes, and compare theproportions of these attributes relative to engage-ment on itunes.
our work follows a similar spirit,but we address some limitations of their study,namely, they use a small set of podcasts (250), andmanually annotate the attributes for every podcastrather than deriving them from the raw data.
sincewe derive all features automatically, we limitourselves to concrete, easily quantiﬁable features,whereas the above paper considers higher levelattributes like ‘one topic per episode’ or ‘ﬂuent’..predicting performance from language pre-vious research in natural language processinghas explored the connections between textual fea-tures and audience engagement in books (ganji-gunte ashok et al., 2013; maharjan et al., 2018),youtube (kleinberg et al., 2018), news (naseriand zamani, 2019), ted talks (tanveer et al.,2018), and tweets (tan et al., 2014; lampos et al.,2014).
other works have modeled the relation-ship between text and various performance met-rics such as movie quote memorability (danescu-niculescu-mizil et al., 2012), forecasting ability(zong et al., 2020), congressional bill survival(yano et al., 2012), success of job interviews (naimet al., 2016), and impact of academic papers (yo-gatama et al., 2011; li et al., 2019), in addition tothe entire ﬁeld of sentiment and opinion mining ofdata such as user reviews (pang et al., 2002)..3 dataset construction.
the spotify podcast dataset (clifton et al., 2020;jones et al., 2020) is a recently released corpusof over 100, 000 podcast episodes, mostly in en-glish, that are transcribed with google’s speechto text commercial speech recognition, reportedin the paper to have an 18% word error on pod-casts.
a podcast, also known as a ‘show’ in thedataset, is a collection of episodes.
in addition tothe speech transcripts, the textual information as-sociated with each podcast episode includes thetitle and description of the episode and the parent.
show (table 1).
in this paper, we consider descrip-tions and transcripts as the text representation ofan episode.
all textual data was normalized andpart-of-speech tagged with spacy.1.
3.1 ads and promotions.
since many episode descriptions contain promo-tions, advertisements, and show notes, which areextraneous to the main content of the podcast, weremove such material before analysis (althoughwe also measure the amount of ad content as afeature).2 promotional and extraneous materialwas detected by the classiﬁer described by reddyet al.
(2021), a model using bert with a classiﬁ-cation head, trained on a manually annotated set ofepisode descriptions.
this classiﬁer is reported tohave a sentence classiﬁcation accuracy of 95% onepisode descriptions..3.2 engagement metric.
we obtained streaming numbers for the episodesin the corpus from spotify, a music and podcaststreaming platform.
the numbers were aggregatedfrom the date of the episode’s publication on theplatform until december 2020. since the mostrecently published episode in the dataset is fromfebruary 2020, all episodes had several months ofexposure by the time of collection..we speciﬁcally consider streaming by ‘ﬁrst-timelisteners’ who are not already familiar with theshow, i.e., those who have not previously streamedany other episode of that show for more than ﬁveminutes.
listeners who are familiar with the showthrough other episodes are ignored since they maybe habituated and primed for the content.
as de-scribed in the introduction, we use stream rate asthe engagement metric, deﬁned as the proportion ofthe show’s ﬁrst-time listeners who stream at leastﬁve minutes of the episode.
stream rate in thedataset shows a weak but statistically signiﬁcant in-verse rank correlation with popularity (spearman’sρ = −0.12, p < 0.001).
this may be because pop-ular podcasts attract more listeners who may realizethey are not interested in the content soon after theybegin streaming, while the listeners of less popu-lar podcasts may have actively sought them out.
70% stream rate in a well-known podcast which.
1spacy.io (honnibal et al., 2020), with the large en-.
glish web model, en core web lg v.2.3.1..2initial experiments showed weaker effects of stylisticfeatures on engagement when such extraneous content wasincluded in the analysis..633show titleshow description.
episode titleepisode description.
automatic transcript.
witch wednesdaysa weekly podcast covering all things witchcraft in the modern world.
join us, two best friends and midwestern witches (one wiccan,one not), as we dive into all things witchy.
we’re starting at the beginning, making this podcast a great resource for newbies...episode 1 - what you’re in for this yearhappy new year!
welcome to witch wednesdays!
join us every wednesday morning for all things witch and witchcraft.
in thisﬁrst episode, we’re introducing ourselves and this podcast so you can get an idea about what you’re getting yourself into this year...you’re listening to which wednesday’s your weekly podcast source for all things witchcraft in the modern world.
join your hoststephen tara every wednesday morning that they dive into a new wiki topic.
hello and welcome to the very ﬁrst episode of whichwednesdays.
i’m steph and i’m terrell and together will be co-hosting this podcast adventure this year rather than....table 1: example of textual content (truncated) associated with a podcast in the dataset..would have attracted a broad array of listeners isnot comparable to 70% stream rate in a relativelyunknown podcast.
therefore, we bin the datasetinto popularity quartiles for analysis on stream rate,which is found to be uncorrelated with popularitywithin each quartile.
stream rate is uncorrelatedwith the time of publication..3.3 filters.
we ﬁlter out all episodes that are shorter than tenminutes and fewer than a threshold number of to-tal streams.
to control for duration effects in theanalysis of transcripts, we truncate transcripts atten minutes.
the original podcast corpus containsmultiple episodes for many of the show while othershow have only one episode.
we select the most-streamed episode from each show as its represen-tative, thereby ensuring that every show is repre-sented by a single episode in the data.
this is doneso that shows with several episodes do not have anoutsize inﬂuence on the models..since the original corpus is an english-languagecollection, all of our analysis is constrained toenglish, and we ﬁlter out any stray examples inthe corpus that are detected as non-english afterrunning language identiﬁcation (lui and baldwin,2011) on the descriptions.
the resulting datasethas 5371 episodes..3.4 topics and genre.
the norms of language usage may vary depend-ing on the genre and topics being discussed.
forexample, technical podcasts are expected to con-tain more complex language compared to chit-chat,crime podcasts to contain words with negative sen-timents as opposed to motivational podcasts, andso on.
the rss feed of a podcast show containsone or more categories selected by the creatorsfrom the apple itunes taxonomy; however, theseare unreliable, since many of the categories are am-biguous or ill-deﬁned, (e.g.
‘leisure’ which mainlyincludes gaming podcasts but also general leisuretopics, ‘kids & family’ which includes podcasts.
genremysterymusicinvestingworking outentertainmentadcultureeducationgamingfoodtvharry pottercareersportsbiologycrimelanguageastronomyﬁllers 1ﬁllers 2effusiveness.
words in topicdoor, eye, room, hand, head, night, face, away, lookedsong, music, album, artist, listen, love, record, hip, hopmarket, company, stock, investment, investor, tradetraining, gym, ﬁtness, coach, workout, muscle, bodyjacob, alice, edward, vampire, max, bella, hamilton, johnfree, episode, app, download, podcasts, listen, placeworld, sort, idea, human, interesting, sense, fact, societyschool, student, class, teacher, college, high, kid, gradegame, play, playing, new, nintendo, stuff, played, switchfood, eat, coffee, drink, chicken, restaurant, beer, tasteepisode, character, show, scene, season, end, pointharry, mr, potter, charlie, ron, fred, hermione, professorjob, company, team, working, career, industry, experienceworld, team, australia, cup, ﬁnal, club, week, playercell, dna, bond, virus, geneticmurder, police, crime, case, found, death, killerword, language, english, spanish, use, learn, speakspace, science, earth, planet, light, solar, scientist, staryeah, oh, okay, yes, exactly, gonna, feel, guess, sure, coolfeel, stuff, still, never, went, remember, thought, whateverlove, great, thank, different, amazing, bit, awesome.
table 2: some examples of lda topics.
the genrelabels are manually assigned only to aid interpretation..for kids as well as about parenting), and podcastcreators may not always select the most appropriatecategories (sharpe, 2020).
furthermore, podcastsspan multiple themes and structures, making theassignment of one or two categories per podcasttoo restrictive..instead, we ﬁt an lda topic model (blei et al.,2003) with 100 topics3 to transcripts of the entire100k podcast corpus as in previous works (cliftonet al., 2020; yang et al., 2019), represent eachepisode by the topic distribution, and measure topicproportions relative to the target metrics in order tocontextualize our results on stylistic features.
table2 shows a sample of the inferred topics..4 linguistic features.
we deﬁne a set of explainable linguistic featuresthat are hypothesized to affect engagement.
thesefeatures have been drawn from different podcastingadvice blogs, alongside some of our own intuitions..length descriptions are known to be importantfor listeners on their ﬁrst encounter with the pod-.
3the number of topics is selected by optimizing for topiccoherence as implemented by the coherence model in thegensim toolkit ( ˇreh˚uˇrek and sojka, 2010)..634cast.
we also measure audio duration, since surveysshow it is a consideration (mclean, 2020)..proportion of ads and show notes descriptionsof well-known podcasts tend to contain advertise-ments of other podcasts made by the same network,links to the hosts’ or guests’ social media pres-ence and websites, or show notes and transcripts,and podcast creators are often advised to includesuch information (dennis, 2020), and surveys haveshown that the majority of podcast listeners do notmind sponsor ads in the content (mclean, 2020).
we measure the the proportion of text detected onepisode descriptions by the extraneous content clas-siﬁer described in §3.1.
the proportion of ads intranscripts is given by a manually identiﬁed ldatopic that corresponds to words indicative of ads..faithfulness of episode descriptions to tran-scripts length is a weak signal of informative-ness.
do listeners seem to prefer descriptions thataccurately convey the topics and synopsis of theepisode?
we measure faithfulness of the episodedescription to the ﬁrst ten minutes of the transcriptas the cosine similarity between the tf-idf bag ofwords representation of both texts.
while we do nothave ground-truth labels to evaluate this deﬁnitionof faithfulness, we assessed it to be a good heuristicby anecdotally reviewing some examples.4.
distinctiveness podcast creators are often en-couraged to develop a distinctive style (gray,2021a).
we deﬁne distinctiveness as the perplexityof the given text under a unigram language modeltrained over all the episodes in the dataset.
to con-trol for length, we follow the protocol in zhanget al.
(2019) of randomly sampling a constant num-ber of words from each text and taking the meancross entropy over a few samples.5.
‘difﬁculty’ using a lookup table.
while cautionmust be taken on interpreting reading grade levelfor transcribed speech, these measures have beenexplored for speech in prior work (schumacher andeskenazi, 2016)..vocabulary diversity we examine whether pod-cast creators of high engagement podcasts use morediverse vocabularies, quantiﬁed by the entropy ofthe unigram words in the text, motivated by adviceto avoid word repetition (bellis, 2017)..sentiment and emotion popular advice oftenencourages podcast creators to be upbeat and pos-itive (briggman, 2020).
the nrc emotion lexi-con (mohammad and turney, 2013) contains posi-tive and negative sentiment assignments, as well asemotions such as anger, trust, and fear, for 14182words.6 we measure the proportion of words as-sociated to each of the emotions and sentiments.
since a lexicon lookup for sentiment is naturallylimited in that it does not account for composi-tionality and cannot model words and variants thatare missing in the lexicon, we also apply a full-sentence classiﬁer, the sentiment model from thegoogle natural language api7.
the output of theclassiﬁer is a score between +1 and −1 for eachsentence.
we deﬁne positive and negative polaritiesfor each text as as the proportion of sentences inthe text with highly positive (over +0.5) or highlynegative (under −0.5) scores..syntax syntactic features are measured by therelative frequencies of each part-of-speech tag.
while previous work of this nature ﬁnds strongeffects of syntactic patterns from parses (ganji-gunte ashok et al., 2013), we ﬁnd that the noisyspeech transcripts result in particularly noisy parsesfrom off-the-shelf parsers..reading grade level similarly to zong et al.
(2020), we make two measurements: the flesch-kincaid grade level (flesch, 1948) that measuresthe number of syllables per word and the numberof words per sentence, and the dale-chall gradelevel (chall and dale, 1948) which measures word.
swearing and ﬁllers we conjecture that pod-casts with swearing and adult language may nothave broad appeal.
public speaking recommen-dations in podcasting guides (coips and kramer,2020) emphasize the reduction of ﬁller words like‘yeah’ or ‘okay’, and the use of professional speech..4we found that bert and related pretrained transformermodels are not well suited for this similarity estimation, possi-bly because of speech recognition errors in the transcripts.
ifground-truth faithfulness labels were available, such modelscould be trained to make accurate judgments..5the text was lightly normalized by case-folding and re-placing urls and social media handles with special tokens.
we ﬁxed the constant number of words as 100 for descriptionsand 1000 for transcripts, and sampled over 5 runs..6we experimented with the method of demszky et al.
(2019) to expand the lexicon for the domain by training gloveembeddings on the dataset, and then for each emotion, retriev-ing the words that have the highest mean cosine similarity tothe words associated with that emotion.
however, an examina-tion of the expansions for our dataset showed that they includetoo many false positives..7https://cloud.google.com/natural-language, accessed dec 2020..635we attempted to manually deﬁne lexicons of thesetypes of categories, but found that it is challengingand prone to human biases, especially given thenovel domain and automatic transcripts.
instead,we take advantage of the observation that some ofthe topics inferred by the lda model correspondto swear words and ﬁller terms, and measure theproportions of these topics..speech rate and non-speech time podcastcreators are often encouraged to speak slowly, sincenovice speakers tend to rush their delivery (gray,2021b).
since the transcripts in the dataset containtime alignments of each word, we measure the dura-tion of speech segments in the audio, giving us thespeech rate in terms of words per minute.
we alsomeasure the amount of time spent on non-speech..5 models and analysis.
5.1 group means differences.
in this section, we analyze the different linguisticfeatures by comparing group means between thetop and bottom 25% of podcasts by engagementwithin each popularity quartile (approximately 335podcasts per group) with bootstrapped welch’s t-tests.
we report the group mean differences oflda topic proportions in order to contextualizeresults on the other features.
for lda features, wenote signiﬁcance after a bonferroni correction ofα = 0.05/100, and for the other linguistic features,a bonferroni correction of α = 0.05/30..in the results, ‘description’ refers to the concate-nation of the show description and the representa-tive episode’s description.
when there is an effectfrom the show description but not the episode’s orvice versa, they are explicitly identiﬁed as such..5.1.1 genres.
among the podcasts in the top popularity quartile,high engagement is associated with topics aroundlifestyle and culture, mental health, spirituality, andcrime, while in the lower popularity quartiles, highengagement podcasts include those about investing,working out, careers, business, parenting, health,art, and relationships..5.1.2 linguistic features.
table 3 shows the features with signiﬁcant differ-ences across between the high and low engagementgroups.
we review the main takeaways from theseresults..measurement.
popularity quartile2.
3.
1 (top).
length and durationaudio durationnon-speech time in ﬁrst 10 minlength of descriptionsproportion of adsepisode descriptiontranscriptfaithfulness of descriptiondistinctivenessdescriptionstranscriptreading grade leveldescriptions: flesch-kincaiddescriptions: dale-challtranscript: flesch-kincaidtranscript: dale-challvocabulary diversitydescriptionstranscriptword-level sentiment and emotionpositive sentiment in transcripttrust in descriptionstrust in transcriptjoy in transcriptanticipation in descriptionsanticipation in transcriptsurprise in transcriptnegative sentiment in descriptionsnegative sentiment in transcriptfear in descriptionsfear in transcriptsadness in transcriptanger in transcriptdisgust in transcriptsentence-level sentimentpositive in descriptionspositive in transcriptnegative in descriptionsnegative in transcriptsyntaxadjectives in descriptionsadpositions in transcriptadverbs in descriptionsadverbs in transcriptconjunctions in transcriptdeterminers in transcriptinterjections in transcriptnouns in descriptionsnouns in transcriptpronouns in descriptionspronouns in transcriptparticles in transcriptproper nouns in transcriptpunctuation in descriptionsswearing and ﬁllers in transcriptsswearingfillersspeech rate.
↑↓↑.
↑↓↑.
↓↓.
↑↑.
↑↑.
↑.
↑.
↑.
↓.
↓↑↓↑↑↑↓↓.
↓.
↓↑.
↓.
↑.
4.
↑↓↑.
↑.
↓↓.
↑↑↑↑.
↑↑.
↑↑↑↑↑↑.
↓↑↓.
↓↓.
↑↓↓.
↓↑↓↑↑↑↓↓↑.
↓.
↑↑.
↓↓↑.
↑↓↑.
↓↑.
↓↓.
↑↑↑↑.
↑↑.
↑↑.
↓↓↓↑↓↓↓↓.
↑↑↓↓.
↓↑.
↑↑↑↓.
↓.
↑↓↑.
↓.
↑.
↑↓↑.
↓↑.
↓↓.
↑↑↑↑.
↑↑.
↑.
↑.
↓.
↓.
↓↓↓↓.
↑↓↓.
↓↑↓↑↑↑↓↓↑.
↑↓↑.
↓.
↑.
table 3: group mean differences between linguisticfeatures of high and low engagement podcasts in eachpopularity quartile, with the ↑ (↓) arrow indicating in-crease (decrease) in mean value of the feature for thehigh group compared to the low.
differences thatare not signiﬁcant after a bonferroni correction (p <0.05/30 for linguistic features, p < 0.05/100 for lda)are left blank..high engagement podcasts are longer, and haveappropriate descriptions across all quartiles,podcasts with high engagement tend to be longeron the whole (contrary to advice to keep episodesshort), and contain less non-speech in the ﬁrst tenminutes than the low engagement group.
they.
636also have descriptions that are more similar to theﬁrst ten minutes of the transcripts, which may bebecause long, faithful descriptions better preparelisteners for the episode..the correlation between ads and engagementis mixed large amounts of ads in transcripts areassociated with lower engagement in all but thebottom popularity quartile.
while this may be ex-plained by the fact that many listeners skip over adsin the audio stream (reddy et al., 2021), the effectis strong enough to indicate that ads seem to hurtengagement, even though surveys report that mostlisteners do not mind ads.
the negative associationcould be a result of our dataset being constrainedto ﬁrst-time listeners; further analysis needs to bedone to understand if it holds of returning listen-ers.
ads in episode descriptions, on the other hand,do not hurt engagement on the whole, and in fact,are associated with higher engagement in the topquartile, likely because much of the detected ‘ad’content in popular podcasts consists of promotionalmaterial about the podcast itself, which often in-cludes useful information such as links to the hosts’websites and show notes..high engagement podcasts tend to use diverseand mainstream language vocabulary diversityin descriptions and transcripts is consistently largerin the high engagement group, as is reading gradelevel.
high engagement podcasts have more punc-tuation in their descriptions and more conjunctions(arising from the use of long sentences), adverbs,adpositions, and determiners in their transcripts.
these syntactic features correlate with the topicssuch as culture, mental health, investing, and art..at the same time, surprisingly, high engagementpodcasts use less distinctive language comparedto the rest of the corpus than the low engagementgroup.
on closer examination, we ﬁnd that pod-casts scoring low on reading grade level also scorehigh on distinctiveness..high engagement podcasts tend to contain pos-itive sentiments and suspense on the whole,high engagement is associated with more positiveand less negative emotions and sentiment.
this re-lationship is stronger outside of the top popularityquartile.
a notable exception is ‘fear’ in the toppopularity quartile, which is explained by the highengagement of popular crime-related podcasts..high engagement podcasts are less likely to con-tain interjections and swearing as expected,words such as ‘oh’, ‘right’, and ‘cool’ in contextsthat the tagger infers as interjections are signif-icantly less likely to occur in high engagementpodcasts.
similarly, swearing is associated withlow engagement.
filler words are only negativelyassociated with engagement in the lowest popular-ity quartile, though the lack of correlation in otherquartiles could be because the lda topics repre-senting ﬁllers don’t model context, and thereforedo not capture their discourse function in the waythe tagger does for interjections..high engagement podcast creators tend tospeak relatively fast while popular advicewarns presenters against rushing their speech, thedata indicates that on average, high engagement isassociated with high speech rates, which is also aﬁnding in previous work (tsagkias et al., 2010)..5.2 predictive models.
next, we build classiﬁers to automatically distin-guish high and low engagement podcasts.
the pre-diction task is treated as a balanced binary classiﬁ-cation problem.
we make a single dataset for pod-casts across all quartiles by aggregating the top andbottom k% podcasts by stream rate within eachquartile.
this aggregation is to ensure fair com-parisons of podcasts in different quartiles, since astream rate value that is considered high for a pop-ular podcast, for example, may not be so in the lowquartiles.
models are trained and evaluated withthe same stratiﬁed 5-fold cross validation splits..we train logistic regression classiﬁers using dif-ferent representations of the content: the linguisticfeatures listed previously, the non-stylistic ldatopic proportions, and bag-of-ngrams (unigramand bigram words) with tf-idf scoring.
in ad-dition, we train two neural classiﬁers – a feedfor-ward neural network with a single hidden layer,using a paragraph vector representation (le andmikolov, 2014) of the document as input8, and thepre-trained bert (devlin et al., 2019) uncased en-glish model9 with a classiﬁcation head, ﬁne-tunedon this task.
with the linguistic features, we alsoconduct an ablation study, removing one group offeatures at a time, to estimate their contributions.
8paragraph vector embeddings were trained on the descrip-.
tions and transcripts of the full 100k+ podcast corpus.
9we used the implementation in the hugging face li-brary (wolf et al., 2020), https://huggingface.co/bert-base-uncased..637featureschance.
logistic regressionwith linguisticfeatures.
feed-forward networkwith paragraph vectors.
bert.
descriptions- reading grade leveltranscript- reading grade level- part-of-speechdescriptions + transcript.
descriptionstranscriptdescriptions + transcriptdescriptionstranscriptdescriptions + transcriptdescriptionstranscriptdescriptions + transcript.
accuracy50.0066.5264.5169.2464.9965.2171.5172.4175.3575.9876.2574.3176.0377.8577.3378.5280.54.logistic regression with non stylistic lda topicslogistic regressionwith bag-of-ngrams.
table 4: accuracy of predicting whether a podcast ishigh or low engagement (top or bottom 25% by streamrate), averaged over 5 cross validation splits.
ablationresults, shown by ‘- feature group’, are included whenthere is a signiﬁcant difference of more than 1 percent-age point from the full reference feature set.
all pair-wise differences are signiﬁcant..to predictive performance.
prediction accuracies(table 4) are over 70% with linguistic features only,indicating that the features that we have identiﬁedare relatively strong predictors of engagement.
thereading grade level of descriptions and transcriptsmakes a big contribution as shown in the ablationresults, as do the syntactic features on transcripts..analysis of the weights of the bag of n-gramsmodels surface patterns in language usage that cor-roborate our analysis on linguistic features – swear-ing and negative sentiment is predictive of low en-gagement, for example.
they also suggest subtledimensions of variation to complement our set oflinguistic features.
in table 5, we collect some ofthe most predictive terms and manually group theminto classes.
first or second person pronouns arepredictive of high engagement in contrast to thirdperson pronouns.
this aligns with the ﬁnding bytsagkias et al.
(2010) that personal experiences arefavored in high engagement podcasts.
while ﬁllersexist in both groups, the speciﬁc terms used aredifferent, with ‘kind of’ and ‘literally’ being pre-dictive of high engagement in contrast to ‘um’ and‘but like’.
the conjunction ‘and’ is preferred byhigh engagement podcasts over ‘but’, and ‘so’ over‘because’.
interrogative words are more predictiveof high engagement with the exception of ‘which’,as are open-ended and future looking terms like‘asking’, ‘explore’, and ‘started’ over grounded,immediate terms like ‘make’, ‘use’, ‘today’, and‘quickly’.
we emphasize that this is a small quali-tative analysis of the most predictive features, and.
low engagementhe, she, they, his, her, him, it.
um, gonna, oh, like like, becauselike, but like, such as, okay, all right,you guys, basicallybut, becausewhichallsayscan, cannotmake, usetoday, still, quickly.
high engagementme, you, us, we, my, our, their, my-self, someoneand and, sort of, kind of, was like,you know, quite, literally.
and, sowhen, what, who, howlot of, little bitaskingwas, were, wasn’texplore, wantedalways, started, the time.
table 5: terms in descriptions and transcripts sampledfrom the top 200 unigrams and bigrams that are highlypredictive of engagement.
the terms are manually ar-ranged to indicate contrasting usage of similar classesof words for qualitative analysis..all linguistic featuresbag-of-ngramsbert.
k=1074.7279.6683.21.k=1573.6679.0783.98.k=2071.8578.1581.36.k=2571.5176.2580.54.k=5063.3369.0368.19.table 6:classiﬁcation accuracy (using descrip-tions+transcript) tends to goes down as the gap betweenhigh and low engagement groups decreases..more work needs to done to establish which termsare actually used in semantically similar contextsin the data.
we leave explorations of computablefeatures that encode these aspects to future work..on the whole, models with lexical content fea-tures perform better than the linguistic signals,which is expected since these models encode moreinformation than a small set of hand-designed fea-tures.
the bert classiﬁers achieve nearly 81%accuracy, indicating that podcast content is highlypredictive of engagement..table 6 shows how classiﬁcation accuracieschange when the task is to distinguish the top andbottom k% podcasts, with k ranging from 10 to50 (all reports thus far have been with k = 25).
performance drops as k increases (and the gap be-tween the two sets thereby decreases) although theamount of training data goes up, showing that thedifferences in language usage are more predictableat the extremes of engagement..6 podcasting vs public speaking:.
modeling engagement with ted talks.
to understand how the relationship between linguis-tic features and engagement in podcasts comparesto other spoken media, we carry out the same analy-sis on a corpus of 2480 talks from the ted confer-ences (tanveer et al., 2018; acharyya et al., 2020).
while we don’t have access to the stream rate of thelectures, the data includes the total view count and.
638measurement.
popularity quartile2.
3.
1 (top).
length and durationaudio durationlength of descriptionfaithfulness of descriptiondistinctivenessdescriptiontranscriptreading grade leveltranscript: flesch-kincaidtranscript: dale-challvocabulary diversitydescriptiontranscriptword-level sentiment and emotionpositive sentiment in descriptiontrust in descriptiontrust in transcriptjoy in descriptionanger in transcriptfear in transcriptdisgust in descriptiondisgust in transcriptsadness in transcriptsyntaxadjectives in descriptionconjunctions in transcriptparticles in descriptionparticles in transcriptpronouns in descriptionpronouns in transcriptpunctuation in description.
↑↓↓.
↓.
↑↑.
↑↑.
↑↑↑.
4.
↓↓.
↑↓.
↑↑.
↑.
↑.
↓.
↓.
↑↓.
↑↓.
↓.
↑↑.
↑↑.
↑.
↑.
↓↑↓.
↑.
↓↓.
↑↓.
↓↓.
↑↑.
↑.
↓↓↓↓↓.
↓.
table 7: signiﬁcance of group mean differences be-tween linguistic features of higher and lower engage-ment (top and bottom 25%) ted talks as given by theproportion of views that left ratings.
red arrows showwhere the direction of correlation differs from podcasts..ratings.
we deﬁne engagement as the proportionof total views that left a rating, with the rationalethat the act of leaving a rating is roughly analo-gous to the podcast engagement metric of listeningfor several minutes.
another point of differencebetween this dataset and the podcasts is that theted lectures are manually transcribed.
therefore,the data is not directly comparable to the podcastdataset, but we carry out the experiment to try toidentify which features of high-engagement speechmay be universal, and which are podcast-speciﬁc.
we test the same features that we formulated forpodcasts, except for lda topic distributions (dueto the small size of the ted corpus relative to thefull 100k+ podcast data), and ads and swear wordssince these occur rarely if at all in ted talks..table 7 shows the group means differences be-tween high and low engagement lectures.
on thewhole, there are fewer signiﬁcant differences, be-cause either the ted data is more homogenousthan podcasts, the metric isn’t directly indicative ofengagement, or the features that we designed forpodcasts don’t apply as much for ted talks..featureschancelogistic regression withlinguistic features.
logistic regression withbag-of-ngrams.
bert.
descriptiontranscriptdescription + transcriptdescriptiontranscriptdescriptions + transcriptdescriptionstranscriptdescription + transcript.
accuracy50.0064.0167.9971.1567.0267.3468.4068.6766.7271.92.table 8: accuracy of predicting whether a ted talk ishigh or low engagement..tions are actually associated with lower engage-ment.
vocabulary diversity is associated with highengagement, but unlike podcasts, high engagementlectures have lower reading grade levels.
since weﬁnd that lecture transcripts measure over one gradelevel higher than podcasts, it could be that after apoint, simplicity is rewarded.
positive emotionsare more signiﬁcantly associated with engagementcompared to the podcast data, which may be be-cause of the inspirational nature of the talks andthe relative paucity of crime-related content (andin fact, positive sentiment overall is more prevalentcompared to the podcast data).
there is less varia-tion in syntactic features, possibly because talks arescripted and follow similar templates.
the syntac-tic features with correlations tend to follow similarpatterns as in podcasts..on the prediction task, we achieve up to 71.15%(table 8) accuracy using only linguistic features,similar to the performance on podcasts.
how-ever, the bag-of-ngrams features are less predictivethan linguistic features, and the bert model onlymatches the classiﬁer with linguistic features ratherthan exceeding it.
this may be because there isn’tas much variation in topical content as in podcasts..7 what does engagement favor?.
our paper centers ﬁve minute stream rate as thetarget metric for analysis and prediction.
systemsoptimized for engagement on social media plat-forms have the potential to spread misinformationand radical content (ribeiro et al., 2020), or be ma-nipulated by bad actors (sehgal et al., 2021).
onthe other side of the coin, studies have found that al-gorithms driven by engagement do not spread falsenews at a higher rate than true news (vosoughi et al.,2018), and that under certain conditions, engage-ment metrics may actually reward quality content(ciampaglia et al., 2018)..like podcasts, higher engagement lectures arelonger; however, longer and more faithful descrip-.
aggregate stream rate in podcasts is a speciﬁcengagement metric distinct from metrics and media.
639in previous studies.
there is limited previous workon engagement in podcasts.
holtz et al.
(2020)ﬁnd that algorithms driven by engagement lead toless diverse recommendations; however, that workdoes not study the relationship between the type ofcontent that is favored by the engagement metric.
while a comprehensive analysis of podcast en-gagement is beyond the scope of this work, wemanually examine the top 10% of podcast episodesby engagement in our collection, a total of 537episodes.
as we noted in §5.1.1, the lda topics as-sociated with high engagement are broad: lifestyle,mental health, spirituality, crime, investing, work-ing out, careers, business, parenting, health, art,and relationships.
our manual audit conﬁrms thathigh engagement podcast do primarily span thesetopics.
in particular, we do not ﬁnd any episodescontaining harmful content, incendiary language,or politically controversial topics in this set.
weconclude that while the connection between any ab-solute measure of intrinsic quality and engagementis unknown, high engagement in our study does notcorrespond to harmful content..8 conclusion.
this paper presents the ﬁrst quantitative analysisof how linguistic style and textual attributes inpodcasts relate to listener engagement using au-tomatically computed features.
we test severalhypotheses, and identify factors that validate pop-ular advice on podcast creation, as well as thosewith unexpected correlations.
our predictive mod-els perform well at distinguishing high and lowengagement podcasts using only textual informa-tion.
our comparison with a similar task on teddata shows similarities and differences betweenpodcasts and public lectures vis a vis engagement.
opportunities for future research include the in-vestigation of other podcast creation advice basedon paralinguistic features from the podcast audio(such as pitch and intonation), speaker identitiesand shifts within a conversation, trajectories of lin-guistic features over the course of the episode, andmodels using manual transcripts..acknowledgements.
impact statement.
since our dataset consists of a few thousand pod-casts, uses automatically generated transcripts, andonly contains podcasts from publishers owned oroperated by spotify (clifton et al., 2020), care mustbe taken when generalizing from these results todeploying automatic recommendation systems, oradvising podcast creators..it is also worth noting that aggregated engage-ment data may reﬂect the language preferencesof the dominant community, and may be biasedagainst minority cultural and linguistic subcom-munities.
while this dataset lacks self-identiﬁedlabels on demographics and sociolinguistic iden-tities, there are opportunities for future work (ineither podcasts or other media) to collect these self-identiﬁcations in order to study questions such asdisparities in automatic speech recognition perfor-mance by race or gender (koenecke et al., 2020;tatman, 2017), and whether engagement is biasedtowards certain dialects..this paper deﬁned a speciﬁc metric, namely, therate of streaming for at least ﬁve minutes; resultsrelated to this metric may or may not apply to otherengagement metrics.
as with all user data, theengagement metric is inﬂuenced by the interfaceand recommendations of the streaming platformfrom which the data was collected, and may nottranslate to other platforms, nor reﬂect an objectivenotion of listener engagement.
we also reiterate(from §7) that listener engagement must not beused as a proxy for intrinsic quality or success..it must also be emphasized that the stylistic asso-ciations that were observed to distinguish high andlow engagement podcasts in this particular datasetare correlations with no causality established, andtherefore must be interpreted with caution..references.
rupam acharyya, shouman das, ankani chattoraj,and md iftekhar tanveer.
2020. fairyted: a fair rat-ing predictor for ted talk data.
in proceedings ofthe aaai conference on artiﬁcial intelligence, vol-ume 34, pages 338–345..rich bellis.
2017. let your favorite podcast hosts ﬁxyour public speaking problems.
fast company.
ac-cessed dec 2020..we thank ann clifton, bernd huber, jussi karl-gren, mi tian, and zahra nazari for their input anddiscussions..david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..640salvador briggman.
2020.
8 important speaking andvoice tips for podcasters.
podcasting hacks.
ac-cessed dec 2020..jeanne chall and edgar dale.
1948. a formula for pre-dicting readability.
educational research bulletin,27:11–20..giovanni luca ciampaglia, azadeh nematzadeh, fil-ippo menczer, and alessandro flammini.
2018.how algorithmic popularity bias hinders or pro-motes quality.
scientiﬁc reports, 8(1):1–7..ann clifton, sravana reddy, yongze yu, aasish pappu,rezvaneh rezapour, hamed bonab, maria eske-vich, gareth jones, jussi karlgren, ben carterette,and rosie jones.
2020.
100,000 podcasts: a spo-in proceedingsken english document corpus.
of the 28th international conference on compu-tational linguistics, pages 5903–5917, barcelona,spain (online).
international committee on compu-tational linguistics..melvin coips and gwyneth kramer.
2020.
13 essentialvocal tips for podcasters for stronger voice.
improvepodcast.
accessed dec 2020..cristian danescu-niculescu-mizil, justin cheng, jonkleinberg, and lillian lee.
2012. you had me athello: how phrasing affects memorability.
in pro-ceedings of the 50th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 892–901, jeju island, korea.
associ-ation for computational linguistics..dorottya demszky, nikhil garg, rob voigt, jameszou, jesse shapiro, matthew gentzkow, and dan ju-rafsky.
2019. analyzing polarization in social me-dia: method and application to tweets on 21 massshootings.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 2970–3005, minneapolis, minnesota.
associ-ation for computational linguistics..dennis.
2020. engaging podcast content: 13 tips tocreate better content.
castos.
accessed dec 2020..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..rudolph flesch.
1948. a new readability yardstick..journal of applied psychology, 32(3):221..vikas ganjigunte ashok, song feng, and yejin choi.
2013. success with style: using writing style toin proceedings ofpredict the success of novels..the 2013 conference on empirical methods in natu-ral language processing, pages 1753–1764, seattle,washington, usa.
association for computationallinguistics..colin gray.
2021a.
how to make your podcast unique:what’s your usp?
the podcast host.
accessed jan2021..colin gray.
2021b.
i speak too fast!
how can i slowdown my podcast delivery?
the podcast host.
ac-cessed jan 2021..david holtz, ben carterette, praveen chandar, zahranazari, henriette cramer, and sinan aral.
2020.the engagement-diversity connection: evidencefrom a ﬁeld experiment on spotify.
in proceedingsof the 21st acm conference on economics and com-putation, pages 75–76..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..rosie jones, ben carterette, ann clifton, maria es-kevich, gareth jones, jussi karlgren, aasish pappu,sravana reddy, and yongze yu.
2020. overview ofin the 29th textthe trec 2020 podcasts track.
retrieval conference (trec 2020) notebook.
nist..rosie jones, hamed zamani, markus schedl, ching-wei chen, sravana reddy, ann clifton, jussi karl-gren, helia hashemi, aasish pappu, zahra nazari,longqi yang, oguz semerci, hugues bouchard, andben carterette.
2021. current challenges and futurein pro-directions in podcast information access.
ceedings of the 44th international acm sigir con-ference on research and development in informa-tion retrieval: perspectives track..bennett kleinberg, maximilian mozes, and isabellevan der vegt.
2018. identifying the sentiment stylesof youtube’s vloggers.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3581–3590, brussels, bel-gium.
association for computational linguistics..allison koenecke, andrew nam, emily lake, joenudell, minnie quartey, zion mengesha, connortoups, john r rickford, dan jurafsky, and sharadgoel.
2020. racial disparities in automated speechrecognition.
proceedings of the national academyof sciences, 117(14):7684–7689..vasileios lampos, nikolaos aletras, daniel preot¸iuc-pietro, and trevor cohn.
2014. predicting and char-in proceedingsacterising user impact on twitter.
of the 14th conference of the european chapterof the association for computational linguistics,pages 405–413, gothenburg, sweden.
associationfor computational linguistics..quoc le and tomas mikolov.
2014. distributed repre-sentations of sentences and documents.
in interna-tional conference on machine learning, pages 1188–1196. pmlr..641siqing li, wayne xin zhao, eddy jing yin, and ji-rong wen.
2019. a neural citation count predictionmodel based on peer review text.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4914–4924, hong kong,china.
association for computational linguistics..marco lui and timothy baldwin.
2011. cross-domainfeature selection for language identiﬁcation.
in pro-ceedings of 5th international joint conference onnatural language processing, pages 553–561, chi-ang mai, thailand.
asian federation of natural lan-guage processing..suraj maharjan, sudipta kar, manuel montes, fabio a.gonz´alez, and thamar solorio.
2018. letting emo-tions ﬂow: success prediction by modeling the ﬂowin proceedings of the 2018of emotions in books.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 259–265, new orleans, louisiana.
associa-tion for computational linguistics..matthew mclean.
2020. podcast discovery stats in2020: how listeners discover new shows.
the pod-cast host.
accessed dec 2020..saif m. mohammad and peter d. turney.
2013. crowd-sourcing a word-emotion association lexicon.
com-putational intelligence, 29(3):436–465..iftekhar naim, md iftekhar tanveer, daniel gildea,and mohammed ehsan hoque.
2016. automatedanalysis and prediction of job interview perfor-mance.
ieee transactions on affective computing,9(2):191–204..mohammad naseri and hamed zamani.
2019. ana-lyzing and predicting news popularity in an instantmessaging service.
in proceedings of the 42nd inter-national acm sigir conference on research anddevelopment in information retrieval, pages 1053–1056..bo pang, lillian lee, and shivakumar vaithyanathan.
2002. thumbs up?
sentiment classiﬁcation usingmachine learning techniques.
in proceedings of the2002 conference on empirical methods in naturallanguage processing (emnlp 2002), pages 79–86.
association for computational linguistics..sravana reddy, yongze yu, aasish pappu, aswinsivaraman, rezvaneh rezapour, and rosie jones.
2021. detecting extraneous content in podcasts.
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1166–1173, online.
association for computational linguistics..radim ˇreh˚uˇrek and petr sojka.
2010. software frame-work for topic modelling with large corpora.
inproceedings of the lrec 2010 workshop on new.
challenges for nlp frameworks, pages 45–50, val-letta, malta.
elra..manoel horta ribeiro, raphael ottoni, robert west,virg´ılio af almeida, and wagner meira jr. 2020.auditing radicalization pathways on youtube.
inproceedings of the 2020 conference on fairness, ac-countability, and transparency, pages 131–141..elliot schumacher and maxine eskenazi.
2016. areadability analysis of campaign speeches from thearxiv preprint2016 us presidential campaign.
arxiv:1603.05739..vibhor sehgal, ankit peshin, sadia afroz, and hanyfarid.
2021. mutual hyperlinking among misinfor-mation peddlers.
arxiv preprint arxiv:2104.11694..matthew sharpe.
2020. a review of metadata ﬁelds as-sociated with podcast rss feeds.
in podrecs: work-shop on podcast recommendations..chenhao tan, lillian lee, and bo pang.
2014. the ef-fect of wording on message propagation: topic- andauthor-controlled natural experiments on twitter.
inproceedings of the 52nd annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 175–185, baltimore, maryland.
association for computational linguistics..md iftekhar tanveer, samiha samrose, raiyan abdulbaten, and m ehsan hoque.
2018. awe the audi-ence: how the narrative trajectories affect audienceperception in public speaking.
in proceedings of the2018 chi conference on human factors in comput-ing systems, pages 1–12..rachael tatman.
2017. gender and dialect bias inin proceedings ofyoutube’s automatic captions.
the first acl workshop on ethics in natural lan-guage processing, pages 53–59, valencia, spain.
as-sociation for computational linguistics..manos tsagkias, martha larson, and maarten de ri-jke.
2010. predicting podcast preference: an anal-ysis framework and its application.
journal of theamerican society for information science and tech-nology, 61(2):374–391..soroush vosoughi, deb roy, and sinan aral.
2018.the spread of true and false news online.
science,359(6380):1146–1151..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..642longqi yang, yu wang, drew dunne, michaelsobolev, mor naaman, and deborah estrin.
2019.more than just words: modeling non-textual charac-teristics of podcasts.
in proceedings of the twelfthacm international conference on web search anddata mining, pages 276–284..tae yano, noah a. smith, and john d. wilkerson.
2012. textual predictors of bill survival in con-gressional committees.
in proceedings of the 2012conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 793–802, montr´eal,canada.
association for computational linguistics..dani yogatama, michael heilman, brendan o’connor,chris dyer, bryan r. routledge, and noah a. smith.
2011. predicting a scientiﬁc community’s responseto an article.
in proceedings of the 2011 conferenceon empirical methods in natural language process-ing, pages 594–604, edinburgh, scotland, uk.
asso-ciation for computational linguistics..justine zhang, robert filbin, christine morrison, ja-clyn weiser, and cristian danescu-niculescu-mizil.
2019. finding your voice: the linguistic devel-in proceed-opment of mental health counselors.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 936–947, flo-rence, italy.
association for computational linguis-tics..shi zong, alan ritter, and eduard hovy.
2020. mea-in proceedingssuring forecasting skill from text.
of the 58th annual meeting of the association forcomputational linguistics, pages 5317–5331, on-line.
association for computational linguistics..643