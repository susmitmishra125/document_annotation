meta-learning to compositionally generalize.
henry conklin1∗, bailin wang1∗, kenny smith1 and ivan titov1,2.
1university of edinburgh.
2university of amsterdam.
{henry.conklin, bailin.wang, kenny.smith}@ed.ac.uk, ititov@inf.ed.ac.uk.
abstract.
natural language is compositional; the mean-ing of a sentence is a function of the mean-ing of its parts.
this property allows hu-mans to create and interpret novel sentences,generalizing robustly outside their prior ex-perience.
neural networks have been shownto struggle with this kind of generalization,in particular performing poorly on tasks de-signed to assess compositional generalization(i.e.
where training and testing distributionsdiffer in ways that would be trivial for a com-positional strategy to resolve).
their poor per-formance on these tasks may in part be dueto the nature of supervised learning which as-sumes training and testing data to be drawnfrom the same distribution.
we implementa meta-learning augmented version of super-vised learning whose objective directly op-timizes for out-of-distribution generalization.
we construct pairs of tasks for meta-learningby sub-sampling existing training data.
eachpair of tasks is constructed to contain relevantexamples, as determined by a similarity metric,in an effort to inhibit models from memorizingtheir input.
experimental results on the cogsand scan datasets show that our similarity-driven meta-learning can improve generaliza-tion performance..1.introduction.
compositionality is the property of human lan-guage that allows for the meaning of a sentenceto be constructed from the meaning of its parts andthe way in which they are combined (cann, 1993).
by decomposing phrases into known parts we cangeneralize to novel sentences despite never havingencountered them before.
in practice this allowsus to produce and interpret a functionally limitlessnumber of sentences given ﬁnite means (chomsky,1965)..∗equal contribution..whether or not neural networks can generalizein this way remains unanswered.
prior work as-serts that there exist fundamental differences be-tween cognitive and connectionist architecturesthat makes compositional generalization by the lat-ter unlikely (fodor and pylyshyn, 1988).
however,recent work has shown these models’ capacity forlearning some syntactic properties.
hupkes et al.
(2018) show how some architectures can handlehierarchy in an algebraic context and generalize ina limited way to unseen depths and lengths.
worklooking at the latent representations learned bydeep machine translation systems show how thesemodels seem to extract constituency and syntacticclass information from data (blevins et al., 2018;belinkov et al., 2018).
these results, and the moregeneral fact that neural models perform a varietyof nlp tasks with high ﬁdelity (eg.
vaswani et al.,2017; dong and lapata, 2016), suggest these mod-els have some sensitivity to syntactic structure andby extension may be able to learn to generalizecompositionally..recently there have been a number of datasetsdesigned to more formally assess connectionistmodels’ aptitude for compositional generalization(kim and linzen, 2020; lake and baroni, 2018;hupkes et al., 2019).
these datasets frame the prob-lem of compositional generalization as one of out-of-distribution generalization: the model is trainedon one distribution and tested on another whichdiffers in ways that would be trivial for a composi-tional strategy to resolve.
a variety of neural net-work architectures have shown mixed performanceacross these tasks, failing to show conclusively thatconnectionist models are reliably capable of gener-alizing compositionally (keysers et al., 2020; lakeand baroni, 2018).
natural language requires amixture of memorization and generalization (jianget al., 2020), memorizing exceptions and atomicconcepts with which to generalize.
previous work.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3322–3335august1–6,2021.©2021associationforcomputationallinguistics3322looking at compositional generalization has sug-gested that models may memorize large spans ofsentences multiple words in length (hupkes et al.,2019; keysers et al., 2020).
this practice may notharm in-domain performance, but if at test time themodel encounters a sequence of words it has notencountered before it will be unable to interpret ithaving not learned the atoms (words) that compriseit.
grifﬁths (2020) looks at the role of limitations inthe development of human cognitive mechanisms.
humans’ ﬁnite computational ability and limitedmemory may be central to the emergence of robustgeneralization strategies like compositionality.
ahard upper-bound on the amount we can memo-rize may be in part what forces us to generalizeas we do.
without the same restriction modelsmay prefer a strategy that memorizes large sectionsof the input potentially inhibiting their ability tocompositionally generalize..in a way the difﬁculty of these models to gener-alize out of distribution is unsurprising: supervisedlearning assumes that training and testing data aredrawn from the same distribution, and thereforedoes not necessarily favour strategies that are ro-bust out of distribution.
data necessarily under-speciﬁes for the generalizations that produced it.
accordingly for a given dataset there may be alarge number of generalization strategies that arecompatible with the data, only some of which willperform well outside of training (d’amour et al.,2020).
it seems connectionist models do not reli-ably extract the strategies from their training datathat generalize well outside of the training distribu-tion.
here we focus on an approach that tries to tointroduce a bias during training such that the modelarrives at a more robust strategy..to do this we implement a variant of the modelagnostic meta-learning algorithm (maml, finnet al., 2017a).
the approach used here followswang et al.
(2020a) which implements an objec-tive function that explicitly optimizes for out-of-distribution generalization in line with li et al.
(2018).
wang et al.
(2020a) creates pairs of tasksfor each batch (which here we call meta-train andmeta-test) by sub-sampling the existing trainingdata.
each meta-train, meta-test task pair is de-signed to simulate the divergence between trainingand testing: meta-train is designed to resemble thetraining distribution, and meta-test to resemble thetest distribution.
the training objective then re-quires that update steps taken on meta-train are.
also beneﬁcial for meta-test.
this serves as a kindof regularizer, inhibiting the model from taking up-date steps that only beneﬁt meta-train.
by manipu-lating the composition of meta-test we can controlthe nature of the regularization applied.
unlikeother meta-learning methods this is not used forfew or zero-shot performance.
instead it acts as akind of meta-augmented supervised learning, thathelps the model to generalize robustly outside ofits training distribution..the approach taken by wang et al.
(2020a) re-lies on the knowledge of the test setting.
whileit does not assume access to the test distribution,it assumes access to the family of test distribu-tions, from which the actual test distribution will bedrawn.
while substantially less restrictive than thestandard iid setting, it still poses a problem if wedo not know the test distribution, or if the model isevaluated in a way that does not lend itself to beingrepresented by discrete pairs of tasks (i.e.
if testand train differ in a variety of distinct ways).
herewe propose a more general approach that aims togenerate meta-train, meta-test pairs which are popu-lated with similar (rather than divergent) examplesin an effort to inhibit the model from memorizingits input.
similarity is determined by a string ortree kernel so that for each meta-train task a corre-sponding meta-test task is created from examplesdeemed similar..by selecting for similar examples we design themeta-test task to include examples with many ofthe same words as meta-train, but in novel com-binations.
as our training objective encouragesgradient steps that are beneﬁcial for both tasks weexpect the model to be less likely to memorizelarge chunks which are unlikely to occur in bothtasks, and therefore generalize more composition-ally.
this generalizes the approach from wanget al.
(2020a), by using the meta-test task to applya bias not-strictly related to the test distribution:the design of the meta-test task allows us to de-sign the bias which it applies.
it is worth notingthat other recent approaches to this problem haveleveraged data augmentation to make the trainingdistribution more representative of the test distribu-tion (andreas, 2020).
we believe this line of workis orthogonal to ours as it does not focus on gettinga model to generalize compositionally, but rathermaking the task simple enough that compositionalgeneralization is not needed.
our method is modelagnostic, and does not require prior knowledge of.
3323the target distribution..we summarise our contributions as follows:• we approach the problem of compositionalgeneralization with a meta-learning objectivethat tries to explicitly reduce input memoriza-tion using similarity-driven virtual tasks..• we perform experiments on two text-to-semantic compositional datasets: cogs andscan.
our new training objectives lead tosigniﬁcant improvements in accuracy over abaseline parser trained with conventional su-pervised learning.
1.
2 methods.
we introduce the meta-learning augmented ap-proach to supervised learning from li et al.
(2018);wang et al.
(2020a) that explicitly optimizes for out-of-distribution generalization.
central to this ap-proach is the generation of tasks for meta-learningby sub-sampling training data.
we introduce threekinds of similarity metrics used to guide the con-struction of these tasks..2.1 problem deﬁnition.
compositional generalization lake and ba-roni (eg.
2018); kim and linzen (eg.
2020) in-troduce datasets designed to assess compositionalgeneralization.
these datasets are created by gen-erating synthetic data with different distributionsfor testing and training.
the differences betweenthe distributions are trivially resolved by a compo-sitional strategy.
at their core these tasks tend toassess three key components of compositional abil-ity: systematicity, productivity, and primitive appli-cation.
systematicity allows for the use of knownparts in novel combinations as in (a).
productiv-ity enables generalization to longer sequences thanthose seen in training as in (b).
primitive applica-tion allows for a word only seen in isolation duringtraining to be applied compositionally at test timeas in (c)..(a) the cat gives the dog a gift → the dog gives.
the cat a gift.
(b) the cat gives the dog a gift → the cat gives.
the dog a gift and the bird a gift.
algorithm 1 maml training algorithmrequire: original training set trequire: learning rate α, batch size n.1: for step ← 1 to t do2:.
sample a random batch from t as a virtualtraining set btinitialize an empty generalization set bgfor i ← 1 to n do.
sample an example from ˜p(· | bt[i])add it to bg.
end forconstruct a virtual task τ := (bt, bg).
8:9: meta-train update:.
3:.
4:.
5:.
6:.
7:.
10:.
θ(cid:48) ← θ − α∇θlbt(θ)compute meta-test objective:lτ (θ) = lbt(θ) + lbg (θ(cid:48)).
11:.
final update:.
θ ← update(θ, ∇θlτ (θ)).
12: end for.
a compositional grammar like the one that gener-ated the data would be able to resolve these threekinds of generalization easily, and therefore perfor-mance on these tasks is taken as an indication of amodel’s compositional ability..conventional supervised learning the com-positional generalization datasets we look at aresemantic parsing tasks, mapping between naturallanguage and a formal representation.
a usual su-pervised learning objective for semantic parsing isto minimize the negative log-likelihood of the cor-rect formal representation given a natural languageinput sentence, i.e.
minimising.
lb(θ) = −.
log pθ(y|x).
(1).
1n.n(cid:88).
i=1.
where n is the size of batch b, y is a formal rep-resentation and x is a natural language sentence.
this approach assumes that the training and testingdata are independent and identically distributed..task distributions following from wang et al.
(2020a), we utilize a learning algorithm that canenable a parser to beneﬁt from a distribution ofvirtual tasks, denoted by p(τ ), where τ refers to aninstance of a virtual compositional generalizationtask that has its own training and test examples..(c) made → the cat made the dog a gift.
2.2 maml training.
1our.
implementations are available at https://.
github.com/berlino/tensor2struct-public..once we have constructed our pairs of virtualtasks we need a training algorithm that encourages.
3324compositional generalization in each.
like wanget al.
(2020a), we turn to optimization-based meta-learning algorithms (finn et al., 2017b; li et al.,2018) and apply dg-maml (domain generaliza-tion with model-agnostic meta-learning), a vari-ant of maml (finn et al., 2017b).
intuitively, dg-maml encourages optimization on meta-trainingexamples to have a positive effect on the meta-testexamples as well..during each learning episode of maml trainingwe randomly sample a task τ which consists of atraining batch bt and a generalization batch bg andconduct optimization in two steps, namely meta-train and meta-test..meta-train the meta-train task is sampled atrandom from the training data.
the model performsone stochastic gradient descent step on this batch.
θ(cid:48) ← θ − α∇θlbt(θ).
(2).
where α is the meta-train learning rate..meta-test the ﬁne-tuned parameters θ(cid:48) are eval-uated on the accompanying generalization task,meta-test, by computing their loss on it denotedas lbg (θ(cid:48)).
the ﬁnal objective for a task τ is thento jointly optimize the following:.
lτ (θ) = lbt(θ) + lbg (θ(cid:48)).
= lbt(θ) + lbg (θ − α∇θlβ(θ)).
(3).
the objective now becomes to reduce the joint lossof both the meta-train and meta-test tasks.
opti-mizing in this way ensures that updates on meta-train are also beneﬁcial to meta-test.
the loss onmeta-test acts as a constraint on the loss from meta-train.
this is unlike traditional supervised learning(lτ (θ) = lbt(θ) + lbg (θ)) where the loss on onebatch does not constrain the loss on another..with a random bt and bg, the joint loss func-tion can be seen as a kind of generic regularizer,ensuring that update steps are not overly beneﬁcialto meta-train alone.
by constructing bt and bg inways which we expect to be relevant to composi-tionality, we aim to allow the maml algorithmto apply specialized regularization during training.
here we design meta-test to be similar to the meta-train task because we believe this highlights thesystematicity generalization that is key to compo-sitional ability: selecting for examples comprisedof the same atoms but in different arrangements.
in constraining each update step with respect tometa-train by performance on similar examples.
source example: the girl changed a sandwich beside the table ..neighbours using tree kernela sandwich changed .
the girl changed .
the block was changed by the girl .
the girl changed the cake .
change.
neighbours using string kernelthe girl rolled a drink beside the table .
the girl liked a dealer beside the table .
the girl cleaned a teacher beside the table .
the girl froze a bear beside the table .
the girl grew a pencil beside the table ..neighbours using levdistancethe girl rolled a drink beside the table .
the girl liked a dealer beside the table .
the girl cleaned a teacher beside the table .
the girl froze a bear beside the table .
the girl grew a pencil beside the table ..similarity0.550.550.390.390.32.
0.350.350.350.350.35.
-2.00-2.00-2.00-2.00-2.00.table 1: top scoring examples according to the treekernel, string kernel and levenshtein distance for thesentence ‘the girl changed a sandwich beside the ta-ble .’ and accompanying scores..in meta-test we expect the model to dis-prefer astrategy that does not also work for meta-test likememorization of whole phrases or large sections ofthe input..2.3 similarity metrics.
ideally, the design of virtual tasks should reﬂectspeciﬁc generalization cases for each dataset.
how-ever, in practice this requires some prior knowledgeof the distribution to which the model will be ex-pected to generalize, which is not always available.
instead we aim to naively structure the virtual tasksto resemble each other.
to do this we use a numberof similarity measures intended to help select ex-amples which highlight the systematicity of naturallanguage..inspired by kernel density estimation (parzen,1962), we deﬁne a relevance distribution for eachexample:.
˜p(x(cid:48), y(cid:48)|x, y) ∝ exp (cid:0)k([x, y], [x(cid:48), y(cid:48)]/η(cid:1).
(4).
where k is the similarity function, [x, y] is a train-ing example, η is a temperature that controls thesharpness of the distribution.
based on our ex-tended interpretation of relevance, a high ˜p impliesthat [x, y] is systematically relevant to [x(cid:48), y(cid:48)] - con-taining many of the same atoms but in a novelcombination.
we look at three similarity metricsto guide subsampling existing training data intometa-test tasks proportional to each example’s ˜p..3325sentence:logical form:dependency tree:.
a rose was helped by emma .
∃x help(cid:48)(rose(cid:48)(x), emma)help.
partial trees:.
help.
rose.
rose.
emma.
help.
emma.
help.
rose.
emma.
sentence:logical form:dependency tree:.
a rose was helped by a dog .
∃x,y help(cid:48)(rose(cid:48)(x), dog(cid:48)(y))help.
partial trees:.
help.
rose.
help.
rose.
rose.
dog.
help.
dog.
dog.
figure 1: the dependency-tree forms for the logicalforms of two sentences.
shown below each tree are itspartial trees.
as there are three partial trees shared bythe examples their un-normalized tree kernel score is 3..levenshtein distance first, we consider leven-shtein distance, a kind of edit distance widely usedto measure the dissimilarity between strings.
wecompute the negative levenshtein distance at theword-level between natural language sentences oftwo examples:.
k([x, y], [x(cid:48), y(cid:48)]) = −1 ∗ levdistance(x, x(cid:48)) (5).
where levdistance returns the number of edit oper-ations required to transform x into x(cid:48).
see table 1for examples..another family of similarity metrics for discretestructures are convolution kernels (haussler, 1999)..string-kernel similarity we use the string sub-sequence kernel (lodhi et al., 2002):.
k([x, y], [x(cid:48), y(cid:48)]) = ssk(x, x(cid:48)).
(6).
where ssk computes the number of common sub-sequences between natural language sentences atthe word-level.
see table 1 for examples.
2.tree-kernel similarity in semantic parsing, theformal representation y usually has a known gram-mar which can be used to represent it as a treestructure.
in light of this we use tree convolutionkernels to compute similarity between examples: 3.k([x, y], [x(cid:48), y(cid:48)]) = treekernel(y, y(cid:48)).
(7).
where the treekernel function is a convolution ker-nel (collins and duffy, 2001) applied to trees.
herewe consider a particular case where y is representedas a dependency structure, as shown in figure 1.we use the partial tree kernel (moschitti, 2006)which is designed for application to dependencytrees.
for a given dependency tree partial tree ker-nels generate a series of all possible partial trees:any set of one or more connected nodes.
given twotrees the kernel returns the number of partial treesthey have in common, interpreted as a similarityscore.
compared with string-based similarity, thiskernel prefers sentences that share common syntac-tic sub-structures, some of which are not assignedhigh scores in string-based similarity metrics, asshown in table 1..though tree-structured formal representationsare more informative in obtaining relevance, not alllogical forms can be represented as tree structures.
in scan (lake and baroni, 2018) y are actionsequences without given grammars.
as we willshow in the experiments, string-based similaritymetrics have a broader scope of applications butare less effective than tree kernels in cases where ycan be tree-structured..sampling for meta-test using our kernels wecompute the relevance distribution in eq 4 to con-struct virtual tasks for maml training.
we showthe resulting procedure in algorithm 1. in order toconstruct a virtual task τ , a meta-train batch is ﬁrstsampled at random from the training data (line 2),then the accompanying meta-test batch is createdby sampling examples similar to those in meta-train(line 5)..we use lev-maml, str-maml and tree-mamlto denote the meta-training using levenshtein dis-tance, string-kernel and tree-kernel similarity, re-spectively..2we use the normalized convolution kernels in this work,.
i.e., k(cid:48)(x1, x2) = k(x1, x2)/(cid:112)k(x1, x1)k(x2, x2).
shasha, 1989)..3alternatively, we can use tree edit-distance (zhang and.
33263 experiments.
3.1 datasets and splits.
we evaluate our methods on the following seman-tic parsing benchmarks that target compositionalgeneralization..scan contains a set of naturallanguagecommands and their corresponding action se-quences (lake and baroni, 2018).
we use the max-imum compound divergence (mcd) splits (key-sers et al., 2020), which are created based on theprinciple of maximizing the divergence betweenthe compound (e.g., patterns of 2 or more actionsequences) distributions of the training and testtests.
we apply lev-maml and str-maml toscan where similarity measures are applied to thenatural language commands.
tree-maml (whichuses a tree kernel) is not applied as the action se-quences do not have an underlying dependencytree-structure..cogs contains a diverse set of natural lan-guage sentences paired with logical forms basedon lambda calculus (kim and linzen, 2020).
com-pared with scan, it covers various systematic lin-guistic abstractions (e.g., passive to active) includ-ing examples of lexical and structural generaliza-tion, and thus better reﬂects the compositionalityof natural language.
in addition to the standardsplits of train/dev/test, cogs provides a gen-eralization (gen) set drawn from a different dis-tribution that speciﬁcally assesses compositionalgeneralization.
we apply lev-maml, str-mamland tree-maml to cogs; lev-maml and str-maml make use of the natural language sentenceswhile tree-maml uses the dependency structuresreconstructed from the logical forms..3.2 baselines.
in general, our method is model-agnostic and canbe coupled with any semantic parser to improveits compositional generalization.
additionally lev-maml, and str-maml are dataset agnostic pro-vided the dataset has a natural language input.
inthis work, we apply our methods on two widelyused sequence-to-sequences models.
4.lstm-based seq2seq has been the backboneof many neural semantic parsers (dong and la-it utilizespata, 2016; jia and liang, 2016)..lstm (hochreiter and schmidhuber, 1997) andattention (bahdanau et al., 2014) under an encoder-decoder (sutskever et al., 2014) framework..transformer-based seq2seq also follows theencoder-decoder framework, but it uses transform-ers (vaswani et al., 2017) to replace the lstmfor encoding and decoding.
it has proved success-ful in many nlp tasks e.g., machine translation.
recently, it has been adapted for semantic pars-ing (wang et al., 2020b) with superior performance.
we try to see whether our maml training canimprove the compositional generalization of con-temporary semantic parsers, compared with stan-dard supervised learning.
moreover, we includea meta-baseline, referred to as uni-maml, thatconstructs meta-train and meta-test splits by uni-formly sampling training examples.
by compar-ing with this meta-baseline, we show the effectof similarity-driven construction of meta-learningsplits.
note that we do not focus on making compar-isons with other methods that feature specializedarchitectures for scan datasets (see section 5),as these methods do not generalize well to morecomplex datasets (furrer et al., 2020)..geca we additionally apply the good enoughcompositional augmentation (geca) method laidout in andreas (2020) to the scan mcd splits.
data augmentation of this kind tries to make thetraining distribution more representative of the testdistribution.
this approach is distinct from ourswhich focuses on the training objective, but thetwo can be combined with better overall perfor-mance as we will show.
speciﬁcally, we show theresults of geca applied to the mcd splits as wellas geca combined with our lev-maml variant.
note that we elect not to apply geca to cogs, asthe time and space complexity 5 of geca provesvery costly for cogs in our preliminary experi-ments..3.3 construction of virtual tasks.
the similarity-driven sampling distribution ˜p ineq 4 requires computing the similarity between ev-ery pair of training examples, which can be very ex-pensive depending on the size of of the dataset.
asthe sampling distributions are ﬁxed during training,we compute and cache them beforehand.
however,they take an excess of disk space to store as essen-tially we need to store an n × n matrix where n.4details of implementations and hyperparameters can be.
found in the appendix..5see the original paper for details..3327model.
mcd1 mcd2 mcd3.
model.
gen dev test.
gen.7.9.
16.8.
27.4 ±8.2 31.0 ±0.4.
4.7 ±2.20.4 ±0.426.2 ±1.7.
1.8 ±0.77.3 ±2.11.8 ±0.40.5 ±0.17.9 ±1.6 12.1 ±0.12.4.lstmtransformert5-baset5-11b9.6 ±3.7lstmw. uni-maml 44.8 ±5.4 31.9 ±3.4 10.0 ±1.4w. lev-maml 47.6 ±2.3 35.2 ±3.9 11.4 ±3.0w. str-maml42.2 ±2.6 33.6 ±4.3 11.4 ±2.22.3 ±1.33.1 ±1.0transformerw. uni-maml3.2 ±1.63.2 ±1.0w. lev-maml6.5 ±1.26.7 ±1.4w. str-maml6.7 ±1.45.6 ±1.6geca + lstm 51.5 ±4.4 30.4 ±4.8 12.0 ±6.8w. lev-maml 58.9 ±6.4 34.5 ±2.5 12.3 ±4.9.
2.6 ±0.82.8 ±0.74.7 ±1.82.8 ±0.6.
table 2: main results on scan mcd splits.
we showthe mean and variance (95% conﬁdence interval) of 10runs.
cells with a grey background are results obtainedin this paper, whereas cells with a white backgroundare from furrer et al.
(2020)..is the number of training examples.
to allow efﬁ-cient storage and sampling, we use the followingapproximation.
first, we found that usually eachexample only has a small set of neighbours thatare relevant to it.
6 motivated by this observation,we only store the top 1000 relevant neighbours foreach example sorted by similarity, and use it to con-struct the sampling distribution denoted as ˜ptop1000.
to allow examples out of top 1000 being sampled,we use a linear interpolation between ˜ptop1000 and auniform distribution.
speciﬁcally, we end up usingthe following sampling distribution:.
˜p(x(cid:48), y(cid:48)|x, y) = λ ˜ptop1000(x(cid:48), y(cid:48)|x, y) + (1 − λ).
1n.where ˜ptop1000 assigns 0 probability to out-of top1000 examples, n is the number of training exam-ples, and λ is a hyperparameter for interpolation.
inpractice, we set λ to 0.5 in all experiments.
to sam-ple from this distribution, we ﬁrst decide whetherthe sample is in the top 1000 by sampling from abernoulli distribution parameterized by λ. if it is,we use ˜ptop1000 to do the sampling; otherwise, weuniformly sample an example from the training set..3.4 development set.
many tasks that assess out-of-distribution (o.o.d.)
generalization (e.g.
cogs) do not have an o.o.d..--.
16 ±899lstm35 ±696transformer34.5 ±4.530.3 ±7.3 99.7lstmw. uni-maml 36.1 ±6.7 99.736.4 ±3.6w. lev-maml 35.6 ±5.3 99.736.4 ±5.2w. str-maml36.8 ±3.536.3 ±4.2 99.7w. tree-maml 41.2 ±2.8 99.7 41.0 ±4.958.6 ±3.754.7 ±4.0 99.5transformerw. uni-maml 60.9 ±2.8 99.664.4 ±4.0w. lev-maml 62.7 ±3.8 99.764.9 ±6.3w. str-maml64.8 ±5.562.3 ±3.0 99.6w. tree-maml 64.1 ±3.2 99.6 66.7 ±4.4.
table 3: main results on the cogs dataset.
we showthe mean and variance (standard deviation) of 10 runs.
cells with a grey background are results obtained inthis paper, whereas cells with a white background arefrom kim and linzen (2020)..dev set that is representative of the generalizationdistribution.
this is desirable as a parser in prin-ciple should never have knowledge of the gen setduring training.
in practice though the lack of ano.o.d.
dev set makes model selection extremelydifﬁcult and not reproducible.
7 in this work, wepropose the following strategy to alleviate this is-sue: 1) we sample a small subset from the gen set,denoted as ‘gen dev’ for tuning meta-learning hy-perparmeters, 2) we use two disjoint sets of randomseeds for development and testing respectively, i.e.,retraining the selected models from scratch beforeapplying them to the ﬁnal test set.
in this way, wemake sure that our tuning is not exploiting the mod-els resulting from speciﬁc random seeds: we donot perform random seed tuning.
at no point areany of our models trained on the gen dev set..3.5 main results.
on scan, as shown in table 2, lev-maml sub-stantially helps both base parsers achieve better per-formance across three different splits constructedaccording to the mcd principle.
8 though ourmodels do not utilize pre-training such as t5 (raf-fel et al., 2019), our best model (lev-maml +lstm) still outperforms t5 based models sig-niﬁcantly in mcd1 and mcd2.
we show thatgeca is also effective for mcd splits (especially.
6for example, in cogs, each example only retrieves 3.6%of the whole training set as its neighbours (i.e., have non-zerotree-kernel similarity) on average..7we elaborate on this issue in the appendix.
8our base parsers also perform much better than previous.
methods, likely due to the choice of hyperparameters..3328in mcd1).
more importantly, augmenting gecawith lev-maml further boosts the performancesubstantially in mcd1 and mcd2, signifying thatour maml training is complementary to geca tosome degree..table 3 shows our results on cogs.
tree-maml boosts the performance of both lstm andtransformer base parsers by a large margin: 6.5%and 8.1% respectively in average accuracy.
more-over, tree-maml is consistently better than othermaml variants, showing the effectiveness of ex-ploiting tree structures of formal representation toconstruct virtual tasks.
9.
4 discussion.
4.1 scan discussion.
the application of our string-similarity driven meta-learning approaches to the scan dataset improvedthe performance of the lstm baseline parser.
ourresults are reported on three splits of the datasetgenerated according to the maximum compounddivergence (mcd) principle.
we report resultson the only mcd tasks for scan as these tasksexplicitly focus on the systematicity of language.
as such they assess a model’s ability to extractsufﬁciently atomic concepts from its input, suchthat it can still recognize those concepts in a newcontext (i.e.
as part of a different compound).
tosucceed here a model must learn atoms from thetraining data and apply them compositionally attest time.
the improvement in performance ourapproach achieves on this task suggests that it doesdisincentivise the model from memorizing largesections - or entire compounds - from its input..geca applied to the scan mcd splits doesimprove performance of the baseline, however notto the same extent as when applied to other scantasks in andreas (2020).
geca’s improvementis comparable to our meta-learning method, de-spite the fact that our method does not leverage anydata augmentation.
this means that our methodachieves high performance by generalizing robustlyoutside of its training distribution, rather than bymaking its training data more representative ofthe test distribution.
the application of our lev-maml approach to geca-augmented data resultsin further improvements in performance, suggest-.
9the improvement of all of our maml variants appliedto the transformer are signiﬁcant (p < 0.03) compared to thebaseline, of our methods applied to lstms, tree-maml issigniﬁcant (p < 0.01) compared to the baseline..ing that these approaches aid the model in distinctyet complementary ways..4.2 cogs discussion.
all variants of our meta-learning approach im-proved both the lstm and transformer baselineparsers’ performance on the cogs dataset.
thetree-maml method outperforms the lev-maml,str-maml, and uni-maml versions.
the onlydifference between these methods is the similar-ity metric used, and so differences in performancemust be driven by what each metric selects for.
forfurther analysis of the metrics refer to the appendix.
the strong performance of the uni-maml vari-ant highlights the usefulness of our approach gen-erally in improving models’ generalization perfor-mance.
even without a specially designed meta-test task this approach substantially improves onthe baseline transformer model.
we see this as evi-dence that this kind of meta-augmented supervisedlearning acts as a robust regularizer particularly fortasks requiring out of distribution generalization..although the uni-maml, lev-maml, and str-maml versions perform similarly overall on thecogs dataset they may select for different gener-alization strategies.
the cogs generalization setis comprised of 21 sub-tasks which can be used tobetter understand the ways in which a model is gen-eralizing (refer to table 4 for examples of subtaskperformance).
despite having very similar overallperformance uni-maml and str-maml performdistinctly on individual cogs tasks - with theirperformance appearing to diverge on a number ofof them.
this would suggest that the design of themeta-test task may have a substantive impact onthe kind of generalization strategy that emerges inthe model.
for further analysis of cogs sub-taskperformance see the appendix..our approaches’ strong results on both of thesedatasets suggest that it aids compositional gener-alization generally.
however it is worth nothingthat both datasets shown here are synthetic, andalthough cogs endeavours to be similar to natu-ral data, the application of our methods outside ofsynthetic datasets is important future work..5 related work.
compositional generalization a large body ofwork on compositional generalization provide mod-els with strong compositional bias, such as special-ized neural architectures (li et al., 2019; russin.
3329case.
primitive noun → subject(common noun).
training.
shark.
primitive noun → subject(proper noun).
paula.
primitive noun → object(common noun).
shark.
primitive noun → object(proper noun).
paula.
generalization.
accuracy distribution.
a shark examined the child..tree-maml.
paula sketched william..a chief heard the shark..the child helped paula..baseline.
tree-maml.
baseline.
tree-maml.
baseline.
tree-maml.
baseline.
0.5.
1.
0.4.
0.6.
0.8.
1.
0.
0.2.
0.4.
0.
0.5.
1.table 4: accuracy on cogs by generalization case.
each dot represents a single run of the model..et al., 2019; gordon et al., 2019), or grammar-basedmodels that accommodate alignments betweennatural language utterances and programs (shawet al., 2020; herzig and berant, 2020).
an-other line of work utilizes data augmentationvia ﬁxed rules (andreas, 2020) or a learned net-work (akyürek et al., 2020) in an effort to trans-form the out-of-distribution compositional general-ization task into an in-distribution one.
our workfollows an orthogonal direction, injecting composi-tional bias using a specialized training algorithm.
a related area of research looks at the emergenceof compositional languages, often showing thatlanguages which seem to lack natural-languagelike compositional structure may still be able togeneralize to novel concepts (kottur et al., 2017;chaabouni et al., 2020).
this may help to explainthe ways in which models can generalize robustlyon in-distribution data unseen during training whilestill struggling on tasks speciﬁcally targeting com-positionality..meta-learning for nlp meta-learning meth-ods (vinyals et al., 2016; ravi and larochelle,2016; finn et al., 2017b) that are widely used forfew-shot learning, have been adapted for nlp ap-plications like machine translation (gu et al., 2018)and relation classiﬁcation (obamuyide and vla-chos, 2019).
in this work, we extend the conven-tional maml (finn et al., 2017b) algorithm, whichwas initially proposed for few-shot learning, as atool to inject inductive bias, inspired by li et al.
(2018); wang et al.
(2020a).
for compositional gen-eralization, lake (2019) proposes a meta-learningprocedure to train a memory-augmented neuralmodel.
however, its meta-learning algorithm isspecialized for the scan dataset (lake and baroni,2018) and not suitable to more realistic datasets..6 conclusion.
our work highlights the importance of training ob-jectives that select for robust generalization strate-gies.
the meta-learning augmented approach tosupervised learning used here allows for the speci-ﬁcation of different constraints on learning throughthe design of the meta-tasks.
our similarity-driventask design improved on baseline performance ontwo different compositional generalization datasets,by inhibiting the model’s ability to memorize largesections of its input.
importantly though the overallapproach used here is model agnostic, with portionsof it (str-maml, lev-maml, and uni-maml)proving dataset agnostic as well requiring only thatthe input be a natural language sentence.
our meth-ods are simple to implement compared with otherapproaches to improving compositional general-ization, and we look forward to their use in com-bination with other techniques to further improvemodels’ compositional ability..acknowledgements.
this work was supported in part by the ukri cen-tre for doctoral training in natural language pro-cessing, funded by the ukri (grant ep/s022481/1)and the university of edinburgh, school of infor-matics and school of philosophy, psychology &language sciences.
we also acknowledge the ﬁ-nancial support of the european research council(titov, erc stg broadsem 678254) and the dutchnational science foundation (titov, nwo vidi639.022.518)..references.
ekin akyürek, afra feyza akyürek, and jacob an-dreas.
2020. learning to recombine and resam-.
3330ple data for compositional generalization.
preprint arxiv:2010.03706..arxiv.
jacob andreas.
2020. good-enough compositionaldata augmentation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7556–7566, online.
associationfor computational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..yonatan belinkov, lluís màrquez, hassan sajjad,nadir durrani, fahim dalvi, and james glass.
2018.evaluating layers of representation in neural ma-chine translation on part-of-speech and semanticarxiv:1801.07772 [cs].
arxiv:tagging tasks.
1801.07772..terra blevins, omer levy, and luke zettlemoyer.
2018. deep rnns encode soft hierarchical syntax.
arxiv:1805.04218 [cs].
arxiv: 1805.04218..ronnie cann.
1993. formal semantics an introduc-tion.
cambridge university press, cambridge [etc.
oclc: 1120437841..rahma chaabouni, eugene kharitonov, diane boucha-court, emmanuel dupoux, and marco baroni.
2020.compositionality and generalization in emergentlanguages.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4427–4442, online.
association for computa-tional linguistics..noam chomsky.
1965. aspects of the theory of syn-tax, 50th anniversary edition edition.
number no.
11in massachusetts institute of technology.
researchlaboratory of electronics.
special technical report.
the mit press, cambridge, massachusetts..michael collins and nigel duffy.
2001. convolutionkernels for natural language.
in advances in neuralinformation processing systems, pages 625–632..alexander d’amour, katherine heller, dan moldovan,ben adlam, babak alipanahi, alex beutel,christina chen, jonathan deaton, jacob eisen-stein, matthew d. hoffman, farhad hormozdiari,neil houlsby, shaobo hou, ghassen jerfel, alankarthikesalingam, mario lucic, yian ma, corymclean, diana mincu, akinori mitani, andreamontanari, zachary nado, vivek natarajan, christo-pher nielson, thomas f. osborne, rajiv raman,kim ramasamy, rory sayres, jessica schrouff, mar-tin seneviratne, shannon sequeira, harini suresh,victor veitch, max vladymyrov, xuezhi wang, kel-lie webster, steve yadlowsky, taedong yun, xi-aohua zhai, and d. sculley.
2020. underspeciﬁ-cation presents challenges for credibility in mod-ern machine learning.
arxiv:2011.03395 [cs, stat].
arxiv: 2011.03395..li dong and mirella lapata.
2016. language to logi-cal form with neural attention.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages33–43, berlin, germany.
association for computa-tional linguistics..chelsea finn, pieter abbeel, and sergey levine.
2017a.
model-agnostic meta-learning for fast adaptationof deep networks.
arxiv:1703.03400 [cs].
arxiv:1703.03400..chelsea finn, pieter abbeel, and sergey levine.
2017b.
model-agnostic meta-learning for fast adaptation ofdeep networks.
in proceedings of the 34th interna-tional conference on machine learning-volume 70,pages 1126–1135.
jmlr.
org..jerry a. fodor and zenon w. pylyshyn.
1988. connec-tionism and cognitive architecture: a critical analy-sis.
cognition, 28(1-2):3–71..daniel furrer, marc van zee, nathan scales, andnathanael schärli.
2020. compositional generaliza-tion in semantic parsing: pre-training vs. specializedarchitectures.
arxiv preprint arxiv:2007.08970..jonathan gordon, david lopez-paz, marco baroni,and diane bouchacourt.
2019. permutation equiv-ariant models for compositional generalization inlanguage.
in international conference on learningrepresentations..thomas l grifﬁths.
2020. understanding human intel-ligence through human limitations.
trends in cogni-tive sciences..jiatao gu, yong wang, yun chen, victor o. k. li,and kyunghyun cho.
2018. meta-learning for low-in proceed-resource neural machine translation.
ings of the 2018 conference on empirical methodsin natural language processing, pages 3622–3631,brussels, belgium.
association for computationallinguistics..david haussler.
1999. convolution kernels on discretestructures.
technical report, technical report, de-partment of computer science, university of cali-fornia .
.
.
..jonathan herzig and jonathan berant.
2020. span-based semantic parsing for compositional general-ization.
arxiv preprint arxiv:2009.06040..sepp hochreiter and jürgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..dieuwke hupkes, verna dankers, mathijs mul, andelia bruni.
2019. the compositionality of neu-integrating symbolism and connec-ral networks:arxiv:1908.08351 [cs, stat].
arxiv:tionism.
1908.08351..3331dieuwke hupkes, sara veldhoen, and willem zuidema.
2018. visualisation and’diagnostic classiﬁers’ re-veal how recurrent and recursive neural networksprocess hierarchical structure.
journal of artiﬁcialintelligence research, 61:907–926..robin jia and percy liang.
2016. data recombinationfor neural semantic parsing.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages12–22, berlin, germany.
association for computa-tional linguistics..ziheng jiang, chiyuan zhang, kunal talwar, andmichael c. mozer.
2020. characterizing struc-tural regularities of labeled data in overparameter-ized models.
arxiv:2002.03206 [cs, stat].
arxiv:2002.03206..daniel keysers, nathanael schärli, nathan scales,hylke buisman, daniel furrer, sergii kashubin,nikola momchev, danila sinopalnikov, lukaszstaﬁniak, tibor tihon, dmitry tsarkov, xiao wang,marc van zee, and olivier bousquet.
2020. measur-ing compositional generalization: a comprehensivein international confer-method on realistic data.
ence on learning representations..najoung kim and tal linzen.
2020. cogs: a compo-sitional generalization challenge based on seman-tic interpretation.
arxiv:2010.05465 [cs].
arxiv:2010.05465..satwik kottur, josé mf moura, stefan lee, anddhruv batra.
2017. natural language does notarxivemerge’naturally’in multi-agent dialog.
preprint arxiv:1706.08502..brenden lake and marco baroni.
2018. generalizationwithout systematicity: on the compositional skillsof sequence-to-sequence recurrent networks.
in in-ternational conference on machine learning, pages2873–2882.
pmlr..brenden m lake.
2019. compositional generalizationthrough meta sequence-to-sequence learning.
arxivpreprint arxiv:1906.05381..da li, yongxin yang, yi-zhe song, and timothy mhospedales.
2018. learning to generalize: meta-in thirty-learning for domain generalization.
second aaai conference on artiﬁcial intelligence..yuanpeng li, liang zhao, jianyu wang, and joel hest-ness.
2019. compositional generalization for primi-tive substitutions.
arxiv preprint arxiv:1910.02612..2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..alessandro moschitti.
2006..efﬁcient convolutionkernels for dependency and constituent syntactictrees.
in european conference on machine learn-ing, pages 318–329.
springer..abiola obamuyide and andreas vlachos.
2019.model-agnostic meta-learning for relation classiﬁca-tion with limited supervision.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5873–5879, florence,italy.
association for computational linguistics..emanuel parzen.
1962. on estimation of a probabilitydensity function and mode.
the annals of mathemat-ical statistics, 33(3):1065–1076..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucapytorch: an imperativeantiga, et al.
2019.style, high-performance deep learning library.
arxivpreprint arxiv:1912.01703..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..sachin ravi and hugo larochelle.
2016. optimization.
as a model for few-shot learning..jake russin, jason jo, randall c o’reilly, and yoshuabengio.
2019. compositional generalization in adeep seq2seq model by separating syntax and seman-tics.
arxiv preprint arxiv:1904.09708..peter shaw, ming-wei chang, panupong pasupat, andkristina toutanova.
2020. compositional general-ization and natural language variation: can a seman-arxiv preprinttic parsing approach handle both?
arxiv:2010.12725..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
arxiv preprint arxiv:1409.3215..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..huma lodhi, craig saunders, john shawe-taylor,nello cristianini, and chris watkins.
2002. textclassiﬁcation using string kernels.
journal of ma-chine learning research, 2(feb):419–444..oriol vinyals, charles blundell, timothy lillicrap,daan wierstra, et al.
2016. matching networks forin advances in neural informa-one shot learning.
tion processing systems, pages 3630–3638..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation..bailin wang, mirella lapata, and ivan titov.
2020a.
meta-learning for domain generalization in seman-tic parsing.
arxiv preprint arxiv:2010.11988..3332bailin wang, richard shin, xiaodong liu, oleksandrpolozov, and matthew richardson.
2020b.
rat-sql: relation-aware schema encoding and linkingfor text-to-sql parsers.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 7567–7578, online.
asso-ciation for computational linguistics..kaizhong zhang and dennis shasha.
1989. simplefast algorithms for the editing distance between treesand related problems.
siam journal on computing,18(6):1245–1262..the ten random seeds used for lev-maml + trans-former on cogs, the best performing seed obtains73% whereas the lowest performing seed obtains54%.
thus, it is important to compare differentmodels using the same set of random seeds, andnot to tune the random seeds in any model.
to alle-viate these two concerns, we choose the protocolthat is mentioned in the main paper.
this proto-col helps to make the results reported in our paperreproducible..a experiments.
a.1 details of base parsers.
we implemented all models with pytorch (paszkeet al., 2019).
for the lstm parsers, we use a two-layer encoder and one-layer decoder with atten-tion (bahdanau et al., 2014) and input-feeding (lu-ong et al., 2015).
we only test bidirectional lstmencoders, as unidirectional lstm models do notperform very well in our preliminary experiments.
for transformer parsers, we use 2 encoder and de-coder layers, 4 attention heads, and a feed-forwarddimension of 1024. the hidden size for both lstmand transformer models are 256. the hyparame-ters of base parsers are mostly borrowed from re-lated work and not tuned, as the primary goal ofthis work is the maml training algorithm.
to ex-periment with a wide variety of possible seq2seqmodels, we also try a transformer encoder + lstmdecoder and ﬁnd that this variant actually performsslightly better than both vanilla transformer andlstm models.
further exploration of this combi-nation in pursuit of a better neural architecture forcompositional generalization might be interestingfor future work..a.2 model selection protocol.
in our preliminary experiments on cogs, we ﬁndalmost all the seq2seq models achieve > 99% inaccuracy on the original dev set.
however, theirperformance on the gen set diverge dramatically,ranging from 10% to 70%.
the lack of an infor-mative dev set makes model selection extremelydifﬁcult and difﬁcult to reproduce.
this issue mightalso be one of the factors that results in the largevariance of performance reported in previous work.
meanwhile, we found that some random seeds 10yield consistently better performance than othersacross different conditions.
for example, among.
10random seeds control the initialization of parameters and.
the order of training batches..a.3 details of training and evaluation.
following kim and linzen (2020), we train allmodels from scratch using randomly initializedembeddings.
for scan, models are trained for1,000 steps with batch size 128. we choose modelcheckpoints based on their performance on the devset.
for cogs, models are trained for 6,000 stepswith batch size of 128. we choose the meta-trainlearning rate α in equation 2, temperature η inequation 4 based on the performance on the gendev set.
finally we use the chosen α, η to trainmodels with new random seeds, and only the lastcheckpoints (at step 6,000) are used for evaluationon the test and gen set..a.4 other splits of scan.
the scan dataset contains many splits, such asadd-jump, around right, and length split, eachassessing a particular case of compositional gener-alization.
we think that mcd splits are more rep-resentative of compositional generalization due tothe nature of the principle of maximum compounddivergence.
moreover, it is more challenging thanother splits (except the length split) according tofurrer et al.
(2020).
that geca, which obtains82% in accuracy on jump and around right splits,only obtains < 52% in accuracy on mcd splits inour experiments conﬁrms that mcd splits are morechallenging..a.5 kernel analysis.
the primary difference between the tree-kernel andstring-kernel methods is in the diversity of the ex-amples they select for the meta-test task.
the treekernel selects a broader range of lengths, often in-cluding atomic examples, a single word in length,matching a word in the original example from meta-train (see table 5).
by design the partial tree kernelwill always assign a non-zero value to an examplethat is an atom contained in the original sentence.
we believe the diversity of the sentences selected.
3333partial tree kernel.
top 10.
100.
1000.levdistance.
top 10.
100.
1000.mean example length (chars)std devmean no.
of atomsstd dev.
26.71.
29.8726.59± 6.80 ± 7.61 ± 8.851.13± 0.67 ± 1.05 ± 0.81.
0.46.
0.81.mean example length (chars)std devmean no.
of atomsstd dev.
31.04.
29.2830.45± 2.80 ± 3.77 ± 4.780.02± 0.00 ± 0.02 ± 0.17.
0.00.
0.00.table 5: analyses of kernel diversity.
reporting mean example length and number of atoms for the top k highestscoring examples for each kernel.
note that atoms are only counted that also occur in the original example..source example: emma lended the donut to the dog ..source example: the crocodile valued that a girl snapped ..neighbours using tree kernelemma was lended the donut .
the donut was lended to emma .
emma lended the donut to a dog .
emma lended liam the donut .
emma lended a girl the donut ..neighbours using string kernelemma lended the donut to a dog .
emma lended the box to a dog .
emma gave the cake to the dog .
emma lended the cake to the girl .
emma lended the liver to the girl ..similarity0.740.620.550.550.55.neighbours using tree kernela girl snapped .
a rose was snapped by a girl .
the cookie was snapped by a girl .
girlvalue.
neighbours using string kernelthe crocodile liked a girl .
the girl snapped .
the crocodile hoped that a boy observed a girl .
the boy hoped that a girl juggled .
the cat hoped that a girl sketched ..neighbours using levdistanceemma lended the donut to a dog .
emma loaned the donut to the teacher .
emma forwarded the donut to the monster .
emma gave the cake to the dog .
charlotte lended the donut to the ﬁsh ..neighbours using levdistancethe crocodile liked a girl .
the boy hoped that a girl juggled .
the cat hoped that a girl sketched .
the cat hoped that a girl smiled .
emma liked that a girl saw ..0.610.360.330.330.33.
-1.00-2.00-2.00-2.00-2.00.similarity0.550.390.390.320.32.
0.280.270.260.150.15.
-3.00-3.00-3.00-3.00-4.00.table 6: top scoring examples according to the tree kernel, string kernel and levenshtein distance for two sentencesand accompanying scores..by the tree kernel accounts for the superior perfor-mance of tree-maml compared with the othermaml conditions.
the selection of a variety oflengths for meta-test constrains model updates onthe meta-train task such that they must also accom-modate the diverse and often atomic examples se-lected for meta-test.
this constraint would seem tobetter inhibit memorizing large spans of the inputunlikely to be present in meta-test..a.7 cogs subtask analysis.
we notice distinct performance for different con-ditions on the different subtasks from the cogsdataset.
in figure 2 we show the performanceof the uni-maml and str-maml conditionscompared with the mean of those conditions.
where the bars are equal to zero the models’performance on that task is roughly equal..a.6 meta-test examples.
in table 6, we show top scoring examples retrievedby the similarity metrics for two sentences.
wefound that in some cases (e.g., the right part of ta-ble 6), the tree-kernel can retrieve examples thatdiverge in length but are still semantically relevant.
in contrast, string-based similarity metrics, espe-cially levdistance, tends to choose examples withsimilar lengths..full task names for ﬁgure 2:(1) prim→subj proper,(2) active→passive,(3) only seen as unacc subj → unerg subj,(4) subj→obj proper,(5) only seen as unacc subj → obj omitted transitivesubj,(6) pp recursion,(7) cp recursion,(8) obj pp→subj pp,(9) obj→subj common,(10) do dative→pp dative,(11) passive→active,.
3334figure 2: performance for the uni-maml and lev-maml conditions compared to the mean of those twoconditions..(12) only seen as transitive subj → unacc subj,(13) obj omitted transitive→transitive,(14) subj→obj common,(15) prim→obj proper,(16) obj→subj proper,(17) pp dative→do dative,(18) unacc→transitive,(19) prim→subj common,(20) prim→obj common,(21) prim→inf arg..3335