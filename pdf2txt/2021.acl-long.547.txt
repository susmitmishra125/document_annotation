dialoguecrn: contextual reasoning networks for emotion recognitionin conversations.
dou hu1, lingwei wei2,3, xiaoyong huai11 national computer system engineering research institute of china2 institute of information engineering, chinese academy of sciences3 school of cyber security, university of chinese academy of sciences{hudou18, weilingwei18}@mails.ucas.edu.cnhuaixy@sina.com.
abstract.
emotion recognition in conversations (erc)has gained increasing attention for develop-ing empathetic machines.
recently, many ap-proaches have been devoted to perceiving con-versational context by deep learning models.
however, these approaches are insufﬁcient inunderstanding the context due to lacking theability to extract and integrate emotional clues.
in this work, we propose novel contextualreasoning networks (dialoguecrn) to fullyunderstand the conversational context from ainspired by the cog-cognitive perspective.
nitive theory of emotion, we design multi-turn reasoning modules to extract and integrateemotional clues.
the reasoning module itera-tively performs an intuitive retrieving processand a conscious reasoning process, which imi-tates human unique cognitive thinking.
exten-sive experiments on three public benchmarkdatasets demonstrate the effectiveness and su-periority of the proposed model..1.introduction.
emotion recognition in conversation (erc) aims todetect emotions expressed by the speakers in eachutterance of the conversation.
the task is an im-portant topic for developing empathetic machines(zhou et al., 2020) in a variety of areas includingsocial opinion mining (kumar et al., 2015), intel-ligent assistant (k¨onig et al., 2016), health care(pujol et al., 2019), and so on..a conversation often contains contextual clues(poria et al., 2019) that trigger the current utter-ance’s emotion, such as the cause or situation.
re-cent context-based works (poria et al., 2017; haz-arika et al., 2018b; majumder et al., 2019) on erchave been devoted to perceiving situation-levelor speaker-level context by deep learning models.
however, these methods are insufﬁcient in under-standing the context that usually contains rich emo-tional clues.
we argue they mainly suffer from the.
following challenges.
1) the extraction of emo-tional clues.
most approaches (hazarika et al.,2018a,b; jiao et al., 2020b) generally retrieve therelevant context from a static memory, which lim-its the ability to capture richer emotional clues.
2)the integration of emotional clues.
many works(majumder et al., 2019; ghosal et al., 2019; luet al., 2020) usually use the attention mechanism tointegrate encoded emotional clues, ignoring theirintrinsic semantic order.
it would lose logical re-lationships between clues, making it difﬁcult tocapture key factors that trigger emotions..the cognitive theory of emotion (schachterand singer, 1962; scherer et al., 2001) suggeststhat cognitive factors are potently determined forthe formation of emotional states.
these cognitivefactors can be captured by iteratively performingthe intuitive retrieving process and conscious rea-soning process in our brains (evans, 1984, 2003,2008; sloman, 1996).
motivated by them, this pa-per attempts to model both critical processes toreason emotional clues and sufﬁciently understandthe conversational context.
by following the mech-anism of working memory (baddeley, 1992) in thecognitive phase, we can iteratively perform bothcognitive processes to guide the extraction and in-tegration of emotional clues, which imitates humanunique cognitive thinking..in this work, we propose novel contextual rea-soning networks (dialoguecrn) to recognize theutterance’s emotion by sufﬁciently understandingthe conversational context.
the model introduces acognitive phase to extract and integrate emotionalclues from the context retrieved by the perceivephase.
firstly, in the perceptive phase, we lever-age long short-term memory (lstm) (hochre-iter and schmidhuber, 1997) networks to capturesituation-level and speaker-level context.
based onthe above context, global memories can be obtainedto storage different contextual information.
sec-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7042–7052august1–6,2021.©2021associationforcomputationallinguistics7042ondly, in the cognitive phase, we design multi-turnreasoning modules to iteratively extract and inte-grate the emotional clues.
the reasoning moduleperforms two processes, i.e., an intuitive retrievingprocess and a conscious reasoning process.
theformer utilizes the attention mechanism to matchrelevant contextual clues by retrieving static globalmemories, which imitates the intuitive retrievingprocess.
the latter adopts lstm networks to learnintrinsic logical order and integrate contextual cluesby retaining and updating dynamic working mem-ory, which imitates the conscious reasoning pro-cess.
it is slower but with human-unique rationality(baddeley, 1992).
finally, according to the abovecontextual clues at situation-level and speaker-level,an emotion classiﬁer is used to predict the emotionlabel of the utterance..to evaluate the performance of the proposedmodel, we conduct extensive experiments on threepublic benchmark datasets, i.e., iemocap, se-maine and meld datasets.
results consistentlydemonstrate that our proposed model signiﬁcantlyoutperforms comparison methods.
moreover, un-derstanding emotional clues from a cognitive per-spective can boost the performance of emotionrecognition..the main contributions of this work are summa-.
rized as follows:.
• we propose novel contextual reasoning net-works (dialoguecrn) to fully understand theconversational context from a cognitive per-spective.
to the best of our knowledge, thisis the ﬁrst attempt to explore cognitive factorsfor emotion recognition in conversations..• we design multi-turn reasoning modules toextract and integrate emotional clues by itera-tively performing the intuitive retrieving pro-cess and conscious reasoning process, whichimitates human unique cognitive thinking..• we conduct extensive experiments on threepublic benchmark datasets.
the results con-sistently demonstrate the effectiveness and su-periority of the proposed model1..2 methodology.
2.1 problem statement.
there are m speakers/parties p1, p2, ..., pm (m ≥2).
each utterance ui is spoken by the speakerpφ(ui), where φ maps the index of the utteranceinto that of the corresponding speaker.
moreover,for each λ ∈ [1, m ], we deﬁne uλ to representthe set of utterances spoken by the speaker pλ, i.e.,uλ = {ui | ui ∈ u and ui spoken by pλ, ∀i ∈[1, n ]}..the task of emotion recognition in conversations(erc) aims to predict the emotion label yi for eachutterance ui from the pre-deﬁned emotions y..2.2 textual features.
convolutional neural networks (cnns) (kim,2014) are capable of capturing n-grams informa-tion from an utterance.
following previous works(hazarika et al., 2018b; majumder et al., 2019;ghosal et al., 2019), we leverage a cnn layer withmax-pooling to exact context-free textual featuresfrom the transcript of each utterance.
concretely,the input is the 300 dimensional pre-trained 840bglove vectors (pennington et al., 2014).
we em-ploy three ﬁlters of size 3, 4 and 5 with 50 featuremaps each.
these feature maps are further pro-cessed by max-pooling and relu activation (nairand hinton, 2010).
then, these activation featuresare concatenated and ﬁnally projected onto a denselayer with dimension du = 100, whose outputforms the representation of an utterance.
we de-i=1, ui ∈ rdu as the representation for nnote {ui}nutterances..2.3 model.
then, we propose contextual reasoning networks(dialoguecrn) for emotion recognition in conver-sations.
dialoguecrn is comprised of three inte-gral components, i.e., the perception phase (section2.3.1), the cognition phase (section 2.3.2), and anemotion classiﬁer (section 2.3.3).
the overall ar-chitecture is illustrated in figure 1..2.3.1 perception phase.
in the perceptive phase, based on the input textualfeatures, we ﬁrst generate the representation of con-versational context at situation-level and speaker-level.
then, global memories are obtained to stor-age different contextual information..formally, let u = [u1, u2, ..., un ] be a conversa-tion, where n is the number of utterances.
and.
1the source code is available at https://github..com/zerohd4869/dialoguecrn.
conversationalrepresentation.
contextlong short-term memory (lstm) (hochreiterand schmidhuber, 1997) introduces the gatingmechanism into recurrent neural networks to.
7043figure 1: the architecture of the proposed model dialoguecrn..capture long-term dependencies from the inputsequences.
in this part, two bi-directional lstmnetworks are leveraged to capture situation-level and speaker-level context dependencies,respectively..for learning the context representation at thesituation level, we apply a bi-directional lstmnetwork to capture sequential dependencies be-tween adjacent utterances in a conversational situa-tion.
the input is each utterance’s textual featuresui ∈ rdu.
the situation-level context representa-tion cs.
i ∈ r2du can be computed as:.
i , hscs.
i =.
←−−−→lst m.s.(ui, hs.
i−1),.
(1).
where hssituation-level lstm..i ∈ rdu is the i-th hidden state of the.
for.
learning the context.
representation atthe speakerlevel, we also employ anotherbi-directional lstm network to capture self-dependencies between adjacent utterances of thesame speaker.
given textual features ui of eachutterance, the speaker-level context representationi ∈ r2du is computed as:cv.
i , hvcv.
λ,j =.
←−−−→lst m v(ui, hv.
λ,j−1), j ∈ [1, |uλ|], (2).
where λ = φ(ui).
uλ refers to all utterances of theλ,j ∈ rdu is the j-th hidden state ofspeaker pλ.
hvspeaker-level lstm for the speaker pλ..global memory representation.
based on theabove conversational context representation, globalmemories can be obtained to storage different con-textual information via a linear layer.
that is,global memory representation of situation-level.
figure 2: the detailed structure of reasoning module..1, gs.
context gs = [gslevel context gv = [gvputed as:.
2, ..., gs1, gv.
n ] and that of speaker-2, ..., gvn ] can be com-.
gsi = wsi = wvgv.
gcsgcv.
i + bsg,i + bvg,.
(3).
(4).
where wsg, wvlearnable parameters..g ∈ r2du×2du, bs.
g, bv.
g ∈ r2du are.
2.3.2 cognition phaseinspired by the cognitive theory of emotion(schachter and singer, 1962; scherer et al., 2001),cognitive factors are potently determined for theformation of emotional states.
therefore, in thecognitive phase, we design multi-turn reasoningmodules to iteratively extract and integrate the emo-tional clues.
the architecture of a reasoning mod-ule is depicted in figure 2..the reasoning module performs two processes,the intuitive retrieving process, and the consciousreasoning process.
in the t-th turn, for the rea-soning process, we adopt the lstm network tolearn intrinsic logical order and integrate contextual.
7044……timecurrent utterancereasoning modulereasoning modulereasoning module…classifierinputsituation-level contextspeaker-level contextcurrent utterancereasoning modulereasoning modulereasoning module……contextual clueoutput😄😐😊😞😐😐😊………situation-level cluespeaker-level clueperception phasecognition phaseemotion recognitionglobalmemory𝑮𝒉𝒕−1𝒉𝒕workingmemoryattentionreasoningmodulelstm𝒒𝒕−1𝒓𝒕−𝟏෥𝒒𝒕−1𝒒𝒕clues in the working memory, which is slower butwith human-unique rationality (baddeley, 1992).
that is,.
˜q(t−1)i., h(t).
i =.
−−−−→lst m (q(t−1).
i., h(t−1)i.
),.
(5).
i.
∈ r2du is the output vector.
q(t).
where ˜q(t−1)i ∈r4du is initialized by the context representation ciof the current utterance, i.e., q(0)i = wqci + bq,where wq ∈ r4du×2du and bq ∈ r4du are learn-able parameters.
h(t)i ∈ r2du refers to the workingmemory, which can not only storage and updatethe previous memory h(t−1), but also guide the ex-traction of clues in the next turn.
during sequentialﬂowing of the working memory, we can learn im-plicit logical order among clues, which resemblesthe conscious thinking process of humans.
h(t)isiinitialized with zero.
t is the index that indicateshow many “processing steps” are being carried tocompute the ﬁnal state..i.for the retrieving process, we utilize an atten-tion mechanism to match relevant contextual cluesfrom the global memory.
the detailed calculationsare as follows:.
e(t−1)ij.
= f (gj, ˜q(t−1).
),iexp(e(t−1)ijj=1 exp(e(t−1).
ij.
).
,.
).
α(t−1)ij.
=.
r(t−1)i.
=.
α(t−1)ij.
gj,.
(cid:80)n.n(cid:88).
j=1.
(6).
(7).
(8).
where f is a function that computes a single scalarfrom gj and ˜q(t−1).
(e.g., a dot product)..i.then, we concatenate the output of reasoningwith the resulting attention readout.
to form the next-turn query q(t)i..
that is,.
process ˜q(t−1)r(t−1)i.i.i = [˜q(t−1)q(t).
i.; r(t−1)i.
]..(9).
the query q(t)of working memory h(t)ican be retrieved from the global memory..i will be updated under the guidance, and more contextual clues.
to sum up, given context representation ci ofthe utterance ui, global memory representationg, and the number of turns t , the whole cog-nitive phase (eq.5-9) can be denoted as, qi =cognition(ci, g; t ).
in this work, we design twoindividual cognition phases to explore contextual.
clues at situation-level and speaker-level, respec-tively.
the outputs are deﬁned as:.
qsi = cognitions(csi = cognitionv(cvqv.
i , gs; t s),i , gv; t v),.
(10).
(11).
where t s and t v are the number of turns insituation-level and speaker-level cognitive phases,respectively..based on the above output vectors, the ﬁnal rep-resentation o can be deﬁned as a concatenation ofboth vectors, i.e.,.
oi = [qs.
i ; qvi ]..(12).
2.3.3 emotion classiﬁer.
finally, according to the above contextual clues, anemotion classiﬁer is used to predict the emotionlabel of the utterance..ˆyi = sof tmax(wooi + bo),.
(13).
where wo ∈ r8du×|y| and bo ∈ r|y| are trainableparameters.
|y| is the number of emotion labels..cross entropy loss is used to train the model..the loss function is deﬁned as:.
l = −.
1l=1 τ (l).
(cid:80)l.l(cid:88).
τ (i)(cid:88).
i=1.
k=1.
i,klog(ˆylyl.
i,k),.
(14).
where l is the total number of conversa-tions/samples in the training set.
τ (i) is the numberof utterances in the sample i. yli,k denotethe one-hot vector and probability vector for emo-tion class k of utterance i of sample l, respectively..i,k and ˆyl.
3 experimental setups.
3.1 datasets.
we evaluate our proposed model on followingbenchmark datasets, iemocap (busso et al.,2008), semaine (mckeown et al., 2012), andmeld (poria et al., 2019) datasets.
the statisticsare reported in table 1. the above datasets aremultimodal datasets with textual, visual, and acous-tic features.
in this paper, we focus on emotionrecognition in textual conversations.
multimodalemotion recognition in conversations is left as fu-ture work..iemocap2: the dataset (busso et al., 2008)contains videos of two-way conversations of ten.
2https://sail.usc.edu/iemocap/.
7045dataset.
iemocapsemainemeld.
# dialoguesval.
train.
# utterancesval.
train.
12063.
5,8104,368.test3132280.avg.
length507210.test1,6231,4302,610.
# classes.
64∗7.
114∗ refers to the number of real valued attributes..1,039.
9,989.
1,109.table 1: the statistics of three datasets..unique speakers, where only the ﬁrst eight speak-ers from session one to four belong to the trainingset.
the utterances are annotated with one of sixemotion labels, namely happy, sad, neutral, an-gry, excited, and frustrated.
following previousworks (hazarika et al., 2018a; ghosal et al., 2019;jiao et al., 2020b), the validation set is extractedfrom the randomly shufﬂed training set with theratio of 80:20 since no pre-deﬁned train/val split isprovided in the iemocap dataset..semaine3: the dataset (mckeown et al.,2012) is a video database of human-agent inter-actions.
it is available at avec 2012’s fully con-tinuous sub-challenge (schuller et al., 2012) thatrequires predictions of four continuous affectiveattributes: arousal, expectancy, power, and va-lence.
the gold annotations are available for every0:2 seconds in each video (nicolle et al., 2012).
following (hazarika et al., 2018a; ghosal et al.,2019), the attributes are averaged over the span ofan utterance to obtain utterance-level annotations.
we utilize the standard both training and testingsplits provided in the sub-challenge..meld4: multimodal emotion lines dataset(meld) (poria et al., 2019), a extension of theemotionlines (hsu et al., 2018), is collected fromtv-series friends containing more than 1400 multi-party conversations and 13000 utterances.
eachutterance is annotated with one of seven emotionlabels (i.e., happy/joy, anger, fear, disgust, sad-ness, surprise, and neutral).
we use the pre-deﬁnedtrain/val split provided in the meld dataset..3.2 comparisons methods.
we compare the proposed model against the follow-ing baseline methods.
textcnn (kim, 2014) is aconvolutional neural network trained on context-independent utterances.
memnet (sukhbaataret al., 2015) is an end-to-end memory networkand update memories in a multi-hop fashion.
bc-lstm+att (poria et al., 2017) adopts a bidirec-tional lstm network to capture the contextual con-tent from the surrounding utterances.
additionally,.
an attention mechanism is adopted to re-weightfeatures and provide a more informative output.
cmn (hazarika et al., 2018b) encodes conversa-tional context from dialogue history by two dis-tinct grus for two speakers.
icon (hazarikaet al., 2018a) extends cmn by connecting outputsof individual speaker grus using another gru forperceiving inter-speaker modeling.
dialoguernn(majumder et al., 2019) is a recurrent network thatconsists of two grus to track speaker states andcontext during the conversation.
dialoguegcn(ghosal et al., 2019) a graph-based model wherenodes represent utterances and edges represent thedependency between the speakers of the utterances..3.3 evaluation metrics.
following previous works (hazarika et al., 2018a;jiao et al., 2020b), for iemocap and melddatasets, we choose the accuracy score (acc.)
tomeasure the overall performance.
we also re-port the weighted-average f1 score (weighted-f 1) and macro-averaged f1 score (macro-f 1)to evaluate the model performance on both ma-jority and minority classes, respectively.
for thesemaine dataset, we report mean absolute er-ror (mae) for each attribute.
the lower mae, thebetter the detection performance..3.4.implementation details.
we use the validation set to tune hyperparameters.
in the perceptive phase, we employ two-layer bi-directional lstm on iemocap and semainedatasets and single-layer bi-directional lstm onthe meld dataset.
in the cognitive phase, single-layer lstm is used on all datasets.
the batch sizeis set to 32. we adopt adam (kingma and ba,2015) as the optimizer with an initial learning rateof {0.0001, 0.001, 0.001} and l2 weight decayof {0.0002, 0.0005, 0.0005} for iemocap, se-maine, meld datasets, respectively.
the dropoutrate is set to 0.2. we train all models for a max-imum of 100 epochs and stop training if the val-idation loss does not decrease for 20 consecutiveepochs..for results of dialoguegcn and dialoguernn,we implement them according to the public code5provided by majumder et al.
(2019); ghosal et al.
(2019) under the same environment..3https://semaine-db.eu4https://github.com/senticnet/meld.
5https://github.com/declare-lab/.
conv-emotion.
7046iemocap.
acc.
weighted-f1 macro-f1methods49.35textcnn55.70memnet56.32bc-lstm+att56.56cmnicon59.09dialoguernn 63.03dialoguegcn 64.02dialoguecrn 66.053.2%improve.
48.1355.4054.8454.3056.5260.6663.4366.384.7%.
49.2153.1056.1956.1358.5462.5063.6566.204.0%.
table 2: experimental results on the iemocapdataset..methods.
textcnnmemnetbc-lstm+attcmnicondialoguernndialoguegcndialoguecrnimprove.
semaine.
maevalence arousal expectancy power0.5428.710.5450.2118.970.2020.2138.670.1890.2138.740.1920.1908.450.1800.1718.660.1758.650.2100.1768.200.1730.1522.9%1.1% 11.1%.
0.6050.2160.1900.1950.1800.1810.1930.1752.8%.
table 3: experimental results on the semaine dataset..4 results and analysis.
4.1 experimental results.
table 2, 3 and 4 show the comparison resultsfor emotion recognition in textual conversations.
dialoguecrn consistently achieves better perfor-mance than the comparison methods on all datasets,while also being statistically signiﬁcant under thepaired t-test (p<0.05)..iemocap and semaine.
both iemocapand semaine datasets have long conversationlengths and the average length is not less than50. the fact implies that the two datasets con-tain richer contextual information.
textcnn ig-noring conversational context obtains the worst per-formance.
memnet and bc-lstm+att perceivethe situation-level context of the current utterance.
cmn perceives the speaker-level context.
thereby,memnet, bc-lstm+att and cmn slightly out-performs textcnn.
icon, dialoguernn, anddialoguegcn consider both situation-level andspeaker-level context to model the perceptive phaseof context.
they achieve better performance thanthe above methods.
compared with baseline meth-ods, dialoguecrn can extract and integrate rich.
meld.
acc.
weighted-f1 macro-f159.6957.50--.
methodstextcnnbc-lstm+attcmnicondialoguernn 59.54dialoguegcn 59.46dialoguecrn 60.732.0%improve.
56.8355.9054.5054.6056.3956.7758.392.9%.
33.8034.84--32.9334.0535.511.9%.
table 4: experimental results on the meld dataset..emotional clues by exploring cognitive factors.
ac-cordingly, our model obtains more effective per-formance.
that is, as shown in table 2 and 3, forthe iemocap dataset, dialoguecrn gains 3.2%,4.0%, 4.7% relative improvements over the previ-ous best baselines in terms of acc., weighted-f1,and macro-f1, respectively.
for the semainedataset, dialoguecrn achieves a large margin of11.1% mae for the arousal attribute..meld.
from table 1, the number of speakers ofeach conversation in the meld dataset is large (upto 9), and the average length of conversations is10. the shorter conversation length of the melddataset indicates it contains less contextual infor-mation.
from the result in table 4, interestingly,textcnn ignoring conversational context achievesbetter results than most baselines.
it indicates thatit is difﬁcult to learn useful features from perceiv-ing a limited and missing context.
besides, dia-loguegcn leverages graph structure to perceivethe interaction of multiple speakers, which is sufﬁ-cient to perceive the speaker-level context.
thereby,the performance is slightly improved.
comparedwith baselines, dialoguecrn enables to performsequential thinking of context and understand emo-tional clues from a cognitive perspective.
there-fore, it achieves the best recognition results, e.g.,2.9% improvements on weighted-f1..4.2 ablation study.
to better understand the contribution of differentmodules in dialoguecrn to the performance, weconduct several ablation studies on both iemocapand semaine datasets.
different modules thatmodel the situation-level and speaker-level con-text in both perceptive and cognitive phases areremoved separately.
the results are shown in ta-ble 5. when cognition and perception modules areremoved successively, the performance is greatly.
7047cognition.
perception.
iemocap.
acc.
weighted-f1 macro-f1.
situationcontext(cid:88)(cid:88)×××××.
speakercontext(cid:88)×(cid:88)××××.
situationcontext(cid:88)(cid:88)(cid:88)(cid:88)×(cid:88)×.
speakercontext(cid:88)(cid:88)(cid:88)(cid:88)(cid:88)××.
66.0564.2663.2863.2263.5060.0749.35.
66.2064.4363.863.3763.6860.1449.21.
66.3862.8663.3162.0862.4059.5848.13.semainemaevalence arousal expectancy power8.2010.1738.2530.1738.2010.1748.2370.1778.7400.1928.9000.1948.7100.545.
0.1520.1620.1650.1710.2130.2120.542.
0.1750.1810.1790.1800.1950.2010.605.table 5: experimental results of ablation studies on iemocap and semaine datasets..declined.
it indicates the importance of both theperception and cognition phases for erc..effect of cognitive phase.
when only remov-ing cognition phase, as shown in the third block oftable 5, the performance on the iemocap datasetdecreases 4.3%, 4.3% and 6.5% in terms of acc.,weighted-f1, and macro-f1, respectively.
and onthe semaine dataset, the mae scores of valence,arousal, and expectancy attributes are increased by2.3%, 12.5% and 2.9%, respectively.
these resultsindicate the efﬁcacy of the cognitive phase, whichcan reason based on the perceived contextual infor-mation consciously and sequentially.
besides, if re-moving the cognitive phase for either speaker-levelor situation-level context, as shown in the secondblock, the results decreased on both datasets.
thefact reﬂects both situational factors and speakerfactors are critical in the cognitive phase..effect of perceptive phase.
as shown in thelast row, when removing the perception module,the performance is dropped sharply.
the inferiorresults reveal the necessity of the perceptive phaseto unconsciously match relevant context based onthe current utterance..effect of different context.
when removingeither situation-level or speaker-level context inboth cognitive and perceptive phases, respectively,the performance has a certain degree of decline.
the phenomenon shows both situation-level andspeaker-level context play an effective role in theperceptive and cognitive phases.
besides, the mar-gin of dropped performance is different on bothdatasets.
this suggests speaker-level context playsa greater role in the perception phase while morecomplex situation-level context works well in thecognitive phase.
the explanation is that it is limitedto learn informative features from context by intu-itive matching perception, but conscious cognitivereasoning can boost better understanding..figure 3: results against the number of turns.
we re-port the weighted-f1 score on the iemocap datasetand mae of arousal attribute on the semaine dataset.
the lighter the color, the better the performance..4.3 parameter analysis.
we investigate how our model performs w.r.t thenumber of turns in the cognitive phase.
from fig-ure 3, the best {t s, t v} is {2, 2} and {1, 3} oniemocap and semaine datasets, which obtain66.20% weighted-f1 and 0.1522 mae of arousalattribute, respectively.
note that the semainedataset needs more turns for the speaker-level cog-nitive phase.
it implies speaker-level contextualclues may be more vital in arousal emotion, espe-.
7048such as individual reviews or documents.
theycan roughly divided into two parts, i.e., feature-engineering based (devillers and vidrascu, 2006),and deep-learning based methods (tang et al., 2016;wei et al., 2020)..5.2 emotion recognition in conversations.
recently, the task of emotion recognition in con-versations (erc) has received attention from re-searchers.
different traditional emotion recogni-tion, both situation-level and speaker-level contextplays a signiﬁcant role in identifying the emotionof an utterance in conversations (li et al., 2020).
the neglect of them would lead to quite limitedperformance (bertero et al., 2016).
existing worksgenerally capture contextual characteristics for theerc task by deep learning methods, which canbe divided into sequence-based and graph-basedmethods..sequence-based methods.
many works cap-ture contextual information in utterance sequences.
poria et al.
(2017) employed lstm (hochreiterand schmidhuber, 1997) to capture conversationalcontext features.
hazarika et al.
(2018a,b) usedend-to-end memory networks (sukhbaatar et al.,2015) to capture contextual features that distin-guish different speakers.
zhong et al.
(2019); liet al.
(2020) utilized the transformer (vaswani et al.,2017) to capture richer contextual features based onthe attention mechanism.
majumder et al.
(2019)introduced a speaker state and global state for eachconversation based on grus (cho et al., 2014).
moreover, jiao et al.
(2020a) introduced a conver-sation completion task to learn from unsupervisedconversation data.
jiao et al.
(2020b) proposed ahierarchical memory network for real-time emo-tion recognition without future context.
wang et al.
(2020) modeled erc as sequence tagging to learnthe emotional consistency.
lu et al.
(2020) pro-posed an iterative emotion interaction network toexplicitly model the emotion interaction..graph-based methods.
some works (zhanget al., 2019; ghosal et al., 2019; ishiwatari et al.,2020; lian et al., 2020) model the conversationalcontext by designing a speciﬁc graphical struc-ture.
they utilize graph neural networks (kipf andwelling, 2017; velickovic et al., 2017) to capturemultiple dependencies in the conversation, whichhave achieved appreciable performance.
different from previous works,.
inspired bythe cognitive theory of emotion (schachter and.
figure 4: the case study..cially empathetic clues that require complex rea-soning..besides, if we solely consider either situation-level or speaker-level context in the cognitive phase,results on the two datasets are signiﬁcantly im-proved within a certain number of turns.
the factindicates the effectiveness of using multi-turn rea-soning modules to understand contextual clues..4.4 case study.
figure 4 shows a conversation sampled from theiemocap dataset.
the goal is to predict the emo-tion label of utterance 8. methods such as dia-loguernn and dialoguegcn lack the ability toconsciously understand emotional clues, e.g., thecause of the emotion (failed expectation).
they areeasy to mistakenly identify the emotion as angryor neutral..our model dialoguecrn can understand theconversational context from a cognitive perspec-in the cognitive phase, the following twotive.
processes are performed iteratively: the intuitiveretrieving process of 8-7-2-1 (blue arrows) and theconscious reasoning process of a-b-c (red arrows),to extract and integrate emotional clues.
we canobtain that utterance 8 implied that more compen-sation expected by female was not achieved.
thefailed compensation leads to more negative of hisemotion and thus correctly identiﬁed as depression..5 related work.
5.1 emotion recognition.
emotion recognition (er) has been drawing in-creasing attention to natural language processing(nlp) and artiﬁcial intelligence (ai).
existingworks generally regard the er task as a classiﬁ-cation task based on context-free blocks of data,.
7049all you’re going to do is just give me fifty dollars and say go have fun on your vacation without any of your stuff?we’d be willing to give you fifty dollars to reimburse you for the bag.
[neutral][angry][angry][angry][neutral][neutral][neutral][frustrated]there are some shops here in the airport.i realize this.
but i have to ask you to move so i can help the next person.what am i gonnado without anything for three weeks?but fifty dollars isn’t going to get me anything.it sounds like it’s no big deal what-ever.
it’s a big deal to me.(male)(female)12356847(cid:56)(cid:75)(cid:71)(cid:89)(cid:85)(cid:84)1278(cid:56)(cid:75)(cid:90)(cid:88)(cid:79)(cid:75)(cid:92)(cid:75)…(cid:9)(cid:135)(cid:143)(cid:131)(cid:142)(cid:135)(cid:3)(cid:139)(cid:149)(cid:3)(cid:144)(cid:145)(cid:150)(cid:3)(cid:149)(cid:131)(cid:150)(cid:139)(cid:149)(cid:136)(cid:139)(cid:135)(cid:134)(cid:3)(cid:153)(cid:139)(cid:150)(cid:138)(cid:3)(cid:150)(cid:138)(cid:135)(cid:3)compensation(cid:137)(cid:139)(cid:152)(cid:135)(cid:144)(cid:3)(cid:132)(cid:155)(cid:3)(cid:16)(cid:131)(cid:142)(cid:135)(cid:484)(cid:16)(cid:131)(cid:142)(cid:135)(cid:3)(cid:148)(cid:135)(cid:136)(cid:151)(cid:149)(cid:135)(cid:149)(cid:3)(cid:9)(cid:135)(cid:143)(cid:131)(cid:142)(cid:135)(cid:495)(cid:149)(cid:3)(cid:148)(cid:135)(cid:147)(cid:151)(cid:135)(cid:149)(cid:150)(cid:3)(cid:136)(cid:145)(cid:148)(cid:3)(cid:143)(cid:145)(cid:148)(cid:135)(cid:3)compensation.
(cid:9)(cid:135)(cid:143)(cid:131)(cid:142)(cid:135)(cid:495)(cid:149)(cid:3)(cid:135)(cid:154)(cid:146)(cid:135)(cid:133)(cid:150)(cid:131)(cid:150)(cid:139)(cid:145)(cid:144)(cid:136)(cid:145)(cid:148)(cid:3)(cid:143)(cid:145)(cid:148)(cid:135)(cid:3)compensation (cid:138)(cid:131)(cid:149)(cid:3)(cid:133)(cid:145)(cid:143)(cid:135)(cid:3)(cid:150)(cid:145)(cid:3)(cid:144)(cid:145)(cid:150)(cid:138)(cid:139)(cid:144)(cid:137)(cid:484)can i at least have my fifty dollars, please?cbasinger, 1962; scherer et al., 2001), this papermakes the ﬁrst attempt to explore cognitive fac-tors for emotion recognition in conversations.
tosufﬁciently understand the conversational context,we propose a novel dialoguecrn to extract andthen integrate rich emotional clues in a cognitivemanner..6 conclusion.
this paper has investigated cognitive factors forthe task of emotion recognition in conversations(erc).
we propose novel contextual reasoning net-works (dialoguecrn) to sufﬁciently understandboth situation-level and speaker-level context.
di-aloguecrn introduces the cognitive phase to ex-tract and integrate emotional clues from contextretrieved by the perceptive phase.
in the cognitivephase, we design multi-turn reasoning modules toiteratively perform the intuitive retrieving processand conscious reasoning process, which imitateshuman unique cognitive thinking.
finally, emo-tional clues that trigger the current emotion aresuccessfully obtained and used for better classiﬁ-cation.
experiments on three benchmark datasetshave proved the effectiveness and superiority ofthe proposed model.
the case study shows thatconsidering cognitive factors can better understandemotional clues and boost the performance of erc..references.
alan baddeley.
1992. working memory..science,.
255(5044):556–559..dario bertero, farhad bin siddique, chien-shengwu, yan wan, ricky ho yin chan, and pascalefung.
2016. real-time speech emotion and senti-ment recognition for interactive dialogue systems.
in emnlp, pages 1042–1047.
the association forcomputational linguistics..carlos busso, murtaza bulut, chi-chun lee, abekazemzadeh, emily mower, samuel kim, jean-nette n. chang, sungbok lee, and shrikanth s.narayanan.
2008. iemocap: interactive emotionaldyadic motion capture database.
lang.
resour.
eval-uation, 42(4):335–359..kyunghyun cho, bart van merrienboer, c¸ aglarg¨ulc¸ehre, dzmitry bahdanau, fethi bougares, hol-ger schwenk, and yoshua bengio.
2014. learningphrase representations using rnn encoder-decoderfor statistical machine translation.
in emnlp, pages1724–1734.
the association for computer linguis-tics..laurence devillers and laurence vidrascu.
2006. real-life emotions detection with lexical and paralinguis-tic cues on human-human call center dialogs.
in in-terspeech.
isca..jonathan st bt evans.
1984. heuristic and analyticprocesses in reasoning.
british journal of psychol-ogy, 75(4):451–468..jonathan st bt evans.
2003..in two minds: dual-process accounts of reasoning.
trends in cognitivesciences, 7(10):454–459..jonathan st bt evans.
2008. dual-processing ac-counts of reasoning, judgment, and social cognition.
annu.
rev.
psychol., 59:255–278..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander f. gelbukh.
2019.dialoguegcn: a graph convolutional neural net-inwork for emotion recognition in conversation.
emnlp/ijcnlp (1), pages 154–164.
associationfor computational linguistics..devamanyu hazarika, soujanya poria, rada mihal-cea, erik cambria, and roger zimmermann.
2018a.
icon: interactive conversational memory networkin emnlp,for multimodal emotion detection.
pages 2594–2604.
association for computationallinguistics..devamanyu hazarika, soujanya poria, amir zadeh,erik cambria, louis-philippe morency, and rogerzimmermann.
2018b.
conversational memory net-work for emotion recognition in dyadic dialoguevideos.
in naacl-hlt, pages 2122–2132.
associa-tion for computational linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural comput., 9(8):1735–1780..chao-chun hsu, sheng-yeh chen, chuan-chun kuo,ting-hao k. huang, and lun-wei ku.
2018. emo-tionlines: an emotion corpus of multi-party conver-in lrec.
european language resourcessations.
association (elra)..taichi ishiwatari, yuki yasuda, taro miyazaki, andjun goto.
2020. relation-aware graph attention net-works with relational position encodings for emo-in emnlp (1),tion recognition in conversations.
pages 7360–7370.
association for computationallinguistics..wenxiang jiao, michael r. lyu, and irwin king.
2020a.
exploiting unsupervised data for emotion recogni-tion in conversations.
in emnlp (findings), pages4839–4846.
association for computational linguis-tics..wenxiang jiao, michael r. lyu, and irwin king.
2020b.
real-time emotion recognition via attention gated hi-erarchical memory network.
in aaai, pages 8002–8009. aaai press..7050yoon kim.
2014. convolutional neural networks forin emnlp, pages 1746–.
sentence classiﬁcation.
1751. the association for computer linguistics..diederik p. kingma and jimmy ba.
2015. adam:in iclr.
a method for stochastic optimization.
(poster)..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in iclr (poster).
openreview.net..alexandra k¨onig, linda e. francis, aarti malhotra,and jesse hoey.
2016. deﬁning affective identitiesin elderly nursing home residents for the design ofan emotionally intelligent cognitive assistant.
in per-vasivehealth, pages 206–210.
acm..akshi kumar, prakhar dogra, and vikrant dabas.
2015.emotion analysis of twitter using opinion mining.
inic3, pages 285–290.
ieee computer society..jingye li, donghong ji, fei li, meishan zhang, andyijiang liu.
2020. hitrans: a transformer-basedcontext- and speaker-sensitive model for emotion de-tection in conversations.
in coling, pages 4190–4200. international committee on computationallinguistics..zheng lian, jianhua tao, bin liu, jian huang, zhanleiyang, and rongjun li.
2020. conversational emo-tion recognition using self-attention mechanismsin interspeech 2020,and graph neural networks.
21st annual conference of the international speechcommunication association, virtual event, shang-hai, china, 25-29 october 2020, pages 2347–2351.
isca..xin lu, yanyan zhao, yang wu, yijian tian, huipengchen, and bing qin.
2020. an iterative emotioninteraction network for emotion recognition in con-versations.
in coling, pages 4078–4088.
interna-tional committee on computational linguistics..navonil majumder, soujanya poria, devamanyu haz-arika, rada mihalcea, alexander f. gelbukh, anderik cambria.
2019. dialoguernn: an attentivernn for emotion detection in conversations.
inaaai, pages 6818–6825.
aaai press..gary mckeown, michel franc¸ois valstar, roddycowie, maja pantic, and marc schr¨oder.
2012. thesemaine database: annotated multimodal recordsof emotionally colored conversations between a per-son and a limited agent.
ieee trans.
affect.
com-put., 3(1):5–17..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp, pages 1532–1543.
the as-sociation for computer linguistics..soujanya poria, erik cambria, devamanyu hazarika,navonil majumder, amir zadeh, and louis-philippemorency.
2017. context-dependent sentiment analy-sis in user-generated videos.
in acl (1), pages 873–883. association for computational linguistics..soujanya poria, devamanyu hazarika, navonil ma-jumder, gautam naik, erik cambria, and rada mi-halcea.
2019. meld: a multimodal multi-partydataset for emotion recognition in conversations.
inacl (1), pages 527–536.
association for computa-tional linguistics..francisco a. pujol, higinio mora, and ana mart´ınez.
2019. emotion recognition to improve e-healthcaresystems in smart cities.
in riiforum, pages 245–254. springer..stanley schachter and jerome singer.
1962. cognitive,social and physiological determinants of emotionalstate.
psychological review, 69:378–399..klaus r scherer, angela schorr, and tom johnstone.
2001. appraisal processes in emotion: theory,methods, research.
oxford university press..bj¨orn w. schuller, michel franc¸ois valstar, roddythecowie, and maja pantic.
2012. avec 2012:continuous audio/visual emotion challenge - an in-troduction.
in icmi, pages 361–362.
acm..steven a sloman.
1996..the empirical case fortwo systems of reasoning.
psychological bulletin,119(1):3..sainbayar sukhbaatar, arthur szlam, jason weston,and rob fergus.
2015. end-to-end memory net-works.
in nips, pages 2440–2448..duyu tang, bing qin, xiaocheng feng, and ting liu.
2016. effective lstms for target-dependent senti-ment classiﬁcation.
in coling, pages 3298–3307.
association for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..vinod nair and geoffrey e. hinton.
2010. rectiﬁedlinear units improve restricted boltzmann machines.
in icml, pages 807–814.
omnipress..petar velickovic, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
2017. graph attention networks.
in iclr..j´er´emie nicolle, vincent rapp, kevin bailly, lionelprevost, and mohamed chetouani.
2012. robustcontinuous prediction of human emotions using mul-in icmi, pages 501–508.
tiscale dynamic cues.
acm..yan wang, jiayu zhang, jun ma, shaojun wang, andjing xiao.
2020. contextualized emotion recogni-tion in conversation as sequence tagging.
in sigdial,pages 186–195.
association for computational lin-guistics..7051lingwei wei, dou hu, wei zhou, xuehai tang, xi-aodan zhang, xin wang, jizhong han, and songlinhu.
2020. hierarchical interaction networks withrethinking mechanism for document-level sentimentanalysis.
in ecml/pkdd..dong zhang, liangqing wu, changlong sun,shoushan li, qiaoming zhu, and guodong zhou.
2019. modeling both context- and speaker-sensitivedependence for emotion detection in multi-speakerin ijcai, pages 5415–5421.
conversations.
ijcai.org..peixiang zhong, di wang, and chunyan miao.
2019.knowledge-enriched transformer for emotion detec-in emnlp/ijcnlption in textual conversations.
(1), pages 165–176.
association for computationallinguistics..li zhou, jianfeng gao, di li, and heung-yeung shum.
2020. the design and implementation of xiaoice,an empathetic social chatbot.
comput.
linguistics,46(1):53–93..7052