breaking corpus bottleneck for context-aware neural machinetranslation with cross-task pre-training.
1linqing chen.
1junhui li∗.
1zhengxian gong.
2boxing chen.
2weihua luo.
1min zhang1school of computer science and technology, soochow university, suzhou, china2alibaba damo academy{lijunhui, zhxgong, minzhang, gdzhou}@suda.edu.cn,lqchen21@gmail.com, {boxing.cbx, weihua.luowh}@alibaba-inc.com.
1guodong zhou.
abstract.
context-aware neural machine translation(nmt) remains challenging due to the lackof large-scale document-level parallel dataset.
to break the corpus bottleneck,in this pa-per we aim to improve context-aware nmtby taking the advantage of the availability ofboth large-scale sentence-level parallel datasetand source-side monolingual documents.1 tothis end, we propose two pre-training tasks.
one learns to translate a sentence from sourcelanguage to target language on the sentence-level parallel dataset while the other learnsto translate a document from deliberatelynoised to original on the monolingual docu-ments.
importantly, the two pre-training tasksare jointly and simultaneously learned via thesame model,thereafter ﬁne-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives.
exper-imental results on four translation tasks showthat our approach signiﬁcantly improves trans-lation performance.
one nice property ofour approach is that the ﬁne-tuned model canbe used to translate both sentences and docu-ments..1.introduction.
document-level context-aware neural machinetranslation (nmt) aims to translate sentences ina document under the guidance of document-levelcontext.
recent years have witnessed great im-provement in context-aware nmt with extensiveattempts at effectively leveraging document-levelcontext ((tiedemann and scherrer, 2017; marufand haffari, 2018; maruf et al., 2019), to namea few).
however, the performance of context-aware nmt still suffers from the size of paral-lel document dataset.
on the one hand, unlike.
sentence-level translation models which could bewell trained on large-scale sentence-level paralleldatasets, the translation models of context-awarenmt may result in insufﬁcient training.
on theother hand, with only scale-limited source-sidedocuments, the context encoders may fail to ef-fectively extract useful context from the whole doc-ument.2 on the contrary, large-scale of parallelsentence corpora, and especially monolingual doc-ument corpora are much easier to ﬁnd.
in thispaper, our goal is to break the corpus bottleneckfor context-aware nmt by leveraging both large-scale sentence-level parallel dataset and monolin-gual documents.
speciﬁcally, we aim to use theformer to boost the performance of translation mod-els while employ the latter to enhance the contextencoders’ capability of capturing useful contextinformation..there have been several attempts to boostcontext-aware nmt performance in the scenar-ios where the document-level parallel dataset isscale-limited, or even not available.
on the onehand, sentence-level parallel dataset is a natural re-source to use.
for example, zhang et al.
(2018) pro-pose a two-stage training strategy for context-awarenmt by pre-training the model on a sentence-level parallel dataset.
on the other hand, junczys-dowmunt (2019) leverage large-scale source-sidemonolingual documents, in which they simply con-catenate sentences within a document into a longsequence and explore multi-task training via thebert-objective (devlin et al., 2019) on the en-coder.
due to that different models are usuallyrequired to model sentences and documents, how-ever, it is challenging to effectively take them bothin a single model..in order to effectively and simultaneously model.
∗corresponding author: junhui li.
1if not speciﬁed, monolingual documents are all for source-.
side through this paper..2we note that not all, but many context-aware nmt mod-els contain a context encoder to extract global context infor-mation from the document..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2851–2861august1–6,2021.©2021associationforcomputationallinguistics2851document-level restoration given monolin-gual documents, our pre-training task is to re-store a document from a noised version.
to thisend, we deliberately corrupt documents by follow-ing the two pre-training objectives, which are in-spired by both gap sentence objective (zhang et al.,2020) and masked language model objective (de-vlin et al., 2019)..• context-aware gap sentence restoration(ca-gsr).
given a document s with n sen-tences, we randomly select m sentences asgap sentences and replace them with a masktoken [mask1] to inform the model.
thegap sentence ratio is, therefore m/n .
foreach selected gap sentence, we use its left andright neighbours as input while the gap sen-tence serves as output.
to mimic document-level translation task, in the selection the ﬁrstand the last sentences are always not selectedwhile any two consequent sentences are notboth selected..• context-aware masked sentence restoration(ca-msr).
given a sentence x, we followbert and randomly select 15% tokens in it.
the selected tokens are (1) 80% of time re-placed by a mask token [mask2], or (2) 10%of time replaced by a random token, or (3)10% of time unchanged.
for a sentence, weuse its masked (cid:98)x as input while the originalx serves as output..both ca-gsr and ca-msr are applied simul-taneously with the noised document as context.
forconvenience of presentation, we use a concreteexample to illustrate the input and output of ourdocument-level restoration task.
as shown in fig-ure 2, let assume that a document x contains 6sentences and the third and ﬁfth sentences (i.e., x3and x5) are selected as gap sentences while the oth-ers are not.
on the one hand, for a sentence whichis not selected as gap sentence, e.g., x1, we useits masked version (e.g., (cid:99)x1) as input while try topredict its original sentence (e.g., x1).
on the otherhand, for a gap sentence, e.g., x3, we concatenateits left and right neighbouring sentences with sepa-rator [mask1] and try to predict the gap sentence(e.g., x3).
as shown in figure 2, sentences from s1to s6 constitute document-level input s while sen-tences from t1 to t6 make up output t .
note thatwe do not include either gap sentences themselvesor their masked version in s, in case the document.
figure 1: illustration of the proposed cross-task pre-training (upper) and ﬁne-tuning with two perspectives(below)..both sentence-level parallel dataset and monolin-gual documents, in this paper we propose a novelcross-task pre-training approach.
as shown in fig-ure 1, we deﬁne two pre-training tasks.
one learnsto translate a sentence from source language totarget language while the other learns to translatea document from deliberately noised to original.
importantly, the two pre-training tasks are jointlylearned via the same model synchronously.
thenwe use document-level parallel dataset to ﬁne-tunethe properly pre-trained models.
similarly to thepre-training, we can ﬁne-tune the models fromboth sentence-level and document-level perspec-tives.
experimental results on four document-leveltranslation tasks show that our approach signiﬁ-cantly improves translation performance, suggest-ing the effectiveness of our approach in modelingboth sentence-level parallel dataset and monolin-gual documents.
one nice property of our approachis that the ﬁne-tuned models can be used to trans-late both sentences and documents..2 cross-task pre-training.
in the following, we ﬁrst describe our pre-trainingtasks deﬁned upon sentence-level parallel datasetand large-scale monolingual documents (sec-tion 2.1).
then we detail our model which caterssuch pre-training tasks (section 2.2).
finally, wepresent our joint pre-training (section 2.3)..2.1 pre-training tasks.
we deﬁne two pre-training tasks in our pre-training.
one is on sentence-level parallel dataset while theother is on monolingual documents..sentence-level translation given large-scalesentence-level parallel dataset, our pre-training taskis quite straight, i.e., sentence-level translation..2852cross-taskpre-trainingtwo-perspectivefine-tuningsource sentencescorrupted documentstarget sentencesoriginal documentssource documentstarget documentssource sentencestarget sentencessub-layer, a global context attention sub-layer anda feed-forward sub-layer..in the k-th encoder layer, the self-attention sub-i ∈ rn×dm as input and computes ai with the same length via multi-.
layer takes a(k)new sequence b(k)head attention function:.
b(k).
i = multihead.
(cid:16)q = a(k).
i., k = a(k).
, v = a(k).
i.i., (1).
(cid:17).
i.is in the shape of rn×dm,3where the output b(k)and q, k, v represent the query and key-value pairsin attention mechanism respectively.
for the ﬁrstencoder layer, a(1)is the addition of si’s word em-bedding and its position embedding while for otherlayers, a(k)is the output of the proceeding encoderlayer..i.i.in the k-th encoder layer, the sentence represen-tation sub-layer takes b(k)as input and computesia vector to represent the sentence through a linearcombination with a vector of weights as:.
α(k).
i = softmax.
w 2 tanh.
(cid:18).
(cid:18)w 1 (cid:16).
b(k)i.
(cid:17)t (cid:19)(cid:19).
(2).
where w 1 ∈ rdm×dm and w 2 ∈ rdm are modelparameters.
the output α(k)is a n-sized vector.
then the representation vector of sentence si is theweighted sum of its hidden states:.
i.c (k).
i = α(k).
i b(k).
i.,.
(3).
i.
(cid:105).
, · · · , c (k)n.where c (k)is a dm-sized vector.
we then stackvectors of all sentences in s into c(k), i.e., c(k) =(cid:104)c (k).
note that c(k) ∈ rn ×dm is at1document-level and represents the global context.
in the k-th encoder layer, the global contextattention sub-layer extracts useful global contextfor si,j in si.
this is also done via multi-headattention function:.
d(k).
i = multihead.
q = b(k).
i.
(cid:16).
, k = c(k), v = c(k)(cid:17).
,.
(4).
where the output d(k).
is in the shape of rn×dm..i.in the k-th encoder layer, the feed forward sub-layer is applied to each position separately and.
3the actual output of this sub-layer is layernorm(b(k)i +a(k)), where layernorm is the layer normalization function.
ifor simplicity, we do not include the residual addition andlayer normalization functions in our sub-layers.
note that thesentence representation sub-layer is the only exception whichdoes not have residual addition and layer normalization..figure 2: illustration of the proposed document-levelrestoration task..context contains obvious hints for generating gapsentences..overall, the pre-training task of document-levelrestoration is to predict target output t by giv-ing source input s, which is the same as the taskof document-level translation, except that in therestoration s and t are in the same language whilein the latter the two are in different languages..2.2.joint modeling of pre-training tasks.
we use the same model to cater the above twopre-training tasks.
since the task of document-level restoration is more complicated than thetask of sentence-level translation, we ﬁrst describethe model for document-level restoration (sec-tion 2.2.1).
then we apply the model for sentence-level translation (section 2.2.2)..2.2.1 context-aware modeling fordocument-level restoration.
we deﬁne some notations before describing ourmodel.
given a document-level source input s =(s1, · · · , sn ) and target output t = (t1, · · · , tn ) withn sentence pairs, we assume each source sentencesi = (si,1, · · · , si,n) consists of n words.
we use dmas the size of embedding and hidden state through-out the entire model..figure 3 shows our context-aware model.
it con-tains two parts, namely a global context encoderand a seq2seq model augmented by context repre-sentation.
note that for document-level restoration,we take documents as input units..global context encoder for the i-th input sen-tence si in document s, the global context encoderaims to extract useful global context for every wordsi,j in it.
as shown in figure 3(a), the encoder con-sists of a stack of ng identical encoder layers.
eachencoder layer consists of four major sub-layers: aself-attention sub-layer, a sentence representation.
2853𝑆(cid:2869):𝑆(cid:2870):𝑆(cid:2871):𝑆(cid:2872):𝑆(cid:2873):𝑆(cid:2874):(a) document-level input𝑆;𝑇(cid:2869):𝑇(cid:2870):𝑇(cid:2871):𝑇(cid:2872):𝑇(cid:2873):𝑇(cid:2874):(b) document-level output𝑇;𝑋(cid:2869)(cid:3554)𝑋(cid:2870)(cid:3554)𝑋(cid:2870)[mask1]𝑋(cid:2872)𝑋(cid:2872)(cid:3554)𝑋(cid:2872)[mask1] 𝑋(cid:2874)𝑋(cid:2874)(cid:3554)𝑋(cid:2869)𝑋(cid:2870)𝑋(cid:2871)𝑋(cid:2872)𝑋(cid:2873)𝑋(cid:2874)tsfigure 3: illustration of the proposed context-aware model.
note that 1) we share the two sub-layers of self-attention and feed forward between the global context encoder and the sentence encoder; 2) the model uses thesame vocabulary for the tasks in pre-training and ﬁne-tuning since we share vocabulary for the source and targetlanguages; 3) we use (b) for sentence-level translation and turn off the gate mechanism..identically by two linear transformations with arelu activation in between..e(k).
i = max.
(cid:16)0, d(k).
i w f 1 + bf 1(cid:17).
w f 2 + bf 2,.
(5).
where w f 1, w f 2 ∈ rdm×dm, and bf 1, bf 2 ∈rdm are model parameters..we denote gi ∈ rn×dm as the ﬁnal output ofthe global context encoder, i.e., gi = e(ng).
thatis to say, gi represents the context representationfor sentence si..i.context-aware model as shown in figure 3 (b),the seq2seq model is very similar to the standardtransformer, except that it is now equipped withcontext representation obtained by the global con-text encoder.
for sentence si, we denote the sen-tence encoder output as hi ∈ rn×dm.
to leverageits context representation gi, we deﬁne a gate tolinearly combine the two kinds of representationvia:.
h (cid:48).
i = λhi + (1 − λ) gi,.
where the gating weight is computed by.
λ = sigmoid.
(cid:16).
[hi; gi]w g(cid:17).
,.
(6).
(7).
where w g ∈ r2dm×dm are model parameters..then we use h (cid:48).
i to replace hi as the input tothe decoder.
we point out that in the global con-text encoder and sentence encoder, we share the.
self-attention sub-layer and the feed forward sub-layer.
that is to say, compared to the standardtransformer, we introduce new parameters to caterthe sentence representation sub-layers, the globalcontext sub-layers, and the gate mechanism to com-bine the two kinds of representation in eq.
6..2.2.2 adapting context-aware model tosentence-level translation.
in the ﬁrst pre-training task, sentence-level trans-lation is context-agnostic and does not require theglobal context encoder.
therefore, it only uses thesentence encoder and decoder, as shown in fig-ure 3 (b).
moreover, we turn off the gate mech-anism by setting h (cid:48)i = hi.
since we share thetwo sub-layers of self-attention and feed forwardbetween the sentence encoder and the global con-text encoder, updating the model by sentence-leveltranslation will have direct impact on the globalcontext encoder too..2.3.joint pre-training process.
as shown in our experimentation, we share thesame vocabulary for pre-training tasks.
to train theabove two pre-training tasks with a single model,we follow the strategy used in johnson et al.
(2017)and add a preceding language tag to each sourceand target sentence..our joint pre-training on two tasks falls into theparadigm of multi-task learning (mtl).
in training.
2854feed forwardself attentionfeed forwardself attentioncross attentionne×nd×feed forwardself attentionglobal context attentionng×feed forwardself attentionglobal context attentionfeed forwardself attentionglobal context attentionword embeddingsentence representationaibiciglobal context encoderb1bnsi=(si,1, …, si,j, …, si,n)s1=(s1,1, …, s1,j, …, s1,n)sn=(sn,1, …, sn,j, …, sn,n)……si=(si,1, …, si,j, …, si,n)decoderglobal contextti=(ti,1, …, ti,j, …, ti,n)sentenceencoderword embeddingword embeddingword embeddingword embeddinghigihi’(a)(b)sentence representationsentence representationdieistage, we take turns to load the training data ofthese pre-training tasks.
for example, we updatemodel parameters on a batch of training instancesfrom the ﬁrst task, and then update parameters ona batch of training instances of the other, and theprocess repeats..3 fine-tuning on document-level.
parallel dataset.
3.1 fine-tuning tasks.
similar to pre-training tasks, we deﬁne the fol-lowing two different ﬁne-tuning tasks from bothsentence-level and document-level..sentence-level translation we ﬁrst extractsentence-level parallel sentence pairs from thedocument-level parallel dataset for ﬁne-tuning.
this ﬁne-tuning task enables the ﬁne-tuned modelto translate sentences.
in ﬁne-tuning, this task isprocessed as same as the sentence-level translationtask in pre-training..document-level translation given a paralleldocument (x , y) with n sentence pairs (xi, yi) |n1 .
this ﬁne-tune task is to translate source documentx into target document y. in ﬁne-tuning, this tasktakes parallel documents as input units and is pro-cessed as same as the document-level restorationtask in pre-training..3.2 fine-tuning process.
the ﬁne-tuning process is quite similar as the pre-training process in section 2.3. speciﬁcally, we adda preceding language tag to each sentence.
mean-while in ﬁne-tuning, we alternatively load batchesof the two ﬁne-tuning tasks..4 experimentation.
to test the effect of our approach in leveragingsentence-level parallel dataset and monolingualdocuments, we carry out experiments on chinese-to-english (zh-en) and english-to-german (en-de) translation..4.1 experimental settings.
data.
settings.
the.
pre-trainingzh-ensentence-level parallel dataset contains 2.0msentence pairs with 54.8m chinese words and60.8m english words.4 we use wmt14 en-de.
4it.
consists.
ldc2003e07,ldc2003e14, news part of ldc2004t08, ldc2002t01,ldc2004t07, ldc2005t06, ldc2005t10, ldc2009t02,.
ldc2002e18,.
of.
translation dataset as the en-de sentence-levelparallel dataset which consists of 4.4m sentencepairs.5.
we use chinese gigaword (ldc2009t27) andenglish gigaword (ldc2012t21) as monolingualdocument dataset for zh-en and en-de trans-lation, respectively.
for efﬁcient training, wesplit long documents into sub-documents with atmost 30 sentences.
we have 2.6m (7.3m) sub-documents with 24m (102m) sentences in totalfor chinese (english).
upon the monolingualdocuments, we prepare training instances for thedocument-level restoration task and set gap sen-tence ratio to 20%..all chinese sentences are segmented by jieba6while all english and german sentences are tok-enized by moses scripts (koehn et al., 2007).7 forzh-en (en-de) translation, we merge the sourceand target sentences of the parallel dataset and themonolingual document and segment words intosub-words by a bpe model with 30k (25k) opera-tions (sennrich et al., 2016)..fine-tuning data settings.
for zh-en, we haveone translation task on news domain.
thedocument-level parallel corpus of training set in-clude 41k documents with 780k sentence pairs.8we use the nist mt 2006 dataset as the develop-ment set, and combine the nist mt 2002, 2003,2004, 2005, 2008 datasets as test set...for en-de, we test three translation tasks indomains of ted talks, news-commentary and eu-roparl..• ted, which is from iwslt 2017 mttrack (cettolo et al., 2012).
we combinetest2016 and test2017 as our test set whilethe rest as the development set..• news, which is from news commentary v11corpus.9 we use news-test2015 and news-test2016 as the development set and test set,respectively..ldc2009t15, ldc2010t03..5https://www.statmt.org/wmt14/transla.
tion-task.html.
6https://github.com/messense/jieba-rs7as related studies, we lowercase english sentences in zh-en while truecase english and german sentences in en-de.
ldc2004t07,ldc2005t06, ldc2005t10, ldc2009t02, ldc2009t15,ldc2010t03.
note that they are also included in zh-enparallel dataset..ldc2002t01,.
consists.
8it.
of.
9http://www.casmacat.eu/corpus/news-co.mmentary.html.
2855zh-en.
# model.
en-de (ted) en-de (news) en-de (europarl)bi-sentbleu meteor bleu meteor bleu meteor bleu meteor bleu meteor(cid:55)40.4340.32doct (zhang et al., 2018)han (miculicich et al., 2018) (cid:55)40.9440.83(cid:55)41.2641.01san (maruf et al., 2019)(cid:55)--qcn (yang et al., 2019)(cid:55)-40.92mcn (zheng et al., 2020)(cid:55)39.6139.64#1 transformer(cid:55)#2 ours-sent41.3340.73(cid:55)41.76#3 ours-doc41.27(cid:51) (cid:51) 46.3043.62#4 transformer(cid:51) (cid:51) 49.58#5 ours-sent45.40(cid:51) (cid:51) 50.0345.94#6 ours-doc.
27.9328.1928.37-28.2527.5627.9728.4632.9135.9736.50.
46.7246.0947.2247.86-45.8347.5547.9347.5048.2949.02.
24.0024.5824.4225.1925.1023.0224.7525.3126.9428.7329.31.
23.0825.0324.8422.3724.9122.0324.1924.7026.8028.4129.01.
44.6945.4845.2646.09-43.6645.8346.3047.0648.8049.40.
42.4044.0244.1741.88-41.3743.9644.3846.9948.5248.83.
29.3228.6029.7529.8230.4028.6529.1030.0729.9030.6131.52.
29.1829.7630.00-30.3328.3329.6930.3432.4834.3334.97.mo-doc(cid:55)(cid:55)(cid:55)(cid:55)(cid:55)(cid:55)(cid:55)(cid:55).
avg..table 1: performance (bleu and meteor scores) on test sets.
bi-sent/mo-doc indicates if the models are pre-trained on sentence-level parallel dataset or monolingual documents ((cid:55) for no and (cid:51) for yes).
ours-sent/ours-docindicates that we use sentences or documents as input units, i.e., performing sentence-level nmt or context-awarenmt.
scores are obtained by running their source code with our model settings..• europarl, which is extracted from the europarlv7.
the training, development and test setsare obtained through randomly splitting thecorpus..all above en-de document-level parallel datasetsare downloaded from maruf et al.
(2019).10 simi-lar to ﬁne-tuning datasets, the pre-processing stepsconsist of word segmentation, tokenization, longdocument split.
then we segment the words intosubwords using the bpe models trained on pre-training datasets.
see appendix a for more statis-tics of the ﬁne-tuning datasets..model settings.
we use opennmt (klein et al.,2017) as the implementation of transformer andimplement our models based on it.11 for all trans-lation models, the numbers of layers in the contextencoder, sentence encoder and decoder (i.e., ng,ne, and nd in fig 3) are set to 6. the hidden sizeand the ﬁlter size are set to 512 and 2048, respec-tively.
the number of heads in multi-head attentionis 8 and the dropout rate is 0.1. in pre-training, wetrain the models for 500k steps on four v100 gpuswith batch-size 8192. we use adam (kingma andba, 2015) with β1 = 0.9, β2 = 0.98 for optimiza-tion, and learning rate as 1, the warm-up step as16k.
in ﬁne-tuning, we ﬁne-tune the models for200k steps on a single v100 gpu with batch-size8192, learning rate 0.3, and warm-up step 4k.
ininferring, we set the beam size to 5..10https://github.com/sameenmaruf/selec.
tive-attn/tree/master/data.
evaluation.
for evaluation, we use two metrics:bleu (papineni et al., 2002) and meteor (lavieand agarwal, 2007) to evaluate translation quality..4.2 experimental results.
main results.
table 1 shows the performanceof our approach, where ours-sent andours-doc indicate the performance achieved byour approach when we use sentences or documentsas input units, respectively.
in the scenario whereboth sentence-level parallel dataset and monolin-gual documents are not used, we directly train ourmodels from scratch with the two ﬁne-tuning taskson the ﬁne-tuning datasets.
#2 and #3 in the ta-ble show that our model is capable of translatingboth sentences and documents.
interestingly, whenwe use sentences as translation units, our models(i.e., #2 ours-sent) outperform sentence-leveltransformer baseline (i.e., #1 who uses sentencesas input units in both training and inferring) overall translation tasks with improvement of averaged1.36 bleu and 1.72 meteor.
moreover, when weuse documents as translation units, our models (i.e.,#3 ours-doc) achieve further improvement bymodeling document-level context.
compared toprevious studies, it also shows that our approachsurpasses all context-aware baselines on zh-enand en-de (ted) tasks and achieves the state-of-the-art on average..in the scenario where both sentence-level paral-lel dataset and monolingual documents are used,12similar performance trends also hold.
for example,#5 ours-sent signiﬁcantly exceeds transformer.
11our code is available at https://github.com/strawberry116/breaking-corpus-bottleneck-for-context-aware-nmt.
12for transformer baseline (i.e., #4 in the table), thetwo pre-training objectives in document-level restoration arecontext-agnostic..2856model.
trans.
ourstrans.
ourstrans.
ourstrans.
ours.
bi-sent(cid:55)(cid:55)(cid:51)(cid:51)(cid:55)(cid:55)(cid:51)(cid:51).
zh-en.
en-de (news).
mo-docbleu meteor bleu meteor(cid:55)39.64(cid:55)41.27(cid:55)46.99(cid:55)48.03(cid:51) 40.32(cid:51) 42.64(cid:51) 46.30(cid:51) 50.03.
22.0324.7026.8928.3224.6225.3026.8029.01.
41.3744.3847.0148.1644.8345.6046.9948.83.
27.5628.4633.4634.2728.6430.1932.9136.50.table 2: ablation studies on zh-en and en-de(news) translation tasks.
hereafter, we use ours forours-doc, i.e., using documents as input units..baseline with 1.85 bleu and 1.78 meteor on aver-age while #6 outs-doc further achieves the bestperformance..ablation study.
we take zh-en and en-de(news) translations as representatives to study theeffect of leveraging sentence-level parallel datasetand monolingual documents..table 2 compares the performance on the the testsets of zh-en and en-de (news) translations indifferent scenarios.
from it, we have the followingobservations..• using either sentence-level parallel dataset ormonolingual documents helps translation forboth transformer baselines and our context-aware models.
however, in the presenceof sentence-level parallel dataset, the trans-former baselines fail to achieve higher perfor-mance with monolingual documents, as weobserve performance drops from 46.99 bleuto 46.30 on zh-en, and from 26.89 to 26.80on en-de.
in contrary, our models achievethe highest performance by leveraging the tworesources.
this suggests the effectiveness ofour approach in employing the two resources..• it is not surprising to ﬁnd out that the improve-ment is mainly contributed by using sentence-level parallel dataset, as translation model ismore important than context encoder.
• finally, our approach consistently outper-forms sentence-level transformer in all sce-narios.
encouraging, the performance gapbecomes even larger on zh-en when moreresources are used..fine-tuning.
w/ sentence-level.
w/o sentence-level.
inferring-input bleu50.03document49.58sentence50.10document48.33sentence.
table 3: performance on zh-en translation with re-spect to different ﬁne-tuning strategies and different in-put units in inferring..model bi-sent mo-doc deixis50.0trans.
62.3ours.
(cid:55)(cid:55).
(cid:55)(cid:55).
trans.
ours.
(cid:51)(cid:51).
(cid:51)(cid:51).
50.981.9.lex.c45.347.9.
46.461.7.ell.inﬂ.
52.064.9.ell.vp27.336.0.
67.270.6.
75.680.5.table 4: accuracy (%) of discourse phenomena..5 discussion.
next we use zh-en translation to analyze more onhow our approach affects translation performance.
see appendix b for parameter analysis and statis-tics of the pre-trained models..5.1 effect of joint fine-tuning.
in section 3 we alternate sentence-level transla-tion and document-level translation in ﬁne-tuning.
we investigate the effect of including sentence-level translation as a ﬁne-tuning task.
table 3compares the performance with respect to differentﬁne-tuning strategies and different input units ininferring.
when we use documents as input units ininferring, the joint ﬁne-tuning strategy provides noadvantage.
however, when the input units are sen-tences, the joint ﬁne-tuning strategy outperformsthe one not including sentence-level translation inﬁne-tuning..5.2 analysis of discourse phenomena.
we also want to examine whether the proposedapproach actually learns to utilize document con-text to resolve discourse inconsistencies.
follow-ing voita et al.
(2019b) and zheng et al.
(2020),we use the same datasets to train model and con-trastive test set for the evaluation of discourse phe-nomena for english-russian by voita et al.
(2019b).
there are four test sets in the suite regarding deixis,lexicon consistency, ellipsis (inﬂection and verbphrase).
each testset contains groups of contrastiveexamples consisting of a positive translation withcorrect discourse phenomenon and negative trans-lations with incorrect phenomena.
the goal is toﬁgure out if a model is more likely to generate a cor-.
2857bi-sent mo-doc.
modeltrans.
ours.
trans.
ours.
(cid:55)(cid:55).
(cid:51)(cid:51).
(cid:55)(cid:55).
(cid:51)(cid:51).
dev67.3068.33.
71.0272.11.test68.6069.73.
70.5170.89.table 5: evaluation on pronoun translations of zh-en..ratio (%)102030.dev50.6450.9050.59.test49.8950.0349.70.table 6: performance (bleu scores) on dev and testsets of zh-en translation with respect to different gapsentence ratios in pre-training task of document-levelrestoration..rect translation compared to the incorrect variation.
we summarize the results in table 4, which showsthat in different scenarios our models are betterat resolving discourse consistencies than context-agnostic baselines..5.3 pronoun translation.
we follow miculicich et al.
(2018) and tan et al.
(2019) to evaluate coreference and anaphora usingthe reference-based metric: accuracy of pronountranslation (werlen and popescu-belis, 2017)..table 5 lists the performance of pronoun trans-lation.
from it we observe that our proposed ap-proach can well improve the performance of pro-noun translations..5.4 effect of gap sentence ratio.
a signiﬁcant hyper-parameter in the pre-trainingtask of document-level restoration is the gap sen-tence ratio.
a low ratio makes the document-levelrestoration less challenging while choosing gap sen-tences at a high ratio makes the global context havemore overlapped.
table 6 shows that we achievethe best performance when the ratio is set as 20%..5.5 effect of pre-training objectives.
as shown in figure 2, we include two pre-trainingobjectives in document-level restoration, i.e, ca-gsr and ca-msr.
to investigate the effect ofca-gsr, we use ca-msr as the only objectivein this pre-training task.
in this way, the s3 ands5 in figure 2 (a), for example, will be (cid:99)x3 and (cid:99)x5,respectively.
table 7 compares the performancewhen the pre-training task is of ca-msr objec-tive or combination of ca-gsr and ca-msr.it.
pre-training objectiveca-gsr + ca-msrca-msr.
dev50.9050.61.test50.0349.73.table 7: performance (bleu scores) on dev and testsets of zh-en translation with respect to different pre-training objectives in document-level restoration..shows the combining objective achieves better per-formance than using ca-msr alone..6 related work.
we describe related studies in the following twoperspectives..6.1 context-aware nmt.
cache/memory-based approaches (tu et al., 2018;kuang et al., 2018; maruf and haffari, 2018; wanget al., 2017) store word/sentence translation in pre-vious sentences for future sentence translation.
var-ious approaches with an extra context encoders areproposed to model either local context, e.g., previ-ous sentences (jean et al., 2017; wang et al., 2017;zhang et al., 2018; bawden et al., 2018; voita et al.,2018, 2019b; yang et al., 2019; huo et al., 2020), orentire document (maruf and haffari, 2018; maceand servan, 2019; maruf et al., 2019; tan et al.,2019; xiong et al., 2019; zheng et al., 2020; kanget al., 2020)..besides, there have been several attempts toimprove context-aware nmt with monolingualdocument data.
to make translations more co-herent within a document, voita et al.
(2019a)propose docrepair trained on monolingual tar-get language documents to correct the inconsis-tencies in sentence-level translation while yu et al.
(2020) train a context-aware language model to re-rank sentence-level translations.
finally, junczys-dowmunt (2019) use source-side monolingual doc-uments to explore multi-task training via the bert-objective on the encoder.
they simply concatenatesentences within a document into a long sequence,which is different from our approach..6.2 pre-training for document-level nmt.
while there are substantial studies on improvingsentence-level nmt with pre-training, we limitourselves here to pre-training for document-level(context-aware) nmt.
bart (lewis et al., 2020)is a denoising auto-encoder model which learns toreconstruct the original document from a noisedversion.
inspired by bart, mbart (liu et al.,.
28582020) is a model trained on a mixed corpus con-taining monolingual documents of different lan-guages.
both bart and mbart concatenatesentences in one document into a long sequence,and thus fall into a standard sequence-to-sequence(seq2seq) framework.
this is very different fromour cross-task pre-training, in which we combineboth context-agnostic learning and context-awarelearning in a single model..7 conclusion.
in order to leverage both large-scale sentence-levelparallel dataset and source-side monolingual doc-uments for context-aware nmt, in this paper, wehave proposed a novel cross-task pre-training ap-proach, which simultaneously learns to translatea sentence from source language to target lan-guage while denoising a document from deliber-ately noised to original.
upon the pre-trained mod-els, we ﬁne-tune them with document-level paralleldataset from both sentence-level and document-level perspectives.
experimental results on multi-ple document-level translation tasks have demon-strate the effectiveness of our approach.
finally, wealso provide insights on how context-aware nmtbeneﬁts from our approach..acknowledgments.
this work was supported by the national naturalscience foundation of china (grant no.
62036004and 61876120)..references.
rachel bawden, rico sennrich, alexandra birch, andbarry haddow.
2018. evaluating discourse phenom-ena in neural machine translation.
in proceedings ofnaacl, pages 1304–1313..mauro cettolo, christian girardi, and marcello fed-erico.
2012. wit3: web inventory of transcribedand translated talks.
in proceedings of eamt, pages261–268..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in proceedings of naacl, pages 4171–4186..sebastien jean, stanislas lauly, orhan firat, andkyunghyun cho.
2017. does neural machinetrans-lation beneﬁt from larger context?
computing re-search repository, arxiv:1704.05135..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,and fernanda vi´egas.
2017. google’s multilingualneural machine translation system: enabling zero-shot translation.
tacl, 5:339–351..marcin junczys-dowmunt.
2019. microsoft translatorat wmt 2019: towards large-scale document-levelneural machine translation.
in proceedings of wmt,pages 225–233..xiaomian kang, yang zhao,.
jiajun zhang, andchengqing zong.
2020. dynamic context selectionfor document-level neural machine translation via re-in proceedings of emnlp,inforcement learning.
pages 2242–2254..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof iclr..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander rush.
2017. opennmt: open-insource toolkit for neural machine translation.
proceedings of acl 2017, system demonstrations,pages 67–72..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of acl 2007, system demonstrations,pages 177–180..shaohui kuang, deyi xiong, weihua luo, andguodong zhou.
2018. modeling coherence forneural machine translation with dynamic and topiccaches.
in proceedings of coling, pages 596–606..alon lavie and abhaya agarwal.
2007. meteor: anautomatic metric for mt evaluation with high levelsin proceed-of correlation with human judgments.
ings of wmt, pages 228–231..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2020.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andin proceedings of acl, pagescomprehension.
7871–7880..jingjing huo, christian herold, yingbo gao, leonarddahlmann, shahram khadivi, and hermann ney.
2020. diving deep into context-aware neural ma-in proceedings of wmt, pageschine translation.
604–616..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
tacl,8:726–742..2859lesly miculicich werlen and andrei popescu-belis.
2017. validation of an automatic metric for the ac-curacy of pronoun translation (apt).
in proceedingsof workshop on discourse in machine translation,pages 17–25..hao xiong, zhongjun he, hua wu, and haifeng wang.
2019. modeling coherence for discourse neural ma-in proceedings of aaai, pageschine translation.
7338–7345..zhengxin yang,.
jinchao zhang, fandong meng,shuhao gu, yang feng, and jie zhou.
2019. en-hancing context modeling with a query-guided cap-sule network for document-level translation.
in pro-ceedings of emnlp-ijcnlp, pages 1527–1537..lei yu, laurent sartran, wojciech stokowiec, wangling, lingpeng kong, phil blunsom, and chrisdyer.
2020. better document-level machine trans-lation with bayes rule.
tacl, 8:346–360..jiacheng zhang, huanbo luan, maosong sun, feifeizhai, jingfang xu, min zhang, and yang liu.
2018.improving the transformer translation model withdocument-level context.
in proceedings of emnlp,pages 533–542..jingqing zhang, yao zhao, mohammad saleh, and pe-ter j. liu.
2020. pegasus: pre-training with ex-tracted gap-sentences for abstractive summarization.
in proceedings of icml..zaixiang zheng, xiang yue, shujian huang, jiajunchen, and alexandra birch.
2020. towards makingthe most of context in neural machine translation.
inproceedings of ijcai, pages 3983–3989..valentin mace and christophe servan.
2019. usingwhole document context in neural machine transla-tion.
in proceedings of iwslt..sameen maruf and gholamreza haffari.
2018. docu-ment context neural machine translation with mem-ory networks.
in proceedings of acl, pages 1275–1284..sameen maruf, andr´e f. t. martins, and gholam-reza haffari.
2019. selective attention for context-aware neural machine translation.
in proceedings ofnaacl, pages 3092–3102..lesly miculicich, dhananjay ram, nikolaos pappas,and james henderson.
2018. document-level neu-ral machine translation with hierarchical attentionnetworks.
in proceedings of emnlp, pages 2947–2954..kishore papineni, salim roukos, ward todd, and wei-jing zhu.
2002. bleu: a method for automatic evalu-ation of machine translation.
in proceedings of acl,pages 311–318..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of acl, pages 1715–1725..xin tan, longyin zhang, deyi xiong, and guodongzhou.
2019. hierarchical modeling of global con-text for document-level neural machine translation.
in proceedings of emnlp-ijcnlp, pages 1576–1585..j¨org tiedemann and yves scherrer.
2017. neural ma-chine translation with extended context.
in proceed-ings of the third workshop on discourse in machinetranslation, pages 82–92..zhaopeng tu, yang liu, shuming shi, and tong zhang.
2018. learning to remember translation history witha continuous cache.
tacl, 6:407–420..elena voita, rico sennrich, and ivan titov.
2019a.
context-aware monolingual repair for neural ma-in proceedings of emnlp-chine translation.
ijcnlp, pages 877–886..elena voita, rico sennrich, and ivan titov.
2019b.
when a good translation is wrong in context:context-aware machine translation improves ondeixis, ellipsis, and lexical cohesion.
in proceedingsof acl, pages 1198–1212..elena voita, pavel serdyukov, rico sennrich, and ivantitov.
2018. context-aware neural machine transla-tion learns anaphora resolution.
in proceedings ofacl, pages 1264–1274..longyue wang, zhaopeng tu, andy way, and qun liu.
2017. exploiting cross-sentence context for neu-ral machine translation.
in proceedings of emnlp,pages 2826–2831..2860a experimental datasets.
table 8 summarizes statistics of the four translationtasks.
note that we split long documents into sub-documents with at most 30 sentences for efﬁcienttraining..set.
trainingdevtest.
set.
zh-en.
en-de (europarl).
#subdoc47,75882627.
#sent781,5241,6645,833.
#subdoc132,721273415.
#sent1,666,9043,5875,134.en-de (ted).
en-de (news).
trainingdevtest.
#subdoc7,49132687.
#sent#sent236,287206,1262,1698,9672,9992,271table 8: statistics of the training, development, and testsets of the four translation tasks..#subdoc10,552112184.b more result analysis.
b.1 model parameters.
table 9 presents the numbers of parameters for zh-en and en-de translations.
note that for all en-de translation tasks, the numbers of parameters aresame as the vocabulary for them are shared.
thetable shows that our models introduce very limitedparameters to encode document-level context..modeltransformerours.
zh-en en-de80.6m 61.4m86.2m 64.0 m.table 9: model parameters for zh-en and en-detranslations..b.2 statistics on our pre-trained models.
table 10 presents statistics on our two pre-trainedmodels for zh-en and en-de translations.
with500k training steps, and within 120 (130) hourswe complete 3.0 (1.2) and 35 (20) passes over thesentence-level parallel dataset and monolingualdocument dataset for chinese (english), respec-tively..translationzh-enen-de.
#epochon bi-sent3520.
#epoch.
on mo-doc time120h130h.
3.01.2.table 10: statistics on our two pre-trained models forzh-en and en-de translations..2861