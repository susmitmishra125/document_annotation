bertifying the hidden markov model for multi-source weaklysupervised named entity recognition.
yinghao li1, pranav shetty1, lucas liu1, chao zhang1, and le song21 georgia institute of technology, atlanta, usa2 mohamed bin zayed university of artiﬁcial intelligence, abu dhabi, united arab emirates{yinghaoli, pranav.shetty, lucasliu, chaozhang}@gatech.edule.song@mbzuai.ac.ae.
abstract.
we study the problem of learning a named en-tity recognition (ner) tagger using noisy la-bels from multiple weak supervision sources.
though cheap to obtain, the labels from weaksupervision sources are often incomplete, in-accurate, and contradictory, making it difﬁcultto learn an accurate ner model.
to addressthis challenge, we propose a conditional hid-den markov model (chmm), which can effec-tively infer true labels from multi-source noisylabels in an unsupervised way.
chmm en-hances the classic hidden markov model withthe contextual representation power of pre-trained language models.
speciﬁcally, chmmlearns token-wise transition and emission prob-abilities from the bert embeddings of the in-put tokens to infer the latent true labels fromnoisy observations.
we further reﬁne chmmwith an alternate-training approach (chmm-alt).
it ﬁne-tunes a bert-ner model withthe labels inferred by chmm, and this bert-ner’s outputis regarded as an additionalweak source to train the chmm in return.
ex-periments on four ner benchmarks from var-ious domains show that our method outper-forms state-of-the-art weakly supervised nermodels by wide margins..1.introduction.
named entity recognition (ner), which aims toidentify named entities from unstructured text, is aninformation extraction task fundamental to manydownstream applications such as event detection(li et al., 2012), relationship extraction (bach andbadaskar, 2007), and question answering (khalidet al., 2008).
existing ner models are typicallysupervised by a large number of training sequences,each pre-annotated with token-level labels.
in prac-tice, however, obtaining such labels could be pro-hibitively expensive.
on the other hand, many do-mains have various knowledge resources such as.
knowledge bases, domain-speciﬁc dictionaries, orlabeling rules provided by domain experts (far-makiotou et al., 2000; nadeau and sekine, 2007).
these resources can be used to match a corpus andquickly create large-scale noisy training data forner from multiple views..learning an ner model from multiple weak su-pervision sources is a challenging problem.
whilethere are works on distantly supervised ner thatuse only knowledge bases as weak supervision(mintz et al., 2009; shang et al., 2018; cao et al.,2019; liang et al., 2020), they cannot leveragecomplementary information from multiple annota-tion sources.
to handle multi-source weak super-vision, several recent works (nguyen et al., 2017;safranchik et al., 2020; lison et al., 2020) leveragethe hidden markov model (hmm), by modelingtrue labels as hidden variables and inferring themfrom the observed noisy labels through unsuper-vised learning.
though principled, these modelsfall short in capturing token semantics and contextinformation, as they either model input tokens asone-hot observations (nguyen et al., 2017) or donot model them at all (safranchik et al., 2020; li-son et al., 2020).
moreover, the ﬂexibility of hmmis limited as its transitions and emissions remainconstant over time steps, whereas in practice theyshould depend on the input words..we propose the conditional hidden markovmodel (chmm) to infer true ner labels frommulti-source weak annotations.
chmm conditionsthe hmm training and inference on bert by pre-dicting token-wise transition and emission proba-bilities from the bert embeddings.
these token-wise probabilities are more ﬂexible than hmm’sconstant counterpart in modeling how the true la-bels should evolve according to the input tokens.
the context representation ability they inherit frombert also relieves the markov constraint and ex-pands hmm’s context-awareness..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6178–6190august1–6,2021.©2021associationforcomputationallinguistics6178further, we integrate chmm with a supervisedbert-based ner mode with an alternate-trainingmethod (chmm-alt).
it ﬁne-tunes bert-nerwith the denoised labels generated by chmm.
tak-ing advantage of the pre-trained knowledge con-tained in bert, this process aims to reﬁne thedenoised labels by discovering the entity patternsneglected by all of the weak sources.
the ﬁne-tuned bert-ner serves as an additional supervi-sion source, whose output is combined with otherweak labels for the next round of chmm train-ing.
chmm-alt trains chmm and bert-neralternately until the result is optimized..our contributions include:.
• a multi-source label aggregator chmm withtoken-wise transition and emission probabili-ties for aggregating multiple sets of ner la-bels from different weak labeling sources..• an alternate-training method chmm-altthat trains chmm and bert-ner in turn uti-lizing each other’s outputs for multiple loopsto optimize the multi-source weakly super-vised ner performance..• a comprehensive evaluation on four nerbenchmarks from different domains demon-strates that chmm-alt achieves a 4.83 aver-age f1 score improvement over the strongestbaseline models..the code and data used in this work are availableat github.com/yinghao-li/chmm-alt..2 related work.
weakly supervised ner there have beenworks that train ner models with different weaksupervision approaches.
distant supervision, a spe-ciﬁc type of weak supervision, generates traininglabels from knowledge bases (mintz et al., 2009;yang et al., 2018; shang et al., 2018; cao et al.,2019; liang et al., 2020).
but such a method islimited to one source and falls short of acquiringsupplementary annotations from other available re-sources.
other works adopt multiple additionallabeling sources, such as heuristic functions thatdepend on lexical features, word patterns, or docu-ment information (nadeau and sekine, 2007; rat-ner et al., 2016), and unify their results throughmulti-source label denoising.
several multi-sourceweakly supervised learning approaches are de-signed for sentence classiﬁcation (ratner et al.,.
2017, 2019; ren et al., 2020; yu et al., 2020).
although these methods can be adapted for se-quence labeling tasks such as ner, they tend tooverlook the internal dependency relationship be-tween token-level labels during the inference.
frieset al.
(2017) target the ner task, but their methodﬁrst generates candidate named entity spans andthen classiﬁes each span independently.
this inde-pendence makes it suffer from the same drawbackas sentence classiﬁcation models..a few works consider label dependency whiledealing with multiple supervision sources.
lanet al.
(2020) train a bilstm-crf network (huanget al., 2015) with multiple parallel crf layers, eachfor an individual labeling source, and aggregatetheir transitions with conﬁdence scores predictedby an attention network (bahdanau et al., 2015; lu-ong et al., 2015).
hmm is a more principled modelfor multi-source sequential label denoising as thetrue labels are implicitly inferred through unsuper-vised learning without deliberately assigning anyadditional scores.
following this track, nguyenet al.
(2017) and lison et al.
(2020) use a stan-dard hmm with multiple observed variables, eachfrom one labeling source.
safranchik et al.
(2020)propose linked hmm, which differs from ordinaryhmm by introducing unique linking rules as anadjunct supervision source additional to generaltoken labels.
however, these methods fail to utilizethe context information embedded in the tokens aseffectively as chmm, and their ner performanceis further constrained by the markov assumption..neuralizing the hidden markov model someworks attempt to neuralize hmm in order to re-lax the markov assumption while maintaining itsgenerative property (kim et al., 2018).
for ex-ample, dai et al.
(2017) and liu et al.
(2018)incorporate recurrent units into the hidden semi-markov model (hsmm) to segment and label high-dimensional time series; wiseman et al.
(2018)learn discrete template structures for conditionaltext generation using neuralized hsmm.
wesselsand omlin (2000) and chiu and rush (2020) fac-torize hmm with neural networks to scale it andimprove its sequence modeling capacity.
the workmost related to ours leverages neural hmm for se-quence labeling (tran et al., 2016).
chmm differsfrom neural hmm in that the tokens are treated asa dependency term in chmm instead of the obser-vation in neural hmm.
besides, chmm is trainedwith generalized em, whereas neural hmm opti-.
6179figure 1: an example of label aggregation with twoweak labeling sources.
we use bio labeling scheme.
per represents person; loc is location..mizes the marginal likelihood of the observations..3 problem setup.
in this section, we formulate the multi-sourceweakly supervised ner problem.
consider an in-put sentence that contains t tokens w(1:t ), nercan be formulated as a sequence labeling task thatassigns a label to each token in the sentence.1 as-suming the set of target entity types is e and the tag-ging scheme is bio (ramshaw and marcus, 1995),ner models assign one label from the label setl ∈ l to each token, where the size of the label setis |l| = 2|e| + 1, e.g., if e = {per, loc}, thenl = {o, b-per, i-per, b-loc, i-loc}..suppose we have a sequence with k weaksources, each of which can be a heuristic rule,knowledge base, or existing out-of-domain nermodel.
each source serves as a labeling functionthat generates token-level weak labels from the in-put corpus, as shown in figure 1. for the input se-quence w(1:t ), we use x(1:t ), k ∈ {1, .
.
.
, k} tokrepresent the weak labels from the source k, wherex(t)k ∈ r|l|, t ∈ {1, .
.
.
, t } is a probability dis-tribution over l. multi-source weakly supervisedner aims to ﬁnd the underlying true sequence oflabels ˆy(1:t ), ˆy(t) ∈ l given {w(1:t ), x(1:t ).
1:k }..4 methodology.
in this section, we describe our proposed methodchmm-alt.
we ﬁrst sketch the alternate-trainingprocedure (§ 4.1), then explain the chmm com-ponent (§ 4.2) and how bert-ner is involved(§ 4.3)..4.1 alternate-training procedure.
the alternate-training method trains two models—a multi-source label aggregator chmm and abert-ner model—in turn with each other’s out-put.
chmm aggregates multiple sets of labelsfrom different sources into a uniﬁed sequence of.
1we represent vectors, matrices or tensors with bold fonts.
and scalars with regular fonts; 1 : a (cid:44) {1, 2, .
.
.
, a}..labels, while bert-ner reﬁnes them by its lan-guage modeling ability gained from pre-training.
the training process is divided into two phases..• in phase i, chmm takes the annotationsx(1:t )1:k from existing sources and gives a setof denoised labels y∗(1:t ), which are usedto ﬁne-tune the bert-ner model.
then,we regard the ﬁne-tuned model as an addi-tional labeling source, whose outputs ˜y(1:t )are added into the original weak label setsto give the updated observation instances:1:k+1 = {x(1:t )x(1:t ).
1:k , ˜y(1:t )}..• in phase ii, chmm and bert-ner mutu-ally improve each other iteratively in severalloops.
each loop ﬁrst trains chmm with theobservation x(1:t )1:k+1 from the previous one.
then, its predictions are adopted to ﬁne-tunebert-ner, whose output updates x(1:t )k+1 ..figure 2 illustrates the alternate-training method.
in general, chmm gives high precision predic-tions, whereas bert-ner trades recall with preci-sion.
in other words, chmm can classify namedentities with high accuracy but is slightly disadvan-taged in discovering all entities.
bert-ner in-creases the coverage with a certain loss of accuracy.
combined with the alternate-training approach, thiscomplementarity between these models further in-creases the overall performance..4.2 conditional hidden markov model.
the conditional hidden markov model is an hmmvariant for multi-source label denoising.
it mod-els true entity labels as hidden variables and infersthem from the observed noisy labels.
tradition-ally, discrete hmm uses one transition matrix tomodel the probability of hidden label transitioningand one emission matrix to model the probabilityof the observations from the hidden labels.
thesetwo matrices are constant, i.e., their values do notchange over time steps.
chmm, on the contrary,conditions both its transition and emission matri-ces on the bert embeddings e(1:t ) of the inputtokens w(1:t ).
this design not only allows chmmto leverage the rich contextual representations ofthe bert embeddings but relieves the constantmatrices constraint as well..in phase i, chmm takes k sets of weak labelsfrom the provided k weak labeling sources.
inphase ii, in addition to the existing sources, it takes.
6180centerinnewyorkwas...b-persource 1oob-loci-loco...b-locsource 2i-locoob-loco...b-loctargeti-locoi-loco...b-locrockefellerfigure 2: the illustration of the alternate-training method.
phase i is acyclic, starting from getting k weak labelsfrom supervision sources and ending at the ﬁne-tuning of bert-ner with chmm’s denoised output.
phase iicontains several loops, each trains chmm with k + 1 sources, including the additional bert predictions fromthe previous loop, and ﬁne-tunes bert-ner using the updated denoised labels..hidden label is i at time step t..for each step, e(t) ∈ rdemb is the output of apre-trained bert with demb being its embeddingdimension.
ψ(t) and φ(t)1:k are calculated by apply-ing a multi-layer perceptron (mlp) to e(t):.
s(t) ∈ r|l|2.
= mlp(e(t)),.
h(t) ∈ r|l|·|l|·k = mlp(e(t))..(1).
(2).
since the mlp outputs are vectors, we need toreshape them to matrices or tensors:.
s(t) ∈ r|l|×|l| = reshape(s(t)),h (t) ∈ r|l|×|l|×k = reshape(h(t))..(3).
(4).
to achieve the proper probability distributions, weapply the softmax function along the label axis sothat these values are positive and sum up to 1:.
ψ(t).
i,1:|l| = σ(s(t).
i,1:|l|), φ(t).
i,1:|l|,k = σ(h (t).
i,1:|l|,k),.
σ(a)i =.
exp (ai)j exp (aj).
..(cid:80).
(5).
a is an arbitrary vector.
the formulae in the fol-lowing discussion always depend on e(1:t ), but wewill omit the dependency term for simplicity..model training according to the generative pro-cess of chmm, the joint distribution of the hid-den states and the observed weak labels for onesequence p(z(0:t ), x(1:t )|θ) can be factorized as:.
p(z(0:t ), x(1:t )|θ) = p(z(0))p(x(1:t )|z(1:t )).
= p(z(0)).
p(z(t)|z(t−1)).
p(x(t)|z(t)),.
t(cid:89).
t=1.
t(cid:89).
t=1.
(6).
figure 3: an illustration of chmm’s architecture.
shaded circles are observed elements; white circles arehidden elements; rectangles are matrices.
roundedrectangles are multi-layer perceptrons containing thetrainable parameters.
the arrows between w(t) ande(t) denote the context representation ability of bert.
mlp denotes the “multi-layer perceptron”..another set of labels from the previously ﬁne-tunedbert-ner, making the total number of sourcesk + 1. for convenience, we use k as the numberof weak sources below..where.
model architecture figure 3 shows a sketch ofchmm’s architecture.2 z(1:t ) denotes the discretehidden states of chmm with z(t) ∈ l, repre-senting the underlying true labels to be inferredfrom multiple weak annotations.
ψ(t) ∈ r|l|×|l|is the transition matrix, whose element ψ(t)i,j =p(z(t) = j|z(t−1) = i, e(t)), i, j ∈ {1, .
.
.
, |l|}denotes the probability of moving from label i to la-bel j at time step t. φ(t)k ∈ r|l|×|l| is the emissionmatrix of weak source k, each element in whichi,j,k = p(x(t)φ(t)j,k = 1|z(t) = i, e(t)) represents theprobability of source k observing label j when the.
2we relax plate notation here to present details..where θ represents all the trainable parameters..6181chmmaggregated labelstrain with generalized emtrain with kld lossphase iphase iibert predictions bert-nerbert embeddings weak labels 1: bertinput sentenceweak source 1......weak labels k: weak source k......mlpmlpmlpmlpmlphmm is generally trained with an expectation-maximization (em, also known as baum-welch)algorithm.
in the expectation step (e-step), wecompute the expected complete data log likelihood:.
q(θ, θold) (cid:44) ez[(cid:96)c(θ)|θold].
(7)θold is the parameters from the previous trainingstep, ez[·] is the expectation over variable z, and.
(cid:96)c(θ) (cid:44) log p(z(0:t ), x(1:t )|θ).
is the comptelete data log likelihood.
let ϕ(t) ∈r|l| be the observation likelihood where.
ϕ(t)i.
(cid:44) p(x(t)|z(t) = i) =.
i,j,kx(t)φ(t).
j,k.
(8).
k(cid:89).
|l|(cid:88).
k=1.
j=1.
combining (6)–(8) together, we have.
q(θ, θold) =.
γ(0)i.log πi+.
|l|(cid:88).
i=1.
t(cid:88).
|l|(cid:88).
|l|(cid:88).
t=1.
i=1.
j=1.
i,j log ψ(t)ξ(t).
i,j +.
γ(t)i.log ϕ(t)i.,.
t(cid:88).
|l|(cid:88).
t=1.
i=1.
(9)where π1 = 1, π2:|l| = 0;3 γ(t)(cid:44) p(z(t) =ii|x(1:t )) is the smoothed marginal; ξ(t)(cid:44)i,jp(z(t−1) = i, z(t) = j|x(1:t )) is the expected num-ber of transitions.
these parameters are computedusing the forward-backward algorithm.4.
in the maximization step (m-step), traditionalhmm updates parameters θhmm = {ψ, φ, π} byoptimizing (7) with pseudo-statistics.5 however,as the transitions and emissions in chmm are notstandalone parameters, we cannot directly optimizechmm by this method.
instead, we update themodel parameters through gradient descent w.r.t.
θchmm using (9) as the objective function:.
∇θchmm =.
∂q(θchmm, θold.
chmm).
∂θchmm.
..(10).
in practice, the calculation is conducted in thelogarithm domain to avoid the loss of precisionissue that occurs when the ﬂoating-point numbersbecome too small..to solve the label sparsity issue, i.e., some en-tities are only observed by a minority of the weak.
3this assumes the initial hidden state is always o. in prac-tice, we set π(cid:96) = (cid:15), ∀(cid:96) ∈ 2 : |l| and π1 = 1 − (|l| − 1)(cid:15),where (cid:15) is a small value, to avoid getting −∞ from log..4details are presented in appendix a.1.
5details are presented in appendix a.2..sources, we modify the observations x(1:t ) beforeif one source k observes an entity attraining.
time step t: x(t)j(cid:54)=1,k > 0, the observation of non-observing sources at t will be modiﬁed to x(t)1,κ =(cid:15); x(t)j(cid:54)=1,κ = (1 − (cid:15))/|l|, ∀κ ∈ {1, .
.
.
, k}\k,where (cid:15) is an arbitrary small value.
note that x(t)1,κcorresponds to the observed label o..chmm initialization generally, hmm has itstransition and emission probabilities initializedwith the statistics ψ∗ and φ∗ computed from theobservation set.
but it is impossible to directlyset ψ(t) and φ(t) in chmm to these values, asthese matrices are the output of the mlps ratherthan standalone parameters.
to address this issue,we choose to pre-train the mlps before startingchmm’s training by minimizing the mean squarederror (mse) loss between their outputs and thetarget statistics:.
(cid:96)mse =.
(cid:107)ψ∗ − s(t)(cid:107)2.f + (cid:107)φ∗ − h (t)(cid:107)2f ,.
1t.(cid:88).
t.where (cid:107) · (cid:107)f is the frobenius norm.
right afterinitialization, mlps can only output similar prob-abilities for all time steps: ψ(t) ≈ ψ∗, φ(t) ≈φ∗, ∀t ∈ {1, 2, .
.
.
, t }.
but their token-wise pre-diction divergence will emerge when chmm hasbeen trained.
the initial hidden state z(0) is ﬁxedto o as it has no corresponding token..inference once trained, chmm can provide themost probable sequence of hidden labels ˆz(1:t )along with the probabilities of all labels y∗(1:t )..ˆz(1:t ) = arg max.
p ˆθchmm.
(z(1:t )|x(1:t ).
1:k , e(1:t )),.
z(1:t )i = p ˆθchmm.
y∗(t).
(z(t) = i|x(1:t ).
1:k , e(1:t )),.
where ˆθchmm represents the trained parameters.
these results can be calculated by either the viterbidecoding algorithm (viterbi, 1967) or directly max-imizing the smoothed marginal γ(1:t )..4.3.improving denoised labels with bert.
the pre-trained bert model encodes semanticand structural knowledge, which can be distilledto further reﬁne the denoised labels from chmm.
speciﬁcally, we construct the bert-ner modelby stacking a feed-forward layer and a softmaxlayer on top of the original bert to predict theprobabilities of the classes that each token belongs.
6182to (sun et al., 2019).
the probability predictionsof chmm, y∗(1:t ), often referred to as soft labels,are chosen to supervise the ﬁne-tuning procedure.
compared with the hard labels ˆz(1:t ), soft labelslead to a more stable training process and highermodel robustness (thiel, 2008; liang et al., 2020).
we train bert-ner by minimizing thekullback-leibler divergence (kl divergence) be-tween the soft labels y∗ and the model output y:.
ˆθbert = arg minθbert.
d[y∗(1:t )(cid:107)y(1:t )].
= arg min.
θbert.
t(cid:88).
|l|(cid:88).
t=1.
i=1.
y∗(t)i.log.
y∗(t)iy(t)i.,.
(11).
where θbert denotes all the trainable parametersin the bert model.
bert-ner does not updatethe embeddings e(1:t ) that chmm depends on..we obtain the reﬁned labels ˜y(1:t ) ∈ rt ×|l|from the ﬁne-tuned bert-ner directly through aforward pass.
different from chmm, we continuebert-ner’s training with parameter weights fromthe last loop’s checkpoint so that the model is ini-tialized closer to the optimum.
correspondingly,phase ii trains bert-ner with a smaller learn-ing rate, fewer epoch iterations, and batch gradientdescent instead of the mini-batch version.6 thisstrategy speeds up phase ii training without sacri-ﬁcing the model performance as y∗(1:t ) does notchange signiﬁcantly from loop to loop..5 experiments.
we benchmark chmm-alt on four datasetsagainst state-of-the-art weakly supervised nerbaselines, including both distant learning modelsand multi-source label aggregation models.
wealso conduct a series of ablation studies to evaluatethe different components in chmm-alt’s design..5.1 setup.
datasets we consider four ner datasets cover-ing the general, technological and biomedical do-mains: 1) conll 2003 (english subset) (tjongkim sang and de meulder, 2003) is a general do-main dataset containing 22,137 sentences manu-ally labelled with 4 entity types.
2) laptopre-view dataset (pontiki et al., 2014) consists of 3,845sentences with laptop-related entity mentions.
3)ncbi-disease dataset (dogan et al., 2014) con-tains 793 pubmed abstracts annotated with disease.
co03.
ncbi.
cdr.
lr.
# instance# training# development# test.
22,13714,0413,2503,453.
1,500500500500.
3,8452,436609800.ave# tokens.
14.5.
219.8.
217.7.
16.4.
# entities# sources.
413.
28.
14.
793593100100.
15.table 1: dataset statistics.
co03 is conll 2003; lris laptopreview; cdr is bc5cdr.
“# sources” indi-cates the number of labeling sources for each dataset..mentions.
4) bc5cdr (li et al., 2016), the datasetaccompanies the biocreative v cdr challenge,consists of 1,500 pubmed articles, annotated withchemical disease mentions..table 1 shows dataset statistics, including the av-erage number of tokens, entities and weak labelingsources.
we use the original word tokens in thedataset if provided and use nltk (bird and loper,2004) otherwise for sentence tokenization..for weak labeling sources, we use the ones fromlison et al.
(2020) for conll 2003, and the onesfrom safranchik et al.
(2020) for laptopreview,ncbi-disease and bc5cdr.7.
baselines we compare our model to the follow-ing state-of-the-art baselines: 1) majority votingreturns the label for a token that has been observedby most of the sources and randomly chooses oneif it’s a tie; 2) snorkel (ratner et al., 2017) treatseach token in a sequence as i.i.d.
and conductsthe label classiﬁcation without considering its con-text; 3) swellshark (fries et al., 2017) improvessnorkel by predicting all the target entity spansbefore classifying them using na¨ıve bayes; 4) au-toner (shang et al., 2018) augments distant su-pervision by predicting whether two consecutivetokens should be in the same entity span; 5) bond(liang et al., 2020) adopts self-training and high-conﬁdence selection to further boost the distantsupervision performance.
6) hmm is the multi-observation generative model used in lison et al.
(2020) that does not have the integrated neural net-work; 7) linked hmm (safranchik et al., 2020)uses linking rules to provide additional inter-tokenstructural information to the hmm model..for the ablation study, we modify chmm toanother type of i.i.d.
model by taking away its tran-sition matrices.
this model, named chmm-i.i.d.,.
6hyper-parameter values are listed in appendix c..7details are presented in appendix b..6183models.
conll 2003.ncbi-disease.
bc5cdr.
laptopreview.
supervised bert-ner ‡ (cid:92)best consensus (cid:92).
90.74 (90.37/91.10)89.18 (100.0/80.47).
88.89 (87.05/90.82)81.60 (100.0/68.91).
88.81 (87.12/90.57)87.58 (100.0/77.89).
81.34 (82.02/80.67)77.72 (100.0/63.55).
swellshark (noun-phrase) †‡swellshark (hand-tuned) †‡autoner †‡snorkel †‡linked hmm †‡bond-mv †‡ (cid:92).
--67.00 (75.21/60.40)66.40 (71.40/62.10)-65.96 (64.22/67.82).
67.10 (64.70/69.70)80.80 (81.60/80.10)75.52 (79.42/71.98)73.41 (71.10/76.00)79.03 (83.46/75.05)80.33 (84.77/76.34).
84.23 (84.98/83.49)84.21 (86.11/82.39)82.13 (83.23/81.06)82.24 (80.23/84.35)82.96 (82.65/83.28)83.18 (82.90/83.49).
--65.44 (72.27/59.79)63.54 (64.09/63.09)69.04 (77.74/62.11)67.19 (68.90/65.75).
majority voting † (cid:92)hmm † (cid:92)chmm-i.i.d.
† (cid:92).
58.40 (49.01/72.24)68.84 (70.80/66.98)68.57 (69.67/67.50).
73.94 (79.76/68.91)73.06 (83.88/64.70)71.69 (83.49/62.87).
80.73 (83.79/77.88)80.57 (88.75/73.76)79.37 (85.68/73.92).
67.92 (72.93/63.55)66.96 (77.46/58.96)65.89 (75.70/58.34).
chmm † (cid:92)chmm + bert-ner †‡ (cid:92)chmm-alt †‡ (cid:92).
70.11 (72.98/67.47)74.30 (75.02/73.58)75.54 (76.22/74.86).
78.88 (93.37/68.28)82.87 (89.42/77.22)85.02 (87.92/82.47).
82.39 (89.93/76.02)84.33 (85.58/83.12)85.12 (84.97/85.28).
73.02 (87.23/62.79)69.67 (75.48/64.70)76.55 (81.39/72.32).
table 2: evaluation results on four datasets.
the results are presented in the “f1 (precision/recall)” format.
“chmm + bert-ner” is essentially chmm-alt’s phase i output.
“bond-mv” is the bond model trainedwith majority voted labels.
† indicates unsupervised label denoiser; ‡ represents fully supervised models.
a modelwith †‡ is either distantly supervised or trains a supervised by labels from the denoiser.
(cid:92) signiﬁes the results fromour experiments.
in addition to models with (cid:92), snorkel and linked hmm also share our labeling sources..directly predicts the hidden steps from the bertembeddings, while otherwise identical to chmm.
we also investigate how chmm-alt performswith other aggregators other than chmm..we also introduce two upper bounds from dif-ferent aspects: 1) a fully supervised bert-nermodel trained with manually labeled data is re-garded as a supervised reference; 2) the best pos-sible consensus of the weak sources.
the latterassumes an oracle that always selects the correctannotations from these weak supervision sources.
according to the deﬁnition, its precision is always100% and its recall is non-decreasing with the in-crease of the number of weak sources..evaluation metrics we evaluate the perfor-mance of ner models using entity-level precision,recall, and f1 scores.
all scores are presented aspercentages.
the results come from the average of5 trials with different random seeds..implementation details we use bert pre-trained on different domains for different datasets,both for embedding construction and as the compo-nent of the supervised bert-ner model.
the orig-inal bert (devlin et al., 2019) is used for conll2003 and laptopreview datasets, biobert (leeet al., 2019) for ncbi-disease and scibert (belt-agy et al., 2019) for bc5cdr.
instances withlengths exceeding bert’s maximum length limita-tion (512) are broken into several shorter segments.
the only tunable hyper-parameter in chmm isthe learning rate.
but its inﬂuence is negligible—.
beneﬁtted from the stability of the generalized em,the model is guaranteed to converge to a local op-timum if the learning rate is small enough.
for allthe bert-ner models used in our experiments,the hyper-parameters except the batch size are ﬁxedto the default values (appendix c)..to prevent overﬁtting, we use a two-scale earlystopping strategy for model choosing at two scalesbased on the development set.
the micro-scaleearly stopping chooses the best model parametersfor each individual training process of both chmmand bert-ner; the macro-scale early stoppingselects the best-performing model in phase ii it-erations, which reports the test results.
in our ex-periments, phase ii exits if the macro-scale devel-opment score has not increased in 5 loops or themaximum number of loops (10) is reached..5.2 main results.
table 2 presents the model performance from dif-ferent domains.
we ﬁnd that our alternate-trainingframework outperforms all weakly supervised base-line models.
in addition, chmm-alt approachesor even exceeds the best source consensus, whichsufﬁciently proves the effectiveness of the design.
for general hmm-based label aggregators such aschmm, it is impossible to exceed the best con-sensus since they can only predict an entity ob-served by at least one source.
based on this fact,chmm is designed to select the most accurate ob-servations from the weak sources without shrinkingtheir coverage.
in comparison, bert’s language.
6184(a).
(b).
(c).
(d).
figure 4: f1 score evolution across the alternate-training phases.
“pi” is phase i; “pii-i” is the ith loop of phase ii.
the “strongest baseline” reports the result from the best-performed baseline in table 2 for each dataset..representation ability enables it to generalize theentity patterns and successfully discovers those en-tities annotated by none of the sources.
compar-ing chmm + bert to chmm, we can concludethat bert basically exchanges recall with preci-sion, and its high-recall predictions can improvethe result of chmm in return.
the complementarynature of these two models is why chmm-altimproves the overall performance of weakly super-vised ner..5.3 analysis of chmm.
looking at table 2, we notice that chmm per-forms the best amongst all generative models in-cluding majority voting, hmm and chmm-i.i.d.
the performance of conventional hmm is largelylimited by the markov assumption with the un-changing transition and emission probabilities.
theresults in the table validate that conditioning themodel on bert embedding alleviates this limita-tion.
however, the transition matrices in hmm areindispensable, implied by chmm-i.i.d.’s results, asthey provide supplemental information about howthe underlying true labels should evolve..5.4 analysis of alternate-training.
performance evolution figure 4 reveals the de-tails of the alternate-training process.
for less am-biguous tasks including ncbi-disease, bc5cdrand laptopreview with fewer entity types, bertgenerally has better performance in phase i butgets surpassed in phase ii.
interestingly, bert’sperformance never exceeds that of chmm onthe laptopreview dataset.
this may be becausebert fails to construct sufﬁciently representativepatterns from the denoised labels for this dataset.
for conll 2003, where it is harder for the label-ing sources to model the language structures, thestrength of a pre-trained language model in patternrecognition becomes more prominent.
from the re-.
models.
co03 ncbi.
cdr.
laptop.
mv † (cid:92)mv-alt †‡ (cid:92).
hmm † (cid:92)hmm-alt †‡ (cid:92).
i.i.d.
† (cid:92)i.i.d.-alt †‡ (cid:92).
chmm † (cid:92)chmm-alt †‡ (cid:92).
58.4066.64.
68.8474.04.
68.5773.84.
70.1175.54.
73.9480.83.
73.0682.99.
71.6983.15.
78.8885.02.
80.7382.78.
80.5783.34.
79.3783.17.
82.3985.12.
67.9270.45.
66.9672.90.
65.8972.61.
73.0276.55.table 3: alternate-training f1 scores with different la-bel aggregators.
mv denotes majority voting; i.i.d.
represents chmm-i.i.d.
the model names withoutthe “alt” sufﬁx are the multi-source label aggregatorswhereas the sufﬁx indicates that the result comes fromthe alternate-training framework with the correspond-ing model as the label aggregator..sults it seems that the performance increment of thedenoised labels y∗(1:t ) provides marginally extrainformation to bert after phase ii, as most of theincrement comes from the information provided bybert itself.
even so, keeping phase ii is reason-able when we want to get the best out of the weaklabeling sources and the pre-trained bert..bert-ner initialization chmm-alt initial-izes bert-ner’s parameters from its previouscheckpoint at the beginning of each loop in phase iito reduce training time (§ 4.3).
if we insteadﬁne-tune bert-ner from the initial parametersof the pre-trained bert model for each loop,chmm-alt gets 84.30, 84.71, and 76.68 f1scores on ncbi-disease, bc5cdr, and laptopre-view datasets.
these scores are close to the resultsin table 2, but the training takes much longer.
con-sequently, our bert-ner initialization strategy isa more practical choice overall..applying alternate-training to other methodstable 3 shows the alternate-training performanceacquired with different label aggregators.
the ac-.
61856870727476pipii-1pii-2pii-3pii-4pii-5pii-6pii-7pii-8pii-9pii-10conll 2003chmmbert-nerstrongest baseline7880828486pipii-1pii-2pii-3pii-4pii-5pii-6pii-7pii-8pii-9pii-10ncbi-diseasechmmbert-nerstrongest baseline8283848586pipii-1pii-2pii-3pii-4pii-5pii-6pii-7pii-8pii-9pii-10bc5cdrchmmbert-nerstrongest baseline667176pipii-1pii-2pii-3pii-4pii-5pii-6pii-7pii-8pii-9pii-10laptopreviewchmmbert-nerstrongest baselinecompanying bert-ner models are identical tothose described in § 5.1. the results in the ta-ble suggest that the performance improvement ob-tained by using alternate-training on the label ag-gregators is stable and generalizable to any othermodels yet to be proposed..6 conclusion.
in this work, we present chmm-alt, a multi-source weakly supervised approach that does notdepend on manually labeled data to learn an accu-rate ner tagger.
it integrates a label aggregator—chmm and a supervised model—bert-ner to-gether into an alternate-training procedure.
chmmconditions hmm on bert embeddings to achievegreater ﬂexibility and stronger context-awareness.
fine-tuned with chmm’s prediction, bert-nerdiscovers patterns unobserved by the weak sourcesand complements chmm.
training these modelsin turn, chmm-alt uses the knowledge encodedin both the weak sources and the pre-trained bertmodel to improve the ﬁnal ner performance.
inthe future, we will consider imposing more con-straints on the transition and emission probabilities,or manipulating them according to sophisticateddomain knowledge.
this technique could be alsoextended to other sequence labeling tasks such assemantic role labeling or event extraction..acknowledgments.
this work was supported by onr muri n00014-17-1-2656, nsf iii-2008334, kolon industries, andresearch gifts from google and amazon.
in addi-tion, we would like to thank yue yu for his insight-ful suggestions for this work..references.
nguyen bach and sameer badaskar.
2007. a review ofrelation extraction.
literature review for languageand statistics ii, 2:1–15..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁc text.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the.
9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3615–3620, hong kong, china.
association for computa-tional linguistics..steven bird and edward loper.
2004. nltk: the nat-ural language toolkit.
in proceedings of the acl in-teractive poster and demonstration sessions, pages214–217, barcelona, spain.
association for compu-tational linguistics..yixin cao, zikun hu, tat-seng chua, zhiyuan liu,and heng ji.
2019. low-resource name taggingin proceedingslearned with weakly labeled data.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 261–270, hongkong, china.
association for computational lin-guistics..justin chiu and alexander rush.
2020. scaling hid-den markov language models.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 1341–1349,online.
association for computational linguistics..hanjun dai, bo dai, yan-ming zhang, shuang li,and le song.
2017. recurrent hidden semi-markovmodel.
in 5th international conference on learningrepresentations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..rezarta islamaj dogan, robert leaman, and zhiyonglu.
2014. ncbi disease corpus: a resource for dis-ease name recognition and concept normalization.
j.biomed.
informatics, 47:1–10..dimitra farmakiotou, vangelis karkaletsis, john kout-sias, george sigletos, constantine d. spyropoulos,and panagiotis stamatopoulos.
2000. rule-basednamed entity recognition for greek ﬁnancial texts.
inin proceedings of the workshop on computationallexicography and multimedia dictionaries (com-lex 2000, pages 75–78..jason a. fries, sen wu, alexander ratner, and christo-pher r´e.
2017. swellshark: a generative modelfor biomedical named entity recognition without la-beled data.
corr, abs/1704.06360..zhiheng huang, wei xu, and kai yu.
2015. bidi-rectional lstm-crf models for sequence tagging.
corr, abs/1508.01991..6186mahboob alam khalid, valentin jijkoun, and maartende rijke.
2008. the impact of named entity nor-malization on information retrieval for question an-in proceedings of the ir research, 30thswering.
european conference on advances in informationretrieval, ecir’08, pages 705–710, berlin, heidel-berg.
springer-verlag..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..yoon kim, sam wiseman, and alexander m. rush.
2018. a tutorial on deep latent variable models ofnatural language.
corr, abs/1812.06834..ouyu lan, xiao huang, bill yuchen lin, he jiang,liyuan liu, and xiang ren.
2020. learning to con-textually aggregate multi-source supervision for se-quence labeling.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 2134–2146, online.
association forcomputational linguistics..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2019.biobert: a pre-trained biomedical language representation modelbioinformatics,text mining.
for biomedical36(4):1234–1240..chenliang li, aixin sun, and anwitaman datta.
2012. twevent: segment-based event detectionfrom tweets.
in proceedings of the 21st acm inter-national conference on information and knowledgemanagement, cikm ’12, pages 155–164, new york,ny, usa.
association for computing machinery..jiao li, yueping sun, robin j. johnson, daniela sci-aky, chih-hsuan wei, robert leaman, allan peterdavis, carolyn j. mattingly, thomas c. wiegers,and zhiyong lu.
2016. biocreative v cdr task cor-pus: a resource for chemical disease relation extrac-tion.
database j. biol.
databases curation, 2016..chen liang, yue yu, haoming jiang, siawpeng er,ruijia wang, tuo zhao, and chao zhang.
2020.bond: bert-assisted open-domain named entityrecognition with distant supervision.
in proceedingsof the 26th acm sigkdd international conferenceon knowledge discovery & data mining, kdd ’20,pages 1054–1064, new york, ny, usa.
associationfor computing machinery..pierre lison, jeremy barnes, aliaksandr hubin, andsamia touileb.
2020. named entity recognitionwithout labelled data: a weak supervision approach.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages1518–1533, online.
association for computationallinguistics..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation ex-in proceedings oftraction without labeled data.
the joint conference of the 47th annual meeting ofthe acl and the 4th international joint conferenceon natural language processing of the afnlp,pages 1003–1011, suntec, singapore.
associationfor computational linguistics..david nadeau and satoshi sekine.
2007. a survey ofnamed entity recognition and classiﬁcation.
linguis-ticae investigationes, 30(1):3–26.
publisher: johnbenjamins publishing company..an thanh nguyen, byron wallace, junyi jessy li, aninenkova, and matthew lease.
2017. aggregatingand predicting sequence labels from crowd annota-tions.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 299–309, vancouver,canada.
association for computational linguistics..maria pontiki, dimitris galanis, john pavlopoulos,harris papageorgiou,ion androutsopoulos, andsuresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in proceedings of the8th international workshop on semantic evaluation(semeval 2014), pages 27–35, dublin, ireland.
as-sociation for computational linguistics..lance ramshaw and mitch marcus.
1995. text chunk-in third.
ing using transformation-based learning.
workshop on very large corpora..alexander ratner, stephen h. bach, henry ehrenberg,jason fries, sen wu, and christopher r´e.
2017.snorkel: rapid training data creation with weak su-pervision.
proc.
vldb endow., 11(3):269–282..alexander ratner, braden hancock, jared dunnmon,frederic sala, shreyash pandey, and christopher r´e.
2019. training complex models with multi-taskweak supervision.
proceedings of the aaai confer-ence on artiﬁcial intelligence, 33:4763–4771..alexander ratner, christopher de sa, sen wu, danielselsam, and christopher r´e.
2016. data program-ming: creating large training sets, quickly.
in pro-ceedings of the 30th international conference onneural information processing systems, nips’16,pages 3574–3582, red hook, ny, usa.
curran as-sociates inc..hao liu, lirong he, haoli bai, bo dai, kun bai, andzenglin xu.
2018. structured inference for recur-in proceedingsrent hidden semi-markov model.
of the 27th international joint conference on artiﬁ-cial intelligence, ijcai’18, page 2447–2453.
aaaipress..wendi ren, yinghao li, hanting su, david kartchner,cassie mitchell, and chao zhang.
2020. denoisingmulti-source weak supervision for neural text classi-ﬁcation.
in findings of the association for computa-tional linguistics: emnlp 2020, pages 3739–3754,online.
association for computational linguistics..6187reinforcement learning.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 2159–2169, santa fe, new mexico, usa.
association for computational linguistics..yue yu, simiao zuo, haoming jiang, wendi ren, tuofine-tuning pre-zhao, and chao zhang.
2020.trained language model with weak supervision: acontrastive-regularized self-training approach..esteban safranchik, shiying luo, and stephen h. bach.
2020. weakly supervised sequence tagging fromnoisy rules.
in the thirty-fourth aaai conferenceon artiﬁcial intelligence, aaai 2020, the thirty-second innovative applications of artiﬁcial intelli-gence conference, iaai 2020, the tenth aaai sym-posium on educational advances in artiﬁcial intel-ligence, eaai 2020, new york, ny, usa, february7-12, 2020, pages 5570–5578.
aaai press..jingbo shang, liyuan liu, xiaotao gu, xiang ren,teng ren, and jiawei han.
2018. learning namedentity tagger using domain-speciﬁc dictionary.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2054–2064, brussels, belgium.
association forcomputational linguistics..chi sun, xipeng qiu, yige xu, and xuanjing huang.
2019. how to ﬁne-tune bert for text classiﬁcation?
chinese computational linguistics, page 194–206..christian thiel.
2008. classiﬁcation on soft labelsin proceedings ofis robust against label noise.
the 12th international conference on knowledge-based intelligent information and engineering sys-tems, part i, kes ’08, pages 65–73, berlin, heidel-berg.
springer-verlag..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ke m. tran, yonatan bisk, ashish vaswani, danielmarcu, and kevin knight.
2016. unsupervised neu-in proceedings of theral hidden markov models.
workshop on structured prediction for nlp, pages63–71, austin, tx.
association for computationallinguistics..a. viterbi.
1967. error bounds for convolutional codesand an asymptotically optimum decoding algorithm.
ieee trans.
inf.
theor., 13(2):260–269..t. wessels and christian w. omlin.
2000. reﬁning hid-den markov models with recurrent neural networks.
in proceedings of the ieee-inns-enns interna-tional joint conference on neural networks, ijcnn2000, neural computing: new challenges and per-spectives for the new millennium, como, italy, july24-27, 2000, volume 2, pages 271–278.
ieee com-puter society..sam wiseman, stuart shieber, and alexander rush.
2018. learning neural templates for text genera-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 3174–3187, brussels, belgium.
associationfor computational linguistics..yaosheng yang, wenliang chen, zhenghua li,zhengqiu he, and min zhang.
2018. distantly su-pervised ner with partial annotation learning and.
6188a technical details.
a.1 chmm training.
i.following the discussion in § 4.2, we usethe forward-backward algorithm to calculate thesmoothed marginal γ(t)(cid:44) p(z(t) = i|x(1:t )), i ∈{1, 2, .
.
.
, |l|}, t ∈ {1, 2, .
.
.
, t } and the expectednumber of transitions ξ(t)(cid:44) p(z(t−1) = i, z(t) =i,jj|x(1:t )), i, j ∈ {1, 2, .
.
.
, |l|}.8 |l| is the num-ber of bio formatted entity labels, which are re-garded as hidden states; t is the total number ofhidden steps in a sequence, which equals the num-ber of tokens..(cid:44) p(z(t) = i|x(1:t)) and β(t)and ξ(t).
deﬁning α(t)ip(x(t+1:t )|z(t) = i), γ(t)i,j can be rep-iresented by α and β using the bayes’ rule andmarkov assumption:.
(cid:44).
i.γ(t)i.
(cid:44) p(z(t) = i|x(1:t )).
=.
p(x(t+1:t ), z(t) = i|x(1:t))p(x(t+1:t )|x(1:t )).
∝ p(z(t) = i|x(1:t))p(x(t+1:t )|z(t) = i)= α(t).
,.
i β(t).
i.ξ(t)i,j.
(cid:44) p(z(t−1) = i, z(t) = j|x(1:t ))∝ p(z(t−1) = i|x(1:t−1)).
p(z(t) = j|z(t−1) = i, x(t:t )).
∝ p(z(t−1) = i|x(1:t−1))p(x(t)|z(t) = j).
p(x(t+1:t )|z(t) = j)p(z(t) = j|z(t−1) = i).
= α(t−1)i.j β(t)ϕ(t).
j ψ(t)i,j ..ϕ(t)i ∈ r|l| (cid:44) p(x(t)|z(t) = i) is the likelihood ofthe observation when the hidden state is i (§ 4.2).
written in the matrix form, (12) and (13) be-.
come:.
γ(t) ∝ α(t) (cid:12) β(t),.
ξ(t) ∝ ψ(t) (cid:12) (α(t−1)(ϕ(t) (cid:12) β(t))t),.
where (cid:12) is the element-wise product.
note that theelements in both γ(t) and ξ(t) should sum up to 1..(12).
(13).
(14).
(15).
the forward pass the ﬁltered marginal α(t)ican be computed iteratively:.
α(t)i.
(cid:44) p(z(t) = i|x(1:t))= p(z(t) = i|x(t), x(1:t−1))∝ p(x(t)|z(t) = i)p(z(t) = i|x(1:t−1)).
(16).
(cid:88).
=.
i ψ(t)ϕ(t).
j,i α(t−1).
j.
..j.written in the matrix form, (16) becomes.
α(t) ∝ ϕ(t) (cid:12) (ψ(t)t.α(t−1))..(17).
we initialize α with α(0) = π (§ 4.2) since wehave no observation at time step 0. as α(t) is aprobability distribution, the elements in it sum upto 1. the calculation of α is the forward pass..the backward passin the same way, we do thebackward pass to compute the conditional futureevidence β(t)(cid:44) p(x(t+1:t )|z(t) = i):.
β(t−1)i.
(cid:44) p(x(t+1:t )|z(t) = j).
p(z(t) = j, x(t), x(t+1:t )|z(t−1) = i).
[p(x(t+1:t )|z(t) = j).
p(x(t), z(t) = j|z(t−1) = i)]j ϕ(t)β(t).
j ψ(t)i,j ..(cid:88).
=.
in the matrix form, (18) becomes:.
(18).
β(t−1) = ψ(t)(ϕ(t) (cid:12) β(t)),.
(19).
whose base case is.
β(t )i = p(x(t +1:t )|z(t ) = i) = 1,.
∀i ∈ {1, .
.
.
, |l|}..a.2 the maximization step for unsupervised.
hmm.
for traditional unsupervised hmm, the expectedcomplete data log likelihood is maximized by up-dating the matrices with the approximated pseudo-statistics.
different from chmm, hmm has con-stant transition and emission for all time steps, i.e.:.
(cid:88).
j(cid:88).
=.
=.
i.j.j.
8same as § 4.2, we omit the dependency term e(1:t )..ψ(1) = ψ(t); φ(1) = φ(t);.
∀t ∈ {2, .
.
.
, t }..6189for simplicity, we remove the term t for the tran-sition and emission matrices.
suppose we are up-dating hmm based on one instance with t startingfrom 1:.
source name.
precision.
recall.
f1.
coredictionaryistuffextractedphraseconsecutivecapitals.
72.6326.6797.4535.29.
51.610.6129.250.92.
60.341.245.01.8.πi = γ(1)i(cid:80)t.;t=2 ξ(t)i,j(cid:80)|l|.
(cid:80)t.t=2(cid:80)t.i,(cid:96).
(cid:96)=1 ξ(t)i x(t).
j,k.
t=1 γ(t)(cid:80)tt=1 γti.;.
..ψi,j =.
φi,j,k =.
(20).
(21).
(22).
note that the observation has property 0 ≤ x(t)1 and (cid:80)|l|index of the weak labeling source..j,k ≤j,k = 1, where k ∈ {1, .
.
.
k} is the.
j=1 x(t).
b labeling source performance.
the weak labeling sources of the conll 2003dataset come from lison et al.
(2020), whereassafranchik et al.
(2020) provide the sources forthe laptopreview, ncbi-disease and bc5cdrdataset.
for safranchik et al.
(2020)’s labelingsources, we apply a majority voting using their tag-ging results to the spans detected by their linkingrules to convert the linking results to token anno-tations.
in consideration of the training time andresource consumption, we only adopt a subset ofthe labeling sources provided by the authors.
theperformance of the labeling sources is presented inthe tables below..source name.
precision.
recall.
f1.
coredictionaryuncasedcoredictionaryexactcancerlikebodytermsextractedphrase.
81.0380.6934.8868.5297.12.
41.4117.181.583.9032.03.
5.4828.323.027.3848.18.table 4: the performance of the labeling sources usedin the ncbi-disease dataset..source name.
precision.
recall.
f1.
dictcore-chemicaldictcore-chemical-exactdictcore-diseasedictcore-disease-exactorganic chemicaldisease or syndromeposthyphenextractedphrase.
91.8185.8881.5781.492.6777.3684.4786.8.
29.553.1626.321.0930.0711.6708.0717.96.
44.76.139.82.1645.420.2814.7429.76.table 5: the performance of the labeling sources usedin the bc5cdr dataset..table 6: the performance of the labeling sources usedin the laptopreview dataset..source name.
precision.
recall.
f1.
btc+csec+ccore web md+ccrunchbase casedcrunchbase uncaseddoc majority caseddoc majority uncasedfull name detectorgeo casedgeo uncasedmisc detectorwiki casedwiki uncased.
61.5639.5469.5338.2637.8865.8161.6987.7968.1665.185.1475.2772.26.
46.3524.5960.045.596.240.2140.1711.3315.3518.8921.5132.6535.61.
52.8830.3264.449.7610.6649.9248.6620.0625.0629.2834.3445.5447.7.table 7: the performance of the labeling sources usedin the conll 2003 dataset..please refer to lison et al.
(2020) for the in-formation about the construction of the labelingsources on the conll 2003 dataset; please referto safranchik et al.
(2020) for the labeling sourceson other three datasets..c hyper-parameters.
the experiments are conducted on one geforcertx 2080 ti gpu.
for ncbi-disease, bc5cdrand laptopreview datasets, chmm is pre-trainedfor 5 epochs and trained for 20 epochs.
the learn-ing rates for these three datasets are 5×10−4, 10−3and 10−4, respectively, and the batch sizes are 64,64 and 128. in phase i, bert-ner is trained withthe default learning rate (5 × 10−5) for 100 epochs.
the batch sizes are 8, 8, and 48, respectively.
notethat for laptopreview, the maximum length limi-tation of bert-ner is set to 128 whereas the limi-tation is 512 for the other two datasets.
in phase ii,we use half the learning rate with 20 epochs foreach loop..for conll 2003, chmm has the same numberof training epochs as for other datasets.
the batchsize is 32, and the learning rate is 10−5.
bert-ner has a maximum sequence length of 256. itis trained for 15 epochs in phase i and 5 epochs inphase ii.
other hyper-parameters are identical toother bert-ner models’..6190