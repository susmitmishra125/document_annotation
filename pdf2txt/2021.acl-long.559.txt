structformer: joint unsupervised induction of dependency andconstituency structure from masked language modeling.
yikang shen∗mila/universit´e de montr´eal.
yi taygoogle research.
che zhenggoogle research.
dara bahrigoogle research.
donald metzlergoogle research.
aaron courvillemila/universit´e de montr´eal.
abstract.
there are two major classes of natural lan-guage grammars — the dependency grammarthat models one-to-one correspondences be-tween words and the constituency grammarthat models the assembly of one or severalcorresponded words.
while previous unsuper-vised parsing methods mostly focus on only in-ducing one class of grammars, we introduce anovel model, structformer, that can simulta-neously induce dependency and constituencystructure.
to achieve this, we propose a newparsing framework that can jointly generate aconstituency tree and dependency graph.
thenwe integrate the induced dependency relationsinto the transformer, in a differentiable man-ner, through a novel dependency-constrainedself-attention mechanism.
experimental re-sults show that our model can achieve strongresults on unsupervised constituency parsing,unsupervised dependency parsing, and maskedlanguage modeling at the same time..1.introduction.
human languages have a rich latent structure.
thisstructure is multifaceted, with the two major classesof grammar being dependency and constituencystructures.
there has been an exciting breath ofrecent work targeted at learning this structure in adata-driven unsupervised fashion (klein and man-ning, 2002; klein, 2005; le and zuidema, 2015;shen et al., 2018c; kim et al., 2019a).
the coreprinciple behind recent methods that induce struc-ture from data is simple - provide an inductivebias that is conducive for structure to emerge asa byproduct of some self-supervised training, e.g.,language modeling.
to this end, a wide range ofmodels have been proposed that are able to success-fully learn grammar structures (shen et al., 2018a,c;.
∗ corresponding author: yikang.shn@gmail.ca..work done while interning at google reseach..wang et al., 2019; kim et al., 2019b,a).
however,most of these works focus on inducing either con-stituency or dependency structures alone..in this paper, we make two important techni-cal contributions.
first, we introduce a new neu-ral model, structformer, that is able to simultane-ously induce both dependency structure and con-stituency structure.
speciﬁcally, our approach aimsto unify latent structure induction of different typesof grammar within the same framework.
second,structformer is able to induce dependency struc-tures from raw data in an end-to-end unsupervisedfashion.
most existing approaches induce depen-dency structures from other syntactic informationlike gold pos tags (klein and manning, 2004; co-hen and smith, 2009; jiang et al., 2016).
previousworks, having trained from words alone, often re-quires additional information, like pre-trained wordclustering (spitkovsky et al., 2011), pre-trainedword embedding (he et al., 2018), acoustic cues(pate and goldwater, 2013), or annotated data fromrelated languages (cohen et al., 2011)..we introduce a new inductive bias that enablesthe transformer models to induce a directed depen-dency graph in a fully unsupervised manner.
toavoid the necessity of using grammar labels duringtraining, we use a distance-based parsing mecha-nism.
the parsing mechanism predicts a sequenceof syntactic distances t (shen et al., 2018b) and asequence of syntactic heights ∆ (luo et al., 2019)to represent dependency graphs and constituencytrees at the same time.
examples of ∆ and t areillustrated in figure 1a.
based on the syntacticdistances (t) and syntactic heights (∆), we pro-vide a new dependency-constrained self-attentionlayer to replace the multi-head self-attention layerin standard transformer model.
more speciﬁcally,the new attention head can only attend its parent (toavoid confusion with self-attention head, we use“parent” to denote “head” in dependency graph) or.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7196–7209august1–6,2021.©2021associationforcomputationallinguistics7196(a) an example of syntactic distances t (grey bars) andsyntactic heights ∆ (white bars).
in this example, likeis the parent (head) of constituent (like cats) and(i like cats)..(b) two types of dependency relations.
the parent distributionallows each token to attend on its parent.
the dependent distribu-tion allows each token to attend on its dependents.
for examplethe parent of cats is like.
cats and i are dependents oflike each attention head will receive a different weighted sumof these relations..figure 1: an example of our parsing mechanism and dependency-constrained self-attention mechanism.
theparsing network ﬁrst predicts the syntactic distance t and syntactic height ∆ to represent the latent structure of theinput sentence i like cats.
then the parent and dependent relations are computed in a differentiable mannerfrom t and ∆..its dependents in the predicted dependency struc-ture, through a weighted sum of relations shownin figure 1b.
in this way, we replace the completegraph in the standard transformer model with adifferentiable directed dependency graph.
duringthe process of training on a downstream task (e.g.
masked language model), the model will gradu-ally converge to a reasonable dependency graphvia gradient descent..incorporating the new parsing mechanism, thedependency-constrained self-attention, and thetransformer architecture, we introduce a newmodel named structformer.
the proposed modelcan perform unsupervised dependency and con-stituency parsing at the same time, and can leveragethe parsing results to achieve strong performanceon masked language model tasks..2 related work.
previous works on unsupervised dependency pars-ing are primarily based on the dependency modelwith valence (dmv) (klein and manning, 2004)and its extension (daum´e iii, 2009; gillenwateret al., 2010).
to effectively learn the dmv modelfor better parsing accuracy, a variety of inductive bi-ases and handcrafted features, such as correlationsbetween parameters of grammar rules involvingdifferent part-of-speech (pos) tags, have been pro-posed to incorporate prior information into learning.
the most recent progress is the neural dmv model(jiang et al., 2016), which uses a neural networkmodel to predict the grammar rule probabilities.
based on the distributed representation of pos tags.
however, most existing unsupervised dependencyparsing algorithms require the gold pos tags toge provided as inputs.
these gold pos tags are la-beled by humans and can be potentially difﬁcult (orprohibitively expensive) to obtain for large corpora.
spitkovsky et al.
(2011) proposed to overcome thisproblem with unsupervised word clustering thatcan dynamically assign tags to each word consid-ering its context.
he et al.
(2018) overcame theproblem by combining dmv model with invertibleneural network to jointly model discrete syntacticstructure and continuous word representations..unsupervised constituency parsing has recentlyreceived more attention.
prpn (shen et al., 2018a)and on-lstm (shen et al., 2018c) induce treestructure by introducing an inductive bias to re-current neural networks.
prpn proposes a pars-ing network to compute the syntactic distance ofall word pairs, while a reading network uses thesyntactic structure to attend to relevant memories.
on-lstm allows hidden neurons to learn long-term or short-term information by a novel gatingmechanism and activation function.
in urnng(kim et al., 2019b), amortized variational infer-ence was applied between a recurrent neural net-work grammar (rnng) (dyer et al., 2016) decoderand a tree structure inference network, which en-courages the decoder to generate reasonable treestructures.
diora (drozdov et al., 2019) proposedusing inside-outside dynamic programming to com-pose latent representations from all possible binary.
7197trees.
the representations of inside and outsidepasses from the same sentences are optimized tobe close to each other.
the compound pcfg (kimet al., 2019a) achieves grammar induction by max-imizing the marginal likelihood of the sentenceswhich are generated by a probabilistic context-freegrammar (pcfg).
tree transformer (wang et al.,2019) adds extra locality constraints to the trans-former encoder’s self-attention to encourage theattention heads to follow a tree structure such thateach token can only attend on nearby neighbors inlower layers and gradually extend the attention ﬁeldto further tokens when climbing to higher layers.
neural l-pcfg (zhu et al., 2020) demonstratedthat pcfg can beneﬁt from modeling lexical de-pendencies.
similar to structformer, the neurall-pcfg induces both constituents and dependen-cies within a single model..though large scale pre-trained models havedominated most natural language processing tasks,some recent work indicates that neural networkmodels can see accuracy gains by leveraging syn-tactic information rather than ignoring it (marcheg-giani and titov, 2017; strubell et al., 2018).
strubell et al.
(2018) introduces syntactically-informed self-attention that force one attentionhead to attend on the syntactic governor of theinput token.
omote et al.
(2019) and deguchiet al.
(2019) argue that dependency-informed self-attention can improve transformer’s performanceon machine translation.
kuncoro et al.
(2020)shows that syntactic biases help large scale pre-trained models, like bert, to achieve better lan-guage understanding..3 syntactic distance and height.
in this section, we ﬁrst reintroduce the conceptsof syntactic distance and height, then discuss theirrelations in the context of structformer..in other words, each syntactic distance di is as-sociated with a split point (i, i + 1) and specify therelative order in which the sentence will be splitinto smaller components.
thus, any sequence ofn − 1 real values can unambiguously map to anunlabeled binary constituency tree with n leavesthrough the algorithm 1 (shen et al., 2018b).
asshen et al.
(2018c,a); wang et al.
(2019) pointedout, the syntactic distance reﬂects the informationcommunication between constituents.
more con-cretely, a large syntactic distance τi represents thatshort-term or local information should not be com-municated between (x≤i) and (x>i).
while cooper-ating with appropriate neural network architectures,we can leverage this feature to build unsuperviseddependency parsing models..if d = [] then.
t ⇐ leaf(w).
algorithm 1 distance to binary constituencytree1: function constituent(w, d)2:3:4:5:6:7:8:9:.
i ⇐ arg maxi(d)childl ⇐ constituent(w≤i, d<i)childr ⇐ constituent(w>i, d>i)t ⇐ node(childl, childr).
return t.else.
else.
if t = w then.
d ⇐ [], parent ⇐ w.algorithm 2 converting binary constituencytree to dependency graph1: function dependent(t, ∆)2:3:4:5:6:7:8:9:10:11:12:13:14:15:.
childl, childr ⇐ tdl, parentl ⇐ dependent(childl, ∆)dr, parentr ⇐ dependent(childr, ∆)d ⇐ union(dl, dr)if ∆(parentl) > ∆(parentr) thend.add(parentl ← parentr)parent ⇐ parentl.
d.add(parentr ← parentl)parent ⇐ parentr.
return d, parent.
else.
3.1 syntactic distance.
3.2 syntactic height.
syntactic distance is proposed in shen et al.
(2018b) to quantify the process of splitting sen-tences into smaller constituents..deﬁnition 3.1. let t be a constituency tree forsentence (w1, ..., wn).
the height of the lowestcommon ancestor for consecutive words xi andxi+1 is ˜τi.
syntactic distances t = (τ1, ..., τn−1)are deﬁned as a sequence of n − 1 real scalars thatshare the same rank as (˜τ1, ..., ˜τn−1)..syntactic height is proposed in luo et al.
(2019),where it is used to capture the distance to the rootnode in a dependency graph.
a word with highsyntactic height means it is close to the root node.
in this paper, to match the deﬁnition of syntacticdistance, we redeﬁne syntactic height as:.
deﬁnition 3.2. let d be a dependency graph forsentence (w1, ..., wn).
the height of a token wiin d is ˜δi.
the syntactic heights of d can be any.
7198sequence of n real scalars ∆ = (δ1, ..., δn) thatshare the same rank as (˜δ1, ..., ˜δn)..although the syntactic height is deﬁned basedon the dependency structure, we cannot rebuild theoriginal dependency structure by syntactic heightsalone, since there is no information about whether atoken should be attached to the left side or the rightside.
however, given an unlabelled constituent tree,we can convert it into a dependency graph withthe help of syntactic distance.
the converting pro-cess is similar to the standard process of convertingconstituency treebank to dependency treebank (gel-bukh et al., 2005).
instead of using the constituentlabels and pos tags to identify the parent of eachconstituent, we simply assign the token with thelargest syntactic height as the parent of each con-stituent.
the conversion algorithm is described inalgorithm 2. in appendix a.1, we also propose ajoint algorithm, that takes t and ∆ as inputs andjointly outputs a constituency tree and dependencygraph..figure 2: an example of t, ∆ and respective depen-dency graph d. solid lines represent dependency rela-tions between tokens.
structformer only allow tokenswith dependency relation to attend on each other..3.3 the relation between syntactic distance.
and height.
as discussed previously, the syntactic distance con-trols information communication between the twosides of the split point.
the syntactic height quanti-ﬁes the centrality of each token in the dependencygraph.
a token with large syntactic height tendsto have more long-term dependency relations toconnect different parts of the sentence together.
instructformer, we quantify the syntactic distanceand height on the same scale.
given a split point(i, i + 1) and it’s syntactic distance δi, only tokens.
(a) model architecture.
(b) parsing network.
figure 3: the architecture of structformer.
the parsertakes shared word embeddings as input, outputs syntac-tic distances t, syntactic heights ∆, and dependencydistributions between tokens.
the transformer layerstake word embeddings and dependency distributionsas input, output contextualized embeddings for inputwords..xj with τj > δi can attend across the split point(i, i + 1).
thus tokens with small syntactic heightare limited to attend to nearby tokens.
figure 2provides an example of t, ∆ and respective depen-dency graph d..however, if the left and right boundary syntac-tic distance of a constituent [l, r] are too large, allwords in [l, r] will be forced to only attend to otherwords in [l, r].
their contextual embedding willnot be able to encode the full context.
to avoid thisphenomena, we propose calibrating t according to∆ in appendix a.2.
4 structformer.
in this section, we present the structformer model.
figure 3a shows the architecture of structformer,which includes a parser network and a transformermodule.
the parser network predicts t and ∆,then passes them to a set of differentiable func-tions to generate dependency distributions.
thetransformer module takes these distributions andthe sentence as input to computes a contextual em-bedding for each position.
the structformer canbe trained in an end-to-end fashion on a maskedlanguage model task.
in this setting, the gradientback propagates through the relation distributionsinto the parser..4.1 parsing network.
as shown in figure 3b, the parsing network takesword embeddings as input and feeds them into sev-eral convolution layers:.
sl,i = tanh (conv (sl−1,i−w , ..., sl−1,i+w )).
(1).
where sl,i is the output of l-th layer at i-th position,s0,i is the input embedding of token wi, and 2w +1is the convolution kernel size..7199given the output of the convolution stack sn,i,.
we parameterize the syntactic distance t as:.
τi =.
.
(cid:18).
wτ.
1 tanh.
wτ2.,.
(cid:21)(cid:19).
(cid:20) sn,isn,i+11 ≤ i ≤ n − 1i = n.∞,.
i = 0 or.
(2).
where τi is the contextualized distance for the i-th split point between token wi and wi+1.
thesyntactic height ∆ is parameterized in a similarway:.
δi = wδ.
1 tanh.
wδ.
2sn,i + bδ2.
+ bδ1.
(3).
(cid:16).
(cid:17).
4.2 estimate the dependency distribution.
given t and ∆, we now explain how to estimatethe probability p(xj|xi) such that the j-th token isthe parent of the i-th token.
the ﬁrst step is iden-tifying the smallest legal constituent c(xi), thatcontains xi and xi is not c(xi)’s parent.
the sec-ond step is identifying the parent of the constituentxj = pr(c(xi)).
given the discussion in section3.2, the parent of c(xi) must be the parent of xi.
thus, the two-stages of identifying the parent of xican be formulated as:.
d(xi) = pr(c(xi)).
(4).
in structformer, c(xi) is represented as con-stituent [l, r], where l is the starting index (l ≤ i) ofc(xi) and r is the ending index (r ≥ i) of c(xi).
in a dependency graph, xi is only connected toits parent and dependents.
this means that xi doesnot have direct connection to the outside of c(xi).
in other words, c(xi) = [l, r] is the smallest con-stituent that satisﬁes:.
δi < τl−1,.
δi < τr.
(5).
where τl−1 is the ﬁrst τ<i that is larger then δiwhile looking backward, and τr is the ﬁrst τ≥i thatis larger then δi while looking forward.
for ex-ample, in figure 2, δ4 = 3.5, τ3 = 4 > δ4 andτ8 = ∞ > δ4, thus c(x4) = [4, 8].
to makethis process differentiable, we deﬁne τk as a realvalue and δi as a probability distribution p(˜δi).
forthe simplicity and efﬁciency of computation, wedirectly parameterize the cumulative distributionfunction p(˜δi > τk) with sigmoid function:p(˜δi > τk) = σ((δi − τk)/µ1).
(6).
where σ is the sigmoid function, δi is the mean ofdistribution p(˜δi) and µ1 is a learnable temperature.
term.
thus the probability that the l-th (l < i)token is inside c(xi) is equal to the probability that˜δi is larger then the maximum distance τ betweenl and i:.
p(l ∈ c(xi)) = p(˜δi > max(τi−1, ..., τl)) (7)= σ((δi − max(τl, ..., τi−1))/µ).
then we can compute the probability distributionfor l:.
p(l|i) = p(l ∈ c(xi)) − p(l − 1 ∈ c(xi))= σ((δi − max(τl, ..., τi−1))/µ) −.
σ((δi − max(τl−1, ..., τi−1))/µ) (8).
similarly, we can compute the probability distribu-tion for r:.
p(r|i) = σ((δi − max(τi, ..., τr−1))/µ) −.
σ((δi − max(τi, ..., τr))/µ).
(9).
the probability distribution for [l, r] = c(xi) canbe computed as:.
pc([l, r]|i) =.
(cid:26)p(l|i)p(r|i),0,.l ≤ i ≤ rotherwise.
(10).
the second step is to identify the parent of [l, r].
for any constituent [l, r], we choose the j =argmaxk∈[l,r](δk) as the parent of [l, r].
in theprevious example, given constituent [4, 8], themaximum syntactic height is δ6 = 4.5,thuspr([4, 8]) = x6.
we use softmax function to pa-rameterize the probability ppr(j|[l, r]):.
ppr(j|[l, r]) =.
(cid:40).
(cid:80).
exp(hj /µ2)l≤k≤r exp(hk/µ2) ,0,.l ≤ t ≤ r.otherwise.
(11).
given probability p(j|[l, r]) and p([l, r]|i), we cancompute the probability that xj is the parent of xi:.
pd(j|i) =.
(cid:26)(cid:80).
[l,r] ppr(j|[l, r])pc([l, r]|i),0,.i (cid:54)= ji = j(12).
4.3 dependency-constrained multi-head.
self-attention.
the multi-head self-attention in the transformercan be seen as a information propagation mecha-nism on the complete graph g = (x, e), wherethe set of vertices x contains all n tokens in thesentence, and the set of edges e contains all possi-ble word pairs (xi, xj).
structformer replace the.
7200complete graph g with a soft dependency graphd = (x, a), where a is the matrix of n × n prob-abilities.
aij = pd(j|i) is the probability of thej-th token depending on the i-th token.
the reasonthat we called it a directed edge is that each speciﬁchead is only allow to propagate information eitherfrom parent to dependent or from from dependentto parent.
to do so, structformer associate eachattention head with a probability distribution overparent or dependent relation..pparent =.
pdep =.
exp(wparent)exp(wparent) + exp(wdep)exp(wdep)exp(wparent) + exp(wdep).
(13).
(14).
where wparent and wdep are learnable parametersthat associated with each attention head, pparent isthe probability that this head will propagate infor-mation from parent to dependent, vice versa.
themodel will learn to assign this association from thedownstream task via gradient descent.
then wecan compute the probability that information canbe propagated from node j to node i via this head:.
pi,j = pparentpd(j|i) + pdeppd(i|j).
(15).
however, htut et al.
(2019) pointed out that differ-ent heads tend to associate with different type ofuniversal dependency relations (including nsubj,obj, advmod, etc), but there is no generalist headcan that work with all different relations.
to ac-commodate this observation, we compute a indi-vidual probability for each head and pair of tokens(xi, xj):.
qi,j = sigmoid.
(16).
(cid:19).
(cid:18) qkt√dk.
where q and k are query and key matrix in astandard transformer model and dk is the dimen-sion of attention head.
the equation is inspiredby the scaled dot-product attention in transformer.
we replace the original softmax function with asigmoid function, so qi,j became an independentprobability that indicates whether xi should attendon xj through the current attention head.
in theend, we propose to replace transformer’s scaled dot-product attention with our dependency-constrainedself-attention:.
attention(qi, kj, vj, d) = pi,jqi,jvj.
(17).
5 experiments.
we evaluate the proposed model on three tasks:masked language modeling, unsupervised con-stituency parsing and unsupervised dependencyparsing..our implementation of structformer is close tothe original transformer encoder (vaswani et al.,2017).
except that we put the layer normalizationin front of each layer, similar to the t5 model (raf-fel et al., 2019).
we found that this modiﬁcationallows the model to converges faster.
for all exper-iments, we set the number of layers l = 8, the em-bedding size and hidden size to be dmodel = 512,the number of self-attention heads h = 8, the feed-forward size df f = 2048, dropout rate as 0.1, andthe number of convolution layers in the parsingnetwork as lp = 3..5.1 masked language model.
masked language modeling (mlm) has beenwidely used as a pretraining object for larger-scalepretraining models.
in bert (devlin et al., 2018)and roberta (liu et al., 2019), authors foundthat mlm perplexities on held-out evaluation sethave a positive correlation with the end-task per-formance.
we trained and evaluated our modelon 2 different datasets: the penn treebank (ptb)and bllip.
in our mlm experiments, each tokenhas an independent chance to be replaced by amask token <mask>, except that we never replace< unk > token.
the training and evaluation ob-ject for masked language model is to predict thereplaced tokens.
the performance of mlm is eval-uated by measuring perplexity on masked words..ptb is a standard dataset for language model-ing (mikolov et al., 2012) and unsupervised con-stituency parsing (shen et al., 2018c; kim et al.,2019a).
following the setting proposed in shenet al.
(2018c), we use mikolov et al.
(2012)’sprepossessing process, which removes all punc-tuations, and replaces low frequency tokens with<unk>.
the preprocessing results in a vocabu-lary size of 10001 (including <unk>, <pad> and<mask>).
for ptb, we use a 30% mask rate..bllip is a large penn treebank-style parsedcorpus of approximately 24 million sentences.
wetrain and evaluate structformer on three splits ofbllip: bllip-xs (40k sentences, 1m tokens),bllip-sm (200k sentences, 5m tokens), andbllip-md (600k sentences, 14m tokens).
theyare obtained by randomly sampling sections from.
7201model.
ptb.
bllip bllip bllip-md-sm.
-xs.
transformerstructformer.
64.0560.94.
93.9057.28.
19.9218.70.
14.3113.70.table 1: masked language model perplexities on dif-ferent datasets..bllip 1987-89 corpus release 1. all models aretested on a shared held-out test set (20k sentences,500k tokens).
following the settings provided in(hu et al., 2020), we use subword-level vocabularyextracted from the gpt-2 pre-trained model ratherthan the bllip training corpora.
for bllip, weuse a 15% mask rate..the masked language model results are shown intable 1. structformer consistently outperforms ourtransformer baseline.
this result aligns with previ-ous observations that linguistically informed self-attention can help transformers achieve strongerperformance.
we also observe that structformerconverges much faster than the standard trans-former model..5.2 unsupervised constituency parsing.
the unsupervised constituency parsing task com-pares the latent tree structure induced by the modelwith those annotated by human experts.
we usethe algorithm 1 to predict the constituency treesfrom t predicted by structformer.
following theexperiment settings proposed in shen et al.
(2018c),we take the model trained on ptb dataset and eval-uate it on wsj test set.
the wsj test set is section23 of wsj corpus, it contains 2416 human expertlabeled sentences.
punctuation is ignored duringthe evaluation..methods.
randomlbranchrbranch.
uf1.
21.69.039.8.prpn (shen et al., 2018a)on-lstm (shen et al., 2018c)tree-t (wang et al., 2019)urnng (kim et al., 2019b)c-pcfg (kim et al., 2019a)neural l-pcfgs (zhu et al., 2020)structformer.
37.4 (0.3)47.7 (1.5)49.552.455.255.3154.0 (0.3).
table 2: unsupervised constituency parsing tesults.
*results are from kim et al.
(2020).
uf1 stands for un-labeled f1..table 2 shows that our model achieves strong re-sults on unsupervised constituency parsing.
while.
prpn.
on.
c-pcfg tree-t.ours.
sbarnpvpppadjpadvp.
50.0% 52.5%59.2% 64.5%46.7% 41.0%57.2% 54.4%44.3% 38.1%32.8% 31.6%.
56.1% 36.4% 48.7%74.7% 67.6% 72.1%38.5% 43.0%41.7%52.3% 74.1%68.8%24.7% 51.9%40.4%55.1% 69.5%52.5%.
table 3: fraction of ground truth constituents that werepredicted as a constituent by the models broken downby label (i.e.
label recall).
the c-pcfg (kim et al., 2019a) achieve a strongerparsing performance with its strong linguistic con-straints (e.g.
a ﬁnite set of production rules), struct-former may have a border domain of application.
for example, it can replace the standard trans-former encoder in most of the popular large-scalepre-trained language models (e.g.
bert and re-berta) and transformer based machine translationmodels.
different from the transformer-based tree-t (wang et al., 2019), we did not directly use con-stituents to restrict the self-attention receptive ﬁeld.
but structformer achieves a stronger constituencyparsing performance.
this result may suggest thatdependency relations are more suitable for gram-mar induction in transformer-based models.
table3 shows that our model achieves strong accuracywhile predicting noun phrase (np), prepositionphrase (pp), adjective phrase (adjp), and adverbphrase (advp)..5.3 unsupervised dependency parsing.
the unsupervised dependency parsing evaluationcompares the induced dependency relations withthose in the reference dependency graph.
the mostcommon metric is the unlabeled attachment score(uas), which measures the percentage that a tokenis correctly attached to its parent in the referencetree.
another widely used metric for unsuperviseddependency parsing is undirected unlabeled at-tachment score (uuas) measures the percentagethat the reference undirected and unlabeled connec-tions are recovered by the induced tree.
similarto the unsupervised constituency parsing, we takethe model trained on ptb dataset and evaluate iton wsj test set (section 23).
for the wsj test set,reference dependency graphs are converted fromits human-annotated constituency trees.
however,there are two different sets of rules for the conver-sion: the stanford dependencies and the conll de-pendencies.
while stanford dependencies are usedas reference dependencies in previous unsupervised.
7202relations.
mlmppl.
constituencyuf1.
stanford.
conll.
uas.
uuas.
uas.
uuas.
parent+depparentdep.
60.9 (1.0)63.0 (1.2)63.2 (0.6).
54.0 (0.3)40.2 (3.5)51.8 (2.4).
46.2 (0.4)32.4 (5.6)15.2 (18.2).
61.6 (0.4)49.1 (5.7)41.6 (16.8).
36.2 (0.1)30.0 (3.7)20.2 (12.2).
56.3 (0.2)50.0 (5.3)44.7 (13.9).
table 4: the performance of structformer with different combinations of attention masks.
uas stands for unla-beled attachment score.
uuas stands for undirected unlabeled attachment score..methods.
uas.
w/o gold pos tags.
dmv (klein and manning, 2004)e-dmv (headden iii et al., 2009)ur-a e-dmv (tu and honavar, 2012)cs* (spitkovsky et al., 2013)neural e-dmv (jiang et al., 2016)gaussian dmv (he et al., 2018)inp (he et al., 2018)neural l-pcfgs (zhu et al., 2020)structformer.
w/ gold pos tags (for reference only)dmv (klein and manning, 2004)ur-a e-dmv (tu and honavar, 2012)maxenc (le and zuidema, 2015)neural e-dmv (jiang et al., 2016)crfae (cai et al., 2017)l-ndmv† (han et al., 2017).
35.838.246.164.4*42.743.1 (1.2)47.9 (1.2)40.5 (2.9)46.2 (0.4).
39.757.065.857.655.763.2.table 5: dependency parsing results on wsj testset.
starred entries (*) beneﬁt from additional punctuation-based constraints.
daggered entries (†) beneﬁt fromlarger additional training data.
baseline results arefrom he et al.
(2018)..parsing papers, we noticed that our model some-times output dependency structures that are closerto the conll dependencies.
therefore, we reportuas and uuas for both stanford and conlldependencies.
following the setting of previouspapers (jiang et al., 2016), we ignored the punctua-tion during evaluation.
to obtain the dependencyrelation from our model, we compute the argmaxfor dependency distribution:.
k = argmaxj(cid:54)=ipd(j|i).
(18).
and assign the k-th token as the parent of i-th token.
table 5 shows that our model achieves competi-tive dependency parsing performance while com-paring to other models that do not require goldpos tags.
while most of the baseline models stillrely on some kind of latent pos tags or pre-trainedword embeddings, structformer can be seen as aneasy-to-use alternative that works in an end-to-endfashion.
table 6 shows that our model recovers61.6% of undirected dependency relations.
given.
the strong performances on both dependency pars-ing and masked language modeling, we believethat the dependency graph schema could be a vi-able substitute for the complete graph schema usedin the standard transformer.
appendix a.4 providesexamples of parent distribution..since our model uses a mixture of the relationprobability distribution for each self-attention head,we also studied how different combinations of re-lations affect the performance of our model.
table6 shows that the model can achieve the best per-formance while using both parent and dependentrelations.
the model suffers more on dependencyparsing if the parent relation is removed.
and if thedependent relationship is removed, the model willsuffer more on the constituency parsing.
appendixa.3 shows the weight for parent and dependentrelations learnt from mlm tasks.
it’s interestingto observe that structformer tends to focus on theparent relations in the ﬁrst layer, and start to useboth relations from the second layer..6 conclusion.
in this paper, we introduce a novel dependency andconstituency joint parsing framework.
based onthe framework, we propose structformer, a newunsupervised parsing algorithm that does unsuper-vised dependency and constituency parsing at thesame time.
we also introduced a novel dependency-constrained self-attention mechanism that allowseach attention head to focus on a speciﬁc mixtureof dependency relations.
this brings transformerscloser to modeling a directed dependency graph.
the experiments show promising results that struct-former can induce meaningful dependency andconstituency structures and achieve better perfor-mance on masked language model tasks.
this re-search provides a new path to build more linguisticbias into a pre-trained language model..7203references.
jiong cai, yong jiang, and kewei tu.
2017. crfautoencoder for unsupervised dependency parsing.
arxiv preprint arxiv:1708.01018..shay b cohen, dipanjan das, and noah a smith.
2011.unsupervised structure prediction with non-parallelmultilingual guidance.
in proceedings of the 2011conference on empirical methods in natural lan-guage processing, pages 50–61..shay b cohen and noah a smith.
2009. shared logis-tic normal distributions for soft parameter tying inunsupervised grammar induction.
in proceedings ofhuman language technologies: the 2009 annualconference of the north american chapter of the as-sociation for computational linguistics, pages 74–82..hal daum´e iii.
2009. unsupervised search-basedin proceedings of the 26thstructured prediction.
annual international conference on machine learn-ing, pages 209–216..hiroyuki deguchi, akihiro tamura, and takashi ni-nomiya.
2019. dependency-based self-attention forin proceedings of the interna-transformer nmt.
tional conference on recent advances in naturallanguage processing (ranlp 2019), pages 239–246..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..andrew drozdov, patrick verga, mohit yadav, mohitiyyer, and andrew mccallum.
2019. unsupervisedlatent tree induction with deep inside-outside recur-sive auto-encoders.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 1129–1141..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a smith.
2016. recurrent neural networkin proceedings of naacl-hlt, pagesgrammars.
199–209..alexander gelbukh, sulema torres, and hiram calvo.
2005. transforming a constituency treebank into adependency treebank.
procesamiento del lenguajenatural, (35):145–152..jennifer gillenwater, kuzman ganchev, jo˜ao grac¸a,fernando pereira, and ben taskar.
2010. sparsityin dependency grammar induction.
acl 2010, page194..wenjuan han, yong jiang, and kewei tu.
2017.dependency grammar induction with neural lexi-arxiv preprintcalization and big training data.
arxiv:1708.00801..junxian he, graham neubig,.
and taylor berg-kirkpatrick.
2018. unsupervised learning of syntac-tic structure with invertible neural projections.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages1292–1302..william p headden iii, mark johnson, and david mc-closky.
2009. improving unsupervised dependencyparsing with richer contexts and smoothing.
in pro-ceedings of human language technologies: the 2009annual conference of the north american chapter ofthe association for computational linguistics, pages101–109..phu mon htut, jason phang, shikha bordia, andsamuel r bowman.
2019. do attention heads inbert track syntactic dependencies?
arxiv preprintarxiv:1911.12246..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger p levy.
2020. a systematic assessmentof syntactic generalization in neural language mod-els.
arxiv preprint arxiv:2005.03692..yong jiang, wenjuan han, kewei tu, et al.
2016. un-supervised neural dependency parsing.
associationfor computational linguistics (acl)..taeuk kim, jihun choi, daniel edmiston, and sang-goo lee.
2020. are pre-trained language mod-simple but strong base-els aware of phrases?
arxiv preprintlines for grammararxiv:2002.00737..induction..yoon kim, chris dyer, and alexander m rush.
2019a.
compound probabilistic context-free grammars forgrammar induction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 2369–2385..yoon kim, alexander m rush, lei yu, adhiguna kun-coro, chris dyer, and g´abor melis.
2019b.
unsuper-in pro-vised recurrent neural network grammars.
ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 1105–1117..dan klein.
2005. the unsupervised learning of naturallanguage structure.
stanford university stanford..dan klein and christopher d manning.
2002. a gener-ative constituent-context model for improved gram-in proceedings of the 40th annualmar induction.
meeting of the association for computational lin-guistics, pages 128–135..dan klein and christopher d manning.
2004. corpus-based induction of syntactic structure: models of de-in proceedings of thependency and constituency.
42nd annual meeting of the association for computa-tional linguistics (acl-04), pages 478–485..7204international conference on learning representa-tions..valentin i spitkovsky, hiyan alshawi, angel chang,and dan jurafsky.
2011. unsupervised dependencyin pro-parsing without gold part-of-speech tags.
ceedings of the 2011 conference on empirical meth-ods in natural language processing, pages 1281–1290..valentin i spitkovsky, daniel jurafsky, and hiyan al-shawi.
2013. breaking out of local optima withcount transforms and model recombination: a studyin grammar induction..patrick verga, daniel andor,emma strubell,david weiss,and andrew mccallum.
2018.linguistically-informed self-attention for semanticrole labeling.
arxiv preprint arxiv:1804.08199..kewei tu and vasant honavar.
2012. unambiguity reg-ularization for unsupervised learning of probabilis-in proceedings of the 2012 jointtic grammars.
conference on empirical methods in natural lan-guage processing and computational natural lan-guage learning, pages 1324–1334..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..yaushian wang, hung-yi lee, and yun-nung chen.
2019. tree transformer: integrating tree structuresinto self-attention.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1060–1070..hao zhu, yonatan bisk, and graham neubig.
2020.the return of lexical dependencies: neural lexical-ized pcfgs.
transactions of the association for com-putational linguistics, 8:647–661..adhiguna kuncoro, lingpeng kong, daniel fried,dani yogatama, laura rimell, chris dyer, and philblunsom.
2020. syntactic structure distillation pre-training for bidirectional encoders.
arxiv preprintarxiv:2005.13482..phong le and willem zuidema.
2015. unsuperviseddependency parsing: let’s use supervised parsers.
arxiv preprint arxiv:1504.04666..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..hongyin luo, lan jiang, yonatan belinkov, and jamesglass.
2019. improving neural language models bysegmenting, attending, and predicting the future.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 1483–1493..diego marcheggiani and ivan titov.
2017..en-coding sentences with graph convolutional net-works for semantic role labeling.
arxiv preprintarxiv:1703.04826..tom´aˇs mikolov et al.
2012. statistical language mod-presentation at.
els based on neural networks.
google, mountain view, 2nd april, 80:26..yutaro omote, akihiro tamura, and takashi ninomiya.
2019. dependency-based relative positional encod-in proceedings of the in-ing for transformer nmt.
ternational conference on recent advances in natu-ral language processing (ranlp 2019), pages 854–861..john k pate and sharon goldwater.
2013. unsuper-vised dependency parsing with acoustic cues.
trans-actions of the association for computational lin-guistics, 1:63–74..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..yikang shen, zhouhan lin, chin-wei huang, andaaron courville.
2018a.
neural language modelingin interna-by jointly learning syntax and lexicon.
tional conference on learning representations..yikang shen, zhouhan lin, athul paul jacob, alessan-dro sordoni, aaron courville, and yoshua bengio.
2018b.
straight to the tree: constituency parsingwith neural syntactic distance.
in proceedings of the56th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1171–1180..yikang shen, shawn tan, alessandro sordoni, andaaron courville.
2018c.
ordered neurons: integrat-ing tree structures into recurrent neural networks.
in.
7205height, we use a relu activation function to keeponly the positive values:.
(cid:15)[l,r] = relu (cid:0)min (cid:0)τl−1 − δ[l,r], τr − δ[l,r].
(cid:1)(cid:1).
(20)to make sure all constituent are not isolated andmaintain the rank of t, we subtract all t by themaximum of (cid:15):.
ˆδi = δi − max.
{[l,r]}/[1,n].
(cid:0)(cid:15)[l,r].
(cid:1).
(21).
a appendix.
a.1.
joint dependency and constituencyparsing.
algorithm 3 the joint dependency and con-stituency parsing algorithm.
inputs are a sequenceof words w, syntactic distances d, syntactic heightsh. outputs are a binary constituency tree t, a de-pendency graph d that is represented as a set ofdependency relations, the parent of dependencygraph d, and the syntactic height of parent.
1: function buildtree(w, d, h)2:3:.
t ⇐ leaf(w), d ⇐ [], parent ⇐ w, height.
if d = [] and w = [w] and h = [h] then.
⇐ h.else.
i ⇐ arg max(d)tl, dl, parentl, heightl.
buildtree(d<i, w≤i, h≤i).
tr, dr, parentr, heightr.
buildtree(d>i, w>i, h>i).
⇐.
⇐.
t ⇐ node(childl ⇐ tl, childr ⇐ tr)d ⇐ union(dl, dr)if heightl > heightr then.
d.add(parentl ← parentr)parent ⇐ parentl, height ⇐ heightl.
else.
d.add(parentr ← parentl)parent ⇐ parentr, height ⇐ heightr.
return t, d, parent, height.
4:5:6:.
7:.
8:9:10:11:12:13:14:15:16:.
a.2 calibrating the syntactic distance and.
height.
in section 3.3, we explained the relation between∆ and t, that if δi < τj, the i-th word won’t beable to attend beyond the j-th split point.
how-ever, in a speciﬁc case, the constraint will isolatea constituent [l, r] from the rest of the sentence.
if τl−1 and τr are larger then all height δl,...,r inthe constituent, then all words in [l, r] won’t beable to attend on the outside of the constituent.
this phenomenon will prevent their output contex-tual embedding from encoding the full context.
toavoid this phenomenon, we propose to calibratethe syntactic distance t according to the syntacticheight ∆.
first, we compute the maximum syntac-tic height for each constituent:.
δ[l,r] = max (δl, ..., δr) ,.
l < r.(19).
then we compute the minimum difference be-tween δ[l,r] and [l, r]’s left and right boundary dis-tance.
since we only care about constituents thatthe boundary distance is larger than its maximum.
7206a.3 dependency relation weights for self-attention heads.
(a) dependency relation weights learnt on ptb.
(b) dependency relation weights learnt on bllip-sm.
figure 4: dependency relation weights learnt on different datasets.
row i constains relation weights for all at-tention heads in the i-th transformer layer.
p represents the parent relation.
d represents the dependent relation.
we observe a clearer preference for each attention head in the model trained on bllip-sm.
this probably due tobllip-sm has signﬁcantly more training data.
it’s also interesting to notice that the ﬁrst layer tend to focus onparent relations..7207a.4 dependency distribution examples.
(a).
(b).
figure 5: dependency distribution examples from wsj test set.
each row is the parent distribution for the respec-tive word.
the sum of each distribution may not equal to 1. against our intuition, the distribution is not verysharp.
this is partially due to the ambiguous nature of the dependency graph.
as we previously discussed, at leasttwo styles of dependency rules (conll and stanford) exist.
and without extra constraint or supervision, the modelseems trying to model both of them at the same time.
one interesting future work will be ﬁnding an inductive biasthat can encourage the model to converge to a speciﬁc style of dependency graph..7208a.5 the performance of structformer with different mask rates.
mask rate.
mlmppl.
constituencyuf1.
stanford.
conll.
uas.
uuas.
uas.
uuas.
0.10.20.30.40.5.
45.3 (1.2)50.4 (1.3)60.9 (1.0)76.9 (1.2)100.3 (1.4).
51.45 (2.7)54.0 (0.6)54.0 (0.3)53.5 (1.5)53.2 (0.9).
31.4 (11.9)37.4 (12.6)46.2 (0.4)34.0 (10.3)36.3 (9.8).
51.2 (8.1)55.6 (8.8)61.6 (0.4)52.0 (7.4)53.6 (6.8).
32.3 (5.2)33.0 (5.7)36.2 (0.1)29.5 (5.4)30.6 (4.2).
52.4 (4.5)53.5 (4.7)56.3 (0.2)50.6 (4.1)51.3 (3.2).
table 6: the performance of structformer on ptb dataset with different mask rates.
dependency parsing isespecially affected by the masks.
mask rate 0.3 provides the best and the most stable performance..7209