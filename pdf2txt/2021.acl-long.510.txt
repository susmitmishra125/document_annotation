super tickets in pre-trained language models: from modelcompression to improving generalization.
chen liang∗ 1, simiao zuo1, minshuo chen1, haoming jiang1,xiaodong liu2, pengcheng he3, tuo zhao1, weizhu chen31 georgia institute of technology, 2 microsoft research, 3 microsoft azure ai{cliang73, simiaozuo, mchen393, jianghm, tourzhao}@gatech.edu{xiaodl,penhe,wzchen}@microsoft.com.
abstract.
the lottery ticket hypothesis suggests that anover-parametrized network consists of “lottery tick-ets”, and training a certain collection of them (i.e.,a subnetwork) can match the performance of thefull model.
in this paper, we study such a collec-tion of tickets, which is referred to as “winningtickets”, in extremely over-parametrized models,e.g., pre-trained language models.
we observe thatat certain compression ratios, the generalizationperformance of the winning tickets can not onlymatch but also exceed that of the full model.
in par-ticular, we observe a phase transition phenomenon:as the compression ratio increases, generalizationperformance of the winning tickets ﬁrst improvesthen deteriorates after a certain threshold.
we referto the tickets on the threshold as “super tickets”.
we further show that the phase transition is taskand model dependent — as the model size becomeslarger and the training data set becomes smaller, thetransition becomes more pronounced.
our experi-ments on the glue benchmark show that the supertickets improve single task ﬁne-tuning by 0.9 pointson bert-base and 1.0 points on bert-large, interms of task-average score.
we also demonstratethat adaptively sharing the super tickets across tasksbeneﬁts multi-task learning1..1.introduction.
the lottery ticket hypothesis (lth, frankle andcarbin (2018)) suggests that an over-parameterizednetwork consists of “lottery tickets”, and training acertain collection of them (i.e., a subnetwork) can1) match the performance of the full model; and 2).
∗work was done at microsoft azure ai.
1our.
codes.
are.
available.
https://github.com/cliang1453/super-structured-lottery-tickets..at.
outperform randomly sampled subnetworks of thesame size (i.e., “random tickets”).
the existenceof such a collection of tickets, which is usuallyreferred to as “winning tickets”, indicates the po-tential of training a smaller network to achieve thefull model’s performance.
lth has been widelyexplored in across various ﬁelds of deep learning(frankle et al., 2019; zhou et al., 2019; you et al.,2019; brix et al., 2020; movva and zhao, 2020;girish et al., 2020)..aside from training from scratch, such winningtickets have demonstrated their abilities to transferacross tasks and datasets (morcos et al., 2019; yuet al., 2019; desai et al., 2019; chen et al., 2020a).
in natural language processing, chen et al.
(2020b);prasanna et al.
(2020) have shown existence ofthe winning tickets in pre-trained language models.
these tickets can be identiﬁed when ﬁne-tuningthe pre-trained models on downstream tasks.
asthe pre-trained models are usually extremely over-parameterized (e.g., bert devlin et al.
(2019),gpt-3 brown et al.
(2020), t5 raffel et al.
(2019)),previous works mainly focus on searching for ahighly compressed subnetwork that matches theperformance of the full model.
however, behav-ior of the winning tickets in lightly compressedsubnetworks is largely overlooked..in this paper, we study the behavior of the win-ning tickets in pre-trained language models, witha particular focus on lightly compressed subnet-works.
we observe that generalization performanceof the winning tickets selected at appropriate com-pression ratios can not only match, but also exceedthat of the full model.
in particular, we observea phase transition phenomenon (figure 1): thetest accuracy improves as the compression ratiogrows until a certain threshold (phase i); passingthe threshold, the accuracy deteriorates, yet is stillbetter than that of the random tickets (phase ii).
inphase iii, where the model is highly compressed,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6524–6538august1–6,2021.©2021associationforcomputationallinguistics6524figure 1: illustrations of the phase transition phenomenon.
left: generalization performance of the ﬁne-tunedsubnetworks (the same as figure 4 in section 5).
middle and right: an interpretation of bias-variance trade-off..training collapses.
we refer to the set of winningtickets selected on that threshold as “super tickets”..of super tickets forms a compressed network thatexhibits a large performance gain..we interpret the phase transition in the context oftrade-offs between model bias and variance (fried-man et al., 2001, chapter 7).
it is well understoodthat an expressive model induces a small bias, anda large model induces a large variance.
we classifythe tickets into three categories: non-expressivetickets, lightly expressive tickets, and highly ex-pressive tickets.
the full model has a strong expres-sive power due to over-parameterization, so thatits bias is small.
yet its variance is relatively large.
in phase i, by removing non-expressive tickets,variance of the selected subnetwork reduces, whilemodel bias remains unchanged and the expressivepower sustains.
accordingly, generalization per-formance improves.
we enter phase ii by furtherincreasing the compression ratio.
here lightly ex-pressive tickets are pruned.
consequently, modelvariance continues to decrease.
however, modelbias increases and overturns the beneﬁt of the re-duced variance.
lastly for phase iii, in the highlycompressed region, model bias becomes notori-ously large and reduction of the variance pales.
asa result, training breaks down and generalizationperformance drops signiﬁcantly..we conduct systematic experiments and analysesto understand the phase transition.
our experimentson multiple natural language understanding (nlu)tasks in the glue (wang et al., 2018) benchmarkshow that the super tickets can be used to improvesingle task ﬁne-tuning by 0.9 points over bert-base (devlin et al., 2019) and 1.0 points over bert-large, in terms of task-average score.
moreover,our experiments show that the phase transition phe-nomenon is task and model dependent.
it becomesmore pronounced as a larger model is used to ﬁt atask with less training data.
in such a case, the set.
the existence of super tickets suggests poten-tial beneﬁts to applications, such as multi-tasklearning (mtl).
in mtl, different tasks requiredifferent capacities to achieve a balance betweenmodel bias and variance.
however, existing meth-ods do not speciﬁcally balance the bias and vari-ance to accommodate each task.
in fact, the ﬁne-tuning performance on tasks with a small datasetis very sensitive to randomness.
this suggests thatmodel variance in these tasks are high due to over-parameterization.
to reduce such variance, wepropose a tickets sharing strategy.
speciﬁcally, foreach task, we select a set of super tickets duringsingle task ﬁne-tuning.
then, we adaptively sharethese super tickets across tasks..our experiments show that tickets sharing im-proves mtl by 0.9 points over mt-dnnbase (liuet al., 2019) and 1.0 points over mt-dnnlarge, interms of task-average score.
tickets sharing furtherbeneﬁts downstream ﬁne-tuning of the multi-taskmodel, and achieves a gain of 1.0 task-averagescore.
in addition, the multi-task model obtainedby such a sharing strategy exhibits lower sensitiv-ity to randomness in downstream ﬁne-tuning tasks,suggesting a reduction in variance..we summarize our contributions as follows:• our result is the ﬁrst to identify the phasetransition phenomenon in pruning large neural lan-guage models..• our result is the ﬁrst to show that pruning canimprove the generalization when the models arelightly compressed, which has been overlookedby previous works.
our analysis paves the wayfor understanding the connection between modelcompression and generalization..• motivated by our observed phase transition,.
6525biasvariancepercent of weight remainingphase iphase iiphase iiibiasvariancepercent of weight remainingphase iphase iiphase iii0.80.60.41.0percent of weight remainingphase iphase iiphase iiiwinning random accuracy0.80.60.41.0biasvariancepercent of weight remainingphase iphase iiphase iiipercent of weight remainingphase iphase iiphase iiiwinningrandomaccuracy0.80.60.41.00.80.60.41.0percent of weight remainingphase iphase iiphase iiiwe further propose a new pruning approach formulti-task ﬁne-tuning of neural language models..2 background.
we brieﬂy introduce the transformer architectureand the lottery ticket hypothesis..2.1 transformer architecture.
the transformer (vaswani et al., 2017) encoderis composed of a stack of identical transformerlayers.
each layer consists of a multi-head attentionmodule (mha) followed by a feed-forward module(ffn), with a residual connection around each.
thevanilla single-head attention operates as.
att(q, k, v ) = softmax.
(cid:19).
(cid:18) qk(cid:62)√d.v,.
where q, k, v ∈ rl×d are d-dimensional vectorrepresentations of l words in sequences of queries,keys and values.
in mha, the h-th attention headis parameterized by w qh , w k.h ∈ rd×dh as.
h , w v.hh(q, x, w {q,k,v }.
h.) = att(qw q.h , xw k.h , xw v.h ),.
where q ∈ rl×d and x ∈ rl×d are the query andkey/value vectors.
in mha, h independently pa-rameterized attention heads are applied in parallel,h ∈ rdh×d:and the outputs are aggregated by w o.over-parametrized pre-trained models across var-ious tasks and datasets (morcos et al., 2019; yuet al., 2019; desai et al., 2019; chen et al., 2020a).
for example, chen et al.
(2020b); prasanna et al.
(2020) have shown the existence of winning ticketswhen ﬁne-tuning bert on downstream tasks..there is also a surge of research exploringwhether certain structures, e.g., channels in convo-lutional layers and attention heads in transformers,exhibit properties of the lottery tickets.
comparedto unstructured tickets, training with structured tick-ets is memory efﬁcient (cao et al., 2019).
liu et al.
(2018); prasanna et al.
(2020) suggest that there isno clear evidence that structured winning ticketsexist in randomly initialized or pre-trained weights.
prasanna et al.
(2020) observe that, in highly com-pressed bert (e.g., the percent of weight remain-ing is around 50%), all tickets perform equallywell.
however, prasanna et al.
(2020) have notinvestigated the cases where the percent of weightremaining is over 50%..3 finding super tickets.
we identify winning tickets in bert through struc-tured pruning of attention heads and feed-forwardlayers.
speciﬁcally, in each transformer layer, weassociate mask variables ξh to each attention headand ν to the ffn (prasanna et al., 2020):.
mha(q, x)=.
hh(q, x, w {q,k,v }.
h.)w oh ..mha(q, x) =.
ξhhh(q, x, w {q,k,v }.
h.)w oh ,.
h(cid:88).
h.h(cid:88).
h.ffn(z) = νffn(z)..each ffn module contains a two-layer fully con-nected network.
given the input embedding z, welet ffn(z) denote the output of a ffn module..2.2 structured and unstructured lths.
lth (frankle and carbin, 2018) has been widelyexplored in various applications of deep learning(brix et al., 2020; movva and zhao, 2020; girishet al., 2020).
most of existing results focus onﬁnding unstructured winning tickets via iterativemagnitude pruning and rewinding in randomly ini-tialized networks (frankle et al., 2019; renda et al.,2020), where each ticket is a single parameter.
re-cent works further investigate learning dynamics ofthe tickets (zhou et al., 2019; frankle et al., 2020)and efﬁcient methods to identify them (you et al.,2019; savarese et al., 2020).
besides training fromscratch, researchers also explore the existence ofwinning tickets under transfer learning regimes for.
here, we set ξh, ν ∈ {0, 1}, and a 0 value indicatesthat the corresponding structure is pruned..we adopt importance score (michel et al., 2019)as a gauge for pruning.
in particular, the impor-tance score is deﬁned as the expected sensitivityof the model outputs with respect to the mask vari-ables.
speciﬁcally, in each transformer layer,.
i hmha = e.x∼dx.
iffn = e.x∼dx.
(cid:12)∂l(x)(cid:12)(cid:12)∂ξh(cid:12)(cid:12)∂l(x)(cid:12)(cid:12)∂ν(cid:12).
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12).
,.
,.
where l is a loss function and dx is the data distri-bution.
in practice, we compute the average overthe training set.
we apply a layer-wise (cid:96)2 normal-ization on the importance scores of the attentionheads (molchanov et al., 2016; michel et al., 2019)..6526the importance score is closely tied to expres-sive power.
a low importance score indicates thatthe corresponding structure only has a small con-tribution towards the output.
such a structure haslow expressive power.
on the contrary, a largeimportance score implies high expressive power..we compute the importance scores for all themask variables in a single backward pass at the endof ﬁne-tuning.
we perform one-shot pruning of thesame percent of heads and feed-forward layers withthe lowest importance scores.
we conduct pruningmultiple times to obtain subnetworks, or winningtickets, at different compression ratios..we adopt the weight rewinding technique inrenda et al.
(2020): we reset the parameters ofthe winning tickets to their values in the pre-trainedweights, and subsequently ﬁne-tune the subnetworkwith the original learning rate schedule.
the supertickets are selected as the winning tickets with thebest rewinding validation performance..4 multi-task learning with tickets.
sharing.
in multi-task learning, the shared model is highlyover-parameterized to ensure a sufﬁcient capacityfor ﬁtting individual tasks.
thus, the multi-taskmodel inevitably exhibits task-dependent redun-dancy when being adapted to individual tasks.
suchredundancy induces a large model variance..we propose to mitigate the aforementionedmodel redundancy by identifying task-speciﬁcsuper tickets to accommodate each task’s need.
speciﬁcally, when viewing an individual task inisolation, the super tickets can tailor the multi-taskmodel to strike an appealing balance between themodel bias and variance (recall from section 3 thatsuper tickets retain sufﬁcient expressive power, yetkeep the model variance low).
therefore, we ex-pect that deploying super tickets can effectivelytame the model redundancy for individual tasks..given the super tickets identiﬁed by each task,we exploit the multi-task information to reinforceﬁne-tuning.
speciﬁcally, we propose a tickets shar-ing algorithm to update the parameters of the multi-task model: for a certain network structure (e.g., anattention head), if it is identiﬁed as super tickets bymultiple tasks, then its weights are jointly updatedby these tasks; if it is only selected by one speciﬁctask, then its weights are updated by that task only;otherwise, its weights are completely pruned.
seefigure 2 for an illustration..figure 2: illustration of tickets sharing..h=1,(cid:96)=1.
(cid:83){νi.
h,(cid:96)}h,l.
in more detail, we denote the weight parametersin the multi-task model as θ. suppose there are ntasks.
for each task i ∈ {1, .
.
.
, n }, we denoteωi = {ξi(cid:96)}l(cid:96)=1 as the collection ofthe mask variables, where (cid:96) is the layer index andh is the head index.
then the parameters to beupdated in task i are denoted as θi = m (θ, ωi),where m (·, ωi) masks the pruned parameters ac-cording to ωi.
we use stochastic gradient descent-type algorithms to update θi.
note that the task-shared and task-speciﬁc parameters are encodedby the mask variable ωi.
the detailed algorithm isgiven in algorithm 1..tickets sharing has two major difference com-pared to sparse sharing (sun et al., 2020): 1) sunet al.
(2020) share winning tickets, while our strat-egy focuses on super tickets, which can better gen-eralize and strike a sensible balance between modelbias and variance.
2) in tickets sharing, tickets arestructured and chosen from pre-trained weight pa-rameters.
it does not require multi-task warmup,which is indispensable in sun et al.
(2020) to stabi-lize the sharing among unstructured tickets selectedfrom randomly initialized weight parameters..5 single task experiments.
5.1 data.
general language understanding evaluation(glue, wang et al.
(2018)) is a standard bench-mark for evaluating model generalization perfor-mance.
it contains nine nlu tasks, including ques-tion answering, sentiment analysis, text similarity.
6527algorithm 1 tickets sharinginput: pre-trained base model parameters θ. num-i=1.
lossi=1 di..ber of tasks n .
mask variables {ωi}nfunctions {li}nnumber of epochs tmax..i=1.
dataset d = (cid:83)n.1: for i in n do2:.
m (θ, ωi)..initialize the super tickets for task i: θi =.
3: end for4: for epoch in 1, .
.
.
, tmax doshufﬂe dataset d.5:for a minibatch bi of task i in d do.
6:.
7:.
8:.
9:.
compute loss li(θi).
compute gradient ∇θli(θi).
update θi using sgd-type algorithm..end for.
10:11: end for.
and textual entailment.
details about the bench-mark are deferred to appendix a.1.1..5.2 models & training.
we ﬁne-tune a pre-trained bert model with task-speciﬁc data to obtain a single task model.
we ap-pend a task-speciﬁc fully-connected layer to bertas in devlin et al.
(2019).
• st-dnnbase/large is initialized with bert-base/large followed by a task-speciﬁc layer.
• supertbase/large is initialized with the chosenset of super tickets in bert-base/large followedby a task-speciﬁc layer.
speciﬁcally, we prunebert-base/large in unit of 10% heads and 10%feed-forward layers (ffn) at 8 different sparsitylevels (10% heads and 10% ffn, 20% heads and20% ffn, etc).
among them, the one with the bestrewinding validation result is chosen as the set ofsuper tickets.
we randomly sample 10% gluedevelopment set for tickets selection..our implementation is based on the mt-dnncode base3.
we use adamax (kingma and ba,2014) as our optimizer.
we tune the learning ratein {5×10−5, 1×10−4, 2×10−4} and batch size in{8, 16, 32}.
we train for a maximum of 6 epochswith early-stopping.
all training details are sum-marized in appendix a.1.2..5.3 generalization of the super tickets.
we conduct 5 trails of pruning and rewinding ex-periments using different random seeds.
table 1.and 2 show the averaged evaluation results on theglue development and test sets, respectively.
weremark that the gain of supertbase/large over st-dnnbase/large is statistically signiﬁcant.
all theresults4 have passed a paired student t-test withp-values less than 0.05. more validation statisticsare summarized in appendix a.1.3..our results can be summarized as follows.
1) in all the tasks, supert consistently achievesbetter generalization than st-dnn.
the task-averaged improvement is around 0.9 over st-dnnbase and 1.0 over st-dnnlarge..figure 3: single task ﬁne-tuning validation resultsin different glue tasks.
upper: performance gain.
lower: percent of weight remaining..2) performance gain of the super tickets is moresigniﬁcant in small tasks.
for example, in ta-ble 1, we obtain 3.3 points gain on rte (2.5kdata), but only 0.4/0.3 on qqp (364k data) in thesupertbase experiments.
furthermore, from fig-ure 3, note that the super tickets are more heavilycompressed in small tasks, e.g., for supertbase,83% weights remaining for rte, but 93% forqqp.
these observations suggest that for smalltasks, model variance is large, and removing non-expressive tickets reduces variance and improvesgeneralization.
for large tasks, model variance islow, and all tickets are expressive to some extent.
3) performance of the super tickets is relatedto model size.
switching from supertbase tosupertlarge, the percent of weights remainingshrinks uniformly across tasks, yet the generaliza-tion gains persist (figure 3).
this suggests that inlarge models, more non-expressive tickets can bepruned without performance degradation..3https://github.com/namisan/mt-dnn.
value is 0.37..4except for sts-b (supertbase, table 1), where the p-.
652801234performance gainsupert-large supert-baserte 2.49kmrpc 3.67ksts-b 5.75kcola 8.55ksst-2 67.3kqnli 108kqqp 364kmnli 393ktask (size)0.70.80.91.0% of weight remaining0.820.660.840.770.790.890.840.920.830.860.890.860.860.930.930.93rte mrpc cola sst sts-b qnli qqp mnli-m/mm averageacc acc/f1 mcc acc p/s corr acc.
acc/f1.
acc.
score compression.
average.
st-dnnbase.
69.2 86.2/90.4.
57.8.
92.9 89.7/89.2.
91.2.
90.9/88.0.
84.5/84.4.
supertbase.
72.5 87.5/91.1.
58.8.
93.4 89.8/89.4.
91.3.
91.3/88.3.
84.5/84.5.
st-dnnlarge 72.1 85.2/89.5.
62.1.
93.3 89.9/89.6.
92.2.
91.3/88.4.
86.2/86.1.
supertlarge.
74.1 88.0/91.4.
64.3.
93.9 89.9/89.7.
92.4.
91.4/88.5.
86.5/86.2.
82.8.
83.7.
84.1.
85.1.
100%.
86.8%.
100%.
81.7%.
table 1: single task ﬁne-tuning evaluation results on the glue development set.
st-dnn and supert results arethe averaged score over 5 trails with different random seeds..rte mrpc cola sst sts-b qnli qqp mnli-m/mm averageacc.
mcc acc s corr acc.
acc.
f1.
f1.
score compression.
average.
st-dnnbase 66.4.supertbase.
69.6.
88.9.
89.4.
52.1.
93.5.
85.8.
54.3.
94.1.
86.2.
90.5.
90.5.
71.2.
71.3.
84.6/83.4.
84.6/83.8.
79.6.
80.4.
100%.
86.8%.
table 2: single task ﬁne-tuning test set results scored using the glue evaluation server2.
results of st-dnnbaseare from devlin et al.
(2019)..figure 5: phase transition under different randomlysampled training subsets.
note that the settings are thesame as figure 4 (bottom left), except the data size..5.4 phase transition.
phase transitions are shown in figure 4. we plotthe evaluation results of the winning, the random,and the losing tickets under 8 sparsity levels us-ing bert-base and bert-large.
the winningtickets contain structures with the highest impor-tance scores.
the losing tickets are selected re-versely, i.e., the structures with the lowest im-portance scores are selected, and high-importancestructures are pruned.
the random tickets are sam-pled uniformly across the network.
we plot theaveraged scores over 5 trails using different ran-dom seeds5.
phase transitions of all the gluetasks are in appendix a.5..we summarize our observations:1) the winning tickets are indeed the “winners”.
in phase i and early phase ii, the winning ticketsperform better than the full model and the randomtickets.
this demonstrates the existence of struc-.
5except for mnli, where we plot 3 trails as the there are.
less variance among trails..figure 4: single task ﬁne-tuning evaluation results ofthe winning (blue), the random (orange), and the losing(green) tickets on the glue development set under var-ious sparsity levels..6529rte mrpc cola sst sts-b qnli qqp mnli-m/mm averageacc acc/f1 mcc acc p/s corr acc.
acc/f1.
acc.
score compression.
average.
mt-dnnbase+ st fine-tuning.
79.0 80.6/86.279.1 86.8/89.2.
ticket-sharebase+ st fine-tuning.
81.2 87.0/90.583.0 89.2/91.6.
mt-dnnlarge+ st fine-tuning.
83.0 85.2/89.483.4 87.5/91.0.
ticket-sharelarge 80.5 88.4/91.584.5 90.2/92.9+ st fine-tuning.
54.059.5.
52.059.7.
56.263.5.
61.865.0.
92.2 86.2/86.493.6 90.6/90.4.
92.7 87.7/87.593.5 91.1/91.0.
93.5 87.2/86.994.3 90.7/90.6.
93.2 89.2/89.194.1 91.3/91.1.
90.591.0.
91.091.9.
92.292.9.
92.193.0.
90.6/87.491.6/88.6.
90.7/87.591.6/88.7.
91.2/88.191.9/89.2.
91.3/88.491.9/89.1.
84.6/84.285.3/85.0.
84.5/84.185.0/85.0.
86.5/86.087.1/86.7.
86.7/86.087.0/86.8.
82.484.6.
83.385.6.
84.486.4.
85.487.1.
100%100%.
92.9%92.9%.
100%100%.
83.3%83.3%.
table 3: multi-task learning evaluation results on the glue development set.
results of mt-dnnbase/large withand without st fine-tuning are from liu et al.
(2020)..tured winning tickets in lightly compressed bertmodels, which prasanna et al.
(2020) overlook..2) phase transition is pronounced over differenttasks and models.
accuracy of the winning ticketsincreases up till a certain compression ratio (phasei); passing the threshold, the accuracy decreases(phase ii), until its value intersects with that ofthe random tickets (phase iii).
note that phaseiii agrees with the observations in prasanna et al.
(2020).
accuracy of the random tickets decreasesin each phase.
this suggests that model bias in-creases steadily, since tickets with both low andhigh expressive power are discarded.
accuracy ofthe losing tickets drops signiﬁcantly even in phasei, suggesting that model bias increases drasticallyas highly expressive tickets are pruned..3) phase transition is more pronounced in largemodels and small tasks.
for example, in figure 4,the phase transition is more noticeable in bert-large than in bert-base, and is more pronouncedin rte (2.5k) and mrpc (3.7k) than in sst (67k)and mnli (393k).
the phenomenon becomesmore signiﬁcant for the same task when we onlyuse a part of the data, e.g., figure 5 vs. figure 4(bottom left)..6 multi-task learning experiments.
6.1 model & training.
we adopt the mt-dnn architecture proposed inliu et al.
(2020).
the mt-dnn model consistsof a set of task-shared layers followed by a set oftask-speciﬁc layers.
the task-shared layers take inthe input sequence embedding, and generate sharedsemantic representations by optimizing multi-taskobjectives.
our implementation is based on themt-dnn code base.
we follow the same trainingsettings in liu et al.
(2020) for multi-task learn-.
ing, and in section 5.2 for downstream ﬁne-tuning.
more details are summarized in appendix a.2.
• mt-dnnbase/large.
an mt-dnn model re-ﬁned through multi-task learning, with task-sharedlayers initialized by pre-trained bert-base/large.
• mt-dnnbase/large + st fine-tuning.
a sin-gle task model obtained by further ﬁne-tuning mt-dnn on an individual downstream task.
• ticket-sharebase/large.
an mt-dnn modelreﬁned through the ticket sharing strategy, withtask-shared layers initialized by the union of thesuper tickets in pre-trained bert-base/large.
• ticket-sharebase/large + st fine-tuning.
aﬁne-tuned single-task ticket-share model..6.2 experimental results.
table 3 summarizes experimental results.
the ﬁne-tuning results are averaged over 5 trails using differ-ent random seeds.
we have several observations:1) ticket-sharebase and ticket-sharelargeachieve 0.9 and 1.0 gain in task-average score overmt-dnnbase and mt-dnnlarge, respectively.
in some small tasks (rte, mrpc), ticket-shareachieves better or on par results compared to mt-dnn+fine-tuning.
this suggests that by balancingthe bias and variance for different tasks, the multi-task model’s variance is reduced.
in large tasks(qqp, qnli and mnli), ticket-share behavesequally well with the full model.
this is becausetask-shared information is kept during pruning andstill beneﬁts multi-task learning..2) ticket-sharebase+fine-tuning and ticket-sharelarge+fine-tuning achieve 1.0 and 0.7 gainsin task-average score over mt-dnnbase+fine-tuning and mt-dnnlarge+fine-tuning, respec-tively.
this suggests that reducing the variancein the multi-task model beneﬁts ﬁne-tuning down-stream tasks..6530figure 6: illustration of tickets importance across tasks.
each ticket is represented by a pie chart.
the size of apie indicates the ticket importance, where a larger pie suggests the ticket exhibits higher expressivity.
each taskis represented by a color.
the share of a color indicates the task share, where a even share suggests the ticketexhibits equal expressivity in all tasks..model.
0.1% 1%.
10% 100%.
snli (dev acc%)5493.
549.
# training data.
mnli-st-dnnbasemnli-supertbase.
mt-dnnbaseticket-sharebase.
# training data.
mnli-st-dnnbasemnli-supertbase.
mt-dnnbaseticket-sharebase.
scitail (dev acc%)235.
23.
82.182.9.
82.183.3.
80.682.9.
81.983.1.
85.185.5.
85.285.8.
88.889.8.
88.390.1.
54k.
88.488.8.
88.488.9.
23k.
92.092.8.
91.193.5.
549k.
90.791.4.
91.191.5.
235k.
95.796.2.
95.796.5.table 4: domain adaptation evaluation results on snliand scitail development set.
results of mt-dnnbaseare from liu et al.
(2020)..7 domain adaptation.
to demonstrate that super tickets can quickly gen-eralize to new tasks/domains, we conduct few-shotdomain adaptation on out-of-domain nli datasets..7.1 data & training.
we brieﬂy introduce the target domain datasets.
the data and training details are summarized inappendix a.3.1 and a.3.2, respectively.
snli.
the stanford natural language inference.
dataset (bowman et al., 2015) is one of the mostwidely used entailment dataset for nli.
it contains570k sentence pairs, where the premises are drawnfrom the captions of the flickr30 corpus and hy-potheses are manually annotated..scitail is a textual entailment dataset derived froma science question answering (sciq) dataset (khotet al., 2018).
the hypotheses are created fromscience questions, rendering scitail challenging..7.2 experimental results.
we consider domain adaptation on both singletask and multi-task super tickets.
speciﬁcally,we adapt supertbase and st-dnnbase frommnli to snli/scitail, and adapt the shared em-beddings generated by ticket-sharebase and bymt-dnnbase to snli/scitail.
we adapt thesemodels to 0.1%, 1%, 10% and 100% snli/scitailtraining sets6, and evaluate the transferred modelson snli/scitail development sets.
table 4 showsthe domain adaptation evaluation results.
as wecan see, supert and ticket-share can better adaptto snli/scitail than st-dnn and mt-dnn, espe-cially under the few shot setting..6we use the subsets released in mt-dnn code base..65318 analysis.
sensitivity to random seed.
to better demon-strate that training with super tickets effectivelyreduces model variance, we evaluate models’ sen-sitivity to changes in random seeds during singletask ﬁne-tuning and multi-task downstream ﬁne-tuning.
in particular, we investigate ﬁtting smalltasks with highly over-parametrized models (vari-ance is often large in these models, see section 5and 6).
as shown in table 5, supertlarge andticket-sharelarge induce much smaller standarddeviation in validation results.
experimental detailsand further analyses are deferred to appendix a.4..rte mrpc cola sts-b sst-2.
st-dnnlargesupertlarge.
1.170.72.mt-dnnlarge1.43ticket sharelarge 0.99.
0.610.20.
0.780.67.
1.320.97.
1.140.81.
0.160.07.
0.150.08.
0.170.16.
0.180.16.table 5: standard deviation of tasks in glue (dev)over 5 different random seeds..tickets importance across tasks.
we analyzethe importance score of each ticket computed indifferent glue tasks.
for each ticket, we computethe importance score averaged over tasks as theticket importance, and the proportion of the task-speciﬁc importance score out of the sum of all tasks’scores as the task share, as illustrated in figure 6.we observe that many tickets exhibit almostequal task shares for over 5 out of 8 tasks (fig-ure 6(a)(b)).
while these tickets contribute to theknowledge sharing in the majority of tasks, theyare considered non-expressive for tasks such assst-2 (see figure 6(a)(c)(d)).
this explains whysst-2 beneﬁts little from tickets sharing.
further-more, a small number of tickets are dominated bya single task, e.g., cola (figure 6(c)), or domi-nated jointly by two tasks, e.g., cola and sts-b(figure 6(d)).
this suggests that some tickets onlylearn task-speciﬁc knowledge, and the two tasksmay share certain task-speciﬁc knowledge..9 discussion.
structured lottery tickets.
lth hypothesizesthat a subset of unstructured parameters can betrained to match the full model’s performance.
in-stead, we question whether a subset of structuredweight matrices, e.g., ffn layers and attentionheads, can also be trained to match the full model’sperformance.
this question is more practically.
important than the unstructured one: training andinference on structured matrices are better opti-mized for hardware acceleration.
our results givea positive answer to this question, while previousworks show that the structured tickets do not ex-ist in highly compressed models (prasanna et al.,2020).
searching better generalized super tickets.
we select winning tickets according to the sensitiv-ity of the model outputs with respect to the maskvariables of each structure (michel et al., 2019;prasanna et al., 2020), as this measure is closelytied to the structure’s expressive power (section 3).
in addition, we conduct an one-shot pruning forcomputational simplicity.
we leave other impor-tance measures and pruning schedules, which mayhelp identifying better generalized super tickets,for future works (voita et al., 2019; behnke andheaﬁeld, 2020; wang et al., 2019; fan et al., 2019;zhou et al., 2020; sajjad et al., 2020).
searching super tickets efﬁciently.
determin-ing the compression ratio of the super tickets re-quires rewinding models at multiple sparsity levels.
to leverage super tickets in practice, a potential di-rection of research is to ﬁnd heuristics to determinethis ratio prior or early-on in training.
we leavethis for future works..10 conclusion.
we study the behaviors of the structured lotterytickets in pre-trained bert.
we observe that thegeneralization performance of the winning ticketsexhibits a phase transition phenomenon, suggestingpruning can improve generalization when modelsare lightly compressed.
based on the observation,we further propose a tickets sharing strategy toimprove multi-task ﬁne-tuning.
our analysis pavesthe way for understanding the connection betweenmodel compression and generalization..broader impact.
this paper studies the behavior of the structuredlottery tickets in pre-trained language models.
ourinvestigation neither introduces any social/ethicalbias to the model nor ampliﬁes any bias in the data.
we do not foresee any direct social consequences orethical issues.
furthermore, our proposed methodimproves performance through model compression,rendering it energy efﬁcient..6532references.
roy bar-haim, ido dagan, bill dolan, lisa ferro, anddanilo giampiccolo.
2006. the second pascalin pro-recognising textual entailment challenge.
ceedings of the second pascal challenges work-shop on recognising textual entailment..maximiliana behnke and kenneth heaﬁeld.
2020. los-ing heads in the lottery: pruning transformer atten-tion in neural machine translation.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 2664–2674, online.
association for computational lin-guistics..luisa bentivogli, ido dagan, hoa trang dang, danilogiampiccolo, and bernardo magnini.
2009. theﬁfth pascal recognizing textual entailment challenge.
in in proc text analysis conference (tac’09)..samuel r bowman, gabor angeli, christopher potts,and christopher d manning.
2015. a large anno-tated corpus for learning natural language inference.
arxiv preprint arxiv:1508.05326..christopher brix, parnia bahar, and hermann ney.
2020. successfully applying the stabilized lotteryticket hypothesis to the transformer architecture.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3909–3915..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
arxiv preprint arxiv:2005.14165..shijie cao, chen zhang, zhuliang yao, wencong xiao,lanshun nie, dechen zhan, yunxin liu, ming wu,and lintao zhang.
2019. efﬁcient and effectivesparse lstm on fpga with bank-balanced sparsity.
in proceedings of the 2019 acm/sigda interna-tional symposium on field-programmable gate ar-rays, pages 63–72..daniel cer, mona diab, eneko agirre, i˜nigo lopez-semeval-2017gazpio, and lucia specia.
2017.task 1: semantic textual similarity multilingual andcrosslingual focused evaluation.
in proceedings ofthe 11th international workshop on semantic evalu-ation (semeval-2017), pages 1–14..tianlong chen,.
jonathan frankle, shiyu chang,sijia liu, yang zhang, michael carbin, andzhangyang wang.
2020a.
the lottery tickets hy-pothesis for supervised and self-supervised pre-training in computer vision models.
arxiv preprintarxiv:2012.06908..tianlong chen, jonathan frankle, shiyu chang, sijialiu, yang zhang, zhangyang wang, and michaelthe lottery ticket hypothesiscarbin.
2020b.
arxiv preprintfor pre-trained bert networks.
arxiv:2007.12223..in proceedings of.
ido dagan, oren glickman, and bernardo magnini.
2006. the pascal recognising textual entailmentthe first inter-challenge.
national conference on machine learning chal-lenges: evaluating predictive uncertainty visualobject classiﬁcation, and recognizing textual en-tailment, mlcw’05, pages 177–190, berlin, hei-delberg.
springer-verlag..shrey desai, hongyuan zhan, and ahmed aly.
2019.evaluating lottery tickets under distributional shifts.
in proceedings of the 2nd workshop on deep learn-ing approaches for low-resource nlp (deeplo2019), pages 153–162..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..william b dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in proceedings of the third international workshopon paraphrasing (iwp2005)..angela fan, edouard grave, and armand joulin.
2019.reducing transformer depth on demand with struc-tured dropout.
arxiv preprint arxiv:1909.11556..jonathan frankle and michael carbin.
2018. the lot-tery ticket hypothesis: finding sparse, trainable neu-ral networks.
arxiv preprint arxiv:1803.03635..jonathan frankle, gintare karolina dziugaite,daniel m roy, and michael carbin.
2019. stabi-lizing the lottery ticket hypothesis.
arxiv preprintarxiv:1903.01611..jonathan frankle, david j schwab, and ari s morcos.
2020. the early phase of neural network training.
arxiv preprint arxiv:2002.10365..jerome friedman, trevor hastie, robert tibshirani,et al.
2001. the elements of statistical learning, vol-ume 1. springer series in statistics new york..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recogniz-ing textual entailment challenge.
in proceedings ofthe acl-pascal workshop on textual entailmentand paraphrasing, pages 1–9, prague.
associationfor computational linguistics..sharath girish, shishira r maiya, kamal gupta, haochen, larry davis, and abhinav shrivastava.
2020.the lottery ticket hypothesis for object recognition.
arxiv preprint arxiv:2012.04643..tushar khot, ashish sabharwal, and peter clark.
2018.scitail: a textual entailment dataset from sciencein proceedings of the aaaiquestion answering.
conference on artiﬁcial intelligence, volume 32..6533diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019. multi-task deep neural networksfor natural language understanding.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4487–4496..xiaodong liu, yu wang, jianshu ji, hao cheng,xueyun zhu, emmanuel awa, pengcheng he,weizhu chen, hoifung poon, guihong cao, et al.
2020. the microsoft toolkit of multi-task deepneural networks for natural language understanding.
in proceedings of the 58th annual meeting of theassociation for computational linguistics: systemdemonstrations, pages 118–126..zhuang liu, mingjie sun, tinghui zhou, gao huang,and trevor darrell.
2018. rethinking the value ofnetwork pruning.
arxiv preprint arxiv:1810.05270..paul michel, omer levy, and graham neubig.
2019.in ad-are sixteen heads really better than one?
vances in neural information processing systems,pages 14014–14024..pavlo molchanov, stephen tyree, tero karras, timoaila, and jan kautz.
2016. pruning convolutionalneural networks for resource efﬁcientinference.
arxiv preprint arxiv:1611.06440..ari s morcos, haonan yu, michela paganini, and yuan-dong tian.
2019. one ticket to win them all: gen-eralizing lottery ticket initializations across datasetsand optimizers.
arxiv preprint arxiv:1906.02773..rajiv movva and jason y zhao.
2020. dissecting lot-tery ticket transformers: structural and behavioralstudy of sparse neural machine translation.
arxivpreprint arxiv:2009.13270..sai prasanna, anna rogers, and anna rumshisky.
2020. when bert plays the lottery, all tickets arewinning.
arxiv preprint arxiv:2005.00561..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..alex renda, jonathan frankle, and michael carbin.
comparing rewinding and ﬁne-tuningarxiv preprint.
2020.in neural network pruning.
arxiv:2003.02389..hassan sajjad, fahim dalvi, nadir durrani, andpreslav nakov.
2020. poor man’s bert: smallerarxiv preprintand faster transformer models.
arxiv:2004.03844..pedro savarese, hugo silva, and michael maire.
2020.winning the lottery with continuous sparsiﬁcation.
advances in neural information processing systems,33..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew ng, andchristopher potts.
2013. recursive deep modelsfor semantic compositionality over a sentiment tree-in proceedings of the 2013 conference onbank.
empirical methods in natural language processing,pages 1631–1642..tianxiang sun, yunfan shao, xiaonan li, pengfei liu,hang yan, xipeng qiu, and xuanjing huang.
2020.learning sparse sharing architectures for multiplein proceedings of the aaai conference ontasks.
artiﬁcial intelligence, volume 34, pages 8936–8943..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-head self-attention: specialized heads do the heavyarxiv preprintlifting,arxiv:1905.09418..the rest can be pruned..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2018.glue: a multi-task benchmark and analysis platformfor natural language understanding.
arxiv preprintarxiv:1804.07461..ziheng wang, jeremy wohlwend, and tao lei.
2019.structured pruning of large language models.
arxivpreprint arxiv:1910.04732..alex warstadt, amanpreet singh, and samuel r bow-man.
2019. neural network acceptability judgments.
transactions of the association for computationallinguistics, 7:625–641..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122.
association forcomputational linguistics..haoran you, chaojian li, pengfei xu, yonggan fu,yue wang, xiaohan chen, richard g baraniuk,zhangyang wang, and yingyan lin.
2019. drawingearly-bird tickets: towards more efﬁcient training ofdeep networks.
arxiv preprint arxiv:1909.11957..6534haonan yu, sergey edunov, yuandong tian, and ari smorcos.
2019. playing the lottery with rewardsand multiple languages: lottery tickets in rl and nlp.
arxiv preprint arxiv:1906.02768..hattie zhou, janice lan, rosanne liu, and jasonyosinski.
2019. deconstructing lottery tickets: ze-arxiv preprintros, signs, and the supermask.
arxiv:1905.01067..wangchunshu zhou, tao ge, ke xu, furu wei, andming zhou.
2020. scheduled drophead: a regu-arxivlarization method for transformer models.
preprint arxiv:2004.13342..6535a appendix.
a.1 single task experiments.
a.1.1 dataglue.
glue is a collection of nine nlu tasks.
the benchmark includes question answering (ra-jpurkar et al., 2016), linguistic acceptability (cola,warstadt et al.
2019), sentiment analysis (sst,socher et al.
2013), text similarity (sts-b, ceret al.
2017), paraphrase detection (mrpc, dolanand brockett 2005), and natural language inference(rte & mnli, dagan et al.
2006; bar-haim et al.
2006; giampiccolo et al.
2007; bentivogli et al.
2009; williams et al.
2018) tasks.
details of theglue benchmark, including tasks, statistics, andevaluation metrics, are summarized in table 9..a.1.2 training.
we use adamax as the optimizer.
a linear learningrate decay schedule with warm-up over 0.1 is used.
we apply a gradient norm clipping of 1. we setthe dropout rate of all task speciﬁc layers as 0.1,except 0.3 for mnli and 0.05 for cola.
all thetexts were tokenized using wordpieces, and werechopped to spans no longer than 512 tokens.
allexperiments are conducted on nvidia v100 gpus..a.1.3 evaluation results statisticswe conduct 5 sets of experiments on different ran-dom seeds.
each set of experiment consists ofﬁne-tuning, pruning, and rewinding at 8 sparsitylevels.
for results on glue dev set (table 1), wereport the average score of super tickets rewindingresults over 5 sets of experiments.
the standarddeviation of the results is shown in table 6. thestatistics of the percent of weight remaining in theselected super tickets are shown in table 7..for results on glue test set (table 2), as theevaluation server sets an limit on submission times,we only evaluate the test prediction under a sin-gle random seed that gives the best task-averagevalidation results..a.2 multi-task learning experiments.
a.2.1 multi-task model training.
we adopt the mt-dnn code base and adopt theexact optimization settings in liu et al.
(2020).
weuse adamax as our optimizer with a learning rateof 5 × 10−5 and a batch size of 32. we train for amaximum number of epochs of 5 with early stop-ping.
a linear learning rate decay schedule withwarm-up over 0.1 was used.
the dropout rate of all.
the task speciﬁc layers is set to be 0.1, except 0.3for mnli and 0.05 for cola.
we clipped the gra-dient norm within 1. all the texts were tokenizedusing wordpieces, and were chopped to spans nolonger than 512 tokens..worth mentioning, the task-speciﬁc super ticketsused in ticket share are all selected during the casewhere a matched learning rate (i.e., 5 × 10−5) isused in single task ﬁne-tuning.
we empirically ﬁndthat, rewinding the super tickets selected under amatched optimization settings usually outperformsthose selected under a mismatched settings (i.e.
us-ing two different learning rates in single-task ﬁne-tuning and rewinding/multi-task learning).
thisagrees with previous observation in literature oflottery ticket hypothesis, which shows that un-structured winning tickets are not only related to itsweight initialization, but also model optimizationpath..a.2.2 multi-task model downstream.
fine-tuning.
we follow the exact optimization setting as in sec-tion 5.2 and in section a.1.2, except we chooselearning rate in {1 × 10−5, 2 × 10−5, 5 × 10−5, 1 ×10−4, 2 × 10−4}, and choose the dropout rate of alltask speciﬁc layers in {0.05, 0.1, 0.2, 0.3}..a.3 domain adaptation experiments.
a.3.1 data.
snli.
is one of the most widely used entailmentdataset for nli.
scitail involves assessing whether a given premiseentails a given hypothesis.
in contrast to other en-tailment datasets, the hypotheses in scitail is cre-ated from science questions.
these sentences arelinguistically challenging.
the corresponding an-swer candidates and premises come from relevantweb sentences.
the lexical similarity of premiseand hypothesis is often high, making scitail partic-ularly challenging..details of the snli and scitail, including tasks,statistics, and evaluation metrics, are summarizedin table 9..a.3.2 training.
for single task model domain adaptation frommnli to snli/scitail, we follow the exact op-timization setting as in section 5.2 and in sec-tion a.1.2, except we choose the learning rate in{5 × 10−5, 1 × 10−4, 5 × 10−4}..6536rte mrpc cola sts-b sst-2 qnli qqp mnli.
0.91supertbasesupertlarge 0.72.
0.740.20.
1.510.97.
0.490.07.
0.500.16.
0.100.07.
0.080.11.
0.040.02.table 6: standard deviation of the evaluation results on glue development set over 5 different random seeds..rte mrpc cola sts-b sst-2 qnli qqp mnli.
0.83supertbase (mean)0.07supertbase (std dev)0.82supertlarge (mean)supertlarge (std dev) 0.04.
0.860.080.660.04.
0.890.040.840.00.
0.860.060.770.10.
0.930.070.790.05.
0.930.000.900.03.
0.930.000.840.00.
0.930.000.920.00.table 7: statistics of the percent of weight remaining of the selected super tickets over 5 different random seeds..a.4 sensitivity analysis.
a.4.1 randomness analysisfor single task experiments in table 5, we varythe random seeds only and keep all other hyper-parameters ﬁxed.
we present the standard deviationof the validation results over 5 trails rewinding ex-periments.
for multi-task downstream ﬁne-tuningexperiments, we present the standard deviation ofthe validation results over 5 trails, each result aver-aged over learning rates in {5×10−5, 1×10−4, 2×10−4}.
this is because the downstream ﬁne-tuningperformance is more sensitive to hyper-parameters..a.4.2 hyper-parameter analysiswe further analyze the sensitivity of ticketsharelarge model to changes in hyper-parametersin downstream ﬁne-tuning in some glue tasks.
we vary the learning rate in {5 × 10−5, 1 ×10−4, 2×10−4} and keep all other hyper-parameterﬁxed.
table 8 shows the standard deviation of thevalidation results over different learning rates, eachresult averaged over 5 different random seeds.
ascan be seen, task sharelarge exhibits stronger ro-bustness to changes in learning rate in downstreamﬁne-tuning..rte mrpc cola sts-b sst-2.
mt-dnnlarge1.26ticket sharelarge 0.44.
0.860.58.
1.050.61.
0.420.36.
0.260.25.table 8: standard deviation of some tasks in glue(dev) over 3 different learning rates..a.5 phase transition on glue tasks.
figure 7 shows the phase transition plots on win-ning tickets on glue tasks absent from figure 4.all experimental settings conform to figure 4..figure 7: single task ﬁne-tuning evaluation results ofthe winning tickets on the glue development set un-der various sparsity levels..6537bert-base 94 bert-large 92 92 >,90 u 90 �ro cl !3 88 08 88 < 8686 84 84 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 92 92 >,90 u 90 ,..... ro � 8 8888 cl u < 8686 84 84 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 70 70 60 >,u 60 < ro � !3 50 0 u uu50 < 40 40 30 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 90.0 90.0 >, a::l � 87.587.5 i !3rn u� < 85.085.0 82.5 82.5 80.0 80.0 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 percent of weight remaining percent of weight remaining corpus task.
#train.
#dev.
#test.
#label.
metrics.
single-sentence classiﬁcation (glue).
colasst.
acceptabilitysentiment.
8.5k67k.
1k872.
1k1.8k.
pairwise text classiﬁcation (glue).
mnlirteqqpmrpcqnli.
nlinliparaphraseparaphraseqa/nli.
393k2.5k364k3.7k108k.
20k27640k4085.7k.
20k3k391k1.7k5.7k.
22.
32222.text similarity (glue)1.5k.
1.4kpairwise text classiﬁcation9.8k2.1k.
9.8k1.3k.
32.
1.
549k23.5k.
snliscitail.
nlinli.
matthews corraccuracy.
accuracyaccuracyaccuracy/f1accuracy/f1accuracy.
accuracyaccuracy.
sts-b.
similarity.
7k.
pearson/spearman corr.
table 9: summary of the glue benchmark, snli and scitail..6538