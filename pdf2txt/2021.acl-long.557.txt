transition-based bubble parsing:improvements on coordination structure prediction.
tianze shicornell universitytianze@cs.cornell.edu.
lillian leecornell universityllee@cs.cornell.edu.
abstract.
we propose a transition-based bubble parserto perform coordination structure identiﬁca-tion and dependency-based syntactic analysissimultaneously.
bubble representations wereproposed in the formal linguistics literaturedecades ago; they enhance dependency treesby encoding coordination boundaries and in-ternal relationships within coordination struc-tures explicitly.
in this paper, we introduce atransition system and neural models for pars-ing these bubble-enhanced structures.
ex-perimental results on the english penn tree-bank and the english genia corpus show thatour parsers beat previous state-of-the-art ap-proaches on the task of coordination structureprediction, especially for the subset of sen-tences with complex coordination structures.1.
1.introduction.
coordination structures are prevalent in treebankdata (ficler and goldberg, 2016a), especially inlong sentences (kurohashi and nagao, 1994), andthey are among the most challenging constructionsfor nlp models.
difﬁculties in correctly identify-ing coordination structures have consistently con-tributed to a signiﬁcant portion of errors in state-of-the-art parsers (collins, 2003; goldberg and el-hadad, 2010; ficler and goldberg, 2017).
theseerrors can further propagate to downstream nlpmodules and applications, and limit their perfor-mance and utility.
for example, saha et al.
(2017)report that missing conjuncts account for two-thirdsof the errors in recall made by their open informa-tion extraction system..coordination constructions are particularly chal-lenging for the widely-adopted dependency-basedparadigm of syntactic analysis, since the asymmet-ric deﬁnition of head-modiﬁer dependency rela-tions is not directly compatible with the symmetric.
bubble tree:.
obj.
nsubj.
amod.
conj.
conj.
cc.
cc.
det.
conj.
conjtea and a bun.
i prefer hot coffee or.
ud tree:.
nsubj.
conj.
obj.
amod.
conj.
cc.
cc.
det.
i prefer hot coffee or.
tea and a bun.
figure 1: bubble tree and (basic) ud tree for the same(for clarity, we omit punctuationexample sentence.
and single-word bubble boundaries.)
bubbles explic-itly encode the scope of the shared modiﬁer hot withrespect to the nested coordination, whereas the ud treegives both tea and bun identical relationships to hot..nature of the relations among the participating con-juncts and coordinators.2 existing treebanks usu-ally resort to introducing special relations to rep-resent coordination structures.
but, there remaintheoretical and empirical challenges regarding howto most effectively encode information like modi-ﬁer sharing relations while still permitting accuratestatistical syntactic analysis..in this paper, we explore kahane’s (1997) alter-native solution: extend the dependency-tree repre-sentation by introducing bubble structures to ex-plicitly encode coordination boundaries.
the co-heads within a bubble enjoy a symmetric relation-ship, as beﬁts a model of conjunction.
further, bub-ble trees support representation of nested coordina-tion, with the scope of shared modiﬁers identiﬁableby the attachment sites of bubble arcs.
figure 1compares a bubble tree against a universal depen-dencies (ud; nivre et al., 2016, 2020) tree for thesame sentence..yet, despite theses advantages, implementation.
2rambow (2010) comments on other divergences between.
1code at github.com/tzshi/bubble-parser-acl21..syntactic representation and syntactic phenomena..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7167–7182august1–6,2021.©2021associationforcomputationallinguistics7167of the formalism was not broadly pursued, for rea-sons unknown to us.
given its appealing and in-tuitive treatment of coordination phenomena, werevisit the bubble tree formalism, introducing andimplementing a transition-based solution for pars-ing bubble trees.
our transition system, bubble-hybrid, extends the arc-hybrid transition system(kuhlmann et al., 2011) with three bubble-speciﬁctransitions, each corresponding to opening, expand-ing, and closing bubbles.
we show that our tran-sition system is both sound and complete with re-spect to projective bubble trees (deﬁned in § 2.2).
experiments on the english penn treebank(ptb; marcus et al., 1993) extended with coordina-tion annotation (ficler and goldberg, 2016a) andthe english genia treebank (kim et al., 2003)demonstrate the effectiveness of our proposedtransition-based bubble parsing on the task of coor-dination structure prediction.
our method achievesstate-of-the-art performance on both datasets andimproves accuracy on the subset of sentences ex-hibiting complex coordination structures..2 dependency trees and bubble trees.
2.1 dependency-based representations for.
coordination structures.
a dependency tree encodes syntactic relations viadirected bilexical dependency edges.
these are nat-ural for representing argument and adjunct modiﬁ-cation, but popel et al.
(2013) point out that “depen-dency representation is at a loss when it comes torepresenting paratactic linguistic phenomena suchas coordination, whose nature is symmetric (twoor more conjuncts play the same role), as opposedto the head-modiﬁer asymmetry of dependencies”(pg.
517)..if one nonetheless persists in using dependencyrelations to annotate all syntactic structures, as iscommon practice in most dependency treebanks(hajiˇc et al., 2001; nivre et al., 2016, inter alia),then one must introduce special relations to repre-sent coordination structures and promote one ele-ment from each coordinated phrase to become the“representational head”.
one choice is to specifyone of the conjuncts as the “head” (mel’ˇcuk, 1988,2003; järvinen and tapanainen, 1998; lombardoand lesmo, 1998) (e.g., in figure 1, the visuallyasymmetric “conj” relation between “coffee” and“tea” is overloaded to admit a symmetric relation-ship), but it is then non-trivial to distinguish sharedmodiﬁers from private ones (e.g., in the ud tree.
at the bottom of figure 1, it is difﬁcult to tell that“hot” is private to “coffee” and “tea”, which share it,but “hot” does not modify “bun”).
another choiceis let one of the coordinators dominate the phrase(hajiˇc et al., 2001, 2020), but the coordinator doesnot directly capture the syntactic category of thecoordinated phrase.
decisions on which of thesedependency-based ﬁxes is more workable are fur-ther complicated by the interaction between rep-resentation styles and their learnability in statisti-cal parsing (nilsson et al., 2006; johansson andnugues, 2007; rehbein et al., 2017)..enhanced ud a tactic used by many recent re-leases of ud treebanks is to introduce certain extraedges and non-lexical nodes (schuster and man-ning, 2016; nivre et al., 2018; bouma et al., 2020).
while some of the theoretical issues still persist inthis approach with respect to capturing the sym-metric nature of relations between conjuncts, thissolution better represents shared modiﬁers in coor-dinations, and so is a promising direction.
in workconcurrent with our own, grünewald et al.
(2021)manually correct the coordination structure anno-tations in an english treebank under the enhancedud representation format.
we leave it to futurework to explore the feasibility of automatic conver-sion of coordination structure representations be-tween enhanced ud trees and bubble trees, whichwe discuss next..2.2 bubble trees.
an alternative solution to the coordination-in-dependency-trees dilemma is to permit certain re-stricted phrase-inspired constructs for such struc-tures.
indeed, tesnière’s (1959) seminal work ondependency grammar does not describe all syntac-tic relations in terms of dependencies, but ratherreserves a primitive relation for connecting coor-dinated items.
hudson (1984) further extends thisidea by introducing explicit markings of coordina-tion boundaries..in this paper, we revisit bubble trees, a represen-tational device along the same vein introduced bykahane (1997) for syntactic representation.
(ka-hane credits gladkij (1968) with a formal study.)
bubbles are used to denote coordinated phrases;otherwise, asymmetric dependency relations areretained.
conjuncts immediately within the bub-ble may co-head the bubble, and the bubble itselfmay establish dependencies with its governor andmodiﬁers.
figure 1 depicts an example bubble tree..7168we now formally deﬁne bubble trees and theirprojective subset, which will become the focus ofour transition-based parser in §3.
the following for-mal descriptions are adapted from kahane (1997),tailored to the presentation of our parser..• continuous projections: for any bubble α ∈ b, ifwi, wj ∈ ψ(α) and i < k < j, then wk ∈ ψ(α);• contained projections:for α1, α2 ∈ b,α1φ(α1) = ∅..if∗→ α2, then either ψ(α2) ⊂ φ(α1) or ψ(α2) ∩.
formal deﬁnition given a dependency-relationlabel set l, we deﬁne a bubble tree for a length-n sentence w = w1, .
.
.
, wn to be a quadruple(v, b, φ, a), where v = {rt, w1, .
.
.
, wn} is theground set of nodes (rt is the dummy root), b isa set of bubbles, the function φ : b (cid:55)→ (2v \{∅})gives the content of each bubble as a non-empty3subset of v , and a ⊂ b × l × b deﬁnes a labeleddirected tree over b. given labeled directed tree a,we say α1 → α2 if and only if (α1, l, α2) ∈ a forsome l. we denote the reﬂexive transitive closureof relation → by.
∗→..bubble tree (v, b, φ, a) is well-formed if and.
only if it satisﬁes the following conditions:4• no partial overlap: ∀α1, α2 ∈ b, either φ(α1) ∩φ(α2) = ∅ or φ(α1) ⊆ φ(α2) or φ(α2) ⊆ φ(α1);• non-duplication: there exists no non-identicalα1, α2 ∈ b such that φ(α1) = φ(α2);• lexical coverage: for any singleton (i.e., one-element) set s in 2v , ∃α ∈ b such that φ(α) = s;• roothood: the root rt appears in exactly one bub-ble, a singleton that is the root of the tree deﬁnedby a.
• containment: if ∃α1, α2 ∈ b such that φ(α2) ⊂φ(α1), then α1.
∗→ α2..projectivity our parser focuses on the subclassof projective well-formed bubble trees.
visually, aprojective bubble tree only contains bubbles cover-ing a consecutive sequence of words (such that wecan draw boxes around the span of words to repre-sent them) and can be drawn with all arcs arrangedspatially above the sentence where no two arcs orbubble boundaries cross each other.
the bubbletree in figure 1 is projective..formally, we deﬁne the projection ψ(α) ∈ 2vof a bubble α ∈ b to be all nodes the bubble andits subtree cover, that is, v ∈ ψ(α) if and only ifα ∗→ α(cid:48) and v ∈ φ(α(cid:48)) for some α(cid:48).
then, we candeﬁne a well-formed bubble tree to be projective ifand only if it additionally satisﬁes the following:• continuous coverage: for any bubble α ∈ b, ifwi, wj ∈ φ(α) and i < k < j, then wk ∈ φ(α);.
3our deﬁnition does not allow empty nodes; we leave it to.
future work to support them for gapping constructions..4we do not use β for bubbles because we reserve the β.symbol for our parser’s buffer..3 our transition system for parsing.
bubble trees.
although, as we have seen, bubble trees have theo-retical beneﬁts in representing coordination struc-tures that interface with an overall dependency-based analysis, there has been a lack of parser im-plementations capable of handling such representa-tions.
in this section, we ﬁll this gap by introducinga transition system that can incrementally build pro-jective bubble trees..transition-based approaches are popular in de-pendency parsing (nivre, 2008; kübler et al., 2009).
we propose to extend the arc-hybrid transitionsystem (kuhlmann et al., 2011) with transitionsspeciﬁc to bubble structures.5.
3.1 bubble-hybrid transition system.
a transition system consists of a data structure de-scribing the intermediate parser states, called con-ﬁgurations; speciﬁcations of the initial and termi-nal conﬁgurations; and an inventory of transitionsthat advance the parser in conﬁguration space to-wards reaching a terminal conﬁguration..our transition system uses a similar conﬁgura-tion data structure to that of arc-hybrid, whichconsists of a stack, a buffer, and the partially-committed syntactic analysis.
initially, the stackonly contains a singleton bubble corresponding to{rt}, and the buffer contains singleton bubbles,each representing a token in the sentence.
then,through taking transitions one at a time, the parsercan incrementally move items from the buffer tothe stack, or reduce items by attaching them toother bubbles or merging them into larger bubbles.
eventually, the parser should arrive at a terminalconﬁguration where the stack contains the single-ton bubble of {rt} again, but the buffer is emptyas all the tokens are now attached to or containedin other bubbles that are now descendants of the.
5our strategy can be adapted to other transition systemsas well; we focus on arc-hybrid here because of its com-paratively small inventory of transitions, absence of spuriousambiguities (there is a one-to-one mapping between a gold treeand a valid transition sequence), and abundance of existingimplementations (e.g., kiperwasser and goldberg, 2016)..7169transition.
(pre-conditions).
shift.
(|β| ≥ 1).
leftarclbl.
from.
stack σ.buffer β.to.
stack σ(cid:48).
buffer β(cid:48).
.
.
..b1 .
.
...
.
.
b1.
.
.
...
.
.
s1.
b1 .
.
...
.
..b1 .
.
..(|σ| ≥ 1; |β| ≥ 1; s1, b1 /∈ o; φ(s1) (cid:54)= {rt}).
rightarclbl.
(|σ| ≥ 2; s1, s2 /∈ o).
.
.
.
s2 s1.
.
.
..bubbleopenlbl.
(|σ| ≥ 2; s1, s2 /∈ o; φ(s2) (cid:54)= {rt}).
.
.
.
s2 s1.
.
.
..bubbleattachlbl.
(|σ| ≥ 2; s1 /∈ o; s2 ∈ o).
bubbleclose.
(|σ| ≥ 1; s1 ∈ o).
.
.
...
.
..conj.
.
.
s2a .
.
.
s1.
.
.
..conjlbl.
.
.
s2a .
.
.
s1.
.
.
...
.
.
conj.
.
.
s1a .
.
...
.
.
conjs1a .
.
...
.
...
.
...
.
.
s2.
lbl.
s1.
conj lbl.
.
.
s2 s1.
lbl.
s1.
.
...
.
...
.
..table 1: illustration of our bubble-hybrid transition system.
we give the pre-conditions for each transition andvisualizations of the affected stack and buffer items comparing the conﬁgurations before and after taking thattransition.
o denotes the set of currently open bubbles and rt is the dummy root symbol..(initial).
shift===⇒.
leftarcnsubj=======⇒.
shift===⇒.
shift===⇒.
shift===⇒.
shift===⇒.
bufferi prefer hot coffee or tea and a bun.
prefer hot coffee or tea and a bun.
prefer.
hot coffee or tea and a bun.
stackrt.
rt i.rt.
nsubjirt prefer.
hot coffee or tea and a bun.
rt prefer hot.
coffee or tea and a bun.
rt prefer hot coffee.
or tea and a bun.
rt prefer hot coffee or.
tea and a bun.
ccbubbleopencc========⇒ rt prefer hot coffee or.
conj.
tea and a bun.
shift===⇒.
shift===⇒.
rt prefer.
and a bun.
rt prefer.
and.
a bun.
bubbleopencc========⇒ rt prefer.
conj.
ccand.
a bun.
shift===⇒.
rt prefer.
conj.
ccand a.bun.
leftarcdet======⇒.
rt prefer.
conj.
ccand.
conj.
cc.
bun.
det.
a.shift===⇒.
rt prefer.
and bun.
bubbleattachconj==========⇒ rt prefer.
conj.
cc.
conjand bun.
∅.
∅.
shift===⇒.
rt prefer hot coffee or tea.
and a bun.
bubbleclose========⇒.
rt prefer.
conj.
cc.
cc conjbubbleattachconj==========⇒ rt prefer hot coffee or tea.
conj.
and a bun.
bubbleclose========⇒.
rt prefer hot.
and a bun.
leftarcamod=======⇒.
rt prefer.
and a bun.
amod.
hot.
shift===⇒.
rightarcobj=======⇒.
rt prefer.
rt prefer.
∅.
∅.
rightarcroot=======⇒.
rt.
(terminal).
dobj.
∅.
rootprefer.
figure 2: step-by-step visualization of the stack and buffer during parsing of the example sentence in figure 1.for steps following an attachment or bubbleclose transition, the detailed subtree or internal bubble structure isomitted for visual clarity.
for the same reason, we omit drawing the boundaries around singleton bubbles..7170{rt} singleton, and we can retrieve a completedbubble-tree parse..table 1 lists the available transitions in ourbubble-hybrid system.
the shift, leftarc, andrightarc transitions are as in the arc-hybrid sys-tem.
we introduce three new transitions to handlecoordination-related bubbles: bubbleopen putsthe ﬁrst two items on the stack into an open bubble,with the ﬁrst item in the bubble, i.e., previously thesecond topmost item on the stack, labeled as theﬁrst conjunct of the resulting bubble; bubbleat-tach absorbs the topmost item on the stack into theopen bubble that is at the second topmost position;and ﬁnally, bubbleclose closes the open bubbleat the top of the stack and moves it to the buffer,which then allows it to take modiﬁers from its leftthrough leftarc transitions.
figure 2 visualizesthe stack and buffer throughout the process of pars-ing the example sentence in figure 1. in particular,the last two steps in the left column of figure 2show the bubble corresponding to the phrase “cof-fee or tea” receiving its left modiﬁer “hot” througha leftarc transition after it is put back on thebuffer by a bubbleclose transition..formal deﬁnition our transition system is aquadruple (c, t, ci, cτ ), where c is the set of con-ﬁgurations to be deﬁned shortly, t is the set oftransitions with each element being a partial func-tion t ∈ t : c (cid:55)(cid:42) c, ci maps a sentence to its intialconﬁguration, and cτ ⊂ c is a set of terminal con-ﬁgurations.
each conﬁguration c ∈ c is a septuple(σ, β, v, b, φ, a, o), where v , b, φ, and a deﬁnea partially-recognized bubble tree, σ and β are eachan (ordered) list of items in b, and o ⊂ b is a setof open bubbles.
for a sentence w = w1, .
.
.
, wn,we let ci(w ) = (σ0, β0, v, b0, φ0, {}, {}), wherev = {rt, w1, .
.
.
, wn}, b0 contains n + 1 items,φ0(b0i ) = {wi} for i from 1 to n,σ0 = [b0n].
we write σ|s1and b1|β to denote a stack and a buffer with theirtopmost items being s1 and b1 and the remaindersbeing σ and β respectively.
we also omit the con-stant v in describing c when the context is clear..0) = {rt}, φ0(b0.
0], and β0 = [b0.
1, .
.
.
, b0.
for the transitions t , we have:• shift[(σ, b1|β, b, φ, a, o)] =.
(σ|b1, β, b, φ, a, o);.
• leftarclbl[(σ|s1, b1|β, b, φ, a, o)] =(σ, b1|β, b, φ, a ∪ {(b1, lbl, s1)}, o);• rightarclbl[(σ|s2|s1, β, b, φ, a, o)] =(σ|s2, β, b, φ, a ∪ {(s2, lbl, s1)}, o);.
• bubbleopenlbl[(σ|s2|s1, β, b, φ, a, o)] =.
(σ|α, β, b ∪ {α}, φ(cid:48), a ∪ {(α, conj, s2), (α, lbl,s1)}, o ∪ {α}), where α is a new bubble, andφ(cid:48) = φ (cid:100) {α (cid:55)→ ψ(s2) ∪ ψ(s1)} (i.e., φ(cid:48) is almostthe same as φ, but with α added to the function’sdomain, mapped by the new function to cover theprojections of both s2 and s1);• bubbleattachlbl[(σ|s2|s1, β, b, φ, a, o)] =(σ|s2, β, b, φ(cid:48), a∪{s2, lbl, s1}, o), where φ(cid:48) =.
φ (cid:100) {s2 (cid:55)→ φ(s2) ∪ ψ(s1)};• bubbleclose[(σ|s1, β, b, φ, a, o)] =.
(σ, s1|β, b, φ, a, o\{s1})..3.2 soundness and completeness.
in this section, we show that our bubble-hybridtransition system is both sound and complete (de-ﬁned below) with respect to the subclass of projec-tive bubble trees.6.
deﬁne a valid transition sequence π =t1, .
.
.
, tm for a given sentence w to be a sequencesuch that for the corresponding sequence of con-ﬁgurations c0, .
.
.
, cm, we have c0 = ci(w ), ci =ti(ci−1), and cm ∈ cτ , we can then state sound-ness and completeness properties, and present high-level proof sketches below, adapted from nivre’s(2008) proof frameworks..lemma 1.
(soundness) every valid transition se-quence π produces a projective bubble tree.
proof sketch.
we examine the requirements for aprojective bubble tree one by one.
the set of edgessatisﬁes the tree constraints since every bubble ex-cept for the singleton bubble of rt must have anin-degree of one to have been reduced from thestack, and the topological order of reductions im-plies acyclicness.
lexical coverage is guaranteedby ci.
roothood is safeguarded by the transitionpre-conditions.
non-duplication is ensured becausenewly-created bubbles are strictly larger.
all theother properties can be proved by induction overthe lengths of transition sequence preﬁxes sinceeach of our transitions preserves zero partial over-lap, containment, and projectivity constraints..lemma 2.
(completeness) for every projectivebubble tree over any given sentence w , there existsa corresponding valid transition sequence π.proof sketch.
the proof proceeds by strong in-duction on sentence length.
we omit relation la-bels without loss of generality.
the base case of|w | = 1 is trivial.
for the inductive step, weenumerate how to decompose the tree’s top-level.
6more precisely, our transition system handles the subset.
where each non-singleton bubble has ≥ 2 internal children..7171structure.
(1) when the root has multiple children:due to projectivity, each child bubble tree τi coversa consecutive span of words wxi, .
.
.
, wyi that areshorter than |w |.
based on the induction hypoth-esis, there exisits a valid transition sequence πi toconstruct the child tree over rt, wxi, .
.
.
, wyi.
herewe let πi to denote the transition sequence exclud-ing the always-present ﬁnal rightarc transitionthat attaches the subtree to rt; this is for explicitillustration of what transitions to take once the sub-trees are constructed.
the full tree can be con-structed by π = π1, rightarc, π2, rightarc,.
.
.
(expanding each πi sequence into its componenttransitions), where we simply attach each subtreeto rt immediately after it is constructed.
(2) whenthe root has a single child bubble α, we cannotdirectly use the induction hypothesis since α cov-ers the same number of words as w .
thus weneed to further enumerate the top-level structureof α.
(2a) if α has children with their projectionsoutside of φ(α), then we can ﬁnd a sequence π0for constructing the shorter-length bubble α andplacing it on the buffer (this corresponds to anempty transition sequence if α is a singleton; oth-erwise, π0 ends with a bubbleclose transition)and πis for α’s outside children; say it has l chil-dren left of its contents.
we construct the entiretree via π = π1,.
.
.
,πl, π0, leftarc, .
.
.
, left-arc, shift, πl+1, rightarc, .
.
.
, rightarc,where we ﬁrst construct all the left outside chil-dren and leave them on the stack, next build thebubble α and use leftarc transitions to attachits left children while it is on the buffer, then shiftα to the stack before ﬁnally continuing on build-ing its right children subtrees, each immediatelyfollowed by a rightarc transition.
(2b) if α is anon-singleton bubble without any outside children,but each of its inside children can be parsed throughπi based on the inductive hypothesis, then we candeﬁne π = π1,π2, bubbleopen, π3, bubbleat-tach, .
.
.
, bubbleclose, shift, rightarc,where we use a bubbleopen transition once theﬁrst two bubble-internal children are built, eachsubsequent child is attached via bubbleattachimmediately after construction, and the ﬁnal threetransitions ensure proper closing of the bubble andits attachment to rt..4 models.
our model architecture largely follows that ofkiperwasser and goldberg’s (2016) neural arc-.
hybrid parser, but we additionally introduce fea-ture composition for non-singleton bubbles, and arescoring module to reduce frequent coordination-boundary prediction errors.
our model has ﬁvefeature extraction, bubble-featurecomponents:composition, transition scoring, label scoring, andboundary subtree rescoring..feature extraction we ﬁrst extract contextual-ized features for each token using a bidirectionallstm (graves and schmidhuber, 2005):.
[w0, w1, .
.
.
, wn] = bi-lstm([rt, w1, .
.
.
, wn]),.
where the inputs to the bi-lstm are concatena-tions of word embeddings, pos-tag embeddings,and character-level lstm embeddings.
we alsoreport experiments replacing the bi-lstm withpre-trained bert features (devlin et al., 2019)..bubble-feature composition we initialize thefeatures7 for each singleton bubble bi in the initialconﬁguration to be vbi = wi.
for a non-singletonbubble α, we use recursively composed features.
vα = g({vα(cid:48)|(α, conj, α(cid:48)) ∈ a}),.
where g is a composition function combining fea-tures from the co-heads (conjuncts) immediatelyinside the bubble.8 for our model, for any v (cid:48) ={vi1, .
.
.
, vin }, we set.
g(v (cid:48)) = tanh(wg · mean(v (cid:48))),.
where mean() computes element-wise averagesand wg is a learnable square matrix.
we also ex-periment with a parameter-free version: g = mean.
neither of the feature functions distinguishes be-tween open and closed bubbles, so we appendto each v vector an indicator-feature embeddingbased on whether the bubble is open, closed, orsingleton..transition scoring given the current parser con-ﬁguration c, the model predicts the best unla-beled transition to take among all valid transitionsvalid(c) whose pre-conditions are satisﬁed.
we.
7we adopt the convenient abuse of notation of allowing.
indexing by arbitrary objects..8comparing with the subtree-feature composition func-tions in dependency parsing that are motivated by asymmetricheaded constructions (dyer et al., 2015; de lhoneux et al.,2019; basirat and nivre, 2021), our deﬁnition focuses oncomposing features from an unordered set of vectors repre-senting the conjuncts in a bubble.
the composition functionis recursively applied when there are nested bubbles..7172model the log-linear probability of taking an actionwith a multi-layer perceptron (mlp):.
t.p (t|c) ∝ exp(mlptrans.
([vs3 ◦ vs2 ◦ vs1 ◦ vb1])),where ◦ denotes vector concatenation, s1 throughs3 are the ﬁrst through third topmost items on thestack, and b1 is the immediately accessible bufferitem.
we experiment with varying the number ofstack items to extract features from..label scoring we separate edge-label predic-tion from (unlabeled) transition prediction, but thescoring function takes a similar form:.
p (l|c, t) ∝ exp(mlplbl.
l ([vh(c,t) ◦ vd(c,t)])),where (h(c, t), l, d(c, t)) is the edge to be addedinto the partial bubble tree in t(c)..boundary subtree rescoring in our prelimi-nary error analysis, we ﬁnd that our models tendto make more mistakes at the boundaries of fullcoordination phrases than at the internal conjunctboundaries, due to incorrect attachments of chil-dren choosing between the phrasal bubble and theﬁrst/last conjunct.
for example, our initial modelpredicts “if you owned it and liked it friday” in-stead of the annotated “if you owned it and liked itfriday” (the predicted and gold conjuncts are bothitalicized and underlined), incorrectly attaching“friday” to “liked”.
we attribute this problem to thegreedy nature of our ﬁrst formulation of the parser,and propose to mitigate the issue through rescoring.
to rescore boundary attachments of a non-singletonbubble α, for each of the left dependents d of α andits ﬁrst conjunct αf , we (re)-decide the attachmentviap (α → d|αf ) = logistic(mlpre([vd◦vα◦vαf ])),and similarly for the last conjunct αl and a potentialright dependent..training and inference our parser is a locally-trained greedy parser.
in training, we optimize themodel parameters to maximize the log-likelihoodsof predicting the target transitions and labels alongthe paths generating the gold bubble trees, andthe log-likelihoods of the correct attachments inrescoring;9 during inference, the parser greedilycommits to the highest-scoring transition and labelfor each of its current parser conﬁgurations, andafter reaching a terminal conﬁguration, it rescoresand readjusts all boundary subtree attachments..9we leave the deﬁnition of dynamic oracles (goldberg and.
nivre, 2013) for bubble tree parsing to future work..exact.
inner.
all.
np.
all.
np.
fg16tsm17tsm19ours+bert.
–71.0875.4776.4883.74.
–75.0177.8381.6385.26.
72.7073.7477.7478.3084.46.
76.1077.2580.0684.0386.22.table 2: f1 scores on the ptb test set.
see appendix cfor precision, recall and dev set results..exact whole.
hsom09fg16tsm17tsm19ours+bert.
––55.2261.2267.0979.18.
61.564.1466.3161.3168.2380.41.table 3: recall results on the genia dataset (we reportrecall instead of f1 scores following prior work).
seeappendix c for detailed results per constituent type..5 empirical results.
task and evaluation we validate the utility ofour transition-based parser using the task of coor-dination structure prediction.
given an input sen-tence, the task is to identify all coordination struc-tures and the spans for all their conjuncts withinthat sentence.
we mainly evaluate based on exactmetrics which count a prediction of a coordinationstructure as correct if and only if all of its conjunctspans are correct.
to facilitate comparison withpre-existing systems that do not attempt to identifyall conjunct boundaries, following teranishi et al.
(2017, 2019), we also consider inner (=only con-sider the correctness of the two conjuncts adjacentto the coordinator) and whole (=only consider theboundary of the whole coordinated phrase) metrics..data and experimental setup we experimentwith two english datasets, the penn treebank (ptb;marcus et al., 1993, newswire) with added coor-dination annotations (ficler and goldberg, 2016a)and the genia treebank (kim et al., 2003, re-search abstracts).
we use the conversion tool dis-tributed with the stanford parser (schuster andmanning, 2016) to extract ud trees from the ptb-style phrase-structure annotations, which we thenmerge with coordination annotations to form bub-.
7173bubble-hybrid (ours)prec..rec..edge-factoredrec.
prec..92.5697.4694.1298.8597.6993.0494.52.
.
.
92.68.punctcasecompounddetnsubjnmodamod.
.
.
conj.
uaslas.
92.5298.1495.1899.1397.7293.4593.43.
.
.
93.20.
95.8194.46.
92.9297.7194.2498.7098.0193.2094.61.
.
.
92.52.
92.8598.2695.0299.0697.9293.6293.95.
.
.
93.04.
95.9994.56.table 4: ptb test-set results, comparing our transition-based bubble parser and an edge-factored graph-basedparser, both using a bert-based feature encoder.
therelation labels are ordered by decreasing frequency.
while our transition-based bubble parser slightly under-performs the graph-based dependency parser generally,perhaps due to the disadvantage of greedy decoding, itgives slightly better precision and recall on the “conj”relation type..rescoring.
+.
−.
ours (bi-lstm)• g = mean• {vs2, vs1, vb1}• {vs1, vb1}• {vb1}• +bert.
77.1075.5176.0576.3350.2784.40.
76.2774.1674.8773.8535.1483.70.table 5: exact f1 scores of different model variationson the ptb dev set, w/ and w/o the rescoring module..ble trees.
we follow prior work in reporting ptbresults on its standard splits and genia resultsusing 5-fold cross-validation.10 during training(but not test), we discard all non-projective sen-tences.
see appendix a for dataset pre-processingand statistics and appendix b for implementationdetails..baseline systems we compare our models withseveral baseline systems.
hara et al.
(2009,hsom09) use edit graphs to explicitly align co-ordinated conjuncts based on the idea that they areusually similar; ficler and goldberg (2016b, fg16)score candidate coordinations extracted from aphrase-structure parser by modeling their symme-.
10we afﬁrm that, as is best practice, only two test-set/crossval-suite runs occurred (one with bert and one with-out), happening after we ﬁxed everything else; that is, no othermodels were tried after seeing the ﬁrst test-set/cross-validationresults with and without bert..complexity.
all.
simple complex.
tsm17tsm19ours+bert.
66.0970.9072.9780.07.
72.9078.1679.9783.74.
50.3754.1656.8271.59.table 6: per-sentence exact match on the ptb test set.
simple includes sentences with only one two-conjunctcoordination, and complex contains the other cases..try and replaceability properties; teranishi et al.
(2017, tsm17) directly predict boundaries of co-ordinated phrases and then split them into con-juncts;11 teranishi et al.
(2019, tsm19) use sep-arate neural models to score the inner and outerboundaries of conjuncts relative to the coordina-tors, and then use a chart parser to ﬁnd the globally-optimal coordination structures..main results table 2 and table 3 show themain evaluation results on the ptb and geniadatasets.
our models surpass all prior results onboth datasets.
while the bert improvements maynot seem surprising, we note that teranishi et al.
(2019) report that their pre-trained language mod-els — speciﬁcally, static elmo embeddings — failto improve their model performance..general parsing results we also evaluate ourmodels on standard parsing metrics by convert-ing the predicted bubble trees to ud-style depen-dency trees.
on ptb, our parsers reach unla-beled and labeled attachment scores (uas/las)of 95.81/94.46 with bert and 94.49/92.88 with bi-lstm, which are similar to the scores of priortransition-based parsers equipped with similar fea-ture extractors (kiperwasser and goldberg, 2016;mohammadshahi and henderson, 2020).12 table 4compares the general parsing results of our bub-ble parser and an edge-factored graph-based de-pendency parser based on dozat and manning’s(2017) parser architecture and the same feature en-coder as our parser and trained on the same data.
our bubble parser shows a slight improvement onidentifying the “conj” relations, despite having alower overall accuracy due to the greedy natureof our transition-based decoder.
additionally, our.
11we report results for the extended model of tsm17 as.
described by teranishi et al.
(2019)..12results are not strictly comparable with previous ptbevaluations that mostly focus on non-ud dependency conver-sions.
table 4 makes a self-contained comparison using thesame ud-based and coordination-merged data conversions..7174bubble parser simultaneously predicts the bound-aries of each coordinated phrase and conjuct, whilea typical dependency parser cannot produce suchstructures..functional grammar (kaplan and maxwell iii,1988), tree-adjoining grammar (sarkar and joshi,1996; han and sarkar, 2017), and combinatorycategorial grammar (steedman, 1996, 2000)..model analysis table 5 shows results of ourmodels with alternative bubble-feature compositionfunctions and varying feature-set sizes.
we ﬁndthat the parameterized form of composition func-tion g performs better, and the f1 scores mostlydegrade as we use fewer features from the stack.
in-terestingly, the importance of our rescoring modulebecomes more prominent when we use fewer fea-tures.
our results resonate with shi et al.’s (2017)ﬁndings on arc-hybrid that we need at least onestack item but not necessarily two.
table 6 showsthat our model performs better than previous meth-ods on complex sentences with multiple coordi-nation structures and/or more than two conjuncts,especially when we use bert as feature extractor..6 related work.
coordination structure prediction very earlywork with heuristic, non-learning-based ap-proaches (agarwal and boggess, 1992; kurohashiand nagao, 1994) typically report difﬁculties indistinguishing shared modiﬁers from private ones,although such heuristics have been recently incor-porated in unsupervised work (sawada et al., 2020).
generally, researchers have focused on symmetryprinciples, seeking to align conjuncts (kurohashiand nagao, 1994; shimbo and hara, 2007; haraet al., 2009; hanamoto et al., 2012), since coordi-nated conjuncts tend to be semantically and syn-tactically similar (hogan, 2007), as attested to bypsycholinguistic evidence of structural parallelism(frazier et al., 1984, 2000; dubey et al., 2005).
ficler and goldberg (2016a) and teranishi et al.
(2017) additionally leverage the linguistic principleof replaceability — one can typically replace a co-ordinated phrase with one of its conjuncts withoutthe sentence becoming incoherent; this idea hasresulted in improved open information extraction(saha and mausam, 2018).
using these principlesmay further improve our parser..coordinationin constituency grammarwhile our paper mainly focuses on enhancingdependency-based syntactic analysis with coordi-nation structures, coordination is a well-studiedtopic in constituency-based syntax (zhang, 2009),including proposals and treatments under lexical.
tesnière dependency structure sangati andmazza (2009) propose a representation that is faith-ful to tesnière’s (1959) original framework.
simi-lar to bubble trees, their structures include specialattention to coordination structures respecting con-junct symmetry, but they also include constructsto handle other syntactic notions currently beyondour parser’s scope.13 such representations havebeen used for re-ranking (sangati, 2010), but notfor (direct) parsing.
perhaps our work can inspirea future tesnière dependency structure parser..non-constituent coordination seemingly in-complete (non-constituent) conjuncts are particu-larly challenging (milward, 1994), and our bubbleparser currently has no special mechanism for them.
dependency-based analyses have adapted by ex-tending to a graph structure (gerdes and kahane,2015) or explicitly representing elided elements(schuster et al., 2017).
it may be straightforwardto integrate the latter into our parser, à la kahane’s(1997) proposal of phonologically-empty bubbles..7 conclusion.
we revisit kahane’s (1997) bubble tree representa-tions for explicitly encoding coordination bound-aries as a viable alternative to existing mechanismsin dependency-based analysis of coordination struc-tures.
we introduce a transition system that is bothsound and complete with respect to the subclassof projective bubble trees.
empirically, our bub-ble parsers achieve state-of-the-art results on thetask of coordination structure prediction on twoenglish datasets.
future work may extend the re-search scope to other languages, graph-based, andnon-projective parsing methods..acknowledgements we thank the anonymousreviewers for their constructive comments, yueguo for discussion, and hiroki teranishi for helpwith experiment setup.
this work was supportedin part by a bloomberg data science ph.d. fellow-ship to tianze shi and a gift from bloomberg tolillian lee..13for example, differentiating content and function wordswhich has recently been explored by basirat and nivre (2021)..7175references.
rajeev agarwal and lois boggess.
1992. a simplebut useful approach to conjunct identiﬁcation.
inproceedings of the 30th annual meeting of the asso-ciation for computational linguistics, pages 15–21,newark, delaware, usa.
association for computa-tional linguistics..ali basirat and joakim nivre.
2021. syntactic nuclei independency parsing – a multilingual exploration.
inproceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1376–1387, online.
association for computational linguistics..gosse bouma, djamé seddah, and daniel zeman.
2020. overview of the iwpt 2020 shared task onparsing into enhanced universal dependencies.
inproceedings of the 16th international conferenceon parsing technologies and the iwpt 2020 sharedtask on parsing into enhanced universal dependen-cies, pages 151–161, online.
association for com-putational linguistics..michael collins.
2003. head-driven statistical modelsfor natural language parsing.
computational lin-guistics, 29(4):589–637..miryam de lhoneux, miguel ballesteros, and joakimnivre.
2019. recursive subtree composition inin proceedingslstm-based dependency parsing.
of the 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 1566–1576, minneapo-lis, minnesota, usa.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186, minneapolis, minnesota, usa.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in proceedings of the 5th international confer-ence on learning representations, toulon, france.
openreview.net..amit dubey, patrick sturt, and frank keller.
2005. par-allelism in coordination as an instance of syntacticpriming: evidence from corpus-based modeling.
inproceedings of human language technology con-ference and conference on empirical methods innatural language processing, pages 827–834, van-couver, british columbia, canada.
association forcomputational linguistics..chris dyer, miguel ballesteros, wang ling, austinmatthews, and noah a. smith.
2015. transition-based dependency parsing with stack long short-in proceedings of the 53rd annualterm memory.
meeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing (volume 1: longpapers), pages 334–343, beijing, china.
associa-tion for computational linguistics..jessica ficler and yoav goldberg.
2016a.
coordina-tion annotation extension in the penn tree bank.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 834–842, berlin, germany.
as-sociation for computational linguistics..jessica ficler and yoav goldberg.
2016b.
a neural net-work for coordination boundary prediction.
in pro-ceedings of the 2016 conference on empirical meth-ods in natural language processing, pages 23–32,austin, texas, usa.
association for computationallinguistics..jessica ficler and yoav goldberg.
2017..improvinga strong neural parser with conjunction-speciﬁc fea-tures.
in proceedings of the 15th conference of theeuropean chapter of the association for computa-tional linguistics: volume 2, short papers, pages343–348, valencia, spain.
association for computa-tional linguistics..lyn frazier, alan munn, and charles clifton.
2000.journal of psy-.
processing coordinate structures.
cholinguistic research, 29(4):343–370..lyn frazier, lori taft, tom roeper, charles clifton,and kate ehrlich.
1984. parallel structure: a sourceof facilitation in sentence comprehension.
memory& cognition, 12(5):421–430..kim gerdes and sylvain kahane.
2015..non-constituent coordination and other coordinative con-structions as dependency graphs.
in proceedings ofthe third international conference on dependencylinguistics (depling 2015), pages 101–110, upp-sala, sweden.
uppsala university..aleksej v. gladkij.
1968.
"on describing the syntac-tic structure of a sentence" (in russian with englishsummary).
computational linguistics, 7:21–44..yoav goldberg and michael elhadad.
2010..inspect-ing the structural biases of dependency parsing al-gorithms.
in proceedings of the fourteenth confer-ence on computational natural language learning,pages 234–242, uppsala, sweden.
association forcomputational linguistics..yoav goldberg and joakim nivre.
2013. training de-terministic parsers with non-deterministic oracles.
transactions of the association for computationallinguistics, 1:403–414..7176alex graves and jürgen schmidhuber.
2005. frame-wise phoneme classiﬁcation with bidirectionallstm and other neural network architectures.
neu-ral networks, 18(5):602–610..stefan grünewald, prisca piccirilli, and annemariefriedrich.
2021. coordinate constructions in en-glish enhanced universal dependencies: analysisand computational modeling.
in proceedings of the16th conference of the european chapter of the as-sociation for computational linguistics: main vol-ume, pages 795–809, online.
association for com-putational linguistics..jan hajiˇc, eduard bejˇcek, jaroslava hlavacova, mariemikulová, milan straka, jan štˇepánek, and barboraštˇepánková.
2020. prague dependency treebank -in proceedings of the 12th lan-consolidated 1.0.guage resources and evaluation conference, pages5208–5218, marseille, france.
european languageresources association..jan hajiˇc, barbora vidová hladká, jarmila panevová,eva hajiˇcová, petr sgall, and petr pajas.
2001.prague dependency treebank 1.0 (ldc2001t10)..chung-hye han and anoop sarkar.
2017. coordina-tion in tag without the conjoin operation.
inproceedings of the 13th international workshop ontree adjoining grammars and related formalisms,pages 43–52, umeå, sweden.
association for com-putational linguistics..atsushi hanamoto, takuya matsuzaki, and jun’ichitsujii.
2012. coordination structure analysis usingdual decomposition.
in proceedings of the 13th con-ference of the european chapter of the associationfor computational linguistics, pages 430–438, avi-gnon, france.
association for computational lin-guistics..kazuo hara, masashi shimbo, hideharu okuma, andyuji matsumoto.
2009. coordinate structure analy-sis with global structural constraints and alignment-in proceedings of the jointbased local features.
conference of the 47th annual meeting of the acland the 4th international joint conference on natu-ral language processing of the afnlp, pages 967–975, suntec, singapore.
association for computa-tional linguistics..deirdre hogan.
2007. coordinate noun phrase disam-biguation in a generative parsing model.
in proceed-ings of the 45th annual meeting of the association ofcomputational linguistics, pages 680–687, prague,czech republic.
association for computational lin-guistics..richard a. hudson.
1984. word grammar.
blackwell,.
oxford..timo järvinen and pasi tapanainen.
1998. towards anin process-implementable dependency grammar.
ing of dependency-based grammars, pages 1–10,montreal, quebec, canada.
association for compu-tational linguistics..richard johansson and pierre nugues.
2007..ex-tended constituent-to-dependency conversion for en-glish.
in proceedings of the 16th nordic conferenceof computational linguistics (nodalida 2007),pages 105–112, tartu, estonia.
university of tartu,estonia..sylvain kahane.
1997. bubble trees and syntactic rep-in proceedings of the 5th meetingresentations.
of mathematics of language, pages 70–76, saar-brücken, germany..ronald m. kaplan and john t. maxwell iii.
1988.constituent coordination in lexical-functional gram-in proceedings of international conferencemar.
on computational linguistics, pages 303–305, bu-dapest, hungary..jin dong kim, tomoko ohta, yuka tateisi, andjun’ichi tsujii.
2003. genia corpus—a semanti-cally annotated corpus for bio-textmining.
bioinfor-matics, 19(suppl_1):i180–i182..eliyahu kiperwasser and yoav goldberg.
2016. sim-ple and accurate dependency parsing using bidirec-tional lstm feature representations.
transactionsof the association for computational linguistics,4:313–327..sandra kübler, ryan mcdonald, and joakim nivre.
2009. dependency parsing.
synthesis lectureson human language technologies.
morgan & clay-pool publishers..marco kuhlmann, carlos gómez-rodríguez, and gior-gio satta.
2011. dynamic programming algorithmsin pro-for transition-based dependency parsers.
ceedings of the 49th annual meeting of the associ-ation for computational linguistics: human lan-guage technologies, pages 673–682, portland, ore-gon, usa.
association for computational linguis-tics..sadao kurohashi and makoto nagao.
1994. a syn-tactic analysis method of long japanese sentencesbased on the detection of conjunctive structures.
computational linguistics, 20(4):507–534..vincenzo lombardo and leonardo lesmo.
1998. unitcoordination and gapping in dependency theory.
inprocessing of dependency-based grammars, pages11–20, montreal, quebec, canada.
association forcomputational linguistics..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
computa-tional linguistics, 19(2):313–330..igor mel’ˇcuk.
1988. dependency syntax: theory andpractice.
state university press of new york, al-bany..igor mel’ˇcuk.
2003. levels of dependency in linguis-tic description: concepts and problems.
in vilmoságel, ludwig m. eichinger, hans werner eroms,.
7177peter hellwig, hans jürgen heringer, and henninglobin, editors, dependency and valency: an inter-national handbook of contemporary research, vol-ume 1, pages 188–229.
walter de gruyter, berlin..david milward.
1994. non-constituent coordination:theory and practice.
in proceedings of the 15th in-ternational conference on computational linguis-tics, volume 2, pages 935–941, kyoto, japan..alireza mohammadshahi and james henderson.
2020.graph-to-graph transformer for transition-based de-in findings of the associationpendency parsing.
for computational linguistics: emnlp 2020, pages3278–3289, online.
association for computationallinguistics..jens nilsson, joakim nivre, and johan hall.
2006.graph transformations in data-driven dependencyin proceedings of the 21st internationalparsing.
conference on computational linguistics and 44thannual meeting of the association for computa-tional linguistics, pages 257–264, sydney, aus-tralia.
association for computational linguistics..joakim nivre.
2008. algorithms for deterministic in-cremental dependency parsing.
computational lin-guistics, 34(4):513–553..joakim nivre, marie-catherine de marneffe, filip gin-ter, yoav goldberg, jan hajiˇc, christopher d. man-ning, ryan mcdonald, slav petrov, sampo pyysalo,natalia silveira, reut tsarfaty, and daniel zeman.
2016. universal dependencies v1: a multilingualtreebank collection.
in proceedings of the tenth in-ternational conference on language resources andevaluation (lrec’16), pages 1659–1666, portorož,slovenia.
european language resources associa-tion..joakim nivre, marie-catherine de marneffe, filip gin-ter, jan hajiˇc, christopher d. manning, sampopyysalo, sebastian schuster, francis tyers, anddaniel zeman.
2020. universal dependencies v2:an evergrowing multilingualtreebank collection.
in proceedings of the 12th language resourcesand evaluation conference, pages 4034–4043, mar-seille, france.
european language resources asso-ciation..joakim nivre, paola marongiu, filip ginter, jennakanerva, simonetta montemagni, sebastian schus-ter, and maria simi.
2018. enhancing universalin proceed-dependency treebanks: a case study.
ings of the second workshop on universal depen-dencies (udw 2018), pages 102–107, brussels, bel-gium.
association for computational linguistics..martin popel, david mareˇcek, jan štˇepánek, danielzeman, and zdˇenˇek žabokrtský.
2013. coordina-tion structures in dependency treebanks.
in proceed-ings of the 51st annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 517–527, soﬁa, bulgaria.
associationfor computational linguistics..owen rambow.
2010. the simple truth about de-pendency and phrase structure representations: anin human language technologies:opinion piece.
the 2010 annual conference of the north americanchapter of the association for computational lin-guistics, pages 337–340, los angeles, california,usa.
association for computational linguistics..ines rehbein, julius steen, bich-ngoc do, and anettefrank.
2017. universal dependencies are hard toparse — or are they?
in proceedings of the fourthinternational conference on dependency linguis-tics (depling 2017), pages 218–228, pisa, italy.
linköping university electronic press..swarnadeep saha and mausam.
2018. open informa-tion extraction from conjunctive sentences.
in pro-ceedings of the 27th international conference oncomputational linguistics, pages 2288–2299, santafe, new mexico, usa.
association for computa-tional linguistics..swarnadeep saha, harinder pal, and mausam.
2017.bootstrapping for numerical open ie.
in proceed-ings of the 55th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 317–323, vancouver, canada.
associa-tion for computational linguistics..federico sangati.
2010. a probabilistic generativemodel for an intermediate constituency-dependencyin proceedings of the acl 2010representation.
student research workshop, pages 19–24, uppsala,sweden.
association for computational linguistics..federico sangati and chiara mazza.
2009. an englishdependency treebank à la tesnière.
in proceedingsof the 8th international workshop on treebanks andlinguistic theories, pages 173–184, milan, italy..anoop sarkar and aravind joshi.
1996. coordina-tion in tree adjoining grammars: formalization andin proceedings of the 16th inter-implementation.
national conference on computational linguistics,pages 610–615, copenhagen, denmark..yuya sawada, takashi wada, takayoshi shibahara,hiroki teranishi, shuhei kondo, hiroyuki shindo,taro watanabe, and yuji matsumoto.
2020. coordi-nation boundary identiﬁcation without labeled datain proceed-for compound terms disambiguation.
ings of the 28th international conference on com-putational linguistics, pages 3043–3049, barcelona,spain (online).
international committee on compu-tational linguistics..sebastian schuster, matthew lamm, and christo-pher d. manning.
2017. gapping constructions inuniversal dependencies v2.
in proceedings of thenodalida 2017 workshop on universal dependen-cies (udw 2017), pages 123–132, gothenburg, swe-den.
association for computational linguistics..sebastian schuster and christopher d. manning.
2016.enhanced english universal dependencies: an im-.
7178proved representation for natural language under-standing tasks.
in proceedings of the tenth interna-tional conference on language resources and eval-uation (lrec 2016), pages 2371–2378, portorož,slovenia.
european language resources associa-tion..tianze shi, liang huang, and lillian lee.
2017.fast(er) exact decoding and globaltraining fortransition-based dependency parsing via a minimalfeature set.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 12–23, copenhagen, denmark.
associa-tion for computational linguistics..masashi shimbo and kazuo hara.
2007. a discrim-inative learning model for coordinate conjunctions.
in proceedings of the 2007 joint conference onempirical methods in natural language process-ing and computational natural language learning(emnlp-conll), pages 610–619, prague, czechrepublic.
association for computational linguis-tics..mark steedman.
1996. surface structure and interpre-tation.
number 30 in linguistic inquiry monograph.
the mit press, cambridge, massachusetts..mark steedman.
2000. the syntactic process.
mit.
press, cambridge, ma, usa..hiroki teranishi, hiroyuki shindo, and yuji mat-sumoto.
2017. coordination boundary identiﬁcationwith similarity and replaceability.
in proceedings ofthe eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 264–272, taipei, taiwan.
asian federation ofnatural language processing..hiroki teranishi, hiroyuki shindo, and yuji mat-sumoto.
2019. decomposed local models for coor-dinate structure parsing.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3394–3403, minneapolis, minnesota,usa.
association for computational linguistics..lucien tesnière.
1959. eléments de syntaxe struc-.
turale.
librairie c. klincksieck, paris..niina ning zhang.
2009. coordination in syntax.
cambridge studies in linguistics.
cambridge uni-versity press, new york..7179appendix a dataset processing andstatistics.
we follow teranishi et al.
(2019) and use the samedataset splits and pre-processing steps.
for thepenn treebank (ptb; marcus et al., 1993) datawith added coordination annotations (ficler andgoldberg, 2016a), we use wsj sections 02-21for training, section 22 for development, and sec-tion 23 for test sets respectively.
we also useteranishi et al.’s (2019) pre-processing steps instripping quotation marks surrounding ptb coor-dinated phrases to normalize irregular coordina-tions.
this results in 39,832/1,700/2,416 sentencesand 19,890/848/1,099 coordination structures intrain/dev/test splits respectively.
for the geniatreebank (kim et al., 2003), we use the beta ver-sion of the corpus and follow the same 5-fold cross-validation splits as teranishi et al.
(2019).
in total,genia contains 2,508 sentences and 3,598 coor-dination structures..to derive bubble tree representations, we ﬁrstconvert the ptb-style phrase-structure trees in bothtreebanks with the conversion tool (schuster andmanning, 2016) provided by the stanford corenlptoolkit version 4.2.0 into universal dependencies(ud; nivre et al., 2016) style.
we then mergethe ud trees with the bubbles formed by the co-ordination boundaries.
we deﬁne the boundariesto be from the beginning of the ﬁrst conjunct tothe end of the last conjunct for each coordinatedphrase.
we attach all conjuncts to their corre-sponding bubbles with a “conj” label, and map any“conj”-labeled dependencies outside an annotatedcoordination to “dep”.
we resolve modiﬁer scopeambiguities according to conjunct annotations: ifthe modiﬁer is within the span of a conjunct, thenit is a private modiﬁer; otherwise, it is a sharedmodiﬁer to the entire coordinated phrase and weattach it to the phrasal bubble.
since our transitionsystem targets projective bubble trees, we ﬁlter outany non-projective trees during training (but stillevaluate on them during testing).
we retain 39,678sentences, or 99.6% of the ptb training set, and2,429 sentences, or 96.9% of the genia dataset..appendix b implementation details.
implementation.
ourtzshi/bubble-parser-acl21) is based on pytorch..(https://www.github.com/.
we train our models by using the adam opti-mizer (kingma and ba, 2015).
after a ﬁxed num-ber of optimization steps (3,200 steps for ptb and.
adam optimizer:initial learning rate for bi-lstminitial learning rate for bertβ1β2(cid:15)minibatch sizelinear warmup stepsgradient clipping l2 norm.
inputs to bi-lstm:word-embedding dimensionalitypos tag-embedding dimensionalitycharacter bi-lstm layerscharacter bi-lstm dimensionality.
bi-lstm:number of layersdimensionalitydropout.
mlps (same for all mlps):number of hidden layershidden layer dimensionalityactivation functiondropout.
10−310−50.90.99910−888005.0.
100321128.
38000.3.
1400relu0.3.table a1: hyperparameters of our models..800 steps for genia, based on their training setsizes), we perform an evaluation on the dev set.
if the dev set performance fails to improve within5 consecutive evaluation rounds, we multiply thelearning rate by 0.1. we terminate model trainingwhen the learning rate has dropped three times,and select the best model checkpoint based on devset f1 scores according to the “exact” metrics.14for the bert feature extractor, we ﬁnetune thepretrained case-sensitive bertbase model throughthe transformers package.15 for the non-bertmodel, we use pre-trained glove embeddings (pen-nington et al., 2014)..following prior practice, we embed gold postags as input features when using bi-lstm for themodels trained on the genia dataset, but we omitthe pos tag embeddings for the ptb dataset.
the training process for each model.
takesroughly 10 hours using an rtx 2080 ti gpu;model inference speed is 41.9 sentences per sec-ond.16.
we select our hyperparameters by hand.
dueto computational constraints, our hand-tuning hasbeen limited to setting the dropout rates, and fromthe candidates set of {0.0, 0.1, 0.3, 0.5} we chose.
14even though we report recall on genia, model selection.
is still performed using f1..15github.com/huggingface/transformers16we have not yet done extensive optimization regarding.
gpu batching for greedy transition-based parsers..7180on empirical methods in natural language process-ing (emnlp), pages 1532–1543, doha, qatar.
asso-ciation for computational linguistics..sebastian schuster and christopher d. manning.
2016.enhanced english universal dependencies: an im-proved representation for natural language under-standing tasks.
in proceedings of the tenth interna-tional conference on language resources and eval-uation (lrec 2016), pages 2371–2378, portorož,slovenia.
european language resources associa-tion..hiroki teranishi, hiroyuki shindo, and yuji mat-sumoto.
2017. coordination boundary identiﬁcationwith similarity and replaceability.
in proceedings ofthe eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 264–272, taipei, taiwan.
asian federation ofnatural language processing..hiroki teranishi, hiroyuki shindo, and yuji mat-sumoto.
2019. decomposed local models for coor-dinate structure parsing.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 3394–3403, minneapolis, minnesota,usa.
association for computational linguistics..0.3 based on dev-set performance.
our hyperpa-rameters are listed in table a1..appendix c extended results.
table a2 and table a3 include detailed evaluationresults on the ptb and genia datasets..references.
jessica ficler and yoav goldberg.
2016a.
coordina-tion annotation extension in the penn tree bank.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 834–842, berlin, germany.
as-sociation for computational linguistics..jessica ficler and yoav goldberg.
2016b.
a neural net-work for coordination boundary prediction.
in pro-ceedings of the 2016 conference on empirical meth-ods in natural language processing, pages 23–32,austin, texas, usa.
association for computationallinguistics..kazuo hara, masashi shimbo, hideharu okuma, andyuji matsumoto.
2009. coordinate structure analy-sis with global structural constraints and alignment-in proceedings of the jointbased local features.
conference of the 47th annual meeting of the acland the 4th international joint conference on natu-ral language processing of the afnlp, pages 967–975, suntec, singapore.
association for computa-tional linguistics..jin dong kim, tomoko ohta, yuka tateisi, andjun’ichi tsujii.
2003. genia corpus—a semanti-cally annotated corpus for bio-textmining.
bioinfor-matics, 19(suppl_1):i180–i182..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof the 3rd international conference on learningrepresentations, san diego, california, usa..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
computa-tional linguistics, 19(2):313–330..joakim nivre, marie-catherine de marneffe, filip gin-ter, yoav goldberg, jan hajiˇc, christopher d. man-ning, ryan mcdonald, slav petrov, sampo pyysalo,natalia silveira, reut tsarfaty, and daniel zeman.
2016. universal dependencies v1: a multilingualtreebank collection.
in proceedings of the tenth in-ternational conference on language resources andevaluation (lrec’16), pages 1659–1666, portorož,slovenia.
european language resources associa-tion..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conference.
7181dev.
test.
f.p.f.p.f.p.p.74.1376.9577.1984.25.
72.3476.0479.1978.6185.19.allr.73.3476.7677.0084.55.
72.2575.2379.0078.4285.50.
73.7476.8577.1084.40.
72.2975.6379.1078.5185.34.
76.2178.1180.0088.53.
75.1777.8280.6482.0789.45.npr.75.5177.5779.6388.33.
74.8277.1180.0981.6989.24.allr.70.7075.6176.3483.62.
72.6173.3377.8878.1684.35.npr.74.8477.7081.3785.19.
75.3177.0779.9383.7686.15.f.75.0177.8381.6385.26.
76.1077.2580.0684.0386.22.
71.0875.4776.4883.74.
72.7073.7477.7478.3084.46.
75.2077.9581.8985.33.
76.9177.4480.1984.2986.28.
75.8677.8479.8288.43.
74.9977.4780.3681.8889.35.
71.4875.3376.6283.85.
72.8174.1477.6078.4584.58.exact.
inner.
tsm17tsm19ours+bert.
fg16tsm17tsm19ours+bert.
table a2: precision, recall, and f1 scores on the ptb dev and test sets..count.
tsm17tsm19ours+bert.
hsom09fg16tsm17tsm19ours+bert.
np2,317.
57.1459.2168.2879.41.
64.265.0867.1959.3069.4080.58.vp465.
54.8364.9458.7176.34.
54.271.8263.6565.1659.3576.77.adjp321.
72.2778.1986.2988.79.
80.474.7676.6378.1987.8590.03.s188.
8.5153.1956.3877.13.
22.917.0253.1953.1957.4578.19.pp167.
55.6855.6855.0973.05.
59.956.2861.6755.6856.8976.65.ucp60.
28.3348.3351.6761.67.
36.751.6735.0048.3351.6761.67.sbar advp others21.
56.
3.
57.1466.0758.9376.79.
51.891.0778.5766.0762.5082.14.
85.7190.4795.24100.0.
85.780.9585.7190.4795.24100.0.
0.000.000.0033.33.
66.733.3333.330.000.0033.33.all3,598.
55.2261.2267.0979.18.
61.564.1466.3161.3168.2380.41.exact.
whole.
table a3: recall on the genia dataset..7182