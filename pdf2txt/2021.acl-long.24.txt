a bidirectional transformer based alignment model for unsupervisedword alignment.
jingyi zhang1 and josef van genabith1,21german research center for artiﬁcial intelligence (dfki),saarland informatics campus, saarbr¨ucken, germany2department of language science and technology, saarland university,saarland informatics campus, saarbr¨ucken, germanyjingyi.zhang@dfki.de,josef.van genabith@dfki.de.
abstract.
word alignment and machine translation aretwo closely related tasks.
neuraltransla-tion models, such as rnn-based and trans-former models, employ a target-to-source at-tention mechanism which can provide roughword alignments, but with a rather low accu-racy.
high-quality word alignment can helpneural machine translation in many differentways, such as missing word detection, anno-tation transfer and lexicon injection.
existingmethods for learning word alignment includestatistical word aligners (e.g.
giza++) and re-cently neural word alignment models.
this pa-per presents a bidirectional transformer basedalignment (btba) model for unsupervisedlearning of the word alignment task.
ourbtba model predicts the current target wordby attending the source context and both left-side and right-side target context to produceaccurate target-to-source attention (alignment).
we further ﬁne-tune the target-to-source atten-tion in the btba model to obtain better align-ments using a full context based optimizationmethod and self-supervised training.
we testour method on three word alignment tasks andshow that our method outperforms both previ-ous neural word alignment approaches and thepopular statistical word aligner giza++..1.introduction.
neural machine translation (nmt) (bahdanauet al., 2014; vaswani et al., 2017) achieves state-of-the-art results for various translation tasks (bar-rault et al., 2019, 2020).
neural translation models,such as rnn-based (bahdanau et al., 2014) andtransformer (vaswani et al., 2017) models, gen-erally have an encoder-decoder structure with atarget-to-source attention mechanism.
the target-to-source attention in nmt can provide rough wordalignments but with a rather low accuracy (koehnand knowles, 2017).
high-quality word alignment.
can be used to help nmt in many different ways,such as detecting source words that are missingin the translation (lei et al., 2019), integrating anexternal lexicon into nmt to improve translationfor domain-speciﬁc terminology or low-frequencywords (chatterjee et al., 2017; chen et al., 2020),transferring word-level annotations (e.g.
under-line and hyperlink) from source to target for docu-ment/webpage translation (m¨uller, 2017)..a number of approaches have been proposed tolearn the word alignment task, including both statis-tical models (brown et al., 1993) and recently neu-ral models (zenkel et al., 2019; garg et al., 2019;zenkel et al., 2020; chen et al., 2020; stengel-eskin et al., 2019; nagata et al., 2020).
the pop-ular word alignment tool giza++ (och and ney,2003) is based on statistical ibm models (brownet al., 1993) which learn the word alignment taskthrough unsupervised learning and do not requiregold alignments from humans as training data.
asdeep neural networks have been successfully ap-plied to many natural language processing (nlp)tasks, neural word alignment approaches have de-veloped rapidly and outperformed statistical wordaligners (zenkel et al., 2020; garg et al., 2019).
neural word alignment approaches include both su-pervised and unsupervised approaches: supervisedapproaches (stengel-eskin et al., 2019; nagataet al., 2020) use gold alignments from human an-notators as training data and train neural models tolearn word alignment through supervised learning;unsupervised approaches do not use gold humanalignments for model training and mainly focus onimproving the target-to-source attention in nmtmodels to produce better word alignment, suchas performing attention optimization during infer-ence (zenkel et al., 2019), encouraging contiguousalignment connections (zenkel et al., 2020) or us-ing alignments from giza++ to supervise/guidethe attention in nmt models (garg et al., 2019)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages283–292august1–6,2021.©2021associationforcomputationallinguistics283we propose a bidirectional transformer basedalignment (btba) model for unsupervised learn-ing of the word alignment task.
our btba modelpredicts the current target word by paying atten-tion to the source context and both left-side andright-side target context to produce accurate target-to-source attention (alignment).
compared to theoriginal transformer translation model (vaswaniet al., 2017) which computes target-to-source at-tention based on only the left-side target contextdue to left-to-right autoregressive decoding, ourbtba model can exploit both left-side and right-side target context to compute more accurate target-to-source attention (alignment).
we further ﬁne-tune the btba model to produce better alignmentsusing a full context based optimization methodand self-supervised training.
we test our methodon three word alignment tasks and show that ourmethod outperforms previous neural word align-ment approaches and also beats the popular statisti-cal word aligner giza++..2 background.
2.1 word alignment task.
the goal of the word alignment task (och and ney,2003) is to ﬁnd word-level alignments for paral-lel source and target sentences.
given a sourcesentence si−10 = s0, ..., si, ..., si−1 and its paralleltarget sentence tj−10 = t0, ..., tj, ..., tj−1, the wordalignment g is deﬁned as a set of links that link thecorresponding source and target words as shown inequation 1..g ⊆ {(i, j) : i = 0, ..., i − 1; j = 0, ..., j − 1}.
(1).
the word alignment g allows one-to-one, one-to-many, many-to-one, many-to-many alignments andalso unaligned words (och and ney, 2003).
due tothe lack of labelled training data (gold alignmentsannotated by humans) for the word alignment task,most word alignment methods learn the word align-ment task through unsupervised learning (brownet al., 1993; zenkel et al., 2020; chen et al., 2020)..2.2 neural machine translation.
neural translation models (bahdanau et al., 2014;vaswani et al., 2017) generally have an encoder-decoder structure with a target-to-source attentionmechanism: the encoder encodes the source sen-tence; the decoder generates the target sentenceby attending the source context and performing.
left-to-right autoregressive decoding.
the target-to-source attention learned in nmt models can pro-vide rough word alignments between source andtarget words.
among various translation models,the transformer translation model (vaswani et al.,2017) achieves state-of-the-art results on varioustranslation tasks and is based solely on attention:source-to-source attention in the encoder; target-to-target and target-to-source attention in the decoder.
the attention networks used in the transformermodel are called multi-head attention which per-forms attention using multiple heads as shown inequation 2..m ultihead (q, k, v ).
= concat (head0, ..., headn −1) w o.headn = an · vn.
an = sof tmaxqn = qw q.
(cid:17).
(cid:16) qnktn√dk.
n , kn = kw k.n , vn = v w vn.(2).
n , w k.where q, k and v are query, keys, values forthe attention function; w o, w qn and w vnare model parameters; dk is the dimension of thekeys.
based on parallelizable attention networks,the transformer can be trained much faster thanrnn-based translation models (bahdanau et al.,2014)..3 related work.
3.1 statistical alignment models.
word alignment is a key component in traditionalstatistical machine translation (smt), such asphrase-based smt (koehn et al., 2003) which ex-tracts phrase-based translation rules based on wordalignments.
the popular statistical word alignmenttool giza++ (och and ney, 2003) implements thestatistical ibm models (brown et al., 1993).
thestatistical ibm models are mainly based on lexicaltranslation probabilities.
words that co-occur fre-quently in parallel sentences generally have higherlexical translation probabilities and are more likelyto be aligned.
the statistical ibm models aretrained using parallel sentence pairs with no word-level alignment annotations and therefore learn theword alignment task through unsupervised learn-ing.
based on a reparameterization of ibm model2, dyer et al.
(2013) presented another popular sta-tistical word alignment tool fast align which can betrained faster than giza++, but giza++ generallyproduces better word alignments than fast align..2843.2 neural alignment models.
with neural networks being successfully appliedto many nlp tasks, neural word alignment ap-proaches have received much attention.
the ﬁrstneural word alignment models are based on feed-forward neural networks (yang et al., 2013) andrecurrent neural networks (tamura et al., 2014)which can be trained in an unsupervised manner bynoise-contrastive estimation (nce) (gutmann andhyv¨arinen, 2010) or in a supervised manner by us-ing alignments from human annotators or existingword aligners as labelled training data..as nmt (bahdanau et al., 2014; vaswani et al.,2017) achieves great success, the target-to-sourceattention in nmt models can be used to infer roughword alignments, but with a rather low accuracy.
a number of recent works focus on improving thetarget-to-source attention in nmt to produce betterword alignments (garg et al., 2019; zenkel et al.,2019; chen et al., 2020; zenkel et al., 2020).
garget al.
(2019) trained the transformer translationmodel to jointly learn translation and word align-ment through multi-task learning using word align-ments from existing word aligners such as giza++as labelled training data.
chen et al.
(2020) pro-posed a method to infer more accurate word align-ments from the transformer translation model bychoosing the appropriate decoding step and layerfor word alignment inference.
zenkel et al.
(2019)proposed an alignment layer for the transformertranslation model and they only used the outputof the alignment layer for target word predictionwhich forces the alignment layer to produce bet-ter alignment (attention).
zenkel et al.
(2019) alsoproposed an attention optimization method whichdirectly optimizes the attention for the test set toproduce better alignment.
zenkel et al.
(2020) pro-posed to improve the attention in nmt by using acontiguity loss to encourage contiguous alignmentconnections and performing direct attention opti-mization to maximize the translation probabilityfor both the source-to-target and target-to-sourcetranslation models.
compared to these methodsthat infer word alignments based on nmt target-to-source attention which is computed by consideringonly the left-side target context, our btba modelcan exploit both left-side and right-side target con-text to compute better target-to-source attention(alignment)..there are also a number of supervised neuralapproaches that require gold alignments from hu-.
mans for learning the word alignment task (stengel-eskin et al., 2019; nagata et al., 2020).
becausegold alignments from humans are scarce, stengel-eskin et al.
(2019); nagata et al.
(2020)’s modelsonly have a small size of task-speciﬁc training dataand exploit representations from pre-trained nmtand bert models.
compared to these supervisedmethods, our method does not require gold humanalignments for model training..4 our approach.
we present a bidirectional transformer based align-ment (btba) model for unsupervised learning ofthe word alignment task.
motivated by bertwhich learns a masked language model (devlinet al., 2019), we randomly mask 10% of the wordsin the target sentence and then train our btbamodel to predict the masked target words by pay-ing attention to the source context and both left-side and right-side target context.
therefore, ourbtba model can exploit both left-side and right-side target context to compute more accurate target-to-source attention (alignment) compared to theoriginal transformer translation model (vaswaniet al., 2017) which computes the target-to-sourceattention based on only the left-side target contextdue to left-to-right autoregressive decoding.
wefurther ﬁne-tune the target-to-source attention inthe btba model to produce better alignments us-ing a full context based optimization method andself-supervised training..4.1 bidirectional transformer based.
alignment (btba).
figure 1 shows the architecture of the proposedbtba model.
the encoder is used to encode thesource sentence1 and has the same structure asthe original transformer encoder (vaswani et al.,2017).
the input of the decoder is the maskedtarget sentence and 10% of the words in the tar-get sentence are randomly masked2.
as shown infigure 1, the target sentence contains a maskedword <x>.
the decoder contains 6 layers.
eachof the ﬁrst 5 layers of the decoder has 3 sub-layers:.
1following och and ney (2003)’s work, we add a <bos>token at the beginning of the source sentence for target wordsthat are not aligned with any source words..2during training, we randomly mask 10% of the words inthe target sentences for each training epoch, i.e., one targetsentence is masked differently for different training epochs.
if a target sentence contains less than 10 words, then we justrandomly mask one word in this sentence..285original.
masked.
the cake is very delicious<x> cake is very deliciousthe <x> is very deliciousthe cake <x> very deliciousthe cake is <x> deliciousthe cake is very <x>.
table 1: masking target sentences in the test set..the last target-to-source attention sub-layer to payattention to the most important source words forpredicting the target word and therefore producebetter word alignments..in figure 1, aijn is the attention value of thejth target word paying to the ith source word usingthe nth head in the last target-to-source multi-headattention sub-layer.
v0, v1, v2, v3, v4 are the out-puts of the decoder for the 5 target words and v1is used to predict the masked target word “cake”.
because v1 is used to predict “cake”, the attentionvalue a21n should be learned to be high in orderto make v1 contain the most useful source infor-mation (“kuchen”).
therefore, aijn can be usedto infer word alignment for the target word “cake”effectively.
however, aijn cannot provide goodword alignments for unmasked target words suchas “delicious” in figure 1 because v4 is not used topredict any target word and a54n is not necessarilylearned to be high..because aijn can only be used to infer accu-rate word alignment for masked target words butwe want to get alignments for all target words inthe test set, we mask a target sentence tj−1in thetest set j times and each time we mask one targetword as shown in table 1. each masked target sen-tence is fed into the btba model together with thesource sentence and then we collect the attentionaijn for the masked target words.
suppose the j(cid:48)thtarget word is masked, then we compute the sourceposition that it should be aligned to as,.
0.i(cid:48) = arg max.
aij(cid:48)n.n −1(cid:88).
n=0.
i.
(3).
4.2 full context based optimizationin equation 3, the attention aij(cid:48)n for the j(cid:48) tar-get word is computed by considering both left-sideand right-side target context, but information aboutthe current target word is not used since the j(cid:48) tar-get word is masked.
for example in figure 1, thebtba model does not know that the second targetword is “cake” because it is masked, therefore thebtba model computes the attention (alignment).
figure 1: architecture of our btba model..a multi-head self-attention sub-layer, a target-to-source multi-head attention sub-layer and a feedforward sub-layer, like a standard transformer de-coder layer except that the self-attention sub-layerin the standard transformer decoder can only at-tend left-side target context while the self-attentionsub-layer in our btba decoder can attend all targetwords and make use of both left-side and right-sidetarget context to compute better target-to-sourceattention (alignment).
the last layer of the btbadecoder contains a self-attention sub-layer and atarget-to-source attention sub-layer like the ﬁrst 5layers of the btba decoder but without the feed-forward sub-layer.
we use the output of the lasttarget-to-source attention sub-layer for predictingthe masked target words and we use the attention ofthe last target-to-source attention sub-layer for in-ferring word alignments between source and targetwords.
our design that only uses the last target-to-source attention sub-layer output for predictingthe masked target words is motivated by the align-ment layer of zenkel et al.
(2019) in order to force.
286multi-headattentionadd & normfeedforwardadd & normmulti-headattentionadd & normfeedforwardadd & normmulti-headattentionadd & normembeddingembedding6 x5 xmulti-headattentionmulti-headattentionadd & normpositionalencodingpositionalencodingv0v1v2v3v4aijncaketarget:the <x> is very delicioussource:<bos> derkuchenistsehrleckern and w k.for “cake” only using the left-side and right-sidecontext of “cake” without knowing that the wordthat needs to be aligned is “cake”.
we propose anovel full context based optimization method to usefull target context, including the current target wordinformation, to improve the target-to-source atten-tion in the btba model to produce better align-ments.
that is for the last 50 training steps of thebtba model, we do not mask the target sentenceany more and we only optimize parameters w qnand w kn in the last target-to-source multi-head at-tention sub-layer.
as shown in equation 2, w qnand w kn are parameters that are used to computethe attention values in multi-head attention.
opti-mizing w qn based on full target contextcan help the btba model to produce better atten-tion (alignment) while at the same time freezingother parameters can make the btba model keepthe knowledge learned from masked target wordprediction.
after full target context based optimiza-tion, we do not need to mask target sentences inthe test set as shown in table 1 any more.
wecan directly feed the original source and target testsentences into the btba model and compute atten-tion (alignment) for all target words in the sentence.
the full context based optimization method can beseen as a ﬁne-tuning of the original btba model,i.e.
we ﬁne-tune the two parameters w qn and w knin the last target-to-source attention layer based onfull target context to compute more accurate wordalignments..4.3 self-supervised training.
the btba model learns word alignment throughunsupervised learning and does not require labelleddata for the word alignment task.
we train twounsupervised btba models, one for the forwarddirection (source-to-target) and one for the back-ward direction (target-to-source), and then sym-metrize the alignments using heuristics such asgrow-diagonal-ﬁnal-and (och and ney, 2003) asthe symmetrized alignments have better qualitythan the alignments from a single forward or back-ward model.
after unsupervised learning, we usethe symmetrized word alignments ga inferred fromour unsupervised btba models as labelled data tofurther ﬁne-tune each btba model for the wordalignment task through supervised training usingthe alignment loss in equation 4 following garg.
et al.
(2019)’s work.3 during supervised train-ing, the btba model is trained to learn the align-ment task instead of masked target word prediction,therefore the target sentence does not need to bemasked..la (a) = −.
1|ga|.
(cid:88).
n −1(cid:88).
(p,q)∈ga.
n=0.
log (apqn).
(4).
note that we apply byte pair encoding (bpe)(sennrich et al., 2016) for both source and tar-get sentences before we feed them into the btbamodel.
therefore the alignments inferred fromthe btba model is on bpe-level.
we convert4bpe-level alignments to word-level alignments be-fore we perform alignment symmetrization.
af-ter alignment symmetrization, we want to usethe symmetrized alignments to further ﬁne-tuneeach btba model through supervised learning andtherefore we convert5 the word-level alignmentsback to bpe-level for supervised training of thebtba models..5 experiments.
5.1 settings.
in order to compare with previous work, we usedthe same datesets6 as zenkel et al.
(2020)’s workand conducted word alignment experiments forthree language pairs: german ↔ english (deen),english ↔ french (enfr) and romanian ↔ en-glish (roen).
each language pair contains a testset and a training set: the test set contains paral-lel sentences with gold word alignments annotatedby humans; the training set contains only parallelsentences with no word alignments.
table 2 givesnumbers of sentence pairs contained in the train-ing and test sets.
parallel sentences from both thetraining set and the test set can be used to train.
3we optimize all model parameters during supervised ﬁne-.
tuning..4to convert bpe-level alignments to word-level align-ments, we add an alignment between a source word and atarget word if any parts of these two words are aligned.
align-ments between the source <bos> token and any target wordare deleted; alignments between the last source word “.” (fullstop) and a target word which is not the last target word arealso deleted..5to convert word-level alignments to bpe-level align-ments, we add an alignment between a source bpe tokenand a target bpe token if the source word and the target wordthat contain these two bpe tokens are aligned; we add analignment between the source <bos> token and a target bpetoken if the target word that contains this target bpe token isnot aligned with any source words..6https://github.com/lilt/alignment-scripts.
287deen.
roentrain 1.91m 1.13m 447ktest.
enfr.
248.
508.
447.table 2: numbers of sentence pairs in the datasets..unsupervised word alignment models.
we use bpe(sennrich et al., 2016) to learn a joint source and tar-get vocabulary of 40k.
after bpe, we train btbamodels to learn the word alignment tasks.
we usea word embedding size of 512. the feed forwardlayer contains 2048 hidden units.
the multi-headattention layer contains 8 heads.
we use the adam(kingma and ba, 2014) algorithm for optimiza-tion and set the learning rate to 0.0002. we use adropout of 0.3. each training batch contains 40kmasked target words.
since the word alignmenttasks do not provide validation data, we trainedall btba models for a ﬁxed number of trainingepochs: 50 for deen, 100 for enfr and 200 forroen.7 for the last 50 training steps of each btbamodel, we performed full context based optimiza-tion..for each language pair, we trained two btbamodels, one for the forward direction and onefor the backward direction, and then symmetrizedthe alignments.
we tested different heuristics foralignment symmetrization, including the standardmoses heuristics, grow-diagonal, grow-diagonal-ﬁnal, grow-diagonal-ﬁnal-and.
we also tested an-other heuristic grow-diagonal-and which is slightlydifferent from grow-diagonal: the grow-diagonal-and heuristic only adds a new alignment (i, j)when both si and tj are unaligned while grow-diagonal adds a new alignment (i, j) when anyof the two words (si and tj) are unaligned.
we ﬁndthat the moses heuristic grow-diagonal-ﬁnal-andgenerally achieved the best results for symmetriz-ing the btba alignments, but grow-diagonal-andworked particularly good for the enfr task..finally, we used the symmetrized alignmentsinferred from our unsupervised btba models aslabelled data to further ﬁne-tune each btba modelto learn the alignment task through supervised train-ing.
we ﬁne-tuned each btba model for 50 train-ing steps using the alignment loss in equation 4.in addition, we also tested to use alignments fromgiza++ instead of alignments inferred from our.
7the training time (time of one training epoch × numberof training epochs) of one btba model for different tasks(deen, enfr and roen) is roughly the same, 30 hours using 4gpus..methodzenkel et al.
(2019)garg et al.
(2019)zenkel et al.
(2020)chen et al.
(2020)giza++.
ours.
btba-leftbtba-rightbtba.
+ fcbo+ sst+ gst.
deenroenenfr21.2% 10.0% 27.6%23.1%16.0% 4.6%23.4%16.3% 5.0%21.2%15.4% 4.7%18.4% 5.2%24.2%30.3% 20.2% 33.0%32.3% 14.9% 38.6%22.9%17.8% 9.5%20.6%16.3% 8.9%14.3% 6.7%18.5%14.5% 4.2% 19.7%.
table 3: aer results.
fcbo: full context based opti-mization; sst: self-supervised training; gst: giza++supervised training..unsupervised btba models as labelled data forsupervised ﬁne-tuning of the btba models..5.2 results.
table 3 gives alignment error rate (aer) (och andney, 2000) results of our btba model and com-parison with previous work.
table 3 also givesresults of btba-left and btba-right: btba-leftmeans that the btba decoder only attends left-side target context; btba-right means that thebtba decoder only attends right-side target con-text.
as shown in table 3, the btba model, whichuses both left-side and right-side target context,signiﬁcantly outperformed btba-left and btba-right.
results also show that the performance ofour btba model can be further improved by fullcontext based optimization (fcbo) and supervisedtraining including both self-supervised training andgiza++ supervised training.
for deen and roentasks, the self-supervised btba (s-btba) modelachieved the best results, outperforming previousneural and statistical methods.
for the enfr task,as the statistical aligner giza++ performed welland achieved better results than our unsupervisedbtba model, the giza++ supervised btba (g-btba) model achieved better results than the s-btba model and also outperformed the originalgiza++ and previous neural models..tables 4, 5 and 6 give results of using differ-ent heuristics for symmetrizing alignments pro-duced by btba, giza++ and g-btba, respec-tively.
for our unsupervised and self-supervisedbtba models, grow-diagonal-ﬁnal-and achievedthe best results on deen and roen tasks whilegrow-diagonal-and achieved the best results on theenfr task.
for giza++ and g-btba, the bestheuristics for different language pairs are quite dif-ferent, though grow-diagonal-ﬁnal-and generally.
288enfrroenbtba +fcbo +sst btba.
forwardbackwardunionintersectiongrow-diagonalgrow-diagonal-andgrow-diagonal-ﬁnalgrow-diagonal-ﬁnal-and.
+fcbo +sst.
deenbtba14.3% 13.6% 12.8%20.2% 18.3%17.2% 14.6% 13.3%23.8% 23.3%14.5% 15.7% 14.3%20.6% 18.3%17.1% 11.6% 11.2%23.7% 23.9%14.3% 11.2% 10.7%19.9% 18.5%17.3% 9.5% 8.9%21.0% 20.6%19.5% 17.3%14.4% 14.4% 13.4%17.8% 16.3% 14.3% 11.9% 11.2%.
+fcbo +sst20.5%7.3% 24.7% 22.4%22.0%7.5% 27.3% 26.1%18.9%7.5% 24.1% 21.2%24.0%7.4% 28.3% 27.9%18.6%6.9% 23.6% 21.6%6.7% 26.1% 25.4%23.6%7.4% 23.4% 20.8%18.6%7.0% 22.9% 20.6% 18.5%.
table 4: comparison of different heuristics for symmetrizing the btba alignments.
fcbo: full context basedoptimization.
sst: self-supervised training..forwardbackwardunionintersectiongrow-diagonalgrow-diagonal-andgrow-diagonal-ﬁnalgrow-diagonal-ﬁnal-and.
deenroenenfr19.0% 10.3% 25.6%22.5% 9.1%29.7%22.1% 12.9% 27.5%19.0% 5.2% 27.8%18.4% 7.7%24.5%26.1%18.9% 5.7%21.1% 11.7% 26.0%24.2%18.9% 8.5%.
forwardbackwardunionintersectiongrow-diagonalgrow-diagonal-andgrow-diagonal-ﬁnalgrow-diagonal-ﬁnal-and.
deenroenenfr14.5% 5.8% 21.4%17.6% 4.2% 21.9%15.1% 5.3% 19.9%17.2% 4.7% 23.6%14.7% 4.6% 19.7%17.5% 4.4% 23.7%15.1% 5.3% 19.8%14.8% 4.7% 19.8%.
table 5: comparison of different heuristics for sym-metrizing giza++ alignments..table 6: comparison of different heuristics for sym-metrizing g-btba alignments..during fcbo, the aer result (the red curve) ﬁrstincreased a little, then decreased sharply, and soonincreased again.
in contrast, when we freeze mostof the parameters, the aer result (the blue curve)decreased stably and eventually got better results(16.3%) than no parameter freezing (16.7%).
notethat the results in figure 2 are computed basedon full target context, i.e., the target sentence isnot masked.
as we explained in section 4.1, thebtba model without fcbo should only be usedto infer word alignments for masked target words.
without fcbo, using the btba model to inferword alignments for unmasked target words pro-duces poor aer results (26.9% as shown in fig-ure 2) compared to using the btba model to inferword alignments for masked target words (17.8%as shown in table 3).
fcbo can quickly improvethe results of using the btba model for inferringword alignments for unmasked target words, andeventually after fcbo, the btba model can effec-tively use full target context to compute better wordalignment compared to the original btba modelwithout fcbo (16.3% versus 17.8% as shown intable 3)..training data for supervised learning be-cause the symmetrized btba alignments have bet-ter quality compared to alignments from a singleunidirectional (forward or backward) btba modelas shown in table 4, we used the symmetrized.
figure 2: deen test aer per training step duringfcbo with/without parameter freezing..obtained good (best or close to best) results ondeen and roen tasks while grow-diagonal-andgenerally obtained good (close to best) results onthe enfr task..n and w k.fcbo with/without parameter freezing aswe explained in section 4.2, during full contextbased optimization (fcbo), we only optimizew qn in the last target-to-source attentionsub-layer and freeze all other parameters so thebtba model can keep the knowledge learned frommasked target word prediction.
we also testedto optimize all parameters of the btba modelwithout parameter freezing during fcbo.
figure 2shows how the aer results on the deen test setchanged during fcbo with and without param-eter freezing.
without freezing any parameters.
28916182022242628300102030405060freezeno freezeaertraining stepfigure 3: an example of gold alignments and alignments produced by our s-btba model..deen12.3.s-btba.
ffcc 6.1fcg-btba ff.
44.413.2.cc 7.1fc.
43.3.enfr roen11.33.312.85.12.99.3.
18.27.841.118.68.346.1.table 7: aer for different types of alignments..figure 4: aer results of the forward btba model dur-ing self-supervised training.
uni: using unidirectionalbtba alignments as labelled training data.
sym: us-ing symmetrized btba alignments as labelled trainingdata..word alignments inferred from our unsupervisedbtba models as labelled data to further ﬁne-tuneeach unidirectional btba model for the alignmenttask through supervised training.
we also testedto use unidirectional btba alignments instead ofsymmetrized btba alignments as labelled datafor supervised training.
figure 4 (the blue curve)shows how the performance of the forward btbamodel of the deen task changes during supervisedtraining when using unidirectional alignments in-ferred from itself (the forward btba model) aslabelled training data, which demonstrates thatthe forward btba model can be signiﬁcantly im-proved through supervised training even when thetraining data is inferred from itself and not im-proved by alignment symmetrization.
figure 4 alsoshows that using symmetrized alignments for su-pervised training (the red curve) did achieve betterresults than using unidirectional alignments for su-pervised training.
in addition, it is worth noting thatsupervised training can improve the btba modeleven if the quality of the labelled training data issomewhat worse than the btba model itself, e.g.
for the roen task, using the giza++ alignmentsfor ﬁne-tuning the forward btba model throughsupervised training improved the result of the for-ward btba model (22.4% → 21.4% as shown in.
table 4 and table 6) even though giza++ pro-duced worse alignments (24.2% in table 3) thanthe forward btba model..alignment error analysis we analyze thealignment errors produced by our system and ﬁndthat most of the alignment errors are caused byfunction words.
as shown in the alignment exam-ple in figure 3, source and target correspondingcontent words (e.g.
“deﬁniert” and “deﬁnes”) areall correctly aligned by our model, but functionwords such as “the”, “im” and “wird” are not cor-rectly aligned.
to give a more detailed analysis, wecompute aer results of our model for 3 differenttypes of alignments: ff (alignments between twofunction words), cc (alignments between two con-tent words) and fc (alignments between a functionword and a content word).8 table 7 shows that ourmodels achieved signiﬁcantly better results for ccalignments than for ff and fc alignments.
func-tion words are more difﬁcult to align than contentwords most likely because content words in a paral-lel sentence pair usually have very clear correspond-ing relations (such as “deﬁnes” clearly correspondsto “deﬁniert” in figure 3), but function words (suchas “the”, “es” and “im”) are used more ﬂexibly andfrequently do not have clear corresponding wordsin parallel sentences, which increases the alignmentdifﬁculty signiﬁcantly..8for each language, we judge whether a word is a function.
word or a content word using a list of stopwords from nltk,https://www.nltk.org/.
290goldours,.dürfenwerdenabgefülltschaumweinflascheningetränkewelcheklardefiniertberichtsvorschlagimeswirdmaybeverageswhichdefinesclearlyreportthe.bottleswinesparklinginbottledbemaybeverageswhichdefinesclearlyreportthe.bottleswinesparklinginbottledbe1416182022240102030405060unisymtraining  stepaershift-aetours.
de→en34.835.1.en→de28.028.7.funding code 01iw20010 (cora4nlp)..table 8: translation results (bleu) for dictionary-guided nmt..references.
5.3 dictionary-guided nmt via word.
alignment.
for downstream tasks, word alignment can be usedto improve dictionary-guided nmt (song et al.,2020; chen et al., 2020).
speciﬁcally, at each de-coding step in nmt, chen et al.
(2020) used ashift-aet method to compute word alignmentfor the newly generated target word and then re-vised the newly generated target word by encour-aging the pre-speciﬁed translation from the dictio-nary.
the shift-aet alignment method adds aseparate alignment module to the original trans-former translation model (vaswani et al., 2017)and trains the separate alignment module usingalignments induced from the attention weightsof the original transformer.
to test the effec-tiveness of our alignment method for improvingdictionary-guided nmt, we used the alignmentsinferred from our btba models as labelled datafor supervising the shift-aet alignment moduleand performed dictionary-guided translation for thegerman↔english language pair following chenet al.
(2020)’s work.
table 8 gives the translation re-sults of dictionary-guided nmt and shows that ouralignment method led to higher translation qualitycompared to the original shift-aet method..6 conclusion.
this paper presents a novel btba model for unsu-pervised learning of the word alignment task.
ourbtba model predicts the current target word bypaying attention to the source context and bothleft-side and right-side target context to produceaccurate target-to-source attention (alignment).
wefurther ﬁne-tune the target-to-source attention inthe btba model to obtain better alignments usinga full context based optimization method and self-supervised training.
we test our method on threeword alignment tasks and show that our method out-performs both previous neural alignment methodsand the popular statistical word aligner giza++..acknowledgments.
this work is supported by the german federal min-istry of education and research (bmbf) under.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..loc barrault, magdalena biesialska, ondej bo-jar, marta r. costa-juss, christian federmann,yvette graham, roman grundkiewicz, barry had-dow, matthias huck, eric joanis, tom kocmi,philipp koehn, chi-kiu lo, nikola ljubei, christofmonz, makoto morishita, masaaki nagata, toshi-aki nakazawa, santanu pal, matt post, and marcoszampieri.
2020. findings of the 2020 conference onmachine translation (wmt20).
in proceedings of thefifth conference on machine translation, pages 1–55, online.
association for computational linguis-tics..loc barrault, ondej bojar, marta r. costa-juss, chris-tian federmann, mark fishel, yvette graham, barryhaddow, matthias huck, philipp koehn, shervinmalmasi, christof monz, mathias mller, santanupal, matt post, and marcos zampieri.
2019. find-ings of the 2019 conference on machine translation(wmt19).
in proceedings of the fourth conferenceon machine translation (volume 2: shared task pa-pers, day 1), pages 1–61, florence, italy.
associa-tion for computational linguistics..peter f. brown, stephen a. della pietra, vincent j.della pietra, and robert l. mercer.
1993. the math-ematics of statistical machine translation: parameterestimation.
computational linguistics, 19(2):263–311..rajen chatterjee, matteo negri, marco turchi, mar-cello federico, lucia specia, and fr´ed´eric blain.
2017. guiding neural machine translation decodingwith external knowledge.
in proceedings of the sec-ond conference on machine translation, pages 157–168, copenhagen, denmark.
association for com-putational linguistics..yun chen, yang liu, guanhua chen, xin jiang, andqun liu.
2020. accurate word alignment inductionfrom neural machine translation.
in proceedings ofthe 2020 conference on empirical methods in natu-ral language processing (emnlp), pages 566–576,online.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..291chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameter-in proceedings of theization of ibm model 2.
2013 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 644–648, at-lanta, georgia.
association for computational lin-guistics..sarthak garg, stephan peitz, udhyakumar nallasamy,and matthias paulik.
2019. jointly learning to alignand translate with transformer models.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4453–4462, hongkong, china.
association for computational lin-guistics..michael gutmann and aapo hyv¨arinen.
2010. noise-contrastive estimation: a new estimation principlefor unnormalized statistical models.
in proceedingsof the thirteenth international conference on artiﬁ-cial intelligence and statistics, pages 297–304..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..philipp koehn and rebecca knowles.
2017. six chal-in proceed-lenges for neural machine translation.
ings of the first workshop on neural machine trans-lation, pages 28–39, vancouver.
association forcomputational linguistics..philipp koehn, franz j. och, and daniel marcu.
2003.statistical phrase-based translation.
in proceedingsof the 2003 human language technology confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 127–133..wenqiang lei, weiwen xu, ai ti aw, yuanxin xiang,and tat seng chua.
2019. revisit automatic errordetection for wrong and missing translation – a su-pervised approach.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 942–952, hong kong, china.
as-sociation for computational linguistics..mathias m¨uller.
2017. treatment of markup in sta-in proceedings of thetistical machine translation.
third workshop on discourse in machine transla-tion, pages 36–46, copenhagen, denmark.
associa-tion for computational linguistics..masaaki nagata, chousa katsuki,.
and masaakinishino.
2020.a supervised word alignmentmethod based on cross-language span predic-arxiv preprinttion using multilingual bert.
arxiv:2004.14516..franz josef och and hermann ney.
2000..improvedstatistical alignment models.
in proceedings of the.
38th annual meeting of the association for com-putational linguistics, pages 440–447, hong kong.
association for computational linguistics..franz josef och and hermann ney.
2003. a systematiccomparison of various statistical alignment models.
computational linguistics, 29(1):19–51..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..kai song, kun wang, heng yu, yue zhang,zhongqiang huang, weihua luo, xiangyu duan,and min zhang.
2020. alignment-enhanced trans-former for constraining nmt with pre-speciﬁed trans-lations.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 34, pages 8886–8893..elias stengel-eskin, tzu-ray su, matt post, and ben-jamin van durme.
2019. a discriminative neuralmodel for cross-lingual word alignment.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 910–920, hongkong, china.
association for computational lin-guistics..akihiro tamura, taro watanabe, and eiichiro sumita.
2014. recurrent neural networks for word align-in proceedings of the 52nd annualment model.
meeting of the association for computational lin-guistics (volume 1: long papers), pages 1470–1480, baltimore, maryland.
association for compu-tational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..nan yang, shujie liu, mu li, ming zhou, and neng-hai yu.
2013. word alignment modeling with con-in proceed-text dependent deep neural network.
ings of the 51st annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 166–175, soﬁa, bulgaria.
associationfor computational linguistics..thomas zenkel, joern wuebker, and john denero.
2019. adding interpretable attention to neural trans-arxivlation models improves word alignment.
preprint arxiv:1901.11359..thomas zenkel, joern wuebker, and john denero.
2020. end-to-end neural word alignment outper-forms giza++.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 1605–1617, online.
association forcomputational linguistics..292