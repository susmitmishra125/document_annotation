weakly supervised named entity tagging with learnable logical rules.
jiacheng li1∗, haibo ding2, jingbo shang1, julian mcauley1, zhe feng2university of california, san diego1bosch research north america2{j9li,jshang,jmcauley}@eng.ucsd.edu1{haibo.ding, zhe.feng2}@us.bosch.com2.
abstract.
we study the problem of building entity tag-ging systems by using a few rules as weaksupervision.
previous methods mostly focuson disambiguating entity types based on con-texts and expert-provided rules, while assum-ing entity spans are given.
in this work, wepropose a novel method tallor that boot-straps high-quality logical rules to train a neu-ral tagger in a fully automated manner.
specif-ically, we introduce compound rules that arecomposed from simple rules to increase theprecision of boundary detection and generatemore diverse pseudo labels.
we further designa dynamic label selection strategy to ensurepseudo label quality and therefore avoid over-ﬁtting the neural tagger.
experiments on threedatasets demonstrate that our method outper-forms other weakly supervised methods andeven rivals a state-of-the-art distantly super-vised tagger with a lexicon of over 2,000 termswhen starting from only 20 simple rules.
ourmethod can serve as a tool for rapidly buildingtaggers in emerging domains and tasks.
casestudies show that learned rules can potentiallyexplain the predicted entities..1.introduction.
entity tagging systems that follow supervised train-ing, while accurate, often require a large amountof manual, domain-speciﬁc labels, making themdifﬁcult to apply to emerging domains and tasks.
to reduce manual effort, previous works resort tomanual lexicons (shang et al., 2018b; peng et al.,2019) or heuristic rules provided by domain ex-perts (fries et al., 2017; safranchik et al., 2020;lison et al., 2020b) as weak supervision.
for ex-ample, linkedhmm (safranchik et al., 2020) canachieve performance close to supervised modelsusing 186 heuristic rules in addition to a lexicon ofover two million terms.
however, it is challenging.
∗work done during an internship at bosch research..figure 1: examples of a seed logical rule and a newlyinduced rule from labeled data for recognizing loca-tions.
‘x’ denotes a token span from a given sentence..for experts to write complete and accurate rulesor lexicons in emerging domains, which requiresboth a signiﬁcant amount of manual effort and adeep understanding of the target data.
how to buildaccurate entity tagging systems using less manualeffort is still an open problem..in this work, we explore methods that can auto-matically learn new rules from unlabeled data and asmall set of seed rules (e.g.
20 rules).
such methodsare desirable in real-world applications not only be-cause they can be rapidly deployed to new domainsor customized entity types, but also because thelearned rules are often effective, interpretable, andsimple for non-experts to “debug” incorrect predic-tions.
as explained in figure 1, new rules can belearned from seed rules.
speciﬁcally, we proposea novel iterative learning method tallor thatcan learn accurate rules to train a neural tagger inan automated manner, with goal to address two keyissues during learning process: (1) how to detectentity boundaries and predict their types simultane-ously with rules, (2) how to generate accurate anddiverse pseudo labels from rules..with such a small set of seed rules as supervision,previous works (niu et al., 2003; huang and riloff,2010; gupta and manning, 2014) only focus on dis-ambiguating entity types assuming entity spans aregiven or just syntactic chunks (e.g., noun phrases)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4568–4581august1–6,2021.©2021associationforcomputationallinguistics4568induce new ruleif tokenstring(x)==“dallas”,    then label(x)=“location”ryn lives in dallas.
john lives in dallas where he was born.he lives in dallas this year.
if pos(x)==“propn”             and prengram(x)==“lives in”,     then label(x)=“location”seed rulefobes lives in seattle.she lives in vancouver.the man lives in california.v3noun phrase.
tallor.
p.bc5cdr.
17.1chem 3.24.1conll.
r.50.135.647.3.f1.
25.55.87.5.p.69.863.086.9.r.67.860.286.7.f1.
68.761.686.8.table 1: boundary detection performance from ourmethod and parsing based noun phrases..however, we ﬁnd that syntactic chunks often donot align well with target entity spans.
for example,given a sentence from conll2003: “germany’srepresentative to the european union’s veterinarycommittee...”, the noun phrases1 are “germany’srepresentative” and “the european union’s veteri-nary committee”, but gold entities in the sentenceare “germany” and “european union”.
we usednoun phrases extracted from spacy as predictedentity boundaries and compared them with groundtruth entity boundaries, which are extracted basedon the results from syntactic parsing.
this settingof using noun phrases as entity candidates is simi-lar to previous work (niu et al., 2003; huang andriloff, 2010).
the results are shown in table 1,a majority of target entities are missed if we usenoun phrases as entity candidates, which will notbe recognized correctly later..to address both entity boundary detection andtype classiﬁcation simultaneously, we ﬁrst deﬁneﬁve types of simple logical rules considering thelexical, local context, and syntax information ofentities.
we notice that simple logical rules areoften inaccurate when detecting entity boundaries.
therefore, we propose to learn compound logicalrules, which are composed from multiple simplerules and logical connectives (e.g.
“and”).
for ex-ample, given the sentence “john lives in dallaswhere he was born”, the simple rule “lives in ”,which is a preceding context clue, will match mul-tiple token spans such as “dallas”, “dallas where”,“dallas where he” etc.
in contrast, compound logi-cal rules can both detect entity boundaries and clas-sify their types accurately.
for example, using boththe preceding context and the part-of-speech (pos)tag rule (e.g.
“lives in ” and pos is a proper noun)can correctly identify the location entity “dallas”.
though we aim to learn accurate rules, automat-ically acquired rules can be noisy.
to ensure thequality of generated pseudo labels, we design adynamic label selection strategy to select highly.
1noun phrases are extracted using spacy noun chunks..accurate labels so that the neural tagger can learnnew entities instead of overﬁtting to the seed rules.
speciﬁcally, we maintain a high-precision label setduring our learning process.
for each learning it-eration, we ﬁrst automatically estimate a ﬁlteringthreshold based on the high-precision set.
then,we ﬁlter out low-conﬁdence pseudo labels by con-sidering both their maximum and average distancesto the high-precision set.
highly conﬁdent labelsare added into the high-precision set for the next it-eration of learning.
our dynamic selection strategyenables our framework to maintain the precision ofrecognized entities while increasing recall duringthe learning process, as shown in our experiments.
we evaluate our method on three datasets.
exper-imental results show that tallor outperformsexisting weakly supervised methods and can in-crease the average f1 score by 60% across threedatasets over methods using seed rules.
furtheranalysis shows that tallor can achieve sim-ilar performance with a state-of-the-art distantlysupervised method trained using 1% of the humaneffort2.
we also conduct a user study concerningthe explainability of learned logical rules.
in ourstudy, annotators agree that 79% (on average overthree annotators) of the matched logical rules canbe used to explain why a span is predicted as atarget entity..in summary, our main contributions are:.
• we deﬁne ﬁve types of logical rules and intro-duce compound logical rules that can accuratelydetect entity boundaries and classify their types.
automatically learned rules can signiﬁcantly re-duce manual effort and provide explanations forentity predictions..• to effectively learn rules, we propose a novelweakly supervised method with a dynamic labelselection strategy that can ensure the quality ofpseudo labels..• we conduct experiments on both general anddomain-speciﬁc datasets and demonstrate the ef-fectiveness of our method..2 tagging with learned logical rules.
we study named entity tagging under a weakly su-pervised setting, and propose tallor ( taggingwith learnable logical rules) to build a taggerwith only a small set of rules.
compared withprevious work, our framework requires less human.
2in experiments, our method used 20 rules, the other sys-tem used a manually constructed lexicon of over 2000 terms..4569figure 2: overview of our tagging framework with logical rules and examples for each step..effort via the use of learned rules; we also show thatthese rules can be used to explain tagging results.
instead of treating tagging as a sequence labelingtask, we formulate tagging as a span labeling task,in which named entities are modeled as spans overone or more tokens.
with this setting, logical rulescan easily be used for labeling entities..overview figure 2 shows the ﬂow of our iter-ative learning framework, which consists of thefollowing components.
first, we generate all entitycandidates and rule candidates from unlabeled data.
then, for each iteration, we apply logical rules tothe unlabeled data and select a set of high-qualityweak training examples.
next, we train a neural tag-ger with the selected training examples and predictthe labels of unlabeled data using the trained model.
finally, we select new accurate logical rules fromcandidate rules using the predictions.
the newlylearned rules will further be used to obtain weaktraining labels for the next iteration..2.1 logical rule extraction.
in our work, a logical rule is deﬁned in the form of“if p then q” (or “p → q”).3 for entity tagging, q isone of the target entity classes, and p can be anymatching logic.
for example, “if a span’s precedingtokens are ‘lives in’, then it is a location”.
wedesign the following ﬁve types of simple logicalrules to consider the lexical, local context, andsyntax information of an entity candidate.
simple logical rules.
a simple logical ruleis deﬁned as a logical rule that contains a sin-gle condition predicate.
we design the follow-ing ﬁve predicates to represent common logi-cal conditions.
given a candidate entity, (1).
3“heuristic rules” and “labeling rules” can also be con-.
verted to logical rules, so they can be used interchangeably..tokenstring matches its lexical string;(2)prengram matches its preceding context tokens;(3) postngram matches its succeeding context to-kens; (4) postag matches its part-of-speech tags;(5) dependencyrel matches the dependency rela-tions of its head word..given a candidate entity “united states” in theabove example, we can extract the followingexample logical rules for recognizing locations:4.tokenstring==“united state”→ location,prengram==“move to the”→ location,postngram==“in 1916”→ location,→ location,postag==“propn propn”dependencyrel==“to” (via pobj) → location..more details about extraction of each condition.
predicate are included in appendix a.1.
compound logical rules.
a compound logi-cal rule is formed with multiple condition pred-icates and logical connectives including and (∧),or (∨), and negation (¬).
in this work, we fo-cus on learning compound logical rules connectedwith conjunctions (∧) to recognize entities pre-cisely, because simple logical rules are often in-sufﬁcient to identify entity boundaries.
in theabove example, the rule prengram==“move to the”can match multiple candidates such as “united”,“united states”, and “united states in” etc., ofwhich many are inaccurate.
however, with acompound rule, e.g.
prengram==“move to the” ∧postag==“propn propn”, we can correctly rec-ognize that “unitied states” is a location..4all words in rules are lower-case and lemmatized..4570unlabeled datarule candidatestrain & apply neural taggerscore and select new rulesapply rules & select training instancesseed ruleslogical rulesentity candidates  <s>barack obama lives in washington.</s><s>lori lightfoot lives in chicago.</s><s>she received education in hawaii.</s>example ruleselected instance examplesentity prediction examplesnewly selected rule examplesdallas→location lives in      .</s> → location        0.9lives in     (propn) → location  0.8in     (propn) → location          0.1<s>ryan lives in dallas.</s>   <s>john moved to dallas.</s>   <s>george dallas was a politician.</s>     stateshemovedin1916totheunitedpronverbadpnumadpdetpropnpropnpobjwe enumerate and extract all possible logicalrules from unlabeled data based on our pre-deﬁnedrule types before the training process..2.2 applying logical rules.
at each iteration, we apply both seed and learnedlogical rules to unlabeled entity candidates to ob-tain a set of weakly labeled instances.
in case anentity candidate is matched by multiple rules (po-tentially conﬂicting), we use the majority vote asthe ﬁnal weak label.
entity candidates.
in this work, we treat taggingas a span labeling task as described earlier.
be-fore our learning process, we enumerate all tokenspans up to a maximum length from unlabeled dataas entity candidates.
we also notice that commonphrases (e.g., “united states”) are rarely split intodifferent entities (e.g.
“united”, “states”).
there-fore, we generate a list of common phrases usingthe unsupervised autophrase method (shang et al.,2018a) and merge two continuous spans together asa single entity candidate if they can form a commonphrase..2.3 dynamic training label selection.
after applying the learned rules to unlabeled data,some of the weakly generated labels can be incor-rect, which will lead to poor performance of ourneural tagger in the next step.
to ﬁlter out noisylabels, we propose to maintain a high-precisionentity set to keep the accurately labeled trainingexamples from each iteration..inspired by zhang et al.
(2020), we design amethod to select high-quality labels from weaklygenerated labels by seed logical rules into the high-precision set.
speciﬁcally, given an entity cate-gory i, its corresponding high-precision set hi, anda weakly labeled instance eq, we ﬁrst compute aconﬁdence score of eq belonging to category i byconsidering both its maximum pair similarity tothe high-precision set hi (called local score) andits average similarity to hi (called global score).
then, the weakly labeled instance eq will be se-lected into the high-precision set if its conﬁdencescore is larger than a threshold that is also estimatedbased on the high-precision set.
instance embedding.
we compute the embed-ding of an entity instance as the mean of the em-beddings of its tokens.
a token’s embedding iscomputed as the average of the ﬁrst three layers’.
outputs from a pre-trained language model 5.local score.
given a weakly labeled instance eqand an example ei from the high-precision set, weﬁrst compute their similarity as the cosine scorebetween their embeddings.
then, we compute thelocal conﬁdence score of eq belonging to categoryi as the maximum of its similarities between allexamples in the high-precision set.
global score.
the local score is estimated basedon a single instance in the high-precision set.
though it can help explore new entities, it can alsobe inaccurate in some cases.
therefore, we proposeto compute a more reliable score to estimate theaccuracy of an instance eq belonging to a categoryi, which is called the global score.
speciﬁcally, weﬁrst sample a small set es from the high precisionset hi and then compute the prototypical embed-ding xes of es as the average of embeddings of allinstances in es.
in our work, we sample n timesand compute the global score as:.
scoreglb.
i =.
1n.(cid:88).
1≤j≤n.
cos(xjes.
, xeq ).
(1).
to balance the exploration ability and reliability,we compute the ﬁnal conﬁdence score of a weaklylabeled instance belonging to a category as thegeometric mean of its local and global scores.
dynamic threshold estimation.
we hypothesizethat different categories of entities may have dif-ferent thresholds for selecting high-quality weaklabels.
we may also need to use different thresh-olds at different iterations to dynamically balanceexploration and reliability.
for example, we mayexpect our learning process to be reliable at ear-lier iterations and be exploratory at later stages.
motivated by this hypothesis, we propose to usea dynamic threshold to select high-quality weaklabels.
speciﬁcally, we hold out one entity instancein the high precision set and compute its conﬁdencescore with respect to the rest of the examples in thehigh-precision set.
we randomly repeat t timesand use the minimum value as the threshold.
forcategory i, it is calculated as:.
threshold = τ · min.
scorei(ek).
(2).
k≤t,ek∈hi.
where ek is the held-out entity instance and τ ∈[0, 1] is a temperature to control the ﬁnal threshold..5we used different pre-trained language models for differ-.
ent domains.
details are in section 3.1..45712.4 neural tagging model.
3 experiments.
following jiang et al.
(2020), we treat tagging as aspan labeling problem.
the key idea is to representeach span as a ﬁxed-length embedding and makepredictions based on its embedding.
brieﬂy, givena span and its corresponding sentence, we ﬁrst ini-tialize all tokens in a sentence using a pre-trainedlanguage model, and then apply a bi-lstm andself-attention layer, and obtain the contextual em-bedding of the sentence.
finally, we compute thespan embedding by concatenating two components:a content representation calculated as the weightedaverage across all token embeddings in the span,and a boundary representation that concatenatesthe embeddings at the start and end positions of thespan.
then, we predict the label of a span usinga multilayer perceptron (mlp).
for our detailedformulation please refer to appendix a.2..2.5 logical rule scoring and selection.
every iteration, we ﬁrst predict the labels of alltext spans using our neural tagging model.
then,we rank and select the 70%6 most conﬁdent spansper category based on their prediction probabilitiesfrom the tagging model as weak labels for com-puting rule scores.
we select new rules from rulecandidates based on their conﬁdence scores.
weadopt the rlogf method (thelen and riloff, 2002)to compute the conﬁdence score of a rule r:.
(3).
f (r) =.
log2(fi).
finiwhere fi is the number of spans predicted withcategory label i and matched by rule r, and niis the total number of spans matched by rule r.intuitively, this method considers both the accuracyand coverage of rules because fiis the accuracyniof the rule and log2(fi) represents the rule’s abilityto cover more spans..in our experiments, we select the top k rules foreach entity class per iteration.
we increase k by ηper iteration to be more exploratory in later itera-tions.
we also use a threshold θ of rule accuracy(i.e.
fi) to ﬁlter out noisy rules.
this method al-nilows a variety of logical rules to be considered, yetis precise enough that all logical rules are stronglyassociated with the category..6different categories and datasets may require differentthresholds to select high-quality labels.
setting a percentagemeans we will have dynamic thresholds for different cate-gories so that the model will be robust to different categoriesand domains..we ﬁrst compare our method with baselines onthree datasets and further analyze the importanceof each component in an ablation study.
we alsoreport the performance of our method with differentnumbers of seed rules and at different iterations.
finally, we show an error analysis and present auser study to analyze how many logical rules canbe used as understandable explanations..3.1 experimental setting.
we evaluate our method on the following threedatasets.
note that we use each training set withoutlabels as our unlabeled data..bc5cdr (li et al., 2016) is the biocreative vcdr task corpus.
it contains 500 train, 500 dev,and 500 test pubmed articles, with 15,953 chemi-cal and 13,318 disease entities.
chemdner (krallinger et al., 2015) contains10,000 pubmed abstracts with 84,355 chemicalentities, in which the training/dev/test set contain14,522/14,572/12,434 sentences respectively.
conll2003 (sang and meulder, 2003) con-sists of 14,041/3,250/3,453 sentences in the train-ing/dev/test set extracted from reuters news arti-cles.
we use person, location, and organizationentities in our experiments.7seed rules and parameters.
in our experiments,we set the maximum length of spans to 5, and se-lect the top k = 20 rules in the ﬁrst iteration forbc5cdr and conll2003, and k = 60 for thechemdner dataset.
since it is relatively easyfor users to manually give some highly accuratetokenstring rules (i.e., entity examples), we usetokenstring as seed rules for all experiments.
to be speciﬁc, we manually select 20 highly fre-quent tokenstring rules as seeds for bc5cdrand conll2003 and 40 for chemdner becauseof its large number of entities.
the manual seedsfor each dataset are shown in appendix a.7.
forpre-trained language models, we use bert (devlinet al., 2019) for conll2003, and scibert (beltagyet al., 2019) for bc5cdr and chemdner.
allour hyperparameters are selected on dev sets.
moresetting details are in appendix a.4..7we do not evaluate on misc category because it doesnot represent a single semantic category, which cannot berepresented with a small set of seed rules..4572methods.
bc5cdr.
chemdner.
conll2003.
precision recall.
f1.
precision recall.
f1.
precision recall.
f1.
seed ruleslinkedhmmhmm-agg.
cgexpanautonerseed rules + neural taggerself-training.
our learned rulesours w/o autophraseours w/o instance selectiontallor.
94.0910.1843.7040.9642.2278.3373.69.
79.2974.5658.7066.53.
3.8115.6021.6024.7530.6621.6029.55.
18.4632.9363.3766.94.
7.3312.3229.0030.8635.5233.8642.19.
29.9445.6860.9566.73.
91.6023.9949.6045.7066.8384.1885.06.
69.8667.7442.6463.01.
13.1910.7718.4025.5827.5921.9120.03.
21.9755.9948.2560.18.
23.0714.8626.8032.8039.0534.7832.42.
33.4361.3145.2761.56.
95.7719.7852.0055.9732.0772.5772.80.
65.5171.3758.5164.29.
2.7631.518.5028.75.9824.6824.83.
21.1225.5058.864.14.
5.3624.3014.6037.9510.0836.8337.03.
31.9437.5758.6564.22.table 2: performance of baselines (in upper section), our method and its sub-components (in lower section)..3.2 compared baseline methods.
seed rules.
we apply only seed rules to each testset directly and evaluate their performance.
cgexpan (zhang et al., 2020) is a state-of-the-art lexicon expansion method by probing a lan-guage model.
since tokenstring seed rulescan be viewed as a seed lexicon, we expand itssize to 1,000 using this method and use them astokenstring rules.
we apply the top 200, 500,800, and 1,000 rules to test sets and report the bestperformance.
autoner (shang et al., 2018b) takes lexicons oftyped terms and untyped mined phrases as input.
we use the best expanded lexicon from cgexpanas typed terms, and both of the expanded lexiconand the mined phrases from autophrase (shanget al., 2018a) as untyped mined phrases.
for de-tailed information on the autoner dictionary, re-fer to appendix a.6linkedhmm (safranchik et al., 2020) introducesa new generative model to incorporate noisy rulesas supervision and predict entities using a neuralner model.
in our experiments, we use the ex-panded lexicon by cgexpan as tagging rules andautophrase mined phrases as linking rules.
hmm-agg.
(lison et al., 2020a) proposes a hid-den markov model to ﬁrst generate weak labelsfrom labeling functions and train a sequence tag-ging model.
we convert the expanded lexicon bycgexpan to labeling functions and report resultsof the tagging model.
seed rule + neural tagger.
this method is ourframework without iteration learning.
after apply-ing seed rules, we use the weakly generated labelsto train our neural tagger and report the result ofthe tagger.
self-training.
we ﬁrst obtain weak labels by ap-.
plying seed rules.
then, we build a self-trainingsystem using the weak labels as initial supervisionand our neural tagger as the base model..methods (fries et al., 2017; ratner et al., 2017;huang and riloff, 2010) which use noun phrasesas entity candidates are not included here be-cause noun phrases have poor recall on the threedatasets as shown in table 1. cgexpan outper-forms other entity set expansion methods (e.g., yanet al.
(2019)) so we use cgexpan as our baselinefor automatic lexicon expansion..3.3 performance of compared methods.
the precision,.
we presentrecall, and micro-averaged f1 scores on three datasets in table 2.results show that our method signiﬁcantly outper-forms baseline methods obtaining an average of 24-point f1 improvement across three datasets overthe best baseline..we see that the precision of our seed rules ishigh, but the recall is lower.
the lexicon expansionmethod (cgexpan) can recognize more entities butalso introduces errors resulting in an improvementto recall but a dramatic decrease in precision..existing weakly supervised methods (i.e., au-toner, linkedhmm and hmm-agg.)
cannot rec-ognize entities effectively with either seed rulesor expanded rules by cgexpan.
these methodsrequire a high-precision lexicon as input; however,the precision of the automatically expanded lexiconis not sufﬁcient to meet this requirement.
thoughseed rules are very accurate, they lack coverage ofvarious entities..our method without iteration (seed rules + neu-ral tagger) and self-training can achieve high pre-cision because of the accurate pseudo labels gener-ated from seed rules.
it is interesting to note that.
4573(a).
(b).
(c).
figure 3: (a) iterations vs. performance of our method on bc5cdr.
(b) performance with different numbers ofseed rules.
(c) performance of autoner with different sizes of manual lexicon and our method on bc5cdr..the self-training method based on our neural taggeralso achieved low recall.
we hypothesize that thisis mainly due to the neural tagger overﬁtting thesmall set of labels from seed rules.
ablation study.
we also performed an ablationstudy to analyze the importance of some compo-nents in our framework, and report the performancein table 2 (the lower section).
results show that ourlearned rules are accurate but lack coverage.
with-out using common phrases mined by autophrase(i.e., ours w/o autophrase), our method achievesdramatically lower recall demonstrating the effec-tiveness of common phrases for improving cover-age.
without high-quality training instance selec-tion (ours w/o instance selection), the precision islower than our best method indicating the impor-tance of the instance selection step..3.4 analysis of learning iterations and seeds.
performance vs. iterations.
figure 3a shows theperformance of our method at different iterations.
we see that our method improves the recall from20% to over 60% during the learning process witha slight decease in precision, and achieves the bestf1 score after 25 iterations.
results on other twodatasets show the same trend (in appendix a.8).
performance with different numbers of seeds.
figure 3b shows the performance of our methodusing different numbers of manually selected seedrules on three datasets.
we see that our methodcan achieve continuous improvement using moreseeds.
we also notice that our method can achieveover 55% f1 on chemdner with only 10 seedsdemonstrating the effectiveness of our frameworkunder minimal supervision setting.
our methodobtains signiﬁcantly better results (around 65% f1)when using 20 seeds than 10 seeds on bc5cdrand conll indicating that 20 seeds is a reasonablestarting point for building a tagging system without.
much manual effort..3.5 comparison with distant supervision.
autoner (shang et al., 2018b) is a distantly super-vised method using a manually created lexicon assupervision.
we also compared our method to thismethod to ﬁgure out how many terms we need tomanually created for autoner to achieve similarperformance with our method.
we conducted ex-periments on bc5cdr and used only 20 seeds forour method.
for autoner, we used additional mterms from a manually created lexicon (shang et al.,2018b)8. figure 3c shows the performance withdifferent values of m. results show that autonerneeds an additional ∼ 2000 terms to achieve simi-lar performance (around 66% f1) with our method,which demonstrates that our method is effective un-der minimal supervision without access to a largemanual lexicon..3.6 analysis of rule selection strategies.
in our work, we designed three rule selection strate-gies: (1) entity type selects the top k rules for eachentity category; (2) rule type selects the top k rulesfor each logical rule type; (3) entity&rule type se-lects the top k rules for each entity category andlogical rule type.
results in table 4 show that entitytype based selection achieves the best performance..3.7 error analysis of learned logical rules.
we show the statistics of different types of ruleslearned after all iterations in table 5.9 we see thattokenstring rule is the most rule type for domain-speciﬁc datasets (bc5cdr and chemdner).
for.
8autoner authors compiled the lexicon from meshdatabase and ctd chemical and disease vocabularies, whichare manually created by experts..9tokenstr, pre, post, postag, and dep are short for token-string, prengram, postngram, postag, and dependencyrel..4574051015202530iterations2030405060708090f1precisionrecall10152025303540number of seeds455055606570f1bc5cdrchemdnerconll200305001000150020002500size of dictionary for autoner020406080f1autoneroursexamples.
predicted labels gold label.
error type: similar semantic concepts (56%).
the aim of this work is to call attention to the risk of tacrolimus use in patientswith ssc.
we recorded time to ﬁrst dysrhythmia occurrence , respective times to 25 % and50 % reduction of the heart rate ( hr ) and mean arterial pressure , and time toasystole and total amount of bupivacaine consumption.
the severity of pain due to etomidate injection , mean arterial pressure , heartrate , and adverse effects were also evaluated..error type: inaccurate boundary (20%)furthermore ameliorating effect of crocin on diazinon induced disturbedcholesterol homeostasis was studied.
pretreatment with s. virgaurea extract for 5 weeks at a dose of 250 mg / kg fol-lowed by isoproterenol injection signiﬁcantly prevented the observed alterations.
this depressive -like proﬁle induced by meth was accompanied by a markeddepletion of frontostriatal dopaminergic and serotonergic neurotransmission ,indicated by a reduction in the levels of dopamine , dopac and hva , tyrosinehydroxylase and serotonin , observed at both 3 and 49 days post - administration..error type: nested entity (20%).
early postoperative delirium incidence risk factors were then assessed throughthree different multiple regression models.
the impact of immune - mediated heparin -induced thrombocytopenia type ii(hit type ii ) as a cause of thrombocytopenia.
extensive literature search revealed multiple cases of coronary artery vasospasmsecondary to zolmitriptan , but none of the cases were associated with ts..error type: others (4%).
it is characterized by its intense urotoxic action , leading to hemorrhagic cystitis.
famotidine is a histamine h2-receptor antagonist used in inpatient settings forprevention of stress ulcers and is showing increasing popularity because of itslow cost .
it is characterized by its intense urotoxic action , leading to hemorrhagic cystitis..disease.
disease.
notentity.
notentity.
disease.
notentity.
disease.
disease.
chemical.
chemical.
chemical.
chemical.
disease.
disease.
disease.
disease.
disease.
disease.
diseasechemical.
notentitynotentity.
disease.
notentity.
table 3: gold entities are underlined, predicted entities are in red.
error type “similar semantic concepts” meansthat our rules cannot distinguish two closely related semantic concepts.
error type “inaccurate boundary” meansour rules label incorrectly about the boundaries of entities.
error type “nested entity” means the error is due tomultiple possible entities are nested.
notentity means the predicted span is not an entity..rule selection strategy.
precision recall.
rule typeentity&rule typeentity type.
57.1461.7366.53.
63.0064.9766.94.f1.
59.9363.3166.73.table 4: performance on the bc5cdr dataset withthree different rule selection strategies..rule type.
bc5cdr.
chemdner.
conll.
tokenstrpre ∧ postpre ∧ pospos ∧ postdep ∧ pos.
503 (41%)203 (17%)288 (24%)149 (12%)79 (6%).
1667 (44%)629 (17%)585 (16%)418 (11%)432 (12%).
779 (25%)956 (31%)455 (15%)438 (14%)469 (15%).
table 5: number and ratio of different type rules..the general domain task, prengram∧postngram isthe most rule type learned by our model..we also performed an error analysis on thebc5cdr dataset.
speciﬁcally, we sampled 100entities predicted incorrectly by our learned rulesand analyzed their error types.
analysis results.
show that 56% of errors are caused by an inabil-ity to distinguish closely related entity categories(chemicals vs medications), and another 20% aredue to incorrect detection of entity boundaries.
wealso notice that some spans (e.g.
“hit type ii”) andtheir sub-spans (e.g.
“hit”) are both disease enti-ties (i.e., nested entities), but only the longer onesare annotated with gold labels.
our rules some-times only predict the sub-spans as diseases, whichcontributes to 20% of the errors.
we put examplesof each error type in table 3..3.8 user study of explainable logical rules.
since our logical rules are intuitive clues for recog-nizing entities, we hypothesize that automaticallylearned rules can be used as understandable expla-nations for the predictions of entities.
therefore,we conducted a user study to ﬁnd out how manylogical rules are explainable.
speciﬁcally, we ap-plied the learned rules in bc5cdr and sampled100 entities labeled by at least one logical rule other.
4575labeled entities and sentences.
learned logical rules.
entity type.
this occlusion occurred after eaca therapy in a patient withsah and histopathological documentation of recurrent sah.
we also analyzed published and unpublished follow-up data to.
determine the risk of.
propnich in antithrombotic users with mb..3 weeks after initiation of amiodarone therapy fornoun.
although 25 mg of100mg (p=.011) and 300 mg ( p=.005)..lamivudine was slightly less effective than.
adjatrial.
nounﬁbrillation..prengram=“a patient with”∧ postngram=“and”prengram=“the risk of”∧ postag=propnprengram=“therapy for”∧ postag=adj noun.
prengram=“mg of”.
∧ postag=noun.
disease.
disease.
disease.
chemical.
these results suggest that the renal protective effects ofis dose - dependent..nounmisoprostol.
prengram=“protective effect of”.
∧ postag=noun.
chemical.
table 6: examples of learned rules and correctly labeled entities (in red) by the learned rules in bc5cdr dataset..than tokenstring10 for our user study.
some exam-ples are shown in table 6. we asked two annotatorswithout domain knowledge and one biological ex-pert to annotate whether our learned logical rulescan be understood and used as explanations for whya span is predicted as a disease or chemical.
man-ual annotation results show that the two annotatorsand the biological expert agree that 81%, 87%, and70% of the predicted entities can be explained bylogical rules, respectively..4 related work.
different types of methods have been proposed tobuild named entity tagging systems using indirector limited supervision.
distant supervision (mintzet al., 2009) is one kind of methods that have beenproposed to alleviate human effort by training mod-els using existing lexicons or knowledge bases.
recently, there have been attempts to build nersystems with distant supervision (ren et al., 2015;fries et al., 2017; giannakopoulos et al., 2017).
au-toner (shang et al., 2018b) trained a ner systemby using both typed lexicons and untyped minedphrases as supervision.
peng et al.
(2019) proposedan adapu algorithm to incorporate an incompletedictionary as supervision.
however, lexicons orknowledge bases are not always available for newdomains and tasks, especially in speciﬁc domainsand low-resource settings.
manually constructingthese lexicons is often very expensive..bootstrapping is a technique to learn modelsfrom a small set of seeds, which has been pro-posed for word sense disambiguation (yarowsky,1995) and product attribute extraction (putthivid-hya and hu, 2011).
bootstrapping methods (niuet al., 2003; huang and riloff, 2010) have been.
10we exclude tokenstring rules because they are self-.
explainable..proposed for building entity tagging systems byassuming target entities are just proper names ornoun phrases.
gupta and manning (2014) usedan improved pattern scoring method to bootstrapdomain-speciﬁc terminologies with restricted part-of-speech patterns.
however, previous works onlyfocused on disambiguating entity types by assum-ing target entities are given or just syntactic chunks.
but, as we shown earlier, target entities often do notalign well with simple syntactic chunks.
bootstrap-ping methods that can automatically detect entityboundaries and predict their types simultaneouslyare desirable in real-world applications..recently, methods have been proposed to ob-tain weak labels by manually writing labeling func-tions (bach et al., 2017).
based on this idea, sev-eral methods (safranchik et al., 2020; lison et al.,2020a) have been proposed for ner by assum-ing the availability of a sufﬁcient amount of hand-crafted labeling functions and lexicons.
however,manually designing labeling rules is challenging,which requires a signiﬁcant amount of manual ef-fort and domain expertise.
our work aims to learnlogical rules automatically to reduce human effort..5 conclusion.
in this work, we explored how to build a taggerfrom a small set of seed logical rules and unlabeleddata.
we deﬁned ﬁve types of simple logical rulesand introduced compound logical rules that arecomposed from simple rules to detect entity bound-aries and classify their types simultaneously.
wealso design a dynamic label selection method to se-lect accurate pseudo labels generated from learnedrules for training a discriminative tagging model.
experimental results demonstrate that our methodis effective and outperforms existing weakly super-vised methods..4576references.
stephen h. bach, b. he, a. ratner, and c. r´e.
2017.learning the structure of generative models withoutlabeled data.
proceedings of machine learning re-search, 70:273–82..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inemnlp/ijcnlp..j. devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidirec-tional transformers for language understanding.
innaacl-hlt..jason alan fries, sen wu, a. ratner, and christo-pher r´e.
2017. swellshark: a generative modelfor biomedical named entity recognition without la-beled data.
arxiv, abs/1704.06360..athanasios giannakopoulos, c. musat, andreea hoss-mann, and michael baeriswyl.
2017. unsupervisedaspect term extraction with b-lstm crf using auto-matically labelled datasets.
in wassa@emnlp..s. gupta and christopher d. manning.
2014. improvedpattern learning for bootstrapped entity extraction.
in conll..ruihong huang and e. riloff.
2010. inducing domain-speciﬁc semantic class taggers from (almost) noth-ing.
in acl..zhengbao jiang, w. xu, j. araki, and graham neu-big.
2020. generalizing natural language analysisthrough span-relation representations.
in acl..martin krallinger, o. rabal, florian leitner, miguelvazquez, david salgado, zhiyong lu, robert lea-man, yanan lu, dong-hong ji, d. m. lowe,r. sayle, r. batista-navarro, r. rak, torstenhuber, tim rockt¨aschel, s´ergio matos, d. cam-pos, buzhou tang, h. xu, tsendsuren munkhdalai,k. ryu, s. v. ramanan, p. s. nathan, s. zitnik,m. bajec, l. weber, matthias irmer, s. akhondi,j. kors, s. xu, x. an, utpal kumar sikdar, a. ekbal,m. yoshioka, thaer m. dieb, miji choi, karin m.verspoor, madian khabsa, c. lee giles, h. liu,k. ravikumar, andre lamurias, f. couto, hong-jie dai, r. tsai, c. ata, t. can, anabel usie,rui alves, isabel segura-bedmar, paloma mart´ınez,j. oyarz´abal, and a. valencia.
2015. the chemd-ner corpus of chemicals and drugs and its annotationprinciples.
journal of cheminformatics, 7:s2 – s2..kenton lee, luheng he, m. lewis, and luke zettle-moyer.
2017. end-to-end neural coreference resolu-tion.
in emnlp..j. li, yueping sun, robin j. johnson, daniela sci-aky, chih-hsuan wei, robert leaman, a. p. davis,c. mattingly, thomas c. wiegers, and zhiyong lu.
2016. biocreative v cdr task corpus: a resourcefor chemical disease relation extraction.
database:the journal of biological databases and curation,2016..p. lison, a. hubin, jeremy barnes, and samia touileb.
la-2020a.
belled data: a weak supervision approach.
arxiv,abs/2004.14723..named entity recognition without.
pierre lison, jeremy barnes, aliaksandr hubin, andsamia touileb.
2020b.
named entity recognitionwithout labelled data: a weak supervision approach.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages1518–1533, online.
association for computationallinguistics..m. mintz, steven bills, r. snow, and dan jurafsky.
2009. distant supervision for relation extractionwithout labeled data.
in acl/ijcnlp..cheng niu, wei li, jihong ding, and rohini k srihari.
2003. a bootstrapping approach to named entityclassiﬁcation using successive learners.
in proceed-ings of the 41st annual meeting of the associationfor computational linguistics, pages 335–342..minlong peng, xiaoyu xing, qi zhang, jinlan fu,and x. huang.
2019. distantly supervised namedentity recognition using positive-unlabeled learning.
arxiv, abs/1906.01378..jeffrey pennington, r. socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in emnlp..duangmanee putthividhya and junling hu.
2011. boot-strapped named entity recognition for product at-tribute extraction.
in proceedings of the 2011 con-ference on empirical methods in natural languageprocessing, pages 1557–1567, edinburgh, scotland,uk.
association for computational linguistics..a. ratner, stephen h. bach, henry r. ehrenberg, ja-son alan fries, sen wu, and c. r´e.
2017. snorkel:rapid training data creation with weak supervision.
proceedings of the vldb endowment.
internationalconference on very large data bases, 11 3:269–282..xiang ren, ahmed el-kishky, c. wang, fangbo tao,clare r. voss, and jiawei han.
2015. clustype:effective entity recognition and typing by relationphrase-based clustering.
kdd : proceedings.
inter-national conference on knowledge discovery datamining, 2015:995–1004..esteban safranchik, shiying luo, and stephen h. bach.
2020. weakly supervised sequence tagging fromnoisy rules.
in aaai..e. t. k. sang and f. d. meulder.
2003. introduction tothe conll-2003 shared task: language-independentnamed entity recognition.
arxiv, cs.cl/0306050..jingbo shang,.
jialu liu, meng jiang, x. ren,clare r. voss, and jiawei han.
2018a.
automatedieeephrase mining from massive text corpora.
transactions on knowledge and data engineering,30:1825–1837..4577jingbo shang, liyuan liu, x. ren, x. gu, tengren, and jiawei han.
2018b.
learning named en-tity tagger using domain-speciﬁc dictionary.
arxiv,abs/1809.03599..m. thelen and e. riloff.
2002. a bootstrappingmethod for learning semantic lexicons using extrac-tion pattern contexts.
in emnlp..lingyong yan, xianpei han, l. sun, and b. he.
2019.inlearning to bootstrap for entity set expansion.
emnlp/ijcnlp..david yarowsky.
1995. unsupervised word sense dis-ambiguation rivaling supervised methods.
in acl..yunyi zhang, jiaming shen, jingbo shang, and jiaweihan.
2020. empower entity set expansion via lan-guage model probing.
in acl..4578a appendices.
a.1 details of logical rule extraction.
in this section, we present details of the extrac-tion and the matching logic of our designed logicalrules, using the following sentence with a locationentity united states as an example..example 1..we ﬁrst obtain a parsed dependency treeofthe sentence using the spacy pipeline(en core web sm model).
then our frameworkwill generate all candidate rules for each candi-date entity.
here, we use the token span unitedstates as the target candidate entity to show howthese rules are extracted.
tokenstring.
we use the lower-case and lemma-tized tokens of an entity candidate as a tokenstringrule.
given the above example, we will extract atokenstring=“united state” rule.
prengram.
it matches preceding n tokens.
alltokens in rules will be lower cased and lemmatized.
in our experiments, we set n to 3. in example 1,we extract prengram=“the”, prengram=“to the”,and prengram=“move to the” as candidate rules.
postngram.
it matches the succeeding n tokens,which are also lower cased and lemmatized.
n isset to 3 in our experiments.
in example 1, we canextract postngram=“in”, postngram=“in 1916”,and postngram=“in 1916 .” as candidate rules.
postag.
we extract the part-of-speech tags of to-kens in a span text using the spacy pipeline.
in ex-ample 1, we can extract postag=“propn propn”as a candidate rule.
dependencyrel.
we ﬁrst ﬁnd the head word11 inthe text span.
then, we extract the governor (i.e.
head) of the head word as a dependency rule withdepth 1. in example 1, state is the head wordof text span united states.
to is the gover-nor of head word state, so dependencyrel=“to”is the dependencyrel rule with depth 1. next,all tokens dependent on the head word are con-sidered as dependencyrel rules with depth 2.in example 1, word move is logical rule withdepth 2. we use (cid:107) to connect token with depth1 and token with depth 2. finally, in example.
11for simplicity, we just used the last token as the head.
word of a token span..1, we have logical rule dependencyrel=“to” anddependencyrel=“move(cid:107)to”..the numbers of rule candidates for each datasetare: bc5cdr (108,756), chemdner (441, 595),conll2003 (142, 976)..a.2 details of neural tagger.
in this section, we present details of span represen-tation and prediction in our neural tagger.
span representation.
given a sentence x =[w1, w2, .
.
.
, wn] of n tokens, a span si =[wbi, wbi+1, .
.
.
, wei], where bi, ei are the start andend indices respectively.
the representation ofspans contains two components: a content repre-sentation zci calculated as the weighted averageacross all token embeddings in the span, and aboundary representation zui that concatenates theembeddings at the start and end positions of thespan.
speciﬁcally,.
c1, c2, .
.
.
, cn = tokenrepr(w1, w2, .
.
.
, wn),u1, u2, ..., un = bilstm(c1, c2, .
.
.
, cn),.
zci = selfattn(cbi, cbi+1, .
.
.
, cei),i = [ubi; uei], zi = [zczu.
i ; zui ],.
where tokenrepr could be non-contextualized,such as glove (pennington et al., 2014), or con-textualized, such as bert (devlin et al., 2019).
bilstm is a bi-directional lstm layer andselfattn is a self-attention layer.
for further de-tails please refer to lee et al.
(2017).
span prediction.
we predict labels for all spansup to a ﬁxed length of l words using a multilayerperceptron (mlp):.
oi = softmax(mlpspan(zi)).
(4).
where oi is prediction for the span.
we intro-duce one negative label neg as an additional labelwhich indicates invalid spans (i.e., spans that arenot named entities in the corpus)..a.3 negative instances for training.
to provide negative supervision for neural networktraining, we pre-process unlabeled data and collectall noun phrases.
token spans outside noun phrasesare used as initial negative supervision.
comparedwith previous works (ratner et al., 2017; fries et al.,2017) that directly use noun phrases as entity can-didates, in our work, noun phrases only providenegative supervision.
in the following iterations,these negative instances still have a chance to berecognized correctly..4579  stateshemovedin1916totheunitedpronverbadpnumadpdetpropnpropnpobjcondition.
label.
tokenstring(x)==“nicotine”tokenstring(x)==“morphine”tokenstring(x)==“haloperidol”tokenstring(x)==“warfarin”tokenstring(x)==“clonidine”tokenstring(x)==“creatinine”tokenstring(x)==“isoproterenol”tokenstring(x)==“cyclophosphamide”tokenstring(x)==“sirolimus”tokenstring(x)==“tacrolimus”tokenstring(x)==“proteinuria”tokenstring(x)==“esrd”tokenstring(x)==“thrombosis”tokenstring(x)==“tremor”tokenstring(x)==“hepatotoxicity”tokenstring(x)==“hypertensive”tokenstring(x)==“thrombotic”tokenstring(x)==“microangiopathy”tokenstring(x)==“thrombocytopenia”tokenstring(x)==“akathisia”.
chemical.
disease.
table 7: seed logical rules for bc5cdr dataset..implementation.
a.5we implement our framework with pytorch 1.4.012and our rule labeling is based on snorkel 0.9.513.we train our framework on nvidia quadrortx 8000 gpu.
our neural ner module has114,537,220 parameters.
it takes about 30 minutesto complete a whole iteration..a.6 dictionary for autoner.
in table 2, we used the same manual seed rulesas supervision for all experiments.
for autoner,all phrases generated from autophrase are usedas untyped phrases (i.e., full dictionary in au-toner), the sizes are: bc5cdr (6,619), chemd-ner (15,995), conll2003 (4,137).
we expandedseeds with cgexpan and used the expansion asthe typed terms for autoner (i.e., the core dictio-nary in autoner).
we experimented with differentsizes of dictionaries and reported the best results.
the sizes for the best performance are: bc5cdr(800), chemdner (500), conll2003 (1000).
we found that the performance will be lower whenwe try to use larger automatically expanded dictio-naries..a.7 seed logical rules.
in this section, we show the seeds used in experi-ments of table 2..seed logical rules for bc5dcr, conll2003and chemdner is shown in table 7, 8 and 9.
12https://pytorch.org/13https://www.snorkel.org/.
figure 4: iterations vs. performance of the neural nertagger on chemdner datasets..figure 5: iterations vs. performance of the neural nertagger on conll2003 datasets..a.4 parameters.
in our neural ner tagger, we use the adam opti-mizer with learning rate 2e−5, a dropout ratio 0.5,and a batch size of 32 for all experiments.
for bet-ter stability, we use gradient clipping of 5.0. inaddition, the maximum length of spans is 5, andprecision thresholds for rules are 0.9 for all experi-ments..in the dynamic label selection step, we set thetemperature of thresholds to 0.8, sample timesn = 50, es = 3, and the temperature τ = 0.8to control threshold.
in logical rule scoring and se-lection step, we set η = 1, and threshold θ = 0.9..in our experiments, we use scibert for twobiomedical datasets and bert for conll2003dataset.
during training, we run the frameworkfor 32 iterations for all datasets and select the bestmodel based on development sets..4580051015202530iterations20406080f1precisionrecall051015202530iterations1020304050607080f1precisionrecallcondition.
label.
tokenstring(x)==“britain”tokenstring(x)==“italy”tokenstring(x)==“russia”tokenstring(x)==“sweden“tokenstring(x)==“belgium”tokenstring(x)==“iraq”tokenstring(x)==“south africa”tokenstring(x)==“united states”tokenstring(x)==“wasim akram”tokenstring(x)==“waqar younis”tokenstring(x)==“mushtaq ahmed”tokenstring(x)==“mother teresa”tokenstring(x)==“aamir sohail”tokenstring(x)==“bill clinton”tokenstring(x)==“saeed anwar”tokenstring(x)==“osce”tokenstring(x)==“nato”tokenstring(x)==“honda”tokenstring(x)==“interfax”tokenstring(x)==“marseille”.
location.
organization.
table 8: seed logical rules for conll2003 dataset..respectively..a.8.
iterations vs. performance.
figure 4 and figure 5 show the performance vs. iter-ations on chemdner and conll 2003 dataset..person.
condition.
label.
tokenstring(x)==“glucose”tokenstring(x)==“oxygen”tokenstring(x)==“cholesterol”tokenstring(x)==“glutathione”tokenstring(x)==“ethanol”tokenstring(x)==“ca ( 2 + )”tokenstring(x)==“calcium”tokenstring(x)==“androgen”tokenstring(x)==“copper”tokenstring(x)==“graphene”tokenstring(x)==“glutamate”tokenstring(x)==“dopamine”tokenstring(x)==“cocaine”tokenstring(x)==“cadmium”tokenstring(x)==“serotonin”tokenstring(x)==“estrogen”tokenstring(x)==“nicotine”tokenstring(x)==“tyrosine”tokenstring(x)==“resveratrol”tokenstring(x)==“nitric oxide”tokenstring(x)==“cisplatin”tokenstring(x)==“alcohol”tokenstring(x)==“superoxide”tokenstring(x)==“curcumin”tokenstring(x)==“( 1 ) h”tokenstring(x)==“metformin”tokenstring(x)==“amino acid”tokenstring(x)==“arsenic”tokenstring(x)==“zinc”tokenstring(x)==“testosterone”tokenstring(x)==“ﬂavonoids”tokenstring(x)==“camp”tokenstring(x)==“methanol”tokenstring(x)==“amino acids”tokenstring(x)==“mercury”tokenstring(x)==“fatty acids”tokenstring(x)==“polyphenols”tokenstring(x)==“nmda”tokenstring(x)==“silica”tokenstring(x)==“5 - ht”.
chemical.
table 9: seed logical rules for chemdner dataset..4581