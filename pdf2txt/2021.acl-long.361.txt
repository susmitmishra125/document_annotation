element intervention for open relation extraction.
fangchao liu1,3, lingyong yan1,3, hongyu lin1,‚àó , xianpei han1,2,‚àó, le sun1,21chinese information processing laboratory 2state key laboratory of computer scienceinstitute of software, chinese academy of sciences, beijing, china3university of chinese academy of sciences, beijing, china{fangchao2017,lingyong2014,hongyu,xianpei,sunle}@iscas.ac.cn.
abstract.
open relation extraction aims to cluster rela-tion instances referring to the same underlyingrelation, which is a critical step for general re-lation extraction.
current openre models arecommonly trained on the datasets generatedfrom distant supervision, which often resultsin instability and makes the model easily col-lapsed.
in this paper, we revisit the procedureof openre from a causal view.
by formu-lating openre using a structural causal mod-el, we identify that the above-mentioned prob-lems stem from the spurious correlations fromentities and context to the relation type.
toaddress this issue, we conduct element inter-vention, which intervenes on the context andentities respectively to obtain the underlyingcausal effects of them.
we also provide twospeciÔ¨Åc implementations of the interventionsbased on entity ranking and context contrast-ing.
experimental results on unsupervised re-lation extraction datasets show that our meth-ods outperform previous state-of-the-art meth-ods and are robust across different datasets1..1.introduction.
relation extraction (re) is the task to extract re-lation between entity pair in plain text.
for ex-ample, when given the entity pair (obama, theunited states) in the sentence ‚Äúobama was swornin as the 44th president of the united states‚Äù, anre model should accurately predict the relation-ship ‚Äúpresident of‚Äù and extract the correspondingtriplet (obama, president of, the united states) fordownstream tasks.
despite the success of manyre models (zeng et al., 2014; baldini soares et al.,2019), most previous re paradigms rely on the pre-deÔ¨Åned relation types, which are always unavail-able in open domain scenario and thereby limitstheir capability in real applications..‚àócorresponding authors.
1code available at https://github.com/lfc1993/ei ore.figure 1: the structural causal model demonstratesthe procedure of openre.
(a) is the original scm; (b)entity intervention that Ô¨Åxes the entity pair and adjustsdifferent contexts; (c) context intervention that Ô¨Åxesthe context and adjusts different entity pairs..open relation extraction (openre), on the oth-er hand, has been proposed to extract relation factswithout pre-deÔ¨Åned relation types neither annotat-ed data.
given a relation instance consisting of twoentities and their context, openre aims to identifyother instances which mention the same relation.
to achieve this, openre is commonly formulatedas a clustering or pair-matching task.
therefore themost critical challenge for openre is how to learneffective representations for relation instances andthen cluster them.
to this end, yao et al.
(2011)adopts topic model (blei et al., 2003) to generatelatent relation type for unlabelled instances.
lat-er works start to utilize datasets collected usingdistant supervision for model training.
along thisline, marcheggiani and titov (2016) utilizes anauto-encoder model and trains the model throughself-supervised signals from entity link predictor.
hu et al.
(2020) encodes each instance with pre-trained language model (devlin et al., 2019; baldi-ni soares et al., 2019) and learn the representationby self-supervised signals from pseudo labels..unfortunately, current openre models are oftenunstable and easily collapsed (simon et al., 2019)..pecxpecx(a)(b)(c)ùëëùëú(ùê∏)prelation prototypeeentity pairccontextxrelation instanceyypecxyùëëùëú(ùê∂)for example, openre models frequently clusterall relation instances with context ‚Äúwas born in‚Äùinto the relation type born in place becausethey share similar context information.
howev-er, ‚Äúwas born in‚Äù can also refer to the relationborn in time.
furthermore, current models alsotend to cluster two relation instances with the sameentities (i.e., relation instances with the same headand tail entities) or the same entity types into onerelation.
this problem can be even more severe ifthe dataset is generated using distant supervisionbecause it severely relies on prototypical contextand entity information as supervision signals andtherefore lacks of diversity..in this paper, we attempt to explain and resolvethe above-mentioned problem in openre from acausal view.
speciÔ¨Åcally, we formulate the pro-cess of openre using a structural causal mod-el (scm) (pearl, 2009), as shown in figure 1. themain assumption behind the scm is that distantsupervision will generate highly correlated relationinstances to the original prototypical instance, andthere is a strong connection between the generatedinstance to the prototypical instance through eithertheir entities or their context.
for example, ‚Äù[jobs]was born in [california]‚Äù and ‚Äù[jobs] was born in[1955]‚Äù are highly correlated because they sharesimilar context ‚Äúwas born in‚Äù and entity ‚Äújobs‚Äù.
such connection will result in spurious correlation-s, which appear in the form of the backdoor pathsin the scm.
then the spurious correlations willmislead openre models, which are trained to cap-ture the connection between entities and context tothe relation type..based on the above observations, we proposeelement intervention, which conducts backdoor ad-justment on entities and context respectively toblock the backdoor paths.
however, due to thelack of supervision signals, we cannot directly op-timize towards the underlying causal effects.
tothis end, we further propose two surrogate imple-mentations on the adjustments on context and en-tities, respectively.
speciÔ¨Åcally, we regard the in-stances in the original datasets as the relation pro-totypes.
then we implement the adjustment oncontext through a hierarchy-based entity rank-ing (hyber), which Ô¨Åxes the context, samples re-lated entities from an entity hierarchy tree andlearns the causal relation through rank-based learn-ing.
besides, we implement the adjustment onentities through a generation-based context con-.
trasting (gcc), which Ô¨Åxes the entities, generatespositive and negative contexts from a generation-based model and learns the causal effects throughcontrastive learning..we conduct experiments on different unsuper-vised relation extraction datasets.
experimentalresults show that our method outperforms previ-ous state-of-the-art methods with a large marginand suffers much less performance discrepancy be-tween different datasets, which demonstrate theeffectiveness and robustness of the proposed meth-ods..2 openre from causal view.
in this section, we formulate openre from theperspective of structural causal model and givethe theoretical proof for intervention methodsthat block the backdoor paths from relation ele-ments (i.e., context and entity pair) to the latentrelation types..2.1 task deÔ¨Ånition.
relation extraction (re) is the task of extract-ing the relationship between two given entitiesin the context.
considering the sequence exam-ple: s = [s0, ..., sn‚àí1] which contains n words,e1 = [i, j] and e2 = [k, l] indicate the entity pair,where 0 ‚â§ i ‚â§ j < k ‚â§ l ‚â§ n ‚àí 1, a relationinstance x is deÔ¨Åned as x = (s, e1, e2), (i.e.
thetuple of entity pair and the corresponding context).
the element of a relation instance is the entity pairand the corresponding context.
traditional re taskis to predict the relations type when given x. how-ever, the target relation types are not pre-deÔ¨Ånedin openre.
consequently, openre is commonlyformulated as a clustering task or a pair-matchingtask by considering whether two relation instancesxi and xj refer to the same relation..unfortunately, current openre models are oftenunstable and easily collapsed (simon et al., 2019).
in the next section, we formulate openre using astructural causal model and then identify the rea-sons behind these deÔ¨Åciencies from the scm..2.2 structural causal model for openre.
figure 1 (a) shows the structural causal model foropenre.
the main idea behind the scm is distantsupervision will generate highly correlated relationinstances to the original prototypical instance, andthere is a strong connection between the generat-ed instance to the prototypical instance through.
figure 2: framework of element intervention..either their entities or their context.
speciÔ¨Åcally, inthe scm, we describe openre with Ô¨Åve criticalvariables: 1) the prototypical relation instance p ,which is a representative relation instance of onerelation type cluster; 2) the entity pair e, whichencodes the entity information of one relation in-stance; 3) the context c, which encodes the contextinformation of one relation instance; 4) a relationinstance x (which can be generated from distan-t supervision or other strategies) and 5) the Ô¨Ånalpair-wise matching result y , which corresponds towhether instance x and the prototypical relationinstance p entail the same relation..given the variables mentioned above, we formu-late the process of generating openre instancesbased on the following causal relations:.
‚Ä¢ e ‚Üê p ‚Üí c formulates the process of sam-pling related entities and context respectivelyfrom the prototypical relation instance p ..‚Ä¢ e ‚Üí x ‚Üê c formulates the relation instancegenerating process.
given the context c andentities e from the prototypical relation in-stance p , a new relation instance x is gen-erated based on the information in c and e.this process can be conducted through distantsupervision..‚Ä¢ p ‚Üí y ‚Üê x formulates the openre clus-tering or pair-wise matching process.
given aprototypical relation instance p and anotherrelation instance x, this process will deter-mine whether x belongs to the relation clusterof p ..2.3 spurious correlations in openre.
given a relation prototypical instance p , the learn-ing process of openre is commonly to maximizethe probability p(y, p |x) = p(y, p |e, c).
how-ever, as it can be observed from the scm, thereexists a backdoor path p ‚Üí e ‚Üí x when welearn the underlying effects of context c. that isto say, the learned effect of c to y is confoundedby e (through p ).
for example, when we learnedthe effects of context ‚Äúwas born in‚Äù to the relation‚Äúborn in place‚Äù, the backdoor path will leadthe model to mistake the contribution of the entities(person, place) to the contribution of context,and therefore resulted in spurious correlation.
thesame thing happens when we learn the effects ofentities e, which is inÔ¨Çuenced by the backdoorpath p ‚Üí c ‚Üí x. as a result, optimizing thesespurious correlations will result in an unstable andcollapsed openre model..2.4 resolving spurious correlations via.
element intervention.
to resolve the spurious correlations, we adopt thebackdoor adjustment (pearl, 2009) to block thebackdoor paths.
speciÔ¨Åcally, we separately inter-vene on context c and entities e by applying thedo‚Äìoperation..entity intervention.
as shown in figure 1 (b),to avoid the spurious correlations of entities to re-lation types, we conduct the do-operation by inter-.
hugo was born in [paris], [france].samplesamplehugo was born in [aube], [france].hugo was born in [occitanie], [france].hugo was born in [ùüèùüóùê≠ùê°century], [france].rank0123contrastive loss<h> paris <r> locate in <t> france<h> paris <r> in the country of <t> france<h> paris <r> locate in <t> france <h> paris <r> birth place <t> hugo <h> paris <r> capital <t> franceencodergenerator[paris] is located in [france].
[paris] is in the country of [france].
[paris] is located in [france], where is the birth place of hugo.encoderlabelanchorppnranking loss[paris] is the capital of [france].paris1 sibling2 cousin3 otheroriginal tripletrenaming tripletexpansion tripletreplacing tripletvening on the entities e:p(y, p |do(e = e0))(cid:88).
p(c, p )p(x, y |e0, c, p ).
p(c, p )p(y |e0, c, p ).
(1).
p(p )p(c|p )p(y |e0, c, p ).
=.
=.
=.
c,x(cid:88).
c(cid:88).
c.since p(p ) is uniformly distributed in the realworld, this equation can be rewritten as:.
p(y, p |do(e = e0))(cid:88).
‚àù.
p(c|p )p(y |e0, c, p ).
(2).
c.e.this equation means the causal effect from the enti-ties e to its matching result y can be estimated byconsidering the corresponding possibility of eachcontext given the prototypical relation instance p .
the detailed implementation will be described inthe next section..context intervention.
similarly, we conductcontext intervention to avoid the spurious corre-lations of context to relation types, as shown infigure 1 (c):.
p(y, p |do(c = c0))(cid:88).
‚àù.
p(e|p )p(y |c0, e, p ).
(3).
which means the causal effect from the contextc to its matching result y can be estimated byconsidering the corresponding possibility of eachentity e given p .
the detailed implementationwill also be described in the next section..2.5 optimizing causal effects for openre.
to effectively capture the causal effects of entitiese and context c to openre, a matching modelp(y |c, e, p ; Œ∏) should be learned by optimizingthe causal effects:.
l(Œ∏) =i(x, p ) ¬∑ p(y = 1, p |do(e = e(x)).
+ i(x, p ) ¬∑ p(y = 1, p |do(c = c(x))+ [1 ‚àí i(x, p )] ¬∑ p(y = 0, p |do(e = e(x))+ [1 ‚àí i(x, p )] ¬∑ p(y = 0, p |do(c = c(x)).
(4)where e(x) and c(x) represents the entities andcontext in relation instance x, i(x, p ) is an in-dicator which represents whether x and p be-long to the same relation.
p(y |c, e, p ; Œ∏) =p(y |x, p ; Œ∏) is a matching model, which is de-Ô¨Åned using a prototype-based measurements:.
p(y |x, p ; Œ∏) ‚àù ‚àíd(r(x; Œ∏), r(p ; Œ∏)).
(5).
where d is a distance measurement and r(x; Œ∏)is a representation learning model parametrizedby Œ∏, which needs to be optimized during learn-in the following, we will use d(x, p ) =ing.
d(r(x; Œ∏), r(p ; Œ∏)) for short..however, it is difÔ¨Åcult to directly optimize theabove loss function because 1) in unsupervisedopenre, we are unable to know whether the rela-tion instance x generated from (e, c) matches theprototypical relation instance p ; 2) we are unableto traverse all possible e and c in equation (2) and(3).
to resolve these problems, in the next section,we will describe how we implement the contextintervention via hierarchy-based entity ranking andthe entity intervention via generation-based contextcontrasting..3 element intervention implementation.
as we mentioned above, it is difÔ¨Åcult to directlyoptimize the causal effects via equation (4).
totackle this issue, this section provides a detailedimplementation to approximate the causal effect-s. speciÔ¨Åcally, we regard all relation instancesin the original data as the prototypical relation in-stance p , and then generate highly correlated re-lation instances x from p via a hierarchy-basedsampling and generation-based contrasting.
thenwe regard structural signals from the entity hier-archy and conÔ¨Ådence score from the generator asdistant supervision signals, and learn the causaleffects via ranking-based learning and contrastivelearning..3.1 hierarchy-based entity ranking for.
context intervention.
to implement context intervention, we propose toformulate p(e|p ) using an entity hierarchy, andapproximately learn to optimize the causal effectsof p(y = 1, p |do(c)) and p(y = 0, p |do(c))in equation (4) via a hierarchy-based entity rank-ing loss.
speciÔ¨Åcally, we Ô¨Årst regard all relation in-stances in the data as prototypical relation instancep .
then we formulate the distribution p(e|p ) byÔ¨Åxing the context in p and replacing entities bysampling from an entity hierarchy.
each sampledentity is regarded as the same p(e|p ).
intuitively,the entity closer to the original entities in p tend-s to generate more consistent relation instance top .
to approximate this semantic similarity, weutilize the meta-information in wikidata (i.e., the‚Äúinstance of‚Äù and ‚Äúsubclass of‚Äù statements, which.
describe the basic property and concept of each en-tity), and construct a hierarchical entity tree forranking the similarity between entities.
in thiswork, we apply a three-level hierarchy throughthese two statements:.
‚Ä¢ sibling entities: the entities belonging to thesame parent category as the original entity.
forexample, ‚Äúaube‚Äù and ‚Äúparis‚Äù are sibling entitiessince they are both the child entity of ‚Äúdepart-ment of france‚Äù, and both express the conceptsof location and gpe.
these sibling entities canbe considered as golden entities to replace..‚Ä¢ cousin entities: the entities belonging to thesame grandparent category but the different par-ent category from the original entity.
for ex-ample, ‚Äúoccitanie‚Äù and ‚Äúparis‚Äù is of the samegrandparent category ‚Äúfrench administrativedivision‚Äù, but shares different parent category.
these entities can be considered as silver entitiessince they are likely to be the same type as theoriginal one but less possible than the siblingentities..‚Ä¢ other entities: the entities beyond the grand-parent category, which are much less likely to bethe same type as the original one..for the example in figure 2, the prototypical rela-tion instance ‚Äúhugo was born in [paris], [france]‚Äùis sampled to be intervened.
we Ô¨Årst Ô¨Åx the contextand randomly choose one of the head or tail entityto be replaced.
in this case, we choose ‚Äùparis‚Äù.
then, entities that correspond to different hierar-chies are sampled and to replace the original entity.
in this case, ‚Äúaube‚Äù is sampled as the sibling enti-ty, ‚Äúoccitanie‚Äù to be the cousin entity and ‚Äú19thcentury‚Äù to be the other entity..after sampled these intervened instances, we ap-proximately optimize p(y, p |do(c)) using a rank-based loss function:.
le(Œ∏; x ) =.
max(0,.n‚àí1(cid:88).
i=1.
d(p, xi) ‚àí d(p, xi+1) + me),(6)where Œ∏ is the model parameters, d(xi, p ) is thedistances between representations of generated rela-tion instance xi and prototypical relation instancep .
x is the intervened relation instance set, me isthe margin for entity ranking loss, and n = 3 is thedepth of the entity hierarchy..3.2 generation-based context contrasting.
for entity intervention.
different from the context intervention that can eas-ily replace entities, it is more difÔ¨Åcult to interveneon entities and modify the context.
fortunately,the rapid progress in pre-trained language mod-el (radford et al., 2019; lewis et al., 2020; raffelet al., 2020) makes the language generation fromrdf data2 available (ribeiro et al., 2020).
so inthis work, we take a different paradigm namedgeneration-based context contrasting, which di-rectly generates different relation instances fromspeciÔ¨Åcally designed relation triplets, and approx-imately learn to optimize the causal effects ofp(y = 1, p |do(e)) and p(y = 0, p |do(e)) inequation (4) via contrastive learning.
speciÔ¨Åcally,we Ô¨Årst sample relation triplets from wikidata asprototypical relation instance p , and then generatesrelation triplets with the same entities but differentrelation context using the following strategies:.
‚Ä¢ relation renaming, which contains the sameentity pair with the original one, but an aliasrelation name for generating a sentence with dif-ferent expressions.
then this instance is consid-ered as a positive sample to prototypical relationinstance..‚Ä¢ context expansion, which extends the originalrelation instance with an additional triplet.
theadded triplet owns the same head/tail entity withthe original instance but differs in the relationand tail/head entity.
this variety aims to add ir-relative context, which forces the model to focuson the important part of the context and is alsoconsidered as a positive sample to prototypicalrelation instance..‚Ä¢ relation replacing, which contains the sameentity pair as the original one, but with otherrelations between these two entities.
this varietyaims to avoid spurious correlations that extractsonly based on the entity pair and is consideredas a negative instance to the prototypical relationinstance..then we use the generator to generate texts basedon these triplets.
speciÔ¨Åcally, we Ô¨Årst wrap thetriplets with special markers ‚Äú[h], [t],[ r]‚Äù corre-sponds to head entity, tail entity, and relation name.
then we input the concatenated texts for relationinstance generation.
in our implementation, we.
2https://www.w3.org/tr/wd-rdf-syntax-971002/.
use t5 (raffel et al., 2020; ribeiro et al., 2020) asthe base generator, and pre-train the generator onwebnlg data (gardent et al., 2017).
after sam-pled these intervened instances, we approximatelyoptimize p(y, p |do(e)) using the following con-trastive loss function:.
lc(Œ∏; x ) =.
max(d(p, xp).
(cid:88).
(cid:88).
xp‚ààp.
xn‚ààn.
‚àíd(p, xn) + mc, 0),.
(7)where Œ∏ is the model parameters, x is the inter-vened instance set, p is the positive instance setgenerated from relation renaming and context ex-pansion, n is the negative instance set generatedfrom relation replacing, p is the original prototypi-cal relation instance, mc is the margin..3.3 surrogate loss for optimizing causal.
effects.
based on entity ranking and context contrasting, weapproximate the causal effects optimized in equa-tion (4) with the following ranking and contrastiveloss:.
(8).
l(Œ∏; x ) = le(Œ∏; x ) + lc(Œ∏; x ).
which involves both the entity ranking loss and thecontext contrastive loss.
during inference, we Ô¨Årstencode each instance into its representation usingthe learned model.
then we apply a clustering al-gorithm to cluster the relation representations, andthe relation for each instance is predicted throughthe clustering results..4 experiments.
4.1 dataset.
we conduct experiments on two openre datasets‚Äì t-rex spo and t-rex ds, since these datasetsare from the same data source but only differ inconstructing settings, which is very suitable for e-valuating the stability of openre methods.
thesedatasets are both from t-rex3 (elsahar et al., 2018)‚Äì a dataset consists of wikipedia sentences that aredistantly aligned with wikidata relation triplets;and these aligned sentences are further collected ast-rex spo and t-rex ds according to whetherthey have surface-form relations or not.
as a result,t-rex spo contains 763,000 sentences of 615 re-lations, and t-rex ds contains nearly 12 millionsentences of 1189 relations.
for both datasets, we.
3https://hadyelsahar.github.io/t-rex/.
use 20% for validation and the remaining for modeltraining as hu et al.
(2020)..4.2 baseline and evaluation metrics.
baseline methods.
we compare our model withthe following baselines: 1) rel-lda (yao et al.,2011), a generative model that considers the un-supervised relation extraction as a topic model.
we choose the full rel-lda with a total numberof 8 features for comparison in our experiment.
2) march (marcheggiani and titov, 2016), a vae-based model learned by self-supervised signal ofentity link predictor.
3) uie (simon et al., 2019), adiscriminative model that adopts additional regular-ization to guide model learning.
and it has differ-ent versions according to the choices of differentrelation encoding models (e.g., pcnn).
we reportthe results of two versions‚Äìuie-pcnn and uie-bert (i.e., using pcnn and bert as the relationencoding models) with the highest performance.
4) selfore (hu et al., 2020), a self-supervisedframework that bootstraps to learn a contextual re-lation representation through adaptive clusteringand pseudo label..evaluation metrics.
we adopt three commonly-used metrics to evaluate different methods:b3 (bagga and baldwin, 1998), v-measure (rosen-berg and hirschberg, 2007) and adjusted randindex (ari) (hubert and arabie, 1985)..speciÔ¨Åcally, b3 contains the precision and recallmetrics to correspondingly measure the correct rateof putting each sentence in its cluster or clusteringall samples into a single class, which are deÔ¨Åned asfollows:b3.
p (g(x) = g(y )|c(x) = c(y )).
prec.
= ex,yrec.
= ex,y.
b3.
p (c(x) = c(y )|g(x) = g(y )).
then b3 f1 is computed as the harmonic mean ofthe precision and recall..similar to b3, v-measure focuses more on smallimpurities in a relatively ‚Äúpure‚Äù cluster than less‚Äúpure‚Äù cluster, and use the homogeneity and com-pleteness metrics:.
vhomo.
=1 ‚àí h(c(x)|g(x))/h(c(x))vcomp.
=1 ‚àí h(g(x)|c(x))/h(g(x))ari is a normalization of the rand index, whichmeasures the agreement degree between the clus-ter and golden distribution.
this metric rangesin [-1,1], a more accurate cluster will get a high-er score.
different from previous metrics, ari is.
dataset.
model.
t-rex spo.
t-rex ds.
rel-lda-full (yao et al., 2011)‚àómarch (marcheggiani and titov, 2016)‚àóuie-pcnn (simon et al., 2019)uie-bert (simon et al., 2019)selfore (hu et al., 2020)ourw/o hyberw/o gccrel-lda-full (yao et al., 2011)‚àómarch (marcheggiani and titov, 2016)‚àóuie-pcnn (simon et al., 2019)uie-bert (simon et al., 2019)selfore (hu et al., 2020)ourw/o hyberw/o gcc.
b3prec.
rec.
26.114.331.320.650.328.450.330.742.839.443.446.742.040.940.444.226.68.315.56.433.414.030.817.636.829.745.940.242.739.242.940.1.v-measuref1 homo.
comp.
24.519.430.623.653.641.440.839.142.541.445.345.245.243.745.745.223.517.07.95.736.826.638.331.235.132.447.347.843.643.045.645.2.
16.119.133.737.640.345.442.344.713.34.520.826.330.146.942.544.8.ari.
8.612.621.323.533.736.633.234.73.41.99.412.320.125.022.421.7.f118.524.836.338.141.045.041.442.212.79.019.722.432.942.940.941.5.table 1: results (%) on unsupervised relation extraction datasets.
the results of * are reproduced in simon et al.
(2019), hyber refers to our hierarchy-based entity ranking methods and gcc refers to generation-based contextcontrasting method..less sensitive to precision/homogeneity and recal-l/completeness..4.3 hyperparameters and implementation.
details.
in the training period, we manually search the hy-perparameters of learning rate in [5e-6,1e-5, 5e-5],and Ô¨Ånd 1e-5 is optimal, search weight decay in[1e-6, 3e-6, 5e-5] and choose 3e-6, and use otherhyperparameters without search: the dropout rateof 0.6, a batch size of 32, and a linear learningschedule with a 0.85 decay rate per 1000 mini-batches.
in the evaluation period, we simply adoptthe pre-trained models for representation extrac-tion, then cluster the evaluate instances based onthese representations.
for clustering, we followprevious work (simon et al., 2019; hu et al., 2020)and set k=10 as the number of clusters.
the train-ing period of each epoch costs about one day.
inour implementation, we adopt bert-base-uncasedmodel 4 as the base model for relation extractionand a modiÔ¨Åed t5-base model 5 for text generation.
the entity hierarchical tree is constructed basedon wikidata and Ô¨Ånally contains 589,121 entities.
the generation set contains about 530,000 triplets,and each triplet corresponds to 5 positive/negativetriplets and generated texts.
we use one titan rtxfor element intervention training and four cards ofrtx for text generation..4https://github.com/huggingface/transformers5https://github.com/ukplab/plms-graph2text.
source.
b3t-rex spo 45.046.0generated.
v-meas.
ari36.636.7.
45.344.6.table 2: the results (%) of entity ranking based on d-ifferent data sources.
these results are reported on t-rex spo.
and we only report the f1 scores of b3 andv-measure for simplicity..4.4 overall results.
table 1 shows the overall results on t-rex spoand t-rex ds.
from this table, we can see that:.
1. our method.
performance.
outperforms.
previousopenre models and achieves the newstate-of-the-art performance.
comparingwith all baseline models, our method achievessigniÔ¨Åcantimprovements:on t-rex spo, our method improves thesota b3 f1 and v-measure f1 by at least3.9%, and ari by 2.9%; on t-rex ds, theimprovements are more evident, where sotab3 f1 and v-measure f1 are improved by atleast 10.0%, and ari is improved by 4.9%..2. our methods perform robustly in differen-t datasets.
comparing the performances onthese two datasets, we can see that almost allbaseline methods suffer dramatic performancedrops on all these metrics, which veriÔ¨Åes thatprevious openre methods can be easily inÔ¨Çu-enced by the spurious correlations in datasets,as t-rex ds involves much more noisy in-stances without relation surface forms.
as.
metrics both seen unseenbleu 60.9chrf++76.0.
54.972.5.
65.979.2.table 3: quantitative performance of our generator onwebnlg.
seen stands for generating from seen rela-tion triplets, unseen stands for generating from unseenrelation triplets.
both stands for a combination of seenand unseen relation triplets..contrast, our methods have marginal perfor-mance differences, which indicates both theeffectiveness and robustness of our methods..4.5 detailed analysis.
in this section, we conduct several experiments fordetailed analysis of our method..ablation study.
to study the effect of differentintervention modules, we conduct an ablation studyon each intervention module by correspondinglyablating one.
the other setting remains the sameas the main model.
from table 1, we can see that,in both t-rex spo and ds, combining these t-wo modules can result in a noticeable performancegain, which demonstrates that both two modulesare important to the Ô¨Ånal model performance andthey are complementary on alleviating unnecessaryco-dependencies: hyber aims to alleviate the spuri-ous correlations between the context and the Ô¨Ånalrelation prediction, and gcc aims to alleviate thespurious correlations between entity pair and theÔ¨Ånal relation prediction.
besides, in t-rex ds, wecan see that hyber or gcc only is effective enoughto outperform previous sota methods, which indi-cates that element intervention has clearly unbiasedrepresentation on either entity pair or context..entity ranking on generated texts.
this ex-periment studies the effect of different data sourcesfor hyber module.
as shown in table 2, we can seethat hyber based on t-rex spo dataset or the gen-erated texts has marginal difference.
that meanshyber is robust to the source context.
on the otherhand, the quality of the generated texts satisÔ¨Åes thedemand of this task..quality of context generation(unseen relation-s).
this experiment gives a quantitative analy-sis of the generator used in our work.
we selectwebnlg (gardent et al., 2017) to test the gener-ator, and adopt the widely-used metrics includingbleu (papineni et al., 2002) and chrf++ (popovi¬¥c,2017) for evaluation.
as shown in table 3, we can.
figure 3: visualization of relation representationlearned by element intervention.
each relation instanceis colored with the ground-truth label..see that our generator is quite effective on seen re-lation generation.
though the generator suffers aperformance drop in unseen relations, the scoresare still receptible.
combined with results fromother experiments, the generator is sufÔ¨Åcient forthis task..visualization of relation representations.
inthis experiment, we visual the representations ofthe validation instances.
we sample 10 relationsfrom the t-rex spo validation set and each rela-tion with 200 instances for visualization.
to reducethe dimension, we use t-sne (van der maaten andhinton, 2008) to map each representation to the di-mension of 2. for the convenience of comparison,we color each instance with its ground-truth rela-tion label.
since the visualization results of onlyhyber or gcc are marginally different from the fullmodel, so we only choose the full model for visual-ization.
as shown in figure 3, we can see that eachrelation is mostly separate from others.
however,there still be some instances misclassiÔ¨Åed due tothe overlapping in the representation space..5 related work.
current success of supervised relation extractionmethods (bunescu and mooney, 2005; qian et al.,2008; zeng et al., 2014; zhou et al., 2016; velikoviet al., 2018) depends heavily on large amount ofannotated data.
due to this data bottleneck, someweakly-supervised methods are proposed to learnrelation extraction models from distantly labeleddatasets (mintz et al., 2009; hoffmann et al., 2011;lin et al., 2016) or few-shot datasets (han et al.,2018; baldini soares et al., 2019; peng et al., 2020).
however, these paradigms still require pre-deÔ¨Åned.
relation types and therefore restricts their applica-tion to open scenarios..open relation extraction, on the other hand, aimsto cluster relation instances referring to the sameunderlying relation without pre-deÔ¨Åned relationtypes.
previous methods for openre can be rough-ly divided into two categories.
the generativemethod (yao et al., 2011) formulates openre us-ing a topic model, and the latent relations are gen-erated based on the hand-crafted feature represen-tations of entities and context.
while the discrimi-native method is Ô¨Årst proposed by marcheggianiand titov (2016), which learns the model throughthe self-supervised signal from entity link predictor.
along this line, hu et al.
(2020) propose the self-ore that learns the model through pseudo labeland bootstrapping technology.
however, simonet al.
(2019) point out that previous openre meth-ods severely suffer from the instability, and theyalso propose two regularizers to guide the learn-ing procedure.
but the fundamental cause of theinstability is still undiscovered.
in this paper, we revisit.
the procedure ofopenre from a causal view.
by formulatingopenre using a structural causal model, we iden-tify the cause of the above-mentioned problems,and alleviate the problems by element intervention.
there are also some recent studies try to introducecausal theory to explain the spurious correlationsin neural models (feng et al., 2018; gururanganet al., 2018; tang et al., 2020; qi et al., 2020; zenget al., 2020; wu et al., 2020; qin et al., 2020; fuet al., 2020).
however, to the best of our knowl-edge, this is the Ô¨Årst work to revisit openre fromthe perspective of causality..6 conclusions.
in this paper, we revisit openre from the perspec-tive of causal theory.
we Ô¨Ånd that the strong con-nections between the generated instance to the pro-totypical instance through either their entities ortheir context will result in spurious correlations,which appear in the form of the backdoor pathsin the scm.
then the spurious correlations willmislead openre models.
based on the observa-tions, we propose element intervention to block thebackdoor paths, which intervenes on the contextand entities respectively to obtain the underlyingcausal effects of them.
we also provide two specif-ic implementations of the interventions based onentity ranking and context contrasting.
experimen-.
tal results on two openre datasets show that ourmethods outperform previous methods with a largemargin, and suffer the least performance discrep-ancy between datasets, which indicates both theeffectiveness and stability of our methods..acknowledgements.
we thank all reviewers for their insightful sugges-tions.
moreover, this work is supported by thenational key research and development programof china under grant no.2019yfc1521200, thenational natural science foundation of china un-der grants no.
u1936207 and 61772505, and inpart by the youth innovation promotion associa-tion cas(2018141)..references.
amit bagga and breck baldwin.
1998. entity-basedcross-document coreferencing using the vector s-pace model.
in 36th annual meeting of the associ-ation for computational linguistics and 17th inter-national conference on computational linguistics,volume 1, pages 79‚Äì85, montreal, quebec, canada.
association for computational linguistics..livio baldini soares, nicholas fitzgerald, jeffreyling, and tom kwiatkowski.
2019. matching theblanks: distributional similarity for relation learn-in proceedings of the 57th annual meetinging.
of the association for computational linguistics,pages 2895‚Äì2905, florence, italy.
association forcomputational linguistics..david m. blei, andrew y. ng, and michael i. jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3:993‚Äì1022..razvan bunescu and raymond mooney.
2005. ashortest path dependency kernel for relation extrac-tion.
in proceedings of human language technolo-gy conference and conference on empirical meth-ods in natural language processing, pages 724‚Äì731, vancouver, british columbia, canada.
associ-ation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171‚Äì4186, minneapolis, minnesota.
associ-ation for computational linguistics..hady elsahar, pavlos vougiouklis, arslen remaci,christophe gravier,jonathon hare, frederiquelaforest, and elena simperl.
2018. t-rex: a largescale alignment of natural language with knowledge.
in proceedings of the eleventh inter-base triples.
national conference on language resources and e-valuation (lrec 2018), miyazaki, japan.
europeanlanguage resources association (elra)..shi feng, eric wallace, alvin grissom ii, mohit iyy-er, pedro rodriguez, and jordan boyd-graber.
2018.pathologies of neural models make interpretations d-ifÔ¨Åcult.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 3719‚Äì3728, brussels, belgium.
associationfor computational linguistics..l. hubert and p. arabie.
1985. comparing partitions..journal of classiÔ¨Åcation, 2(1):193‚Äì218..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871‚Äì7880, online.
associationfor computational linguistics..tsu-jui fu, xin wang, scott grafton, miguel eckstein,and william yang wang.
2020. sscr: iterativelanguage-based image editing via self-supervisedin proceedings of thecounterfactual reasoning.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 4413‚Äì4422,online.
association for computational linguistics..yankai lin, shiqi shen, zhiyuan liu, huanbo luan,and maosong sun.
2016. neural relation extractionwith selective attention over instances.
in proceed-ings of the 54th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 2124‚Äì2133, berlin, germany.
associa-tion for computational linguistics..claire gardent, anastasia shimorina, shashi narayan,and laura perez-beltrachini.
2017. the webnl-g challenge: generating text from rdf data.
inproceedings of the 10th international conference onnatural language generation, pages 124‚Äì133, san-tiago de compostela, spain.
association for compu-tational linguistics..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah a.smith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107‚Äì112, new orleans, louisiana.
associa-tion for computational linguistics..xu han, hao zhu, pengfei yu, ziyun wang, yuanyao, zhiyuan liu, and maosong sun.
2018. fewrel:a large-scale supervised few-shot relation classiÔ¨Åca-tion dataset with state-of-the-art evaluation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4803‚Äì4809, brussels, belgium.
association for computa-tional linguistics..raphael hoffmann, congle zhang, xiao ling, lukezettlemoyer, and daniel s. weld.
2011. knowledge-based weak supervision for information extractionof overlapping relations.
in proceedings of the 49thannual meeting of the association for computa-tional linguistics: human language technologies,pages 541‚Äì550, portland, oregon, usa.
associa-tion for computational linguistics..xuming hu, lijie wen, yusong xu, chenwei zhang,and philip yu.
2020. selfore: self-supervised re-lational feature learning for open relation extraction.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3673‚Äì3682, online.
association for computa-tional linguistics..laurens van der maaten and geoffrey hinton.
2008.visualizing data using t-sne.
journal of machinelearning research, 9:2579‚Äì2605..diego marcheggiani and ivan titov.
2016. discrete-state variational autoencoders for joint discovery andfactorization of relations.
transactions of the asso-ciation for computational linguistics, 4:231‚Äì244..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation extrac-tion without labeled data.
in proceedings of the join-t conference of the 47th annual meeting of the acland the 4th international joint conference on natu-ral language processing of the afnlp, pages 1003‚Äì1011, suntec, singapore.
association for computa-tional linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic e-valuation of machine translation.
in proceedings ofthe 40th annual meeting of the association for com-putational linguistics, pages 311‚Äì318, philadelphia,pennsylvania, usa.
association for computationallinguistics..judea pearl.
2009. causality: models, reasoning, and.
inference.
cambridge university press..hao peng, tianyu gao, xu han, yankai lin, pengli, zhiyuan liu, maosong sun, and jie zhou.
2020.learning from context or names?
an empiricalin proceed-study on neural relation extraction.
ings of the 2020 conference on empirical method-s in natural language processing (emnlp), pages3661‚Äì3672, online.
association for computationallinguistics..maja popovi¬¥c.
2017. chrf++: words helping charac-in proceedings of the second con-ter n-grams.
ference on machine translation, pages 612‚Äì618,copenhagen, denmark.
association for computa-tional linguistics..2018. graph attention networks.
conference on learning representations..in international.
yiquan wu, kun kuang, yating zhang, xiaozhong liu,changlong sun, jun xiao, yueting zhuang, luo si,and fei wu.
2020. de-biased court‚Äôs view genera-tion with causality.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 763‚Äì780, online.
asso-ciation for computational linguistics..limin yao, aria haghighi, sebastian riedel, and an-drew mccallum.
2011. structured relation discov-ery using generative models.
in proceedings of the2011 conference on empirical methods in naturallanguage processing, pages 1456‚Äì1466, edinburgh,scotland, uk.
association for computational lin-guistics..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiÔ¨Åcation via con-volutional deep neural network.
in proceedings ofcoling 2014, the 25th international conferenceon computational linguistics: technical papers,pages 2335‚Äì2344, dublin, ireland.
dublin city uni-versity and association for computational linguis-tics..xiangji zeng, yunliang li, yuchen zhai, and yinzhang.
2020. counterfactual generator: a weakly-supervised method for named entity recognition.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7270‚Äì7280, online.
association for computa-tional linguistics..peng zhou, wei shi, jun tian, zhenyu qi, bingchen li,hongwei hao, and bo xu.
2016. attention-basedbidirectional long short-term memory networks forin proceedings of the 54threlation classiÔ¨Åcation.
annual meeting of the association for computation-al linguistics (volume 2: short papers), pages 207‚Äì212, berlin, germany.
association for computation-al linguistics..jiaxin qi, yulei niu, jianqiang huang, and hanwangzhang.
2020. two causal principles for improvingvisual dialog.
in proceedings of the ieee/cvf con-ference on computer vision and pattern recognition(cvpr)..longhua qian, guodong zhou, fang kong, qiaomingzhu, and peide qian.
2008. exploiting constituentdependencies for tree kernel-based semantic relationextraction.
in proceedings of the 22nd internation-al conference on computational linguistics (col-ing 2008), pages 697‚Äì704, manchester, uk.
coling2008 organizing committee..lianhui qin, vered shwartz, peter west, chandra bha-gavatula, jena d. hwang, ronan le bras, antoinebosselut, and yejin choi.
2020. back to the future:unsupervised backprop-based decoding for counter-factual and abductive commonsense reasoning.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 794‚Äì805, online.
association for computa-tional linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniÔ¨Åed text-to-text transformer.
journal of machine learning re-search, 21(140):1‚Äì67..leonardo f. r. ribeiro, martin schmitt, hinrichsch¬®utze, and iryna gurevych.
2020.investigatingpretrained language models for graph-to-text gener-ation.
corr, abs/2007.08426..andrew rosenberg and julia hirschberg.
2007. v-measure: a conditional entropy-based external clus-ter evaluation measure.
in proceedings of the 2007joint conference on empirical methods in naturallanguage processing and computational naturallanguage learning (emnlp-conll), pages 410‚Äì420, prague, czech republic.
association for com-putational linguistics..¬¥etienne simon, vincent guigue, and benjamin pi-wowarski.
2019. unsupervised information extrac-tion: regularizing discriminative approaches within proceedings of therelation distribution losses.
57th annual meeting of the association for compu-tational linguistics, pages 1378‚Äì1387, florence, i-taly.
association for computational linguistics..kaihua tang, jianqiang huang, and hanwang zhang.
2020. long-tailed classiÔ¨Åcation by keeping thegood and removing the bad momentum causal effect.
in neurips..petar velikovi, guillem cucurull, arantxa casanova,adriana romero, pietro li, and yoshua bengio..