automated concatenation of embeddings for structured predictionxinyu wang(cid:5)‡, yong jiang†∗, nguyen bach†, tao wang†,zhongqiang huang†, fei huang†, kewei tu(cid:5)∗(cid:5)school of information science and technology, shanghaitech universityshanghai engineering research center of intelligent vision and imagingshanghai institute of microsystem and information technology, chinese academy of sciencesuniversity of chinese academy of sciences†damo academy, alibaba group{wangxy1,tukw}@shanghaitech.edu.cn, yongjiang.jy@alibaba-inc.com{nguyen.bach,leeo.wangt,z.huang,f.huang}@alibaba-inc.com.
abstract.
pretrained contextualized embeddings arepowerful word representations for structuredprediction tasks.
recent work found that bet-ter word representations can be obtained byconcatenating different types of embeddings.
however, the selection of embeddings to formthe best concatenated representation usuallyvaries depending on the task and the collec-tion of candidate embeddings, and the ever-increasing number of embedding types makesit a more difﬁcult problem.
in this paper, wepropose automated concatenation of embed-dings (ace) to automate the process of ﬁnd-ing better concatenations of embeddings forstructured prediction tasks, based on a formu-lation inspired by recent progress on neuralarchitecture search.
speciﬁcally, a controlleralternately samples a concatenation of embed-dings, according to its current belief of the ef-fectiveness of individual embedding types inconsideration for a task, and updates the be-lief based on a reward.
we follow strategiesin reinforcement learning to optimize the pa-rameters of the controller and compute the re-ward based on the accuracy of a task model,which is fed with the sampled concatenationas input and trained on a task dataset.
empir-ical results on 6 tasks and 21 datasets showthat our approach outperforms strong base-lines and achieves state-of-the-art performancewith ﬁne-tuned embeddings in all the evalua-tions.1.
1.introduction.
recent developments on pretrained contextualizedembeddings have signiﬁcantly improved the per-formance of structured prediction tasks in natural.
∗ yong jiang and kewei tu are the corresponding authors.
‡: this work was conducted when xinyu wang was interningat alibaba damo academy..1our code is publicly available at https://github..com/alibaba-nlp/ace..language processing.
approaches based on contex-tualized embeddings, such as elmo (peters et al.,2018), flair (akbik et al., 2018), bert (devlinet al., 2019), and xlm-r (conneau et al., 2020),have been consistently raising the state-of-the-artfor various structured prediction tasks.
concur-rently, research has also showed that word represen-tations based on the concatenation of multiple pre-trained contextualized embeddings and traditionalnon-contextualized embeddings (such as word2vec(mikolov et al., 2013) and character embeddings(santos and zadrozny, 2014)) can further improveperformance (peters et al., 2018; akbik et al., 2018;straková et al., 2019; wang et al., 2020b).
giventhe ever-increasing number of embedding learn-ing methods that operate on different granularities(e.g., word, subword, or character level) and withdifferent model architectures, choosing the best em-beddings to concatenate for a speciﬁc task becomesnon-trivial, and exploring all possible concatena-tions can be prohibitively demanding in computingresources..neural architecture search (nas) is an activearea of research in deep learning to automati-cally search for better model architectures, and hasachieved state-of-the-art performance on varioustasks in computer vision, such as image classiﬁ-cation (real et al., 2019), semantic segmentation(liu et al., 2019a), and object detection (ghiasiet al., 2019).
in natural language processing, nashas been successfully applied to ﬁnd better rnnstructures (zoph and le, 2017; pham et al., 2018b)and recently better transformer structures (so et al.,2019; zhu et al., 2020).
in this paper, we proposeautomated concatenation of embeddings (ace)to automate the process of ﬁnding better concatena-tions of embeddings for structured prediction tasks.
ace is formulated as an nas problem.
in thisapproach, an iterative search process is guided bya controller based on its belief that models the ef-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2643–2660august1–6,2021.©2021associationforcomputationallinguistics2643fectiveness of individual embedding candidates inconsideration for a speciﬁc task.
at each step, thecontroller samples a concatenation of embeddingsaccording to the belief model and then feeds theconcatenated word representations as inputs to atask model, which in turn is trained on the taskdataset and returns the model accuracy as a rewardsignal to update the belief model.
we use the policygradient algorithm (williams, 1992) in reinforce-ment learning (sutton and barto, 1992) to solvethe optimization problem.
in order to improve theefﬁciency of the search process, we also designa special reward function by accumulating all therewards based on the transformation between thecurrent concatenation and all previously sampledconcatenations..our approach is different from previous work on.
nas in the following aspects:.
1. unlike most previous work, we focus on search-ing for better word representations rather thanbetter model architectures..2. we design a novel search space for the embed-instead of usingding concatenation search.
rnn as in previous work of zoph and le (2017),we design a more straightforward controller togenerate the embedding concatenation.
we de-sign a novel reward function in the objective ofoptimization to better evaluate the effectivenessof each concatenated embeddings..3. ace achieves high accuracy without the needfor retraining the task model, which is typicallyrequired in other nas approaches..4. our approach is efﬁcient and practical.
al-though ace is formulated in a nas framework,ace can ﬁnd a strong word representation ona single gpu with only a few gpu-hours forstructured prediction tasks.
in comparison, a lotof nas approaches require dozens or even thou-sands of gpu-hours to search for good neuralarchitectures for their corresponding tasks..empirical results show that ace outperformsstrong baselines.
furthermore, when ace isapplied to concatenate pretrained contextualizedembeddings ﬁne-tuned on speciﬁc tasks, we canachieve state-of-the-art accuracy on 6 structuredprediction tasks including named entity recog-nition (sundheim, 1995), part-of-speech tagging(derose, 1988), chunking (tjong kim sang andbuchholz, 2000), aspect extraction (hu and liu,.
2004), syntactic dependency parsing (tesnière,1959) and semantic dependency parsing (oepenet al., 2014) over 21 datasets.
besides, we alsoanalyze the advantage of ace and reward functiondesign over the baselines and show the advantageof ace over ensemble models..2 related work.
2.1 embeddings.
non-contextualized embeddings, such as word2vec(mikolov et al., 2013), glove (pennington et al.,2014), and fasttext (bojanowski et al., 2017), helplots of nlp tasks.
character embeddings (san-tos and zadrozny, 2014) are trained together withthe task and applied in many structured predictiontasks (ma and hovy, 2016; lample et al., 2016;dozat and manning, 2018).
for pretrained contex-tualized embeddings, elmo (peters et al., 2018),a pretrained contextualized word embedding gen-erated with multiple bidirectional lstm layers,signiﬁcantly outperforms previous state-of-the-artapproaches on several nlp tasks.
following thisidea, akbik et al.
(2018) proposed flair embed-dings, which is a kind of contextualized characterembeddings and achieved strong performance insequence labeling tasks.
recently, devlin et al.
(2019) proposed bert, which encodes contex-tualized sub-word information by transformers(vaswani et al., 2017) and signiﬁcantly improvesthe performance on a lot of nlp tasks.
much re-search such as roberta (liu et al., 2019c) hasfocused on improving bert model’s performancethrough stronger masking strategies.
moreover,multilingual contextualized embeddings becomepopular.
pires et al.
(2019) and wu and dredze(2019) showed that multilingual bert (m-bert)could learn a good multilingual representation ef-fectively with strong cross-lingual zero-shot trans-fer performance in various tasks.
conneau et al.
(2020) proposed xlm-r, which is trained on alarger multilingual corpus and signiﬁcantly outper-forms m-bert on various multilingual tasks..2.2 neural architecture search.
recent progress on deep learning has shown thatnetwork architecture design is crucial to the modelperformance.
however, designing a strong neu-ral architecture for each task requires enormousefforts, high level of knowledge, and experiencesover the task domain.
therefore, automatic designof neural architecture is desired.
a crucial part of.
2644nas is search space design, which deﬁnes the dis-coverable nas space.
previous work (baker et al.,2017; zoph and le, 2017; xie and yuille, 2017)designs a global search space (elsken et al., 2019)which incorporates structures from hand-craftedarchitectures.
for example, zoph and le (2017) de-signed a chained-structured search space with skipconnections.
the global search space usually has aconsiderable degree of freedom.
for example, theapproach of zoph and le (2017) takes 22,400 gpu-hours to search on cifar-10 dataset.
based on theobservation that existing hand-crafted architecturescontain repeated structures (szegedy et al., 2016;he et al., 2016; huang et al., 2017), zoph et al.
(2018) explored cell-based search space which canreduce the search time to 2,000 gpu-hours..in recent nas research, reinforcement learningand evolutionary algorithms are the most usual ap-proaches.
in reinforcement learning, the agent’sactions are the generation of neural architecturesand the action space is identical to the search space.
previous work usually applies an rnn layer (zophand le, 2017; zhong et al., 2018; zoph et al., 2018)or use markov decision process (baker et al., 2017)to decide the hyper-parameter of each structure anddecide the input order of each structure.
evolution-ary algorithms have been applied to architecturesearch for many decades (miller et al., 1989; ange-line et al., 1994; stanley and miikkulainen, 2002;floreano et al., 2008; jozefowicz et al., 2015).
thealgorithm repeatedly generates new populationsthrough recombination and mutation operationsand selects survivors through competing amongthe population.
recent work with evolutionary al-gorithms differ in the method on parent/survivorselection and population generation.
for exam-ple, real et al.
(2017), liu et al.
(2018a), wistuba(2018) and real et al.
(2019) applied tournamentselection (goldberg and deb, 1991) for the par-ent selection while xie and yuille (2017) keepsall parents.
suganuma et al.
(2017) and elskenet al.
(2018) chose the best model while real et al.
(2019) chose several latest models as survivors..3 automated concatenation of.
embeddings.
in ace, a task model and a controller interact witheach other repeatedly.
the task model predicts thetask output, while the controller searches for betterembedding concatenation as the word representa-tion for the task model to achieve higher accuracy..given an embedding concatenation generated fromthe controller, the task model is trained over thetask data and returns a reward to the controller.
thecontroller receives the reward to update its param-eter and samples a new embedding concatenationfor the task model.
figure 1 shows the generalarchitecture of our approach..3.1 task model.
for the task model, we emphasis on sequence-structured and graph-structured outputs.
given astructured prediction task with input sentence xand structured output y, we can calculate the prob-ability distribution p (y|x) by:.
p (y|x) =.
exp (score(x, y))y(cid:48)∈y(x) exp (score(x, y(cid:48))).
(cid:80).
where y(x) represents all possible output struc-tures given the input sentence x. depending ondifferent structured prediction tasks, the outputstructure y can be label sequences, trees, graphsor other structures.
in this paper, we use sequence-structured and graph-structured outputs as twoexemplar structured prediction tasks.
we usebilstm-crf model (ma and hovy, 2016; lampleet al., 2016) for sequence-structured outputs anduse bilstm-biafﬁne model (dozat and manning,2017) for graph-structured outputs:.
p seq(y|x) = bilstm-crf(v , y)p graph(y|x) = bilstm-biafﬁne(v , y).
where v = [v1; · · · ; vn], v ∈ rd×n is a matrixof the word representations for the input sentencex with n words, d is the hidden size of the concate-nation of all embeddings.
the word representationvi of i-th word is a concatenation of l types ofword embeddings:.
i = embedlvl.
i(x); vi = [v1.
i ; v2.
i ; .
.
.
; vli ].
where embedl is the model of l-th embeddings,vi ∈ rd, vl.
dl is the hidden size of embedl..i ∈ rdl.
3.2 search space design.
the neural architecture search space can be repre-sented as a set of neural networks (elsken et al.,2019).
a neural network can be represented as adirected acyclic graph with a set of nodes and di-rected edges.
each node represents an operation,while each edge represents the inputs and outputsbetween these nodes.
in ace, we represent each.
2645embedding candidate as a node.
the input to thenodes is the input sentence x, and the outputs arethe embeddings vl.
since we concatenate the em-beddings as the word representation of the taskmodel, there is no connection between nodes inour search space.
therefore, the search space canbe signiﬁcantly reduced.
for each node, there area lot of options to extract word features.
takingbert embeddings as an example, devlin et al.
(2019) concatenated the last four layers as wordfeatures while kondratyuk and straka (2019) ap-plied a weighted sum of all twelve layers.
however,the empirical results (devlin et al., 2019) do notshow a signiﬁcant difference in accuracy.
we fol-low the typical usage for each embedding to furtherreduce the search space.
as a result, each embed-ding only has a ﬁxed operation and the resultingsearch space contains 2l−1 possible combinationsof nodes..in nas, weight sharing (pham et al., 2018a)shares the weight of structures in training differ-ent neural architectures to reduce the training cost.
in comparison, we ﬁxed the weight of pretrainedembedding candidates in ace except for the char-acter embeddings.
instead of sharing the parame-ters of the embeddings, we share the parametersof the task models at each step of search.
how-ever, the hidden size of word representation variesover the concatenations, making the weight shar-ing of structured prediction models difﬁcult.
in-stead of deciding whether each node exists in thegraph, we keep all nodes in the search space andadd an additional operation for each node to in-dicate whether the embedding is masked out.
torepresent the selected concatenation, we use a bi-nary vector a = [a1, · · · , al, · · · , al] as an maskto mask out the embeddings which are not selected:.
vi = [v1.
i a1; .
.
.
; vl.
ial; .
.
.
; vl.
i al].
(1).
where al is a binary variable.
since the input v isapplied to a linear layer in the bilstm layer, multi-plying the mask with the embeddings is equivalentto directly concatenating the selected embeddings:.
w (cid:62)vi =.
w (cid:62).
l vl.
ial.
(2).
l(cid:88).
l=1.
where w =[w1; w2; .
.
.
; wl] and w ∈rd×hand wl∈rdl×h.
therefore, the model weights canbe shared after applying the embedding mask toall embedding candidates’ concatenation.
another.
beneﬁt of our search space design is that we can re-move the unused embedding candidates and the cor-responding weights in w for a lighter task modelafter the best concatenation is found by ace..3.3 searching in the space.
during search, the controller generates the embed-ding mask for the task model iteratively.
we useparameters θ = [θ1; θ2; .
.
.
; θl] for the controllerinstead of using the rnn structure applied in pre-vious approaches (zoph and le, 2017; zoph et al.,2018).
the probability distribution of selecting anconcatenation a is p ctrl(a; θ) = (cid:81)l(al; θl).
each element al of a is sampled independentlyfrom a bernoulli distribution, which is deﬁned as:.
l=1 p ctrll.(cid:40).
p ctrll.(al; θl)=.
σ(θl)1−p ctrll.al=1(al=1; θl) al=0.
(3).
where σ is the sigmoid function.
given the mask,the task model is trained until convergence and re-turns an accuracy r on the development set.
asthe accuracy cannot be back-propagated to thecontroller, we use the reinforcement algorithmfor optimization.
the accuracy r is used as thereward signal to train the controller.
the con-troller’s target is to maximize the expected rewardj(θ) = ep ctrl(a;θ)[r] through the policy gradientmethod (williams, 1992).
in our approach, sincecalculating the exact expectation is intractable, thegradient of j(θ) is approximated by sampling onlyone selection following the distribution p ctrl(a; θ)at each step for training efﬁciency:.
∇θj(θ) ≈.
∇θ log p ctrl.
l.(al; θl)(r − b).
(4).
l(cid:88).
l=1.
where b is the baseline function to reduce the highvariance of the update function.
the baseline usu-ally can be the highest accuracy during the searchprocess.
instead of merely using the highest accu-racy of development set over the search process asthe baseline, we design a reward function on howeach embedding candidate contributes to accuracychange by utilizing all searched concatenations’ de-velopment scores.
we use a binary vector |at − ai|to represent the change between current embeddingconcatenation at at current time step t and ai atprevious time step i. we then deﬁne the rewardfunction as:.
rt =.
(rt − ri)|at − ai|.
(5).
t−1(cid:88).
i=1.
2646figure 1: the main paradigm of our approach is shown in the middle, where an example of reward function isrepresented in the left and an example of a concatenation action is shown in the right..where rt is a vector with length l representingthe reward of each embedding candidate.
rtand ri are the reward attime step t and i.when the hamming distance of two concatena-tions hamm(at, ai) gets larger, the changed can-didates’ contribution to the accuracy becomes lessnoticeable.
the controller may be misled to re-ward a candidate that is not actually helpful.
weapply a discount factor to reduce the reward for twoconcatenations with a large hamming distance toalleviate this issue.
our ﬁnal reward function is:.
rt=.
(rt−ri)γhamm(at,ai)−1|at−ai|.
(6).
t−1(cid:88).
i=1.
where γ ∈ (0, 1).
eq.
4 is then reformulated as:.
∇θjt(θ) ≈.
∇θ log p ctrl.
l.(at.
l; θl)rtl.(7).
l(cid:88).
l=1.
3.4 trainingto train the controller, we use a dictionary d tostore the concatenations and the corresponding val-idation scores.
at t = 1, we train the task modelwith all embedding candidates concatenated.
fromt = 2, we repeat the following steps until a maxi-mum iteration t :.
1. sample a concatenation at based on the proba-.
bility distribution in eq.
3..2. train the task model with at following eq.
1and evaluate the model on the development setto get the accuracy rt..3. given the concatenation at, accuracy rt and d,compute the gradient of the controller followingeq.
7 and update the parameters of controller..4. add at and rt into d, set t = t + 1.when sampling at, we avoid selecting the previousconcatenation at−1 and the all-zero vector (i.e., se-lecting no embedding).
if at is in the dictionary d,.
we compare the rt with the value in the dictionaryand keep the higher one..4 experiments.
we use iso 639-1 language codes to representlanguages in the table2..4.1 datasets and conﬁgurations.
to show ace’s effectiveness, we conduct extensiveexperiments on a variety of structured predictiontasks varying from syntactic tasks to semantic tasks.
the tasks are named entity recognition (ner), part-of-speech (pos) tagging, chunking, aspect ex-traction (ae), syntactic dependency parsing (dp)and semantic dependency parsing (sdp).
the de-tails of the 6 structured prediction tasks in our ex-periments are shown in below:.
• ner: we use the corpora of 4 languages fromthe conll 2002 and 2003 shared task (tjongkim sang, 2002; tjong kim sang and de meul-der, 2003) with standard split..• pos tagging: we use three datasets, ritter11-t-pos (ritter et al., 2011), ark-twitter (gimpelet al., 2011; owoputi et al., 2013) and tweebank-v2 (liu et al., 2018b) datasets (ritter, ark andtb-v2 in simpliﬁcation).
we follow the datasetsplit of nguyen et al.
(2020)..• chunking: we use conll 2000 (tjongkim sang and buchholz, 2000) for chunking.
since there is no standard development set forconll 2000 dataset, we split 10% of the train-ing data as the development set..• aspect extraction: aspect extraction is a sub-task of aspect-based sentiment analysis (pontikiet al., 2014, 2015, 2016).
the datasets are fromthe laptop and restaurant domain of semeval.
2https://en.wikipedia.org/wiki/list_.
of_iso_639-1_codes.
2647000111rfcontrollertask model[reward]011choice[action]flairelmobertpreviouschoicecurrentchoiceneresen.
deall83.1 92.4 88.9 89.8 90.6 92.1random 84.0 92.6 88.8 91.9 91.3 92.684.2 93.0 88.9 92.1 91.7 92.8ace.
pos.
chunk.
dp.
94.694.694.8.
82.783.683.9.
88.588.188.6.
74.273.574.9.sdp.
nl ritter ark tb-v2 14lap 14res 15res 16res.
ae.
tr.
es.
ru.
nl73.2 74.6 75.0 67.1 67.574.7 75.0 73.6 68.0 70.075.6 75.7 75.3 70.6 71.1.conll 2000 uas las dm-id dm-ood pas-id pas-ood psd-id psd-ood.
allrandomace.
96.796.796.8.
96.7 95.196.8 95.296.9 95.3.
94.394.494.5.
90.890.890.9.
94.694.694.5.
92.993.093.1.
82.482.382.5.
81.781.882.1.avg.
85.385.786.2.table 1: comparison with concatenating all embeddings and random search baselines on 6 tasks..14, restaurant domain of semeval 15 and restau-rant domain of semeval 16 shared task (14lap,14res, 15res and 16res in short).
addition-ally, we use another 4 languages in the restaurantdomain of semeval 16 to test our approach inmultiple languages.
we randomly split 10% ofthe training data as the development set followingli et al.
(2019)..• syntactic dependency parsing: we use penntree bank (ptb) 3.0 with the same dataset pre-processing as (ma et al., 2018)..• semantic dependency parsing: we use dm,pas and psd datasets for semantic dependencyparsing (oepen et al., 2014) for the semeval2015 shared task (oepen et al., 2015).
the threedatasets have the same sentences but with dif-ferent formalisms.
we use the standard split forsdp.
in the split, there are in-domain test setsand out-of-domain test sets for each dataset..among these tasks, ner, pos tagging, chunk-ing and aspect extraction are sequence-structuredoutputs while dependency parsing and semanticdependency parsing are the graph-structured out-puts.
pos tagging, chunking and dp are syntacticstructured prediction tasks while ner, ae, sdpare semantic structured prediction tasks..we train the controller for 30 steps and save thetask model with the highest accuracy on the devel-opment set as the ﬁnal model for testing.
pleaserefer to appendix a for more details of other set-tings..4.2 embeddings.
basic settings: for the candidates of embed-dings on english datasets, we use the language-speciﬁc model for elmo, flair, base bert, gloveword embeddings, fasttext word embeddings, non-contextual character embeddings (lample et al.,2016), multilingual flair (m-flair), m-bert and.
xlm-r embeddings.
the size of the search spacein our experiments is 211−1=20473.
for language-speciﬁc models of other languages, please refer toappendix a for more details.
in ae, there is noavailable russian-speciﬁc bert, flair and elmoembeddings and there is no available turkish-speciﬁc flair and elmo embeddings.
we use thecorresponding english embeddings instead so thatthe search spaces of these datasets are almost iden-tical to those of the other datasets.
all embeddingsare ﬁxed during training except that the characterembeddings are trained over the task.
the empiri-cal results are reported in section 4.3.1..embedding fine-tuning: a usual approach toget better accuracy is ﬁne-tuning transformer-basedembeddings.
in sequence labeling, most of thework follows the ﬁne-tuning pipeline of bert thatconnects the bert model with a linear layer forword-level classiﬁcation.
however, when multipleembeddings are concatenated, ﬁne-tuning a speciﬁcgroup of embeddings becomes difﬁcult because ofcomplicated hyper-parameter settings and massivegpu memory consumption.
to alleviate this prob-lem, we ﬁrst ﬁne-tune the transformer-based em-beddings over the task and then concatenate theseembeddings together with other embeddings in thebasic setting to apply ace.
the empirical resultsare reported in section 4.3.2..4.3 results.
we use the following abbreviations in our experi-ments: uas: unlabeled attachment score; las:labeled attachment score; id: in-domain test set;ood: out-of-domain test set.
we use languagecodes for languages in ner and ae..3flair embeddings have two models (forward and back-.
ward) for each language..26484.3.1 comparison with baselinesto show the effectiveness of our approach, we com-pare our approach with two strong baselines.
forthe ﬁrst one, we let the task model learn by itselfthe contribution of each embedding candidate thatis helpful to the task.
we set a to all-ones (i.e.,the concatenation of all the embeddings) and trainthe task model (all).
the linear layer weightw in eq.
2 reﬂects the contribution of each can-didate.
for the second one, we use the randomsearch (random), a strong baseline in nas (liand talwalkar, 2020).
for random, we run thesame maximum iteration as in ace.
for the exper-iments, we report the averaged accuracy of 3 runs.
table 1 shows that ace outperforms both baselinesin 6 tasks over 23 test sets with only two exceptions.
comparing random with all, random outper-forms all by 0.4 on average and surpasses theaccuracy of all on 14 out of 23 test sets, whichshows that concatenating all embeddings may notbe the best solution to most structured predictiontasks.
in general, searching for the concatenationfor the word representation is essential in mostcases, and our search design can usually lead tobetter results compared to both of the baselines..4.3.2 comparison with state-of-the-art.
approaches.
as we have shown, ace has an advantage insearching for better embedding concatenations.
we further show that ace is competitive or evenstronger than state-of-the-art approaches.
weadditionally use xlnet (yang et al., 2019) androberta as the candidates of ace.
in some tasks,we have several additional settings to better com-pare with previous work.
in ner, we also conducta comparison on the revised version of germandatasets in the conll 2006 shared task (buch-holz and marsi, 2006).
recent work such as yuet al.
(2020) and yamada et al.
(2020) utilizes doc-ument contexts in the datasets.
we follow theirwork and extract document embeddings for thetransformer-based embeddings.
speciﬁcally, wefollow the ﬁne-tune process of yamada et al.
(2020)to ﬁne-tune the transformer-based embeddings overthe document except for bert and m-bert em-beddings.
for bert and m-bert, we follow thedocument extraction process of yu et al.
(2020)because we ﬁnd that the model with such docu-ment embeddings is signiﬁcantly stronger than themodel trained with the ﬁne-tuning process of ya-in sdp, the state-of-the-artmada et al.
(2020)..approaches used pos tags and lemmas as addi-tional word features to the network.
we add thesetwo features to the embedding candidates and trainthe embeddings together with the task.
we usethe ﬁne-tuned transformer-based embeddings oneach task instead of the pretrained version of theseembeddings as the candidates.4.
we additionally compare with ﬁne-tuned xlm-r model for ner, pos tagging, chunking and ae,and compare with ﬁne-tuned xlnet model for dpand sdp, which are strong ﬁne-tuned models inmost of the experiments.
results are shown in ta-ble 2, 3, 4. results show that ace with ﬁne-tunedembeddings achieves state-of-the-art performancein all test sets, which shows that ﬁnding a good em-bedding concatenation helps structured predictiontasks.
we also ﬁnd that ace is stronger than theﬁne-tuned models, which shows the effectivenessof concatenating the ﬁne-tuned embeddings5..5 analysis.
5.1 efﬁciency of search methods.
to show how efﬁcient our approach is comparedwith the random search algorithm, we compare thealgorithm in two aspects on conll english nerdataset.
the ﬁrst aspect is the best developmentaccuracy during training.
the left part of figure 2shows that ace is consistently stronger than therandom search algorithm in this task.
the secondaspect is the searched concatenation at each timestep.
the right part of figure 2 shows that the ac-curacy of ace gradually increases and gets stablewhen more concatenations are sampled..5.2 ablation study on reward function.
design.
to show the effectiveness of the designed rewardfunction, we compare our reward function (eq.
6)with the reward function without discount factor(eq.
5) and the traditional reward function (rewardterm in eq.
4).
we sample 2000 training sentenceson conll english ner dataset for faster train-ing and train the controller for 50 steps.
table 5shows that both the discount factor and the binaryvector |at − ai| for the task are helpful in bothdevelopment and test datasets..4please refer to appendix for more details about the em-.
5we compare ace with other ﬁne-tuned embeddings in.
beddings..appendix..2649baevski et al.
(2019)straková et al.
(2019)yu et al.
(2020)yamada et al.
(2020)xlm-r+fine-tuneace+fine-tune.
de-85.186.4-87.788.3.de06--90.3-91.491.7.neren93.593.493.594.394.194.6.es-88.890.3-89.395.9.nl-92.793.7-95.395.7.owoputi et al.
(2013)gui et al.
(2017)gui et al.
(2018)nguyen et al.
(2020)xlm-r+fine-tuneace+fine-tune.
posritter ark tb-v294.693.290.492.890.9--92.491.295.294.190.195.493.792.395.894.493.4.table 2: comparison with state-of-the-art approaches in ner and pos tagging.
†: models are trained on bothtrain and development set..chunkconll 2000.ae14lap 14res 15res 16res.
akbik et al.
(2018)clark et al.
(2018)liu et al.
(2019b)chen et al.
(2020)xlm-r+fine-tuneace+fine-tune.
96.797.097.395.597.097.3.xu et al.
(2018)†xu et al.
(2019)wang et al.
(2020a)wei et al.
(2020)xlm-r+fine-tuneace+fine-tune.
84.284.3-82.785.987.4.
84.6--87.190.592.0.
72.0--72.776.480.3.
75.478.072.877.778.981.3.es.
nl.
ru.
--.
-.
--.
-.
--.
-.
74.3 72.9 71.8 59.3.tr.
--.
-.
77.0 77.6 77.7 74.179.9 80.5 79.4 81.9.table 3: comparison with state-of-the-art approaches in chunking and aspect extraction.
†: we report the resultsreproduced by wei et al.
(2020)..ycarucca.tseb.
96.6.
96.4.
96.2.
96.ycaruccaelpmas.96.
95.
94.
93.ace.
random.
ace.
random.
0.
5.
10.
15.
20.
25.
30.
0.
5.
10.
15.
20.
25.
30.figure 2: comparing the efﬁciency of random search (random) and ace.
the x-axis is the number of time steps.
the left y-axis is the averaged best validation accuracy on conll english ner dataset.
the right y-axis is theaveraged validation accuracy of the current selection..5.3 comparison with embedding weighting.
& ensemble approaches.
we compare ace with two more approaches tofurther show the effectiveness of ace.
one is avariant of all, which uses a weighting param-eter b = [b1, · · · , bl, · · · , bl] passing through asigmoid function to weight each embedding can-didate.
such an approach can explicitly learn theweight of each embedding in training instead of abinary mask.
we call this approach all+weight.
another one is model ensemble, which trains thetask model with each embedding candidate indi-vidually and uses the trained models to make jointprediction on the test set.
we use voting for ensem-ble as it is simple and fast.
for sequence labelingtasks, the models vote for the predicted label ateach position.
for dp, the models vote for thetree of each sentence.
for sdp, the models votefor each potential labeled arc.
we use the conﬁ-.
dence of model predictions to break ties if thereare more than one agreement with the same counts.
we call this approach ensemble.
one of the ben-eﬁts of voting is that it combines the predictionsof the task models efﬁciently without any trainingprocess.
we can search all possible 2l−1 modelensembles in a short period of time through cachingthe outputs of the models.
therefore, we searchfor the best ensemble of models on the develop-ment set and then evaluate the best ensemble onthe test set (ensembledev).
moreover, we addi-tionally search for the best ensemble on the test setfor reference (ensembletest), which is the upperbound of the approach.
we use the same setting asin section 4.3.1 and select one of the datasets fromeach task.
for ner, pos tagging, ae, and sdp,we use conll 2003 english, ritter, 16res, anddm datasets, respectively.
the results are shownin table 6. empirical results show that ace out-.
2650dpptb.
uas.
las.
zhou and zhao (2019)†mrini et al.
(2020)†li et al.
(2020)zhang et al.
(2020)wang and tu (2020)xlnet+fine-tuneace+fine-tune.
97.297.496.696.196.997.097.2.he and choi (2020)‡d & m (2018).
95.796.394.8 wang et al.
(2019)94.595.395.695.8.jia et al.
(2020)f & g (2020)xlnet+fine-tuneace+fine-tune.
dm.
ood.
90.888.989.789.191.090.692.6.id.
94.693.794.093.694.494.295.6.sdppas.
id.
96.193.994.1-95.194.895.8.ood.
94.490.691.3-93.493.494.6.psd.
ood.
79.579.479.6-82.081.883.4.id.
86.881.081.4-82.682.783.8.table 4: comparison with state-of-the-art approaches in dp and sdp.
†: for reference, they additionally usedconstituency dependencies in training.
we also ﬁnd that the ptb dataset used by mrini et al.
(2020) is not identicalto the dataset in previous work such as zhang et al.
(2020) and wang and tu (2020).
‡: for reference, we conﬁrmedwith the authors of he and choi (2020) that they used a different data pre-processing script with previous work.
..aceno discount (eq.
5)simple (eq.
4).
dev93.1892.9892.89.test90.0089.9089.82.table 5: comparison of reward functions..dp.
sdp.
ner pos ae chk.
uas las id ood92.4 90.6 73.2 96.7 96.7 95.1 94.3 90.8all92.6 91.3 74.7 96.7 96.8 95.2 94.4 90.8random93.0 91.7 75.6 96.8 96.9 95.3 94.5 90.9ace92.7 90.4 73.7 96.7 96.7 95.1 94.3 90.7all+weightensemble92.2 90.6 68.1 96.5 96.1 94.3 94.1 90.3ensembledev 92.2 90.8 70.2 96.7 96.8 95.2 94.3 90.7ensembletest 92.7 91.4 73.9 96.7 96.8 95.2 94.4 90.8.table 6: a comparison among all, random, ace,all+weight and ensemble.
chk: chunking..performs all the settings of these approaches andeven ensembletest, which shows the effective-ness of ace and the limitation of ensemble mod-els.
all, all+weight and ensembledev arecompetitive in most of the cases and there is noclear winner of these approaches on all the datasets.
these results show the strength of embedding con-catenation.
concatenating the embeddings incor-porates information from all the embeddings andforms stronger word representations for the taskmodel, while in model ensemble, it is difﬁcult forthe individual task models to affect each other..6 discussion: practical usability of ace.
concatenating multiple embeddings is a commonlyused approach to improve accuracy of structuredprediction.
however, such approaches can be com-putationally costly as multiple language modelsare used as input.
ace is more practical than con-catenating all embeddings as it can remove those.
embeddings that are not very useful in the con-catenation.
moreover, ace models can be usedto guide the training of weaker models throughtechniques such as knowledge distillation in struc-tured prediction (kim and rush, 2016; kuncoroet al., 2016; wang et al., 2020a, 2021b), leading tomodels that are both stronger and faster..7 conclusion.
in this paper, we propose automated concatena-tion of embeddings, which automatically searchesfor better embedding concatenation for structuredprediction tasks.
we design a simple search spaceand use the reinforcement learning with a novelreward function to efﬁciently guide the controllerto search for better embedding concatenations.
wetake the change of embedding concatenations intothe reward function design and show that our newreward function is stronger than the simpler ones.
results show that ace outperforms strong base-lines.
together with ﬁne-tuned embeddings, aceachieves state-of-the-art performance in 6 tasksover 21 datasets..acknowledgments.
this work was supported by the national natu-ral science foundation of china (61976139) andby alibaba group through alibaba innovative re-search program.
we thank chengyue jiang for hiscomments and suggestions on writing..references.
alan akbik, tanja bergmann, and roland vollgraf.
2019. pooled contextualized embeddings for namedentity recognition.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-.
2651guage technologies, volume 1 (long and short pa-pers), pages 724–728, minneapolis, minnesota.
as-sociation for computational linguistics..steven j. derose.
1988. grammatical category disam-biguation by statistical optimization.
computationallinguistics, 14(1):31–39..alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, pages1638–1649, santa fe, new mexico, usa.
associ-ation for computational linguistics..peter j angeline, gregory m saunders, and jordan bpollack.
1994. an evolutionary algorithm that con-ieee transac-structs recurrent neural networks.
tions on neural networks, 5(1):54–65..alexei baevski, sergey edunov, yinhan liu, lukezettlemoyer, and michael auli.
2019. cloze-drivenin proceed-pretraining of self-attention networks.
ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5360–5369, hongkong, china.
association for computational lin-guistics..bowen baker, otkrist gupta, nikhil naik, and rameshraskar.
2017. designing neural network architec-tures using reinforcement learning.
in internationalconference on learning representations..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..sabine buchholz and erwin marsi.
2006. conll-x shared task on multilingual dependency parsing.
in proceedings of the tenth conference on com-putational natural language learning (conll-x),pages 149–164, new york city.
association forcomputational linguistics..luoxin chen, weitong ruan, xinyue liu, and jianhualu.
2020. seqvat: virtual adversarial training forsemi-supervised sequence labeling.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 8801–8811, on-line.
association for computational linguistics..kevin clark, minh-thang luong, christopher d. man-ning, and quoc le.
2018.semi-supervised se-quence modeling with cross-view training.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 1914–1925, brussels, belgium.
association for computa-tional linguistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d manning.
2017.deep biafﬁne attention for neural dependency pars-ing.
in international conference on learning rep-resentations..timothy dozat and christopher d. manning.
2018.simpler but more accurate semantic dependencyin proceedings of the 56th annual meet-parsing.
ing of the association for computational linguis-tics (volume 2: short papers), pages 484–490, mel-bourne, australia.
association for computationallinguistics..thomas elsken, jan-hendrik metzen, and frank hut-ter.
2018. simple and efﬁcient architecture searchfor convolutional neural networks.
in internationalconference on learning representations workshop..thomas elsken, jan hendrik metzen, and frank hutter.
2019. neural architecture search: a survey.
journalof machine learning research, 20:1–21..daniel fernández-gonzález.
and carlos gómez-transition-based semanticrodríguez.
2020.independency parsing with pointer networks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages7035–7046, online.
association for computationallinguistics..dario floreano, peter dürr, and claudio mattiussi.
2008. neuroevolution: from architectures to learn-ing.
evolutionary intelligence, 1(1):47–62..golnaz ghiasi, tsung-yi lin, and quoc v le.
2019.nas-fpn: learning scalable feature pyramid architec-ture for object detection.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 7036–7045..kevin gimpel, nathan schneider, brendan o’connor,dipanjan das, daniel mills,jacob eisenstein,michael heilman, dani yogatama, jeffrey flanigan,and noah a. smith.
2011. part-of-speech taggingfor twitter: annotation, features, and experiments.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 42–47, portland, ore-gon, usa.
association for computational linguis-tics..2652david e goldberg and kalyanmoy deb.
1991. a com-parative analysis of selection schemes used in ge-in foundations of genetic algo-netic algorithms.
rithms, volume 1, pages 69–93.
elsevier..tao gui, qi zhang, jingjing gong, minlong peng,di liang, keyu ding, and xuanjing huang.
2018.transferring from formal newswire domain with hy-pernet for twitter pos tagging.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 2540–2549, brus-sels, belgium.
association for computational lin-guistics..tao gui, qi zhang, haoran huang, minlong peng, andxuanjing huang.
2017. part-of-speech tagging forin pro-twitter with adversarial neural networks.
ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 2411–2420, copenhagen, denmark.
association for com-putational linguistics..han he and jinho choi.
2020. establishing strongbaselines for the new decade: sequence tagging,in thesyntactic and semantic parsing with bert.
thirty-third international flairs conference..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..minqing hu and bing liu.
2004. mining and sum-in proceedings of themarizing customer reviews.
tenth acm sigkdd international conference onknowledge discovery and data mining, kdd ’04,page 168–177, new york, ny, usa.
association forcomputing machinery..gao huang, zhuang liu, laurens van der maaten, andkilian q weinberger.
2017. densely connected con-in proceedings of the ieeevolutional networks.
conference on computer vision and pattern recogni-tion, pages 4700–4708..zixia jia, youmi ma, jiong cai, and kewei tu.
2020.semi-supervised semantic dependency parsing us-ing crf autoencoders.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 6795–6805, online.
asso-ciation for computational linguistics..rafal.
jozefowicz, wojciech zaremba,.
and ilyasutskever.
2015. an empirical exploration of recur-rent network architectures.
in international confer-ence on machine learning, pages 2342–2350..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in natu-ral language processing, pages 1317–1327, austin,texas.
association for computational linguistics..diederik p kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in internationalconference on learning representations..dan kondratyuk and milan straka.
2019..75 lan-guages, 1 model: parsing universal dependenciesin proceedings of the 2019 confer-universally.
ence on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 2779–2795, hong kong, china.
as-sociation for computational linguistics..adhiguna kuncoro, miguel ballesteros, lingpengkong, chris dyer, and noah a. smith.
2016. distill-ing an ensemble of greedy dependency parsers intoone mst parser.
in proceedings of the 2016 con-ference on empirical methods in natural languageprocessing, pages 1744–1753, austin, texas.
asso-ciation for computational linguistics..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 260–270, san diego, california.
associationfor computational linguistics..liam li and ameet talwalkar.
2020. random searchand reproducibility for neural architecture search.
in uncertainty in artiﬁcial intelligence, pages 367–377. pmlr..xin li, lidong bing, wenxuan zhang, and wai lam.
2019. exploiting bert for end-to-end aspect-basedsentiment analysis.
in proceedings of the 5th work-shop on noisy user-generated text (w-nut 2019),pages 34–41, hong kong, china.
association forcomputational linguistics..zuchao li, hai zhao, and kevin parnow.
2020. globalin proceedings ofgreedy dependency parsing.
the aaai conference on artiﬁcial intelligence, vol-ume 34, pages 8319–8326..chenxi liu, liang-chieh chen, florian schroff,hartwig adam, wei hua, alan l yuille, and li fei-fei.
2019a.
auto-deeplab: hierarchical neural ar-chitecture search for semantic image segmentation.
in proceedings of the ieee conference on computervision and pattern recognition, pages 82–92..hanxiao liu, karen simonyan, oriol vinyals, chrisan-tha fernando, and koray kavukcuoglu.
2018a.
hi-erarchical representations for efﬁcient architecturein international conference on learningsearch.
representations..yijia liu, yi zhu, wanxiang che, bing qin, nathanschneider, and noah a. smith.
2018b.
parsingtweets into universal dependencies.
in proceedingsof the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 965–975, new orleans, louisiana.
as-sociation for computational linguistics..2653yijin liu, fandong meng, jinchao zhang, jinan xu,yufeng chen, and jie zhou.
2019b.
gcdt: a globalcontext enhanced deep transition architecture for se-quence labeling.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2431–2441, florence, italy.
associa-tion for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019c.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2018. decoupledin international con-.
weight decay regularization.
ference on learning representations..jouni luoma and sampo pyysalo.
2020. exploringcross-sentence contexts for named entity recogni-tion with bert.
in proceedings of the 28th inter-national conference on computational linguistics,pages 904–914, barcelona, spain (online).
interna-tional committee on computational linguistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1064–1074, berlin, ger-many.
association for computational linguistics..xuezhe ma, zecong hu, jingzhou liu, nanyun peng,graham neubig, and eduard hovy.
2018. stack-in pro-pointer networks for dependency parsing.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1403–1414, melbourne, australia.
association for computational linguistics..ryan mcdonald, fernando pereira, kiril ribarov, andjan hajiˇc.
2005. non-projective dependency pars-in proceed-ing using spanning tree algorithms.
ings of human language technology conferenceand conference on empirical methods in naturallanguage processing, pages 523–530, vancouver,british columbia, canada.
association for compu-tational linguistics..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-in advances in neural information processingity.
systems, pages 3111–3119..geoffrey miller, peter todd, and shailesh hegde.
1989.designing neural networks using genetic algorithms.
in 3rd international conference on genetic algo-rithms, pages 379–384..khalil mrini, franck dernoncourt, quan hung tran,trung bui, walter chang, and ndapa nakashole.
2020. rethinking self-attention: towards inter-pretability in neural parsing.
in findings of the as-sociation for computational linguistics: emnlp.
2020, pages 731–742, online.
association for com-putational linguistics..dat quoc nguyen, thanh vu, and anh tuan nguyen.
2020. bertweet: a pre-trained language model forin proceedings of the 2020 con-english tweets.
ference on empirical methods in natural languageprocessing: system demonstrations..stephan oepen, marco kuhlmann, yusuke miyao,daniel zeman, silvie cinková, dan flickinger, janhajic, and zdenka uresova.
2015. semeval 2015task 18: broad-coverage semantic dependency pars-ing.
in proceedings of the 9th international work-shop on semantic evaluation (semeval 2015), pages915–926..stephan oepen, marco kuhlmann, yusuke miyao,daniel zeman, dan flickinger, jan hajic, angelinaivanova, and yi zhang.
2014. semeval 2014 task 8:broad-coverage semantic dependency parsing.
se-meval 2014..olutobi owoputi, brendan o’connor, chris dyer,kevin gimpel, nathan schneider, and noah a.improved part-of-speech tagging forsmith.
2013.online conversational text with word clusters.
inproceedings of the 2013 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 380–390, atlanta, georgia.
association forcomputational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..hieu pham, melody guan, barret zoph, quoc le,and jeff dean.
2018a.
efﬁcient neural architecturesearch via parameters sharing.
in international con-ference on machine learning, pages 4095–4104..hieu pham, melody y guan, barret zoph, quoc v le,and jeff dean.
2018b.
efﬁcient neural architecturesearch via parameter sharing.
in international con-ference on machine learning..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..2654maria pontiki, dimitris galanis, haris papageorgiou,ion androutsopoulos, suresh manandhar, moham-mad al-smadi, mahmoud al-ayyoub, yanyanzhao, bing qin, orphée de clercq, véroniquehoste, marianna apidianaki, xavier tannier, na-talia loukachevitch, evgeniy kotelnikov, nuria bel,salud maría jiménez-zafra, and gül¸sen eryi˘git.
2016. semeval-2016 task 5: aspect based senti-ment analysis.
in proceedings of the 10th interna-tional workshop on semantic evaluation (semeval-2016), pages 19–30, san diego, california.
associa-tion for computational linguistics..maria pontiki, dimitris galanis, haris papageorgiou,suresh manandhar, and ion androutsopoulos.
2015.semeval-2015 task 12: aspect based sentimentin proceedings of the 9th internationalanalysis.
workshop on semantic evaluation (semeval 2015),pages 486–495, denver, colorado.
association forcomputational linguistics..maria pontiki, dimitris galanis, john pavlopoulos,harris papageorgiou,ion androutsopoulos, andsuresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in proceedings of the8th international workshop on semantic evaluation(semeval 2014), pages 27–35, dublin, ireland.
as-sociation for computational linguistics..esteban real, alok aggarwal, yanping huang, andquoc v le.
2019. regularized evolution for imageclassiﬁer architecture search.
in proceedings of theaaai conference on artiﬁcial intelligence, volume 33,pages 4780–4789..esteban real, sherry moore, andrew selle, saurabhsaxena, yutaka leon suematsu, jie tan, quoc v le,and alexey kurakin.
2017. large-scale evolutionof image classiﬁers.
in international conference onmachine learning, pages 2902–2911..alan ritter, sam clark, mausam, and oren etzioni.
2011. named entity recognition in tweets: an ex-perimental study.
in proceedings of the 2011 con-ference on empirical methods in natural languageprocessing, pages 1524–1534, edinburgh, scotland,uk.
association for computational linguistics..cicero d santos and bianca zadrozny.
2014. learningcharacter-level representations for part-of-speechin proceedings of the 31st internationaltagging.
conference on machine learning (icml-14), pages1818–1826..tal schuster, ori ram, regina barzilay, and amirgloberson.
2019. cross-lingual alignment of con-textual word embeddings, with applications to zero-in proceedings of theshot dependency parsing.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 1599–1613, minneapolis, min-nesota.
association for computational linguistics..david r so, chen liang, and quoc v le.
2019. theevolved transformer.
in international conference onmachine learning..kenneth o stanley and risto miikkulainen.
2002.evolving neural networksthrough augmentingtopologies.
evolutionary computation, 10(2):99–127..jana straková, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through lineariza-in proceedings of the 57th annual meetingtion.
of the association for computational linguistics,pages 5326–5331, florence, italy.
association forcomputational linguistics..masanori suganuma, shinichi shirakawa, and tomo-haru nagao.
2017. a genetic programming ap-proach to designing convolutional neural network ar-chitectures.
in proceedings of the genetic and evolu-tionary computation conference, pages 497–504..beth m. sundheim.
1995. named entity task deﬁnition,in proceedings of the sixth message.
version 2.1.understanding conference, pages 319–332..richard s sutton and andrew g barto.
1992. rein-forcement learning: an introduction.
mit press..christian szegedy, vincent vanhoucke, sergey ioffe,jon shlens, and zbigniew wojna.
2016. rethinkingthe inception architecture for computer vision.
inproceedings of the ieee conference on computer vi-sion and pattern recognition, pages 2818–2826..lucien tesnière.
1959. éléments de syntaxe structurale..editions klincksieck..erik f. tjong kim sang.
2002..introduction to theconll-2002 shared task: language-independentin coling-02: thenamed entity recognition.
6th conference on natural language learning 2002(conll-2002)..erik f. tjong kim sang and sabine buchholz.
2000.introduction to the conll-2000 shared task chunk-in fourth conference on computational nat-ing.
ural language learning and the second learninglanguage in logic workshop..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..xinyu wang, jingxian huang, and kewei tu.
2019.second-order semantic dependency parsing withend-to-end neural networks.
in proceedings of the.
265557th annual meeting of the association for com-putational linguistics, pages 4609–4618, florence,italy.
association for computational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,fei huang, and kewei tu.
2020a.
structure-levelknowledge distillation for multilingual sequence la-in proceedings of the 58th annual meet-beling.
ing of the association for computational linguistics,pages 3317–3330, online.
association for computa-tional linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,zhongqiang huang, fei huang, and kewei tu.
2021a.
improving named entity recognition by ex-ternal context retrieving and cooperative learning.
in the joint conference of the 59th annual meet-ing of the association for computational linguisticsand the 11th international joint conference on natu-ral language processing (acl-ijcnlp 2021).
as-sociation for computational linguistics..xinyu wang, yong jiang, nguyen bach, tao wang,huang zhongqiang, fei huang, and kewei tu.
2020b.
more embeddings, better sequence labelers?
in findings of emnlp, online..xinyu wang, yong jiang, zhaohui yan, zixia jia,nguyen bach, tao wang, zhongqiang huang, feihuang, and kewei tu.
2021b.
structural knowl-edge distillation: tractably distilling informationin the joint conferencefor structured predictor.
of the 59th annual meeting of the association forcomputational linguistics and the 11th interna-tional joint conference on natural language pro-cessing (acl-ijcnlp 2021).
association for com-putational linguistics..xinyu wang and kewei tu.
2020. second-order neuraldependency parsing with message passing and end-in proceedings of the 1st confer-to-end training.
ence of the asia-paciﬁc chapter of the associationfor computational linguistics and the 10th interna-tional joint conference on natural language pro-cessing, pages 93–99, suzhou, china.
associationfor computational linguistics..zhenkai wei, yu hong, bowei zou, meng cheng, andjianmin yao.
2020. don’t eclipse your arts due tosmall discrepancies: boundary repositioning within pro-a pointer network for aspect extraction.
ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3678–3684, online.
association for computational lin-guistics..ronald j williams.
1992. simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
machine learning, 8(3-4):229–256..shijie wu and mark dredze.
2019. beto, bentz, be-cas: the surprising cross-lingual effectiveness ofbert.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages833–844, hong kong, china.
association for com-putational linguistics..l. xie and a. yuille.
2017. genetic cnn..in 2017ieee international conference on computer vision(iccv), pages 1388–1397..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 2324–2335, minneapolis,minnesota.
association for computational linguis-tics..hu xu, bing liu, lei shu, and philip s. yu.
2018. dou-ble embeddings and cnn-based sequence labelingfor aspect extraction.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 2: short papers), pages 592–598, melbourne, australia.
association for compu-tational linguistics..ikuya yamada, akari asai, hiroyuki shindo, hideakitakeda, and yuji matsumoto.
2020. luke: deepcontextualized entity representations with entity-in proceedings of the 2020aware self-attention.
conference on empirical methods in natural lan-guage processing (emnlp), pages 6442–6454, on-line.
association for computational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..juntao yu, bernd bohnet, and massimo poesio.
2020.named entity recognition as dependency parsing.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 6470–6476, online.
association for computational lin-guistics..yu zhang, zhenghua li, and min zhang.
2020. efﬁ-cient second-order treecrf for neural dependencyin proceedings of the 58th annual meet-parsing.
ing of the association for computational linguistics,pages 3295–3305, online.
association for computa-tional linguistics..martin wistuba.
2018. deep learning architecturesearch by neuro-cell-based evolution with function-in joint european confer-preserving mutations.
ence on machine learning and knowledge discov-ery in databases, pages 243–258.
springer..zhao zhong, junjie yan, wei wu, jing shao, andcheng-lin liu.
2018. practical block-wise neuralnetwork architecture generation.
in proceedings ofthe ieee conference on computer vision and patternrecognition, pages 2423–2432..2656junru zhou and hai zhao.
2019. head-driven phraseinstructure grammar parsing on penn treebank.
proceedings ofthethe 57th annual meeting ofassociation for computational linguistics, pages2396–2408, florence, italy.
association for compu-tational linguistics..wei zhu, xiaoling wang, xipeng qiu, yuan ni, andguotong xie.
2020. autotrans: automating trans-former design via reinforced architecture search.
arxiv preprint arxiv:2009.02070..barret zoph and quoc v le.
2017. neural architecturesearch with reinforcement learning.
in internationalconference on learning representations..barret zoph, vijay vasudevan, jonathon shlens, andquoc v le.
2018. learning transferable architec-tures for scalable image recognition.
in proceedingsof the ieee conference on computer vision and pat-tern recognition, pages 8697–8710..a detailed conﬁgurations.
evaluation to evaluate our models, we use f1score to evaluate ner, chunking and ae, use ac-curacy to evaluate pos tagging, use unlabeledattachment score (uas) and labeled attachmentscore (las) to evaluate dp, and use labeled f1score to evaluate sdp..task models and controller for sequence-structured tasks (i.e., ner, pos tagging, chunking,aspect extraction), we use a batch size of 32 sen-tences and an sgd optimizer with a learning rate of0.1. we anneal the learning rate by 0.5 when thereis no accuracy improvement on the developmentset for 5 epochs.
we set the maximum trainingepoch to 150. for graph-structured tasks (i.e., dpand sdp), we use adam (kingma and ba, 2015)to optimize the model with a learning rate of 0.002.we anneal the learning rate by 0.75 for every 5000iterations following dozat and manning (2017).
we set the maximum training epoch to 300. fordp, we run the maximum spanning tree (mcdon-ald et al., 2005) algorithm to output valid trees intesting.
we ﬁx the hyper-parameters of the taskmodels..we tune the learning rate for the controlleramong {0.1, 0.2, 0.3, 0.4, 0.5} and the discountfactor among {0.1, 0.3, 0.5, 0.7, 0.9} on the samedataset in section 5.2. we search for the hyper-parameter through grid search and ﬁnd a learningrate of 0.1 and a discount factor of 0.5 performsthe best on the development set.
the controller’sparameters are initialized to all 0 so that each can-didate is selected evenly in the ﬁrst two time steps..we use stochastic gradient descent (sgd) to opti-mize the controller.
the training time depends onthe task and dataset size.
take the conll englishner dataset as an example.
it takes 45 gpu hoursto train the controller for 30 steps on a single teslap100 gpu, which is an acceptable training time inpractice..sources of embeddings the sources of the em-beddings that we used are listed in table 7..b additional analysis.
b.1 document-level and sentence-level.
representations.
recently, models with document-level word repre-sentations extracted from transformer-based em-beddings signiﬁcantly outperform models withsentence-level word representations in ner (de-vlin et al., 2019; yu et al., 2020; yamada et al.,2020).
however, there are a lot of application sce-narios that document contexts are unavailable.
wereplace the document-level word representationsfrom transformer-based embeddings (i.e., xlm-r and bert embeddings) with the sentence-levelword representations.
results are shown in table8. we report the test results of all to show howthe gap between ace and all changes with dif-ferent kinds of representations.
we report the testaccuracy of the models with the highest develop-ment accuracy following yamada et al.
(2020) fora fair comparison.
empirical results show that thedocument-level representations can signiﬁcantlyimprove the accuracy of ace.
comparing withmodels with sentence-level representations, the av-eraged accuracy gap between ace and all is en-hanced from 0.7 to 1.7 with document-level repre-sentations, which shows that the advantage of acebecomes stronger with document-level representa-tions..b.2 fine-tuned models versus ace.
to ﬁne-tune the embeddings, we use adamw(loshchilov and hutter, 2018) optimizer with alearning rate of 5 × 10−6 and trained the contex-tualized embeddings with the task for 10 epochs.
we use a batch size of 32 for bert, m-bert anduse a batch size of 4 for xlm-r, roberta andxlnet.
a comparison between ace and the ﬁne-tuned embeddings that we used in ace is shownin table 9, 10. results show that ace can furtherimprove the accuracy of ﬁne-tuned models..2657embeddingglovefasttextelmoelmo (other languages)bertm-bertbert (dutch)bert (german)bert (spanish)bert (turkish)xlm-rrobertaxlnet.
resourcepennington et al.
(2014)bojanowski et al.
(2017)peters et al.
(2018)schuster et al.
(2019)devlin et al.
(2019)devlin et al.
(2019)wietsedvdbmdzdccuchiledbmdzconneau et al.
(2020)liu et al.
(2019c)yang et al.
(2019).
urlnlp.stanford.edu/projects/glovegithub.com/facebookresearch/fasttextgithub.com/allenai/allennlpgithub.com/talschuster/crosslingualcontextualembhuggingface.co/bert-base-casedhuggingface.co/bert-base-multilingual-casedhuggingface.co/wietsedv/bert-base-dutch-casedhuggingface.co/bert-base-german-dbmdz-casedhuggingface.co/dccuchile/bert-base-spanish-wwm-casedhuggingface.co/dbmdz/bert-base-turkish-casedhuggingface.co/xlm-roberta-largehuggingface.co/roberta-largehuggingface.co/xlnet-large-cased.
table 7: the embeddings we used in our experiments.
the url is where we downloaded the embeddings..es.
de06.
all+sentace+sentbert (2019)akbik et al.
(2019)yu et al.
(2020)yamada et al.
(2020)luoma and pyysalo (2020) 87.3wang et al.
(2021a)all+docace+doc.
denlen86.8 90.1 93.3 90.0 94.487.1 90.5 93.6 92.4 94.692.888.3 93.2.
-90.486.4 90.3 93.5 90.3 94.7-94.393.7 88.3 93.5-93.987.5 90.8 94.0 90.7 93.788.3 91.7 94.6 95.9 95.7.
---.
--.
--.
-.
-.
-.
-.
-.
table 8: comparison of models with and without doc-ument contexts on ner.
+sent/+doc: models withsentence-/document-level embeddings..b.3 retraining.
most of the work (zoph and le, 2017; zoph et al.,2018; pham et al., 2018b; so et al., 2019; zhu et al.,2020) in nas retrains the searched neural archi-tecture from scratch so that the hyper-parametersof the searched model can be modiﬁed or trainedon larger datasets.
to show whether our searchedembedding concatenation is helpful to the task, weretrain the task model with the embedding concate-nations on the same dataset from scratch.
for theexperiment, we use the same dataset settings as insection 4.3.1. we train the searched embeddingconcatenation of each run from ace 3 times (there-fore, 9 runs for each dataset)..table 12 shows the comparison between re-trained models with the searched embedding con-catenation from ace and all.
the results showthat the retrained models are competitive with acein sdp and in chunking.
however, in another threetasks, the retrained models perform inferior to ace.
the possible reason is that the model at each stepis initialized by the trained model of previous step.
the retrained models outperform all in all tasks,which shows the effectiveness of the searched em-bedding concatenations..b.4 effect of embeddings in the searched.
embedding concatenations.
there is no clear conclusion on what concate-nation of embeddings is helpful to most of thetasks.
we analyze the best searched embeddingconcatenations by ace over different structuredoutputs, semantic/syntactic type, and monolin-gual/multilingual tasks.
the percentage of each em-bedding selected by the best concatenations fromall experiments of ace are shown in table 13.the best embedding concatenation varies over theoutput structure, syntactic/semantic level of under-standing, and the language.
the experimental re-sults show that it is essential to select embeddingsfor each kind of task separately.
however, we alsoﬁnd that the embeddings are strong in speciﬁc set-tings.
in comparison to the sequence-structured andgraph-structured tasks, we ﬁnd that m-bert andelmo are only frequently selected in sequence-structured tasks while xlm-r embeddings arealways selected in graph-structured tasks.
forflair embeddings, the forward and backward modelare evenly selected.
we suspect one direction offlair embeddings is strong enough.
therefore con-catenating the embeddings from two directions to-gether cannot further improve the accuracy.
fornon-contextualized embeddings, pretrained wordembeddings are frequently selected in sequence-structured tasks, and character embeddings are not.
when we dig deeper into the semantic and syntactictype of these two structured outputs, we ﬁnd thatin all best concatenations, bert embeddings areselected in all syntactic sequence-structured tasks,and flair, m-flair, word, and xlm-r embeddingsare selected in syntactic graph-structured tasks.
inmultilingual tasks, all best concatenations in mul-tilingual ner tasks select m-bert embeddingswhile m-bert is rarely selected in multilingualae tasks.
the monolingual flair embeddings arealways selected in ner tasks, and xlm-r is more.
2658frequently selected in multilingual tasks than mono-lingual sequence-structured tasks (ss)..2659bert+fine-tunembert+fine-tunexlm-r+fine-tuneroberta+fine-tunexlnet+fine-tuneace+fine-tune.
ner.
de76.981.687.7--88.3.de (revised)79.486.791.4--91.7.en89.292.094.193.993.694.6.es83.387.189.3--95.9.nl83.887.295.3--95.7.posritter ark tb-v294.491.791.293.991.590.895.493.792.395.493.992.094.492.488.495.894.493.4.table 9: a comparison between ace and the ﬁne-tuned embeddings that are used in ace for ner and postagging..bert+fine-tunembert+fine-tunexlm-r+fine-tuneroberta+fine-tunexlnet+fine-tuneace+fine-tune.
chunk.
aeconll 2000 14lap 14res 15res 16res73.973.678.980.773.481.3.
71.869.576.478.572.880.3.
81.283.585.983.984.587.4.
96.796.697.097.297.197.3.
87.785.090.590.288.992.0.tr.
nl.
ru.
es76.9 73.1 64.3 75.674.5 72.6 71.6 58.877.0 77.6 77.7 74.1.
--.
--.
--.
--.
79.9 80.5 79.4 81.9.table 10: a comparison between ace and the ﬁne-tuned embeddings we used in ace for chunking and ae..dpptb.
uas96.696.596.796.997.097.2.las95.194.995.495.695.695.7.dm.
id94.493.994.293.094.295.6.ood91.490.490.489.390.692.6.sdppas.
id94.493.994.694.394.895.8.ood93.092.193.292.893.494.6.psd.
id82.081.282.982.082.783.8.ood81.380.081.780.681.883.4.bert+fine-tunembert+fine-tunexlm-r+fine-tuneroberta+fine-tunexlnet+fine-tuneace+fine-tune.
table 11: a comparison between ace and the ﬁne-tuned embeddings that are used in ace for dp and sdp..allretrainace.
ner pos chunk96.790.692.496.890.892.696.891.793.0.ae73.273.675.6.dp-uas dp-las.
sdp-id sdp-ood.
96.796.896.9.
95.195.295.3.
94.394.594.5.
90.890.990.9.table 12: a comparison among retrained models, all and ace.
we use the one dataset for each task..ssgssem.
sssyn.
sssem.
gssyn.
gsm-nerm-ae.
bert m-bert char elmo f f-bw f-fw mf mf-bw mf-fw word xlm-r0.810.750.671.000.780.670.671.00.
0.85 0.70 0.48 0.59 0.780.25 0.83 0.75 0.42 0.830.80 0.60 0.40 0.53 0.870.92 0.83 0.58 0.67 0.670.33 0.78 0.67 0.56 0.780.00 1.00 1.00 0.00 1.000.83 1.00 0.78 1.00 0.890.33 0.58 0.42 0.42 0.75.
0.701.000.600.831.001.000.890.92.
0.590.580.600.580.560.670.780.25.
0.370.500.400.330.670.000.560.75.
0.740.170.730.750.220.001.000.33.
0.810.500.800.830.331.000.780.50.
0.410.580.530.250.670.330.440.75.table 13: the percentage of each embedding candidate selected in the best concatenations from ace.
f and mfare monolingual and multilingual flair embeddings.
we count these two embeddings are selected if one of theforward/backward (fw/bw) direction of flair is selected in the concatenation.
we count the word embedding isselected if one of the fasttext/glove embeddings is selected.
ss: sequence-structured tasks.
gs: graph-structuredtasks.
sem.
: semantic-level tasks.
syn.
: syntactic-level tasks.
m-ner: multilingual ner tasks.
m-ae: mul-tilingual ae tasks.
we only use english datasets in ss and gs.
english datasets are removed for m-ner andm-ae..2660