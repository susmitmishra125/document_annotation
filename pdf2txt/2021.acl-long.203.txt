missing modality imagination network for emotion recognition withuncertain missing modalities.
jinming zhao∗school of informationrenmin university of chinazhaojinming@ruc.edu.cn.
ruichen li∗school of informationrenmin university of chinaruichen@ruc.edu.cn.
qin jin †school of informationrenmin university of chinaqjin@ruc.edu.cn.
abstract.
multimodal fusion has been proved to improveemotion recognition performance in previousworks.
however, in real-world applications,we often encounter the problem of missingmodality, and which modalities will be miss-ing is uncertain.
it makes the ﬁxed multimodalfusion fail in such cases.
in this work, we pro-pose a uniﬁed model, missing modality imag-ination network (mmin), to deal with theuncertain missing modality problem.
mminlearns robust joint multimodal representations,which can predict the representation of anymissing modality given available modalitiesunder different missing modality conditions.
comprehensive experiments on two bench-the uniﬁedmark datasets demonstrate thatmmin model signiﬁcantly improves emotionrecognition performance under both uncertainmissing-modality testing conditions and full-modality ideal testing condition.
the codewill be available at https://github.com/aim3-ruc/mmin..1.introduction.
automatic multimodal emotion recognition is veryimportant to natural human-computer interactions(fragopanagos and taylor, 2002).
it aims to un-derstand and interpret human emotions expressedthrough multiple modalities such as speech con-tent, voice tones and facial expression.
previousworks have shown that these different modalitiesare complimentary for emotion expression, and pro-posed many effective multimodal fusion methodsto improve the emotion recognition performance(baltruˇsaitis et al., 2018; tsai et al., 2019; zhaoet al., 2018).
however, in real applications, manycommon causes can lead to the missing modalityproblem.
for example, the camera is turned off or.
∗equal contribution†corresponding author.
figure 1: illustration of a missing modality scenario formultimodal emotion recognition systems.
as shownin this video segment, we encounter the missing vi-sual modality problem due to the person’s face was ob-scured by her hands..blocked due to privacy issues; the speech contentis unavailable due to automatic speech recognitionerrors; the voice and text are missing due to thesilence of the user; or the faces cannot be detecteddue to lighting or occlusion issues as shown in fig-ure 1. existing multimodal fusion models trainedon full-modality samples usually fail when partialmodalities are missing (aguilar et al., 2019; phamet al., 2019; cai et al., 2018; parthasarathy andsundaram, 2020)..the missing modality problem has attractedmore research attention in the past years, and theexisting solutions for this problem are mainly basedon learning joint multimodal representation so thatall modality information can be encoded.
han et al.
(han et al., 2019) propose a joint training approachthat implicitly fuses multimodal information fromauxiliary modalities, which improves the mono-modal emotion recognition performance.
the re-cent cross-modality sequential translation-basedmethods proposed in (pham et al., 2019; wanget al., 2020) learn the joint multimodal representa-tions via translating a source modality to multipletarget modalities, which improves the performance.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2608–2618august1–6,2021.©2021associationforcomputationallinguistics2608...i can’tbelieveit!acousticvisualtextualmissingemotionrecognitionof the source modality as input at the test time.
however, these methods can only deal with thescenario where the source modality is input to thetrained model.
different models need to be built fordifferent missing modality cases1.
additionally, thesequential translation-based models require trans-lation and generation of videos, audios, and text,which are difﬁcult to train especially with limitedtraining samples (li et al., 2018; pham et al., 2019).
in this work, we propose a novel uniﬁed model,missing modality imagination network (mmin),to address the above issues.
speciﬁcally, the pro-posed mmin learns the robust joint multimodalrepresentations through cross-modality imagina-tion with cascade residual autoencoder (cra)(tran et al., 2017) and cycle consistency learning(zhu et al., 2017) based on sentence-level modality-speciﬁc representations, as the sentence-level rep-resentation is more reasonable for modeling thecross-modality emotion correlation.
the imagina-tion module aims to predict the sentence-level emo-tional representation of the missing modality fromthe other available modalities.
to the best of ourknowledge, this is the ﬁrst work that investigates auniﬁed model for multimodal emotion recognitionwith uncertain missing-modality..extensive experiments are carried out ontwo benchmark datasets, iemocap and msp-improv, under both uncertain missing-modalityand full-modality conditions.
the proposed mminmodel as a uniﬁed multimodal emotion recognitionmodel can learn robust joint multimodal represen-tations and outperforms the standard multimodalfusion models on both benchmark datasets underboth the uncertain missing-modality and the full-modality conditions.
furthermore, to evaluate theimagination ability of our mmin model, we visual-ize the distributions of the imagined representationsof the missing modalities and its ground-truth rep-resentations and ﬁnd they are very similar, whichdemonstrates that mmin can imagine the repre-sentations of the missing modalities based on therepresentations of the available modalities..in summary, the main contributions of this workare: 1) we propose a uniﬁed model, missingmodality imagination network (mmin), to im-prove the robustness of emotion recognition sys-tems under uncertain missing-modality testing con-.
1if there are audio(a),visual(v) and textual(t) three modal-ities, then the system needs 6 models trained under 6 missingmodality conditions {a}, {v}, {t}, {a,v}, {a,t} and {v,t}, plusone model trained under the full-modality data..ditions.
2) we design cross-modality imaginationbased on paired multimodal data and adopt cas-cade residual autoencoder (cra) and cycle con-sistency learning to learn the robust joint multi-modal representations.
3) extensive experimentson two benchmark datasets demonstrate the effec-tiveness of the proposed model which improves theemotion recognition performance under both theuncertain missing-modality and the full-modalityconditions..2 related work.
multimodal emotion recognition many previ-ous works have focused on fusing multimodal in-formation to improve emotion recognition perfor-mance.
temporal attention-based methods areproposed to use the attention mechanism to se-lectively fuse different modalities based on theframe-level or word-level temporal sequence, suchas gated multimodal unit (gmu) (aguilar et al.,2019), multimodal alignment model (mman)(xu et al., 2019) and multi-modal attention mech-anism (clstm-mma) (pan et al., 2020).
thesemethods use different uni-modal sub-networksto model the contextual representations for eachmodality and then use the multimodal attentionmechanism to selectively fuse the representationsof different modalities.
liang et al.
(lianget al., 2020) propose a semi-supervised multimodal(ssmm) emotion recognition model which usescross-modality emotional distribution matching toleverages unlabeled data to learn the robust rep-resentations and achieves state-of-the-art perfor-mance.
missing modality problem existing methods formissing modality problem can mainly be dividedinto three groups.
the ﬁrst group features thedata augmentation approach, which randomly ab-lates the inputs to mimic missing modality cases(parthasarathy and sundaram, 2020).
the secondgroup is based on generative methods to directlypredict the missing modalities given the availablemodalities (li et al., 2018; cai et al., 2018; suoet al., 2019; du et al., 2018).
the third group aimsto learn the joint multimodal representations thatcan contain related information from these modal-ities (aguilar et al., 2019; pham et al., 2019; hanet al., 2019; wang et al., 2020)..data augmentation methods: parthasarathy etal.
(parthasarathy and sundaram, 2020) proposea strategy to randomly ablate visual inputs during.
2609training at the clip or frame level to mimic real-world missing modality scenarios for audio-visualmultimodal emotion recognition, which improvesthe recognition performance under missing modal-ity conditions..generative methods: tran et al.
(tran et al.,2017) propose cascaded residual autoencoder(cra) to utilize the residual mechanism over theautoencoder structure, which can take the corrupteddata and estimate a function to well restore the in-complete data.
cai et al.
(cai et al., 2018) proposean encoder-decoder deep neural network to gen-erate the missing modality (positron emission to-mography, pet) given the available modality (mag-netic resonance imaging, mri), and the generatedpet can provide complementary information toimprove the detection and tracking of alzheimersdisease..learning joint multimodal representations:han et al.
(han et al., 2019) propose a joint train-ing model that consists of two modality-speciﬁcencoders and one shared classiﬁer, which implic-itly fuse the audio and visual information as jointrepresentations and improve the performance ofthe mono-modality emotion recognition.
phamet al.
(pham et al., 2019) propose a sequentialtranslation-based model to learn the joint repre-sentation between the source modality and multi-ple target modalities.
the hidden vectors of thesource modality encoder work as the joint repre-sentations, which improve the emotion recognitionperformance of the source modality.
wang et al.
(wang et al., 2020) follow this translation-basedmethod and propose a more efﬁcient transformer-based translation model with parallel translationincluding textual features to acoustic features andtextual features to visual features.
moreover, theabove two translation-based models adopt the for-ward translation and backward translation trainingstrategy to ensure that joint representations can re-tain maximal information from all modalities..3 method.
given a set of video segments s, we use x =(xa, xv, xt) to represent the raw multimodal fea-tures for a video segment s ∈ s, where xa, xv andxt represent the raw features of acoustic, visualand textual modalities respectively.
|s| representsthe number of video segments in set s. we denotethe target set y = {yi}|s|i=1, yi ∈ {0, 1, .
.
.
, c},where yi is the target emotion category of the video.
(available,missing)((xa), (xv, xt))((xv), (xa, xt))((xt), (xa, xv))((xa, xv), (xt))((xa, xt), (xv))((xv, xt), (xa)).
123456.uniﬁed triplet format pairsmiss), (xamiss), (xa, xvmiss, xt), (xa, xv, xtmiss), (xamiss, xt), (xamiss, xv, xt), (xa, xv.
((xa, xvmiss, xt((xamiss, xv, xt((xamiss, xv((xa, xv, xt((xa, xv(xa.
miss, xv, xt))miss, xt))miss))miss, xt))miss))miss)).
miss, xvmiss, xv, xtmiss, xt.
table 1: the six possible missing-modality conditionsand their uniﬁed format cross-modality pairs..segment si and |c| is the number of emotion cat-egories.
our proposed method aims to recognizethe emotion category yi for every video segment siwith full modalities, or with only partial modalitiesavailable, for the example shown in figure 1, thereexist only acoustic and textual modalities whenvisual modality is missing..3.1 missing modality imagination network.
in order to learn robust joint multimodal representa-tions, we propose a uniﬁed model, missing modal-ity imagination network (mmin), which can dealwith different uncertain missing-modality condi-tions in real application scenarios.
figure 2 illus-trates the framework of our proposed mmin modelwhich contains three main modules: 1) modalityencoder network for extracting modality-speciﬁcembeddings; 2) imagination module based on thecascade residual autoencoder (cra) and cycleconsistency learning for imagining the represen-tations of missing modalities given the represen-tations of the corresponding available modalities.
the latent vectors of the autoencoders in cra arecollected to form the joint multimodal represen-tations; 3) emotion classiﬁer for predicting theemotion category based on the joint multimodalrepresentations.
we introduce each module in de-tails in the following subsections..3.1.1 modality encoder network.
the modality encoder network is used to ex-tract the modality-speciﬁc utterance-level embed-dings based on the raw modality features x. asshown in figure 2(b), we ﬁrst pretrain the modal-ity encoder network in a multimodal emotionrecognition model and it is further trained withinmmin model.
we deﬁne the modality-speciﬁcembeddings of each modality as ha = enca(xa),hv = encv(xv), ht = enct(xt), where enca,encv and enct represent the acoustic, visual andtextual encoders respectively, and ha, hv and htrepresent the modality-speciﬁc embeddings gener-ated by the corresponding encoders respectively..2610figure 2: illustration of the missing modality imagination network (mmin) framework.
(a) mmin at the trainingstage (taking the visual modality missing condition as example).
mmin is trained with all six possible missingmodality conditions (table 1).
(b) modality encoder network.
the modality encoder network is pretrained in themultimodal emotion recognition task on the full-modality data and then it is updated during the mmin trainingas shown in the orange colored block in mmin.
the pretrained modality encoder network (gray colored blockin mmin) is similar to the modality encoder network, and the only difference is that it is ﬁxed during training.
(c) missing modality imagination network (mmin) at the inference stage (taking the visual modality missingcondition as an example).
mmin can inference under different missing modality conditions..3.1.2 missing modality condition creation.
given a training sample with all three modalities(xa, xv, xt), there are 6 different possible missing-modality conditions as shown in table 1. we canbuild a cross-modality pair (available, missing)under each missing-modality condition, where theavailable and missing mean the available modal-ities and the corresponding missing modalities re-spectively.
in order to ensure a uniﬁed model thatcan handle various missing-modality conditions,we enforce a uniﬁed triplet input format for themodality encoder network as (xa, xv, xt).
underthe missing-modality conditions, the raw featuresof the corresponding missing modalities are re-placed by zero vectors.
for example, the uniﬁedformat input of the available modalities under thevisual modality missing condition (case 1 in ta-ble 1) is formatted as (xa, xvrefers to zero vectors..miss, xt), where xv.
miss.
under the missing-modality training conditions,the input includes the cross-modality pairs refer-ring to available modalities and missing modali-ties in the uniﬁed triplet format (as shown in ta-.
ble 1).
the multimodal embeddings of these cross-modality pairs can be represented as (taking thevisual modality missing condition as example):h = concat(ha, hv.
ˆh = concat(ha.
miss, hv, ht.
miss, ht)miss).
(1).
miss, hv.
miss and htwhere hamiss represent themodality-speciﬁc embedding when the correspond-ing modality is missing, which is produced by thecorresponding modality encoder with input zerovectors..3.1.3.imagination module.
we propose an autoencoder-based imaginationmodule to predict the multimodal embeddingsof the missing modalities given the multimodalembeddings of the available modalities.
theimagination module is expected to learn the ro-bust joint multimodal representations through thecross-modality imagination.
as illustrated in fig-ure 2(a), we employ the cascade residual autoen-coder (cra) (tran et al., 2017) structure, whichhas sufﬁcient learning capacity and more stableconvergence than the standard autoencoder.
the.
2611modality encoder networkclassifier.... .
.
[]forward imaginationmodule𝑙!𝑙"𝑙#𝑐!𝑐"𝑐#𝑒𝑚𝑜𝑡𝑖𝑜𝑛category(𝑥$,𝑥%&’’(,𝑥))modality encoder networkclassifier.... .
.
[]forward imaginationmodule......𝑙!𝑙"𝑙#𝑐!𝑐"𝑐#ℒ*+’backward imagination moduleℒ,-./$.0pretrained modality encoder networkℎ!
imaginedembeddingℒ1$*2/$.0(𝑥$,𝑥%&’’(,𝑥))ℎ!!
imaginedembedding𝑙"𝑙#𝑙$(𝑥%&’’$,𝑥(,𝑥%&’’))joint representation(a)(b)(c)v-encoderl-encoderconcatenationℎ%a-encoderclassifierℎ&ℎ’modality encoder network𝑥&𝑥%𝑥’ℎℎ+ℎℎcra structure is constructed by connecting a se-ries of residual autoencoders (ras).
we fur-ther employ cycle consistency learning (zhu et al.,2017; wang et al., 2020) with a coupled net ar-chitecture with two independent networks to per-form imagination in two directions, including theforward (available → missing) and backward(missing → available) imagination directions..to be speciﬁc, we use a cra model with b rasand each ra is represented by φk, k = 1, 2, .
.
.
, b,and the calculation of each ra can be deﬁned as:.
(cid:26) ∆zk = φk(h),.
k = 1∆zk = φk(h + (cid:80)k−1j=1 ∆zj),.
k > 1.
(2).
where h is the extracted multimodal embeddingbased on the available modalities in a uniﬁed cross-modality pair format (eq.
(1)) and ∆zk representsthe output of the kth ra.
taking the visual modal-ity missing condition as example (as shown in fig-ure 2(a)), the forward imagination aims to predictthe multimodal embedding of the missing visualmodality based on the available acoustic and tex-tual modalities.
the forward imagined multimodalembedding is expressed as:.
3.2.joint optimization.
the loss function for mmin training includes threeparts: the emotion recognition loss lcls, forwardimagination loss lf orward, and backward imagina-tion loss lbackward:.
lcls = −.
h(p, q).
1|s|.
|s|(cid:88).
i=1.
lf orward =.
lbackward =.
1|s|.
|s|(cid:88).
i=1.
(cid:13)(cid:13)(cid:13).
ˆhi − h.(cid:48)i.
(cid:13)(cid:13)(cid:13).
2.
2.
1|s|.
|s|(cid:88).
i=1.
(cid:13)(cid:13)(cid:13)hi − h.(cid:48)(cid:48)i.
(cid:13)(cid:13)(cid:13).
2.
2.
(6).
where p is the true distribution of one-hot label andq is the prediction distribution calculated in eq.(5).
h(p, q) is the cross-entropy between distributionsp and q. hi and ˆhi are the ground-truth representa-tions extracted by the modality encoder network asshown in eq.(1).
we combine all the three lossesinto the joint objective function as below to jointlyoptimize the model parameters:.
l = lcls + λ1lf orward + λ2lbackward.
(7).
where λ1 and λ2 are weighting hyper parametersfor lf orward and lbackward respectively..(cid:48).
h.= imaginef orward(h) = h +.
∆zk.
(3).
4 experiments.
b(cid:88).
k=1.
4.1 dataset.
where imagine(·) represents the function of theimagination module.
the backward imaginationaims to predict the multimodal embedding of theavailable modalities based on the forward imaginedmultimodal embedding h(cid:48)(eq.(3)).
the backwardimagined multimodal embedding is expressed as:.
(cid:48)(cid:48).
h.= imaginebackward(h.).
(cid:48).
(4).
3.1.4 classiﬁer.
we collect the latent vectors of each auto-encoderin the forward imagination module and concatenatethem together to form the joint multimodal repre-sentation: r = concat(c1, c2, .
.
.
, cb), where ckis the latent vector of the autoencoder in the kthra.
based on the joint multimodal representationr, we calculate the probability distribution q as:.
q = sof tmax(fcls(r)).
(5).
where fcls(·) denotes the emotion classiﬁer thatconsists of several fully-connected layers..we evaluate our proposed model on two benchmarkmultimodal emotion recognition datasets, interac-tive emotional dyadic motion capture (iemo-cap) (busso et al., 2008) and msp-improv(busso et al., 2016).
the statistics of the twodatasets are shown in table 2..iemocap contains recorded videos in 5dyadic conversation sessions.
in each session, thereare multiple scripted plays and spontaneous dia-logues between a male and a female speaker and10 speakers in total in the database.
we followthe emotional label processing in (xu et al., 2019;liang et al., 2020) to form the four-class emotionrecognition setup..msp-improv contains recorded segmentsvideos in dyadic conversation scenarios with 12actors.
we ﬁrst remove videos that are shorterthan 1 second.
then we select the videos inthe “other-improvised” group which are recordedduring the improvisation scenarios with happy,anger, sadness, or neutral labels to form thefour-class emotion recognition setup..2612datasetiecmoapmsp-improv.
happy anger sadness neutral total5531108416363819627999.
17081733.
1103460.table 2: data statistics of datasets.
4.1.1 missing-modality training setwe ﬁrst deﬁne the original training set which con-tains all the three modalities as the full-modalitytraining set.
based on the full-modality trainingset, we construct another training set that con-tains cross-modality pairs to simulate the possiblemissing-modality conditions and we deﬁne it asthe missing-modality training set, which we useto train the proposed mmin.
six different cross-modality pairs (table 1) for each training sampleare generated.
therefore, the number of the gen-erated cross-modality pairs is six times as large asthe number of the full-modality training samples..4.1.2 missing-modality testing setwe ﬁrst deﬁne the original testing set which con-tains all the three modalities as the full-modalitytesting set.
to evaluate the performance of theproposed mmin under the uncertain missing-modality conditions, we construct six differentmissing modality testing subsets corresponding tothe six possible missing modality conditions re-spectively.
for example, in the inference stage,under the missing visual modality condition asshown in figure 2(c), the raw feature of a missing-modality testing sample in the uniﬁed format ismiss, xt).
we combine all the six missing-(xa, xvmodality testing subsets together and denote it asthe missing-modality testing set..et al., 2017) which is trained based on the facialexpression recognition plus (fer+) corpus (bar-soum et al., 2016).
we denote the facial expres-sion features as “denseface”.
the “denseface” areframe-level sequential features based on the de-tected faces from the video frames, and the featurevectors are in 342 dimensions..textual features: we extract contextual wordembeddings using a pretrained bert-large model(devlin et al., 2019) which is one of the state-of-the-art language representations.
we denote theword embeddings as “bert” and the features are in1024 dimensions..4.3 higher-level feature encoder.
to generate more efﬁcient sentence-level modality-speciﬁc representations for the imagination mod-ule, we design different modality encoders for dif-ferent modalities.
acoustic modality encoder (enca): we apply along short-term memory (lstm) network (saket al., 2014) to capture the temporal informationbased on the sequential frame-level raw acous-tic features xa.
then we use max-pooling to getutterance-level acoustic embedding ha based onthe lstm hidden states.
visual modality encoder (encv): we adopta similar method with enca on the sequentialframe-level facial expression features xv and getutterance-level visual embedding hv.
textual modality encoder (enct): we apply atextcnn (kim, 2014) to get the utterance-leveltextual embedding as ht based on the sequentialword-level features xt..4.2 raw feature extraction.
4.4 recognition baselines.
we follow feature extraction methods described in(liang et al., 2020; pan et al., 2020) and extract theframe-level raw features of each modality 2..acoustic features: opensmile toolkit (ey-ben et al., 2010) with the conﬁguration of“is13 compare” is used to extract frame-levelfeatures, which have similar performance withthe is10 utterance-level acoustic features used in(liang et al., 2020).
we denote the features as“compare” and the feature vectors are in 130 di-mensions..visual features: we extract the facial expres-sion features using a pretrained densenet (huang.
2to facilitate fair comparison with the sequentialtranslation-based missing modality method mctn, we adoptframe-level features which can be directly used in the mctnmethod.
our baseline model takes the structure as shownin figure 2(b), which is trained based on the full-modality training set and we use it as our full-modality baseline.
to improve the system robust-ness against the missing modality problem, one in-tuitive solution is to add samples under the missing-modality conditions into the training set.
we, there-fore, pool the missing-modality training set andfull-modality training set together to train the base-line model and use it as our augmented baseline..4.5.implementation details.
table 3 presents our implementation details.
weuse the 10-fold and 12-fold speaker-independentcross-validation to evaluate the models on iemo-cap and msp-improv respectively.
for the ex-periments on iemocap, we take four sessions for.
2613acoustic encodervisual encodertextual encoder.
emotion classiﬁercra.
parameters λ1, λ2learning rate.
single layer lstm with hidden size of 128single layer lstm with hidden size of 1283 conv blocks in textcnn with kernel size{3,4,5} and output layer with 128 channels3 fc layers of size {128,64,4}5 residual-ras with ra-layers in size 384-256-128-64-128-256-384 (latent-vector size: 64)both set as 0.1adam optimizer with learning rate of 0.001,relu activation.
table 3: implementation details.
our full-modality baselineclstm-mma(pan et al., 2020)ssmm(liang et al., 2020).
train.
test.
{a, v, t}.
{a, v, t}.
wa0.76510.73940.7560.ua0.7779–0.7450.table 4: multimodal emotion recognition results oniemocap under full-modality condition..training, and the remaining session is split by speak-ers into the validation and testing sets.
for msp-improv, we take the utterances of 10 speakers fortraining, the remaining 2 speakers are divided intovalidation set and testing set by speakers.
we trainthe model with at most 100 epochs for each experi-ment.
we select the best model on the validationset and report its performance on the testing set.
todemonstrate the robustness of our models, we runeach model three times to alleviate the inﬂuencesof random initialization of parameters and apply asigniﬁcance test for model comparison.
all modelsare implemented with pytorch deep learning toolkitand run on a single nvidia gtx 1080ti graphiccard..for the experiments on iemocap, we use twoevaluation metrics: weighted accuracy (wa) andunweighted accuracy (ua).
due to the imbalanceof emotion categories on msp-improv, we usethe f-score as the evaluation metric..4.6 full-modality baseline results.
we ﬁrst compare our full-modality baseline withseveral state-of-the-art multimodal recognitionmodels under the full-modality condition.
resultsin table 4 show that our full-modality baselineoutperforms other state-of-the-art models, whichproves that our modality encoder network can ex-tract effective representations for multimodal emo-tion recognition..4.7 uncertain missing-modality results.
table 5 presents the experimental results of ourproposed mmin model under different missing-modality testing conditions and full-modality test-ing condition.
on iemocap, comparing to the.
“full-modality baseline” results in table 4, wesee a signiﬁcant performance drop under uncer-tain missing-modality testing conditions, whichindicates that the model trained under the full-modality condition is very sensitive to the missingmodality problem.
the intuitive solution “aug-mented baseline”, which combines the missing-modality training set with the full-modality train-ing set to train the baseline model, does signif-icantly improves over the full-modality baselineunder missing-modality testing conditions, whichindicates that data augmentation can help alleviatethe problem of data mismatch between training andtesting.
more notably, our proposed mmin signiﬁ-cantly outperforms both the full-modality baselineand the augmented baseline under every possiblemissing-modality testing condition.
it also outper-forms the two baselines under the full-modalitytesting condition, even though the mmin modeldoes not use the full-modality training data.
theseresults indicate that our proposed mmin modelcan learn robust joint multimodal representation sothat it can achieve consistently better performanceunder both the different missing-modality and thefull-modality testing conditions.
this is becauseour proposed mmin method not only has the dataaugmentation capability, but also can learn betterjoint representation, which can preserve informa-tion of other modalities..we further analyze the performance under dif-ferent missing modality conditions.
our mminmodel achieves signiﬁcant improvement under onemodality available conditions ({a}, {v}, or {t})compared with the augmented baseline, especiallyfor the weak modalities {a} and {v}.
it bringssome improvements as well over the augmentedbaseline even for the strong modality combinations,such as {a, t}.
these experimental results indicatethat the learned joint representation via mmin didlearn complementary info from the other modalitiesto compensate for the weak modalities..the bottom block in table 5 shows the perfor-mance comparison on the msp-improv dataset.
our proposed mmin model again signiﬁcantlyoutperforms the two baselines under differentmissing-modality and full-modality testing condi-tions, which demonstrates the good generalizationability of mmin across different datasets..we also compare to the mctn (pham et al.,2019) model which is the state-of-the-art modelfor the missing modality problem.
as mctn can-.
2614dateset.
model.
iemocap.
msp-improv.
full-modality baseline.
augmented baseline.
proposed mmin.
mctn (pham et al., 2019).
full-modality baselineaugmented baselineproposed mminmctn (pham et al., 2019).
metric.
wa(↑)ua(↑)wa(↑)ua(↑)wa(↑)ua(↑)wa(↑)ua(↑)f1(↑)f1(↑)f1(↑)f1(↑).
testing condition.
{a}0.41900.47190.53030.54400.56580.59000.49750.51620.28240.42780.46470.3285.
{v}0.45740.39660.48640.45980.52520.51600.48920.45730.32950.41850.44710.3810.
{t}0.56460.55490.65640.66910.66570.68020.62420.63780.45760.55440.55730.5050.
{a, v}0.54880.57620.63950.64340.63990.65430.56340.55840.47210.53960.57400.4683.
{a, t}0.70180.72570.72510.74350.72940.75140.68340.69460.56550.60380.61880.5611.
{v, t}0.62170.59710.70820.71620.72670.73610.67840.68340.53680.62950.64110.5886.
{a, v, t}0.76510.77790.76170.77670.7650.average0.55220.55370.6243∗0.6293∗0.6410∗(cid:78)0.6524∗(cid:78) 0.7812∗(cid:78)0.5894∗–0.5913∗–0.65230.45430.5455∗0.6663∗0.5649∗(cid:78) 0.6855∗(cid:78)0.4721∗.
–.
table 5: performance comparison under six possible missing-modality testing conditions and the full-modalitytesting condition (i.e.
testing condition “{a}” means that only the acoustic modality is available and both visual andtextual modalities are missing.
“{a, v, t}” refers to the full-modality testing condition where all acoustic, visualand textual modalities are available) “average” refers to the average performance over all six missing-modalityconditions.
t-test is conducted on average and {a,v,t} column.
∗ indicates that p-value < 0.05 (compared withfull-modality baseline).
(cid:78) indicates that p-value < 0.05 (compared with augmented baseline)..not handle different missing-modality conditionsin one uniﬁed model, so we have to train a particu-lar model under each missing-modality condition3.
the comparison results demonstrate that our pro-posed mmin model not only can handle both thedifferent missing-modality and the full-modalitytesting condition with a uniﬁed model, but also canconsistently outperform the mctn models underall missing-modality conditions..4.8 ablation study.
we conduct experiments to ablate the contributionsof different components in mmin, including thestructure of the imagination module and the cyclicconsistency learning..structure of the imagination module.
we ﬁrstinvestigate the impact of different network struc-tures on the performance in the imagination mod-ule.
speciﬁcally, we compare the autoencoder andthe cra structure in mmin, and we adopt thesame parameter scale to ensure the fairness of thecomparison.
as shown in table 6, the performanceof the imagination module with autoencoder struc-ture “mmin-ae” is worse than that with the crastructure under both different missing-modality andfull-modality testing conditions.
the performancecomparison indicates that the cra has a strongerimagination ability than the autoencoder model..cycle consistency learning.
to evaluate theimpact of the cyclic consistency learning in mmin,.
3we use features described in sec.
4.3 and follow thetraining setting in (pham et al., 2019) to conduct the mctnexperiments.
the mctn model cannot be evaluated under thefull-modality testing condition because the target modalitiescannot be none..we conduct experiments using mmin with or with-out cycle consistency learning.
as shown in ta-ble 6, the model trained without cycle consistencylearning results in performance loss under all con-ditions, which indicates that the cycle consistencylearning can enhance the imagination ability andlearn more robust joint multimodal representations..4.9 analysis of mmin core competence.
we conduct detailed experiments on iemocap todemonstrate the joint representation learning abilityand the imagination ability of our mmin model.
joint representation learning ability: since thejoint representation is expected to retain informa-tion of multiple modalities, we conduct experi-ments to evaluate the joint representation learningability of mmin.
we compare mmin to the base-line model under the matched-modality conditionin which the training data and the test data containthe same modalities.
as shown in table 7, com-paring to the baseline model, mmin achieves onpar with or even better performance, which demon-strates that mmin has the ability to learn effec-tive joint multimodal representations.
we also no-tice that the data-augmented model cannot beat thecorresponding matching partial-modality baselinemodel, which indicates the data-augmented modelcannot learn the joint representation.
imagination ability: figure 3 visualizes the distri-bution of the ground-truth multimodal embeddings(ˆh in figure 2) and mmin imagined multimodalembeddings (h(cid:48)in figure 2) for a male speakerand female speaker using t-sne (maaten and hin-ton, 2008).
we observe that the distribution of.
2615model.
mmin-ae.
mmin-nocycle.
mmin.
metric.
wa(↑)ua(↑)wa(↑)ua(↑)wa(↑)ua(↑).
{a}0.54040.56250.55030.58210.56580.5900.
{v}0.50250.48360.51160.50060.52520.5160.
{t}0.65880.66890.65770.67050.66570.6802.testing condition{a, v}0.61150.62460.62390.64540.63990.6543.
{a, t}0.72030.73740.71850.74380.72940.7514.
{v, t} average0.62440.71250.63680.71870.63040.72020.64540.73010.64100.72670.65240.7361.
{a, v, t}0.76190.76770.74980.77090.76500.7812.table 6: experimental results for component contribution evaluation on iemocap.
“mmin-ae” denotes replac-ing the cra structure with the autoencoder structure in the imagination module.
“mmin-nocycle” denotesremoving the cycle consistency learning in mmin..comparedensefacebertcompare+densefacecompare+bertbert+densefacecompare+bert+denseface.
trainavta, va, tv, ta, v, t.testavta, va, tv, ta, v, t.baseline augmented mmin0.59000.54400.57600.51600.45980.50640.68020.66910.68730.65430.64340.63800.75140.74350.75330.73610.71620.71770.78120.77670.7779.table 7: evaluation (ua) of the joint representation learning ability on iemocap.
“baseline” denotes the resultsindividually train with cross-entropy loss on partial modalities samples.
“augmented” and “mmin” denote theevaluation results of our uniﬁed data-augmented baseline model and mmin model under different test conditions,which are the same as in table 5..robust joint multimodal representations throughcross-modality imagination via the cascade resid-ual autoencoder and cycle consistency learning.
extensive experiments on two public benchmarkdatasets demonstrate the effectiveness and robust-ness of our proposed model, which signiﬁcantlyoutperforms other baselines under both uncertainmissing-modality and full-modality conditions..in the future work, we will explore ways to fur-ther improve the robust joint multimodal represen-tation..acknowledgments.
this work wassupported by the nationalkey r&d program of china under grant no.
2020aaa0108600, national natural sciencefoundation of china (no.
62072462), na-tional natural science foundation of china (no.
61772535), beijing natural science foundation(no.
4192028)..references.
gustavo aguilar, viktor rozgic, weiran wang, andchao wang.
2019. multimodal and multi-view mod-els for emotion recognition.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 991–1002..tadas baltruˇsaitis, chaitanya ahuja,.
and louis-philippe morency.
2018. multimodal machine learn-ieee transac-ing: a survey and taxonomy.
tions on pattern analysis and machine intelligence,41(2):423–443..(a) a female speaker.
(b) a male speaker.
figure 3: visualization of the ground-truth and imag-ined multimodal embeddings.
for example, a denotesthe ground-truth multimodal embeddings of the acous-tic modality.
a imagined denotes the mmin imaginedmultimodal embeddings of the acoustic modality basedon visual and textual modalities..the ground-truth embeddings and imagined embed-dings are very similar, although the distribution ofvisual modality embeddings deviates a little, it ismainly because the quality of the visual modality ispoor in this dataset.
it demonstrates that mmin canimagine the representations of the missing modali-ties based on the available modalities..5 conclusion.
in this paper, we propose a novel uniﬁed multi-modal emotion recognition model, missing modal-ity imagination network (mmin), to improve theemotion recognition performance under uncertainmissing-modality conditions in real applicationscenarios.
the proposed mmin can learn the.
2616emad barsoum, cha zhang, cristian canton ferrer,and zhengyou zhang.
2016. training deep net-works for facial expression recognition with crowd-sourced label distribution.
in icmi, icmi ’16, page279283, new york, ny, usa.
association for com-puting machinery..carlos busso, murtaza bulut, chi-chun lee, abekazemzadeh, emily mower, samuel kim, jean-nette n. chang, sungbok lee, and shrikanth s.interactive emotionalnarayanan.
2008.language re-dyadic motion capture database.
sources and evaluation, 42(4):335–359..iemocap:.
carlos busso, srinivas parthasarathy, alec burmania,mohammed abdelwahab, najmeh sadoughi, andemily mower provost.
2016. msp-improv: anacted corpus of dyadic interactions to study emotionperception.
ieee transactions on affective comput-ing, 8(1):67–80..lei cai, zhengyang wang, hongyang gao, dinggangshen, and shuiwang ji.
2018. deep adversariallearning for multi-modality missing data completion.
in proceedings of the 24th acm sigkdd interna-tional conference on knowledge discovery & datamining, pages 1158–1166..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..changde du, changying du, hao wang, jinpeng li,wei-long zheng, bao-liang lu, and huiguang he.
2018. semi-supervised deep generative modellingof incomplete multi-modality emotional data.
inproceedings of the 26th acm international confer-ence on multimedia, pages 108–116..florian eyben, martin w¨ollmer, and bj¨orn schuller.
the munich versatile and fastin acm mul-.
2010. opensmile:open-source audio feature extractor.
timedia, pages 1459–1462..n fragopanagos and j. g. taylor.
2002. emotionieee.
recognition in human-computer interaction.
signal processing magazine, 18(1):32–80..jing han, zixing zhang, zhao ren, and bj¨orn schuller.
2019.implicit fusion by joint audiovisual train-ing for emotion recognition in mono modality.
in icassp 2019-2019 ieee international confer-ence on acoustics, speech and signal processing(icassp), pages 5861–5865.
ieee..yoon kim.
2014..works for sentence classiﬁcation.
arxiv:1408.5882..convolutional neural net-arxiv preprint.
yitong li, martin min, dinghan shen, david carlson,and lawrence carin.
2018. video generation fromtext.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, volume 32..jingjun liang, ruichen li, and qin jin.
2020. semi-supervised multi-modal emotion recognition withcross-modal distribution matching.
in proceedingsof the 28th acm international conference on multi-media, pages 2852–2861..laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(nov):2579–2605..zexu pan, zhaojie luo, jichen yang, and haizhou li.
2020. multi-modal attention for speech emotionrecognition.
proc.
interspeech 2020, pages 364–368..srinivas parthasarathy and shiva sundaram.
2020.training strategies to handle missing modalities foraudio-visual expression recognition.
in companionpublication of the 2020 international conference onmultimodal interaction, pages 400–404..hai pham, paul pu liang, thomas manzini, louis-philippe morency, and barnab´as p´oczos.
2019.found in translation: learning robust joint represen-tations by cyclic translations between modalities.
inproceedings of the aaai conference on artiﬁcial in-telligence, volume 33, pages 6892–6899..has¸im sak, andrew senior, and franc¸oise bea-ufays.
2014.long short-term memory basedrecurrent neural network architectures for largearxiv preprintvocabulary speech recognition.
arxiv:1402.1128..qiuling suo, weida zhong, fenglong ma, ye yuan,jing gao, and aidong zhang.
2019. metric learn-ing on healthcare data with incomplete modalities.
in ijcai, pages 3534–3540..luan tran, xiaoming liu, jiayu zhou, and rong jin.
2017. missing modalities imputation via cascadedin proceedings of the ieeeresidual autoencoder.
conference on computer vision and pattern recog-nition, pages 1405–1414..yao-hung hubert tsai, shaojie bai, paul pu liang,j zico kolter, louis-philippe morency, and rus-lan salakhutdinov.
2019. multimodal transformerfor unaligned multimodal language sequences.
inproceedings of the conference.
association for com-putational linguistics.
meeting, volume 2019, page6558. nih public access..gao huang, zhuang liu, laurens van der maaten, andkilian q weinberger.
2017. densely connected con-in proceedings of the ieeevolutional networks.
conference on computer vision and pattern recogni-tion, pages 4700–4708..zilong wang, zhaohong wan, and xiaojun wan.
2020.transmodality: an end2end fusion method withtransformer for multimodal sentiment analysis.
inproceedings of the web conference 2020, pages2514–2520..2617haiyang xu, hui zhang, kun han, yun wang, yipingpeng, and xiangang li.
2019. learning alignmentfor multimodal emotion recognition from speech.
arxiv preprint arxiv:1909.05645..jinming zhao, ruichen li, shizhe chen, and qin jin.
2018. multi-modal multi-cultural dimensional con-tinues emotion recognition in dyadic interactions.
inproceedings of the 2018 on audio/visual emotionchallenge and workshop, pages 65–72..jun-yan zhu, taesung park, phillip isola, and alexei aefros.
2017. unpaired image-to-image translationusing cycle-consistent adversarial networks.
in pro-ceedings of the ieee international conference oncomputer vision, pages 2223–2232..2618