lgesql: line graph enhanced text-to-sql model with mixed localand non-local relationsruisheng cao1, lu chen1,2∗, zhi chen1, yanbin zhao1,su zhu3 and kai yu1,2∗1x-lance lab, department of computer science and engineeringmoe key lab of artiﬁcial intelligence, ai institute, shanghai jiao tong universityshanghai jiao tong university, shanghai, china2state key lab of media convergence production technology and systems, beijing, china3aispeech co., ltd., suzhou, china{211314,chenlusz,kai.yu}@sjtu.edu.cn.
abstract.
fail.
this work aims to tackle the challenging het-erogeneous graph encoding problem in thetext-to-sql task.
previous methods are typi-cally node-centric and merely utilize differentweight matrices to parameterize edge types,which 1) ignore the rich semantics embed-ded in the topological structure of edges,and 2)to distinguish local and non-local relations for each node.
to this end,we propose a line graph enhanced text-to-sql (lgesql) model to mine the underlyingrelational features without constructing meta-paths.
by virtue of the line graph, messagespropagate more efﬁciently through not onlyconnections between nodes, but also the topol-ogy of directed edges.
furthermore, both lo-cal and non-local relations are integrated dis-tinctively during the graph iteration.
we alsodesign an auxiliary task called graph pruningto improve the discriminative capability of theencoder.
our framework achieves state-of-the-art results (62.8% with glove, 72.0% withelectra) on the cross-domain text-to-sqlbenchmark spider at the time of writing..1.introduction.
the text-to-sql task (zhong et al., 2017; xu et al.,2017) aims to convert a natural language questioninto a sql query, given the corresponding databaseschema.
it has been widely studied in both aca-demic and industrial communities to build naturallanguage interfaces to databases (nlidb, androut-sopoulos et al., 1995)..one daunting problem is how to jointly encodethe question words and database schema items (in-cluding tables and columns), as well as variousrelations among these heterogeneous inputs.
typ-ically, previous literature utilizes a node-centricgraph neural network (gnn, scarselli et al., 2008).
figure 1: two limitations if edge features are retrievedfrom a ﬁxed-size embedding matrix: (a) fail to discoveruseful meta-paths, and (b) unable to differentiate localand non-local neighbors..to aggregate information from neighboring nodes.
gnnsql (bogin et al., 2019a) adopts a relationalgraph convolution network (rgcn, schlichtkrullet al., 2018) to take into account different edgetypes between schema items, such as t-has-crelationship 1, primary key and foreign key con-straints.
however, these edge features are directlyretrieved from a ﬁxed-size parameter matrix andmay suffer from the drawback: unaware of con-textualized information, especially the structuraltopology of edges.
meta-path is deﬁned as a com-posite relation linking two objects, which can beused to capture multi-hop semantics.
for example,in figure 1(a), relation q-exactmatch-c andc-belongsto-t can form a 2-hop meta-path in-dicating that some table t has one column exactlymentioned in the question..although ratsql (wang et al., 2020a) in-troduces some useful meta-paths such as c-sametable-c, it treats all relations, either 1-hop.
∗the corresponding authors are lu chen and kai yu..and c represent table and column nodes..1for abbreviation, q represents question node, while t.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2541–2555august1–6,2021.©2021associationforcomputationallinguistics2541or multi-hop, in the same manner (relative posi-tion embedding, shaw et al., 2018) in a completegraph.
without distinguishing local and non-localneighbors, see figure 1(b), each node will attendto all the other nodes equally, which may lead tothe notorious over-smoothing problem (chen et al.,2020a).
besides, meta-paths are currently con-structed by domain experts or explored by breadth-ﬁrst search (kong et al., 2012).
unfortunately, thenumber of possible meta-paths increases exponen-tially with the path length, and selecting the mostimportant subset among them is an np-completeproblem (lao and cohen, 2010)..to address the above limitations, we pro-pose a line graph enhanced text-to-sqlmodel (lgesql), which explicitly considers thetopological structure of edges.
according to thedeﬁnition of a line graph (gross and yellen, 2005),we ﬁrstly construct an edge-centric graph from theoriginal node-centric graph.
these two graphs cap-ture the structural topology of nodes and edges,respectively.
iteratively, each node in either graphgathers information from its neighborhood and in-corporates edge features from the dual graph toupdate its representation.
as for the node-centricgraph, we combine both local and non-local edgefeatures into the computation.
local edge featuresdenote 1-hop relations and are dynamically pro-vided by node embeddings in the line graph, whilenon-local edge features are directly extracted froma parameter matrix.
this distinction encouragesthe model to pay more attention to local edge fea-tures while maintaining information from multi-hop neighbors.
additionally, we propose an aux-iliary task called graph pruning.
it introduces aninductive bias that the heterogeneous graph encoderof text-to-sql should be intelligent to extract thegolden schema items related to the question fromthe entire database schema graph..experimental results on benchmark spider (yuet al., 2018b) demonstrate that our lgesqlmodel promotes the exact set match accuracy to62.8% (with glove, pennington et al.
2014) and72.0% (with pretrained language model elec-tra, clark et al.
2020).
our main contributions aresummarized as follows:.
• we propose to model the 1-hop edge featureswith a line graph in text-to-sql.
both non-local and local features are integrated duringthe iteration process of node embeddings..• we design an auxiliary task called graph prun-.
ing, which aims to determine whether eachnode in the database schema graph is relevantto the given question..• empirical results on dataset spider demon-strate that our model is effective, and weachieve state-of-the-art performances bothwithout and with pre-trained language models..2 preliminaries.
1 , ct1.
1 , ct2.
2 , · · · , ct2.
problem deﬁnition given a natural languagequestion q = (q1, q2, · · · , q|q|) with length |q|and the corresponding database schema s =t ∪ c, the target is to generate a sql queryy. the database schema s contains multipletables t = {t1, t2, · · · } and columns c ={ct12 , · · · }.
each table ti is de-scribed by its name and is further composed of sev-eral words (ti1, ti2, · · · ).
similarly, we use wordj1, ctiphrase (ctij ∈ ti.
besides, each column ctij0 toconstrain its cell values (e.g.
text and number).
the entire input node-centric heterogeneousgraph gn = (v n, rn) consists of all three types ofnodes mentioned above, that is v n = q ∪ t ∪ cwith the number of nodes |v n| = |q| + |t | + |c|,where |t | and |c| are the number of tables andcolumns respectively..j2, · · · ) to represent column cti.
j also has a type ﬁeld cti.
r2→ · · ·.
meta-path as shown in figure 1(a), a meta-pathrl→ τl+1, wherer1→ τ2represents a path τ1the target vertex type of previous relation ri−1equals to the source vertex type τi of the currentrelation ri.
it describes a composite relation r =r1 ◦ r2 · · · ◦ rl between nodes with type τ1 and τl+1.
in this work, τi ∈ {question,table,column}.
throughout our discussion, we use the term localto denote relations with path length 1, while non-local relations refer to meta-paths longer than 1.the relational adjacency matrix rn contains bothlocal and non-local relations, see appendix a forenumeration..st ∈ rn, or vn.
i , i = 1, 2, · · · , |v e|line graph each vertex vein the line graph ge = (v e, re) can be uniquelys → vnmapped to a directed edge rnt ,in the original node-centric graph gn = (v n, rn).
function f maps the source and target node indextuple (s, t) into the “edge” index i = f (s, t) inge.
the reverse mapping is f -1. in the line graphij ∈ re exists from node vege, a directed edge reito vef -1(i) and the.
j , iff the target node of edge rn.
2542f -1(j) in gn are exactly thesource node of edge rnsame node.
actually, reij captures the informationﬂow in meta-path rnf -1(i) ◦ rnf -1(j).
we prevent back-tracking cases where two reverse edges will not beconnected in ge, illustrated in figure 2..we only utilize local relations in rn as the nodeset v e to avoid creating too many nodes in theline graph ge.
symmetrically, each edge in recan be uniquely identiﬁed by the node in v n. forexample, in the upper right part of figure 2, theedge between nodes “e1” and “e2” in the line graphcan be represented by the middle node with doublesolid borderlines in the original graph..figure 2: construction of a line graph.
for clarity, wesimplify the notation of edges..3 method.
after constructing the line graph, we utilize theclassic encoder-decoder architecture (sutskeveret al., 2014; bahdanau et al., 2015) as the backboneof our model.
lgesql consists of three parts: agraph input module, a line graph enhanced hiddenmodule, and a graph output module (see figure 3for an overview).
the ﬁrst two modules aim tomap the input heterogeneous graph gn into nodeembeddings x ∈ r|v n|×d, where d is the graphhidden size.
the graph output module retrieves andtransforms x into the target sql query y..3.1 graph input module.
this module aims to provide the initial embed-dings for both nodes and edges.
initial local edgefeatures z0 ∈ r|v e|×d and non-local edge fea-tures znlc ∈ r(|rn|−|v e|)×d are directly retrievedfrom a parameter matrix.
for nodes, we can ob-tain their representations from either word vectorsglove (pennington et al., 2014) or a pre-trainedlanguage model (plm) such as bert (devlin et al.,2019)..glove each word qi in the question q orschema item ti ∈ t or ctij ∈ c can be initialized bylooking up the embedding dictionary without con-sidering the context.
then, these vectors are passedinto three type-ware bidirectional lstms (bil-stm, hochreiter and schmidhuber, 1997) respec-tively to attain contextual information.
we con-catenate the forward and backward hidden statesfor each question word qi as the graph input x0qi.
as for table ti, after feeding (ti0, ti1, ti2, · · · ) intothe bilstm (special type ti0 = “table”, ∀i), weconcatenate the last hidden states in both direc-tions as the graph input x0ti (similarly for columnctij ).
these node representations are stacked to-gether to form the initial node embeddings matrixx0 ∈ r|v n|×d..2 · · ·.
1 ct1.
20ct2.
20ct1.
10ct1.
plm firstly, we ﬂatten all question words andschema items into a sequence, where columnsbelong to the same table are clustered together 2:[cls]q1q2 · · · q|q|[sep]t10t1ct11 ct2t20t2ct210ct22 · · · [sep].
the type informationti0 or ctij0 is inserted before each schema item.
since each word w is tokenized into sub-words,we append a subword attentive pooling layerafter plm to obtain word-level representations.
concretely, given the output sequence of subword2, · · · , wsfeatures wsi inw, the word-level representation w is 3.
|w| for each subword ws.
1, ws.
ai =softmaxi tanh(wsaiwsi ,.
w =.
(cid:88).
i ws)vts ,.
i.where vs and ws are trainable parameters.
afterobtaining the word vectors, we also feed them intothree bilstms according to the node types andget the graph inputs x0 for all nodes..3.2 line graph enhanced hidden module.
it contains a stack of l dual relational graph atten-tion network (dual rgat) layers.
in each layerl, two rgats (wang et al., 2020b) capture thestructure of the original graph and line graph, re-spectively.
node embeddings in one graph play therole of edge features in another graph.
for example,the edge features used in graph gn are provided bythe node embeddings in graph ge..we use xl ∈ r|v n|×d to denote the inputnode embedding matrix of graph gn in the l-th.
2we randomly shufﬂe the order of tables and columns in.
different mini-batches to discourage over-ﬁtting..3vectors throughout this paper are all row vectors..2543figure 3: the overall model architecture.
we use bidirectional edges in practice but only draw unidirectional edgesfor better understanding.
in the dual rgat module, we take the node with index 4 and the edge with label 4-5 asthe main focuses..i ∈ v n, we use xl.
layer, l ∈ {0, 1, · · · , l − 1}.
as for each spe-ciﬁc node vni. similarly, ma-trix zl ∈ r|v e|×d and vector zli are used to denotenode embeddings in the line graph.
following rat-sql (wang et al., 2020a), we use multi-head scaleddot-product (vaswani et al., 2017) to calculate theattention weights.
for brevity, we formulate the en-tire computation in one layer as two basic modules:.
xl+1 =rgatn(xl, [zl; znlc], gn),zl+1 =rgate(zl, xl, ge),.
where znlc is the aforementioned non-local edgefeatures in the original graph gn..3.2.1 rgat for the original graphgiven the node-centric graph gn, the output repre-sentation xl+1.
of the l-th layer is computed by.
i.
˜αhq )(xliwhji =(xlji =softmaxj(˜αhαh.
jwhk + [ψ(rnji/(cid:112)d/h),.
ji)]h.h )t,.
˜xl.
i =.
h(cid:110)h=1.
(cid:88).
ji(xlαh.
jwh.
v + [ψ(rn.
ji)]h.h ),.
vnj ∈n ni˜xl+1i =layernorm(xli =layernorm(˜xl+1xl+1.
i + ˜xliwo),i + ffn(˜xl+1.
i.
)),.
q , wh.
k , wh.
where (cid:107) represents vector concatenation, matricesv ∈ rd×d/h , wo ∈ rd×d are train-whable parameters, h is the number of heads andffn(·) denotes a feedforward neural network.
n ni.ji.
operator [·]h.represents the receptive ﬁeld of node vni and func-tion ψ(rnji) returns a d-dim feature vector of rela-tion rnh ﬁrst evenly splits the vectorinto h parts and returns the h-th partition.
sincethere are two genres of relations (local and non-local), we design two schemes to integrate them:.
if rnmixed static and dynamic embeddingsjiis a local relation, ψ(rnji) returns the node embed-ding zlf (j,i) from the line graph4.
otherwise, ψ(rnji)directly retrieves the vector from the non-local em-bedding matrix znlc, see figure 4. the neighbor-hood function n nfor node vni returns the entireinode set v n and is shared across different heads..figure 4: mixed static and dynamic embeddings..multi-head multi-view concatenation an al-ternative is to split the muli-head attention moduleinto two parts.
in half of the heads, the neighbor-hood function n ni of node vni only contains nodesthat are reachable within 1-hop.
in this case, ψ(rnji)returns the layer-wise updated feature zlf (j,i) from.
4function f maps the tuple of source and target node.
indices in gn into the corresponding node index in ge..2544zl.
in the other heads, each node has access toboth local and non-local neighbors, and ψ(·) al-ways returns static entries in the embedding matrixznlc ∪ z0, see figure 5 for illustration..figure 5: multi-head multi-view concatenation..in either scheme, the rgat module treats localand non-local relations differently and relativelymanipulates the local edge features more carefully..3.2.2 rgat for the line graphsymmetrically, given edge-centric graph ge, theupdated node representation zl+1i is calcu-lated similarly with little modiﬁcations:.
from zl.
i.q + [φ(re.
˜βhiuhji =(zlji =softmaxj( ˜βhβh.
h )(zlji)]hji/(cid:112)d/h),.
juh.
k)t,.
ji(zlβh.
juh.
v + [φ(re.
ji)]h.h ),.
˜zli =.
h(cid:110)h=1.
(cid:88).
vej ∈n ei˜zl+1i =layernorm(zli =layernorm(˜zl+1zl+1.
i + ˜zliuo),i + ffn(˜zl+1.
i.
))..here φ(reji) returns the feature vector of relationji in ge.
since we only consider local relations inrethe line graph, n ei only includes 1-hop neighbousand φ(reji) equals to the source node embedding inxl of edge vei .
attention that the relational featureis added on the “query” side instead of the “key”side when computing attention logits ˜βhji cause it isirrelevant to the incoming edges.
for example, infigure 3, the connecting nodes of two edge pairs(1-4, 4-5) and (2-4, 4-5) are the same node withv ∈ rd×d/h , uo ∈ rd×d areindex 4. uhk, uhtrainable parameters..q , uh.
the output matrices of the ﬁnal layer l are thedesired outputs of the encoder: x = xl, z = zl..3.3 graph output module.
this module includes two tasks: one decoder forthe main focus text-to-sql and the other one toperform an auxiliary task called graph pruning.
weuse the subscript to denote the collection of node.
embeddings with a speciﬁc type, e.g., xq is thematrix of all question node embeddings..3.3.1 text-to-sql decoder.
we adopt the grammar-based syntactic neural de-coder (yin and neubig, 2017) to generate the ab-stract syntax tree (ast) of the target query y indepth-ﬁrst-search order.
the output at each decod-ing timestep is either 1) an applyrule actionthat expands the current non-terminal node in thepartially generated ast, or 2) selecttable orselectcolumn action that chooses one schemaitem xsi from the encoded memory xs = xt ∪ xc.
mathematically, p (y|x) = (cid:81)j p (aj|a<j, x),where aj is the action at the j-th timestep.
formore implementation details, see appendix b..3.3.2 graph pruning.
we hypothesize that a powerful encoder shoulddistinguish irrelevant schema items from goldenschema items used in the target query.
in figure 6,the question-oriented schema sub-graph (above theshadow region) can be easily extracted.
the intentc2 and the constraint c5 are usually explicitly men-tioned in the question, identiﬁed by dot-productattention mechanism or schema linking.
the link-ing nodes such as t1, c3, c4, t2 can be inferred bythe 1-hop connections of the schema graph to forma connected component.
to introduce this induc-tive bias, we design an auxiliary task that aims toclassify each schema node si ∈ s = t ∪ c basedon its relevance with the question and the sparsestructure of the schema graph..figure 6: a delexicalized example of graph pruning.
circles with dashed borderlines are irrelevant schemaitems, thus labeled with 0..firstly, we compute the context vector ˜xsi fromthe question node embeddings xq for each schema.
2545node si via multi-head attention..γhji =softmaxj.
(xsiwh.
sq)(xqj wh(cid:112)d/h.
sk)t.,.
˜xsi =(.
(cid:88).
jixqj whγh.
sv)wso,.
h(cid:110)h=1sq, wh.
jsk, wh.
sv ∈ rd×d/h and wso ∈where whrd×d are network parameters.
then, a bi-afﬁne (dozat and manning, 2017) binary classiﬁeris used to determine whether the compressed con-text vector ˜xsi and the schema node embeddingxsi are correlated.
biafﬁne(x1, x2) =x1usxtp gp(ysi|xsi, xq) =σ(biafﬁne(xsi, ˜xsi)).
the ground truth label ygsi of a schema item is 1 iffsi appears in the target sql query.
the trainingobject can be formulated as.
2 + [x1; x2]ws + bs,.
lgp = −.
(cid:88).
[yg.
si log p gp(ysi|xsi, xq).
si+ (1 − yg.
si) log(1 − p gp(ysi|xsi, xq))]..this auxiliary task is combined with the maintext-to-sql task in a multitasking way.
similarideas (bogin et al., 2019b; yu et al., 2020) and otherassociation schemes are discussed in appendix c..4 experiments.
in this section, we evaluate our lgesql model indifferent settings.
codes are public available 5..4.1 experiment setup.
dataset spider (yu et al., 2018b) is a large-scale cross-domain zero-shot text-to-sql bench-mark 6. it contains 8659 training examples across146 databases in total, and covers several domainsfrom other datasets such as restaurants (popescuet al., 2003), geoquery (zelle and mooney, 1996),scholar (iyer et al., 2017), academic (li and ja-gadish, 2014), yelp and imdb (yaghmazadehet al., 2017) datasets.
the detailed statistics areshown in table 1. we follow the common practiceto report the exact set match accuracy on the valida-tion and test dataset.
the test dataset contains 2147samples with 40 unseen databases but is not publicavailable.
we submit our model to the organizer ofthe challenge for evaluation..5https://github.com/rhythmcao/.
text2sql-lgesql.git..6leaderboard of the challenge: https://yale-lily..github.io//spider..# of samples# of databasesavg # of question nodesavg # of table nodesavg # of column nodesavg # of nodesavg # of actions.
train865914613.46.633.153.116.3.dev10342013.84.525.844.115.4.table 1: statistics for dataset spider.
the action se-quence is created with our designed grammar..implementations we preprocess the questions,table names, and column names with toolkitstanza (qi et al., 2020) for tokenization and lemma-tization.
our model is implemented with py-torch (paszke et al., 2019), and the original and linegraphs are constructed with library dgl (wanget al., 2019a).
within the encoder, we useglove (pennington et al., 2014) word embeddingswith dimension 300 or pretrained language mod-els (plms), bert (devlin et al., 2019) or elec-tra (clark et al., 2020), to leverage contextualinformation.
with glove, embeddings of themost frequent 50 words in the training set are ﬁxedduring training while the remaining will be ﬁne-tuned.
the schema linking strategy is borrowedfrom ratsql (wang et al., 2020a), which is alsoour baseline system.
during evaluation, we adoptbeam search decoding with beam size 5..hyper-parametersin the encoder, the gnn hid-den size d is set to 256 for glove and 512 forplms.
the number of gnn layers l is 8. in thedecoder, the dimension of hidden state, action em-bedding and node type embedding are set to 512,128 and 128 respectively.
the recurrent dropoutrate (gal and ghahramani, 2016) is 0.2 for decoderlstm.
the number of heads in multi-head atten-tion is 8 and the dropout rate of features is set to0.2 in both the encoder and decoder.
throughoutthe experiments, we use adamw (loshchilov andhutter, 2019) optimizer with linear warmup sched-uler.
the warmup ratio of total training steps is0.1. for glove, the learning rate is 5e-4 and theweight decay coefﬁcient is 1e-4; for plms, we usesmaller leaning rate 2e-5 (base) or 1e-5 (large),and larger weight decay rate 0.1. the optimizationof the plm encoder is carried out more carefullywith layer-wise learning rate decay coefﬁcient 0.8.batch size is 20 and the maximum gradient normis 5. the number of training epochs is 100 forglove, and 200 for plms respectively..25464.2 main results.
split easy medium hard extra.
all.
model.
dev.
test.
without plm.
gnn (bogin et al., 2019a)global-gnn (bogin et al., 2019b)editsql (zhang et al., 2019b)irnet (guo et al., 2019)ratsql (wang et al., 2020a)lgesql.
with plm: bert.
irnet (guo et al., 2019)gazp (zhong et al., 2020)editsql (zhang et al., 2019b)bridge (lin et al., 2020)bridge + ensembleratsql (wang et al., 2020a)lgesqlwith task adaptive plm.
shadowgnn (chen et al., 2021)ratsql+strug (deng et al., 2021)ratsql+grappa (yu et al., 2020)smbop (rubin and berant, 2021)ratsql+gap (shi et al., 2020)dt-fixup sql-sp (xu et al., 2021)lgesql+electra.
40.752.736.453.262.767.6.
53.259.157.670.071.169.774.1.
72.372.673.474.771.875.075.1.
39.447.432.946.757.262.8.
46.753.353.465.067.565.668.3.
66.168.469.669.569.770.972.0.table 2: comparison to previous methods..the main results of the test set are provided intable 2. our proposed line graph enhanced text-to-sql (lgesql) model achieves state-of-the-artresults in all conﬁgurations at the time of writ-ing.
with word vectors glove, the performanceincreases from 57.2% to 62.8%, 5.6% absoluteimprovements.
with plm bert-large-wwm,lgesql also surpasses all previous methods, in-cluding the ensemble model, and attains 68.3%accuracy.
recently, more advanced approaches allleverage the beneﬁts of larger plms, more taskadaptive data (text-table pairs), and tailored pre-training tasks.
for example, gap (shi et al., 2020)designs some task adaptive self-supervised taskssuch as column prediction and column recovery tobetter address the downstream joint encoding prob-lem.
we utilize electra-large for its compati-bility with our model and achieves 72.0% accuracy.
taking one step further, we compare more ﬁne-grained performances of our model to the baselinesystem ratsql (wang et al., 2020a) classiﬁedby the level of difﬁculty in table 3. we observethat lgesql surpasses ratsql across all sub-divisions in both the validation and test datasetsregardless of the application of a plm, especiallyat the medium and extra hard levels.
this vali-dates the superiority of our model by exploiting thestructural relations among edges in the line graph..ratsql.
lgesql.
63.960.7.
69.568.1.
73.671.3.
76.774.7.
55.753.6.
61.554.0.
62.158.3.
66.760.9.
40.631.5.
41.037.5.
42.938.4.
48.841.5.
62.757.2.
67.662.8.
69.765.6.
74.168.3.ratsql+plm: bert-large-wwm.
lgesql+plm: bert-large-wwm.
devtest.
devtest.
devtest.
devtest.
80.474.8.
86.380.9.
86.483.0.
91.584.5.table 3: a detailed comparison to the reported resultsin the original paper ratsql (wang et al., 2020a) ac-cording to the level of difﬁculty..4.3 ablation studies.
in this section, we investigate the contribution ofeach design choice.
we report the average accuracyon the validation dataset with 5 random seeds..4.3.1 different components of lgesql.
techniquewithout line graph: rgatsql.
dev acc.
w/ sew/ mmcw/o nlcw/o gpwith line graph: lgesql.
66.266.263.365.5.w/ msdew/ mmcw/o nlcw/o gp.
67.367.465.366.2.table 4: ablation study of different modules.
se: staticembeddings; mmc: multi-head multi-view concatena-tion; msde: mixed static and dynamic embeddings;nlc: non-local relations; gp: graph pruning..rgatsql is our baseline system where the linegraph is not utilized.
it can be viewed as a variantof ratsql with our tailored grammar-based de-coder.
from table 4, we can discover that: 1) ifnon-local relations or meta-paths are removed (w/onlc), the performance will decrease roughly by 2points in lgesql, while 3 points drop in rgat-sql.
however, our lgesql with merely localrelations is still competitive.
it consolidates ourmotivation that by exploiting the structure amongedges, the line graph can capturing long-range re-lations to some extent.
2) graph pruning task con-.
2547tributes more in lgesql (+1.2%) than rgat-sql (+0.7%) on account of the fact that local re-lations are more critical to structural inference.
3)two strategies of combining local and non-local re-lations introduced in § 3.2.1 (w/ msde or mmc)are both beneﬁcial to the eventual performancesof lgesql (2.0% and 2.1% gains, respectively).
it corroborates the assumption that local and non-local relations should be treated with distinction.
however, the performance remains unchanged inrgatsql, when merging a different view of thegraph (w/ mmc) into multi-head attention.
thismay be caused by the over-smoothing problem ofa complete graph..4.3.2 pre-trained language models.
plm.
rgatsql lgesql.
bert-baseelectra-base.
bert-largegrappa-largeelectra-large.
70.572.8.
72.373.174.8.
71.473.4.
73.574.075.1.table 5: ablation study of different plms..in this part, we analyze the effects of differentpre-trained language models in table 5. from theoverall results, we can see that: 1) by involvingthe line graph into computation, lgesql outper-forms the baseline model rgatsql with differentplms, further demonstrating the effectiveness ofexplicitly modeling edge features.
2) large se-ries plms consistently perform better than basemodels on account of their model capacity andgeneralization capability to unseen domains.
3)task adaptive plms especially electra are su-perior to vanilla bert irrespective of the uppergnn architecture.
we hypothesize the reason isthat electra is pre-trained with a tailored binaryclassiﬁcation task, which aims to individually dis-tinguish whether each input word is substitutedgiven the context.
essentially, this self-supervisedtask is similar to our proposed graph pruning task,which focuses on enhancing the discriminative ca-pability of the encoder..4.4 case studies.
in figure 7, we compare the sql queries gener-ated by our lgesql model with those createdby the baseline model rgatsql.
we notice that.
figure 7: case study: the ﬁrst three cases are positivesamples while the last one is negative.
the input ques-tion is represented by its level of difﬁculty.
from con-ditions are omitted here for brevity and cell values inthe sql queries are replaced with placeholders “val”..lgesql performs better than the baseline system,especially on examples that involve the join oper-ation of multiple tables.
for instance, in the secondcase where the connection of three tables are in-cluded, rgatsql fails to identify the existenceof table flights.
thus, it is unable to predictthe where condition about the destination city anddoes repeat work.
in the third case, our lgesqlstill successfully constructs a connected schemasub-graph by linking table “template” to “docu-ments”.
sadly, the rgatsql model neglects theoccurrence of “documents” again.
however, in thelast case, our lgesql is stupid to introduce an un-necessary table “airports”.
it ignores the situationthat table “ﬂights” has one column “source airport”which already satisﬁes the requirement..5 related work.
encoding problem for text-to-sql to tacklethe joint encoding problem of the question anddatabase schema, xu et al.
(2017) proposes “col-umn attention” strategy to gather information fromcolumns for each question word.
typesql (yuet al., 2018a) incorporates prior knowledge of col-umn types and schema linking as additional input.
2548features.
bogin et al.
(2019a) and chen et al.
(2021)deal with the graph structure of database schemavia gnn.
editsql (zhang et al., 2019b) considers“co-attention” between question words and databaseschema nodes similar to the common practice intext matching (chen et al., 2017).
bridge (linet al., 2020) further leverages the database contentto augment the column representation.
the mostadvanced method ratsql (wang et al., 2020a),utilizes a complete relational graph attention neu-ral network to handle various pre-deﬁned relations.
in this work, we further consider both local andnon-local, dynamic and static edge features amongdifferent types of nodes with a line graph..heterogeneous graph neural network apartfrom the structural topology, a heterogeneousgraph (shi et al., 2016) also contains multiple typesof nodes and edges.
to address the heterogene-ity of node attributes, zhang et al.
(2019a) de-signs a type-based content encoder and fu et al.
(2020) utilizes a type-speciﬁc linear transforma-tion.
for edges, relational graph convolution net-work (rgcn, schlichtkrull et al., 2018) and rela-tional graph attention network (rgat, wang et al.,2020b) have been proposed to parameterize differ-ent relations.
han (wang et al., 2019b) convertsthe original heterogeneous graph into multiple ho-mogeneous graphs and applies a hierarchical atten-tion mechanism to the meta-path-based sub-graphs.
similar ideas have been adopted in dialogue statetracking (chen et al., 2020b, 2019a), dialogue pol-icy learning (chen et al., 2018) and text match-ing (chen et al., 2020c; lyu et al., 2021) to handleheterogeneous inputs.
in another branch, chen et al.
(2019b), zhu et al.
(2019) and zhao et al.
(2020)construct the line graph of the original graph andexplicitly model the computation over edge fea-tures.
in this work, we borrow the idea of a linegraph and update both node and edge features viaiteration over dual graphs..6 conclusion.
in this work, we utilize the line graph to updatethe edge features in the heterogeneous graph forthe text-to-sql task.
through the iteration overthe structural connections in the line graph, localedges can incorporate multi-hop relational featuresand capture signiﬁcant meta-paths.
by further inte-grating non-local relations, the encoder can learnfrom multiple views and attend to remote nodeswith shortcuts.
in the future, we will investigate.
more useful meta-paths and explore more effec-tive methods to deal with different meta-path-basedneighbors..acknowledgments.
we thank tao yu, yusen zhang and bo pangfor their careful assistance with the evaluation.
we also thank the anonymous reviewers fortheir thoughtful comments.
this work has beensupported by shanghai municipal science andtechnology major project (2021shzdzx0102),no.sklmcpts2020003 project and startup fundfor youngman research at sjtu (sfyr at sjtu)..references.
ion androutsopoulos, graeme d ritchie, and pe-ter thanisch.
1995. natural language interfacesto databases-an introduction.
arxiv preprint cmp-lg/9503016..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..ben bogin, jonathan berant, and matt gardner.
2019a.
representing schema structure with graph neuralnetworks for text-to-sql parsing.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 4560–4565, florence,italy.
association for computational linguistics..ben bogin, matt gardner, and jonathan berant.
2019b.
global reasoning over database structures for text-in proceedings of the 2019 con-to-sql parsing.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 3659–3664, hong kong, china.
as-sociation for computational linguistics..deli chen, yankai lin, wei li, peng li, jie zhou, andxu sun.
2020a.
measuring and relieving the over-smoothing problem for graph neural networks fromin proceedings of the aaaithe topological view.
conference on artiﬁcial intelligence, volume 34,pages 3438–3445..lu chen, zhi chen, bowen tan, sishan long, milicagasic, and kai yu.
2019a.
agentgraph: towardsuniversal dialogue management with structured deepreinforcement learning.
corr, abs/1905.11259..lu chen, boer lv, chi wang, su zhu, bowen tan, andkai yu.
2020b.
schema-guided multi-domain dia-logue state tracking with graph attention neural net-works.
in proceedings of the aaai conference onartiﬁcial intelligence, volume 34, pages 7521–7528..2549lu chen, bowen tan, sishan long, and kai yu.
2018. structured dialogue policy with graph neu-in proceedings of the 27th inter-ral networks.
national conference on computational linguistics(coling), pages 1257—-1268..lu chen, yanbin zhao, boer lyu, lesheng jin, zhichen, su zhu, and kai yu.
2020c.
neural graphmatching networks for chinese short text matching.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages6152–6158, online.
association for computationallinguistics..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstmin proceedings offor natural language inference.
the 55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1657–1668, vancouver, canada.
associationfor computational linguistics..zhengdao chen, lisha li, and joan bruna.
2019b.
su-pervised community detection with line graph neuralnetworks.
in 7th international conference on learn-ing representations, iclr 2019, new orleans, la,usa, may 6-9, 2019. openreview.net..zhi chen, lu chen, yanbin zhao, ruisheng cao, zi-han xu, su zhu, and kai yu.
2021. shadowgnn:graph projection neural network for text-to-sqlin proceedings of the 2021 conference ofparser.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 5567–5577, online.
association forcomputational linguistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin 8th international conference ongenerators.
learning representations, iclr 2020, addis ababa,ethiopia, april 26-30, 2020. openreview.net..xiang deng, ahmed hassan awadallah, christophermeek, oleksandr polozov, huan sun, and matthewrichardson.
2021. structure-grounded pretrainingin proceedings of the 2021 con-for text-to-sql.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 1337–1350, online.
as-sociation for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-in 5th international conference on learninging..representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..xinyu fu, jiani zhang, ziqiao meng, and irwin king.
2020. magnn: metapath aggregated graph neuralinnetwork for heterogeneous graph embedding.
proceedings of the web conference 2020, pages2331–2341..yarin gal and zoubin ghahramani.
2016. a theoret-ically grounded application of dropout in recurrentin advances in neural informa-neural networks.
tion processing systems 29: annual conference onneural information processing systems 2016, de-cember 5-10, 2016, barcelona, spain, pages 1019–1027..jonathan l gross and jay yellen.
2005. graph theory.
and its applications.
crc press..jiaqi guo, zecheng zhan, yan gao, yan xiao,jian-guang lou, ting liu, and dongmei zhang.
2019.towards complex text-to-sql in cross-domain database with intermediate representation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages4524–4535, florence, italy.
association for compu-tational linguistics..sepp hochreiter and j¨urgen schmidhuber.
1997.neural computation,.
long short-term memory.
9(8):1735–1780..srinivasan iyer, ioannis konstas, alvin cheung, jayantkrishnamurthy, and luke zettlemoyer.
2017. learn-ing a neural semantic parser from user feedback.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 963–973, vancouver, canada.
association for computational linguistics..xiangnan kong, philip s yu, ying ding, and david jwild.
2012. meta path-based collective classiﬁca-tion in heterogeneous information networks.
in pro-ceedings of the 21st acm international conferenceon information and knowledge management, pages1567–1571..ni lao and william w cohen.
2010. relational re-trieval using a combination of path-constrained ran-dom walks.
machine learning, 81(1):53–67..fei li and hv jagadish.
2014. constructing an in-teractive natural language interface for relationaldatabases.
proceedings of the vldb endowment,8:73–84..xi victoria lin, richard socher, and caiming xiong.
2020. bridging textual and tabular data for cross-domain text-to-sql semantic parsing.
in findingsof the association for computational linguistics:emnlp 2020, pages 4870–4888, online.
associa-tion for computational linguistics..2550ilya loshchilov and frank hutter.
2019. decou-in 7th inter-pled weight decay regularization.
national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..boer lyu, lu chen, su zhu, and kai yu.
2021. let:linguistic knowledge enhanced graph transformerfor chinese short text matching..adam paszke, sam gross, francisco massa, adamlerer, james bradbury, gregory chanan, trevorkilleen, zeming lin, natalia gimelshein, lucaantiga, alban desmaison, andreas k¨opf, edwardyang, zachary devito, martin raison, alykhan te-jani, sasank chilamkurthy, benoit steiner, lu fang,junjie bai, and soumith chintala.
2019.py-torch: an imperative style, high-performance deepin advances in neural informa-learning library.
tion processing systems 32: annual conferenceon neural information processing systems 2019,neurips 2019, december 8-14, 2019, vancouver,bc, canada, pages 8024–8035..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..ana-maria popescu, oren etzioni, and henry kautz.
2003. towards a theory of natural language inter-in proceedings of the 8th in-faces to databases.
ternational conference on intelligent user interfaces,pages 149–157..peng qi, yuhao zhang, yuhui zhang, jason bolton,stanza: aand christopher d. manning.
2020.python natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, pages 101–108, online.
association for computational linguis-tics..ohad rubin and jonathan berant.
2021..smbop:semi-autoregressive bottom-up semantic parsing.
inproceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 311–324, online.
association for computa-tional linguistics..franco scarselli, marco gori, ah chung tsoi, markushagenbuchner, and gabriele monfardini.
2008. thegraph neural network model.
ieee transactions onneural networks, 20(1):61–80..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607.
springer..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..yikang shen, shawn tan, alessandro sordoni, andaaron c. courville.
2019. ordered neurons: inte-grating tree structures into recurrent neural networks.
in 7th international conference on learning repre-sentations, iclr 2019, new orleans, la, usa, may6-9, 2019. openreview.net..chuan shi, yitong li, jiawei zhang, yizhou sun, ands yu philip.
2016. a survey of heterogeneous in-formation network analysis.
ieee transactions onknowledge and data engineering, 29(1):17–37..peng shi, patrick ng, zhiguo wang, henghui zhu,alexander hanbo li, jun wang, c´ıcero nogueirados santos, and bing xiang.
2020.learn-ing contextual representations for semantic pars-ing with generation-augmented pre-training.
corr,abs/2012.10309..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..bailin wang, richard shin, xiaodong liu, oleksandrpolozov, and matthew richardson.
2020a.
rat-sql: relation-aware schema encoding and linkingfor text-to-sql parsers.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 7567–7578, online.
asso-ciation for computational linguistics..daniel c wang, andrew w appel, jeffrey l korn, andchristopher s serra.
1997. the zephyr abstract syn-tax description language.
in dsl, volume 97, pages17–17..kai wang, weizhou shen, yunyi yang, xiaojun quan,and rui wang.
2020b.
relational graph attentionnetwork for aspect-based sentiment analysis.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3229–3238, online.
association for computational lin-guistics..minjie wang, da zheng, zihao ye, quan gan, mufeili, xiang song, jinjing zhou, chao ma, ling-fan yu, yu gai, tianjun xiao, tong he, georgekarypis, jinyang li, and zheng zhang.
2019a.
deep.
2551graph library: a graph-centric, highly-performantpackage for graph neural networks.
arxiv preprintarxiv:1909.01315..25th acm sigkdd international conference onknowledge discovery & data mining, pages 793–803..rui zhang, tao yu, heyang er, sungrok shim,eric xue, xi victoria lin, tianze shi, caim-ing xiong, richard socher, and dragomir radev.
2019b.
editing-based sql query generation forcross-domain context-dependent questions.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 5338–5349,hong kong, china.
association for computationallinguistics..yanbin zhao, lu chen, zhi chen, ruisheng cao,su zhu, and kai yu.
2020. line graph enhancedamr-to-text generation with mix-order graph at-in proceedings of the 58th an-tention networks.
nual meeting of the association for computationallinguistics, pages 732–741, online.
association forcomputational linguistics..victor zhong, mike lewis, sida i. wang, and lukezettlemoyer.
2020. grounded adaptation for zero-shot executable semantic parsing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6869–6882, online.
association for computational lin-guistics..victor zhong, caiming xiong, and richard socher.
2017.seq2sql: generating structured queriesfrom natural language using reinforcement learning.
arxiv preprint arxiv:1709.00103..shichao zhu, chuan zhou, shirui pan, xingquan zhu,and bin wang.
2019. relation structure-aware het-in 2019 ieeeerogeneous graph neural network.
international conference on data mining (icdm),pages 1534–1539..xiao wang, houye ji, chuan shi, bai wang, yanfangye, peng cui, and philip s yu.
2019b.
heteroge-neous graph attention network.
in the world wideweb conference, pages 2022–2032..peng xu, dhruv kumar, wei yang, wenjie zi, keyitang, chenyang huang, jackie chi kit cheung, si-mon j. d. prince, and yanshuai cao.
2021. optimiz-ing deeper transformers on small datasets..xiaojun xu, chang liu, and dawn song.
2017. sqlnet:generating structured queries from natural languagearxiv preprintwithout reinforcementarxiv:1711.04436..learning..navid yaghmazadeh, yuepeng wang, isil dillig, andthomas dillig.
2017. sqlizer: query synthesis fromnatural language.
proceedings of the acm on pro-gramming languages, 1:1–26..pengcheng yin and graham neubig.
2017. a syntacticneural model for general-purpose code generation.
in proceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 440–450, vancouver, canada.
association for computational linguistics..tao yu, zifan li, zilin zhang, rui zhang, anddragomir radev.
2018a.
typesql: knowledge-based type-aware neural text-to-sql generation.
inproceedings of the 2018 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 2 (short papers), pages 588–594, neworleans, louisiana.
association for computationallinguistics..tao yu, chien-sheng wu, xi victoria lin, bailinwang, yi chern tan, xinyi yang, dragomir r.radev, richard socher, and caiming xiong.
2020.grappa: grammar-augmented pre-training for tablesemantic parsing.
corr, abs/2009.13845..tao yu, rui zhang, kai yang, michihiro yasunaga,irene li,dongxu wang, zifan li, james ma,qingning yao, shanelle roman, zilin zhang,and dragomir radev.
2018b.
spider: a large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages3911–3921, brussels, belgium.
association forcomputational linguistics..john m zelle and raymond j mooney.
1996. learn-ing to parse database queries using inductive logicprogramming.
in proceedings of the national con-ference on artiﬁcial intelligence, pages 1050–1055..chuxu zhang, dongjin song, chao huang, ananthramswami, and nitesh v chawla.
2019a.
heteroge-neous graph neural network.
in proceedings of the.
2552a local and non-local relations.
in this work, meta-paths with length 1 are localrelations, and other meta-paths are non-local re-lations.
speciﬁcally, table 6 provides the list ofall local relations according to the types of sourceand target nodes.
notice that we preserve the no-match relation because there is no overlappingbetween the entire question and any schema itemin some cases.
this relaxation will dramaticallyincrease the number of edges in the line graph.
toresolve it, we remove edges in the line graph thatthe source and target nodes both represent relationtypes of match series.
in other words, we preventinformation propagating between these bipartiteconnections during the iteration of the line graph.
the checklist in table 6 is only a subset of allrelations deﬁned in ratsql (wang et al., 2020a).
for the remaining relations, we treat them as non-local relations for a fair comparison to the baselinesystem ratsql..b details of text-to-sql decoder.
b.1 asdl grammar.
the complete grammar used to translate the sqlinto a series of actions is provided in figure 8.here are some criteria when we design the abstractsyntax description language (asdl, wang et al.,1997) for the target sql queries:.
1. keep the length of the action sequence shortto prevent the long-term forgetting problemin the auto-regressive decoder.
to achievethis goal, we remove the optional operator“?” deﬁned in wang et al.
(1997) and extendthe number of constructors by enumeration.
for example, we expand all solutions of typesql unit according to the existence of dif-ferent clauses..2. hierarchically, group and re-use the same typein a top-down manner for parameter shar-ing.
for example, we use the same typecol unit when choosing columns in dif-ferent clauses and create the type val unitsuch that both the select clause and con-dition clauses can refer to it..3. when generating a list of items of the sametype, instead of emitting a special action re-duce as the symbol of termination (yin andneubig, 2017), we enumerate all possiblenumber of occurrences in the training set (see.
the constructors for type select and fromin figure 8).
then, we generate each itembased on this quantitative limitation.
prelimi-nary experimental results prove that thinkingin advance is better than a lazy decision..our grammar can cover 98.7% and 98.2% cases inthe training and validation dataset, respectively..b.2 decoder architecture.
given the encoded memory x = [xq; xt; xc] ∈r|v n|×d, where |v n| = |q|+|t |+|c|, the goal ofa text-to-sql decoder is to produce a sequence ofactions which can construct the corresponding astof the target sql query.
in our experiments, weutilize a single layer ordered neurons lstm (on-lstm, shen et al., 2019) as the auto-regressivedecoder.
firstly, we initialize the decoder state h0via attentive pooling over the memory x..ai =softmaxi tanh(xiw0)vt0 ,˜h0 =.
aixi,.
(cid:88).
i.h0 =tanh(˜h0w1),.
where v0 is a trainable row vector and w0, w1 areparameter matrices.
then, in the structured on-lstm decoder, the hidden states at each timestepj is updated as.
mj, hj = on-lstm([aj−1; apj ;hpj ; nj],.
mj−1, hj−1),.
where mj is the cell state of the j-th timestep, aj−1is the embedding of the previous action, apj is theembedding of parent action, hpt is the embeddingof parent hidden state, and nj denotes the type em-bedding of the current frontier node 7. given thecurrent decoder state hj, we adopt multi-head atten-tion (8 heads) mechanism to calculate the contextvector ˜hj over x. this context vector is concate-nated with hj and passed into a 2-layer mlp withtanh activation unit to obtain the attention vectorhattj.jfor applyrule action, the probability distribu-tion is computed by a softmax classiﬁcation layer:.
.
the dimension of hatt.
is 512..p (aj = applyrule[r]|a<j, x) =.
softmaxr(hatt.
j wr)..7the frontier node is the current non-terminal node in thepartially generated ast to be expanded and we maintain anembedding for each node type..2553source x target y.qc.t.q.q.qc.c.t.c.relationdistance+1foreignkeyhasprimarykeynomatch.
descriptiony is the next word of x.y is the foreign key of x.the column y belongs to the table x.the column y is the primary key of the table x.no overlapping between x and y..partialmatch x is part of y, but the entire question does not contain y.exactmatchnomatch.
x is part of y, and y is a span of the entire question.
no overlapping between x and y..partialmatch x is part of y, but the entire question does not contain y.exactmatchvaluematch.
x is part of y, and y is a span of the entire question.
x is part of the candidate cell values of column y..table 6: the checklist of all local relations used in our experiments.
all relations above are asymmetric.
for brevity,we only show one direction, and the opposite can be easily inferred.
q/t/c stands for question/table/columnnode respectively..for selecttable action, we directly copy the.
table ti from the encoded memory xt..ji =softmaxi(hattζ h.j wh.
tq)(xtiwh.
tk)t,.
p (aj =selecttable[ti]|a<j, x) =.
1h.h(cid:88).
h=1.
ζ hji..to be consistent, we also apply the multi-head at-tention mechanism here with h = 8 heads.
thecalculation of selectcolumn action is similarwith different network parameters..c graph pruning.
similar ideas have been proposed by bogin et al.
(2019b) and yu et al.
(2020).
our proposed taskdiffers from their methods in two aspects:.
prediction target yu et al.
(2020) devises sev-eral syntactic roles for schema items and performsmulti-class classiﬁcation instead of binary discrim-ination.
based on our assumption, the encoder isresponsible for the discrimination capability whilethe decoder organizes different schema items andcomponents into a complete semantic frame.
thus,we simplify the training target into binary labels..combination method bogin et al.
(2019b) uti-lizes another rgcn to calculate the relevance scorefor each schema item in global-gnnsql.
thisscore is incorporated into the encoder rgcn as asoft input coefﬁcient.
different from this cascadedmethod, graph pruning is employed in a multitask-ing manner.
we have tried different approachesto combine this auxiliary module with the primary.
text-to-sql model in our preliminary experiments,such as:.
1) similar to bogin et al.
(2019b), we utilize aseparate graph encoder to conduct graph pruningﬁrstly, and use another reﬁned graph encoder (thesame architecture, e.g., rgat) to jointly encodethe pruned schema graph and the question.
thesetwo encoders can share network parameters of onlythe embeddings or more upper gnn layers.
if theyshare all 8 layers, the entire encoder will degener-ate from the pipelined mode into our multitaskingfashion.
empirical results in table 7 demonstratethat when these two encoders share more layers,the performance of the text-to-sql model is better..mode.
# layers shared.
dev acc.
pipeline.
⇓.
multitasking.
0.
4.
8.
60.74.
61.63.
62.53.table 7: variation of performances when gradually in-creasing the number of layers shared between the prun-ing and the main encoders..2) we can constrain the text-to-sql decoderto only attend and retrieve schema items fromthe pruned encoded memory when calculating at-tention vectors and select columns or tables.
inother words, the graph pruning module and thetext-to-sql decoder are connected in a cascadedway.
through pilot experiments, we observe theﬂagrant training-inference inconsistency problem.
the text-to-sql decoder is trained upon the golden.
2554figure 8: the asdl grammar for sql in our implementation..schema items, but it depends on the predicted op-tions from the graph pruning module during eval-uation.
even if we endeavor various sampling-based methods (such as random sampling, sam-pling from current module predictions, or sam-pling from neighboring nodes of the golden schemagraph) to inject some noise during training, the per-formance is merely competitive to that with multi-tasking.
therefore, based on occam’s razor the-orem, we only treat graph pruning as an auxiliaryoutput module..2555