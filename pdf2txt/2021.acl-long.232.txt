unsupervised extractive summarization-based representationsfor accurate and explainable collaborative filtering.
reinald adrian pugoy1,2 and hung-yu kao11intelligent knowledge management labnational cheng kung university, tainan city, taiwan2faculty of information and communication studiesuniversity of the philippines open university, los ba˜nos, philippinesrdpugoy@up.edu.ph, hykao@mail.ncku.edu.tw.
abstract.
reviews received by the ‘journaling bible’ item.
we pioneer the ﬁrst extractive summarization-based collaborative ﬁltering model called es-cofilt.
our proposed model speciﬁcally pro-duces extractive summaries for each item anduser.
unlike other types of explanations,summary-level explanations closely resemblereal-life explanations.
the strength of es-cofilt lies in the fact that it uniﬁes repre-sentation and explanation.
in other words, ex-tractive summaries both represent and explainthe items and users.
our model uniquely inte-grates bert, k-means embedding clustering,and multilayer perceptron to learn sentenceembeddings, representation-explanations, anduser-item interactions, respectively.
we arguethat our approach enhances both rating pre-diction accuracy and user/item explainability.
our experiments illustrate that escofilt’sprediction accuracy is better than the otherstate-of-the-art recommender models.
further-more, we propose a comprehensive set of cri-teria that assesses the real-life explainability ofexplanations.
our explainability study demon-strates the superiority of and preference forsummary-level explanations over other expla-nation types..1.introduction.
collaborative ﬁltering (cf) approaches are themost dominant and outstanding models in recom-mender systems literature.
cf mainly focuseson learning accurate representations of users anditems, denoting user preferences and item charac-teristics, respectively (chen et al., 2018; tay et al.,2018).
the earliest cf models learned such rep-resentations based on user-given numeric ratings,but employing them is an oversimpliﬁcation of userpreferences and item characteristics (koren et al.,2009; musto et al., 2017).
in this regard, reviewtexts have been utilized to alleviate this issue..1..2..i was not expecting this bible to be so beautiful when i pre-orderedit 5 months ago, but it arrived in the mail today and it is just gor-geous!
i love the concept of bible journaling, but was always abit intimidated by where/how to start.
this removes that concernthrough some beautifully done artwork and lettering.
i am ecstaticat the quality of this bible!.
i brought this as i wanted a separate bible to do bible journaling.
it is very beautiful and has many images that can be coloured.
thepages are similar to bible paper and cream in colour.
overall awonderful bible to do journaling and meditate god’s word..generated explanations.
• review-level: i brought this as i wanted a separate bible to dobible journaling.
it is very beautiful and has many images that canbe coloured.
the pages are similar to bible paper and cream incolour.
overall a wonderful bible to do journaling and meditategod’s word..• word-level: i brought this as i wanted a separate bible to dobible journaling.
it is very beautiful and has many images that canbe coloured.
the pages are similar to bible paper and cream incolour.
overall a wonderful bible to do journaling and meditategod’s word..• summary-level: i was not expecting this bible to be so beautifulwhen i pre-ordered it 5 months ago, but it arrived in the mail todayand it is just gorgeous!
this removes that concern through somebeautifully done artwork and lettering.
the pages are similar tobible paper and cream in colour.
overall a wonderful bible to dojournaling and meditate god’s word..table 1:illustration of the different types of explana-tions.
a review-level explanation is simply the high-est weighted review.
a word-level explanation is com-prised of highlighted words or tokens with the highestattention scores.
our proposed summary-level explana-tion closely resembles real-life explanations, whereinthe explanation text is derived from multiple reviews..the primary beneﬁt of using reviews as thesource of features is that they can cover the inher-ently multi-faceted nature of user opinions.
userscan explain their rationales for the ratings they giveto items.
thus, reviews contain a large quantityof rich latent information that cannot be otherwiseacquired solely from ratings (chen et al., 2018)..still, a typical limitation exists for most review-based recommender systems recently; the intrin-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2981–2990august1–6,2021.©2021associationforcomputationallinguistics2981sic black-box nature of neural networks (nn)makes the explainability behind predictions ob-scure (ribeiro et al., 2016; wang et al., 2018b).
theintricate architecture of hidden layers has opaquedthe decision-making processes of neural models(peake and wang, 2018).
providing explanationsis essential as they could help persuade users todevelop further trust in a recommender systemand make eventual purchasing decisions (peakeand wang, 2018; ribeiro et al., 2016; zhang et al.,2014)..in light of this, current research efforts have at-tempted to improve the explainability aspect ofrecommender systems.
common types of expla-nations include review-level and word-level.
in areview-level explanation, the attention mechanismis applied to measure every review’s contributionto the item (or user) embedding (chen et al., 2018;feng and zeng, 2019).
high-scoring reviews arethen selected to serve as explanations.
on the otherhand, in a word-level or token-level explanation, in-formative words in a local window or textual blockare selected together (liu et al., 2019a; pugoy andkao, 2020; seo et al., 2017).
similar to the ﬁrstmechanism, top words are chosen due to their highattention weights..evidently, review-level and word-level expla-nations are side-effects of applying the attentionmechanism to reviews and words.
these have beenintegral and beneﬁcial in formulating better userand item representations.
however, we contendthat both types of explanations may not completelyresemble real-life explanations.
in logic, an expla-nation is a set of intelligible statements usually con-structed to describe and clarify the causes, context,and consequences of objects, events, or phenomenaunder examination (drake, 2018).
based on our ex-ample in table 1, the review-level explanation is ex-actly the same as the second item review, assumingthat it has the higher attention weight.
due to this, italso inadvertently disregards other possibly usefulsentences from other reviews with lower attentionscores.
furthermore, even though the word-levelexplanation contains informative words, it may notbe practical in an actual recommendation scenariosince it typically appears as fragments.
word-levelexplanations may not be intelligible enough due tohumans’ natural bias toward sentences, which aredeﬁned to express complete thoughts (andersen,2014)..therefore, in this paper, we propose the ﬁrst.
summarization-based.
extractivecollaborativeﬁltering model, escofilt.
for every itemand user, our novel model generates extrac-tive summaries that bear more resemblanceto real-life explanations, as seen in table 1’slast row.
unlike a review-level explanation, asummary-level explanation (which we also callextractive summary, representative summary, andrepresentation-explanation in different sections ofthis paper) is composed of informative statementsgathered from different reviews.
as opposed to aword-level explanation, an escofilt-producedexplanation is more comprehensible as it canconvey complete thoughts.
it should be noted thatour model performs extractive summarization in anunsupervised manner since expecting ground-truthsummaries for all items and users in a large datasetis unrealistic.
the strength of escofilt lies inthe fact that it uniquely uniﬁes representation andexplanation.
in other words, an extractive summaryboth represents and explains a particular item(or user).
we argue that our approach enhancesboth rating prediction accuracy and user/itemexplainability, which are later validated by ourexperiments and explainability study..1.1 contributions.
these are the main contributions of our paper:.
• to the best of our knowledge, we pioneerthe ﬁrst extractive summarization-based cfframework..• our proposed model uniquely integratesbert, k-means embedding clustering, andmultilayer perceptron (mlp)to respec-tively learn sentence embeddings, extractiverepresentation-explanations, and user-item in-teractions..• to the extent of our knowledge, escofiltis one of the ﬁrst recommender models thatemploy bert as a review feature extractor.
• we also propose a comprehensive set of crite-ria that assesses the explainability of explana-tion texts in real life..• our experiments illustrate that the rating pre-diction accuracy of escofilt is better thanthe other state-of-the-art models.
moreover,our explainability study shows that summary-level explanations are superior and more pre-ferred than the other types of explanations..29822 related work.
developing a cf model involves two crucial steps,i.e., learning user and item representations andmodeling user-item interactions based on those rep-resentations (he et al., 2018).
one of the foun-dational works in utilizing nn for cf is neuralcollaborative ﬁltering or ncf (he et al., 2017).
originally implemented for implicit feedback data-driven cf, ncf learns non-linear interactions be-tween users and items by employing mlp layersas its interaction function..deepconn is the ﬁrst deep learning-basedmodel representing users and items from reviewsin a coordinated manner (zheng et al., 2017).
themodel consists of two parallel networks poweredby convolutional neural networks (cnn).
onenetwork learns user behavior by examining allreviews he has written, and the other networkmodels item properties by exploring all reviewsit has received.
a shared layer connects thesetwo networks, and factorization machines captureuser-item interactions.
another notable model isnarre, which shares several similarities withdeepconn.
narre is also composed of two par-allel cnn-based networks for user and item mod-eling (chen et al., 2018).
for the ﬁrst time, thismodel incorporates the review-level attention mech-anism that determines each review’s usefulness orcontribution based on attention weights.
as a side-effect, this also leads to review-level explanations;reviews with the highest attention scores are pre-sented as explanations.
these weights are then in-tegrated into the representations of users and itemsto enhance embedding quality and prediction accu-racy..other related studies include d-attn (seo et al.,2017), mpcn (tay et al., 2018) daml (liu et al.,2019a), and huita (wu et al., 2019).
these allemploy different types of attention mechanisms todistinguish informative parts of a given data sample,resulting in simultaneous accuracy and explainabil-ity improvements.
d-attn integrates global andlocal attention to score each word to determine itsrelevance in a review text.
mpcn is similar tonarre, but the former relies solely on attentionmechanisms without any need for convolutionallayers.
daml utilizes cnn’s local and mutual at-tention to learn review features, and huita incor-porates a hierarchical, three-tier attention network.
most of these aforementioned models take ad-vantage of cnns as automatic review feature ex-.
tractors.
coupling them with mainstream word em-beddings leads to the formulation of user and itemrepresentations.
however, such approaches fail toconsider global context and word frequency infor-mation.
the two said factors are crucial as they canaffect recommendation performance (pilehvar andcamacho-collados, 2019; wang et al., 2018a).
todeal with such dilemmas, ncem (feng and zeng,2019) and benefict (pugoy and kao, 2020) usea pre-trained bert model to obtain review features.
bert’s advantage lies in its full retention of globalcontext and word frequency information (feng andzeng, 2019).
for explainability, ncem similarlyadopts narre’s review-level attention.
on thecontrary, benefict utilizes bert’s self-attentionweights in conjunction with a solution to the maxi-mum subarray problem (msp).
benefict’s ap-proach produces an explanation based on a subarrayof contiguous tokens with the largest possible sumof self-attention weights..in summary, there appears to be a trend; tack-ling explainability improves prediction and recom-mendation performance consequentially.
whilemost recommender models address this via atten-tion mechanisms, our proposed model solves thisby unifying representation and explanation in theform of extractive summaries.
as evidenced in thesucceeding sections of this paper, we argue that ourapproach can further enhance cf’s accuracy andexplainability..3 methodology.
escofilt, whose architecture is illustrated infigure 1, has two parallel components that learnsummarization-based user and item representations.
from sections 3.2 to 3.3, we will only discuss theitem modeling process as it is nearly identical touser modeling, with their inputs as the only differ-ence..3.1 deﬁnition and notation.
the training dataset τ consists of n tuples, withthe latter denoting the size of the dataset.
eachtuple follows this form: (u, i, rui, vui) where ruiand vui respectively refer to the ground-truth ratingand review accorded by user u to item i. more-over, let vu = {vu1, vu2, ..., vuj} be the set ofall j reviews written by user u. similarly, letvi = {v1i, v2i, ..., vki} be the set of all k reviewsreceived by item i. both vu and vi are obtainedfrom scanning τ itself..2983in si are stored in ¯si ∈ rg×w×1024; w pertains tothe amount of words in a sentence, and 1024 is theembedding size of bert.
then, we average everysentence’s word embeddings in ¯si to produce theset of sentence embeddings s(cid:48)ig},with s(cid:48).
i2, ..., s(cid:48).
i = {s(cid:48).
i ∈ rg×1024..i1, s(cid:48).
3.3 embedding clustering.
k-means clustering is next performed to partitionthe sentence embeddings in s(cid:48)i into k clusters.
itsobjective is to minimize the intra-cluster sum of thedistances from each sentence to its nearest centroid,given by the following equation (xia et al., 2020):.
ji =.
k(cid:88).
(cid:88).
x=1.
s(cid:48)iy∈cx.
||s(cid:48).
iy − cx||2.
(3).
where cx is the centroid of cluster cx that is clos-est to the sentence embedding s(cid:48)iy.
the objectivefunction ji is optimized for item i by running theassignment and update steps until the cluster cen-troids stabilize.
the assignment step assigns eachsentence to a cluster based on the shortest sentenceembedding-cluster centroid distance, provided bythe formula below:.
d(s(cid:48).
iy) = argminx=1,...,k{||s(cid:48).
iy − cx||2}.
(4).
where d is a function that obtains the cluster closestto s(cid:48)iy.
furthermore, the update step recomputes thecluster centroids based on new assignments fromthe previous step.
this is deﬁned as:.
figure 1: the proposed escofilt architecture..the input of escofilt is a user-item pair (u, i)from each tuple in τ .
we particularly feed vu andvi to the model as they initially represent u and i.the output is the predicted rating ˆrui ∈ r that useru may give to item i. thus, the rating predictiontask r can be expressed as:.
r(u, i) = (vu, vi) → ˆrui.
(1).
its corresponding objective function, the meansquared error (mse), is given below:.
m se =.
(rui − ˆrui)2.
(2).
1|τ |.
(cid:88).
u,i∈τ.
cx =.
1|cx|.
g(cid:88).
y=1.
{s(cid:48).
iy|d(s(cid:48).
iy) = x}.
(5).
3.2 sentence extraction and bert encoding.
first, the reviews in vi are concatenated togetherto form a single document.
a sentence segmenta-tion component called sentencizer (by spacy) isutilized to split this document into individual sen-tences (gupta and nishu, 2020).
the set of all sen-tences in vi is now given by si = {si1, si2, ..., sig}where g refers to the total number of sentences..afterward, si is fed to a pre-trained bertlargemodel.
it should be noted that we opt not to use[cls] representations as these may not necessar-ily provide the best sentence embeddings (miller,2019).
in this regard, we tap bert’s penultimateencoder layer to obtain the contextualized word em-beddings.
the word embeddings of each sentence.
where |cx| refers to the number of sentences thatcluster cx contains.
by introducing clustering, re-dundant and related sentences are grouped in thesame cluster.
concerning this, k is derived usingthis equation:.
k = φi × g.(6).
where φi pertains to the item summary ratio, i.e.,the percentage of sentences that comprise an item’sextractive summary.
this subsequently implies thatk denotes the actual number of sentences in thesummary.
sentences closest to each cluster cen-troid are selected and combined to form the item’srepresentation-explanation.
this is mathematically.
2984user-item fusion layer...sentenceextractionbertuserrxembeddingclusteringuserivuser fusion layeruser modeling...sentenceextractionbertitemrxembeddingclusteringitemivitem fusion layeritem modelingmlp layer 1mlp layer 2mlp layer lrelureluinteractionfunction...fusionexpressed as:.
e(cx) = argminy=1,...,g{||s(cid:48).
iy − cx||2}.
itemrxi =.
1k.k(cid:88).
x=1.
s(cid:48)i,e(cx).
(7).
where e is a function that returns the near-est sentence to the centroid cx of cluster cx,and itemrxi ∈ r1×1024 is the representation-explanation embedding of item i..3.4 fusion layers.
inspired by narre (chen et al., 2018), we alsodraw some principles from the traditional latent fac-tor model by incorporating rating-based hidden vec-tors that depict users and items to a certain extent.
these are represented by u seriv and itemiv ,both in r1×m where m is the dimension of thelatent vectors.
such vectors are fused with theirrespective representation-explanation embeddings.
this is facilitated by these fusion levels, illustratedby the following formulas:.
dataset.
#reviews.
#users.
#items.
automotivedigital musicinstant videopatio, lawn,& garden.
20,47364,70637,126.
13,272.
2,9285,5415,130.
1,686.
1,8353,5681,685.
962.table 2: statistics of the datasets utilized in our study..where hl represents the l-th mlp layer, and wland bl pertain to the l-th layer’s weight matrixand bias vector, respectively.
as far as the mlp’sactivation function is concerned, we select the rec-tiﬁed linear unit (relu), which yields better per-formance than other activation functions (he et al.,2017).
finally, the mlp’s output is fed to one morelinear layer to produce the predicted rating:.
ˆrui = hl × wl+1 + bl+1.
(10).
4 empirical evaluation.
4.1 research questions.
fu = (u serrxu × wu + bu) + u serivufi = (itemrxi × wi + bi) + itemivifui = [fu, fi].
(8).
in this section, we detail our experimental setup de-signed to answer the following research questions(rqs):.
where fu and fi pertain to the preliminary fusionlayers and both are in r1×m; wu and wi areweight matrices in r1024×m; bu and bi refer tobias vectors; and fui ∈ r1×2m denotes the initialuser-item interactions from the third fusion layerand is later fed to the mlp..3.5 multilayer perceptron and rating.
prediction.
the mlp is necessary to model the cf effect, i.e.,to learn meaningful non-linear interactions betweenusers and items.
an mlp with multiple hiddenlayers typically implies a higher degree of non-linearity and ﬂexibility.
similar to the strategyof he et al.
(2017), escofilt adopts an mlpwith a tower pattern; the bottom layer is the widestwhile every succeeding top layer has fewer neurons.
a tower structure enables the mlp to learn moreabstractive data features.
speciﬁcally, we halvethe size of hidden units for each successive higherlayer.
escofilt’s mlp component is deﬁned asfollows:.
h1 = relu (fui × w1 + b1)hl = relu (hl−1 × wl + bl).
(9).
• rq1: does escofilt outperform the other.
state-of-the-art recommender baselines?
• rq2: is embedding clustering effective?
• rq3: can our model produce explanations.
acceptable to humans in real life?.
4.2 datasets, baselines, and evaluation.
metric.
table 2 summarizes the four public datasets1 thatwe utilized in our study.
these datasets are ama-zon 5-core, wherein users and items are guaranteedto have at least ﬁve reviews each (mcauley et al.,2015; he and mcauley, 2016).
the ratings acrossall datasets are in the range of [1, 5].
we split eachdataset into training (80%), validation (10%), andtest (10%) sets.
next, to validate the effectivenessof escofilt, we compared its prediction perfor-mance against four state-of-the-art baselines:.
• benefict (pugoy and kao, 2020): this re-cent recommender model uniquely integratesbert, msp, and mlp to learn representa-tions, explanations, and interactions..1http://jmcauley.ucsd.edu/data/amazon/.
2985• deepconn (zheng et al., 2017): this is theﬁrst deep collaborative neural network modelthat is based on two parallel cnns to jointlylearn user and item features..• mpcn (tay et al., 2018): akin to narre,this cnn-less model employs a new type ofdual attention for identifying relevant reviews.
• narre (chen et al., 2018): similar to deep-conn, it is a neural attentional regressionmodel that integrates two parallel cnns andthe review-level attention mechanism..all these recommender models employed thesame dataset split.
we then computed the rootmean square error (rmse) on the test dataset (¯τ ),as indicated by the formula below.
rmse is awidely used metric for evaluating a model’s ratingprediction accuracy (steck, 2013)..rm se =.
(rui − ˆrui)2.
(11).
(cid:115) 1|¯τ |.
(cid:88).
u,i∈¯τ.
4.3 experimental settings.
for escofilt, we mainly based its summarizationcomponent on bert extractive summarizer2 bymiller (2019).
we also utilized the pre-trainedbertlarge model afforded by the transformerslibrary of huggingface3.
in our implementation4,the following hyperparameters were ﬁxed:.
• learning rate: 0.006• quantity of mlp layers: 4• item summary ratio (φi): 0.4• user summary ratio (φu): 0.4.on the other hand, we operated an exhaustive gridsearch over these hyperparameters:.
• number of epochs: [1, 30]• latent vector dimension (m): {32, 128, 220}.
due to its architectural similarity to escofilt,we reimplemented benefict by augmenting itwith the pre-trained bertlarge model and adopt-ing our model’s fusion and latent vector dimensionstrategies.
for deepconn, mpcn, and narre,we employed the extensible nrrec framework5and retained the other hyperparameters reported inthe framework (liu et al., 2019b)..2https://github.com/dmmiller612/bert-extractive-.
summarizer.
3https://github.com/huggingface/transformers4https://github.com/reinaldncku/escofilt5https://github.com/shomyliu/neu-review-rec.
for the four baselines, we also performed an.
exhaustive grid search over the following:.
• number of epochs: [1, 30]• learning rates: {0.003, 0.004, 0.006}.
all models, including escofilt, used the sameoptimizer, adam, which leverages the power ofadaptive learning rates during training (kingmaand ba, 2014).
this makes the selection of a learn-ing rate less cumbersome, leading to faster conver-gence (chen et al., 2018).
without special mention,the models shared the same random seed, batchsize (128), and dropout rate (0.5).
we selected themodel conﬁguration with the lowest rmse on thevalidation set.
we ran our experiments on nvidiageforce rtx 2080 ti..4.4 prediction results and discussion.
4.4.1 performance comparisonthe overall performances of our model and theother baselines are summarized in table 3. it isessential to remark that although utilizing informa-tion derived from reviews is beneﬁcial, a model’sperformance can vary contingent on how the saidinformation is considered.
these are our generalﬁndings:.
first, our proposed model consistently outper-forms all baselines across all datasets.
this ascer-tains the effectiveness of escofilt and clearlyanswers rq1.
moreover, this validates our casethat coupling bert (a superior review feature ex-tractor) with embedding clustering enables userand item representations to have ﬁner granularityand fewer redundancies..second, receiving the two lowest average rmsevalues, bert-based models (escofilt andbenefict) have generally better prediction ac-curacies than the rest of the mostly cnn-poweredbaselines.
this particular observation veriﬁes thenecessity of integrating bert in a cf architec-ture.
unlike its mainstream counterparts, bertproduces more semantically meaningful embed-dings that keep essential elements such as globalcontext and word frequency information..4.4.2 efﬁcacy of embedding clusteringthis section further discusses the efﬁcacy of k-means embedding clustering, instrumental in pro-ducing user and item representative summaries.
concerning this, we prepared three variants of ourmodel.
first is escofilt-n, which does not uti-lize any embedding clustering.
instead, it relies on.
2986model.
automotive.
benefictdeepconnmpcnnarreescofilt.
0.90230.90760.91070.91440.8968.digitalmusic.
0.89100.89040.92980.89150.8831.instantvideo.
0.97460.97780.99760.97580.9742.patio, lawn,& garden.
0.93520.93160.93620.95390.9298.average.
0.92580.92690.94360.93390.9210.table 3: performance comparison of the recommender models.
the best rmse values are boldfaced..figure 2: performance comparison of escofilt variants for illustrating the effectiveness of embedding cluster-ing..traditional embeddings that are neither pre-trainednor review-based.
they are randomly initializedyet optimized during training.
another variant isescofilt-i, wherein only item reviews undergoembedding clustering while the user componentis based on traditional embeddings.
escofilt-u also operates the same way; the difference isthat only user reviews are processed by embeddingclustering..based on figure 2, having the lowest validationrmse values, the default escofilt conﬁgura-tion is the best across the datasets, while the worstvariant is escofilt-n. this gives credence to em-bedding clustering’s effectiveness and addressesrq2; it can simultaneously capture user prefer-ences and item characteristics, resulting in preciserepresentations and accurate rating prediction..there appears to be a trend as well: the second-best and the third-best variants are escofilt-iand escofilt-u, respectively.
in some instances,escofilt-i seems to be on par with the defaultescofilt variant.
this implies that items stand tobeneﬁt more than users from embedding clustering.
one possible explanation is that each item normallyreceives a far greater quantity of reviews than eachuser actually writes, translating to more possiblyextractable information and features.
hence, itemreviews have a more signiﬁcant inﬂuence than user.
reviews in determining ratings.
still, this does notimmediately suggest that user embedding cluster-ing is not helpful.
it needs to be integrated ﬁrstwith item embedding clustering via the mlp todiscover relevant user-item interactions, leading toour original model’s performance..5 explainability study.
5.1 real-life explainability criteria.
the assessment of explanations in existing recom-mender systems literature is generally limited tospeciﬁc case studies.
most of these relied on simplequalitative analysis of attention weights and high-scoring reviews on selected samples (liu et al.,2019a; seo et al., 2017; wu et al., 2019).
theassessment criterion provided in the narre andbenefict papers went a little further by askinghuman raters to score each explanation’s helpful-ness or usefulness on a given likert scale (chenet al., 2018; pugoy and kao, 2020).
nevertheless,to the best of our knowledge, there does not appearto be a comprehensive set of criteria that assessesthe real-life explainability of explanations.
we con-tend that it is increasingly necessary to measurehow people actually perceive explanation texts gen-erated by recommender models; after all, thesetexts aim to explain entities in real life.
hence, we.
2987model.
cohe-rence.
comple-teness.
novelty.
quality.
perceivedtruth.
visuali-zation.
benefictnarreescofilt.
3.523.683.92.
3.823.823.87.
3.583.723.75.
3.873.753.92.
3.653.723.72.
3.653.923.78.lack ofalterna-tives.
3.753.823.73.table 4: comparison of the three explanation types based on the real-life explainability criteria (pointwise evalua-tion).
the best mean values for each criterion are boldfaced..figure 3: distribution of the judges’ helpfulness rankings for the three explanation types (listwise evaluation)..propose the following explainability criteria, whichare inspired by zemla et al.
(2017):.
1. coherence: “parts of the explanation ﬁt to-.
2. completeness: “there are no gaps in the ex-.
gether coherently.”.
planation.”.
3. lack of alternatives: “there are probablyless to no reasonable alternative explanations.”4. novelty: “i learned something new from the.
5. perceived truth: “i believe this explanation.
explanation.”.
to be true.”.
6. quality: “this is a good explanation.”7. visualization: “it is easy to visualize what.
the explanation is saying.”.
5.2 human assessment of explanations.
we generated a total of 90 item explanations,30 each from benefict (token-level), narre(review-level), and escofilt (summary-level).
for pointwise evaluation, we asked two humanjudges to assess the explanations based on our pro-posed real-life explainability criteria on a ﬁve-pointlikert scale.
for listwise evaluation, we instructedthem to rank the three explanation types for everytext according to helpfulness.
we further exam-ined these results by determining the strength ofagreement between the two judges, using cohen’skappa coefﬁcient (κ) wherein -1 indicates a less.
than chance agreement, 0 refers to a random agree-ment, and 1 denotes a perfect agreement (borromeoand toyama, 2015; landis and koch, 1977)..5.3 explainability results and discussion.
table 4 summarizes the results of the human judges’pointwise evaluation.
for ﬁve out of seven crite-ria, escofilt-derived explanations have the high-est explainability scores.
speciﬁcally, summary-level explanations are most coherent, most com-plete, most novel, and most truthful.
escofilt’sstrongest aspect is its perceived truth, obtaining amean rating of 3.92 and κ = 0.28 that indicates afair inter-judge agreement..interestingly, both escofilt and narre havethe best quality, with the same mean rating of3.72. the kappa coefﬁcient is 0.11, implying thatthe judges agree with each other to a certain ex-tent.
considering that a review-level explanationis simply the highest weighted review, our model-generated explanations are assessed on par withthe former.
furthermore, review-level explanationshave the highest explainability scores in two othercriteria, i.e., lack of alternatives and visualization.
narre’s strongest aspect is that its explanationsare easiest to visualize, having a mean rating of3.92 and κ = 0.27 that denotes a fair inter-judgeagreement..lastly, figure 3 shows the results of the humanjudges’ listwise evaluation.
our model producesthe most helpful explanations; such explanations.
2988are ranked ﬁrst for almost 83% of the items.
theseare followed far behind by narre’s explanations,ranked ﬁrst for nearly 17% of the items.
none ofbenefict’s explanations are ranked ﬁrst.
withκ = 0.45 for ranking consistency, there is a moder-ate agreement between the judges..in summary, these results clearly illustrate thesuperiority of summary-level explanations in reallife that can present necessary guidance to users inmaking future purchasing decisions, thereby satis-fying rq3..6 conclusion and future work.
in this study, unifying representations and expla-nations, in the form of extractive summaries, havefurther enhanced collaborative ﬁltering accuracyand explainability.
we have successfully developeda model that uniquely integrates bert, embeddingclustering, and mlp.
our experiments on variousdatasets verify escofilt’s predictive capability,and the human judges’ assessments validate its ex-plainability in real life.
in the future, we shallconsider expanding our model’s explainability ca-pability by possibly incorporating other nlp princi-ples such as abstractive summarization and naturallanguage generation..acknowledgment.
this work was funded in part by qualcommthrough a taiwan university research collabora-tion project and also in part by the ministry of sci-ence and technology, taiwan, under ncku b109-k027d and most 109-2221-e-006-173 grants,respectively..references.
sarah andersen.
2014. sentence types and functions..san jose state university writing center..ria mae borromeo and motomichi toyama.
2015. au-tomatic vs. crowdsourced sentiment analysis.
inproceedings of the 19th international database en-gineering & applications symposium, pages 90–95..chong chen, min zhang, yiqun liu, and shaopingma.
2018. neural attentional rating regression within proceedings of thereview-level explanations.
2018 world wide web conference, pages 1583–1592..jess drake.
2018..introduction to logic.
ed-tech.
press..xingjie feng and yunze zeng.
2019. neural collabo-rative embedding from reviews for recommendation.
ieee access, 7:103263–103274..sarang gupta and kumari nishu.
2020. mapping localnews coverage: precise location extraction in textualnews content using ﬁne-tuned bert based languagemodel.
in proceedings of the 4th workshop on natu-ral language processing and computational socialscience, pages 155–162..ruining he and julian mcauley.
2016. ups and downs:modeling the visual evolution of fashion trends withone-class collaborative ﬁltering.
in proceedings ofthe 25th international conference on world wideweb, pages 507–517..xiangnan he, xiaoyu du, xiang wang, feng tian, jin-hui tang, and tat-seng chua.
2018. outer product-based neural collaborative ﬁltering.
arxiv preprintarxiv:1808.03912..xiangnan he, lizi liao, hanwang zhang, liqiang nie,xia hu, and tat-seng chua.
2017. neural collabo-rative ﬁltering.
in proceedings of the 26th interna-tional conference on world wide web, pages 173–182..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..yehuda koren, robert bell, and chris volinsky.
2009.matrix factorization techniques for recommendersystems.
computer, 42(8):30–37..j richard landis and gary g koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, pages 159–174..donghua liu, jing li, bo du, jun chang, and ronggao.
2019a.
daml: dual attention mutual learningbetween ratings and reviews for item recommenda-tion.
in proceedings of the 25th acm sigkdd in-ternational conference on knowledge discovery &data mining, pages 344–352..hongtao liu, fangzhao wu, wenjun wang, xianchenwang, pengfei jiao, chuhan wu, and xing xie.
2019b.
nrpa: neural recommendation with person-in proceedings of the 42nd inter-alized attention.
national acm sigir conference on research anddevelopment in information retrieval, pages 1233–1236..julian mcauley, christopher targett, qinfeng shi, andanton van den hengel.
2015. image-based recom-in proceed-mendations on styles and substitutes.
ings of the 38th international acm sigir confer-ence on research and development in informationretrieval, pages 43–52..derek miller.
2019. leveraging bert for extrac-tive text summarization on lectures.
arxiv preprintarxiv:1906.04165..2989chuhan wu, fangzhao wu, junxin liu, and yongfenghuang.
2019. hierarchical user and item represen-tation with three-tier attention for recommendation.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 1818–1826..shuyin xia, daowan peng, deyu meng, changqingzhang, guoyin wang, elisabeth giem, wei wei, andzizhong chen.
2020. a fast adaptive k-means withno bounds.
ieee transactions on pattern analysisand machine intelligence..jeffrey c zemla, steven sloman, christos bechlivani-dis, and david a lagnado.
2017. evaluating every-day explanations.
psychonomic bulletin & review,24(5):1488–1500..yongfeng zhang, guokun lai, min zhang, yi zhang,yiqun liu, and shaoping ma.
2014. explicit fac-tor models for explainable recommendation basedon phrase-level sentiment analysis.
in proceedingsof the 37th international acm sigir conference onresearch and development in information retrieval,pages 83–92..lei zheng, vahid noroozi, and philip s yu.
2017. jointdeep modeling of users and items using reviews forrecommendation.
in proceedings of the 10th acminternational conference on web search and datamining, pages 425–434..cataldo musto, marco de gemmis, giovanni semer-aro, and pasquale lops.
2017. a multi-criteriarecommender system exploiting aspect-based senti-ment analysis of users’ reviews.
in proceedings ofthe 11th acm conference on recommender systems,pages 321–325..georgina peake and jun wang.
2018. explanation min-ing: post hoc interpretability of latent factor mod-els for recommendation systems.
in proceedings ofthe 24th acm sigkdd international conference onknowledge discovery & data mining, pages 2060–2069..mohammad taher pilehvar and jose camacho-collados.
2019. wic: the word-in-context datasetfor evaluating context-sensitive meaning representa-in proceedings of the 2019 conference oftions.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages1267–1273..reinald adrian pugoy and hung-yu kao.
2020. bert-based neural collaborative ﬁltering and ﬁxed-lengthin proceedings ofcontiguous tokens explanation.
the 1st conference of the asia-paciﬁc chapter of theassociation for computational linguistics and the10th international joint conference on natural lan-guage processing, pages 143–153..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016. why should i trust you?
: explain-in proceed-ing the predictions of any classiﬁer.
ings of the 22nd acm sigkdd international con-ference on knowledge discovery & data mining,pages 1135–1144..sungyong seo, jing huang, hao yang, and yan liu.
interpretable convolutional neural networks2017.with dual local and global attention for review ratingprediction.
in proceedings of the 11th acm confer-ence on recommender systems, pages 297–305..harald steck.
2013. evaluation of recommendations:in proceedings ofrating-prediction and ranking.
the 7th acm conference on recommender systems,pages 213–220..yi tay, anh tuan luu, and siu cheung hui.
2018.multi-pointer co-attention networks for recommen-in proceedings of the 24th acm sigkdddation.
international conference on knowledge discovery& data mining, pages 2309–2318..qianqian wang, si li, and guang chen.
2018a.
word-driven and context-aware review modeling for rec-ommendation.
in proceedings of the 27th acm in-ternational conference on information and knowl-edge management, pages 1859–1862..xiang wang, xiangnan he, fuli feng, liqiang nie,and tat-seng chua.
2018b.
tem: tree-enhancedembedding model for explainable recommendation.
in proceedings of the 2018 world wide web confer-ence, pages 1543–1552..2990