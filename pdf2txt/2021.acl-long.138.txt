a joint model for dropped pronoun recovery and conversationaldiscourse parsing in chinese conversational speechjingxuan yang1, kerui xu1, jun xu2,3âˆ—, si li1, sheng gao1, jun guo1,nianwen xue4 and ji-rong wen2,31school of artiï¬cial intelligence, beijing university of posts and telecommunications2gaoling school of artiï¬cial intelligence, renmin university of china3beijing key laboratory of big data management and analysis methods4department of computer science, brandeis university{yjx, xukerui, lisi, gaosheng, guojun}@bupt.edu.cnjunxu@ruc.edu.cn, jirong.wen@gmail.com, xuen@brandeis.edu.
abstract.
in this paper, we present a neural model forjoint dropped pronoun recovery (dpr) andconversational discourse parsing (cdp) in chi-nese conversational speech.
we show thatdpr and cdp are closely related, and a jointmodel beneï¬ts both tasks.
we refer to ourmodel as discproreco, and it ï¬rst encodesthe tokens in each utterance in a conversa-tion with a directed graph convolutional net-work (gcn).
the token states for an utter-ance are then aggregated to produce a sin-gle state for each utterance.
the utterancestates are then fed into a biafï¬ne classiï¬erto construct a conversational discourse graph.
a second (multi-relational) gcn is then ap-plied to the utterance states to produce a dis-course relation-augmented representation forthe utterances, which are then fused togetherwith token states in each utterance as inputto a dropped pronoun recovery layer.
thejoint model is trained and evaluated on a newstructure parsing-enhanced dropped pronounrecovery (spdpr) dataset that we annotatedwith both two types of information.
experi-mental results on the spdpr dataset and otherbenchmarks show that discproreco signiï¬-cantly outperforms the state-of-the-art base-lines of both tasks..1.introduction.
pronouns are often dropped in chinese conversa-tions as the identity of the pronoun can be inferredfrom the context (kim, 2000; yang et al., 2015)without causing the sentence to be incomprehensi-ble.
the task of dropped pronoun recovery (dpr)aims to locate the position of the dropped pronounand identify its type.
conversational discourse pars-ing (cdp) is another important task that aims toanalyze the discourse relations among utterances.
âˆ— corresponding author.
figure 1: top: a conversation snippet in which thedropped pronoun is shown in bracket.
bottom: pro-noun recovery results by two baselines and the pro-posed discproreco.
baselines which ignore the rela-tion â€œ(b3 expands b2) replies a2â€ mistakenly recoverthe dropped pronoun ä½ (you) as æˆ‘(i) since the utter-ance b3 is considered semantically similar to a2..in a conversation, and plays a vital role in under-standing multi-turn conversations..existing work regards dpr and cdp as two in-dependent tasks and tackles them separately.
asan early attempt of dpr, yang et al.
(2015) em-ploy a maximum entropy classiï¬er to predict theposition and type of dropped pronouns.
zhang et al.
(2019) and yang et al.
(2019) attempt to recoverthe dropped pronouns by modeling the referentswith deep neural networks.
more recently, yanget al.
(2020) attempt to jointly predict all droppedpronouns in a conversation snippet by modeling de-pendencies between pronouns with general condi-tional random ï¬elds.
a major shortcoming of thesedpr methods is that they overlook the discourserelation (e.g., reply, question) between conversa-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1752â€“1763august1â€“6,2021.Â©2021associationforcomputationallinguistics1752a1:      å¼ è€å¸ˆåœ¨å®¶å—ï¼Ÿis miss zhang at home?b1: å¥¹ç°åœ¨ä¸åœ¨å®¶ï¼Œä»Šå¤©å¥¹åœ¨å¤–é¢ä¸Šè¯¾ã€‚she is not home right now.
she is teaching outside.a2:  æˆ‘å¯ä»¥ç»™å¥¹æ‰“ä¸ªç”µè¯å—ï¼Ÿcan i give her a call?b2:  ä¸‹åˆå››ç‚¹ä»¥åå¥¹åº”è¯¥æœ‰æ—¶é—´æ¥ç”µè¯ã€‚sheshouldhave time to answer the phone after 4 p.m.b4:  æˆ–è€…ä½ æ˜å¤©æ—©ä¸Šå†æ‰“or youcan call her tomorrow morning.b5:  æ˜å¤©å¥¹åœ¨å®¶ã€‚she will be at home tomorrow.questionreplyexpansionexpansionreplyreplyb3:  (ä½ )å¯ä»¥ç»™å¥¹æ‰“ä¸ªç”µè¯(you) can give her a call.
(pro-drop utterance)pleonastictional utterances when exploiting the context of thedropped pronoun.
at the same time, previous cdpmethods (li et al., 2014; afantenos et al., 2015;shi and huang, 2019) ï¬rst predict the relation foreach utterance pair and then construct the discoursestructure for the conversation with a decoding al-gorithm.
the effectiveness of these methods arecompromised since the utterances might be incom-plete when they have dropped pronouns..to overcome these shortcomings, we propose anovel neural model called discproreco to performdpr and cdp jointly.
figure 1 is a chinese conver-sation snippet between two speakers a and b thatillustrates the advantages of such a joint approach.
in this example, a pronoun â€œä½  (you)â€ is droppedin utterance b3.
it is critical for the dpr model toknow that both utterances b2 and b3 are in replyto the utterance a2, when recovering this droppedpronoun.
methods which ignore the structure (â€œ(b3expands b2) replies a2â€) will more likely considerthe utterance b3 to be semantically similar to a2,and wrongly recover the pronoun as â€œæˆ‘ (i)â€..given a pro-drop utterance and its context, dis-cproreco parses the discourse structure of the con-versation and recovers the dropped pronouns inthe utterance in four steps: (i) each utterance isparsed into its dependency structure and fed into adirected gcn to output the syntactic token states.
the utterance state is then obtained by aggregat-ing the token states in the utterance.
(ii) the ut-terance states of a conversation are fed into a bi-afï¬ne classiï¬er to predict the discourse relationbetween each utterance pair and the discourse struc-ture of the conversation is constructed.
(iii) takingthe discourse structure as input, another (multi-relational) gcn updates the utterance states andfuses them into the token states for each utteranceto produce discourse-aware token representations.
(iv) based on the discourse structure-aware con-text representation, a pronoun recovery module isdesigned to recover the dropped pronouns in the ut-terances.
when training this model, all componentsare jointly optimized by parameter sharing so thatcdp and dpr can beneï¬t each other.
as there isno public dataset annotated with both dropped pro-nouns and conversational discourse structures, wealso construct structure parsing-enhanced droppedpronoun recovery (spdpr) corpus, which is theï¬rst corpus annotated with both types of informa-tion.
experimental results show that discprorecooutperforms all baselines of cdp and dpr..contributions: this work makes the followingcontributions: (i) we propose a uniï¬ed frameworkdiscproreco to jointly perform cdp and dpr, andshow that these two tasks can beneï¬t each other.
(ii) we construct a new large-scale dataset spdpr(section 4) which supports fair comparison acrossdifferent methods and facilitates future research onboth dpr and cdp.
(iii) we present experimentalresults which show that discproreco with its jointlearning mechanism realizes knowledge sharingbetween its cdp and dpr components and resultsin improvements for both tasks (section 5).
thecode and spdpr dataset is available at https://github.com/ningningyang/discproreco..2 problem formulation.
we ï¬rst introduce the problem formulation of thesetwo tasks.
following the practices in (yang et al.,2015, 2019, 2020), we formulate dpr as a se-quence labeling problem.
dpr aims to recover thedropped pronouns in an utterance by assigning oneof 17 labels to each token that indicates the typeof pronoun that is dropped before the token (yanget al., 2015).
cdp is the task of constructing theconversational discourse structure by predicting thediscourse relation (xue et al., 2016) among utter-ances.
the discourse relations may characterizeone utterance as agreeing with, responding to, orindicate understanding of another utterance in theconversational context..let us denote an input pro-drop utterance of n to-kens as x = (w1, w2, Â· Â· Â· , wn), and its contextualutterances as c = (x1, x2, Â· Â· Â· , xm) where the i-th contextual utterance xi is a sequence of li tokens:xi = (wi,1, Â· Â· Â· , wi,li).
our task aims to (1) modelthe distribution p(xj|xi, c) to predict the relationbetween each pair of utterances (i.e., (xi, xj)) forcdp, and (2) model Ë†y = arg maxy p(y|x, c)to predict the recovered pronoun sequence Ë†y forthe input utterance x. each element of Ë†y ischosen from one of the t possible labels fromy = {y1, Â· Â· Â· , ytâˆ’1}âˆª{none} to indicate whethera pronoun is dropped before the corresponding to-ken in utterance x and the type of the droppedpronoun.
the label â€œnoneâ€ means no pronoun isdropped before this token..3 the discproreco framework.
3.1 model overview.
the architecture of discproreco is illustrated infigure 2. given a pro-drop utterance x and its.
1753figure 2: overview of discproreco, which explores conversational discourse structures to learn effective referentrepresentations that are used to recover dropped pronouns.
discproreco consists of four components, and thedetails are introduced in section 3..context c, discproreco ï¬rst represents tokens ofthese utterances as d-dimensional pre-trained wordembeddings (li et al., 2018), and then feed theminto a bigru (chung et al., 2014) network, torepresent sequential token states x âˆˆ rnÃ—d andc âˆˆ rmÃ—lmÃ—d as the concatenation of forwardand backward hidden states outputted from bigru.
the syntactic dependency encoding layer then re-vises the sequential token states by exploiting thesyntactic dependencies between tokens in the sameutterance using a directed gcn and generates utter-ance representations.
after that, the biafï¬ne rela-tion prediction layer predicts the relation betweeneach pair of utterances.
the discourse structurethen is constructed based on the utterance nodesand the predicted relations.
the discourse structureencoding layer further encodes the inter-utterancediscourse structures with a multi-relational gcn,and employs the discourse-based utterance repre-sentations to revise the syntactic token states.
fi-nally, the pronoun recovery layer explores the ref-erent semantics from the context c and predicts thedropped pronouns in each utterance..3.2 syntactic dependency encoding layer.
as the sequentialtoken states overlook long-distance dependencies among tokens in a utterance,this layer takes in the sequential token states x andc, and revises them as syntactic token states as hxand hc by exploring the syntactic dependencies.
between the tokens based on a directed gcn..speciï¬cally, for each input utterance in x andc, we ï¬rst extract syntactic dependencies betweenthe tokens with stanfordâ€™s stanza dependencyparser (qi et al., 2020).
using the output of thedependency parser, we construct a syntactic depen-dency graph for each utterance in which the nodesrepresents the tokens and the edges correspond tothe extracted syntactic dependencies between thetokens.
following the practices of (marcheggianiand titov, 2017; vashishth et al., 2018), three typesof edges are deï¬ned in the graph.
the node statesare initialized by the sequential token states x andc, and then message passing is performed over theconstructed graph using the directed gcn (kipfand welling, 2017), referred to as syngcn.
thesyntactic dependency representation of token wi,nafter (k + 1)-th gcn layer is deï¬ned as:.
hk+1.
wi,n = relu.
(cid:16)(cid:80).
uâˆˆn+(wi,n) gk.
e Â· (cid:0)wk.
e hk.
u + bke.(cid:1)(cid:17).
,.
e âˆˆ rdÃ—d and bk.
e âˆˆ rd are the edge-where wkspeciï¬c parameters, n+(wi,n) = n (wi,n) âˆª{wi,n} is the set of wi,nâ€™s neighbors including it-self, and relu(Â·) = max(0, Â·) is the rectiï¬edlinear unit.
gke is an edge-wise gating mechanismwhich incorporates the edge importance as:(cid:16).
(cid:17).
gke = Ïƒ.
Ë†wk.
e hk.
u + Ë†bke.,.
where Ë†wk.
e âˆˆ r1Ã—d and Ë†bk.
e âˆˆ r are independent.
1754â‹¯â‹¯â‹¯word embeddingsğ‘¤",$ğ‘¤",%ğ‘¤",&syntactic dependency graphbigruencoderencoderencoderencoderencoderx"($x"()x"*$x"*%x"syngcnå¬ä¸è§hear    canâ€™tğ“§,ğ“’ğ“—.,ğ“—0â‹¯mlpğ’‰"()ğ’‰"($ğ’‰"ğ’‰"*%ğ’‰"*$+Ï„Ï„333biaffineclassifierrelgcnâ‹¯â¨‚â‹¯ğ“©.,ğ“©0(æˆ‘)å¬ä¸è§ğœ™ğœ™(i)  hear    canâ€™tsyntactic dependency encoding layerbiaffinerelation prediction layerpronoun recovery layerğ‘Ÿ"89:_<=8>ğ‘Ÿ?89:_>=@output pronoun recovery resultsoutput discourse parsing resultsinput pro-drop utterancex"trainable parameters for each layer, and Ïƒ(Â·) isthe sigmoid function.
the revised syntactic to-ken states hx and hc of the pro-drop utteranceand context are outputted for subsequent discoursestructure prediction and pronoun recovery..the t-th relation as the arc label prediction func-tion in (dozat and manning, 2017).
in the trainingphase, we also minimize the cross-entropy betweengold relation labels and the predicted relations be-tween utterances as:.
3.3 biafï¬ne relation prediction layer.
i,j.
i,j.
and relation s(rel).
for conversational discourse parsing, we jointlypredict the arc s(arc)betweeneach pair of utterances utilizing the biafï¬ne atten-tion mechanism proposed in (dozat and manning,2017).
given the syntactic token states hx andhc, we make an average aggregation on thesetoken states of each utterance xi to obtain the syn-tactic utterance representation hxi..for a pair of utterances (xi, xj) in the conversa-tion snippet, we feed the representations of thesetwo utterances into a biafï¬ne function to predictthe probability of an arc from xi to xj as:.
r(arc head)i.
= mlp(arc head)(hxi),.
r(arc dep)j.
= mlp(arc dep)(hxj ),.
i.i.u(arc),.
+ r(arc head)t.u(arc)r(arc dep)j.i,j = r(arc head)s(arc)where mlp is the multi-layer perceptron that trans-forms the original utterance representation hxiand hxj into head or dependent-speciï¬c utterancestates r(arc head)and r(arc dep).
u(arc) and u(arc)jiare weight matrix and bias term used to determinethe probability of a arc..one distinctive characteristics of conversationaldiscourse parsing is that the head of each depen-dent utterance must be chosen from the utterancesbefore the dependent utterance.
thus we add anupper triangular mask operation on the results ofarc prediction to regularize the predicted arc head:.
s(arc) = mask(s(arc))..we minimize the cross-entropy of gold head-.
dependent pair of utterances as:.
lossarc = âˆ’.
Î´(xj|xi, c) log(parc(xj|xi, c)),.
m(cid:88).
j=1.
parc(xj|xi, c) = softmax(s(arc).
)..i.after obtaining the predicted directed unlabeledarc between each utterance pair, we calculate thescore distribution s(rel)i,j âˆˆ rk of each arc xi â†’ xj,in which the t-th element indicates the score of.
lossrel = âˆ’ (cid:80)n.j=1 Î´(xj|xi, c) log(prel(xj|xi, c)),.
prel(xj|xi, c) = softmax(s(rel).
i,j )..3.4 discourse structure encoding layer.
after the relations are predicted, we construct thediscourse structure as a multi-relational graph inwhich each node indicates an utterance, and eachedge represents the relation between a pair of utter-ances.
in order to utilize the discourse informationin dropped pronoun recovery process, we ï¬rst en-code the discourse structure, and then utilize thediscourse information-based utterance representa-tions to improve token states which are used tomodel the pronoun referent..speciï¬cally, we apply a multiple relationalgcn (vashishth et al., 2020), referred to as rel-gcn, over the graph to encode the discourse struc-ture based utterance representations r and utilizethe updated representations to further revise syn-tactic token states hx and hc for outputting dis-course structure based token states z x and z c.the node states of the graph are initialized as theaverage aggregation of token states of correspond-ing utterances.
the representation of utterance xiin the (k + 1)-th layer is updated by incorporatingthe discourse relation state hk.
rel as:.
rk+1i = f.(cid:16)(cid:80).
(j,rel)âˆˆn (xi) prel(xj|xi, c)wk.
Î»(rel)Ï†.j , hkrkrel.
(cid:16).
(cid:17)(cid:17).
,.
j and hk.
where rkrel denote the updated representa-tion of utterance j and relation rel after the k-thÎ»(rel) âˆˆ rdÃ—d is a relation-gcn layers, and wktype speciï¬c parameter.
following the practiceof(vashishth et al., 2020), we take the compo-sition operator Ï† as multiplication in this work.
please note that we take in the label distributionprel(xj|xi, c) from the relation prediction layerand compute the weighted sum of each kind of re-lation to update the utterance representation, ratherthan taking the hard predicted relation by applyingan argmax operation over the distribution..after encoding the constructed discourse struc-ture with a message passing process, we obtain thediscourse relation-augmented utterance represen-tations r, and then utilize the updated utterance.
1755representations to revise the syntactic token stateswith a linear feed-forward network:.
3.6 training objective.
zwi,n = w1 Â·.
wi,n; rk+1hk+1.
i.
+ b1,.
(cid:104).
(cid:105).
where hk+1wi,n refers to the token state of wi,n out-putted from the (k + 1)-th layer of syngcn,rk+1refers to the state of the corresponding ut-iterance that the token belongs to, outputted fromthe (k + 1)-th layer of relgcn.
the operationthus augments syntactic token states hx and hcwith discourse information-based utterance rep-resentation to obtain discourse context-based to-ken states z x = (zw1, .
.
.
, zwn) and z c =), which will be used to model the(zw1,i, .
.
.
, zwi,lireferent semantics of the dropped pronoun in thedropper pronoun recovery layer..3.5 pronoun recovery layer.
this layer takes in the revised token representationsz x and z c, and attempts to ï¬nd tokens in contextc that describe the referent of the dropped pronounin the pro-drop utterance x with an attention mech-anism.
the referent representation is then capturedas the weighted sum of discourse context-basedtoken states as:.
awi,i(cid:48),n(cid:48) = softmax(w2.
zwi (cid:12) zwi(cid:48),n(cid:48).
+ b2),.
(cid:16).
(cid:17).
rwi =.
awi,i(cid:48),n(cid:48) Â· zwi(cid:48),n(cid:48) ..m(cid:88).
li(cid:48)(cid:88).
i(cid:48)=1.
n(cid:48)=1.
then we concatenate the referent representationrwi with the syntactic token representation hk+1towipredict the dropped pronoun category as follows:.
hrwi = tanh.
w3 Â·.
(cid:16).
(cid:104).
hk+1wi.
; rwi.
(cid:105).
(cid:17).
,.
+ b3.
p (yi|wi, c) = softmax (w4 Â· hrwi + b4) ..the objective of dropped pronoun recovery aimsto minimize cross-entropy between the predictedlabel distributions and the annotated labels for allsentences as:.
we train our discproreco by jointly optimizingthe objective of both discourse relation predictionand dropped pronoun recovery.
the total trainingobjective is deï¬ned as:.
loss = Î± Â· (lossarc + losslabel) + Î² Â· lossdp,.
(1).
where Î± and Î² are weights of cdp objective func-tion and dpr objective function respectively..4 the spdpr dataset.
to verify the effectiveness of discproreco, weneed a conversational corpus containing the an-notation of both dropped pronouns and discourserelations.
to our knowledge, there is no such a pub-lic available corpus.
therefore, we constructed theï¬rst structure parsing-enhanced dropped pronounrecovery (spdpr) dataset by annotating the dis-course structure information on a popular droppedpronoun recovery dataset (i.e., chinese sms)..the chinese sms/chat dataset consists of 684multi-party chat ï¬les and is a popular benchmarkfor dropped pronoun recovery (yang et al., 2015).
in this study, we set the size of the context snip-pet to be 8 utterances which include the currentpro-drop utterance plus 5 utterances before and 2utterances after.
when performing discourse re-lation annotation we ask three linguistic expertsto independently choose a head utterance for thecurrent utterance from its context and annotate thediscourse relation between them according to aset of 8 pre-deï¬ned relations (see appendix a).
the inter-annotator agreement for discourse rela-tion annotation is 0.8362, as measured by fleissâ€™skappa.
the resulting spdpr dataset consists of292,455 tokens and 40,280 utterances, averaging4,949 utterance pairs per relation, with a minimumof 540 pairs for the least frequent relation and amaximum of 12,252 for the most frequent relation.
the spdpr dataset also annotates 31,591 droppedpronouns (except the â€œnoneâ€ category)..5 experiments.
5.1 experimental settings.
lossdp = âˆ’.
Î´ (yi|wi, c) log (p (yi|wi, c)) ,.
(cid:88).
li(cid:88).
qâˆˆq.
i=1.
where q represents all training instances, li repre-sents the number of words in pro-drop utterance;Î´ (yi|wi, c) represents the annotated label of wi..in this work, 300-dimensional pre-trained embed-dings (li et al., 2018) were input to the bigruencoder, and 500-dimensional hidden states wereuitilized.
for syngcn and relgcn, we set thenumber of gcn layers as 1 and 3 respectively,and augment them with a dropout rate of 0.5. the.
1756spdpr.
tc of ontonotes.
baiduzhidao.
model.
p(%) r(%) f(%) p(%) r(%) f(%) p(%) r(%) f(%).
meprnrmbigrundprxlm-roberta-ndprtransformer-gcrfdiscprorecodiscproreco(xlm-r-w/o relgcn)discproreco(xlm-r).
37.2737.1140.1849.3954.0352.5159.5856.3261.13.
45.5744.0745.3244.8950.1848.1253.6852.2854.26.
38.7639.0342.6746.3952.4649.8157.3755.6759.47.
-23.1225.6439.6343.1440.48-44.62-.
-26.0936.8243.0946.3744.64-47.14-.
-22.8030.9339.7745.1342.45-46.98-.
-26.8729.3541.0446.0443.30-47.31-.
-49.4442.3846.5549.1246.54-50.43-.
-34.5435.8342.9447.5443.92-48.19-.
table 1: experimental results produced by the baseline models, the proposed model discproreco and two variantsof discproreco on all three conversation datasets in terms of precision, recall and f-score..stanza dependency parser (qi et al., 2020) returns41 kinds of dependency edges.
we remove 13 typesof them which connects the punctuation with othertokens, and irrelevant to referent description.
dur-ing training, we utilized adam optimizer (kingmaand ba, 2015) with a 0.005 learning rate and trainedour model for 30 epochs.
the model performedbest on the validation set is used to make predic-tions on the test set.
we repeat each experiment 10times and records the average results..5.2 dropped pronoun recovery.
datasets and evaluation metrics we tested theperformance of discproreco for dpr on threedatasets: (1) tc section of ontonotes release5.0, which is a transcription of chinese telephoneconversations, and is released in the conll 2012shared task.
(2) baiduzhidao, which is a questionanswering corpus (zhang et al., 2019).
ten typesof concrete pronouns were annotated according tothe pre-deï¬ned guidelines.
these two benchmarksdo not contain the discourse structure informationand are mainly used to evaluate the effectiveness ofour model for dpr task.
(3) the spdpr dataset,which contains 684 conversation ï¬les annotatedwith dropped pronouns and discourse relations.
fol-lowing practice in (yang et al., 2015, 2019), wereserve the same 16.7% of the training instancesas the development set, and a separate test set wasused to evaluate the models.
the statistics of thethree datasets are shown in appendix b..same as existing efforts (yang et al., 2015, 2019),we use precision(p), recall(r) and f-score(f)as metrics when evaluating the performance ofdropped pronoun models.
baselines we compared discproreco against ex-.
isting baselines, including: (1) mepr (yang et al.,2015), which leverages a maximum entropy clas-siï¬er to predict the type of dropped pronoun be-fore each token; (2) nrm (zhang et al., 2019),which employs two mlps to predict the positionand type of a dropped pronoun separately; (3) bi-gru, which utilizes a bidirectional gru to encodeeach token in a pro-drop sentence and then makesprediction; (4) ndpr (yang et al., 2019), whichmodels the referents of dropped pronouns from alarge context with a structured attention mecha-nism; (5) transformer-gcrf (yang et al., 2020),which jointly recovers the dropped pronouns ina conversational snippet with general conditionalrandom ï¬elds; (6) xlm-roberta-ndpr, whichutilizes the pre-trained multilingual masked lan-guage model (conneau et al., 2020) to encode thepro-drop utterance and its context, and then em-ploys the attention mechanism in ndpr to modelthe referent semantics..we also compare two variants of discproreco:(1) discproreco (xlm-r-w/o relgcn), whichreplaces the bigru encoder with the pre-trainedxlm-roberta model, removes the relgcnlayer, and only utilizes syngcn to encode syntac-tic token representations for predicting the droppedpronouns.
(2) discproreco(xlm-r) which usesthe pre-trained xlm-roberta model as an en-coder to replace the bigru network in our pro-posed model.
experimental results table 1 reports the resultsof discproreco and the baseline methods on dpr.
please note that for the baseline methods, we di-rectly used the numbers originally reported in thecorresponding papers.
from the results, we ob-served that our variant model discproreco(xlm-.
1757stac.
spdpr.
modelmstilpdeep+mstdeep+ilpdeep+greedydeep sequentialdiscproreco(w/o dpr)discproreco.
arc68.868.669.669.069.373.274.1-.
rel50.452.152.153.151.955.757.0-.
arc--81.0680.5381.3283.0084.5187.97.rel--40.9341.3842.3843.4551.3453.07.figure 3: results of different dpr models..table 2: micro-averaged f-score (%) of conversationaldiscourse parsing on two standard benchmarks..r-w/o relgcn) outperforms existing baselineson three datasets by all evaluation metrics, whichprove the effectiveness of our system as a stand-alone model for recovering dropped pronouns.
we attribute this to the ability of our model toconsider long-distance syntactic dependencies be-tween tokens in the same utterance.
note thatthe results for feature-based baseline mepr (yanget al., 2015) on ontonotes, and baiduzhidao arenot available because several essential featurescannot been obtained.
however, our proposeddiscproreco still signiï¬cantly outperforms dis-cproreco (xlm-r-w/o relgcn) as it achieved3.26%, 1.40%, and 1.70% absolute improvementsin terms of precision, recall and f-score respec-tively on spdpr corpus.
this shows that discourserelations between utterances are crucially impor-tant for modeling the referent of dropped pronounsand achieving better performance in dropped pro-noun recovery.
this is consistent with the obser-vation in (ghosal et al., 2019).
the best resultsare achieved when our model uses uses the pre-trained xlm-roberta (i.e., discproreco(xlm-r)).
note that discourse relations are not availablefor ontonotes and baiduzhidao datasets and thuswe do not have joint learning results for these twodata sets.
error analysis we further investigated some typ-ical mistakes made by our discproreco for dpr.
resolving dpr involves effectively modeling thereferent of each dropped pronoun from the contextto recover the dropped pronoun.
as illustrate infigure 3, both discproreco and ndpr model thereferent from the context.
the former outperformsthe latter since it considers the conversation struc-ture that the utterance b3 is a reply to a3 but not anexpansion to the utterance b1.
however, just mod-eling the referent from the context is insufï¬cient.
in figure 3, the referent of the dropped pronoun.
was correctly identiï¬ed but the dropped pronoun ismistakenly identiï¬ed as â€œ(ä»–ä»¬/they)â€.
this indi-cates that the model needs to be augmented withsome additional knowledge, such as the differencebetween singular and plural pronouns..5.3 conversational discourse parsing.
datasets and evaluation metrics we evaluatedthe effectiveness of our discproreco frameworkfor cdp task on two datasets as: (1) stac, whichis a standard benchmark for discourse parsing onmulti-party dialogue (asher and lascarides, 2005).
the dataset contains 1,173 dialogues, 12,867 edusand 12,476 relations.
same as existing studies,we set aside 10% of the training dialogues as thevalidation data.
(2) spdpr, which is constructedin our work containing 684 dialogues and 39,596annotated relations.
following (shi and huang,2019), we also utilized micro-averaged f-score asthe evaluation metric.
baselines we compared our discproreco with ex-isting baseline methods: (1) mst (afantenos et al.,2015): a approach that uses local information intwo utterances to predict the discourse relation, anduses the maximum spanning tree (mst) to con-struct the discourse structure; (2) ilp (perret et al.,2016): same as mst except that the mst algo-rithm is replaced with integer linear programming(ilp); (3) deep+mst: a neural network that en-codes the discourse representations with gru, andthen uses mst to construct the discourse structure;(4) deep+ilp: same as deep+mst except thatthe mst algorithm is replaced with integer linearprogramming (ilp); (5) deep+greedy: similar todeep+mst and deep+ilp except that this modeluses a greedy decoding algorithm to select the par-ent for each utterance; (6) deep sequential (shiand huang, 2019): a deep sequential neural net-.
1758a1:ä½ åƒé¥­äº†å—ï¼Ÿhaveyoueatenyet?b1:æˆ‘åƒäº†ã€‚i have eaten.a3:æˆ‘ç»™çˆ·çˆ·ä¹°çš„è¯ä»–åƒäº†å—ï¼Ÿdid my grandfather take the medicine i bought for him?b3:(*pro*)  åƒäº†ã€‚(*pro*) had takenthe medicine.contextb3:  (ä»–)åƒäº†(he) had takenthe medicine.b3:  (æˆ‘)åƒäº†(i)had takenthe medicine.b3:  (ä»–ä»¬)åƒäº†(they) had takenthe medicine.goldndprdiscprorecowork which predicts the discourse relation utilizingboth local and global context..in order to explore the effectiveness of jointlearning scheme, we also make a comparison ofour discproreco with its variant, referred to asdiscproreco(w/o dpr), which predict the dis-course relation independently, without recoveringthe dropped pronouns.
experimental results we list the experimentalresults of our approach and the baselines in ta-ble 2. for the stac dataset, we also reported theoriginal results of the stac benchmark from anexisting paper (shi and huang, 2019), and applyour discproreco to this corpus.
for the spdprdataset, we ran the baseline methods with the sameparameter settings.
from the results we can seethat the variant of our approach discproreco (w/odpr) outperforms the baselines of discourse pars-ing.
we attribute this to the effectiveness of thebiafï¬ne attention mechanism for dependency pars-ing task (yan et al., 2020; ji et al., 2019).
however,our approach discproreco still signiï¬cantly outper-forms all the compared models.
we attribute this tothe joint training of the cdp task and the dpr task.
the parameter sharing mechanism makes these twotasks beneï¬ts each other.
note that the results forthe joint model is not available for stac as stacis not annotated with dropped pronouns..5.4.interaction between dpr and cdp.
we also conducted experiments on spdpr to studythe quantitative interaction between dpr and cdp.
firstly, during the training process, we optimize ourdiscproreco model utilizing the objective functionin eq.
1 until the cdp task achieves a speciï¬c f-score (i.e., gradually increases from 30.64 to 50.38).
then we ï¬x the cdp components and continue tooptimize the components of dpr task.
we conductthis experiment to explore the inï¬‚uence of cdptask on the dpr task.
secondly, we set the ratiobetween Î± and Î² in eq.
1 varies from 0.25 to 1.25and record the f-score of dpr and cdp respec-tively.
we conduct this experiment to study theinteranction between these two tasks by modifyingtheir weights in the objective function..results of these two experiments are shown infigure 4. according to figure 4 (a), the perfor-mance of dpr is increased in terms of all eval-uation metrics as the f-score of cdp increases,which indicates that exploring the discourse rela-tions between utterances beneï¬ts dropped pronoun.
figure 4: exploratory results.
(a) interaction betweendpr and cdp; (b) effects of parameters (i.e., Î± and Î²).
recovery.
moreover, figure 4 (b) illustrate the per-formance of dpr and cdp when the ratio betweenÎ± to Î² varies gradually.
results show that the per-formance of cdp remains stable, while the per-formance of dpr increases at beginning and thendecrease sharply as the ratio increases, indicatingthat discproreco framework should pay more at-tention to dpr during the optimizing process..6 related work.
dropped pronoun recovery is a criticaltech-nique that can beneï¬t many downstream appli-cations (wang et al., 2016, 2018; su et al.,2019).
yang et al.
(2015) for the ï¬rst time pro-posed this task, and utilized a maximum entropyclassiï¬er to recover the dropped pronouns in textmessages.
giannella et al.
(2017) further employeda linear-chain crf to jointly predict the positionand type of the dropped pronouns in a single utter-ance using hand-crafted features.
due to the power-ful semantic modeling capability of deep learning,zhang et al.
(2019); yang et al.
(2019) introducedneural network methods to recover the dropped pro-noun by modeling its semantics from the context.
all these methods represent the utterances withoutconsidering the relationship between utterances,which is important to identify the referents.
zeropronoun resolution is also a closely related line ofresearch to dpr (chen and ng, 2016; yin et al.,2017, 2018).
the main difference between dprand zero pronoun resolution task is that dpr con-siders both anaphoric and non-anaphoric pronouns,and doesnâ€™t attempt to resolve it to a referent..17593035404550f-score of arc prediction in cdp444648505254565860performance of dprprecisionrecallf-score0.20.40.60.81.01.2ratio between  to 4446485052545658f-score of dpr and cdpdprcdpexisting discourse parsing methods ï¬rst pre-dicted the probability of discourse relation, andthen applied a decoding algorithm to construct thediscourse structure (muller et al., 2012; li et al.,2014; afantenos et al., 2015; perret et al., 2016).
a deep sequential model (shi and huang, 2019)was further presented to predict the discourse de-pendencies utilizing both local information of twoutterances and the global information of existingconstructed discourse structure.
all these methodsconsider how to do relation prediction indepen-dently.
however, in this work, we explore the con-nection between the cdp and dpr, and attempt tomake these two tasks mutually enhance each other..7 conclusion.
this paper presents that dropped pronoun recov-ery and conversational discourse parsing are twostrongly related tasks.
to make them beneï¬t fromeach other, we devise a novel framework calleddiscproreco to tackle these two tasks simultane-ously.
the framework is trained in a joint learningparadigm, and the parameters for the two tasks arejointly optimized.
to facilitate the study of theproblem, we created a large-scale dataset calledspdpr which contains the annotations of bothdropped pronouns and discourse relations.
experi-mental results demonstrated that discproreco out-performed all baselines on both tasks..acknowledgments.
this work was supported by the national key r&dprogram of china (2019yfe0198200), the na-tional natural science foundation of china (no.
61872338, no.
61832017), beijing academy ofartiï¬cial intelligence (baai2019zd0305), bei-jing outstanding young scientist program no.
bjjwzyjh012019100020098 and bupt excel-lent ph.d. students foundation (no.cx2020305)..references.
stergos d. afantenos, eric kow, nicholas asher, andjÂ´erÂ´emy perret.
2015. discourse parsing for multi-in proceedings of the 2015party chat dialogues.
conference on empirical methods in natural lan-guage processing, emnlp 2015, lisbon, portugal,september 17-21, 2015, pages 928â€“937.
the asso-ciation for computational linguistics..nicholas asher and alex lascarides.
2005. logics ofconversation.
studies in natural language process-ing.
cambridge university press..chen chen and vincent ng.
2016. chinese zero pro-noun resolution with deep neural networks.
in pro-ceedings of the 54th annual meeting of the associ-ation for computational linguistics, acl 2016, au-gust 7-12, 2016, berlin, germany, volume 1: longpapers.
the association for computer linguistics..junyoung chung, cÂ¸ aglar gÂ¨ulcÂ¸ehre, kyunghyun cho,and yoshua bengio.
2014. empirical evaluation ofgated recurrent neural networks on sequence model-ing.
corr, abs/1412.3555..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmÂ´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 8440â€“8451.
associa-tion for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafï¬ne attention for neural dependency pars-in 5th international conference on learninging.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander f. gelbukh.
2019.dialoguegcn: a graph convolutional neural networkfor emotion recognition in conversation.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 154â€“164.
associationfor computational linguistics..chris giannella, ransom k. winder, and stacy pe-tersen.
2017. dropped personal pronoun recoveryin chinese sms.
nat.
lang.
eng., 23(6):905â€“927..tao ji, yuanbin wu, and man lan.
2019. graph-based dependency parsing with graph neural net-in proceedings of the 57th conference ofworks.
the association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 2475â€“2485.
associationfor computational linguistics..young joo kim.
2000. subject/object drop in the ac-quisition of korean: a cross-linguistic comparison.
journal of east asian linguistics, 9(4):325â€“351..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..thomas n. kipf and max welling.
2017..semi-supervised classiï¬cation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations, iclr 2017, toulon, france,.
1760april 24-26, 2017, conference track proceedings.
openreview.net..jiwei li, rumeng li, and eduard h. hovy.
2014. re-cursive deep models for discourse parsing.
in pro-ceedings of the 2014 conference on empirical meth-ods in natural language processing, emnlp 2014,october 25-29, 2014, doha, qatar, a meeting ofsigdat, a special interest group of the acl, pages2061â€“2069.
acl..shen li, zhe zhao, renfen hu, wensi li, tao liu,and xiaoyong du.
2018. analogical reasoning onchinese morphological and semantic relations.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume 2:short papers, pages 138â€“143.
association for com-putational linguistics..diego marcheggiani and ivan titov.
2017. encodingsentences with graph convolutional networks for se-in proceedings of the 2017mantic role labeling.
conference on empirical methods in natural lan-guage processing, emnlp 2017, copenhagen, den-mark, september 9-11, 2017, pages 1506â€“1515.
as-sociation for computational linguistics..philippe muller, stergos d. afantenos, pascal denis,and nicholas asher.
2012. constrained decoding fortext-level discourse parsing.
in coling 2012, 24thinternational conference on computational linguis-tics, proceedings of the conference: technical pa-pers, 8-15 december 2012, mumbai, india, pages1883â€“1900.
indian institute of technology bombay..jÂ´erÂ´emy perret, stergos d. afantenos, nicholas asher,and mathieu morey.
2016. integer linear program-in naacl hlt 2016,ming for discourse parsing.
the 2016 conference of the north american chap-ter of the association for computational linguistics:human language technologies, san diego califor-nia, usa, june 12-17, 2016, pages 99â€“109.
the as-sociation for computational linguistics..peng qi, yuhao zhang, yuhui zhang, jason bolton,stanza: aand christopher d. manning.
2020.python natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations, acl 2020, on-line, july 5-10, 2020, pages 101â€“108.
associationfor computational linguistics..zhouxing shi and minlie huang.
2019. a deep sequen-tial model for discourse parsing on multi-party dia-logues.
in the thirty-third aaai conference on ar-tiï¬cial intelligence, aaai 2019, the thirty-first in-novative applications of artiï¬cial intelligence con-ference, iaai 2019, the ninth aaai symposiumon educational advances in artiï¬cial intelligence,eaai 2019, honolulu, hawaii, usa, january 27 -february 1, 2019, pages 7007â€“7014.
aaai press..hui su, xiaoyu shen, rongzhi zhang, fei sun, peng-improv-wei hu, cheng niu, and jie zhou.
2019.ing multi-turn dialogue modelling with utterancerewriter.
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 22â€“31.
association forcomputational linguistics..shikhar.
sankar.
vashishth,.
dasgupta,shibswayambhu nath ray, and partha p. talukdar.
2018. dating documents using graph convolutionin proceedings ofthe 56th annualnetworks.
meeting ofthe association for computationallinguistics, acl 2018, melbourne, australia, july15-20, 2018, volume 1: long papers, pages 1605â€“1615. association for computational linguistics..shikhar vashishth, soumya sanyal, vikram nitin, andpartha p. talukdar.
2020. composition-based multi-in 8threlational graph convolutional networks.
international conference on learning representa-tions, iclr 2020, addis ababa, ethiopia, april 26-30, 2020. openreview.net..longyue wang, zhaopeng tu, shuming shi, tongzhang, yvette graham, and qun liu.
2018. trans-lating pro-drop languages with reconstruction mod-in proceedings of the thirty-second aaaiels.
(aaai-18),conference on artiï¬cial intelligence,the 30th innovative applications of artiï¬cial intel-ligence (iaai-18), and the 8th aaai symposiumon educational advances in artiï¬cial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 4937â€“4945.
aaai press..longyue wang, zhaopeng tu, xiaojun zhang, hangli, andy way, and qun liu.
2016. a novel ap-proach to dropped pronoun translation.
in naaclhlt 2016, the 2016 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, sandiego california, usa, june 12-17, 2016, pages983â€“993.
the association for computational lin-guistics..nianwen xue, qishen su, and sooyoung jeong.
2016.annotating the discourse and dialogue structure ofsms message conversations.
in proceedings of the10th linguistic annotation workshop held in con-junction with acl 2016, law@acl 2016, august11, 2016, berlin, germany.
the association forcomputer linguistics..hang yan, xipeng qiu, and xuanjing huang.
2020. agraph-based model for joint chinese word segmenta-tion and dependency parsing.
trans.
assoc.
comput.
linguistics, 8:78â€“92..jingxuan yang, jianzhuo tong, si li, sheng gao,jun guo, and nianwen xue.
2019. recoveringdropped pronouns in chinese conversations via mod-in proceedings of the 2019eling their referents.
conference of the north american chapter of theassociation for computational linguistics: human.
1761language technologies, naacl-hlt 2019, min-neapolis, mn, usa, june 2-7, 2019, volume 1 (longand short papers), pages 892â€“901.
association forcomputational linguistics..jingxuan yang, kerui xu, jun xu, si li, sheng gao,jun guo, ji-rong wen, and nianwen xue.
2020.transformer-gcrf: recovering chinese dropped pro-nouns with general conditional random ï¬elds.
inproceedings of the 2020 conference on empiricalmethods in natural language processing: findings,emnlp 2020, online event, 16-20 november 2020,pages 137â€“147.
association for computational lin-guistics..yaqin yang, yalin liu, and nianwen xue.
2015. re-covering dropped pronouns from chinese text mes-in proceedings of the 53rd annual meet-sages.
ing of the association for computational linguisticsand the 7th international joint conference on natu-ral language processing of the asian federation ofnatural language processing, acl 2015, july 26-31, 2015, beijing, china, volume 2: short papers,pages 309â€“313.
the association for computer lin-guistics..qingyu yin, yu zhang, weinan zhang, and ting liu.
2017. chinese zero pronoun resolution with deepmemory network.
in proceedings of the 2017 con-ference on empirical methods in natural languageprocessing, emnlp 2017, copenhagen, denmark,september 9-11, 2017, pages 1309â€“1318.
associa-tion for computational linguistics..qingyu yin, yu zhang, weinan zhang, ting liu, andwilliam yang wang.
2018. deep reinforcementlearning for chinese zero pronoun resolution.
inproceedings of the 56th annual meeting of the as-sociation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume 1:long papers, pages 569â€“578.
association for com-putational linguistics..weinan zhang, ting liu, qingyu yin, and yu zhang.
2019. neural recovery machine for chinese droppedpronoun.
frontiers comput.
sci., 13(5):1023â€“1033..1762relation.
description.
different participant.
agreementunderstandingdirectivequestionanswerfeedback.
a participant provides a response to a previous request or suggestiona participant indicates understanding of a previous utterancea participant asks another one to do somethinga general request for another participanta participant provides the information requested by another participanta participant responds to another speakerâ€™s utterancesame participant.
expansioncontingency.
a participant provides an elaboration of a previous utterancea participant continues to say something else.
table 3: explanation of discourse relations..training#sentences #dps28,0525,0905,097.spdpr 35,9336,7347,970.tczhidao.
test#sentences #dps3,539774786.
4,3461,1221,406.table 4: statistics of training and test sets on threeconversational datasets..a discourse relations.
the discourse relation describes a participant mayspeak a utterance to agree with, respond to, or indi-cate understanding of another utterance in the con-versational context.
according to (xue et al., 2016), each utterance is assumed only related to one pre-vious utterance.
all relations are summarized as6 types between same-participant utterance pairs,and 2 types between different-participant utterancepairs, as summarized in table 3..b statistics of dpr datasets.
the statistics of three dropped pronoun recoverybenchmarks (i.e., spdpr, tc section of ontonotesand baiduzhidao) are shown in table 4..1763