a uniﬁed generative framework for various ner subtasks.
hang yan1, tao gui2, junqi dai1, qipeng guo1, zheng zhang3, xipeng qiu1,4∗1shanghai key laboratory of intelligent information processing, fudan university1school of computer science, fudan university2institute of modern languages and linguistics, fudan university3new york university4pazhou lab, guangzhou, china{hyan19,tgui16,jqdai19,qpguo16,xpqiu}@fudan.edu.cnzz@nyu.edu.
abstract.
named entity recognition (ner) is the taskof identifying spans that represent entities insentences.
whether the entity spans are nestedor discontinuous, the ner task can be cate-gorized into the ﬂat ner, nested ner, anddiscontinuous ner subtasks.
these subtaskshave been mainly solved by the token-levelsequence labelling or span-level classiﬁcation.
however, these solutions can hardly tackle thethree kinds of ner subtasks concurrently.
tothat end, we propose to formulate the nersubtasks as an entity span sequence genera-tion task, which can be solved by a uniﬁedsequence-to-sequence (seq2seq) framework.
based on our uniﬁed framework, we can lever-age the pre-trained seq2seq model to solveall three kinds of ner subtasks without thespecial design of the tagging schema or waysto enumerate spans.
we exploit three typesof entity representations to linearize entitiesinto a sequence.
our proposed framework iseasy-to-implement and achieves state-of-the-art (sota) or near sota performance on eightenglish ner datasets, including two ﬂat nerdatasets, three nested ner datasets, and threediscontinuous ner datasets 1..1.introduction.
named entity recognition (ner) has been a funda-mental task of natural language processing (nlp),and three kinds of ner subtasks have been recog-nized in previous work (sang and meulder, 2003;pradhan et al., 2013a; doddington et al., 2004; kimet al., 2003; karimi et al., 2015), including ﬂatner, nested ner, and discontinuous ner.
asshown in figure 1, the nested ner contains over-lapping entities, and the entity in the discontinuousner may contain several nonadjacent spans..∗corresponding author.
1code is available at https://github.com/yhcc/.
bartner..figure 1: examples of three kinds of ner subtasks.
(a) - (c) illustrate ﬂat ner, nested ner, discontinuousner, and their corresponding mainstream solutions re-spectively.
(d) our proposed generative solution tosolve all ner subtasks in a uniﬁed way..the sequence labelling formulation, which willassign a tag to each token in the sentence, hasbeen widely used in the ﬂat ner ﬁeld (mccal-lum and li, 2003; collobert et al., 2011; huanget al., 2015; chiu and nichols, 2016; lample et al.,2016; strakov´a et al., 2019; yan et al., 2019; liinspired by sequence labelling’set al., 2020a).
success in the ﬂat ner subtask, metke-jimenezand karimi (2016); muis and lu (2017) tried toformulate the nested and discontinuous ner intothe sequence labelling problem.
for the nested anddiscontinuous ner subtasks, instead of assigninglabels to each token directly, xu et al.
(2017); wangand lu (2019); yu et al.
(2020); li et al.
(2020b)tried to enumerate all possible spans and conductthe span-level classiﬁcation.
another way to efﬁ-ciently represent spans is to use the hypergraph (lu.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5808–5822august1–6,2021.©2021associationforcomputationallinguistics5808(d)  a unified generative solution for all ner tasksusthelincolnmemorialthelincolnmemorialthe lincolnlincoln memorialthe lincoln memorials2:s1: barackobamawasborninthepersonlocationpersonlocationhavemuchmusclepainandfatiguedisorderdisorders3:b-peri-peroooob-loc(a) sequence labelling for flat ner(b) span-based classification for nested nerbarack obama <person> us  <location>s1:the lincoln memorial  <location>  lincoln  <person>s2:muscle pain < disorder >  muscle fatigue  <disorder>s2:(c) transition-based method for discontinuous nerout           outshift           shiftleft-reduce  complete …actions: and roth, 2015; katiyar and cardie, 2018; wangand lu, 2018; muis and lu, 2016)..although the sequence labelling formulation hasdramatically advanced the ner task, it has to de-sign different tagging schemas to ﬁt various nersubtasks.
one tagging schema can hardly ﬁt forall three ner subtasks2 (ratinov and roth, 2009;metke-jimenez and karimi, 2016; strakov´a et al.,2019; dai et al., 2020).
while the span-based mod-els need to enumerate all possible spans, which isquadratic to the length of the sentence and is almostimpossible to enumerate in the discontinuous nerscenario (yu et al., 2020).
therefore, span-basedmethods usually will set a maximum span length(xu et al., 2017; luan et al., 2019; wang and lu,2018).
although hypergraphs can efﬁciently rep-resent all spans (lu and roth, 2015; katiyar andcardie, 2018; muis and lu, 2016), it suffers fromthe spurious structure problem, and structural am-biguity issue during inference and the decoding isquite complicated (muis and lu, 2017).
becausethe problems lie in different formulations, no publi-cation has tested their model or framework in threener subtasks simultaneously to the best of ourknowledge..in this paper, we propose using a novel and sim-ple sequence-to-sequence (seq2seq) frameworkwith the pointer mechanism (vinyals et al., 2015)to generate the entity sequence directly.
on thesource side, the model inputs the sentence, andon the target side, the model generates the entitypointer index sequence.
since ﬂat, continuous anddiscontinuous entities can all be represented as en-tity pointer index sequences, this formulation cantackle all the three kinds of ner subtasks in a uni-ﬁed way.
besides, this formulation can even solvethe crossing structure entity3 and multi-type en-tity4.
by converting the ner task into a seq2seqgeneration task, we can smoothly use the seq2seqpre-training model bart (lewis et al., 2020) toenhance our model.
to better utilize the pre-trainedbart, we propose three kinds of entity representa-tions to linearize entities into entity pointer indexsequences..our contribution can be summarized as follows:.
2attempts made for discontinuous constituent parsing maytackle three ner subtasks in one tagging schema (vilares andg´omez-rodr´ıguez, 2020)..3namely, for span abcd, both abc and bcd are entities..although this is rare, it exists (dai et al., 2020)..4an entity can have multiple entity types, as proteins canbe annotated as drug/compound in the eppi corpus (alexet al., 2007)..• we propose a novel and simple generativesolution to solve the ﬂat ner, nested ner,and discontinuous ner subtasks in a uniﬁedframework, in which ner subtasks are for-mulated as an entity span sequence generationproblem..• we incorporate the pre-trained seq2seqmodel bart into our framework and exploitthree kinds of entity representations to lin-earize entities into sequences.
the resultscan shed some light on further explorationof bart into the entity sequence generation.
• the proposed framework not only avoids thesophisticated design of tagging schema orspan enumeration but also achieves sotaor near sota performance on eight popu-lar datasets, including two ﬂat ner datasets,three nested ner datasets, and three discon-tinuous ner datasets..2 background.
2.1 ner subtasks.
the term “named entity” was coined in the sixthmessage understanding conference (muc-6) (gr-ishman and sundheim, 1996).
after that, the re-lease of conll-2003 ner dataset has greatly ad-vanced the ﬂat ner subtask (sang and meulder,2003).
kim et al.
(2003) found that in the ﬁeld ofmolecular biology domain, some entities could benested.
karimi et al.
(2015) provided a corpus thatcontained medical forum posts on patient-reportedadverse drug events (ades), some entities recog-nized in this corpus may be discontinuous.
despitethe difference between the three kinds of ner sub-tasks, the methods adopted by previous publica-tions can be roughly divided into three types..token-level classiﬁcation the ﬁrst line of workviews the ner task as a token-level classiﬁcationtask, which assigns to each token a tag that usuallycomes from the cartesian product between entitylabels and the tag scheme, such as bio and bilou(ratinov and roth, 2009; collobert et al., 2011;huang et al., 2015; chiu and nichols, 2016; lam-ple et al., 2016; alex et al., 2007; strakov´a et al.,2019; metke-jimenez and karimi, 2016; muis andlu, 2017; dai et al., 2020), then conditional ran-dom fields (crf) (lafferty et al., 2001) or tagsequence generation methods can be used for de-coding.
though the work of (strakov´a et al., 2019;wang et al., 2019; zhang et al., 2018; chen andmoschitti, 2018) are much like our method, they all.
5809tried to predict a tagging sequence.
therefore, theystill need to design tagging schemas for differentner subtasks..span-level classiﬁcation when applying the se-quence labelling method to the nested ner anddiscontinous ner subtasks, the tagging will becomplex (strakov´a et al., 2019; metke-jimenez andkarimi, 2016) or multi-level (ju et al., 2018; fisherand vlachos, 2019; shibuya and hovy, 2020).
therefore, the second line of work directly con-ducted the span-level classiﬁcation.
the main dif-ference between publications in this line of work ishow to get the spans.
finkel and manning (2009)regarded the parsing nodes as a span.
xu et al.
(2017); luan et al.
(2019); yamada et al.
(2020); liet al.
(2020b); yu et al.
(2020); wang et al.
(2020a)tried to enumerate all spans.
following lu androth (2015), hypergraph methods which can effec-tively represent exponentially many possible nestedmentions in a sentence have been extensively stud-ied in the ner tasks (katiyar and cardie, 2018;wang and lu, 2018; muis and lu, 2016)..combined token-level and span-level classiﬁ-cation to avoid enumerating all possible spansand incorporate the entity boundary informationinto the model, wang and lu (2019); zheng et al.
(2019); lin et al.
(2019); wang et al.
(2020b); luoand zhao (2020) proposed combining the token-level classiﬁcation and span-level classiﬁcation..2.2 sequence-to-sequence models.
the seq2seq framework has been long studied andadopted in nlp (sutskever et al., 2014; cho et al.,2014; luong et al., 2015; vaswani et al., 2017;vinyals et al., 2015).
gillick et al.
(2016) pro-posed a seq2seq model to predict the entity’s start,span length and label for the ner task.
recently,the amazing performance gain achieved by ptms(pre-trained models) (qiu et al., 2020; peters et al.,2018; devlin et al., 2019; dai et al., 2021; yanet al., 2020) has attracted several attempts to pre-train a seq2seq model (song et al., 2019; lewiset al., 2020; raffel et al., 2020).
we mainly focuson the newly proposed bart (lewis et al., 2020)model because it can achieve better performancethan mass (song et al., 2019).
and the sentence-piece tokenization used in t5 (raffel et al., 2020)will cause different tokenizations for the same to-ken, making it hard to generate pointer indexes toconduct the entity extraction..bart is formed by several transformer encoder.
and decoder layers, like the transformer model usedin the machine translation (vaswani et al., 2017).
bart’s pre-training task is to recover corruptedtext into the original text.
bart uses the encoderto input the corrupted sentence and the decoderto recover the original sentence.
bart has baseand large versions.
the base version has 6 encoderlayers and 6 decoder layers, while the large versionhas 12. therefore, the number of parameters issimilar to its equivalently sized bert 5..3 proposed method.
in this part, we ﬁrst introduce the task formulation,then we describe how we use the seq2seq modelwith the pointer mechanism to generate the entityindex sequences.
after that, we present the detailedformulation of our model with bart..3.1 ner task.
the three kinds of ner tasks can all be formulatedas follows, given an input sentence of n tokensx = [x1, x2, ..., xn], the target sequence is y =[s11, e11, ..., s1j, e1j, t1, ..., si1, ei1, ..., sik, eik, ti],where s, e are the start and end index of a span,since an entity may contain one (for ﬂat andnested ner) or more than one (for discontinu-ous ner) spans, each entity is represented as[si1, ei1, ..., sij, eij, ti], where ti is the entity tagindex.
we use g = [g1, ..., gl] to denote the entitytag tokens (such as “person”, “location”, etc.
),where l is the number of entity tags.
we maketi ∈ (n, n + l], the n shift is to make sure ti is notconfusing with pointer indexes (pointer indexeswill be in range [1, n])..3.2 seq2seq for uniﬁed decoding.
since we formulate the ner task in a generativeway, we can view the ner task as the followingequation:.
p (y |x) =.
p (yt|x, y<t).
(1).
m(cid:89).
t=1.
where y0 is the special “start of sentence” controltoken..we use the seq2seq framework with the pointermechanism to tackle this task.
therefore, ourmodel consists of two components:.
5because of the cross-attention between encoder and de-coder, the number of parameters of bart is about 10% largerthan its equivalently sized of bert (lewis et al., 2020)..5810figure 2: model structure used in our method.
the encoder encodes input sentences, and the decoder uses thepointer mechanism to generate indexes autoregressively.
“<s>” and “</s>” are the predeﬁned start-of-sentenceand end-of-sentence tokens in bart.
in the output sequence, “7” means the entity tag “<dis>”, and other numbersindicate the pointer index (in range [1, 6])..(1) encoder encodes the input sentence x into.
achieve the index probability distribution pt.
vectors he, which formulates as follows:.
he = encoder(x).
(2).
where he ∈ rn×d, and d is the hidden dimension.
(2) decoder is to get the index probability distri-bution for each step pt = p (yt|x, y<t).
however,since y<t contains the pointer and tag index, it can-not be directly inputted to the decoder.
we use theindex2token conversion to convert indexes intotokens.
ˆyt =.
®xyt,gyt−n,.
if yt ≤ n,if yt > n.(3).
after converting each yt this way, we can get thet ∈ rd with ˆy<t = [ˆy1, ..., ˆyt−1].
last hidden state hdas follows.
t = decoder(he; ˆy<t)hd.
(4).
then, we can use the following equations to.
ee = tokenembed(x)ˆhe = mlp(he)¯he = α ∗ ˆhe + (1 − α) ∗ eegd = tokenembed(g)pt = softmax([ ¯he ⊗ hd.
t ; gd ⊗ hd.
t ]).
(5).
(6).
(7).
(8).
(9).
where tokenembed is the embeddings shared be-tween the encoder and decoder; ee, ˆhe, ¯he ∈rn×d; α ∈ r is a hyper-parameter; gd ∈ rl×d;[ · ; · ] means concatenation in the ﬁrst dimension;⊗ means the dot product..during the training phase, we use the negativelog-likelihood loss and the teacher forcing method.
during the inference, we use an autoregressivemanner to generate the target sequence.
we usethe decoding algorithm presented in algorithm 1to convert the index sequence into entity spans..3.3 detailed entity representation with.
bart.
since our model is a seq2seq model, it is naturalto utilize the pre-training seq2seq model bart toenhance our model.
we present a visualization of.
5811bartencodermusclepainandfatiguehave×𝛼×(1−𝛼)<s>2position embedding:muscle3pain7<dis>2muscle⨂⨂entity tag embeddingbartdecoderscorescoreprob.final prediction prob.input:token embedding:⨂dot-productshared embeddingindex2token conversiondecoder  input:output:mlp1234560<s></s>+++++++01234+++++<dis>tgtgtgtgtg512345</s><dis>+input: <s>  have  muscle  pain  and  fatigue  </s>output: 2  3  7  2  5  6pointer distributiontag distributiontgtarget generator67musclepainandfatiguehavealgorithm 1 decoding algorithm to convert theentity representation sequence into entity spansinput: target sequence y = [y1, ..., ym] and yi ∈.
[1, n + |g|].
entity word.
if this entity includes multiple discon-tinuous spans of words, each span is represented inthe same way..bpe the position indexes of all bpes of the.
output: entity spans e = {(e1, t1), ..., (ei, ti)}.
entity words..1: e = {}, e = [], i = 12: while i <= m doyi = y [i]3:if yi > n then.
4:.
if len(e) > 0 then.
e.add((e, gyi−n)).
end ife = [].
else.
e.append(yi).
5:.
6:.
7:.
8:.
9:.
10:.
11:.
end ifi = i + 1.
12:13: end while14: return e.figure 3: the bottom three lines are examplesof the three kinds of entity representations to de-termine the entity in the sentence unambiguously.
words in the boxes are entity words, words withinthe same color box belong to the same entity,and their corresponding entity representation is alsothere are three entities,with the same color.
(x1, x3, p er),(x4, f ac),where loc, p er, f ac are their corresponding en-tity tags.
the underlined position index means this isthe starting bpe of a word..(x1, x2, x3, x4, loc),.
our model based on bart in figure 2. however,bart’s adoption is non-trivial because the byte-pair-encoding (bpe) tokenization used in bartmight tokenize one token into several bpes.
toexploit how to use bart efﬁciently, we proposethree kinds of pointer-based entity representationsto locate entities in the original sentence unam-biguously.
the three entity representations are asfollows:.
span the position index of the ﬁrst bpe of thestarting entity word and the last bpe of the ending.
word only the position index of the ﬁrst bpe.
of each entity word is used..for all cases, we will append the entity tag tothe entity representation.
an example of the entityrepresentations is presented in figure 3. if a worddoes not belong to any entity, it will not appear inthe target sequence.
if a whole sentence has noentity, the prediction should be an empty sequence(only contains the “start of sentence” (<s>) tokenand the “end of sentence” (</s>) token )..4 experiment.
4.1 datasets.
to show that our proposed method can be used invarious ner subtasks, we conducted experimentson eight datasets..flat ner datasets we adopt the conll-2003(sang and meulder, 2003) and the ontonotesdataset 6 (pradhan et al., 2013b).
for conll-2003,we follow lample et al.
(2016); yu et al.
(2020) totrain our model on the concatenation of the trainand development sets.
for the ontonotes dataset,we use the same train, development, test splits aspradhan et al.
(2012); yu et al.
(2020), and the newtestaments portion were excluded since there is noentity in this portion (chiu and nichols, 2016)..nested ner datasets we conduct experimentson ace 20047 (doddington et al., 2004), ace20058 (walker and consortium, 2005), geniacorpus (kim et al., 2003).
for ace2004 andace2005, we use the same data split as lu androth (2015); muis and lu (2017); yu et al.
(2020),the ratio between train, development and test set is8:1:1. for genia, we follow wang et al.
(2020b);shibuya and hovy (2020) to use ﬁve types of enti-ties and split the train/dev/test as 8.1:0.9:1.0..6https://catalog.ldc.upenn.edu/.
7https://catalog.ldc.upenn.edu/.
8https://catalog.ldc.upenn.edu/.
ldc2013t19.
ldc2005t09.
ldc2006t06.
9in the reported experiments, they included the documentcontext.
we rerun their code with only the sentence context.
the lack of document context might cause performancedegradation is also conﬁrmed by the author himself inhttps://github.com/juntaoy/biaffine-ner/issues/8#issuecomment-650813813..5812sentence:after bpe:b111b1211b13111b2111b22111b31111b4111b42111b51 position index:01    2     34    567    8bpe:word:span:[0,1,2,5,per] [0,5,per][0,2,5,5,per]perlocorg[0,3,5,6,loc][0,1,2,3,4,5,6,7,loc][6,7, org][6, org][0,7,loc][6,7, org]x5x4x3x2x1three entity representations:models.
conll2003frp.ontonotesr.p.f.clark et al.
(2018)[glove300d]peters et al.
(2018)[elmo]akbik et al.
(2019)[flair]strakov´a et al.
(2019)[bert-large]yamada et al.
(2020)[roberta-large]li et al.
(2020b)[bert-large]†yu et al.
(2020)[bert-large]‡.
-----.
-----.
92.692.2293.1893.0792.40.
-----.
-----.
-----.
92.47 93.27 92.87 91.34 88.39 89.8492.85 92.15 92.5 89.92 89.74 89.83.ours(span)[bart-large]ours(bpe)[bart-large]ours(word)[bart-large].
92.31 93.45 92.88 88.94 90.33 89.6392.60 93.22 92.96 90.00 89.52 89.7692.61 93.87 93.24 89.99 90.77 90.38.table 1: results for the ﬂat ner datasets.
“†” indicates we rerun their code.
“‡” means our reproduction with onlythe sentence-level context 9..models.
ace2004r.p.f.ace2005r.p.f.geniar.p.f.luan et al.
(2019)[elmo]strakov´a et al.
(2019)[bert-large].
76.276.44shibuya and hovy (2020)[bert-large](cid:63) 85.23 84.72 84.97 83.30 84.69 83.99 77.46 76.65 77.0585.83 85.77 85.80 85.01 84.13 84.57 81.25 76.36 78.7285.42 85.92 85.67 84.50 84.72 84.61 79.43 78.32 78.8786.08 86.48 86.28 83.95 85.39 84.66 79.45 78.94 79.19.li et al.
(2020b)[bert-large]†yu et al.
(2020)[bert-large] ‡wang et al.
(2020a)[bert-large](cid:63).
82.983.42.
84.784.33.
--.
--.
--.
--.
--.
--.
ours(span)[bart-large]ours(bpe)[bart-large]ours(word)[bart-large].
84.81 83.64 84.22 81.41 83.24 82.31 78.87 79.6 79.2386.69 83.83 85.24 82.08 83.44 82.75 78.15 79.06 78.6087.27 86.41 86.84 83.16 86.38 84.74 78.57 79.3 78.93.table 2: results for nested ner datasets,“†” means our rerun of their code.
“‡” means our reproduction with onlysentence-level context9.
“(cid:63)” for a fair comparison, we only present results with the bert-large model..discontinuous ner datasets we follow daiet al.
(2020) to use cadec (karimi et al., 2015),share13 (pradhan et al., 2013a) and share14(mowery et al., 2014) corpus.
since only the ad-verse drug events (ades) entities include discon-tinuous annotation, only these entities were consid-ered (dai et al., 2020; metke-jimenez and karimi,2016; tang et al., 2018)..4.2 experiment setup.
we use the bart-large model, whose encoderand decoder each has 12 layers for all experiments,making it the same number of transformer layers asthe bert-large and roberta-large model.
wedid not use any other embeddings, and the bartmodel is ﬁne-tuned during the optimization.
weput more detailed experimental settings in the sup-plementary material.
we report the span-level f1..5 results.
5.1 results on flat ner.
results are shown in table 1. we do not com-pare with yamada et al.
(2020) since they addedentity information during the pre-training process.
clark et al.
(2018); peters et al.
(2018); akbik et al.
(2019); strakov´a et al.
(2019) assigned a label toeach token, and li et al.
(2020b); yu et al.
(2020)are based on span-level classiﬁcations, while ourmethod is based on the entity sequence generation.
and for both datasets, our method achieves betterperformance.
we will discuss the performance dif-ference between our three entity representations insection 5.4..5.2 results on nested ner.
table 2 presents the results for the three nestedner datasets, and our proposed bart-based gen-.
5813model.
cadecr.p.f.share13r.p.f.share14r.p.f.-metke-jimenez and karimi (2016) 64.4 56.5 60.267.8 64.9 66.3-68.9 69.0 69.0 80.5 75.0 77.7 78.1 81.2 79.6.tang et al.
(2018)dai et al.
(2020)[elmo].
--.
--.
--.
--.
--.
ours(span)[bart-large]ours(bpe)[bart-large]ours(word)[bart-large].
71.55 68.59 70.04 80.42 78.15 79.27 76.85 83.59 80.0869.45 70.51 69.97 82.07 76.45 79.16 75.88 84.37 79.9070.08 71.21 70.64 82.09 77.42 79.69 77.2 83.75 80.34.table 3: results for discontinuous ner datasets..entity.
flat ner.
nested ner.
discontinuous ner.
representation conll2003 ontonotes ace2004 ace2005 genia cadec share13 share14.
spanbpeword.
3.0/3.03.55/3.02.44/2.0.
3.0/3.03.39/3.02.86/2.0.
3.0/3.04.15/3.03.53/2.0.
3.0/3.0 3.17/3.0 3.15/3.03.0/3.03.84/3.0 5.21/5.0 4.08/4.0 3.92/3.03.26/2.0 3.09/3.0 2.72/3.0 2.63/3.0.
3.2/3.04.34/4.03.74/3.0.
table 4: the average (before /) and median entity length (including the entity label) for each entity representationsin the respective testing set..erative models are comparable to the token-levelclassication (strakov´a et al., 2019; shibuya andhovy, 2020) and span-level classiﬁcation (luanet al., 2019; li et al., 2020b; wang et al., 2020a)models..5.3 results on discontinuous ner.
results in table 3 show the comparison betweenour model and other models in three discontinuousner datasets.
although dai et al.
(2020) tried toutilize bert to enhance the model performance,they found that elmo worked better.
in all threedatasets, our model achieves better performance..5.4 comparison between different entity.
representations.
in this part, we discuss the performance differ-ence between the three entity representations.
the“word” entity representation achieves better perfor-mance almost in all datasets.
and the comparisonbetween the “span” and “bpe” representations ismore involved.
to investigate the reason behindthese results, we calculate the average and medianlength of entities when using different entity rep-resentations, and the results are presented in table4. it is clear that for a generative framework, theshorter the entity representation the better perfor-mance it should achieve.
therefore, as shown intable 4, the “word” representation with smaller.
average entity length in conll2003, ontonotes,cadec, share13 achieves better performancein these datasets.
however, although the aver-age entity length of the “bpe” representation islonger than the “span” representation, it achievesbetter performance in conll2003, ontonotes,ace2004, ace2005, this is because the “bpe”representation is more similar to the pre-trainingtask, namely, predicting continuous bpes.
andwe believe this task similarity is also the reasonwhy the “word” representation (most of the wordswill be tokenized into a single bpe, making the“word” representation still continuous.)
achievesbetter performance than the “span” representationin ace2004, ace2005, and share14, althoughthe former has longer entity length..a clear outlier is the genia dataset, where the“span” representation achieves better performancethan the other two.
we presume this is becausein this dataset, a word will be tokenized into alonger bpe sequence (this can be inferred from thelarge entity length gap between the “word” and“bpe” representation.)
so that the “word” repre-sentation will also be dissimilar to the pre-trainingtasks.
for example, the protein “lipoxygenase iso-forms” will be tokenized into the sequence “[‘ ˙glip’,‘oxy’, ‘gen’, ‘ase’, ‘ ˙giso’, ‘forms’]”, which makesthe target sequence of the “word” representation be“[‘ ˙glip’, ‘ ˙giso’]”, resulting a discontiguous bpe.
5814nested nererrors conll2003 ontonotes ace2004 ace2005 genia cadec share13 share14.
discontinuous ner.
flat ner.
e1e2e3.
0.05%.
0.04%.
0.05%.
0.02%.
0.03%.
0.02%.
0.23%.
0.13%.
0.30%.
0.06% 0.0% 0.31%.
0.0%.
0.22% 0.11% 1.02% 0.18%.
0.26% 0.06% 0.0%.
0.08%.
0.01%.
0.16%.
0.02%.
table 5: different invalid prediction probability for the “word” entity representation.
e1 means the predictedindexes contain index which is not the start index of a word, e2 means the predicted indexes within an entity arenot increasing, e3 means duplicated entity prediction..ontonotes.
genia.
share14.
llacer.93.
92.
91.
90.llacer.80.
78.
76.
74.
72.llacer.92.
90.
88.
86.
84.
82.
1(4.6) 2(2.7) 3(1.7).
4(1).
5(0.6) 6+(0.7).
entity position(# of entities).
1(1.6) 2(1.3) 3(1.0) 4(0.7) 5(0.4) 6+(0.5)entity position(# of entities).
1(4.9) 2(1.7) 3(0.7) 4(0.3) 5(0.1) 6+(0.2)entity position(# of entities).
figure 4: the recall of entities in different entity sequence positions, the number of entities in that position is thenumber in the bracket (the unit is 1000)..sequence.
therefore, the shorter “span” represen-tation achieves better performance in this dataset..6 analysis.
6.1 recall of discontinuous entities.
since only about 10% of entities in the discontin-uous ner datasets are discontinuous, only evalu-ating the whole dataset may not show our modelcan recognize the discontinuous entities.
therefore,like in dai et al.
(2020); muis and lu (2016) we re-port our model’s performance on the discontinuousentities in table 6. as shown in table 6, our modelcan predict the discontinuous named entities andachieve better performance..model.
share13frp.share14frp.dai et al.
(2020) 78.5 39.4 52.5 56.1 43.8 49.257.5 52.8 55.0 49.6 56.2 52.7.ours(word).
table 6: performance on the discontinuous entities ofthe tesing dataset of share13 and share14..table shows that the bart model can learn theprediction representations quite well since, in mostcases, the invalid prediction is less than 1%.
weexclude all these invalid predictions during evalua-tion..6.3 entity order vs. entity recall.
its appearance order in the sentence determinesthe entity order, and we want to study whether theentity that appears later in the target sequence willhave worse recall than entities that appear early.
the results are provided in figure 4. the latter theentity appears, the larger probability that it can berecalled for the ﬂat ner and discontinuous ner.
while for the nested ner, the recall curve is quiteinvolved.
we assume this phenomenon is because,for the ﬂat ner and discontinuous ner (more than91.1% of entities are continuous) datasets, differententities have less dependence on each other.
whilein the nested ner dataset, entities in the latterposition may be the outermost entity that containsthe former entities.
the wrong prediction of formerentities may negatively inﬂuence the later entities..6.2.invalid prediction.
7 conclusion.
in this part, we mainly focus on the analysis of the“word” representation since it generally achievesbetter performance.
we do not restrict the outputdistribution; therefore, the entity prediction maycontain invalid predictions as show in table 5, this.
in this paper, we formulate ner subtasks as an en-tity span sequence generation problem, so that wecan use a uniﬁed seq2seq model with the pointermechanism to tackle ﬂat, nested, and discontinu-ous ner subtasks.
the seq2seq formulation en-.
5815ables us to smoothly incorporate the pre-trainingseq2seq model bart to enhance the performance.
to better utilize bart, we test three types of en-tity representation methods to linearize the entityspan into sequences.
results show that the entityrepresentation with a shorter length and more sim-ilar to continuous bpe sequences achieves betterperformance.
our proposed method achieves sotaor near sota performance for eight different nerdatasets, proving its generality to various ner sub-tasks..acknowledgements.
we would like to thank the anonymous reviewersfor their insightful comments.
the discussion withcolleagues in aws shanghai ai lab was quitefruitful.
we also thank the developers of fastnlp10and ﬁtlog11.
we thank juntao yu for helpful discus-sion about dataset processing.
this work was sup-ported by the national key research and develop-ment program of china (no.
2020aaa0106700)and national natural science foundation of china(no.
62022027)..ethical considerations.
for the consideration of ethical concerns, we wouldmake detailed description as following:.
(1) all of the experiments are conducted on ex-isting datasets, which are derived from public sci-entiﬁc papers..(2) we describe the characteristics of the datasetsin a speciﬁc section.
our analysis is consistent withthe results..(3) our work does not contain identity character-.
istics.
it does not harm anyone..(4) our experiments do not need a lots of com-.
puter resources compared to pre-trained models..references.
alan akbik, tanja bergmann, and roland vollgraf.
2019. pooled contextualized embeddings for namedentity recognition.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, naacl-hlt 2019, minneapo-lis, mn, usa, june 2-7, 2019, volume 1 (long andshort papers), pages 724–728.
association for com-putational linguistics..10https://github.com/fastnlp/fastnlp.
fastnlp is a natural language processing python package.
11https://github.com/fastnlp/fitlog..fit-.
log is an experiment tracking package..beatrice alex, barry haddow, and claire grover.
2007.recognising nested named entities in biomedicaltranslational, and clinicaltext.
language processing, bionlp@acl 2007, prague,czech republic, june 29, 2007, pages 65–72.
asso-ciation for computational linguistics..in biological,.
lingzhen chen and alessandro moschitti.
2018.learning to progressively recognize new named en-in pro-tities with sequence to sequence models.
ceedings of the 27th international conference oncomputational linguistics, coling 2018, santafe, new mexico, usa, august 20-26, 2018, pages2181–2191.
association for computational linguis-tics..jason p. c. chiu and eric nichols.
2016. named en-tity recognition with bidirectional lstm-cnns.
trans.
assoc.
comput.
linguistics, 4:357–370..kyunghyun cho, bart van merrienboer, c¸ aglarg¨ulc¸ehre, dzmitry bahdanau, fethi bougares, hol-ger schwenk, and yoshua bengio.
2014. learningphrase representations using rnn encoder-decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing, emnlp 2014, october25-29, 2014, doha, qatar, a meeting of sigdat, aspecial interest group of the acl, pages 1724–1734.
acl..kevin clark, minh-thang luong, christopher d. man-ning, and quoc v. le.
2018. semi-supervised se-quence modeling with cross-view training.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, brussels, bel-gium, october 31 - november 4, 2018, pages 1914–1925. association for computational linguistics..ronan collobert, jason weston, l´eon bottou, michaelkarlen, koray kavukcuoglu, and pavel p. kuksa.
2011. natural language processing (almost) fromscratch.
j. mach.
learn.
res., 12:2493–2537..junqi dai, hang yan, tianxiang sun, pengfei liu, andxipeng qiu.
2021. does syntax matter?
a strongbaseline for aspect-based sentiment analysis withroberta.
corr, abs/2104.04986..xiang dai, sarvnaz karimi, ben hachey, and c´ecileparis.
2020. an effective transition-based model fordiscontinuous ner.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 5860–5870.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,.
5816usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..george r. doddington, alexis mitchell, mark a. przy-bocki, lance a. ramshaw, stephanie m. strassel,and ralph m. weischedel.
2004. the automatic con-tent extraction (ace) program - tasks, data, and eval-uation.
in proceedings of the fourth internationalconference on language resources and evaluation,lrec 2004, may 26-28, 2004, lisbon, portugal.
eu-ropean language resources association..jenny rose finkel and christopher d. manning.
2009.nested named entity recognition.
in proceedings ofthe 2009 conference on empirical methods in natu-ral language processing, emnlp 2009, 6-7 august2009, singapore, a meeting of sigdat, a specialinterest group of the acl, pages 141–150.
acl..joseph fisher and andreas vlachos.
2019. merge andlabel: a novel neural network architecture for nestedin proceedings of the 57th conference ofner.
the association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 5840–5850.
associationfor computational linguistics..dan gillick, cliff brunk, oriol vinyals, and amarnagsubramanya.
2016. multilingual language process-in naacl hlt 2016, the 2016ing from bytes.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, san diego california, usa,june 12-17, 2016, pages 1296–1306.
the associa-tion for computational linguistics..ralph grishman and beth sundheim.
1996. mes-sage understanding conference- 6: a brief history.
in 16th international conference on computationallinguistics, proceedings of the conference, col-ing 1996, center for sprogteknologi, copenhagen,denmark, august 5-9, 1996, pages 466–471..jiatao gu, james bradbury, caiming xiong, vic-tor o. k. li, and richard socher.
2018. non-in 6thautoregressive neural machine translation.
international conference on learning representa-tions, iclr 2018, vancouver, bc, canada, april 30- may 3, 2018, conference track proceedings.
open-review.net..zhiheng huang, wei xu, and kai yu.
2015. bidi-rectional lstm-crf models for sequence tagging.
corr, abs/1508.01991..meizhi ju, makoto miwa, and sophia ananiadou.
2018. a neural layered model for nested named en-tity recognition.
in proceedings of the 2018 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2018, new or-leans, louisiana, usa, june 1-6, 2018, volume 1(long papers), pages 1446–1459.
association forcomputational linguistics..sarvnaz karimi, alejandro metke-jimenez, madonnakemp, and chen wang.
2015. cadec: a corpus ofadverse drug event annotations.
j. biomed.
infor-matics, 55:73–81..arzoo katiyar and claire cardie.
2018. nested namedin proceedings of theentity recognition revisited.
2018 conference of the north american chapterof the association for computational linguistics:human language technologies, naacl-hlt 2018,new orleans, louisiana, usa, june 1-6, 2018, vol-ume 1 (long papers), pages 861–871.
associationfor computational linguistics..jin-dong kim, tomoko ohta, yuka tateisi, andjun’ichi tsujii.
2003. genia corpus - a semanti-in pro-cally annotated corpus for bio-textmining.
ceedings of the eleventh international conferenceon intelligent systems for molecular biology, june29 - july 3, 2003, brisbane, australia, pages 180–182..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
in naacl hlt 2016, the 2016 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, san diego california, usa, june 12-17, 2016,pages 260–270.
the association for computationallinguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 7871–7880.
association for computationallinguistics..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020a.
flat: chinese ner using ﬂat-latticein proceedings of the 58th annualtransformer.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages6836–6842.
association for computational linguis-tics..xiaoya li, jingrong feng, yuxian meng, qinghonghan, fei wu, and jiwei li.
2020b.
a uniﬁed mrcin pro-framework for named entity recognition.
ceedings of the 58th annual meeting of the associ-ation for computational linguistics, acl 2020, on-line, july 5-10, 2020, pages 5849–5859.
associationfor computational linguistics..5817hongyu lin, yaojie lu, xianpei han, and le sun.
sequence-to-nuggets: nested entity men-2019.in pro-tion detection via anchor-region networks.
ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 5182–5192.
association for computa-tional linguistics..wei lu and dan roth.
2015. joint mention extractionand classiﬁcation with mention hypergraphs.
in pro-ceedings of the 2015 conference on empirical meth-ods in natural language processing, emnlp 2015,lisbon, portugal, september 17-21, 2015, pages857–867.
the association for computational lin-guistics..yi luan, dave wadden, luheng he, amy shah, mariostendorf, and hannaneh hajishirzi.
2019. a gen-eral framework for information extraction using dy-namic span graphs.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, naacl-hlt 2019, minneapo-lis, mn, usa, june 2-7, 2019, volume 1 (long andshort papers), pages 3036–3046.
association forcomputational linguistics..ying luo and hai zhao.
2020. bipartite ﬂat-graph net-in pro-work for nested named entity recognition.
ceedings of the 58th annual meeting of the associ-ation for computational linguistics, acl 2020, on-line, july 5-10, 2020, pages 6408–6418.
associationfor computational linguistics..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, emnlp 2015, lisbon, portu-gal, september 17-21, 2015, pages 1412–1421.
theassociation for computational linguistics..andrew mccallum and wei li.
2003. early results fornamed entity recognition with conditional randomﬁelds, feature induction and web-enhanced lexicons.
in proceedings of the seventh conference on natu-ral language learning, conll 2003, held in coop-eration with hlt-naacl 2003, edmonton, canada,may 31 - june 1, 2003, pages 188–191.
acl..alejandro metke-jimenez and sarvnaz karimi.
2016.concept identiﬁcation and normalisation for adversedrug event discovery in medical forums.
in proceed-ings of the first international workshop on biomed-ical data integration and discovery (bmdid 2016)co-located with the 15th international semanticweb conference (iswc 2016), kobe, japan, octo-ber 17, 2016, volume 1709 of ceur workshop pro-ceedings.
ceur-ws.org..danielle l. mowery, sumithra velupillai, brett r.south, lee m. christensen, david mart´ınez, liadhkelly, lorraine goeuriot, no´emie elhadad, sameer.
pradhan, guergana k. savova, and wendy w. chap-man.
2014. task 2: share/clef ehealth evaluationlab 2014. in working notes for clef 2014 confer-ence, shefﬁeld, uk, september 15-18, 2014, volume1180 of ceur workshop proceedings, pages 31–42.
ceur-ws.org..aldrian obaja muis and wei lu.
2016. learning to rec-ognize discontiguous entities.
in proceedings of the2016 conference on empirical methods in naturallanguage processing, emnlp 2016, austin, texas,usa, november 1-4, 2016, pages 75–84.
the asso-ciation for computational linguistics..aldrian obaja muis and wei lu.
2017. labeling gapsbetween words: recognizing overlapping mentionswith mention separators.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, emnlp 2017, copenhagen, den-mark, september 9-11, 2017, pages 2608–2618.
as-sociation for computational linguistics..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2018, new or-leans, louisiana, usa, june 1-6, 2018, volume 1(long papers), pages 2227–2237.
association forcomputational linguistics..sameer pradhan, no´emie elhadad, brett r. south,david mart´ınez, lee m. christensen, amy vogel,hanna suominen, wendy w. chapman, and guer-gana k. savova.
2013a.
task 1: share/clef ehealthin working notes for clefevaluation lab 2013.
2013 conference , valencia, spain, september 23-26, 2013, volume 1179 of ceur workshop proceed-ings.
ceur-ws.org..sameer pradhan, alessandro moschitti, nianwen xue,hwee tou ng, anders bj¨orkelund, olga uryupina,yuchen zhang, and zhi zhong.
2013b.
towardsin pro-robust linguistic analysis using ontonotes.
ceedings of the seventeenth conference on compu-tational natural language learning, conll 2013,soﬁa, bulgaria, august 8-9, 2013, pages 143–152.
acl..sameer pradhan, alessandro moschitti, nianwen xue,olga uryupina, and yuchen zhang.
2012. conll-2012 shared task: modeling multilingual unre-in joint con-stricted coreference in ontonotes.
ference on empirical methods in natural lan-guage processing and computational natural lan-guage learning - proceedings of the shared task:modeling multilingual unrestricted coreference inontonotes, emnlp-conll 2012, july 13, 2012,jeju island, korea, pages 1–40.
acl..xipeng qiu, tianxiang sun, yige xu, yunfan shao,ning dai, and xuanjing huang.
2020. pre-trainedmodels for natural language processing: a survey.
corr, abs/2003.08271..5818colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
j. mach.
learn.
res., 21:140:1–140:67..lev-arie ratinov and dan roth.
2009. design chal-lenges and misconceptions in named entity recog-in proceedings of the thirteenth confer-nition.
ence on computational natural language learning,conll 2009, boulder, colorado, usa, june 4-5,2009, pages 147–155.
acl..erik f. tjong kim sang and fien de meulder.
2003.introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on naturallanguage learning, conll 2003, held in cooper-ation with hlt-naacl 2003, edmonton, canada,may 31 - june 1, 2003, pages 142–147.
acl..takashi shibuya and eduard h. hovy.
2020. nestednamed entity recognition via second-best sequencelearning and decoding.
trans.
assoc.
comput.
lin-guistics, 8:605–620..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in pro-ceedings of the 36th international conference onmachine learning, icml 2019, 9-15 june 2019,long beach, california, usa, volume 97 of pro-ceedings of machine learning research, pages5926–5936.
pmlr..jana strakov´a, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through lineariza-in proceedings of the 57th conference oftion.
the association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 5326–5331.
associationfor computational linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..buzhou tang, jianglu hu, xiaolong wang, and qing-cai chen.
2018. recognizing continuous and dis-continuous adverse drug reaction mentions from so-cial media using lstm-crf.
wirel.
commun.
mob.
comput., 2018..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..david vilares and carlos g´omez-rodr´ıguez.
2020.discontinuous constituent parsing as sequence la-beling.
in proceedings of the 2020 conference on.
empirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 2771–2785.
association for computationallinguistics..oriol vinyals, meire fortunato, and navdeep jaitly.
in advances in neural2015. pointer networks.
information processing systems 28: annual con-ference on neural information processing systems2015, december 7-12, 2015, montreal, quebec,canada, pages 2692–2700..c. walker and linguistic data consortium.
2005. ace2005 multilingual training corpus.
ldc corpora.
linguistic data consortium..bailin wang and wei lu.
2018. neural segmental hy-pergraphs for overlapping mention recognition.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, brussels,belgium, october 31 - november 4, 2018, pages204–214.
association for computational linguis-tics..bailin wang and wei lu.
2019. combining spansinto entities: a neural two-stage approach for rec-in proceedings ofognizing discontiguous entities.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing,emnlp-ijcnlp 2019, hong kong, china, novem-ber 3-7, 2019, pages 6215–6223.
association forcomputational linguistics..jue wang, lidan shou, ke chen, and gang chen.
2020a.
pyramid: a layered model for nested namedin proceedings of the 58th an-entity recognition.
nual meeting of the association for computationallinguistics, acl 2020, online, july 5-10, 2020,pages 5918–5928.
association for computationallinguistics..yu wang, yun li, hanghang tong, and ziye zhu.
2020b.
hit: nested named entity recognition viain proceed-head-tail pair and token interaction.
ings of the 2020 conference on empirical methodsin natural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 6027–6036.
as-sociation for computational linguistics..yu wang, yun li, ziye zhu, bin xia, and zhengliu.
2019.sc-ner: a sequence-to-sequencemodel with sentence classiﬁcation for named entityrecognition.
in advances in knowledge discoveryand data mining - 23rd paciﬁc-asia conference,pakdd 2019, macau, china, april 14-17, 2019,proceedings, part i, volume 11439 of lecture notesin computer science, pages 198–209.
springer..mingbin xu, hui jiang, and sedtawut watcharawit-tayakul.
2017. a local detection approach for namedin pro-entity recognition and mention detection.
ceedings of the 55th annual meeting of the associa-tion for computational linguistics, acl 2017, van-couver, canada, july 30 - august 4, volume 1: long.
5819papers, pages 1237–1247.
association for computa-tional linguistics..ikuya yamada, akari asai, hiroyuki shindo, hideakitakeda, and yuji matsumoto.
2020. luke: deepcontextualized entity representations with entity-in proceedings of the 2020aware self-attention.
conference on empirical methods in natural lan-guage processing, emnlp 2020, online, novem-ber 16-20, 2020, pages 6442–6454.
association forcomputational linguistics..hang yan, bocao deng, xiaonan li, and xipeng qiu.
2019. tener: adapting transformer encoder fornamed entity recognition.
corr, abs/1911.04474..hang yan, xipeng qiu, and xuanjing huang.
2020. agraph-based model for joint chinese word segmenta-tion and dependency parsing.
trans.
assoc.
comput.
linguistics, 8:78–92..juntao yu, bernd bohnet, and massimo poesio.
2020.named entity recognition as dependency parsing.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 6470–6476.
associa-tion for computational linguistics..yuan zhang, hongshen chen, yihong zhao, qun liu,and dawei yin.
2018. learning tag dependenciesfor sequence tagging.
in proceedings of the twenty-seventh international joint conference on artiﬁcialintelligence, ijcai 2018, july 13-19, 2018, stock-holm, sweden, pages 4581–4587.
ijcai.org..changmeng zheng, yi cai, jingyun xu, ho-fung le-ung, and guandong xu.
2019. a boundary-awareneural model for nested named entity recognition.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 357–366.
association for computational linguistics..5820a supplemental material.
a.1 hyper-parameters.
the detailed hyper-parameter used in differentdatasets are listed in table 7. we use the slantedtriangular learning rate warmup.
all experimentsare conducted in the nvidia ge-force rtx-3090graphical card with 24g graphical memory..hyper.
value.
epochwarmup steplearning ratebatch sizebartαbeam size.
300.01[1e-5,2e-5,4e-5]16large0.5[1, 4].
table 7: hyper-parameters used for conll2003,ontonotes, ace2004, ace2005, genia, cadec,share13, share14..a.2 beam search.
since our framework is based on generation, wewant to study whether using beam search will in-crease the performance, results are depicted in fig-ure 5, it shows the beam search almost has no effecton the model performance.
the litte effect on thef1 value might be caused the the small searchingspace when generating..a.3 efﬁciency metrics.
in this section, we compare the memory footprint,training and inference time of our proposed modeland bert-based models.
the experiments areconducted on the ﬂat ner datasets, conll-2003(sang and meulder, 2003) and ontonotes (pradhanet al., 2012).
we use the bert-mlp and bert-crf models as our baseline models.
bert-mlpand bert-crf are sequence labelling based mod-els.
for an input sentence x = [x1, ..., xn], bothmodels use bert (devlin et al., 2019) to encodex as follows.
h = bert(x).
(10).
where h ∈ rn×d, d is the hidden state dimension.
then for the bert-mlp model, it decodes the.
tags as follows.
f = softmax(max(hwb + bb, 0)wa + ba).
ontonotes.
1.
2.
5.
6.
34beam size.
ace2004.
1.
2.
5.
6.
34beam size.
share13.
89.49.
89.48.
1f.89.47.
89.46.
89.45.
86.67.
86.66.
1f.86.65.
86.64.
86.63.
76.40.
76.20.
76.00.
75.80.
1f.75.60.
75.40.
75.20.
1.
2.
5.
6.
34beam size.
figure 5: the f1 change curve with the increment ofbeam size.
the beam size has limited effect on the f1score..where wa ∈ rd×|t | and |t | is the number of tags,ba ∈ r|t |, wb ∈ rd×d, bb ∈ rd, f ∈ rn×|t |is the tag probability distribution.
then we usethe negative log likelihood loss.
and during theinference, for each token, the tag index with thelargest probability is deemed as the prediction..for the bert-crf model, we use the condi-tional random ﬁelds (crf) (lafferty et al., 2001)to decode tags.
we assue the golden label sequenceis y = [y1, ..., yn], then we use the following equa-tions to get the probability of y.m = max(hwb + bb, 0)wa + bam = log softmax(m)(cid:80)n.i=1 em[i,yi]+t[yi−1,yi](cid:80)ni]+t[y(cid:48)i=1 em[i,y(cid:48).
,.
i−1,y(cid:48)i].
(cid:80)y(s)y(cid:48).
(12).
(13).
p (y |x) =.
(14).
where m ∈ rn×|t |, y(s) is all valid label se-quences, t ∈ r|t |×|t | is the transitation matrix, anentry (i, j) in t means the transition score from tagi to tag j. after getting the p (y |x), we use nega-tive log likelihood loss to optimize the model.
dur-.
(11).
5821dataset.
model.
memory training time evaluation time.
conll-2003.
ontonotes.
bert-mlpbert-crfours(word)[bart].
bert-mlpbert-crfours(word)[bart].
7g7g8g.
7g7g7g.
98s122s115s.
421s523s493s.
3s5s12s.
9s13s38s.
table 8: the training memory usage, training time and evaluation time comparison between three models..ing the inference, the viterbi algorithm is used toﬁnd the label sequence achieves the highest score.
we use the bert-base version and bart-baseversion to calculate the memory footprint duringtraining, seconds needed to iterate one epoch (oneepoch means iterating over all training samples),and seconds needed to evaluate the developmentset.
the batch size is 16 and 48 for training andevaluation, respectively.
the comparison is pre-sented in table 8..during the training phase, we can use the casualmask to make the training of our model in paral-lel.
therefore, our proposed model can train fasterthan the bert-crf model, which needs sequentialcomputation.
while during the evaluating phase,we have to autoregressively generate tokens, whichwill make the inference slow.
therefore, furtherwork like the usage of a non-autoregressive methodcan be studied to speed up the decoding (gu et al.,2018)..5822