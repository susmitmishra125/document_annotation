improving speech translation by understanding and learning from theauxiliary text translation task.
yun tang, juan pino, xian li, changhan wang, dmitriy genzelfacebook ai{yuntang,juancarabina,xianl,changhan,dgenzel}@fb.com.
abstract.
pretraining and multitask learning are widelyused to improve the speech to text translationperformance.
in this study, we are interestedin training a speech to text translation modelalong with an auxiliary text to text translationtask.
we conduct a detailed analysis to un-derstand the impact of the auxiliary task onthe primary task within the multitask learningframework.
our analysis conﬁrms that multi-task learning tends to generate similar decoderrepresentations from different modalities andpreserve more information from the pretrainedtext translation modules.
we observe mini-mal negative transfer effect between the twotasks and sharing more parameters is helpfulto transfer knowledge from the text task tothe speech task.
the analysis also revealsthat the modality representation difference atthe top decoder layers is still not negligible,and those layers are critical for the transla-tion quality.
inspired by these ﬁndings, wepropose three methods to improve translationquality.
first, a parameter sharing and ini-tialization strategy is proposed to enhance in-formation sharing between the tasks.
second,a novel attention-based regularization is pro-posed for the encoders and pulls the represen-tations from different modalities closer.
third,an online knowledge distillation is proposedto enhance the knowledge transfer from thetext to the speech task.
our experiments showthat the proposed approach improves transla-tion performance by more than 2 bleu overa strong baseline and achieves state-of-the-art results on the must-c english-german,english-french and english-spanish languagepairs..1.introduction.
end-to-end methods have achieved signiﬁcantprogress in speech to text translation (st) and evensurpassed the traditional pipeline-based methods.
in some applications (niehues et al., 2019; saleskyand black, 2020).
however, the success of end-to-end methods relies on large amounts of trainingdata, which is quite expensive to obtain and rela-tively small in practice.
building st systems frompretrained models with multitask learning (mtl)is widely used to overcome the limited training dataissue (weiss et al., 2017; anastasopoulos and chi-ang, 2018; bahar et al., 2019; indurthi et al., 2020;wang et al., 2020b; li et al., 2020).
nevertheless,little prior work has been devoted to understandingthe interactions between different tasks.
standleyet al.
(2020) conduct an empirical study on com-puter vision tasks for mtl.
they ﬁnd many “as-sumptions” for mtl may not be held for speciﬁcapplications.
for example, “similar” tasks do notnecessarily train better together..in this study, we focus on training the st modelalong with an auxiliary text to text machine trans-lation (mt) task.
we are interested in the taskinteractions with different modalities and in im-proving the primary st task with the help from theauxiliary mt task.
the model is initialized withpretrained modules from automatic speech recog-nition (asr) and mt.
two types of analysis areconducted on the ﬁne-tuned multitask learned mod-els.
the ﬁrst focuses on the model variation bycomparing ﬁne-tuned models with pretrained mod-els for different tasks.
the second aims to measureinternal representation differences due to differentmodalities.
the analysis leads to three main ﬁnd-ings.
first, the analysis conﬁrms that mtl tends togenerate similar model representations for differentinput modalities and preserves more informationfrom the pretrained mt modules.
second, we donot observe signiﬁcant negative transfer effect fromthe mt task to the corresponding st task.
sharingmore parameters is helpful to transfer knowledgeto the primary st task.
finally, the top layers inthe st decoder are more critical to the translation.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4252–4261august1–6,2021.©2021associationforcomputationallinguistics4252performance and they are also more sensitive tothe modality difference.
the model representationsfrom different modalities demonstrate larger differ-ence for the top layers in our analysis..inspired by these ﬁndings, we propose three tech-niques to enhance the performance of the primaryst task.
first, we propose to maximize parametersharing between the st and mt tasks, i.e.
the entiredecoder and the top encoder layers.
those sharedparameters are initialized with the correspondingmt models.
second, a cross-attentive regulariza-tion is introduced for the encoders.
it minimizesthe l2 distance between two reconstructed encoderoutput sequences and encourages the encoder out-puts from different modalities to be closer to eachother.
finally, an online knowledge distillationlearning is introduced for mtl in order to enhanceknowledge transfer from the mt to the st task.
our contributions are summarized as follows:.
1. a detailed analysis is conducted on the inter-action between the primary st task and theauxiliary mt task..2. a parameter sharing and initialization strat-egy are proposed to encourage informationsharing between tasks..3. cross-attentive regularization and onlineknowledge distillation are proposed to reducethe model representation difference betweendifferent modalities and enhance the knowl-edge transfer from the mt task to the st task..4. our system achieves state of the art resultson the must-c english-german (en-de),english-french (en-fr) and english-spanish(en-es) language pairs, with 2 or morebleu gains over strong baselines..2 related work.
multitask learning aims to improve general-ization by leveraging domain-speciﬁc informa-tion contained in the training signals of relatedtasks (vandenhende et al., 2020).
compared withsingle task, mtl has many advantages, such asthe potential to improve performance by sharingcomplementary information or acting as a regu-larizer.
many previous works focus on learning agood model for all tasks.
chen et al.
(2018) studythe gradients from different tasks and conduct taskdependent gradient normalization to encourage dif-ferent tasks to learn at similar speed.
maninis et al..figure 1: joint training framework.
the speech to texttranslation task is depicted as dark gray line, text to texttranslation task is illustrated as light gray line.
the pa-rameters in blue modules are shared between two tasks..(2019); liu et al.
(2019a); pfeiffer et al.
(2020)introduce task-dependent components to enhanceindividual task performance..weiss et al.
(2017) explore different multitasktraining strategies for st, and they ﬁnd the one-to-many strategy, in which an encoder is sharedbetween the st and asr tasks, is more effective.
anastasopoulos and chiang (2018) further extendit to a triangle structure by concatenating asr andst models.
bahar et al.
(2019) compare differentmultitask strategies for the st task, and they con-ﬁrm many-to-one strategy, in which mt and st aretrained together and the decoder is shared betweentwo tasks, is effective if extra bitext data is used.
inthis work, we carefully study the relation betweenco-trained tasks in the many-to-one strategy, andthe analysis results guide us to propose three tech-niques to learn more from the auxiliary mt taskand enhance the st performance further..model analysis chatterji et al.
(2020) propose crit-icality analysis to measure the importance of dif-ferent modules from the trained model.
parameters.
4253in the selected module or layer are partially rolledback to the initial values, and the module critical-ity or importance is measured by the performancedrop after modiﬁcation.
larger performance dropsindicate a more critical module.
inspired by theirwork, we extend it to the analysis on the jointlytrained models with different pretrained modulesand schemes.
raghu et al.
(2017); morcos et al.
(2018) propose to employ canonical correlation tomeasure the similarity between different modelsgiven the same input.
we extend their work to studya model with inputs from different modalities..3 methods.
the proposed st system is co-trained with the mttask as depicted in figure 1. the modules in theprimary st task are connected with dark gray linesand the auxiliary mt task is illustrated with lightgray lines.
the parameters in the blue modules areshared between the two tasks.
during inferencewith speech input, only modules related to the sttask are used..the model has two encoders, a text encoder anda speech encoder, to take text and speech input re-spectively.
the decoder is shared between the twotasks.
to encourage knowledge sharing betweenthe two tasks, the top encoder layers are also shared.
the parameters of the shared modules are initial-ized with a pretrained mt model.
a novel cross-attentive regularization is proposed to reduce thedistance between encoder outputs from differentinput modalities.
we also introduce a novel onlineknowledge distillation method where the outputfrom the auxiliary mt task is used to guide the stmodel training.
the cross-attentive regularizationand online knowledge distillation are illustratedas orange modules in figure 1 and the details arepresented in the following two subsections..3.1 cross-attentive regularization.
the cross-attentive regularization (car) is pro-posed to increase the similarity between the textencoder outputs and their corresponding speechencoder outputs.
hence, the performance of themore difﬁcult st task can be improved by learn-ing from the relatively easier mt task.
encoderoutput sequences from different modalities cannot be compared directly since they have differentlengths.
in car, the two reconstructed sequencesare calculated from the text output sequence viaself-attention or the speech output sequence via.
cross attention over the text output sequence.
thetwo reconstructed sequences have the same lengthand the distance is simply measured as the l2 dis-tance between the two sequences..formally, we denote a speech to text translationtraining sample as a triplet o = (xs, xt, y).
xs ∈rds×n , xt ∈ rm , and y ∈ rk are the speechfeature input, text token input and target text outputrespectively.
n , m and k are the correspondingsequence lengths.
assume hs = (hs1, hs2, · · ·, hsn )and ht = (htm ∈ rdh aren, htoutputs from the speech encoder and text encoderrespectively, where dh is the dimension of the out-put states.
a similarity matrix s ∈ rn ×m is de-ﬁned as the cosine distance between the tensors inthe two sequences:.
2, · · ·, ht.
m ), hs.
1, ht.
si,j =.
(hsi )(cid:48) · htji ||2||ht||hsj||2.
(1).
where si,j is the ith row and jth column compo-nent in s. the text encoder outputs ht are recon-structed through the speech encoder outputs hsand similarity matrix s as below..hs→t = hs · softmax(s).
(2).
ht→t, the reconstruction of ht from itself, canbe computed similarly via self-attention.
car isdeﬁned as the l2 distance between the two recon-struction encoder outputs:.
lcar(θs) =.
1m.(cid:13)(cid:13)(cid:13)hs→t − sg[ht→t](cid:13)(cid:13)(cid:13)2.
(3).
where sg[·] is the stop-gradient operator and θs arethe st model parameters.
by optimizing the modelwith car, the speech encoder is encouraged tolearn from more accurate text encoder and gener-ates similar encoder outputs after reconstruction.
car is inspired by the attention mechanism be-tween the encoder and decoder where the decoderstates are reconstructed through encoder outputstates via the attention mechanism..3.2 online knowledge distillation.
knowledge distillation (kd) is widely used formodel compression (hinton et al., 2015; kimand rush, 2016) where a smaller student networkis trained to mimic the original teacher networkby minimizing the loss between the student andteacher outputs.
the st task is considerably moredifﬁcult than the mt task since the speech inputis noisier and more ambiguous than the text input..4254the accuracy of the mt model is usually muchhigher than the corresponding st model.
knowl-edge distillation from a well trained mt modelto a st model has been proved to be an effectiveway to improve the st performance (liu et al.,2019b; gaido et al., 2020).
in this work, we ex-tend knowledge distillation to the mtl frameworkwhere both st and mt are ﬁne-tuned simultane-ously with shared parameters..concretely, we assume an mtl model learnsfrom a data set d with target vocabulary size|v |.
the training criterion is to minimize nega-tive log likelihood (nll) for each example o =(xs, xt, y) ∈ d from the training data:.
ln ll(θs) = −.
δ(yk = v).
d(cid:88).
k(cid:88).
|v |(cid:88).
o.k=1.
v=1.
log p(yk = v|y<k, xs, θs) (4).
where δ(·) is the indicator function and p the distri-bution from the st model (parameterized by θs).
assume the probability distribution for yk givenis q(yk =textv|y<k, xt, θt), the knowledge distillation loss isdeﬁned as minimizing the cross-entropy with themt’s probability distribution.
input xt and mt model θt.
lkd(θs) = −.
q(yk = v|y<k, xt, θt).
d(cid:88).
k(cid:88).
|v |(cid:88).
o.k=1.
v=1log p(yk = v|y<k, xs, θs).
(5).
the overall loss is the combination of cross-attentive regularization, knowledge distillation loss,negative log likelihood loss for both st and mt, asfollows:.
l(θs, θt) = αln ll(θs) + (1 − α)lkd(θs)(6).
+λlcar(θs) + ln ll(θt).
where α and λ are predeﬁned hyper-parameters..4 experimental setup.
experiments are conducted on three must-c (gangi et al., 2019a) language pairs: en-de,en-es and en-fr.
the models are developed andanalyzed on the dev set and the ﬁnal results arereported on the tst-common set.
we use wmtparallel data from different years, 2013 for spanish,2014 for german, and 2016 for french, as extratext training corpus for mtl.
case-sensitive deto-kenized bleu is reported by sacrebleu withdefault options (post, 2018)..we use the “t-md” conﬁguration from (wanget al., 2020a) in all experiments.
the speech en-coder has 12 transformer layers while the decoderis with 6 transformer layers.
for the mtl model,the text encoder has 6 transformer layers.
the trans-former layer has an input embedding size of 512and middle layer dimension 2048. we share pa-rameters of all 6 text encoder transformer layerswith the top 6 transformer layers in the speech en-coder, hence both encoders use the same modulesto generate the encoder outputs..the adam optimizer (kingma and ba, 2014)with a learning rate 0.002 is employed in the ex-periments.
label smoothing and dropout rate areboth set to 0.1. we choose α = 0.8 and λ = 0.02in equation 6 through grid search ([0.1, 1.0] for αand [0.01, 0.05] for λ)..input speech is represented as 80d log mel-ﬁlterbank coefﬁcients computed every 10ms with a25ms window.
global channel mean and variancenormalization is applied.
the specaugment (parket al., 2019) data augmentation with the lb pol-icy is applied in all experiments.
the input texttokens are converted into their corresponding pro-nunciation form as phoneme sequences (tang et al.,2021; renduchintala et al., 2018).
the grapheme tophoneme conversion is done through the “g2p en”python package (lee and kim, 2018).
the leadingphoneme in a word is appended with an extra “ ”to mark word boundaries.
in total, the vocabularysize for the input phonemes is 134. the target vo-cabulary consists of 10k “unigram” subword unitslearned by sentencepiece (kudo and richardson,2018) with full character coverage of all trainingtext data..all st or jointly trained models are initializedwith pretrained asr and mt modules.
the asrmodel is trained on the same english speech train-ing data from must-c with the “t-md” conﬁgura-tion too.
the pretrained mt models are trained foreach language pair with the aforementioned wmtdata.
the mt encoder and decoder conﬁgurationsare the same as the text encoder and decoder in themtl model mentioned above..the models are ﬁne-tuned to 100 epochs using8 v100 gpus for approximate one day.
the batchsize is 10,000 frames for speech to text translationsamples and 10,000 tokens for parallel text samplesper gpu.
the model parameters are updated every4 batches.
speech training samples and text inputsamples are used to update the model alternatively..4255model.
conﬁguration speechasrasrasrasr.
stjtjt-s-asrjt-s-mt.
encodersharedtextnone nonenonemtasrmtmtmt.
table 1: model initialization schemes.
the models are trained with fairseq (ott et al.,2019; wang et al., 2020a).
the last 10 checkpointsare averaged for inference with beam size 5.
1..5 mtl analysis.
5.1 model variation.
we extend chatterji et al.
(2020)’s work to analyzea mtl model.
we initialize models with differ-ent pretrained modules and ﬁne-tune them for stand mt tasks within the mtl framework.
thepretrained modules come from asr and mt tasks.
criticality analysis is conducted on the st modelafter the mtl ﬁne-tuning step.
the parametersin the selected modules are interpolated with cor-responding parameters in the pretrained modules.
must-c en-de dev set is used for bleu com-putation.
with different interpolation ratios, weobtain different bleu scores.
the bleu differ-ence comes from two sources.
the ﬁrst one comesfrom the selected module itself.
if the module is im-portant and sensitive, very small perturbation couldresult in a nontrivial bleu difference as (chatterjiet al., 2020).
another source of difference is that ifthe selected module changes signiﬁcantly to adaptto the st task, rewinding the parameters back tothe initial task may lead to a substantial decreasein bleu.
we attempt to quantify the extent of thedegradation from the second source, which canbe indicative of the model variation from the pre-trained task to the st task.
this is accomplishedby comparing the bleu differences for the samemodule but using different initialization and train-ing schemes..table 1 lists models initialized with differentpretrained modules.
“st” designates a st modeltrained with the single st task, “jt” correspondsto a st model trained with the primary st task andauxiliary mt task together.
“jt-s-asr” and “jt-s-mt” are another two jointly trained models but.
1the.
source.
athttps://github.com/pytorch/fairseq/tree/master/examples/speechtext joint to text.
released.
code.
will.
be.
(a) st enc..(b) st dec..figure 2: criticality analysis for the “st” model..with the top encoder layers shared as described insection 4. the difference between the two modelsis how we initialized the shared encoder layers,either from the pretrained asr model for “jt-s-asr” or from the pretrained mt model for “jt-s-mt”.
st figure 2 shows the analysis for the “st”model.
the x-axis is the interpolation ratio and“1.0” means the pretrained parameters are used.
the y-axis is the relative change in bleu com-pared with the well-trained st model.
it is clearthat higher layers are more critical to the per-formance.
around 5 bleu decrease is observedon the top encoder layer (11) and top decoderlayer (5) during the criticality tests.
the follow-ing analysis will compare with figure 2 and we canseparate the aforementioned second source fromthe ﬁrst one.
jt figure 3 presents the analysis for the “jt”model.
the jointly trained model shows smallerdegradation compared with “st” for the decoderlayers.
this indicates that training the st andmt tasks together helps to preserve more infor-mation from the original mt decoder and par-tially remedies the catastrophic forgetting (mc-closkey and cohen, 1989) during the ﬁne-tuning phase.
on the other hand, after rolling pa-rameters back to the initial asr model, the jointlytrained model shows a larger degradation for theencoder layers.
this means that the speech encoderin the jointly trained model has deviated far awayfrom the speech encoder in the initial asr task.
we conclude that the shared decoder is subject tomore constraints since it is optimized toward bothmt and st tasks while the speech encoder has toundergo larger changes in order to align with thetext encoder, although there is no parameter sharingbetween two encoders.
jt-s-asr and jt-s-mt results for models with.
4256(a) jt enc..(b) jt dec..(a) jt-s-mt enc..(b) jt-s-mt dec..figure 3: criticality analysis for the “jt” model..figure 5: criticality analysis for the “jt-s-mt” model.
the shared encoder layers are initialized with the layersfrom the mt encoder..(a) jt-s-asr enc..(b) jt-s-asr dec..figure 4: criticality analysis for the “jt-s-asr”model.
the shared encoder layers are initialized withthe layers from the asr encoder..the top encoder layers shared are presented in fig-ure 4 and 5. in “jt-s-mt”, the top 6 shared en-coder layers are initialized with the pretrained mtencoder.
we illustrate their bleu difference trajec-tories with dotted lines in figure 5 (a) so they canbe easily distinguished from other layers initializedfrom the asr encoder..the bleu difference for the top encoder layer isdown from 20.2 to 17.6 when the parameters are re-placed with the ones in the pretrained asr encoder.
it is further reduced to 10.0 if the shared layers areinitialized with mt encoder layers.
the bleudifferences in the decoder layers are mixed.
theperformance of “jt-s-asr” degrades quickly inthe criticality test for the top decoder layer, while“jt-s-mt performs similarly in the test as “jt”decoder.
we argue that the top layers in the ﬁne-tuned st encoder might be closer to the mt en-coder than the asr encoder.
it preserves moreinformation from the mt task by sharing moreparameters between two tasks and initializingthem with pretrained mt modules.
this is a de-sirable property since we want to transfer moreknowledge from the text corpus to the st task..figure 6: comparison of decoder layers correlation co-efﬁcients between text and speech input (“jt-s-mt”)..5.2 modality variation.
the jointly trained model takes input from twomodalities, i.e.
text or speech, and we are inter-ested in the model internal representation differ-ence for paired inputs.
given text target y, weextract the decoder hidden state representations forthe corresponding text input xt and speech inputxs.
the decoder representation difference solelycomes from different input modalities.
the differ-ence is quantiﬁed by the correlation coefﬁcient overall samples evaluated between two input modali-ties:.
rs,t(l, d) =.
σst(l, d)σs(l, d)σt(l, d).
(7).
where σz(l, d), z ∈ [s, t] is the standard deviationsof decoder hidden states at layer l for componentd in all samples, and σst(l, d) is the correspondingcovariance.
the layer-wise correlation coefﬁcientis the average of all components:.
rs,t(l) =.
(cid:88).
rs,t(l, d).
(8).
1d.d.figure 6 depicts the correlation coefﬁcient be-tween speech input and text input for each decoderlayer in the model “jt-s-mt”.
the x-axis is thenumber of training epochs and the y-axis representsthe correlation coefﬁcient for each layer.
there.
4257data corpusgangi et al.
(2019b)inaguma et al.
(2020)pino et al.
(2020)stjtjt proposed.
#pars(m)30-435767676.de17.722.925.221.524.126.8.es20.928.0-28.129.031.0.fr26.532.734.533.835.137.4.table 2: bleu on three language pairs in the must-ctst-common datasets..are two observations.
first, the correlation coef-ﬁcients become larger and close to “1.0” as train-ing converges.
second, the higher the layer, thesmaller the correlation coefﬁcient.
we hypothe-size that the inputs to the lower layers are domi-nated by the decoder text embeddings, which arethe same for both modalities, and the inputs to thehigher layers would contain more information fromthe encoder outputs, which result in the decoderinternal representation differences.
the analysisshows a well trained mtl decoder has similarrepresentations for paired text and speech in-put.
however, the top decoder layers still havenontrivial representation differences due to dif-ferent modalities..6 experimental results.
6.1 main results.
the main st results are presented in table 2. theﬁrst three rows are results from the literature.
“st”and “jt” are models initialized as table 1 and stud-ied in section 5. the last row (“jt proposed”)presents results from the proposed system, in whichthe top encoder layers and decoder are shared, andthe models are optimized following equation 6.the second column (“pars(m)”) lists the number ofparameters used during inference.
from table 2,our “st” baseline is comparable to the previouslyreported results except (pino et al., 2020), who usea much larger model and additional weakly super-vised speech training data.
as expected, the vanillajoint training baseline (“jt”) outperforms the “st”baseline with the help of extra bitext training data.
finally, the proposed joint training model (“jt pro-posed”) achieves 2.0∼2.7 bleu gains over thestrong joint training baseline (“jt”)..6.2 ablation.
table 3 breaks down the performance gains into in-dividual components/changes.
sharing encoder lay-ers improves the quality for all three language pairs.
jtjt-s-asrjt-s-mt+ car+ car + kd.
en-de24.124.424.725.026.8.en-es29.029.429.730.431.0.en-fr35.135.435.336.237.4.table 3: ablation study..(a) jt proposed enc..(b) jt proposed dec..figure 7: criticality analysis for “jt proposed”..(“jt” v.s.
“jt-s-asr”).
initializing the shared en-coder layers with pretrained mt modules leads tobleu increase for two of the three evaluated trans-lation pairs (“jt-s-asr” v.s.
“jt-s-mt”).
foren-fr, the degradation is minimal (-0.1 bleu).
overall, sharing top encoder layers can increasebleu by 0.2∼0.7 (“jt-s-mt” v.s.
“jt”).
carfurther improves the translation by another 0.3∼0.9bleu.
the best results are achieved by applyingthe shared top encoder layers, car and online kdtogether.
they are about 2.9+ bleu better thanthe single task based system (“st”) and achieve 2+bleu increase on top of the strong vanilla jointtraining system(“jt”)..figure 7 demonstrates the model variation forthe proposed system on the must-c en-de devset.
compared with figure 5, the decoder showsless degradation during the criticality test and itshows car and online kd help to preserve moreinformation from the mt task.
figure 8 showsthe corresponding correlation coefﬁcients betweenpaired text and speech input from the top decoder.
figure 8: correlation coefﬁcient for the top decoderlayers (epoch 100)..4258modeljt-s-mtjt-s-mt + adapterjt-s-mt + dedicated attention.
bleu24.724.724.2.table 4: bleu score for models with task dependentcomponents.
layer from different model conﬁgurations.
it alsoconﬁrms that the proposed methods, i.e., sharedtop encoder layers, car and online kd, all reducethe modality difference substantially..6.3 task dependent components.
in mlt, many works (maninis et al., 2019; liuet al., 2019a; zhang et al., 2020; pfeiffer et al.,2020) employ task-dependent components to allevi-ate the negative transfer effect.
in table 4, we com-pare the “jt-s-mt” model with two variants usingdifferent task-dependent components.
the ﬁrst one(“jt-s-mt + adapter”) (bapna et al., 2019) addsan extra adapter module on the top of the speechencoder.
hence, the speech encoder outputs, whichare generated from shared encoder layers, are fur-ther processed to reduce the difference betweenspeech input and text input.
the adapter moduleconsists of a linear layer and layer normalizationlayer.
the second variant (“jt-s-mt + dedicatedattention”) (blackwood et al., 2018) introducesdedicated decoder modules for different tasks.
at-tention layers between encoder and decoder, andthe layer normalization modules are not shared be-tween the st and mt tasks.
it gives the decodermore ﬂexibility to handle information from differ-ent modalities..the results show the extra adapter layer doesn’tbring gain while the task dependent attention mod-ule actually makes the performance worse.
it indi-cates that the negative transfer effect is not signiﬁ-cant in this study and adding extra task-dependentcomponents might not be necessary..6.4.impact on the mt task.
as shown in table 2, training st models with anauxiliary mt task improves the translation qualitysubstantially.
it may be interesting to examine theimpact on the auxiliary task itself.
we evaluate themt model jointly trained with the st task.
resultsare shown in table 5.
“st (jt proposed)” in theﬁrst row corresponds to the best results obtainedfor the st task.
the detailed experimental setup isdescribed in appendix a. for reference, we also.
st (jt proposed)mt (gangi et al., 2019a)mtmt (tuned)mt (jt)mt (jt proposed).
en-de26.828.125.429.628.930.5.en-es31.034.227.734.333.934.7.en-fr37.442.233.541.441.642.3.table 5: comparison between st and mt..include the mt evaluation results from must-c (gangi et al., 2019a) in the second row.
all mtmodels (in the last 4 rows) take phoneme sequencesas input instead of sentencepiece..“mt” (row 3) shows the results from pretrainedmt models on wmt.
in the “mt (tuned)” row,the mt models pretrained on wmt are ﬁne-tunedon the must-c datasets.
the large improvementsclearly show a domain mismatch between wmtand must-c. the mt models trained with wmtdata are improved after ﬁne-tuning, and they arecomparable with the ones reported in (gangi et al.,2019a), though the input token is in pronunciationform, which is more ambiguous than the corre-sponding sentencepiece unit..“mt (jt)” and “mt (jt proposed)” are resultsfrom the co-trained mt models in “jt” and “jtproposed” respectively.
after ﬁne-tuning usingboth must-c (speech and text) and wmt (textonly) training data, the auxiliary mt models per-form better than the corresponding st models.
theproposed techniques further improve the co-trainedmt models by 0.7∼1.6 bleu.
while this is a sur-prising result, we note that the dedicated mt mod-els may be improved with better hyperparametertuning.
in conclusion, the results show the pro-posed methods are effective to unify two tasks intoone model with minimal negative transfer effect..7 conclusions.
in this study, we focus on understanding the inter-actions between the st and mt tasks under themtl framework, and on boosting the performanceof the primary st model with the auxiliary mttask.
two types of analysis on model variationand modality variation, are conducted on the mtlmodels.
the analysis demonstrates mtl helps topreserve information from the mt task and gen-erates similar model representations for differentmodalities.
we observe a minimal negative transfereffect between the two tasks.
sharing more parame-ters can further boost the information transfer from.
4259the mt task to the st model.
the analysis alsoreveals that the model representation difference dueto modality difference is nontrivial, especially forthe top decoder layers, which are critical for thetranslation performance.
inspired by the ﬁndings,we propose three techniques to increase knowledgetransfer from the mt task to the st task.
thesetechniques include parameter sharing and initial-ization strategy to improve the information sharingbetween tasks, car and online kd to encouragethe st system to learn more from the auxiliary mttask and then generate similar model representa-tions from different modalities.
our results showthat the proposed methods improve translation per-formance and achieve state-of–the-art results onthree must-c language pairs..references.
antonios anastasopoulos and david chiang.
2018.tied multitask learning for neural speech translation.
in naacl-hlt..parnia bahar, tobias bieschke, and hermann ney.
2019. a comparative study on end-to-end speechto text translation.
in asru..ankur bapna, n. arivazhagan, and orhan firat.
2019.simple, scalable adaptation for neural machine trans-lation.
in emnlp/ijcnlp..g. blackwood, miguel ballesteros, and t. ward.
2018.multilingual neural machine translation with task-speciﬁc attention.
in coling..niladri s. chatterji, behnam neyshabur, and h. sedghi.
2020. the intriguing role of module criticality in thegeneralization of deep networks.
in iclr..z. chen, vijay badrinarayanan, chen-yu lee, and an-drew rabinovich.
2018. gradnorm: gradient nor-malization for adaptive loss balancing in deep multi-task networks.
in icml..marco gaido, mattia antonino di gangi, mat-teo negri, and marco turchi.
2020.end-to-end speech-translation with knowledge distillation:fbk@iwslt2020..mattia antonino di gangi, roldano cattoni, luisabentivogli, matteo negri, and marco turchi.
2019a.
must-c: a multilingual speech translation corpus.
in naacl-hlt..mattia antonino di gangi, matteo negri, and marcoturchi.
2019b.
one-to-many multilingual end-to-end speech translation.
in asru..geoffrey e. hinton, oriol vinyals, and j. dean.
2015.distilling the knowledge in a neural network.
arxiv,abs/1503.02531..h. inaguma, s. kiyono, k. duh, s. karita, n. soplin,t. hayashi, and s. watanabe.
2020. espnet-st: all-in-one speech translation toolkit.
in acl..sathish reddy indurthi, houjeung han, nikhil ku-mar lakumarapu, beom seok lee, insoo chung,end-sang-ha kim, and chanwoo kim.
2020.end speech-to-text translation with modality agnos-tic meta-learning.
in icassp..yoon kim and alexander m. rush.
2016. sequence-.
level knowledge distillation.
in emnlp..diederik p kingma and jimmy ba.
2014. adam: a.method for stochastic optimization.
in iclr..t. kudo and j. richardson.
2018..sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inemnlp..y. lee and t. kim.
2018. learning pronunciation froma foreign language in speech synthesis networks.
arxiv..xian li, changhan wang, yun tang, chau tran,yuqing tang, juan pino, alexei baevski, alexisconneau, and michael auli.
2020. multilingualspeech translation with efﬁcient ﬁnetuning of pre-trained models.
arxiv: computation and language..shikun liu, edward johns, and a. davison.
2019a.
end-to-end multi-task learning with attention.
2019ieee/cvf conference on computer vision and pat-tern recognition (cvpr), pages 1871–1880..yuchen liu, hao xiong, zhongjun he, jiajun zhang,hua wu, haifeng wang, and chengqing zong.
2019b.
end-to-end speech translation with knowl-edge distillation.
in interspeech..k. maninis, ilija radosavovic, and i. kokkinos.
2019.
2019attentive single-tasking of multiple tasks.
ieee/cvf conference on computer vision and pat-tern recognition (cvpr), pages 1851–1860..m. mccloskey and n. j. cohen.
1989. catastrophicinterference in connectionist networks: the sequen-tial learning problem.
psychology of learning andmotivation, 24:109–165..ari s. morcos, m. raghu, and s. bengio.
2018..in-sights on representational similarity in neural net-works with canonical correlation.
in neurips..jan niehues, r. cattoni, sebastian st¨uker, matteonegri, marco turchi, elizabeth salesky, ramonsanabria, lo¨ıc barrault, lucia specia, and marcellofederico.
2019. the iwslt 2019 evaluation cam-paign..myle ott, sergey edunov, alexei baevski, angela fan,s. gross, nathan ng, david grangier, and m. auli.
2019. fairseq: a fast, extensible toolkit for sequencemodeling.
in naacl..4260a appendix.
the detailed experimental setup for “mt” and“mt(tuned)” in table 5 are described as below..we trained mt models for each language pairin “en-de”, “en-es”, and “en-fr”.
the train-ing data is from wmt from different years, 2013for “en-es”, 2014 for “en-de” and 2016 for “en-fr”.
we use “transformer wmt en de” architecturefrom fairseq.
the models are with embedding size512 and feed-forward layer dimension 2048. bothencoder and decoder are with 6 transformer layers.
the input is phoneme sequence and output is sen-tencepiece sequence.
the vocabularies are sharedwith the corresponding speech to text translationmodels.
the models are optimized with adam withlearning rate equal to 0.001. beside experimentsin table 5, the trained mt models are used to ini-tialize the jointly trained models..we further ﬁne-tuned the “mt” models trainedfrom wmt data to must-c data sets using sourcetranscripts and target translation labels.
no speechdata is used.
similar to the “mt” models, adamoptimizer with learning rate equal to 0.001 is used.
the models are ﬁne-tuned on the correspondingmust-c data sets for 15 epochs and the check-points from the last 5 epochs are averaged for eval-uation..d. park, w. chan, y. zhang, c. chiu, b. zoph,e. cubuk, and q. le.
2019. specaugment: a sim-ple data augmentation method for automatic speechrecognition.
interspeech..jonas pfeiffer, ivan vulic, iryna gurevych, and se-bastian ruder.
2020. mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
inemnlp..j. pino, q. xu, x. ma, m. dousti, and y. tang.
2020.self-training for end-to-end speech translation.
ininterspeech..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191, brussels, belgium.
association for computa-tional linguistics..m. raghu, j. gilmer, j. yosinski, and jascha sohl-dickstein.
2017. svcca: singular vector canonicalcorrelation analysis for deep learning dynamics andinterpretability.
in nips..a. renduchintala, s. ding, m. wiesner, and s. watan-abe.
2018. multi-modal data augmentation for end-to-end asr.
in interspeech..elizabeth salesky and alan w black.
2020. phone fea-.
tures improve speech translation.
in acl..t. standley, a. zamir, dawn chen, l. guibas, jitendramalik, and s. savarese.
2020. which tasks shouldbe learned together in multi-task learning?
in icml..yun tang, juan pino, changhan wang, xutai ma, anddmitriy genzel.
2021. a general multi-task learn-ing framework to leverage text data for speech to texttasks.
in icassp..simon vandenhende, s. georgoulis, wouter van gans-beke, m. proesmans, dengxin dai, and l. gool.
2020. multi-task learning for dense prediction tasks:arxiv: computer vision and patterna survey.
recognition..c. wang, y. tang, x. ma, a. wu, d. okhonko, andj. pino.
2020a.
fairseq s2t: fast speech-to-text mod-eling with fairseq.
in aacl (demo)..chengyi wang, yu wu, shujie liu, zhenglu yang, andming zhou.
2020b.
bridging the gap between pre-training and ﬁne-tuning for end-to-end speech trans-lation.
in aaai..ron j. weiss, jan chorowski, navdeep jaitly, yonghuiwu, and zhifeng chen.
2017.sequence-to-sequence models can directly translate foreignspeech.
in interspeech..biao zhang, philip williams, ivan titov, and rico sen-nrich.
2020. improving massively multilingual neu-ral machine translation and zero-shot translation.
inacl..4261