pos-constrained parallel decoding for non-autoregressive generation.
kexin yang♠ wenqiang lei♦∗ dayiheng liu♠ weizhen qi♣ jiancheng lv♠♠college of computer science, sichuan university♦national university of singapore♣university of science and technology of china{kexinyang0528, wenqianglei}@gmail.com.
abstract.
the multimodality problem has become a ma-jor challenge of existing non-autoregressivegeneration (nag) systems.
a common solu-tion often resorts to sequence-level knowledgedistillation by rebuilding the training datasetthrough autoregressive generation (hereinafterknown as “teacher ag”).
the success of suchmethods may largely depend on a latent as-sumption, i.e., the teacher ag is superior to thenag model.
however, in this work, we exper-imentally reveal that this assumption does notalways hold for the text generation tasks liketext summarization and story ending genera-tion.
to provide a feasible solution to the mul-timodality problem of nag, we propose incor-porating linguistic structure (part-of-speechsequence in particular) into nag inference in-stead of relying on teacher ag.
more specif-ically, the proposed pos-constrained paralleldecoding (pospd) method aims at provid-ing a speciﬁc pos sequence to constrain thenag model during decoding.
our experi-ments demonstrate that pospd consistentlyimproves nag models on four text generationtasks to a greater extent compared to knowl-edge distillation.
this observation validatesthe necessity of exploring the alternatives forsequence-level knowledge distillation..1.introduction.
unlike autoregressive generation (ag) that gener-ates tokens step-by-step, non-autoregressive gener-ation (nag) parallelly generates all tokens in onetime step and thus the inference could be signiﬁ-cantly speeded up (ma et al., 2019; ran et al., 2020;susanto et al., 2020).
despite the computationaladvantage of nag, it has faced the multimodalityproblem (gu et al., 2018) caused by the condition-ally independent decoding.
a typical example ofthe problem is illustrated in figure 1, where either.
∗ correspondence to wenqiang lei..figure 1: an example to explain “multimodality prob-lem”.
the german sentence “vielen dank.” can betranslated into “many thanks.” and “thank you.”..of “thank you.” and “many thanks.” is the correcttranslation (i.e., generation modes).
in this exam-ple, a mixed mode “many you.” / “thank thanks.”will be generated by nag.
it is because the con-ditional dependence among target words will bebroken in parallel decoding.
a typical manifesta-tion is that words are usually missing (e.g., “manyyou.”) and repeating (e.g., “thank thanks.”) innag’s sentences.
to solve this problem, the key ishelping nag models to deal with various genera-tion modes..to date, one of the most widely used solutionsis sequence-level knowledge distillation (kim andrush, 2016) which aims to reduce the generationmodes of the raw data (zhou et al., 2019).
tak-ing machine translation as an example, the knowl-edge distillation based methods rebuild the targetsequence in the training set by employing an agmodel to translate the training samples.
the as-sumption is that the target sentences generated byone ag model tend to have less modality.
de-spite the success of the above studies, there arestill two major limitations: (1) most existing worksmainly focus on machine translation where the per-formance of ag is generally assumed to be better.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5990–6000august1–6,2021.©2021associationforcomputationallinguistics5990than nag.
clearly, such a solution will degradethe performance of nag on the task where the agmodel cannot obtain a better result.
as demon-strated in our experiments (see § 4.5), there are anumber of such tasks beyond the assumption liketext summarization and story ending generation.
(2) the knowledge distillation based methods maycost a tremendous amount of time to rebuild a large-scale training set with ag, which runs counter tothe initial goal of nag to improve the speed..to overcome the aforementioned limitations,we explore to alleviate the multimodality problemin short, we aim to con-in a different manner.
strain nag generation modes in the inference stage,rather than directly reducing generation modes inthe training stage.
more speciﬁcally, our basic ideais that the linguistic structure of the target sentencecould be helpful to alleviate the multimodality prob-lem.
in this paper, we show that the part-of-speech(pos) sequence, one of most simple solutions inmodeling the linguistic structure (cutting et al.,1992), could effectively verify our idea and showpromising performance in four different tasks.
inmore details, the proposed pos-constrained paral-lel decoding (pospd) trains a pos predictor to ob-tain pos tags of target sequences.
in the inferencestage, pospd constrains nag models to choosethe ﬁnal outputs that satisfy the pre-speciﬁed possequence.
as the pos predictor with a shallowdecoder is separately trained, our pospd could actas a plug-and-play method to assistant nag mod-els with negligible extra time.
meanwhile, it alsoshows the speed advantage of our method even con-sidering the time cost in building the pos dataset,since pos tagging is much faster than sentencegenerating due to the small pos dictionary..to conduct a comprehensive empirical evalua-tion, we examine the generalizability of pospd byapplying it to two widely-used nag models (i.e.,cmlm and disco) over four text generation tasks,including text summarization, story ending genera-tion, question generation, and machine translation.
experiments demonstrate that pospd signiﬁcantlyand consistently improves the two nag modelsand beats the sequence-level knowledge distillationwith a considerable performance gap.
the maincontributions of this work could be summarized asfollows:.
• for the ﬁrst time, we experimentally revealthat the implicit assumption of knowledge dis-tillation does not always hold for the tasks.
(e.g., text summarization, story ending gener-ation, as demonstrated in our experiments).
inother words, ag cannot guarantee better per-formance than nag, thus resulting in the un-desirable performance of nag if using knowl-edge distillation to alleviate the multimodal-ity problem.
this empirical result could pro-vide novel insight to revisiting the role of theknowledge distillation in nag..• to alleviate the multimodality problem in var-ious tasks, we propose pospd by employingpos sequences to constrain the nag gener-ation modes in the inference stage.
it is sim-ple but effective, being able to act as a plug-and-play assistant for nag models.
such alinguistic structure based solution shows aneffective and efﬁcient alternative to the knowl-edge distillation paradigm in alleviating themultimodality problem1..2 related works.
in this section, we ﬁrst analyze related works onalleviating the multimodality problem.
then, wereview some representative works which introducethe linguistic structure into some text generationscenarios..2.1 the multimodality problem in nag.
recently, various attempts have been made to al-leviate the multimodality problem, which can beroughly divided into two types: (1) reducing thediversity of generation modes in training; (2) help-ing models select one generation mode in inference.
the ﬁrst type usually trains the nag model underthe guidance of an ag model (called teacher ag),e.g., sequence-level knowledge distillation (kimand rush, 2016), learning from ag model’s hid-den state (li et al., 2019) and the curriculum learn-ing with ag model (liu et al., 2020d; guo et al.,2020a).
however, these methods implicitly assumethat the teacher ag can achieve better performancethan nag models, otherwise it may degrade theperformance of the nag models.
as two typicalmethods of the second type, iterative and dynamicprogramming methods have achieved promisingperformance.
in short, iterative models generatethe target sentence by iteratively reﬁning the lat-est output (ghazvininejad et al., 2019; kasai et al.,.
1the source code and dataset are available at https:.
//github.com/yangkexin/pospd.
5991figure 2: an overview of the pos-constrained parallel decoding.
2020a; guo et al., 2020b).
alternatively, dynamicprogramming methods use a heuristic searchingstrategy to select a better output from multiple de-coded candidates (sun et al., 2019; saharia et al.,2020; ghazvininejad et al., 2020).
the biggest dif-ference is prespecifying the linguistic structure toconstrain the generation of nag in a plug-and-playway.
extensive experiments verify the effective-ness and efﬁciency of our idea..2.2 leveraging the linguistic structure.
text generation involves multiple tasks, such asstyle transfer (liu et al., 2020a) and text ﬁlling (liuet al., 2019).
dating back to the period of statisticalmachine translation (liu et al., 2006; galley et al.,2006), linguistic structure prediction has long beeninvestigated for it.
previous works often modeland leverage syntactic structures on the decoderside, such as modeling long-distance word cor-respondence by syntactic dependency trees (wuet al., 2017), implicitly incorporate linguistic priorin decoder (eriguchi et al., 2017) and joint decod-ing with syntactic structure (feng et al., 2020).
innag, linguistic structures can also be helpful.
asa global pattern of target sentence, it could serveas the complementary to the parallel decoding byhelping models capture words dependency.
how-ever, directly incorporating aforementioned meth-ods into nag are less portable for current nagmodels, since they are originally designed for ag.
in comparison, pospd can act as a plug-and-playcomponent that uses a separate pos predictor toconstrain nag models during inference.
there-fore, the nag model can enjoy the beneﬁts of thesyntactical structure constraining while retaining.
its original model structure..3 methodology.
in this section, we elaborate our pospd for thenag model.
to ease of presentation, we start froma toy example to illustrate the overview of pospdin § 3.1, and then give a detailed explanation of theimplementation in § 3.2. after that, we present thetraining details of pospd in § 3.3..3.1 overview.
an overview of our pospd method is demon-strated in figure 2, where a toy example of ma-chine translation is used as a showcase.
to beexact, the german sentence “vielen dank.” is fedsimultaneously into both the pos predictor and thenag model, and then the pos predictor generatesa pos sequence jj nns pct which is furtherconverted into a binarized mask matrix througha conversion dictionary.
meanwhile, the nagmodel generates the primary probability distribu-tions through a softmax layer.
here, from figure1, words “many” and “you” get the highest prob-ability, resulting in the mix mode “many you” iffollowing the primary distribution.
to avoid suchan undesirable result, our pospd automaticallyadjusts the probability according to the binarizedmask matrix.
for example, the probability of “you”is adjusted to 0, since the pos tag of “you” is prprather than nns.
as a result, “many thanks.” getsthe highest probability hence to be generated as theoutput..59923.2 pospd in details.
in this part, we detail the pospd by introducingthe conversion dictionary building, the workﬂow ofpospd, and the core module—the pos predictor.
building a conversion dictionary the key ideaof pospd is ﬁltering out the words that dissatisfythe prespeciﬁed pos sequence in the primary re-sults of nag.
to implement our idea, we need aconversion dictionary dc that contains the mappingfrom pos tags to words.
given a target vocabularyvw with the length of |vw| and a pos tag set vs,each key of dc is a pos tag in vs and the value is aset of words that can be assigned to this pos tag.
itis worth noting that a word may have multiple postags.
therefore, one word may appear in multiplesets in dc.
the pospd workﬂow the workﬂow of pospdis as follows: given a source sentence x, pospdfeeds it into both the nag model’s encoder andthe pos predictor.
after that, the pos predictoroutputs a pos sequence s = (s1, s2, ..., sl) forthe target sentence.
meanwhile, the decoder of thenag model generates a preliminary distributionmatrix d = (d1, d2, ..., dl), where di representsthe distribution of all words2 in the i-th position.
note that, the sentence length follows the length ofthe predicted pos tag l..for the ease of implementation, the pos se-quence s is converted into a binarized mask ma-trix m = (m1, m2, ..., ml).
in details, for eachpos tag si, the corresponding binarized vector ismi = (m1) and the j-th positionmj.
i , m2i is deﬁned as:.
i , ..., m|vw|.
i.
(cid:40).
mj.
i =.
1, wj ∈ dsic ;0, wj /∈ dsi,c.(1).
where wj is the j-th word token in vw.
as a result,the pos sequence s is replaced by m. finally, weget the new generation results by:.
y = arg max(m · d)..(2).
the pos predictor as the core module of thepospd, our pos predictor is dedicated to out-put the pos tag sequence of the target sentencewhen accepting the source sentence as the input.
to train the pos predictor, we need to create a posdataset where each sample is a pair consisting of asource sentence and a pos sequence of the target.
2the length of di is |vw|..sentence3.
as shown in figure 3, the architectureof our pos predictor is a variant of the standardtransformer (vaswani et al., 2017).
as shown inthe gray arrow ﬂow, the main difference betweenour pos predictor and the vanilla transformer isthe layer number of encoder and decoder.
to bespeciﬁc, unlike the vanilla transformer which con-tains six layers for both encoder and decoder, weuse a multi-layer encoder and a one-layer decoderto reduce the inference time, because the complex-ity for decoding the pos sequence is much lowerthan that for the original sentence..figure 3: the overview of the pos predictor in pospd.
the linear layer (the red arrow points to) is only usedin the training stage..pos predictor optimization to optimize thepos predictor, we take a multi-task learning (evge-niou and pontil, 2004) paradigm to jointly decodethe word sequence and pos sequence on the tar-get side.
the underlying hypothesis is that thetarget word sentence is highly related to the possequence.
given a source sentence x, a pos se-quence s and a target sentence y = (y1, y2, ..., yl),the learning objective is then deﬁned as the sumof the pos tagging loss (the ﬁrst term) and thesentence prediction loss (the second term):.
l = lpos + lword,.
(3).
where the pos sentence prediction loss can be writ-ten as:.
lpos =.
log p (st|s<t, x),.
(4).
and the target sentence prediction loss is:.
lword =.
log p (yt|s<t, x)..(5).
3we use nltk pos tagger to create the pos sequence,which can be found at https://www.nltk.org/book/ch05.html..l(cid:88).
t=1.
l(cid:88).
t=1.
5993in our method, the pos predictor uses an extralinear layer after the decoder to generate the targetsentence, as shown in figure 3. after training,we only need the pos predicting linear layer forinference, thus enjoying the better performance forthe pos sequence prediction..3.3 training under the bpe condition.
almost all nag models use the byte pair en-coding (bpe) (sennrich et al., 2016) techniqueto build the word vocabulary with subword-leveltokens.
however, these tokens cannot be taggedby the mainstream pos taggers (yarowsky andngai, 2001), which makes difﬁculties in buildingthe pos dataset.
to address this issue, we proposea simple but effective subword-level pos taggingmethod for our pos predictor.
a simple exampleis demonstrated in table 1, the nltk toolkit tagsthe word “gutacht” as nn in the original sentencebut cannot handle the bpe form “gut ##ach ##t”.
intuitively, we can assign the bpe form to have thepos tag as “gutacht” (i.e.
nn nn nn).
however,this method increases the number of repeated to-kens in generation sentences of nag models andeven worsens the performance.
the possible reasonis that the aforementioned method cannot explicitlydistinguish whether a pos tag is associated witha bpe token or a complete word.
in contrast, ourmethod tags the bpe form as nn1 nn2 nn3.
asa result, the conversion dictionary is more sparsewhile improving the mapping between the pos tagand the corresponding words.
in addition, the word“question” is tagged as nn, since it doesn’t have anysub-word tokens after the bpe..ori..bpe.
wp.
swp.
yesterday , gutacht’ s mayor gave a clear answerto this question.
yesterday , gut ##ach ##t ’ s mayor gave a clearanswer to this question ..nn , nn sym$ jj nn vbd dt jj nnto dt nn .
nn pct nn1 nn2 nn3 sym$ jj nnvbd dt jj nn to dt nn pct.
an extensive comparison, we compare our pospdwith the sequence-level knowledge distillation, andprovide detailed analyzes in alleviating the mul-timodality problem and the time cost in datasetbuilding..4.1 datasets.
we conduct experiments on four widely-usedbenchmark datasets to evaluate pospd: xsum fortext summarization, rocstories corpus for storyending generation, squad 1.1 for question genera-tion, and wmt14 (de-en) for machine translation.
meanwhile, we use bert-based bpe tokenizer4for all datasets.
the details are as follows:xsum (narayan et al., 2018) includes the 227kbritish broadcasting corporation (bbc) online ar-ticles and the corresponding single-sentence sum-maries.
the average sentence lengths are 358.5words for input and 21.1 words for output.
rocstories corpus5 (mostafazadeh et al., 2016)contains 98k ﬁve-sentence stories.
for each story,we use the last sentence as the target output whilethe other four sentences as the source input.
we ran-domly sample 90k/4k stories for training/validation,and the remaining 4160 for testing.
the averagesentence lengths are 39.64 words for input and10.72 words for output.
squad 1.16 (rajpurkar et al., 2016) is a machinereading comprehension data set containing 98kpassage-question-answer triples (liu et al., 2020b).
after processing, we obtain a question genera-tion dataset.
following glge (liu et al., 2020c),the input sentence is formatted as (cid:104)answer [sep]passage(cid:105).
the average sentence lengths are 149.4words for input and 11.5 words for output.
wmt14 (de-en)7 contains 4.5m translationpairs and 3k/3k pairs for validation/testing.
theaverage sentence lengths are 25.07 words for inputand 26.53 words for output..4.2 evaluation metrics.
table 1: an example of the subword-level pos taggingmethod, where “wp” denotes the pos sequence gener-ated by nltk, and “swp” is the “wp” of sub-wordlevel.
“##” denotes the subword token marker..follow glge (liu et al., 2020c), we use rouge-1(r-1), rouge-2 (r-2), and rouge-l (r-l) (lin,2004) as evaluation metrics for text summarization,.
4 experiments.
in this section, we use multiple text generationdatasets to comprehensively evaluate the effective-ness and efﬁciency of the proposed pospd.
for.
4https://pypi.org/project/.
transformers/..rocstories/.
5https://cs.rochester.edu/nlp/.
6https://rajpurkar.github.io/.
squad-explorer/.
7https://www.statmt.org/wmt14/.
translation-task.html.
5994while bleu-4 (b-4) (papineni et al., 2002), me-teor (denkowski and lavie, 2014), and r-l areused in question generation and story ending gen-eration.
meanwhile, bleu-4 is also the evaluationmetric for machine translation to keep in line withprevious works (gu et al., 2018)..4.3 baselines and comparison.
in this work, we focus on using iteration-basednag models as backbones, because they areone of the mainstream nag structures in cur-rent works and perform competitively to ag mod-els without any external system (kasai et al.,2020b).
speciﬁcally, we use two representativeiteration-based nag models from recent work, i.e.,cmlm (ghazvininejad et al., 2019) and disco(kasai et al., 2020a).
the details are as follows:cmlm the conditional masked language modelrandomly masks some target tokens and predictsin inference, itthem with the remaining ones.
masks several tokens with the lower “conﬁdence”and retains other tokens with higher “conﬁdence”during iterations, which is called mask-predict in-ference.
following ghazvininejad et al.
(2019), weuse same settings for all generation tasks8.
disco the disentangled context transformer aimsto use different context information when predict-ing each token, being regarded as an effective im-provement of cmlm.
for better comparison, wealso use mask-predict inference as same as cmlm.
meanwhile, we use the model settings describedin kasai et al.
(2020a) for all generation tasks9.
knowledge distillation following gu et al.
(2018) which uses a standard transformer (vaswaniet al., 2017) as the teacher model to regeneratetraining set in the greedy method for nag mod-els (hereinafter described as “transformer-1 (6-6)”), we report nag models’ performances onall text generation task when using the distilledtraining dataset.
in the following discussion, the“transformer-1” and “transformer-4” denote thebeam size of 1 and 4 in the beam search, respec-tively.
meanwhile, we also report the results ofdifferent transformer model structures, where the“(6-6)” and “(12-1)” denote the version of six en-coder layers, six decoder layers and the version of12 encoder layers, one decoder layers, respectively..8https://github.com/facebookresearch/.
9https://github.com/facebookresearch/.
mask-predict.
disco.
4.4 experimental settings.
we follow the hyperparameters for standard trans-former in (vaswani et al., 2017) for our pos predic-tor.
one minor difference is the layers of encoderand decoder are set to 12 and 1 to make a faircomparison with ag models, respectively.
all ofthe models are implemented based on fairseq (ottet al., 2019), and we follow the other speciﬁc pa-rameter settings for both ag and nag modelsin (kasai et al., 2020b).
in inference, the lengthbeam, length penalty, and batch size are all set to1 to calculate the main results (without any post-processing) and latency.
the latency is calculatedthrough using the built-in time statistics function infairseq, which is tested on a single nvidia teslap100 gpu to keep in line with previous works (guet al., 2018).
meanwhile, the beam size of our pospredictor is set to 5. for the number of iterations,we report the iterations when the nag model re-sults are converged.
in practice, the iterations oftwo nag models are 4, 3, 3 and 10 on xsum,squad1.1, rocstories and wmt14 (de-en)..4.5 main results.
we evaluate the performance of two nag mod-els (cmlm and disco) on four text generationdatasets, and further provide the results when us-ing sequence-level data distillation (i.e., “+distill”)and the pospd (i.e., “+pospd”), respectively.
wereport the main results in table 2 and the inferencetime comparison in table 3, from which we canmake the following conclusions:1. pospd consistently improve nag models onfour text generation dataset to a greater extentcompared to knowledge distillation.
pospdconsistently improve nag models on four text gen-eration tasks while knowledge distillation may evendegrade performances of the nag models such asxsum (row 5 vs. row 6) and squad 1.1 (row 8 vs.row 9).
more importantly, although the knowledgedistillation improves nag models by 1.04/1.56(row 5 vs. row 6, row 8 vs. row 9) on bleu-4 inwmt14 (de-en), pospd still beats the knowl-edge distillation version by 0.24/0.19 (row 6 vs.row 7, row 9 vs. row 10) on bleu-4.
2. knowledge distillation does not always im-prove the nag model as the ag models may getworse performance than nag.
in both text sum-marization (xsum) and story ending generation(rocstories) tasks, the two original nag mod-els cmlm and disco outperform the ag model..5995patterns.
xsum.
squad 1.1.rocstories.
wmt14.
ag (row 1-4).
nag (row 5-10).
models.
metrics.
transformer-1 (6-6)transformer-1 (12-1)transformer-4 (6-6)transformer-4 (12-1).
cmlm+distill+pospddisco+distill+pospd.
r-1/r-2/r-l.19.53/3.38/15.3617.51/2.63/14.1822.98/5.88/18.5617.69/2.72/14.30.
24.95/5.07/19.7320.22/3.49/16.2925.22/5.49/19.9326.85/6.86/21.7218.42/3.27/14.9227.39/7.26/22.15.
b-4/meteor/r-l.3.87/9.73/29.342.84/7.78/26.584.69/9.95/29.763.55/7.73/28.15.
3.49/10.68/30.483.03/9.13/28.914.29/11.00/30.663.38/10.33/31.213.25/8.78/29.574.20/10.80/30.59.
1.89/8.70/23.981.03/6.99/20.462.45/8.67/23.851.52/7.26/20.66.
1.61/9.24/25.010.30/5.14/16.581.79/9.37/24.961.68/9.06/25.100.00/4.59/15.721.72/9.25/25.07.
b-4.
31.6127.2533.0728.28.
26.4827.2827.5227.2128.0428.23.table 2: results on four text generation datasets.
bold values represent the maximum values of each column inthe nag pattern..models.
xsum.
squad 1.1.rocstories wmt14 (de-en).
pospd 105 (1.00×)cmlm 132 (0.79×)107 (0.98×)disco.
69 (1.00×)110 (0.62×)105 (0.66×).
66 (1.00×)74 (0.89×)76 (0.87×).
105 (1.00×)172 (0.61×)168 (0.63×).
table 3: inference speed (ms/sample) comparisons on four text generation datasets..it is obvious that the adoption of sequence-levelknowledge distillation limits the performance ofnag models in these case.
more interestingly, inquestion generation, the ag model outperformsthe nag models with blue-4 by 0.4/0.5 (row 3vs. row 5/row 8), but knowledge distillation de-grades nag models’ performance with bleu-4by 0.46/0.13 (row 5 vs. row 6, row 8 vs. row 9).
3. pospd does not bring signiﬁcant extra timein constraining nag models’ generation whiledecoding.
pospd maintains its advantage in high-speed inference across all data sets.
for example,on the dataset squad 1.1, the inference latency ofpospd is much lower than nag models (1.00× vs.0.62×/0.66×).
meanwhile, on the wmt14 (de-en) that has the longest average length of the targetsentence, pospd still maintains its advantage inthe inference speed.
therefore, our pospd couldconstrain the nag model with the negligible extratime, since pospd and the nag model predictsequences (i.e., pos sequence and target sentence)in parallel..4.6 further discussions.
there is a loose ending towards the discussion ofour pospd solution.
in this section, we conductdiscussions to shed light on other interesting prop-erties of pospd.
the discussions are guided bythe following three research questions:q1: how does pospd alleviate the multimodalityproblem?.
q2: is it time-consuming to build the pos dataseton the new task?
q3: does multi-tasking learning object help thepos tag prediction?.
4.6.1 discussion on generated results (q1).
to further analyze the role of pospd and thesequence-level knowledge distillation in alleviat-ing the multimodality problem, we conduct furtherstatistical analyses on the generated results of fourdatasets.
considering the multimodality problemusually manifests as repeating or missing tokensin the generation sentences, we use two indicators,i.e., the repetition rate and the total number of to-kens, to quantify them separately.
concretely, werefer to a “single-token repeat” metric (wellecket al., 2020) and deﬁne the repetition rate here asthe percentage of the repeated times between twoadjacent tokens in the total number of tokens in asentence, and then average it over the dataset..the results are shown in table 4, from which wecan see both knowledge distillation and pospd canreduce the repetition rate in nag models on fourdatasets, and they are more effective on xsumdatasets with longer sentences.
while in tokennumbers, using knowledge distillation signiﬁcantlyreduces the number of tokens generated by nagmodels on xsum.
in contrast, using pospd re-markably make the length of generated sentencesby nag models close to the reference without in-creasing the repetition rate..5996squad 1.1.xsum.
wmt14.
rocstories.
repetition / tokens.
reference ≈0.0/140786 ≈0.0/275003.
0.01/67617 ≈0.0/44731.
0.09/-110360.05/-254180.09/+247.
0.05/-243640.06/-307230.02/+1945.
0.15/-26160.06/-526430.07/+4570.
0.14/-46410.06/-526430.09/-10871.
0.01/-19570.03/+12140.01/+1247.
0.01/-19570.01/-20260.01/+1257.
0.06/+690.03/+200580.05/+1297.
0.06/+22340.02/+90200.05/+1408.
models.
metrics.
cmlm+distill+pospd.
disco+distill+pospd.
table 4: statistical analysis of nag models’ generations.
“reference” denotes the target sentence’s reference.
“repetition” and “tokens” represent the repetition rate and tokens number gap between reference and model out-puts, respectively..4.6.2 time cost in building datasets (q2).
4.6.3 multi-task learning strategy (q3).
considering that both pospd and knowledge distil-lation require the processing of the training datasetwhen it comes to a new task/dataset (i.e., build-ing the pos data set for pospd / regenerating thetraining set for knowledge distillation), we furtheranalyze the time consumption of the two process-ing steps.
as shown in table 5, pospd has asigniﬁcant advantage over knowledge distillationin the time consuming of dataset building.
espe-cially on the larger dataset wmt14 (de-en), itcan save even more time in building datasets, whichis beneﬁcial for rapid deployment on new tasks..method.
distill.
pospd.
samples.
squad 1.1rocstoriesxsumwmt14.
1086402185044258.
45 (24.1×)49 (8.2×)240 (7.71×)5220 (8.48×).
75k90k200k4500k.
table 5: time cost (seconds) comparisons in rebuildingdatasets on four datasets.
the batch size are 100 and 1for the knowledge distillation and pospd..models.
squad 1.1.xsum.
metrics.
b-4/meteor/r-l.r-1/r-2/r-l.cmlm 3.49/10.68/30.484.18/10.80/31.04mt w/o4.29/11.00/30.66mt w/.
24.95/5.07/19.7325.12/5.24/19.9125.22/5.49/19.93.
discomt w/omt w/.
3.38/10.33/31.214.17/10.56/31.054.20/10.80/30.59.
26.85/6.86/21.7227.00/6.89/21.8127.39/7.26/22.15.
table 6: the ablation study on using multi-task learn-ing strategy in pospd’s training stage.
“mt w/o” and“mt w/” denote training the pos predictor in pospdwith/without the multi-tasking learning, respectively..in this part, we analyze the impact of using a multi-task learning strategy in pospd’s training stage.
for lack of space, we take the ablation study ontwo datasets of different sizes, i.e., squad 1.1and xsum.
the results are shown in table 6. in-terestingly, predicting the pos sequence directlyfrom the original sentence (i.e., “pospd w/o”) canalso improve the performance of the nag models.
more importantly, multi-task learning strategy canimprove the performance of pospd in two datasetswith a tiny increase in model parameters (only onelinear layer).
meanwhile, it is only used duringthe pospd’s training stage and does not affect theinference time of pospd..5 conclusion.
in this paper, we revisit the role of the knowledgedistillation in alleviating the multimodality prob-lem of nag.
in brief, we experimentally reﬂectthat the basic assumption of these knowledge distil-lation methods, the ag model is superior to nagmodel, does not always hold for all text genera-tion tasks.
to alleviate the multimodality problem,we show a different solution by incorporating lin-guistic structure into nag.
extensive experimentsdemonstrate that our pospd signiﬁcantly and con-sistently improves the nag models in effectivenessand computational efﬁcacy..as we tentatively give a successful implemen-tation of leveraging one of the simplest linguisticstructures to beneﬁt the nag models in inference,such paradigm deserves a closer and more detailedexploration.
thus in the future, we will investi-gate to make the nag models enjoy the beneﬁtsof incorporating diverse and abundant linguisticstructures in a more superior way.
in addition, our.
5997experimental results suggest that future work mightneed to consider wider ranges of generation tasksinstead of only machine translation when assessingthe performance of nag models..acknowledgments.
this work was supported in part by the na-tional key rd program of china under grant2020yfb1406702, in part by nfsc under grant61625204, 61836006, and the science and tech-nology major project of sichuan province undergrant 2020yfg0478..references.
douglass cutting, julian kupiec, jan pedersen, andpenelope sibun.
1992. a practical part-of-speechtagger.
in third conference on applied natural lan-guage processing, pages 133–140..michael j. denkowski and alon lavie.
2014. me-teor universal: language speciﬁc translation eval-in proceedings ofuation for any target language.
the ninth workshop on statistical machine trans-lation, wmt@acl 2014, june 26-27, 2014, balti-more, maryland, usa, pages 376–380.
the associ-ation for computer linguistics..akiko eriguchi, yoshimasa tsuruoka, and kyunghyuncho.
2017. learning to parse and translate improvesin proceedings of theneural machine translation.
55th annual meeting of the association for compu-tational linguistics, acl 2017, vancouver, canada,july 30 - august 4, volume 2: short papers, pages72–78.
association for computational linguistics..theodoros evgeniou and massimiliano pontil.
2004.regularized multi–task learning.
in proceedings ofthe tenth acm sigkdd international conference onknowledge discovery and data mining, pages 109–117..xiaocheng feng, zhangyin feng, wanlong zhao,bing qin, and ting liu.
2020. enhanced neuralmachine translation by joint decoding with wordand pos-tagging sequences.
mob.
networks appl.,25(5):1722–1728..michel galley, jonathan graehl, kevin knight, danielmarcu, steve deneefe, wei wang, and ignaciothayer.
2006. scalable inference and training ofin pro-context-rich syntactic translation models.
ceedings of the 21st international conference oncomputational linguistics and 44th annual meet-ing of the association for computational linguistics,pages 961–968..marjan ghazvininejad, vladimir karpukhin, lukezettlemoyer, and omer levy.
2020. aligned crossentropy for non-autoregressive machine translation.
in proceedings of the 37th international conference.
on machine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 3515–3523.
pmlr..marjan ghazvininejad, omer levy, yinhan liu, andluke zettlemoyer.
2019. mask-predict: paralleldecoding of conditional masked language models.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 6111–6120. association for computational linguistics..jiatao gu, james bradbury, caiming xiong, vic-tor o. k. li, and richard socher.
2018. non-in 6thautoregressive neural machine translation.
international conference on learning representa-tions, iclr 2018, vancouver, bc, canada, april 30- may 3, 2018, conference track proceedings.
open-review.net..junliang guo, xu tan, linli xu, tao qin, enhongchen, and tie-yan liu.
2020a.
fine-tuning by cur-riculum learning for non-autoregressive neural ma-chine translation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 34, pages7839–7846..junliang guo, linli xu, and enhong chen.
2020b.
jointly masked sequence-to-sequence model for non-in pro-autoregressive neural machine translation.
ceedings of the 58th annual meeting of the associa-tion for computational linguistics, pages 376–385..jungo kasai, james cross, marjan ghazvininejad, andjiatao gu.
2020a.
non-autoregressive machinetranslation with disentangled context transformer.
in proceedings of the 37th international conferenceon machine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 5144–5155.
pmlr..jungo kasai, nikolaos pappas, hao peng, james cross,and noah a. smith.
2020b.
deep encoder, shallowdecoder: reevaluating the speed-quality tradeoff inmachine translation.
corr, abs/2006.10369..yoon kim and alexander m. rush.
2016. sequence-level knowledge distillation.
in proceedings of the2016 conference on empirical methods in naturallanguage processing, emnlp 2016, austin, texas,usa, november 1-4, 2016, pages 1317–1327.
theassociation for computational linguistics..zhuohan li, zi lin, di he, fei tian, tao qin, li-wei wang, and tie-yan liu.
2019. hint-basedtraining for non-autoregressive machine translation.
in proceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 5707–5712. association for computational linguistics..5998chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..dayiheng liu, jie fu, pengfei liu, and jiancheng lv.
2019. tigs: an inference algorithm for text inﬁllingwith gradient search.
in acl..dayiheng liu, jie fu, yidan zhang, chris pal, andjiancheng lv.
2020a.
revision in continuous space:unsupervised text style transfer without adversariallearning.
in aaai..dayiheng liu, yeyun gong, jie fu, yu yan, jiushengchen, jiancheng lv, nan duan, and ming zhou.
2020b.
tell me how to ask again: question dataaugmentation with controllable rewriting in contin-in proceedings of the 2020 confer-uous space.
ence on empirical methods in natural languageprocessing, emnlp 2020, online, november 16-20,2020, pages 5798–5810.
association for computa-tional linguistics..dayiheng liu, yu yan, yeyun gong, weizhen qi,hang zhang, jian jiao, weizhu chen, jie fu, linjunshou, ming gong, pengcheng wang, jiusheng chen,daxin jiang, jiancheng lv, ruofei zhang, winniewu, ming zhou, and nan duan.
2020c.
glge: anew general language generation evaluation bench-mark.
corr, abs/2011.11928..jinglin liu, yi ren, xu tan, chen zhang, tao qin,zhou zhao, and tie-yan liu.
2020d.
task-level cur-riculum learning for non-autoregressive neural ma-in proceedings of the twenty-chine translation.
ninth international joint conference on artiﬁcial in-telligence, ijcai 2020, pages 3861–3867.
ijcai.org..yang liu, qun liu, and shouxun lin.
2006. tree-to-string alignment template for statistical machinein proceedings of the 21st interna-translation.
tional conference on computational linguistics and44th annual meeting of the association for compu-tational linguistics, pages 609–616, sydney, aus-tralia.
association for computational linguistics..xuezhe ma, chunting zhou, xian li, graham neu-big, and eduard h. hovy.
2019. flowseq: non-autoregressive conditional sequence generation within proceedings of the 2019 con-generative ﬂow.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing, emnlp-ijcnlp 2019, hong kong, china, november 3-7,2019, pages 4281–4291.
association for computa-tional linguistics..nasrin mostafazadeh, nathanael chambers, xiaodonghe, devi parikh, dhruv batra, lucy vanderwende,pushmeet kohli, and james f. allen.
2016. a cor-pus and cloze evaluation for deeper understandingof commonsense stories.
in naacl hlt 2016, the2016 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, san diego california,.
usa, june 12-17, 2016, pages 839–849.
the associ-ation for computational linguistics..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, brussels, belgium, october 31 -november 4, 2018, pages 1797–1807.
associationfor computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
naacl-hlt 2019: demonstrations..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, july 6-12, 2002, philadelphia,pa, usa, pages 311–318.
acl..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100, 000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in nat-ural language processing, emnlp 2016, austin,texas, usa, november 1-4, 2016, pages 2383–2392.
the association for computational linguistics..qiu ran, yankai lin, peng li, and jie zhou.
2020.learning to recover from multi-modality errors fornon-autoregressive neural machine translation.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 3059–3069.
associa-tion for computational linguistics..chitwan saharia, william chan, saurabh saxena, andmohammad norouzi.
2020. non-autoregressive ma-chine translation with latent alignments.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing, emnlp 2020, on-line, november 16-20, 2020, pages 1098–1108.
as-sociation for computational linguistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, acl 2016, august 7-12, 2016, berlin, ger-many, volume 1: long papers.
the association forcomputer linguistics..zhiqing sun, zhuohan li, haoqing wang, di he,zi lin, and zhi-hong deng.
2019. fast structureddecoding for sequence models.
in advances in neu-ral information processing systems 32: annual con-ference on neural information processing systems2019, neurips 2019, december 8-14, 2019, vancou-ver, bc, canada, pages 3011–3020..5999raymond hendy susanto, shamil chollampatt, andliling tan.
2020. lexically constrained neural ma-chine translation with levenshtein transformer.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 3536–3543.
associa-tion for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..sean welleck, ilia kulikov, stephen roller, emily di-nan, kyunghyun cho, and jason weston.
2020. neu-ral text generation with unlikelihood training.
in8th international conference on learning represen-tations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..shuangzhi wu, dongdong zhang, nan yang, mu li,sequence-to-dependencyand ming zhou.
2017.in proceedings of theneural machine translation.
55th annual meeting of the association for compu-tational linguistics, acl 2017, vancouver, canada,july 30 - august 4, volume 1: long papers, pages698–707.
association for computational linguis-tics..david yarowsky and grace ngai.
2001. inducing mul-tilingual pos taggers and np bracketers via robustprojection across aligned corpora.
in second meet-ing of the north american chapter of the associa-tion for computational linguistics..chunting zhou, graham neubig, and jiatao gu.
understanding knowledge distillation incorr,.
2019.non-autoregressive machine translation.
abs/1911.02727..6000