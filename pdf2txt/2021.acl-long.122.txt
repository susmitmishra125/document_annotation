factuality assessment as modal dependency parsing.
jiarui yao1, haoling qiu2, jin zhao1, bonan min2, and nianwen xue1.
1brandeis university{jryao, jinzhao, xuen}@brandeis.edu2raytheon bbn technologies{haoling.qiu, bonan.min}@raytheon.com.
abstract.
as the sources of information that we consumeeveryday rapidly diversify, it is becoming in-creasingly important to develop nlp tools thathelp to evaluate the credibility of the informa-tion we receive.
a critical step towards thisgoal is to determine the factuality of eventsin text.
in this paper, we frame factuality as-sessment as a modal dependency parsing taskthat identiﬁes the events and their sources, for-mally known as conceivers, and then deter-mine the level of certainty that the sourcesare asserting with respect to the events.
wecrowdsource the ﬁrst large-scale data set an-notated with modal dependency structures thatconsists of 353 covid-19 related news arti-cles, 24,016 events, and 2,938 conceivers.1we also develop the ﬁrst modal dependencyparser that jointly extracts events, conceiversand constructs the modal dependency structureof a text.
we evaluate the joint model againsta pipeline model and demonstrate the advan-tage of the joint model in conceiver extractionand modal dependency structure constructionwhen events and conceivers are automaticallyextracted.
we believe the dataset and the mod-els will be a valuable resource for a whole hostof nlp applications such as fact checking andrumor detection..1.introduction.
“we’re not just ﬁghting an epidemic; we’re ﬁghtingan infodemic.”.
— tedros adhanom, who.
the ongoing covid-19 pandemic taught us theimportance of determining factuality of events at atime when the sources of media we consume havegreatly diversiﬁed.
this is compounded by thefact that the information that we receive from these.
1https://github.com/jryao/modal_.
dependency.
news sources usually does not go through as a rig-orous editing and veriﬁcation process as traditionalmedia do.
the sheer volume of the new media con-tent makes human veriﬁcation impossible and thereis thus an increasing need for nlp tools that helpverify statements made in these media sources..to verify if an event has indeed happened we ﬁrstneed to determine the level of certainty with whichthe event is asserted by the information source,which defaults to the author of a document butcan also be another source in the text that the au-thor attributes the information to.
the factuality ofan event cannot be fully determined without alsotaking into account the credibility of informationsource.
consider the text snippet in (1):.
(1) wbur: a man in his 20s from worcestercounty tested positive tuesday for the new,apparently more contagious coronavirus vari-ant, public health ofﬁcials said.
the variantwas ﬁrst detected in the united kingdom, andexperts have warned that it could soon be-come widespread in the u.s..suppose our goal here is to determine the fac-tuality of the statements in (1).
we ﬁrst need todetermine the level of certainty with which thesource is committed to the factuality of the state-ments.
while the “public health ofﬁcials” are fairlycertain that a man from worcester county testedpositive for the coronavirus variant, the “experts”were not as certain that the virus variant will deﬁ-nitely become widespread, as indicated by the lin-guistic cues like “could”.
in other words, thereare different levels of certainty with which the twoevents are asserted.
in addition, the credibility ofinformation source is also crucially important whenevaluating the factuality of the events (de marn-effe et al., 2012).
if the information source is not“public health ofﬁcials” and instead it is an anony-mous source, the information that the worcester.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1540–1550august1–6,2021.©2021associationforcomputationallinguistics1540man tested positive will be less credible.
in fact, thefactuality of the events also depends on the wbur,the “author” of this text.
if the author made upthese statements, then the worcester man testingpositive would not be a fact, regardless of the levelof certainty with which the events are asserted.
ul-timately, it is impossible to fully determine thecredibility of a source purely based on the informa-tion within a single text, but linking each event toits source or chain of sources allows us to verifythe factuality of the event against other sources andour world knowledge.
therefore, identifying thelevel of certainty with which is an event is assertedtogether with its source is a crucial ﬁrst step inassessing the factuality of the event..previous work on factuality assessment has fo-cused on determining the level of certainty thatis asserted on events and framed it as a classi-ﬁcation or regression problem (saur´ı and puste-jovsky, 2012; lee et al., 2015; stanovsky et al.,2017; rudinger et al., 2018; qian et al., 2018).
however, as we discussed above, the level of cer-tainty alone is insufﬁcient in determining the fac-tuality of an event.
in this work, we adopt a factu-ality representation framework proposed in (viguset al., 2019) called modal dependency structure(mds).
a modal dependency structure is formallya document-level structure where nodes are eventsand sources, known as conceivers while edges rep-resent the modal strength, or the level of certaintythat the conceiver holds towards an event.
figure 1shows the modal dependency structure of the textin (1)..root.
modal.
auth (wbur).
pos.
pos.
pos.
pos.
pos.
public health ofﬁcials.
experts.
warned.
said.
detected.
pos.
tested positive.
neut.
become widespread.
ble.
for example, in figure 1, the factuality of thetested positive event depends on both the credibilityof “public health ofﬁcials”, as well as the author(auth) of this text..representing factuality as a modal dependencystructure also allows us to cast factuality assess-ment as a modal dependency parsing problem, andopens up the door for using various structured pre-diction approaches to tackle this problem.
since nolarge-scale data annotated with mds exists, we ﬁrstneed to develop a data set annotated with modaldependency structures to train and evaluate modaldependency parsing models.
the main contribu-tions of this work are as follows:.
• we construct a large corpus annotated withmodal dependency structures via crowdsourc-ing.
it consists of 353 covid-19 related newsarticles, in which 24,016 events and 2,938conceivers are annotated.
to our best knowl-edge, this corpus is the ﬁrst large-scale corpusannotated with modal dependency structures.
• although modal dependency structure is acomplicated representation, we show that ourdata set is annotated with high consistency.
we believe the crowdsourcing techniques wehave developed will add to the knowledge ofhow to develop large-scale data sets via crowd-sourcing, especially for complicated represen-tations..• we develop a joint modal dependency pars-ing model that extracts events, conceivers andparses a document into its modal dependencystructure.
we present experimental resultsthat show the effectiveness of our parsing ap-proach and the consistency of the data set.
inaddition, we evaluate the joint model againstthe pipeline model, and show the advantageof the joint model in overall end-to-end modaldependency parsing performance..figure 1: a modal dependency tree for example 1..set.
2 acquiring a modal dependency data.
one main advantage of mds over previous ap-proaches to factuality is that an mds explicitlyrepresents both the conceiver and the event andrepresents the modal strength as the level of cer-tainty that the conceiver holds towards the events.
it is also a hierarchical structure that allows nestedrepresentations when multiple sources are possi-.
in this section we ﬁrst provide additional detail forthe modal dependency structure, and then presentour strategy of decomposing the modal dependencystructure into subtasks that are suitable for crowd-sourcing.
we also evaluate the quality of our an-notated data set, and provide statistics relevant fortraining mds parsing models..15412.1 modal dependency structure.
the modal dependency structure builds on the anno-tation scheme of factbank (saur´ı and pustejovsky,2009) and is inspired by the structured approach intemporal dependency annotation in zhang and xue(2018b).
like factbank, the modal dependencystructure combines epistemic strength full, partial,neutral with polarity values positive, negative todeﬁne a set of six values for modal strength.
ta-ble 1 shows the modal strength values (i.e.
labels)used in modal dependency structures and their cor-responding values in factbank.
as illustrated infigure 1, these values are represented as edge la-bels in the modal dependency structure.
readersare referred to (vigus et al., 2019) for how thesesix values are deﬁned..modal dependencyfull positive (pos)partial positive (prt)neutral positive (neut)neutral negative (neutneg)partial negative (prtneg)full negative (neg).
factbankct+pr+ps+ps-pr-ct-.
table 1: modal strength values in the modal depen-dency structure and factbank..while vigus et al.
(2019) show that modal de-pendency structures (mds) can be annotated withhigh inter-annotator agreement by expert annota-tors in a pilot annotation of six documents, a corpusthat is much larger in scale is needed in order totrain modal dependency parsers that can be usedfor downstream applications..2.2 crowdsourcing modal dependency.
structures.
to make mds feasible for crowdsourcing, we haveadopted a number of strategies.
the ﬁrst strategy isto decompose mds annotation into four subtasks:event identiﬁcation, event attachment, event modalstrength annotation and modal superstructure con-struction.
the instructions to crowd-workers foreach subtask are piloted to ensure that the subtaskcan be performed with high consistency beforethey are set up for productive annotation.
sec-ond, where possible, we have applied a numberof pre-processing steps to simplify the tasks forcrowd-workers.
in addition, we have also adopted apayment structure to incentivize high-quality work.
in all subtasks, we require three crowd-workers.
to complete one assignment and use the majorityvote answer as the ﬁnal decision.
all annotationsare conducted on the amazon mechanic turk plat-form..task 1: event identiﬁcation event annotationinvolves identifying event trigger words, whichare typically verbs and nouns.
we ﬁrst extractevent candidates using a publicly available, com-mon event trigger word dictionary.2 we then ranthe stanford corenlp dependency parser (man-ning et al., 2014) on raw text to extract the verbsand the root of each syntactic dependency parseas candidate events.
a pilot study shows that 90%of the verbs in the extracted candidate events areevent triggers, so we decide to treat all verbs asevent triggers and launch an event identiﬁcationtask for about 10k non-verb event candidates.
wepresent crowd-workers with event candidates andask them to decide if they are events..task 2: event attachment the next subtaskis to attach a child event to a parent, which canbe a conceiver (2a), another event (2b), or in thecase of hypothetical situations, an abstract have-condition node (2c).
to simplify things for crowdworkers, we made the decision not to introduce ab-stract nodes in the modal dependency tree.
eventsin hypothetical situation are annotated as neutralevents and attached to their conceivers directly..a child event is attached to a parent event onlywhen the parent event is a modal predicate.
forexample, in (2b), the parent of visit is wants.
themodal predicates form a closed set and can be ex-tracted with a list of modal event triggers.
using adependency parser, we can reliably identify eventsthat should be attached to modal events, so we cando this part of the annotation without solicitingjudgments from crowd workers..(2) a. a 72-year-old man died, the police said..pos (died, police).
b. john wants to visit japan..neut (visit, wants).
c. if it rains tomorrow, i will stay at home..neut (have-condition, author)pos (rains, have-condition).
in the majority of cases, the parent of an eventis a conceiver (or the author), as in (2a), wherethe conceiver of died is the police.
for any given.
2https://github.com/jryao/temporal_.
dependency_graphs_crowdsourcing.
1542event, the list of candidate conceivers can be verylarge, so some ﬁltering is needed to shrink it downso that a smaller list of candidates is presented tocrowd-workers..to collect possible conceivers, we ﬁrst constructa list of common conceiver-introducing predicates(cips) following saur´ı and pustejovsky (2009) andvigus et al.
(2019).
then, we extract possible con-ceivers with the stanford corenlp parser fromthree sources: the subject of common cips in ourlist, the subject of all other events, and named enti-ties that are possible conceivers, such as organiza-tion, person.
for each event, we limited the candi-date conceivers to those in the same paragraph asthe event, and further ﬁlter out unlikely conceiversby their hypernyms in wordnet.3 we present a listof possible conceivers and ask workers to select themost appropriate one for the event in question..task 3: event modal strength annotation af-ter attaching the events to their parent, the third taskis to annotate modal strength from the conceiver’sperspective, which are edge labels in modal de-pendency structures.
vigus et al.
(2019) deﬁnesix modal strength values listed in table 1. in ourpilot annotation, however, we found partial nega-tive and neutral negative events only account forless than 2% of all events.
to have a manageablecrowdsourcing task and given their low frequency,we decide to merge partial negative and neutralnegative events to negative events, and only usefour labels: full positive, partial positive, neutralpositive and negative..the event modal strength task is annotated intwo steps.
in the ﬁrst step, events are classiﬁed intothree classes: full positive, negative, and neither.
inthe second step, events in the third class are furtherclassiﬁed into partial positive and neutral positive.
for example, (3a) is annotated as a full positiveevent, (3c) is annotated as a negative event, and(3b) is annotated as neither in the ﬁrst step.
(3b)is further labeled as a neutral positive event in thesecond step..(3) a. the dog barked..b. the dog might have barked.
c. the dog didn’t bark..task 4: conceiver superstructure construc-tion the parent of a conceiver is the author inthe majority of cases, but it could also be another.
3https://wordnet.princeton.edu.
conceiver in some cases.
(4a) and (4b) are twocommon cases where the parent of a conceiver isanother conceiver.
in (4a), the conceiver mary andthe embedded conceiver john are in the same sen-tence, and in (4b), the conceiver john is in quotedspeech.
for the cases like (4b), the two conceiversare not necessarily in the same sentence, but theyare usually close to each other in the text..(4) a. mary says john wants to visit japan..pos (john, mary).
b.
“john wants to visit japan.
he wants to.
go next summer.” mary says.
pos (john, mary).
conceivers that don’t have any neighboring con-ceivers are directly attached to the author.4 for therest, we design a conceiver attachment task similarto the event attachment task.
the modal strengthof conceivers is decided by the modal strength oftheir cips, which is available after task 3. forthe conceivers that don’t have an associated cip,such as named entities, we ask crowd-workers toannotate their modal strength..2.3 quality control strategy.
our basic quality control strategy involves usingtwo tests to select crowd-workers: a qualiﬁcationtest and a survival test.
workers need to ﬁrst passthe qualiﬁcation test in order to be eligible forworking on a task.
in addition, test questions withground truth answers are embedded in each hit.
workers need to maintain a high cumulative ac-curacy through the annotation process to remaineligible for the task.
for the event identiﬁcationsubtask, our qualiﬁcation test threshold and sur-vival test threshold are both set to 80% accuracy.
for the event attachment task, they are both set to70% as it is a more challenging task..we also developed a stratiﬁed payment approachto incentivize high-quality work.
there is no guar-antee that workers who have passed the qualiﬁca-tion test will continue to perform well in the actualannotation task.
to incentivize high-quality anno-tation, we adopt a stratiﬁed payment approach forevent modal strength annotation.
we offer a basepayment of $ 0.01 per question, and increase it to $0.02 if the worker achieves a 70% accuracy on thetest questions in that hit, and further increase it to$ 0.03 if the worker achieves a 90% accuracy.
the.
4in practice, if there is no other conceivers in the same.
paragraph, we attach that conceiver to the author..1543additional payment is paid using the bonus featureon amazon’s mechanical turk..2.4 annotation evaluation.
we measure annotation quality with two metrics.
first, we compute the agreement among crowd-workers using worker agreement with aggregate(wawa) (ning et al., 2018), which measures theaverage agreement between each crowd-workerand the aggregate answer.
second, we comparecrowd-workers’ annotation with the annotation ofan expert annotator and compute the f-score..table 2 presents the wawa scores for each sub-task.
the statistics show good agreement amongcrowd-workers for all subtasks, with a moderatelylower agreement for task 4, the construction of theconceiver superstructure..wawa.
task 1 task 2 task 3 task 478.084.4.
92.7.
88.9.table 2: agreement scores among crowd-workers..we also evaluate the agreement between the ma-jority opinion of crowd-workers and the expert an-notator with an 11-document subset that are an-notated by both the expert annotator and crowd-workers.
in this evaluation, we attempt to measurethe agreement between the crowd-sourced modaldependency structures and the modal dependencystructures annotated by the expert annotator.
af-ter assembling the modal dependency structuresfrom the full annotation pipeline, we also report theoverall agreement between crowd-workers and theexpert annotator in table 3. our overall unlabeledattachment agreement (uaa) is 78.6%, labeledattachment agreement (laa) is 72.1%..since we decompose the mds annotation intosmaller steps, the annotation of an earlier step willaffect that of a later step.
for instance, the resultsof the event identiﬁcation step (task 1) are used asinput to set up the event attachment (task 2) andmodal strength annotation (task 3).
an incorrectannotation in task 1 will “propagate” to the othertasks that are based on the event annotation.
table3 presents the agreement statistics for the subtasks.
it is important to note that the agreement statisticsfor the subtasks include disagreements in eventidentiﬁcation and are thus generally a bit lowerthan the stand-alone tasks..metricf1uaa (f1 )laa (f1 ).
ev id event92.7.conc.
all.
78.871.7.
80.077.3.
78.672.1.table 3: agreements between crowd-workers and theexpert annotator.
“ev id” refers to event identiﬁcation,uaa and laa refer to unlabeled and labeled attach-ment agreement respectively..2.5 corpus statistics.
we downloaded the coronavirus news data set us-ing aylien api.5 we sampled 353 news articlesfrom 11 media sources, including business stan-dard, business insider, nbc news, the new yorktimes, reuters, the guardian, the washingtonpost, cnn, fox news, yahoo news and wikinews.
as shown in table 4, our mds data set has24,016 events and 2,938 conceivers, a much largercorpus than factbank (saur´ı and pustejovsky,2009), which has 208 articles and 9,488 events.
a more detailed breakdown shows that for event at-tachment annotation, 29% events are attached to anon-author conceiver, and 66% events are attachedto the author.
the rest of the events either have aunspeciﬁed conceiver or an event as parent..mdsfactbank.
doc conc2,938353-208.event24,0169,488.table 4: number of documents, conceivers, and eventsin this corpus and factbank..3 neural modal dependency parsing.
in this section, we introduce our parser for modaldependency parsing.
our modal dependency parseris inspired by zhang and xue (2018a), who in-troduce a ranking model for temporal dependencyparsing.
as the temporal dependency tree usedto train their model is similar in structure to ourmodal dependency tree, it is reasonable to adopttheir model as the starting point.
our model is alsoinspired by ross et al.
(2020), who extend zhangand xue (2018a) by replacing the bi-lstm en-coder with contextualized neural language models,such as bert (devlin et al., 2019).
speciﬁcally,our modal dependency parser constructs a modal.
5https://aylien.com/blog/free-coronavirus-news-dataset.
1544dependency tree by incrementally identifying theparent node for each child node in textual order.
for each child node, the parser ranks the candidateparent nodes and selects the one with the highestscore as its parent node.
since the nodes in a modaldependency tree are events or conceivers, to parse atext into a modal dependency tree, we need to ﬁrstextract the events and conceivers, then build themodal dependency structure.
since zhang and xue(2018a)’s pipeline system suffers from error prop-agation, we adopt a multi-task learning approachthat jointly trains the event and conceiver extractionand structure building components..3.1 model description.
figure 2 shows the model architecture.
given an in-put text, we obtain the token representation wk foreach token by encoding the text with a pre-trainedbert encoder (devlin et al., 2019).
to ﬁt the longdocument to bert, we treat one document as abatch, and encode it sentence by sentence.
thiscontextualized token representation is shared bythe mention extraction stage and structure buildingstage.
we then label the k-th token with a bio tag-ger by mapping wk to a tag logit using a standardfeed-forward neural network.
in our experiment,we use a single tagger to extract both events andconceivers because recognizing certain events suchas reporting events (e.g.
said) might be helpful toextract conceivers.
in the structure building stage,the goal is to ﬁnd the most appropriate parent foreach event and conceiver node.
in theory, everynode in the document can be a candidate parentfor a given child node.
to reduce the search space,we only consider candidate parent nodes withina 5-sentence window of the child node plus twometa nodes (the author and root nodes) as candi-date parents and include at most n candidate par-ents for each child.
the representation for nodexi is the concatenation of the start token represen-tation, the end token representation, and the spanrepresentation of the node.
following zhang andxue (2018a), we use an attention vector (bahdanauet al., 2016) computed over the tokens in node spani as its span representation.
let wt be the tokens innode i, the span representation ˆxi is computed asfollowing:.
αt = ffnα(wt).
ai,t =.
exp[αt]k=start(i) exp[αk].
(cid:80)end(i).
figure 2: model architecture for the joint model.
theinput is a document, which is a list of sentences.
xi isthe child node.
for simplicity, we consider xa, xb, xc,xd as the candidate parent nodes..ˆxi =.
ai,t · wt.
end(i)(cid:88).
t=start(i).
the pair representation of node i and one of itscandidate parent y(cid:48)i is the concatenation of theirnode representations.
the pair score is computedby sending the pair representation of node i and y(cid:48)ito a feed-forward neural network:.
si,y(cid:48).
i,lk = ffnp([xi, yi(cid:48)]).
where ffnp denotes the feed-forward neural net-work that computes a pair score.
si,y(cid:48)i,lk representsthe score of node y(cid:48)i being the parent of node i withthe modal relation label lk..after computing all the pair scores for the nodei, we concatenate them into a vector ci.
then asoftmax function is applied to get a probabilitydistribution for the node i having each candidateparent (with each relation).
we use cross-entropyloss for both mention extraction and dependencyparsing.
we optimize the following joint loss dur-ing training:.
l = le + λlp.
1545where le and lp refer to extraction loss and pars-ing loss respectively, and λ is a hyper-parametercontrolling weights between extraction and pars-ing..3.2 experiments and discussion.
data split out of the 353 documents in our mdsdataset, we use 289 of them as training data, 32as validation data, and 32 as testing data in our ex-periments.
the test set includes the 11 documentsthat are annotated by the expert annotator.
table5 shows the data split.
the expert annotator alsoadjudicated some of the more challenging aspectsof mds annotation to improve the quality of thevalidation and test sets..doc2893232.event conceiver19,5412,3072,168.
2,344298296.traindevtest.
table 5: data split for the experiments..implementation details we use bert-large-cased (wolf et al., 2020) for all experiments.
foreach child, we include at most n = 16 candidateparent nodes.
our hyper-parameter settings can befound in the appendix..results we evaluate our joint model against thepipeline model.
the pipeline model trains the eventand conceiver mention extractor separately fromthe structure building component without sharingthe bert encoder.
we use micro-average f1 scoreas the evaluation metric, and for the mention ex-traction task, an event or conceiver is only correctlyidentiﬁed if there is an exact match between theextracted mention with the gold mention..as we can see in table 6, the pipeline modeland joint model achieve similar results on eventextraction, indicating that event extraction does notbeneﬁt from a joint model.
this shows that eventextraction can by and large be extracted indepen-dently without taking into account their relations totheir conceivers.
however, the joint model outper-forms the pipeline model in conceiver extractionby 0.2% and 2.9% on the development and test setrespectively.
this improvement is consistent withthe observation that conceiver extraction is closelyrelated to the structure building stage of mds pars-ing because an entity (e.g.
a person or organization)is a conceiver only if it serves as the conceiver of an.
event or another conceiver in the structure buildingstage.
not all entities in a text are conceivers.
inboth models, the conceiver extraction scores arelower than the event extraction scores due to thescarcity of conceivers in the data set..when evaluating the performance of the struc-ture building component of the parsing model withgold mentions as input (the parsing (gold) column),the pipeline model achieves slightly higher scoresthan the joint model.
however, when using theautomatically extracted events and conceivers fromthe mention extraction stage as input to the struc-ture building stage (the parsing (auto) column), ina setting that really matters for downstream appli-cations, the joint model outperforms the pipelinemodel on both the development and test set by 0.6%and 1.5% respectively.
this shows that the jointmodel reduces inconsistent predictions between themention extraction and structure building stagesresulting from a pipeline model not sharing param-eters, and improves the overall result..3.3 error analysis.
since the 11-document subset of the test set areannotated by both the expert and crowd-workers,we can conduct a comparative error analysis of thesystem and crowd-workers and see if they make thesame mistakes.
for this particular analysis, we fo-cus on the structure building stage with gold eventand conceiver mentions as input.
we only look atwhether a child event or conceiver is attached to itscorrect parent..in the majority of cases, the author node is theconceiver of a child node.
however, ﬁnding thenon-author conceiver for a child is more revealingabout the effectiveness of the model.
so we focuson nodes whose correct conceivers are not the au-thor, and evaluate both crowd-workers’ annotationand the system output against that of the expertannotator..in this subset of the test set, 317 nodes havea non-author conceiver as parent.
among these,crowd-workers disagree with the expert annotatorin 102 cases, while the system disagrees with theexpert annotator in 122 cases (the last row of ta-ble 7).
this shows this is a challenging aspectof the annotation for both crowd-workers and thesystem, with the system performing worse thancrowd-workers..out of the 317 nodes, 59 of them have the con-ceiver in a different sentence while the remaining258 have the conceiver in the same sentence.
we.
1546event idtestdev90.992.792.890.8.conc idtest67.570.4∗.
dev70.971.1.parsing (gold)dev80.779.4.test80.6∗80.1.parsing (auto)dev69.770.3.test67.569.0∗.
pipelinejoint.
table 6: comparison of the pipeline model and the joint model.
the last two columns show the results of using theautomatically predicted mentions as input to the parsing stage.
all parsing scores are labeled attachment scores.
scores with a star are signiﬁcantly better than the other model’s scores on test data with a p-value < 0.05..same sentdiff senttotal.
instances25859317.workers84 (32.6%)18 (30.5%)102 (32.2%).
system91 (35.3%)31 (52.5%)122 (38.5%).
table 7: errors in crowd-workers’ annotation and sys-tem output..can see from table 7 that among those where thechild is in the same sentence as the parent, thesystem and crowd-workers disagree with the ex-pert annotator to a similar extent, 32.6% vs 35.3%.
however, for cases where the child is in a differentsentence from its parent, there is a much biggerdifference in their disagreement with the expertannotator (30.5% vs. 52.5%).
this shows thatwhile crowd-workers can identify conceivers froma different sentence just as easily as from the samesentence, it is much more difﬁcult for the system toattach a child node to a distant conceiver.
address-ing this challenge will be crucial to further improvethe performance of the model..4 related work.
factuality annotation while there is a signiﬁ-cant amount of research on annotating factualityor modality (saur´ı and pustejovsky, 2009; diabet al., 2009; matsuyoshi et al., 2010; soni et al.,2014; lee et al., 2015; prabhakaran et al., 2015;minard et al., 2016), factuality and opinions (sonet al., 2014), senses of modal verbs (ruppenhoferand rehbein, 2012), and credibility in social me-dia (mitra and gilbert, 2015), a few of them areparticularly related to our work.
our annotation isclosely related to factbank saur´ı and pustejovsky(2009) in that both annotate the level of certaintythat the source asserts over an event, but factbankdoes not explicitly represent their relations in ahierarchical structure and is annotated by expertannotators.
like our work, lee et al.
(2015) alsoannotate event factuality via crowdsourcing, butthey only annotate the level of certainty from theperspective of the author, to the exclusion of non-.
author conceivers.
our work is based on viguset al.
(2019), who ﬁrst came up with the model de-pendency annotation scheme.
however, they onlyannotate 377 events from 6 documents in a pilot ef-fort with expert annotators.
we have shown that itis feasible for crowd-workers to annotate modal de-pendency structures with considerable consistencyand produce modal dependency annotation at scale..automatic factuality assessment existing worktypically casts factuality assessment as a classiﬁca-tion or regression problem.
for example, saur´ı andpustejovsky (2012) and prabhakaran et al.
(2015)adopt rule-based and feature-based machine learn-ing approaches to factuality classiﬁcation.
morerecently, qian et al.
(2018) predict event factual-ity via a generative adversarial networks basedmodel.
rudinger et al.
(2018) design two lstmbased models, and pouran ben veyseh et al.
(2019)use a graph-based neural network model for eventfactuality prediction.
our work departs from pre-vious practice and recasts factuality assessment asmodal dependency parsing to simultaneously pre-dict the source and its level of certainty over anevent, and exposes both for downstream applica-tions..5 conclusion and future work.
in this paper, we proposed a novel approach to fac-tuality assessment by casting it as a modal depen-dency parsing problem.
we ﬁrst built a large dataset annotated with modal dependency structuresvia crowdsourcing, and demonstrated the qualityof this data set with a careful evaluation of each as-pect of the annotation.
we then developed the ﬁrstmodal dependency parser, adopting a joint learningapproach to alleviate error propagation, and demon-strated its advantage over the pipeline approach inan end-to-end evaluation.
future work involvesfurther improving the accuracy of the parser andapplying the parser to perform large-scale factualityassessments of events in news media..1547acknowledgments.
we thank the anonymous reviewers for their helpfulcomments..this work is supported in part by a grant fromthe iis division of national science foundation(award no.
1763926) entitled “building a uniformmeaning representation for natural language pro-cessing”.
all views expressed in this paper arethose of the authors and do not necessarily repre-sent the view of the national science foundation.
this research is based upon work supported inpart by the ofﬁce of the director of national intel-ligence (odni), intelligence advanced researchprojects activity (iarpa), via contract no.
: 2021-20102700002. the views and conclusions con-tained herein are those of the authors and shouldnot be interpreted as necessarily representing theofﬁcial policies, either expressed or implied, ofodni, iarpa, or the u.s. government.
the u.s.government is authorized to reproduce and dis-tribute reprints for governmental purposes not with-standing any copyright annotation therein..ethical considerations.
for our annotation, we use publicly available datasources, partly from wikinews and partly from apublicly available data set that was aggregated, an-alyzed, and enriched by aylien using aylien’snews intelligence platform..we use the amazon mturk platform to collectthe annotation, a common practice in the nlp com-munity.
in the annotation task 1, 2 and 4, we pay$ 0.02 ∼ $ 0.03 per question.
in task 3, we adopta stratiﬁed payment approach to incentivize high-quality work.
we pay $ 0.01 ∼ $ 0.03 per question,based on the quality of the annotation.
we believeour crowd workers are fairly compensated.
theexpert annotator is one of the authors of this paper..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2016. neural machine translation by jointlylearning to align and translate..marie-catherine de marneffe, christopher d man-ning, and christopher potts.
2012. did it happen?
the pragmatic complexity of veridicality assessment.
computational linguistics, 38(2):301–333..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
deep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..mona diab, lori levin, teruko mitamura, owen ram-bow, vinodkumar prabhakaran, and weiwei guo.
2009. committed belief annotation and tagging.
in proceedings of the third linguistic annotationworkshop (law iii), pages 68–73, suntec, singa-pore.
association for computational linguistics..kenton lee, yoav artzi, yejin choi, and luke zettle-moyer.
2015. event detection and factuality as-sessment with non-expert supervision.
in proceed-ings of the 2015 conference on empirical meth-ods in natural language processing, pages 1643–1648, lisbon, portugal.
association for computa-tional linguistics..christopher d. manning, mihai surdeanu, john bauer,jenny finkel, steven j. bethard, and david mc-closky.
2014. the stanford corenlp natural lan-guage processing toolkit.
in association for compu-tational linguistics (acl) system demonstrations,pages 55–60..suguru matsuyoshi, megumi eguchi, chitose sao,koji murakami, kentaro inui, and yuji matsumoto.
2010. annotating event mentions in text with modal-in proceed-ity, focus, and source information.
ings of the seventh international conference on lan-guage resources and evaluation (lrec’10), val-letta, malta.
european language resources associ-ation (elra)..anne-lyse minard, manuela speranza, ruben urizar,bego˜na altuna, marieke van erp, anneleen schoen,and chantal van son.
2016. meantime, thenewsreader multilingual event and time corpus.
inproceedings of the tenth international conferenceon language resources and evaluation (lrec’16),pages 4417–4422, portoroˇz, slovenia.
europeanlanguage resources association (elra)..t. mitra and e. gilbert.
2015. credbank: a large-scalesocial media corpus with associated credibility anno-tations.
in icwsm..qiang ning, hao wu, and dan roth.
2018. a multi-axis annotation scheme for event temporal relations.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 1318–1328, melbourne, aus-tralia.
association for computational linguistics..amir pouran ben veyseh, thien huu nguyen, and de-jing dou.
2019. graph based neural networks forevent factuality prediction using syntactic and se-in proceedings of the 57th an-mantic structures.
nual meeting of the association for computationallinguistics, pages 4393–4399, florence, italy.
asso-ciation for computational linguistics..1548meagan vigus, jens e. l. van gysel, and williamcroft.
2019. a dependency structure annotationin proceedings of the first interna-for modality.
tional workshop on designing meaning representa-tions, pages 182–198, florence, italy.
associationfor computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..yuchen zhang and nianwen xue.
2018a.
neural rank-ing models for temporal dependency structure pars-ing.
in proceedings of the 2018 conference on em-pirical methods in natural language processing,pages 3339–3349, brussels, belgium.
associationfor computational linguistics..yuchen zhang and nianwen xue.
2018b.
structured in-terpretation of temporal relations.
in proceedings ofthe eleventh international conference on languageresources and evaluation (lrec 2018), miyazaki,japan.
european language resources association(elra)..a hyperparameters.
we optimize our model with bertadam for 50epochs with a learning rate of 2e-5 and weight de-cay of 0.01. in the parsing stage, we tried λ valuesin {1.0, 0.5, 0.2} and chose λ = 0.2 for the parsingloss weight based on the end-to-end performance.
we use a dropout rate of 0.1 on bert’s output.
foreach child, we include at most n = 16 candidateparent nodes.
we run our experiments on a 16 gbgpu.
it takes about 21 minutes for the joint modelto run one epoch..b annotation interface.
we present our annotation interface in this section.
the task design of task 4 (modal superstructureconstruction) is similar to task 2 (event attach-ment).
we give detailed instructions to crowd work-ers, and explain each choice with examples..vinodkumar prabhakaran, michael bloodgood, monadiab, bonnie dorr, lori levin, christine d. piatko,owen rambow, and benjamin van durme.
2015.statistical modality tagging from rule-based annota-tions and crowdsourcing..zhong qian, peifeng li, yue zhang, guodong zhou,and qiaoming zhu.
2018. event factuality identiﬁ-cation via generative adversarial networks with aux-iliary classiﬁcation.
in ijcai..hayley ross, jonathon cai, and bonan min.
2020. ex-ploring contextualized neural language models forin proceedings oftemporal dependency parsing.
the 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 8548–8553, online.
association for computational lin-guistics..rachel rudinger, aaron steven white, and benjaminvan durme.
2018. neural models of factuality.
inproceedings of the 2018 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 1 (long papers), pages 731–744, neworleans, louisiana.
association for computationallinguistics..josef ruppenhofer and ines rehbein.
2012. yes wecan!?
annotating english modal verbs.
in proceed-ings of the eighth international conference on lan-guage resources and evaluation (lrec’12), pages1538–1545, istanbul, turkey.
european languageresources association (elra)..r. saur´ı and j. pustejovsky.
2009. factbank: a cor-pus annotated with event factuality.
language re-sources and evaluation, 43:227–268..roser saur´ı and james pustejovsky.
2012. are yousure that this happened?
assessing the factuality de-gree of events in text.
computational linguistics,38(2):261–299..c. v. son, m. erp, antske fokkens, and p. vossen.
2014. hope and fear: interpreting perspectives byintegrating sentiment and event factuality.
in lrec2014..sandeep soni, tanushree mitra, eric gilbert, and jacobeisenstein.
2014. modeling factuality judgments inin proceedings of the 52nd an-social media text.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 415–420, baltimore, maryland.
association for compu-tational linguistics..gabriel stanovsky, judith eckle-kohler, yevgeniypuzikov, ido dagan, and iryna gurevych.
2017. in-tegrating deep linguistic features in factuality pre-diction over uniﬁed datasets.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages352–357..1549figure 3: annotation interface for event identiﬁcation..figure 4: annotation interface for event attachment..figure 5: annotation interface for event modal strength annotation, step 1..figure 6: annotation interface for event modal strength annotation, step 2..1550