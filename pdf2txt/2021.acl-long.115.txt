towards table-to-text generation with numerical reasoning.
lya hulliyyatus suadaa1, hidetaka kamigaito1, kotaro funakoshi1,manabu okumura1 and hiroya takamura1,21tokyo institute of technology2national institute of advanced industrial science and technology (aist)lya@stis.ac.id{kamigaito,funakoshi,oku}@lr.pi.titech.ac.jptakamura@pi.titech.ac.jp.
abstract.
recent neural text generation models haveshown signiﬁcant improvement in generatingdescriptive text from structured data such astable formats.
one of the remaining impor-tant challenges is generating more analyticaldescriptions that can be inferred from facts ina data source.
the use of a template-based gen-erator and a pointer-generator is among the po-tential alternatives for table-to-text generators.
in this paper, we propose a framework consist-ing of a pre-trained model and a copy mecha-nism.
the pre-trained models are ﬁne-tuned toproduce ﬂuent text that is enriched with numer-ical reasoning.
however, it still lacks ﬁdelityto the table contents.
the copy mechanism isincorporated in the ﬁne-tuning step by usinggeneral placeholders to avoid producing hallu-cinated phrases that are not supported by a ta-ble while preserving high ﬂuency.
in summary,our contributions are (1) a new dataset for nu-merical table-to-text generation using pairs ofa table and a paragraph of a table descriptionwith richer inference from scientiﬁc papers,and (2) a table-to-text generation frameworkenriched with numerical reasoning..1.introduction.
recent data-to-text generation studies have shownsigniﬁcant improvement in generating faithful textaligned with data sources.
a copy mechanism hasbeen widely explored to improve faithfulness invarious ways.
wiseman et al.
(2017) used jointprobabilities to let models choose between copy-ing records from data sources or generating froma vocabulary.
puduppully et al.
(2019) improveda similar approach by modeling entity represen-tations as a unit of copying.
this approach hasproven to be effective in generating descriptive textthat explicitly mentions facts from sources..however, as introduced by chen et al.
(2020a),humans have the ability to produce more analyti-.
figure 1: example of table and description in numericnlg dataset..cal text with richer inference, including numericalreasoning.
making inferences beyond texts is stillan open question due to the limitation of languagemodels in handling numeric operations.
in thisstudy, we further encourage research by elaborat-ing numerical tables to initialize the ability to injectreasoning while maintaining high ﬂuency..our contributions are summarized as follows..• we introduce a new dataset for table-to-text generation focusing on numerical rea-soning.
the dataset consists of textual de-scriptions of numerical tables from scientiﬁcpapers.
our dataset is publicly available onhttps://github.com/titech-nlp/numeric-nlg..• we adopt template-guided text generation(kale and rastogi, 2020a) for a table-to-textgeneration task and propose injecting pre-executed numerical operations in the templateto guide numerical-reasoning-based text gen-eration.
we compare different types of tem-plates for table representations in pre-trainedmodels..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1451–1465august1–6,2021.©2021associationforcomputationallinguistics1451modelprecisionrecallf1our full model89.682.285.7lee et al.
(2018)86.283.784.9table 2: the overall mention detection results on the test set of ontonotes.target headerour full modeldescriptiontable 2 shows the mention detection results on the test set.
similar tocoreference linking results, our model achieves higher precision and f1 score, which indicates that our model can significantly reduce false positive mentions while it can still find a reasonable number of mentions.
• we propose a copy mechanism for pre-trainedmodels, that uses general placeholders cover-ing table contents and results of pre-executednumerical operations to avoid fact hallucina-tion..• we conduct experiments with current state-of-the-art neural generation models and a sim-ple template-based system to demonstrate thechallenges and opportunities for future re-search on text generation with numerical rea-soning..2 related work.
the power of tables in presenting data efﬁcientlyfurther encourages research done by exploring thetables as data sources in natural language tasks,such as table-to-text generation (liang et al., 2009;wiseman et al., 2017; lebret et al., 2016; parikhet al., 2020), table question answering (pasupat andliang, 2015; wang et al., 2018), and table-basedfact veriﬁcation (chen et al., 2020b; gupta et al.,2020).
recent research on the table-to-text gen-eration task is starting to generate text with morereasoning.
murakami et al.
(2017) explored stockprices to generate market comments by adding gen-eralization tags of possible arithmetic operationsto cover mathematical reasoning.
nie et al.
(2018)proposed operation-guided attentions by exploringthe results of pre-executed numerical operations.
the dataset closest to ours is logicnlg, by chenet al.
(2020a), who ﬁrst introduced logical text gen-eration using open-domain tables with unknownschemas.
different from our target text for gen-eration, which consists of several sentences in aparagraph, they proposed a task of generating onlyone sentence from selected table contents..3 numerical table-to-text dataset.
we created numericnlg, a new table-to-textdataset focusing on a text generation task with nu-merical reasoning.
we collected table descriptionsfrom scientiﬁc papers, that are naturally producedby experts with richer inference..3.1 dataset creation.
data acquisition we constructed a table-to-textdataset based on numerical tables of experimentalresults, extracted from pdf ﬁles of scientiﬁc pa-pers on the acl anthology website,1 introduced.
by suadaa et al.
(2021).
then, we collected can-didates for corresponding descriptions from thesource ﬁles using pdfminer.2 we used table num-bers in their captions as keywords for the collection.
an example of a table and its description is shownin figure 1..data cleansing and annotation extracted ta-ble descriptions can be noisy since they may con-tain only table numbers without any sentences de-scribing table facts.
we hired experts in the com-puter science ﬁeld to clean and annotate the ex-tracted descriptions in the following steps:.
• examine tables and their corresponding de-scriptions and then recommend only the de-scriptions that have at least one sentence rep-resenting numerical facts in the table..• categorize each sentence of the recommendeddescription into three fact-checking classes:data description, supporting description, andnot-related-to-table description.
as a ﬁnaldataset, we used only sentences classiﬁed asbelonging to the data description category toreduce fact hallucination..• identify a content plan of table description byselecting part of table headers which directlystated or logically inferred in the description,called target header.
for example, refer to thetable description shown in figure 1, “our fullmodel” is selected as the target header..we used the same split of training, validation,and test sets as the source table dataset (suadaaet al., 2021)..3.2 dataset comparison.
table 1 provides a comparison of numericnlgwith other related table-to-text datasets.
the ro-towire (wiseman et al., 2017) dataset consistsof summaries of nba basketball games contain-ing several paragraphs, paired with their corre-sponding box-score tables.
since rotowirehas only 39 record types, each table contains sim-ilar record types with limited schemas.
althoughmost of the rotowire table contents are in nu-merical values, the summaries contain only a fewnumerical-reasoning sentences, such as a compari-son of scores between two basketball teams.
whileour dataset consists of closed domain articles as.
1https://www.aclweb.org/anthology/.
2http://pypi.python.org/pypi/pdfminer/.
1452rotowirelogicnlgnumericnlg.
tables4.9k7.3k1.3k.
examples unit of desc.
4.9k document.
37.0k sentence1.3k paragraph.
vocab.
11.3k122.0k19.6k.
token/desc.
domain.
inferencefewrichscientiﬁc rich.
sport33711 open94.schemaknownunlimitedunlimited.
table 1: dataset comparison..with rotowire, it is of shorter text (a paragraph)and with unlimited table schemas..chen et al.
(2020a) introduced the logicnlgdataset to facilitate the study of table-to-text gen-eration tasks with richer inference.
the datasetcontains unlimited schemas of open-domain tablescrawled from wikipedia, paired with ﬁve annotatedsentences covering different logical inferences.
al-though most inferences are numerical reasoning,the table contents are not fully numeric..similar in motivation to logicnlg in gener-ating text that can be logically entailed by factsin tables, numericnlg consists of collections ofparagraphs that are naturally produced by humanexperts in scientiﬁc papers, paired with their corre-sponding numerical tables.
our dataset has fewertables than logicnlg, focusing on numerical-reasoning text in the scientiﬁc domain..4 table representation.
due to rotowire’s limited schemas, wisemanet al.
(2017) viewed a table input as a set of records(entity, value, type), where the entity and the typeare the extracted row and column names, respec-tively.
because of the unlimited table schemas inour dataset, by capturing the original table struc-ture in real-world tables, this paper uses the repre-sentations which consist of captions, row headers,column headers, cell values, and metrics, called adata table.
using only descriptive facts from thedata table as input representations is sufﬁcient togenerate descriptive texts that explicitly mentionfacts in the table.
however, since we intend to pro-duce more analytical text with numerical reason-ing, we propose adding inferred facts to the inputrepresentation by computing a set of arithmetic op-erations on the data table beforehand, deﬁned as apre-executed operation table..data table we view t as a set of cells with theircorresponding row header (rh), column header(ch), numerical value (val), and metric-type (m),deﬁned as a data table (td).
a data table for theexample in figure 1 consists of rh: ((model, ourfull model), (model, lee et al.
(2018))); ch: ();val: ((89.6, 82.2, 85.7), (86.2, 83.7, 84.9); and m:.
(precision, recall, f1).
since our tables are anno-tated with a targeted header as a content plan fortable descriptions, we mark cells correspondingto the targeted header with a target ﬂag (tgt) tohighlight the marked cells in text generation.
weset tgt = 1 for targeted cells and tgt = 0 fornon-targeted cells.
in this study, we preprocess theheader name by concatenating the row and columnheaders (h = [rh; ch]) and keep information aboutthe header category by extracting overlapping to-kens of row and column headers as th.
as a re-sult, we deﬁne td = (hij, thij, valij, mij, tgtij),where 1 ≤ i ≤ nr, 1 ≤ j ≤ nc; nr and nc are thenumbers of rows and columns, respectively..pre-executed operation table we provide a ta-ble of pre-executed cell operations (top ) by do-ing mathematical operations only on targeted cellsto limit the calculation.
in this study, we covermaximum, minimum, and difference operations.
examples of a preprocessed table, data table, andpre-executed operation table are shown in figure2..linearized table supporting transfer learningof pre-trained transformers to our table-to-text gen-eration task, we prepare a linearized table pt as aninput representation so that it similar to the repre-sentation that encoder has seen during pre-training.
t is converted to a ﬂat string pt = w1, ..., w|pt |,similar to that used in many prior work (wanget al., 2020; chen et al., 2020a; kale and rastogi,2020b), where wi denotes the i-th word in para-graph pt with length |pt |.
in this study, we adoptthe template-based input representation, introducedby kale and rastogi (2020a), to handle representa-tion bias between a structured data t and a naturallanguage utterance pt , where pt is generated us-ing a manually deﬁned template.
we propose notonly covering data table td in the template but alsoinjecting the pre-executed numerical operations oftable t through top to guide numerical-reasoning-based text generation.
we consider four differentmethods3 for converting t into sequences, the lasttwo being our contributions..3an example is shown in table 6 in the appendix..1453figure 2: examples of preprocessed table, data table, and pre-executed operation table..1. naive representation.
t is simply ﬂattened into a sequence ignoringits table structure by concatenating captions,headers, metrics, and targeted cell values:.
caption: <table id> <caption>.
rowname: <rh1> .
.
.
column<ch1> .
.
.
name:met-ric:<m1>, ..., <mnr/nc>.
value: <val1.1>.
.
.
<valnr.nc> ..<rhnr>..<chnc>..this naive representation omits the rela-tion between rows and columns.
note that<table id> is extracted from the captionto support table mentioning in generating ta-ble descriptions..2. data-based template (td temp).
t is transformed into a natural language sen-tence by scanning each row of td with tgt =1 to ﬁll a manually deﬁned template:.
<caption>.
<table id>shows<m1.1> of <h1.1> is <val1.1> .
.
.
<mnr.nc> of <hnr.nc> is <valnr.nc>..this representation covers the semantics ofdata in the original table..3. reasoning-based template (top temp).
mathematical operation arguments and resultsfrom top are injected in this representationto cover the numerical reasoning of data in the.
original table.
we deﬁne hop and valop as aheader and a value of an operation result re-spectively, where op ={max, min, diff}.
spe-ciﬁc to the difference operation, hdif f 1 andhdif f 2 refer to the ﬁrst and second header ar-guments, respectively.
then, t is representedby concatenating the templatized representa-tion for each row of top :.
largest.
showsthe.
<caption>.
<table id><mmax><hmax>has(<valmax>) of <thmax>.
<hmin>has the smallest <mmin> (<valmax>)<mdif f > of <hdif f 1> isof <thmin>.
larger/smaller than <hdif f 2>..4. data and reasoning-based template (td +.
top temp)t is converted by combining templatized sen-tences of td and top .
this representationcovers both data and their numerical reason-ing..5 generation models.
the task is to generate text by translating tablerepresentation pt into table description y =y1, y2, ..., yn.
we apply a series of generation mod-els to solve the proposed task.
while our focus isprimarily on pre-trained models since they havebeen most widely used for limited data settings,.
1454header namevalmetric (m)targeththour full modelmodel89.6precision1our full modelmodel82.2recall1our full modelmodel85.7f11lee et al.
(2018)model86.2precision0lee et al.
(2018)model83.7recall0lee et al.
(2018)model84.9f10target header: our full modelmetric-type: precision, recall, f1op nameop argumentsmetricresulththhvalmaxour full model, lee et al.
(2018)modelprecisionour full model89.6maxour full model, lee et al.
(2018)modelf1our full model85.7maxour full model, lee et al.
(2018)modelrecalllee et al.
(2018)83.7minour full model, lee et al.
(2018)modelrecallour full model82.2minour full model, lee et al.
(2018)modelprecisionlee et al.
(2018)86.2minour full model, lee et al.
(2018)modelf1lee et al.
(2018)84.9diffour full model, lee et al.
(2018)modelprecision3.4diffour full model, lee et al.
(2018)modelrecall-1.5diffour full model, lee et al.
(2018)modelf10.8data table (td)pre-executed operation table (top)precisionrecallf189.682.285.786.283.784.9table 2: the overall mention detection results on the test set of ontonotes.modelour full modelmodellee et al.
(2018)figure 3: placeholder alignment in copy-based pre-trained model..like ours, we also include a template-based genera-tor and a pointer-generator network as baselines..5.1 non-pre-trained models.
design.
template-based generator weadomain-speciﬁc template-based generator cov-ering two types of sentences in producing tabledescriptions:table referring sentences and datadescription sentences.
since our task focuseson numerical-reasoning descriptions, we deﬁnetemplatized sentences using maximum records intable top :.
<table id> shows <caption>.
we can seethat <hmax> outperforms other <thmax> with<valmax> of <mmax>..pointer-generator pointer-generator (see et al.,2017) is a sequence-to-sequence model with atten-tion and a copy mechanism.
this model copes withthe out-of-vocabulary problem in data-to-text gen-eration by jointly copying from source texts andgenerating from a vocabulary..5.2 pre-trained models.
fine-tuned gpt2 gpt2 (radford et al., 2019)is a pre-trained language model with a decoder-onlytransformer architecture.
we ﬁne-tuned the gpt2model by using table representation pt as a preﬁxof our input.
speciﬁcally, we fed the concatenationof table representation pt and table description yto the model and generated y .
in the inferencephase, we used only pt as the input to generate ˆystarting after the last token of pt ..fine-tuned t5 t5 (raffel et al., 2020) is a pre-trained transformer model with an encoder-decoderarchitecture, that solves natural language tasks byconverting into a text-to-text format.
we ﬁne-tunedthe t5 model in our dataset by adding a “summa-rize” preﬁx to table representation pt producingoutput ˆy ..copy mechanism pre-trained language modelshave proven their effectiveness in handling the openvocabulary problem through subword tokenization.
supported by attention layers of the transformerin their architecture, the models learn to attendto source inputs while generating target texts insubword units.
however, pre-trained generatorsoften produce texts that are not aligned to tablesources.
in this study, we propose strengtheningtheir copying ability by incorporating a copy mech-anism into the pre-trained models.
although a copymechanism based on pointer-generator (see et al.,2017) was used for pre-trained models (chen et al.,2020c) and is well-known in the community, itcannot maintain the global logical structure of sen-tences with richer inference.
we instead employeda simpler copy mechanism based on placeholders(murakami et al., 2017) with more speciﬁc tagsthan in chen et al.
(2020a).
we further propose aranking-based placeholder alignment algorithm, asillustrated in figure 3..first, we align entities and numbers in y withthe data tables td and pre-executed arithmetic op-eration results top by using string matching.
thealignment starts from the ﬁrst row to the last rowof top .
if no matched token is found, it continues.
1455<table_id> shows that <header_max> achieves higher <metric_max> and <metric_max>-score.table 2 shows that our full model achieves higher precisionand f1-score.topof table 2op…mh resultvalresultmaxprecisionour full model89.6maxf1our full model85.7…difff10.8td of table 2hthvalmtargetour full modelmodel89.6precision1our full modelmodel82.2recall1…lee et al 2018model84.9f10table alignmentfine-tuning phaseas shown in table 2, our full model achieves higher precisionand f1.as shown in <table_id>, <header_max> achieves higher <metric_max> and <metric_max>.placeholder memory                           actions                                sourcesstep 0: none                                   <table_id> → table 2                            captionstep 1: none                                   <header_max> → our full model      1st max in topstep 2: <metric_max>: precision   <metric_max> → precision           found in memory<value_max>: 89.6            step 3: <value_max>: 89.6             <metric_max> → f1                         2nd max in topselect from placeholder memory or table sourcesy:𝒀𝒕𝒆𝒎𝒑∶𝒀#𝒕𝒆𝒎𝒑:𝒀#:inference phasemodel.
template-basedpointer-generator (naive)fine-tuned gpt2 (naive)fine-tuned gpt2 (td temp)fine-tuned gpt2 (top temp)fine-tuned gpt2 (td + top temp)fine-tuned gpt2 (naive) + copyfine-tuned gpt2 (td temp) + copyfine-tuned gpt2 (top temp) + copyfine-tuned gpt2 (td + top temp) + copyfine-tuned t5 (naive)fine-tuned t5 (td temp)fine-tuned t5 (top temp)fine-tuned t5 (td + top temp)fine-tuned t5 (naive) + copyfine-tuned t5 (td temp) + copyfine-tuned t5 (top temp) + copyfine-tuned t5 (td + top temp) + copy.
bleu rouge-l meteor bertscore86.8876.3885.12*84.68*85.66*85.40*78.73*77.76*73.83*70.8787.6487.68*87.1787.34*86.37*86.12*86.52*86.54.
15.827.8218.84*17.1018.8519.14*6.94*6.43*4.42*5.4718.94*20.1118.8518.4618.4918.2318.6819.16.
26.9715.2623.722.97*25.39*25.13*11.66*11.23*9.40*9.6229.7130.2528.6329.13*27.40*27.08*28.02*28.15.
2.822.803.063.014.635.051.291.361.181.224.255.024.994.835.144.965.245.45.parent17.151.406.566.537.728.05*2.45*2.10*0.91*1.5513.0915.0912.2512.78*12.47*11.65*11.96*12.95.table 2: experimental results of different models with various types of table representations and proposed copymechanism.
scores with asterisk * symbol were signiﬁcantly different from those of naive models under wilcoxontest (p < 0.05)..to the rows of td.
we set a higher rank to topthan td in the alignment since we focus on logi-cal text generation.
then, we replace the matchedtokens with corresponding placeholders4 in a tem-platized description ytemp.
as depicted in figure3, since “our full model” in sentence y is matchedwith the header result of the maximum operation,we replace it with <header max> placeholder.
during the ﬁne-tuning phase, instead of directlygenerating y , the models learn to produce a tem-platized description ytemp including placeholdersas well as words..in the inference phase, we design a ranking al-gorithm with a placeholder memory to select thebest-replaced tokens for placeholders of a predictedtemplatized description ˆytemp in producing a gener-ated description ˆy .
we deﬁne a set of values in thesame row of source tables as a content set and pri-oritize replacing placeholders in one sentence withthe same content set, ensuring sentence coherence.
a content set of td is a tuple of header, metric, andvalue.
for top , a content set consists of header,metric, and value of the operation results.
speciﬁcto the difference operation, we add the header ofthe ﬁrst and second arguments to the content setsince the header arguments are important to captureentity comparison in a sentence..we utilize a placeholder memory to temporarilysave prioritized placeholder candidates from thesame content set that is previously chosen.
for.
4details of placeholders and their deﬁnition are in tables.
7 and 8 in the appendix..example, as shown in figure 3, after replacing theheader max placeholder with the header resultfrom the ﬁrst row of maximum records of topin step 1, the related placeholders from the samecontent set (metric max and value max) areadded to the placeholder memory as higher-rankedcandidates in the searching space.
the placeholdermemory is reset to empty in the following sentenceof ˆytemp and the alignment starts again from thenext content set of table sources..6 experiments.
we conducted experiments on the proposed datasetto evaluate the performance of the text generationmodels and verify the effectiveness of the approachof using different table representations..6.1 automatic evaluation metrics.
we used bleu (papineni et al., 2002), rouge-l(lin, 2004), and meteor (banerjee and lavie,2005) to evaluate the informativeness of generatedtexts.
we computed the bertscore (zhang et al.,2020) to assess the similarity between the gener-ated texts and the ground-truth table descriptionsby using contextualized token embeddings of pre-trained bert (devlin et al., 2019), which havebeen shown to be effective for paraphrase detection.
considering both references and table contents, wealso used the parent metric, proposed by dhin-gra et al.
(2019).
in our experiments, we modiﬁedthe parent calculation by adding noun phrasesof table captions as table contents and used only.
1456targeted table contents for table sources..6.2.implementation details.
we trained a pointer-generator model using theadagrad optimizer with a batch size of 8 and alearning rate of 0.15. for ﬁne-tuning the gpt2model, the adam optimizer set weight decay to3 × 10−5.
following raffel et al.
(2020), the t5model was ﬁne-tuned with a constant learning rateof 0.001. we trained all models for a maximum often epochs with early stopping based on the lossscore on the validation set (patience of 3).
at thetime of decoding, the generated text was producedthrough a beam search of size 5..7 results.
7.1 automatic evaluation.
table 2 shows our experimental results.
the ﬁne-tuned t5 models performed better than the oth-ers in terms of bleu, rouge-l, meteor, andbertscore.
the slightly lower parent of thebest ﬁne-tuned t5 model than the template-basedgenerator implies that the ﬁne-tuned t5 model wasalso comparable in terms of generating related ta-ble descriptions.
the pointer-generator model hadthe lowest score since our dataset consists of lim-ited table collections with a broad vocabulary andchallenging target texts..effect of table representation comparing theperformance between table representation types inthe pre-trained models, we can see a different ten-dency between gpt2 and t5.
the more similar thetable representation used as an input, the higher thescore of gpt2.
since gpt2 had only a decoder, theinputs including reasoning-based templates (topand td + top ), which are more similar to our tar-get with numerical reasoning, performed the bestfor several metrics with more than 1 point improve-ment.
in t5 with an encoder-decoder architecture,on the contrary, there was only a slight marginbetween different table representations.
this in-dicates that the encoder part of t5 can capturetable contexts from various input templates.
forvariants without a copy mechanism, t5 with onlydata representation (td) outperformed the otherrepresentation types with longer sentences for allmetrics.
because of the gap between the encoderand decoder, t5 still had difﬁculty aligning theinformation of longer inputs and outputs..effect of copy mechanism the worst scores ofthe ﬁne-tuned gpt2+copy models indicate thatour proposed copy mechanism failed to learnthe templatized target patterns in the ﬁne-tuningstep.
the decoder-only gpt2 could not handlethe sparse distributions of target texts with place-holders.
conversely, the copy-based ﬁne-tuned t5models achieved a better bleu score due to theirencoder and decoder ability in handling output textswith placeholders..7.2 qualitative analysis.
table 3 shows table descriptions generated by thetemplate-based, pointer-generator, and ﬁne-tunedpre-trained models (gpt2 and t5), using data andreasoning-based templates5 for our table examplein figure 2. we marked sentences related to tablecaptions in green, correct facts based on table con-tents in blue, and incorrect facts in red.
in this study,since we had a limited training set with a broadervocabulary, the pointer-generator model tended toresult in repetitive words and failed to generatewell-described descriptions.
the pre-trained mod-els, gpt2 and t5, generated more natural descrip-tions.
while several pieces of text generated bygpt2 included numerical facts, they used numbersthat were not extracted from table contents.
thet5 models produced descriptions that were morerelated to table contents than gpt2..considering our lengthy output examples in ta-ble 3, unlike the ﬁne-tuned gpt2 model, whichgenerated longer sentences,the ﬁne-tuned t5model generated shorter sentences than the refer-ences.6 the length gap between the referencesand outputs of the ﬁne-tuned t5 model affectedthe f1-based metrics of rouge-l, meteor,bertscore, and parent.
note that bleu isa precision-based metric that can handle shorteroutputs through a brevity penalty (papineni et al.,2002).
therefore, we assume that bleu better rep-resents the performance of the ﬁne-tuned t5 modelthan the other metrics..7.3 human evaluationwe conducted a human evaluation7 to better as-sess the quality of the generated text.
we com-pared our copy-based ﬁne-tuned t5 model with.
5examples using other table representations are shown in.
table 9 in the appendix..6average token length of references: 80.57, gpt2: 87.39,.gpt2+copy: 73.58, t5: 39.81, t5+copy: 41.81..7the interfaces are shown in figures 4 and 5 in the ap-.
pendix..1457modelreference.
template-based.
pointer-generator.
fine-tuned gpt2(td + top temp).
fine-tuned gpt2(td + top temp) + copy.
fine-tuned t5(td + top temp)fine-tuned t5(td + top temp) + copy.
generated texttable 2 shows the mention detection results on the test set.
similar to coreference linkingresults, our model achieves higher precision and f1 score, which indicates that our modelcan signiﬁcantly reduce false positive mentions while it can still ﬁnd a reasonable numberof mentions.
table 2 shows the overall mention detection results on the test set of ontonotes.
we cansee that our full outperforms other model with 85.7 f1.
the table 2 shows the results of the results of the results of the results of the the art of theart of...the table shows the recall performance with our full model.
the result of our full modelis 88.7, which is comparable with the 89.3 performance of our full model but still betterthan the 89.9 and 89.2 performance in both the f1, prec and full models.
we also ﬁndthat our full model does not perform very well when compared against a full one, with89.4% and 89.4% recall and 89.2% recall respectively.
we can also ﬁnd that our fullmodel is slightly inferior in terms of recall.
table 2 : ( the - d model : a ) the : aa .
the: the, the: and, the: the: the,the and, theand:the and, theand: theand: theand, theand: ...(<table id>: table 2; <cat header>: model)table 2 presents the overall mention detection results on ontonotes.
our full modeloutperforms all the state-of-the-art systems in terms of recall and f1 score.
table 2 shows the overall mention detection results on the test set of ontonotes.
ourfull model outperforms the previous state-of-the-art models by a large margin, whichconﬁrms the effectiveness of our proposed approach.
(<table id>: table 2; <header max>: our full model).
table 3: example of generated table description..model.
template-basedpointer-generatorfine-tuned gpt2fine-tuned t5fine-tuned t5 + copy.
descriptive facts.
inferred facts.
#supp1.000.000.030.050.04.
#cont %cont0.010.0097.4654.5550.00.
0.010.001.280.070.04.
#supp0.930.000.430.500.78.
#cont %cont10.640.0081.7868.7542.62.
0.110.001.941.100.57.relevance.
3.891.502.363.513.78.table 4: average number of supporting and contradicting facts in generated table descriptions, percentage ofcontradicting to total facts, and levels of relevance to table captions..modeltemplate-basedpointer-generatorfine-tuned gpt2fine-tuned t5fine-tuned t5 + copy.
gram7.78.conccoher−9.4411.11−72.78 −77.22 −78.3327.7828.8939.4417.2220.5619.44.
31.1118.3315.56.table 5: grammaticality, coherence, and concisenesslevels of table description generators..the template-based, pointer-generator, ﬁne-tunedgpt2, and ﬁne-tuned t5 models.
we did not com-pare it against the copy-based ﬁne-tuned gpt2since gpt2 failed to incorporate our proposed copymechanism.
we used the best table representationwith majority metrics for each model on the basisof the experimental results in table 2..in the ﬁrst study, we evaluated the correctnessof the generated text on the basis of facts in ta-bles.
we randomly selected 30 tables in the test setand elicited responses from three graduate studentsper table.
following wiseman et al.
(2017), theraters were asked to count how many facts in the.
descriptions were supported by numerical data inthe tables and how many were contradicted.
sinceour task covers numerical-reasoning text, we distin-guished descriptive numerical facts from inferrednumerical facts.
we also measured the level of rel-evance of the generated text to the table captionsby using a four-point likert scale (highly relevant,relevant, somewhat relevant, and irrelevant)..the results are shown in table 4. the pointer-generator failed to reﬂect facts due to the widevariety of our table schemas.
while the ﬁne-tunedgpt2 model generated sentences with a largernumber of descriptive and inferred facts than theothers on average, most of the facts were contra-dictive.
the ﬁne-tuned t5 model generated fewersentences than gpt2, with the average number ofinferred facts being larger than that of descriptivefacts.
our model based on the ﬁne-tuned t5 modelwith a copy mechanism reduced the ratio of con-tradictive facts for both descriptive and inferredfacts..following earlier work (puduppully et al., 2019),.
1458we also evaluated text ﬂuency in terms of gram-maticality, coherence, and conciseness by usingbest-worst scaling (bws) (louviere and wood-worth, 1991; louviere et al., 2015).
we divided theoutputs of the ﬁve models into ten pairs of descrip-tions.
we presented workers with two descriptionsand asked them to decide which one is best for eachﬂuency category..the score of each model was calculated by usingthe maxdiff approach (orme, 2009): the numberof times a description was chosen as the best minusthe number of times it was chosen as the worst.
scores range from −100 (absolutely worst) to 100(absolutely best).
we elicited judgments with ama-zon mechanical turk for the 30 descriptions, ratedby 3 participants.
the results are shown in ta-ble 5. most of the pre-trained models achievedbetter scores than the others.
the ﬁne-tuned gpt2model achieved the highest score in terms of gram-maticality and coherence.
the ﬁne-tuned t5 modelachieved the highest score in terms of conciseness.
adding a copy mechanism to the t5 slightly de-creased the grammaticality and conciseness butimproved the coherence..8 conclusion.
we proposed numericnlg, a new dataset for table-to-text generation using a table and its correspond-ing description from scientiﬁc papers, focusing onnumerical-reasoning texts.
even though our pro-posed dataset is not a large-scale table collection,we provided pairs of a table and its rich inferencedescription, that are naturally written by expertsin scientiﬁc papers, supporting further research ontable-to-text generation with numerical reasoning.
we conducted experiments with ﬁne-tuned pre-trained models by using several types of table lin-earization as input representations, comparing witha template-based generator and pointer-generator.
the experiments showed that transfer-learning ofpre-trained language models leads to an improve-ment in our settings, that resulted in more ﬂuenttext while it still lacked ﬁdelity to table contents.
we then proposed incorporating a copy mechanismby using general placeholders to avoid the produc-tion of hallucinated phrases, that are not supportedby tables while preserving high ﬂuency.
eventhough our proposed copy mechanism failed tolearn to generate better outputs in the decoder-onlypre-trained models, we showed that a copy-basedpre-trained model with an encoder-decoder archi-.
tecture leads to a better bleu score and improvescorrectness..acknowledgements.
lya hulliyyatus suadaa is supported by the in-donesian endowment fund for education (lpdp)and the okumura-takamura-funakoshi laboratory,tokyo institute of technology.
this work is par-tially supported by jst presto (grant numberjpmjpr1655).
we thank the anonymous review-ers for their helpful discussion on this work andcomments on the previous draft of the paper..references.
satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in pro-ceedings of the acl workshop on intrinsic and ex-trinsic evaluation measures for machine transla-tion and/or summarization, pages 65–72, ann ar-bor, michigan.
association for computational lin-guistics..wenhu chen, jianshu chen, yu su, zhiyu chen, andwilliam yang wang.
2020a.
logical natural lan-guage generation from open-domain tables.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7929–7942, online.
association for computational lin-guistics..wenhu chen, hongmin wang, jianshu chen, yunkaizhang, hong wang, shiyang li, xiyou zhou, andwilliam yang wang.
2020b.
tabfact : a large-scalein inter-dataset for table-based fact veriﬁcation.
national conference on learning representations(iclr), addis ababa, ethiopia..zhiyu chen, harini eavani, wenhu chen, yinyin liu,and william yang wang.
2020c.
few-shot nlgin proceedingswith pre-trained language model.
of the 58th annual meeting of the association forcomputational linguistics, pages 183–190, online.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..bhuwan dhingra, manaal faruqui, ankur parikh,ming-wei chang, dipanjan das, and william co-hen.
2019. handling divergent reference texts whenevaluating table-to-text generation.
in proceedingsof the 57th annual meeting of the association for.
1459computational linguistics, pages 4884–4895, flo-rence, italy.
association for computational linguis-tics..vivek gupta, maitrey mehta, pegah nokhiz, and viveksrikumar.
2020. infotabs: inference on tables assemi-structured data.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 2309–2324, online.
associationfor computational linguistics..mihir kale and abhinav rastogi.
2020a.
templateguided text generation for task-oriented dialogue.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 6505–6520, online.
association for computa-tional linguistics..mihir kale and abhinav rastogi.
2020b.
text-to-textpre-training for data-to-text tasks.
in proceedings ofthe 13th international conference on natural lan-guage generation, pages 97–102, dublin, ireland.
association for computational linguistics..r´emi lebret, david grangier, and michael auli.
2016.neural text generation from structured data within proceed-application to the biography domain.
ings of the 2016 conference on empirical methodsin natural language processing, pages 1203–1213,austin, texas.
association for computational lin-guistics..percy liang, michael jordan, and dan klein.
2009.learning semantic correspondences with less super-in proceedings of the joint conference ofvision.
the 47th annual meeting of the acl and the 4th in-ternational joint conference on natural languageprocessing of the afnlp, pages 91–99, suntec, sin-gapore.
association for computational linguistics..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..jordan j. louviere, terry n. flynn, and a. a. j. mar-ley.
2015. best-worst scaling: a model for thelargest difference judgments.
in cambridge univer-sity press..jordan j. louviere and george g. woodworth.
1991.best-worst scaling: a model for the largest differ-ence judgments.
in university of alberta: workingpaper..soichiro murakami, akihiko watanabe, akiramiyazawa, keiichi goshima, toshihiko yanase, hi-roya takamura, and yusuke miyao.
2017. learningto generate market comments from stock prices.
in proceedings of the 55th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 1374–1384, vancouver,canada.
association for computational linguistics..feng nie, jinpeng wang, jin-ge yao, rong pan,and chin-yew lin.
2018. operation-guided neu-ral networks for high ﬁdelity data-to-text genera-in proceedings of the 2018 conference ontion.
empirical methods in natural language processing,pages 3879–3889, brussels, belgium.
associationfor computational linguistics..bryan orme.
2009. maxdiff analysis: simple counting,individual-level logit, and hb.
in sawtooth software,inc..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..ankur parikh, xuezhi wang, sebastian gehrmann,manaal faruqui, bhuwan dhingra, diyi yang, anddipanjan das.
2020. totto: a controlled table-to-text generation dataset.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 1173–1186, on-line.
association for computational linguistics..panupong pasupat and percy liang.
2015. compo-sitional semantic parsing on semi-structured tables.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing (volume 1: long papers), pages1470–1480, beijing, china.
association for compu-tational linguistics..ratish puduppully, li dong, and mirella lapata.
2019.indata-to-text generation with entity modeling.
proceedings ofthethe 57th annual meeting ofassociation for computational linguistics, pages2023–2035, florence, italy.
association for compu-tational linguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..lya hulliyyatus suadaa, hidetaka kamigaito, manabuokumura, and hiroya takamura.
2021. metric-type.
1460identiﬁcation for multi-level header numerical tablesin scientiﬁc papers.
in proceedings of the 16th con-ference of the european chapter of the associationfor computational linguistics: main volume, pages3062–3071, online.
association for computationallinguistics..hao wang, xiaodong zhang, shuming ma, xu sun,houfeng wang, and mengxiang wang.
2018. aneural question answering model based on semi-structured tables.
in proceedings of the 27th inter-national conference on computational linguistics,pages 1941–1951, santa fe, new mexico, usa.
as-sociation for computational linguistics..zhenyi wang, xiaoyang wang, bang an, dong yu,and changyou chen.
2020. towards faithful neuraltable-to-text generation with content-matching con-in proceedings of the 58th annual meet-straints.
ing of the association for computational linguistics,pages 1072–1086, online.
association for computa-tional linguistics..sam wiseman, stuart shieber, and alexander rush.
2017. challenges in data-to-document generation.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2253–2263, copenhagen, denmark.
association forcomputational linguistics..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..a table representation.
an example of table representation for figure 2 isshown in table 6..typenaive.
tdtemp.
toptemp.
td+toptemp.
table representationcaption: table 2 the overall mention detection re-sults on the test set of ontonotes.
row name: modelour full, model lee et al.
(2018).
metric: prec., rec.,f1.
value: 89.6 82.2 85.7.table 2 shows the overall mention detection resultson the test set of ontonotes .
prec.
of model ourfull model is 89.6 .
rec.
of model our full model is82.2 .
f1 of model our full model is 85.7 .
table 2 shows the overall mention detection resultson the test set of ontonotes .
our full model has thelargest prec.
(89.6) of model.
lee et al.
(2018) hasthe largest rec.
(83.7) of model .
our full model hasthe largest f1 (85.7) of model .
prec.
of model ourfull model is larger than model lee et al.
(2018) .
rec.
of model our full model is smaller than modellee et al.
(2018) .
f1 of model our full model islarger than model lee et al.
(2018) .
table 2 shows the overall mention detection resultson the test set of ontonotes .
prec.
of model ourfull model is 89.6 .
rec.
of model our full model is82.2 .
f1 of model our full model is 85.7 .
modelour full model has the largest prec.
(89.6) .
modellee et al.
(2018) has the largest rec.
(83.7) .
modelour full model has the largest f1 (85.7) .
prec.
ofmodel our full model is larger than model lee et al.
(2018) .
rec.
of model our full model is smallerthan model lee et al.
(2018) .
f1 of model our fullmodel is larger than model lee et al.
(2018) ..table 6: example of table representation..b placeholders of copy-based.
pre-trained models.
tables 7 and 8 describe placeholders of our pro-posed copy-based pre-trained models..placeholder.
description.
<header target>.
targeted header.
<metric target> metric of.
tar-.
<value target>.
<header other>.
<metric other>.
<value other>.
<cat header><metric>.
geted headervalue of targetedheadernon-targetedheadermetric of non-targeted headervalue of non-targeted headerheader categorymetric lists.
exampleof figure 2ourfullmodelprecision.
89.6.lee et al.
(2018)precision.
86.2.modelprecision,recall, f1.
table 7: general placeholder descriptions for td..1461placeholder.
description.
<header max>.
<cat header max>.
<metric max>.
<value max>.
<header min>.
<cat header min>.
<metric min>.
<value min>.
<header diff1>.
<header diff2>.
<cat header diff>.
<metric diff>.
<value diff>.
header of maxi-mum recordsheader categoryofmaximumrecordsmetric of maxi-mum recordsmaximum valueof recordsheader of mini-mum recordsheader categoryofminimumrecordsmetric of mini-mum recordsminimum valueof recordsthe ﬁrst headerargument of dif-ference recordsthesecondheader argumentofdifferencerecordsheader categoryofdifferencerecordsmetric of differ-ence recordsdifference valueof records.
exampleof figure 2ourfullmodelmodel.
precision.
full.
89.6.ourmodelmodel.
recall.
82.2.full.
ourmodel.
lee et al.
(2018).
model.
precision.
3.4.table 8: general placeholder descriptions for top ..c system output examples.
table 9 shows table descriptions generated by theﬁne-tuned gpt2 and ﬁne-tuned t5 models withand without a copy mechanism, using differenttypes of table representations for our table examplein figure 2..d human evaluation.
figures 4 and 5 show the user interface for eval-uating correctness and relevance and for evaluat-ing grammaticality, coherence, and conciseness,respectively..1462modelfine-tuned gpt2(naive).
fine-tuned gpt2(td temp).
fine-tuned gpt2(top temp).
fine-tuned gpt2(naive) + copy.
fine-tuned gpt2(td temp) + copyfine-tuned gpt2(top temp) + copyfine-tuned t5(naive)fine-tuned t5(td temp)fine-tuned t5(top temp)fine-tuned t5(naive) + copyfine-tuned t5(td temp) + copy.
fine-tuned t5(top temp) + copy.
generated textwe compare our results using the model with that of the other model, which showsthe performance on both models.
we can conclude that the models are more reliablein predicting the usefulness of the model and more reliable than the other models.
wesuggest that the more accurate the model, the higher the mention detection results onboth models.
the model with the highest mentions detection yields a better model, andalso shows the best performance on both models.
and ﬁnally we have seen that our model does not do the right thing by simply using theword ”we.” as a noun we do not do the right thing by using the verb ”we.” as a noun ourword ”we” does not even have a verb, ”we do.” as a verb we do not even have a verb,”we do.
the table shows the recall results for our full model.
the f1 performance was better thanthat in our full model, indicating that we did not have any other models in the dataset.
the rec.
performance was comparable to that for the full model with a f1+ performanceof 82.7%.
table 2 a new version of the ﬁrst game of a new series , f1 a new version of a game ofan a new series , the new and another version of the original character , the one of thecharacter of the character of the person of the person of the character of a person , ...table 2 the best way to lee et al.
(2018) the best for the .
the ﬁrst time, in the history, a.the, it has.
is that the , the , the 86.2 .
the, the.
, model in our full and 3.4 .
model .
(,and, (and in) and inand(.
(i. in).
table 2 presents the overall mention detection results on the test set of ontonotes.
we cansee that our full model outperforms all the baselines in terms of recall f1 score.
table 2 shows the mention detection performance on the test set of ontonotes.
our fullmodel outperforms all the baselines in terms of recall and f1 score.
table 2 shows the overall mention detection results on the test set of ontonotes.
our fullmodel outperforms the state-of-the-art in both precision and recall.
table 2 shows the overall model results on ontonotes.
we can see that our full modeloutperforms all baselines, which demonstrates the effectiveness of our approach.
table 2 shows the overall mention detection results on the test set of ontonotes dataset.
our full model outperforms the state - of - the - art by a large margin , with an absolutedifference of 0.8% over the state of the art.
table 2 shows the overall mention detection results on the test set of ontonotes.
our modeloutperforms the state - of - the - art ( lee et al .
, 2018 ) and is comparable to the state -of - the - art ( lee et al .
, 2018 )..table 9: example of generated table description..1463figure 4: interface for evaluating correctness and relevance..1464figure 5: interface for evaluating grammaticality, coherence, and conciseness..1465