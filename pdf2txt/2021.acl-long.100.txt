measure and evaluation of semantic divergence across two languages.
syrielle montariol ∗inria parissyrielle.montariol@inria.fr.
alexandre allauzenespci parisdauphine universityalexandre.allauzen@espci.psl.eu.
abstract.
languages are dynamic systems: word usagemay change over time, reﬂecting various so-cietal factors.
however, all languages do notevolve identically: the impact of an event, theinﬂuence of a trend or thinking, can differ be-tween communities.
in this paper, we pro-pose to track these divergences by compar-ing the evolution of a word and its translationacross two languages.
we investigate severalmethods of building time-varying and bilin-gual word embeddings, using contextualisedand non-contextualised embeddings.
we pro-pose a set of scenarios to characterize semanticdivergence across two languages, along with asetup to differentiate them in a bilingual cor-pus.
we evaluate the different methods by gen-erating a corpus of synthetic semantic changeacross two languages, english and french, be-fore applying them to newspaper corpora todetect bilingual semantic divergence and pro-vide qualitative insight for the task.
we con-clude that bert embeddings coupled with aclustering step lead to the best performance onsynthetic corpora; however, the performanceof cbow embeddings is very competitive andmore adapted to an exploratory analysis on alarge corpus..1.introduction.
languages evolve throughout time:for manywords, their usages along with their frequent collo-cations and associations can change, revealing theevolution of the society (aitchison, 2001).
how-ever, all languages do not evolve identically: the im-pact of an event, the inﬂuence of a trend or thinking,can differ between communities.
moreover, lan-guages do not evolve independently; some wordscan be inherited and borrowed between languages.
for example, cognates — words that have the same.
∗ this work was carried out while the author was working.
at lisn-cnrs..etymological origin and similar meaning in two lan-guages — can sometimes diverge into false friends,due to particular features of one language and itsassociated culture and history..a more speciﬁc example is the russian word“ukrop”, meaning “dill”.
it started to be used byrussian people as an ethnic slur—a pejorativeterm—to talk about ukrainian soldiers at the be-ginning of the russian-ukrainian conﬂict (stewartet al., 2017).
then, ukrainian people started touse it to designate their own patriots, in a positiveway.
analysing the evolution of this word can leadto a better understanding of the evolution of theconﬂict; on the contrary, without suitable tools andmethods to detect the divergence in its usage andconnotation between communities, one might drawspurious results when analysing texts of this period.
diachronic semantic change detection is anemerging ﬁeld in natural language processing,building upon the growing number of digitisedtexts with temporal metadata publicly availablein various languages.
it opens new perspectivesof improvement for downstream tasks (using time-aware word representation for tasks ranging fromtext classiﬁcation to information retrieval in tem-poral corpora) or for socio-linguistic and historicallinguistics analysis (kutuzov et al., 2018)..the goal of this paper is to extend the analysisof lexical semantic change across two languages,aiming at estimating the degree of diachronic se-mantic divergence between a word and its transla-tion across time in a bilingual corpus.
we proposean experimental framework to learn word represen-tations that are comparable across both time andlanguages, and to detect and classify semantic di-vergence in a bilingual setting.
we compare: (i)diachronic word embeddings, which allow staticembeddings such as cbow (mikolov et al., 2013)to drift through time, and (ii) contextualised em-beddings, relying on a pre-trained multilingual lan-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1247–1258august1–6,2021.©2021associationforcomputationallinguistics1247guage model (m-bert, devlin et al., 2019).1 wealso propose an anchored-alignment strategy totackle the bilingual setting for non-contextual em-beddings.
then, we suggest a metric to measure thedivergence of word usage between two languages,the bilingual divergence.
given the lack of a bilin-gual dataset annotated with semantic divergence,we generate a corpus of synthetic semantic driftacross two languages using eurosense (delli boviet al., 2017), a sense-disambiguated and alignedbilingual corpus.
to do so, we deﬁne a set of mono-lingual and bilingual semantic change scenariosand evaluate our different approaches on them.
fi-nally, we apply our systems to newspaper corporain two languages, english and french, covering thesame time period, from 1987 to 2006. we classifyall words of a bilingual lexicon into the scenariosdeﬁned for the synthetic drift generation..to sum up, we extend the most appropriatemethods from the literature of diachronic semanticchange to build a framework for the measure ofsemantic divergence across languages (sections 3and 4), for which we propose a deﬁnition of thetask, a measure of semantic divergence (section 5),and a process to evaluate the presented methods(section 6)..2 related work.
diachronic embedding models.
the ﬁrst ap-proaches to diachronic modeling were based onrelative word frequencies and distributional sim-ilarities (gulordava and baroni, 2011).
follow-ing the generalisation of word embeddings, di-achronic word embeddings models emerged (tah-masebi et al., 2018).
a ﬁrst line of work, led bykim et al.
(2014), learns an embedding matrix onthe ﬁrst time slice of a temporal corpus, and in-crementally ﬁne-tune it at each time step.
thismethod has the advantage of simplicity but face agreater sensitivity to noise (shoemark et al., 2019;kaiser et al., 2020).
another method, proposed byhamilton et al.
(2016) and kulkarni et al.
(2015),train word embeddings on each time slice indepen-dently and align the representation spaces to makethe embeddings comparable.
finally, rudolph andblei (2018); jawahar and seddah (2019) and bam-ler and mandt (2017) deﬁne probabilistic modelsof word embeddings, able to capture the drifts bytraining embeddings jointly on all time slices..1code.
is.
available.
at https://github.com/.
smontariol/bilingualsemanticchange.
these methods average all the senses of a wordinto a unique vector at each time step.
pre-trainedlanguage models such as bert (devlin et al.,2019) allow each occurrence of a word to havea contextualised vector representation.
these mod-els, pre-trained on large datasets, improved the stateof the art on numerous nlp tasks.
similarly, con-textualised embeddings can be applied to semanticchange detection (giulianelli et al., 2020; mon-tariol et al., 2021) using several aggregation tech-niques to measure the degree of semantic change ofa word from all its contextualised representationsover time.
however, these methods are still outper-formed by non-contextualised embeddings for thistask (schlechtweg et al., 2020)..semantic change across languages.
while thistopic is actively researched in the linguistic andsociology research communities (boberg, 2012), itis fairly new in the nlp literature.
many authorsapply diachronic embeddings models to more thanone language (hamilton et al., 2016; schlechtweget al., 2020).
however, prior work comparing theevolution of word usage across languages is verylimited.
some work studies variations betweenlanguages or dialects, without looking into the tem-poral dimension (hovy and purschke, 2018; bein-born and choenni, 2020).
uban et al.
(2019) com-pare present meanings of cognate words across 5romance languages to differentiate true cognatesfrom false friends and measure the divergence be-tween languages.
in a temporal fashion, martincet al.
(2020a) study the evolution of 4 word pairs inan english-slovenian corpus of newspaper articles.
finally, frossard et al.
(2020) propose a list of cog-nates for analysing the similarities in the evolutionof english and french, along with a preliminaryanalysis focusing on the differences in word fre-quency over time..3 diachronic words embeddings.
before presenting systems based on contextualisedembeddings, we introduce two methods using non-contextualised ones, as they are known to per-form best for the task of semantic change detection(schlechtweg et al., 2020).
we use the continuousbag of words (cbow) architecture of word2vec(mikolov et al., 2013); we apply two different train-ing methods to train it in a diachronic way.
then,we describe an anchored-alignment method to ob-tain bilingual diachronic word embeddings..12483.1 diachronic training.
in this section, we consider a monolingual corpusdivided into t time slices.
we rely on a ﬁne-tuningmethod rather than an alignment-based method,where a new model would be trained from scratchat each time step (hamilton et al., 2016).
indeed,for our cross-lingual task an alignment is alreadyneeded to map the embedding spaces of the twolanguages together; it would not be desirable tomultiply this type of transformation, as each align-ment introduces uncertainty in the system..to begin with, as advised by rudolph and blei(2018), we pre-train our cbow models on a shuf-ﬂed version of the full corpus for each language.
we use two methods for diachronic training.
theﬁrst on is incremental training (kim et al., 2014):we incrementally ﬁne-tune the model on each timeslice by initialising the weights with those of theprevious time slice.
the second variant is inde-pendent training: the model is ﬁne-tuned on eachtime slice independently by initialising it with thepre-trained embeddings.
compared to the incre-mental method, the latter does not take into accountthe chronology of the corpus and can lead to lessdirected drifts.
however, the fact that the embed-dings do not go through a large amount of succes-sive training updates, contrarily to the incrementalmethod, prevents the embeddings from undergoingtoo extreme drifts (shoemark et al., 2019)..3.2 bilingual alignment.
we now consider a bilingual corpus, and embed-dings trained separately on each language.
wewant to align the representation spaces to make theembeddings comparable..anchoring.
the supervision signal for the align-ment is key to the performance of the overall sys-tem, even more than the model architecture itself(ruder et al., 2019).
anchoring is a form of su-pervision commonly used in nlp to obtain cross-lingual word embeddings.
the supervision comesfrom a bilingual dictionary, whose words – the an-chors – are used as seeds during the alignment.
itcan be transparent words such as named entities,or an exhaustive bilingual dictionary with the fullvocabulary.
however, aligning the vectors of thewhole vocabulary is not appropriate for semanticchange detection, as it tends to lower the dispari-ties between the different vector spaces (tsakalidiset al., 2019).
in our case, the alignment forcesthe embeddings of the word pairs from the super-.
vision dictionary to be the same in the two lan-guages.
this might hide some behavior such as ahigh disparity at the beginning of the full periodand a convergence of meanings over time.
con-sequently, we use a seed dictionary with only thewords that we assume are stable during the periodin both languages.
a ﬁrst set of “stable” wordsare stopwords (azarbonyad et al., 2017; martincet al., 2020b); however, by deﬁnition they do notcarry much meaning.
relying only on them for thesupervision might result in a poor alignment.
wecomplement the list of seed words with word pairsthat have the same relative frequency in the cor-pora of each language; with this frequency being inthe top 10% of the full corpus (azarbonyad et al.,2017; zhang et al., 2015).
for all experiments inthis paper, we use the bilingual dictionary from themuse tool2 (lample et al., 2018).
it includes 5000word pairs and handles word polysemy..alignment.
first, we train monolingual cbowembeddings on each language independently, with-out dividing the corpora into time slices.
to pre-pare for the alignment, we apply mean-centering tothe embeddings of each language, as schlechtweget al.
(2019) showed the positive impact of thispreprocessing step for vector space alignment.
for the alignment, we use orthogonal procrustes(sch¨onemann, 1966).
it consists in ﬁnding the map-ping w between two embedding spaces e1 and e2which minimizes the sum of squared euclidean dis-tances between the image of the source embeddingsspace e1 ∗ w and the target embedding space e2for the set of selected anchor words in both spaces.
these aligned embedding vectors are used to ini-tialise the diachronic embeddings, which can thenbe trained on all the time slices in both languages,incrementally or independently..4 contextualised embeddings.
to challenge the systems based on aligned cbowembeddings, we use m-bert, the multilingual ver-sion of bert (devlin et al., 2019).
it is trained onwikipedia content on 104 languages, without anyadditional multilingual mechanism nor languageidentiﬁer..applying a pre-trained multilingual model on abilingual temporal corpus enables immediate com-parison without requiring any alignment.
each se-quence is labelled with the time it was written and.
2https://github.com/facebookresearch/.
muse.
1249its language.
we extract contextualised represen-tations for each token of a sequence by summingthe top four hidden layers of the pre-trained model.
bert representation relies on a system of word-pieces; if a word is divided into several wordpieces,we take the average of all the wordpiece embed-dings as representation for the word.
to sum upall the information about a word from the set ofcontextual embeddings of all its occurrences in atime slice, we experiment with two aggregationtechniques: averaging and clustering..averaging : proposed by martinc et al.
(2020a),this method averages all the token embeddings of aword for each time period and each language.
weend up with a set of time-speciﬁc and language-speciﬁc vector representations of a word.
they canbe compared using the cosine distance (shoemarket al., 2019).3.clustering: this method, ﬁrst used by giu-lianelli et al.
(2020), groups the set of token em-beddings of a word into types of usages.
we applya clustering algorithm, k-means, to all the embed-dings of a word and its translation, on all the timeperiods jointly.
then, we compute the normaliseddistributions of clusters, for each language and pe-riod.
more precisely, for a given word, we extractthe number of tokens in each cluster and for eachpair (period, language); we normalise it by the totalnumber of occurrences of the word in the corpus.
we obtain the probability distributions of the us-ages of this word at each time slice and in bothlanguages.
these distributions can be comparedbetween two periods or two languages using thejensen-shannon divergence (jsd, lin, 2006)..5 drift measures.
after applying the described systems to a bilingualcorpus divided into t time slices, for a given tar-get word in a given language l, we obtain either asequence of t embeddings u(t)in each languagel(for cbow and m-bert with averaging), or avector of t cluster distributions c(t)(for m-bertlwith clustering).
we compute the distance betweenrepresentations: the cosine distance between non-contextual embeddings and the jsd between clus-.
3we deﬁne the cosine distance as (1 - cosine similarity)..ters distributions...
cos(u(t1).
l1.
(averaging.
).
, u(t2)l2, c(t2)l2.
).
l1.
.
jsd(c(t1).
d(t1, t2, l1, l2) =.
or cbow)(clustering)(1)in a monolingual setting, we use two metricscommonly used to measure the drifts of a wordin each language (rodina et al., 2019): the incre-mental drift, from each time slice to the next one,and the inceptive drift, from the beginning of theperiod to each time slice.
we obtain drift vectorsin rt −1 for each word in each language, by com-puting d(t1, t2, l, l)..in a bilingual setting, drift measures can be com-puted for each word pair (one word and its transla-tion).
first, we compute the distance inside eachword pair at each time step.
we call it the bilingualdistance: s(t)b = d(t, t, l1, l2) for t = 1, 2, .
.
.
, t .
second, the temporal drift of this distance is mea-sured similarly to the monolingual drift, either in-crementally or inceptively.
the distance is thenorm between the bilingual distance s(t)b at twotime steps, measuring the divergence of the usageof a word and its translation.
we call it bilingualdivergence.
for example, the incremental bilingualdivergence is computed as follows:.
dincr.
b =.
(2).
.
.
b − s(1)|s(0)b |b − s(2)|s(1)b |...− s(t )b |.
|s(t −1)b.
.
.
various information can be extracted from thevector of bilingual divergence of a word db: thetrend (no trend i.e.
stable distance between a wordand its translation, decreasing i.e.
convergence, orincreasing i.e.
divergence), the degree of diver-gence (e.g.
by summing all its elements), and thespeed of divergence (by estimating the slope)..6 synthetic drift generation.
the study of semantic change faces the issue ofevaluation, as few labeled corpora exist for thistask.
recent initiatives from the nlp communitystart to produce more annotated data (schlechtweget al., 2020); however, no corpus is available forbilingual analysis.
consequently, we generate acorpus of bilingual synthetic semantic change, fol-lowing common practice in the literature of mono-lingual semantic change detection (shoemark et al.,.
12502019; schlechtweg and schulte im walde, 2020).
it allows us to control exactly the shape and de-gree of semantic change in the corpus and thusgain a deeper understanding of the impact of eachmodeling choice..to create synthetic semantic change, commonpractice involve to merge two words that do notshare a common sense, creating a pseudo-word;then, generate synthetic change by controlling theproportion of sentences using each of the two orig-inal words in the successive time slices of a tem-poral corpus (rosenfeld and erk, 2018; shoemarket al., 2019).
however, as advised by schlechtwegand schulte im walde (2020), it is preferable touse the natural polysemy of words for the syntheticdrift to be as close as possible to reality: insteadof controlling the proportion of sentences contain-ing two unrelated words merged as a pseudo-word,we use sentences containing several senses of aunique word.
to this end, we need a bilingualsense-annotated corpus with consistent annotationsbetween languages (pasini and camacho-collados,2020).
the eurosense corpus4 (delli bovi et al.,2017) is derived from the europarl corpus, a largepublic corpus of proceedings of the european par-liament.
it has a full and a reﬁned version.
weuse the latter to build our synthetic corpus; it ishalf the size of the ﬁrst one but more reliable.
theframework babelnet (navigli and ponzetto, 2012)is used for annotation.
eurosense contains paralleltext in 21 european languages.
we focus on thetwo languages with the highest amount of annota-tions in the reﬁned corpus: english and french.
anexample of aligned sentences in these languagescan be found in table 1..6.1 semantic change scenarios.
in order to generate and capture variations of dis-tributions of word senses through time and acrosstwo languages, we deﬁne several scenarios of wordusage variations.
first, we choose two monolin-gual scenarios of semantic change (labeled “m”)and generate them using sentences extracted fromthe eurosense corpus.
assuming we have a targetword with at least two senses, the scenarios are:.
• m 0: all senses are fully stable.
• m 1: one sense gradually appears / disappears,.
the others stay stable..second, we deﬁne scenarios of semantic diver-gence (bilingual scenarios, labeled “b”) derived.
4http://lcl.uniroma1.it/eurosense/.
sentence.
english.
french.
the besttoolsfor this are lib-anderalisationfreer competition, which causestraincompa-to take aniesgreaterinterestin the wishes ofcustomers ..meilleurslesmoyensd’yparvenir sont lalib´eralisation etune concurrenceplus libre , quiincite les compag-nies ferroviairessoucier`asedavantagedessouhaits de leursclients .
clientbn:00019763n.
lemmasense.
customerbn:00019763n.
table 1: example of aligned sentences in english andfrench in the eurosense corpus, with annotated anchorand corresponding sense in the babelnet framework..from the monolingual scenarios.
assuming wehave a target words w1 and its translation w2 withat least two senses in common:.
• b0: w1 and w2 are m 0.
• b1: w1 is m 0, w2 is m 1.
• b2: w1 and w2 are the same m 1 (theygain/lose the same sense, drifting in the samedirection)..• b3: w1 and w2 are different m 1 (onegains/loses one sense, the other gains/losesanother sense: they diverge)..these 4 scenarios can be linked with distinctphenomena.
examples of words for each of them,extracted from a bilingual english-french corpusof newspaper articles spanning 20 years, can befound in table 3. first, scenario b0 deals withwords which have a stable meaning and an equiva-lent word with equally stable meaning in the otherlanguage (e.g.
dinosaurs).
scenario b1 can becaused by a word being borrowed from one lan-guage to another: a loanword.
after the borrowing,its usage can evolve, for example due to socio-cultural speciﬁcity impacting the second language,while it stays stable in the source language.
simi-larly, an example of b3 scenario are cognate wordswhose usage evolve in their respective languages,diverging into false friends.
for example, the en-glish noun affair has common etymology with oldfrench and used to mean “what one has to do, or-dinary business”.
its usage evolved across time,.
1251gaining in english the new sense of “a love rela-tionship, usually secret” while it often refers infrench to “a business case”.
the word ukrop pre-sented in the introduction is also an example of b3scenario.
finally, scenario b2 deals with wordsthat go through the same semantic change as theirequivalent in another language.
among other phe-nomenon, a common cause is when a languageevolution is triggered by a cultural or technologicalchange that is common to the societies speakingthe two languages.
for example, the sense of theword conﬁnement related to pandemic became themajority meaning in many languages worldwidefollowing the covid-19 pandemic..6.2 building the synthetic corpus.
step 1: selection of target lemma pairs..for all the sense-annotated lemmas in englishand french in eurosense, we extract their sets ofsenses.
we only keep the senses with more than200 occurrences per language.
we associate en-glish and french lemmas together if they have atleast two senses in common, creating a bilingualdictionary.
from these lemma pairs, we extract theset of sentences annotated with one of the senses incommon to build the pool of sentences for the nextstep.
in total, we have 115 english-french lemmapairs, of which 66 have 2 senses (low polysemy)and 49 have between 3 and 5 senses.
for example,a low-polysemy lemma pair is (project, projet) anda high-polysemy one is (measure, mesure).
step 2: creation of sense distributions.
for each monolingual scenario, we create prob-ability distributions of senses at each time slice.
we denote by p(s | t , w, l) the probability thatthe lemma w conveys the sense s at time t inlanguage l. we generate t = 10 time slices andapply each scenario to all the target lemmas pairs.
since our variables are discrete, for a given lemmaw in language l, the probability distribution of a setof 2 senses {s1, s2} over time can be characterisedby a 2 × t stochastic matrix, where the lines sumto 1:.
.
p(s1 | t = 1, w, l)p(s1 | t = 2, w, l)· · ·.
.
p(s2 | t = 1, w, l)p(s2 | t = 2, w, l)· · ·.
...
p(s1 | t = t, w, l) p(s2 | t = t, w, l).
for a given target lemma, for the m 0 scenario,we randomly draw an initial distribution over theset of senses and repeat it at each time slice:p(s | t = t, w, l) = p(s | t = 1, w, l) for.
t = 2, 3, .
.
.
, t .
for the m 1 scenario, we gradu-ally increase or decrease the probability of appear-ance through time of one of the senses, either lin-early or logarithmically, following shoemark et al.
(2019).
the other senses have a stable distributionacross time..step 3: creation of the synthetic corpus.
for each monolingual scenario, we build the syn-thetic corpus time slice after time slice, using theset of target lemmas, the pool of sense-annotatedsentences and the generated distributions of senses.
for each target lemma, at each time step t, wesample 200 sentences for each of its senses.
then,we add each sampled sentence to time step t withthe probability speciﬁed in the corresponding dis-tribution of senses of the scenario.
to avoid thesynthetic sense distribution for a target lemma to bedisturbed by noise from its appearance as a contextword in other sentences, when adding a sentence tothe synthetic corpus, we attach the sufﬁx “ l” to itstarget lemma.
note that the 200 sentences sampledfor each sense of a lemma can appear only once ineach time slice, but can appear in other time slicesof the corpus..all the bilingual scenarios are built from themonolingual ones.
generating them reduces to us-ing the right monolingual scenarios for each wordand its translation.
for example in the b3 sce-nario, we generate a corpus using the m 1 scenariofor both the target lemma and its translation, butselect a different sense to appear or disappear in or-der to induce a divergence.
the synthetic corpora,for each scenario and each language, have around7.5m words distributed into the 10 time slices..6.3 evaluation method.
to sum up, at each time t, a word w in a languagel is characterised by its sense distribution in thesynthetic corpus p(s | t, w, l).
this information issimilar to the cluster distributions extracted whenapplying clustering to contextualised embeddings;we can compute the drift measures deﬁned in sec-tion 5, using the jsd to compare the sense distribu-tions.
the drifts obtained from these measures canthen be used as gold standard for the evaluation ofour systems..for each system described in sections 3 and 4and for each target lemma pair, we output the vec-tors of monolingual drift computed on the mono-lingual scenario synthetic corpora and the vectorsof bilingual divergences computed for the bilin-.
1252model.
diachrony.
stablem 0.driftm 1.both stableb0.
stable&driftb1.
same driftb2.
divergeb3.
cbow incrementalindependent.
0.65 - 0.160.84 - 0.83.
0.54 - 0.960.63 - 0.86.
0.87 - 0.820.83 - 0.89.bert.
averagingk-means 5.
0.86 - 0.870.85 - 0.86.
0.34 - 0.550.61 - 0.19.
0.84 - 0.900.86 - 0.97.
0.66 - 0.460.70 - 0.45.
0.79 - 0.40.78 - 0.41.
0.76 - 0.680.80 - 0.66.
0.63 - 0.470.67 - 0.50.
0.71 - 0.690.77 - 0.91.
0.63 - 0.470.66 - 0.40.table 2: accuracy measure of each system for the different semantic change scenarios.
the numbers on the leftare incremental drift while the ones on the right are inceptive drift..gual scenarios (see section 5).
we wish to evaluatewhether these series have the same trend as thegold standard.
for this, we use the mann-kendall(mk) trend test (mann, 1945; kendall, 1975), anon-parametric statistical test used to detect trendsof variables.
it is particularly suited to monotonictrends, which is how we designed the semantic driftin our data..the null hypothesis of the test is the absence ofmonotonic trend.
the mann-kendall test statisticzm k relies on comparing every value in the timeseries with all the values preceding it.
the sign ofthe statistic test indicates the trend of the data, givena conﬁdence level of 0.05: no monotonic trend (thenull hypothesis), increasing trend (zm k > 0), ordecreasing trend (zm k < 0).
for a given targetlemma, if the direction of the detected trend in ourdata is the same as the one from the gold standarddrift, we consider that the semantic change hasbeen correctly identiﬁed.
we compute the accuracyas the proportion of correctly identiﬁed trends inthe full list of target lemmas..7 experiments on synthetic data.
we compare the accuracy of our systems on thesynthetic corpora generated in the previous section..7.1 experimental setup.
cbow processing.
as we rely on stopwords(on top of frequent words) for the alignment, we donot discard them during preprocessing.
the con-text size is set to 5 words, and the dimension ofword embeddings to 50. preliminary experimentswith larger embedding dimensions exhibited no sig-niﬁcant improvement.
we posit this is due to thesmall size of the dataset.
moreover, the accuracyof incremental ﬁne-tuning of cbow embeddingsfor semantic change detection is very sensitive todimensionality (kaiser et al., 2020); the optimalembedding dimension is usually quite low, with aclear drop in performance with high embeddings.
dimensions.
we train all models using 10 epochs.
for each language, a static model is ﬁrst trainedon the set of all sentences containing the targetlemmas.
then, we proceed with incremental anindependent training..bert processing.
we use the pre-trainedbert-base-multilingual-uncased model from thetransformers library.
we extract the contextu-alised embeddings from the corpus and apply thetwo aggregation methods, averaging and clustering.
we choose k = 5 clusters for k-means, as it is themaximum number of senses that can be found inour list of target lemmas.
experiments with highervalues of k did not improve the accuracy.
we re-move the “ l” sufﬁx of the target lemmas beforeextracting their embeddings..7.2 results on synthetic data.
table 2 summarises the accuracies measured usingthe mann-kendall trend test (hussain and mah-mud, 2019) on the 115 lemma pairs.
it comparesthe trend of the drift of all systems with the goldstandard trend, for each scenario.
we have threescenarios with stable monolingual drift or stablebilingual divergence (m 0 and b0, with all thesenses being stable; and b2, where a word andits translation drift in the same direction) and threedrifting scenarios (m 1 and b1, where one sensedrifts; and b3, where a word and its translationdrift in different directions).
the results show thatstable scenarios are generally easier to detect accu-rately compared to the changing ones, especiallyin the monolingual analysis..the best results are obtained with bert usingk-means clustering.
this system focuses on thevariation of proportion of the different usages, in-stead of the evolution of the average word represen-tation; it provides a better focus on the meaningfulchanges in word usage.
in the case of cbow, inde-pendent training leads to better performances thanincremental training.
this is in line with the ﬁnd-.
1253ings of shoemark et al.
(2019): the large amount oftraining updates, especially in such a small corpus,is harmful for the quality of the representation..overall, the inceptive drift measure leads to bet-ter accuracy for stable scenarios, while the incre-mental drift is more suited to scenarios where thesense distributions change across time.
thus, weadvise towards always computing both measuresfor diachronic studies..8 experiment on newspaper corpora.
we analyse the semantic divergence of word-translation pairs in a bilingual corpus of news arti-cles.
our goal is to classify all words of a bilinguallexicon into the semantic divergence scenarios de-ﬁned in section 6.1..8.1 data description & experimental setup.
the new york times annotated corpus (sandhaus,2008) gathers around 1 855 000 articles from jan-uary 1987 to june 2007. we scrape le monde, oneof the most read daily newspapers in france, onthe same time period.
we divide both corpora intot = 20 yearly time steps, as a trade-off betweengetting precise information on semantic drift thanksto a low granularity and reducing noise that appearsdue to a too low granularity.
finally, we select avocabulary containing the v = 40 000 most fre-quent words for each corpora.
the average numberof words is around 3.5 m for one time step in thefrench corpus and 9 m in the english one.
first,a bilingual lexicon is built using the intersectionof the muse bilingual dictionary with the frenchand english vocabularies from our corpora.
wemanually update the bilingual lexicon with domain-speciﬁc vocabulary such as named entities, in orderto improve the coverage on the corpora.
the ﬁnalbilingual dictionary has 27 351 words..to obtain bilingual diachronic embeddings, weuse cbow with incremental training.
indeed, eventhough bert with k-means clustering lead to bet-ter results overall on synthetic corpora, the extrac-tion of each token embedding and the clusteringstep are computationally heavy.
moreover, in alarge corpus such as ours, saving in memory asmany embedding vectors as occurrences of wordsfrom the bilingual lexicon is not feasible.
thus, theclustering method is more suited for a ﬁne-grainedanalysis of the divergence of senses of a limited setof target words, rather than an exploratory analysison the full vocabulary..the experimental setup is the same as the oneused on the synthetic corpus; the volume of databeing much higher in the newspaper corpus, weincrease the capacity of our model by setting thedimension of cbow embeddings to 100, in orderto retain more information.
we pre-train cbowmodels on the english and french corpora andnormalise the embeddings to prepare for the align-ment.
the french corpus being the smallest, itsembeddings are mapped to the english embeddingspace.
then, we incrementally update the alignedembeddings on both corpora.
for each word ofthe bilingual vocabulary, we compute its monolin-gual drift and its bilingual divergence, followingthe methodology applied on the synthetic corpora.
it allows us to identify the words belonging to eachof the bilingual scenarios..8.2 results on bilingual divergence.
on top of classifying all words into the differentbilingual divergence scenarios, we quantify the de-gree of divergence by summing up the elements ofthe vectors of inceptive drift and of inceptive bilin-gual divergence respectively.
the proportion ofeach scenario as well as examples selected amongthe words with the most extreme drifts are in ta-ble 3. for example, words belonging to scenariob3 have the highest monolingual drifts in both theenglish and french corpora, while their bilingualdivergence is among the lowest..words that are stable in both languages (b0) aremostly daily life words (e.g.
mayonnaise).
wordsthat drift in the same direction in both languages(b2) are concepts related to technology and societythat are common to the english and french culture(e.g.
renewable); while the words that diverge be-tween the two languages (b1-fr (english stable,french drifting), b1-en and b3) belong to moreculture-speciﬁc concepts (e.g.
francs) or contro-versial topics (e.g.
terrorist).
for example, francsdrifts in french, while it is stable in english.
thisis probably due to the change of currency in francein 2002 that had much lower media coverage in theus.
similarly, terrorist drifts in both languages butin different directions.
indeed, the two countrieswent through many terrorist attacks during the pe-riod under study, but from very different groups,leading to different contexts for this word..overall, the exploratory results on the bilingualnewspaper corpora offer interesting insights on per-spectives for many applications; both for long-term.
1254b0: both stable b1-fr: stable&drift b1-en: drift&stable b2: same drift b3: different drifts16.2%.
15.5%.
58.2%.
4.9%.
5.2%.
dinosaurspotteryanniversariesmayonnaisejoke.
reformsdelinquencyfrancsfeminineprovincial.
bushhorriﬁcmaidhostagesdealers.
genomicsrenewablecondomcinemasrobotic.
steroidrocketsgaykatrinaterrorist.
table 3: proportion and example words for the different categories of bilingual divergence..semantic change, studying the joint evolution ofcognate words and borrowings; and for short-termchange in word usage, for example when studyingthe disparity in the media resonance of an event indifferent countries..9 discussion.
in this paper, we deﬁne an experimental frameworkto measure and classify the semantic divergence ofa word and its translation in a bilingual corpus.
wecompare different kinds of word embeddings onvarious bilingual divergence scenarios generatedin a synthetic corpus.
we apply our conclusionsto a bilingual newspaper corpus to identify wordsundergoing different types of semantic divergence.
bert embeddings coupled with a clustering steplead to the best performance on synthetic corpora.
the performance of cbow embeddings is never-theless very competitive, and more adapted to anexploratory analysis on a large corpus..there is a large margin for future work; be itin terms of quality of diachronic bilingual repre-sentation, metric to measure semantic divergence,and evaluation method.
our evaluation focuses onthe trend of the drift, but its degree and its speedcan also be quantiﬁed and analysed.
in addition,the underlying bilingual representation learning ap-proach is key for the detection of drifts.
the trans-formations applied to create a cross-lingual wordembedding space might result in information lossor generation of spurious drifts in the embeddings.
to compare word embeddings with the purpose ofdetecting semantic divergence, the anchored align-ment method presented here is not the only option;promising candidates are temporal referencing(schlechtweg et al., 2019) and the global anchormethod (yin et al., 2018)..a limitation of our work is the use of an injec-tion to deﬁne word pairs.
in his general linguisticscourse, de saussure (1916) states that there is no.
bijective relationship between words in differentlanguages.
the different meanings and uses of aword in a language cannot have a perfectly identi-cal equivalent in another language.
moreover, asnoted by frossard et al.
(2020), a word can havesynonyms in one language while the word bearingthe same meaning in another language has none; inthat case, the usage of the word in the ﬁrst languageis divided into all its synonyms..another limitation is evaluation with syntheticdata.
this method is common in monolingual se-mantic change analysis, but there is no guaranteethat the generated phenomenon is similar to real-world data.
for example, a degree of freedom isthe shape of the synthetic drifts generated.
in thispaper, we used logarithmic and linear shapes; butsome literature hint that a logistic shape is also agood match for semantic drift (bailey, 1973; blytheand croft, 2012).
furthermore, in real data thegranularity (the size of the periods used to dividethe corpus) might have an important impact on theshape of the semantic evolution..finally, as we build all bilingual scenarios fromcombinations of two monolingual scenarios, theﬂaws of the monolingual scenarios are inheritedby the bilingual scenarios.
it can potentially mul-tiply the noise by propagation of uncertainty.
wewished to overcome the limitations of syntheticevaluation with the application on real corpora,but more thorough interpretation would be neces-sary for a solid qualitative evaluation.
to performquantitative evaluation on real data, an annotateddataset similar to the ones for monolingual seman-tic change (e.g.
schlechtweg et al., 2020) would benecessary.
however, the annotation task would beeven more complex than for monolingual data.
aneasier entrance point towards annotating data forthis task could be loanwords and cognate words.
overall, this is a challenging task and we hope toattract more people to work on it in the future..1255references.
jean aitchison.
2001. language change: progress orin cambridge approaches to linguistics..decay?
cambridge university press..hosein azarbonyad, mostafa dehghani, kaspar bee-len, alexandra arkut, maarten marx, and jaapkamps.
2017. words are malleable: computing se-mantic shifts in political and media discourse.
inproceedings of the 2017 acm on conference on in-formation and knowledge management, cikm ’17,page 1509–1518.
association for computing ma-chinery..charles-james n bailey.
1973. variation and linguistic.
theory..robert bamler and stephan mandt.
2017. dynamicin proceedings of the 34th in-word embeddings.
ternational conference on machine learning, vol-ume 70 of proceedings of machine learning re-search, pages 380–389, international conventioncentre, sydney, australia.
pmlr..lisa beinborn and rochelle choenni.
2020. semanticdrift in multilingual representations.
computationallinguistics, 46(3):571–603..richard a blythe and william croft.
2012. s-curvesand the mechanisms of propagation in languagechange.
language, pages 269–304..charles boberg.
2012. english as a minority language.
in quebec.
world englishes, 31..ferdinand de saussure.
1916. cours de linguistique.
g´en´erale..claudio delli bovi, jose camacho-collados, alessan-dro raganato, and roberto navigli.
2017.eu-rosense: automatic harvesting of multilingual senseannotations from parallel text.
in proceedings of the55th annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages594–600, vancouver, canada.
association for com-putational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..esteban frossard, mickael coustaty, antoine doucet,adam jatowt, and simon hengchen.
2020. datasetfor temporal analysis of english-french cognates.
in proceedings of the 12th language resources andevaluation conference, pages 855–859, marseille,france.
european language resources association..mario giulianelli, marco del tredici, and raquelfern´andez.
2020.analysing lexical semanticchange with contextualised word representations.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 3960–3973, online.
association for computational lin-guistics..kristina gulordava and marco baroni.
2011. a dis-tributional similarity approach to the detection of se-mantic change in the google books ngram corpus.
inproceedings of the gems 2011 workshop on ge-ometrical models of natural language semantics,pages 67–71.
association for computational lin-guistics..william l. hamilton, jure leskovec, and dan jurafsky.
2016. diachronic word embeddings reveal statisti-cal laws of semantic change.
in proceedings of the54th annual meeting of the association for compu-tational linguistics (volume 1: long papers), pages1489–1501.
association for computational linguis-tics..dirk hovy and christoph purschke.
2018. capturingregional variation with distributed place representa-tions and geographic retroﬁtting.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4383–4394, brus-sels, belgium.
association for computational lin-guistics..md.
hussain and ishtiak mahmud.
2019. pymannk-endall: a python package for non parametric mannjournal of openkendall family of trend tests.
source software, 4(39):1556..ganesh jawahar and djam´e seddah.
2019. contextual-ized diachronic word representations.
pages 35–47..jens kaiser, dominik schlechtweg, sean papay, andsabine schulte im walde.
2020.ims at semeval-2020 task 1: how low can you go?
dimensionality inin proceedingslexical semantic change detection.
of the fourteenth workshop on semantic evaluation,pages 81–89, barcelona (online).
international com-mittee for computational linguistics..maurice g. kendall.
1975. rank correlation measures..yoon kim, yi-i chiu, kentaro hanaki, darshan hegde,and slav petrov.
2014. temporal analysis of lan-guage through neural language models.
in proceed-ings of the acl 2014 workshop on language tech-nologies and computational social science, pages61–65.
association for computational linguistics..vivek kulkarni, rami al-rfou, bryan perozzi, andsteven skiena.
2015. statistically signiﬁcant de-tection of linguistic change.
in proceedings of the24th international conference on world wide web,www ’15, pages 625–635, republic and canton ofgeneva, switzerland.
international world wide webconferences steering committee..1256andrey kutuzov, lilja øvrelid, terrence szymanski,and erik velldal.
2018. diachronic word embed-in proceed-dings and semantic shifts: a survey.
ings of the 27th international conference on compu-tational linguistics, pages 1384–1397.
associationfor computational linguistics..guillaume lample, alexis conneau, marc’aurelioranzato, ludovic denoyer, and herv´e j´egou.
2018.word translation without parallel data.
in 6th inter-national conference on learning representations,iclr 2018, vancouver, bc, canada, april 30 - may3, 2018, conference track proceedings.
openre-view.net..jianhua lin.
2006. divergence measures based onieee trans.
inf.
theor.,.
the shannon entropy.
37(1):145–151..henry b. mann.
1945. nonparametric tests againsttrend.
econometrica: journal of the econometricsociety, pages 245–259..matej martinc, petra kralj novak, and senja pollak.
2020a.
leveraging contextual embeddings for de-tecting diachronic semantic shift.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 4811–4819, marseille, france.
euro-pean language resources association..matej martinc, syrielle montariol, elaine zosa, andlidia pivovarova.
2020b.
discovery team atsemeval-2020 task 1: context-sensitive embed-dings not always better than static for semanticthe four-change detection.
teenth workshop on semantic evaluation, pages 67–73, barcelona (online).
international committee forcomputational linguistics..in proceedings of.
tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems 26, pages 3111–3119.
curran associates,inc..syrielle montariol, matej martinc, and lidia pivo-varova.
2021. scalable and interpretable semanticchange detection.
in proceedings of the 2021 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, pages 4642–4652, online.
as-sociation for computational linguistics..roberto navigli and simone ponzetto.
2012. babel-net: the automatic construction, evaluation and ap-plication of a wide-coverage multilingual semanticnetwork.
artiﬁcial intelligence, 193:217–250..tommaso pasini and jose camacho-collados.
2020. ain pro-short survey on sense-annotated corpora.
ceedings of the 12th language resources and eval-uation conference, pages 5759–5765, marseille,france.
european language resources association..julia rodina, daria bakshandaeva, vadim fomin, an-drey kutuzov, samia touileb, and erik velldal.
2019. measuring diachronic evolution of evaluativeadjectives with word embeddings: the case for en-in proceedings ofglish, norwegian, and russian.
the 1st international workshop on computationalapproaches to historical language change, pages202–209, florence, italy.
association for computa-tional linguistics..alex rosenfeld and katrin erk.
2018. deep neuralmodels of semantic shift.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long papers),pages 474–484, new orleans, louisiana.
associa-tion for computational linguistics..sebastian ruder, ivan vuli´c, and anders søgaard.
2019. a survey of cross-lingual word embeddingmodels.
journal of artiﬁcial intelligence research,65:569–631..maja rudolph and david blei.
2018. dynamic embed-dings for language evolution.
in proceedings of the2018 world wide web conference, www ’18, page1003–1011, republic and canton of geneva, che.
international world wide web conferences steeringcommittee..evan sandhaus.
2008. the new york times annotatedcorpus.
in philadelphia : linguistic data consor-tium.
vol.
6, no.
12..dominik schlechtweg, anna h¨atty, marco del tredici,and sabine schulte im walde.
2019. a wind ofchange: detecting and evaluating lexical seman-in proceed-tic change across times and domains.
ings of the 57th annual meeting of the associationfor computational linguistics, pages 732–746, flo-rence, italy.
association for computational linguis-tics..dominik schlechtweg, barbara mcgillivray, simonhengchen, haim dubossarsky, and nina tahmasebi.
2020. semeval-2020 task 1: unsupervised lexicalin proceedings of thesemantic change detection.
fourteenth workshop on semantic evaluation, pages1–23, barcelona (online).
international committeefor computational linguistics..dominik schlechtweg and sabine schulte im walde.
simulating lexical semantic change from.
2020.sense-annotated data.
corr, abs/2001.03216..peter h. sch¨onemann.
1966. a generalized solution ofthe orthogonal procrustes problem.
psychometrika,31(1):1–10..philippa shoemark, farhana ferdousi liza, dongnguyen, scott hale, and barbara mcgillivray.
2019.room to glo: a systematic comparison of semanticchange detection approaches with word embeddings.
in proceedings of emnlp-ijcnlp 2019, pages 66–76, hong kong, china.
association for computa-tional linguistics..1257ian stewart, dustin arendt, eric bell, and svitlanavolkova.
2017. measuring, predicting and visual-izing short-term change in word representation andusage in vkontakte social network.
in icwsm..nina tahmasebi, lars borin, and adam jatowt.
2018.survey of computational approaches to diachronicconceptual change.
arxiv, abs/1811.06278..adam tsakalidis, marya bazzi, mihai cucuringu, pier-paolo basile, and barbara mcgillivray.
2019. min-ing the uk web archive for semantic change detec-tion.
in proceedings of the international conferenceon recent advances in natural language process-ing (ranlp 2019), pages 1212–1221, varna, bul-garia.
incoma ltd..ana uban, alina maria ciobanu, and liviu p. dinu.
2019. studying laws of semantic divergence acrossin proceedings oflanguages using cognate sets.
the 1st international workshop on computationalapproaches to historical language change, pages161–166, florence, italy.
association for computa-tional linguistics..zi yin, vin sachidananda, and balaji prabhakar.
2018.the global anchor method for quantifying linguisticshifts and domain adaptation.
in proceedings of the32nd international conference on neural informa-tion processing systems, nips’18, page 9434–9445,red hook, ny, usa.
curran associates inc..yating zhang, adam jatowt, sourav bhowmick, andkatsumi tanaka.
2015. omnia mutantur, nihil in-terit: connecting past with present by ﬁnding cor-in proceedings ofresponding terms across time.
the 53rd annual meeting of the association forcomputational linguistics and the 7th internationaljoint conference on natural language processing(volume 1: long papers), pages 645–655, beijing,china.
association for computational linguistics..1258