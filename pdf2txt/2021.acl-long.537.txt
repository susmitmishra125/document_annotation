emailsum: abstractive email thread summarization.
shiyue zhang♠ asli celikyilmaz♥.
jianfeng gao♣ mohit bansal♠.
♠unc chapel hill ♥facebook ai research ♣microsoft research{shiyue, mbansal}@cs.unc.eduaslic@fb.com jfgao@microsoft.com.
abstract.
recent years have brought about an interest inthe challenging task of summarizing conver-sation threads (meetings, online discussions,etc.).
such summaries help analysis of thelong text to quickly catch up with the deci-sions made and thus improve our work orcommunication efﬁciency.
to spur researchin thread summarization, we have developedan abstractive email thread summarization(emailsum) dataset, which contains human-annotated short (<30 words) and long (<100threadswords) summaries of 2,549 email(each containing 3 to 10 emails) over a widevariety of topics.
we perform a comprehensiveempirical study to explore different summa-rization techniques (including extractive andabstractive methods, single-document and hi-erarchical models, as well as transfer and semi-supervised learning) and conduct human eval-uations on both short and long summary gen-eration tasks.
our results reveal the key chal-lenges of current abstractive summarizationmodels in this task, such as understandingthe sender’s intent and identifying the rolesof sender and receiver.
furthermore, we ﬁndthat widely used automatic evaluation metrics(rouge, bertscore) are weakly correlatedwith human judgments on this email threadsummarization task.
hence, we emphasize theimportance of human evaluation and the devel-opment of better metrics by the community.1.
1.introduction.
as one of the major natural language generationtasks, automatic summarization has been studiedfor decades.
most research efforts were focused onsingle-document summarization tasks, e.g., newsdocument summarization (hermann et al., 2015;narayan et al., 2018).
however, living in an in-formation era, we are facing with diverse content.
1our code and summary data have been made available at:.
https://github.com/zhangshiyue/emailsum.
email thread:subject: lunch this weeksusan: all, regarding our lunch this week to celebrate theone year anniversaries for michelle & david, and mark’sbirthday, i have a request to make it wednesday instead oftuesday.
does anyone have an objection to this?
susandavid: i have another lunch engagement wed, but i willskip it if everyone else wants to move our lunch.
davidtamra: susan, wednesday works out better for me as well.
i have a doctor’s appointment tomorrow during lunch.
tamra.
short summary:susan emails everyone about an anniversary and offers tochange the date.
david says he is busy but is willing to gowith the majority.
tamra agrees with susan’s date..long summary:susan emails everyone about a lunch to celebrate a oneyear anniversary as well as mark’s birthday.
she says shewould change the date to a different day.
david says he isbusy that day with his own appointment but is willing togo with the majority and cancel that appointment to makethis one.
tamra agrees with susan’s date as she is busytuesday with an appointment..table 1: an email thread and human-written short andlong summaries from our emailsum dataset..in different structures.
the summarization need isvaried along with different application scenarios.
recently, there is an increasing research interest indiverse summarization tasks (gao et al., 2020), e.g.,timeline (allan et al., 2001), query-based (li andli, 2014), multi-modal (zhu et al., 2018), meeting(carletta et al., 2006), dialogue or discussion thread(misra et al., 2015; gliwa et al., 2019; rameshku-mar and bailey, 2020), etc.
following the branchof dialogue or thread summarization, we introducea new abstractive email thread summarization(emailsum) dataset..email threads are widely used at work.
an emailthread is a special type of dialogue that usuallyhas a speciﬁc structure (sender, receiver, greetingline, main body, and the signature), contains tech-nical information, and involves multiple speakers.
unlike a conversational dialog turn, an email in a.proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6895–6909august1–6,2021.©2021associationforcomputationallinguistics6895thread is much longer with longer sentences, multi-ple action items or requests, and stylistically similarto written text.
studies have shown that on aver-age a worker sends/receives 122 business emails(radicati, 2015) and spends more than 3 hours onthose emails (adobe, 2019) per day.
one possi-ble reason is that sometimes people have to readthrough the entire conversation before replying tothe latest email.
this happens when you forgetthe main points of previous discussions or you arenewly included in a discussion thread.
therefore,automatically summarizing email threads can im-prove our work efﬁciency and provides practicalbeneﬁts.
email thread summarization is not a newtask.
carenini et al.
(2007) collected extractivesummaries of 39 email threads from enron emailcorpus (klimt and yang, 2004) and proposed touse a fragment quotation graph and clue words toconduct summarization.
ulrich et al.
(2008) col-lected both extractive and abstractive summariesof 40 threads from w3c email corpus (craswellet al., 2006) plus speech acts, meta sentences, etc.
however, this task has been much less studied com-pared to other summarization tasks, partially dueto the lack of large labeled email thread datasets..in this paper, we collect human-written short(< 30 words) and long (< 100 words) abstrac-tive summaries of 2,549 email threads constructedfrom avocado research email collection (oardet al., 2015), which is 64× the size of previouslylabeled email thread datasets (carenini et al., 2007;craswell et al., 2006).
we limit each thread to aminimum of 3 and a maximum of 10 emails, anexample is given in table 1. we also extract 8,594unlabeled email threads from both avocado andw3c to facilitate semi-supervised learning.2 seesection 2 for details of data collection..next, we present comprehensive baselines fromdifferent learning paradigms as a benchmark forour new email summarization dataset.
speciﬁcally,we explore different summarization techniques, in-cluding extractive and abstractive summarizationmethods, single-document and hierarchical mod-els, transfer learning, and semi-supervised learn-ing for both short and long summary generation.
experiments demonstrate that utilizing pretrainedlanguage model (e.g., t5 (raffel et al., 2020)) iscritical due to the small size of our data; takingthe email thread as a single document sets up a.
2we apply strict criteria for thread extraction (see sec-tion 2).
more threads can be extracted by relaxing thoseconstraints..good baseline; transferring from news or dialoguedatasets barely improve the performance; using hi-erarchical encoders only marginally improves it;while semi-supervised learning by using unlabelledemail threads signiﬁcantly (p < 0.01) improvesrouge (lin, 2004) scores in some cases..lastly, to better understand how well the emailthread summarization models perform and inves-tigate the correlation between automatic metricsand human judgment, we ask humans to rate the“salience” (how well the model summarizes salientpoints) and “faithfulness” (how well the modelstays true to the email thread) of model-generatedsummaries, as well as to perform a pairwise com-parison between our best and base models.
weﬁnd that even though semi-supervised learning im-proves rouge scores, human judges still favor thesummary generated by the baseline model (t5base).
two frequent errors made by the model are (1)failing to understand the sender’s intent and (2)failing to identify the roles of the sender and re-ceiver.
relatedly, human correlation analysis re-veals that automatic metrics (rouge (lin, 2004),bertscore (zhang et al., 2019)) are poorly cor-related with human judgment, which stresses theimportance of human evaluation in this task andthe requirement for better metrics to be proposed.
overall, in this work, we propose the new email-sum dataset that provides a larger resource forstudying the email thread summarization task.
weconduct a comprehensive empirical model studyand human evaluation analysis, which will serve asan important starting point for future studies..2 emailsum dataset.
to collect email thread summarization data, weﬁrst need to obtain unlabelled email threads.
weresort to existing email collections: enron (klimtand yang, 2004), w3c (craswell et al., 2006), andavocado (oard et al., 2015).
however, none ofthem provides explicit thread structure.
therefore,in this section, we will introduce our email threadpreprocessing and summary collection procedures..2.1 email thread preprocessing.
we extract email threads from the ﬂat email col-lections in the following steps: (1) we give everyemail a “normalized subject” by removing the replyor forward tags (e.g., “re:”, “fwd:”, etc.)
from itsoriginal subject; (2) we group emails by the normal-ized subjects and sort emails in the same group (i.e.,.
6896domain.
dataset.
news.
dialogue.
email thread.
cnn/dm xsum.
samsum crd3.
bc3.
emailsumshort.
emailsumlong.
# of documentsavg.
document length# of turns per doc.
avg.
turn lengthavg.
summary lengthext-oracle-r1.
312,085786.4--55.258.2.
226,677409.5--23.223.8.
16,369124.110.211.123.445.3.
32,720615.827.519.458.350.4.
40550.46.485.3134.336.5.
2,549233.24.550.327.139.0.
2,549233.24.550.368.546.0.table 2: the statistics of different summarization datasets.
ext-oracle-r1s are the rouge-1 scores of the oracleextractive method, which shows the abstractiveness of the summary (the lower the more abstractive)..thread) by timestamp; (3) we de-duplicate emailsin every thread by sender’s email plus timestamp;(4) we traverse emails in every thread in tempo-ral order and cut off the thread when none of thesenders plus receivers of the current email appearsin previous emails; (5) we ﬁlter out threads thatonly contain single repeated content..to obtain a cleaner dataset, we remove threadsthat do not comply with the following constraints:(1) 3 ≤ the number of emails ≤ 10; (2) 5 < thenumber of words in each email < 200; (3) 30 < thetotal number of words < 1000; (4) does not containnon-english (e.g., german) tokens; (5) does notcontain reply or forward tags in the subject of theﬁrst email..emails often contain personal information suchas full name, email/physical address, phone num-ber, etc.
to protect privacy, we anonymize allemail threads before annotation: (1) only keep ﬁrstnames; (2) remove threads that have “password”,“pwd”, “conﬁdential”, etc.
; (3) replace email ad-dress, physical address, phone number, url, ipaddress, local path, and other sensitive numberswith username@domain.com, address,phonenumber, http://link, ipaddress,path, and number, respectively..we conduct an extensive manual quality scanto make sure that the extracted threads are trulythreads (instead of random emails grouped) andproperly anonymized.
finally, we obtain 8,116threads from avocado and 3,478 threads fromw3c.3 we randomly sample 3k avocado threadsfor summary annotation, and the remaining threadsare used as unlabelled data..2.2 thread summary collection.
we collect summary annotations on amazon me-chanical turk.
since summarizing text is not aneasy task, to get acceptable english summaries we.
3we ﬁnd that the extracted threads from enron are usually.
short (fewer than 3 emails) and noisy..use several quality control strategies: (1) we se-lect annotators that are located in the us, have anapproval rate greater than 97%, and have at least10,000 approved hits; (2) during annotation, weperiodically sample summaries, manually checktheir quality, and reject or block poor-quality anno-tators; (3) after annotation, we randomly sample2 examples per annotator and manually categorizeannotators into “good”, “fair”, and “bad” groups,then ﬁlter examples written by bad annotators..email threads oftentimes contain technical infor-mation, we instruct annotators not to get stuck ontechnical details, instead, focus on the major con-cerns, decisions, and consensus.
we collect bothshort (< 30 words) and long (< 100 words) abstrac-tive summaries per thread.
for the short summary,we instruct annotators to write a concise descrip-tion of what the thread is mainly talking about;while for the long summary, we instruct them towrite a a narrative of what happens.
we are in-tent to provide summaries with two different levelsof abstractiveness, length, and concreteness.
weshow annotators an example written by an expert(a cs graduate student).
more summary collectiondetails can be found in appendix a..2.3 final dataset description.
the summary collection and ﬁltering process yield2,549 email threads each with a long and a shortsummary.
we randomly sample 500 examples fromthe “good” annotator group as our testing set andsplit the remaining examples into training (1,800threads) and development (249 threads) sets.
ta-ble 2 shows the statistics of emailsum.4.
for easeof benchmarking, we also include statistics on other.
4since comparing the model-generated summary to onlyone human-written reference may not be fully informative,recently we have also collected one more reference for eachemail thread in our test set, i.e., each test example will havetwo gold references now in our ﬁnal dataset.
the results inthe paper are all still based on the original one-reference setupbut we will release the updated two-reference results for ourbest baselines on github..6897commonly used summarization datasets: cnn/dm(hermann et al., 2015) and xsum (narayan et al.,2018) are about news summarization; samsum(gliwa et al., 2019) is about chit-chat summariza-tion; crd3 (rameshkumar and bailey, 2020) isa role-play dialogue summarization dataset; bc3(ulrich et al., 2008) is another email thread summa-rization with 40 threads from w3c.
compared tothe other datasets, the average document length inthe emailsum dataset is not very long, containing233 words; long summaries are more than twiceas longer than short summaries.
“ext-oracle-r1”in table 2 indicates how abstractive the summariesare.
it computes the rouge-1 scores of an oracleextractive method (see section 3.1 for details of theoracle extractive method).
the lower it is, the moreabstractive the dataset is.
according to this score,the abstractiveness of the emailsum summariesis lower than the xsum summaries, while higherthan the cnndm summaries.
furthermore, theshort summaries of emailsum dataset are moreabstractive than its long summaries..3 models.
the summarization models we explore in this worktake the email thread as input and generate thesummary as output.
we experiment on email-sumshort and emailsumlong tasks separately..3.1 extractive.
oracle.
this method maximize an evaluationmetric w.r.t.
the gold summary.
“ext-oracle-r1”in table 2 is computed from an oracle summarythat maximizes rouge-1 (lin, 2004)..lead.
this model simply picks the ﬁrst sentencefrom the source document as the summary, whichhas surprisingly good performance on cnn/dmdataset (narayan et al., 2018).
we test two vari-ants by selecting: (1) the ﬁrst sentence of the emailthread, which is usually the subject (see the exam-ple in table 1), referred as lead-1; (2) the ﬁrst sen-tence of the email thread (the subject) plus the ﬁrstsentences of every email, named lead-1-email.5.
textrank.
this is a graph-based method (mihal-cea and tarau, 2004).
it ﬁrst builds a graph betweensentences by their embedding similarities; then thepagerank algorithm is applied to obtain the rank.
5we also tested some other heuristics: e.g., the ﬁrst sen-tence of the last email, the last 3-5 sentences of the emailthread, etc.
however, none of them perform better than lead-1-email..scores for each sentence, and top-rank sentencesare selected as the summary..bertsumext.
liu and lapata (2019b) proposeto build a sentence extractor upon bert (devlinet al., 2019) to perform extractive summarization,which achieves a good performance on cnn/dm..3.2 abstractive.
fast abs rl.
as the simple non-pretrained ab-stractive baseline, we use chen and bansal (2018),which is a hybrid model that ﬁrst extracts sen-tences from the source document, then rewritesthe extracted sentences by an abstractive rewriter.
they pair summary sentences with the extractedsentences to train the abstractive rewriter.
adapt-ing their model to our email thread summariza-tion task, we make two adjustments: (1) we ex-tract emails instead of sentences, which is a natu-ral unit for email thread; (2) since summary sen-tences usually follow the temporal order of theemails, we enhance this pairing procedure by usingthe neeleman-wunsch algorithm (needleman andwunsch, 1970; rameshkumar and bailey, 2020) toimpose the order constraint to the alignment (seedescription and comparison in appendix b)..t5.
t5 (raffel et al., 2020) is a transformer(vaswani et al., 2017) based seq-to-seq model pre-trained with large-scale english data.
it achievesstate-of-the-art performances on a lot of nlp tasksincluding the cnn/dm summarization task.
asour main baseline, we take the email thread as asingle document and ﬁnetune a t5 base to generatethe summary (t5base).
a similar setup is also usedin transfer and semi-supervised learning.
since ourtraining dataset is small, we ﬁnd that using the pre-trained knowledge transfer is crucial.
training at5 model from scratch performs poorly (see theresults in appendix table 7)..transfer learning.
to analyze how informationfrom other summarization datasets (listed in ta-ble 2) can be transferred to this new task and itsimpact on the performance, we investigate two sim-ple transfer learning methods: (1) pre-ﬁnetuning,in which we ﬁrst ﬁnetune t5 on a bigger summa-rization dataset (e.g., cnn/dm) then continue theﬁnetuning on our dataset, referred as xpre (x isthe bigger dataset’s name, e.g., cnndmpre) inour result tables.
this is analogous to the continualtraining method proposed for multilingual transferlearning of machine translation (kocmi and bojar,.
6898thread (e.g., e1, e2, e3, e4) while the email-level re-ceives mean-pooled email-level representations asinput.
the decoder has two cross attentions thatattend to the outputs of the email-level and thetoken-level encoders respectively.
both token-leveland email-level encoders are sharing the weightsof the t5 encoder.
we add a small number of newparameters by adding new cross attention betweenthe decoder and the email-level encoder..4 experiments.
4.1 evaluation metrics.
rouge (lin, 2004) is a commonly used auto-matic metric for summarization tasks.
it has severalvariants: (1) rouge-1 (r1) measures the unigramoverlap between the generated and reference sum-maries; (2) rouge-2 (r2) measures the bi-gramoverlap; (2) rouge-l (rl) computes the longestcommon subsequence (lcs); (4) summary-levelrouge-l (rlsum) computes lcs between eachpair of reference and candidate sentences and re-turns the union-lcs.
we use the rouge scorepackage7 and report f1 scores..bertscore(zhang et al., 2019) goes beyondn-gram overlap to provide contextualized seman-tic similarity.
speciﬁcally, it uses bert (devlinet al., 2019) (or roberta (liu et al., 2019)) repre-sentations to “softly” align the words in candidateand reference summaries and then computes a “soft”uni-gram f1 score.
we use the bert score pack-age8 and report rescaled numbers with a baseline..4.2 results.
table 3 shows the evaluation results on the testingset of different models (the corresponding resultson the development set can be found in appendixtable 7).
it can be observed that the oracle ex-tractive model sets up a high upper bound on allmetrics except for bertscore (berts).
amongnon-oracle extractive methods, the lead-1-emailheuristic works best and even better than the deepextractive method, bertsumext.
the hybrid fastabs rl model outperforms purely extractive meth-ods but works worse than purely abstractive meth-ods with large-scale pretraining (e.g., t5)..6the signiﬁcance test is following the bootstrap test setup.
(efron and tibshirani, 1994) and sample for 100k times.
7https://github.com/google-research/.
google-research/tree/master/rouge.
8https://github.com/tiiiger/bert_score.
figure 1: the architecture of our hierarchical t5..2018).
(2) joint-training, in which we upsampleemailsum data and mix it with another dataset,then use the combined data to ﬁnetune t5, simi-larly denoted as xjoint.
this is analogous to themultilingual joint training method used in machinetranslation (johnson et al., 2017)..semi-supervised learning.
since we only have2.5k labeled email threads, another important tech-nique to improve the performance is to utilize un-labelled data (i.e., email threads without labeledsummaries).
as introduced in section 2.1, inaddition to the 3k email threads used for sum-mary collection, we have 8,594 unlabelled emailthreads (5,116 from avocado; 3,478 from w3c).
we explore semi-supervised learning via the sim-ple self-training technique (scudder, 1965).
weuse a trained model (a ﬁnetuned t5) to generatesummaries for unlabelled threads, then mix themodel-labeled and human-labeled data to ﬁnetunet5 again, referred as semisupx (x stands for theunlabelled data source we use, i.e., w3c, avocado,or together)..hierarchical t5.
hierarchical summarizationmodels have been shown to improve the perfor-mance of multi-document summarization task (liuand lapata, 2019a).
although an email thread canbe treated as a single document due to the tem-poral dependency between consecutive emails, italso has a clear turn structure that encourages usingof the hierarchical encoders.
recently, zhu et al.
(2020) proposed a hierarchical model (hmnet) formeeting summarization.
inspired by their work,we propose a hierarchical model that is similar tohmnet in structure but uses t5 as the backbone,therefore, it can take advantage of both the hier-archical structure and the pre-trained knowledge.
as shown in figure 1, this model contains two en-coders: the token-level encodes the whole email.
6899self-attnnormfeedforwardx ntoken-levelencodernorme1e2e3e4h1h2h3h4email-levelencoderh1h2h3h4mean poolingself-attnnormfeedforwardnormcross-attnnormcross-attnnormself-attnnormfeedforwardnormx ndecoderx nmodels.
emailsumshort.
emailsumlong.
r1.
r2.
rl.
rlsum berts.
r1.
rlsum berts.
oraclelead-1lead-1-emailtextrankbertsumext.
fast abs rlt5base.
cnndmprexsumpresamsumprecrd3pre.
cnndmjointxsumjointsamsumjointcrd3joint.
semisupw3csemisupavocadosemisuptogether.
39.0423.3526.6222.5224.84.
31.1536.57.
35.4336.1434.6836.05.
34.3834.1835.5734.66.
35.4336.7336.98.
12.475.575.604.545.15.
6.5910.56.
10.7510.2610.5610.04.
9.278.1710.078.81.
10.6410.8211.21∗.
30.1718.2219.7216.5617.81.
22.7328.3.
27.4928.6626.6227.21.
27.2025.9427.9526.95.
28.5928.4428.76.
35.6119.6123.7720.2421.81.
29.0332.76.
32.1533.4731.2232.06.
31.3030.6832.5731.59.
32.3133.2533.70∗∗.
hier.
t5base.
36.17.
10.37.
28.44.
33.34.
22.3212.2513.005.897.51.
6.4933.90.
33.6133.9733.2533.52.
32.7031.8333.5533.29.
33.6133.7633.91.
33.39.
45.9819.7535.7128.4230.23.
39.3543.81.
44.1543.4842.8343.60.
43.2842.3642.9642.8144.56∗∗43.8344.0844.50∗.
r2.
15.494.848.696.207.08.
10.5814.08.
14.2013.8213.5413.93.rl.
32.4014.2424.7019.0819.59.
27.0130.47.
30.8430.1430.0030.49.
42.1416.8832.1325.1926.68.
36.5139.88.
40.2139.8039.1339.97.
26.316.8716.935.677.78.
10.0332.09.
32.5331.6031.8231.53.
12.3711.8513.4412.9614.60∗∗14.61∗∗14.0614.53∗.
28.8428.2329.9929.3531.38∗∗31.21∗∗31.17∗∗30.89∗.
39.3938.3139.5439.3340.73∗∗40.52∗40.67∗∗.
29.9529.2231.8232.1432.81∗∗32.71∗∗32.30.
40.22.
32.30.table 3: summarization performance on the testing set of different models.
we test the signiﬁcance6 of theimprovement over t5base (∗: p < 0.05, ∗∗: p < 0.01)..emailsumshort.
emailsumlong.
eo-r1↓.
le-r1↓.
eo-r1↓.
le-r1↓.
humant5baser1-best.
39.050.2752.50.
26.6236.8839.22.
46.055.4360.04.
35.7143.6549.14.table 4: the extractive oracle (eo) and lead-1-email(le) models’ rouge-1 by taking human summary,base or best model generated summary as the ground-truth.
the lower the scores are, the more abstractive thesummaries are (↓)..taking the email thread as one single documentand ﬁnetuning t5 (i.e., t5base in table 3) sets upa strong baseline.
upon this baseline model, wetest the transfer learning from four different sum-marization datasets (cnn/dm, xsum, samsum,and crd3).
however, as shown in table 3, trans-fer learning barely improves over baseline, andtransferring by pre-ﬁnetuning always works bet-ter than joint-training.
since our emailsum hasa quite different domain as existing news or di-alogue datasets, we conjecture that it is hard totransfer knowledge between them or better trans-ferring techniques need to be applied.
similarly,we test the semi-supervised learning with unla-belled data from w3c, avocado, and both of them(together).
this method can mostly (or signiﬁ-cantly in some cases) outperform the baseline’sperformance for both emailsumshort and email-sumlong.
lastly, the hierarchical t5base modelonly marginally outperforms the non-hierarchical.
figure 2: the impact of the number of emails inthe thread on summarization performance (rouge-1).
the results are on the testing set.
short/long de-notes emailsumshort/emailsumlong; base/best de-notes the baseline/best model..baseline for emailsumlong task.
it is notable thatoverall emailsumlong has higher rouge scoresbut lower bertscore than emailsumshort..since we focus on generating abstractive sum-maries for email threads and the human-writtensummaries are fairly abstractive (as shown in ta-ble 2), we further investigate the abstractiveness ofmodel-generated summaries.
we take summariesgenerated by the baseline (t5base) and the bestrouge-1 models (semisuptogether for email-sumshort, semisupw3c for emailsumlong) as thepseudo ground-truth, respectively.
then, we eval-uate the rouge-1 of extractive oracle and lead-1-email models; higher scores means more extrac-tive summaries.
as shown in table 4, compared.
6900emailsumshort.
emailsumlong.
semisuptogether vs t5base.
semisupw3c vs t5base.
saliencefaithfulnessoverall quality.
win109116120.lose133123138.tie555839.win109126125.lose130122140.tie504124.table 5: pairwise comparison between summaries generated by the best rouge-1 models and t5base..to humans, models generate much more extractivesummaries.
moreover, the semi-supervised models(r1-best) are even more extractive than the base-line, which is probably because the self-trainingprocedure ampliﬁes the extraction tendency.
lastly,for both base and best models as well as for bothshort and long summaries, the model performance(rouge-1) decreases as the number of emails inthe thread increases (shown in figure 2)..5 human evaluation.
5.1 human rating collection.
to better understand where the model still fallsshort and investigate if the automatic metrics corre-late well with human judgments, we conduct a hu-man evaluation on amazon mechanical turk.
ini-tially, by manually checking the quality of model-generated summaries, we ﬁnd that models canmostly generate grammatical, relevant, and ﬂu-ent summaries; however, they often fail to besalient and faithful, i.e., models tend to be over-detailed or do not stay true to the source thread.
therefore, we ask human annotators to rate the“salience” and “faithfulness” of model-generatedsummaries.
we choose the best rouge-1 models,semisuptogether for emailsumshort, semisupw3cfor emailsumlong, to evaluate, then we sample100 examples, and collect 3 responses for eachexample.
human judges are asked to rate on a5-point likert scale for salience and faithfulness re-spectively and annotate which summary sentencesare not salient or unfaithful.
we explain the mean-ing of “salience” and “faithfulness” to annotatorsand instruct them how to rate from 1 to 5. mean-while, to verify the improvement obtained by bestr1 models over t5base, we ask them to comparethe summaries generated by these models and thosefrom t5base, and judge which one is more salient,more faithful, and has overall higher quality.
morecollection details can be found in the appendix d.we check the average inter-rater agreement(krippendorff’s alpha (krippendorff, 2011)) of“salience” and “faithfulness” ratings.
it is around.
0.09 to 0.23, i.e., slight to fair agreement (fleissand cohen, 1973).
however, when we convert theratings to 3-point by taking {3}, {4 and 5}, {1 and2} as 3 classes, the agreement increases to 0.36 to0.63, i.e., fair to substantial agreement.
this indi-cates that humans’ subjectivity affects the ratingsand people have a hard time distinguishing ‘bad’from ‘very bad’ as well as ‘good’ from ‘very good’.
meanwhile, the ratings for short summaries are al-ways less agreed across raters (0.36-0.38) than thatfor long summaries (0.58-0.63).
this indicates thatthere might be multiple different ways of summa-rizing an email thread into a short summary.
theagreement of pairwise comparison is around 0.20to 0.24 (fair agreement), which is because the base-line and the best models have non-distinguishableperformance (shown in table 5).
finally, we takethe 3-rater average as the ﬁnal human rating foreach example..in addition, we evaluate the correlations (pear-son correlation (benesty et al., 2009)) among dif-ferent human ratings.
the correlation betweensalience and faithfulness ratings is 0.36/0.45 forshort/long summarization.
and the correlationsamong salience, faithfulness, and overall qualitypairwise preferences are around 0.53 to 0.79. over-all, moderate to large (cohen, 2013) correlationsare observed..5.2 generated summary’s quality analysis.
surprisingly, human evaluators are mostly satis-ﬁed with the salience and faithfulness of model-generated summaries, ratings are around 4 out of5. on average, humans rate 3.89 and 4.04 forthe salience and faithfulness of semisuptogethergenerated short summaries, respectively; and theyrate 4.22 and 4.29 for the salience and faithfulnessof semisupw3c generated long summaries, respec-tively.
examples with low or high ratings are shownin table 6 or appendix table 8. humans rate higherfor model-generated long summaries, which is cor-related to the trend of rouge, and they are moresatisﬁed with faithfulness than salience..table 5 presents the human pairwise compari-.
6901fail to understand the sender’s intent..thread: subject: minutes of meeting: 3.5 plan ||| om: 1. nihar mentioned that we spent about 3 weeks in redeﬁning thelanguage, which was not originally planned.
this is the major reason for moving the code freeze date from 8/24 to 9/21.
2. forphase-i code drop to qa on 8/28 the conﬁdence in date is : 90% the conﬁdence in statbility of build is : 80% 3.
... ||| sharon:hi om - we also need to lock down the date for: 1 - service pack merge 2 - bug ﬁx freeze and, javascript library testing (ofﬂine)resource thanks, sharon ||| rajeev: thanks for the meeting minutes.
nihar, sharon can you list the risks to the phase 1 & phaseii schedules and what we are doing to manage the risk.
rajeevgenerated summary: om tells nihar that he spent 3 weeks redeﬁning the language.
sharon tells om that she needs to lockdown the date for 1 - service pack merge 2 - bug ﬁx freeze and javascript library testing.
(salience=4, faithfulness=3.3)ground-truth: om gives everyone minutes for a meeting.
sharon updates om on some other plans and rajeev asks ni-har/sharon for some technical details..fail to identify the roles of the sender and receiver..thread: subject: latest 4.0 ga palladium install for biogen ||| nilesh: path/patchinstaller i tested this with build version 377 andit works ﬁne.
||| diana: this one looks good.
i have veriﬁed that the 2 ﬁxes in 382 are in the patch installer.
just to clarify, this isreally a 382 patch installer that falls under the 377 directory?
... ||| nilesh: wilhan, i have deleted build 382 as there was nospace to create patch installer.
(as we discussed in the lab) and as we speciﬁed the build version to be 377 when creating thepatch installer i thought we will need to put it under build 377 and use the jar ﬁles for that.
can you please clarify this.
...generated summary: nilesh tells diana that the 2 ﬁxes in 382 are in the patch installer.
nileshe also asks wilhan to clarifythe deﬁnition of the build.
(salience=3.3, faithfulness=3.3)ground-truth: nilesh says he tested something with a build.
diana thinks it looks good after verifying it but asks some ques-tions.
nilesh updates wilhan and has some questions..table 6: error analysis examples.
emails are separated by ‘|||’ and some content is omitted by ‘...’.
(salience=xx,faithfulness=xx) gives the average human rating for that summary..son between the best rouge-1 models and t5base.
except for the faithfulness of emailsumlong, thebest rouge-1 models mostly lose to the base-line (though the loss and win are mostly marginal).
together with table 4, we conjecture that the im-provement obtained by semi-supervised learningexploits n-gram matching accuracy by making thesummary more extractive, while humans prefermore abstractive summaries..lastly, we analyze the non-salient and unfaithfulsentences labeled by the human evaluators.
weﬁnd that two errors are frequently made by thesummarization model: (1) failing to understandthe sender’s intent.
usually, when we send anemail, there is a high-level intention behind thedetailed content we write, e.g., start up a discus-sion, bring up a concern, broadcast a decision, etc.
however, models are oftentimes unable to capturethe intention and thus overly focus on details.
asshown in the ﬁrst example of table 6, om intendsto summarize the important points from a meeting,while the model only picks the ﬁrst piece of de-tail in that email as the summary.
this problemis also related to the over-extractive issue (shownin table 4).
the model tends to extract detailsfrom the source thread and the extraction is biasedto the ﬁrst sentence of each email.
(2) failing toidentify the roles of the sender and receiver.
anemail thread is a special type of conversation withmultiple speakers involved.
one important task.
figure 3: correlation between automatic metrics andhuman judgements.
short and long refer to email-sumshort and emailsumlong tasks, respectively..for the model is to identify the roles of differentspeakers and their relations, i.e., who does what towhom.
as shown in the second example of table 6,the model wrongly takes “2 ﬁxes in 382 are in thepatch installer” as information provided by nilesh,whereas it is supposed to be by diana.
the sameissue can also be observed in the ﬁrst example:om is just summarizing what nihar said insteadof telling nihar.
this is considered as a type ofunfaithfulness, which has been widely identiﬁedas a common issue of abstractive summarizationmodels (wang et al., 2020; durmus et al., 2020;maynez et al., 2020)..6902pearson coefficient5.3 correlation with human judgement.
rouge (lin, 2004) measures n-gram overlapand bertscore (zhang et al., 2019) is essentiallybased on “soft” uni-gram matching.
however, ac-cording to our analysis presented above, the emailthread summarization models mainly fail to be ab-stractive, salient, and faithful, which are hard tobe evaluated by n-gram overlap.
furthermore, aspointed out by bhandari et al.
(2020), differentdatasets usually require different evaluation metrics.
therefore, here, we study the correlation betweenautomatic metrics and human judgments..speciﬁcally, we evaluate the pearson correla-tion between human ratings and automatic metricscores on the 100 examples used in the humanevaluation.
besides, as described above, we con-duct a pairwise model comparison between thebest rouge-1 models and t5base for “salience”,“faithfulness”, and “overall quality”.
we convertthem to a pairwise ranking score, i.e., -1 if t5baseis better; 1 if t5base is worse; 0 if two models arenon-distinguishable.
in the same way, we convertdifferent metric scores to ranking scores.
then,we also evaluate the pearson correlation betweenhuman and metric ranking scores.
figure 3 illus-trates the results.
overall, the correlations are fairlypoor.
the best correlation is between rouge-1and human overall quality ranking for short sum-mary generation (coefﬁcient=0.14, p=0.16).
thereis little or negative correlation between metrics andhuman judgment for the long summary generation.
therefore, we emphasize the importance of humanevaluation and better automatic proxies need to beproposed in the future..6 conclusion.
in this work, we propose an abstractive emailthread summarization dataset, emailsum, thatcontains 2,549 email threads with human-writtenshort and long summaries.
we explore differ-ent summarization paradigms and ﬁnd that tak-ing the email thread as a single document andﬁnetuning t5 (raffel et al., 2020) sets up a goodbaseline.
transferring from other summarizationdatasets barely improves it.
using hierarchicalstructure also only marginally improves the per-formance.
semi-supervised learning by using un-labelled email threads improves automatic metrics(rouge) but still loses to the baseline in humanevaluation.
finally, our human evaluation revealsthat the model fails to understand the sender’s main.
intention and the roles of different speakers.
au-tomatic metrics are poorly correlated with humanjudgment, which emphasizes the importance of hu-man evaluation and designing new metrics for thistask in the future..7 broader impact statement.
we use two email collections in this work: avo-cado (oard et al., 2015) and w3c (craswell et al.,2006).
w3c is derived from w3c public mailinglist that is open-source available online.
avocadoconsists of emails and attachments taken from 279accounts of a defunct information technology com-pany referred to as “avocado”.
its copyright isprotected by linguistic data consortium.
basedon the license agreement, we will only open-sourceour collected summaries and provide scripts to ob-tain email threads from the original avocado emailcollection.
to further protect copyright and theprivacy of the persons involved in the emails, asintroduced in section 2, we carefully anonymizeall the email threads we construct from both emailcollections.
we fairly pay crowd-source workers$1.37 (for threads with 5 or fewer emails) or $2 (forthreads with more than 5 emails) for writing theshort and long summaries and $0.6 for human rat-ing such that the pay rate is higher than the federalminimum wage requirement..acknowledgments.
we thank the reviewers for their helpful commentsand xiang zhou for useful discussions.
we thanksaadia gabriel, yichen jiang, tom mccoy, andyixin nie for helping write summary examples (toshow as initial examples to mturk annotators) andestimate the workload for deciding the fair payment.
this work was partially done while sz was intern-ing at msr and later extended at unc, where itwas supported by nsf-career award 1846185,onr grant n00014-18-1-2871, and a microsoftinvestigator fellowship.
the views contained inthis article are those of the authors and not of thefunding agency..references.
adobe.
2019. adobe email usage study..james allan, rahul gupta, and vikas khandelwal.
2001. temporal summaries of new topics.
in pro-ceedings of the 24th annual international acm si-gir conference on research and development in in-formation retrieval, pages 10–18..6903jacob benesty, jingdong chen, yiteng huang, and is-rael cohen.
2009. pearson correlation coefﬁcient.
in noise reduction in speech processing, pages 1–4.
springer..manik bhandari, pranav narayan gour, atabak ash-faq, pengfei liu, and graham neubig.
2020. re-evaluating evaluation in text summarization.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9347–9359, online.
association for computa-tional linguistics..giuseppe carenini, raymond t ng, and xiaodongzhou.
2007. summarizing email conversations withclue words.
in proceedings of the 16th internationalconference on world wide web, pages 91–100..jean carletta, simone ashby, sebastien bourban, mikeflynn, mael guillemot, thomas hain, jaroslavkadlec, vasilis karaiskos, wessel kraaij, melissakronenthal, guillaume lathoud, mike lincoln,agnes lisowska,iain mccowan, wilfried post,dennis reidsma, and pierre wellner.
2006. the amimeeting corpus: a pre-announcement.
in machinelearning for multimodal interaction, pages 28–39,berlin, heidelberg.
springer berlin heidelberg..yen-chun chen and mohit bansal.
2018. fast abstrac-tive summarization with reinforce-selected sentencerewriting.
in proceedings of the 56th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 675–686..jacob cohen.
2013. statistical power analysis for the.
behavioral sciences.
academic press..nick craswell, arjen p vries, and ian m soboroff.
2006. overview of the trec-2005 enterprise track.
in text retrieval conference (trec)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..esin durmus, he he, and mona diab.
2020. feqa: aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5055–5070..bradley efron and robert j tibshirani.
1994. an intro-.
duction to the bootstrap.
crc press..joseph l fleiss and jacob cohen.
1973. the equiv-alence of weighted kappa and the intraclass corre-lation coefﬁcient as measures of reliability.
educa-tional and psychological measurement, 33(3):613–619..shen gao, xiuying chen, zhaochun ren, dongyanzhao, and rui yan.
2020. from standard summa-rization to new tasks and beyond: summarizationin proceedings of thewith manifold information.
twenty-ninth international joint conference on ar-tiﬁcial intelligence, ijcai-20, pages 4854–4860.
in-ternational joint conferences on artiﬁcial intelli-gence organization.
survey track..bogdan gliwa, iwona mochol, maciej biesek, andsamsum corpus: aaleksander wawer.
2019.human-annotated dialogue dataset for abstractivesummarization.
in proceedings of the 2nd workshopon new frontiers in summarization, pages 70–79..karl moritz hermann, tomas kocisky, edward grefen-stette, lasse espeholt, will kay, mustafa suleyman,and phil blunsom.
2015. teaching machines to readand comprehend.
in advances in neural informationprocessing systems, pages 1693–1701..melvin johnson, mike schuster, quoc v le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,et al.
2017. google’s multilingual neural machinetranslation system: enabling zero-shot translation.
transactions of the association for computationallinguistics, 5:339–351..bryan klimt and yiming yang.
2004. the enroncorpus: a new dataset for email classiﬁcation re-search.
in european conference on machine learn-ing, pages 217–226.
springer..tom kocmi and ondrej bojar.
2018. trivial transferlearning for low-resource neural machine translation.
wmt 2018, page 244..klaus krippendorff.
2011. computing krippendorff’s.
alpha-reliability..yanran li and sujian li.
2014. query-focused multi-document summarization: combining a topic modelwith graph-based semi-supervised learning.
in pro-ceedings of coling 2014, the 25th internationalconference on computational linguistics: techni-cal papers, pages 1197–1207..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..yang liu and mirella lapata.
2019a.
hierarchicaltransformers for multi-document summarization.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 5070–5081..yang liu and mirella lapata.
2019b.
text summariza-in proceedings oftion with pretrained encoders.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 3721–3731..6904yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..joshua maynez, shashi narayan, bernd bohnet, andryan mcdonald.
2020. on faithfulness and factu-ality in abstractive summarization.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 1906–1919..alex wang, kyunghyun cho, and mike lewis.
2020.asking and answering questions to evaluate the fac-in proceedings oftual consistency of summaries.
the 58th annual meeting of the association for com-putational linguistics, pages 5008–5020..rada mihalcea and paul tarau.
2004. textrank: bring-ing order into text.
in proceedings of the 2004 con-ference on empirical methods in natural languageprocessing, pages 404–411..amita misra, pranav anand, jean e fox tree, and mar-ilyn walker.
2015. using summarization to discoverargument facets in online idealogical dialog.
in pro-ceedings of the 2015 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, pages430–440..shashi narayan, shay b cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, pages 1797–1807..saul b needleman and christian d wunsch.
1970. ageneral method applicable to the search for simi-larities in the amino acid sequence of two proteins.
journal of molecular biology, 48(3):443–453..douglas oard, william webber, david kirsch, andavocado researchsergey golitsynskiy.
2015.email collection ldc2015t03.
dvd.
philadelphia:linguistic data consortium..radicati.
2015. email statistics report, 2015-2019..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2020. exploring the lim-its of transfer learning with a uniﬁed text-to-texttransformer.
journal of machine learning research,21(140):1–67..revanth rameshkumar and peter bailey.
2020. story-telling with dialogue: a critical role dungeons anddragons dataset.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5121–5134..h scudder.
1965. probability of error of some adap-ieee transac-.
tive pattern-recognition machines.
tions on information theory, 11(3):363–371..jan ulrich, gabriel murray, and giuseppe carenini.
2008. a publicly available annotated corpus forin proc.
of aaaisupervised email summarization.
email-2008 workshop, chicago, usa..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..tianyi zhang, varsha kishore, felix wu, kilian qweinberger, and yoav artzi.
2019. bertscore: eval-in internationaluating text generation with bert.
conference on learning representations..chenguang zhu, ruochen xu, michael zeng, and xue-dong huang.
2020. a hierarchical network for ab-stractive meeting summarization with cross-domainpretraining.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing: findings, pages 194–203..junnan zhu, haoran li, tianshang liu, yu zhou, ji-ajun zhang, and chengqing zong.
2018. msmo:multimodal summarization with multimodal output.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages4154–4164..appendix.
a summary collection.
figure 4 illustrates the questions we asked humanannotators on amazon mechanical turk duringsummary collection.
before these questions, hereare some important instructions we listed on thewebpage: (1) long summary must be longer thanshort summary; (2) summary length can be dynam-ically decided based on the content of the thread;(3) short summary should be a concise and ab-stractive description of what the thread is mainlytalking about; (4) long summary can be a narrativeof what happens.
but do not simply summarizeeach email separately.
the summary should be co-herent; (5) it is not necessary to summarize everyemail in the long summary, i.e., it is ok to skip.
6905figure 4: a part of the amazon mechanical turk webpage used for collecting summaries..unimportant ones and merge similar ones if needed;(6) you are encouraged to include important senderand/or receiver names in long summary; (7) youare disencouraged to copy a lot from emails forboth short and long summaries; you are supposedto write in your own words as much as you can;(8) you may ﬁnd some content are technical.
wedo not expect any background knowledge.
justfocus on the major concerns, decisions, and consen-sus.
(9) in the thread, emails are ordered by time.
however, one email does not necessarily reply tothe previous one.
it can reply to an earlier emailor forward to new receivers.
in other words, thestructure is not always continuous, so please becareful when you read..b fast abs rl.
the original fast abs rl method (chen andbansal, 2018) uses rouge-lrecall to align ex-tracted source sentences and target summary sen-tences.
in our case, we extract emails and alignthem with summary sentences.
since the emailsand summary sentences usually follow the sametemporal order, we enhance the alignment proce-dure by the neeleman-wunsch algorithm (needle-man and wunsch, 1970; rameshkumar and bai-ley, 2020) to imposing strict order constraints, e.g.,there should not be “emaili is aligned to sentencejwhile emaili+1 is aligned to sentencej−1” cases..meanwhile, we modify it to allow one email tobe aligned with multiple summary sentences butavoid one summary sentence aligning with multipleemails.
speciﬁcally, we ﬁrst obtain the similaritymatrix m of size ne × ns between each email andsummary sentence by rouge-lrecall (ne is thenumber of emails, ns is the number of summarysentences); then the alignment score matrix h ofsize (ne+1)×(ns+1) is initialized as all-zero thencomputed as follows for 1 ≤ x ≤ ne, 1 ≤ y ≤ ns:.
hx,y = max.
.
.
hx−1,y−1 + mx−1,y−1hx,y−1 + mx−1,y−1hx−1,y.
then we traceback from hne,ns to h0,0 to obtainthe ﬁnal alignment.
as shown in table 7, the “fastabs rl (default)” model refers to this method withthe default setting which works mostly worse thanour enhanced fast abs rl..c experimental details & additional.
results.
we implement the textrank (mihalcea and ta-rau, 2004) model via the summa python package9and set the summarization ratio as the averagesummary lengthratio in the training set, which isthread length0.22 for short summary and 0.38 for long summary..9https://github.com/summanlp/textrank.
6906we test fast abs rl (chen and bansal, 2018) viathe author’s open-source code.10 most of our mod-els are built on t5 (raffel et al., 2020) and we usethe base version that has 220 million parameters.
our hierarchical t5 shares the same t5 encoderparameters between the token-level and email-levelencoders.
the only new parameters added are fromthe ﬁrst cross attention between decoder and email-level encoder.
we use transformers (wolfet al., 2020)11 to run all the t5 based models.
werun experiments on a single tesla v100 gpu.
weset the max input sequence length as 512 tokensand max output length as 56 tokens during training(200 tokens during evaluation).
the total batch size(with gradient accumulation) is 128. the learningrate is 5e-4, except for training the t5base fromscratch, we use 1e-4 instead.
since our trainingset only contains 1.8k examples, it only takes 2-4minutes per epoch.
we train models for 70 epochs.
our model selection is based on each of the ﬁveevaluation metrics, rouge-1/rouge-2/rouge-l/summary-level rouge-l/bertscore.
we se-lect the best checkpoints for each of the ﬁve metricson our development set, then test those checkpointson the testing set to report the ﬁnal numbers foreach metric.
table 7 shows all the results on ourdevelopment set.
table 8 shows two examples thathave high-rating model-generated summaries..d human evaluation.
figure 5 shows the questions we asked to humanjudges to evaluate the quality of model-generatedsummaries.
before these questions, we instructannotators how to rate on a 5-point likert scalefor “salience” and “faithfulness”: (1) rate saliencefrom 1 to 5: 1 is the worst, none of the points in thesummary is important enough to be summarized; 5is the best, all of the points mentioned in the sum-mary are important and worth to be summarized;(2) rate faithfulness from 1 to 5: 1 is the worst, allof the sentences in the summary are either wrongor not existing in the email thread; 5 is the best, allof the points mentioned in the summary are trueto the thread.
plus, we also prompt examples of“non-salient” and “unfaithful” summaries on thewebpage.
we pay annotators $0.60 per hit..10https://github.com/chenrocks/fast_.
abs_rl.
transformers.
11https://github.com/huggingface/.
6907models.
emailsumshort.
emailsumlong.
r1.
r2.
rl.
rlsum berts.
r1.
r2.
rl.
rlsum berts.
oraclelead-1lead-1-emailtextrankbertsumext.
fast abs rl (default)fast abs rlt5base (from scratch)t5base.
cnndmprexsumpresamsumprecrd3pre.
cnndmjointxsumjointsamsumjointcrd3joint.
semisupw3csemisupavocadosemisuptogether.
39.825.6326.3721.9125.76.
29.6731.5619.7136.78.
37.0036.6336.7236.84.
35.8935.0736.5936.24.
37.0337.7837.43.
13.056.565.884.206.02.
6.086.521.9511.93.
11.2611.4311.111.57.
10.419.2611.2010.43.
11.9212.5612.26.
31.1719.9719.6816.1218.74.
22.6823.0114.8829.50.
28.9729.4328.7329.19.
28.0227.1829.2028.55.
29.3030.0929.84.
36.1521.5123.6119.5722.59.
27.6629.5116.7533.58.
33.4933.7533.2133.38.
32.4131.5333.4932.72.
33.7834.5034.32.
28.5013.5512.986.568.34.
6.925.5922.5234.92.
35.0935.2935.8235.37.
34.0234.2735.4435.52.
35.6034.8835.08.
46.7420.7236.6529.0030.90.
39.4339.2424.5144.94.
44.8344.5544.3144.57.
43.9243.3644.3844.25.
45.0345.4945.73.
17.135.8710.447.158.29.
11.0811.253.7215.94.
15.8815.2915.3615.73.
14.4813.3515.2314.87.
16.0916.2116.27.
33.9215.2326.0020.0020.91.
25.7827.7715.7232.33.
32.0231.5031.4531.87.
30.5429.4431.6831.24.
32.5032.9732.65.
43.118.0133.2725.9227.55.
36.8136.7221.9141.22.
41.2540.8740.6340.91.
39.9939.4540.6940.38.
41.5241.8241.91.hier.
t5base.
36.67.
11.79.
29.13.
33.58.
35.71.
45.26.
16.13.
32.62.
41.55.table 7: summarization performance of different models on the development set..28.388.0918.1110.448.92.
7.149.639.7033.67.
33.8933.4733.6033.47.
31.6730.9733.6533.57.
33.9534.4234.09.
33.99.examples of high-quality summaries generated by the model..thread: subject: faa demos ||| dan: pm team, attached are some general ideas and issues around developing new demosfor our new target markets.
please review and provide feedback.
also, please provide links where we can learn more aboutvarious faa applications.
thanx, dan.
||| dan, thanks for putting the high level descriptions together.
my questions are: *is itpractical to do an eai demo given the inherent complexity of application integration?
... *should we delay looking at outlookfor now?...
*what do you think that timelines are developing these demos?
... alex ||| alex, thanks for the feedback, please seemy comments below:generated short summary: dan asks the pm team to review and provide feedback on ffa demos.
alex responds with questions.
dan thanks alex and gives his feedback.
(salience=4.3, faithfulness=4.7)ground-truth: dan talks about general ideas about demos to his pm team.
alex provides some feedback and asks questions.
danthanks alex for the feedback and adds comments..thread: subject: sun performance report ||| mahesh: hi, i am attaching the draft of the performance/sizing report for emason sun.
please send me your comments.
i am also attaching a list of features that would be good to have.
thanks, mahesh |||amitabh: do we have a side-by-side comparison of solaris, hp-ux, and nt?
also, a price-performance comparison might also beuseful ||| rajeev: dan, please consider amitabh’s suggestions for the sizing requirement document that you are prepaing... |||mahesh: we do not have comparison stats.
it would be good to have them.
||| dan: good points, we should have side-by-sidecomparisons and also price/performance...generated long summary: mahesh is attaching a draft of the performance/sizing report for emas on sun and asking forcomments.
amitabh asks if there is a side-by-side comparison of solaris, hp-ux, and nt.
rajeev asks dan to consider amibh’ssuggestions for the sizing requirement document.
mahesesh says there are no comparison stats, but it would be good to havethem.
dan says there should be side- by-side comparies and also price/performance.
(salience=4.3, faithfulness=5)ground-truth: mahesh shows everyone a performance report for a future meeting and attaches his feedback.
amitabh givesfeedback which rajeev asks dan to consider in a different task.
mahesh and dan make suggestions about comparisons..table 8: examples of high-quality summaries generated by model.
emails are separated by ‘|||’ and some contentare omit by ‘...’.
(salience=xx, faithfulness=xx) gives the average human rating for that summary..6908figure 5: a part of the amazon mechanical turk webpage used for human evaluation..6909