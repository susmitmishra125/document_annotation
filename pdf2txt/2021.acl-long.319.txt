breaking down walls of text: how can nlp beneﬁt consumer privacy?.
abhilasha ravichander♦ alan w black♦ thomas norton♠shomir wilson♥ norman sadeh♦♦carnegie mellon university, pittsburgh, pa♠fordham law school, new york, ny ♥penn state university, university park, pa{aravicha, awb, sadeh}@cs.cmu.edu{shomir}@psu.edu, {tnorton1}@law.fordham.edu.
abstract.
privacy plays a crucial role in preservingdemocratic ideals and personal autonomy.
the dominant legal approach to privacy inmany jurisdictions is the “notice and choice”paradigm, where privacy policies are the pri-mary instrument used to convey informationto users.
however, privacy policies are longand complex documents that are difﬁcult forusers to read and comprehend.
we discuss howlanguage technologies can play an importantrole in addressing this information gap, report-ing on initial progress towards helping threespeciﬁc categories of stakeholders take advan-tage of digital privacy policies: consumers, en-terprises, and regulators.
our goal is to pro-vide a roadmap for the development and useof language technologies to empower users toreclaim control over their privacy, limit pri-vacy harms, and rally research efforts fromthe community towards addressing an issuewith large social impact.
we highlight manyremaining opportunities to develop languagetechnologies that are more precise or nuancedin the way in which they use the text of privacypolicies..1.introduction.
privacy is a fundamental right central to a demo-cratic society, in which individuals can operate asautonomous beings free from undue interferencefrom other individuals or entities (assembly, 1948).
however, certain functions of privacy, such as thepower to grant or deny access to one’s personalinformation, are eroded by modern commercialand business practices that involve vast collection,linking, sharing, and processing of digital personalinformation through an opaque network, often with-out data subjects’ knowledge or consent.
in manyjurisdictions, online privacy is largely governedby “notice and choice” (federal trade commis-sion, 1998).
under this framework, data-collecting.
and data-processing entities publish privacy poli-cies that disclose their data practices.
theoreti-cally, users are free to make choices about whichservices and products they use based on the disclo-sures made in these policies.
thus, the legitimacyof this framework hinges on users reading a largenumber of privacy policies to understand what datacan be collected and how that data can be processedbefore making informed privacy decisions..in practice, people seldom read privacy policies,as this would require prohibitive amounts of theirtime (mcdonald and cranor, 2008; cate, 2010;cranor, 2012; reidenberg et al., 2015; schaubet al., 2015; jain et al., 2016).
thus, an oppor-tunity exists for language technologies to bridgethis gap by processing privacy policies to meet theneeds of internet and mobile users.
nlp has madeinroads in digesting large amounts of text in do-mains such as scientiﬁc publications and news (jainet al., 2020; cachola et al., 2020; kang et al., 2018;rush et al., 2015; see et al., 2017), with severalpractical tools based on these technologies help-ing users every day (cachola et al., 2020; tldr,2021; news, 2021).
these domains have alsoreceived considerable research attention: severalbenchmark datasets and technologies are based intexts from these domains (nallapati et al., 2016;see et al., 2017; narayan et al., 2018; beltagy et al.,2019).
we highlight that the privacy domain canalso beneﬁt from increased research attention fromthe community.
moreover, technologies developedin the privacy domain have potential for signiﬁcantand large-scale positive social impact—the affectedpopulation includes virtually every internet or mo-bile user (sadeh et al., 2013)..automated processing of privacy policies opensthe door to a number of scenarios where languagetechnologies can be developed to support users inthe context of different tasks.
this includes sav-ing data subjects the trouble of having to read the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4125–4140august1–6,2021.©2021associationforcomputationallinguistics4125entire text of policies when they are typically onlyconcerned about one or a small number of issues(e.g., determining whether they can opt out of somepractices or whether some of their data might beshared with third parties).
it includes helping com-panies ensure that they are compliant and that theirprivacy policies are consistent with what their codeactually does.
it also includes supporting regula-tors, as they face the daunting task of enforcingcompliance across an ever-growing collection ofsoftware products and processes, including sophis-ticated data collection and use practices.
in thiswork, we conduct an extensive survey of initialprogress in applying nlp to address limitations ofthe notice and choice model.
we expect our workto serve as a useful starting point for practitioners tofamiliarize themselves with technological progressin this domain, by providing both an introductionto the basic privacy concerns and frameworks sur-rounding privacy policies, as well as an accountof applications for which language technologieshave been developed.
finally, we highlight manyremaining opportunities for nlp technologies toextract more precise, more nuanced, and ultimatelymore useful information from privacy policy text—describing key challenges in this area and layingout a vision for the future..2 privacy as a social good.
in 1890, warren and brandeis deﬁned the right toprivacy as “the right to be let alone”(warren andbrandeis, 1890).
more recently, westin deﬁned theright as “the claim of individuals, groups, or institu-tions to determine for themselves when, how, andto what extent information about them is communi-cated to others” (westin, 1968).
a primary aspira-tion of privacy is to allow for the separation of indi-vidual and society as a means of fostering personalautonomy.
to that end, privacy “protects the sit-uated practices of boundary management throughwhich the capacity for self-determination develops,”and further “shelters dynamic, emergent subjectiv-ity from the efforts of commercial and governmentactors to render individuals and communities ﬁxed,transparent, and predictable” (cohen, 2012).
pri-vacy, therefore, is “foundational to the practice ofinformed and reﬂective citizenship,” and serves as“an indispensable structural feature of liberal demo-cratic political systems” (cohen, 2012)..when privacy is threatened, we risk losing thechance for critical self-reﬂection of political pro-.
cesses and social norms.
indeed, privacy under-girds the concepts of human dignity and other keyvalues, such as the freedoms of association andspeech.
for these reasons and others, privacy isregarded as a fundamental human right (assem-bly, 1948).
in the digital age, privacy is threatenedby aggressive, rapid, and largely automated col-lection, linking, sharing, and processing of digitalpersonal information.
digital privacy is intrinsi-cally linked to the fundamental ethical principlesof transparency, fairness and agency..• transparency: users have a right to know how in-formation about them is collected and used.
enti-ties collecting user data stay clear of manipulativeschemes designed to inﬂuence the data subject’swillingness to disclose their data (e.g.
overem-phasizing beneﬁts while remaining silent aboutpotential risks associated with the disclosure ofdata in a given context)..• fairness: users should receive perceived valuecommensurate to the perceived loss of privacyassociated with disclosure and use of their data..• agency: users should have a choice about whatdata is collected about them and how it is used..the dominant paradigm to address these princi-ples in the united states and most legal jurisdic-tions around the world, is the ’notice and choice’regulatory framework (westin, 1968; federal tradecommission, 1998).
’notice and choice’ regimesare based on the presupposition that consumerswill adequately manage their privacy, if providedsufﬁcient information about how their data will becollected, used and managed, as well as offeredmeaningful choices.
today, ’notice’ is often prac-tically realized through publishing privacy policies,which are long and verbose documents that usersare expected to read and understand.
‘choice’ isoften limited to the user clicking ‘i agree’ to theprivacy policy, or even interpreting their contin-ued use of the service as some sort of meaningfulconsent to the terms of the policy..the ’notice and choice’ framework is funda-mentally broken.
in practice, users seldom read pri-vacy policies (mcdonald and cranor, 2008; cate,2010; us federal trade commission et al., 2012)and it is prohibitively expensive for them to even doso.
mcdonald and cranor (2008) estimate that ifinternet users were to actually read the privacy poli-cies of the websites they visited, they would haveto spend roughly 250 hours each year just reading.
4126challenge.
example.
ambiguity we may also use aggregate personal information for regulatory compliance, industry and market analysis,research, demographic proﬁling, marketing and advertising, and other business purposes..vagueness.
[x] collects, or may have a third-party service providers collect, non-personally-identifying information ofthe sort that mobile applications typically make available, such as the type of device using the application,the operating system, location information, and aggregated user statistics..modality.
if you use our services to make and receive calls or send and receive messages, we may collect call andmessage log information like your phone number, calling-party number, receiving-party number....negation.
no apps have access to contact information, nor do they read or store any contact information.
lists anddocumentstructure.
we may collect data or ask you to provide certain data when you visit and use our websites, products andservices.
the sources from which we collect personal data include:.
• data collected directly from you or your device .... ;.
• if we link other data relating to you with your personal data, we will treat that linked data as personal.
data; and.
• we may also collect personal data from trusted third-party sources.....tabularunder-standing.
reasons we can share your personal information does x share?
can you limit this sharing?
for our everyday business purposes ...for our everyday marketing purposes ...for joint marketing with other companies.
nonowe don’t share.
yesyesno.
table 1: examples of some challenging aspects for language understanding in privacy policies, including reasoningover ambiguity and vagueness, modality, negation (including scope),lists and document structure, and tables..privacy policies.
a 2014 report from the presidentscouncil of advisors on science and technologystated that “only in some fantasy world” were usersreading and understanding privacy policies beforegiving their consent (of the president’s councilof advisors on science and technology, 2014).
indeed, 91% of people in the u.s have reportedfeeling like they have lost control over their infor-mation (madden et al., 2014).
moreover, recentprivacy laws such as the eu’s general data pro-tection regulation (gdpr) (regulation, 2016) stillfail to address the critical limitation of notice andchoice:the continued reliance on users to readand understand a large number of privacy policies.
studies have shown that gdpr requirements haveactually resulted in longer privacy policies (lindenet al., 2020), and users still encounter unreadableprivacy policies (becher and benoliel, 2019)..the lack of respect for individuals’ rights to pri-vacy also has implications for society.
with socialplatforms in particular having access to an unprece-dented scale of information about human behaviour,vicario et al.
(2019) discuss that users’ polarizationand conﬁrmation bias can play a role in spread-ing misinformation on social platforms.
maddenet al.
(2017) report that particular groups of less-privileged users on the internet are uniquely vulner-able to various forms of surveillance and privacyharms, which could widen existing economic gaps..introna (1997) describe privacy as central to humanautonomy in social relationships.
in this work, weexamine the potential of language technologies inenabling people to derive the beneﬁts of their rightsto transparency, fairness and agency..3 can nlp help privacy?.
privacy policies present interesting challenges fornlp practitioners, as they often feature characteris-tic aspects of language that remain under-examinedor difﬁcult to process (table.
1).
for example,while many policies discuss similar issues sur-rounding how user data is collected, managed andstored, policy silence about certain data practicesmay carry great weight from a legal, policy, and reg-ulatory perspective.1 in the privacy policy domain,understanding what has not been said in a privacypolicy (policy silence) is just as important as un-derstanding what is said (zimmeck et al., 2019a;marotta-wurgler, 2019)..further, though policies tend to feature literallanguage (compared to more subjective domainslike literature or blog posts), processing them ef-.
1for example, in united states v. path, the defendant’s(path) privacy policy described that its app collected ”certaininformation such as your internet protocol (ip) address, youroperating system, the browser type.” the federal trade com-mission found this disclosure to be incomplete and insufﬁcientto provide notice about the collection of users’ contact data(ftc, 2013)..4127task.
data practice identiﬁcation(wilson et al., 2016b).
annotate segments of privacy policies withdescribed data practices..goal.
consumer regulator enterprise.
opt-out identiﬁcation(sathyendra et al., 2017; bannihatti kumar et al., 2020).
extract opt-out choices buried inprivacy policy text..compliance analysis(zimmeck et al., 2017, 2019a).
analyze mobile app code and privacy policyto identify potential compliance issues..privacy question-answering(ravichander et al., 2019; ahmad et al., 2020).
allow consumers to selectively queryprivacy policies for issues that are important to them..policy summarization(zaeem et al., 2018; keymanesh et al., 2020).
construct summaries to aid consumersto quickly digest the content of privacy policies..readability analysis(massey et al., 2013; meiselwitz, 2013).
characterize the ease of understanding or comprehensionof privacy policies..(cid:51).
(cid:51).
(cid:51).
(cid:51).
(cid:51).
(cid:51).
(cid:51).
(cid:51).
(cid:51).
table 2: overview of some applications of nlp to privacy policies, and primary stakeholders they are intended tobeneﬁt..fectively also requires several additional capabil-ities such as reasoning over vagueness and am-biguity, understanding elements such as lists (in-cluding when they are intended to be exhaustiveand when they are not (bhatia et al., 2016)), ef-fectively incorporating ‘co-text’- aspects of webdocument structure such as document headers thatare meaningful semantically to the content of pri-vacy policies(mysore gopinath et al., 2018) andincorporating domain knowledge (for example, un-derstanding whether information is sensitive re-quires background knowledge in the form of ap-plicable regulation).
privacy policies also differfrom several closely related domains, such as le-gal texts which are largely meant to be processedby domain experts.
in contrast, privacy policiesare legal documents with legal effects—generallydrafted by experts—that are ostensibly meant to beunderstood by everyday users.
nlp applications inthe privacy domain also need to be designed withend user requirements in mind.
for example, froma legal standpoint, when generating answers to auser’s question about the content of a privacy pol-icy, it is generally advisable to include disclaimers,but users may prefer to be presented with shorteranswers, where disclaimers are kept as short as pos-sible.
challenges are described in more detail in(§4)..we survey current efforts to apply nlp in theprivacy domain, discussing both existing task for-mulations as well as future areas in this domainwhere language technologies can have impact.
2.
2our survey includes relevant papers from major nlpvenues, including acl, emnlp, naacl, eacl, coling,conll, semeval, tacl, and cl.
we supplemented thesepublications with a review of the literature at venues such assoups, pets, www, acm, and ndss.
we also includedrelevant legal venues, such as law reviews and journals..3.1 data practice identiﬁcation.
initial efforts in applying nlp in the privacy do-main have largely focused on discovering or iden-tifying data practice categories in privacy poli-cies (costante et al., 2012a; ammar et al., 2012;costante et al., 2012b; liu et al., 2014b; ramanathet al., 2014a; wilson et al., 2016b).
automating theidentiﬁcation of such data practices could poten-tially support users in navigating privacy policiesmore effectively3, as well as automate analysis forregulators who currently do not have techniques toassess a large number of privacy policies.
wilsonet al.
(2016b) create a corpus of 115 website pri-vacy policies annotated with detailed informationof the privacy policies described.
the corpus andassociated taxonomy have been of utility in the de-velopment of several subsequent privacy-enhancinglanguage technologies (mysore sathyendra et al.,2017a; zimmeck et al., 2017; ravichander et al.,2019; ahmad et al., 2020)..3.2 choice identiﬁcation.
studies have shown that consumers desire controlover the use of their information for marketingcommunication, and object to the use of their in-formation for web tracking or marketing purposesincluding targeted advertising (cranor et al., 2000;turow et al., 2009; ur et al., 2012; bleier andeisenbeiss, 2015).
however, mcdonald and cra-nor (2010) ﬁnd that many people are unaware of theopt-out choices available to them.
these choicesare often buried in policy text, and thus there hasbeen interest in applying nlp to extract choicelanguage.
mysore sathyendra et al.
(2017b) auto-matically identify choice instances within a privacy.
3for example, through the data exploration tool developedby the usable privacy policy project: https://explore.
usableprivacy.org/?view=machine.
4128aid compliance analysis is detailed by zimmecket al.
(2017), including results of a systematic anal-ysis of 17,991 apps using both natural languageprocessing and code analysis techniques.
classi-ﬁers are trained to identify data practices based onthe opp-115 ontology (wilson et al., 2016b), andstatic code analysis techniques are employed to ex-tract app’s privacy behaviors.
the results from thetwo procedures are compared to identify potentialcompliance issues.
the system was piloted withpersonnel at the california ofﬁce of the attorneygeneral.
users reported that the system could sig-niﬁcantly increase productivity, and decrease theeffort and time required to analyze practices in appsand audit compliance.
zimmeck et al.
(2019b) re-view 1,035,853 apps from the google play storefor compliance issues.
their system identiﬁes dis-closed privacy practices in policies using classi-ﬁers trained on the app-350 corpus (story et al.,2019), and static code analysis techniques to iden-tify apps’ privacy behaviors.
results of the analysisof this large corpus of privacy policies revealed aparticularly large number of potential complianceproblems, with a subset of results shared with thefederal trade commission.
the system was alsoreported to have been used by a large electronicsmanufacturer to verify compliance of legacy mo-bile apps prior to the introduction of gdpr..3.4 policy summarization.
due to the lengthy and verbose nature of privacypolicies, it is appealing to attempt to develop auto-mated text summarization techniques to generateshort and concise summaries of a privacy policy’scontents (liu et al., 2015).
tomuro et al.
(2016)develop an extractive summarization system thatidentiﬁes important sentences in a privacy policyalong ﬁve categories: purpose, third parties, limitedcollection, limited use and data retention.
zaeemet al.
(2018, 2020) identify ten questions about pri-vacy policies, and automatically categorize ‘risklevels’ associated with each of the questions, asshown in table.
3. keymanesh et al.
(2020) focuson extractive summarization approaches to iden-tify ‘risky sections’ of the privacy policy, which aresentences that are likely to describe a privacy riskposed to the end-user.
however, while automatedsummarization seems like a promising applicationof language technologies, identifying which partsof a policy should be shown to users is exceedinglydifﬁcult, and studies by privacy experts have shown.
figure 1: the results from opt-out easy, a browser ex-tension to extract opt-out choices from privacy policies,for overleaf.com (bannihatti kumar et al., 2020)..policy, labeling different types of opt-out choices,with a particular emphasis on extracting actionablechoices in the policy, i.e.
those associated with hy-perlinks.
bannihatti kumar et al.
(2020) develop aweb-browser extension to present extracted choiceinstances to users (figure.
1), ﬁnding that the toolcan considerably increase awareness of choicesavailable to users and reduce the time taken to iden-tify actions the users can take..3.3 compliance analysis.
in 2012, six major mobile app stores entered intoan agreement with the california attorney gen-eral, where they agreed to adopt privacy princi-ples that require mobile apps to have privacy poli-cies(justice, 2012).
regulations such as the theeu general data protection directive (gdpr) andthe california consumer protection act (ccpa)impose further requirements on what entities col-lecting and using personal data need to disclose intheir privacy policies and what rights they need tooffer to their users (e.g.
privacy controls, option torequest deletion of one’s data).
however, regula-tors lack the necessary resources to systematicallycheck that these requirements are satisﬁed.
in fact,even app stores lack the resources to systematicallycheck that disclosures made in privacy policies areconsistent with the code of apps and comply withrelevant regulatory requirements.
thus, there hasbeen interest in developing technologies to automat-ically identify potential compliance issues (encket al., 2014; zimmeck et al., 2017; wang et al.,2018; libert, 2018a; zimmeck et al., 2019b)..a ﬁrst application of language technologies to.
4129#.
(1).
(2).
(3)(4)(5)(6)(7).
(8).
(9).
(10).
question.
green risk level.
yellow risk level.
red risk level.
not asked for.
not asked for.
how well does this website protect your email address?
how well does this website protect your credit cardinformation and address?
how well does this website handle your social security number?
not asked fordoes this website use or share your pii for marketing purposes?
does this website track or share your location?
does this website collect pii from children under 13?
does this website share your information with law enforcement?
pii not recordeddoes this website notify or allow you to opt-outafter changing their privacy policy?
does this website allow you to edit or delete your informationfrom its records?
does this website collect or share aggregated data relatedto your identity or behavior?.
not aggregated.
edit/delete.
used for intended service.
shared w/ third parties.
used for intended service.
shared w/ third parties.
used for intended service.
pii not used for marketing pii used for marketingnot trackednot collected.
used for intended servicenot mentionedlegal docs required.
shared w/ third partiespii shared for marketingshared w/ third partiescollectedlegal docs not required.
posted w/ opt-out option.
posted w/o opt-out option not posted.
edit only.
no edit/delete.
aggregated w/o pii.
aggregated w/ pii.
table 3: ten privacy questions used for summarization, and associated ‘risk levels’ from (zaeem et al., 2018)..that such ‘one-size-ﬁts-all’ approaches are unlikelyto be effective (gluck et al., 2016; rao et al., 2016)..3.5 privacy question-answering.
a desire to move away from ‘one-size-ﬁts-all’ ap-proaches has led to increased interest in supportingautomated privacy question-answering (qa) capa-bilities.
if realized, such functionality will helpusers selectively and iteratively explore issues thatmatter most to them.
table 4 lists current efforts todevelop resources for privacy question-answering.
amongst the initial explorations in this area, hark-ous et al.
(2018) examine privacy questions askedby twitter users to companies, with answers an-notated by the paper’s authors.
ravichander et al.
(2019) collect questions asked by crowdworkersabout a mobile app without seeing the app’s pri-vacy policy, and hire legal experts to identify sen-tences in the privacy policy relevant for each ques-tion.
(ahmad et al., 2020) provide ‘skilled anno-tators’ with privacy policy segments drawn fromthe opp-115 corpus (wilson et al., 2016b), and askthem to construct questions based on the providedspan of text.
ravichander et al.
(2019) and ahmadet al.
(2020) both ﬁnd that current qa baselinesbased on pretrained language models(devlin et al.,2019) are inadequate for answering privacy ques-tions.
ahmad et al.
(2020) indicate that identifyinglonger evidence spans are challenging and describetransfer learning as a potential direction to improveperformance.
ravichander et al.
(2019) examineunanswerability as a challenge to privacy qa sys-tems, highlighting the many facets of unanswerablequestions that can be asked.
it is worth noting thatall three resources formulate ground truth based inthe text of the privacy policy, but policy languageis difﬁcult for non-experts to understand (reiden-berg et al., 2015).
future qa dataset architects.
could consider abstractive answers as ground truths,which are validated by legal experts for correctnessand evaluated by users for helpfulness.
it may alsobe desirable for benchmarks to aim for ecologicalvalidity (de vries et al., 2020), with users askingquestions, and legal experts constructing answers..3.6 other applications.
in this section, we survey further tasks wherenlp has been applied to consumer privacy, includ-ing analyzing privacy policy readability, with thegoal of aiding writers of privacy policies (fabianet al., 2017; massey et al., 2013; meiselwitz,2013; ermakova et al., 2015), and understandingdata practice categories are described in a pol-icy, known as measuring policy coverage (lin-den et al., 2020; shvartzshnaider et al., 2020).
a signiﬁcant amount of recent work has also fo-cused on information extraction from privacy poli-cies (costante et al., 2012a).
shvartzshanider et al.
(2018); shvartzshnaider et al.
(2019, 2020) iden-tify contextual integrity parameters (nissenbaum,2004) in policy text.
studies have also tried toextract other, more speciﬁc kinds of informationfrom policies, such as third party entities (libert,2018b; bokaie hosseini et al., 2020) and infor-mation about regulated information types (bhatiaet al., 2016; evans et al., 2017) as well as theirsimilarity (hosseini et al., 2016).
there have alsobeen efforts to analyze vague statements in privacypolicies (liu et al., 2016b; lebanoff and liu, 2018),and explore how benchmarks in this domain canbe constructed through crowdsourcing (ramanathet al., 2014b; wilson et al., 2016c; audich et al.,2018).
lastly, there has been research focusedon identifying header information in privacy poli-cies (mysore gopinath et al., 2018) and generat-ing them (gopinath et al., 2020).
techniques to.
4130dataset.
#questions.
questionscenario.
legal expertannotator.
askercannot seeevidence.
unanswerablequestions.
non-contiguousanswer.
polisis(harkous et al., 2018).
privacyqa(ravichander et al., 2019).
policyqa(ahmad et al., 2020).
120.
1750.
714.twitter users ask questionsto a company..crowdworkers ask questionsabout a mobile app..skilled annotators are shown a text spanand data practice, and asked to constructa question..(cid:55).
(cid:51).
(cid:55).
(cid:51).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:55).
(cid:55).
(cid:51).
(cid:55).
table 4: comparison of polisis (harkous et al., 2018), privacyqa (ravichander et al., 2019) and policyqa (ahmadet al., 2020) qa datasets.
question scenario describes conditions under which the questions were generated.
‘asker cannot see evidence’ indicates the asker of the question was not shown evidence from the document whenformulating questions.
unanswerable questions indicates if the corpus includes unanswerable questions.
‘noncontriguous answer’ indicates the answers are allowed to be from non-adjacent segments of the privacy policy..process privacy policies have largely followed suc-cessful approaches elsewhere in nlp, starting fromfeature-based approaches (sathyendra et al., 2017;zimmeck et al., 2019a), training domain-speciﬁcword embeddings (kumar et al., 2019) and ﬁne-tuning pretrained language models on privacy poli-cies (nejad et al., 2020; mustapha et al., 2020)..3.7 towards new tasks and formulations.
we discuss a vision of future applications of nlpin aiding consumer privacy.
we believe these ap-plications present interesting opportunities for thecommunity to develop technologies, both becauseof the technical challenges they offer and the im-pact they are likely to have..detecting surprising statements: since usersdo not read privacy policies, their expectations forthe data practices of services might not align withservices’ actual practices.
these mismatches mayresult in unexpected privacy risks which lead to lossof user trust (rao et al., 2016).
identifying such‘surprising’ statements will require understandingsocial context and domain knowledge of privacyinformation types.
for example, it is natural fora banking website to collect payment information,but not health information.
moreover, understand-ing what statements will be surprising for each in-dividual user requires understanding their personal,social and cultural backrounds (rao et al., 2016).
we speculate that nlp can potentially be leveragedto increase transparency by identifying discordantstatements within privacy policies..detecting missing information:in contrast todetecting surprising statements, privacy policiesmay be underspeciﬁed.
story et al.
(2018) ﬁnd thatmany policies contain language appearing in unre-lated privacy policies, indicating that policy writers.
may use privacy policy generators not suited totheir application, potentially resulting in missinginformation.
techniques from compliance analysiscould help in ﬂagging some of these issues (zim-meck et al., 2017, 2019a)..generating privacy nutrition labels: one pro-posal to overcome the gap in communicating pri-vacy information to users has been the privacy ‘nu-trition label’ approach (kelley et al., 2009, 2013),as shown in fig.
2. the proposal draws from indus-tries such as nutrition, warning and energy labelingwhere information has to be communicated to con-sumers in a standardized way.
recently, appleannounced that developers will be required to pro-vide information for these labels (campbell, 2020),which disclose to the user the information a com-pany and third parties collect.4 this approach couldpotentially be helpful to users to understand privacyinformation at a glance, but presents challengesto both developers and app platforms.
develop-ers need to ensure their nutrition label is accurateand platforms need to enforce compliance to theserequirements.
potentially, early successes of lan-guage technologies in compliance systems can beextended to analyzing a speciﬁed nutrition label,policy and application code.
nlp may also beused to generate nutrition labels which developersinspect, as opposed to the more costly process ofdevelopers specifying nutrition labels from scratchwhich may hinder adoption (fowler, 2021)..personalized privacy summaries: one ap-proach to mitigating inadequacies of policysummarization—where generic summaries maynot be sufﬁciently complete —is personalized sum-marization (d´ıaz and gerv´as, 2007; hu et al.,.
4an example of such a nutrition label can be found in.
appendix.
a.
41312012).
in this formulation, policies are summarizedfor each user based on issues that matter most tothem.
this formulation may alleviate some down-sides of qa approaches, which require the userknow how to manage their privacy by asking theright questions.
personalized summarization sys-tems would beneﬁt from modeling users’ level ofknowledge, as well as their beliefs, desires andgoals.
in nlp, there has been effort towards ad-dressing similar challenges for personalized learn-ing in intelligent tutoring (mclaren et al., 2006;malpani et al., 2011)..assistive policy writing: we speculate ad-vances in natural language generation and compli-ance analysis techniques may jointly be leveragedto help app developers create more accurate pri-vacy policies, rather than relying on policy genera-tors (story et al., 2018).
privacy policies generallycover a known set of data practices (wilson et al.,2016a), providing potential statistical commonali-ties to aid natural language generation.
code anal-ysis can be leveraged to constrain generation toaccurately describe data practices of a service..4 progress and challenges.
although privacy policies have legal effects formost internet users, these types of texts constitutean underserved domain in nlp.
nlp has the po-tential to play a role in easing user burden in un-derstanding salient aspects of privacy policies, helpregulators enforce compliance and help developersenhance the quality of privacy policies by reduc-ing the effort required to construct them.
yet, theprivacy domain presents several challenges that re-quire specialized resources to deal with effectively.
we describe some of these distinctive challenges,as well as the capabilities that will need to be de-veloped to process policies satisfactorily..• disagreeable privacy policies: privacy policiesare complex, but are the most important sourceof information about how user data is collected,managed and used.
reidenberg et al.
(2015) ﬁndthat sometimes discrepancies can arise in the in-terpretation of policy language, even betweenexperts.
this additional complexity should betaken into consideration by those developing lan-guage technologies in this domain..• difﬁculty or validity of collecting annotations:privacy policies are legal documents that havelegal effects on how user data is collected and.
used.
while crowdworkers have been found toprovide non-trivial annotations for some tasksin this domain (wilson et al., 2016c), individ-ual practitioners constructing applications mustcarefully consider the consequences of sourcingnon-expert annotations in the context of theirtask and the impacted stakeholders, and not relyon crowdsourced annotation simply because it ischeaper or easier to scale..• difﬁcult for users to articulate their needsand questions: developing effective privacyqa functionality will require understanding thekinds of questions users ask and quantifyingto what extent privacy literacy affects users’ability to ask the right questions.
ravichanderet al.
(2019) ﬁnd many questions collected fromcrowdworkers were either incomprehensible, ir-relevant or atypical.
understanding these factorscould lead to the development of more proactiveqa functionality- for example, rather than waitfor users to form questions, the qa system couldprompt users to reﬂect on certain privacy issues..• challenges.
to qa: additionally,.
privacyquestion-answering systems themselves will re-quire several capabilities in order to have largerimpact.
these systems must be capable of do-ing question-answering iteratively, working withthe user towards resolving information-seekingneeds.
they will also need to consider unan-swerability(rajpurkar et al., 2018; ravichanderet al., 2019; asai and choi, 2020) as a gradedproblem, recognizing to what extent the privacypolicy contains an answer and communicatingboth what is known and what is not known to theuser.
qa systems must also consider what kindsof answers are useful, identifying appropriate re-sponse format and tailoring answers to the user’slevel of knowledge and individual preferences.
• domain knowledge: it remains an open questionhow to best incorporate expert knowledge intothe processing of privacy policies.
although pri-vacy policies are intended to be read by everydayusers, experts and users often disagree on theirinterpretations (reidenberg et al., 2015)..• combining disparate sources of information:while privacy policies are the single most impor-tant source of information about collection andsharing practices surrounding user data, tech-nologies to address users’ personalized concernscould leverage additional sources of information-such as analyzing the code of a given technology.
4132such as a mobile app, news articles, or back-ground knowledge of a legal, technical or sta-tistical nature.
for example, when the policyis silent on an issue- a qa system could reportthe practices of other similiar services to theuser, or if a user asks about the likelihood of adata breach, the qa system could refer to newssources for information about the service..• user modeling:.
personalized privacy ap-proaches will also need to model individualuser’s personal, social and cultural contexts todeliver impact.
this could include informa-tion about the issues likely to matter most tousers, their background knowledge, privacy pref-erences and expectations (liu et al., 2014a; linet al., 2014; liu et al., 2016a)..• accessibility: efforts to help users understandprivacy policies by breaking through walls oftext to identify salient aspects, are expected tohelp users with a range of visual impairmentsnavigate their privacy.
future work would con-duct user studies to determine the extent to whichdeveloped technologies ease visually-impairedusers’ accessibility to learn about the content ofpolicies, related to their interests or concerns..5 ethical considerations.
while nlp has the potential to beneﬁt consumerprivacy, we emphasize there are also ethical con-siderations to be taken in account.
these include:.
bias of agent providing technology: a factorthat must be considered in the practical deploymentof nlp systems in this domain is the incentives ofthe entity creating or providing the technology.
forexample, the incentives of a company that developsa qa system to answer questions about its ownprivacy policy may not align with those of a trustedthird-party privacy assistant that reviews the pri-vacy policies of many different companies.
thisinformation also needs to be communicated in anaccurate and unbiased fashion to users..user trust: while nlp systems have the poten-tial to digest policy text and present informationto users, nlp systems are seldom completely ac-curate, and therefore it is important that users beappropriately informed of these limitations.
forexample, if a qa system communicates a data prac-tice incorrectly in response to a users’ question andthe user encounters privacy harms contrary to theirexpectations as a result, they may lose trust in the.
system.
it is important to also identify appropriatedisclaimers to accompany nlp systems to manageuser expectations..discriminatory outcomes:it is possible thatdifferent populations will beneﬁt to different ex-tents from the developed technologies, and we areyet unable to anticipate precisely where the beneﬁtswill accrue.
for example, users with higher degreesof privacy literacy may be able to take better advan-tage of a developed qa system..technological solutionism:it is important toconsider that while language technologies have thepotential to considerably alleviate user burden inreading privacy policies, they are unlikely to com-pletely resolve the issue that users are unable toread and review a multitude of privacy policieseveryday.
advances toward addressing the limita-tions of notice and choice will also require progressin regulation and enforcement by regulatory bodiesto ensure that enterprises are more accurate in theirdisclosures and use clearer language, in tandemwith creative technological solutions..6 conclusion.
privacy is about the right of people to control thecollection and use of their data.
today privacy re-lies on the ’notice and choice’ framework, whichassumes that people actually read the text of pri-vacy policies.
this is a fantasy as users do not havethe time to do so.
in this article, we summarize howlanguage technologies can help overcome this chal-lenge and support the development of solutions thatassist customers, technology providers and regula-tors.
we reviewed early successes and presented avision of how nlp could further help in the future.
we hope this article will motivate nlp researchersto contribute to this vision and empower people toregain control over their privacy..acknowledgements.
this research was supported in part by grants fromthe national science foundation secure and trust-worthy computing program (cns-1330596, cns-1330214, cns-15-13957, cns-1801316, cns-1914486, cns-1914444) and darpa(fa8750-15-2-0277).
part of the work summarized in this pa-per was conducted by the usable privacy policyproject(https://usableprivacy.org).
the au-thors would like to thank siddhant arora, rex chenand aakanksha naik for valuable discussion..4133references.
wasi ahmad, jianfeng chi, yuan tian, and kai-weichang.
2020. policyqa: a reading comprehensiondataset for privacy policies.
in findings of the as-sociation for computational linguistics: emnlp2020, pages 743–749, online.
association for com-putational linguistics..waleed ammar, shomir wilson, norman sadeh, andnoah a smith.
2012. automatic categorization ofprivacy policies: a pilot study.
technical reportcmu-lti-12-019, carnegie mellon university..third-party entities in natural language privacy poli-cies.
in proceedings of the second workshop on pri-vacy in nlp, pages 18–27, online.
association forcomputational linguistics..isabel cachola, kyle lo, arman cohan, and danielweld.
2020. tldr: extreme summarization of sci-in findings of the associationentiﬁc documents.
for computational linguistics: emnlp 2020, pages4766–4777, online.
association for computationallinguistics..ian carlos campbell.
2020. apple will require apps toadd privacy ‘nutrition labels’ starting december 8th..akari asai and eunsol choi.
2020..challengesin information seeking qa: unanswerable ques-arxiv preprinttions and paragraph retrieval.
arxiv:2010.11915..fred h cate.
2010. the limits of notice and choice..ieee security & privacy, 8(2)..julie e cohen.
2012. what privacy is for.
harv.
l. rev.,.
un general assembly.
1948. universal declaration of.
human rights.
un general assembly, 302(2)..dhiren a audich, rozita dara, and blair nonnecke.
2018. privacy policy annotation for semi-automatedanalysis: a cost-effective approach.
in ifip interna-tional conference on trust management, pages 29–44. springer..vinayshekhar bannihatti kumar, roger iyengar, na-mita nisal, yuanyuan feng, hana habib, peterstory, sushain cherivirala, margaret hagan, lor-rie cranor, shomir wilson, et al.
2020. finding achoice in a haystack: automatic extraction of opt-in pro-out statements from privacy policy text.
ceedings of the web conference 2020, pages 1943–1954..shmuel i becher and uri benoliel.
2019. law in booksthe readability of privacy poli-and law in action:cies and the gdpr.
in consumer law and economics,pages 179–204.
springer..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁc text.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3615–3620, hong kong, china.
association for computa-tional linguistics..jaspreet bhatia, morgan c evans, sudarshan wadkar,and travis d breaux.
2016. automated extractionof regulated information types using hyponymy re-in 2016 ieee 24th international require-lations.
ments engineering conference workshops (rew),pages 19–25.
ieee..alexander bleier and maik eisenbeiss.
2015. the im-portance of trust for personalized online advertising.
journal of retailing, 91(3):390–409..mitra bokaie hosseini, pragyan k c, irwin reyes, andidentifying and classifying.
serge egelman.
2020..126:1904..elisa costante, jerry den hartog, and milan petkovi´c.
in data2012a.
what websites know about you.
privacy management and autonomous spontaneoussecurity, pages 146–159.
springer..elisa costante, yuanhao sun, milan petkovi´c, andjerry den hartog.
2012b.
a machine learning solu-tion to assess privacy policy completeness: (short pa-per).
in proceedings of the 2012 acm workshop onprivacy in the electronic society, wpes ’12, page91–96, new york, ny, usa.
association for com-puting machinery..lorrie faith cranor.
2012. necessary but not suf-ﬁcient: standardized mechanisms for privacy no-tice and choice.
j. on telecomm.
& high tech.
l.,10:273..lorrie faith cranor, joseph reagle, and mark s ack-erman.
2000. beyond concern: understanding netthe inter-users’ attitudes about online privacy.
net upheaval: raising questions, seeking answers incommunications policy, pages 47–70..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..alberto d´ıaz and pablo gerv´as.
2007. user-modelinformationbased personalized summarization.
processing and management: an international jour-nal, 43(6):1715–1734..william enck, peter gilbert, seungyeop han, vas-ant tendulkar, byung-gon chun, landon p cox,jaeyeon jung, patrick mcdaniel, and anmol nsheth.
2014. taintdroid: an information-ﬂow track-ing system for realtime privacy monitoring on smart-phones.
acm transactions on computer systems(tocs), 32(2):1–29..4134tatiana ermakova, benjamin fabian, and e. babina.
2015. readability of privacy policies of health-internationale tagungcare websites.
wirtschaftsinformatik..in 12..morgan c evans, jaspreet bhatia, sudarshan wad-kar, and travis d breaux.
2017. an evaluation ofconstituency-based hyponymy extraction from pri-in 2017 ieee 25th internationalvacy policies.
requirements engineering conference (re), pages312–321.
ieee..benjamin fabian, tatiana ermakova, and tino lentz.
2017. large-scale readability analysis of privacypolicies.
in proceedings of the international confer-ence on web intelligence, wi ’17, page 18–25, newyork, ny, usa.
association for computing machin-ery..federal trade commission.
1998. privacy online: areport to congress.
washington, dc, june, pages 10–11..geoffrey fowler.
2021. i checked apple’s new privacy.
‘nutrition labels.’ many were false..ftc.
2013. path social networking app settles ftccharges it deceived consumers and improperlyinformation from users’ mo-collected personalhttps://www.ftc.gov/news-bile address books.
events/press-releases/2013/02/path-social-networking-app-settles-ftc-charges-it-deceived..joshua gluck, florian schaub, amy friedman, hanahabib, norman sadeh, lorrie faith cranor, and yu-vraj agarwal.
2016. how short is too short?
impli-cations of length and framing on the effectiveness ofprivacy notices.
in 12th symposium on usable pri-vacy and security (soups), pages 321–340..abhijith athreya mysore gopinath, vinayshekhar ban-nihatti kumar, shomir wilson, and norman sadeh.
2020. automatic section title generation to improvethe readability of privacy policies..hamza harkous, kassem fawaz, r´emi lebret, flo-rian schaub, kang g shin, and karl aberer.
2018.polisis: automated analysis and presentation of pri-vacy policies using deep learning.
arxiv preprintarxiv:1802.02561..mitra bokaei hosseini, sudarshan wadkar, travis dbreaux, and jianwei niu.
2016. lexical similarityof information type hypernyms, meronyms and syn-onyms in privacy policies..po hu, donghong ji, chong teng, and yujing guo.
2012. context-enhanced personalized social sum-marization.
in proceedings of coling 2012, pages1223–1238, mumbai, india.
the coling 2012 or-ganizing committee..lucas d introna.
1997. privacy and the computer: whywe need privacy in the information society.
metaphi-losophy, 28(3):259–275..priyank jain, manasi gyanchandani, and nilay khare.
2016. big data privacy: a technological perspectiveand review.
journal of big data, 3(1):25..sarthak jain, madeleine van zuylen, hannaneh ha-jishirzi, and iz beltagy.
2020. scirex: a challengedataset for document-level information extraction.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages7506–7516..california department of justice.
2012. attorney gen-eral kamala d. harris secures global agreement tostrengthen privacy protections for users of mobileapplications..dongyeop kang, waleed ammar, bhavana dalvi,madeleine van zuylen, sebastian kohlmeier, ed-uard hovy, and roy schwartz.
2018. a dataset ofpeer reviews (peerread): collection, insights andnlp applications.
in proceedings of the 2018 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages1647–1661, new orleans, louisiana.
associationfor computational linguistics..patrick gage kelley, joanna bresee, lorrie faith cra-nor, and robert w reeder.
2009. a nutrition labelfor privacy.
in proceedings of the 5th symposium onusable privacy and security, page 4. acm..patrick gage kelley, lorrie faith cranor, and normansadeh.
2013. privacy as part of the app decision-making process, page 3393–3402.
association forcomputing machinery, new york, ny, usa..moniba keymanesh, micha elsner, and srinivasanparthasarathy.
2020. toward domain-guided con-trollable summarization of privacy policies.
in nat-ural legal language processing workshop.
kdd..vinayshekhar bannihatti kumar, abhilasha ravichan-der, peter story, and norman sadeh.
2019. quan-tifying the effect of in-domain distributed word rep-resentations: a study of privacy policies.
in aaaispring symposium on privacy enhancing ai andlanguage technologies: pal 2019..logan lebanoff and fei liu.
2018. automatic detec-tion of vague words and sentences in privacy poli-in proceedings of the 2018 conference oncies.
empirical methods in natural language processing,pages 3508–3517, brussels, belgium.
associationfor computational linguistics..timothy libert.
2018a.
an automated approach toauditing disclosure of third-party data collection inin proceedings of thewebsite privacy policies.
2018 world wide web conference, www ’18, page207–216, republic and canton of geneva, che.
in-ternational world wide web conferences steeringcommittee..4135timothy libert.
2018b.
an automated approach toauditing disclosure of third-party data collection inwebsite privacy policies.
in proceedings of the 2018world wide web conference, pages 207–216..jialiu lin, bin liu, norman sadeh, and jason i hong.
2014. modeling users’ mobile app privacy prefer-ences: restoring usability in a sea of permission set-in 10th symposium on usable privacy andtings.
security ({soups} 2014), pages 199–212..thomas linden, rishabh khandelwal, hamza hark-ous, and kassem fawaz.
2020. the privacy policylandscape after the gdpr.
proceedings on privacyenhancing technologies, 2020(1):47–64..bin liu, mads schaarup andersen, florian schaub,hazim almuhimedi, sa zhang, norman sadeh,alessandro acquisti, and yuvraj agarwal.
2016a.
follow my recommendations: a personalized pri-vacy assistant for mobile app permissions.
in sym-posium on usable privacy and security..bin liu, jialiu lin, and norman sadeh.
2014a.
rec-onciling mobile app privacy and usability on smart-phones: could user privacy proﬁles help?
in pro-ceedings of the 23rd international conference onworld wide web, pages 201–212..fei liu, nicole lee fella, and kexin liao.
2016b.
modeling language vagueness in privacy policies us-ing deep neural networks.
in 2016 aaai fall sym-posium series..fei liu, jeffrey flanigan, sam thomson, normansadeh, and noah a smith.
2015. toward abstrac-tive summarization using semantic representations.
in proceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1077–1086..fei liu, rohan ramanath, norman sadeh, and noah a.smith.
2014b.
a step towards usable privacy policy:automatic alignment of privacy statements.
in pro-ceedings of coling 2014, the 25th internationalconference on computational linguistics: techni-cal papers, pages 884–894, dublin, ireland.
dublincity university and association for computationallinguistics..mary madden, michele gilman, karen levy, and alicemarwick.
2017. privacy, poverty, and big data: amatrix of vulnerabilities for poor americans.
wash.ul rev., 95:53..florencia marotta-wurgler.
2019. does “notice andchoice” disclosure regulation work?
an empiricalstudy of privacy policies,”..aaron k massey, jacob eisenstein, annie i ant´on, andpeter p swire.
2013. automated text mining for re-in 2013quirements analysis of policy documents.
21st ieee international requirements engineeringconference (re), pages 4–13.
ieee..aleecia m mcdonald and lorrie faith cranor.
2008.the cost of reading privacy policies.
isjlp, 4:543..aleecia m mcdonald and lorrie faith cranor.
2010.americans’ attitudes about internet behavioral adver-in proceedings of the 9th annualtising practices.
acm workshop on privacy in the electronic society,pages 63–72..bruce m mclaren, sung-joo lim, france gagnon,david yaron, and kenneth r koedinger.
2006.studying the effects of personalized language andworked examples in the context of a web-based in-telligent tutor.
in international conference on intel-ligent tutoring systems, pages 318–328.
springer..gabriele meiselwitz.
2013. readability assessment ofpolicies and procedures of social networking sites.
in international conference on online communitiesand social computing, pages 67–75.
springer..majd mustapha, katsiaryna krasnashchok, anasprivacy pol-al bassit, and sabri skhiri.
2020.icy classiﬁcation with xlnet (short paper).
indata privacy management, cryptocurrencies andblockchain technology, pages 250–257.
springer..abhijith athreya mysore gopinath, shomir wilson,and norman sadeh.
2018. supervised and unsuper-vised methods for robust separation of section titlesin proceedingsand prose text in web documents.
of the 2018 conference on empirical methods innatural language processing, pages 850–855, brus-sels, belgium.
association for computational lin-guistics..kanthashree mysore sathyendra, shomir wilson, flo-rian schaub, sebastian zimmeck, and normansadeh.
2017a.
identifying the provision of choicesin proceedings of the 2017in privacy policy text.
conference on empirical methods in natural lan-guage processing, pages 2774–2779, copenhagen,denmark.
association for computational linguis-tics..mary madden, lee rainie, kathryn zickuhr, maeveduggan, and aaron smith.
2014. public perceptionsof privacy and security in the post-snowden era.
pewresearch center, 12..ankit malpani, balaraman ravindran, and hema amurthy.
2011. personalized intelligent tutoring sys-tem using reinforcement learning..kanthashree mysore sathyendra, shomir wilson, flo-rian schaub, sebastian zimmeck, and normansadeh.
2017b.
identifying the provision of choicesin proceedings of the 2017in privacy policy text.
conference on empirical methods in natural lan-guage processing, pages 2774–2779, copenhagen,denmark.
association for computational linguis-tics..4136ramesh nallapati, bowen zhou, cicero dos santos,c¸ a˘glar gulc¸ehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290, berlin, germany.
association for computational linguistics..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, pages 1797–1807, brussels, bel-gium.
association for computational linguistics..najmeh mousavi nejad, pablo jabat, rostislavnedelchev, simon scerri, and damien graux.
2020.establishing a strong baseline for privacy policy clas-ifip international conference on ictsiﬁcation.
systems security and privacy protection..sansa news.
2021. sansa news.
https://sansa..news/..helen nissenbaum.
2004. privacy as contextual in-.
tegrity.
wash. l.
rev., 79:119..pranav rajpurkar, robin jia, and percy liang.
2018.know what you don’t know: unanswerable ques-in proceedings of the 56th an-tions for squad.
nual meeting of the association for computationallinguistics (volume 2: short papers), pages 784–789, melbourne, australia.
association for compu-tational linguistics..rohan ramanath, fei liu, norman sadeh, and noah a.smith.
2014a.
unsupervised alignment of privacypolicies using hidden markov models.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 605–610, baltimore, maryland.
associ-ation for computational linguistics..rohan ramanath, florian schaub, shomir wilson,fei liu, norman sadeh, and noah smith.
2014b.
identifying relevant text fragments to help crowd-in proceedingssource privacy policy annotations.
of the aaai conference on human computation andcrowdsourcing, volume 2..ashwini rao, florian schaub, norman sadeh, alessan-dro acquisti, and ruogu kang.
2016. expecting theunexpected: understanding mismatched privacy ex-pectations online.
in twelfth symposium on usableprivacy and security ({soups} 2016), pages 77–96..(emnlp-ijcnlp), pages 4947–4958, hong kong,china.
association for computational linguistics..general data protection regulation.
2016. regulation(eu) 2016/679 of the european parliament and of thecouncil of 27 april 2016 on the protection of natu-ral persons with regard to the processing of personaldata and on the free movement of such data, and re-pealing directive 95/46.
ofﬁcial journal of the euro-pean union (oj), 59(1-88):294..joel r reidenberg, travis breaux, lorrie faith cranor,brian french, amanda grannis, james t graves,fei liu, aleecia mcdonald, thomas b norton, androhan ramanath.
2015. disagreeable privacy poli-cies: mismatches between meaning and users’ un-derstanding.
berkeley tech.
lj, 30:39..alexander m. rush, sumit chopra, and jason weston.
2015. a neural attention model for abstractive sen-in proceedings of the 2015tence summarization.
conference on empirical methods in natural lan-guage processing, pages 379–389, lisbon, portugal.
association for computational linguistics..norman sadeh, ro acquisti, travis d breaux, lor-rie faith cranor, aleecia m mcdonalda, joel r rei-denbergb, noah a smith, fei liu, n cameron rus-sellb, florian schaub, et al.
2013. the usable pri-vacy policy project: combining crowdsourcing, ma-chine learning and natural language processing tosemi-automatically answer those privacy questionsusers care about.
technical report cmu-isr-13-119, carnegie mellon university..kanthashree mysore sathyendra, abhilasha ravichan-der, peter garth story, alan w black, and normansadeh.
2017. helping users understand privacy no-tices with automated query answering functional-ity: an exploratory study.
technical report..florian schaub, rebecca balebako, adam l durity,and lorrie faith cranor.
2015. a design space for ef-fective privacy notices.
in eleventh symposium onusable privacy and security (soups 2015), pages1–17..executive ofﬁce of the president’s council of advi-sors on science and technology.
2014. big data andprivacy: a technological perspective..abigail see, peter j. liu, and christopher d. manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083, vancouver, canada.
association for computa-tional linguistics..abhilasha ravichander, alan w black, shomir wilson,thomas norton, and norman sadeh.
2019. ques-tion answering for privacy policies: combining com-putational and legal perspectives.
in proceedings ofthe 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing.
yan shvartzshanider, ananth balashankar, thomaswies, and lakshminarayanan subramanian.
2018.recipe: applying open domain question answer-ing to privacy policies.
in proceedings of the work-shop on machine reading for question answering,pages 71–77, melbourne, australia.
association forcomputational linguistics..4137yan shvartzshnaider, noah apthorpe, nick feamster,and helen nissenbaum.
2019. going against the (ap-propriate) ﬂow: a contextual integrity approach toprivacy policy analysis.
in proceedings of the aaaiconference on human computation and crowd-sourcing, volume 7, pages 162–170..yan shvartzshnaider, ananth balashankar, vikas pati-dar, thomas wies, and lakshminarayanan subrama-nian.
2020. beyond the text: analysis of privacystatements through syntactic and semantic role label-ing.
arxiv preprint arxiv:2010.00678..in pro-user input data for android applications.
ceedings of the 40th international conference onsoftware engineering, icse ’18, page 37–47, newyork, ny, usa.
association for computing machin-ery..samuel d warren and louis d brandeis.
1890. theright to privacy.
harvard law review, pages 193–220..alan f westin.
1968. privacy and freedom.
washing-.
ton and lee law review, 25(1):166..peter story, sebastian zimmeck, abhilasha ravichan-der, daniel smullen, ziqi wang, joel reidenberg,n cameron russell, and norman sadeh.
2019. nat-ural language processing for mobile app privacycompliance.
in aaai spring symposium on privacyenhancing ai and language technologies: pal2019..s wilson, f schaub, a dara, f liu, s cherivirala, p gleon, m s andersen, s zimmeck, k sathyendra,n c russell, t b norton, e hovy, j r reidenberg,and n sadeh.
2016a.
the creation and analysis of awebsite privacy policy corpus.
in annual meeting ofthe association for computational linguistics, aug2016. acl..peter story, sebastian zimmeck, and norman sadeh.
2018. which apps have privacy policies?
in annualprivacy forum, pages 3–23.
springer..auto tldr.
2021. auto tl;dr. http://autotldr..io/..noriko tomuro, steven lytinen, and kurt hornsburg.
2016. automatic summarization of privacy policiesusing ensemble learning.
in proceedings of the sixthacm conference on data and application securityand privacy, codaspy ’16, page 133–135, newyork, ny, usa.
association for computing machin-ery..joseph turow, jennifer king, chris jay hoofnagle,amy bleakley, and michael hennessy.
2009. amer-icans reject tailored advertising and three activitiesthat enable it.
available at ssrn 1478214..blase ur, pedro giovanni leon, lorrie faith cranor,richard shay, and yang wang.
2012. smart, use-ful, scary, creepy: perceptions of online behavioraladvertising.
in proceedings of the eighth symposiumon usable privacy and security, pages 1–15..ftc us federal trade commission et al.
2012. pro-tecting consumer privacy in an era of rapid change:recommendations for businesses and policymakers.
ftc report..michela del vicario, walter quattrociocchi, antonioscala, and fabiana zollo.
2019. polarization andfake news: early warning of potential misinforma-tion targets.
acm trans.
web, 13(2)..harm de vries, dzmitry bahdanau, and christophermanning.
2020.towards ecologically valid re-search on language user interfaces.
arxiv preprintarxiv:2007.14435..xiaoyin wang, xue qin, mitra bokaei hosseini,rocky slavin, travis d. breaux, and jianwei niu.
2018. guileak: tracing privacy policy claims on.
shomir wilson, florian schaub, aswarth abhilashdara, frederick liu, sushain cherivirala, pedro gio-vanni leon, mads schaarup andersen, sebas-tian zimmeck, kanthashree mysore sathyendra,n cameron russell, et al.
2016b.
the creation andanalysis of a website privacy policy corpus.
in pro-ceedings of the 54th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), volume 1, pages 1330–1340..shomir wilson, florian schaub, rohan ramanath,norman sadeh, fei liu, noah a smith, and fred-erick liu.
2016c.
crowdsourcing annotations forwebsites’ privacy policies: can it really work?
inproceedings of the 25th international conference onworld wide web, pages 133–143..razieh nokhbeh zaeem, safa anya, alex issa, jakenimergood, isabelle rogers, vinay shah, ayush sri-vastava, and k suzanne barber.
2020. privacycheckv2: a tool that recaps privacy policies for you.
in29th acm international conference on informationand knowledge management (cikm).
acm.
to ap-pear..razieh nokhbeh zaeem, rachel l german, andk suzanne barber.
2018. privacycheck: automaticsummarization of privacy policies using data mining.
acm transactions on internet technology (toit),18(4):1–18..sebastian zimmeck, peter story, daniel smullen, ab-hilasha ravichander, ziqi wang, joel reidenberg,n cameron russell, and norman sadeh.
2019a.
maps: scaling privacy compliance analysis to a mil-lion apps.
proceedings on privacy enhancing tech-nologies, 2019(3):66–86..sebastian zimmeck, peter story, daniel smullen, ab-hilasha ravichander, ziqi wang, joel r. reiden-berg, n. russell, and n. sadeh.
2019b.
maps: scal-ing privacy compliance analysis to a million apps.
proceedings on privacy enhancing technologies,2019:66 – 86..4138sebastian zimmeck, ziqi wang, lieyong zou, rogeriyengar, bin liu, florian schaub, shomir wilson,norman m sadeh, steven m bellovin, and joel rreidenberg.
2017. automated analysis of privacyrequirements for mobile apps.
in ndss..4139figure 2: example of privacy nutrition labels, disclos-ing information collected by companies and third par-ties through an application.
source: apple..a privacy nutrition labels.
figure.2 includes an example of a privacy nutritionlabel, intended to disclose to a user the informationa company and any third parties collect through anapp.
apple requires developers to self-report theinformation for these nutrition labels..4140