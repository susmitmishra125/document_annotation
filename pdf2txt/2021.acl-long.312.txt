handling extreme class imbalance in technical logbook datasets.
farhad akhbardeh, cecilia ovesdotter alm, marcos zampieri, travis desellrochester institute of technologyrochester, ny, usa{fa3019, coagla, mazgla, tjdvse}@rit.edu.
abstract.
technicallogbooks are a challenging andunder-explored text type in automated eventidentiﬁcation.
these texts are typically shortand written in non-standard yet technical lan-guage, posing challenges to off-the-shelf nlppipelines.
the granularity of issue types de-scribed in these datasets additionally leads toclass imbalance, making it challenging formodels to accurately predict which issue eachlogbook entry describes.
in this paper we fo-cus on the problem of technical issue classi-ﬁcation by considering logbook datasets fromthe automotive, aviation, and facilities mainte-nance domains.
we adapt a feedback strategyfrom computer vision for handling extremeclass imbalance, which resamples the trainingdata based on its error in the prediction process.
our experiments show that with statistical sig-niﬁcance this feedback strategy provides thebest results for four different neural networkmodels trained across a suite of seven differenttechnical logbook datasets from distinct tech-nical domains.
the feedback strategy is alsogeneric and could be applied to any learningproblem with substantial class imbalances..1.introduction.
predictive maintenance techniques are applied toengineering systems to estimate when maintenanceshould be performed to reduce costs and improveoperational efﬁciency (carvalho et al., 2019), aswell as mitigate risk and increase safety.
mainte-nance records are an important source of informa-tion for predictive maintenance (mcarthur et al.,2018).
these records are often stored in the formof technical logbooks in which each entry containsﬁelds that identify and describe a maintenance issue(akhbardeh et al., 2020a).
being able to classifythese technical events is an important step in thedevelopment of predictive maintenance systems..in most technical logbooks, issues are manually.
labeled by domain experts (e.g., mechanics) in freetext ﬁelds.
this text can then be used to classify orcluster events by semantic similarity.
classifyingevents in technical logbooks is a challenging prob-lem for the nlp community for several reasons: (a)the technical logbooks are written by various do-main experts and contain short text entries with non-standard language including domain-speciﬁc ab-breviated words (see table 1 for examples), whichmakes them distinct from other short non-standardtext corpora (e.g., social media); (b) off-the-shelfnlp tools struggle to perform well on this type ofdata as they tend to be trained on standard contem-porary corpora such as newspaper texts; (c) outsideof the clinical and biomedical sciences, there is alack of domain-speciﬁc, expert-based datasets forstudying expert-based event classiﬁcation, and inparticular few resources are available for techni-cal problem domains; and (d) technical logbookstend to be characterized by a large number of eventclasses that are highly imbalanced..original entry.
pre-processed entry.
fwd eng baff seealneeds resecured.
r/h eng #3 intake gskleaking.
bird struck on p/w attwy.
bird rmvd.
location rptd as nmfrom rwy aprch end..forward engine bafﬂe seal needsresecured.
right engine number 3 intakegasket leaking.
bird struck on pilot window attaxiway.
bird removedlocation reported as new mexicofrom runway approach end..table 1: original and text-normalized example datainstances illustrating that domain-speciﬁc terms (baf-ﬂe), abbreviations (gsk - gasket, eng - engine), and mis-spellings (seeal - seal) are abundant in logbook data..we address the aforementioned challenges witha special focus on exploring strategies to addressclass imbalance.
there is wide variation in the num-ber of instances among the technical event classesexamined in this work, as shown in figure 1 and ta-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4034–4045august1–6,2021.©2021associationforcomputationallinguistics4034figure 1: number of instances in 39 unbalanced classes of the aviation maintenance (avi-main) dataset..ble 3. this extreme class imbalance is an obstaclewhen processing logbooks as it causes most learn-ing algorithms to become biased and mainly predictthe large classes (kim et al., 2019).
to overcomethis issue, we introduce a feedback loop strategy,which is a repurposing of a method used to addressextreme class imbalance in computer vision (bow-ley et al., 2019), and examine it for classiﬁcation oftextual technical event descriptions.
this techniqueis applied in the training of a suite of common clas-siﬁcation models on seven predictive maintenancedatasets representing the aviation, automotive, andfacility maintenance domains..this paper addresses these research questions:rq1: to which extent does the class granularityand class imbalance present in technical logbooksimpact technical event classiﬁcation performance,and can a feedback loop for training data selectioneffectively address this issue?
rq2: which classiﬁcation models are better suitedto classify technical events for predictive mainte-nance across logbook datasets representing differ-ent technical domains?
the main contributions of this work include:.
1. experimental results showing strong perfor-mance of the feedback loop in addressing theclass imbalance problem in technical eventclassiﬁcation across all datasets and models;2. a thorough empirical evaluation of the per-formance of the technical event classiﬁer con-sidering multiple models and seven logbookdatasets from three different domains..2 related work.
most expert-domain datasets containing eventshave focused on healthcare.
for instance, altuncuet al.
(2019) analyzed patient incidents in unstruc-tured electronic health records provided by theu.k. national health service.
they evaluated adeep artiﬁcial neural network model on the expert-annotated textual dataset of a safety incident toidentify similar events that occurred.
del´eger et al.
(2010) proposed a method to deal with unstructuredclinical records, using rule-based techniques to ex-tract names of medicines and related informationsuch as prescribed dosage.
savova et al.
(2010)considered free-text electronic medical records forinformation extraction purposes and developed asystem to obtain clinical domain knowledge..patrick and li (2009) proposed the cascade meth-ods of extracting the medication records such astreatment duration or reason, obtained from pa-tient’s historical records.
their approach for eventextraction includes text normalization, tokeniza-tion, and context identiﬁcation.
a system usingmultiple features outperformed a baseline methodusing a bag of words model.
yetisgen-yildiz et al.
(2013) proposed the lung disease phenotypes iden-tiﬁcation method to prevent the use of a hand-operated identiﬁcation strategy.
they employednlp pipelines including text pre-processing andfurther text classiﬁcation on the textual reports toidentify the patients with a positive diagnosis forthe disease.
based on the outcome, they achieve.
4035tech.
event or issue label example instance of technical logbook entry.
abbr., misspelling, terminology.
substantial damage (1) aft on taxi, wing strueck fuel truck, chandler, azbaffle damageminor damageunknownpm servicedriving issuestop sign runningbuilding pmeng need repairpreventive maint.
(2) r/h fwd upper baff seal needs to be resecured(1) saw sml flock flying upon ldg flare, across rwy(1) no dmg.
bird remains on f/o windscreen(3) pm services check tires for leaks check plow batt(4) failure to yield right, ove correcting over steering ove, steering(4) motorists regularly illegal u-turns in r/hour(5) the a/c unit in the kitchen on 3th floor dmg/leak(3) change oil & filter: l/h eng, check comp & plugs(5) reset boiler #2 tmer, checked bldg.
throughout.
aft, wing, strueck, fuelr/h, fwd, baflsml, ldg, rwydmg, f/o, windscreenpm,tires, plow, batt.
u-turns, r/houra/c, dmgoil, eng, l/h, comp, plugsboiler, bldg.
table 2: example instances of technical logbook entries spanning the aviation accident (1), aviation maintenance(2), automotive maintenance (3), automotive safety (4), and facility maintenance (5).
each instance shows howdomain-speciﬁc terminology, abbreviations (abbr.
), and misspelled words (in bold font) are used by the domainexpert, and also illustrates some of the event types covered.
more details are provided in section 3..notable performance by using the n-gram featureswith the maximum entropy (maxent) classiﬁer..there is also relevant research on event classi-ﬁcation in social media.
for example, ritter et al.
(2012) proposed an open-source event extractionand supervised tagger for noisy microblogs.
cherryand guo (2015) applied word embedding-basedmodeling for information extraction on news-wireand tweets, comparing named entity taggers to im-prove their method.
hammar et al.
(2018) per-formed experimental work on instagram text usingweakly supervised text classiﬁcation to extractedclothing brand based on user descriptions in posts..the problem of class imbalance has been stud-ied in recent years for numerous natural languageprocessing tasks.
tayyar madabushi et al.
(2019)studied automatic propaganda event detection froma news dataset using a pre-trained bert model.
they recognized that the bert model had issuesin generalizing.
to overcome this issue, they pro-posed a cost-weighting method.
al-azani and el-alfy (2017) analyzed polarity measurement in im-balanced tweet datasets utilizing features learnedwith word embeddings.
li and nenkova (2014)studied the class imbalance problem in the taskof discourse relation identiﬁcation by comparingthe accuracy of multiple classiﬁers.
they showedthat utilizing a uniﬁed method and further down-sampling the negative instances can signiﬁcantlyenhance the performance of the prediction modelon unbalanced binary and multi-classes..dealing with unbalance classes is also studiedwell in the sentiment classiﬁcation task.
li et al.
(2012) introduced an active learning method thatovercomes the problem of data class unbalance bychoosing the signiﬁcant sample of minority class.
for manual annotation and majority class for au-tomatic annotation to lower the amount of humanannotation required.
furthermore, damaschk et al.
(2019) examined techniques to overcome the prob-lem of dealing with high-class imbalance in classi-fying a collection of song lyrics.
they employedneural network models including a multi-layer per-ceptron and a doc2vec model in their experimentswhere the ﬁnding was that undersampling the ma-jority class can be a reasonable approach to removethe data sparsity and further improve the classiﬁca-tion performance..li et al.
(2020) also explored the problem ofhigh data imbalance using cross-entropy criteriaas well as standard performance metrics.
theyproposed a loss function called dice loss that as-signs equal importance to the false negatives andthe false positives.
in computer vision, bowleyet al.
(2019) developed an automated feedback loopmethod to identify and classify wildlife speciesfrom unmanned aerial systems imagery, for train-ing cnns to overcome the unbalanced class issue.
on their expert imagery dataset, the error rate de-creased substantially from 0.88 to 0.05. this workadapts this feedback loop strategy to the nlp prob-lem of classifying technical events..3 technical event datasets.
in this work, we used a set of 7 logbook datasetsfrom the aviation, automotive, and facility domainsavailable at maintnet (akhbardeh et al., 2020a).
maintnet is a collaborative open-source platformfor predictive maintenance language resources fea-turing multiple technical logbook datasets and tools.
these datasets include: 1) avi-main contains sevenyears of maintenance logbook reports collected by.
4036code.
inst.
avg ntoks cls min med.
class size.
avg max.
avi-mainavi-accavi-safe.
6,169 13.85 3954,130 14.31217,718 19.52.
211792,134.
569668,859.
158826.
1,6741,5958,859 15,584.auto-mainauto-accauto-safe.
617.
57.3452,707 4.5934,824 25.11 17.
48.
23.
123.
2681,085 11,060 17,569 40,562678.
284.
213.
86.faci-main 74,360 31.50 70.
25.
303.
1,062 10,748.table 3: number of instances (inst), average numberof tokens per instance (avg toks), number of classes(n cls), and class size statistics: minimum, average,median, and maximum (min, med, avg, max) for eachdataset..the university of north dakota aviation programon aircraft maintenance that were reported by themechanic or pilot.
2) avi-acc contains four yearsof aviation accident and reported damages.
3) avi-safe contains eleven years of aviation safety andincident reports.
accidents were caused by foreignobjects/birds during the ﬂights which led to safetyinspection and maintenance, where safety crewsindicated the damage (safety) level for further anal-ysis.
4) auto-main is a single year report withmaintenance records for cars.
5) auto-acc containstwelve years of car accidents and crash reports de-scribing the related car maintenance issue and prop-erty damaged in the accident.
6) auto-safe containsfour years of noted hazards and incidents on theroadway from the driver.
7) faci-main containssix years of logbook reports collected for buildingmaintenance..these technical logbooks include short, com-pact, and descriptive domain-speciﬁc english textssingle instances usually contain between 2 and20 tokens on average including abbreviations anddomain-speciﬁc words.
an example instance fromtable 2, r/h fwd upper baff seal needs to be rese-cured, shows how the instances for a speciﬁc issueclass are comprised from speciﬁc vocabulary (lessambiguity), and therefore contain a high level ofgranularity (level of description for an event frommultiple words) (mulkar-mehta et al., 2011).
ta-ble 3 presents statistics for each dataset, in terms ofthe number of instances, average instance length,number of classes, and the minimum, average, me-dian and maximum class size to represent how im-balanced the datasets are..an instance in the logbook can be formed as acomplete description of the technical event (such as.
a safety or maintenance inspection) like: #2 & #4cyl rocker cover gsk are leaking, or it might containan incomplete description by solely referring tothe damaged part/section of machinery (hyd capchck eng light on) using few domain words.
ineither form of the problem description, the givenannotation (label) is at the issue type-level, e.g.,bafﬂe damage.
table 2 shows multiple exampleswith associated instances..further characteristics of these log entries in-clude compound words (antifreeze, engine-holder,driftangle, dashboard).
many of these words (e.g.,a compound word: dashboard) essentially repre-sent the items, or domain-speciﬁc parts used in thedescriptions.
additionally, function words (e.g.,prepositions) are important and removing themcould alter the meaning of the entry.
the logbookdatasets also have both the following shared anddistinct characteristics:.
shared characteristics: each instance contains adescriptive observation of the issue and/or the sug-gested action that should be taken (eng inspectionpanel missing screw).
each instance also refers tomaintaining a single event, which means the recog-nized problem applies to the only single-issue type.
as an example, the instance cyl #1 baff cracked atscrew support & forward baff below #1 includes acombination of sequences that refers to the locationand/or speciﬁc part of the machinery..distinct characteristics: in each domain, termi-nologies, a list of terms, and abbreviations are dis-tinct, and an abbreviation can have different ex-pansion depending on the domain context (sproatet al., 2001), e.g., a/c can mean aircraft in avia-tion and in the automotive domain air conditioner.
however, the abbreviations and acronyms of thedomain words (e.g.
atc - air trafﬁc control) in thesetechnical datasets should not be approached as aword sense disambiguation problem as they requirecharacter level expansion..4 methods and models.
4.1 handling class imbalance.
collecting additional data to augment datasets isa common approach for tackling the problem ofskewed class distributions.
however, as discussedearlier, technical logbooks are proprietary and veryhard to obtain.
in addition, each domain capturesdomain-speciﬁc lexical semantics, preventing theuse of techniques such as domain adaption (ma.
4037algorithm 1 feedback loop pseudocode(cid:46) gets mcs random instances from each classfunction samplerandom(c, mcs).
array afor i ← 1 to size(c) do.
shuffle(ci)a ← a ∪ getfirstn(mcs, ci).
return a.
(cid:46) gets mcs instances from each class with the worst errorfunction resample(c, m, mcs).
array afor i ← 1 to size(c) do.
calculateerror(ci)sortbyerror(ci)a ← a ∪ getfirstn(mcs, ci).
return a.input: training data d = instance(1, 2, .
.
.
, n )input: feedback loop iterations f liinput: epochs per loop iteration f leinput: minimum class size mcs.
(cid:46) divide training data by classarray c ← splitbyclass(d).
(cid:46) get initial active training data a randomlyarray a ← samplerandom(c, mcs)model mfor l ← 1 to f li do.
(cid:46) train the model for the number of epochs per iterationm ← train(m, f le, a)(cid:46) update the active training dataa ← resample(d, m, mcs).
output: m.et al., 2019) to apply a large class data from onetechnical domain to another.
for example, in-stances that describe an engine failure in the avi-ation domain are distinct from engine failure in-stances reported in the automotive domain.
in thispaper we apply ﬁve different methods for selectingtraining data for the models to analyze their effectson classiﬁcation performance: (1) under(down)-and (2) over-sampling, (3) random down-sampling,(4) a feedback loop strategy, and (5) a baselinestrategy which simply uses all available data..re-sampling under- and over-sampling are re-sampling techniques (maragoudakis et al., 2006)that were used to create balanced class sizes formodel training.
for over-sampling, instances of theminority classes are randomly copied so that allclasses would have the same number of instancesas the largest class.
for under-sampling, obser-vations are randomly removed from the majorityclasses, so that all classes have the same numberof instances as the smallest class.
for both ap-proaches, we ﬁrst divided our datasets into test and.
training sets before performing over-sampling toprevent contamination of the test set by having thesame observations in both the training and test data..feedback loop to address class imbalances intext classiﬁcation, this work adapts the approachin bowley et al.
(2019) from the computer visiondomain.
the goal of this approach is not only toalleviate the bias towards majority classes but alsoto adjust the training data instances such that themodels are always being trained on the instancesthat was performing the worst on.
it should benoted that this approach is very similar to adaptivelearning strategies which have been shown to aidin human learning (kerr, 2015; midgley, 2014)..algorithm 1 presents pseudocode for the feed-back loop.
in this process, the active training data(the data used to actually train the models in eachiteration of the loop) is continually resampled fromthe training data.
the model is ﬁrst initially trainedwith an undersampled number of random instancesfrom each class, which becomes the initial activetraining data.
the model m then performs infer-ence over the entire training set, and then selectsmcs instances from each class ci which had theworst error during inference, where mcs is theminority (smallest) class size.
the model is thenretrained with this new active training data and theprocess of training, inference and selection of themcs worst instances repeats for a ﬁxed numberof feedback loop iterations, fli.
in this way themodel is always being trained on the instances ithas classiﬁed the worst..to measure the effect of resampling the worstperforming instances, the feedback loop approachwas also compared to a random downsampling(ds) loop, where instead of evaluating the modelover each instance and selecting the worst perform-ing instances, mcs instances from each classare instead randomly sampled.
as performinginference over the entire training set adds over-head, a comparison to the random ds loop methodwould show if performing this inference is worththe performance cost over simple random resam-pling.
this approach is the same as algorithm 1except that samplerandom is used instead ofresample in the feedback loop.
section 4.3 de-scribes how the number of training epochs and loopiterations were determined such that all the trainingdata selection methods are given a fair evaluationwith the same amount of computational time..4038evaluation metrics for imbalanced datasets,simply using precision, recall or f1 score metricsfor the entire datasets would not accurately reﬂecthow well a model or method performs, as they em-phasize the majority classes.
to overcome this,alternative evaluation metrics to handle the classimbalance problem were used, as recommendedby banerjee et al.
(2019).
speciﬁcally, we reportthe models performance based on precision, recall,and f1 score by utilizing a macro-average over allclasses, as this gives every class equal weight, andhence reveals how well the models and trainingdata selection strategies perform..4.2 model architecture and training.
different machine learning methods were consid-ered for technical event/issue classiﬁcation (e.g.
engine failure, turbine failure).
each instance isan individual short logbook entry and contains ap-proximately 2 to 20 tokens (12 words on averageper instance including function words), as shownin table 3.the methods used in this study were adeep neural network (dnn) (dernoncourt et al.,2017), a long short-term memory (lstm) (suz-gun et al., 2019), recurrent neural network (rnn)(pascanu et al., 2013), a convolutional neural net-work (cnn) (lin et al., 2018), and bert (devlinet al., 2019)..deep neural network a deep artiﬁcial neuralnetwork (dnn), as described by dernoncourt et al.
(2017), can learn abstract representation and fea-tures of the input instances that would help toachieve better performance on predicting the is-sue type in the logbook dataset.
the dnn usedwas a 3 layer, fully connected feed forward neuralnetwork with an input embedding layer of dimen-sion 300 and equal size number of words followedby 2 dense layers with 512 hidden units with reluactivation functions followed by a dropout layer.
finally, we added a fully connected dense layerwith size equal to the number of classes, with asoftmax activation function..long short-term memory an lstm rnnwas also used to perform a sequence-to-label clas-siﬁcation.
as described by suzgun et al.
(2019)lstm rnns utilize several vector gates at eachstate to regulate the passing of data by the sequencewhich enhances the modeling of the long-term de-pendencies.
we used a 3 layer lstm model witha word embedding layer of dimension 300 and theequal size number of words followed by an lstm.
layer with setting the number of hidden units equalto the embedding dimension, followed by a dropoutlayer.
finally, we added a fully connected layerwith size equal to the number of classes, with asoftmax activation function..convolutional neural network convolutionalneural networks (cnns) have demonstrated excep-tional success in nlp tasks such as document clas-siﬁcation, language modeling, or machine trans-lation (lin et al., 2018).
as xu et al.
(2020) de-scribed, cnn models can produce consistent per-formance when applied to the various text typessuch as short sequences.
we evaluated a cnn ar-chitecture (shen et al., 2018) with a convolutionallayer, followed by batch normalization, relu, anda dropout layer, which was followed by a max-pooling layer.
the model contained 300 convolu-tional ﬁlters with the size of 1 by n-gram lengthpooling with the size of 1 by the length of the inputsequence, followed by concatenation layer, thenﬁnally connected to a fully connected dense layer,and an output layer equal to the size of the datasetclass using a softmax activation function..bidirectional encoder representations wealso evaluated using the pre-trained uncased bidi-rectional encoder representations (bert) for en-glish (devlin et al., 2019).
we ﬁne-tuned the model,and used a word piece based bert tokenizer forthe tokenization process and the randomsamplerand sequentialsampler for training and testing re-spectively.
to better optimize this model, a sched-ule was created for the learning rate that decayedlinearly from the initial learning rate we set in theoptimizer to 0..4.3 experimental settings.
datasets and baselines first, the technical textpre-processing pipeline developed by akhbardehet al.
(2020b) was applied, which comprisesdomain-speciﬁc noise entity removal, dictionary-based standardization, lexical normalization, partof speech tagging, and domain-speciﬁc lemmatiza-tion.
we divided the datasets selecting randomlyfrom each class independently to maintain a similarclass size distribution, using 80% of the instancesfor training and 20% of the instances for testingdata.
for feature extraction, two methods wereconsidered: a bag-of-word model (n-grams:1) (pe-dregosa et al., 2011) and pre-trained 300 dimen-sional glove word embeddings (pennington et al.,2014)..4039hyperparameter and tuning the coarse toﬁne learning (cfl) approach (lee et al., 2018)was used to set parameters and hyperparametersfor the dnn, lstm, and cnn models.
experi-ments considered batch sizes of 32, 64, and 128,an initial learning rate ranging from 0.01 to 0.001with a learning decay rate of 0.9, and dropout regu-larization in the range from 0.2 to 0.5 in all models,as well as relu and softmax activation functions(nair and hinton, 2010), categorical cross-entropy(zhang and sabuncu, 2018) as the loss function,and the adam optimizer (kingma and ba, 2015) inthe dnn, lstm, cnn and bert models.
basedon experiments and network training accuracy, abatch size of 64 and drop out regularization of 0.3was selected for model training..each model with each training data selectionstrategy was trained 20 times to generate results foreach dataset.
to ensure each training data selectionstrategy was fairly compared with a similar com-putational budget, the number of training epochsand loop iterations (if the strategy had a feedbackor random downsampling loop) were adjusted sothat the total number of training instances evalu-ations each model performed was the same.
foreach dataset, the number of forward and backwardpasses, ‘t’ for 100 epochs of the baseline strategywas used as the standard.
as an example, table 4shows how many loop iterations, epochs per loop,and inference passes were done for each trainingdata selection strategy on the auto-safe dataset.
given the differences between the min and maxclass sizes it was not possible to get exact matchesbut the strategies came as close as possible.
wecounted each inference pass for the feedback loopthe same as a forward and backward training pass,which actually was a slight computational disad-vantage for the feedback loop, as a forward andbackward pass in training takes approximately 1xto 2x the time as an inference pass..5 results.
table 5 shows a comparison between the base-line and the four different class balancing methods(over-sampling, under-sampling, the random down-sampling (ds) loop and the feedback loop).
basedon these outcomes, the feedback loop strategy al-most entirely outperforms the other methods overall datasets and models, showing that performinginference over the training set and reselecting thetraining data from the worst performing instances.
dataset.
l epl.
lti.
inm.
t.1baseline1downsamplingoversampling1random ds loop 3325feedback loop.
100329421010.
0000.
385,9003,859385,9171,173386,9889,214387,0901,1731,173 3,859 389,725.table 4: details regarding different training process us-ing the various methods for handling the unbalancedclass in automotive safety (auto-safe) dataset with 17total classes.
loop (l), epochs per loop (epl), activetraining instance size (lti), inference for new mis-classiﬁed (inm) and total instances evaluated (t)..does provide a beneﬁt to the learning process.
aplausible explanation is that this strategy does notintroduce bias into the larger class and also doesnot effect the minority class size distribution.
italso does not waste training time on instances themodel has already well learned..table 5 also shows the empirical analysis of thefour classiﬁcation models, with the model and train-ing data selection strategy providing the overallbest results shown in bold and italics.
using techni-cal text pre-processing techniques described in sec-tion 4.3, and the feedback loop strategy describedin section 4.1, the precision, recall, and f1 scoreimproved compared to the baseline performance.
the cnn model outperformed the other algorithmswith improved precision, recall, and f1 score for al-most all datasets except for avi-main, where berthad the similar results, and auto-main where cnnand bert tied.
this is interesting, given the cur-rent popularity of the bert model, however it maybe due to the substantial lexical, topical, and struc-tural linguistic differences between the technicallogbook data and the english corpus that bertwas pre-trained on..furthermore, we conducted the mann-whitneyu-test of statistical signiﬁcance by using the f1scores of each of the 20 repeated experiments ofthe classiﬁcation models, using the baseline andthe feedback loop approach as the two differentpopulations.
the outcomes are shown in table6, with the differences being highly statisticallysigniﬁcant..6 discussion.
to investigate the optimal strategies for dealingwith these imbalanced technical datasets, we stud-ied various methods on how to process the data,extract features, and classify the type of event.
re-.
4040dataset.
model.
baseline (%).
pre rec.
f1.
downsampling (%)f1.
pre rec.
oversampling (%)f1.
pre rec.
randomds loop (%)f1.
pre rec.
feedbackloop (%)rec.
avi-main.
avi-acc.
avi-safe.
dnn 0.90 0.89 0.89 0.67 0.78 0.70 0.90 0.90 0.90 0.90 0.90 0.90lstm 0.84 0.85 0.84 0.81 0.83 0.81 0.85 0.84 0.84 0.84 0.84 0.84cnn 0.93 0.92 0.92 0.89 0.88 0.88 0.94 0.92 0.92 0.93 0.91 0.91bert 0.93 0.93 0.93 0.85 0.86 0.85 0.94 0.94 0.94 0.94 0.93 0.93.dnn 0.47 0.44 0.43 0.35 0.45 0.35 0.48 0.47 0.47 0.50 0.44 0.46lstm 0.38 0.37 0.37 0.35 0.35 0.35 0.39 0.39 0.39 0.38 0.39 0.38cnn 0.50 0.49 0.49 0.43 0.42 0.42 0.52 0.44 0.47 0.51 0.44 0.47bert 0.48 0.42 0.44 0.41 0.40 0.40 0.50 0.44 0.46 0.50 0.44 0.46.dnn 0.43 0.41 0.41 0.36 0.36 0.36 0.50 0.50 0.50 0.50 0.49 0.49lstm 0.47 0.46 0.46 0.43 0.42 0.42 0.49 0.50 0.49 0.48 0.46 0.47cnn 0.59 0.57 0.57 0.50 0.50 0.50 0.60 0.59 0.59 0.59 0.59 0.59bert 0.50 0.50 0.50 0.44 0.46 0.44 0.54 0.54 0.54 0.53 0.53 0.53.auto-main.
dnn 0.58 0.45 0.49 0.33 0.49 0.39 0.60 0.55 0.56 0.58 0.54 0.55lstm 0.49 0.55 0.51 0.41 0.42 0.41 0.50 0.60 0.54 0.51 0.58 0.54cnn 0.61 0.61 0.61 0.53 0.53 0.53 0.64 0.64 0.64 0.63 0.64 0.63bert 0.60 0.60 0.60 0.54 0.53 0.53 0.63 0.64 0.63 0.63 0.63 0.63.auto-acc.
auto-safe.
faci-main.
dnn 0.43 0.34 0.30 0.35 0.42 0.27 0.39 0.42 0.31 0.40 0.39 0.39lstm 0.45 0.39 0.41 0.40 0.40 0.40 0.42 0.41 0.41 0.42 0.40 0.40cnn 0.46 0.43 0.44 0.44 0.41 0.42 0.49 0.50 0.49 0.50 0.51 0.50bert 0.50 0.49 0.49 0.47 0.47 0.47 0.50 0.50 0.50 0.51 0.49 0.50.dnn 0.52 0.46 0.48 0.40 0.47 0.41 0.54 0.51 0.51 0.54 0.51 0.51lstm 0.40 0.40 0.40 0.38 0.39 0.38 0.41 0.42 0.41 0.41 0.41 0.41cnn 0.59 0.58 0.58 0.52 0.51 0.51 0.59 0.60 0.59 0.59 0.59 0.59bert 0.57 0.56 0.56 0.52 0.50 0.50 0.58 0.56 0.56 0.57 0.57 0.57.dnn 0.57 0.48 0.50 0.33 0.40 0.34 0.56 0.48 0.50 0.57 0.50 0.53lstm 0.56 0.56 0.56 0.53 0.52 0.52 0.59 0.55 0.56 0.59 0.56 0.57cnn 0.64 0.64 0.64 0.61 0.60 0.60 0.66 0.66 0.66 0.65 0.65 0.65bert 0.63 0.64 0.63 0.60 0.60 0.60 0.65 0.64 0.64 0.64 0.65 0.64.pre.
0.930.860.950.95.
0.520.400.520.51.
0.530.490.620.56.
0.610.530.650.64.
0.480.480.510.52.
0.550.430.620.58.
0.590.630.690.68.
0.910.880.940.96.
0.450.390.460.45.
0.510.500.610.57.
0.550.610.640.64.
0.400.410.530.51.
0.520.420.600.59.
0.510.560.670.67.f1.
0.910.870.940.95.
0.480.390.480.47.
0.510.490.610.56.
0.570.550.640.64.
0.400.440.520.51.
0.530.420.610.59.
0.540.600.680.67.table 5: comparison of results for the 7 datasets, for the baseline and four methods to address class imbalance forthe four evaluated models (dnn, lstm, cnn and bert).
each model’s macro average performance is shown asprecision (pre), recall (rec) and f1 score.
the best results over the training data selection strategies are shown inbold, and the best results over all models are additionally in italics..garding the discussion provided in section 3 aboutthe nature of such a dataset, there are key chal-lenges that effect the performance of employedalgorithms.
as discussed in section 1, the ex-treme class imbalance observed in these technicaldatasets substantially affects learning algorithms’performance.
to overcome this issue, we ﬁrst ex-plored oversampling and undersampling, whichboth result in balanced class sizes.
undersamplingremoved portions of dataset that could be impor-tant for certain technical events or issues, whichresulted in underﬁtting and weak generalizationfor important classes.
on the other hand, over-sampling may introduce overﬁtting in the minority.
class, as some of the event types are very short to-kens containing domain-speciﬁc words.
followingthis, to minimize the possibility of overﬁtting andunderﬁtting, a random downsampling loop and afeedback loop were investigated to minimize biasin the training process.
it was found that the addedcomputational cost of the feedback loop inferencewas worth the reduction in training time it causedover the random downsampling loop..the scarce data available in a dataset such asauto-main is certainly an issue for deep learningmethods.
examining the accuracy improvement byusing the proposed feedback loop strategy, requiresincorporating more instances to the event classes..4041similar to any supervised learning models, we no-ticed some limitations that could be addressed infuture work.
as shown in the previous sections(such as table 2), logbook instances contain shorttext (ranging from 2 to 20 tokens per instance), andutilizing recurrent deep learning algorithms such aslstm rnns which are heavily based on the con-text leads to weak performance compared to otheralgorithms.
one possible explanation is that log-books with short instances (sequences) are not pro-viding sufﬁcient context for the algorithm to makebetter predictions.
another could be that rnnsare notoriously difﬁcult to train (pascanu et al.,2013), and the lstm models may simply requiremore training time to achieve similar results.
thereis some evidence for this, as the dataset with themost instances, which also had the second largestnumber of tokens per instance on average was faci-main, which is the dataset which the lstm modelhad the closest performance to the cnn and bertmodels, and was also the only one which the lstmmodel outperformed the dnn model..the pre-trained bert model provided a reason-able classiﬁcation performance compared to theother deep learning models, however as bert ispre-trained on standard language, the performancewhen applying to logbook data was not optimal.
training or ﬁne-tunning bert to technical logbookdata is likely to improve performance as observedin the legal and scientiﬁc domains (chalkidis et al.,2020; beltagy et al., 2019).
as training or ﬁne-tuning bert requires large amounts of data, alimitation for ﬁne-tuning a domain-speciﬁc bertis the amount of logbook data available..7 conclusion and future work.
this work focused on predictive maintenance andtechnical event/issue classiﬁcation, with a specialfocus on addressing class imbalance.
we acquiredseven logbook datasets from three technical do-mains containing short instances with non-standardgrammar and spelling, and many abbreviations.
to address rq1, we evaluated multiple strategiesto address the extreme class imbalance in thesedatasets and we showed that the feedback loopstrategy performs best, almost entirely providingthe best results for the 7 different datasets and 4different models investigated.
to address rq2, weempirically compared different classiﬁcation algo-rithms (dnn, lstm, cnn, and pre-tuned bert).
results show that the cnn model outperforms the.
dataset.
dnn.
lstm cnn.
bert.
avi-main.
0.0020.
0.0043.
0.0002.
0.0004.avi-acc.
0.0011.
0.0399.
0.0103.
0.0015.avi-safe.
0.0000.
0.0023.
0.0059.
0.0012.auto-main.
0.0001.
0.0181.
0.0009.
0.0004.auto-acc.
0.0000.
0.0055.
0.0001.
0.0161.auto-safe.
0.0003.
0.0106.
0.0011.
0.0083.faci-main.
0.0002.
0.0001.
0.0003.
0.0005.table 6: statistical signiﬁcance of the various clas-siﬁcation models between the baseline approach andfeedback loop approach f1 scores using the mann-whitney u test.
experiments indicate statistical signif-icance with a p value of 0.05..other classiﬁers.
the methodology presented inthis paper could be applied to other maintenancecorpora from a variety of technical domains.
thefeedback loop approach for selecting training datais generic and could easily be applied to any learn-ing problem with substantial class imbalances.
thisis useful as extreme class imbalance is a challengeat the heart of a number of natural language tasks..in future work, we would like to ﬁne-tune bertusing logbook data, as described in section 6, andextend this work to datasets in other languages.
the biggest challenge for these two research direc-tions is the limited availability of logbook datasets.
furthermore, we are exploring various methods ofdomain adaptation and transfer learning on thesedatasets to further improve the performance of clas-siﬁcation models..acknowledgments.
we would like to thanks the university of northdakota aviation program for providing the valu-able aviation maintenance logbook datasets to themaintnet research.
we further thank the aviationdomain expert zechariah morgan for evaluating theoutcomes of the various algorithms and providingvaluable feedback for the aviation domain dataset.
we also would like to thank the anonymous aclreviewers for providing us with helpful commentsand feedback..4042references.
farhad akhbardeh, travis desell,.
and marcoszampieri.
2020a.
maintnet: a collaborative open-source library for predictive maintenance languageresources.
in proceedings of the 28th internationalconference on computational linguistics, pages 7–11, barcelona, spain.
international committee oncomputational linguistics..farhad akhbardeh, travis desell,.
and marcoszampieri.
2020b.
nlp tools for predictive mainte-nance records in maintnet.
in proceedings of the 1stconference of the asia-paciﬁc chapter of the associ-ation for computational linguistics and the 10th in-ternational joint conference on natural languageprocessing, pages 26–32, suzhou, china.
associa-tion for computational linguistics..sadam al-azani and el-sayed el-alfy.
2017. usingword embedding and ensemble learning for highlyimbalanced data sentiment analysis in short arabictext.
procedia computer science, vol 109:359–366..m. altuncu, erik mayer, sophia yaliraki, and mauri-cio barahona.
2019. from free text to clusters ofcontent in health records: an unsupervised graph par-titioning approach.
applied network science, vol 4..siddhartha banerjee, cem akkaya, francisco perez-sorrosal, and kostas tsioutsiouliklis.
2019. hier-archical transfer learning for multi-label text classi-in proceedings of the 57th annual meet-ﬁcation.
ing of the association for computational linguisticsacl, pages 6295–6300, florence, italy.
associationfor computational linguistics..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁc text.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3615–3620, hong kong, china.
association for computa-tional linguistics..connor bowley, marshall mattingly, andrew barnas,susan ellis-felege, and travis desell.
2019. ananalysis of altitude, citizen science and a convolu-tional neural network feedback loop on object detec-tion in unmanned aerial systems.
journal of compu-tational science, vol 34:102 – 116..thyago p. carvalho, fabr´ızzio a. a. m. n. soares,roberto vita, roberto da p. francisco, jo˜ao p. basto,and symone g. s. alcal´a.
2019. a systematic litera-ture review of machine learning methods applied topredictive maintenance.
computers and industrialengineering, 137:106024..ilias chalkidis, manos fergadiotis, prodromos malaka-siotis, nikolaos aletras, and ion androutsopoulos.
2020. legal-bert: the muppets straight out oflaw school.
in findings of the association for com-putational linguistics: emnlp 2020, pages 2898–2904, online.
association for computational lin-guistics..colin cherry and hongyu guo.
2015. the unreason-able effectiveness of word representations for twit-in proceedings ofter named entity recognition.
the 2015 conference of the north american chap-ter of the association for computational linguistics:human language technologies hlt-naacl, pages735–745, denver, colorado.
association for compu-tational linguistics..matthias damaschk, tillmann d¨onicke, and florianlux.
2019. multiclass text classiﬁcation on unbal-anced, sparse and noisy data.
in proceedings of thefirst nlpl workshop on deep learning for naturallanguage processing, pages 58–65, turku, finland.
link¨oping university electronic press..louise del´eger, cyril grouin, and pierre zweigen-baum.
2010. extracting medical information fromnarrative patient records: the case of medication-related information.
journal of the american medi-cal informatics association, vol 17:555 – 558..franck dernoncourt, ji young lee, and peter szolovits.
2017. neural networks for joint sentence classiﬁca-in proceedings oftion in medical paper abstracts.
the 15th conference of the european chapter of theassociation for computational linguistics: volume2, pages 694–700, valencia, spain.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1, pages 4171–4186, minneapolis,minnesota.
association for computational linguis-tics..kim hammar, shatha jaradat, nima dokoohaki, andmihhail matskin.
2018. deep text mining of insta-in interna-gram data without strong supervision.
tional conference on web intelligence (wi), pages158 – 165, santiago, chile..philip kerr.
2015. adaptive learning.
english lan-.
guage teaching (elt) journal, 70(1):88–93..donghwa kim, deokseong seo, suhyoun cho, andpilsung kang.
2019. multi-co-training for docu-ment classiﬁcation using various document represen-tations: tf-idf, lda, and doc2vec.
informationsciences, vol 477:15 – 29..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..kenton lee, luheng he, and luke zettlemoyer.
2018.higher-order coreference resolution with coarse-to-ﬁne inference.
in proceedings of the 2018 confer-ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 2, pages 687–692, new.
4043orleans, louisiana.
association for computationallinguistics..junyi jessy li and ani nenkova.
2014. addressingclass imbalance for improved recognition of implicitdiscourse relations.
in proceedings of the 15th an-nual meeting of the special interest group on dis-course and dialogue (sigdial), pages 142–150,philadelphia, pa, u.s.a. association for computa-tional linguistics..shoushan li, shengfeng ju, guodong zhou, and xiao-jun li.
2012. active learning for imbalanced senti-ment classiﬁcation.
in proceedings of the 2012 jointconference on empirical methods in natural lan-guage processing and computational natural lan-guage learning, pages 139–148, jeju island, korea.
association for computational linguistics..xiaoya li, xiaofei sun, yuxian meng, junjun liang,fei wu, and jiwei li.
2020. dice loss for data-imbalanced nlp tasks.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 465–476, online.
associ-ation for computational linguistics..junyang lin, qi su, pengcheng yang, shuming ma,and xu sun.
2018. semantic-unit-based dilated con-in pro-volution for multi-label text classiﬁcation.
ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4554–4564, brussels, belgium.
association for computa-tional linguistics..xiaofei ma, peng xu, zhiguo wang, ramesh nalla-pati, and bing xiang.
2019. domain adaptationwith bert-based domain classiﬁcation and data se-in proceedings of the 2nd workshop onlection.
deep learning approaches for low-resource nlp(deeplo 2019), pages 76–83, hong kong, china.
association for computational linguistics..manolis maragoudakis, katia kermanidis, aristogian-nis garbis, and nikos fakotakis.
2006. dealingwith imbalanced data using bayesian techniques.
inproceedings of the fifth international conferenceon language resources and evaluation (lrec’06),genoa, italy.
european language resources associ-ation (elra)..j.j. mcarthur, nima shahbazi, ricky fok, christopherraghubar, brandon bortoluzzi, and aijun an.
2018.machine learning and bim visualization for mainte-nance issue classiﬁcation and enhanced data collec-tion.
advanced engineering informatics, 38:101 –112..carol midgley.
2014. goals, goal structures, and pat-.
terns of adaptive learning.
routledge..rutu mulkar-mehta, jerry hobbs, and eduard hovy.
2011. granularity in natural language discourse.
in proceedings of the ninth international confer-ence on computational semantics, iwcs ’11, page360–364, usa.
association for computational lin-guistics..vinod nair and geoffrey e. hinton.
2010. rectiﬁedlinear units improve restricted boltzmann machines.
in proceedings of the 27 th international conferenceon machine learning, haifa, israel.
icml..razvan pascanu, tomas mikolov, and yoshua bengio.
2013. on the difﬁculty of training recurrent neuralin international conference on machinenetworks.
learning, pages 1310–1318.
pmlr..jon patrick and min li.
2009. a cascade approachto extracting medication events.
in proceedings ofthe australasian language technology associationworkshop 2009, pages 99–103, sydney, australia..f. pedregosa, g. varoquaux, a. gramfort, v. michel,b. thirion, o. grisel, m. blondel, p. prettenhofer,r. weiss, v. dubourg, j. vanderplas, a. passos,d. cournapeau, m. brucher, m. perrot, and e. duch-scikit-learn: machine learning inesnay.
2011.journal of machine learning research,python.
12:2825–2830..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..alan ritter, mausam mausam, oren etzioni, and samclark.
2012. open domain event extraction fromtwitter.
proceedings of the acm sigkdd inter-national conference on knowledge discovery anddata mining, 1104-1112:1104 – 1112..guergana k. savova, james j. masanz, philip v.ogren, jiaping zheng, sunghwan sohn, karin kip-per schuler, and christopher g. chute.
2010. mayoclinical text analysis and knowledge extraction sys-tem (ctakes): architecture, component evaluationand applications.
journal of the american medicalinformatics association : jamia, 17 5:507–13..dinghan shen, martin renqiang min, yitong li, andlawrence carin.
2018. learning context-sensitiveconvolutional ﬁlters for text processing.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, pages 1839–1848,brussels, belgium.
association for computationallinguistics..richard sproat, alan w. black, stanley chen, shankarkumar, mari ostendorf, and christopher richards.
2001. normalization of non-standard words.
com-puter speech & language, 15(3):287 – 333..mirac suzgun, yonatan belinkov, and stuart m.shieber.
2019. on evaluating the generalization oflstm models in formal languages.
in proceedingsof the society for computation in linguistics (scil)2019, pages 277–286..harish tayyar madabushi, elena kochkina, andmichael castelle.
2019. cost-sensitive bert forgeneralisable sentence classiﬁcation on imbalanced.
4044in proceedings of.
the second workshopdata.
on natural language processing for internet free-dom: censorship, disinformation, and propaganda,pages 125–134, hong kong, china.
association forcomputational linguistics..jingyun xu, yi cai, xin wu, xue lei, qingbao huang,ho fung leung, and qing li.
2020. incorporatingcontext-relevant concepts into convolutional neuralnetworks for short text classiﬁcation.
neurocomput-ing, 386:42 – 53..meliha yetisgen-yildiz, cosmin bejan, and mark wur-fel.
2013. identiﬁcation of patients with acute lungin pro-injury from free-text chest x-ray reports.
ceedings of the 2013 workshop on biomedical nat-ural language processing, pages 10–17, soﬁa, bul-garia.
association for computational linguistics..zhilu zhang and mert r. sabuncu.
2018. generalizedcross entropy loss for training deep neural networkswith noisy labels.
in proceedings of the 32nd inter-national conference on neural information process-ing systems, nips’18, page 8792–8802, red hook,ny, usa.
curran associates inc..4045