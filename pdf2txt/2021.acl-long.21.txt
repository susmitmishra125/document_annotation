contrastive learning for many-to-many multilingual neural machinetranslation.
xiao pan, mingxuan wang, liwei wu, lei libytedance ai lab{panxiao.94,wangmingxuan.89,wuliwei.000,lileilab}@bytedance.com.
abstract.
existing multilingual machine translation ap-proaches mainly focus on english-centric di-rections, while the non-english directions stilllag behind.
in this work, we aim to build amany-to-many translation system with an em-phasis on the quality of non-english languagedirections.
our intuition is based on the hy-pothesis that a universal cross-language rep-resentation leads to better multilingual trans-lation performance.
to this end, we pro-pose mrasp2, a training method to ob-tain a single uniﬁed multilingual translationmodel.
mrasp2 is empowered by two tech-niques: a) a contrastive learning scheme toclose the gap among representations of dif-ferent languages, and b) data augmentationon both multiple parallel and monolingualdata to further align token representations.
for english-centric directions, mrasp2 out-performs existing best uniﬁed model andachieves competitive or even better perfor-mance than the pre-trained and ﬁne-tunedmodel mbart on tens of wmt’s transla-tion directions.
for non-english directions,mrasp2 achieves an improvement of average10+ bleu compared with the multilingualtransformer baseline.
code, data and trainedmodels are available at https://github.
com/panxiao1994/mrasp2..1.introduction.
transformer (vaswani et al., 2017) has achieveddecent performance for machine translation withrich bilingual parallel corpora.
recent work onmultilingual machine translation aims to createa single uniﬁed modelto translate many lan-guages (johnson et al., 2017; aharoni et al., 2019;zhang et al., 2020; fan et al., 2020; siddhant et al.,2020).
multilingual translation models are appeal-ing for two reasons.
first, they are model efﬁ-cient, enabling easier deployment (johnson et al.,.
figure 1: the proposed mrasp2.
it takes a pair of par-allel sentences (or augmented pseudo-pair) and com-putes normal cross entropy loss with a multi-lingualencoder-decoder.
in addition, it computes contrastiveloss on the representations of the aligned pair (positiveexample) and randomly selected non-aligned pair (neg-ative example)..2017).
further, parameter sharing across differentlanguages encourages knowledge transfer, whichbeneﬁts low-resource translation directions andpotentially enables zero-shot translation (i.e.
di-rect translation between a language pair not seenduring training) (ha et al., 2017; gu et al., 2019;ji et al., 2020)..despite these beneﬁts, challenges still remain inmultilingual nmt.
first, previous work on mul-tilingual nmt does not always perform well astheir corresponding bilingual baseline especiallyon rich resource language pairs (tan et al., 2019;zhang et al., 2020; fan et al., 2020).
such per-formance gap becomes larger with the increasingnumber of accommodated languages for multilin-gual nmt, as model capacity necessarily must besplit between many languages (arivazhagan et al.,2019).
in addition, an optimal setting for multi-lingual nmt should be effective for any languagepairs, while most previous work focus on improv-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages244–258august1–6,2021.©2021associationforcomputationallinguistics244encoderdecoder<fr> je t’aime.<fr> c’est la vie.
…… <zh> (cid:3)(cid:2)(cid:8)<en> it’s sunny.<en> i love you.…anchor+<fr> je t’aime.
lctr—positivenegative             lceing english-centric1 directions (johnson et al.,2017; aharoni et al., 2019; zhang et al., 2020).
afew recent exceptions are zhang et al.
(2020) andfan et al.
(2020), who trained many-to-many sys-tems with introducing more non-english corpora,through data mining or back translation..in this work, we take a step towards a uni-ﬁed many-to-many multilingual nmt with onlyenglish-centric parallel corpora and additionalmonolingual corpora.
our key insightis toclose the representation gap between different lan-guages to encourage transfer learning as much aspossible..as such, many-to-many translations can makethe most of the knowledge from all superviseddirections and the model can perform well forboth english-centric and non-english settings.
inthis paper, we propose a multilingual contrastivelearning framework for translation (mcolt ormrasp2) to reduce the representation gap of dif-ferent languages, as shown in figure 1..the objective of mrasp2 ensures the modelto represent similar sentences across languages ina shared space by training the encoder to mini-mize the representation distance of similar sen-tences.
in addition, we also boost mrasp2 byleveraging monolingual data to further improvemultilingual translation quality.
we introduce aneffective aligned augmentation technique by ex-tending ras (lin et al., 2020) – on both paralleland monolingual corpora to create pseudo-pairs.
these pseudo-pairs are combined with multilin-gual parallel corpora in a uniﬁed training frame-work..simple yet effective, mrasp2 achieves con-sistent translation performance improvements forboth english-centric and non-english directionson a wide range of benchmarks.
for english-centric directions, mrasp2 outperforms a strongmultilingual baseline in 20 translation directionson wmt testsets.
on 10 wmt translation bench-marks, mrasp2 even obtains better results thanthe strong bilingual mbart model.
for zero-shot and unsupervised directions, mrasp2 ob-tains surprisingly strong results on 36 translationdirections2, with 10+ bleu improvements on av-erage..2 methodology.
mrasp2 uniﬁes both parallel corpora and mono-lingual corpora with contrastive learning.
thissection will explain our proposed mrasp2.
theoverall framework is illustrated in figure 1.
2.1 multilingual transformer.
a multilingual neural machine translation modellearns a many-to-many mapping function f totranslate from one language to another.
to dis-tinguish different languages, we add an additionallanguage identiﬁcation token preceding each sen-tence, for both source side and target side.
thebase architecture of mrasp2 is the state-of-the-art transformer (vaswani et al., 2017).
a littledifferent from previous work, we choose a largersetting with a 12-layer encoder and a 12-layer de-coder to increase the model capacity.
the modeldimension is 1024 on 16 heads.
to ease the train-ing of the deep model, we apply layer normal-ization for word embedding and pre-norm residualconnection following wang et al.
(2019a) for bothencoder and decoder.
therefore, our multilingualnmt baseline is much stronger than that of trans-former big model..more formally, we deﬁne l = {l1, .
.
.
, lm }where l is a collection of m languages involv-ing in the training phase.
di,j denotes a paral-lel dataset of (li, lj), and d denotes all paralleldatasets.
the training loss is cross entropy deﬁnedas:.
(cid:88).
lce =.
− log pθ(xi|xj).
(1).
xi,xj ∈d.
where xi represents a sentence in language li,and θ is the parameter of multilingual transformermodel..2.2 multilingual contrastive learning.
multilingual transformer enables implicitly learn-ing shared representation of different languages.
mrasp2 introduces contrastive loss to explicitlybring different languages to map a shared semanticspace..the key idea of contrastive learning is tominimize the representation gap of similar sen-tences and maximize that ofirrelevant sen-tences.
formally, given a bilingual translationpairs (xi, xj) ∈ d, (xi, xj) is the positive exam-ple and we randomly choose a sentence yj fromlanguage lj to form a negative example3 (xi, yj)..1“english-centric” means that having english as the.
source or target language.
26 unsupervised directions + 30 zero-shot directions.
3it is possible that lj = li.
245(a) aa for parallel corpora.
(b) aa for monolingual corpora.
figure 2: aligned augmentation on both parallel and monolingual data by replacing words with the same meaningin synonym dictionaries.
it either creates a pseudo-parallel example (left) or a pseudo self-parallel example (right)..the objective of contrastive learning is to mini-mize the following loss:.
types of training samples are illustrated in fig-ure 2..lctr = −.
(cid:88).
xi,xj ∈d.
log.
(cid:80).
esim+(r(xi),r(xj ))/τyj esim−(r(xi),r(yj ))/τ.
(2)where sim(·) calculates the similarity of differ-ent sentences.
+ and − denotes positive andnegative respectively.
r(s) denotes the average-pooled encoded output of an arbitrary sentences. τ is the temperature, which controls the difﬁ-culty of distinguishing between positive and neg-ative examples4.
in our experiments, it is set to0.1. the similarity of two sentences is calcu-lated with the cosine similarity of the average-pooled encoded output.
to simplify implementa-tion, the negative samples are sampled from thesame training batch.
intuitively, by maximizingthe softmax term sim+(r(xi), r(xj)), the con-trastive loss forces their semantic representationsprojected close to each other.
in the meantime, thesoftmax function also minimizes the non-matchedpairs sim−(r(xi), r(yj))..during the training of mrasp2, the model canbe optimized by jointly minimizing the contrastivetraining loss and translation loss:.
lin et al.
(2020) propose random aligned sub-stitution technique (or ras5) that builds code-switched sentence pairs (c(xi), xj) for multilin-gual pre-training.
in this paper, we extend it toaligned augmentation (aa), which can also beapplied to monolingual data..for a bilingual or monolingual sentence pair(xi, xj)6, aa creates a perturbed sentence c(xi)by replacing aligned words from a synonym dic-tionary7.
for every word contained in the syn-onym dictionary, we randomly replace it to one ofits synonym with a probability of 90%..a.training.
pseudo-parallel.
for a bilingual sentence pair (xi, xj), aaexamplecreates(c(xi), xj).
for monolingual data, aa takes asentence xi and generates its perturbed c(xi) toform a pseudo self-parallel example (c(xi), xi).
(c(xi), xj) and (c(xi), xi) is then used in thetraining by calculating both the translation lossand contrastive loss.
for a pseudo self-parallelexample (c(xi), xi), the contrastive loss is basi-cally the reconstruction loss from the perturbedsentence to the original one..l = lce + λ|s|lctr.
(3).
3 experiments.
where λ is the coefﬁcient to balance the two train-ing losses.
since lctr is calculated on the sentence-level and lce is calculated on the token-level,therefore lctr should be multiplied by the averagedsequence length |s|..this section shows that mrasp2 can achievesubstantial improvements over previous many-to-many multilingual translation on a wide rangeof benchmarks.
especially, it obtains substantialgains on zero-shot directions..2.3 aligned augmentation.
3.1 settings and datasets.
we then will introduce how to improve mrasp2with data augmentation methods, including the in-troduction of noised bilingual and noised mono-lingual data for multilingual nmt.
the above two.
parallel dataset pc32 we use the paralleldataset pc32 provided by lin et al.
(2020).
it con-.
5they apply ras only on parallel data6xi is in language li and xj is in language lj, where.
4higher temperature increases the difﬁculty to distinguish.
i, j ∈ {l1, .
.
.
, lm }.
positive sample from negative ones..7we will release our synonym dictionary.
246encoderdecoderilike(cid:14)(cid:13)and(cid:18)(cid:19)<en id>j’adorechanteretdanserj’adorechanteretdanser<eos><fr id>singingdancingc(xen)xfrxfrencoderdecoder(cid:3)likeنم عاوناmusik<zh id>quel(cid:4913)(cid:9)<eos>(cid:3)(cid:6)(cid:7)(cid:17)(cid:16)(cid:11)(cid:5)(cid:12)(cid:4)(cid:1)(cid:9)(cid:3)(cid:6)(cid:7)(cid:17)(cid:16)(cid:11)(cid:5)<zh id>(cid:12)(cid:4)(cid:1)(cid:9)(cid:6)(cid:7)(cid:17)(cid:16)(cid:11)(cid:5)(cid:12)(cid:4)c(xzh)xzhxzhen-frwmt14.
en-eswmt13→ ← → ← → ← →(*) ← → ←.
en-rowmt16.
en-trwmt17.
en-fiwmt17.
avg.
∆.
bilingualtransformer-6(lin et al., 2020)transformer-12(liu et al., 2020)pre-train & ﬁne-tunedadapter (bapna and firat, 2019)mbart(liu et al., 2020)xlm(conneau and lample, 2019)mass(song et al., 2019)mrasp(lin et al., 2020)uniﬁed multilingualmulti-distillation (tan et al., 2019)m-transformermrasp w/o ﬁnetune(**)mrasp2.
43.241.4.
39.8-.
-9.5.
-12.2.
-33.2.
--.
34.036.8.
-20.2.
-21.8.
-41.1--44.3.
-42.043.143.5.
----45.4.
-38.139.239.3.
-17.8--20.0.
-18.820.021.4.
-22.5--23.4.
-23.125.225.8.
35.434.0---.
-32.834.034.5.
33.7----.
-33.734.335.0.
--.
-----.
34.334.3.
-37.7--37.6.
31.635.937.538.0.
-38.838.539.138.9.
35.837.738.839.1.
-22.4--24.0.
22.020.022.023.4.
-28.5--28.0.
21.228.229.230.1.
-31.0332.33 +1.3033.01 +1.98.
table 1: performance (tokenized bleu) on wmt supervised translation directions.
consistent bleu gainsare observed in 20 directions (see appendix) and in this table we pick the representative ones.
different from ourwork, ﬁnal bleu scores of mbart, xlm, mass and mrasp are obtained by multilingual pre-training and ﬁne-tuning on a single direction.
adapter is a trade-off between uniﬁed multilingual model and bilingual model (trainedon 6 languages on wmt data).
multi-distillation is improved over adapter with selective distillation methods.
results for transformer-6 (6 layers for encoder and decoder) are from lin et al.
(2020).
results for transformer-12 (12 layers for encoder and decoder separately) are from liu et al.
(2020).
(*) note that for en→ro direction,we follow the previous setting to calculate bleu score after removing romanian dialects.
(**) for mrasp w/oﬁnetune we report the results implemented by ourselves, with 12 layers encoder and decoder and our data.
bothm-transformer and our mrasp2 have 12 layers for encoder and decoder..tains a large public parallel corpora of 32 english-centric language pairs.
the total number of sen-tence pairs is 97.6 million..we apply aa on pc32 by randomly replac-ing words in the source side sentences with syn-onyms from an arbitrary bilingual dictionary pro-vided by (lample et al., 2018)8. for words in thedictionaries, we replace them into one of the syn-onyms with a probability of 90% and keep themunchanged otherwise.
we apply this augmentationin the pre-processing step before training..monolingual dataset mc24 we create adataset mc24 with monolingual text in 24 lan-guages9.
it is a subset of the newscrawl10 datasetby retaining only those languages in pc32, plusthree additional languages that are not in pc32(nl, pl, pt).
in order to balance the volume acrossdifferent languages, we apply temperature sam-.
(cid:16).
ni/ (cid:80).
(cid:17)1/t.
with t =5 over thepling ˜ni =dataset, where ni is the number of sentences in i-th language.
then we apply aa on monolingual.
j nj.
8https://github.com/facebookresearch/muse9bg, cs, de, el, en, es, et, fi, fr, gu, hi, it, ja, kk, lt,.
lv, ro, ru, sr, tr, zh, nl, pl, pt.
10http://data.statmt.org/news-crawl.
data.
the total number of sentences in mc24 is1.01 billion.
the detail of data volume is listed inthe appendix..we apply aa on mc24 by randomly replac-ing words in the source side sentences with syn-onyms from a multilingual dictionary.
thereforethe source side might contain multiple languagetokens (preserving the semantics of the originalsentence), and the target is just the original sen-tence.
the replace probability is also set to 90%.
we apply this augmentation in the pre-processingstep before training.
we will release the multi-lingual dictionary and the script for producing thenoised monolingual dataset..evaluation datasets for supervised directions,most of our evaluation datasets are from wmt andiwslt benchmarks, for pairs that are not avail-able in wmt or iwslt, we use opus-100 in-stead..for zero-shot directions, we follow (zhanget al., 2020) and use their proposed opus-100zero-shot testset.
the testset is comprised of 6 lan-guages (ru, de, fr, nl, ar, zh), resulting in 15language pairs and 30 translation directions..we report de-tokenized bleu with sacre-.
247en-plwmt20.
en-ptopus-100.
en-nliwslt2014→ ← → ← → ← → ←1.30.710.1.
10.711.630.5.
7.010.628.5.
3.25.317.1.
3.73.718.4.nl-pt-.
0.60.56.7.
--9.3.
--8.3.avg.
∆.
4.425.40.
+0.9818.55 +14.13.
m-transformermraspmrasp2.
table 2: mrasp2 outperforms m-transformer in unsupervised translation directions by a large margin.
we reporttokenized bleu above.
for nl↔pt, mrasp2 achieves reasonable results after trained only on monolingual dataof both sides.
the averaged score is calculated without the nl↔pt directions..ar.
zhx→ar ar→x x→zh zh→x x→nl nl→x5.53.75.3.
28.56.729.0.
17.05.617.3.
16.44.114.5.
2.22.35.3.
6.06.36.1.nl(*).
pivotm-transformermrasp2.
fr.
de.
ru.
avg of all.
pivotm-transformermrasp2.
x→fr26.17.723.6.fr→x x→de de→x x→ru ru→x22.34.821.7.
16.65.716.4.
14.44.212.3.
19.94.819.1.
14.24.815.0.
15.565.0515.31.table 3: zero-shot: we report de-tokenized bleu using sacrebleu in opus-100.
we observe consistent bleugains in zero-shot directions on different evaluation sets, see appendix for more details.
mrasp2 further improvesthe quality.
we also list bleu of pivot-based model (x→en then en→y using m-transformer) as a reference,mrasp2 only lags behind pivot by -0.25 bleu.
(*) note that dutch(nl) is not included in pc32..bleu (post, 2018).
for tokenized bleu, wetokenize both reference and hypothesis usingsacremoses11 toolkit then report bleu using themulti-bleu.pl script12.
for chinese (zh),bleu score is calculated on character-level..experiment details we use the transformermodel in our experiments, with 12 encoder layersand 12 decoder layers.
the embedding size andffn dimension are set to 1024. we use dropout= 0.1, as well as a learning rate of 3e-4 with poly-nomial decay scheduling and a warm-up step of10000. for optimization, we use adam optimizer(kingma and ba, 2015) with (cid:15) = 1e-6 and β2 =0.98. to stabilize training, we set the thresholdof gradient norm to be 5.0 and clip all gradientswith a larger norm.
we set the hyper-parameterλ = 1.0 in eq.3 during training.
for multilingualvocabulary, we follow the shared bpe (sennrichet al., 2016) vocabulary of lin et al.
(2020), whichincludes 59 languages.
the vocabulary contains64808 tokens.
after adding 59 language tokens,the total size of vocabulary is 64867..11https://github.com/alvations/sacremoses12https://github.com/moses-smt/mosesdecoder.
4 experiment results.
this section shows that mrasp2 provides consis-tent performance gains for supervised and unsu-pervised english-centric translation directions aswell as for non-english directions..4.1 english-centric directions.
supervised directions as shown in table 1,mrasp2 clearly improves multilingual baselinesby a large margin in 10 translation directions.
pre-viously, multilingual machine translation under-performs bilingual translation in rich-resource sce-narios.
it is worth noting that our multilingual ma-chine translation baseline is already very compet-it is even on par with the strong mbartitive.
bilingual model, which is ﬁne-tuned on a largescale unlabeled monolingual dataset.
mrasp2further improves the performance..we summarize the key factors for the suc-cess training of our baseline13 m-transformer:a) the batch size plays a crucial role in the suc-.
13many-to-many transformer trained on pc32 as in john-son et al.
(2017) except that we apply language indicator thesame way as fan et al.
(2020).
248model.
1 m-transformer2 mrasp w/o f.t.
(*)(cid:88)3 mrasp2 w/o aa4 mrasp2 w/o mc24 (cid:88)(cid:88)5 mrasp2.
(cid:88).
ctl aa mc24 supervised unsupervised zero-shot5.0528.654.9129.8213.5528.7914.6029.9615.3130.36.
4.425.404.755.8018.55.
(cid:88)(cid:88).
(cid:88).
table 4: summary of average bleu of mrasp2 w/o aa and mrasp2 in different scenarios.
we report averagedtokenized bleu.
for supervised translation, we report the average of 20 directions; for zero-shot translation, wereport the average of 30 directions of opus-100.
mrasp excludes mc24 and contrastive loss from mrasp2.
mrasp2 w/o aa only adopts contrastive learning on the basis of m-transformer.
mrasp2 w/o mc24 excludesmc24 from mrasp2.
(*) note that results of mrasp are computed without ﬁne-tuning..cess of training multilingual nmt.
we use 8 × 4nvidia v100 with update frequency 50 to trainthe models and each batch contains about 3 mil-lion tokens.
b) we enlarge the number of layersfrom 6 to 12 and observe signiﬁcant improvementsfor multilingual nmt.
by contrast, the gains fromincreasing the bilingual model size is not thatlarge.
mbart also uses 12 encoder and decoderlayers.
c) we use gradient norm to stable the train-ing.
without this regularization, the large scaletraining will collapse sometimes..unsupervised directionsin table 2, we ob-serve that mrasp2 achieves reasonable results onunsupervised translation directions.
the languagepairs of en-nl, en-pt, and en-pl are never ob-served by m-transformer.
m-transformer some-times achieves reasonable bleu for x→en, e.g.
10.7 for pt→en, since there are many similar lan-guages in pc32, such as es and fr.
not surpris-ingly, it totally fails on en→x directions.
by con-trast, mrasp2 obtains +14.13 bleu score on anaverage without explicitly introducing supervisionsignals for these directions..furthermore, mrasp2 achieves reasonablebleu scores on nl↔pt directions even thoughit has only been trained on monolingual data ofboth sides.
this indicates that by simply incorpo-rating monolingual data with parallel data in theuniﬁed framework, mrasp2 successfully enablesunsupervised translation through its uniﬁed multi-lingual representation..can do zero-shot translation directly.
however,the translation quality is quite poor compared withpivot-based model..we evaluate mrasp2 on the opus-100 (zhanget al., 2020) zero-shot test set, which contains 6languages14 and 30 translation directions in total.
to make the comparison clear, we also report theresults of several different baselines.
mrasp2w/o aa only adopt contrastive learning on thebasis of m-transformer.
mrasp2 w/o mc24 ex-cludes monolingual data from mrasp2..the evaluation results are listed in appendixand we summarize them in table 3. we ﬁndthat our mrasp2 signiﬁcantly outperforms m-transformer and substantially narrows the gapwith pivot-based model.
this is in line with our in-tuition that bridging the representation gap of dif-ferent languages can improve the zero-shot trans-lation..the main reason is that contrastive loss, alignedaugmentation and additional monolingual data en-able a better language-agnostic sentence repre-sentation.
it is worth noting that, zhang et al.
(2020) achieves bleu score improvements onzero-shot translations at sacriﬁce of about 0.5bleu score loss on english-centric directions.
bycontrast, mrasp2 improves zero-shot translationby a large margin without losing performance onenglish-centric directions.
therefore, mrasp2has a great potential to serve many-to-many trans-lations, including both english-centric and non-english directions..4.2 zero-shot translation for non-english.
5 analysis.
directions.
zero-shot translation has been an intriguing topicin multilingual neural machine translation.
previ-ous work shows that the multilingual nmt model.
to understand what contributes to the performancegain, we conduct analytical experiments in this.
14arabic, chinese, dutch, french, german, russian.
249section.
first we summarize and analysis theperformance of mrasp2 in different scenarios.
second we adopt the sentence representation ofmrasp2 to retrieve similar sentences across lan-guages.
this is to verify our argument that theimprovements come from the universal languagerepresentation learned by mrasp2.
finally we vi-sualize the sentence representations, mrasp2 in-deed draws the representations closer..5.1 ablation study.
to make a better understanding of the effective-ness of mrasp2, we evaluate models of differentsettings.
we summarize the experiment results intable 4:.
• 1 v.s.
3 :.
3 performs comparably withm-transformer in supervised and unsuper-vised scenarios, whereas achieves a substan-tial bleu improvement for zero-shot trans-lation.
this indicates that by introducing con-trastive loss, we can improve zero-shot trans-lation quality without harming other direc-tions..• 2 v.s.
4 : 2 performs poorly for zero-shotdirections.
this means contrastive loss is cru-cial for the performance in zero-shot direc-tions..• 5 : mrasp2 further improves bleu in allof the three scenarios, especially in unsuper-vised directions.
therefore it is safe to con-jecture that by accomplishing with monolin-gual data, mrasp2 learns a better represen-tation space..5.2 similarity search.
in order to verify whether mrasp2 learns a bet-ter representation space, we conduct a set of sim-ilarity search experiments.
similarity search is atask to ﬁnd the nearest neighbor of each sentencein another language according to cosine similarity.
we argue that mrasp2 beneﬁts this task in thesense that it bridges the representation gap acrosslanguages.
therefore we use the accuracy of sim-ilarity search tasks as a quantitative indicator ofcross-lingual representation alignment..we conducted comprehensive experiments tosupport our argument and experiment on mrasp2and mrasp2 w/o aa .we divide the experimentsinto two scenarios: first we evaluate our method.
(a) ∆acc of mrasp2 w/oaa over m-transformer.
(b) ∆acc of mrasp2 overmrasp2 w/o aa.
figure 3: accuracy improvements of m-transformer→ mrasp2 w/o aa → mrasp2 for ted-m. darkerred means larger improvements.
mrasp2 w/o aagenerally improves accuracy over m-transformer andmrasp2 especially improves the accuracy x ↔ nlover mrasp2 w/o aa ..on tatoeba dataset (artetxe and schwenk, 2019),which is english-centric.
then we conduct simi-lar similarity search task on non-english languagepairs.
following tran et al.
(2020), we construct amulti-way parallel testset (ted-m) of 2284 sam-ples by ﬁltering the test split of ted15 that havetranslations for all 15 languages16..under both settings, we follow the same strat-egy: we use the average-pooled encoded outputas the sentence representation.
for each sentencefrom the source language, we search the closestsentence in the target set according to cosine sim-ilarity..english-centric: tatoeba we display the eval-uation results in table 5. we detect two trends:(i) the overall accuracy follows the rule: m-transformer < mrasp2 w/o aa < mrasp2.
(ii) mrasp2 brings more signiﬁcant improve-ments for languages with less data volume inpc32.
the two trends mean that mrasp2 in-creases translation bleu score in a sense that itbridges the representation gap across languages..non-english: ted-m it will be more convinc-ing to argue that mrasp2 indeed bridges the rep-resentation gap if similarity search accuracy in-creases on zero-shot directions.
we list the av-eraged top-1 accuracy of 210 non-english direc-tions17 in table 6. the results show that mrasp2increases the similarity search accuracy in zero-shot scenario.
the results support our argument.
15http://phontron.com/data/ted talks.tar.gz16arabic, czech, german, english, spanish, french, ital-ian, japanese, korean, dutch, romanian, russian, turkish,vietnamese, chinese.
1715 languages, resulting in 210 directions.
250arcsdeenesfritjakonlrorutrvizharcsdeenesfritjakonlrorutrvizh0.00.10.2arcsdeenesfritjakonlrorutrvizharcsdeenesfritjakonlrorutrvizh0.00.20.4lang.
fr.
de.
zh.
ro.
cs.
tr.
ru.
nl.
pl.
pt.
m-transformer91.7mrasp2 w/o aa 91.793.0mrasp2.
96.897.398.0.
87.089.990.7.
90.691.491.9.
84.886.189.3.
91.192.492.4.
89.190.492.3.
25.635.760.3.
6.314.328.1.
37.346.558.6.table 5: english-centric: sentence retrieval top-1 accuracy on tatoeba evaluation set.
the reported accuracy isthe average of en→x and x→en accuracy.
mrasp2 outperforms m-transformer on all directions in english-centric sentence retrieval task..top1 acc ∆-79.8m-transformer+4.8mrasp2 w/o aa 84.489.6mrasp2+9.8.
table 6: non-english: the averaged sentence sim-ilarity search top-1 accuracy on ted-m testset.
m-transformer < mrasp2 w/o aa < mrasp2, whichis consistent with the results in english-centric sce-nario..that our method generally narrows the representa-tion gap across languages..to better understanding the speciﬁcs beyond theaveraged accuracy, we plot the accuracy improve-ments in the heat map in figure 3. mrasp2w/o aa brings general improvements over m-transformer.
mrasp2 especially improves ondutch(nl).
this is because mrasp2 introducesmonolingual data of dutch while mrasp2 w/oaa includes no dutch data..5.3 visualization.
in order to visualize the sentence representationsacross languages, we retrieve the sentence repre-sentation r(s) for each sentence in ted-m, re-sulting in 34260 samples in the high-dimensionalspace..to facilitate visualization, we apply t-sne di-mension reduction to reduce the 1024-dim rep-resentations to 2-dim.
then we select 3 repre-sentative languages: english, german, japaneseand depict the bivariate kernel density estimationbased on the 2-dim representations.
it is clear infigure 4 that m-transformer cannot align the 3languages.
by contrast, mrasp2 draws the rep-resentations across 3 languages much closer..6 related work.
multilingual neural machine translationwhile initial research on nmt starts with build-.
ing translation systems between two languages,dong et al.
(2015) extends the bilingual nmt toone-to-many translation with sharing encodersacross 4 language pairs.
hence, there has beena massive increase in work on mt systems thatinvolve more than two languages (chen et al.,2018; choi et al., 2018; chu and dabre, 2019;dabre et al., 2017).
recent efforts mainly focuseson designing languagecomponentsfor multilingual nmt to enhance the modelperformance on rich-resource languages (bapnaand firat, 2019; kim et al., 2019; wang et al.,2019b; escolano et al., 2020).
another promisingthread line is to enlarge the model size withextensive training data to improve the modelcapability (arivazhagan et al., 2019; aharoniet al., 2019; fan et al., 2020).
different from theseapproaches, mrasp2 proposes to explicitly closethe semantic representation of different languagesand make the most of cross lingual transfer..speciﬁc.
zero-shot machine translation typical zero-shot machine translation models rely on a pivotlanguage (e.g.
english) to combine the source-pivot and pivot-target translation models (chenet al., 2017; ha et al., 2017; gu et al., 2019; cur-rey and heaﬁeld, 2019).
johnson et al.
(2017)shows that a multilingual nmt system enableszero-shot translation without explicitly introduc-ing pivot methods.
promising, but the perfor-mance stilllags behind the pivot competitors.
most following up studies focused on data aug-mentation methods.
zhang et al.
(2020) im-proved the zero-shot translation with online backtranslation.
ji et al.
(2020); liu et al.
(2020)shows that large scale monolingual data can im-prove the zero-shot translation with unsupervisedpre-training.
fan et al.
(2020) proposes a sim-ple and effective data mining method to enlargethe training corpus of zero-shot directions.
somework also attempted to explicitly learn shared se-mantic representation of different languages to im-.
251(a) m-transformer.
(b) mrasp2.
figure 4: bivariate kernel density estimation plots of representations after using t-sne dimensionality reductionto 2 dimension.
the blue line is english, the orange line is japanese and the green line is german.
this ﬁgureillustrates that the sentence representations are drawn closer after applying mrasp2.
prove the zero-shot translation.
lu et al.
(2018)suggests that by learning an explicit “interlingual”across languages, multilingual nmt model cansigniﬁcantly improve zero-shot translation quality.
al-shedivat and parikh (2019) introduces a con-sistent agreement-based training method that en-courages the model to produce equivalent transla-tions of parallel sentences in auxiliary languages.
different from these efforts, mrasp2 attempts tolearn a universal many-to-many model, and bridgethe cross-lingual representation with contrastivelearning and m-ras.
the performance is verycompetitive both on zero-shot and supervised di-rections on large scale experiments..contrastive learning contrastive learning hasbecome a rising domain and achieved signiﬁcantsuccess in various computer vision tasks (zhuanget al., 2019; tian et al., 2020; he et al., 2020; chenet al., 2020; misra and van der maaten, 2020).
re-searchers in the nlp domain have also exploredcontrastive learning for sentence representation.
wu et al.
(2020) employed multiple sentence-levelaugmentation strategies to learn a noise-invariantsentence representation.
fang and xie (2020) ap-plies the back-translation to create augmentationsof original sentences.
inspired by these studies, weapply contrastive learning for multilingual nmt..cross-lingual representation cross-lingualrepresentation learning has been intensivelystudied in order to improve cross-lingual un-derstanding (xlu) tasks.
multilingual masked.
language models (mlm), such as mbert(devlinet al., 2019) and xlm(conneau and lample,2019), train large transformer models on mul-tiple languages jointly and have built strongbenchmarks on xlu tasks.
most of the previousworks on cross-lingual representation learningfocus on unsupervised training.
for supervisedlearning, conneau and lample (2019) proposestlm objective that simply concatenates parallelsentences as input.
by contrast, mrasp2 lever-ages the supervision signal by pulling closer therepresentations of parallel sentences..7 conclusion.
we demonstrate that contrastive learning can sig-niﬁcantly improve zero-shot machine translationdirections.
combined with additional unsuper-vised monolingual data, we achieve substantialimprovements on all translation directions of mul-tilingual nmt.
we analyze and visualize ourmethod, and ﬁnd that contrastive learning tendsto close the representation gap of different lan-guages.
our results also show the possibilities oftraining a true many-to-many multilingual nmtthat works well on any translation direction.
in fu-ture work, we will scale-up the current training tomore languages, e.g.
pc150.
as such, a singlemodel can handle more than 100 languages andoutperforms the corresponding bilingual baseline..25210050050100x10050050100ylanguageenjade6040200204060x80604020020406080ylanguageenjadereferences.
roee aharoni, melvin johnson, and orhan firat.
2019.massively multilingual neural machine translation.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages3874–3884.
association for computational linguis-tics..maruan al-shedivat and ankur p. parikh.
2019. con-sistency by agreement in zero-shot neural machinetranslation.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 1184–1197.
association for computa-tional linguistics..naveen arivazhagan, ankur bapna, orhan firat,dmitry lepikhin, melvin johnson, maxim krikun,mia xu chen, yuan cao, george f. foster, colincherry, wolfgang macherey, zhifeng chen, andyonghui wu.
2019. massively multilingual neuralmachine translation in the wild: findings and chal-lenges.
corr, abs/1907.05019..mikel artetxe and holger schwenk.
2019. mas-sively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
trans.
as-soc.
comput.
linguistics, 7:597–610..ankur bapna and orhan firat.
2019. simple, scal-able adaptation for neural machine translation.
inproceedings of the 2019 conference on empiri-cal methods in natural language processing andthe 9th international joint conference on naturallanguage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 1538–1548. association for computational linguistics..ting chen, simon kornblith, mohammad norouzi,and geoffrey e. hinton.
2020. a simple frameworkfor contrastive learning of visual representations.
inproceedings of the 37th international conference onmachine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 1597–1607.
pmlr..yun chen, yang liu, yong cheng, and victor o. k.li.
2017. a teacher-student framework for zero-in proceed-resource neural machine translation.
ings of the 55th annual meeting of the associationfor computational linguistics, acl 2017, vancou-ver, canada, july 30 - august 4, volume 1: longpapers, pages 1925–1935.
association for compu-tational linguistics..of artiﬁcial intelligence (iaai-18), and the 8th aaaisymposium on educational advances in artiﬁcialintelligence (eaai-18), new orleans, louisiana,usa, february 2-7, 2018, pages 5086–5093.
aaaipress..gyu-hyeon choi, jong-hun shin, and young kil kim.
2018.improving a multi-source neural machinetranslation model with corpus extension for low-resource languages.
in proceedings of the eleventhinternational conference on language resourcesand evaluation, lrec 2018, miyazaki, japan, may7-12, 2018. european language resources associ-ation (elra)..chenhui chu and raj dabre.
2019. multilingual multi-domain adaptation approaches for neural machinetranslation.
corr, abs/1906.07978..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, 8-14 december 2019,vancouver, bc, canada, pages 7057–7067..anna currey and kenneth heaﬁeld.
2019..zero-resource neural machine translation with monolin-gual pivot data.
in proceedings of the 3rd workshopon neural generation and translation@emnlp-ijcnlp 2019, hong kong, november 4, 2019,pages 99–107.
association for computational lin-guistics..raj dabre, fabien cromier`es, and sadao kurohashi.
2017. enabling multi-source neural machine trans-lation by concatenating source sentences in multiplelanguages.
corr, abs/1702.06135..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..daxiang dong, hua wu, wei he, dianhai yu, andhaifeng wang.
2015. multi-task learning for mul-in proceedings of thetiple language translation.
53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing of theasian federation of natural language processing,acl 2015, july 26-31, 2015, beijing, china, volume1: long papers, pages 1723–1732.
the associationfor computer linguistics..yun chen, yang liu, and victor o. k. li.
2018. zero-resource neural machine translation with multi-agent communication game.
in proceedings of thethirty-second aaai conference on artiﬁcial intelli-gence, (aaai-18), the 30th innovative applications.
carlos escolano, marta r. costa-juss`a, jos´e a. r.fonollosa, and mikel artetxe.
2020. training mul-tilingual machine translation by alternately freez-ing language-speciﬁc encoders-decoders.
corr,abs/2006.01594..253angela fan, shruti bhosale, holger schwenk, zhiyima, ahmed el-kishky, siddharth goyal, man-deep baines, onur celebi, guillaume wenzek,vishrav chaudhary, naman goyal, tom birch, vi-taliy liptchinsky, sergey edunov, edouard grave,michael auli, and armand joulin.
2020.be-yond english-centric multilingual machine transla-tion.
corr, abs/2010.11125..hongchao fang and pengtao xie.
2020. cert: con-trastive self-supervised learning for language under-standing.
corr, abs/2005.12766..jiatao gu, yong wang, kyunghyun cho, and victoro. k. li.
2019.improved zero-shot neural ma-chine translation via ignoring spurious correlations.
in proceedings of the 57th conference of the asso-ciation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 1258–1268.
association forcomputational linguistics..thanh-le ha, jan niehues, and alexander h. waibel.
2017. effective strategies in zero-shot neural ma-chine translation.
corr, abs/1711.07893..kaiming he, haoqi fan, yuxin wu, saining xie, andross b. girshick.
2020. momentum contrast for un-in 2020supervised visual representation learning.
ieee/cvf conference on computer vision and pat-tern recognition, cvpr 2020, seattle, wa, usa,june 13-19, 2020, pages 9726–9735.
ieee..baijun ji, zhirui zhang, xiangyu duan, min zhang,boxing chen, and weihua luo.
2020. cross-lingualpre-training based transfer for zero-shot neural ma-chine translation.
in the thirty-fourth aaai con-ference on artiﬁcial intelligence, aaai 2020, thethirty-second innovative applications of artiﬁcialintelligence conference, iaai 2020, the tenth aaaisymposium on educational advances in artiﬁcialintelligence, eaai 2020, new york, ny, usa, febru-ary 7-12, 2020, pages 115–122.
aaai press..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda vi´egas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..yunsu kim, yingbo gao, and hermann ney.
2019.effective cross-lingual transfer of neural machinetranslation models without shared vocabularies.
inproceedings of the 57th conference of the asso-ciation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 1246–1257.
association forcomputational linguistics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..guillaume lample, alexis conneau, ludovic de-noyer, and marc’aurelio ranzato.
2018. unsuper-vised machine translation using monolingual cor-in 6th international conference onpora only.
learning representations, iclr 2018, vancouver,bc, canada, april 30 - may 3, 2018, conferencetrack proceedings.
openreview.net..zehui lin, xiao pan, mingxuan wang, xipeng qiu,jiangtao feng, hao zhou, and lei li.
2020. pre-training multilingual neural machine translation byin proceed-leveraging alignmentings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages2649–2663, online.
association for computationallinguistics..information..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
trans.
assoc.
comput.
linguistics, 8:726–742..yichao lu, phillip keung, faisal ladhak, vikas bhard-waj, shaonan zhang, and jason sun.
2018. a neu-ral interlingua for multilingual machine translation.
in proceedings of the third conference on machinetranslation: research papers, pages 84–92, brus-sels, belgium.
association for computational lin-guistics..ishan misra and laurens van der maaten.
2020. self-supervised learning of pretext-invariant representa-tions.
in 2020 ieee/cvf conference on computervision and pattern recognition, cvpr 2020, seat-tle, wa, usa, june 13-19, 2020, pages 6706–6716.
ieee..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, wmt 2018,belgium, brussels, october 31 - november 1, 2018,pages 186–191.
association for computational lin-guistics..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th acl (vol-ume 1: long papers), pages 1715–1725, berlin,germany.
association for computational linguis-tics..aditya siddhant, ankur bapna, yuan cao, orhan fi-rat, mia xu chen, sneha reddy kudugunta, naveenarivazhagan, and yonghui wu.
2020. leveragingmonolingual data with self-supervision for multilin-gual neural machine translation.
in proceedings ofthe 58th annual meeting of the association for com-putational linguistics, acl 2020, online, july 5-10,2020, pages 2827–2835.
association for computa-tional linguistics..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to se-quence pre-training for language generation.
in pro-ceedings of the 36th international conference on.
254a case study.
we plot the location of multi-way parallel sen-tences in the representation space of mrasp2 infigure 5 and list sentences number 1 and 100 intable 7.b details of evaluation results.
we list detailed results of evaluation on a widerange of test sets..b.1 results on opus-100.
detailed results on opus-100 zero-shot evalua-tion set are listed in table 8.b.2 results on wmt.
detailed results on wmt evaluation set are listedin table 9.c example of aa.
we show two results of sentences after aa in fig-ure 6.d details of mc24.
we describe the detail of mc24 in table 10.machine learning, icml 2019, 9-15 june 2019,long beach, california, usa, volume 97 of pro-ceedings of machine learning research, pages5926–5936.
pmlr..xu tan, yi ren, di he, tao qin, zhou zhao, andtie-yan liu.
2019. multilingual neural machinetranslation with knowledge distillation.
in 7th inter-national conference on learning representations,iclr 2019, new orleans, la, usa, may 6-9, 2019.openreview.net..yonglong tian, dilip krishnan, and phillip isola.
2020.contrastive multiview coding.
in computer vision -eccv 2020 - 16th european conference, glasgow,uk, august 23-28, 2020, proceedings, part xi, vol-ume 12356 of lecture notes in computer science,pages 776–794.
springer..chau tran, yuqing tang, xian li, and jiatao gu.
2020.cross-lingual retrieval for iterative self-supervisedin advances in neural information pro-training.
cessing systems 33: annual conference on neu-ral information processing systems 2020, neurips2020, december 6-12, 2020, virtual..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..qiang wang, bei li, tong xiao,.
jingbo zhu,changliang li, derek f. wong, and lidia s. chao.
2019a.
learning deep transformer models for ma-chine translation.
in proceedings of the 57th acl,pages 1810–1822, florence, italy.
association forcomputational linguistics..yining wang, long zhou, jiajun zhang, feifei zhai,jingfang xu, and chengqing zong.
2019b.
a com-pact and language-sensitive multilingual translationmethod.
in proceedings of the 57th conference ofthe association for computational linguistics, acl2019, florence, italy, july 28- august 2, 2019, vol-ume 1: long papers, pages 1213–1223.
associationfor computational linguistics..zhuofeng wu, sinong wang, jiatao gu, madiankhabsa, fei sun, and hao ma.
2020. clear: con-trastive learning for sentence representation.
corr,abs/2012.15466..biao zhang, philip williams, ivan titov, and rico sen-nrich.
2020. improving massively multilingual neu-ral machine translation and zero-shot translation.
inproceedings of the 58th acl, pages 1628–1639, on-line.
association for computational linguistics..chengxu zhuang, alex lin zhai, and daniel yamins.
2019. local aggregation for unsupervised learningof visual embeddings.
in 2019 ieee/cvf interna-tional conference on computer vision, iccv 2019,seoul, korea (south), october 27 - november 2,2019, pages 6001–6011.
ieee..255id.
language sentence.
1.
100.deenja.
deenja.
was sie alle eint, ist, dass sie sterben werden.
the one thing that all of them have in common is that they’re going to die.
１ つ 全員 に 共通 し て 言え る の は 皆 いずれ 死 ぬ と い う こと で す.rechts seht ihr meinen kollegen s¨oren , der sich wirklich in dem raum beﬁndet.
on the right side you can see my colleague soren , who ’s actually in the space.
右側 に は 同僚 ・ ソーレン が 見え ま す 実際 その 場所 に い た の で す.table 7: case study: parallel sentences distributed in english, german and japanese..ar-4.71.93.93.14.83.7.ar-4.03.02.85.35.24.1.ar-6.51.76.24.97.15.3.arzhnlfrderuavg.
arzhnlfrderuavg.
arzhnlfrderuavg.
m-transformerfrnl7.61.27.70.810.8--3.76.54.55.91.57.72.3.de1.81.79.94.3-3.24.2.mraspfr1.23.84.4-1.41.02.4.nl1.64.2-13.32.51.54.6.mrasp2fr22.832.97.5-24.729.923.6.nl3.21.9-7.59.24.55.3.de6.85.47.85.4-5.66.2.de11.27.610.218.9-13.512.3.zh9.2-5.16.54.48.46.7.zh5.7-7.514.86.16.78.2.zh32.5-8.242.321.640.629.0.ru8.25.83.75.35.5-5.7.ru4.02.92.67.43.4-4.1.avg5.6ar4.1zh6.3nl4.8fr4.8de4.8ru5.05 avg.
avg3.9ar4.1zh5.1nl6.4fr3.7de4.0ru4.91 avg.
ru16.723.72.924.414.4-16.4.avgar17.3zh14.5nl6.1fr21.7de15.019.1ru15.31 avg.
mrasp2 w/o aadefr10.519.18.032.110.310.318.0--23.012.230.111.822.9.nl1.20.9-3.74.41.52.3.zh26.1-5.541.519.937.426.1.mrasp2 w/o mc24fr20.933.816.3-24.330.625.2.de7.95.914.316.7-11.111.2.nl1.00.7-3.74.61.12.2.zh28.8-8.141.521.338.027.5.pivotfr22.937.710.1-25.334.526.1.nl1.00.8-3.64.31.42.2.zh31.4-4.944.120.841.528.5.de13.511.99.721.4-15.514.4.ar-5.62.35.64.65.94.8.ar-6.33.26.66.17.15.9.ar-7.31.76.84.96.75.5.ru12.517.33.819.513.6-13.3.ru15.620.06.021.415.0-15.6.ru16.424.23.723.215.5-16.6.avg13.912.85.618.813.117.413.55.avg14.813.39.619.114.317.614.60.avg17.016.46.022.314.219.915.56.table 8: detailed de-tokenized bleu on opus-100 zero-shot test set.
note that results of mrasp are computedwithout ﬁne-tuning..256figure 5: case study: examples of representations of multi-way parallel sentences on mrasp2 representationspace.
we can observe that similar sentences overlap perfectly on the space.
numbers in the legend means theid of sentence in ted-m (see table 7 for detailed sentences).
we can clearly observe that similar sentences areclustered to the neighboring location..figure 6: two examples of sentences with its noised version after aa.
en-dewmt14.
en-rowmt16.
en-frwmt14.
en-zhwmt17→ ← → ← → ← → ← → ←31.342.031.542.132.143.132.543.333.143.5.
34.234.634.634.735.0.
27.126.829.229.129.7.
38.138.739.239.339.3.
37.737.538.839.039.1.
20.920.822.522.424.3.
26.926.628.228.428.7.
32.833.234.835.034.6.
24.224.724.824.523.8.en-cswmt16.
en-ruwmt19.
en-eswmt13.
en-trwmt16.
en-fiwmt17→ ← → ← → ← → ← → ←32.018.232.218.232.420.032.620.432.621.4.
33.733.234.334.335.0.
29.029.230.130.430.8.
20.020.022.022.023.4.
28.227.829.229.430.1.
32.833.134.034.134.5.
17.017.618.618.619.2.
24.324.825.225.725.8.
22.623.223.323.423.2.en-itwmt09.
avg.
∆.
28.6528.79 +0.1429.82 +1.1729.96 +1.3130.36 +1.71.
m-transformermrasp2 w/o aamraspmrasp2 w/o mc24mrasp2.
m-transformermrasp2 w/o aamraspmrasp2 w/o mc24mrasp2.
table 9: tokenized bleu score on public wmt testsets.
mrasp2 w/o aa only adopt contrastive learning onthe basis of m-transformer.
mrasp excludes mc24 and contrastive loss from mrasp2.
mrasp2 w/o mc24excludes monolingual data from mrasp2.
note that results of mrasp are computed without ﬁne-tuning..257402002040402002040dedededeenenenenjajajajacase study of sentence representations visualizationdeenja1100120020001original (en)one more point is lost in this debate: that the eu is proposing far fewer rules now.aaone высокого πόντος той perduti ماعلا tento diskusijos : tuo cette eu is soovitab 遠く 低い    регламент घंटे .2original (en)" if we don 't win , there will be some inquiries of why we haven't , " graves told bbc radio leeds.aa" if noi annetada 't הירוטיו , そこ хочу jet sometime αιτήσεις seine kuna bize haven't , " graves erzählte bbc radio leeds.
lanuage original num.
sampling ratio % of replaced tokens final num.
bgcsdeelenesetﬁfrguhiitjakkltlvrorusrtrzhnlplptsum.
5983963167118121919853532298097085785847217839663892527551368970717601161871649911521321610647973622530233926819318577814703228952685562708399643916754149502982367062891177713340471491030901014480912.
3787062875808960319938740417894322444670017632409497834519954908852741955307476240797391709503250665185372824466271094222920094801893732083801560163375984238918117771334047149103090.
1.580.890.295.500.381.247.822.570.8435.261.851.5611.1418.3013.024.302.620.7910.303.038.661.001.001.00.
/0.290.400.350.620.600.280.290.54/0.460.470.15/0.160.350.340.29/0.290.150.52?
?.
table 10: detail of mc24, ’?’ means the data is missing, and ’/’ means the corresponding language is not containedin the synonym dictionary..258