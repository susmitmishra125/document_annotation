neural bi-lexicalized pcfg induction.
songlin yang♣, yanpeng zhao♦, kewei tu♣˚♣school of information science and technology, shanghaitech universityshanghai engineering research center of intelligent vision and imagingshanghai institute of microsystem and information technology, chinese academy of sciencesuniversity of chinese academy of sciences♦ilcc, university of edinburgh{yangsl,tukw}@shanghaitech.edu.cnyannzhao.ed@gmail.com.
abstract.
neural lexicalized pcfgs (l-pcfgs) (zhuet al., 2020) have been shown effective ingrammar induction.
however, to reduce com-putational complexity, they make a strong in-dependence assumption on the generation ofthe child word and thus bilexical dependen-in this paper, we proposecies are ignored.
an approach to parameterize l-pcfgs with-out making implausible independence assump-tions.
our approach directly models bilexi-cal dependencies and meanwhile reduces bothlearning and representation complexities of l-pcfgs.
experimental results on the englishwsj dataset conﬁrm the effectiveness of ourapproach in improving both running speed andunsupervised parsing performance..1.introduction.
probabilistic context-free grammars (pcfgs) hasbeen an important probabilistic approach to syntac-tic analysis (lari and young, 1990; jelinek et al.,1992).
they assign a probability to each of theparses admitted by cfgs and rank them by the plau-sibility in such a way that the ambiguity of cfgscan be ameliorated.
still, due to the strong indepen-dence assumption of cfgs, vanilla pcfgs (char-niak, 1996) are far from adequate for highly am-biguous text..a common premise for tackling the issue is toincorporate lexical information and weaken the in-dependence assumption.
there have been many ap-proaches proposed under the premise (magerman,1995; collins, 1997; johnson, 1998; klein andmanning, 2003).
among them lexicalized pcfgs(l-pcfgs) are a relatively straightforward formal-ism (collins, 2003).
l-pcfgs extend pcfgs byassociating a word, i.e., the lexical head, with eachgrammar symbol.
they can thus exploit lexical.
˚corresponding author.
information to disambiguate parsing decisions andare much more expressive than vanilla pcfgs.
however, they suffer from representation and in-ference complexities.
for representation, the ad-dition of lexical information greatly increases thenumber of parameters to be estimated and exac-erbates the data sparsity problem during learning,so the expectation-maximisation (em) based esti-mation of l-pcfgs has to rely on sophisticatedsmoothing techniques and factorizations (collins,2003).
as for inference, the cyk algorithm forl-pcfgs has a opl5|g|q complexity, where l isthe sentence length and |g| is the grammar con-stant.
although eisner and satta (1999) manage toreduce the complexity to opl4|g|q, inference withl-pcfgs is still relatively slow, making them lesspopular nowadays..recently, zhu et al.
(2020) combine the ideas offactorizing the binary rule probabilities (collins,2003) and neural parameterization (kim et al.,2019) and propose neural l-pcfgs (nl-pcfgs),achieving good results in both unsupervised depen-dency and constituency parsing.
neural parame-terization is the key to success, which facilitatesinformed smoothing (kim et al., 2019), reducesthe number of learnable parameters for large gram-mars (chiu and rush, 2020; yang et al., 2021)and facilitates advanced gradient-based optimiza-tion techniques instead of using the traditional emalgorithm (eisner, 2016).
however, zhu et al.
(2020) oversimplify the binary rules to decreasethe complexity of the inside/cyk algorithm inlearning (i.e., estimating the marginal sentence log-likelihood) and inference.
speciﬁcally, they make astrong independence assumption on the generationof the child word such that it is only dependent onthe nonterminal symbol.
bilexical dependencies,which have been shown useful in unsupervised de-pendency parsing (han et al., 2017; yang et al.,2020), are thus ignored..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2688–2699august1–6,2021.©2021associationforcomputationallinguistics2688to model bilexical dependencies and meanwhilereduce complexities, we draw inspiration from thecanonical polyadic decomposition (cpd) (koldaand bader, 2009) and propose a latent-variablebased neural parameterization of l-pcfgs.
co-hen et al.
(2013); yang et al.
(2021) have used cpdto decrease the complexities of pcfgs, and ourwork can be seen as an extension of their workto l-pcfgs.
we further adopt the unfold-refoldtransformation technique (eisner and blatz, 2007)to decrease complexities.
by using this technique,we show that the time complexity of the inside al-gorithm implemented by zhu et al.
(2020) can beimproved from cubic to quadratic in the number ofnonterminals m. the inside algorithm of our pro-posed method has a linear complexity in m aftercombining cpd and unfold-refold..we evaluate our model on the benchmarkingwall street journey (wsj) dataset.
our modelsurpasses the strong baseline nl-pcfg (zhu et al.,2020) by 2.9% mean f1 and 1.3% mean uuasunder cyk decoding.
when using the minimalbayes-risk (mbr) decoding, our model performseven better.
we provide an efﬁcient implementationof our proposed model at https://github.com/sustcsonglin/tn-pcfg..2 background.
2.1 lexicalized cfgs.
we ﬁrst introduce the formalization of cfgs.
acfg is deﬁned as a 5-tuple g “ ps, n , p, σ, rqwhere s is the start symbol, n is a ﬁnite set of non-terminal symbols, p is a ﬁnite set of preterminalsymbols,1 σ is a ﬁnite set of terminal symbols, andr is a set of rules in the following form:.
s ñ aa ñ bc,t ñ w,.
a p na p n , b, c p n y pt p p, w p σ.n , p and σ are mutually disjoint.
we will use‘nonterminals’ to indicate n y p when it is clearfrom the context..lexicalized cfgs (l-cfgs) (collins, 2003) ex-tend cfgs by associating a word with each of the.
1an alternative deﬁnition of cfgs does not distinguishnonterminals n (constituent labels) from preterminals p (part-of-speech tags) and treats both as nonterminals..nonterminals:.
s ñ arwpsa p narwps ñ brwpscrwqs, a p n ; b, c p n y parwps ñ crwqsbrwps, a p n ; b, c p n y pt p pt rwps ñ wp,.
where wp, wq p σ are the headwords of the con-stituents spanned by the associated grammar sym-bols, and p, q are the word positions in the sen-tence.
we refer to a, a parent nonterminal an-notated by the headword wp, as head-parent.
inbinary rules, we refer to a child nonterminal ashead-child if it inherits the headword of the head-parent (e.g., brwps) and as non-head-child other-wise (e.g., crwqs).
a head-child appears as eitherthe left child or the right child.
we denote thehead direction by d p tð, ñu, where ð meanshead-child appears as the left child..2.2 grammar induction with lexicalized.
probabilistic cfgs.
lexicalized probabilistic cfgs (l-pcfgs) extendl-cfgs by assigning each production rule r “a ñ γ a scalar πr such that it forms a valid cate-gorical probability distribution given the left handside a. note that preterminal rules always have aprobability of 1 because they deﬁne a deterministicgenerating process..grammar induction with l-pcfgs follows thesame way of grammar induction with pcfgs.
aswith pcfgs, we maximize the log-likelihood ofeach observed sentence w “ w1, .
.
.
, wl:.
log ppwq “ log.
pptq ,.
(1).
ÿ.tptgl pwq.
ś.rpt πr and tglpwq consists ofwhere pptq “all possible lexicalized parse trees of the sentencew under an l-pcfg gl.
we can compute themarginal ppwq of the sentence by using the in-side algorithm in polynomial time.
the core re-cursion of the inside algorithm is formalized inequation 3. it recursively computes the probabilitysa,pi,j of a head-parent arwps spanning the substringwi, .
.
.
, wj´1 (p p ri, j ´ 1s).
term a1 and a2 inequation 3 cover the cases of the head-child as theleft child and the right child respectively..2.3 challenges of l-pcfg induction.
the major difference between l-pcfgs fromvanilla pcfgs is that they use word-annotated non-terminals, so the nonterminal number of l-pcfgs.
2689(b) the parameterization of zhu et al.
(2020): wqfigure 1: (a) the original parameterization of l-pcfgs.
is independent with b, d, a, wp given c. (c) our proposed parameterization.
we slightly abuse the bayesiannetwork notation by grouping variables.
in the standard notation, there would be arcs from the parent variables toeach grouped variable as well as arcs between the grouped variables..is up to |σ| times the number of nonterminals inpcfgs.
as the grammar size is largely determinedby the number of binary rules and increases ap-proximately in cubic of the nonterminal number,representing l-pcfgs has a high space complexityopm3|σ|2q (m is the nonterminal number).
specif-ically, it requires an order-6 probability tensor forbinary rules with each dimension representing a,b, c, wp, wq, and head direction d, respectively.
with so many rules, l-pcfgs are very prone to thedata sparsity problem in rule probability estimation.
collins (2003) suggests factorizing the binary ruleprobabilities according to speciﬁc independenceassumptions, but his approach still relies on com-plicated smoothing techniques to be effective..the addition of lexical heads also scales up thecomputational complexity of the inside algorithmby a factor opl2q and brings it up to opl5m3q.
eis-ner and satta (1999) point out that, by changingthe order of summations in term a1 (a2) of equa-tion 3, one can cache and reuse term b1 (b2) inequation 4 and reduce the computational complex-ity to opl4m2 ` l3m3q.
this is an example applica-tion of unfold-refold as noted by eisner and blatz(2007).
however, the complexity is still cubic in m,making it expensive to increase the total number ofnonterminals..2.4 neural l-pcfgs.
zhu et al.
(2020) apply neural parameterization totackle the data sparsity issue and to reduce the totallearnable parameters of l-pcfgs.
consideringthe head-child as the left child (similarly for theother case), they further factorize the binary ruleprobability as:.
pparwps ñ brwpscrwqsq.
“ ppb, ð, c|a, wpqppwq|cq ..(2).
bayesian networks representing the originalprobability and the factorization are illustrated in.
figure 1 (a) and (b).
with the factorized binary ruleprobability in equation 2, term a1 in equation 3can be rewritten as equation 5. zhu et al.
(2020)implement the inside algorithm by caching termc1-1 in equation 6, resulting in a time complexityopl4m3 ` l3mq, which is cubic in m. we note that,we can use unfold-refold to further cache term c1-2 in equation 6 and reduce the time complexityof the inside algorithm to opl4m2 ` l3m ` l2m2q,which is quadratic in m..although the factorization of equation 2 reducesthe space and time complexity of the inside algo-rithm of l-pcfg, it is based on the independenceassumption that the generation of wq is indepen-dent of a, b, d and wp given the non-head-childc. this assumption can be violated in many sce-narios and hence reduces the expressiveness of thegrammar.
for example, suppose c is noun, theneven if we know b is verb, we still need to knowd to determine if wq is an object or a subject ofthe verb, and then need to know the actual verb wpto pick a likely noun as wq..3 factorization with latent variable.
our main goal is to ﬁnd a parameterization that re-moves the implausible independence assumptionsof zhu et al.
(2020) while decreases the complexi-ties of the original l-pcfgs..to reduce the representation complexity, wedraw inspiration from the canonical polyadic de-composition (cpd).
cpd factorizes an n-th ordertensor into n two-dimensional matrices.
each ma-trix consists of two dimensions: one dimensioncomes from the original n-th order tensor and theother dimension is shared by all the n matrices.
theshared dimension can be marginalized to recoverthe original n-th order tensor.
from a probabilisticperspective, the shared dimension can be regardedas a latent-variable.
in the spirit of cpd, we intro-duce a latent-variable h to decompose the order-6.
2690j´1ÿ.
j´1ÿ.
ÿ.pÿ.
k´1ÿ.
ÿ.sa,pi,j “.
k,j ¨ pparwps ñ brwpscrwqsqloooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooonk“p`1.
i,k ¨ sc,qsb,p.
q“k.
b,c.
`.
k,j ¨ pparwps ñ brwqscrwpsqloooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooonk“i`1.
i,k ¨ sc,psb,q.
b,c.
q“i.
(3).
j´1ÿ.
ÿ.j´1ÿ.
ÿ.term a1.
pÿ.
ÿ.k´1ÿ.
ÿ.term a2.
“.
k“p`1.
b.sb,pi,k.
sc,qk,j ¨ pparwps ñ brwpscrwqsqlooooooooooooooooooooooooomooooooooooooooooooooooooonq“k.
c.`.
k“i`1.
c.sc,pk2,j.
sb,qi,k ¨ pparwps ñ brwqscrwpsqlooooooooooooooooooooooooomooooooooooooooooooooooooonq“i.
b.
(4).
term b2.
term b1.
j´1ÿ.
j´1ÿ.
ÿ.k“p`1.
q“k.
b,c.
term a1 “.
i,k ¨ sc,qsb,p.
k,j ¨ ppb, ð, c|a, wpq ¨ ppwq|cqlooooooooooooooooomooooooooooooooooon.
factorization of pparwpsñbrwpscrwq sq.
j´1ÿ.
ÿ.ÿ.j´1ÿ.
“.
k“p`1.
b.sb,pi,k.
ppb, ð, c|a, wpq.
sc,qk,j ¨ ppwq|cqloooooooooomoooooooooonq“klooooooooooooooooooooooooomooooooooooooooooooooooooon.
term c1-1.
c.term c1-2.
j´1ÿ.
j´1ÿ.
ÿ.ÿ.term a1 “.
k“p`1.
q“k.
b,c.
i,k ¨ sc,qsb,pk,j ¨.
pph|a, wpqppb|hqppc, ð |hqppwq|hqloooooooooooooooooooooooooooomoooooooooooooooooooooooooooonh.“.
pph|a, wpq.
ÿ.h.j´1ÿ.
ÿ.j´1ÿ.
ÿ.sb,pi,k ppb|hqloooooooomoooooooonb.sc,qk,j ppc ð |hqppwq|hqlooooooooooooooooooomooooooooooooooooooonq“k.
c.k“p`1.
factorization of pparwpsñbrwpscrwq sq.
term d1-1.
term d1-2.
table 1: recursive formulas of the inside algorithm for eisner and satta (1999) (equation 4), zhu et al.
(2020)(equation 5- 6), and our formalism (equation 7- 8), respectively.
sa,pindicates the probability of a head nontermi-i,jnal arwps spanning the substring wi, .
.
.
, wj´1, where p is the position of the headword in the sentence..probability tensor ppb, c, d, wq|a, wpq.
insteadof fully decomposing the tensor, we empiricallyﬁnd that binding some of the variables leads to bet-ter results.
our best factorization is as follows (alsoillustrated by a bayesian network in figure 1 (c)):.
rule probability is factorized as.
pparwps ñ brwpscrwqsq “ÿ.pph|a, wpqppb|hqppc ð |hqppwq|hq ,.
h.and.
ppb, c, wq, d|a, wpq “ÿ.pph|a, wpqppb|hqppc, d|hqppwq|hq ..h.h.pparwps ñ brwqscrwpsq “ÿ.
(9).
pph|a, wpqppc|hqppb ñ |hqppwq|hq ..according to d-separation (pearl, 1988), whena and wp are given, b, c, wq, and d are interde-pendent due to the existence of h. in other words,our factorization does not make any independenceassumption beyond the original binary rule.
thedomain size of h is analogous to the tensor rankin cpd and thus inﬂuences the expressiveness ofour proposed model..based on our factorization approach, the binary.
we also follow zhu et al.
(2020) and factorize thestart rule as follows..pps ñ arwpsq “ ppa|sqppwp|aq ..(12).
computational complexity: considering thehead-child as the left child (similarly for the othercase), we apply equation 10 in term a1 of equa-tion 3 and obtain equation 7. rearranging thesummations in equation 7 gives equation 8, whereterm d1-1 and d1-2 can be cached and reused,which also uses the unfold-refold technique.
the ﬁ-nal time complexity of the inside computation with.
(5).
(6).
(7).
(8).
(10).
(11).
2691our factorization approach is opl4dh ` l2mdh q(dh is the domain size of the latent variable h),which is linear in m..choices of factorization:if we follow the intu-ition of cpd, then we shall assume that b, c, d,and wq are all independent conditioned on h. how-ever, properly relaxing this strong assumption bybinding some variables could beneﬁt our model.
though there are many different choices of bindingthe variables, some bindings can be easily ruledout.
for instance, binding b and c inhibits usfrom caching term d1-1 and term d1-2 in equa-tion 7 and thus we cannot implement the insidealgorithm efﬁciently; binding c and wq leads toa high computational complexity because we willhave to compute a high-dimensional (m|σ|) cate-gorical distribution.
in section 6.3, we make anablation study on the impact of different choices offactorizations..neural parameterizations: we follow kimet al.
(2019) and zhu et al.
(2020) and deﬁne thefollowing neural parameterization:.
,.
,.
,.
,.
ř.ř.ř.ř.ř.h wb1q.
ppa|sq “.
ppw|aq “.
ppw|hq “.
ppb|hq “.
s f1pwaqq.
af2pwwqq.
h f2pwwqq.
s f1pwa1qq.
h f2pww1qq.
ppc ð |hq “.
exppuja1pn exppujexppujw1pς exppujaf2pww1qqexppujh wbqb1pn yp exppujexppujw1pς exppujexppujc1pm exppujexppujc1pm exppujexppujh 1ph exppujwhere h “ th1, .
.
.
, hdh u, m “ pn y pq ˆ tð, ñu, u and w are nonterminal embeddings andword embeddings respectively, and f1p¨q, f2p¨q,f3p¨q, f4p¨q are neural networks with residual lay-ers (he et al., 2016) (full parameterization is shownin appendix.)..
h wc1qh f4prwa; wwsqq.
h 1f4prwa; wwsqq.
ppc ñ |hq “.
pph|a, wq “.
h wcðq.
h wcñq.
h wc1q.
ř.ř.,.
,.
,.
4 experimental setup.
4.1 dataset.
we conduct experiments on the wall street journal(wsj) corpus of the penn treebank (marcus et al.,.
1994).
we use the same preprocessing pipeline asin kim et al.
(2019).
speciﬁcally, punctuation isremoved from all data splits and the top 10,000frequent words in the training data are used as thevocabulary.
for dependency grammar induction,we follow (zhu et al., 2020) to use the stanfordtyped dependency representation (de marneffe andmanning, 2008)..4.2 hyperparameters.
we optimize our model using the adam optimizerwith β1 “ 0.75, β2 “ 0.999, and learning rate0.001. all parameters are initialized with xavieruniform initialization.
we set the dimension of allembeddings to 256 and the ratio of the nonterminalnumber to the preterminal number to 1:2. ourbest model uses 15 nonterminals, 30 preterminals,and dh “ 300. we use grid search to tune thenonterminal number (from 5 to 30) and domainsize dh of the latent h (from 50 to 500)..4.3 evaluation.
we run each model four times with different ran-dom seeds and for ten epochs.
we train our modelson training sentences of length ď 40 with batchsize 8 and test them on the whole testing set.
foreach run, we perform early stopping and select thebest model according to the perplexity of the devel-opment set.
we use two different parsing methods:the variant of cyk algorithm (eisner and satta,1999) and minimum bayes-risk (mbr) decoding(smith and eisner, 2006).
2 for constituent gram-mar induction, we report the means and standarddeviations of sentence-level f1 scores.3 for de-pendency grammar induction, we report unlabeleddirected attachment score (udas) and unlabeledundirected attachment score (uuas)..5 main result.
we present our main results in table 2. our modelis referred to as neural bi-lexicalized pcfgs(nbl-pcfgs).
we mainly compare our approachagainst recent pcfg-based models: neural pcfg(n-pcfg) and compound pcfg (c-pcfg) (kimet al., 2019), tensor decomposition based neural.
2in mbr decoding, we use automatic differentiation (eis-ner, 2016; rush, 2020) to estimate the marginals of spansand arcs, and then use the cyk and eisner algorithms forconstituency and dependency parsing, respectively..3following kim et al.
(2019), we remove all trivial spans(single-word spans and sentence-level spans).
sentence-levelmeans that we compute f1 for each sentence and then averageover all sentences..2692pcfg (tn-pcfg) (yang et al., 2021) and neurall-pcfg (nl-pcfg) (zhu et al., 2020).
we reportboth ofﬁcial result of zhu et al.
(2020) and ourreimplementation..we do not use the compound trick (kim et al.,2019) in our implementations of lexicalized pcfgsbecause we empirically ﬁnd that using it results inunstable training and does not necessarily bringperformance improvements..we draw three key observations: (1) our modelachieves the best f1 and uuas scores under bothcyk and mbr decoding.
it is also comparableto the ofﬁcial nl-pcfg in the udas score.
(2)when we remove the compound parameterizationfrom nl-pcfg, its f1 score drops slightly while itsudas and uuas scores drop dramatically.
it im-plies that compound parameterization is the key toachieve excellent dependency grammar inductionperformance in nl-pcfg.
(3) the mbr decodingoutperforms cyk decoding..regarding udas, our model signiﬁcantly out-performs nl-pcfgs in udass if compound pa-rameterization is not used (37.1 vs. 23.8 with cykdecoding), showing that explicitly modeling bilexi-cal relationship is helpful in dependency grammarinduction.
however, when compound parameteri-zation is used, the udas of nl-pcfgs is greatlyimproved, slightly surpassing that of our model.
we believe this is because compound parameteriza-tion greatly weakens the independence assumptionof nl-pcfgs (i.e., the child word is dependent onc only) by leaking bilexical information via theglobal sentence embedding.
on the other hand,nbl-pcfgs are already expressive enough andthus compound parameterization brings no furtherincrease of their expressiveness but makes learningmore difﬁcult..6 analysis.
in the following experiments, we report resultsusing mbr decoding by default.
we also usedh “ 300 by default unless otherwise speciﬁed..6.1.inﬂuence of the domain size of h.dh (the domain size of h) inﬂuences the expres-siveness of our model.
figure 2a illustrates per-plexities and f1 scores with the increase of dh anda ﬁxed nonterminal number of 10 (plots of udasand uuas can be found in appendix).
we cansee that when dh is small, the model has a highperplexity and a low f1 score, indicating the lim-.
model.
wsj.
f1.
udas.
uuas.
ofﬁcial results.
n-pcfg‹c-pcfg‹nl-pcfg‹tn-pcfg:.
50.855.255.357.7.
39.7.
53.3.our results.
nl-pcfg‹nl-pcfg:nbl-pcfg‹nbl-pcfg:.
53.3˘2.157.4˘1.458.2˘1.560.4˘1.6.
23.8˘1.125.3˘1.337.1˘2.839.1˘2.8.
47.4˘1.047.2˘0.754.6˘1.356.1˘1.3.
for reference.
s-diorastructformer.
57.654.0.oracle trees.
84.3.
46.2.
61.6.table 2: unlabeled sentence-level f1 scores, unlabeleddirected attachment scores and unlabeled undirected: indicatesattachment scores on the wsj test data.
using mbr decoding.
‹ indicates using cyk decod-ing.
recall that the ofﬁcial result of zhu et al.
(2020)uses compound parameterization while our reimple-mentation removes the compound parameterization.
s-diora: drozdov et al.
(2020).
structformer: shenet al.
(2020)..ited expressiveness of nbl-pcfgs.
when dh islarger than 300, the perplexity becomes plateauedand the f1 score starts to decrease possibly becauseof overﬁtting..6.2.inﬂuence of nonterminal number.
figure 2b illustrates perplexities and f1 scores withthe increase of the nonterminal number and ﬁxeddh “ 300 (plots of udas and uuas can be foundin appendix).
we observe that increasing the non-terminal number has only a minor inﬂuence onnbl-pcfgs.
we speculate that it is because thenumber of word-annotated nonterminals (m|σ|) isalready sufﬁciently large even if m is small.
onthe other hand, the nonterminal number has a biginﬂuence on nl-pcfgs.
this is most likely be-cause nl-pcfgs make the independence assump-tion that the generation of wq is solely determinedby the non-head-child c and thus require morenonterminals so that c has the capacity of convey-ing information from a, b, d and wp.
using morenonterminals (ą 30) seems to be helpful for nl-.
2693f1.
udas uuas perplexity.
60.4.d-c39.1d-alone 57.2 32.8d-wq45.747.7d-b47.8 36.9.
56.154.158.654.0.
161.9164.8176.8169.6.table 3: binding the head direction d with differentvariables..pcfgs, but would be computationally too expen-sive due to the quadratically increased complexityin the number of nonterminals..6.3.inﬂuence of different variable bindings.
table 3 presents the results of our models with thefollowing bindings:.
• d-alone: d is generated alone..• d-wq: d is generated with wq..• d-b: d is generated with head-child b..• d-c: d is generated with non-head-child c..clearly, binding d and c (the default settingfor nbl-pcfg) results in the lowest perplexityand the highest f1 score.
binding d and wq hasa surprisingly good performance in unsuperviseddependency parsing..we ﬁnd that how to bind the head direction hasa huge impact on the unsupervised parsing perfor-mance and we give the following intuition.
usuallygiven a headword and its type, the children gener-ated in each direction would be different.
so, d isintuitively more related to wq and c than to b. onthe other hand, b is dependent more on the head-word instead.
in table 3 we can see that (d-b)has a lower udas score than (d-c) and (d-wq),which is consistent with this intuition.
notably, inzhu et al.
(2020), their factorization iii has a signif-icantly lower udas than the default model (35.5vs. 25.9), and the only difference is whether thegeneration of c is dependent on the head direction.
this is also consistent with our intuition..6.4 qualitative analysis.
we analyze the parsing performance of differentpcfg extensions by breaking down their recallnumbers by constituent labels (see table 4).
npsand vps cover most of the gold constituents in wsjtest set.
tn-pcfgs have the best performancein predicting nps and nbl-pcfgs have betterperformance in predicting other labels on average..we further analyze the quality of our inducedtrees.
our model prefers to predict left-headed con-stituents (i.e., constituents headed by the leftmostword).
vps are usually left-headed in english, soour model has a much higher recall on vps and cor-rectly predicts their headwords.
sbars often startwith which and that and pps often start with prepo-sitions such as of and for.
our model often relieson these words to predict the correct constituentsand hence erroneously predicts these words as theheadwords, which hurts the dependency accuracy.
for nps, we ﬁnd our model often makes mistakesin predicting adjective-noun phrases.
for example,the correct parse of a rough market is (a (roughmarket)), but our model predicts ((a rough) market)instead..7 discussion on dependency annotation.
schemes.
what should be regarded as the headwords is stilldebatable in linguistics, especially for those aroundfunction words (zwicky, 1993).
for example, inphrase the company, some linguists argue that theshould be the headword (abney, 1972).
thesedisagreements are reﬂected in the dependency an-notation schemes.
researchers have found thatdifferent dependency annotation schemes result invery different evaluation scores of unsupervised de-pendency parsing (noji, 2016; shen et al., 2020).
in our experiments, we use the stanford depen-dencies annotation scheme in order to comparewith nl-pcfgs.
stanford dependencies prefersto select content words as headwords.
however,as we discussed in previous sections, our modelprefers to select function words (e.g., of, which,for) as headwords for sbars or pps.this explainswhy our model can outperform all the baselines onconstituency parsing but not on dependency pars-ing (as judged by stanford dependencies) at thesame time.
table 3 shows that there is a trade-offbetween the f1 score and udas, which suggeststhat adapting our model to stanford dependencieswould hurt its ability to identify constituents..8 speed comparison.
in practice, the forward and backward pass of theinside algorithm consumes the majority of the run-ning time in training a n(b)l-pcfg.
the existingimplementation by zhu et al.
(2020)4 does not em-ploy efﬁcient parallization and has a cubic time.
4https://github.com/neulab/neural-lpcfg.
2694(a).
(b).
figure 2: the change of f1 scores, perplexities with the change of |h| and nonterminal number..n-pcfg: c-pcfg: tn-pcfg: nl-pcfg nbl-pcfg.
npvpppsbaradjpadvp.
72.3%28.1%73.0%53.6%40.8%43.8%.
perplexity.
254.3.
73.6%45.0%71.4%54.8%44.3%61.6%.
196.3.
75.4%48.4%67.0%50.3%53.6%59.5%.
207.3.
74.0%44.3%68.4%49.4%55.5%57.1%.
181.2.
66.2%61.1%77.7%63.8%59.7%59.1%.
161.9.table 4: recall on six frequent constituent labels andperplexities of the wsj test data.
: means that the re-sults are reported by yang et al.
(2021).
complexity in the number of nonterminals.
weprovide an efﬁcient reimplementation (we followzhang et al.
(2020) to batchify) of the inside algo-rithm based on equation 6. we refer to an imple-mentation which caches term c1-1 as re-impl-1and refer to an implementation which caches termc1-2 as re-impl-2..(a).
(b).
figure 3: total time in performing the inside algorithmand automatic differentiation with different sentencelengths and nonterminal numbers..we measure the time based on a single forwardand backward pass of the inside algorithm withbatch size 1 on a single titan v gpu.
figure 3aillustrates the time with the increase of the sentencelength and a ﬁxed nonterminal number of 10. theoriginal implementation of nl-pcfg by zhu et al.
(2020) takes much more time when sentences arelong.
for example, when sentence length is 40, itneeds 6.80s, while our fast implementation takes0.43s and our nbl-pcfg takes only 0.30s.
figure3b illustrates the time with the increase of the non-.
terminal number m and a ﬁxed sentence length of30. the original implementation runs out of 12gbmemory when m “ 30. re-impl-2 is faster thanre-impl-1 when increasing m as it has a better timecomplexity in m (quadratic for re-impl-2, cubic forre-impl-1).
our nbl-pcfgs have a linear com-plexity in m, and as we can see in the ﬁgure, ournbl-pcfgs are much faster when m is large..9 related work.
unsupervised parsing has a long history but hasregained great attention in recent years.
in unsu-pervised dependency parsing, most methods arebased on dependency model with valence (dmv)(klein and manning, 2004).
neurally parameter-ized dmvs have obtained state-of-the-art perfor-mance (jiang et al., 2016; han et al., 2017, 2019;yang et al., 2020).
however, they rely on gold postags and sophisticated initializations (e.g.
k&minitialization or initialization with the parsing resultof another unsupervised model).
noji et al.
(2016)propose a left-corner parsing-based dmv model tolimit the stack depth of center-embedding, which isinsensitive to initialization but needs gold pos tags.
he et al.
(2018) propose a latent-variable baseddmv model, which does not need gold pos tagsbut requires good initialization and high-qualityinduced pos tags.
see han et al.
(2020) for asurvey of unsupervised dependency parsing.
com-pared to these methods, our method does not re-quire gold/induced pos tags or sophisticated ini-tializations, though its performance lags behindsome of these previous methods..recent unsupervised constituency parsers canbe roughly categorized into the following groups:(1) pcfg-based methods.
depth-bounded pcfgs(jin et al., 2018a,b) limit the stack depth of center-embedding.
neurally parameterized pcfgs (jin.
2695100200300400500size of |h|525456586062f1 (%)160170180190200210perplexityf1perplexity51015202530the number of nonterminals455055606570f1 (%)160180200220240260280perplexityf1 of nl-pcfgsf1 of nbl-pcfgsperplexity of nl-pcfgsperplexity of nbl-pcfgs10203040506070sentence length0246810121416time: (s)nbl-pcfgnl-pcfgre-impl-1re-impl-210203040506070the number of nonterminals0.51.01.52.02.53.03.5time: (s)nbl-pcfgnl-pcfgre-impl-1re-impl-2et al., 2019; kim et al., 2019; zhu et al., 2020; yanget al., 2021) use neural networks to produce gram-mar rule probabilities.
(2) deep inside-outsiderecursive auto-encoder (diora) based methods(drozdov et al., 2019a,b, 2020; hong et al., 2020;sahay et al., 2021).
they use neural networks tomimic the inside-outside algorithm and they aretrained with masked language model objectives.
(3) syntactic distance-based methods (shen et al.,2018, 2019, 2020).
they encode hidden syntac-tic trees into syntactic distances and inject theminto language models.
(4) probing based methods(kim et al., 2020; li et al., 2020).
they extractphrase-structure trees based on the attention distri-butions of large pre-trained language models.
inaddition to these methods, cao et al.
(2020) useconstituency tests and shi et al.
(2021) make useof naturally-occurring bracketings such as hyper-links on webpages to train parsers.
multimodalinformation such as images (shi et al., 2019; zhaoand titov, 2020; jin and schuler, 2020) and videos(zhang et al., 2021) have also been exploited forunsupervised constituency parsing..we are only aware of a few previous studies inunsupervised joint dependency and constituencyparsing.
klein and manning (2004) propose a jointdmv and ccm (klein and manning, 2002) model.
shen et al.
(2020) propose a transformer-basedmethod, in which they deﬁne syntactic distances toguild attentions of transformers.
zhu et al.
(2020)propose neural l-pcfgs for unsupervised jointparsing..10 conclusion.
we have presented a new formalism of lexicalizedpcfgs.
our formalism relies on the canonicalpolyadic decomposition to factorize the probabil-ity tensor of binary rules.
the factorization re-duces the space and time complexity of lexicalizedpcfgs while keeping the independence assump-tions encoded in the original binary rules intact.
we further parameterize our model by using neu-ral networks and present an efﬁcient implementa-tion of our model.
on the english wsj test data,our model achieves the lowest perplexity, outper-forms all the existing extensions of pcfgs in con-stituency grammar induction, and is comparable tostrong baselines in dependency grammar induction..acknowledgments.
we thank the anonymous reviewers for their con-structive comments.
this work was supported bythe national natural science foundation of china(61976139)..references.
steven p. abney.
1972. the english noun phrase in its.
sentential aspect..steven cao, nikita kitaev, and dan klein.
2020. unsu-pervised parsing via constituency tests.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages4798–4808, online.
association for computationallinguistics..eugene charniak.
1996. tree-bank grammars.
techni-.
cal report, usa..justin chiu and alexander rush.
2020. scaling hid-den markov language models.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 1341–1349,online.
association for computational linguistics..shay b. cohen, giorgio satta, and michael collins.
2013. approximate pcfg parsing using tensor de-in proceedings of the 2013 confer-composition.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 487–496, atlanta, geor-gia.
association for computational linguistics..michael collins.
1997. three generative, lexicalisedmodels for statistical parsing.
in 35th annual meet-ing of the association for computational linguis-tics and 8th conference of the european chapterof the association for computational linguistics,pages 16–23, madrid, spain.
association for com-putational linguistics..michael collins.
2003. head-driven statistical modelsfor natural language parsing.
computational lin-guistics, 29(4):589–637..andrew drozdov, subendhu rongali, yi-pei chen,tim o’gorman, mohit iyyer, and andrew mccal-lum.
2020. unsupervised parsing with s-diora:single tree encoding for deep inside-outside recur-sive autoencoders.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 4832–4845, online.
as-sociation for computational linguistics..andrew drozdov, patrick verga, mohit yadav, mo-hit iyyer, and andrew mccallum.
2019a.
unsuper-vised latent tree induction with deep inside-outsiderecursive auto-encoders.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and short.
2696papers), pages 1129–1141, minneapolis, minnesota.
association for computational linguistics..andrew drozdov, patrick verga, mohit yadav, mohitiyyer, and andrew mccallum.
2019b.
unsuper-vised latent tree induction with deep inside-outsiderecursive auto-encoders.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1129–1141, minneapolis, minnesota.
association for computational linguistics..jason eisner.
2016..inside-outside and forward-backward algorithms are just backprop (tutorial pa-per).
in proceedings of the workshop on structuredprediction for nlp, pages 1–17, austin, tx.
asso-ciation for computational linguistics..jason eisner and john blatz.
2007. program transfor-mations for optimization of parsing algorithms andin proceedings ofother weighted logic programs.
fg 2006: the 11th conference on formal gram-mar, pages 45–85.
csli publications..jason eisner and giorgio satta.
1999. efﬁcient pars-ing for bilexical context-free grammars and head au-tomaton grammars.
in proceedings of the 37th an-nual meeting of the association for computationallinguistics, pages 457–464, college park, maryland,usa.
association for computational linguistics..wenjuan han, yong jiang, hwee tou ng, and keweitu.
2020. a survey of unsupervised dependencyin proceedings of the 28th internationalparsing.
conference on computational linguistics, pages2522–2533, barcelona, spain (online).
interna-tional committee on computational linguistics..wenjuan han, yong jiang, and kewei tu.
2017. de-pendency grammar induction with neural lexicaliza-in proceedings of thetion and big training data.
2017 conference on empirical methods in natu-ral language processing, pages 1683–1688, copen-hagen, denmark.
association for computationallinguistics..wenjuan han, yong jiang, and kewei tu.
2019. en-hancing unsupervised generative dependency parserwith contextual information.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5315–5325, florence,italy.
association for computational linguistics..sion and pattern recognition, cvpr 2016, las ve-gas, nv, usa, june 27-30, 2016, pages 770–778.
ieee computer society..ruyue hong, jiong cai, and kewei tu.
2020. deepinside-outside recursive autoencoder with all-spanthe 28th interna-objective.
tional conference on computational linguistics,pages 3610–3615, barcelona, spain (online).
inter-national committee on computational linguistics..in proceedings of.
f. jelinek, j. d. lafferty, and r. l. mercer.
1992. basicmethods of probabilistic context free grammars.
inspeech recognition and understanding, pages 345–360, berlin, heidelberg.
springer berlin heidelberg..yong jiang, wenjuan han, and kewei tu.
2016. un-supervised neural dependency parsing.
in proceed-ings of the 2016 conference on empirical methodsin natural language processing, pages 763–771,austin, texas.
association for computational lin-guistics..lifeng jin, finale doshi-velez, timothy miller,william schuler, and lane schwartz.
2018a.
depth-bounding is effective: improvements and evaluationof unsupervised pcfg induction.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 2721–2731, brus-sels, belgium.
association for computational lin-guistics..lifeng jin, finale doshi-velez, timothy miller,william schuler, and lane schwartz.
2018b.
un-supervised grammar induction with depth-boundedpcfg.
transactions of the association for compu-tational linguistics, 6:211–224..lifeng jin, finale doshi-velez, timothy miller, laneschwartz, and william schuler.
2019. unsuper-vised learning of pcfgs with normalizing ﬂow.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2442–2452, florence, italy.
association for compu-tational linguistics..lifeng jin and william schuler.
2020. groundedin proceedings ofpcfg induction with images.
the 1st conference of the asia-paciﬁc chapter of theassociation for computational linguistics and the10th international joint conference on natural lan-guage processing, pages 396–408, suzhou, china.
association for computational linguistics..junxian he, graham neubig,.
and taylor berg-kirkpatrick.
2018. unsupervised learning of syn-tactic structure with invertible neural projections.
the 2018 conference on em-in proceedings ofpirical methods in natural language processing,pages 1292–1302, brussels, belgium.
associationfor computational linguistics..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in 2016 ieee conference on computer vi-.
mark johnson.
1998.tree representations.
24(4):613–632..pcfg models of linguisticcomputational linguistics,.
taeuk kim, jihun choi, daniel edmiston, and sang-goo lee.
2020. are pre-trained language modelsaware of phrases?
simple but strong baselines forin 8th international confer-grammar induction.
ence on learning representations, iclr 2020, ad-dis ababa, ethiopia, april 26-30, 2020. openre-view.net..2697yoon kim, chris dyer, and alexander rush.
2019.compound probabilistic context-free grammars forgrammar induction.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 2369–2385, florence, italy.
asso-ciation for computational linguistics..dan klein and christopher manning.
2004. corpus-based induction of syntactic structure: models of de-in proceedings of thependency and constituency.
42nd annual meeting of the association for com-putational linguistics (acl-04), pages 478–485,barcelona, spain..dan klein and christopher d. manning.
2002. agenerative constituent-context model for improvedgrammar induction.
in proceedings of the 40th an-nual meeting of the association for computationallinguistics, pages 128–135, philadelphia, pennsyl-vania, usa.
association for computational linguis-tics..dan klein and christopher d. manning.
2003. ac-curate unlexicalized parsing.
in proceedings of the41st annual meeting of the association for compu-tational linguistics, pages 423–430, sapporo, japan.
association for computational linguistics..parser evaluation, pages 1–8, manchester, uk.
col-ing 2008 organizing committee..hiroshi noji.
2016. left-corner methods for syntac-tic modeling with universal structural constraints.
corr, abs/1608.00293..hiroshi noji, yusuke miyao, and mark johnson.
2016.using left-corner parsing to encode universal struc-tural constraints in grammar induction.
in proceed-ings of the 2016 conference on empirical meth-ods in natural language processing, pages 33–43,austin, texas.
association for computational lin-guistics..judea pearl.
1988. probabilistic reasoning in intelli-gent systems: networks of plausible inference.
mor-gan kaufmann publishers inc., san francisco, ca,usa..alexander rush.
2020. torch-struct: deep structuredin proceedings of the 58th an-prediction library.
nual meeting of the association for computationallinguistics: system demonstrations, pages 335–342, online.
association for computational linguis-tics..tamara g. kolda and brett w. bader.
2009. ten-sor decompositions and applications.
siam review,51(3):455–500..atul sahay, anshul nasery, ayush maheshwari,ganesh ramakrishnan, and rishabh iyer.
2021.rule augmented unsupervised constituency parsing..karim lari and steve j young.
1990. the estimationof stochastic context-free grammars using the inside-outside algorithm.
computer speech & language,4(1):35–56..bowen li, taeuk kim, reinald kim amplayo, andfrank keller.
2020. heads-up!
unsupervised con-in pro-stituency parsing via self-attention heads.
ceedings of the 1st conference of the asia-paciﬁcchapter of the association for computational lin-guistics and the 10th international joint conferenceon natural language processing, pages 409–424,suzhou, china.
association for computational lin-guistics..david m. magerman.
1995. statistical decision-treemodels for parsing.
in 33rd annual meeting of theassociation for computational linguistics, pages276–283, cambridge, massachusetts, usa.
associ-ation for computational linguistics..mitchell marcus,.
grace kim, mary annmarcinkiewicz, robert macintyre, ann bies,mark ferguson, karen katz, and britta schasberger.
1994. the penn treebank: annotating predicate ar-gument structure.
in human language technology:proceedings of a workshop held at plainsboro, newjersey, march 8-11, 1994..marie-catherine de marneffe and christopher d. man-ning.
2008. the stanford typed dependencies rep-in coling 2008: proceedings of theresentation.
workshop on cross-framework and cross-domain.
yikang shen, zhouhan lin, chin-wei huang, andaaron c. courville.
2018. neural language model-in 6thing by jointly learning syntax and lexicon.
international conference on learning representa-tions, iclr 2018, vancouver, bc, canada, april 30- may 3, 2018, conference track proceedings.
open-review.net..yikang shen, shawn tan, seyedarian hosseini,zhouhan lin, alessandro sordoni, and aaron c.corr,courville.
2019.abs/1910.13466..ordered memory..yikang shen, yi tay, che zheng, dara bahri, donaldmetzler, and aaron courville.
2020. structformer:joint unsupervised induction of dependency and con-stituency structure from masked language modeling..haoyue shi, jiayuan mao, kevin gimpel, and karenlivescu.
2019. visually grounded neural syntax ac-quisition.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 1842–1861, florence, italy.
association forcomputational linguistics..tianze shi, ozan ˙irsoy, igor malioutov, and lillian lee.
2021. learning syntax from naturally-occurringin proceedings of the 2021 confer-bracketings.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, pages 2941–2949, online.
as-sociation for computational linguistics..2698david a. smith and jason eisner.
2006. minimum riskannealing for training log-linear models.
in proceed-ings of the coling/acl 2006 main conferenceposter sessions, pages 787–794, sydney, australia.
association for computational linguistics..songlin yang, yong jiang, wenjuan han, and keweitu.
2020. second-order unsupervised neural depen-in proceedings of the 28th inter-dency parsing.
national conference on computational linguistics,pages 3911–3924, barcelona, spain (online).
inter-national committee on computational linguistics..songlin yang, yanpeng zhao, and kewei tu.
2021.pcfgs can do better: inducing probabilistic context-free grammars with many symbols.
in proceedingsof the 2021 conference of the north american chap-ter of the association for computational linguistics:human language technologies, pages 1487–1498,online.
association for computational linguistics..songyang zhang, linfeng song, lifeng jin, kun xu,dong yu, and jiebo luo.
2021. video-aided unsu-pervised grammar induction.
in proceedings of the2021 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 1513–1524, on-line.
association for computational linguistics..yu zhang, houquan zhou, and zhenghua li.
2020.fast and accurate neural crf constituency parsing.
in proceedings of the twenty-ninth internationaljoint conference on artiﬁcial intelligence, ijcai2020, pages 4046–4053.
ijcai.org..yanpeng zhao and ivan titov.
2020..visuallygrounded compound pcfgs.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 4369–4379,online.
association for computational linguistics..hao zhu, yonatan bisk, and graham neubig.
2020.the return of lexical dependencies: neural lexical-ized pcfgs.
transactions of the association forcomputational linguistics, 8:647–661..a. zwicky.
1993. heads in grammatical theory: heads,.
bases and functors..a full parameterization.
we give the full parameterizations of the followingprobability distributions..ppa|sq “.
ř.ppw|aq “.
ř.ppw|hq “.
ř.pph|a, wq “.
ř.,.
,.
,.
s h1pwaqq.
exppuja1pn exppujexppujw1pς exppujexppujw1pς exppujexppujh 1ph exppuj.
s h1pwa1qq.
ah2pwwqq.
ah2pww1qq.
h h3pwwqq.
h h3pww1qqh f prwa; wwsqq.
h1f prwa; wwsqq.
,.
figure 4: inﬂuence of dh on uuas and udas..figure 5: inﬂuence of the number of nonterminals onuuas and udas..hipxq “ gi,1 pgi,2 pwixqqgi,jpyq “ relu pvi,j relu pui,jyqq ` y.f prx, ysq “ h4prelupwrx; ysq ` yq.
b inﬂuence of the domain size of h and.
the number of nonterminals.
figure 4 illustrates the change of uuas and udaswith the increase of dh .
we ﬁnd similar tendenciescompared to the change of f1 scores and perplexi-ties with the increase of dh .
dh “ 300 performsbest.
figure 5 illustrates the change of uuas andudas when increasing the number of nontermi-nals.
we can see that nl-pcfgs beneﬁt from usingmore nonterminals while nbl-pcfgs have a bet-ter performance when the number of nonterminalsis relatively small..2699100200300400500size of |h|3032343638404244udas (%)515253545556575859uuas (%)udasuuas51015202530the numnber of nonterminals152025303540udas (%)424446485052545658uuas (%)udas of nl-pcfgsudas of nbl-pcfgsuuas of nl-pcfgsuuas of nbl-pcfgs