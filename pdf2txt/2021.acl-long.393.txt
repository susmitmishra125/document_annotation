consert: a contrastive framework for self-supervised sentencerepresentation transfer.
yuanmeng yan1∗, rumei li2∗, sirui wang2, fuzheng zhang2, wei wu2, weiran xu11beijing university of posts and telecommunications, beijing, china2meituan inc., beijing, china{yanyuanmeng,xuweiran}@bupt.edu.cn{lirumei,wangsirui,zhangfuzheng,wuwei30}@meituan.com.
abstract.
learning high-quality sentence representa-tions beneﬁts a wide range of natural languageprocessing tasks.
though bert-based pre-trained language models achieve high perfor-mance on many downstream tasks, the nativederived sentence representations are proved tobe collapsed and thus produce a poor perfor-mance on the semantic textual similarity (sts)tasks.
in this paper, we present consert,a contrastive framework for self-supervisedsentence representation transfer, that adoptscontrastive learning to ﬁne-tune bert in anunsupervised and effective way.
by makinguse of unlabeled texts, consert solves thecollapse issue of bert-derived sentence rep-resentations and make them more applicablefor downstream tasks.
experiments on stsdatasets demonstrate that consert achievesan 8% relative improvement over the previousstate-of-the-art, even comparable to the super-vised sbert-nli.
and when further incorpo-rating nli supervision, we achieve new state-of-the-art performance on sts tasks.
more-over, consert obtains comparable resultswith only 1000 samples available, showing itsrobustness in data scarcity scenarios..1.introduction.
sentence representation learning plays a vital rolein natural language processing tasks (kiros et al.,2015; hill et al., 2016; conneau et al., 2017; ceret al., 2018).
good sentence representations beneﬁta wide range of downstream tasks, especially forcomputationally expensive ones, including large-scale semantic similarity comparison and informa-tion retrieval..recently, bert-based pre-trained languagemodels have achieved high performance on many.
∗work done during internship at meituan inc. the ﬁrsttwo authors contribute equally.
weiran xu is the correspond-ing author..figure 1: the correlation diagram between the goldsimilarity score (x-axis) and the model predicted cosinesimilarity score (y-axis) on the sts benchmark dataset..downstream tasks with additional supervision.
however, the native sentence representations de-rived from bert1 are proved to be of low-quality(reimers and gurevych, 2019; li et al., 2020).
asshown in figure 1a, when directly adopt bert-based sentence representations to semantic textualsimilarity (sts) tasks, almost all pairs of sentencesachieved a similarity score between 0.6 to 1.0 ,even if some pairs are regarded as completely unre-lated by the human annotators.
in other words, thebert-derived native sentence representations aresomehow collapsed (chen and he, 2020), whichmeans almost all sentences are mapped into a smallarea and therefore produce high similarity..such phenomenon is also observed in severalprevious works (gao et al., 2019; wang et al., 2019;li et al., 2020).
they ﬁnd the word representationspace of bert is anisotropic, the high-frequencywords are clustered and close to the origin, whilelow-frequency words disperse sparsely.
when av-eraging token embeddings, those high-frequencywords dominate the sentence representations, in-ducing biases against their real semantics 2. as a.
1typically, we take the output of the [cls] token or av-erage token embeddings at the last few layers as the sentencerepresentations..2we also empirically prove this hypothesis, please refer.
to section 5.1 for more details..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5065–5075august1–6,2021.©2021associationforcomputationallinguistics5065012345the gold similarity score0.20.00.20.40.60.81.0the predicted cosine similaritya) bert derived sentence representation012345the gold similarity score0.20.00.20.40.60.81.0the predicted cosine similarityb) after applying our approachresult, it is inappropriate to directly apply bert’snative sentence representations for semantic match-ing or text retrieval.
traditional methods usuallyﬁne-tune bert with additional supervision.
how-ever, human annotation is costly and often unavail-able in real-world scenarios..to alleviate the collapse issue of bert as wellas reduce the requirement for labeled data, we pro-pose a novel sentence-level training objective basedon contrastive learning (he et al., 2020; chen et al.,2020a,b).
by encouraging two augmented viewsfrom the same sentence to be closer while keep-ing views from other sentences away, we reshapethe bert-derived sentence representation spaceand successfully solve the collapse issue (shownin figure 1b).
moreover, we propose multiple dataaugmentation strategies for contrastive learning, in-cluding adversarial attack (goodfellow et al., 2014;kurakin et al., 2016), token shufﬂing, cutoff (shenet al., 2020) and dropout (hinton et al., 2012),that effectively transfer the sentence representa-tions to downstream tasks.
we name our approachconsert, a contrastive framework for sentencerepresentation transfer..consert has several advantages over previousapproaches.
firstly, it introduces no extra struc-ture or specialized implementation during infer-ence.
the parameter size of consert keeps thesame as bert, making it easy to use.
secondly,compared with pre-training approaches, consertis more efﬁcient.
with only 1,000 unlabeled textsdrawn from the target distribution (which is easyto collect in real-world applications), we achieve35% relative performance gain over bert, and thetraining stage takes only a few minutes (1-2k steps)on a single v100 gpu.
finally, it includes severaleffective and convenient data augmentation meth-ods with minimal semantic impact.
their effectsare validated and analyzed in the ablation studies..our contributions can be summarized as follows:1) we propose a simple but effective sentence-leveltraining objective based on contrastive learning.
it mitigates the collapse of bert-derived repre-sentations and transfers them to downstream tasks.
2) we explore various effective text augmentationstrategies to generate views for contrastive learningand analyze their effects on unsupervised sentencerepresentation transfer.
3) with only ﬁne-tuning onunsupervised target datasets, our approach achievessigniﬁcant improvement on sts tasks.
when fur-ther incorporating with nli supervision, our ap-.
proach achieves new state-of-the-art performance.
we also show the robustness of our approach indata scarcity scenarios and intuitive analysis of thetransferred representations.3.
2 related work.
2.1 sentence representation learning.
supervised approaches several works use super-vised datasets for sentence representation learning.
conneau et al.
(2017) ﬁnds the supervised nat-ural language inference (nli) task is useful totrain good sentence representations.
they use abilstm-based encoder and train it on two nlidatasets, stanford nli (snli) (bowman et al.,2015) and multi-genre nli (mnli) (williamset al., 2018).
universal sentence encoder (ceret al., 2018) adopts a transformer-based architec-ture and uses the snli dataset to augment the unsu-pervised training.
sbert (reimers and gurevych,2019) proposes a siamese architecture with a sharedbert encoder and is also trained on snli andmnli datasets..self-supervised objectives for pre-trainingbert (devlin et al., 2019) proposes a bi-directional transformer encoder for languagemodel pre-training.
it includes a sentence-leveltraining objective, namely next sentence predic-tion (nsp), which predicts whether two sentencesare adjacent or not.
however, nsp is proved tobe weak and has little contribution to the ﬁnalperformance (liu et al., 2019).
after that, var-ious self-supervised objectives are proposed forpre-training bert-like sentence encoders.
cross-thought (wang et al., 2020) and cmlm (yanget al., 2020) are two similar objectives that recovermasked tokens in one sentence conditioned on therepresentations of its contextual sentences.
slm(lee et al., 2020) proposes an objective that re-constructs the correct sentence ordering given theshufﬂed sentences as the input.
however, all theseobjectives need document-level corpus and are thusnot applicable to downstream tasks with only shorttexts..unsupervised approaches bert-ﬂow (liet al., 2020) proposes a ﬂow-based approach thatmaps bert embeddings to a standard gaussianlatent space, where embeddings are more suitablefor comparison.
however, this approach introduces.
3our code is available at https://github.com/.
yym6472/consert..5066extra model structures and need specialized imple-mentation, which may limit its application..2.2 contrastive learning.
contrastive learning for visual representa-tion learning recently, contrastive learning hasbecome a very popular technique in unsupervisedvisual representation learning with solid perfor-mance (chen et al., 2020a; he et al., 2020; chenet al., 2020b).
they believe that good representa-tion should be able to identify the same object whiledistinguishing itself from other objects.
based onthis intuition, they apply image transformations(e.g.
cropping, rotation, cutout, etc.)
to randomlygenerate two augmented versions for each imageand make them close in the representation space.
such approaches can be regarded as the invari-ance modeling to the input samples.
chen et al.
(2020a) proposes simclr, a simple frameworkfor contrastive learning.
they use the normalizedtemperature-scaled cross-entropy loss (nt-xent)as the training loss, which is also called infoncein the previous literature (hjelm et al., 2018)..contrastive learning for textual represen-tation learning recently, contrastive learning hasbeen widely applied in nlp tasks.
many worksuse it for language model pre-training.
is-bert(zhang et al., 2020) proposes to add 1-d convo-lutional neural network (cnn) layers on top ofbert and train the cnns by maximizing the mu-tual information (mi) between the global sentenceembedding and its corresponding local contextsembeddings.
cert (fang and xie, 2020) adoptsa similar structure as moco (he et al., 2020) anduses back-translation for data augmentation.
how-ever, the momentum encoder needs extra memoryand back-translation may produce false positives.
bert-ct (carlsson et al., 2021) uses two individ-ual encoders for contrastive learning, which alsoneeds extra memory.
besides, they only sample 7negatives, resulting in low training efﬁciency.
de-clutr (giorgi et al., 2020) adopts the architec-ture of simclr and jointly trains the model withcontrastive objective and masked language modelobjective.
however, they only use spans for con-trastive learning, which is fragmented in semantics.
clear (wu et al., 2020) uses the same architec-ture and objectives as declutr.
both of them areused to pre-train the language model, which needsa large corpus and takes a lot of resources..figure 2: the general framework of our proposed ap-proach..3 approach.
in this section, we present consert for sentencerepresentation transfer.
given a bert-like pre-trained language model m and an unsuperviseddataset d drawn from the target distribution, weaim at ﬁne-tuning m on d to make the sentencerepresentation more task-relevant and applicableto downstream tasks.
we ﬁrst present the generalframework of our approach, then we introduce sev-eral data augmentation strategies for contrastivelearning.
finally, we talk about three ways to fur-ther incorporate supervision signals..3.1 general framework.
our approach is mainly inspired by simclr (chenet al., 2020a).
as shown in figure 2, there are threemajor components in our framework:.
• a data augmentation module that generatesdifferent views for input samples at the tokenembedding layer..• a shared bert encoder that computes sen-tence representations for each input text.
dur-ing training, we use the average pooling of thetoken embeddings at the last layer to obtainsentence representations..• a contrastive loss layer on top of the bertencoder.
it maximizes the agreement betweenone representation and its corresponding ver-sion that is augmented from the same sentencewhile keeping it distant from other sentencerepresentations in the same batch..5067sentence1sentencen……data augmentationtrmtrmtrmtrmtrmtrmtrmtrmbert encoder layeraverage poolingmaximize agreementkeep distantdataaugmentationmoduleshared bertencodercontrastiveloss layersentencerepresentationsa batch of sentencesfigure 3: the four data augmentation strategies used in our experiments..for each input text x, we ﬁrst pass it to the dataaugmentation module, in which two transforma-tions t1 and t2 are applied to generate two versionsof token embeddings: ei = t1(x), ej = t2(x),where ei, ej ∈ rl×d, l is the sequence lengthand d is the hidden dimension.
after that, both eiand ej will be encoded by multi-layer transformerblocks in bert and produce the sentence represen-tations ri and rj through average pooling..following chen et al.
(2020a), we adopt the nor-malized temperature-scaled cross-entropy loss (nt-xent) as the contrastive objective.
during eachtraining step, we randomly sample n texts fromd to construct a mini-batch, resulting in 2n repre-sentations after augmentation.
each data point istrained to ﬁnd out its counterpart among 2(n − 1)in-batch negative samples:.
li,j = − log.
exp(sim(ri, rj)/τ )1[k(cid:54)=i] exp(sim(ri, rk)/τ ).
(cid:80)2nk=1.
(1).
, where sim(·) indicates the cosine similarity func-tion, τ controls the temperature and 1 is the indi-cator.
finally, we average all 2n in-batch classi-ﬁcation losses to obtain the ﬁnal contrastive losslcon..3.2 data augmentation strategies.
worst-case perturbation to the input sample.
weimplement this strategy with fast gradient value(fgv) (rozsa et al., 2016), which directly uses thegradient to compute the perturbation and thus isfaster than two-step alternative methods.
note thatthis strategy is only applicable when jointly train-ing with supervision since it relies on supervisedloss to compute adversarial perturbations..token shufﬂing in this strategy, we aim to ran-domly shufﬂe the order of the tokens in the inputsequences.
since the bag-of-words nature in thetransformer architecture, the position encoding isthe only factor about the sequential information.
thus, similar to lee et al.
(2020), we implementthis strategy by passing the shufﬂed position ids tothe embedding layer while keeping the order of thetoken ids unchanged..cutoff shen et al.
(2020) proposes a simple andefﬁcient data augmentation strategy called cutoff.
they randomly erase some tokens (for token cut-off), feature dimensions (for feature cutoff), or to-ken spans (for span cutoff) in the l × d featurematrix.
in our experiments, we only use token cut-off and feature cutoff and apply them to the tokenembeddings for view generation..we explore four different data augmentation strate-gies to generate views for contrastive learning, in-cluding adversarial attack (goodfellow et al., 2014;kurakin et al., 2016), token shufﬂing, cutoff (shenet al., 2020) and dropout (hinton et al., 2012), asillustrated in figure 3..adversarial attack adversarial training is gen-erally used to improve the model’s robustness.
they generate adversarial samples by adding a.dropout dropout is a widely used regulariza-tion method that avoids overﬁtting.
however, inour experiments, we also show its effectiveness asan augmentation strategy for contrastive learning.
for this setting, we randomly drop elements in thetoken embedding layer by a speciﬁc probabilityand set their values to zero.
note that this strat-egy is different from cutoff since each element isconsidered individually..50681122334455l × dembedding matrixpositionidsno augmentationdl1122334455adversarial perturbations2255441133shuffled position idsa) adversarial attackb) token shuffling11223344551122334455c) cutofftoken cutofffeature cutoff1122334455d) dropoutunchanged elementsperturbed elementszero-out elementssts12.
sts13.
sts14.
sts15.
sts16.
stsb.
sick-r.total.
number of train samplesnumber of valid samplesnumber of test samplesnumber of unlabeled texts.
0031086216.
0015003000.
0037507500.
00300017000.
00118618366.
57491500137917256.
4500500492719854.
---89192.table 1: the statistics of sts datasets..3.3.incorporating supervision signals.
besides unsupervised transfer, our approach canalso be incorporated with supervised learning.
wetake the nli supervision as an example.
it is a sen-tence pair classiﬁcation task, where the model aretrained to distinguish the relation between two sen-tences among contradiction, entailment and neu-tral.
the classiﬁcation objective can be expressedas following:.
f = concat(r1, r2, |r1 − r2|)lce = crossentropy(w f + b, y), where r1 and r2 denote two sentence representa-tions..(2).
we propose three ways for incorporating addi-.
tional supervised signals:.
• joint training (joint) we jointly train themodel with the supervised and unsupervisedobjectives ljoint = lce + αlcon on nlidataset.
α is a hyper-parameter to balancetwo objectives..• supervised training then unsupervisedtransfer (sup-unsup) we ﬁrst train the modelwith lce on nli dataset, then use lcon to ﬁne-tune it on the target dataset..• joint training then unsupervised transfer(joint-unsup) we ﬁrst train the model withthe ljoint on nli dataset, then use lcon toﬁne-tune it on the target dataset..4 experiments.
to verify the effectiveness of our proposed ap-proach, we conduct experiments on semantic tex-tual similarity (sts) tasks under the unsupervisedand supervised settings..4.1 setups.
dataset following previous works(reimers andgurevych, 2019; li et al., 2020; zhang et al., 2020),we evaluate our approach on multiple sts datasets,including sts tasks 2012 - 2016 (sts12 - sts16)(agirre et al., 2012, 2013, 2014, 2015, 2016), sts.
benchmark (stsb) (cer et al., 2017) and sick-relatedness (sick-r) (marelli et al.).
each samplein these datasets contains a pair of sentences aswell as a gold score between 0 and 5 to indicatetheir semantic similarity.
for our unsupervisedexperiments, we mix the unlabeled texts from thesedatasets to ﬁne-tune our model.
we obtain all 7datasets through the senteval toolkit (conneau andkiela, 2018).
the statistics is shown in table 1..for supervised experiments, we use the combina-tion of snli (570k samples) (bowman et al., 2015)and mnli (430k samples) (williams et al., 2018)to train our model.
in the joint training setting, thenli texts are also used for contrastive objectives.
baselines to show our effectiveness on unsuper-vised sentence representation transfer, we mainlyselect bert-ﬂow (li et al., 2020) for comparison,since it shares the same setting as our approach.
for unsupervised comparison, we use the averageof glove embeddings, the bert-derived nativeembeddings, clear (wu et al., 2020) (trained onbookcorpus and english wikipedia corpus), is-bert (zhang et al., 2020) (trained on unlabeledtexts from nli datasets), bert-ct (carlsson et al.,2021) (trained on english wikipedia corpus).
forcomparison with supervised methods, we select in-fersent (conneau et al., 2017), universal sentenceencoder (cer et al., 2018), sbert (reimers andgurevych, 2019) and bert-ct (carlsson et al.,2021) as baselines.
they are all trained with nlisupervision..evaluation when evaluating the trained model,we ﬁrst obtain the representation of sentences byaveraging the token embeddings at the last twolayers4, then we report the spearman correlation be-tween the cosine similarity scores of sentence rep-resentations and the human-annotated gold scores.
when calculating spearman correlation, we mergeall sentences together (even if some sts datasetshave multiple splits) and calculate spearman corre-lation for only once5..4as shown in li et al.
(2020), averaging the last twolayers of bert achieves slightly better results than averagingthe last one layer..5note that such evaluation procedure is different from.
5069method.
sts12 sts13 sts14 sts15 sts16 stsb sick-r avg..‡.
‡.
avg.
glove embeddings†bertbasebertlargeclearbaseis-bertbase-nli†bertbase-ct†bertlarge-ct†.
†.
bertbase-ﬂow†bertlarge-ﬂow†consertbaseconsertlarge.
‡.
‡.
unsupervised baselines68.2559.7370.6663.3949.3759.5355.8347.9557.6463.657.448.975.2361.2169.2478.5572.3770.9178.8374.2275.97.using sts unlabeled texts73.7768.4272.1474.9269.4273.3979.7269.0778.4982.7874.1382.96.
55.1435.2033.0649.056.7766.8669.50.
63.4865.2064.6470.69.
63.6662.7362.4265.670.1677.7878.92.
75.3777.6375.9576.66.
58.0248.1849.6675.669.21--.
70.7272.2673.9777.53.
53.7658.6053.8772.564.25--.
63.1162.5067.3170.37.
61.3253.8651.4961.866.58--.
69.5770.7672.7476.45.table 2: the performance comparison of consert with other methods in an unsupervised setting.
we report thespearman correlation ρ × 100 on 7 sts datasets.
methods with † indicate that we directly report the scores fromthe corresponding paper, while methods with ‡ indicate our implementation..implementation details our implementationis based on the sentence-bert6 (reimers andgurevych, 2019).
we use both the bert-baseand bert-large for our experiments.
the maxsequence length is set to 64 and we remove thedefault dropout layer in bert architecture con-sidering the cutoff and dropout data augmentationstrategies used in our framework.
the ratio of to-ken cutoff and feature cutoff is set to 0.15 and 0.2respectively, as suggested in shen et al.
(2020).
theratio of dropout is set to 0.2. the temperature τ ofnt-xent loss is set to 0.1, and the α is set to 0.15for the joint training setting.
we adopt adam op-timizer and set the learning rate to 5e-7.
we use alinear learning rate warm-up over 10% of the train-ing steps.
the batch size is set to 96 in most of ourexperiments.
we use the dev set of stsb to tunethe hyperparameters (including the augmentationstrategies) and evaluate the model every 200 stepsduring training.
the best checkpoint on the dev setof stsb is saved for test.
we further discuss theinﬂuence of the batch size and the temperature inthe subsequent section..4.2 unsupervised results.
for unsupervised evaluation, we load the pre-trained bert to initialize the bert encoder inour framework.
then we randomly mix the unla-beled texts from 7 sts datasets and use them toﬁne-tune our model..senteval toolkit, which calculates spearman correlation foreach split and reports the mean or weighted mean scores..6https://github.com/ukplab/.
sentence-transformers.
the results are shown in table 2. we can observethat both bert-ﬂow and consert can improvethe representation space and outperform the gloveand bert baselines with unlabeled texts fromtarget datasets.
however, consertlarge achievesthe best performance among 6 sts datasets, sig-niﬁcantly outperforming bertlarge-ﬂow with an8% relative performance gain on average (from70.76 to 76.45).
moreover, it is worth notingthat consertlarge even outperforms several su-pervised baselines (see figure 3) like infersent(65.01) and universal sentence encoder (71.72),and keeps comparable to the strong supervisedmethod sbertlarge-nli (76.55).
for the bertbasearchitecture, our approach consertbase also out-performs bertbase-ﬂow with an improvement of3.17 (from 69.57 to 72.74)..4.3 supervised results.
for supervised evaluation, we consider the threesettings described in section 3.3. note that in thejoint setting, only nli texts are used for contrastivelearning, making it comparable to sbert-nli.
weuse the model trained under the joint setting as theinitial checkpoint in the joint-unsup setting.
wealso re-implement the sbert-nli baselines anduse them as the initial checkpoint in the sup-unsupsetting..the results are illustrated in table 3. for themodels trained with nli supervision, we ﬁnd thatconsert joint consistently performs better thansbert, revealing the effectiveness of our proposedcontrastive objective as well as the data augmen-tation strategies.
on average, consertbase joint.
5070method.
sts12 sts13 sts14 sts15 sts16 stsb sick-r avg..infersent - glove†universal sentence encoder†sbertbase-nli†sbertlarge-nli†sbertbase-nli (re-impl.
)‡sbertlarge-nli (re-impl.
)‡bertbase-ct†bertlarge-ct†consertbase joint‡consertlarge joint‡.
52.8664.4970.9772.2769.8972.6968.8069.8070.5373.26.using nli supervision66.7567.8076.5378.4675.7778.7774.5875.4579.9682.36.
62.1564.6173.1974.9072.3675.1376.6276.4774.8577.73.
72.7776.8379.0980.9978.5180.9579.7281.3481.4583.84.
66.8773.1874.3076.2573.6776.8977.1478.1176.7278.75.bertbase-ﬂow†bertlarge-ﬂow†consertbase sup-unsup‡consertlarge sup-unsup‡consertbase joint-unsup‡consertlarge joint-unsup‡.
using nli supervision and sts unlabeled texts78.9480.5777.9879.4578.7680.39.
78.4880.2784.8686.0183.9385.45.
81.9582.9783.1183.8883.6685.59.
77.6278.8577.4479.0077.0579.41.
68.9570.1973.5175.2674.0777.47.
68.0374.9277.0379.2376.7579.53--78.8281.54.
81.0381.1881.8082.9581.3683.42.
65.6576.6972.9173.7572.7673.25--77.5378.64.
74.9774.5274.2976.5476.7777.26.
65.0171.2274.8976.5574.2476.74--77.1279.44.
77.4278.3679.0080.4479.3781.28.table 3: the performance comparison of consert with other methods in a supervised setting.
we report thespearman correlation ρ × 100 on 7 sts datasets.
methods with † indicate that we directly report the scores fromthe corresponding paper, while methods with ‡ indicate our implementation..achieves a performance gain of 2.88 over the re-implemented sbertbase-nli, and consertlargejoint achieves a performance gain of 2.70..when further performing representation trans-fer with sts unlabeled texts, our approachachieves even better performance.
on average,consertlarge joint-unsup outperforms the initialcheckpoint consertlarge joint with 1.84 perfor-mance gain, and outperforms the previous state-of-the-art bertlarge-ﬂow with 2.92 performance gain.
the results demonstrate that even for the modelstrained under supervision, there is still a huge po-tential of unsupervised representation transfer forimprovement..5 qualitative analysis.
5.1 analysis of bert embedding space.
to prove the hypothesis that the collapse issue ismainly due to the anisotropic space that is sensitiveto the token frequency, we conduct experimentsthat mask the embeddings of several most frequenttokens when applying average pooling to calculatethe sentence representations.
the relation betweenthe number of removed top-k frequent tokens andthe average spearman correlation is shown in fig-ure 4..we can observe that when removing a few topfrequent tokens, the performance of bert im-proves sharply on sts tasks.
when removing.
figure 4: the average spearman correlation on ststasks w.r.t.
the number of removed top-k frequent to-kens.
note that we also considered the [cls] and[sep] tokens and they are the 2 most frequent tokens.
the frequency of each token is calculated through thetest split of the sts benchmark dataset..34 most frequent tokens, the best performance isachieved (61.66), and there is an improvement of7.8 from the original performance (53.86).
forconsert, we ﬁnd that removing a few most fre-quent tokens only results in a small improvementof less than 0.3. the results show that our approachreshapes the bert’s original embedding space, re-ducing the inﬂuence of common tokens on sentencerepresentations..5.2 effect of data augmentation strategy.
in this section, we study the effect of data augmen-tation strategies for contrastive learning.
we con-sider 5 options for each transformation, includingnone (i.e.
doing nothing), shufﬂe, token cutoff,.
50710255075100125150175200number of removed top-k frequent tokens55.057.560.062.565.067.570.072.5average spearman correlationconsert w/o removingbert w/o removingbert-baseconsert-basefigure 5: the performance visualization with differentcombinations of data augmentation strategies.
the rowindicates the 1st data augmentation strategy while thecolumn indicates the 2nd data augmentation strategy..feature cutoff, and dropout, resulting in 5×5 com-binations.
note that the adversarial attack strategyis not considered here, since it needs additional su-pervision to generate adversarial samples.
all theseexperiments follow the unsupervised setting anduse the bertbase architecture..the results can be found in figure 5. we canmake the following observations.
first, shufﬂeand token cutoff are the two most effective strate-gies (where shufﬂe is slightly better than tokencutoff), signiﬁcantly outperforming feature cutoffand dropout.
this is probably because shufﬂe andtoken cutoff are more related to the downstreamsts tasks since they are directly operated on thetoken level and change the structure of the sentenceto produce hard examples..secondly, feature cutoff and dropout also im-prove performance by roughly 4 points when com-pared with the none-none baseline.
moreover, weﬁnd they work well as a complementary strategy.
combining with another strategy like shufﬂe mayfurther improve the performance.
when combinedshufﬂe with feature cutoff, we achieve the bestresult.
we argue that feature cutoff and dropoutare useful in modeling the invariance of the internalnoise for the sentence encoder, and thus improvethe model’s robustness..finally, we also observe that even without anydata augmentation (the none-none combination),our contrastive framework can improve bert’sperformance on sts tasks (from 53.86 to 63.84).
this none-none combination has no effect on max-imizing agreement between views since the repre-.
figure 6: the few-shot experiments under the unsu-pervised and supervised settings.
we report the aver-age spearman correlation on sts datasets with 1, 10,100, 1,000, and 10,000 unlabeled texts available, re-spectively.
the full dataset indicates all 89192 unla-beled texts from 7 sts datasets..sentations of augmented views are exactly the same.
on the contrary, it tunes the representation spaceby pushing each representation away from others.
we believe that the improvement is mainly due tothe collapse phenomenon of bert’s native repre-sentation space.
to some extent, it also explainswhy our method works..5.3 performance under few-shot settings.
to validate the reliability and the robustness ofconsert under the data scarcity scenarios, weconduct the few-shot experiments.
we limit thenumber of unlabeled texts to 1, 10, 100, 1000, and10000 respectively, and compare their performancewith the full dataset..figure 6 presents the results.
for both the unsu-pervised and the supervised settings, our approachcan make a huge improvement over the baselinewith only 100 samples available.
when the trainingsamples increase to 1000, our approach can basi-cally achieve comparable results with the modelstrained on the full dataset.
the results reveal therobustness and effectiveness of our approach un-der the data scarcity scenarios, which is commonin reality.
with only a small amount of unlabeledtexts drawn from the target data distribution, ourapproach can also tune the representation space andbeneﬁt the downstream tasks..5.4.inﬂuence of temperature.
the temperature τ in nt-xent loss (equation 1) isused to control the smoothness of the distributionnormalized by softmax operation and thus inﬂu-ences the gradients when backpropagation.
a largetemperature smooths the distribution while a smalltemperature sharpens the distribution.
in our ex-periments, we explore the inﬂuence of temperature.
5072noneshuffletokencutofffeaturecutoffdropoutnoneshuffletokencutofffeaturecutoffdropout63.8472.0971.1167.8667.7772.0971.6272.4172.6772.6471.1172.4170.9170.8471.3067.8672.7471.2066.7666.6567.7772.7171.3266.6766.52646566676869707172average spearman correlation1 shot10 shot100 shot1000 shot10000 shotfull datasetnumber of unlabeled texts50556065707580average spearman correlation53.4759.1167.6672.6172.8272.7474.1374.3576.5778.1078.8079.00unsupervisedsupervised (sup-unsup)6 conclusion.
in this paper, we propose consert, a self-supervised contrastive learning framework fortransferring sentence representations to down-stream tasks.
the framework does not need extrastructure and is easy to implement for any encoder.
we demonstrate the effectiveness of our frameworkon various sts datasets, both our unsupervised andsupervised methods achieve new state-of-the-artperformance.
furthermore, few-shot experimentssuggest that our framework is robust in the datascarcity scenarios.
we also compare multiple com-binations of data augmentation strategies and pro-vide ﬁne-grained analysis for interpreting how ourapproach works.
we hope our work will provide anew perspective for future researches on sentencerepresentation transfer..acknowledgements.
we thank keqing he, hongzhi zhang and allanonymous reviewers for their helpful commentsand suggestions.
this work was partially sup-ported by national key r&d program of chinano.
ii no.
2019yff0303300 and subject2019yff0303302, docomo beijing communi-cations laboratories co., ltd, moe-cmcc “artiﬁ-cal intelligence” project no.
mcm20190701..broader impact.
sentence representation learning is a basic taskin natural language processing and beneﬁts manydownstream tasks.
this work proposes a con-trastive learning based framework to solve the col-lapse issue of bert and transfer bert sentencerepresentations to target data distribution.
our ap-proach not only provides a new perspective aboutbert’s representation space, but is also useful inpractical applications, especially for data scarcityscenarios.
when applying our approach, the usershould collect a few unlabeled texts from targetdata distribution and use our framework to ﬁne-tune bert encoder in a self-supervised manner.
since our approach is self-supervised, no bias willbe introduced from human annotations.
moreover,our data augmentation strategies also have littleprobability to introduce extra biases since they areall based on random sampling.
however, it is stillpossible to introduce data biases from the unlabeledtexts.
therefore, users should pay special attentionto ensure that the training data is ethical, unbiased,and closely related to downstream tasks..figure 7: the inﬂuence of different temperatures in nt-xent.
the best performance is achieved when the tem-perature is set to 0.1..batch sizeavg.
spearmannumber of steps.
1672.636175.
4872.602459.
9672.741530.
19272.86930.
28872.98620.table 4: the average spearman correlation as well asthe training steps of our unsupervised approach withdifferent batch sizes..and present the result in figure 7..as shown in the ﬁgure, we ﬁnd the performanceis extremely sensitive to the temperature.
eithertoo small or too large temperature will make ourmodel perform badly.
and the optimal temperatureis obtained within a small range (from about 0.08to 0.12).
this phenomenon again demonstrates thecollapse issue of bert embeddings, as most sen-tences are close to each other, a large temperaturemay make this task too hard to learn.
we select 0.1as the temperature in most of our experiments..5.5.inﬂuence of batch size.
in some previous works of contrastive learning, itis reported that a large batch size beneﬁts the ﬁ-nal performance and accelerates the convergenceof the model since it provides more in-batch nega-tive samples for contrastive learning (chen et al.,2020a).
those in-batch negative samples improvethe training efﬁciency.
we also analyze the inﬂu-ence of the batch size for unsupervised sentencerepresentation transfer..the results are illustrated in table 4. we showboth the spearman correlation and the correspond-ing training steps.
we ﬁnd that a larger batch sizedoes achieve better performance.
however, theimprovement is not so signiﬁcant.
meanwhile, alarger batch size does speed up the training process,but it also needs more gpu memories at the sametime..50730.010.020.050.080.10.120.150.20.30.40.5the temperature of nt-xent6567697173average spearman correlationreferences.
eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, inigo lopez-gazpio, montse maritxalar, radamihalcea, et al.
2015. semeval-2015 task 2: seman-tic textual similarity, english, spanish and pilot onin proceedings of the 9th interna-interpretability.
tional workshop on semantic evaluation (semeval2015), pages 252–263..eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycewiebe.
2014. semeval-2014 task 10: multilingualin proceedings of thesemantic textual similarity.
8th international workshop on semantic evaluation(semeval 2014), pages 81–91..eneko agirre, carmen banea, daniel cer, monadiab, aitor gonzalez agirre, rada mihalcea, ger-man rigau claramunt, and janyce wiebe.
2016.semeval-2016 task 1: semantic textual similar-ity, monolingual and cross-lingual evaluation.
insemeval-2016.
10th international workshop on se-mantic evaluation; 2016 jun 16-17; san diego, ca.
stroudsburg (pa): acl; 2016. p. 497-511. acl (as-sociation for computational linguistics)..eneko agirre, daniel cer, mona diab, and aitorgonzalez-agirre.
2012. semeval-2012 task 6: a pi-lot on semantic textual similarity.
in * sem 2012:the first joint conference on lexical and compu-tational semantics–volume 1: proceedings of themain conference and the shared task, and volume2: proceedings of the sixth international workshopon semantic evaluation (semeval 2012), pages 385–393..eneko agirre, daniel cer, mona diab, aitor gonzalez-agirre, and weiwei guo.
2013.
* sem 2013 sharedin second jointtask: semantic textual similarity.
conference on lexical and computational semantics(* sem), volume 1: proceedings of the main confer-ence and the shared task: semantic textual similar-ity, pages 32–43..samuel bowman, gabor angeli, christopher potts, andchristopher d manning.
2015. a large annotatedcorpus for learning natural language inference.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages632–642..fredrik carlsson, magnus sahlgren, evangeliagogoulou, amaru cuba gyllensten, and erik ylip¨a¨ahellqvist.
2021. semantic re-tuning with contrastivein international conference on learningtension.
representations..daniel cer, mona diab, eneko agirre, i˜nigo lopez-gazpio, and lucia specia.
2017.semeval-2017task 1: semantic textual similarity multilingual andcrosslingual focused evaluation.
in proceedings ofthe 11th international workshop on semantic evalu-ation (semeval-2017), pages 1–14..daniel cer, yinfei yang, sheng-yi kong, nan hua,nicole limtiaco, rhomni st john, noah constant,mario guajardo-cespedes, steve yuan, chris tar,et al.
2018. universal sentence encoder for english.
in proceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 169–174..ting chen, simon kornblith, mohammad norouzi,and geoffrey hinton.
2020a.
a simple frameworkfor contrastive learning of visual representations.
arxiv preprint arxiv:2002.05709..ting chen, simon kornblith, kevin swersky, moham-mad norouzi, and geoffrey hinton.
2020b.
big self-supervised models are strong semi-supervised learn-ers..xinlei chen and kaiming he.
2020. exploring sim-ple siamese representation learning.
arxiv preprintarxiv:2011.10566..alexis conneau and douwe kiela.
2018. senteval: anevaluation toolkit for universal sentence representa-tions.
in proceedings of the eleventh internationalconference on language resources and evaluation(lrec 2018)..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in natu-ral language processing, pages 670–680..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..hongchao fang and pengtao xie.
2020. cert: con-trastive self-supervised learning for language under-standing.
arxiv preprint arxiv:2005.12766..jun gao, di he, xu tan, tao qin, liwei wang, and tie-yan liu.
2019. representation degeneration prob-lem in training natural language generation models.
arxiv preprint arxiv:1907.12009..john m giorgi, osvald nitski, gary d. bader, andbo wang.
2020. declutr: deep contrastive learn-ing for unsupervised textual representations.
arxiv,abs/2006.03659..ian j goodfellow, jonathon shlens, and christianszegedy.
2014. explaining and harnessing adversar-ial examples.
arxiv preprint arxiv:1412.6572..kaiming he, haoqi fan, yuxin wu, saining xie, andross girshick.
2020. momentum contrast for unsu-pervised visual representation learning.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition, pages 9729–9738..5074andras rozsa, ethan m rudd, and terrance e boult.
2016. adversarial diversity and hard positive gen-in proceedings of the ieee conferenceeration.
on computer vision and pattern recognition work-shops, pages 25–32..dinghan shen, mingzhi zheng, yelong shen, yanruqu, and weizhu chen.
2020. a simple but tough-to-beat data augmentation approach for natural lan-guage understanding and generation.
arxiv preprintarxiv:2009.13818..lingxiao wang, jing huang, kevin huang, ziniu hu,guangtao wang, and quanquan gu.
2019. improv-ing neural language generation with spectrum con-trol.
in international conference on learning rep-resentations..shuohang wang, yuwei fang, siqi sun, zhe gan,yu cheng, jingjing liu, and jing jiang.
2020.cross-thought for sentence encoder pre-training.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 412–421..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long papers), pages 1112–1122..zhuofeng wu, sinong wang, jiatao gu, madiankhabsa, fei sun, and hao ma.
2020. clear: con-trastive learning for sentence representation.
arxivpreprint arxiv:2012.15466..ziyi yang, yinfei yang, daniel cer, jax law, anderic darve.
2020. universal sentence representationlearning with conditional masked language model.
arxiv preprint arxiv:2012.14388..yan zhang, ruidan he, zuozhu liu, kwan hui lim,and lidong bing.
2020. an unsupervised sentenceembedding method by mutual information maxi-mization.
in proceedings of the 2020 conference onempirical methods in natural language processing(emnlp), pages 1601–1610..felix hill, kyunghyun cho, and anna korhonen.
2016.learning distributed representations of sentencesin proceedings of the 2016from unlabelled data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 1367–1377..geoffrey e hinton, nitish srivastava, alex krizhevsky,ilya sutskever, and ruslan r salakhutdinov.
2012.improving neural networks by preventing co-arxiv preprintadaptation of feature detectors.
arxiv:1207.0580..r devon hjelm, alex fedorov, samuel lavoie-marchildon, karan grewal, phil bachman, adamtrischler, and yoshua bengio.
2018.learn-informationing deep representations by mutualarxiv preprintestimation and maximization.
arxiv:1808.06670..ryan kiros, yukun zhu, russ r salakhutdinov,richard zemel, raquel urtasun, antonio torralba,and sanja fidler.
2015. skip-thought vectors.
ad-vances in neural information processing systems,28:3294–3302..alexey kurakin, ian goodfellow, and samy bengio.
2016. adversarial examples in the physical world.
arxiv preprint arxiv:1607.02533..haejun lee, drew a hudson, kangwook lee, andchristopher d manning.
2020. slm: learning adiscourse language representation with sentence un-shufﬂing.
in proceedings of the 2020 conference onempirical methods in natural language processing(emnlp), pages 1551–1562..bohan li, hao zhou, junxian he, mingxuan wang,yiming yang, and lei li.
2020. on the sentenceembeddings from pre-trained language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 9119–9130, online.
association for computa-tional linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..marco marelli, stefano menini, marco baroni, luisabentivogli, raffaella bernardi, roberto zamparelli,et al.
a sick cure for the evaluation of compositionaldistributional semantic models..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3973–3983..5075