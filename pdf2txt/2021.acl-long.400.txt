autotinybert: automatic hyper-parameter optimizationfor efﬁcient pre-trained language models.
yichun yin1, cheng chen2*, lifeng shang1, xin jiang1, xiao chen1, qun liu11huawei noah’s ark lab2department of computer science and technology, tsinghua university{yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.comc-chen19@mails.tsinghua.edu.cn.
abstract.
pre-trained language models (plms) haveachieved great success in natural language pro-cessing.
most of plms follow the default set-ting of architecture hyper-parameters (e.g., thehidden dimension is a quarter of the intermedi-ate dimension in feed-forward sub-networks) inbert (devlin et al., 2019).
few studies havebeen conducted to explore the design of archi-tecture hyper-parameters in bert, especiallyfor the more efﬁcient plms with tiny sizes,which are essential for practical deploymentin this pa-on resource-constrained devices.
per, we adopt the one-shot neural architecturesearch (nas) to automatically search architec-ture hyper-parameters.
speciﬁcally, we care-fully design the techniques of one-shot learn-ing and the search space to provide an adaptiveand efﬁcient development way of tiny plmsfor various latency constraints.
we name ourmethod autotinybert1 and evaluate its ef-fectiveness on the glue and squad bench-marks.
the extensive experiments show thatour method outperforms both the sota search-based baseline (nas-bert) and the sotadistillation-based methods (such as distilbert,tinybert, minilm and mobilebert).
in ad-dition, based on the obtained architectures, wepropose a more efﬁcient development methodthat is even faster than the development of asingle plm..1.introduction.
pre-trained language models, such as bert (de-vlin et al., 2019), roberta (liu et al., 2019) andxlnet (yang et al., 2019), have become prevalentin natural language processing.
to improve modelperformance, most plms (e.g.
electra (clarket al., 2019) and gpt-2/3 (radford et al., 2019;.
*contribution during internship at noah’s ark lab.
1our code implementation and pre-trained models areavailable at https://github.com/huawei-noah/pretrained-language-model ..figure 1: inference speedup vs. glue scores.
underthe same speedup constraint, our method outperformsboth the default hyper-parameter setting of bert (de-vlin et al., 2019), pf (turc et al., 2019)) and nas-bert (xu et al., 2021).
more details are in the sec-tion 4.2..brown et al., 2020)) follow the default rule ofhyper-parameter setting2 in bert to scale up theirmodel sizes.
due to its simplicity, this rule hasbeen widely used and can help large plms obtainpromising results (brown et al., 2020)..in many industrial scenarios, we need to deployplms on resource-constrained devices, such assmartphones and servers with limited computationpower.
due to the expensive computation and slowinference speed, it is usually difﬁcult to deployplms such as bert (12/24 layers, 110m/340mparameters) and gpt-2 (48 layers, 1.5b parame-ters) at their original scales.
therefore, there is anurgent need to develop plms with smaller sizeswhich have lower computation cost and inferencelatency.
in this work, we focus on a speciﬁc type ofefﬁcient plms, which we deﬁne to have inferencetime less than 1/4 of bert-base.3.
2the default rule is dm = dq|k|v = 1/4df , which meansthe dimension of hidden vector dm is equal to the dimen-sions of query/key/value vector dq|k|v and a quarter of theintermediate size df in feed-forward networks..3we empirically ﬁnd that being at least 4x faster is a basic.
requirement in practical deployment environment..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5146–5157august1–6,2021.©2021associationforcomputationallinguistics514651015202530speedup (compared to bert-base)687072747678glue score (%)autotinybert(ours)bertpfnas-bertalthough, there have been quite a few workusing knowledge distillation to build smallplms (sanh et al., 2019; jiao et al., 2020b; sunet al., 2019, 2020), all of them focus on the ap-plication of distillation techniques (hinton et al.,2015; romero et al., 2014) and do not study theeffect of architecture hyper-parameter settings onmodel performance.
recently, neural architecturesearch and hyper-parameter optimization (tan andle, 2019; han et al., 2020) have been widely ex-plored in machine learning, mostly in computervision, and have been proven to ﬁnd better designsthan heuristic ones.
inspired by this research, oneproblem that naturally arises is can we ﬁnd bettersettings of hyper-parameters4 for efﬁcient plms?
in this paper, we argue that the conventionalhyper-parameter setting is not best for efﬁcientplms (as shown in figure 1) and introduce amethod to automatically search for the optimalhyper-parameters for speciﬁc latency constraints.
pre-training efﬁcient plms is inevitably resource-consuming (turc et al., 2019).
therefore, it is infea-sible to directly evaluate millions of architectures.
to tackle this challenge, we introduce the one-shotneural architecture search (nas) (brock et al.,2018; cai et al., 2018; yu et al., 2020) to performthe automatic hyper-parameter optimization on ef-ﬁcient plms, named as autotinybert.
speciﬁ-cally, we ﬁrst use the one-shot learning to obtaina big superplm, which can act as proxies for allpotential sub-architectures.
proxy means that whenevaluating an architecture, we only need to extractthe corresponding sub-model from the superplm,instead of training the model from scratch.
super-plm helps avoid the time-consuming pre-trainingprocess and makes the search process efﬁcient.
tomake superplm more effective, we propose prac-tical techniques including the head sub-matrix ex-traction and efﬁcient batch-wise training, and par-ticularly limit the search space to the models withidentical layer structure.
furthermore, by usingsuperplm, we leverage search algorithm (xie andyuille, 2017; wang et al., 2020a) to ﬁnd hyper-parameters for various latency constraints..in the experiments, in addition to the pre-trainingsetting (devlin et al., 2019), we also consider thesetting of task-agnostic bert distillation (sunet al., 2020) that pre-trains with the loss of knowl-edge distillation, to build efﬁcient plms.
exten-.
sive results show that in pre-training setting, au-totinybert not only consistently outperforms thebert with conventional hyper-parameters underdifferent latency constraints, but also outperformsnas-bert based on neural architecture search.
intask-agnostic bert distillation, autotinybertoutperforms a series of existing sota methods ofdistilbert, tinybert and mobilebert..our contributions are three-fold: (1) we explorethe problem of how to design hyper-parameters forefﬁcient plms and introduce an effective and ef-ﬁcient method: autotinybert; (2) we conductextensive experiments in both scenarios of pre-training and knowledge distillation, and the resultsshow our method consistently outperforms base-lines under different latency constraints; (3) wesummarize a fast rule and it develops an autotiny-bert for a speciﬁc constraint with even about 50%of the training time of a conventional plm..2 preliminary.
before presenting our method, we ﬁrst providesome details about the transformer layer (vaswaniet al., 2017) to introduce the conventional hyper-parameter setting.
transformer layer includes twosub-structures:the multi-head attention (mha)and the feed-forward network (ffn)..for clarity, we show the mha as a decompos-able structure, where the mha includes h indi-vidual and parallel self-attention modules (calledheads).
the output of mha is obtained by sum-ming the output of all heads.
speciﬁcally, eachhead is represented by four main matrices w qi ∈i ∈ rdm×dv/hi ∈ rdm×dk/h, w vrdm×dq/h, w kand w o, and takes the hidden states5h ∈ rl×dmof the previous layer as input.
theoutput of mha is given by the following formulas:.
i ∈ rdv/h×do.
qi, ki, vi = hw q.i , hw k.attn(qi, ki, vi) = softmax(.
)vi.
i , hw viqiki(cid:112)dq|k/h.
t.hi = attn(qi, ki, vi)w oih(cid:88).
mha(h) =.
hi,.
i=1.
(1)where qi ∈ rl×dq/h, ki ∈ rl×dk/h, vi ∈rl×dv/h are obtained by the linear transformationsof w qrespectively.
attn(·) is the.
i , w k.i , w vi.
4we abbreviate the phrase architecture hyper-parameter.
as hyper-parameter in the paper..5we omitted the batch size for simplicity..5147figure 2: overview of autotinybert.
we ﬁrst train an effective superplm with one-shot learning, where theobjectives of pre-training or task-agnostic bert distillation are used.
then, given a speciﬁc latency constraint,we perform an evolutionary algorithm on the superplm to search optimal architectures.
finally, we extract thecorresponding sub-models based on the optimal architectures and further train these models..scaled dot-product attention operation.
then out-put of each head is transformed to hi ∈ rl×dobyw oi .
finally, outputs of all heads are summed asthe output of mha.
in addition, residual connec-tion and layer normalization are added on top ofmha to get the ﬁnal output:.
h mha = layernorm(h + mha(h))..(2).
in the conventional setting of the hyper-parametersin bert, all dimensions of matrices are the sameas the dimension of the hidden vector, namely,dq|k|v|o=dm.
in fact, there are only two require-ments of dq=dk and do=dm that must be satisﬁedbecause of the dot-product attention operation inmha and the residual connection..transformer layer also contains an ffn that is.
stacked on the mha, that is:.
3 methodology.
3.1 problem statement.
given a constraint of inference time, our goal is toﬁnd an optimal conﬁguration of architecture hyper-parameters αopt built with which plm can achievethe best performances on downstream tasks.
thisoptimization problem is formulated as:.
αopt = arg max.
perf(α, θ∗.
α),.
s.t..α∈ aθ∗α = arg min.
θ.lα(θ), lat(α) ≤ t,.
(4).
where t is a speciﬁc time constraint, a refers tothe set of all possible architectures (i.e., combi-nation of hyper-parameters), lat(·) is a latencyevaluator, lα(·) denotes the loss function of plmswith the hyper-parameter α, and θ is the corre-sponding model parameters.
we aim to search anoptimal architecture for efﬁcient plm (lat(α) <1/4 × lat(bertbase))..h ffn = max(0, h mhaw 1 +b1)w 2 +b2, (3).
3.2 overview.
, w 2 ∈ rdf ×dm.
, b1 ∈ rdfwhere w 1 ∈ rdm×dfand b2 ∈ rdm.
similarly, there are modules ofresidual connection and layer normalization ontop of ffn.
in the original transformer, df =4dmis assumed.
thus, we conclude that the conven-tional hyper-parameter setting follows the rule of{dq|k|v|o=dm, df =4dm}..a straightforward way to get the optimal archi-tecture is to enumerate all possible architectures.
however, it is infeasible because each trial involvesa time-consuming pre-training process.
therefore,we introduce one-shot nas to search αopt, asshown in the figure 2. the proposed method in-cludes three stages: (1) the one-shot learning toobtain superplm that can be used as the proxy for.
5148huawei confidential63attentionadd&normadd&normmulti-head attentionfeed-forward networks×ltransformerlayers①one-shot learning for superplm.evaluatorperf(·)|lat(·)evolvermut(·)proxy modelsarchitecturesperformance& efficiencyfurther training𝑾𝒗superplm training②search the optimal architecture.③further training sub-models.max input dim.max output dim.
sampled output dim.sampled input dim.sub-matrix (width-wise) extraction.𝑾𝒌𝑾𝒒𝑾𝒐𝑾𝟏𝑾𝟐𝜶𝐨𝐩𝐭𝑾𝒒|𝒌|𝒗|𝒐|1|2algorithm 1 batch-wise training for superplminput: all possible candidates a; training thread(gpu) number n ; large-scale unsuperviseddataset d; training epochs e. sample timesm per batch.
superplm parameters (θ)..output: trained superplm (θ).
1: for t = 1 → e do2:.
for batch in d do.
3:.
4:.
5:.
6:.
7:.
8:.
9:.
10:.
11:.
divide batch into n sub batchesdistribute sub batches to n threadsclear the gradientsfor m = 1 → m do.
sample n sub-models from adistribute sub-models to threadscalculate gradients in each thread.
end forupdate the θ with the average gradients.
end for.
12:13: end for.
few changes, we can use the original code to useautotinybert, as shown in appendix a..3.4 one-shot learning for superplm.
we employ the one-shot learning (brock et al.,2018; yu et al., 2020) to obtain a superplm whosesub-models can act as the proxy for plms trainedfrom scratch.
the conﬁgurations of superplm inthis work are lt=8, dm|q|k|v|o=768, and df =3072.
in each step of the one-shot learning, we trainseveral sub-models randomly sampled from su-perplm to make their performance close to themodels trained from scratch.
although the sam-pling/search space has been reduced to linear com-plexity, there are still more than 10m possible sub-structures in superplm (the details are shown inthe appendix b).
therefore, we introduce an ef-fective batch-wise training method to cover thesub-models as much as possible.
speciﬁcally, inparallel training, we ﬁrst divide each batch into mul-tiple sub-batches and distribute them to differentthreads as parallel training data.
then, we sampleseveral sub-models on each thread for training andmerge the gradients of all threads to update thesuperplm parameters.
we illustrate the trainingprocess in the algorithm 1..given a speciﬁc hyper-parameter setting α ={lt, dm, dq, dk, dv, df , do}, we get a sub-modelfrom superplm by the depth-wise and width-wise extraction.
speciﬁcally, we ﬁrst perform thedepth-wise extraction that extracts the ﬁrst lt trans-.
figure 3: mha sub-matrix extraction.
(a) means thatthe original matrix operation where we take four headsand three hidden vectors as an example.
white boxesrefer to the un-extracted parameters.
(b) means that weextract heads while keeping the dimension per head.
(c)means that we extract parameters from each head whilekeeping the head number as the original matrix..various architectures; (2) the search process for theoptimal hyper-parameters; (3) the further trainingwith the optimal architectures and correspondingsub-models.
in the following sections, we ﬁrst in-troduce the search space, which is the basis forthe one-shot learning and search process.
then wepresent the three stages respectively..3.3 search space.
from the section 2, we know that the conven-tional hyper-parameter setting is: {dq|k|v|o=dm,df =4dm}, which is widely-used in plms.
thearchitecture of a plm is parameterized as: α ={lt, dm, dq, dk, dv, df , do}, which is subjected tothe constraints {dq = dk, do = dm}.
let ltdenote the layer number and d∗ refer to differ-ent dimensions in the transformer layer.
we de-note the search space of lt and d∗ as alt andad∗ respectively.
the overall search space is:a = alt × adm|o × adq|k × adv × adf ..in this work, we only consider the case of identi-cal structure for each transformer layer, instead ofthe non-identical transformer (wang et al., 2020a)or other heterogeneous modules (xu et al., 2021)it has two advan-(such as convolution units).
tages: (1) it reduces an exponential search spaceof o((cid:81)|ad∗||alt |) to a linear search space of∗o((cid:81)|ad∗||alt|), greatly reducing the number of∗.
possible architectures in superplm training andthe exploration space in the search process.
it leadsto a more efﬁcient search process.
(2) an identicaland homogeneous structure is in fact more friendlyto hardware and software frameworks, e.g., hug-ging face transformer (wolf et al., 2020).
with a.
5149huawei confidential63(a)(b)𝑾𝑞|𝑘|𝑣×××=𝑸|𝑲|𝑽𝑯==(c)model.
speedup squad sst-2 mnli mrpc cola qnli qqp sts-b rte score avg..autotinybert-s1bert-s1pf-4l512d‡ (turc et al., 2019)pf-2l768d‡ (turc et al., 2019).
autotinybert-s2bert-s2nas-bert10† (xu et al., 2021)pf-2l512d‡ (turc et al., 2019)pf-6l256d‡ (turc et al., 2019).
autotinybert-s3bert-s3.
autotinybert-s4bert-s4nas-bert5† (xu et al., 2021)pf-6l128d‡ (turc et al., 2019).
7.2×7.1×7.1×7.0×.
15.7×14.8×12.7×12.8×13.3×.
20.2×20.1×.
31.0×31.3×32.0×28.2×.
83.381.581.771.1.
78.177.6-69.277.0.
75.873.7.
71.969.5-63.6.
89.488.989.488.8.
88.287.588.687.187.6.
86.886.4.
86.585.584.984.6.
79.478.478.576.5.
76.876.576.074.776.4.
76.475.0.
74.273.974.272.3.
85.581.382.879.6.
82.879.681.576.980.3.
80.481.3.
81.976.980.078.6.
42.435.835.226.7.
35.532.827.823.233.2.
33.231.2.
17.615.919.60.
87.386.487.084.9.
85.484.486.384.485.7.
85.084.0.
84.683.983.983.3.
88.888.288.688.1.
87.887.088.487.086.7.
87.687.1.
86.585.985.783.8.
87.586.787.486.6.
86.586.684.386.086.0.
86.785.8.
85.985.382.884.5.
66.366.465.767.1.
68.266.468.764.964.9.
66.463.8.
66.761.067.065.7.
78.376.576.874.8.
76.475.175.273.075.1.
75.374.3.
73.071.072.369.1.
78.977.177.474.4.
76.675.4-72.675.3.
75.474.3.
72.970.9-68.5.table 1: comparison between autotinybert and baselines in pre-training setting.
the results are evaluated onthe dev set of glue benchmark and squadv1.1.
we use the metric of matthews correlation for cola, f1 forsquadv1.1, pearson-spearman correlation for sts-b, and accuracy for other tasks.
we report the average scoreexcluding squad (score) in addition to the average score of all tasks (avg.).
the speedup is in terms of thebertbase inference speed and evaluated on a single cpu with a single input of 128 length.
pf-xlyd, the x and yrefer to the layer number and hidden dimension respectively.
†denotes that the results are taken from (xu et al.,2021) and ‡denotes that the results are obtained by ﬁne-tuning the released models..former layers from superplm, and then performthe width-wise extraction that extracts bottom-leftsub-matrices from original matrices.
for mha, weapply two strategies illustrated in figure 3 : (1)keep the dimension of each head same as super-plm, and extract some of the heads; (2) keep thehead number same as superplm, and extract sub-dimensions from each head.
the ﬁrst strategy isthe standard one and we use it for pre-training andthe second strategy is used for task-agnostic distil-lation because that attention-based distillation (jiaoet al., 2020b) requires the student model to havethe same head number as the teacher model..3.5 search process.
in the search process, we adopt an evolutionaryalgorithm (xie and yuille, 2017; jiao et al., 2020a),where evolver and evaluator interact with eachother to evolve better architectures.
our searchprocess is efﬁcient, as shown in the section 4.4..speciﬁcally, evolver ﬁrstly samples a generationof architectures from a. then evaluator extractsthe corresponding sub-models from superplm andranks them based on their performance on tasks ofsquad and mnli.
the architectures with the highperformance are chosen as the winning architec-tures and evolver performs the mutation mut(·)operation on the winning ones to produce a newgeneration of architectures.
this process is con-ducted repeatedly.
finally, we choose several ar-chitectures with the best performance for further.
training.
we use lat(·) to predict the latency ofthe candidates to ﬁlter out the candidates that donot meet the latency constraint.
lat(·) is built withthe method by wang et al.
(2020a), which ﬁrst sam-ples about 10k architectures from a and collectstheir inference time on target devices, and then usesa feed-forward network to ﬁt the data.
for moredetails of evolutionary algorithm, please refer toappendix c. note that we can use different meth-ods in search process, such as random search andmore advanced search, which is left as future work..3.6 further training.
the search process produces top several architec-tures, with which we extract these correspondingsub-models from superplm and continue trainingthem using the pre-training or kd objectives..4 experiment.
4.1 experimental setup.
dataset and fine tuning.
we conduct the experi-ments on the glue benchmark (wang et al., 2018)and squadv1.1 (rajpurkar et al., 2016).
forglue, we set the batch size to 32, choose the learn-ing rate from {1e-5, 2e-5, 3e-5} and choose theepoch number from {4, 5, 10}.
for squadv1.1,we set the batch size to 16, the learning rate to 3e-5 and the epoch number to 4. the details for alldatasets are displayed in appendix d..autotinybert.
both the one-shot and further.
5150model.
speedup squad sst-2 mnli mrpc¶ cola qnli qqp¶ sts-b rte score avg..dev results on glue and dev result on squad.
autotinybert-kd-s1bert-kd-s1mobileberttiny‡(sun et al., 2020).
autotinybert-kd-s2bert-kd-s2minilm-4l312d† (wang et al., 2020b)tinybert-4l312d†§ (jiao et al., 2020b).
autotinybert-kd-s3bert-kd-s3.
autotinybert-kd-s4bert-kd-s4.
test results on glue and dev result on squad.
autotinybert-kd-s1bert-3l-pkd‡ (sun et al., 2019)distilbert-4l‡ (sanh et al., 2019)tinybert-4l516d†§ (jiao et al., 2020b)minilm-4l516d† (wang et al., 2020b)mobileberttiny‡ (sun et al., 2020).
4.6×4.9×3.6*×.
9.0×9.8×9.8×9.8×.
10.7×11.7×.
17.0×17.0×.
4.6×4.1×3.0×4.9×4.9×3.6*×.
87.686.288.6.
84.682.582.181.0.
83.381.6.
78.777.4.
87.6-81.284.685.588.6.
91.489.791.6.
88.887.887.387.8.
88.386.5.
86.885.7.
90.687.591.488.290.091.7.
82.381.182.0.
79.477.978.376.9.
78.276.8.
76.075.4.
81.276.778.980.080.281.5.
88.587.986.7.
87.386.583.677.9.
85.882.5.
81.480.3.
88.980.782.486.387.287.9.
47.341.8-.
32.231.526.322.9.
29.127.6.
20.418.9.
44.7-32.827.939.146.7.
89.787.3-.
88.086.987.186.0.
87.485.6.
85.585.0.
87.484.785.285.686.589.5.
89.988.4-.
87.787.687.387.7.
87.486.5.
86.985.9.
70.568.168.569.170.068.9.
89.088.4-.
88.087.486.383.3.
86.786.2.
86.084.7.
85.1-76.183.083.480.1.
71.168.2-.
68.966.462.458.8.
66.464.9.
64.963.1.
64.858.254.161.563.765.1.
81.279.1-.
77.576.574.872.7.
76.274.6.
73.572.4.
76.7-71.272.775.076.4.
81.979.9-.
78.377.275.673.6.
77.075.4.
74.172.9.
77.9-72.374.076.277.8.table 2: comparison between autotinybert and baselines based on knowledge distillation.
‡ denotes thatthe results are taken from (sun et al., 2020) and † means the models trained using the released code or the re-implemented code with electrabase as the teacher model.
¶ means these tasks use accuracy for dev set and f1for test set respectively.
§ denotes the task-agnostic tinybert without task-speciﬁc distillation.
* means that thespeedup is different from the (sun et al., 2020), because it is evaluated on a pixel phone and not on server cpus.
-means that the results are missing in the original paper.
other information refer to the table 1..training use bookscorpus (zhu et al., 2015) andenglish wikipedia as training data.
the settings forone-shot training are: peak learning rate of 1e-5,warmup rate of 0.1, batch size of 256 and 5 runningepochs.
further training follows the same settingas the one-shot training except for the warmup rateof 0. in the batch-wise training algorithm 1, thethread number n is set to 16, the sample timesm per batch is set to 3, and epoch number e isset to 5. we train the superplm with an archi-tecture of {lt=8, dm|q|k|v|o=768, df =3072}.
in thesearch process, evolver performs 4 iterations with apopulation size of 25 and it chooses top three archi-tectures for further training.
for more details of thesampling/search space and evolutionary algorithm,please refer to appendix b and c..we train autotinybert in both ways of pre-training (devlin et al., 2019) and task-agnosticbert distillation (sun et al., 2020).
for task-agnostic distillation, we follow the ﬁrst stage oftinybert (jiao et al., 2020b) except that only thelast-layer loss is used, and electrabase (clarket al., 2019) is used as the teacher model..baselines.
for the pre-training baselines, we in-clude pf (pre-training + fine-tuning, proposedby turc et al.
(2019)), bert-s* (bert underseveral hyper-parameter conﬁgurations), and nas-bert (xu et al., 2021).
both pf and bert-s* follow the conventional setting rule of hyper-.
parameters.
bert-s* uses the training setting:peak learning rate of 1e-5, warmup rate of 0.1,batch size of 256 and 10 running epochs.
nas-bert searches the architecture built on the non-identical layer and heterogeneous modules.
forthe distillation baselines, we compare some typicalmethods, including distilbert, bert-pkd, tiny-bert, minilm, and mobilebert.
the ﬁrst fourmethods use the conventional architectures.
mo-bilebert is equipped with a bottleneck structureand a carefully designed balance between mhaand ffn.
we also consider bert-kd-s*, whichuse the same training setting of bert-s*, exceptfor the loss of knowledge distillation.
bert-kd-s* also uses electrabase as the teacher model..4.2 results and analysis.
the experiment is conducted under different la-tency constraints that are from 4× to 30× fasterthan the inference of bertbase.
the results of pre-training and task-agnostic distillation are shown inthe table 1 and table 2 respectively..we observe that in the settings of the pre-trainingand knowledge distillation, the performance gap ofdifferent models with similar inference time is ob-vious, which shows the necessity of architecture op-timization for efﬁcient plms.
in the table 1, someobservations are: (1) the architecture optimizationmethods of autotinybert and nas-bert out-perform both bert and pf that use the default.
5151figure 4: ablation study of one-shot superplm learning.
acc.
means the average score on squad and mnli.
the dashed line represents the function of y=x..trainingmethod.
stand-alone.
baseline (hat)ilsils + smeils + sme + ebl (ours).
squad mnlipairwisef1(%) acc.
(%) acc.(%).
71.2.
50.152.959.570.5.
76.3.
72.773.474.174.4.
100.
90.091.694.296.7.table 3: ablation study of superplm.
ils, sme andebl mean that the identical layer structure, mha sub-matrix extraction and effective batch-wise training..architecture hyper-parameters; (2) our method out-performs nas-bert that is built with the non-identical layer and heterogeneous modules, whichshows that the proposed method is effective for thearchitecture search of efﬁcient plms.
in the ta-ble 2, we observe that: (1) our method consistentlyoutperforms the conventional structure in all thespeedup constraints; (2) our method outperformsthe classical distillation methods (e.g., bert-pkd,distilbert, tinybert, and minilm) that use theconventional architecture.
moreover, autotiny-bert achieves comparable results with mobile-bert, and its inference speed is 1.5× faster..4.3 ablation study of one-shot learning.
we demonstrate the effectiveness of one-shot learn-ing by comparing the performance of one-shotmodel and stand-alone trained model on the givenarchitectures.
we choose 16 architectures and theircorresponding pf models6 as the evaluation bench-mark.
the pairwise accuracy is used as a metric toindicate the ranking correction between the archi-tectures under one-shot training and the ones understand-alone full training (luo et al., 2019) and itsformula is described in appendix e..we do the ablation study to analyze the effectof proposed identical layer structure (ils), mhasub-matrix extraction (sme) and effective batch-wise learning (ebl) on superplm learning.
more-.
6the ﬁrst.
16 models https://github.com/.
google-research/bert from 2l128d to 8l768d..version.
bert.
autotinybert.
speedup.
pre-training.
s1s2s3s4.
4-512-2048-8-5124-320-1280-5-3204-256-1024-4-2564-192-768-3-192.
5-564-1054-8-5124-396-624-6-3844-432-384-4-2563-320-608-4-256.
7.1/7.2×14.8/15.7×20.1/20.2×28.4/27.2×.
task-agnostic bert distillation.
kd-s1kd-s2†kd-s3kd-s4.
4-512-2048-12-5164-312-1200-12-3124-264-1056-12-2644-192-768-12-192.
5-564-1024-12-5285-324-600-12-3245-280-512-12-2764-256-480-12-192.
4.9/4.6×9.8/9.0×11.7/10.7×17.0/17.0×.
table 4: bert and autotinybert architectures un-der the different speedup constraints.
the architectureis formatted as “lt-dm|o-df -h-dq|k|v”.
we assume thatdq|k = dv in the experiment for the training and searchefﬁciency.
† means that we use the structure of tiny-bert and do not strictly follow the conventional rule..over, we introduce hat (wang et al., 2020a), as abaseline of one-shot learning.
hat focuses on thesearch space of non-identical layer structures.
theresults are displayed in table 3 and figure 4..it can be seen from the ﬁgure that compared withstand-alone trained models, the hat baseline hasa signiﬁcant performance gap, especially in smallsizes.
both ils and sme beneﬁt the one-shot learn-ing for large and medium-sized models.
whenfurther combined with ebl, superplm can ob-tain similar or even better results than stand-alonetrained models of small sizes and perform close tostand-alone trained models of big sizes.
the resultsof the table show that: (1) the proposed techniqueshave positive effects on superplm learning, andebl brings a signiﬁcant improvement on a chal-lenging task of squad; (2) superplm achieves ahigh pairwise accuracy of 96.7% which indicatesthat the proposed superplm can be a good proxymodel for the search process; (3) the performanceof superplm is still a little worse than the stand-alone trained model and we need to do the furthertraining to boost the performance..51524050607080one-shot acc.
(%)  (a) baseline(hat)5055606570758085stand-alone acc.
(%)4050607080one-shot acc.
(%)  (b) ils5055606570758085stand-alone acc.
(%)4050607080one-shot acc.
(%) (c) ils + sme5055606570758085stand-alone acc.
(%)5055606570758085one-shot acc.
(%) (d) our (ils + sme + ebl)5055606570758085stand-alone acc.
(%)method.
speedup.
search cost(gpu hours).
training cost(gpu hours).
bert-s5autotinybert-s5autotinybert-fast-s5.
9.9×10.8×10.3×.
015012.
580870290.avg..76.177.977.6.table 5: computation cost of different methods.
autotiny-bert and autotinybert-fast have 100 and 8 architectures(×1.5 v100 gpu hour) respectively to be tested in the searchprocess.
autotinybert performs further 5 epochs (×58v100 gpu hours) training for top three architectures, bertis trained from scratch with 10 epochs, and autotinybert-fast does the further training for one architecture.
we givemore information including the model architectures and de-tailed scores of all tasks in the appendix f..4.4 fast development of efﬁcient plm.
in this section, we explore an effective setting ruleof hyper-parameters based on the obtained architec-tures and also discuss the computation cost of thedevelopment of efﬁcient plm.
the conventionaland new architectures are displayed in table 4. weobserve that autotinybert follows an obviousrule (except the s3 model) in the speedup con-straints that are from 4× to 30×.
the rule is sum-marized as: {1.6dm ≤ df ≤ 1.9dm, 0.7dm ≤dq|k|v ≤ 1.0dm}..with the above rule, we propose a faster way tobuild efﬁcient plm, denoted as autotinybert-fast.
speciﬁcally, we ﬁrst obtain the candidates bythe rule, and then select αopt from the candidates.
we observe the fact that the candidates of the samelayer number seem to have similar shapes and weassume that they have similar performance.
there-fore, we only need to test one architecture at eachlayer number and choose the best one as αopt..to demonstrate the effectiveness of the pro-posed method, we evaluate these methods at anew speedup constraint of about 10× under thepre-training setting.
the results are shown in ta-ble 5. we ﬁnd autotinybert is efﬁcient and itsdevelopment time is twice that of the conventionalmethod (bert) and the result is improved by about1.8%.
autotinybert-fast achieves a competitivescore of 77.6 by only about 50% of bert trainingtime.
in addition to the proposed search methodand fast building rule, one reason for the high efﬁ-ciency of autotinybert is that the initializationof superplm helps the model to achieve 2× theconvergence speedup, as illustrated in figure 5..5 related work.
efﬁcient plms with tiny sizes.
there are twowidely-used methods for building efﬁcient plms:.
figure 5: learning curves of autotinybert andthe stand-alone trained model.
tfs means themodel trained from scratch.
autotinybert cansave 50% training time compared with the modeltrained from scratch..pre-training and model compression.
knowledgedistillation (kd) (hinton et al., 2015; romeroet al., 2014) is the most widely studied techniquein plm compression, which uses a teacher-studentframework.
the typical distillation studies includedistilbert (sanh et al., 2019), bert-pkd (sunet al., 2019), minilm (wang et al., 2020b), mobile-bert (sun et al., 2020), minibert (tsai et al.,2019) and etd (chen et al., 2021).
in addition tokd, the techniques of pruning (han et al., 2016;hou et al., 2020), quantization (shen et al., 2020;zhang et al., 2020; wang et al., 2020c) and pa-rameter sharing (lan et al., 2019) introduced forplm compression.
our method is orthogonal tothe building method of efﬁcient plm and is trainedunder the settings of pre-training and task-agnosticbert distillation, which can be used by direct ﬁne-tuning..nas for nlp.
nas is extensively studied in com-puter vision (tan and le, 2019; tan et al., 2020),but relatively little studied in the natural languageprocessing.
evolved transformer (so et al., 2019)and hat (wang et al., 2020a) search architec-ture for transformer-based neural machine transla-tion.
for bert distillation, adabert (chen et al.,2020) focuses on searching the architecture in theﬁne-tuning stage and relies on data augmentationto improve its performance.
schubert (khetanand karnin, 2020) obtains the optimal structuresof plm by a pruning method.
a work similar toours is nas-bert (xu et al., 2021).
it proposessome techniques to tackle the challenging exponen-tial search space of non-identical layer structureand heterogeneous modules.
our method adopts alinear search space and introduces several practicaltechniques for superplm training.
moreover, ourmethod is efﬁcient in terms of computation costand the obtained plms are easy to use..515301020304050607080training step (10k)1234567pre-training loss350k steps700k stepsautotinybert-s5autotinybert-s5-tfs6 conclusion.
we propose an effective and efﬁcient method au-totinybert to search for the optimal architecturehyper-parameters of efﬁcient plms.
we evaluatethe proposed method in the scenarios of both thepre-training and task-agnostic bert distillation.
the extensive experiments show that autotiny-bert can consistently outperform the baselinesunder different latency constraints.
furthermore,we develop a fast development rule for efﬁcientplms which can build an autotinybert modeleven with less training time of a conventional one..acknowledgments.
we thank all the anonymous reviewers for theirvaluable comments.
we thank mindspore7 for thepartial support of this work, which is a new deeplearning computing framework..references.
andrew brock, theo lim, jm ritchie, and nick weston.
2018. smash: one-shot model architecture searchthrough hypernetworks.
in iclr..tom b brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, et al.
2020. language models are few-shotlearners.
neurips..han cai, ligeng zhu, and song han.
2018. proxyless-nas: direct neural architecture search on target taskand hardware.
in iclr..cheng chen, yichun yin, lifeng shang, zhi wang,xin jiang, xiao chen, and qun liu.
2021. extractthen distill: efﬁcient and effective task-agnostic bertdistillation..daoyuan chen, yaliang li, minghui qiu, zhen wang,bofang li, bolin ding, hongbo deng, jun huang,wei lin, and jingren zhou.
2020. adabert: task-adaptive bert compression with differentiable neuralarchitecture search.
in ijcai..kevin clark, minh-thang luong, quoc v le, andchristopher d manning.
2019. electra: pre-trainingtext encoders as discriminators rather than generators.
in iclr..j. devlin, ming-wei chang, kenton lee, and kristinatoutanova.
2019. bert: pre-training of deep bidirec-tional transformers for language understanding.
innaacl-hlt..7mindspore.
https://www.mindspore.cn/.
kai han, yunhe wang, qiulin zhang, wei zhang, chun-jing xu, and tong zhang.
2020. model rubik’s cube:twisting resolution, depth and width for tinynets.
neurips..song han, huizi mao, and william j dally.
2016. deepcompression: compressing deep neural networkswith pruning, trained quantization and huffman cod-ing.
iclr..geoffrey e. hinton, oriol vinyals, and j. dean.
2015.distilling the knowledge in a neural network.
arxiv,abs/1503.02531..lu hou, zhiqi huang, lifeng shang, xin jiang, xiaochen, and qun liu.
2020. dynabert: dynamic bertwith adaptive width and depth.
neurips, 33..xiaoqi jiao, huating chang, yichun yin, lifeng shang,xin jiang, xiao chen, linlin li, fang wang, andqun liu.
2020a.
improving task-agnostic bert dis-tillation with layer mapping search.
arxiv preprintarxiv:2012.06153..xiaoqi jiao, yichun yin, lifeng shang, xin jiang, xiaochen, linlin li, fang wang, and qun liu.
2020b.
tinybert: distilling bert for natural language under-standing.
in emnlp: findings..ashish khetan and zohar karnin.
2020. schubert: opti-.
mizing elements of bert.
in acl..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2019. albert: a lite bert for self-supervised learningof language representations.
in iclr..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..renqian luo, tao qin, and enhong chen.
2019.balanced one-shot neural architecture optimization.
arxiv preprint arxiv:1909.10815..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in emnlp..adriana romero, nicolas ballas, samira ebrahimi ka-hou, antoine chassang, carlo gatta, and yoshuabengio.
2014. fitnets: hints for thin deep nets.
arxiv preprint arxiv:1412.6550..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..5154sheng shen, zhen dong, jiayu ye, linjian ma, zheweiyao, amir gholami, michael w mahoney, and kurtkeutzer.
2020. q-bert: hessian based ultra low pre-cision quantization of bert.
in aaai..scao, sylvain gugger, mariama drame, quentinlhoest, and alexander m. rush.
2020. transform-ers: state-of-the-art natural language processing.
inemnlp: system demonstrations..david so, quoc le, and chen liang.
2019. the evolved.
lingxi xie and alan yuille.
2017. genetic cnn..in.
transformer.
in icml..iccv..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
in emnlp-ijcnlp..jin xu, xu tan, renqian luo, kaitao song, li jian,tao qin, and tie-yan liu.
2021. task-agnostic andadaptive-size bert compression.
in openreview..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020. mobilebert: acompact task-agnostic bert for resource-limited de-vices.
in acl..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining for lan-guage understanding.
in neurips..jiahui yu, pengchong jin, hanxiao liu, gabriel ben-der, pieter-jan kindermans, mingxing tan, thomashuang, xiaodan song, ruoming pang, and quoc le.
2020. bignas: scaling up neural architecture searchwith big single-stage models.
in eccv..wei zhang, lu hou, yichun yin, lifeng shang, xiaochen, xin jiang, and qun liu.
2020. ternarybert:distillation-aware ultra-low bit bert.
in emnlp..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in iccv..mingxing tan and quoc le.
2019. efﬁcientnet: re-thinking model scaling for convolutional neural net-works.
in icml..mingxing tan, ruoming pang, and quoc v le.
2020.efﬁcientdet: scalable and efﬁcient object detection.
in cvpr..henry tsai, jason riesa, melvin johnson, naveen ari-vazhagan, xin li, and amelia archer.
2019. smalland practical bert models for sequence labeling.
inemnlp-ijcnlp..iulia turc, ming-wei chang, kenton lee, and kristinatoutanova.
2019. well-read students learn better:on the importance of pre-training compact models.
arxiv preprint arxiv:1908.08962..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in neurips..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel bowman.
2018. glue:a multi-task benchmark and analysis platform for nat-ural language understanding.
in emnlp workshopblackboxnlp: analyzing and interpreting neuralnetworks for nlp..hanrui wang, zhanghao wu, zhijian liu, han cai,ligeng zhu, chuang gan, and song han.
2020a.
hat:hardware-aware transformers for efﬁcient naturallanguage processing.
in acl..wenhui wang, furu wei, li dong, hangbo bao, nanyang, and ming zhou.
2020b.
minilm: deep self-attention distillation for task-agnostic compressionof pre-trained transformers..ziheng wang, jeremy wohlwend, and tao lei.
2020c.
instructured pruning of large language models.
emnlp..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan funtowicz,joe davison, sam shleifer, patrick von platen, clarama, yacine jernite, julien plu, canwen xu, teven le.
5155bert and baselines follow the same settings..batch learningsize.
rate.
epochs.
tasks.
squadsst-2mnlimrpccolaqnliqqpsts-brte.
163232323232323232.
3e-52e-53e-52e-51e-52e-52e-53e-52e-5.
44410101051010.table d.1: hyper-parameters used for ﬁne-tuning onglue benchmark and squad..e pairwise accuracy..we denote a set of architectures {α1, α2, ..., αn}as aeva and evaluate superplm on this set.
thepairwise accuracy is formulated as bellow:.
(cid:80).
α1∈ aeva, α2 ∈ aeva(cid:80).
1f (α1)≥f (α2)1s(α1)≥s(α2)1.,.
α1∈ aeva, α2 ∈ aeva.
(5)where 1 is the 0-1 indicator function, f (∗) and s(∗)refer to the performance of one-shot model andstand-alone trained model respectively..f more details for fast development of.
we present the detailed results and architecturehyper-parameters for fast development of efﬁcientplm in table f.1..a code modiﬁcations for.
autotinybert..we modify the original code8 to load autotiny-bert model and present the details of code mod-iﬁcations in the figure b.1.
we assume thatdq/k = dv, and more complicated setting is that dvcan be different with dq/k, we can do correspond-ing changes based on the given modiﬁcations..b search space of architecture.
hyper-parameters..we has trained two superplms with a architec-ture of {lt=8, dm/q/k/v=768, df =3072} to coverthe two scenarios of building efﬁcient plms (pre-training and task-agnostic bert distillation).
thesampling space in the superplm training is thesame as the search space in the search process, asshown in the table b.1.
it can be inferred fromthe table that the search spaces of the pre-trainingsetting and the knowledge distillation setting areabout 46m and 10m, respectively..variables.
search space.
superplm in pre-training.
ltdm/odfhdq/k/v.
ltdm/odfhdq/k/v.
[1,2,3,4,5,6,7,8][128,132,...,4k,...,764,768][128,132,...,4k,...,3068,3072][1,2,...,k,...,11,12]64h.
[1,2,3,4,5,6,7,8][128,132,...,4k,...,764,768][128,132,...,4k,...,3068,3072][12][180,192,...,12k,...,756,768].
table b.1: the search space for architecture hyper-parameters.
we assume that dq|k = dv in the exper-iment for the training and search efﬁciency..c evolutionary algorithm..we give a detailed description of evolutionary al-gorithm in algorithm 2..d hyper-parameters for fine-tuning..fine-tuning hyper-parameters of glue benchmarkand squad are displayed in table d.1.
autotiny-.
8https://github.com/huggingface/.
transformers.
superplm in knowledge distillation.
efﬁcient plm..5156class bertselfattention(nn.module):.
def __init__(self, config):.
### before modifications:self.attention_head_size = int(config.hidden_size /.
config.num_attention_heads).
### after modifications:try:.
qkv_size = config.qkv_size.
except:.
qkv_size = config.hidden_size.
self.attention_head_size = int(qkv_size / config.num_attention_heads).
class bertselfoutput(nn.module):def __init__(self, config):.
### before modifications:self.dense = nn.linear(config.hidden_size, config.hidden_size).
### after modifications:try:.
qkv_size = config.qkv_size.
except:.
qkv_size = config.hidden_size.
self.dense = nn.linear(qkv_size, config.hidden_size).
figure b.1: code modiﬁcations to load autotinybert..algorithm 2 the evolutionary algorithm.
1: input: the number of generations t = 4, the number of archtectures αs in each generation s = 25,.the mutation mut(∗) probability pm = 1/2, the exploration probability pe = 1/2..2: sample ﬁrst generation g1 from a, and evoluator produces its performance v1.
3: for t = 2, 3 · · · , t dogt ← {}4:while |gt| < s do.
5:.
sample one architecture: α with a russian roulette process on gt−1 and vt−1.
with probability pm, do mut(∗) for α.with probability pe, sample a new architecture from a.append the newly generated architectures into gt..6:.
7:.
8:.
9:.
10:.
end whileevaluator obtains vt for gt..11:12: end for13: output: output the αopt with best performance in the above process..model.
speedup squad sst-2 mnli mrpc cola qnli qqp sts-b rte score.
bert-s54−384−1536−6−384autotinybert-s55−450−636−6−384autotinybert-fast-s55−432−720−6−384.
9.3×10.8×10.3×.
78.579.780.0.
86.189.188.2.
76.878.377.9.
83.184.684.6.
35.539.037.7.
84.685.986.1.
87.588.288.0.
86.987.487.3.
65.768.768.7.
76.077.877.6.table f.1: detailed results for fast development of efﬁcient plm..5157