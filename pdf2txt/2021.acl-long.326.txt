psycholinguistic tripartite graph network for personality detection.
tao yang, feifan yang, haolan ouyang, xiaojun quan∗school of computer science and engineering, sun yat-sen university, china{yangt225,yangff6,ouyhlan}@mail2.sysu.edu.cnquanxj3@mail.sysu.edu.cn.
abstract.
most of the recent work on personality de-tection from online posts adopts multifariousdeep neural networks to represent the postsand builds predictive models in a data-drivenmanner, without the exploitation of psycholin-guistic knowledge that may unveil the connec-tions between one’s language usage and hispsychological traits.
in this paper, we proposea psycholinguistic knowledge-based tripartitegraph network, trignet, which consists of a tri-partite graph network and a bert-based graphinitializer.
the graph network injects struc-tural psycholinguistic knowledge from liwc,a computerized instrument for psycholinguis-tic analysis, by constructing a heterogeneoustripartite graph.
the graph initializer is em-ployed to provide initial embeddings for thegraph nodes.
to reduce the computationalcost in graph learning, we further proposea novel ﬂow graph attention network (gat)that only transmits messages between neigh-boring parties in the tripartite graph.
beneﬁt-ing from the tripartite graph, trignet can ag-gregate post information from a psychologicalperspective, which is a novel way of exploit-ing domain knowledge.
extensive experimentson two datasets show that trignet outperformsthe existing state-of-art model by 3.47 and2.10 points in average f1.
moreover, the ﬂowgat reduces the flops and memory mea-sures by 38% and 32%, respectively, in com-parison to the original gat in our setting..1.introduction.
personality detection from online posts aims toidentify one’s personality traits from the onlinetexts he creates.
this emerging task has attractedgreat interest from researchers in computationalpsycholinguistics and natural language processingdue to the extensive application scenarios such as.
∗ corresponding author..figure 1: an example of our tripartite graph.
the con-tent of post-1 and post-2 are “a lot of good advise forme.” and “love it!
thanks for sharing!”, respectively..personalized recommendation systems (yang andhuang, 2019; jeong et al., 2020), job screening(hiemstra et al., 2019) and psychological studies(goreis and voracek, 2019)..psychological research shows that the words peo-ple use in daily life reﬂect their cognition, emotion,and personality (gottschalk, 1997; golbeck, 2016).
as a major psycholinguistic instrument, linguisticinquiry and word count (liwc) (tausczik andpennebaker, 2010) divides words into psycholog-ically relevant categories (e.g., function, affect,and social as shown in figure 1) and is commonlyused to extract psycholinguistic features in con-ventional methods (golbeck et al., 2011; sumneret al., 2012).
nevertheless, most recent works (her-nandez and knight, 2017; jiang et al., 2020; kehet al., 2019; lynn et al., 2020; gjurkovi´c et al.,2020) tend to adopt deep neural networks (dnns)to represent the posts and build predictive modelsin a data-driven manner.
they ﬁrst encode eachpost separately and then aggregate the post rep-resentations into a user representation.
althoughnumerous improvements have been made over thetraditional methods, they are likely to suffer fromlimitations as follows.
first, the input of this task.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4229–4239august1–6,2021.©2021associationforcomputationallinguistics4229post-1post-2functionquantaffectsocialdrivespost nodeword nodecategory nodeof methanks love sharingitforalotgood adviceis usually a set of topic-agnostic posts, some ofwhich may contain few personality cues.
hence,directly aggregating these posts based on their con-textual representations may inevitably introducenoise.
second, personality detection is a typicaldata-hungry task since it is non-trivial to obtainpersonality tags, while dnns implicitly extract per-sonality cues from the texts and call for tremendoustraining data.
naturally, it is desirable to explic-itly introduce psycholinguistic knowledge into themodels to capture critical personality cues..motivated by the above discussions, we pro-pose a psycholinguistic knowledge-based tripartitegraph network, namely trignet, which consists ofa tripartite graph network to model the psycholin-guistic knowledge and a graph initializer using apre-trained language model such as bert (devlinet al., 2019) to generate the initial representationsfor all the nodes.
as illustrated in figure 1, a spe-ciﬁc tripartite graph is constructed for each user,where three heterogeneous types of nodes, namelypost, word, and category, are used to represent theposts of a user, the words contained both in hisposts and the liwc dictionary, and the psycholog-ically relevant categories of the words, respectively.
the edges are determined by the subordination be-tween word and post nodes as well as between wordand category nodes.
besides, considering that thereare no direct edges between homogeneous nodes(e.g., between post nodes) in the tripartite graph, anovel ﬂow gat is proposed to only transmit mes-sages between neighboring parties to reduce thecomputational cost and to allow for more effec-tive interaction between nodes.
finally, we regardthe averaged post node representation as the ﬁnaluser representation for personality classiﬁcation.
beneﬁting from the tripartite graph structure, theinteraction between posts is based on psycholog-ically relevant words and categories rather thantopic-agnostic context..we conduct extensive experiments on the kaggleand pandora datasets to evaluate our trignet model.
experimental results show that it achieves consis-tent improvements over several strong baselines.
comparing to the state-of-the-art model, sn+att(lynn et al., 2020), trignet brings a remarkableboost of 3.47 in averaged macro-f1 (%) on kaggleand a boost of 2.10 on pandora.
besides, thoroughablation studies and analyses are conducted anddemonstrate that the tripartite graph and the ﬂowgat play an irreplaceable role in the boosts of.
performance and decreases of computational cost.
our contributions are summarized as follows:.
• this is the ﬁrst effort to use a tripartite graphto explicitly introduce psycholinguistic knowl-edge for personality detection, providing anew perspective of using domain knowledge..• we propose a novel tripartite graph network,trignet, with a ﬂow gat to reduce the com-putational cost in graph learning..• we demonstrate the outperformance of ourtrignet over baselines as well as the effective-ness of the tripartite graph and the ﬂow gatby extensive studies and analyses..2 related work.
2.1 personality detection.
as an emerging research problem, text-based per-sonality detection has attracted the attention of bothnlp and psychological researchers (cui and qi,2017; xue et al., 2018; keh et al., 2019; jiang et al.,2020; tadesse et al., 2018; lynn et al., 2020)..traditional studies on this problem generally re-sort to feature-engineering methods, which ﬁrst ex-tracts various psychological categories via liwc(tausczik and pennebaker, 2010) or statistical fea-tures by the bag-of-words model (zhang et al.,2010).
these features are then fed into a classi-ﬁer such as svm (cui and qi, 2017) and xgboost(tadesse et al., 2018) to predict the personalitytraits.
despite interpretable features that can beexpected, feature engineering has such limitationsas it relies heavily on manually designed features.
with the advances of deep neural networks(dnns), great success has been achieved in person-ality detection.
tandera et al.
(2017) apply lstm(hochreiter and schmidhuber, 1997) on each postto predict the personality traits.
xue et al.
(2018)develop a hierarchical dnn, which depends onan attrcnn and a variant of inception (szegedyet al., 2017) to learn deep semantic features fromthe posts.
lynn et al.
(2020) ﬁrst encode each postby a gru (cho et al., 2014) with attention andthen pass the post representations to another gruto produce the whole contextual representations.
recently, pre-trained language models have beenapplied to this task.
jiang et al.
(2020) simply con-catenate all the utterances from a single user into adocument and encode it with bert (devlin et al.,2019) and roberta (liu et al., 2019).
gjurkovi´c.
4230et al.
(2020) ﬁrst encode each post by bert andthen use cnn (lecun et al., 1998) to aggregatethe post representations.
most of them focus onhow to obtain more effective contextual represen-tations, with only several exceptions that try to in-troduce psycholinguistic features into dnns, suchas majumder et al.
(2017) and xue et al.
(2018).
however, these approaches simply concatenate psy-cholinguistic features with contextual representa-tions, ignoring the gap between the two spaces..2.2 graph neural networks.
graph neural networks (gnns) can effectivelydeal with tasks with rich relational structures andlearn a feature representation for each node in thegraph according to the structural information.
re-cently, gnns have attracted wide attention in nlp(cao et al., 2019; yao et al., 2019; wang et al.,2020b,a).
among these research, graph construc-tion lies at the heart as it directly impacts the ﬁ-nal performance.
cao et al.
(2019) build a graphfor question answering, where the nodes are enti-ties, and the edges are determined by whether twonodes are in the same document.
yao et al.
(2019)construct a heterogeneous graph for text classiﬁ-cation, where the nodes are documents and words,and the edges depend on word co-occurrences anddocument-word relations.
wang et al.
(2020b) de-ﬁne a dependency-based graph by utilizing depen-dency parsing, in which the nodes are words, andthe edges rely on the relations in the dependencyparsing tree.
wang et al.
(2020a) present a het-erogeneous graph for extractive document summa-rization, where the nodes are words and sentences,and the edges depend on sentence-word relations.
inspired by the above successes, we construct atripartite graph, which exploits psycholinguisticknowledge instead of simple document-word orsentence-word relations and is expected to con-tribute towards psychologically relevant node rep-resentations..3 our approach.
personality detection can be formulated as a multi-document multi-label classiﬁcation task (lynnet al., 2020; gjurkovi´c et al., 2020).
formally,each user has a set p = {p1, p2, .
.
.
, pr} of posts.
let pi= [wi,1, wi,2, .
.
.
, wi,s] be the i-th post with swords, where pi can be viewed as a document.
thegoal of this task is to predict t personality traitsy = (cid:8)yt(cid:9)tt=1 for this user based on p , where yt ∈.
{0, 1} is a binary variable..figure 2 presents the overall architecture of theproposed trignet, which consists of a tripartitegraph network and a bert-based graph initial-izer.
the former module aims to explicitly infusepsycholinguistic knowledge to uncover personalitycues contained in the posts and the latter to en-code each post and provide initial embeddings forthe tripartite graph nodes.
in the following sub-sections, we detail how the two modules work infour steps: graph construction, graph initialization,graph learning, and merge & classiﬁcation..3.1 graph construction.
as a major psycholinguistic analysis instrument,liwc (tausczik and pennebaker, 2010) divideswords into psychologically relevant categories andis adopted in this paper to construct a heteroge-neous tripartite graph for each user..as shown in the right part of figure 2, theconstructed tripartite graph g= (v, e) containsthree heterogeneous types of nodes, namelypost, word, and category, where v denotes theset of nodes and e represents the edges be-tween nodes.
speciﬁcally, we deﬁne v=vp ∪vw ∪ vc, where vp=p = {p1, p2, · · · , pr} de-notes r posts, vw= {w1, w2, · · · , wm} denotes munique psycholinguistic words that appear bothin the posts p and the liwc dictionary, andvc= {c1, c2, · · · , cn} represents n psychologicallyrelevant categories selected from liwc.
the undi-rected edge eij between nodes i and j indicatesword i either belongs to a post j or a category j..the interaction between posts in the tripar-tite graph is implemented by two ﬂows: (1)“p↔w↔p”, which means posts interact via theirshared psycholinguistic words (e.g., “p1↔w1↔p2”(2)as shown by the red lines in figure 2);“p↔w↔c↔w↔p”, which suggests that posts in-teract by words that share the same category (e.g.,“p1↔w2↔c2↔w3↔p2” as shown by the greenlines in figure 2).
hence, the interaction betweenposts is based on psychologically relevant wordsor categories rather than topic-agnostic context..3.2 graph initialization.
as shown in the left part of figure 2, we employbert (devlin et al., 2019) to obtain the initial em-beddings of all the nodes.
bert is built upon themulti-layer transformer encoder (vaswani et al.,2017), which consists of a word embedding layer.
4231figure 2: overall architecture of our trignet, which consists of two modules: (1) a tripartite graph network (right)to inject psycholinguistic knowledge and (2) a bert-based graph initializer (left) to initialize node embeddings..and 12 transformer layers.1post node embedding the representations at the12-th layer of bert are usually used to representan input sequence.
this may not be appropriatefor our task as personality is only weakly related tothe higher order semantic features of posts, makingit risky to rely solely on the ﬁnal layer representa-tions.
in our experiments (section 5.4), we ﬁnd thatthe representations at the 11-th and 10-th layers arealso useful for this task.
therefore, we utilize therepresentations at the last three layers to initializethe post node embeddings.
formally, the represen-tations xjpi of the i-th post at the j-th layer can beobtained by:.
pi=bertj ([cls, wi,1, · · · , wi,m, sep]) (1)xj.
where “cls” and “sep” are special tokens todenote the start and end of an input sentence,respectively, and bertj (·) denotes the repre-sentation of the special token “cls” at the j-thlayer.
in this way, we obtain the representations(cid:2)x10(cid:3)t ∈ r3×d of the last three layers,where d is the dimension of each representation.
we then apply layer attention (peters et al., 2018)to collapse the three representations into a singlevector xpi:.
pi , x11.
pi , x12pi.
xpi =.
αjxjpi.
12(cid:88).
j=10.
(2).
where αj are softmax-normalized layer-speciﬁcweights to be learned.
consequently, we can obtain.
1“bert-base-uncased” is used in this study..a set of post representations for the given r postsof a user xp = [xp1, xp2, · · · , xpr ]t ∈ rr×dword node embedding bert applies word-piece (wu et al., 2016) to split words, which alsocuts out-of-vocabulary words into small pieces.
thus, we obtain the initial node embedding of eachword in vw by considering two cases: (1) if theword is not out of vocabulary, we directly look upthe bert embedding layer to obtain its embedding;(2) if the word is out of vocabulary, we use the aver-aged embedding of its pieces as its initial node em-bedding.
the initial word node embeddings are rep-resented as xw=[xw1, xw2, · · · , xwm]t ∈ rm×d.
category node embedding the liwc2 dictio-nary divides words into 9 main categories and64 subcategories.3 empirically, subcategories suchas pronouns, articles, and prepositions are nottask-related.
besides, our initial experiments showthat excessive introduction of subcategories inthe tripartite graph makes the graph sparse andmakes the learning difﬁcult, resulting in perfor-mance deterioration.
for these reasons, we se-lect all 9 main categories and the 6 personal-concern subcategories for our study.
particularly,the 9 main categories function, affect, social,cognitive processes, perceptual processes, bio-logical processes, drives, relativity, and infor-mal language, and 6 personal-concern subcate-gories work, leisure, home, money, religion, anddeath are used as our category nodes.
then, wereplace the “unused” tokens in bert’s vocab-.
2http://liwc.wpengine.com/3details of the categories are listed in appendix..42321p2prp…1w2wmw…1c2cnc…3wtripartite graph networkgraph constructiongraph learning (flow gat)merge & classification……mwx2wx1wxpost node embeddingsword node embeddings…1cxcategory node embeddings2cxncxgraph initialization[cls]1,1w1,sw[sep]transformer layer 1transformer layer 11…transformer layer 12post 1post r…bert-based graph initializerembeding layertransformer layer 10layer attentionmpx2px1px…y…ulary by the 15 category names and look up thebert embedding layer to generate their embed-dings xc=[xc1, xc2, · · · , xcn]t ∈ rn×d..3.3 graph learning.
graph attention network (gat) (veliˇckovi´c et al.,2018) can be applied over a graph to calculate theattention weight of each edge and update the noderepresentations.
however, unlike the traditionalgraph in which any two nodes may have edges,the connections in our tripartite graph only occurbetween neighboring parties (i.e., vw ↔ vp andvw ↔ vc), as shown in figure 3. therefore, apply-ing the original gat over our tripartite graph willlead to unnecessary computational costs.
inspiredby wang et al.
(2020a), we propose a ﬂow gat forthe tripartite graph.
particularly, considering thatthe interaction between posts in our tripartite graphcan be accounted for by two ﬂows “p↔w↔p” and“p↔w↔c↔w↔p”, we design a message passingmechanism that only transmits message by the twoﬂows in the tripartite graph..formally, given a constructed tripartite graphg = (v, e), where v = vp∪vw∪vc, and the initialnode embeddings x=xp∪xw∪xc, we computeh(l+1)as the hidden states ofpvp, vw and vc at the (l +1)-th layer.
the ﬂow gatlayer is deﬁned as follows:.
w , and h(l+1).
, h(l+1).
c.(l+1)hp.(l+1).
(l+1).
,h.w ,h.c = fgat.
(l).
h.p ,h.(l).
(l)w ,hc.(cid:16).
(cid:17).
p = xp, h(1).
(3)where h(1)c = xc.
the function fgat (·) is implemented by the twoﬂows:.
w = xw, and h(1).
ˆh(l)h(l).
w←p=mp.
p←w,p = mp.
h(l)h(l)h(l).
c←w,p = mp.
(cid:16).
w←c,w,p = mp.
p←w,c,w,p = mp.
(cid:17).
w←p.
(cid:16).
(cid:17).
w , h(l)h(l)p(cid:16)p , ˆh(l)h(l)(cid:17).
h(l)c , ˆh(l)w←p(cid:16) ˆh(l)w←p, h(l)(cid:16)p , h(l)h(l).
(cid:17).
(cid:17).
c←w,p.
w←c,w,p.
p = mean.
w = mean.
h(l+1)h(l+1)h(l+1)c.= h(l).
c←w,p.
(cid:16)h(l)(cid:16) ˆh(l).
p←w,p, h(l)w←p, h(l).
p←w,c,w,p.
(cid:17).
w←c,w,p.
(cid:17).
(4).
(5).
(6).
where ← means the message is transmitted fromthe right nodes to the left nodes, mean (·) is themean pooling function, and mp (·) represents the.
figure 3: comparison of adjacent matrices between thetraditional graph (left) and our tripartite graph (right).
edges in the traditional graph may occur in any twonodes, while it only occurs between neighboring par-ties in our tripartite graph..(cid:104).
(cid:16).
(cid:17).
h(l).
we take mp.
message passing function.
eq.
(4) and eq.
(5) il-lustrate that message is transmitted by the ﬂows“p↔w↔p” and p↔w↔c↔w↔p, respectively.
w , h(l)in eq.
(4) as an ex-pample to introduce the massage passing function,w1, h(l)h(l)w2, · · · , h(l)where h(l)are used aswm(cid:105)(cid:104)p1 , h(l)p2 , · · · , h(l)h(l)the attention query and h(l)p =pr(cid:16)(cid:17)w , h(l)p.as the key and value.
mpcan be de-composed into three steps.
first, it calculates theattention weight βkij between node i in vw and itsneighbor node j in vp at the k-th head:.
w =.
h(l).
(cid:105).
(cid:16).
zkij = σ.wkz.
(cid:104)wk.
(cid:105)(cid:17).
ph(l)pj.
wi||wkwh(l)(cid:17)(cid:16).
βkij =.
exp.
zkij(cid:16).
(cid:80).
exp.
q∈ni.
(cid:17).
zkiq.
(7).
(8).
z, wk.
w and wk.
where σ is the leakyrelu activation function,wkp are learnable weights, nimeans that the neighbor nodes of node i in vp,and || is the concatenation operation.
second, theupdated hidden state ˜h(l)wi is obtained by a weightedcombination of its neighbor nodes in vp:.
˜h(l)wi =.
k||k=1.
tanh.
.
.
(cid:88).
j∈ni.
.
ijwkβk.
vh(l)pj.
 (9).
where k is the number of heads and wkv is a learn-able weight matrix.
third, noting that the abovesteps do not take the information of node i itselfinto account and to avoid gradient vanishing, weintroduce a residual connection to produce the ﬁnalupdated node representation:.
ˆh(l)wi = h(l).
wi + ˜h(l)wi.
(10).
4233wcpwcptraditional graphour tripartite graph3.4 merge & classiﬁcation.
dataset traits train (60%) valid (20%) test (20%).
after l layers of iteration, we obtain the ﬁnal noderepresentations h(l)=h(l)c .
then,we merge all post node representations h(l)p viamean pooling to produce the user representation:.
w ∪h(l).
p ∪h(l).
kaggle.
(cid:16)(cid:104).
(cid:105)(cid:17).
pandora.
u = mean.
p1 , h(l)h(l).
p2 , · · · , h(l)pr.
(11).
i/es/nt/fp/j.
i/es/nt/fp/j.
4011 / 1194610 / 44782410 / 27953096 / 2109.
4278 / 1162727 / 48303549 / 18913211 / 2229.
1326 / 409222 / 1513791 / 9441063 / 672.
1427 / 386208 / 16051120 / 6931043 / 770.
1339 / 396248 / 1487780 / 9551082 / 653.
1437 / 377210 / 16041182 / 6321056 / 758.finally, we employ t softmax-normalized lineartransformations to predict t personality traits.
forthe t-th personality trait, we compute:.
p (cid:0)yt(cid:1) = softmax (cid:0)uwt.
u + btu.
(cid:1).
(12).
u is a trainable weight matrix and bt.
where wtu isa bias term.
the objective function of our trignetmodel is deﬁned as:.
j (θ) =.
1v.v(cid:88).
t(cid:88).
v=1.
t=1.
(cid:2)−yt.
v log p (cid:0)yt.
v|θ(cid:1)(cid:3).
(13).
where v is the number of training samples, t isthe number of personality traits, ytv is the true la-bel for the t-th trait, and p(ytv|θ) is the predictedprobability for this trait under parameters θ..4 experiments.
table 1: statistics of the kaggle and pandora datasets..following previous works (majumder et al., 2017;jiang et al., 2020), we delete words that matchany personality label to avoid information leaks.
the macro-f1 metric is adopted to evaluate theperformance in each personality trait since bothdatasets are highly imbalanced, and average macro-f1 is used to measure the overall performance.
weshufﬂe the datasets and split them in a 60-20-20proportion for training, validation, and testing, re-spectively.
according to our statistics, there arerespectively 20.45 and 28.01 liwc words on aver-age in each post in the two datasets, and very fewposts (0.021/0.002 posts per user) are presentedas disconnected nodes in the graph.
we show thestatistics of the two datasets in table 1..in this section, we introduce the datasets, baselines,and settings of our experiments..4.2 baselines.
4.1 datasets.
we choose two public mbti datasets for evalua-tions, which have been widely used in recent stud-ies (tadesse et al., 2018; hernandez and knight,2017; majumder et al., 2017; jiang et al., 2020;gjurkovi´c et al., 2020).
the kaggle dataset4 is col-lected from personalitycafe,5 where people sharetheir personality types and discussions about health,behavior, care, etc.
there are a total of 8675 usersin this dataset and each user has 45-50 posts.
pan-dora6 is another dataset collected from reddit,7where personality labels are extracted from shortdescriptions of users with mbti results to intro-duce themselves.
there are dozens to hundreds ofposts for each of the 9067 users in this dataset..the traits of mbti include introversion vs. ex-troversion (i/e), sensing vs. intuition (s/n), thinkvs. feeling (t/f), and perception vs. judging (p/j)..4kaggle.com/datasnaek/mbti-type5http://personalitycafe.com/forum6https://psy.takelab.fer.hr/datasets/7https://www.reddit.com/.
the following mainstream models are adopted asbaselines to evaluate our model:svm (cui and qi, 2017) and xgboost (tadesseet al., 2018): support vector machine (svm) orxgboost is utilized as the classiﬁer with featuresextracted by tf-idf and liwc from all posts.
bilstm (tandera et al., 2017): bi-directionallstm (hochreiter and schmidhuber, 1997) isﬁrstly employed to encode each post, and thenthe averaged post representation is used for userrepresentation.
glove (pennington et al., 2014) isemployed for the word embeddings.
bert (keh et al., 2019): the ﬁne-tuned bertis ﬁrstly used to encode each post, and then meanpooling is performed over the post representationsto generate the user representation.
attrcnn: this model adopts a hierarchical struc-ture, in which a variant of inception (szegedy et al.,2017) is utilized to encode each post and a cnn-based aggregator is employed to obtain the userrepresentation.
besides, it considers psycholinguis-tic knowledge by concatenating the liwc featureswith the user representation..4234methods.
i/e.
s/n.
p/j.
average.
i/e.
s/n.
p/j.
average.
kagglet/f.
svm (cui and qi, 2017)xgboost (tadesse et al., 2018)bilstm (tandera et al., 2017)bert (keh et al., 2019)attrcnn (xue et al., 2018)sn+attn (lynn et al., 2020).
53.3456.6757.8264.6559.7465.43.
47.7552.8557.8757.1264.0862.15.
76.7275.4269.9777.9578.7778.05.
63.0365.9457.0165.2566.4463.92.pandorat/f.
44.7445.9948.0156.6048.5556.98.
46.9248.9352.0148.7156.1954.78.
64.6263.5163.4864.7064.3960.95.
56.3255.5556.2156.0757.2654.81.
53.1553.5054.9356.5256.6056.88.
58.98.
60.2162.7260.6766.2467.2567.39.
70.86.trignet(our).
69.54.
67.17.
79.06.
67.69.
56.69.
55.57.
66.38.
57.27.table 2: overall results of trignet and baselines in macro-f1(%) score, where the best results are shown in bold..sn+attn (lynn et al., 2020): as the latest model,sn+attn employs a hierarchical attention network,in which a gru (cho et al., 2014) with word-levelattention is used to encode each post and anothergru with post-level attention is used to generatethe user representation..to make a fair comparison between the baselinesand our model, we replace the post encoders inattrcnn and sn+attn with the pre-trained bert..4.3 training detailswe implement our trignet in pytorch8 and trainit on four nvidia rtx 2080ti gpus.
adam(kingma and ba, 2014) is utilized as the optimizer,with the learning rate of bert set to 2e-5 and ofother components set to 1e-3.
we set the maxi-mum number of posts, r, to 50 and the maximumlength of each post, s, to 70, considering the limitof available computational resources.
after tuningon the validation dataset, we set the dropout rate to0.2 and the mini-batch size to 32. the maximumnumber of nodes, r + m + n, is set to 500 for kag-gle and 970 for pandora, which cover 98.95% and97.07% of the samples, respectively.
moreover, thetwo hyperparameters, the numbers of ﬂow gatlayers l and heads k, are searched in {1, 2, 3}and {1, 2, 4, 6, 8, 12, 16, 24}, respectively, and thebest choices are l = 1 and k = 12. the reasonsfor l = 1 are likely twofold.
first, our ﬂow gatcan already realize the interactions between nodeswhen l = 1, whereas the vanilla gat needs tostack 4 layers.
second, after trying l = 2 andl = 3, we ﬁnd that they lead to slight performancedrops compared to that of l = 1..5 results and analyses.
in this section, we report the overall results andprovide thorough analyses and discussions..8https://pytorch.org/.
5.1 overall results.
the overall results are presented in table 2, fromwhich our observations are described as follows.
first, the proposed trignet consistently surpassesthe other competitors in f1 scores, demonstrat-ing the superiority of our model on text-basedpersonality detection with state-of-the-art perfor-mance.
speciﬁcally, compared with the existingstate of the art, sn+attn, trignet achieves 3.47and 2.10 boosts in average f1 on the kaggle andpandora datasets, respectively.
second, comparedwith bert, a basic module utilized in trignet,trignet yields 4.62 and 2.46 improvements in av-erage f1 on the two datasets, verifying that thetripartite graph network can effectively capture thepsychological relations between posts.
third, com-pared with attrcnn, another method of leverag-ing psycholinguistic knowledge, trignet outper-forms it with 3.61 and 2.38 increments in averagef1 on the two datasets, demonstrating that our so-lution that injects psycholinguistic knowledge viathe tripartite graph is more effective.
besides, theshallow models svm and xgboost achieve com-parable performance to the non-pre-trained modelbilstm, further showing that the words peopleused are important for personality detection..5.2 ablation study.
we conduct an ablation study of our trignet modelon the kaggle dataset by removing each componentto investigate their contributions.
table 3 shows theresults which are categorized into two groups..in the ﬁrst group, we investigate the contribu-tions of the network components.
we can see thatremoving the ﬂow “p↔w↔c↔w↔p” deﬁned ineq.
(5) results in higher performance declines thanremoving the ﬂow “p↔w↔p” deﬁned in eq.
(4),implying that the category nodes are helpful tocapture personality cues from the texts.
besides,removing the layer attention mechanism also leads.
4235ave. f1(%) ∆(%).
gat.
params flops memory ave.f1.
model.
trignet.
w/o “p↔w↔p”w/o“p↔w↔c↔w↔p”w/o layer attention.
w/o functionw/o perceptual processesw/o workw/o homew/o drivesw/o relativityw/o cognitive processesw/o biological processesw/o leisurew/o religionw/o moneyw/o informal languagew/o socialw/o deathw/o affect.
70.86.
70.1369.5669.88.
70.4470.2870.2870.0870.0369.9169.6969.6869.6769.5869.5669.5169.3269.3068.60.
-.
0.73↓1.3↓0.98↓.
0.42↓0.58↓0.58↓0.78↓0.83↓0.95↓1.17↓1.18↓1.19↓1.28↓1.30↓1.35↓1.54↓1.56↓2.26↓.
table 3: results of ablation study in average macro-f1on the kaggle dataset, where “w/o” means removal ofa component from the original trignet, and “∆” indi-cates the corresponding performance change..to considerable performance degradation..in the second group, we investigate the contribu-tion of each category node.
the results, sorted byscores of decrease from small to large, demonstratethat the introduction of every category node is ben-eﬁcial to trignet.
among these category nodes,the affect is shown to be the most crucial one to ourmodel, as the average macro-f1 score drops mostsigniﬁcantly after it is removed.
this implies thatthe affect category reﬂects one’s personality obvi-ously.
similar conclusions are reported by depueand collins (1999) and zhang et al.
(2019).
inaddition, the function node is the least impactfulcategory node.
the reason could be that functionalwords reﬂect pure linguistic knowledge and areweakly connected to personality..originalflow(our).
1.8m1.8m.
5.5g3.4g.
7.8gb5.3gb.
69.6970.86.table 4: analysis of the computational cost for orig-inal gat and ﬂow gat on the kaggle dataset.
themetrics include the number of parameters (params)and ﬂoating-point operations per second (flops) ofgat as well as memory size (memory) and the aver-age macro-f1 (ave.f1) of whole model on the kaggledataset..effect, we compare it with vanilla gat (as illus-trated in the left part of figure 3).
the results arereported in table 4, from which we can observethat ﬂow gat successfully reduces the computa-tional cost in flops and memory by 38% and32%, respectively, without extra parameters intro-duced.
besides, ﬂow gat is superior to vanillagat when the number of layers is 1. the causeis that the former can already capture adequate in-teractions between nodes with one layer, while thelatter has to stack four layers to achieve this..we also compare our trignet with the vanillabert in terms of the computational cost.
theresult show that the ﬂow gat takes about 1.14%more flops than the vanilla bert(297.3g)..5.4 layer attention analysis.
this study adopts layer attention (peters et al.,2018) as shown in eq.
(2) to produce initial em-beddings for post nodes.
to show which layersare more useful, we conduct a simple experimenton the two datasets by using all the 12 layer rep-resentations of bert and visualize the attentionweight of each layer.
as plotted in figure 4, weﬁnd that the attention weights from layers 10 to 12are signiﬁcantly greater than that of the rest layerson both datasets, which explains why the last threelayers are chosen for layer attention in our model..6 conclusion.
5.3 analysis of the computational cost.
in this work we propose a ﬂow gat to reduce thecomputational cost of vanilla gat.
to show its.
in this work, we proposed a novel psycholinguisticknowledge-based tripartite graph network, trignet,for personality detection.
trignet aims to introduce.
figure 4: visualization of layer attention weights.
the last three layers supply with more information for this task..4236(cid:20)(cid:21)(cid:22)(cid:23)(cid:24)(cid:25)(cid:26)(cid:27)(cid:28)(cid:20)(cid:19)(cid:20)(cid:20)(cid:20)(cid:21)(cid:47)(cid:68)(cid:92)(cid:72)(cid:85)(cid:86)(cid:46)(cid:68)(cid:74)(cid:74)(cid:79)(cid:72)(cid:51)(cid:68)(cid:81)(cid:71)(cid:82)(cid:85)(cid:68)(cid:19)(cid:17)(cid:19)(cid:22)(cid:28)(cid:19)(cid:17)(cid:19)(cid:26)(cid:20)(cid:19)(cid:17)(cid:19)(cid:25)(cid:19)(cid:17)(cid:19)(cid:21)(cid:22)(cid:19)(cid:17)(cid:19)(cid:22)(cid:21)(cid:19)(cid:17)(cid:19)(cid:25)(cid:27)(cid:19)(cid:17)(cid:19)(cid:24)(cid:25)(cid:19)(cid:17)(cid:19)(cid:24)(cid:20)(cid:19)(cid:17)(cid:19)(cid:23)(cid:24)(cid:19)(cid:17)(cid:19)(cid:23)(cid:23)(cid:19)(cid:17)(cid:19)(cid:27)(cid:19)(cid:17)(cid:19)(cid:26)(cid:19)(cid:17)(cid:19)(cid:22)(cid:20)(cid:19)(cid:17)(cid:19)(cid:23)(cid:22)(cid:19)(cid:17)(cid:19)(cid:28)(cid:19)(cid:17)(cid:19)(cid:26)(cid:20)(cid:19)(cid:17)(cid:19)(cid:24)(cid:26)(cid:19)(cid:17)(cid:19)(cid:23)(cid:24)(cid:19)(cid:17)(cid:19)(cid:19)(cid:17)(cid:20)(cid:19)(cid:17)(cid:21)(cid:19)(cid:17)(cid:20)(cid:23)(cid:19)(cid:17)(cid:20)(cid:28)(cid:19)(cid:17)(cid:20)(cid:27)(cid:19)(cid:17)(cid:20)(cid:23)(cid:19)(cid:17)(cid:20)(cid:27)(cid:19)(cid:17)(cid:20)(cid:27)structural psycholinguistic knowledge from liwcvia constructing a tripartite graph, in which interac-tions between posts are captured through psycho-logically relevant words and categories rather thansimple document-word or sentence-word relations.
besides, a novel ﬂow gat that only transmits mes-sages between neighboring parties was developedto reduce the computational cost.
extensive experi-ments and analyses on two datasets demonstrate theeffectiveness and efﬁciency of trignet.
this workis the ﬁrst effort to leverage a tripartite graph to ex-plicitly incorporate psycholinguistic knowledge forpersonality detection, providing a new perspectivefor exploiting domain knowledge..acknowledgments.
the paper was fully supported by the programfor guangdong introducing innovative and en-trepreneurial teams (no.2017zt07x355)..ethical statement.
this study aims to develop a technical method toincorporate psycholinguistic knowledge into neu-ral models, rather than creating a privacy-invadingtool.
we worked within the purview of accept-able privacy practices and strictly followed the datausage policy.
the datasets used in this study areall from public sources with all user informationanonymized.
the assessment results of the pro-posed model are sensitive and should be sharedselectively and subject to the approval of the in-stitutional review board (irb).
any research orapplication based on this study is only allowed forresearch purposes, and any attempt to use the pro-posed model to infer sensitive user characteristicsfrom publicly accessible data is strictly prohibited.
to get the code, researchers need to sign an ethicalstatement and explain the purpose clearly..references.
yu cao, meng fang, and dacheng tao.
2019. bag:bi-directional attention entity graph convolutionalnetwork for multi-hop reasoning question answering.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 357–362..kyunghyun cho, bart van merri¨enboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerschwenk, and yoshua bengio.
2014.learningphrase representations using rnn encoder–decoder.
for statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734..surveybrandon cui and calvin qi.
2017.forof machineper-languagetypeon-http://cs229.stanford.edu/proj2017/ﬁnal-.
analysisnaturalsonalityline:reports/5242471.pdf (accessed on 26 may 2021)..learning methodsfor mbtiavailable.
prediction..processing.
richard a depue and paul f collins.
1999. neurobi-ology of the structure of personality: dopamine, fa-cilitation of incentive motivation, and extraversion.
behavioral and brain sciences, 22(3):491–517..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..matej gjurkovi´c, mladen karan, iva vukojevi´c, mi-haela boˇsnjak, and jan ˇsnajder.
2020. pandora talks:arxivpersonality and demographics on reddit.
preprint arxiv:2004.04460..jennifer golbeck, cristina robles, and karen turner.
2011. predicting personality with social media.
inchi’11 extended abstracts on human factors incomputing systems, pages 253–262..jennifer ann golbeck.
2016. predicting personalityfrom social media text.
ais transactions on repli-cation research, 2(1):2..andreas goreis and martin voracek.
2019. a system-atic review and meta-analysis of psychological re-search on conspiracy beliefs: field characteristics,measurement instruments, and associations with per-sonality traits.
frontiers in psychology, 10:205..louis a gottschalk.
1997. the unobtrusive measure-ment of psychological states and traits.
text analysisfor the social sciences: methods for drawing sta-tistical inferences from texts and transcripts, pages117–129..r hernandez and is knight.
2017. predicting myers-bridge type indicator with text classiﬁcation.
in pro-ceedings of the 31st conference on neural infor-mation processing systems, long beach, ca, usa,pages 4–9..annemarie mf hiemstra, janneke k oostrom, evaderous, alec w serlie, and marise ph born.
2019.applicant perceptions of initial job candidate screen-ing with asynchronous job interviews: does per-sonality matter?
journal of personnel psychology,18(3):138..4237sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..chi-seo jeong, jong-yong lee, and kye-dong jung.
2020. adaptive recommendation system for tourisminterna-by personality type using deep learning.
tional journal of internet, broadcasting and com-munication, 12(1):55–60..hang jiang, xianzhe zhang, and jinho d choi.
2020.automatic text-based personality recognition onmonologues and multiparty dialogues using atten-tive networks and contextual embeddings (studentin proceedings of the aaai conferenceabstract).
on artiﬁcial intelligence, volume 34, pages 13821–13822..sedrick scott keh, i cheng, et al.
2019. myers-briggs personality classiﬁcation and personality-speciﬁc language generation using pre-trained lan-guage models.
arxiv preprint arxiv:1907.06333..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..yann lecun, l´eon bottou, yoshua bengio, and patrickhaffner.
1998. gradient-based learning applied todocument recognition.
proceedings of the ieee,86(11):2278–2324..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..veronica lynn, niranjan balasubramanian, and h an-drew schwartz.
2020. hierarchical modeling foruser personality prediction: the role of message-level attention.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 5306–5316..navonil majumder, soujanya poria, alexander gel-bukh, and erik cambria.
2017. deep learning-baseddocument modeling for personality detection fromtext.
ieee intelligent systems, 32(2):74–79..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..on machine learning and applications, volume 2,pages 386–393.
ieee..christian szegedy, sergey ioffe, vincent vanhoucke,and alexander alemi.
2017.inception-v4,inception-resnet and the impact of residual connec-tions on learning.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 31..michael m tadesse, hongfei lin, bo xu, and liangyang.
2018. personality predictions based on userbehavior on the facebook social media platform.
ieee access, 6:61959–61969..tommy tandera, derwin suhartono, rini wongso,yen lina prasetio, et al.
2017. personality predic-tion system from facebook users.
procedia com-puter science, 116:604–611..yla r tausczik and james w pennebaker.
2010. thepsychological meaning of words: liwc and comput-erized text analysis methods.
journal of languageand social psychology, 29(1):24–54..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..danqing wang, pengfei liu, yining zheng, xipengqiu, and xuanjing huang.
2020a.
heterogeneousgraph neural networks for extractive document sum-marization.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6209–6219..kai wang, weizhou shen, yunyi yang, xiaojun quan,and rui wang.
2020b.
relational graph attentionnetwork for aspect-based sentiment analysis.
in pro-ceedings of the 58th annual meeting of the associ-ation for computational linguistics, pages 3229—-3238..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of naacl-hlt, pages2227–2237..di xue, lifa wu, zheng hong, shize guo, liang gao,zhiyong wu, xiaofeng zhong, and jianshan sun.
2018. deep learning-based personality recognitionfrom text posts of online social networks.
appliedintelligence, 48(11):4232–4246..chris sumner, alison byers, rachel boochever, andgregory j park.
2012. predicting dark triad person-ality traits from twitter usage and a linguistic analy-sis of tweets.
in 2012 11th international conference.
hsin-chang yang and zi-rui huang.
2019. miningpersonality traits from social messages for gamerecommender systems.
knowledge-based systems,165:157–168..4238liang yao, chengsheng mao, and yuan luo.
2019.graph convolutional networks for text classiﬁcation.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 7370–7377..le zhang, songyou peng, and stefan winkler.
2019.persemon: a deep network for joint analysis of ap-parent personality, emotion and their relationship.
ieee transactions on affective computing..yin zhang, rong jin, and zhi-hua zhou.
2010. un-derstanding bag-of-words model: a statistical frame-international journal of machine learningwork.
and cybernetics, 1(1-4):43–52..a categories of liwc.
as shown in figure 5, a total of 73 categories andsubcategories are deﬁned in the liwc-2015 dictio-nary.
there are 9 main categories: function, affect,social, cognitive processes, perceptual processes,biological processes, drives, relativity, and in-formal language, in which 20 standard linguisticsubcategories are included in the function cate-gory and 44 psychological-relevant subcategoriesare deﬁned in the rest 8 categories..figure 5: detailed division of categories in the liwc-2015 dictionary..4239► function words□ pronouns● personal pronouns◊ i◊ we◊ you◊ she / he◊ they● impersonal pronouns□ articles□ prepositions□ auxiliary verbs□ adverbs□ conjunctions□ negations□ verbs□ adjectives□ comparisons□ interrogatives□ numbers□ quantifiers► affect□ positive emotions□ negative emotions● anx● anger● sad► social□ family□ friends□ female□ male► cognitive processes□ insight□ causal□ discrepancies□ tentative□ certainty□ differentiation► perceptual processes□ see□ hear□ feel► biological processes□ body□ health□ sexual□ ingest► drives□ affiliation□ achievement□ power□ reward□ risk□ past focus□ present focus□ future focus► relativity□ motion□ space□ time□ work□ leisure□ home□ money□ religion□ death► informal language□ swear□ netspeak□ assent□ nonfluencies□ filler words► : the 1st level□ : the 2nd level● : the 3rd level◊ : the 4th levelpersonal-concern