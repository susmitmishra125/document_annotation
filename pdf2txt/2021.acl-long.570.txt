including signed languages in natural language processing.
kayo yin1, amit moryossef2, julie hochgesang3, yoav goldberg2,4, malihe alikhani51language technologies institute, carnegie mellon university2bar-ilan university3department of linguistics, gallaudet university4allen institute for ai5school of computing and information, university of pittsburghkayoy@cs.cmu.edu, amitmoryossef@gmail.comjulie.hochgesang@gallaudet.edu, yogo@cs.biu.ac.il, malihe@pitt.edu.
abstract.
signed languages are the primary means ofcommunication for many deaf and hard ofhearing individuals.
since signed languagesexhibit all the fundamental linguistic proper-ties of natural language, we believe that toolsand theories of natural language processing(nlp) are crucial towards its modeling.
how-ever, existing research in sign language pro-cessing (slp) seldom attempt to explore andleverage the linguistic organization of signedlanguages.
this position paper calls on thenlp community to include signed languagesas a research area with high social and sci-entiﬁc impact.
we ﬁrst discuss the linguisticproperties of signed languages to consider dur-ing their modeling.
then, we review the limi-tations of current slp models and identify theopen challenges to extend nlp to signed lan-guages.
finally, we urge (1) the adoption ofan efﬁcient tokenization method; (2) the devel-opment of linguistically-informed models; (3)the collection of real-world signed languagedata; (4) the inclusion of local signed languagecommunities as an active and leading voice inthe direction of research..1.introduction.
natural language processing (nlp) has revolu-tionized the way people interact with technologythrough the rise of personal assistants and machinetranslation systems, to name a few.
however, thevast majority of nlp models require a spoken lan-guage input (speech or text), thereby excludingaround 200 different signed languages and up to 70million deaf people1 from modern language tech-nologies..1according to world federation of the deaf.
https://wfdeaf.org/our-work/.
figure 1: evolution of the number of publications refer-ring to sign language in its title from computer sciencevenues and in the acl anthology.
publications in com-puter science are extracted from the semantic scholararchive (ammar et al., 2018)..throughout history, deaf communities foughtfor the right to learn and use signed languages, aswell as for the recognition of signed languages as le-gitimate languages (§2).
indeed, signed languagesare sophisticated communication modalities thatare at least as capable as spoken languages in allmanners, linguistic and social.
however, in a pre-dominantly oral society, deaf people are constantlyencouraged to use spoken languages through lip-reading or text-based communication.
the exclu-sion of signed languages from modern languagetechnologies further suppresses signing in favor ofspoken languages.
this disregards the preferencesof the deaf communities who strongly prefer tocommunicate in signed languages both online andfor in-person day-to-day interactions, among them-selves and when interacting with spoken languagecommunities (padden and humphries, 1988; glick-man and hall, 2018).
thus, it is essential to make.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7347–7360august1–6,2021.©2021associationforcomputationallinguistics734720002002200420062008201020122014201620182020year0100200300400# of publicationsall computer scienceacl anthologysigned languages accessible..to date, a large amount of research on sign lan-guage processing (slp) has been focused on thevisual aspect of signed languages, led by the com-puter vision (cv) community, with little nlp in-volvement (figure 1).
this is not unreasonable,given that a decade ago, we lacked the adequatecv tools to process videos for further linguisticanalyses.
however, like spoken languages, signedlanguages are fully-ﬂedged systems that exhibitall the fundamental characteristics of natural lan-guages (§3), and current slp techniques fail to ad-dress or leverage the linguistic structure of signedlanguages (§4).
this leads us to believe that nlptools and theories are crucial to process signedlanguages.
given the recent advances in cv, thisposition paper argues that now is the time to in-corporate linguistic insight into signed languagemodeling..signed languages introduce novel challenges fornlp due to their visual-gestural modality, simul-taneity, spatial coherence, and lack of written form.
by working on signed languages, the communitywill gain a more holistic perspective on naturallanguages through a better understanding of howmeaning is conveyed by the visual modality andhow language is grounded in visuospatial concepts..moreover, slp is not only an intellectually ap-pealing area but also an important research areawith a strong potential to beneﬁt signing communi-ties.
examples of beneﬁcial applications enabledby signed language technologies include better doc-umentation of endangered sign languages; educa-tional tools for sign language learners; tools forquery and retrieval of information from signedlanguage videos; personal assistants that react tosigned languages; real-time automatic sign lan-guage interpretations, and more.
needless to say,in addressing this research area, researchers shouldwork alongside and under the direction of deafcommunities, and to the beneﬁt of the signing com-munities’ interest above all (harris et al., 2009)..after identifying the challenges and open prob-lems to successfully include signed languages innlp (§5), we emphasize the need to: (1) developa standardized tokenization method of signed lan-guages with minimal information loss for its mod-eling; (2) extend core nlp technologies to signedlanguages to create linguistically-informed mod-els; (3) collect signed language data of sufﬁcientsize that accurately represents the real world; (4).
involve and collaborate with the deaf communitiesat every step of research..2 background and related work.
2.1 history of signed languages and deaf.
culture.
over the course of modern history, spoken lan-guages were dominant so much so that signed lan-guages struggled to be recognized as languagesin their own right and educators developed mis-conceptions that signed language acquisition mayhinder the development of speech skills.
for ex-ample, in 1880, a large international conferenceof deaf educators called the second internationalcongress on education of the deaf banned teach-ing signed languages, favoring speech therapy in-stead.
it was not until the seminal work on amer-ican sign language (asl) by stokoe (1960) thatsigned languages started gaining recognition asnatural, independent, and well-deﬁned languages,which then inspired other researchers to furtherexplore signed languages as a research area.
never-theless, antiquated notions that deprioritized signedlanguages continue to do harm and subjects manyto linguistic neglect (humphries et al., 2016).
sev-eral studies have shown that deaf children raisedsolely with spoken languages do not gain enoughaccess to a ﬁrst language during their critical pe-riod of language acquisition (murray et al., 2020).
this language deprivation can lead to life-long con-sequences on the cognitive, linguistic, socioemo-tional, and academic development of the deaf (hallet al., 2017)..signed languages are the primary languages ofcommunication for the deaf2 and are at the heart ofdeaf communities.
failing to recognize signed lan-guages as fully-ﬂedged natural language systemsin their own right has had harmful effects in thepast, and in an increasingly digitized world, thenlp community has an important responsibilityto include signed languages in its research.
nlpresearch should strive to enable a world in whichall people, including the deaf, have access to lan-guages that ﬁt their lived experience..2.2 sign language processing in the.
literature.
jaffe (1994); ong and ranganath (2005); parton.
2when capitalized, “deaf” refers to a community of deafpeople who share a language and a culture, whereas the lower-case “deaf” refers to the audiological condition of not hearing..7348(2006) survey early works in slp that were mostlylimited to using sensors to capture ﬁngerspellingand isolated signs, or use rules to synthesize signsfrom spoken language text, due to the lack of ade-quate cv technology at the time to process videos.
this paper will instead focus on more recent vision-based and data-driven approaches that are non-intrusive and more powerful.
the introduction ofa continuous signed language benchmark dataset(forster et al., 2014; cihan camg¨oz et al., 2018),coupled with the advent of deep learning for visualprocessing, lead to increased efforts to recognizesigned expressions from videos.
recent surveyson slp mostly review these different approachesfor sign language recognition developed by the cvcommunity (koller, 2020; rastgoo et al., 2020;adaloglou et al., 2020)..meanwhile, signed languages have remained rel-atively overlooked in nlp literature (figure 1).
bragg et al.
(2019) argue the importance of an in-terdisciplinary approach to slp, raising the impor-tance of nlp involvement among other disciplines.
we take this argument further by diving into the lin-guistic modeling challenges for signed languagesand providing a roadmap of open questions to beaddressed by the nlp community, in hopes of stim-ulating efforts from an nlp perspective towardsresearch on signed languages..3 sign language lingusitics.
signed languages consist of phonological, morpho-logical, syntactic, and semantic levels of structurethat fulﬁll the same social, cognitive, and com-municative purposes as other natural languages.
while spoken languages primarily channel the oral-auditory modality, signed languages use the visual-gestural modality, relying on the face, hands, bodyof the signer, and the space around them to createdistinctions in meaning.
we present the linguisticfeatures of signed languages3 that must be takeninto account during their modeling..phonology signs are composed of minimal unitsthat combine manual features such as hand conﬁg-uration, palm orientation, placement, contact, pathmovement, local movement, as well as non-manualfeatures including eye aperture, head movement,and torso positioning4 (liddell and johnson, 1989;.
3we mainly refer to asl, where most sign language re-.
search has been conducted, but not exclusively..4in this work, we focus on visual signed languages ratherthan tactile systems such as pro-tactile asl which deafblind.
johnson and liddell, 2011; brentari, 2011; sandler,2012).
in both signed and spoken languages, not allpossible phonemes are realized, and inventories oftwo languages’ phonemes/features may not overlapcompletely.
different languages are also subject torules for the allowed combinations of features..simultaneity though an asl sign takes abouttwice as long to produce than an english word,the rates of transmission of information betweenthe two languages are similar (bellugi and fischer,1972).
one way signed languages compensate forthe slower production rate of signs is through si-multaneity: signed languages make use of multiplevisual cues to convey different information simul-taneously(sandler, 2012).
for example, the signermay produce the sign for ’cup’ on one hand whilesimultaneously pointing to the actual cup with theother to express “that cup”.
similarly to tone inspoken languages, the face and torso can convey ad-ditional affective information (liddell et al., 2003;johnston and schembri, 2007).
facial expressionscan modify adjectives, adverbs, and verbs; a headshake can negate a phrase or sentence; eye directioncan help indicate referents..referencing the signer can introduce referentsin discourse either by pointing to their actual loca-tions in space, or by assigning a region in the sign-ing space to a non-present referent and by pointingto this region to refer to it (rathmann and mathur,2011; schembri et al., 2018).
signers can alsoestablish relations between referents grounded insigning space by using directional signs or em-bodying the referents using body shift or eye gaze(dudis, 2004; liddell and metzger, 1998).
spatialreferencing also impacts morphology when the di-rectionality of a verb depends on the location of thereference to its subject and/or object (de beuzeville,2008; fenlon et al., 2018): for example, a direc-tional verb can move from the location of its sub-ject and ending at the location of its object.
whilethe relation between referents and verbs in spo-ken language is more arbitrary, referent relationsare usually grounded in signed languages.
the vi-sual space is heavily exploited to make referencingclear..another way anaphoric entities are referencedin sign language is by using classiﬁers or depictingsigns (supalla, 1986; wilcox and hafer, 2004; roy,2011) that help describe the characteristics of the.
americans sometimes prefer..7349referent.
classiﬁers are typically one-handed signsthat do not have a particular location or movementassigned to them, or derive features from mean-ingful discourse (liddell et al., 2003), so they canbe used to convey how the referent relates to otherentities, describe its movement, and give more de-tails.
for example, to tell about a car swerving andcrashing, one might use the hand classiﬁer for avehicle, move it to indicate swerving, and crash itwith another entity in space..to quote someone other than oneself, signers per-form role shift (cormier et al., 2015), where theymay physically shift in space to mark the distinc-tion, and take on some characteristics of the peoplethey are representing.
for example, to recount adialogue between a taller and a shorter person, thesigner may shift to one side and look up when tak-ing the shorter person’s role, shift to the other sideand look down when taking the taller person’s role..fingerspelling fingerspelling is a result of lan-guage contact between a signed language and asurrounding spoken language written form (bat-tison, 1978; wilcox, 1992; brentari and padden,2001; patrie and johnson, 2011).
a set of manualgestures correspond with a written orthography orphonetic system.
fingerspelling is often used toindicate names or places or new concepts from thespoken language but often have become integratedinto the signed languages themselves as anotherlinguistic strategy (padden, 1998; montemurro andbrentari, 2018)..4 current state of slp.
in this section, we present the existing methods,resources, and tasks in slp, and discuss their limi-tations to lay the ground for future research..4.1 representations of signed languages.
representation is a signiﬁcant challenge for slp,as unlike spoken languages, signed languages haveno widely adopted written form.
figure 2 illus-trates each signed language representation we willdescribe below..videosare the most straightforward representa-tion of a signed language and can amply incorpo-rate the information conveyed through sign.
onemajor drawback of using videos is their high dimen-sionality: they usually include more informationthan needed for modeling, and are expensive tostore, transmit, and encode.
as facial features are.
essential in sign, anonymizing raw videos also re-mains an open problem, limiting the possibilityof making these videos publicly available (isard,2020)..posesreduce the visual cues from videos toskeleton-like wireframe or mesh representing thelocation of joints.
while motion capture equip-ment can often provide better quality pose estima-tion, it is expensive and intrusive, and estimatingpose from videos is the preferred method currently(pishchulin et al., 2012; chen et al., 2017; caoet al., 2019; g¨uler et al., 2018).
compared to videorepresentations, accurate poses are lower in com-plexity and anonymized, while observing relativelylow information loss.
however, they remain a con-tinuous, multidimensional representation that is notadapted to most nlp models..written notation systemsrepresent signs as dis-crete visual features.
some systems are writtenlinearly and others use graphemes in two dimen-sions.
while various universal (sutton, 1990; prill-witz and zienert, 1990) and language-speciﬁc no-tation systems (stokoe jr, 2005; kakumasu, 1968;bergman, 1979) have been proposed, no writingsystem has been adopted widely by any sign lan-guage community, and the lack of standard hindersthe exchange and uniﬁcation of resources and ap-plications between projects.
figure 2 depicts twouniversal notation systems: signwriting (sutton,1990), a two-dimensional pictographic system, andhamnosys (prillwitz and zienert, 1990), a linearstream of graphemes that was designed to be read-able by machines..glossing is the transcription of signed languagessign-by-sign, where every sign has a unique identi-ﬁer.
while various sign language corpus projectshave provided gloss annotation guidelines (meschand wallin, 2015; johnston and de beuzeville,2016; konrad et al., 2018), again, there is no sin-gle agreed-upon standard.
linear gloss annotationsare also an imprecise representation of signed lan-guage: they do not adequately capture all infor-mation expressed simultaneously through differentcues (i.e.
body posture, eye gaze) or spatial re-lations, which leads to an inevitable informationloss up to a semantic level that affects downstreamperformance on slp tasks (yin and read, 2020b)..7350figure 2: representations of an american sign language phrase with video frames, pose estimations, signwriting,hamnosys and glosses.
english translation: “what is your name?”.
4.2 existing sign language resources.
now, we introduce the different formats of re-sources and discuss how they can be used forsigned language modeling..for.
bilingual dictionariessigned language(mesch and wallin, 2012; fenlon et al., 2015;crasborn et al., 2016; gutierrez-sigut et al., 2016)map a spoken language word or short phrase toa signed language video.
one notable dictionaryis, spreadthesign5 is a parallel dictionary con-taining around 23,000 words with up to 41 differ-ent spoken-signed language pairs and more than500,000 videos in total.
while dictionaries mayhelp create lexical rules between languages, theydo not demonstrate the grammar or the usage ofsigns in context..fingerspelling corpora usually consist ofvideos of words borrowed from spoken languagesthat are signed letter-by-letter.
they can besynthetically created (dreuw et al., 2006) or minedfrom online resources (shi et al., 2018, 2019).
however, they only capture one aspect of signedlanguages..isolated sign corpora are collections of anno-tated single signs.
they are synthesized (eblinget al., 2018; huang et al., 2018; sincan and keles,2020; hassan et al., 2020) or mined from onlineresources (vaezi joze and koller, 2019; li et al.,2020), and can be used for isolated sign languagerecognition or for contrastive analysis of minimalsigning pairs (imashev et al., 2020).
however, likedictionaries, they do not describe relations between.
5https://www.spreadthesign.com/.
signs nor do they capture coarticulation during sign-ing, and are often limited in vocabulary size (20-1000 signs).
continuous sign corpora contain parallel se-quences of signs and spoken language.
availablecontinuous sign corpora are extremely limited, con-taining 4-6 orders of magnitude fewer sentencepairs than similar corpora for spoken language ma-chine translation (arivazhagan et al., 2019).
more-over, while automatic speech recognition (asr)datasets contain up to 50,000 hours of recordings(pratap et al., 2020), the largest continuous sign lan-guage corpus contain only 1,150 hours, and only 50of them are publicly available (hanke et al., 2020).
these datasets are usually synthesized (databases,2007; crasborn and zwitserlood, 2008; ko et al.,2019; hanke et al., 2020) or recorded in studio con-ditions (forster et al., 2014; cihan camg¨oz et al.,2018), which does not account for noise in real-lifeconditions.
moreover, some contain signed inter-pretations of spoken language rather than naturally-produced signs, which may not accurately repre-sent native signing since translation is now a partof the discourse event..availability unlike the vast amount and diversityof available spoken language resources that allowvarious applications, signed language resources arescarce and currently only support translation andproduction.
unfortunately, most of the signed lan-guage corpora discussed in the literature are eithernot available for use or available under heavy re-strictions and licensing terms.
signed languagedata is especially challenging to anonymize due tothe importance of facial and other physical features.
7351yourhamnosysumbrellasignwritingasl glossnamewhatpose streamvideo stream107 framesin signing videos, limiting its open distribution, anddeveloping anonymization with minimal informa-tion loss, or accurate anonymous representations isa promising research problem..4.3 sign language processing tasks.
the cv community has mainly led the researchon slp so far to focus on processing the visualfeatures in signed language videos.
as a result,current slp methods do not fully address the lin-guistic complexity of signed languages.
we surveycommon slp tasks and limitations of current meth-ods by drawing on linguistic theories of signedlanguages..detection sign language detection is the binaryclassiﬁcation task to determine whether a signedlanguage is being used or not in a given video frame.
while recent detection models (borg and camilleri,2019; moryossef et al., 2020) achieve high perfor-mance, we lack well-annotated data that includeinterference and distractions with non-signing in-stances for proper evaluation.
a similar task inspoken languages is voice activity detection (vad)(sohn et al., 1999; ramırez et al., 2004), the de-tection of when a human voice is used in an au-dio signal.
however, as vad methods often relyon speech-speciﬁc representations such as spectro-grams, they are not always applicable to videos..identiﬁcation sign language identiﬁcation clas-siﬁes which signed language is being used in agiven video automatically.
existing works utilizethe distribution of phonemes (gebre et al., 2013)or activity maps in signing space (monteiro et al.,2016) to identify the signed language in videos.
however, these methods only rely on low-levelvisual features, while signed languages have sev-eral distinctive features on a linguistic level, suchas lexical or structural differences (mckee andkennedy, 2000; kimmelman, 2014; ferreira-brito,1984; shroyer and shroyer, 1984) which have notbeen explored for this task..segmentation segmentation consists of detect-ing the frame boundaries for signs or phrases invideos to divide them into meaningful units.
cur-rent methods resort to segmenting units looselymapped to signed language units (santemiz et al.,2009; farag and brock, 2019; bull et al., 2020),and does not leverage reliable linguistic predictorsof sentence boundaries such as prosody in signedlanguages (i.e.
pauses, sign duration, facial expres-.
sions, eye apertures) (sandler, 2010; ormel andcrasborn, 2012)..recognition sign language recognition (slr)detects and label signs from a video, either onisolated (imashev et al., 2020; sincan and keles,2020) or continuous (cui et al., 2017; camg¨ozet al., 2018, 2020b) signs.
though some previousworks have referred to this as “sign language trans-lation”, recognition merely determines the associ-ated label of each sign, without handling the syntaxand morphology of the signed language (padden,1988) to create a spoken language output.
instead,slr has often been used as an intermediate stepduring translation to produce glosses from signedlanguage videos..translation sign language translation (slt)commonly refers to the translation of signed lan-guage to spoken language.
current methods eitherperform translation with glosses (camg¨oz et al.,2018, 2020b; yin and read, 2020a,b; moryossefet al., 2021) or on pose estimations and sign ar-ticulators from videos (ko et al., 2019; camg¨ozet al., 2020a), but do not, for instance, handle spa-tial relations and grounding in discourse to resolveambiguous referents..production sign language production consistsof producing signed language from spoken lan-guage and often use poses as an intermediate rep-resentation to overcome challenges in animation.
to overcome the challenges in generating videosdirectly, most efforts use poses as an intermedi-ate representation, with the goal of either usingcomputer animation or pose-to-video models toperform video production.
earlier methods gen-erate and concatenate isolated signs (stoll et al.,2018, 2020), while more recent methods (saun-ders et al., 2020b,a; zelinka and kanis, 2020; xiaoet al., 2020) autoregressively decode a sequence ofposes from an input text.
due to the lack of suitableautomatic evaluation methods of generated signs,existing works resort to measuring back-translationquality, which cannot accurately capture the qualityof the produced signs nor its usability in real-worldsettings.
a better understanding of how distinc-tions in meaning are created in signed languagemay help develop a better evaluation method..73525 towards including signed languagesin natural language processing.
the limitations in the design of current slp modelsoften stem from the lack of exploring the linguisticpossibilities of signed languages.
we therefore in-vite the nlp community to collaborate with the cvcommunity, for their expertise in visual processing,and signing communities and sign linguists, fortheir expertise in signed languages and the livedexperiences of signers, in researching slp.
we be-lieve that ﬁrst, the development of known tasksin the standard nlp pipeline to signed languageswill help us better understand how to model them,as well as provide valuable tools for higher-levelapplications.
although these tasks have been thor-oughly researched for spoken languages, they poseinteresting new challenges in a different modality.
we also emphasize the need for real-world data todevelop such methods, and a close collaborationwith signing communities to have an accurate un-derstanding of how signed language technologiescan beneﬁt signers, all the while respecting thedeaf community’s ownership of signed languages..5.1 building nlp pipelines.
although signed and spoken languages differ inmodality, we argue that as both express the syntax,semantics, and pragmatics of natural languages,fundamental theories of nlp can and should beextended to signed languages.
nlp applicationsoften rely on low-level tools such as tokenizersand parsers, so we invite more research efforts onthese core nlp tasks that often lay the foundationof other applications.
we also discuss what con-siderations should be taken into account for theirdevelopment to signed languages and raise openquestions that should be addressed..tokenization the vast majority of nlp meth-ods require a discrete input.
to extend nlp tech-nologies to signed languages, we must ﬁrst andforemost be able to develop adequate tokenizationtools that maps continuous signed language videosto a discrete, accurate representation with mini-mal information loss.
while existing slp systemsand datasets often use glosses as discrete lexicalunits of signed phrases, this poses three signiﬁcantproblems: (1) linear, single-dimensional glossescannot fully capture the spatial constructions ofsigned languages, which downgrades downstreamperformance (yin and read, 2020b); (2) glossesare language-speciﬁc and requiring new glossing.
models for each language is impractical given thescarcity of resources; (3) glosses lack standardacross corpora which limits data sharing and addssigniﬁcant overhead in modeling..we thus urge the adoption of an efﬁcient, univer-sal, and standardized method for tokenization ofsigned languages, all the while considering: how dowe deﬁne lexical units in signed languages?
(john-ston and schembri, 1999; johnston, 2010) to whatdegree can phonological units of signed languagesbe mapped to lexical units?
should we model thearticulators of signs separately or together?
whatare the cross-linguistic phonological differencesto consider?
to what extent can ideas used in au-tomatic speech recognition be applied to signedlanguages?.
syntactic analysis part-of-speech (pos) tag-ging and syntactic parsing are fundamental to un-derstand the meaning of words in context.
yet, nosuch linguistic tools for automatic syntactic anal-yses exist.
to develop such tools, we must ﬁrstdeﬁne to what extent pos tagging and syntacticparsing for spoken languages also generalize tosigned languages - do we need a new set of posand dependency tags for signed languages?
howare morphological features expressed?
what arethe annotation guidelines to create datasets on syn-tax?
can we draw on linguistic theories to designfeatures and rules that perform these tasks?
arethere typologically similar spoken languages forsome signed languages we can perform transferlearning with?.
named entity recognition (ner) recogniz-ing named entities and ﬁnding relationships be-tween them are highligh important in informa-tion retrieval and classiﬁcation.
named entitiesin signed languages can be produced by a ﬁnger-spelled sequence, a sign, or even through mouthingof the name while the referent is introduced throughpointing.
bleicken et al.
(2016) attempt nerin german sign language (dgs) to performanonymization, but only do so indirectly, by eitherperforming ner on the gold dgs gloss annota-tions and german translations or manually on thevideos.
we instead propose ner in a fully au-tomated fashion while considering, what are thevisual markers of named entities?
how are theyintroduced and referenced?
how are relationshipsbetween them established?.
7353coreference resolution resolving coreferenceis crucial for language understanding.
in signed lan-guages, present referents, where the signer explic-itly points to the entity in question, are relativelyunambiguous.
in contrast, non-present referentsand classiﬁers are heavily grounded in the signingspace, so good modeling of the spatial coherence insign language is required.
evidence suggests thatclassic theoretical frameworks, such as discourserepresentation theory, may extend to signed lan-guages (steinbach and onea, 2016).
we pose thefollowing questions: to what extent can automaticcoreference resolution of spoken languages be ap-plied to signed languages?
how do we keep trackof referents in space?
how can we leverage spatialrelations to resolve ambiguity?.
towards linguistically informed and multi-modal slp we highly encourage the collabora-tion of multimodal and slp research communitiesto develop powerful slp models informed by corenlp tools such as the ones discussed, all the whileprocessing and relating information from both lin-guistic and visual modalities.
on the one hand, the-ories and methods to reason multimodal messagescan enhance the joint modeling of vision and lan-guage in signed languages.
slp is especially sub-ject to three of the core technical challenges in mul-timodal machine learning (baltruˇsaitis et al., 2018):translation - how do we map visual-gestural in-formation to/from audio-oral and textual informa-tion?
alignment - how do we relate signed lan-guage units to spoken language units?
co-learning- can we transfer high-resource spoken languageknowledge to signed language?
on the other hand,meaning in spoken languages is not only conveyedthrough speech or text but also through the visualmodality.
studying signed languages can give abetter understanding of how to model co-speechgestures, spatial discourse relations, and conceptualgrounding of language through vision..5.2 collect real-world data.
data is essential to develop any of the core nlptools previously described, and current efforts inslp are often limited by the lack of adequate data.
we discuss the considerations to keep in mind whenbuilding datasets, challenges of collecting suchdata, and directions to facilitate data collection..what is good signed language data?
forslp models to be deployable, they must be de-veloped using data that represents the real world ac-.
curately.
what constitutes an ideal signed languagedataset is an open question, we suggest includingthe following requirements: (1) a broad domain; (2)sufﬁcient data and vocabulary size; (3) real-worldconditions; (4) naturally produced signs; (5) a di-verse signer demographic; (6) native signers; andwhen applicable, (7) dense annotations..to illustrate the importance of data quality dur-ing modeling, we ﬁrst take as an example a cur-rent benchmark for slp, the rwth-phoenix-weather 2014t dataset (cihan camg¨oz et al., 2018)of german sign language, that does not meet mostof the above criteria: it is restricted to the weatherdomain (1); contains only around 8k segmentswith 1k unique signs (2); ﬁlmed in studio condi-tions (3); interpreted from german utterances (4);and signed by nine caucasian interpreters (5,6).
although this dataset successfully addressed datascarcity issues at the time and successfully renderedresults comparable and fueled competitive research,it does not accurately represent signed languages inthe real world.
on the other hand, the public dgscorpus (hanke et al., 2020) is an open-domain (1)dataset consisting of 50 hours of natural signing(4) by 330 native signers from various regions ingermany (5,6), annotated with glosses, hamnosysand german translations (7), meeting all but tworequirements we suggest..we train a gloss-to-text sign language translationtransformer (yin and read, 2020b) on both datasets.
on rwth-phoenix-weather 2014t, we obtain22.17 bleu on testing; on public dgs corpus, weobtain a mere 3.2 bleu.
although transformersachieve encouraging results on rwth-phoenix-weather 2014t (saunders et al., 2020b; camg¨ozet al., 2020a), they fail on more realistic, open-domain data.
these results reveal that ﬁrstly, forreal-world applications, we need more data to trainsuch types of models, and secondly, while availabledata is severely limited in size, less data-hungryand more linguistically-informed approaches maybe more suitable.
this experiment reveals how itis crucial to use data that accurately represent thecomplexity and diversity of signed languages toprecisely assess what types of methods are suitable,and how well our models would deploy to the realworld..challenges of data collection collecting andannotating signed data inline with the ideal requiresmore resources than speech or text data, taking upto 600 minutes per minute of an annotated signed.
7354language video (hanke et al., 2020).
moreover, an-notation usually require a speciﬁc set of knowledgeand skills, which makes recruiting or training qual-iﬁed annotators challenging.
additionally, thereis little existing signed language data in the wildthat are open to use, especially from native signersthat are not interpretations of speech.
therefore,data collection often requires signiﬁcant efforts andcosts of on-site recording as well..automating annotation to collect more datathat enables the development of deployable slpmodels, one useful research direction is creatingtools that can simplify or automate parts of the col-lection and annotation process.
one of the largestbottleneck in obtaining more adequate signed lan-guage data is the amount of time and scarcity ofexperts required to perform annotation.
therefore,tools that perform automatic parsing, detection offrame boundaries, extraction of articulatory fea-tures, suggestions for lexical annotations, and allowparts of the annotation process to be crowdsourcedto non-experts, to name a few, have a high potentialto facilitate and accelerate the availability of gooddata..5.3 practice deaf collaboration.
finally, when working with signed languages, it isvital to keep in mind who this technology shouldbeneﬁt, and what they need.
researchers in slpmust honor that signed languages belong to thedeaf community and avoid exploiting their lan-guage as a commodity (bird, 2020)..solving real needs many efforts in slp havedeveloped intrusive methods (e.g.
requiring signersto wear special gloves), which are often rejectedby signing communities and therefore have limitedreal-world value.
such efforts are often marketedto perform “sign language translation” when they,in fact, only identify ﬁngerspelling or recognize avery limited set of isolated signs at best.
these ap-proaches oversimplify the rich grammar of signedlanguages, promote the misconception that signsare solely expressed through the hands, and areconsidered by the deaf community as a manifes-tation of audism, where it is the signers who mustmake the extra effort to wear additional sensorsto be understood by non-signers (erard, 2017).
inorder to avoid such mistakes, we encourage closedeaf involvement throughout the research processto ensure that we direct our efforts towards appli-cations that will be adopted by signers, and do not.
make false assumptions about signed languages orthe needs of signing communities..building collaboration deaf collaborationsand leadership are essential for developing signedlanguage technologies to ensure they address thecommunity’s needs and will be adopted, and thatthey do not rely on misconceptions or inaccuraciesabout signed language (harris et al., 2009; kusterset al., 2017).
hearing researchers cannot relate tothe deaf experience or fully understand the con-text in which the tools being developed would beused, nor can they speak for the deaf.
therefore,we encourage the creation of a long-term collab-orative environment between signed language re-searchers and users, so that deaf users can identifymeaningful challenges, and provide insights on theconsiderations to take, while researchers cater tothe signers’ needs as the ﬁeld evolves.
we alsorecommend reaching out to signing communitiesfor reviewing papers on signed languages, to en-sure an adequate evaluation of this type of researchresults published at acl venues.
there are sev-eral ways to connect with deaf communities forcollaboration: one can seek deaf students in theirlocal community, reach out to schools for the deaf,contact deaf linguists, join a network of researchersof sign-related technologies6, and/or participate indeaf-led projects..6 conclusions.
we urge the inclusion of signed languages innlp.
we believe that the nlp community is well-positioned, especially with the plethora of success-ful spoken language processing methods coupledwith the recent advent of computer vision toolsfor videos, to bring the linguistic insight neededfor better signed language models.
we hope tosee an increase in both the interests and efforts incollecting signed language resources and develop-ing signed language tools while building a strongcollaboration with signing communities..acknowledgements.
we would like to thank marc schulder, claudemauk, david mortensen, chaitanya ahuja, sid-dharth dalmia, shruti palaskar and graham neu-big as well as the anonymous reviewers for theirhelpful feedback and insightful discussions..6https://www.crest-network.com/.
7355references.
nikolas adaloglou, theocharis chatzis, ilias papas-tratis, andreas stergioulas, georgios th papadopou-los, vassia zacharopoulou, george j xydopou-los, klimnis atzakas, dimitris papazachariou, andpetros daras.
2020. a comprehensive study onsign language recognition methods.
arxiv preprintarxiv:2007.12530..waleed ammar, dirk groeneveld, chandra bhagavat-ula, iz beltagy, miles crawford, doug downey, ja-son dunkelberger, ahmed elgohary, sergey feld-man, vu ha, rodney kinney, sebastian kohlmeier,kyle lo, tyler murray, hsu-han ooi, matthew pe-ters, joanna power, sam skjonsberg, lucy wang,chris wilhelm, zheng yuan, madeleine van zuylen,and oren etzioni.
2018. construction of the litera-in proceedings ofture graph in semantic scholar.
the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 3 (industrypapers), pages 84–91, new orleans - louisiana.
as-sociation for computational linguistics..naveen arivazhagan, ankur bapna, orhan firat,dmitry lepikhin, melvin johnson, maxim krikun,mia xu chen, yuan cao, george foster, colincherry, et al.
2019. massively multilingual neuralmachine translation in the wild: findings and chal-lenges.
arxiv preprint arxiv:1907.05019..tadas baltruˇsaitis, chaitanya ahuja,.
and louis-philippe morency.
2018. multimodal machine learn-ieee transac-ing: a survey and taxonomy.
tions on pattern analysis and machine intelligence,41(2):423–443..robbin battison.
1978. lexical borrowing in american.
sign language.
eric..ursula bellugi and susan fischer.
1972. a comparisonof sign language and spoken language.
cognition,1(2-3):173–200..brita bergman.
1979..signed swedish.
national.
swedish board of education [skol¨overstyr.]:..
louise de beuzeville.
2008. pointing and verb modiﬁ-cation: the expression of semantic roles in the auslancorpus.
in workshop programme, page 13. citeseer..steven bird.
2020. decolonising speech and lan-guage technology.
in proceedings of the 28th inter-national conference on computational linguistics,pages 3504–3519, barcelona, spain (online).
inter-national committee on computational linguistics..julian bleicken, thomas hanke, uta salden, and svenwagner.
2016. using a language technology in-frastructure for german in order to anonymize ger-in proceedingsman sign language corpus data.
of the tenth international conference on languageresources and evaluation (lrec’16), pages 3303–3306, portoroˇz, slovenia.
european language re-sources association (elra)..mark borg and kenneth p camilleri.
2019. sign lan-guage detection ”in the wild” with recurrent neu-ral networks.
in icassp 2019-2019 ieee interna-tional conference on acoustics, speech and signalprocessing (icassp), pages 1637–1641.
ieee..danielle bragg, oscar koller, mary bellard, lar-wan berke, patrick boudreault, annelies braffort,naomi caselli, matt huenerfauth, hernisa ka-corri, tessa verhoef, christian vogler, and mered-ith ringel morris.
2019. sign language recognition,generation, and translation: an interdisciplinary per-in the 21st international acm sigac-spective.
cess conference on computers and accessibility,assets ’19, page 16–31, new york, ny, usa.
as-sociation for computing machinery..diane brentari.
2011. sign language phonology.
thehandbook of phonological theory, pages 691–721..diane brentari and carol padden.
2001. a languagewith multiple origins: native and foreign vocabu-lary in american sign language.
foreign vocabularyin sign language: a cross-linguistic investigation ofword formation, pages 87–119..hannah bull, mich`ele gouiff`es, and annelies braffort.
2020. automatic segmentation of sign languageinto subtitle-units.
in european conference on com-puter vision, pages 186–198.
springer..necati cihan camg¨oz, simon hadﬁeld, oscar koller,hermann ney, and richard bowden.
2018. neu-ral sign language translation.
in proceedings of theieee conference on computer vision and patternrecognition, pages 7784–7793..necati cihan camg¨oz, oscar koller, simon hadﬁeld,and richard bowden.
2020a.
multi-channel trans-formers for multi-articulatory sign language transla-tion.
in european conference on computer vision,pages 301–319..necati cihan camg¨oz, oscar koller, simon hadﬁeld,and richard bowden.
2020b.
sign language trans-formers: joint end-to-end sign language recognitionin proceedings of the ieee/cvfand translation.
conference on computer vision and pattern recog-nition, pages 10023–10033..z. cao, g. hidalgo martinez, t. simon, s. wei, andy. a. sheikh.
2019. openpose: realtime multi-person 2d pose estimation using part afﬁnity ﬁelds.
ieee transactions on pattern analysis and machineintelligence..yu chen, chunhua shen, xiu-shen wei, lingqiaoliu, and jian yang.
2017. adversarial posenet:a structure-aware convolutional network for humanin proceedings of the ieee in-pose estimation.
ternational conference on computer vision, pages1212–1221..necati cihan camg¨oz, simon hadﬁeld, oscar koller,hermann ney, and richard bowden.
2018. neu-ral sign language translation.
in proceedings of the.
7356ieee conference on computer vision and patternrecognition, pages 7784–7793..kearsy cormier, sandra smith, and zed sevcikova-sehyr.
2015. rethinking constructed action.
signlanguage & linguistics, 18(2):167–204..onno crasborn, r bank, i zwitserlood, e van derkooij, e ormel, j ros, a sch¨uller, a de meijer,m van zuilen, ye nauta, et al.
2016. ngt sign-bank.
nijmegen: radboud university, centre forlanguage studies..onno a crasborn and iep zwitserlood.
2008. the cor-pus ngt: an online corpus for professionals and lay-men..runpeng cui, hu liu, and changshui zhang.
2017.recurrent convolutional neural networks for con-tinuous sign language recognition by staged opti-in proceedings of the ieee conferencemization.
on computer vision and pattern recognition, pages7361–7369..ncslgr databases.
2007. volumes 2–7..philippe dreuw, thomas deselaers, daniel keysers,and hermann ney.
2006. modeling image vari-ability in appearance-based gesture recognition.
ineccv workshop on statistical methods in multi-image and video processing, pages 7–18..paul g dudis.
2004. body partitioning and real-space.
blends.
cognitive linguistics, 15(2):223–238..sarah ebling, necati cihan camg ¨o z, pennyboyes braem, katja tissi, sandra sidler-miserez,stephanie stoll, simon hadﬁeld, tobias haug,richard bowden, sandrine tornay, marzieh razavi,and mathew magimai-doss.
2018. smile swissin proceedings ofgerman sign language dataset.
the eleventh international conference on languageresources and evaluation (lrec 2018), miyazaki,japan.
european language resources association(elra)..michael erard.
2017. why sign-language gloves don’t.
help deaf people.
the atlantic, 9..iva farag and heike brock.
2019. learning motiondisﬂuencies for automatic sign language segmenta-tion.
in icassp 2019-2019 ieee international con-ference on acoustics, speech and signal processing(icassp), pages 7360–7364.
ieee..jordan fenlon, kearsy cormier, and adam schembri.
2015. building bsl signbank: the lemma dilemmainternational journal of lexicography,revisited.
28(2):169–206..jordan fenlon, adam schembri, and kearsy cormier.
2018. modiﬁcation of indicating verbs in britishsign language: a corpus-based study.
language,94(1):84–118..lucinda ferreira-brito.
1984. similarities & differ-ences in two brazilian sign languages.
sign lan-guage studies, 42:45–56..jens forster, christoph schmidt, oscar koller, martinbellgardt, and hermann ney.
2014. extensions ofthe sign language recognition and translation corpusrwth-phoenix-weather.
in lrec, pages 1911–1916..binyam gebrekidan gebre, peter wittenburg, and tomheskes.
2013. automatic sign language identiﬁca-tion.
in 2013 ieee international conference on im-age processing, pages 2626–2630.
ieee..neil s glickman and wyatte c hall.
2018. languagedeprivation and deaf mental health.
routledge..rıza alp g¨uler, natalia neverova, and iasonas kokki-nos.
2018. densepose: dense human pose estima-tion in the wild.
in proceedings of the ieee confer-ence on computer vision and pattern recognition,pages 7297–7306..eva gutierrez-sigut, brendan costello, cristina baus,and manuel carreiras.
2016. lse-sign: a lexicaldatabase for spanish sign language.
behavior re-search methods, 48(1):123–137..wyatte c hall, leonard l levin, and melissa l an-derson.
2017. language deprivation syndrome: apossible neurodevelopmental disorder with sociocul-tural origins.
social psychiatry and psychiatric epi-demiology, 52(6):761–776..in proceedings of.
thomas hanke, marc schulder, reiner konrad, andelena jahn.
2020. extending the public dgs cor-thepus in size and depth.
lrec2020 9th workshop on the representation andprocessing of sign languages: sign language re-sources in the service of the language commu-nity, technological challenges and application per-spectives, pages 75–82, marseille, france.
europeanlanguage resources association (elra)..raychelle harris, heidi m holmes, and donna mmertens.
2009. research ethics in sign languagesign language studies, 9(2):104–communities.
131..saad hassan, larwan berke, elahe vahdani, long-long jing, yingli tian, and matt huenerfauth.
2020.an isolated-signing rgbd dataset of 100 americansign language signs produced by ﬂuent asl sign-ers.
in proceedings of the lrec2020 9th workshopon the representation and processing of sign lan-guages: sign language resources in the serviceof the language community, technological chal-lenges and application perspectives, pages 89–94,marseille, france.
european language resourcesassociation (elra)..jie huang, wengang zhou, qilin zhang, houqiang li,and weiping li.
2018. video-based sign languagerecognition without temporal segmentation.
in pro-ceedings of the aaai conference on artiﬁcial intel-ligence, volume 32..7357tom humphries, poorna kushalnagar, gaurav mathur,donna jo napoli, carol padden, christian rath-mann, and scott smith.
2016. avoiding linguis-tic neglect of deaf children.
social service review,90(4):589–619..reiner konrad, thomas hanke, gabriele langer, su-sanne k¨onig, lutz k¨onig, rie nishio, and anja re-gen. 2018. public dgs corpus: annotation conven-tions.
technical report, project note ap03-2018-01,dgs-korpus project, idgs, hamburg university..alfarabi imashev, medet mukushev, vadim kimmel-man, and anara sandygulova.
2020. a datasetfor linguistic understanding, visual evaluation, andin pro-recognition of sign languages: the k-rsl.
ceedings of the 24th conference on computationalnatural language learning, pages 631–640..amy isard.
2020. approaches to the anonymisationin proceedings of theof sign language corpora.
lrec2020 9th workshop on the representation andprocessing of sign languages: sign language re-sources in the service of the language community,technological challenges and application perspec-tives, pages 95–100..david l jaffe.
1994. evolution of mechanical ﬁn-gerspelling hands for people who are deaf-blind.
journal of rehabilitation research and development,31(3):236–244..robert e johnson and scott k liddell.
2011. toward aphonetic representation of signs: sequentiality andcontrast.
sign language studies, 11(2):241–274..trevor johnston.
2010. from archive to corpus: tran-scription and annotation in the creation of signed lan-guage corpora.
international journal of corpus lin-guistics, 15(1):106–131..trevor johnston and louise de beuzeville.
2016. aus-lan corpus annotation guidelines.
auslan corpus..trevor johnston and adam schembri.
2007..aus-tralian sign language (auslan): an introductionto sign language linguistics.
cambridge universitypress..trevor johnston and adam c schembri.
1999. onsign lan-.
deﬁning lexeme in a signed language.
guage & linguistics, 2(2):115–185..jim kakumasu.
1968. urubu sign language.
interna-tional journal of american linguistics, 34(4):275–281..vadim kimmelman.
2014. information structure in rus-sian sign language and sign language of the nether-lands..sang-ki ko, chang jo kim, hyedong jung, andchoongsang cho.
2019. neural sign language trans-lation based on human keypoint estimation.
appliedsciences, 9(13):2683..oscar koller.
2020. quantitative survey of the state ofthe art in sign language recognition.
arxiv preprintarxiv:2008.09918..annelies kusters, maartje de meulder, and daio’brien.
2017. innovations in deaf studies: the roleof deaf scholars.
oxford university press..dongxu li, cristian rodriguez, xin yu, and hongdongli.
2020. word-level deep sign language recogni-tion from video: a new large-scale dataset and meth-ods comparison.
in the ieee winter conference onapplications of computer vision, pages 1459–1469..scott k liddell and robert e johnson.
1989. amer-ican sign language: the phonological base.
signlanguage studies, 64(1):195–277..scott k liddell and melanie metzger.
1998. gesturein sign language discourse.
journal of pragmatics,30(6):657–697..scott k liddell et al.
2003. grammar, gesture, andmeaning in american sign language.
cambridgeuniversity press..david mckee and graeme kennedy.
2000..lexi-cal comparison of signs from american, australian,british and new zealand sign languages.
the signsof language revisited: an anthology to honor ursulabellugi and edward klima, pages 49–76..johanna mesch and lars wallin.
2012. from meaningto signs and back: lexicography and the swedishin proceedings of the 5thsign language corpus.
workshop on the representation and processing ofsign languages: interactions between corpus andlexicon [language resources and evaluation con-ference (lrec)], pages 123–126..johanna mesch and lars wallin.
2015. gloss anno-tations in the swedish sign language corpus.
inter-national journal of corpus linguistics, 20(1):102–120..caio dd monteiro, christy maria mathew, ricardogutierrez-osuna, and frank shipman.
2016. detect-ing and identifying sign languages through visualfeatures.
in 2016 ieee international symposium onmultimedia (ism), pages 287–290.
ieee..kathryn montemurro and diane brentari.
2018. em-phatic ﬁngerspelling as code-mixing in americansign language.
proceedings of the linguistic soci-ety of america, 3(1):61–1..amit moryossef, ioannis tsochantaridis, roee aha-roni, sarah ebling, and srini narayanan.
2020. real-time sign language detection using human pose esti-in european conference on computer vi-mation.
sion, pages 237–248.
springer..7358amit moryossef, kayo yin, graham neubig, anddata augmentation forarxiv preprint.
yoav goldberg.
2021.sign language gloss translation.
arxiv:2105.07476..joseph j murray, wyatte c hall, and kristin snoddon.
2020. the importance of signed languages for deafchildren and their families.
the hearing journal,73(3):30–32..sylvie cw ong and surendra ranganath.
2005. auto-matic sign language analysis: a survey and the fu-ture beyond lexical meaning.
ieee computer archi-tecture letters, 27(06):873–891..ellen ormel and onno crasborn.
2012. prosodic cor-relates of sentences in signed languages: a litera-ture review and suggestions for new types of studies.
sign language studies, 12(2):279–315..c. padden.
1988. interaction of morphology and syn-tax in american sign language.
outstanding disclinguistics series.
garland..carol a padden.
1998. the asl lexicon.
sign language.
& linguistics, 1(1):39–60..carol a padden and tom humphries.
1988. deaf in.
america.
harvard university press..becky sue parton.
2006. sign language recognitionand translation: a multidisciplined approach fromthe ﬁeld of artiﬁcial intelligence.
journal of deafstudies and deaf education, 11(1):94–101..carol j patrie and robert e johnson.
2011. finger-spelled word recognition through rapid serial visualpresentation: rsvp.
dawnsignpress..leonid pishchulin, arjun jain, mykhaylo andriluka,thorsten thorm ¨a hlen, and bernt schiele.
2012.articulated people detection and pose estimation:in 2012 ieee conferencereshaping the future.
on computer vision and pattern recognition, pages3178–3185.
ieee..vineel pratap, qiantong xu, anuroop sriram, gabrielsynnaeve, and ronan collobert.
2020. mls: alarge-scale multilingual dataset for speech re-in proc.
interspeech 2020, pages 2757–search.
2761..siegmund prillwitz and heiko zienert.
1990. hamburgnotation system for sign language: development ofa sign writing with computer application.
in currenttrends in european sign language research.
pro-ceedings of the 3rd european congress on sign lan-guage research, pages 355–379..javier ramırez, jos´e c segura, carmen benıtez, angelde la torre, and antonio rubio.
2004. efﬁcientvoice activity detection algorithms using long-termspeech information.
speech communication, 42(3-4):271–287..razieh rastgoo, kourosh kiani, and sergio escalera.
2020. sign language recognition: a deep survey.
expert systems with applications, page 113794..christian rathmann and gaurav mathur.
2011. a feat-ural approach to verb agreement in signed languages.
theoretical linguistics, 37(3-4):197–208..cynthia b roy.
2011. discourse in signed languages..gallaudet university press..wendy sandler.
2010. prosody and syntax in signlanguages.
transactions of the philological society,108(3):298–328..wendy sandler.
2012. the phonological organizationof sign languages.
language and linguistics com-pass, 6(3):162–182..pinar santemiz, oya aran, murat saraclar, and laleakarun.
2009. automatic sign segmentation fromcontinuous signing via multiple sequence align-ment.
in 2009 ieee 12th international conferenceon computer vision workshops, iccv workshops,pages 2001–2008.
ieee..ben saunders, necati cihan camg¨oz, and richardbowden.
2020a.
everybody sign now: translat-ing spoken language to photo realistic sign languagevideo.
arxiv preprint arxiv:2011.09846..ben saunders, necati cihan camg¨oz, and richardbowden.
2020b.
progressive transformers for end-to-end sign language production.
in european con-ference on computer vision, pages 687–705..adam schembri, kearsy cormier, and jordan fenlon.
2018. indicating verbs as typologically unique con-structions: reconsidering verb ‘agreement’in signlanguages.
glossa: a journal of general linguistics,3(1)..b. shi, a. martinez del rio, j. keane, d. brentari,g. shakhnarovich, and k. livescu.
2019. finger-spelling recognition in the wild with iterative visualattention.
iccv..b. shi, a. martinez del rio, j. keane, j. michaux,g. shakhnarovich d. brentari, and k. livescu.
2018.american sign language ﬁngerspelling recognitionin the wild.
slt..edgar h shroyer and susan p shroyer.
1984. signsacross america: a look at regional differences inamerican sign language.
gallaudet universitypress..ozge mercanoglu sincan and hacer yalim keles.
2020.autsl: a large scale multi-modal turkish sign lan-guage dataset and baseline methods.
ieee access,8:181340–181355..jongseo sohn, nam soo kim, and wonyong sung.
1999. a statistical model-based voice activity de-tection.
ieee signal processing letters, 6(1):1–3..7359spain (online).
international committee on compu-tational linguistics..jan zelinka and jakub kanis.
2020. neural sign lan-in theguage synthesis: words are our glosses.
ieee winter conference on applications of com-puter vision, pages 3395–3403..markus steinbach and edgar onea.
2016. a drt anal-ysis of discourse referents and anaphora resolutionin sign language.
journal of semantics, 33(3):409–448..jr. stokoe, william c. 1960. sign language structure:an outline of the visual communication systemsof the american deaf.
the journal of deaf studiesand deaf education, 10(1):3–37..william c stokoe jr. 2005. sign language structure:an outline of the visual communication systems ofthe american deaf.
journal of deaf studies and deafeducation, 10(1):3–37..stephanie stoll, necati cihan camg¨oz, simon had-ﬁeld, and richard bowden.
2018. sign languageproduction using neural machine translation and gen-in proceedings oferative adversarial networks.
the 29th british machine vision conference (bmvc2018).
british machine vision association..stephanie stoll, necati cihan camg¨oz, simon had-ﬁeld, and richard bowden.
2020. text2sign:to-wards sign language production using neural ma-chine translation and generative adversarial net-international journal of computer vision,works.
pages 1–18..ted supalla.
1986. the classiﬁer system in ameri-can sign language.
noun classes and categorization,7:181–214..valerie sutton.
1990. lessons in sign writing.
sign-.
writing..hamid vaezi joze and oscar koller.
2019. ms-asl: alarge-scale data set and benchmark for understand-ing american sign language.
in the british machinevision conference (bmvc)..sherman wilcox.
1992..the phonetics of ﬁnger-.
spelling, volume 4. john benjamins publishing..sherman wilcox and sarah hafer.
2004. rethinkingclassiﬁers.
emmorey, k.(ed.).(2003).
perspectives onclassiﬁer constructions in sign languages.
mahwah,nj: lawrence erlbaum associates.
332 pages.
hard-cover..qinkun xiao, minying qin, and yuting yin.
2020.skeleton-based chinese sign language recognitionand generation for bidirectional communication be-tween deaf and hearing people.
neural networks,125:41–55..kayo yin and jesse read.
2020a.
attention is all yousign: sign language translation with transformers.
in sign language recognition, translation and pro-duction (slrtp) workshop-extended abstracts, vol-ume 4..kayo yin and jesse read.
2020b.
better sign languagein proceed-translation with stmc-transformer.
ings of the 28th international conference on com-putational linguistics, pages 5975–5989, barcelona,.
7360