an empirical study on hyperparameter optimization for fine-tuningpre-trained language models.
xueqing liustevens institute of technologyxueqing.liu@stevens.edu.
chi wangmicrosoft researchwang.chi@microsoft.com.
abstract.
the performance of ﬁne-tuning pre-trainedlanguage models largely depends on the hyper-parameter conﬁguration.
in this paper, we in-vestigate the performance of modern hyperpa-rameter optimization methods (hpo) on ﬁne-tuning pre-trained language models.
first, westudy and report three hpo algorithms’ per-formances on ﬁne-tuning two state-of-the-artlanguage models on the glue dataset.
weﬁnd that using the same time budget, hpo of-ten fails to outperform grid search due to twoinsufﬁcient time budget and overﬁt-reasons:ting.
we propose two general strategies and anexperimental procedure to systematically trou-bleshoot hpo’s failure cases.
by applying theprocedure, we observe that hpo can succeedwith more appropriate settings in the searchspace and time budget; however, in certaincases overﬁtting remains.
finally, we makesuggestions for future work.
our implemen-tation can be found in https://github.com/microsoft/flaml/tree/main/flaml/nlp/..1.introduction.
in the recent years, deep learning and pre-trainedlanguage models (devlin et al., 2019; liu et al.,2019; clark et al., 2020; he et al., 2021) haveachieved great success in the nlp community.
ithas now become a common practice for researchersand practitioners to ﬁne-tune pre-trained languagemodels in down-stream nlp tasks.
for example,the huggingface transformers library (wolf et al.,2020) was ranked no.1 among the most starrednlp libraries on github using python1..same as other deep learning models, the perfor-mance of ﬁne-tuning pre-trained language mod-els largely depends on the hyperparameter con-ﬁguration.
a different setting in the hyperparam-.
1https://github.com/evanli/github-ranking/blob/master/top100/python.md.
eters may cause a signiﬁcant drop in the perfor-mance, turning a state-of-the-art model into a poormodel.
methods for tuning hyperparameters can becategorized as (1) traditional approaches such asmanual tuning and grid search, and (2) automatedhpo methods such as random search and bayesianoptimization (bo).
manual tuning often requiresa large amount of manual efforts; whereas gridsearch often suffers from lower efﬁciency due tothe exponential increase in time cost with the num-ber of hyperparameters.
automated hpo methodswere proposed to overcome these disadvantages.
recently, automated hpo methods also become in-creasingly popular in the nlp community (zhangand duh, 2020; dodge et al., 2019).
for exam-ple, bayesian optimization (bo) (zhang and duh,2020) and population-based training (jaderberget al., 2017) both prove to be helpful for improvingthe performance of the transformer model (vaswaniet al., 2017) for neural machine translation.
thehuggingface library has also added native sup-ports for hpo in a recent update (version 3.1.0,aug 2020)..with improved supports, users can now easilyaccess a variety of hpo methods and apply them totheir ﬁne-tuning tasks.
however, the effectivenessof this step is less understood.
to bridge this gap,in this paper, we propose an experimental study forﬁne-tuning pre-trained language models using thehuggingface library.
this study is motivated bythe following research questions: first, can auto-mated hpo methods outperform traditional tuningmethod such as grid search?
second, on whichnlp tasks do hpo methods work better?
third, ifhpo does not work well, how to troubleshoot theproblem and improve its performance?.
to answer these questions, we start from a sim-ple initial study (section 4) by examining the per-formance of three hpo methods on two state-of-the-art language models on the glue dataset.
the.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2286–2300august1–6,2021.©2021associationforcomputationallinguistics2286time budget for hpo in the initial study is set to bethe same as grid search.
results of the initial studyshow that hpo often fails to match grid search’sperformance.
the reasons for hpo’s failures aretwo folds: ﬁrst, the same budget as grid searchmay be too small for hpo; second, hpo overﬁtsthe task.
with these observations, we propose twogeneral strategies for troubleshooting the failurecases in hpo as well as an overall experimentalprocedure (figure 1).
by applying the procedure(section 5), we ﬁnd that by controlling overﬁttingwith reduced search space and using a larger timebudget, hpo has outperformed grid search in morecases.
however, the overﬁtting problem still ex-ists in certain tasks even when we only search forthe learning rate and batch size.
finally, we makesuggestions for future work (section 7)..the main contributions of this work are:.
• we empirically study the performance of threehpo methods on two pre-trained languagemodels and on the glue benchmark;.
• we design an experimental procedure whichproves useful to systematically troubleshootthe failures in hpo for ﬁne-tuning;.
• we report and analyze the execution results ofthe experimental procedure, which sheds lighton future work;.
2 deﬁnition of hpo on language model.
fine-tuning.
given a pre-trained language model, a ﬁne-tuningtask, and a dataset containing dtrain, dval, dtest,the goal of a hyperparameter optimization algo-rithm is to ﬁnd a hyperparameter conﬁguration c,so that when being trained under conﬁguration c,the model’s performance on a validation set dvalis optimized.
formally, the goal of hpo is to ﬁnd.
c∗ = arg max.
f (c, dtrain, dval).
c∈s.
where s is called the search space of the hpo al-gorithm, i.e., the domain where the hyperparametervalues can be chosen from.
the function f (·, ·, ·)is called the evaluation protocol of hpo, which isdeﬁned by the speciﬁc downstream task.
for exam-ple, many tasks in glue deﬁne f as the validationaccuracy.
if a task has multiple protocols, we ﬁx f.as one of them2.
after ﬁnding c∗, the performanceof hpo will be evaluated using the performance ofthe model trained with c∗ on the test set dtest..to fairly compare the performances of differenthpo algorithms, the above optimization problem isdeﬁned with a constraint in the maximum runningtime of the hpo algorithm, which we call the timebudget for the algorithm, denoted as b. underbudget b, the hpo algorithm can try a numberof conﬁgurations c1, c2, · · · , cn.
the process ofﬁne-tuning with conﬁguration ci is called a trial.
finally, we call the process of running an hpoalgorithm a once one hpo run..3 factors of the study.
in this paper, we conduct an empirical study toanswer the research questions in section 1. first,can automated hpo methods outperform gridsearch?
the answer to this question depends onmultiple factors, i.e., the nlp task on which hpoand grid search are evaluated, the pre-trained lan-guage model for ﬁne tuning, the time budget, thesearch space for grid search and hpo algorithm,and the choice of hpo algorithm.
to providea comprehensive answer, we need to enumeratemultiple settings for these factors.
however, it isinfeasible to enumerate all possible settings foreach factor.
for instance, there exist unlimitedchoices for the search space.
to accomplishour research within reasonable computationalresources3, for each factor, we only explore themost straight-foward settings.
for example, thesearch space for grid search is set as the defaultgrid conﬁguration recommended for ﬁne-tuning(table 1), and the search space for hpo is set as astraightforward relaxation of the grid conﬁguration.
we explain the settings for each factor in detailsbelow..2there are 3 glue tasks with multiple validation scores:mrpc, sts-b, and qqp (not studied).
for mrpc we opti-mize the validation accuracy, and for sts-b we optimize thepearson score on the validation set..3our experiments were run on two gpu servers, server 1 isequipped with 4xv100 gpus (32gb), and server 2 is a dgxserver equipped with 8xv100 gpus (16gb).
to avoid incom-parable comparisons, all experiments on qnli and mnliare run exclusively on server 2, and all other experiments arerun exclusively on server 1. to speed up the training, we usefp16 in all our experiments.
to guarantee the comparabilitybetween different hpo methods, all trials are allocated exactly1 gpu and 1 cpu.
as a result, all trials are executed in thesingle-gpu mode and there never exist two trials sharing thesame gpu..2287hyperparameter.
electra-grid.
electra-hpo.
roberta-grid.
learning rate.
{3e-5,1e-4,1.5e-4}.
warmup ratioattention dropouthidden dropoutweight decaybatch sizeepochs.
0.10.10.103210 for rte/sts-b,3 for other.
log((2.99e-5,1.51e-4))(0, 0.2)(0, 0.2)(0, 0.2)(0, 0.3){16, 32, 64}10 for rte/sts-b,3 for other.
{1e-5,2e-5,3e-5}0.060.10.10.1{16, 32}10.roberta-hpo(0.99e-5,3.01e-5)(0, 0.12)(0, 0.2)(0, 0.2)(0, 0.3){16, 32, 64}10.table 1: the search space for grid search and hpo methods in this paper.
for grid search, we adopt the searchspaces from the electra (clark et al., 2020) and roberta (liu et al., 2019) paper.
for each model, we expand thegrid search space to a larger, simple search space for hpo..nlp tasks.
to study hpo’s performance onmultiple nlp tasks, we use the 9 tasks fromthe glue (general language understandingevaluation) benchmark (wang et al., 2018)..time budget.
we focus on a low-resourcescenario in this paper.
to compare the performanceof grid search vs. hpo, we ﬁrst allocate the sametime budget to hpo as grid search in our initialcomparative study (section 4).
if hpo does notoutperform grid search, we increase the timebudget for hpo.
we require that each hpo runtakes no more than 8 gpu hours with the nvidiatesla v100 gpu under our setting.
we prune atask if the time for grid search exceeds two hours.
a complete list of the time used for each remainingtask can be found in table 2..nlp taskwnlirtemrpccolasts-bsstqnliqqpmnli.
electra epoch42031000 10342042031200 101200 31800 37800 36600 3.
1010101010-.
roberta epoch660720720120010007800---.
--.
table 2: the running time of grid search for each task(in seconds) and the corresponding number of epochs..roberta-base model (liu et al., 2019).
electraand roberta are among the best-performingmodels on the leaderboard of glue as of jan20214. another reason for choosing the twomodels is that they both provide a simple searchspace for grid search, and we ﬁnd it helpful todesign our hpo search space on top of them.
we use both models’ implementations from thetransformers library (wolf et al., 2020) (version =3.4.0).
among all the different sizes of robertaand electra (large, base, small), we choose thebase size, because large models do not ﬁt into our2-hour budget5.
with the 2-hour time constraint,we prune tasks where grid search takes longer thantwo hours.
for electra, qqp is pruned, whereasfor roberta, sst, qnli, qqp, mnli are pruned..search space for grid search and hpo.
it isgenerally difﬁcult to design an hpo search spacefrom scratch.
in our problem, this difﬁculty isfurther ampliﬁed with the limited computationalresources.
fortunately, most papers on pre-trainedlanguage models recommend one or a fewhyperparameter conﬁgurations for ﬁne-tuning.
weuse them as the conﬁgurations for grid search.
for hpo, the performance depends on the searchspace choice, e.g.,it takes more resources toexplore a large space than a smaller space closeto the best conﬁguration.
due to the time budgetlimits, we focus on a small space surroundingthe recommended grid search space, as shown.
pre-trained language models.
in this paper, wefocus on two pre-trained language models:theelectra-base model (clark et al., 2020), and the.
4www.gluebenchmark.com5our empirical observation shows that the large models.
take 1.5 to 2 times the running time of the base models..2288in table 1.6 more speciﬁcally, we convert thelearning rate, warmup ratio, attention dropout,and hidden dropout to a continuous space byexpanding the grid space.
for weight decay, sincethe recommended conﬁguration is 0, we followray tune’s search space and set the hpo space to(0, 0.3) (kamsetty, 2020).
for epoch number, mostexisting work uses an integer value between 3 and10 (clark et al., 2020; liu et al., 2019; dai et al.,2020), resulting in a large range of space we canpossibly search.
to reduce the exploration requiredfor hpo, we skip expanding the search space forepoch number and ﬁx it to the grid conﬁguration..hpo algorithms.
we compare the performancebetween grid search and three hpo algorithms:random search (bergstra and bengio, 2012),asynchronous successive halving (asha) (liet al., 2020), and bayesian optimization (akibaet al., 2019)+asha.
we use all hpo methods’implementations from the ray tune library (liawet al., 2018) (version 1.2.0).
we use bo (with tpesampler) together with the asha pruner, becausewith the small time budget, bo without the prunerreduces to random search.
as ﬁne-tuning innlp usually outputs the checkpoint with thebest validation accuracy, we also let the hpomethods output the best checkpoint of the besttrial.
this choice is explained in more details inappendix a.1..4 experiment #1: comparative study.
using 1gst.
as the performance of hpo depends on the timebudget, to compare between grid search and hpo,we ﬁrst conduct an initial study by setting the timebudget of hpo to the same as grid search.
for therest of this paper, we use agst to denote that thetime budget=a×the running time for grid search.
table 3 shows the experimental results on electraand roberta using 1gst.
for each (hpo method,nlp task) pair, we repeat the randomized experi-ments 3 times and report the average scores.
weanalyze the results in section 4.1..4.1 analysis of the initial results.
electra.
by comparing the performance of gridsearch and hpo in table 3 we can make the fol-lowing ﬁndings.
first, hpo fails to match gridsearch’s validation accuracy in the following tasks:rte, sts-b, sst and qnli.
in certain tasks suchas qnli and rte, grid search outperforms hpoby a large margin.
considering the fact that gridsearch space is a subspace of the hpo space, thisresult shows that with the same time budget as gridsearch (i.e., approximately 3 to 4 trials), it is difﬁ-cult to ﬁnd a conﬁguration which works better thanthe recommended conﬁgurations.
indeed, with 3to 4 trials, it is difﬁcult to explore the search space.
although asha and bo+asha both search formore trials by leveraging early stopping (li et al.,2020), the trial numbers are still limited (the aver-age trial numbers for experiments in table 3 can befound in table 6 of the appendix).
second, amongthe tasks where hpo outperforms grid search’s val-idation accuracy, there are 2 tasks (wnli, mrpc)where the test accuracy of hpo is lower than gridsearch.
as a result, the hpo algorithm overﬁtsthe validation dataset.
overﬁtting in hpo gener-ally happens when the accuracy is optimized on alimited number of validation data points and can-not generalize to unseen test data (feurer and hut-ter, 2019).
(zhang et al., 2021) also found thatﬁne-tuning pre-trained language models is proneto overﬁtting when the number of trials is large,though they do not compare hpo and grid search.
finally, by searching for more trials, asha andbo+asha slightly outperform random search inthe validation accuracy, but their test accuracy isoften outperformed by random search.
roberta.
by observing roberta’s results fromtable 3, we can see that the average validation ac-curacy of hpo outperforms grid search in all tasksexcept for cola.
it may look like hpo is moreeffective; however, most of the individual runs intable 3 overﬁt.
as a result, hpo for ﬁne-tuningroberta is also prone to overﬁtting comparedwith grid search.
the complete lists of the overﬁt-ting cases in table 3 can be found in table 8 andtable 9 of appendix a.3..6the grid search spaces in table 1 are from table 7 ofelectra and table 10 of roberta.
for electra, we ﬁx thehyperparameters for adam; we skip the layer-wise learningrate decay because it is not supported by the huggingfacelibrary.
while electra’s original search space for learning rateis [3e-5, 5e-5, 1e-4, 1.5e-4], we have skipped the learning rate5e-5 in our experiment..4.2 a general experimental procedure for.
troubleshooting hpo failures.
since table 3 shows hpo cannot outperform gridsearch using 1gst, and is prone to overﬁtting, wepropose two general strategies to improve hpo’s.
2289wnli rte mrpc.
cola.
sts-b.
sst qnli mnli.
56.356.857.258.2.electra-base, validationgridrsrs+ashabo+ashaelectra-base, testgridrsrs+ashabo+asha.
65.164.462.661.6.
84.182.280.382.6.
76.875.674.175.1.
92.3/89.293.0/90.493.0/90.393.1/90.4.
91.1/87.990.7/87.590.6/87.390.7/87.4.
67.268.867.969.4.
58.563.061.264.1.
91.5/91.490.1/90.291.4/91.391.5/91.3.
89.7/89.288.0/87.689.5/89.189.7/89.1.
95.194.794.994.7.
95.795.194.994.8.
93.593.093.193.1.
93.593.092.993.0.
88.688.988.689.2.
88.388.788.588.7.wnli rte mrpc.
cola.
sts-b.
roberta-base, validation56.3grid57.8rs57.3rs+ashabo+asha56.3roberta-base, test65.1grid64.9rs65.1rs+asha65.1bo+asha.
79.880.480.880.3.
73.973.574.173.3.
93.1/90.493.3/90.793.4/90.893.7/91.4.
90.5/87.190.1/86.790.6/87.390.4/87.2.
65.164.164.564.5.
61.759.159.460.1.
91.2/90.891.2/90.991.2/90.991.3/91.0.
89.3/88.489.3/88.689.1/88.389.1/88.4.
table 3: results of the initial comparative study on electra (top) and roberta (bottom) by varying the gluetask and hpo method while ﬁxing the search space and time budget.
for each (hpo method, task), we rerun theexperiment 3 times and report the average..performance.
first, we increase the time budget forhpo so that hpo can exploit the space with moretrials.
second, to control overﬁtting, we propose toreduce the search space.
more speciﬁcally, we pro-pose to ﬁx the values of certain hyperparametersto the default values in the grid conﬁguration (ta-ble 3).
the reason is that overﬁtting can be relatedto certain hyperparameter settings of the model.
for example, it was shown in ulmfit (howardand ruder, 2018) that using a non-zero warmupstep number can help reduce overﬁtting.
intuitively,a larger search space is more prone to overﬁtting.
for example, by using a warmup search space =(0, 0.2), the warmup steps in the best trial foundby hpo may be much smaller or larger than thesteps used by grid search.
other hyperparameterswhich are related to overﬁtting of ﬁne-tuning in-clude the learning rate (smith and le, 2017), batchsize (smith et al., 2017), and the dropout rates (sri-vastava et al., 2014; loshchilov and hutter, 2019,2018)..our proposed procedure for troubleshootinghpo failures is depicted in figure 1. starting from.
the full search space and 1gst, we test the hpoalgorithm for a few times.
if any overﬁtting is ob-served, we reduce the search space and go backto testing the hpo algorithm again.
on the otherhand, if no overﬁtting is observed and hpo alsodoes not outperform grid search, we increase thetime budget and also go back to testing the hpoalgorithm again.
we continue this procedure untilany of the following conditions is met: ﬁrst, hposuccessfully outperforms grid search; second, thesearch space cannot be further reduced, thus hpooverﬁts the task; third, the time budget cannot befurther increased under a user-speciﬁed threshold,thus whether hpo can outperform grid search is tobe determined for this speciﬁc task..5 experiment #2: troubleshooting hpo.
in this section, we evaluate the effectiveness ofour proposed procedure in figure 1. to apply theprocedure, we need to further consolidate two com-ponents: ﬁrst, what time budget should we use;second, which hyperparameter to ﬁx for reducingthe search space.
for the ﬁrst component, we use a.
2290relatively small list for time budget options {1gst,4gst}.
for the second component, it is difﬁcult toguarantee to reduce overﬁtting by ﬁxing a speciﬁchyperparameter to its grid search values.
whenchoosing the hyperparameter to ﬁx, we refer to theconﬁgurations of the best trials which cause thehpo results to overﬁt..figure 1: a general experimental procedure for trou-bleshooting hpo failure cases..5.1 choosing the hyperparameter to fix.
electra.
to decide which hyperparameter to ﬁx,we examine the best trial’s conﬁguration for theoverﬁtting hpo runs (compared with the gridsearch performance).
if there is a pattern in acertain hyperparameter of all these conﬁgurations(e.g., warmup ratio below 0.1 for electra), by ﬁxingsuch hyperparameters to the values of grid search,we can exclude the other values which may berelated to overﬁtting.
we apply this analyticalstrategy to the initial electra results in table 3.among the 72 runs, 9 runs overﬁt compared withgrid search.
for each run, we list the hyperpa-rameter conﬁgurations of the best trial in table 8of appendix a.3.
for electra, we have skippedshowing weight decay in table 8, because the hpoconﬁguration is never smaller than the grid conﬁgu-ration, thus does not affect the result of the analysis.
for comparative purpose, we also list the hyperpa-rameter values of the best trial in grid search.
toimprove the readability of table 8, we use 4 dif-ferent colors (deﬁned in appendix a.3) to denotethe comparison between values of the best trial inhpo and values of the best trial in grid search..from table 8, we observe that the warmupratios are often signiﬁcantly lower than 0.1. weskip the analysis on learning rate because itssearch space (log((2.99e-5,1.51e-4))) cannot be.
further reduced without losing coverage of the gridconﬁgurations or continuity; we also skip weightdecay because any trial’s value cannot be smallerthan 0. following this empirical observation, wehypothesize that ﬁxing the warmup ratio to 0.1 canhelp reduce overﬁtting in electra.
we use sf ullto denote the original search space and s−wr todenote the search space by ﬁxing the warmup ratioto 0.1. if hpo overﬁts in both sf ull and s−wr,the procedure will reduce the search space to theminimal continuous space smin containing thegrid search space, which searches for the learningrate only..roberta.
we apply the same analytical strat-egy to the roberta results in table 3 and showthe hyperparameters of the best trials in table 9.for roberta, we propose to ﬁx the values of twohyperparameters at the same time: the warmup ra-tio and the hidden dropout.
we denote the searchspace after ﬁxing them as s−wr−hdo.
if hpo over-ﬁts in both sf ull and s−wr−hdo, the procedure willreduce the search space to smin which contains thelearning rate and batch size only..5.2 execution results of the procedure.
in this section, we apply the troubleshootingprocedure on the initial hpo results from table 3and observe the execution paths.
in table 10and table 11 of appendix a.4, we list the fullexecution results of the procedure for randomsearch and random search + asha.
table 10&11have included only the tasks where the hpo doesnot succeed in the initial study.
in table 10&11,we show the validation and test accuracy for thethree repetitions of hpo runs as well as theiraverage score..an example of executing the procedure.
in fig-ure 4, we show an example of applying the pro-cedure on random search for electra on rte.
inround 0, the validation and test accuracies of allthree repetitions are lower than grid search.
thatimplies rs needs more time budget, therefore weincrease the budget (marked as ↑res) for rs from1gst to 4gst.
after the increase, overﬁtting isdetected in the 1st repetition of round 1 (valida-tion accuracy = 84.5, test accuracy = 74.6).
wethus reduce the search space (marked as ↓ space)from sf ull to s−wr.
in round 2, the 1st repetitionstill shows (weak) overﬁtting: rs has the same.
2291startsearch space=           ,  res=1gstoutperform?hpo succeedsnores > max?repeat hpo a few timesresspaceoverfitting?notbdcan reduce hpo overfits taskyesnoyesnoyesyes<latexit sha1_base64="na5ydgehbeykzihvhvoirp0tip4=">aaab+3icbvdlssnafl3xwesr1qwbwsk4kokouiy6cvnrpqanytkdtemnkzazeuvir7hxoyhbf8sdf+okzujbdwwczrmxofcecwdko863tbk6tr6xwdmqbu/s7u3bb7woilnjajvepja9acvkmabtztsnvurshawcdopjtef3h6lulbypeppql8ijwujgsdasb9cgedzjgnl2n/tzmhke+3bdatgzogxilqqojvq+/tuyxisnqncey6x6rpnol8nsm8jpxh2kiiaytpci9g0voklky2bzc3rilceky2me0gim/t7icktunarmzjfulxqf+j/xt3v45wvmjkmmgsw/mvchhaoicdrkkhlnp4zgipnjisgys0y0qatqsnaxt14mnboge9fw7s7rzeuyjgocwtgcggux0irbaeebcdzbm7zcm5vbl9a79tefxbhknup4a+vzb8vjlo4=</latexit>sfull↑ res.
round 1.round 2.test val.
test val.
test val.
round 3test↓ space.
round 0val84.1 76.881.9 76.1 84.5 74.6 84.1 76.1 84.8 75.381.6 75.1 83.8 74.5 83.0 74.0 84.1 75.783.0 75.7 83.4 74.7 82.3 73.1 83.8 75.282.2 75.6 83.9 74.6 83.1 74.4 84.2 75.4.
↓ space.
gridrep1rep2rep3avg.
table 4: an example of executing the exper-imental procedure applied to random search forelectra on rte.
the grid search accuracy is de-noted using the blue bold font.
an hpoandrun is highlighted in.
dark grey if it overﬁts.
medium grey if it overﬁts weakly ..validation accuracy as grid search (84.1), a smallertest accuracy (76.1), and a smaller validation loss(rs’s validation loss = 0.8233, grid search’s valida-tion loss = 0.9517).
we thus continue reducing thesearch space to smin, and overﬁtting is detectedagain in the 1st repetition of round 3 (validationaccuracy = 84.8, test accuracy = 75.3).
after round3, the search space cannot be further reduced, sowe classify this case as ’hpo overﬁts task’..we analyze the execution results in table 10 and.
11 jointly as follows..effects of reducing the search space.
from thetwo tables we can observe that reducing the searchspace can be effective for controlling overﬁtting.
in wnli (electra), both algorithms outperformgrid search after reducing the search space once.
inwnli (roberta), asha outperforms grid searchafter reducing the search space twice.
we canobserve a similar trend in mrpc (electra), sst(electra), rte (roberta), and cola (roberta).
however, for these cases, overﬁtting still existseven after we reduce the search space twice, i.e.,using the minimal search space..effects of increasing the time budget.
byobserving cases of increased budget in table 10and 11, we can see that this strategy is generally ef-fective for improving the validation accuracy.
afterincreasing the time budget, in sts-b (electra) allhpo methods outperform grid search’s validationand test accuracy; in sst (electra-rs) and cola(roberta) hpo outperforms grid search in onlyin rte (electra) andthe validation accuracy.
qnli (electra), however,this increase is notenough for bridging the gap with grid search, thus.
hpo remains behind.
for rte (electra), sst(electra), qnli (electra), and cola (roberta),overﬁtting happens after increasing the time budgetfrom 1gst to 4gst.
after reducing the searchspace, we still observe overﬁtting in most cases..comparisons between rs and asha.
by com-paring the results between random search andasha in table 10 and 11, we ﬁnd that before in-creasing the budget, rs rarely outperforms ashain the validation accuracy; however, after the bud-get of both rs and asha increases to 4gst, thebest validation accuracy of rs has consistentlyoutperformed asha, i.e., in all of rte (electra),sts-b (electra), sst (electra), and qnli (elec-tra).
that is, the increase in the time budget hasled to more signiﬁcant (validation) increase in rsthan asha.
this result may be caused by tworeasons.
first, at 1gst, asha already samplesa larger number of trials (appendix a.2), whichmay be sufﬁcient to cover its search space; on theother hand, rs cannot sample enough trials, thusincreasing the time budget is more helpful.
second,asha may make mistake by pruning a good trialthat shows a bad performance at the beginning..5.3 summary of the main findings.
in table 5, we list the ﬁnal execution results foreach task in electra and roberta.
our mainﬁndings can be summarized as follows.
afterincreasing the time budget and reducing thesearch space, hpo outperforms grid search in thefollowing cases: (1) in 3 cases (i.e., cola (elec-tra), sts-b (electra) and mnli (electra)), hpooutperforms grid search by using the full searchspace, where sts-b needs more budget; (2) in 4cases (i.e., wnli (electra), wnli (roberta),mrpc (roberta) and sts-b (roberta)), hposucceeds after reducing the search space; (3) inthe other 7 cases, hpo cannot outperform gridsearch even after increasing the time budget andreducing the search space.
this result shows thatwhen searching in a continuous space surroundingthe recommended grid conﬁgurations, it can bedifﬁcult for existing automated hpo methods (e.g.,random search, asha, bayesian optimization)to outperform grid search (with manually tunedgrid conﬁgurations recommended by the languagemodel) within a short amount of time; even if wecan identify a conﬁguration with good validationscore, most likely the test score is still worse than.
2292execution results.
taskwnli all hpo succeed w/ 1gst, s−wr.
rte.
rs overﬁtsasha and bo+asha tbd.
mrpc all hpo overﬁtcola all hpo succeed w/ 1gst, sf ullsts-b all hpo succeed w/ 4gst, sf ullsstqnlimnli all hpo succeed w/ 1gst, sf ull.
all hpo overﬁtall hpo tbd.
task.
wnli.
rte.
mrpc.
sts-b.
execution resultsasha succeeds∗ w/ 1gst, s−wr−hdors and bo+asha overﬁtall hpo overﬁtasha succeeds∗ w/ 1gst, s−wr−hdors and bo+asha overﬁt.
cola all hpo overﬁt.
rs succeeds w/ 1gst, s−wr−hdoasha and bo+asha succeed w/1gst, smin.
table 5: final results of executing the troubleshoot-ing procedure on electra (top) roberta (bottom).
∗means the risk of overﬁtting still exists based on theresult of bo+asha..grid search..the total running time for the procedure.
the execution for all experiments in table 10and 11 took 6.8×4v100 gpu days.
thisis in contrast to the cost if we enumerate all 5factors in section 3, which is 16×4v100 gpu days..a caveat on results in table 5. for all studyresults in this paper (i.e., table 3, table 10 andtable 11), we have repeated each hpo run threetimes.
therefore if a case succeed in table 5,it is because no overﬁtting is detected in the 3repetitions, if we ran more repetitions, the risk ofoverﬁtting can increase.
in addition, all resultsare evaluated under transformers version=3.4.0and ray version=1.2.0.
if these versions change,results in table 5 may change..an analysis on the relation between overﬁt-ting and train/validation/test split.
as overﬁt-ting indicates a negative correlation between thevalidation and test accuracy, one hypothesis is that.
overﬁtting is caused by the different distribution ofthe validation and test set.
we thus compare hporuns using the original glue spilt and a new splitwhich uniformly partition the train/validation/testdata.
the results can be found in appendix a.5..6 related work.
6.1 automated hyperparameter.
optimization.
hyperparameter optimization methods for genericmachine learning models have been studied for adecade (feurer and hutter, 2019; bergstra et al.,2011; bergstra and bengio, 2012; swersky et al.,2013).
prior to that, grid search was the most com-mon tuning strategy (pedregosa et al., 2011).
itdiscretizes the search space of the concerned hy-perparameters and tries all the values in the grid.
itcan naturally take advantage of parallelism.
how-ever, the cost of grid search increases exponen-tially with hyperparameter dimensions.
a simpleyet surprisingly effective alternative is to use ran-dom combinations of hyperparameter values, es-pecially when the objective function has a low ef-fective dimension, as shown in (bergstra and ben-gio, 2012).
bayesian optimization (bo) (bergstraet al., 2011; snoek et al., 2012) ﬁts a probabilis-tic model to approximate the relationship betweenhyperparameter settings and their measured per-formance, uses this probabilistic model to makedecisions about where next in the space to acquirethe function value, while integrating out uncer-tainty.
since the training of deep neural networksis very expensive, new hpo methods have beenproposed to reduce the cost required.
early stop-ping methods (karnin et al., 2013; li et al., 2017,2020) stop training with unpromising conﬁgura-tions at low ﬁdelity (e.g., number of epochs) bycomparing with other conﬁgurations trained at thesame ﬁdelity.
empirical study of these methodsis mostly focused on the vision or reinforcementlearning tasks, there has been few work focusing onnlp models.
asha was evaluated on an lstmmodel proposed in 2014 (zaremba et al., 2014).
in(wang et al., 2015), the authors empirically studiedthe impact of a multi-stage algorithm for hyper-parameter tuning.
in (zhang and duh, 2020), alook-up table was created for hyperparameter op-timization of neural machine translation systems.
in blendsearch (wang et al., 2021), an economicalblended search strategy was proposed to handle het-erogeneous evaluation cost in general and demon-.
2293strates its effectiveness in ﬁne-tuning a transformermodel turing-nlrv2.7 some existing work hasaddressed overﬁtting in hpo (l´evesque, 2018) orneural architecture search (zela et al., 2020).
forhpo, cross validation can help alleviate the overﬁt-ting when tuning svm (l´evesque, 2018), which israrely applied in deep learning due to high compu-tational cost.
for neural architecture search (zelaet al., 2020), the solution also cannot be appliedto our case due to the difference between the twoproblems..6.2 fine-tuning pre-trained language.
models.
as ﬁne-tuning pre-trained language models hasbecome a common practice, existing works havestudied how to improve the performance of the ﬁne-tuning stage.
among them, many has focused onimproving the robustness of ﬁne-tuning.
for exam-ple, ulmfit (howard and ruder, 2018) shows thatan effective strategy for reducing the catastrophicforgetting in ﬁne-tuning is to use the slanted tri-angular learning rate scheduler (i.e., using a non-zero number of warmup steps).
other strategiesfor controlling overﬁtting in ﬁne-tuning includefreezing a part of the layers to reduce the numberof parameters, and gradually unfreezing the lay-ers (peters et al., 2019), adding regularization termto the objective function of ﬁne-tuning (jiang et al.,2020), multi-task learning (phang et al., 2018).
ap-plying these techniques may reduce overﬁtting inour experiments; however, our goal is to comparegrid search and hpo, if these techniques are help-ful, they are helpful to both.
to simplify the com-parison, we thus focus on ﬁne-tuning the originalmodel.
meanwhile, the performance of ﬁne-tuningcan be signiﬁcantly different with different choicesof the random seeds (dodge et al., 2020).
to re-move the variance from random seed, we have ﬁxedall the random seeds to 42, although hpo can beused to search for a better random seed.
(zhanget al., 2021) identiﬁes the instability of ﬁne-tuningbert model in few-sample cases of glue (i.e.,rte, mrpc, sts-b, and cola).
similar to ourwork, they also found that overﬁtting increaseswhen searching for more trials.
however, theyhave not compared grid search with hpo.
thereare also many discussions on how to control over-ﬁtting by tuning hyperparameters (in manual tun-ing), e.g., learning rate (smith and le, 2017), batch.
7msturing.org.
size (smith et al., 2017), dropout rates (srivastavaet al., 2014; loshchilov and hutter, 2019, 2018),which may help with designing a search space forhpo that overﬁts less..7 conclusions, discussions and future.
work.
our study suggests that for the problem of ﬁne-tuning pre-trained language models, it is difﬁcultfor automated hpo methods to outperform manu-ally tuned grid conﬁgurations with a limited timebudget.
however, it is possible to design a system-atic procedure to troubleshoot the performance ofhpo and improve the performance.
we ﬁnd thatsetting the search space appropriately per modeland per task is crucial.
having that setting auto-mated for different models and tasks is beneﬁcial toachieve the goal of automated hpo for ﬁne-tuning.
for example, one may consider automatically min-ing the pattern from table 8&9 to identify the hy-perparameters that likely cause overﬁtting.
further,for the tasks remaining to be unsuitable for hpo,other means to reduce overﬁtting is required.
onepossibility is to use a different metric to optimizeduring hpo as a less overﬁtting proxy of the targetmetric on test data..previous work has shown that random seed iscrucial in the performance of ﬁne-tuning (dodgeet al., 2020).
fine-tuning also beneﬁts from en-sembling or selecting a few of the best performingseeds (liu et al., 2019).
it would be interesting tostudy hpo’s performance by adding the randomseed to the search space for future work..in our study, the simple random search methodstands strong against more advanced bo and earlystopping methods.
it suggests room for research-ing new hpo methods specialized for ﬁne-tuning.
a method that can robustly outperform randomsearch with a small resource budget will be useful.
it is worth mentioning that although we ﬁndhpo sometimes underperforms grid search, thegrid search conﬁgurations we study are the defaultones recommended by the pre-trained languagemodels for ﬁne tuning, therefore they may be al-ready extensively tuned.
we may not conclude thathpo is not helpful when manual tuning has notbeen done.
how to leverage hpo methods in thatscenario is an open question..2294references.
takuya akiba, shotaro sano, toshihiko yanase,takeru ohta, and masanori koyama.
2019. op-tuna: a next-generation hyperparameter optimiza-in sigkdd international con-tion framework.
ference on knowledge discovery & data mining,pages 2623–2631..james bergstra and yoshua bengio.
2012. randomsearch for hyper-parameter optimization.
the jour-nal of machine learning research, 13(1):281–305..james s bergstra, r´emi bardenet, yoshua bengio, andbal´azs k´egl.
2011. algorithms for hyper-parameterin advances in neural informationoptimization.
processing systems..kevin clark, minh-thang luong, quoc v. le, andelectra: pre-christopher d. manning.
2020.training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations..zihang dai, guokun lai, yiming yang, and quoc le.
2020. funnel-transformer: filtering out sequentialredundancy for efﬁcient language processing.
in ad-vances in neural information processing systems,volume 33, pages 4271–4282.
curran associates,inc..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in north american chapter of the asso-standing.
ciation for computational linguistics: human lan-guage technologies, pages 4171–4186, minneapo-lis, minnesota.
association for computational lin-guistics..jesse dodge, suchin gururangan, dallas card, royschwartz, and noah a. smith.
2019. show yourwork: improved reporting of experimental results.
in empirical methods in natural language process-ing and the international joint conference on natu-ral language processing, pages 2185–2194, hongkong, china.
association for computational lin-guistics..jesse dodge, gabriel ilharco, roy schwartz, alifarhadi, hannaneh hajishirzi, and noah smith.
2020.fine-tuning pretrained language models:weight initializations, data orders, and early stop-ping.
arxiv preprint arxiv:2002.06305..matthias feurer and frank hutter.
2019. hyperparam-eter optimization.
in automated machine learning,pages 3–33.
springer, cham..pengcheng he, xiaodong liu, jianfeng gao, andweizhu chen.
2021.deberta: decoding-enhanced bert with disentangled attention.
in inter-national conference on learning representations..jeremy howard and sebastian ruder.
2018. univer-sal language model ﬁne-tuning for text classiﬁca-in annual meeting of the association fortion.
computational linguistics (volume 1: long papers),pages 328–339, melbourne, australia.
associationfor computational linguistics..max jaderberg, valentin dalibard, simon osindero,wojciech m czarnecki, jeff donahue, ali razavi,oriol vinyals, tim green, iain dunning, karen si-monyan, et al.
2017. population based training ofneural networks.
arxiv preprint arxiv:1711.09846..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledregularized optimization.
in annual meeting of theassociation for computational linguistics, pages2177–2190, online.
association for computationallinguistics..amog kamsetty.
2020. hyperparameter optimizationfor huggingface transformers: a guide.
https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b..zohar karnin, tomer koren, and oren somekh.
2013.almost optimal exploration in multi-armed bandits.
in international conference on machine learning,pages 1238–1246.
pmlr..julien-charles l´evesque.
2018. bayesian hyperparam-eter optimization: overﬁtting, ensembles and condi-tional spaces..liam li, kevin jamieson, afshin rostamizadeh, ekate-rina gonina, jonathan ben-tzur, moritz hardt, ben-jamin recht, and ameet talwalkar.
2020. a systemfor massively parallel hyperparameter tuning.
inmachine learning and systems..lisha li, kevin jamieson, giulia desalvo, afshin ros-tamizadeh, and ameet talwalkar.
2017. hyperband:a novel bandit-based approach to hyperparameteroptimization.
the journal of machine learning re-search, 18(1):6765–6816..joseph e gonzalez,.
richard liaw, eric liang, robert nishihara, philippmoritz,and ion stoica.
2018. tune: a research platform for distributedarxiv preprintmodel selection and training.
arxiv:1807.05118..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv preprint arxiv:1907.11692..ilya loshchilov and frank hutter.
2018. fixing weight.
decay regularization in adam..ilya loshchilov and frank hutter.
2019. decoupledin international con-.
weight decay regularization.
ference on learning representations..2295lidan wang, minwei feng, bowen zhou, bing xi-ang, and sridhar mahadevan.
2015. efﬁcient hyper-parameter optimization for nlp applications.
in con-ference on empirical methods in natural languageprocessing, pages 2112–2117..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in empirical methods in natural language pro-cessing: system demonstrations, pages 38–45, on-line.
association for computational linguistics..wojciech zaremba, ilya sutskever, and oriol vinyals.
recurrent neural network regularization..2014.arxiv preprint arxiv:1409.2329..arber zela, thomas elsken, tonmoy saikia, yassinemarrakchi, thomas brox, and frank hutter.
2020.understanding and robustifying differentiable archi-in international conference ontecture search.
learning representations..tianyi zhang, felix wu, arzoo katiyar, kilian qweinberger, and yoav artzi.
2021. revisiting few-sample bert ﬁne-tuning.
in international confer-ence on learning representations..xuan zhang and kevin duh.
2020. reproducible andefﬁcient benchmarks for hyperparameter optimiza-tion of neural machine translation systems.
transac-tions of the association for computational linguis-tics, 8:393–408..fabian pedregosa, ga¨el varoquaux, alexandre gram-fort, vincent michel, bertrand thirion, oliviergrisel, mathieu blondel, peter prettenhofer, ronweiss, vincent dubourg, jake vanderplas, alexan-dre passos, david cournapeau, matthieu brucher,matthieu perrot, and ´edouard duchesnay.
2011.scikit-learn: machine learning in python.
jmlr,12:2825–2830..to tune or not to tune?.
matthew e. peters, sebastian ruder, and noah a.adapt-smith.
2019.ing pretrained representations to diverse tasks.
inworkshop on representation learning for nlp(repl4nlp-2019), pages 7–14, florence, italy.
as-sociation for computational linguistics..jason phang, thibault f´evry, and samuel r bow-man.
2018. sentence encoders on stilts: supple-mentary training on intermediate labeled-data tasks.
arxiv preprint arxiv:1811.01088..samuel l. smith, pieter-jan kindermans, and quoc v.le.
2017. don’t decay the learning rate, increase thebatch size.
in international conference on learningrepresentations..samuel l. smith and quoc v. le.
2017. a bayesianperspective on generalization and stochastic gradientin international conference on learningdescent.
representations..jasper snoek, hugo larochelle, and ryan p adams.
2012. practical bayesian optimization of machinelearning algorithms.
in advances in neural informa-tion processing systems..nitish srivastava, geoffrey hinton, alex krizhevsky,ilya sutskever, and ruslan salakhutdinov.
2014.dropout: a simple way to prevent neural networksfrom overﬁtting.
the journal of machine learningresearch, 15(1):1929–1958..kevin swersky, jasper snoek, and ryan p adams..2013. multi-task bayesian optimization.
26..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-form for naturalinemnlp workshop blackboxnlp: analyzing and in-terpreting neural networks for nlp, pages 353–355,brussels, belgium.
association for computationallinguistics..language understanding..chi wang, qingyun wu, silu huang, and amin saied.
2021. economic hyperparameter optimization within international confer-blended search strategy.
ence on learning representations..2296a appendixa.1 hpo checkpoint settings.
in this paper, we report the validation and test accu-racy of the best checkpoint (in terms of validationaccuracy) of the best trial instead of the last check-point of the best trial.
while the default settingin ray tune uses the last checkpoint, when ﬁne-tuning pretrained language model without hpo,the best checkpoint is more widely used than thelast checkpoint.
to further study the difference be-tween the two settings, we compare their validationand test accuracy of grid search using electra onthree tasks: wnli, rte and mrpc.
the resultshows that the validation and test accuracy of thebest checkpoint of the best trial are both higherthan those of the last checkpoint of the best trial.
as a result, we propose and advocate to report thebest checkpoint of all the trials for hpo ﬁne-tuningpretrained language models.
the checkpoint fre-quencies in our experiment are set to 10 per epochfor larger tasks (sst, qnli, and mnli) and 5per epoch for smaller tasks (wnli, rte, mrpc,cola and sts-b), with lower frequency in smallertasks to reduce the performance drop caused by fre-quent i/os within a short time..a.2 number of trials searched by hpo.
in table 6, we show the number of trials searchedby each hpo algorithms in the initial comparativestudy ( table 3)..hpownlirtemrpccolasts-bsstqnlimnli.
rs asha bo+asha46594547.
1238363033302427.
1227363131332631.table 6: average numbers of trials searched by eachhpo algorithm in the initial experiment on electra..a.3 choosing the hyperparameter to fix.
the hyperparameters of the best trials in overﬁtingruns are shown in table 8 and table 9. we usecolors to denote the comparison with the hyper-parameter value in grid search: dark grey if the.
value is higher than grid search; light grey if thevalue is lower than grid search..a.4 execution results of procedure.
in table 10 and table 11, we show the executionresults of applying the experimental procedure toelectra and roberta respectively..a.5 an analysis on overﬁtting andtrain/validation/test split.
in this paper, we have observed that hpo tendsto overﬁt when the number of trials/time budgetincreases.
in other words, the higher the validationscore, the lower the test score.
one hypothesis forthe reason behind this phenomenon is that the vali-dation set has a different distribution than the testset.
since glue is a collection of nlp datasetsfrom different sources, it is unclear whether thevalidation and test set in all glue tasks share thesame distribution..origin.
resplit.
validation93.393.293.293.1.test93.393.293.193.4.validation91.991.791.691.6.test91.891.691.191.5.table 7: comparison of the orders of validation and testscores for the original split of glue and resplit..to observe whether hpo still overﬁts under a uni-formly random split, we have performed the follow-ing experiment: we merge the training and valida-tion folds of qnli in glue, randomly shufﬂe themerged data, and resplit it into train/validation/testwith the proportion 8:1:1. we run random search,rank all trials based on the validation accuracy,and examine the pearson correlation coefﬁcientbetween the top-4 trials’s validation and test ac-curacies (the trials are ranked by the validationaccuracy), which are listed in table 7. for theoriginal glue dataset, we also save the bestcheckpoints of the top 4 trials and submit themto the glue website to get the test accuracies.
the pearson coefﬁcient of the original dataset is(r = −0.1414, p = 0.858) while for resplit it is(r = 0.6602, p = 0.339).
thus one potential ex-planation of the observed overﬁtting in this workis due to different distribution between validationand test data..2297hpo runmrpc, gridmrpc, rs, rep 1mrpc, rs, rep 2mrpc, asha, rep 1mrpc, asha, rep 2mrpc, asha, rep 3mrpc, opt+asha, rep 1mrpc, opt+asha, rep 2sst, gridsst, rs, rep 1sts-b, gridsts-b, opt+asha, rep 1.val acc92.3/89.292.7/90.093.4/90.992.8/90.093.4/90.992.9/90.093.0/90.493.3/90.795.195.491.5/91.491.6/91.4.
test acc91.1/87.990.4/87.190.6/87.690.8/87.690.5/87.490.4/86.990.7/87.590.4/86.995.795.689.7/89.289.6/89.1.
lr1.0e-43.9e-54.3e-56.5e-53.1e-51.3e-46.4e-58.0e-53.0e-53.1e-51.0e-44.7e-5.
wr0.1000.0140.0050.0750.0300.0660.0840.0100.1000.0110.1000.015.bs321616161632163232323232.hidd.
do0.1000.0500.0440.0380.0670.0970.1960.0310.1000.0060.1000.028.att.
do0.1000.0630.0240.0900.0970.0150.0020.1080.1000.0440.1000.082.table 8: comparison between the hyperparameter values of the best trial of grid search and the best trials (invalidation accuracy) of all the 9 overﬁtting hpo runs (out of 72) in the initial comparative study using electra(table 3).
dark grey indicates the value is higher than grid search;light grey indicates the value is lower thangrid search.
hpo runwnli,gridwnli,rs,rep 3cola,gridcola,asha, rep 1cola,opt+asha,rep 1rte,gridrte,rs,rep 1rte,asha,rep 3rte,opt+asha,rep 2mrpc,gridmrpc,rs,rep 2mrpc,rs,rep 3mrpc,asha,rep 3mrpc,opt+asha,rep 3sts-b,gridsts-b,asha,rep 1sts-b,asha,rep 2sts-b,opt+asha,rep 1.val acc56.360.665.165.565.479.880.580.581.993.1/90.493.2/90.793.2/90.493.3/90.793.5/91.291.2/90.891.3/91.091.4/91.191.3/90.9.
test acc65.164.461.759.559.473.973.673.273.590.5/87.189.6/86.190.3/86.790.3/86.889.6/86.289.3/88.489.0/88.289.0/88.289.1/88.2.
lr-1.8e-53.0e-52.7e-52.3e-53.0e-52.8e-52.4e-52.7e-52.0e-52.4e-51.4e-52.7e-52.7e-52.0e-52.0e-52.1e-42.7e-5.
wr0.0600.1110.0600.0200.0670.0600.0850.0220.0240.0600.0940.0030.0080.0360.0600.0420.0610.052.bs-1616323216161632166416161616161616.hidd.
do0.1000.1280.1000.0900.0630.1000.0250.0530.0830.1000.0190.0110.1400.0940.1000.0040.0560.096.att.
do0.1000.1220.1000.1970.1170.1000.1730.1370.1900.1000.1380.0620.1300.1530.1000.0610.0080.070.wd0.1000.0780.1000.1800.2930.1000.1420.0160.0940.1000.2990.1760.2550.2910.1000.2470.2260.224.table 9: comparison between the hyperparameter values of the best trial of grid search and the best trials (invalidation accuracy) of all the 11 overﬁtting hpo runs (out of 45) in the initial comparative study using roberta(table 3).
dark grey indicates the value is higher than grid search;light grey indicates the value is lower thangrid search.
2298random search.
asha.
↑ res.
↑ res.
↓ space.
↓ space.
↓ space.
↓ space.
round 2.round 1.round 2.round 1.test val.
test val.
test val.
test val.
test val.
round 0val.
round 3test.
round 3test.
57.7 63.0 59.2 65.857.7 59.6 57.7 65.156.3 65.1 57.7 65.857.2 62.6 58.2 65.6.round 0test valval56.3 65.1 ↓ space57.7 62.3 57.7 65.856.3 65.8 57.7 65.156.3 65.1 57.7 65.156.8 64.4 57.7 65.384.1 76.881.9 76.1 84.5 74.6 84.1 76.1 84.8 75.3 81.9 76.2 83.4 75.381.6 75.1 83.8 74.5 83.0 74.0 84.1 75.7 75.5 72.1 81.9 73.983.0 75.7 83.4 74.7 82.3 73.1 83.8 75.2 83.4 74.1 83.8 74.482.2 75.6 83.9 74.6 83.1 74.4 84.2 75.4 80.3 74.1 83.0 74.589.2 87.9 ↓ space90.9 87.6 90.7 86.3 90.4 86.590.0 87.1 90.2 87.2 90.7 86.590.2 87.8 90.7 86.9 90.7 87.890.4 87.5 90.5 86.8 90.6 87.4↑ res91.4 89.290.8 89.1 91.5 89.489.6 85.9 91.4 89.690.1 87.7 91.5 89.990.2 87.6 91.4 89.695.1 95.7 ↓ space95.4 95.6 93.2 93.8 96.0 94.7 95.6 95.2 95.4 95.8 95.5 95.3 95.5 95.2 95.2 94.994.3 95.1 94.7 95.0 95.3 95.7 95.1 95.7 94.4 94.1 95.1 94.7 94.8 94.3 94.2 93.694.5 94.6 95.8 95.7 95.5 95.8 95.0 94.5 95.0 94.9 95.4 95.4 94.5 93.5 94.8 94.594.7 95.1 94.6 94.8 95.6 95.4 95.2 95.1 94.9 94.9 95.3 95.1 94.9 94.3 94.7 94.393.5 93.593.0 92.9 93.2 93.493.1 93.6 93.3 93.392.9 92.5 93.3 93.193.0 93.0 93.3 93.3.
↓ space↓ space90.9 87.4 90.0 87.2 90.2 87.690.0 86.9 90.4 87.8 90.9 88.390.0 87.6 89.5 86.0 90.7 87.690.3 87.3 90.4 87.0 90.6 87.8↑ res.
91.3 89.2 91.5 89.891.5 89.7 91.4 89.291.0 88.3 91.4 89.291.3 89.1 91.4 89.4.
92.5 92.4 93.4 93.293.4 93.0 93.2 93.193.4 93.4 93.2 93.093.1 92.9 93.3 93.1.
↓ space.
↓ space.
↓ space.
↑ res.
↑ res.
↑ res.
↑ res.
wnli.
rte.
mrpc.
sts-b.
sst.
qnli.
table 10: the execution results of applying the procedure on electra.
curacy is denoted using the blue bold font..each task’s grid search ac-an hpo run is highlighted in dark grey if it overﬁts.
and.
medium grey if it overﬁts weakly ..the.
average.
of.
3.repetitions.
is.
highlighted.
in.
light grey if it outperforms grid search’s validation and test accuracy .
man correlation, for mrpc we only report the accuracy..for sts-b we only report the spear-.
2299round 3test.
test val.
random search.
asha.
↓ space.
↓ space.
↓ space.
↓ space.
round 2.round 1.round 2.round 1.test val.
test val.
test val.
test val.
round 0val.
round 3test.
↓ space59.2 65.1 59.2 65.1 57.7 65.856.3 65.1 56.3 65.1 56.3 65.156.3 65.1 56.3 65.1 56.3 65.157.3 65.1 57.3 65.1 56.8 65.3↓ space80.5 73.2 80.5 73.3 79.8 72.580.2 74.9 82.0 72.9 79.1 73.480.5 74.1 80.5 73.5 79.8 73.780.8 74.1 80.5 73.3 79.5 73.2↓ space.
round 0valtest val56.3 65.1 ↓ space60.6 64.4 62.0 64.4 57.7 62.356.3 65.1 56.3 65.1 56.3 65.156.3 65.1 56.3 65.1 56.3 65.157.8 64.9 58.2 64.9 56.8 64.279.8 73.9 ↓ space81.2 73.9 80.1 72.8 81.6 72.280.5 73.6 81.2 72.9 75.5 72.179.4 73.1 79.8 73.6 79.8 72.680.4 73.5 80.4 73.1 78.9 72.390.4 87.1 ↓ space90.7 86.1 90.7 86.9 91.2 86.790.4 86.7 90.4 88.0 90.2 87.690.9 87.2 91.2 87.2 90.4 87.090.7 86.7 90.8 87.4 90.6 87.1↓ space↑ res65.1 61.764.3 60.1 66.0 59.3 65.8 59.2 65.3 60.2 65.5 59.5 65.0 60.9 65.9 58.264.6 60.5 65.0 60.5 65.0 61.7 65.4 62.5 63.6 58.8 62.9 58.4 63.9 58.963.5 56.8 64.4 60.3 65.2 60.7 64.6 58.5 64.6 60.0 64.9 62.0 64.4 59.064.1 59.1 65.1 60.0 65.3 60.5 65.1 60.4 64.5 59.4 64.3 60.4 64.7 58.790.8 88.4 ↓ space90.8 88.3 91.0 88.990.8 88.9 90.8 88.691.2 88.7 90.9 88.990.9 88.6 90.9 88.8.
↓ space91.1 88.2 90.9 88.3 90.8 88.691.0 88.2 90.8 88.5 91.0 88.590.7 88.5 90.9 88.4 90.9 88.790.9 88.3 90.8 88.4 90.9 88.6.
90.7 86.8 91.4 87.790.4 87.4 90.4 87.291.4 87.6 90.4 87.690.8 87.3 90.8 87.5.
↓ space.
↓ space.
↓ space.
↓ space.
↓ space.
wnli.
rte.
mrpc.
cola.
sts-b.
11:.
the.
task’stablegrid search accuracy is denoted using the blue bold font.
an hpo run is highlighted indark grey if it overﬁts and medium grey if it overﬁts weakly .
the average of 3 repetitions is highlighted.
on roberta..procedure.
execution.
applying.
results.
each.
the.
of.
in light grey if it outperforms grid search’s validation and test accuracy .
for sts-b we only report the spearmancorrelation, for mrpc we only report the accuracy..2300