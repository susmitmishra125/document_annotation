de-biasing distantly supervised named entity recognitionvia causal intervention.
wenkai zhang1,3, hongyu lin1,∗, xianpei han1,2,∗, le sun 1,21chinese information processing laboratory 2state key laboratory of computer scienceinstitute of software, chinese academy of sciences, beijing, china3 university of chinese academy of sciences, beijing, china{wenkai2019,hongyu,xianpei,sunle}@iscas.ac.cn.
abstract.
distant supervision tackles the data bottleneckin ner by automatically generating traininginstances via dictionary matching.
unfortu-nately, the learning of ds-ner is severelydictionary-biased, which suffers from spu-rious correlations and therefore underminesthe effectiveness and the robustness of thelearned models.
in this paper, we fundamen-tally explain the dictionary bias via a struc-tural causal model (scm), categorize the biasinto intra-dictionary and inter-dictionary bi-ases, and identify their causes.
based on thescm, we learn de-biased ds-ner via causalinterventions.
for intra-dictionary bias, weconduct backdoor adjustment to remove thespurious correlations introduced by the dictio-nary confounder.
for inter-dictionary bias, wepropose a causal invariance regularizer whichwill make ds-ner models more robust to theperturbation of dictionaries.
experiments onfour datasets and three ds-ner models showthat our method can signiﬁcantly improve theperformance of ds-ner..1.introduction.
named entity recognition (ner) aims to identifytext spans pertaining to speciﬁc semantic types,which is a fundamental task of information extrac-tion, and enables various downstream applicationssuch as relation extraction (lin et al., 2016) andquestion answering (bordes et al., 2015).
thepast several years have witnessed the remarkablesuccess of supervised ner methods using neuralnetworks (lample et al., 2016; ma and hovy, 2016;lin et al., 2020), which can automatically extracteffective features from data and conduct ner inan end-to-end manner.
unfortunately, supervisedmethods rely on high-quality labeled data, whichis very labor-intensive, and thus severely restricts.
∗corresponding authors.
figure 1: dictionary bias in ds-ner happens both atintra-dictionary and inter-dictionary aspects: (a) aver-aged likehoods of mentions in/not in the dictionary sig-niﬁcantly diverge.
(b) mention likehoods of the modelsusing different dictionaries signiﬁcantly diverge..the application of current ner models.
to resolvethe data bottleneck, a promising approach is dis-tant supervision based ner (ds-ner).
ds-nerautomatically generates training data by matchingentities in easily-obtained dictionaries with plaintexts.
then this distantly-labeled data is used totrain ner models, commonly be accompanied bya denoising step.
ds-ner signiﬁcantly reducesthe annotation cost for building an effective nermodel, and therefore has attracted great attention inrecent years (yang et al., 2018; shang et al., 2018;peng et al., 2019; cao et al., 2019; liang et al.,2020; zhang et al., 2021)..however, the learning of ds-ner is dictionary-biased, which severely harms the generalizationand the robustness of the learned ds-ner models.
speciﬁcally, entity dictionaries are often incom-plete (missing entities), noisy (containing wrongentities), and ambiguous (a name can be of differ-ent entity types, such as washington).
and ds willgenerate positively-labeled instances from the in-dictionary names but ignore all other names.
sucha biased dataset will inevitably mislead the learnedmodels to overﬁt in-dictionary names and underﬁtout-of-dictionary names.
we refer to this as intra-dictionary bias.
to illustrate this bias, figure 1(a) shows the predicting likelihood of a representa-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4803–4813august1–6,2021.©2021associationforcomputationallinguistics48030.30.40.50.6d1d2d3d4all mentions(b)0.00.20.40.60.81.0twitterconllwebpagewikigoldin dictionarynot in dictionary(a)figure 2: the proposed structural causal model for ds-ner.
it can be roughly divided into two parts: distantsupervision (ds) and ner.
from the scm, we identify that the intra-dictionary bias stems from the spuriouscorrelations caused by backdoor paths, while the inter-dictionary bias stems from the over-ﬁt on the dictionarycharacteristics.
detailed explanations can be found in section 2..tive ds-ner model (roberta + classiﬁer (lianget al., 2020)).
we can see that there is a remark-able likelihood gap between in-dictionary mentionsand out-of-dictionary mentions: the average like-lihoods of out-of-dictionary mentions are < 0.2,which means that a great majority of them cannotbe recalled.
furthermore, such a skewed distribu-tion makes ds-ner models very sensitive to slightperturbations.
we refer to this as inter-dictionarybias, i.e., different dictionaries can result in verydifferent model behaviors.
in the example shown infigure 1 (b), we train the same ds-ner model byrespectively using 4 dictionaries sampled from thesame original dictionary, where each of them cov-ers 90% of entities in the original one.
we can seethat the predicting likelihood diverges signiﬁcantlyeven these 4 dictionaries share the majority part.
consequently, the dictionary-biased learning willundermine both the effectiveness and robustness ofds-ner models..in this paper, we propose a causal framework tofundamentally explain and resolve the dictionarybias problem in ds-ner.
we ﬁrst formulate theprocedure of ds-ner from the causal view with astructural causal model (scm) (pearl et al., 2000),which is shown in the left part of figure 2. fromthe scm, we identiﬁed that the intra-dictionarybias stemming from the dictionary which serves asa confounder during the model learning.
the dictio-nary confounder will introduce two backdoor paths,one from positively-labeled instances (x p) to entitylabels (y ) and the other from negatively-labeledinstances (x n) to entity labels.
these backdoorpaths introduce spurious correlations during learn-.
ing, therefore result in the intra-dictionary bias.
furthermore, the current learning criteria of ds-ner models is to optimize over the correlations be-tween the instances (x) and entity types (y ) givenone speciﬁc dictionary (d), namely p (y |x, d).
such criteria, however, diverges from the primarygoal of learning a dictionary-free ner model (i.e.,p (y |x)), and results in the inter-dictionary bias.
based on the above analysis, unbiased ds-nershould remove the spurious correlations introducedby backdoor paths and capture the true dictionary-free causal relations..to this end, we conduct causal interventions tode-bias ds-ner from the biased dictionary.
forintra-dictionary bias, we intervene on the positiveinstances and the negative instances to block thebackdoor paths in scm, then the spurious corre-lations introduced by dictionary confounder willbe removed.
speciﬁcally, we conduct backdooradjustment to learn de-biased ds-ner models,i.e., we optimize the ds-ner model based on thecausal distribution, rather than from the spuriouscorrelation distribution.
for inter-dictionary bias,we propose to leverage causal invariance regular-izer (mitrovic et al., 2021), which will make thelearned representation more robust to the perturba-tion of dictionaries.
for each instance in the train-ing data, causal invariance regularizer will preservethe underlying causal effects unchanged across dif-ferent dictionaries.
the proposed method is model-free, which can be used to resolve the dictionarybias in different ds-ner models by being appliedas a plug-in during model training..we conducted experiments on four standard ds-.
4804.02washington in us was named after george washingtongeorge washington,us,billgates,stevejobs...dsnerintra-dictionary biasdxpxnmxrydictionaryinwasnamedaftergeorgewashingtonwashingtonusinter-dictionary biaspositively labelednegatively labeledscmperlocwashingtonperlocusperlocgeorgewashingtonmodelner datasets: conll2003, twitter2005, web-page, and wikigold.
experiments on three state-of-the-art ds-ner models show that the proposedde-biasing method can effectively solve both intra-dictionary and inter-dictionary biases, and there-fore signiﬁcantly improve the performance and therobustness of ds-ner in almost all settings.
gen-erally, the main contributions of this paper are:.
• we proposed a causal framework, which notonly fundamentally formulates the ds-nerprocess, but also explains the causes of bothintra-dictionary bias and inter-dictionary bias..• based on the causal framework, we conductedcausal interventions to de-bias ds-ner.
forintra-dictionary bias, we conduct causal inter-ventions via backdoor adjustment to removespurious correlations introduced by the dictio-nary confounder.
for inter-dictionary bias, wepropose a causal invariance regularizer whichwill make ds-ner models more robust to theperturbation of dictionaries..• experimental results on four standard ds-ner datasets and three ds-ner modelsdemonstrate that our method can signiﬁcantlyimprove the performance and the robustnessof ds-ner..2 a causal view on ds-ner.
in this section, we formulate ds-ner with a struc-tural causal model (scm), then identify the causesof both intra-dictionary bias and inter-dictionarybias using the scm.
an scm captures the causaleffect between different variables and describes thegenerative process of a causal distribution, whichcan be visually presented using a directed acyclicgraph (dag).
in scm, each node represents a ran-dom variable, and a directed edge represents adirect causal relationship between two variables.
based on scm, the confounders and backdoorpaths (pearl et al., 2000) can be identiﬁed.
in thefollowing, we will describe the causal view of ds-ner and then identify the dictionary bias..2.1 structural causal model for ds-ner.
figure 2 shows the structural causal model fords-ner, which contains 7 key variables in theds-ner procedure: 1) the applied dictionary dfor distant annotation; 2) the unlabeled instancesx, where each instance is a pair of (mention can-didate, context), and in training stage x will be.
automatically labeled by d; 3) the positive train-ing instances x p, which are instances in x be-ing labeled as positive instances (i.e., entity men-tions) by dictionary d; 4) the negative traininginstances x n, which are instances being labeled asnegative instances by dictionary d; 5) the learnedds-ner model m , which summarizes ner evi-dences from ds-labeled data during training, andpredicts new instances during testing; 6) the repre-sentations of instances r, which is encoded denserepresentations of instances x using the learnedmodel m ; 7) the predicted entity labels y of in-stances in x based on the representation r..deﬁning these variables, the causal process ofds-ner can be formulated using scm into twosteps: distant supervision (ds) step and ner steprespectively.
for ds step, the procedure will gener-ate ds-labeled data and learn ds-ner models byfollowing causal relations:.
• d→x p←x and d→x n←x represent thedistant annotation process, which uses dictio-nary d to annotate the unlabeled instances xand splits them into two sets: x p and x n..• x p→m ←x n represents the learning pro-cess, where model m is the learned ds-nermodel using x p and x n. we denote thex p and x n generated from dictionary d asx p(d) and x n(d) respectively..and the causal relation in ner step can be summa-rized as:.
• m →r←x is the representation learning pro-cedure, which uses the learned model m toencode instances x..• r→y represents the entity recognition pro-cess, where the labels of instances depend onthe learned representation r and instances x.we denote the entity labels corresponding tox p and x n as y p and y n respectively..2.2 cause of intra-dictionary biasgiven distant annotation x p and x n, the learningprocess of ds-ner will maximize the probabil-ity p (y p=1, y n=0|x p, x n, d).
unfortunately,because d is a confounder for x p and x n inscm, this criteria will introduce spurious cor-relations and result in the intra-dictionary bias:(1) when maximizing p (y =1|x p, d), we wantner models to rely on the actual causal path.
4805x p→y .
however, in scm there exists a back-door path x p←d→x n→m which will introducespurious correlation between y and x p.intu-itively, this backdoor path appears as the false neg-ative instances in x n. because these false nega-tive instances have correct entity contexts but out-of-dictionary names, they will mislead the mod-els to underﬁt the entity context for prediction.
(2) when maximizing p (y =0|x n, d), we wantner models to rely on the actual causal pathx n→y .
however, in scm there exists a backdoorpathx n←d→x p→m which will introduce spu-rious correlation between y and x n. intuitively,this backdoor path appears as the false positiveinstances in x p. because these false positive in-stances have in-dictionary entity names but spuri-ous context, they will mislead the models to overﬁtthe names in dictionary..in general, the intra-dictionary bias is causedby backdoor paths introduced by d, and this biaswill mislead the ner models to overﬁt names indictionary and underﬁt the context of entities..2.3 cause of inter-dictionary bias.
as mentioned above, ds-ner models are learnedby ﬁtting p (y p=1, y n=0|x p, x n, d).
this cri-teria will mislead the model when learning thecorrelation between x and y with spurious in-formation in d because the learning criteria is con-ditioned on it.
however, a robust ner modelshould ﬁt the underlying distribution p (y |x),rather than the dictionary-conditioned distributionp (y |x, d).
from the scm, the dictionary d willsigniﬁcantly inﬂuence the learned ner models m ,and in turn result in different learned causal effectsin the path x → r → y and entity prediction y .
as a result, ds-ner models will ﬁt different un-derlying distributions given different dictionaries,and therefore results in inter-dictionary bias..however, in real-world applications, the dic-tionaries are affected by various factors, such assource, coverage or time.
therefore, to enhancethe robustness of the learning process, it is criticalto alleviate the spurious inﬂuence of dictionary don the learned causal effects between x and y .
that is, we want ds-ner models to capture thedictionary-invariant entity evidence, rather than ﬁtthe dictionary-speciﬁc features..3 de-biasing ds-ner via causal.
intervention.
in this section, we describe how to de-bias ds-ner.
speciﬁcally, for intra-dictionary bias, wepropose to use backdoor adjustment to block thebackdoor paths.
for inter-dictionary bias, we de-sign a causal invariance regularizer to capture thedictionary-invariant evidence for ner..3.1 removing intra-dictionary bias via.
backdoor adjustment.
based on the analysis in section 2.2, the intra-dictionary bias is caused by the backdoor pathsx p←d→x n→m and x n←d→x p→m .
toremove these biases, we block both backdoor pathsby intervening both x p and x n. after causal inter-vention, the learning of ds-ner models will ﬁt thecorrect causal relation p (y p=1|do(x p(d)), x n)andheredo(x p(d))=do(x p=x p(d))themathematical operation to intervene x p andpreserve it to be x p(d) in the whole population..p (y n=0|do(x n(d)), x p)..represents.
backdoor adjustments.
to calculate the distri-bution p (y p=1|do(x p(d))) after causal interven-tion, we conduct backdoor adjustment according tocausal theory (pearl, 2009):.
ppos(d)(cid:44)p (y p=1|do(x p(d))).
(cid:88).
=.
i.p (y p=1|x p(d), x n(di)).
(1).
× p (di).
the.
negative.
where x n(di)in-denotesstances generated from the ds dictionarydi.
p (y p=1|x p(d), x n(di)) is the probabilityof predicting x p(d) into y =1, which can beformulated using a neural network-based ds-nermodel parametrized by θ, i.e., p (y |x p, x n) =p (y |x p, x n; θ).
detailed derivations is shown inappendix a..note the distribution p (y p=1|do(x p(d))) inthe causal framework is not the marginalized distri-bution p (y p=1|x p(d)) in the probability frame-work.
otherwise the marginalization should takeplace in the conditional distribution p (di|x p)rather than p (di).
furthermore, as shown infigure 3, x p=x p(di) and x n=x n(dj) cannot happen together in probabilistic view unlessdi=dj.
however, in the causal view, they canhappened together via the causal intervention.
thatis do(x p=x p(di)) and x n=x n(dj), which is.
4806of previous ds-ner methods by adaptively chang-ing the underlying parametrization of probabilitydistribution p (y |x p, x n; θ) ..3.2 eliminating inter-dictionary bias viacausal invariance regularizer.
this section describes causal invariance regularizerto eliminate the inter-dictionary bias.
speciﬁcally,after backdoor adjustment for intra-dictionary bias,the causal distribution we optimize (i.e., ppos(d)and pneg(d)) still depends on the dictionary d. asa result, given different dictionaries, ds-ner mod-els will ﬁt different underlying causal distributionsand result in inter-dictionary bias..ideally, a robust ds-ner learning algorithmi.e., we should di-should be dictionary-free,rectly optimize towards the implicit distributionof p (y |x).
however, it is impossible to directlyachieve this because the golden answer y of x isinvisible in ds-ner.
to enhance the robustnessof the learning process, this section proposes acausal invariance regularizer, which ensures ds-ner models to learn useful entity evidence forner but not to ﬁt dictionary-speciﬁc features.
speciﬁcally, the goal of causal invariance (pearlet al., 2000) is to ensure learned ner models willkeep similar causal effects using different dictio-naries, which can be formulated as:.
θ∗inv= arg min.
θ.
(cid:107)ppos(di) − ppos(dj).
(4).
+pneg(di) − pneg(dj)(cid:107).
here || ∗ || measures the distance between twodistributions.
however, as we mentioned above,this distance cannot be directly optimized becausethe golden label y of x is unknown.
fortunately,in the scm, the impact from dictionary d tothe entity label y are all through the model mand the representation r, i.e., through the pathd → m → r → y .
as a result, the bias fromthe dictionary d can be eliminated by preservingthe causal effects between x and any node in thepath.
a simple and reasonable solution is to pre-serve the causal invariance of the representationr. that is, given different dictionaries, we keepthe causal effects from x to r unchanged, andtherefore causal effects of x → y will remain un-changed.
speciﬁcally, when learning causal effectsgiven an dictionary d, the causal invariance regu-larizer will further enhance its causal consistencywith other dictionaries by minimizing its represen-.
figure 3: an illustration on causal intervention.
(a)and (b) show the generated instances according to dic-tionary di and dj.
and (c) shows the generated in-stances from do(x p(di)) and x n(dj)..shown in figure 3 (c).
for more details, pleaserefer to (neal, 2020) for a brief introduction..similarly, to block the backdoor paths and calcu-late the causal distribution p (y n=0|do(x n(d))),we can conduct backdoor adjustment on x n by:.
pneg(d)(cid:44)p (y n=0|do(x n(d))).
(cid:88).
=.
i.p (y n=0|x n(d), x p(di)).
(2).
× p (di).
estimating dictionary probabilities.
becausewe only have one global dictionary d, it is hard toestimate the probability of other dictionary di usedin the equation (1) and (2).
to tackle this problem,we sample k sub-dictionaries by sampling entitiesfrom the global dictionary d. the probability ofeach entity being sampled corresponds to its utter-ance frequency in a large-scale corpus.
then weapplied a uniform probability assumption to thesesampled dictionaries, which means that these sub-dictionaries will then be used to conduct backdooradjustment with equal dictionary probabilities, i.e.,p (di) = 1k ..learning ds-ner models with causal rela-tion.
given the above two causal distributionsafter backdoor adjustment, the ds-ner modelscan be effectively learned, and the intra-dictionarybias can be eliminated based on the causal relationsbetween x p, x n and y .
formally, we optimizeds-ner models by minimizing the following neg-ative likelihood based on causal relation:.
lba(θ)= − log ppos(d) − log pneg(d).
(3).
note that the proposed method is model-free,which means that it can be applied to the majority.
4807positivenegative(a)(b)(c)djdididjdidjxp=xp(di)xn=xn(di)xp=xp(dj)xn=xn(dj)do(xp=xp(di))xn=xn(dj)tation distances to other dictionaries:.
lcir(θ; d)=.
||rd(x; θ).
k(cid:88).
(cid:88).
i=1.
x∈x.
− rdi(x)||2.
(5).
here rd(x; θ) is the representation of instance x,which is derived from the ner model m by ﬁttingthe causal effects of dictionary d. the referencedictionary di in the formulations are generated inthe same way as we described in section 3.1 and kis the number of sub-dictionaries.
therefore, thisregularizer ensures that the representations learnedusing different dictionaries will be consistent, andthe inter-dictionary bias is eliminated..finally, we combine (3) and (5) to de-bias bothintra-dictionary bias and inter-dictionary bias andobtain the ﬁnal ds-ner models by optimizing:.
(cid:88).
l =.
li.
ba + λlcir.
(6).
i.where λ is a hyper-parameter which controls therelative importance of these two losses and is tunedon the development set..4 experiments.
4.1 experimental settings.
datasets.
we conduct experiments on four stan-dard datasets: (1) conll2003 (tjong kim sangand de meulder, 2003) is a well known open-domain ner dataset.
it consists of 20744 sen-tences collected from 1393 english news articlesand is annotated with four types: per, org, locand misc.
(2) twitter (godin et al., 2015) is fromthe wnut 2016 ner shared task.
it consists of7236 sentences with 10 entity types.
(3) webpage(ratinov and roth, 2009) contains 20 webpages, in-cluding personal, academic and computer-scienceconference homepages.
it consists of 619 sentenceswith the four types the same as conll2003.
(4)wikigold (balasuriya et al., 2009) contains 149articles from the may 22, 2008 dump of englishwikipedia.
it consists of 1969 sentences with thesame types of conll2003..distant annotation settings.
we use two dis-tant annotation settings: string-matching and kb-matching (liang et al., 2020).
string-matchinglabels dataset by directly matching names in dictio-nary with sentences.
kb-matching is more com-plex, which uses a set of hand-crafted rules to.
match entities.
we ﬁnd kb-matching can gener-ate better data than string-matching, but string-matching is a more general setting.
in our ex-periments, we report performance on both kb-matching and string-matching settings..detail.
we.
implementationimplementbilstm-crf with allennlp (gardner et al.,2017), an open-source nlp research library, andthe input vector is the 100-dimension gloveembeddings (pennington et al., 2014).
for otherbaselines, we use the ofﬁcially released implemen-tation from the authors.
we openly release oursource code at github.com/zwkatgithub/dscau..4.2 baselines.
the proposed de-biased training strategy is bothmodel-free, and learning algorithm-free.
therefore,we use the following base ds-ner baselines andcompare the performance of using/not using ourde-biased training strategy:.
dictmatch , which perform ner by directlymatching text with names in a dictionary, so nolearning is needed..,.
including:.
fully-supervised baselines(i)bilstm-crf (lample et al., 2016), which usesglove (pennington et al., 2014) for word embed-dings; (ii) roberta-base (liu et al., 2019), whichencodes text using roberta-base then predict to-ken label via a multi-layer perceptron..naive distant supervision (naive), which di-rectly uses weakly labeled data to train a fully-supervised model.
it could be considered as thelower bound of ds-ner..positive-unlabeled learning (pu-learning)(peng et al., 2019), which formulates ds-ner asit coulda positive-unlabeled learning problem.
obtain unbiased loss estimation of unlabeled data.
however, it assumes that there are no false positiveinstances which may be incorrect in many datasets..bond (liang et al., 2020), which is a two-stagelearning algorithm: in the ﬁrst stage, it leveragespre-trained language model to improve the recalland precison of the ner model; in the secondstage, it adopts a self-training approach to furtherimprove the model performance..4.3 main results.
table 1 and table 2 show the overall performance(f1 scores) of different baselines and our methods..4808methods.
conll2003.
twitter.
webpage.
wikigold.
conll2003.
twitter.
webpage.
wikigold.
kb-matching.
string-matching.
supervised learning baselines.
bilstm-crfroberta-base.
dictmatchbilstm-crf.
85.9891.12.
71.4064.62.
32.3050.47.
35.8330.25.
51.5974.07.
52.4513.90.
57.0184.02.
47.7637.46.distant supervision baselines.
––.
––.
––.
––.
43.9160.52.
19.1831.67.
2.5624.67.
19.0427.97.roberta-base+ba (ours)+ba+cir (ours).
76.0476.43(+0.40)78.78(+2.74).
46.4046.75(+0.35)47.12(+0.72).
54.0759.28(+5.21)59.06(+4.99).
52.8353.56(+0.73)55.60(+2.77).
73.9475.43(+1.49)75.59(+1.65).
46.0246.69(+0.67)47.27(+1.25).
57.1458.71(+1.57)58.04(+0.90).
37.9442.23(+4.59)44.19(+6.25).
bond.
+ba (ours)+ba+cir (ours).
79.8380.81(+0.98)81.54(+1.71).
47.7248.45(+0.73)49.01(+1.29).
61.2864.65(+3.37)64.71(+3.43).
60.2360.81(+0.58)61.48(+1.25).
75.5176.21(+0.70)76.53(+1.02).
48.7249.12(+0.40)48.82(+0.10).
66.2366.67(+0.44)66.67(+0.44).
42.1742.53(+0.36)45.55(+3.38).
table 1: f1 scores on conll2003, twitter, webpage and wikigold.
ba and cir denotes the proposed back-door adjustment and causal invariance regularizer respectively.
we can see that the proposed causal interventionapproach achieves signiﬁcant improvements on almost all settings..kb-matching string-matching.
pu-learning.
+ba (ours)+ba+cir (ours).
74.9680.93(+5.97)81.96(+7.00).
72.4276.17(+3.75)76.62(+4.20).
table 2: f1 scores on conll2003 dataset based onpu-learning (peng et al., 2019).
we don’t reportthe results on other datasets, because pu-learningneeds high-quality dictionary which only provided byconll2003..for our method, we use ba to denote backdooradjustment, and cir to denote causal invarianceregularizer.
we conduct our debiasing method onthree base models: roberta-base, pu-learningand bond,therefore we have 6 systems ofour methods: roberta+ba, robert+ba+cir,pu-learning+ba, pu-learning+ba+cir, bond+ba, bond+ba+cir.
we can see that:.
(1) ds-ner models areseverely inﬂuenced by the dictionary bias.
withoutdebiasing, the naive ds-ner baselines bilstm-crf and roberta-base can only achieve compa-rable performance with the simple dictmatch base-lines.
and by taking the dictionary bias into con-sideration, pu-learning, bond with our methodcan signiﬁcantly improve the performance of ds-ner.
compared with dictmatch, they correspond-ingly achieve 4.99%, 21.98% f1 improvementson average.
this veriﬁed that the dictionary biasis critical for ds-ner models.
(2) by debias-ing ds-ner models via causal intervention, ourmethod can achieve signiﬁcant improvement.
com-pared with their counterparts, our full methodsroberta+ba+cir, bond+ba+cir correspond-.
ingly achieve 4.91%, 3.18% improvements aver-aged on four datasets in kb-matching (5.75%,2.56% improvements on string-matching) and pu-learning+ba+cir achieves 9.34% improvementon conll2003 dataset in kb-matching (5.80%improvement in string-matching).
this veriﬁedthe effectiveness of using causal intervention fordebiasing ds-ner.
(3) our method can effectivelyresolve both intra-dictionary and inter-dictionarybiases.
both of backdoor adjustment and causalinvariance regularizer can improve the ner perfor-mance.
by conducting backdoor adjustment, ourmethod can achieve a 3.27% f1 improvement av-eraged on all base models and all datasets.
andfurther conducting causal invariance regularizercan future improve 4.63% average f1..4.4 effects on robustness.
to verify whether the causal invariance regular-izer can signiﬁcantly improve the robustness ofds-ner across different dictionaries, we furthercompared the predicting likelihood of golden men-tions using different dictionaries.
speciﬁcally, wetrain the same roberta-classiﬁer ds-ner mod-els by sampling 4 dictionaries.
figure 4 shows theaverage predicting likelihood before/after using ourde-biasing method..from figure 4, we can see that the proposedcausal invariance regularizer signiﬁcantly reducedthe likelihood gaps between different dictionaries.
this veriﬁed that removing the inter-dictionary biascan signiﬁcant beneﬁt the robustness of ds-ner.
furthermore, we can see that the likelihoods ofgolden mentions are remarkably increased, whichrepresents a better ner performance.
these.
4809dictionary coverage.
figure 5 shows the re-sults with different dictionary coverages.
we cansee that our method is not sensitive to the cover-age of sub-dictionaries: it can achieve robust per-formance from 40% to 80% coverage.
all threemodels achieved the best performance at the 70%coverage.
this result demonstrates the robustnessof our method on dictionary coverage..dictionary quantity.
figure 6 shows the resultswith different sub-dictionary quantities.
we can seethat our method can achieve performance improve-ment by sampling more sub-dictionaries.
this isbecause more sub-dictionaries will lead to moreaccurate estimation of both the dictionary proba-bility in backdoor adjustment and the dictionaryvariance in causal invariance regularizer.
futher-more, we can see that the performance using onlyone sub-dictionary (i.e., ds-ner without causalintervention) is signiﬁcantly worse than other set-tings, this further veriﬁes the effectiveness of ourmethod..5 related work.
ds-ner.
supervised ner models have achievedpromising performance (lample et al., 2016; linet al., 2019a,b).
however, the reliance on labeleddata limits their applications in open situations.
distant supervision (mintz et al., 2009) is a promis-ing technique to alleviate the data bottleneck forner, which generates large-scale training data bymatching sentences with external dictionaries.
cur-rent ds-ner studies focus on denoising the dis-tantly labeled training data for better model learn-ing.
yang et al.
(2018) adopted reinforcement learn-ing for denoising.
shang et al.
(2018) proposed asequence labeling framework tieorbreak, whichcan avoid noise caused by a single word.
cao et al.
(2019) promoted the quality of data by exploitinglabels in wikipedia.
peng et al.
(2019) employedpositive-unlabeled learning to obtain unbiased es-timation of the loss value.
liang et al.
(2020) usedself-training method which leverages a pretrainedlanguage model as teacher model to guide the train-ing of student model.
causal inference.
causal inference (pearl, 2009;pearl and mackenzie, 2018) has been widelyadopted in psychology, politics and epidemiologyfor years (mackinnon et al., 2007; richiardi et al.,2013; keele, 2015).
it can provide more reliable ex-planations by removing confounding bias in data,and also provide debiased solutions by learning.
figure 4: the likelihood variance between differentdictionaries before/after using causal invariance regu-larizer (roberta-classiﬁer on conll2003), we cansee thatthe performance variance signiﬁcantly de-creases, which veriﬁes that causal invariance regular-izer can signiﬁcantly improve the robustness of ds-ner..figure 5: f1 scores when with different sub-dictionarycoverages on the test set of conll2003..figure 6: f1 scores when using different sub-dictionaryquantities on the test set of conll2003..all demonstrate the effectiveness of the proposedcausal invariance regularizer..4.5.inﬂuence of sub-dictionaries.
to conduct causal intervention, our method needsto sample sub-dictionaries from the original one.
to analyze the inﬂuence of the coverage and thequantity of sub-dictionaries, we conducted experi-ments on sub-dictionaries with different coveragesand different quantities..48100.652 0.466 0.654 0.499 0.656 0.441 0.673 0.472 0.40.50.60.7afterinterventionbeforeintervention71737577798183851234quantity of dictionarybondpulroberta7778798081828340%50%60%70%80%proportionbondpulroberta71737577798183851234quantity of dictionarybondpulroberta7778798081828340%50%60%70%80%proportionbondpulrobertacausal effect rather than correlation effect.
re-cently, many causal inference techniques are usedin computer vision (tang et al., 2020; qi et al.,2020) and natural language process (wu et al.,2020; zeng et al., 2020)..6 conclusions.
this paper proposes to identify and resolve the dic-tionary bias in ds-ner via causal intervention.
speciﬁcally, we ﬁrst formulate ds-ner using astructural causal model, then identity the causes ofboth intra-dictionary and inter-dictionary biases, ﬁ-nally de-bias ds-ner via backdoor adjustmentand causal invariance regularizer.
experimentson four datasets and three representative ds-nermodels veriﬁed the effectiveness and the robustnessof our method..acknowledgements.
this work is supported by the national natu-ral science foundation of china under grantsno.u1936207, beijing academy of artiﬁcialintelligence (baai2019qn0502), scientiﬁc re-search projects of the state language commission(yw135-78), and in part by the youth innovationpromotion association cas(2018141).
moreover,we thank all reviewers for their valuable commentsand suggestions..references.
dominic balasuriya, nicky ringland, joel nothman,tara murphy, and james r. curran.
2009. namedentity recognition in wikipedia.
in proceedings ofthe 2009 workshop on the people’s web meets nlp:collaboratively constructed semantic resources(people’s web).
association for computational lin-guistics..antoine bordes, nicolas usunier, sumit chopra, andjason weston.
2015. large-scale simple questionanswering with memory networks.
corr..yixin cao, zikun hu, tat-seng chua, zhiyuan liu, andheng ji.
2019. low-resource name tagging learnedwith weakly labeled data.
association for computa-tional linguistics..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthewpeters, michael schmitz, and luke s. zettlemoyer.
2017. allennlp: a deep semantic natural languageprocessing platform..fr´ederic godin, baptist vandersmissen, wesleyde neve, and rik van de walle.
2015. multimedia.
lab @ acl wnut ner shared task: named entityrecognition for twitter microposts using distributedin proceedings of the work-word representations.
shop on noisy user-generated text.
association forcomputational linguistics..luke keele.
2015. the statistics of causal inference: aview from political methodology.
political analysis,pages 313–335..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition.
association for computational linguistics..chen liang, yue yu, haoming jiang, siawpeng er,ruijia wang, tuo zhao, and chao zhang.
2020.bond: bert-assisted open-domain named entityin acmrecognition with distant supervision.
sigkdd international conference on knowledgediscovery and data mining..hongyu lin, yaojie lu, xianpei han, and le sun.
2019a.
sequence-to-nuggets: nested entity mentiondetection via anchor-region networks.
associationfor computational linguistics..hongyu lin, yaojie lu, xianpei han, le sun, bindong, and shanshan jiang.
2019b.
gazetteer-enhanced attentive neural networks for named en-in proceedings of the 2019 con-tity recognition.
ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 6232–6237, hong kong, china.
as-sociation for computational linguistics..hongyu lin, yaojie lu, jialong tang, xianpei han,le sun, zhicheng wei, and nicholas jing yuan.
2020. a rigorous study on named entity recogni-tion: can ﬁne-tuning pretrained model lead to thein proceedings of the 2020 con-promised land?
ference on empirical methods in natural languageprocessing (emnlp), pages 7291–7300, online.
as-sociation for computational linguistics..yankai lin, shiqi shen, zhiyuan liu, huanbo luan,and maosong sun.
2016. neural relation extractionwith selective attention over instances.
associationfor computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr, abs/1907.11692..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-in proceedings of the 54th annual meet-crf.
ing of the association for computational linguistics(volume 1: long papers).
association for computa-tional linguistics..david p mackinnon, amanda j fairchild,.
andmatthew s fritz.
2007. mediation analysis.
annu.
rev.
psychol., 58:593–614..4811erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on naturallanguage learning at hlt-naacl 2003..yiquan wu, kun kuang, yating zhang, xiaozhong liu,changlong sun, jun xiao, yueting zhuang, luo si,and fei wu.
2020. de-biased court’s view genera-tion with causality.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 763–780..yaosheng yang, wenliang chen, zhenghua li,zhengqiu he, and min zhang.
2018. distantly su-pervised ner with partial annotation learning andreinforcement learning.
association for computa-tional linguistics..xiangji zeng, yunliang li, yuchen zhai, and yinzhang.
2020. counterfactual generator: a weakly-supervised method for named entity recognition.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7270–7280..wenkai zhang, hongyu lin, xianpei han, le sun,huidan liu, zhicheng wei, and nicholas yuan.
2021. denoising distantly supervised named en-tity recognition via a hypergeometric probabilisticmodel.
proceedings of the aaai conference on ar-tiﬁcial intelligence, 35(16):14481–14488..mike mintz, steven bills, rion snow, and daniel ju-rafsky.
2009. distant supervision for relation extrac-tion without labeled data.
association for computa-tional linguistics..jovana mitrovic, brian mcwilliams, jacob c walker,lars holger buesing, and charles blundell.
2021.representation learning via invariant causal mech-in international conference on learninganisms.
representations..brady neal.
2020..introduction to causal inference.
from a machine learning perspective..judea pearl.
1995. causal diagrams for empirical re-.
search.
biometrika, 82(4):669–688..judea pearl.
2009. causality.
cambridge university.
press..judea pearl and dana mackenzie.
2018. the book ofwhy: the new science of cause and effect.
basicbooks..judea pearl et al.
2000. models, reasoning and infer-ence.
cambridge, uk: cambridgeuniversitypress..minlong peng, xiaoyu xing, qi zhang, jinlan fu, andxuanjing huang.
2019. distantly supervised namedentity recognition using positive-unlabeled learning.
association for computational linguistics..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for word rep-resentation.
in empirical methods in natural lan-guage processing (emnlp)..jiaxin qi, yulei niu, jianqiang huang, and hanwangzhang.
2020. two causal principles for improvingvisual dialog.
in proceedings of the ieee/cvf con-ference on computer vision and pattern recogni-tion, pages 10860–10869..lev ratinov and dan roth.
2009..design chal-lenges and misconceptions in named entity recog-in proceedings of the thirteenth confer-nition.
ence on computational natural language learning(conll-2009).
association for computational lin-guistics..lorenzo richiardi, rino bellocco, and daniela zugna.
2013. mediation analysis in epidemiology: meth-ods, interpretation and bias.
international journal ofepidemiology, 42(5):1511–1519..jingbo shang, liyuan liu, xiaotao gu, xiang ren,teng ren, and jiawei han.
2018. learning namedentity tagger using domain-speciﬁc dictionary.
as-sociation for computational linguistics..kaihua tang, jianqiang huang, and hanwang zhang.
2020. long-tailed classiﬁcation by keeping thegood and removing the bad momentum causal effect.
advances in neural information processing systems,33..4812a proof of backdoor adjustment.
can obtain:.
p (y p=1|do(x p(d)))(cid:88).
(cid:88).
p (di)p (x n|di)p (y p=1|x p(d), x n).
i(cid:88).
x np (di)p (y p=1|x p(d), x n(di)).
=.
=.
i.we prove the backdoor adjustment for scm usingthe do-calculus (pearl, 1995) and the truncatedfactorization (neal, 2020)..first of all, we write the joint distribution as.
shown in our causal graph:.
p (d,x p, x n, y, m, r, x).
=p (d)p (x)p (x p|d, x)p (x n|d, x)p (m |x p, x n)p (r|m, x)p (y |r).
due to the objective of our method is debiasingds-ner models during training, we ignore the un-labeled instances variable x which is not related tothe training process.
then we obtain the followingequation:.
p (d,x p, x n, y, m, r).
=p (d)p (x p|d)p (x n|d).
p (m |x p, x n)p (r|m )p (y |r).
doesn’t.
note that.
the prediction step of a nermodel m →r→ycausalhaveabbrevi-effects with other variables, wep (m |x p, x n)p (r|m, x)p (y |r)ateasp (y |x p, x n).
finally, we obtain the simpliﬁedjoint distribution:.
p (d, x p, x n, y )=p (d)p (x p|d)p (x n|d)p (y |x p, x n).
then we conduct causal intervention on x p, i.e.,do(x p=x p(d)) where x p(d) denotes positiveinstances generated by dictionary d. here, we ab-breviate it as do(x p(d)).
in practice, do(x p(d))denotes that we use these positive instances to cal-culate loss value, therefore, in order to explicitly in-dicate this, we use y p=1 in the following equation.
according to the truncated factorization (neal,2020), we can know p (x p|d)=1, and obtain thefollowing equation:.
p (d,x n, y p=1|do(x p(d))).
=p (d)p (x n|d)p (y p=1|x p(d), x n).
next, we integrate d and x n:.
p (y p=1|do(x p(d)))(cid:88).
(cid:88).
i.x n.=.
p (di)p (x n|di)p (y p=1|x p(d), x n).
note that p (x n(di)|di)=1 if and only if x n isgenerated by a speciﬁc dictionary di, therefore we.
4813