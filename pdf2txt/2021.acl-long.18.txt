capturing event argument interaction via a bi-directional entity-levelrecurrent decoder.
xi xiangyu1,2, wei ye1,†, shikun zhang1,†, quanxiu wang3, huixing jiang2, wei wu21 national engineering research center for software engineering, peking university,beijing, china2 meituan group, beijing, china3 rich ai, beijing, china{xixy,wye,zhangsk}@pku.edu.cn.
abstract.
capturing interactions among event argumentsis an essential step towards robust event argu-ment extraction (eae).
however, existing ef-forts in this direction suffer from two limita-tions: 1) the argument role type informationof contextual entities is mainly utilized as train-ing signals, ignoring the potential merits ofdirectly adopting it as semantically rich inputfeatures; 2) the argument-level sequential se-mantics, which implies the overall distributionpattern of argument roles over an event men-tion, is not well characterized.
to tackle theabove two bottlenecks, we formalize eae asa seq2seq-like learning problem for the ﬁrsttime, where a sentence with a speciﬁc eventtrigger is mapped to a sequence of event ar-gument roles.
a neural architecture with anovel bi-directional entity-level recurrent de-coder (berd) is proposed to generate argu-ment roles by incorporating contextual enti-ties’ argument role predictions, like a word-by-word text generation process, thereby dis-tinguishing implicit argument distribution pat-terns within an event more accurately..1.introduction.
event argument extraction (eae), which aims toidentify the entities serving as event arguments andclassify the roles they play in an event, is a key steptowards event extraction (ee).
for example, giventhat the word “ﬁred” triggers an attack event inthe sentence “in baghdad, a cameraman died whenan american tank ﬁred on the palestine hotel” ,eae need to identify that “baghdad”, “camera-man”, “american tank”, and “palestine hotel” arearguments with place, target, instrument, and tar-get as roles respectively..recently, deep learning models have beenwidely applied to event argument extraction and.
†corresponding authors..achieved signiﬁcant progress(chen et al., 2015;nguyen et al., 2016; sha et al., 2018; yang et al.,2019; wang et al., 2019b; zhang et al., 2020; duand cardie, 2020).
many efforts have been devotedto improving eae by better characterizing argu-ment interaction, categorized into two paradigms.
the ﬁrst one, named inter-event argument inter-action in this paper, concentrates on mining infor-mation of the target entity (candidate argument)in the context of other event instances (yu et al.,2011; nguyen et al., 2016), e.g., the evidence thata victim argument for the die event is often thetarget argument for the attack event in the samesentence.
the second one is intra-event argu-ment interaction, which exploits the relationshipof the target entity with others in the same eventinstance (yu et al., 2011; sha et al., 2016, 2018).
we focus on the second paradigm in this paper..despite their promising results, existing methodson capturing intra-event argument interaction sufferfrom two bottlenecks..(1) the argument role type information ofcontextual entities is underutilized.
as tworepresentative explorations, dbrnn (sha et al.,2018) uses an intermediate tensor layer to capturelatent interaction between candidate arguments;rbpb (sha et al., 2016) estimates whether twocandidate argument belongs to one event or not,serving as constraints on a beam-search-based pre-diction algorithm.
generally, these works use theargument role type information of contextual enti-ties as auxiliary supervision signals for trainingto reﬁne input representation.
however, one intu-itive observation is that the argument role typescan be utilized straightforwardly as semanticallyrich input features, like how we use entity typeinformation.
to verify this intuition, we conductan experiment on ace 2005 english corpus, inwhich cnn (nguyen and grishman, 2015) is uti-lized as a baseline.
for an entity, we incorporate.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages210–219august1–6,2021.©2021associationforcomputationallinguistics210the ground-truth roles of its contextual argumentsinto the baseline model’s input representation, ob-taining model cnn(w. role type).
as expected,cnn(w. role type) outperforms cnn signiﬁcantlyas shown in table 11..modelcnncnn(w. role type).
p57.859.8.r55.060.3.f156.360.0.table 1: experimental results of cnn and its varianton ace 2005..the challenge of the method lies in knowing theground-truth roles of contextual entities in the infer-ence (or testing) phase.
that is one possible reasonwhy existing works do not investigate in this direc-tion.
here we can simply use predicted argumentroles to approximate corresponding ground truthfor inference.
we believe that the noise brought byprediction is tolerable, considering the stimulatingeffect of using argument roles directly as input..(2) the distribution pattern of multiple argu-ment roles within an event is not well character-ized.
for events with many entities, distinguishingthe overall appearance patterns of argument rolesis essential to make accurate role predictions.
indbrnn (sha et al., 2018), however, there is nospeciﬁc design involving constraints or interactionamong multiple prediction results, though the argu-ment representation fed into the ﬁnal classiﬁer isenriched with synthesized information (the tensorlayer) from other arguments.
rbpb (sha et al.,2016) explicitly leverages simple correlations in-side each argument pair, ignoring more complex in-teractions in the whole argument sequence.
there-fore, we need a more reliable way to learn thesequential semantics of argument roles in an event.
to address the above two challenges, we for-malize eae as a seq2seq-like learning problem(bahdanau et al., 2014) of mapping a sentence witha speciﬁc event trigger to a sequence of event ar-gument roles.
to fully utilize both left- and right-side argument role information, inspired by the bi-directional decoder for machine translation (zhanget al., 2018), we propose a neural architecture witha novel bi-directional entity-level recurrent de-coder (berd) to generate event argument rolesentity by entity.
the predicted argument role of anentity is fed into the decoding module for the next.
1in the experiment we skip the event detection phase and.
directly assume all the triggers are correctly recognized..or previous entity recurrently like a text generationprocess.
in this way, berd can identify candidatearguments in a way that is more consistent with theimplicit distribution pattern of multiple argumentroles within a sentence, similar to text generationmodels that learn to generate word sequences fol-lowing certain grammatical rules or text styles..the contributions of this paper are:.
1. we formalize the task of event argument ex-traction as a seq2seq-like learning problemfor the ﬁrst time, where a sentence with a spe-ciﬁc event trigger is mapped to a sequence ofevent argument roles..2. we propose a novel architecture with a bi-directional entity-level recurrent decoder(berd) that is capable of leveraging the ar-gument role predictions of left- and right-sidecontextual entities and distinguishing argu-ment roles’ overall distribution pattern..3. extensive experimental results show that ourproposed method outperforms several compet-itive baselines on the widely-used ace 2005dataset.
berd’s superiority is more signiﬁ-cant given more entities in a sentence..2 problem formulation.
most previous works formalize eae as either aword-level sequence labeling problem (nguyenet al., 2016; zeng et al., 2016; yang et al., 2019)or an entity-oriented classic classiﬁcation problem(chen et al., 2015; wang et al., 2019b).
we for-malize eae as a seq2seq-like learning problem asfollows.
let s = {w1, ..., wn} be a sentence wheren is the sentence length and wi is the i-th token.
also, let e = {e1, ..., ek} be the entity mentions inthe sentence where k is number of entities.
giventhat an event triggered by t ∈ s is detected in edstage , eae need to map the sentence with the eventto a sequence of argument roles r = {y1, ..., yk},where yi denotes the argument role that entity eiplays in the event..3 the proposed approach.
we employ an encoder-decoder architecture for theproblem deﬁned above, which is similar to mostseq2seq models in machine translation (vaswaniet al., 2017; zhang et al., 2018), automatic textsummarization (song et al., 2019; shi et al., 2021),and speech recognition (t¨uske et al., 2019; hannunet al., 2019) from a high-level perspective..211figure 1: the detailed architecture of our proposed approach.
the ﬁgure depicts a concrete case where a sentencecontains an attack event (triggered by “ﬁred”) and 4 candidate arguments {e1, e2, e3, e4}.
the encoder on theleft converts the sentence into intermediate continuous representations.
then the forward decoder and backwarddecoder generates the argument roles sequences in a left-to-right and right-to-left manner (denoted by −→yi and ←−yi)respectively.
a classiﬁer is ﬁnally adopted to make the ﬁnal prediction yi.
the forward and backward decodershares the instance feature extractor and generate the same instance representation xi for i-th entity.
the histogramin green and brown denotes the probability distribution generated by forward decoder and backward decoder re-spectively.
the orange histogram denotes the ﬁnal predictions.
note that the histograms are for illustration onlyand do not represent the true probability distribution..in particular, as figure 1 shows, our architectureconsists of an encoder that converts the sentence swith a speciﬁc event trigger into intermediate vec-torized representation and a decoder that generatesa sequence of argument roles entity by entity.
thedecoder is an entity-level recurrent network whosenumber of decoding steps is ﬁxed, the same as theentity number in the corresponding sentence.
oneach decoding step, we feed the prediction resultsof the previously-processed entity into the recurrentunit to make prediction for the current entity.
sincethe predicted results of both left- and right-side en-tities can be potentially valuable information,wefurther incorporate a bidirectional decoding mech-anism that integrates a forward decoding processand a backward decoding process effectively..3.1 encoder.
given the sentence s = (w1, ..., wn) containinga trigger t ∈ s and k candidate arguments e ={e1, ..., ek}, an encoder is adopted to encode theword sequence into a sequence of continuous rep-.
resentations as follows2,.
h = (h1, ..., hn) = f (w1, ..., wn).
(1).
where f (·) is the neural network to encode the sen-tence.
in this paper, we select bert (devlin et al.,2019) as the encoder.
considering representationh does not contain event type information, whichis essential for predicting argument roles.
we ap-pend a special phrase denoting event type of t intoeach input sequence, such as “# attack #”..3.2 decoder.
different from traditional token-level seq2seqmodels, we use a bi-directional entity-level recur-rent decoder (berd) with a classiﬁer to generate asequence of argument roles entity by entity.
berdconsists of a forward and backward recurrent de-coder, which exploit the same recurrent unit archi-tecture as follows..3.2.1 recurrent unitthe recurrent unit is designed to explicitly utilizetwo kinds of information: (1) the instance infor-.
2the trigger word can be detected by any event detection.
model, which is not the scope of this paper..212be(cid:85)(cid:87)d(cid:76)ed(cid:90)(cid:75)e(cid:81)a(cid:81)a(cid:80)e(cid:85)(cid:76)ca(cid:81)(cid:87)a(cid:81)(cid:78)(cid:192)(cid:85)edaca(cid:80)e(cid:85)a(cid:80)a(cid:81)w(cid:82)(cid:85)dp(cid:76)ecese(cid:74)(cid:80)e(cid:81)(cid:87)+p(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)+e(cid:81)c(cid:82)d(cid:72)(cid:85)(cid:82)(cid:81)(cid:87)(cid:75)epa(cid:79)e(cid:86)(cid:87)(cid:76)(cid:81)eh(cid:82)(cid:87)e(cid:79)#attack#[sep](cid:72)1a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)starti(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)(cid:72)2a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)i(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)u(cid:83)da(cid:87)e(cid:72)3a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)i(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)(cid:72)4a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)i(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)i(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)start(cid:72)4u(cid:83)da(cid:87)ei(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)(cid:72)3u(cid:83)da(cid:87)ei(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)(cid:72)2u(cid:83)da(cid:87)ei(cid:81)(cid:86)(cid:87)a(cid:81)ce fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)a(cid:85)g(cid:88)(cid:80)e(cid:81)(cid:87) fea(cid:87)(cid:88)(cid:85)ee(cid:91)(cid:87)(cid:85)ac(cid:87)(cid:82)(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)(cid:72)1cla(cid:86)(cid:86)i(cid:192)e(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)cla(cid:86)(cid:86)i(cid:192)e(cid:85)f(cid:82)(cid:85)(cid:90)a(cid:85)d d(cid:72)c(cid:82)d(cid:72)(cid:85)bac(cid:78)(cid:90)a(cid:85)d d(cid:72)c(cid:82)d(cid:72)(cid:85)u(cid:83)da(cid:87)eu(cid:83)da(cid:87)e𝒚𝟏𝒚𝟐𝒚𝟑𝒚𝟒𝒙𝟏𝒂𝒙𝟐𝒂𝒙𝟑𝒂𝒙𝟒𝒂𝒙𝟏𝒂𝒙𝟐𝒂𝒙𝟑𝒂𝒙𝟒𝒂𝒙𝟏𝒙𝟐𝒙𝟑𝒙𝟒𝒚𝟏𝒚𝟐𝒚𝟑𝒚𝟒𝒚𝟏𝒚𝟐𝒚𝟑𝒚𝟒𝒙𝟏𝒂𝒙𝟐𝒂𝒙𝟑𝒂𝒙𝟒𝒂𝒙𝟏𝒙𝟐𝒙𝟑𝒙𝟒𝒙𝟏𝒂𝒙𝟐𝒂𝒙𝟑𝒂𝒙𝟒𝒂𝒙𝟏𝒙𝟐𝒙𝟑𝒙𝟒mation which contains the sentence, event, andcandidate argument (denoted by s, t, e); and (2)contextual argument information which consistsof argument roles of other entities (denoted by a).
the recurrent unit exploits two corresponding fea-ture extractors as follows:instance feature extractor.
given the represen-tation h generated by encoder, dynamic multi-pooling (chen et al., 2015) is then applied to extractmax values of three split parts, which are decidedby the event trigger and the candidate argument.
the three hidden embeddings are aggregated intoan instance feature representation x as follows:.
[x1,pt]i = max{[h1]i, ..., [hpt]i}[xpt+1,pe]i = max{[hpt+1]i, ..., [hpe]i}[xpe+1,n]i = max{[hpe+1]i, ..., [hn]i}x = [x1,pt; xpt+1,pe; xpe+1,n].
(2).
where [·]i is the i-th value of a vector, pt, pe are thepositions of trigger t and candidate argument e 3.argument feature extractor.
to incorporatepreviously-generated arguments, we exploit cnnnetwork to encode the instance with arguments in-formation as follows..input.
different from chen et al.
(2015) whereinput embedding of each word consists of its wordembedding, position embedding, and event typeembedding, we append the embedding of argumentroles into the input embedding for each word bylooking up the vector a, which records argumentrole for each token in s. in a, tokens of previously-predicted arguments are assigned with the gener-ated labels, tokens of the candidate entity e areassigned with a special label “to-predict”, and theother tokens are assigned with label n/a..convolution.the convolution layer is applied toencode the word sequence into hidden embeddings:.
(ha.
1, , ..., ha.
n) = cnn(w1, ..., t, ..., e, ..., wn).
where the upperscript a denotes argument..pooling.
max-pooing operation is then applied.
to extract the argument feature xa as follows,.
xa = maxpooling(ha.
1, ..., han).
(4).
we concatenate the instance feature representa-tion x and the argument feature representation xa.
3equation 2 assumes that the entity mention lies after thetrigger.
if the entity mention lies before the trigger, we switchpt and pe in the equation to get a right split..as the input feature representation for the argumentrole classiﬁer, and estimate the role that e plays inthe event as follows:.
p = f (w [x; xa] + b)o = softmax(p).
(5).
where w and b are weight parameters.
o is theprobability distribution over the role label space..for the sake of simplicity, in rest of the paperwe use unit(s, t, e, a) to represent the calculationof probability distribution o by recurrent unit withs, t, e, a as inputs..3.2.2 forward decoder.
given the sentence s with k candidate argumentse = {e1, ..., ek}, the forward decoder exploitsabove recurrent unit and generates the argumentroles sequence in a left-to-right manner.
the condi-tional probability of the argument roles sequenceis calculated as follows:.
p (r|e, s, t) =.
p(yi|ei; r<i, s, t).
(6).
k(cid:89).
i=1.
where r<i denotes the role sequence {y1, ..., yi−1}for {e1, ..., ei−1}..for i-th entity ei, the recurrent unit generates.
prediction as follows:.
−→ai).
(7).
−→yi = unit(s, t, ei,where −→yi denotes the probability distribution over−→ai denotes the contextuallabel space for ei andargument information of i-th decoding step, whichcontains previously-predicted argument roles r<i.
−−→ai+1 by labeling ei as g(−→yi) forthen we updatenext step i+1, where g(−→yi) denotes the label hasthe highest probability under the distribution −→yi.
the argument feature extracted by recurrent unitsof forward decoder is denoted as −→x ai ..the backward decoder is similar to the forwarddecoder, except that it performs decoding in a right-to-left way as follows:.
p (r|e, s, t) =.
p(yi|ei; r>i, s, t).
(8).
k(cid:89).
i=1.
where r>i denotes the role sequence {yi+1, ..., yk}for {ei+1, ..., ek}.
the probability distribution over.
(3).
3.2.3 backward decoder.
213label space for i-th entity ei is calculated as follows:.
←−yi = unit(s, t, ei,.
←−ai).
(9).
←−ai denotes the contextual argument in-whereformation of i-th decoding step, which containspreviously-predicted argument roles r>i.
we up-←−−ai−1 by labeling ei as g(←−yi) for next step i-1.
datethe argument feature extracted by recurrent unitsof backward decoder is denoted as ←−x ai ..3.2.4 classiﬁer.
to utilize both left- and right-side argument in-formation, a classiﬁer is then adopted to combineargument features of both decoders and make ﬁnalprediction for each entity ei as follows:pi = f (wc[xi; −→x ayi = softmax(pi).
i ] + bc).
i ; ←−x a.
(10).
where yi denotes the ﬁnal probability distributionfor ei.
wc and bc are weight parameters..3.3 training and optimization.
as seen, the forward decoder and backward de-coder in berd mainly play two important roles.
the ﬁrst one is to yield intermediate argument fea-tures for the ﬁnal classiﬁer, and the second one is tomake the initial predictions fed into the argumentfeature extractor.
since the initial predictions ofthe two decoders are crucial to generate accurateargument features, we need to optimize their ownclassiﬁer in addition to the ﬁnal classiﬁer..←−−−−p(yi|ei) to represent theprobability of ei playing role yi estimated by for-ward and backward decoder respectively.
p(yi|ei)denotes the ﬁnal estimated probability of ei playingrole yi by equation 10. the optimization objectivefunction is deﬁned as follows:.
−−−−→p(yi|ei) and.
we use.
j(θ) = −.
(cid:88).
(cid:88).
(cid:88).
α log p(yi|ei; r(cid:54)=i, s, t).
s∈d.
ei∈es.
t∈s−−−−→p(yi|ei) + γ log.
+ β log.
←−−−−p(yi|ei).
(11).
where d denotes the training set and t ∈ s denotesthe trigger word detected by previous event detec-tion model in sentence s. es represents the entitymentions in s. α, β and γ are weights for lossof ﬁnal classiﬁer, forward decoder and backwarddecoder respectively..during training, we apply the teacher forcingmechanism where gold arguments information isfed into berd’s recurrent units, enabling paral-leled computation and greatly accelerates the train-ing process.
once the model is trained, we ﬁrstuse the forward decoder with a greedy search to se-quentially generate a sequence of argument roles ina left-to-right manner.
then, the backward decoderperforms decoding in the same way but a right-to-left manner.
finally, the classiﬁer combines bothleft- and right-side argument features and makeprediction for each entity as equation 10 shows..4 experiments.
4.1 experimental setup.
4.1.1 datasetfollowing most works on eae (nguyen et al.,2016; sha et al., 2018; yang et al., 2019; du andcardie, 2020), we evaluate our models on the mostwidely-used ace 2005 dataset, which contains 599documents annotated with 33 event subtypes and35 argument roles.
we use the same test set con-taining 40 newswire documents, a development setcontaining 30 randomly selected documents andtraining set with the remaining 529 documents..we notice wang et al.
(2019b) used tac kbpdataset, which we can not access online or acquirefrom them due to copyright.
we believe experi-menting with settings consistent with most relatedworks (e.g., 27 out of 37 top papers used only theace 2005 dataset in the last four years) shouldyield convincing empirical results..4.1.2 hyperparameterswe adopt bert (devlin et al., 2019) as encoderand the proposed bi-directional entity-level recur-rent decoder as decoder for the experiment.
thehyperparameters used in the experiment are listed.
bert.
the hyperparameters of bert are the sameas the bertbase model4.
we use a dropout prob-ability of 0.1 on all layers.
argument feature extractor.
dimensions of wordembedding, position embedding, event type em-bedding and argument role embedding for eachtoken are 100, 5, 5, 10 respectively.
we utilize300 convolution kernels with size 3. the gloveembedding(pennington et al., 2014) are utilized forinitialization of word embedding5.
training.
adam with learning rate of 6e-05, β1 =.
4https://github.com/google-research/bert5https://nlp.stanford.edu/projects/glove/.
2140.9, β2 = 0.999, l2 weight decay of 0.01 andlearning rate warmup of 0.1 is used for optimiza-tion.
we set the training epochs and batch sizeto 40 and 30 respectively.
besides, we exploit adropout with rate 0.5 on the concatenated featurerepresentations.
the loss weights α, β and γ areset to 1.0, 0.5 and 0.5 respectively..4.2 baselines.
we compare our method against the following fourbaselines.
the ﬁrst two are state-of-the-art modelsthat separately predicts argument without consid-ering argument interaction.
we also implementtwo variants of dmbert utilizing the latest inter-event and intra-event argument interaction method,named bert(inter) and bert(intra) respectively..1. dmbert which adopts bert as encoderand generate representation for each entitymention based on dynamic multi-pooling op-eration(wang et al., 2019a).
the candidatearguments are predicted separately..2. hmeae which utilizes the concept hierar-chy of argument roles and utilizes hierarchicalmodular attention for event argument extrac-tion (wang et al., 2019b)..3. bert(inter) which enhances dmbertwith inter-event argument interaction adoptedby nguyen et al.
(2016).
the memory ma-trices are introduced to store dependenciesamong event triggers and argument roles..4. bert(intra) which incorporates intra-eventargument interaction adopted by sha et al.
(2018) into dmbert.
the tensor layer andself-matching attention matrix with the samesettings are applied in the experiment..following previous work (wang et al., 2019b),we use a pipelined approach for event extractionand implement dmbert as event detection model.
the same event detection model is used for all thebaselines to ensure a fair comparison..note that nguyen et al.
(2016) uses the last wordto represent the entity mention6, which may leadto insufﬁcient semantic information and inaccurateevaluation considering entity mentions may consistof multiple words and overlap with each other.
wesum hidden embedding of all words when collect-ing lexical features for each entity mention..6sha et al.
(2018) doesn’t introduce the details..modeldmberthmeaebert(inter)bert(intra)berd.
p56.962.258.456.459.1.r57.456.657.161.261.5.f157.259.357.858.760.3.table 2: overall performance on ace 2005 (%)..4.3 main results.
the performance of berd and baselines are shownin table 2 (statistically signiﬁcant with p < 0.05),from which we have several main observations.
(1)compared with the latest best-performed baselinehmeae, our method berd achieves an absoluteimprovement of 1.0 f1, clearly achieving compet-itive performance.
(2) incorporation of argumentinteractions brings signiﬁcant improvements overvanilla dmbert.
for example, bert(intra) gainsa 1.5 f1 improvement compared with dmbert,which has the same architecture except for argu-ment interaction.
(3) intra-event argument interac-tion brings more beneﬁt than inter-event interaction(57.8 of bert(inter) v.s.
58.7 of bert(intra) v.s.
60.3 of berd).
(4) compared with bert(inter)and bert(intra), our proposed berd achieves themost signiﬁcant improvements.
we attribute thesolid enhancement to berd’s novel seq2seq-likearchitecture that effectively exploits the argumentroles of contextual entities..4.4 effect of entity numbers.
to further investigate how our method improvesperformance, we conduct comparison and analysison effect of entity numbers.
speciﬁcally, we ﬁrstdivide the event instances of test set into somesubsets based on the number of entities in an event.
since events with a speciﬁc number of entities maybe too few, results on a subset of a range of entitynumbers will yield more robust and convincingconclusion.
to make the number of events in allsubsets as balanced as possible, we ﬁnally get adivision of four subsets, whose entity numbers arein the range of [1,3], [4,6], [7,9], and [10,] andevent quantities account for 28.4%, 28.2%, 25.9%,and 17.5%, respectively..the performance of all models on the four sub-sets is shown in figure 2, from which we can ob-serve a general trend that berd outperforms otherbaselines more signiﬁcantly if more entities appearin an event.
more entities usually mean more com-.
215subset-o subset-n.modeldmberthmeaebert(inter)bert(intra)berd.
56.458.857.358.560.5.
59.459.658.859.260.1.table 3: comparison on sentences with and with-out overlapping entities (subset-o v.s.
subset-n).
f1-score (%) is listed..signiﬁcant.
we attribute it to berd’s capability ofdistinguishing argument distribution patterns..4.6 effect of the bidirectional decoding.
to further investigate the effectiveness of the bidi-rectional decoding process, we exclude the back-ward decoder or forward decoder from berd andobtain two models with only unidirectional decoder,whose performance is shown in the lines of “-w/forward decoder” and “-w/ backward decoder” intable 4. from the results, we can observe that: (1)when decoding with only forward or backward de-coder, the performance decreases by 1.6 and 1.3 interms of f1 respectively.
the results clearly demon-strate the superiority of the bidirectional decod-ing mechanism (2) though the two model variantshave performance degradation, they still outper-form dmbert signiﬁcantly, once again verifyingthat exploiting contextual argument information,even in only one direction, is beneﬁcial to eae..pmodel56.9dmbert59.1berd58.0-w/ forward decoder58.3-w/ backward decoder56.8-w/ forward decoder x2-w/ backward decoder x257.2-w/o recurrent mechanism 55.3.r57.461.559.459.861.161.060.0.f157.260.358.759.058.959.157.4.table 4: ablation study on ace 2005 dataset (%)..considering number of model parameters willbe decreased by excluding the forward/backwarddecoder, we build another two model variants withtwo decoders of the same direction (denoted by“-w/ forward decoder x2” and “-w/ backward de-coder x2”), whose parameter numbers are exactlyequal to berd.
table 4 shows that the two enlargedsingle-direction models have similar performancewith their original versions.
we can conclude that.
figure 2: comparison on four subsets with differentrange of entity numbers.
f1-score (%) is listed..plex contextual information for a candidate argu-ment, which will lead to a performance degradation.
berd alleviates degradation better because of itscapability of capturing argument role informationof contextual entities.
we notice that bert(intra)also outperforms dmbert signiﬁcantly on subset-4, which demonstrates the effectiveness of intra-event argument interaction..note that the performance on subset-1 is worsethan that on subset-2, looking like an outlier.
thereason lies in that the performance of the ﬁrst-stageevent detection model on subset-1 is much poorer(e.g., 32.8 of f1 score for events with one entity)..4.5 effect of overlapping entity mentions.
though performance improvement can be easilyobserved, it is nontrivial to quantitatively verifyhow berd captures the distribution pattern of mul-tiple argument roles within an event.
in this section,we partly investigate this problem by exploring theeffect of overlapping entities.
since there is usuallyonly one entity serving as argument roles in multi-ple overlapping entities, we believe sophisticatedeae models should identify this pattern.
therefore,we divide the test set into two subsets (subset-oand subset-n ) based on whether an event containsoverlapping entity mentions and check all models’performance on these two subsets.
table 3 showsthe results, from which we can ﬁnd that all base-lines perform worse on subset-o.
it is a naturalresult since multiple overlapping entities usuallyhave similar representations, making the patternmentioned above challenging to capture.
berdperforms well in both subset-o and subset-n, andthe superiority on subset-o over baseline is more.
216subset-1[1,3]subset-2[4,6]subset-3[7,9]subset-4[10,]52545658606264f1-score(%)dmberthmeaebert(inter)bert(intra)berdthe improvement comes from complementation ofthe two decoders with different directions, ratherthan increment of model parameters..besides, we exclude the recurrent mechanism bypreventing argument role predictions of contextualentities from being fed into the decoding module,obtaining another model variant named “-w/o re-current mechanism”.
the performance degradationclearly shows the value of the recurrent decodingprocess incorporating argument role information..4.7 case study and error analysis.
to promote understanding of our method, wedemonstrate three concrete examples in figure 3.sentence s1 contains a transport event triggeredby “sailing”.
dmbert and bert(intra) assignsdestination role to candidate argument “the per-ilous strait of gibraltar ”, “the southern mainland”and “the canary islands out in the atlantic”, theﬁrst two of which are mislabeled.
it’s an unusualpattern that a transport event contains multipledestinations.
dmbert and bert(intra) fail torecognize the information of such patterns, show-ing that they can not well capture this type of corre-lation among prediction results.
our berd, how-ever, leverages previous predictions to generateargument roles entity by entity in a sentence, suc-cessfully avoiding the unusual pattern happening.
s2 contains a transport event triggered by “vis-ited”, and 4 nested entities exists in the phrase“ankara police chief ercument yilmaz”.
sincethese nested entities share the same sentence con-text, it is not strange that dmbert wrongly pre-dicts such entities as the same argument role arti-fact.
thanks to the bidirectional entity-level recur-rent decoder, our method can recognize the distribu-tion pattern of arguments better and hence correctlyidentiﬁes these nested entities as false instances.
in this case, berd reduces 3 false-positive pre-dictions compared with dmbert, conﬁrming theresults and analysis of table 3..as a qualitative error analysis, the last examples3 demonstrates that incorporating previous predic-tions may also lead to error propagation problem.
s3 contains a marry event triggered by “marry”.
entity “home” is mislabeled as time-within roleby berd and this wrong prediction will be usedas argument features to identify entity “later in thisafter”, whose role is time-within.
as analyzed inthe ﬁrst case, berd tends to avoid repetitive rolesin a sentence, leading this entity incorrectly being.
figure 3: case study.
entities and triggers are high-lighted by green and purple respectively.
each tuple(e,g,p1,p2,p3) denotes the predictions for an entity ewith gold label g, where p1, p2 and p3 denotes predic-tion of dmbert, bert(intra) and berd respectively.
incorrect predictions are denoted by a red mark..predicted as n/a..5 related work.
we have covered research on eae in section 1,related work that inspires our technical design ismainly introduced in the following..though our recurrent decoder is entity-level, ourbidirectional decoding mechanism is inspired bysome bidirectional decoders in token-level seq2seqmodels, e.g., of machine translation (zhou et al.,2019), speech recognition (chen et al., 2020) andscene text recognition (gao et al., 2019)..we formalize the task of eae as a seq2seq-likelearning problem instead of a classic classiﬁcationproblem or sequence labeling problem.
we havefound that there are also some works performingclassiﬁcation or sequence labeling in a seq2seqmanner in other ﬁelds.
for example, yang et al.
(2018) formulates the multi-label classiﬁcation taskas a sequence generation problem to capture thecorrelations between labels.
daza and frank (2018)explores an encoder-decoder model for semanticrole labeling.
we are the ﬁrst to employ a seq2seq-like architecture to solve the eae task..6 conclusion.
we have presented berd, a neural architecturewith a bidirectional entity-level recurrent decoderthat achieves competitive performance on the taskof event argument extraction (eae).
one maincharacteristic that distinguishes our techniques.
217s1:tens of thousands of destitute africans try to enter spain illegally each year by crossing the perilous strait of gibraltar to reach the southern mainland or by sailing northwest to the canary islands out in the atlantic(the perilous strait of gibraltar, n/a, destination(), destination(  ), n/a)(the southern mainland, n/a, destination(), destination(), n/a)(the canary islands out in the atlantic, destination, destination, destination, destination)s2: ankara police chief ercumentyilmaz visitedthe site of the morningblastbut refused to say if a bomb had caused the explosion (ankara,n/a,artifact(  ), n/a,n/a)(ankara police,n/a,artifact(  ),n/a, n/a) (ankara police chief,n/a, artifact(  ),artifact(  ),n/a) (ankara police chief ercumentyilmaz,artifact,artifact,artifact, artifact) s3:prison authorities have given the node for anwarto be token homelater in the afternoonto marryhis eldest daughter, nurul izzah, to engineer raja ahmad shaririskandarin a traditional malayceremony, he said (home,place,place,place, time-within(  ) )(later in the afternoon, time-within,time-within, time-within,n/a(  ))from previous works is that we formalize eaeas a seq2seq-like learning problem instead of aclassic classiﬁcation or sequence labeling problem.
the novel bidirectional decoding mechanism en-ables our berd to utilize both the left- and right-side argument predictions effectively to generatea sequence of argument roles that follows overalldistribution patterns over a sentence better..as pioneer research that introduces the seq2seq-like architecture into the eae task, berd alsofaces some open questions.
for example, since weuse gold argument roles as prediction results duringtraining, how to alleviate the exposure bias problemis worth investigating.
we are also interested in in-corporating our techniques into more sophisticatedmodels that jointly extract triggers and arguments..acknowledgements.
we thank anonymous reviewers for valuable com-ments.
this research was supported by the na-tional key research and development programof china (no.2019yfb1405802) and the centralgovernment guided local science and technologydevelopment fund projects (science and technologyinnovation base projects) no.
206z0302g..references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2014. neural machine translation by jointlyarxiv preprintlearning to align and translate.
arxiv:1409.0473..xi chen, songyang zhang, dandan song, pengouyang, and shouyi yin.
2020. transformer withbidirectional decoder for speech recognition.
pages1773–1777..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the acl and the7th ijcnlp, pages 167–176..angel daza and anette frank.
2018. a sequence-to-sequence model for semantic role labeling.
in pro-ceedings of the third workshop on representationlearning for nlp, pages 207–216..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..xinya du and claire cardie.
2020. event extraction byin proceed-answering (almost) natural questions.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages671–683..yunze gao, yingying chen, jinqiao wang, and han-qing lu.
2019. gate-based bidirectional interactivedecoding network for scene text recognition.
in pro-ceedings of the 28th acm international conferenceon information and knowledge management, pages2273–2276..awni hannun, ann lee, qiantong xu, and ronan col-lobert.
2019. sequence-to-sequence speech recogni-tion with time-depth separable convolutions.
proc.
interspeech 2019, pages 3785–3789..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentneural networks.
in proceedings of the 2016 confer-ence of the naacl, pages 300–309..thien huu nguyen and ralph grishman.
2015. eventdetection and domain adaptation with convolutionalneural networks.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing (volume 2: shortpapers), pages 365–371..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conferenceon empirical methods in natural language process-ing (emnlp), pages 1532–1543..lei sha,.
jing liu, chin-yew lin, sujian li,baobao chang, and zhifang sui.
2016. rbpb:regularization-based pattern balancing method forin proceedings of the 54th an-event extraction.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1224–1234..lei sha, feng qian, baobao chang, and zhifang sui.
2018.jointly extracting event triggers and argu-ments by dependency-bridge rnn and tensor-basedin proceedings of the 32argument interaction.
aaai, new orleans, louisiana, usa, pages 5916–5923..tian shi, yaser keneshloo, naren ramakrishnan, andchandan k reddy.
2021. neural abstractive textsummarization with sequence-to-sequence models.
acm transactions on data science, 2(1):1–37..shengli song, haitao huang, and tongxiao ruan.
2019. abstractive text summarization using lstm-cnn based deep learning.
multimedia tools and ap-plications, 78(1):857–875..zolt´an t¨uske, kartik audhkhasi, and george saon.
advancing sequence-to-sequence basedin interspeech, pages.
2019.speech recognition.
3780–3784..218ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..xiaozhi wang, xu han, zhiyuan liu, maosong sun,and peng li.
2019a.
adversarial training for weaklyin proceedings of thesupervised event detection.
2019 conference of the naacl, pages 998–1008..xiaozhi wang, ziqi wang, xu han, zhiyuan liu,juanzi li, peng li, maosong sun, jie zhou, andxiang ren.
2019b.
hmeae: hierarchical modu-in proceedings oflar event argument extraction.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5781–5787..pengcheng yang, xu sun, wei li, shuming ma, weiwu, and houfeng wang.
2018. sgm: sequencegeneration model for multi-label classiﬁcation.
inproceedings of the 27th international conference oncomputational linguistics, pages 3915–3926..s yang, dw feng, lb aqiao, zg kan, and ds li.
2019. exploring pre-trained language models forevent extraction and generation.
in proceedings ofthe 57th annual meeting of the acl..hong yu, zhang jianfeng, ma bin, yao jianmin, zhouguodong, and zhu qiaoming.
2011. using cross-entity inference to improve event extraction.
in pro-ceedings of the 49th annual meeting of the acl,pages 1127–1136.
association for computationallinguistics..ying zeng, honghui yang, yansong feng, zhengwang, and dongyan zhao.
2016. a convolution bil-stm neural network model for chinese event extrac-tion.
in nluia, pages 275–287.
springer..xiangwen zhang, jinsong su, yue qin, yang liu, ron-grong ji, and hongji wang.
2018. asynchronousbidirectional decoding for neural machine transla-tion.
in proceedings of the aaai conference on ar-tiﬁcial intelligence, volume 32..zhisong zhang, xiang kong, zhengzhong liu, xuezhema, and eduard hovy.
2020. a two-step approachin proceed-for implicit event argument detection.
ings of the 58th annual meeting of the associationfor computational linguistics, pages 7479–7485..long zhou, jiajun zhang, and chengqing zong.
2019.synchronous bidirectional neural machine transla-tion.
transactions of the association for computa-tional linguistics, 7:91–105..219