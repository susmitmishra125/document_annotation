dynaeval: unifying turn and dialogue level evaluation.
chen zhang†,(cid:63) yiming chen† luis fernando d’haro‡ yan zhang†thomas friedrichs(cid:63) grandee lee† haizhou li†,(cid:63)(cid:63).
(cid:63)robert bosch (sea), singapore†national university of singapore‡universidad polit´ecnica de madrid, spain(cid:63)(cid:63)kriston ai lab, china{chen zhang,yiming.chen,grandee.lee}@u.nus.edu,{haizhou.li,eleyanz}@nus.edu.sg,luisfernando.dharo@upm.es, thomas.friedrichs@sg.bosch.com.
abstract.
a dialogue is essentially a multi-turn interac-tion among interlocutors.
effective evaluationmetrics should reﬂect the dynamics of suchinteraction.
existing automatic metrics arefocused very much on the turn-level quality,while ignoring such dynamics.
to this end, wepropose dynaeval1, a uniﬁed automatic eval-uation framework which is not only capableof performing turn-level evaluation, but alsoholistically considers the quality of the entiredialogue.
in dynaeval, the graph convolu-tional network (gcn) is adopted to model adialogue in totality, where the graph nodes de-note each individual utterance and the edgesrepresent the dependency between pairs of ut-terances.
a contrastive loss is then applied todistinguish well-formed dialogues from care-fully constructed negative samples.
experi-ments show that dynaeval signiﬁcantly out-performs the state-of-the-art dialogue coher-ence model, and correlates strongly with hu-man judgements across multiple dialogue eval-uation aspects at both turn and dialogue level..1.introduction.
modern dialogue systems (smith et al., 2020;zhang et al., 2020; adiwardana et al., 2020) lever-aging large-scale language model pre-training (de-vlin et al., 2019; radford et al., 2019) are capableof generating ﬂuent and contextually relevant utter-ances.
yet, they still face difﬁculties in mimickinghuman conversations in the sense that they lackcertain conversation-level attributes, such as coher-ence (cervone et al., 2018), consistency (wellecket al., 2019; nie et al., 2020), diversity (li et al.,2016; wu et al., 2020) and engagement (ghande-harioun et al., 2019; ghazarian et al., 2020).
one ofthe main reasons is the dearth of effective dialogue-level evaluation mechanisms to guide the studiesand to monitor progress..1https://github.com/e0397123/dynaeval.
used.
commonly.
and lavie, 2014).
static metrics,et.
suchal., 2002), me-as bleu (papineniteor (denkowskiandrouge (lin, 2004), correlate poorly withhuman judgements (liu et al., 2016) renderingthem unsuitable for dialogue evaluation.
whilesome recent automatic dialogue evaluation met-rics (ghazarian et al., 2019; mehri and eskenazi,2020b; huang et al., 2020; zhang et al., 2021b)demonstrate strong correlations with humanjudgement at the turn-level, they only focus oncontext-response pairs without explicitly modelingthe interaction over an entire dialogue.
to performdialogue-level evaluation, we need to rely on theaggregation of turn-level scores over the dialogueas a proxy for a dialogue-level score..furthermore, a recent study by mehri and eske-nazi (2020a) found out that even though state-of-the-art chatbots outperform humans across multipleturn-level evaluation criteria, such as interesting-ness, engagement and speciﬁcity, their dialogue-level ratings like coherence, likability and diver-sity are still far below human level.
this furtherreinforces the idea that turn-level quality evaluationmay be insufﬁcient to assess the performance ofopen-domain dialogue systems..in this work, we address the problem of auto-matic open-domain dialogue evaluation by focus-ing on the quality of an entire dialogue.
this is adeparture from the way we frame the problem as aweakly supervised next sentence prediction (mehriand eskenazi, 2020b; sato et al., 2020) or languagemodeling tasks (nedelchev et al., 2020; pang et al.,2020) for context-response pairs.
to this end, weneed to answer two important questions: (1) howto effectively represent the entire dialogue?
(2)how to incorporate this dialogue-level knowledgeinto our evaluation framework?
we propose dy-naeval to provide meaningful dialogue-level repre-sentation with explicit modeling of the interactive.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5676–5689august1–6,2021.©2021associationforcomputationallinguistics5676dynamics among interlocutors, for a uniﬁed turnand dialogue level quality assessment..the main contributions of this work include:(1) the uniﬁed turn and dialogue level evaluationrepresents a departure from turn-level evaluationscheme; (2) dynaeval is one of the ﬁrst few met-rics where dialogue level dynamics is consideredwith structured graph representation.
(3) empiricalresults show that dynaeval outperforms the state-of-the-art dialogue coherence model and stronglycorrelates with human judgements at both turn anddialogue level..2 related work.
2.1 open-ended dialogue evaluation.
turn-level evaluation the current trend for au-tomatic dialogue evaluation is shifting towards thereference-free paradigm.
lately, the research com-munity has witnessed a surge in the automatic met-rics along these lines.
many of them focus on eval-uating naturalness of generated responses.
typicalexamples include perplexity (adiwardana et al.,2020), usr-mlm (mehri and eskenazi, 2020b)and gpt-2 (radford et al., 2019) based ﬂuencymetrics (nedelchev et al., 2020; pang et al., 2020).
another group of metrics evaluates contextualrelevance of the responses.
for example, ru-ber (tao et al., 2018), bert-ruber(ghazarianet al., 2019) and usr-dr (mehri and eskenazi,2020b) predict the relatedness between generatedresponses w.r.t the corresponding context by train-ing a discriminative network to distinguish the orig-inal response from negative samples bootstrappedfrom the training set.
sato et al.
(2020) and lanet al.
(2020) provide a better sampling strategy forbootstrapping negative samples..besides these two major aspects,.
there aremany metrics for other qualities, such as ade-quacy (d’haro et al., 2019; zhang et al., 2021a),consistency (welleck et al., 2019; dziri et al.,2019), engagement (ghazarian et al., 2020)..even though all these automatic metrics demon-strate strong correlation with human judgements,they are laser-focused on one aspect of the eval-uation.
in addition, they do not explicitly modelthe speaker-level and utterance-level interactions,which we believe is essential for the dialogue-levelrepresentation, and eventually beneﬁts the dialogueevaluation task..interactive evaluation a popular human eval-uation method is the interactive evaluation whereby.
human judges converse with dialogue systems andmake the assessment at the end of the conversa-tions (see et al., 2019; finch and choi, 2020; liet al., 2019; deriu et al., 2020).
it has been shownto be more reliable than turn-level static evalua-tion (mehri and eskenazi, 2020a)..there are few studies on fully automating thisprocess.
ghandeharioun et al.
(2019) propose aself-play scenario where the dialog system chatswith itself and a combination of three metrics mea-suring sentiment, semantic coherence and engage-ment respectively along the conversation trajectoryis computed to approximate dialogue-level qual-ity estimation.
mehri and eskenazi (2020a) pro-pose the fed metric, which evaluates the qualityof a system utterance in an interactive setting bycomputing the likelihood of a particular follow-uputterance responded by dialogpt (zhang et al.,2020).
moreover, sinha et al.
(2020) come up withmaude, a reference-free metric tailored for onlinedialogue evaluation, which leverages a pre-traineddistilbert (sanh et al., 2019) model to extractthe semantic representation of dialogue turns anduses bidirectional lstm to explicitly model thediscourse structure..while the interactive evaluation is more reliablethan the turn-level static evaluation, it still relieson the aggregation of turn-level scores.
an idealapproximation of the human evaluation processis a top-down approach whereby we examine thequality of the entire dialogue at macro level beforezooming into the dialogue turns.
hence, a uniﬁedframework, which holistically models the entiredialogue, is highly sought after..2.2 dialogue coherence.
examining a dialogue at macro level is related todiscourse coherence (halliday and hasan, 2014;grosz et al., 1995; barzilay and lapata, 2008),which considers whether a piece of text is in a con-sistent and logical manner, as opposed to a randomcollection of sentences.
dialogue is a special kindof discourse structure, of which coherence assess-ment is an essential part of quality evaluation..many studies have followed the standard dis-course coherence evaluation protocol (cervone andriccardi, 2020; zhou et al., 2019; mesgar et al.,2020).
very few have considered customizing theirdialogue coherence models for evaluating the per-formance of dialogue systems.
it is common toleverage supervised approaches (higashinaka et al.,.
56772014; gandhe and traum, 2016; cervone et al.,2018; yi et al., 2019), that is closely linked to mod-eling with entities and dialogue acts (cervone andriccardi, 2020; zhou et al., 2019; mesgar et al.,2020)..hence, we are motivated to study the applicationof dialogue coherence modeling for automatic di-alogue evaluation by designing a self-supervisedframework, without dependence on any human an-notations for coherence features..2.3 graph modeling of dialogue.
the graph neural network (gnn)recently,(scarselli et al., 2008; kipf and welling, 2017;schlichtkrull et al., 2018) has been successfullyapplied in various dialogue applications.
for exam-ple, ghosal et al.
(2019) adopts gcn for utterance-level emotion recognition.
chen et al.
(2018) mod-eled structured dialogue policy with gnn and (qinet al., 2020) proposes a joint framework leveraginggraph attention network (veliˇckovi´c et al., 2018)for both dialogue act recognition and sentimentclassiﬁcation..gnn is useful for dialogue modeling, becausethe relative position of target and context utterancesdecides how past utterances inﬂuence future utter-ances and vice versa (ghosal et al., 2019).
the in-teraction of utterances can be effectively capturedwith a graph structure as long as they are connectedby relation-aware edges.
however, gnn has notbeen well studied for dialogue evaluation.
huanget al.
(2020) recently proposes the grade met-ric, leveraging graph modeling for turn-level coher-ence evaluation.
the way we use gnn is differentfrom huang et al.
(2020) because grade is fo-cused on turn-level coherence evaluation while weare interested in a turn-dialogue joint evaluation.
furthermore, grade considers the keywords incontext-response pairs, and we explicitly use graphstructure to model the speaker and utterance levelinteraction within a dialogue..3 dynaeval framework.
dyaneval represents an integration of several ideas.
it takes advantage of the structured graph repre-sentation of dialogues, useful information on theutterance and speaker level interaction.
it is moti-vated by dialogue coherence modeling..in this paper, we only consider dyadic dialogues,but the formulation can be easily generalized tomulti-party conversations.
formally, let a and b.
1 , ub.
n−1, ub.
2 , .
.
.
, ua.
denote the two speakers participating in the dia-logue.
a dialogue, d, consists of a sequence of nn ]2. let ¯d repre-utterances, [uasent the negative dialogue sample obtained via var-ious sampling strategies described in section 3.5.figure 1 illustrates the learning process of dy-naeval in four steps3: (1) deriving contextualizedrepresentation, ei, for utterances within d. (sec-tion 3.1).
(2) constructing the directed dialoguegraph.
the nodes are initialized with ei and theedges between node pairs represent the speakerand temporal dependencies (section 3.2).
(3) gen-erating utterance-level graph representation, hi, viafeature transformation to aggregate useful contex-tual information from all connected neighbours tothe current node (section 3.3).
(4) producing adialogue-level score, which indicates whether d ispreferred over ¯d (section 3.4)..3.1 dialogue utterance representation.
a sentence-encoder is needed to map the individualutterances within d onto the vector space.
firstly,we ﬁne-tune a roberta-base pre-trained languagemodel (liu et al., 2019) with training data of thetarget dialogue domain, because task-adaptive ﬁne-tuning of the pre-trained language model on the tar-get domain data beneﬁts the ﬁnal performance (gu-rurangan et al., 2020; lee and li, 2020).
next,the mean pooling operation is performed on thetoken embeddings within each utterance of d toderive their respective utterance-level representa-tions.
formally, let sroberta denotes the sen-tence encoder and u∗i in d is mapped into vectorrepresentations, ui ∈ rd, wherebyui = sroberta(u∗i ).
(1).
note that ∗ can be either speaker a or speakerb. then, to capture a more ﬁne-grained temporaldependency among the utterances, a bidirectionallstm is adopted to model the sequential ﬂow ofinformation within d. the context-aware utterancerepresentation, ei is then obtained via:.
ei =.
←−−→lstm(ei(+,−)1, ui).
(2).
3.2 dialogue graph construction.
d is represented with a directed graph, g = (v, e).
v is the sets of graph nodes and e is the set of.
2n is assumed to be even to simplify the mathematical.
expressions..3note that all the operations from section 3.1 throughsection 3.4 are illustrated with d. they are applied in thesame way on ¯d..5678figure 1: the architecture of dynaeval.
the input is a pair of contrasting dialogues, d and ¯d.
the output is auniﬁed score indicating whether d is preferred than ¯d.
utterance-level representation derived from srobertamodel is used for dialogue graph node initialization.
different types of arrows in relation edge connection representdifferent types of relations: (1) solid line denotes intra-speaker dependency.
(2) dotted line denotes inter-speakerdependency.
(3) red color means self-connection.
(4) purple color means connection from future utterances toprevious utterances.
(5) yellow color means connection from previous utterances to future utterances.
since thereare two speakers, a and b. hence, there will be a total of 2 × 2 × 2 + 1 = 9 distinct relation types..edges, which reﬂects the contextual dependenciesamong utterance pairs..graph nodes each graph node corresponds toan utterance within d. hence, for a dialogue withn utterances, v = {v1, v2, .
.
.
, vn−1, vn}.
all thegraph nodes are initialized with utterance-level con-textualized embeddings: vi = ei..edges for short conversations, g will be a fully-connected graph whereby all graph nodes are con-nected to each other, including self-connection.
the intuition is that short conversations tend tofocus on a single topic and thus, each utteranceis contextually dependent on all the other utter-ances in the dialogue.
for long conversations, theremay be frequent topic shifts.
distant utteranceswithin the same dialogue may not be contextu-ally relevant to the current utterance.
sometimes,adding more context leads to diminishing perfor-mance gain or even negative impact (zhong et al.,2019).
therefore, a context window length, m ,is set, which means that vi is only connected tovj ∈ {vi−m , vi−m +1, .
.
.
, vi, vi+1, .
.
.
, vi+m }4.let vij ∈ e denote the edge from vj to vi.
eachedge is associated with an edge weight, aij, and arelation type, θij.
they are illustrated as follows:edge weights the edge weight determines therelative importance of the neighbour nodes w.r.t thecurrent node.
a similarity based attention module.
is applied to determine the edge weights.
for agraph node, vi, the set of weights, ai, w.r.t all itsincoming edges, should sum up to 1. the attentionweight is formulated in the following way:.
ai = softmax(et.
i we[ei−m , .
.
.
, ei+m ]),.
where.
aij = 1, we ∈ rd×d.
(3).
i+m(cid:88).
j=i−m.
more importance is placed upon neighbouring ut-terances on the same topic.
little attention is paidto the irrelevant utterances..edge relations following (ghosal et al., 2019),there are two aspects to take into account whendeﬁning the relation types.
one aspect is to capturespeaker dependencies.
this is because we want tomodel the interaction between the interlocutors ina dialogue.
the other aspect is to consider the tem-poral dependencies.
this pertains to the relativeposition of an utterance w.r.t another.
the explicitmodeling of such dependency is important sincethe ordering of utterances within a dialogue is anessential feature for learning dialogue coherence.
with these considerations, the total number of dis-tinct types of relations5 will be 2 (u∗i occurs beforeor after u∗i ) × 2 (either uaj ) × 2 (either uajor ubj ) plus the self-connection (i = j).
this is de-picted with different arrows connecting the graphnodes in figure 1. we deﬁne this set of 9 relationtypes as θ and θij ∈ θ..i or ub.
4for simplicity purpose, we do not explicitly include thecases when i <= m or i + m is greater than the total numberof utterances in a dialogue in the formula..5since we are considering dyadic dialogues, there are onlytwo speakers involved.
the formulation can be generalized tomulti-party dialogue..5679srobertadialogue graph node initializationrelation edgeconnectionfeaturetransformationgcnpoolingfcthe scoring processsequential contextlstmlstmlstmlstm3.3 feature transformation.
the dialogue-level representation, o:.
this section describes the process of transform-ing the initial node representation, ei, into both aspeaker and context aware vector representation,hi, which captures the dynamics of interaction w.r.tu∗i .
basically, the whole process is a two-stagegraph convolution..the ﬁrst stage aggregates information fromneighbourhood nodes to the current node vi basedon the relation-aware transformation motivatedby (schlichtkrull et al., 2018) whereby edges ofdifferent relation types are associated with differ-ent transformation matrix, w (cid:48)θ:.
(cid:48)i = σ(h.(cid:88).
(cid:88).
(cid:48).
w.θej + aiiw.
0ei).
(cid:48).
aijci,θ.
θ∈θ.
j∈sθi.for i = 1, 2, .
.
.
, n.(4).
(cid:48)in equation 4, hi is the intermediate node repre-sentation and σ denotes the activation function,such as relu.
sθi represents the set of indices ofnodes connected to vi with their edges vij havingthe relation type θ ∈ θ. aij and aii are the edgeweights of vij and vii respectively.
w (cid:48)×dand w (cid:48)×d are learnable parameters of thefeature transformation.
ci,θ is a problem speciﬁcnormalization constant, which can be set as a learn-able parameter or ﬁxed in advance..θ ∈ rd.
0 ∈ rd.
(cid:48).
(cid:48).
the second stage applies another graph convolu-tion operation on the intermediate node represen-(cid:48)tation, hi and the ﬁnal node representation, hi isobtained via:.
(cid:88).
hi = σ(.
w.(cid:48)(cid:48).
(cid:48).
(cid:48)(cid:48).
(cid:48).
h.j + w.0 h.i).
j∈sθifor i = 1, 2, .
.
.
, n.(5).
(cid:48)(cid:48).
(cid:48)×d.
where w (cid:48)(cid:48) ∈ rdand w (cid:48)(cid:48)are twolearnable parameters in the second stage of featuretransformation..0 ∈ rd.
(cid:48)(cid:48).
(cid:48)×d.
through equation 4 and equation 5, relevantcontextual information from neighbouring nodes iseffectively accumulated to the current node whileirrelevant information is ﬁltered out..3.4 the scoring process.
in the scoring step, hi is ﬁrst concatenated withei to obtain the ﬁnal utterance representation, gi.
next, a mean pooling layer is applied on all theutterance representations in a conversation to derive.
o =.
(cid:80)n| (cid:80)n.i=1 gij=1 gj|.
(6).
¯o, which corresponds to ¯d, is obtained in the sameway.
a uniﬁed score, sdial or s ¯dial, is derived bypassing o or ¯o through a fully-connected layer..3.5 training setup.
learning objectiveinspired by the preferencelearning approaches, the label, y for the d and ¯dpair is deﬁned as:.
y =.
(cid:40)1−1.
if d is preferred over ¯dif ¯d is preferred over d.(7).
the margin ranking loss function is adopted to traindynaeval..l = max(0, −y ∗ (sdial − s ¯dial) + 1).
(8).
sampling strategy two negative samplingstrategies are explored in this paper to construct ¯d:utterance replacement (ur) and speaker levelutterance shufﬂing (ss)..utterance replacement (ur) an utterancerandomly selected from a dialogue is replaced withanother utterance randomly chosen from a differ-ent dialogue.
this sampling strategy perturbs a dia-logue at the semantic level.
an utterance from a dif-ferent dialogue is considered topically in-congruentw.r.t the current dialogue context.
it breaks downthe current dialogue by suddenly injecting irrele-vant information..speaker level utterance shufﬂing (ss) withthis strategy, the order of utterances from onespeaker in a dialogue is kept the same while thatfrom another speaker is shufﬂed.
ss changes thecoherence structure of a dialogue w.r.t speciﬁcspeaker.
this strategy is motivated by (healey et al.,2014), which adopts a ”chance other” method tomeasure how much syntactic and lexical repetitionof a speaker happen by chance.
the reason why wedo not randomly permute the order of all utterancesin the dialogue is because random permutation ofall utterances is a very simple discrimination task..4 experiments.
in this work, we consider two experiment settingsto assess the effectiveness of dynaeval.
the ﬁrstsetting (section 4.2) is similar to the studies on.
5680dialogue coherence (cervone et al., 2018; mes-gar et al., 2020) where accuracy score is appliedto evaluate its discrimination capability in distin-guishing original dialogues from negative samples.
the second setting (section 4.3) is to evaluate itsdialogue-level and turn-level judgement capabil-ity via correlation analysis on the human-chatbotconversational datasets.
the domain of the eval-uation set is different from that of human-humanconversation datasets that dyaneval is trained on..4.1 dialogue datasets.
three bench-marking open-domain dialoguedatasets are included in our experiments, empa-thetic dialogue (rashkin et al., 2019), convai2personachat (zhang et al., 2018b; dinan et al.,2020) and dialydialog (li et al., 2017).
for train-ing, we remove dialogues containing less than 4utterances or more than 30 utterances.
statisticsof the three human-human dialogue corpora afterﬁltering is presented in table 1..convai2 personachat.
empathetic dialogue is designed for mim-icking the real-life human conversation scenariowhereby the interlocutors need to recognize andacknowledge the others’ feelings in the conversa-tion.
this dataset pertains to the short conversationscenario where interlocutors stick to a single topic.
is a crowd-sourced dataset where each pair of interlocutorstry to get to know each other by conditioning theirconversations on their respective persona proﬁleprovided in prior.
the dataset contains more num-ber of turns per dialogue as compared to empa-thetic dialogue.
hence, topic shift is more likely tooccur within a dialogue and this simulates the longconversation scenario mentioned in section 3.2..dailydialog is a high-quality human-humanconversation dataset, which reﬂects our day-to-daycommunications and covers different topics aboutour daily life, such as relationship and health.
theaverage dialogue length of dailydialog lies in themiddle of that of empathetic dialogue and con-vai2.
topic shift in the conversations of dailydia-log occurs less frequently as compared to those inconvai2..4.2 the dialogue-level discrimination task.
similar to the previous works (cervone and ric-cardi, 2020; mesgar et al., 2020), 20 perturbationsare created for each dialogue w.r.t both ur andss.
for each perturbation, two pairs are formed,{d, ¯d} with label y = 1 and { ¯d, d} with label.
empathetic dialogue.
training.
validation.
test.
convai2.
training.
validation.
test.
#dialog#turn#word#avg turn per dialogue#avg words per dialogue.
19,53184,1601,306,0604.3166.87.
#dialog#utterance#word#avg turn per dialogue#avg words per dialogue.
17,878262,6263,068,67214.69171.64.
#dialog#utterance#word#avg turn per dialogue#avg words per dialogue.
10,24584,9161,189,5278.29116.11.
2,76812,075201,8164.3672.91.
1,00015,566189,37415.57189.37.
9337,908109,1728.48117.01.
2,54710,973194,7724.3176.47.
-----.
9187,536106,6278.21116.15.dailydialog.
training.
validation.
test.
table 1: human-human dialogue corpora statistics.
y = −1.
then, we train, ﬁne-tune, and evaluatedynaeval on the training, validation, and test setsfor each sampling strategy.
note that all these setsare constructed with the same perturbation method.
baselines we compare dynaeval against threebaselines: random, cosim (xu et al., 2018) ands-dicoh (mesgar et al., 2020).
random baselinearbitrarily assigns a label to the input dialogue pairs.
it suggests the peformance lower bound.
cosimis a common method for dialogue coherence as-sessment (xu et al., 2018; zhang et al., 2018a).
itobtains a dialogue-level score by averaging the co-sine similarities between sentence embeddings ofall adjacent utterance pairs within the dialogue.
forfair comparison, we apply the same procedure de-scribed in section 3.1 to derive the sentence embed-ding of an utterance in cosim.
s-dicoh (mesgaret al., 2020) is a recent state-of-the-art dialogue co-herence model.
it models a dialogue with a neuralnetwork framework consisting of two bidrectionallstm layers with attention mechanism at both thetoken and utterance level..results and analysis it can be observed in ta-ble 2 that on all bench-marking dialogue datasets,dynaeval outperforms the baselines in both urand ss category.
even though the dialogue datasetspossess different characteristics as indicated in sec-tion 4.1, dynaeval exhbits robust performanceacross all the datasets.
this conﬁrms our hypoth-esis that dynaeval provides useful dialogue-levelrepresentation for distinguishing the original dia-logues from the corresponding negative samples.
especially when compared to s-dicoh, which mod-.
5681empathetic.
convai2.
dailydialog.
model.
ur.
ss.
ur.
ss.
ur.
ss.
randomcosims-dicoh.
50.0763.5480.33 ± 2.83.
50.0763.3386.04 ± 0.31.
50.2568.7966.80 ± 1.93.
50.2592.9390.35 ± 0.08.
50.1769.5983.67 ± 0.41.
49.6263.8084.92 ± 0.70.dynaeval.
94.30 ± 0.07.
90.37 ± 0.37.
85.23 ± 0.96.
98.65 ± 0.29.
91.89 ± 0.58.
91.65 ± 0.62.table 2: the accuracy (%) of dynaeval vs baselines on the test sets of empathetic dialogue and dailydialog aswell as the validation set of convai2.
ur & ss are the sampling strategies deﬁned in section 3.5. experimentsinvolving training are repeated ﬁve times with different random seeds for model weights initialization.
the averageand standard deviation are reported in the table..els a dialogue sequentially with bidrectional lstmand does not explicitly incoporate the speaker levelinteraction, the structured graph modeling of a dia-logue in dynaeval is more effective for capturingboth the interaction between the interlocutors andthe contextual information within a dialogue..based on the experimental results, it can be de-duced that the discrimination task with ur strategyis more challenging compared to that with ss strat-egy.
the accuracy scores achieved by s-dicoh inthe ss category is much higher than that in the urcategory on both datasets.
similar observation canbe made w.r.t cosim and dynaeval on the con-vai2 dataset.
dynaeval performs remarkably inthis task as it outperforms s-dicoh by a signiﬁcantmargin of 13.97, 18.43 and 8.22 on empatheticdialogue, convai2 and dailydialog respectively.
given these observations, we further hypothesizethat dynaeval model trained with ur strategy of-fers more useful dialogue representation to the dia-logue evaluation task..4.3 dialogue evaluation task.
to validate the above hypothesis, we assess theusefulness of dynaeval in both the dialogue-leveland turn-level evaluation tasks.
in both settings,spearman correlations between the scores gener-ated by dynaeval and the corresponding humanevaluation scores are computed.
the performanceof dynaeval is compared against several recentlyproposed dialogue evaluators..evaluation dataset fed (mehri and eskenazi,2020a) is a bench-marking dataset useful for bothdialogue-level and turn-level evaluation.
it con-tains both human-human conversations and human-chatbot conversations, which are collected by theauthors of the meena chatbot (adiwardana et al.,2020) in an interactive setup.
in total, 124 conver-sations are collected, out of which 40 come from.
interacting with the meena chatbot, 44 come frominteracting with the mitsuku chatbot and 40 aredrawn from human-human conversations.
the aver-age number of utterances per conversation is 13.72and the average number of words per utterance is9.23. human quality annotations of these conver-sations are performed at both the dialogue and turnlevel.
there are 9 quality aspects for turn-levelannotations and 11 for dialog-level annotations out-lined in the ﬁrst column of table 3. fed includes3348 turn-level and 1364 dialog-level annotations,for a total of 4712. the inter-annotator agreementsfor all the quality aspects, which indicate the met-ric performance upper bound, is shown in the lastcolumn of table 3..metrics to compare the recently proposedreference-free state-of-the-art dialogue metrics, in-cluding usr (mehri and eskenazi, 2020b), bert-ruber (ghazarian et al., 2019) (bert-r), gpt-2based coherence metric (pang et al., 2020) (gpt-2) and fed (mehri and eskenazi, 2020a)6, serveas the baseline dialogue evaluators.
since usr,bert-r and gpt-2 are turn-level metrics, aggre-gation of all the turn-level scores in a dialogue isrequired for dialogue-level evaluation.
the bestcorrelation scores at dialogue level are reported intable 3 among all the aggregation strategies forthese three metrics.
for completeness, we reporttheir correlation scores w.r.t difference aggregationstrategies in appendix a.2.
similar to dynaeval,s-dicoh provides a uniﬁed score for each dialogue.
based on insights from section 4.2, the best per-forming model in the ur category is chosen toscore the dialogues for both s-dicoh and dynae-val..dialogue-level evaluation dynaeval achieves.
6the correlation scores of fed is obtained from the origi-nal paper.
for each evaluation category, the highest score isreported among the scores provided by all its variants..5682dialogue aspects.
bert-r gpt-2.
usr.
s-dicoh.
fed.
dynaeval human.
coherenceerror recoveryconsistencydiversitytopic depthlikabilityunderstandingflexibilityinformativenessinquisitiveness.
overall.
interestingnessengagementspeciﬁcityrelevancecorrectnesssemantically appropriatenessunderstandablefluency.
dialogue-level spearman correlation.
0.2290.2420.1630.1960.1920.2810.1980.2530.2110.337.
0.248.
0.2350.2060.3270.1510.0810.0440.0510.079.
0.195.
0.1230.0960.0910.1470.0970.1790.0700.1340.1160.071.
0.123.
-0.107-0.086-0.112-0.1050.041-0.084-0.071-0.151.
0.1940.1700.1690.2420.3410.2210.1720.2090.2880.188.
0.288.
0.0850.1070.0950.1830.0980.2010.1100.220.
0.038-0.0540.0170.0590.046-0.070-0.1000.0440.028-0.054.
-0.073.
0.0310.0400.062-0.051-0.040-0.069-0.075-0.007.
-0.022.
0.2510.1650.1160.4490.5220.2620.3060.4080.3370.298.
0.443.
0.4310.3180.3260.1520.1330.1770.1110.224.
0.209.turn-level spearman correlation.
0.4230.3110.3520.3320.4390.3980.3610.3890.3960.388.
0.482.
0.2890.2550.2720.2650.2160.2330.1850.096.
0.264.
0.8090.8400.5620.7890.8330.8380.8090.8160.8060.769.
0.830.
0.8190.7980.7900.7530.7800.6820.5220.714.
0.820.overall.
-0.095.
0.137.table 3: comparison of both dialogue and turn level spearman correlations among state-of-the-art automaticmetrics on the fed evaluation dataset.
the results are reported for the 11 and 9 unique quality categories atturn and dialogue level respectively.
scores with p-values larger than 0.05 are italicized (indicating statisticalinsigniﬁcance).
the best score for each category is highlighted in bold..the highest correlation scores in 8 out of 11 dia-logue aspects, including the overall category.
forthe other three categories, dynaeval attains secondhighest correlation scores.
we can see that dy-naeval signiﬁcantly outperforms s-dicoh.
theseresults showcase that structured graph modeling ofa dialogue with explicit incorporation of speakerand utterance level dependencies provides mean-ingful dialogue-level representations.
such repre-sentations capture information of various dialogueattributes that are beneﬁcial for the dialogue-levelevaluation task..moreover, bert-r, gpt-2 and usr are state-of-the-art turn-level evaluation metrics.
they eval-uate a dialogue based on aggregation of scoresof all the context-response pairs within the dia-it can be observed that their correlationlogue.
scores across individual dialogue aspects are notas high as those of dynaeval.
this supports ourhypothesis in section 1 that turn-level quality evalu-ation may be insufﬁcient to assess the performanceof open-domain dialogue systems..in addition, dialogue aspects, including coher-ence, likability, informativeness and inquisitive-ness, are highly dependent on the interaction of theinterlocutors.
amongst all the dialogue aspects,.
dynaeval achieves signiﬁcantly higher scores inthese four categories.
this attributes to its incorpo-ration of the speaker level dependency..turn-level evaluation furthermore, it can beobserved that dynaeval achieves the highest corre-lation in 5 out of 9 categories including the overallcategory.
this demonstrates that dynaeval is notonly useful for holistic evaluation of a dialogue, butalso useful for turn level evaluation.
in this sense,dynaeval serves as a better proxy to the humanevaluation process (li et al., 2019) whereby hu-mans mainly evaluate the conversations in a holisticmanner and laser-focus on the problematic turns..speciﬁcally, dynaeval performs well in turn-level aspects, such as relevance, semantic appro-priateness and correctness.
these aspects highlycorrelate to the dialogue-level attributes, such ascoherence and understanding, suggesting that theevaluation of these turn-level attributes also bene-ﬁt from the explicit modeling of the speaker andutterance level interaction in a uniﬁed framework.
error analysis an interesting ﬁnding is thatdynaeval and fed actually complement eachother at both dialogue and turn level.
for exam-ple, at the dialogue level, fed performs well indiversity and topic depth, but struggles with coher-.
5683ence and consistency.
dynaeval performs well incoherence and consistency, but its performance indiversity is much lower in comparison to fed.
thismay be because dialogpt, the backbone of fed,was trained on a large amount of reddit data, whichcontain diverse amount of topics and variation ofexpressions while dynaeval is trained on a singledialogue domian.
moreover, dialogpt does notexplicitly model such speaker-level interaction, butdynaeval does.
hence, dynaeval is more usefulfor evaluating coherence and consistency aspectsof a dialogue.
one way to improve dynaeval forevaluating topic depth and diversity is to pre-trainon a large amount of dialogue data with a varietyof topics and then ﬁne-tune it on the target domain..another observation is that dynaeval performssigniﬁcantly poorer for the ﬂuency aspect at turn-level than for other turn-level aspects.
additionally,gpt-2, usr and fed, which leverage pretrainedlanguage model, perform signiﬁcantly better thandynaeval in this category.
this may be becausedynaeval directly models a dialogue at the utter-ance level instead of at the token level, while theother metrics consider the language modeling ob-jective, which focuses more on the token-level de-pendencies rendering them effective for evaluatingthe naturalness of a response.
a remedy to thisproblematic aspect of dynaeval is to introduceperturbation strategies targeting the token level,such as word drop, word shufﬂing and word re-placement (sinha et al., 2020; park et al., 2021).
such strategies provide negative samples mimick-ing the non-sensical or non-grammatical responsesproduced by certain seq2seq generative models.
another simple solution is to combine dynaevalwith turn-level metrics speciﬁcally designed forevaluating naturalness of dialogue responses..besides the ﬂuency aspect, dynaeval’s perfor-mance in interestingness, engagement and speci-ﬁcity at the turn level is not as pronounced as thatof fed.
this may be because purely modeling thedialogue itself is not enough for all the aspects.
themodel may need to incorporate external knowledgeconcerning a diverse range of topics to be able to re-ﬂect these attributes.
the same conclusion can alsobe drawn from dynaeval’s relatively weaker per-formance in the diversity category at the dialoguelevel..lastly, dynaeval primarily targets open-domaindialogues where there is no clear or predeﬁnedtask to perform.
when evaluating task-oriented.
dialogues, task completion will take a more centralrole.
meta-information such as intents and requesttypes are important to determine task completionand therefore, the evaluation framework will re-quire further adaptation accounting for these infor-mation when evaluating task-oriented dialogues..5 conclusion & future work.
dynaeval serves as a uniﬁed framework for bothturn and dialogue level evaluation in open-domaindialogue.
it provides meaningful representationsthat incorporate information reﬂecting various im-portant dialogue attributes.
its explicit modeling ofspeaker and utterance level interaction leveraginggcn has been proven beneﬁcial for the evalua-tion task.
lastly, the error analysis in section 4.3sheds light on how dynaeval can be further im-proved.
dynaeval can also be combined with thespecialized turn-level metrics, such as those target-ing ﬂuency and engagement, to fully approximatethe interactive human evaluation process..acknowledgement.
this work is supported by human-robot interac-tion phase 1 (grant no.
19225 00054), nationalresearch foundation (nrf) singapore under thenational robotics programme; human robot col-laborative ai for ame (grant no.
a18a2b0046),nrf singapore; robert bosch (sea) pte ltd un-der edb’s industrial postgraduate programme –ii (edb-ipp), project title: applied natural lan-guage processing; and by the spanish projects:amic (mineco, tin2017-85854-c4-4-r) andcaviar (mineco, tec2017-84593-c2-1-r)projects partially funded by the european union..ethical considerations & broader impact.
this study conforms to the prevailing ethical guide-lines.
all datasets used are in the public domain.
inaddition, we have identiﬁed a way that dynaevalcan help address the ethical concerns.
by explic-itly training the framework to discriminate safedialogues from unsafe ones, it can help detect dia-logues containing inappropriate sentences, such asthose regarding injustice and discrimination.
suchapplication may be useful in many real-life sce-narios where the behaviors of chatbots need to beproperly monitored to avoid insensitive and irre-sponsible comments from the chatbots..5684references.
daniel adiwardana, minh-thang luong, david r so,jamie hall, noah fiedel, romal thoppilan, zi yang,apoorv kulshreshtha, gaurav nemade, yifeng lu,et al.
2020. towards a human-like open-domainchatbot.
arxiv preprint arxiv:2001.09977..regina barzilay and mirella lapata.
2008. modelinglocal coherence: an entity-based approach.
compu-tational linguistics, 34(1):1–34..alessandra cervone and giuseppe riccardi.
2020. isthis dialogue coherent?
learning from dialogue actsin proceedings of the 21th annualand entities.
meeting of the special interest group on discourseand dialogue, pages 162–174, 1st virtual meeting.
association for computational linguistics..alessandra cervone, evgeny stepanov, and giuseppericcardi.
2018. coherence models for dialogue.
proc.
interspeech 2018, pages 1011–1015..lu chen, bowen tan, sishan long, and kai yu.
2018.structured dialogue policy with graph neural net-in proceedings of the 27th internationalworks.
conference on computational linguistics, pages1257–1268, santa fe, new mexico, usa.
associ-ation for computational linguistics..michael denkowski and alon lavie.
2014. meteor uni-versal: language speciﬁc translation evaluation forin proceedings of the ninthany target language.
workshop on statistical machine translation, pages376–380, baltimore, maryland, usa.
associationfor computational linguistics..jan deriu, don tuggener, pius von d¨aniken, jon andercampos, alvaro rodrigo, thiziri belkacem, aitorsoroa, eneko agirre, and mark cieliebak.
2020.spot the bot: a robust and efﬁcient framework forthe evaluation of conversational dialogue systems.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3971–3984, online.
association for computa-tional linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..luis fernando d’haro, rafael e banchs, chiori hori,and haizhou li.
2019. automatic evaluation of end-to-end dialog systems with adequacy-ﬂuency met-rics.
computer speech & language, 55:200–215..lowe, et al.
2020. the second conversational intel-in the neurips’18ligence challenge (convai2).
competition, pages 187–208.
springer..nouha dziri, ehsan kamalloo, kory mathewson, andosmar zaiane.
2019. evaluating coherence in dia-logue systems using entailment.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 3806–3812, minneapolis,minnesota.
association for computational linguis-tics..sarah e. finch and jinho d. choi.
2020. towards uni-ﬁed dialogue system evaluation: a comprehensiveanalysis of current evaluation protocols.
in proceed-ings of the 21th annual meeting of the special inter-est group on discourse and dialogue, pages 236–245, 1st virtual meeting.
association for computa-tional linguistics..sudeep gandhe and david traum.
2016..asemi-automated evaluation metric for dialoguein situated dialog in speech-model coherence.
based human-computer interaction, pages 217–225. springer..asma ghandeharioun, judy hanwen shen, natashajaques, craig ferguson, noah jones, agatalapedriza, and rosalind picard.
2019. approximat-ing interactive human evaluation with self-play foropen-domain dialog systems.
advances in neuralinformation processing systems, 32:13658–13669..sarik ghazarian, johnny wei, aram galstyan, andnanyun peng.
2019. better automatic evaluation ofopen-domain dialogue systems with contextualizedin proceedings of the workshop onembeddings.
methods for optimizing and evaluating neural lan-guage generation, pages 82–89, minneapolis, min-nesota.
association for computational linguistics..sarik ghazarian, ralph weischedel, aram galstyan,and nanyun peng.
2020. predictive engagement:an efﬁcient metric for automatic evaluation of open-in proceedings of thedomain dialogue systems.
aaai conference on artiﬁcial intelligence, vol-ume 34, pages 7789–7796..deepanway ghosal, navonil majumder, soujanya po-ria, niyati chhaya, and alexander gelbukh.
2019.dialoguegcn: a graph convolutional neural net-work for emotion recognition in conversation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 154–164, hong kong, china.
association for computa-tional linguistics..emily dinan, varvara logacheva, valentin malykh,alexander miller, kurt shuster,jack urbanek,douwe kiela, arthur szlam, iulian serban, ryan.
barbara j grosz, aravind k joshi, and scott weinstein.
1995. centering: a framework for modelling thelocal coherence of discourse..5685suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a. smith.
2020. don’t stop pretraining:inadapt language models to domains and tasks.
proceedings ofthethe 58th annual meeting ofassociation for computational linguistics, pages8342–8360, online.
association for computationallinguistics..michael alexander kirkwood halliday and ruqaiyahasan.
2014. cohesion in english.
9. routledge..patrick gt healey, matthew purver, and christinehowes.
2014. divergence in dialogue.
plos one,9(6):e98598..ryuichiro higashinaka, toyomi meguro, kenji ima-mura, hiroaki sugiyama, toshiro makino, andyoshihiro matsuo.
2014. evaluating coherence inin fifteenthopen domain conversational systems.
annual conference of the international speech com-munication association..lishan huang, zheng ye, jinghui qin, liang lin, andxiaodan liang.
2020. grade: automatic graph-enhanced coherence metric for evaluating open-in proceedings of thedomain dialogue systems.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 9230–9240,online.
association for computational linguistics..thomas n. kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in 5th international conference on learn-ing representations, iclr 2017, toulon, france,april 24-26, 2017, conference track proceedings.
openreview.net..tian lan, xian-ling mao, wei wei, xiaoyan gao, andheyan huang.
2020. pone: a novel automatic eval-uation metric for open-domain generative dialoguesystems.
acm transactions on information systems(tois), 39(1):1–37..grandee lee and haizhou li.
2020. modeling code-switch languages using bilingual parallel corpus.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, pages 860–870, online.
association for computational linguis-tics..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2016. a diversity-promoting ob-jective function for neural conversation models.
inproceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 110–119, san diego, california.
associationfor computational linguistics..margaret li, jason weston, and stephen roller.
2019.acute-eval: improved dialogue evaluation with opti-mized questions and multi-turn comparisons.
arxivpreprint arxiv:1909.03087..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-ally labelled multi-turn dialogue dataset.
in proceed-ings of the eighth international joint conference onnatural language processing (volume 1: long pa-pers), pages 986–995, taipei, taiwan.
asian federa-tion of natural language processing..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74–81, barcelona, spain.
association for computational linguistics..chia-wei liu, ryan lowe, iulian serban, mike nose-worthy, laurent charlin, and joelle pineau.
2016.how not to evaluate your dialogue system: anempirical study of unsupervised evaluation metricsfor dialogue response generation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2122–2132, austin,texas.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..shikib mehri and maxine eskenazi.
2020a.
unsu-pervised evaluation of interactive dialog with di-in proceedings of the 21th annual meet-alogpt.
ing of the special interest group on discourse anddialogue, pages 225–235, 1st virtual meeting.
asso-ciation for computational linguistics..shikib mehri and maxine eskenazi.
2020b.
usr: anunsupervised and reference free evaluation metricfor dialog generation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 681–707, online.
association forcomputational linguistics..mohsen mesgar, sebastian b¨ucker,.
and irynagurevych.
2020. dialogue coherence assessmentwithout explicit dialogue act labels.
in proceedingsofthe associationfor computational linguistics, pages 1439–1450,online.
association for computational linguistics..the 58th annual meeting of.
rostislav nedelchev, jens lehmann, and ricardo us-beck.
2020. language model transformers as eval-in proceedingsuators for open-domain dialogues.
of the 28th international conference on compu-tational linguistics, pages 6797–6808, barcelona,spain (online).
international committee on compu-tational linguistics..yixin nie, mary williamson, mohit bansal, douwekiela, and jason weston.
2020.i like ﬁsh, espe-cially dolphins: addressing contradictions in dia-logue modelling.
arxiv preprint arxiv:2012.13391..bo pang, erik nijkamp, wenjuan han, linqi zhou,yixian liu, and kewei tu.
2020. towards holisticand automatic evaluation of open-domain dialogue.
5686generation.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 3619–3629, online.
association for computa-tional linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..chaehun park, eugene jang, wonsuk yang, and jongpark.
2021. generating negative samples by manipu-lating golden responses for unsupervised learning ofa response evaluation model.
in proceedings of the2021 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 1525–1534, on-line.
association for computational linguistics..libo qin, zhouyang li, wanxiang che, minheng ni,and ting liu.
2020. co-gat: a co-interactivegraph attention network for joint dialog act recog-nition and sentiment classiﬁcation.
arxiv preprintarxiv:2012.13260..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..hannah rashkin, eric michael smith, margaret li, andy-lan boureau.
2019. towards empathetic open-domain conversation models: a new benchmark andin proceedings of the 57th annual meet-dataset.
ing of the association for computational linguis-tics, pages 5370–5381, florence, italy.
associationfor computational linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..shiki sato, reina akama, hiroki ouchi, jun suzuki,and kentaro inui.
2020. evaluating dialogue genera-tion systems via response selection.
in proceedingsof the 58th annual meeting of the association forcomputational linguistics, pages 593–599, online.
association for computational linguistics..franco scarselli, marco gori, ah chung tsoi, markushagenbuchner, and gabriele monfardini.
2008. thegraph neural network model.
ieee transactions onneural networks, 20(1):61–80..michael schlichtkrull, thomas n kipf, peter bloem,rianne van den berg, ivan titov, and max welling.
2018. modeling relational data with graph convolu-tional networks.
in european semantic web confer-ence, pages 593–607.
springer..abigail see, stephen roller, douwe kiela, and ja-son weston.
2019. what makes a good conver-sation?
how controllable attributes affect humanjudgments.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 1702–1723, minneapolis, minnesota.
associ-ation for computational linguistics..koustuv sinha, prasanna parthasarathi, jasmine wang,ryan lowe, william l. hamilton, and joelle pineau.
2020. learning an unreferenced metric for onlinedialogue evaluation.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 2430–2441, online.
associationfor computational linguistics..eric michael smith, mary williamson, kurt shuster,jason weston, and y-lan boureau.
2020. can youput it all together: evaluating conversational agents’ability to blend skills.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 2021–2030, online.
associationfor computational linguistics..chongyang tao, lili mou, dongyan zhao, and ruiyan.
2018. ruber: an unsupervised method for au-tomatic evaluation of open-domain dialog systems.
in thirty-second aaai conference on artiﬁcial in-telligence..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
in international2018. graph attention networks.
conference on learning representations..sean welleck, jason weston, arthur szlam, andkyunghyun cho.
2019. dialogue natural languageinference.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 3731–3741, florence, italy.
association forcomputational linguistics..sixing wu, ying li, dawei zhang, yang zhou, andzhonghai wu.
2020. diverse and informative di-alogue generation with context-speciﬁc common-sense knowledge awareness.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5811–5820, online.
as-sociation for computational linguistics..xinnuo xu, ondˇrej duˇsek, ioannis konstas, and ver-ena rieser.
2018. better conversations by model-ing, ﬁltering, and optimizing for coherence and di-versity.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 3981–3991, brussels, belgium.
associationfor computational linguistics..sanghyun yi, rahul goel, chandra khatri, alessan-dra cervone, tagyoung chung, behnam hedayatnia,anu venkatesh, raefer gabriel, and dilek hakkani-tur.
2019. towards coherent and engaging spoken.
5687dialog response generation using automatic conver-in proceedings of the 12th in-sation evaluators.
ternational conference on natural language gen-eration, pages 65–75, tokyo, japan.
association forcomputational linguistics..chen zhang, luis fernando d’haro, rafael e banchs,thomas friedrichs, and haizhou li.
2021a.
deepam-fm: toolkit for automatic dialogue evaluation.
in conversational dialogue systems for the nextdecade, pages 53–69.
springer..chen zhang, grandee lee, luis fernando d’haro, andhaizhou li.
2021b.
d-score: holistic dialogue eval-uation without reference.
ieee/acm transactionson audio, speech, and language processing..hainan zhang, yanyan lan, jiafeng guo, jun xu, andxueqi cheng.
2018a.
reinforcing coherence for se-quence to sequence model in dialogue generation.
inijcai, pages 4567–4573..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018b.
per-sonalizing dialogue agents: i have a dog, do youin proceedings of the 56th an-have pets too?
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 2204–2213, melbourne, australia.
association for com-putational linguistics..yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and bill dolan.
2020. dialogpt : large-scale generative pre-training for conversational re-in proceedings of the 58th an-sponse generation.
nual meeting of the association for computationallinguistics: system demonstrations, pages 270–278, online.
association for computational linguis-tics..peixiang zhong, di wang, and chunyan miao.
2019.knowledge-enriched transformer for emotion de-in proceedingstection in textual conversations.
of the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 165–176, hongkong, china.
association for computational lin-guistics..yunxiao zhou, man lan, and wenting wang.
2019.hierarchical intention enhanced network for auto-in 2019 in-matic dialogue coherence assessment.
ternational joint conference on neural networks(ijcnn), pages 1–8.
ieee..a additional experimental results.
a.1 utterance-level pooling techniques.
to derive the dialogue-level representation, wehave adopted the mean pooling method in dy-naeval.
in this section, we examine the effectsof different pooling methods in the dialogue-leveldiscrimination task.
speciﬁcally, we compare theperformance of mean pooling against max poolingand the concatenation of sentence vectors derivedwith both mean and max pooling.
the performancecomparison is presented in table 4. it can be ob-served that the performance difference across vari-ous pooling strategies is not statistically signiﬁcant..strategy.
ur.
ss.
meanmaxmean+max.
94.30 ± 0.0794.17 ± 0.1694.19 ± 0.04.
90.37 ± 0.3790.75 ± 0.2490.64 ± 0.06.table 4: the accuracy scores (%) of dynaeval onthe test set of empathetic dialogue with differentutterance-level pooling techniques.
the average andstandard deviation are reported in the table..a.2 dialogue-level correlation analysis of.
turn-level metrics.
for each turn-level metric, we have applied foursimple aggregation strategies to derive dialoguelevel scores from their respective constituent turnlevel scores: (1) mean, (2) sum, (3) max and (4)multiplication.
the dialogue level correlation coef-ﬁcients of usr, bert-ruber and gpt-2 basedcoherence metric are reported in table 5, table 6and table 7 correspondingly.
note that for turn-level metrics leveraging the language model objec-tive, we don’t consider token-level aggregation vari-ants.
instead, we follow the same formulations inthe original papers.
for example, the gpt-2 basedcoherence metric (pang et al., 2020) computes aturn-level score based on averaging the token-wiseconditional log probabilities in the correspondingresponse..it can be observed that all three metrics don’t per-form well at dialogue level evaluation.
this furthervalidates our statement in section 1 that turn-levelquality evaluation may be insufﬁcient to assess theperformance of open-domain dialogue systems asthey don’t speciﬁcally model the interaction overan entire dialogue..5688quality.
mean.
sum max.
prod.
quality.
mean.
sum max.
prod.
usr.
gpt-2.
coherenceerror recoveryconsistencydiversitytopic depthlikabilityunderstandingflexibilityinformativenessinquisitiveness.
-0.0020.034-0.0250.0920.0540.072-0.0270.0560.025-0.008.
0.1230.0960.0910.1470.0970.1790.0700.1340.1160.071.
-0.086-0.057-0.048-0.033-0.036-0.047-0.062-0.032-0.100-0.071.
-0.120-0.091-0.088-0.145-0.094-0.175-0.066-0.131-0.112-0.070.overall.
-0.002.
0.123.
-0.086.
-0.120.table 7: dialogue level spearman correlation coefﬁ-cients of gpt-2 based coherence metric w.r.t differentturn-level aggregation strategies on the fed dataset..of 0.5 per epoch.
a dropout of 0.5 is also applied.
for empathetic dialogue and dailydialog, thecontext window length, m is set to 4, because thesetwo datasets contain relatively short conversations(4.31 and 7.90 average number of utterances perdialogue respectively).
a context window size of4 ensures each utterance is connected to all the re-maining utterances in most of the dialogues.
theutterances may provide important contextual in-formation to each other within a dialogue.
forconvai2, m is set to 2 to avoid introducing toomuch irrelavant context information.
this is be-cause most of the conversations in convai2 areabout two people getting to know each other andthere are frequent topic changes in the conversa-tions.
m serves as an important hyperparameter tocontrol the inﬂuence of an utterance on the rest ina dialogue..for training dynaeval, we have ﬁltered out di-alogues of which the number of utterances is lessthan 4 or more than 30. we hypothesize that dia-logues with less than 4 utterances containing littleinformation for modeling speaker and utterancelevel interaction.
moreover, there are very fewdialogues with more than 30 utterances in bothdatasets.
including them leads to large graphs andunnecessary paddings, which slow down the train-ing process..coherenceerror recoveryconsistencydiversitytopic depthlikabilityunderstandingflexibilityinformativenessinquisitiveness.
0.1940.1700.1500.2420.3410.2210.1720.2090.2880.148.
0.1110.0830.1690.1670.1450.1930.1120.1510.1570.099.
0.0210.0750.0380.2350.2550.1090.0040.1640.1710.188.
0.1580.1300.0990.1930.2950.1260.1240.1290.2370.128.overall.
0.288.
0.166.
0.094.
0.212.table 5: dialogue level spearman correlation coef-ﬁcients of usr w.r.t different turn-level aggregationstrategies on the fed dataset.
scores with p-valueslarger than 0.05 are italicized (indicating statistical in-signiﬁcance).
the best score for each category is high-lighted in bold..quality.
mean.
sum max.
prod.
bert-r.coherenceerror recoveryconsistencydiversitytopic depthlikabilityunderstandingflexibilityinformativenessinquisitiveness.
0.2220.2310.1410.1800.1810.2560.1890.2280.1940.326.
0.2210.2420.1630.1960.1920.2810.1980.2530.2110.337.
0.2290.2280.1480.1640.1630.2490.1890.2320.1860.331.
0.0410.005-0.030-0.0510.008-0.037-0.023-0.036-0.0230.056.overall.
0.231.
0.248.
0.224.
-0.021.table 6: dialogue level spearman correlation coefﬁ-cients of bert-ruber w.r.t different turn-level aggre-gation strategies on the fed dataset..b reproducibility.
b.1 training setup & hyperparameters.
for all the experiments involving training, we runthe experiments ﬁve times with different randomseeds for model weights initialization to reducethe risk of randomness.
the experiments are per-formed on a single tesla v100 32gb gpu witha batch size of 512. the model is trained for 20epochs and its parameters are optimized using theadam optimizer.
the average run time for eachepoch is around 8 hours and 15 minutes.
the initiallearning rate is set to 0.002 and decays by a factor.
5689