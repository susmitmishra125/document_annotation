energy-based reranking: improving neural machine translationusing energy-based models.
sumanta bhattacharyya, amirmohammad rooshenas∗department of computer science, college of computing and informaticsuniversity of north carolina charlotte{sbhatta9,rooshenas}@uncc.edu.
subhajit naskar, simeng sun, mohit iyyer, and andrew mccallumcollege of information and computer science, university of massachusetts amherst{snaskar,simeng,miyyer,mccallum}@cs.umass.edu.
abstract.
the discrepancy between maximum likeli-hood estimation (mle) and task measuressuch as bleu score has been studied beforefor autoregressive neural machine translation(nmt) and resulted in alternative training al-gorithms (ranzato et al., 2016; norouzi et al.,2016; shen et al., 2016; wu et al., 2018).
how-ever, mle training remains the de facto ap-proach for autoregressive nmt because of itscomputational efﬁciency and stability.
despitethis mismatch between the training objectiveand task measure, we notice that the samplesdrawn from an mle-based trained nmt sup-port the desired distribution – there are sam-ples with much higher bleu score compar-ing to the beam decoding output.
to bene-ﬁt from this observation, we train an energy-based model to mimic the behavior of the taskmeasure (i.e., the energy-based model assignslower energy to samples with higher bleuscore), which is resulted in a re-ranking algo-rithm based on the samples drawn from nmt:energy-based re-ranking (ebr).
we use bothmarginal energy models (over target sentence)and joint energy models (over both source andtarget sentences).
our ebr with the joint en-ergy model consistently improves the perfor-mance of the transformer-based nmt: +3.7bleu points on iwslt’14 german-english,+3.37 belu points on sinhala-english, +1.4bleu points on wmt’16 english-germantasks..1.introduction.
autoregressive models are widely used for neuralmachine translation (nmt) (bahdanau et al., 2015;gehring et al., 2017; vaswani et al., 2017).
theautoregressive factorization provides a tractablelikelihood computation as well as efﬁcient sam-pling.
the former results in the effective maxi-mum likelihood estimation (mle) for training the.
∗amirmohammad rooshenas is the corresponding author..parameters of nmt models.
however, optimiz-ing likelihood does not guarantee an improvementin task-based measures such as the bleu score,which has motivated directly optimizing task mea-sures with reinforcement learning (ranzato et al.,2016; norouzi et al., 2016; shen et al., 2016; bah-danau et al., 2017; wu et al., 2018).
however, fornmt, these training algorithms are often used inconjunction with mle training (wu et al., 2018)or as ﬁne-tuning (choshen et al., 2020)..interestingly, we observe that samples drawnfrom an nmt model trained using mle may havehigher quality (measured with bleu) than the out-puts of beam search.
in particular, we draw 100target samples for each source sentence from annmt model trained using mle on the iwslt’14german-english task, and observe that an ora-cle ranker – i.e.
argmaxy∼pnmt(y|x) bleu(., y∗),where (x, y∗) is the pair of source and gold targetsentence – achieves the high score of 67.54, whilethe beam decoding achieves 33.87. we also lookat the distribution of the spearman rank correla-tion coefﬁcient of the drawn samples with respectto the log probability score of the baseline nmt(basenmt).
figure 1 shows that there is no strongcorrelation between the bleu score ranking ofsamples and the log probability score ranking forthe majority of source sentences; thus, maximuma priori (map) decoding is incapable of ﬁndingthe desired output.
in parallel to our study, eikemaand aziz (2020) also report that the mismatch re-garding mle training of autoregressive models isattributable to the distribution of the probabilitymass rather than the parameter estimation, result-ing in a poor map decoding..instead of looking for an alternate algorithmfor parameter estimation, these results motivateus to explore training a parametric approxima-tion of the metric, here bleu score: ωθ(y, x) ≈bleu(y, y∗).
therefore the decoding becomes:.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4528–4537august1–6,2021.©2021associationforcomputationallinguistics4528figure 1: distribution of the spearman rank-order correlation coefﬁcients for the training data (left) and test data(right) of the iwslt’14 german-english task..argmaxy∼pnmt(.|x) ωθ(y, x)..we use energy-based models (ebms) to param-eterize ωθ(y, x).
ebms (lecun et al., 2006) aregeneral parametric models that assign a scalar en-ergy value to each conﬁguration of input variables,thus deﬁning an unnormalized probability distribu-tion.
although computing the partition function isintractable for general ebms, we only require therelative energy of the sampled sentences from thebasenmt model, thus canceling out the normal-ization constant.
in this paper we use two differ-ent energy-based models: marginal energy model(marginal-ebm) deﬁned only over target sentencesand joint energy model (joint-ebm) deﬁned overboth source and target sentences..figure 1 also shows the correlation coefﬁcientof the energy ranking and bleu score using bothmarginal-ebm and joint-ebm.
the shift in the co-efﬁcient distribution suggests that decoding basedon energy scores results in better bleu scorescompared to decoding based on the log probabilityscores of the basenmt model.
also we observethat joint-ebm works better than using marginal-ebm as joint-ebm better captures the correlationof source and target sentences, while marginal-ebm is not directly conditioned on the source sen-tence..in this paper, we describe how to train ebms1 toachieve the desired ranking.
our energy ranker con-sistently improves the performance of transformer-based nmt on german-english, romanian-english and italian-english tasks from iwslt’14,the french-english task from iwslt’17, german-english task from wmt’14, and english-germantask from wmt’16, as well as the low-resourcesinhala-english and nepali-english tasks de-scribed in the flores dataset (guzm´an et al.,2019)..1the code is available at https://github.com/.
rooshenas/ebr_mt.
figure 2: the ebm is trained such that its energy land-scape is consistent with the bleu score.
marginal-ebm is not conditioned on the source sentence, thuseach local region is trained to have similar ranking asthat bleu score for the samples in the region..2 energy-based reranking.
t.using ebm eθ to reweight the samples from annmt deﬁnes a new probability distribution overthe output sentences (see grover et al.
(2019)):pθ(y|x) ∝ pnmt(y|x) exp( −eθ(y,x)), where tis temperature.
the ideal re-ranker requires anebm with the energy function eθ(y, x) such thatpθ(y|x) and bleu(y, yi) have similar modes forall (xi, yi) ∈ d, where d is an empirical datadistribution.
to train θ we use rank-based train-ing (rohanimanesh et al., 2011; rooshenas et al.,2018, 2019).
rank-based training enforces thatthe samples from pθ(.)
have similar ranking withrespect to both the energy score and task measure(see figure 2)..to sample from pθ(y|x), we sample k sen-tences from pnmt(y|x) using multinomial sam-pling from locally normalized distributions overthe output and reweight the samples based on theenergy network exp( −eθ(y,x)).
then we resam-tple two sentences, y1 and y2, from the renormal-ized set, which deﬁnes a conditional distribution:exp(−eθ(y,x)/t )p i(y|x) =k exp(−eθ(yk,x)/t ) (a similar sam-pling approach has been used in deng et al.
(2020)).
now we train the energy model such that the rank-ing of y1 and y2 with respect to the energy model.
(cid:80).
4529is consistent with their ranking with respect to thetask metric, bleu score..in general, we assume yh is the sentence withthe higher bleu score and yl is the sentence withwith the lower bleu score.
therefore, the trainingobjective of eθ(y, x) becomes:.
m = α(bleu(yh, yi) − bleu(yl, yi))ξ(yi, xi) = m + eθ(yh, xi) − eθ(yl, xi).
max(ξ(yi, xi), 0)..(1).
(cid:88).
minθ.
(yi,xi)∈d.
where ξ(yi, xi) is the margin violation and α isthe margin weight.
algorithm 1 outlines the wholetraining procedure..if we deﬁne the energy only over sentences of thetarget language, eθ(y), we can share the energy-model among multiple language pairs with thesame target language.
in this case we have to, ﬁrst,sample the language l from our language set andthen sample a sentence pair from the selected lan-guage training set dl.
the probability of selectinga language is proportional to the number of sen-tences in its training set..algorithm 1 rank-based training of ebm.
pnmt(y|x) ← pretrained nmteθ(y, x) ← energy based models for target sentencesrepeat.
l ← 0.for batch size do.
(cid:80).
exp(−eθ (y,x)/t ) for y ∈ yi.
sample (xi, yi) from dyi ← collect k samples from pnmt(.|xi)p i(y) ← exp(−eθ (y,x)/t )y∈yiy1, y2 ← samples from pi(y)yh ← argmaxy1,y2yl ← argminy1,y2m ← α(bleu(yh, yi) − bleu(yl, yi))l ← l + max(m + eθ(yh, xi) − eθ(yl, xi), 0).
{bleu(y1, yi), bleu(y2, yi)}{bleu(y1, yi), bleu(y2, yi)}.
end forθ ← θ − λ∇θl.
until convergence.
// λ is learning rate.
in this paper, we use bert (devlin et al., 2019)to parameterize both eθ(y, x) and eθ(y).
sec-tion 4.3 and 4.4 discuss the construction of eθ indetail..3 related work.
grover et al.
(2019) show that importance weightscan be used to make generative models better ﬁtthe desired data distribution: pθ(y) ∝ q(y)ωθ(y),where q(y) is a generative model that we can efﬁ-ciently take samples from and ωθ(y) is the impor-tance weight function.
the importance weights can.
be determined using a discriminator that differen-tiates the generated samples from the target data.
rosenfeld et al.
; parshakova et al.
(2001; 2019)deﬁne q(y) as autoregressive model and ωθ(y) us-ing a log-linear model: ωθ(y) = exp(θt φ(y)),where φ(y) is the vector of sufﬁcient statistics(features) evaluated at y. the log-linear modelsimpliﬁes training the parameters θ: ∇θpθ(y) =(cid:80)y∈d φ(y)−eˆy∼pθ(.)φ(ˆy).
the expectation termcan be estimated using rejecting sampling or im-portance sampling given the proposal distributionq. deng et al.
(2020) extend this approach fortext generation by using unrestricted ebms insteadof log-linear models: ωθ(y) = exp(−eθ(y)).
they train the ebm using noise contrastive estima-tion (gutmann and hyv¨arinen, 2010).
we ﬁnd thisless suitable for re-ranking in the translation tasks(see section 4)..discriminative re-ranking was ﬁrst introduced byshen et al.
(2004) for improving the performanceof machine translation (mt).
they have trained alinear separator using the perceptron learning algo-rithm to distinguish the top r translations from therest of the translations in the n-best possible outputs.
the features for the discriminator are extractedfrom both source and target sentences.
mizumotoand matsumoto (2016) combine the score of mtand the linear model using more complex syntacti-cal features to re-rank the target sentences.
here,we rely on the features learned by bert, and giventhe high capacity of the energy model, we train theenergy model to respect the ranking of every pairof samples..gulcehre et al.
(2017) describe using languagemodel (lm) to improve the performance of nmtusing shallow and deep fusion.
shallow modelscombine the marginal probability of predictingeach word in nmt and lm: log pnmt(yi|y<i) +λ log plm(yi|y<i), while deep fusion concatenatesthe hidden states of two models before predict-ing each word and uses parallel data to ﬁne-tunethe weights.
similar to deep fusion, domhan andhieber (2017) feed the unnormalized output of lmto the decoder of nmt.
domhan and hieber (2017)jointly train the lm and nmt using monolingualtarget-side data and parallel data, respectively.
sen-nrich et al.
(2016a) augment the parallel trainingdata with monolingual data with the target languageand back-translation..re-ranking with lm has also been explored byng et al.
(2019), where they decode the output.
4530based on log p(y|x) + λ1 log p(x|y) + λ2 log p(y),where p(y|x) is the direct model provided by nmt,p(x|y) is computed via back-translation and p(y)is an lm.
our approach differs from the previousmethods that use lms for re-ranking as we trainour energy-based model to be consistent with thetask measure instead of using pre-trained lms.
inour experiments, we only explore the effect of us-ing the direct model plus lm, nevertheless, back-translation can also be added into our model forfurther improvement..recently, salazar et al.
(2020) use masked lan-guage models (mlm) such as bert to score hy-potheses from nmt.
salazar et al.
(2020) describethe score of a mlm as pseudo-log-likelihood score(pll).
to calculate pll score of a sentence, eachtoken wi in the sentence is sequentially masked,which allows the calculation of log p(wi|w\i) fromthe output of the mlm.
the normalized pseudo-log-probability of the sentence is the average of log-probability of the masked words given the rest ofthe words in the sentence: 1i=1 log p(wi|w\i),nwhere n is the length of the sentence.
we use thisapproach as one of our baselines..(cid:80)n.in parallel to our work, guo et al.
(2020) pro-poses using two different bert models as an en-coder of the source language (x-bert) and adecoder of the target language (y-bert).
guoet al.
(2020) add an extra trainable encoder-decoderadaption module followed by a feed-forward mod-ule to each layer of the decoder and a feed-forwardmodule to each layer of the encoder.
(please seeguo et al.
(2020) for more detail on the architec-ture.)
for ﬁne-tuning xy-bert for translationtasks, guo et al.
(2020) keep all xy-bert’s param-eters ﬁxed except the parameters of the new mod-ules, and use mask-predict decoding (ghazvinine-jad et al., 2019) for running test-time inference.
guo et al.
(2020) report a signiﬁcant improvementover prior non-autoregressive models and superiorperformance comparing to autoregressive methodson iwslt’14 german-english task.
their ﬁnd-ing is consistent with our improvement using thepretained bert model.
however, our joint-ebmmodel is a different way of using bert for transla-tion, which does not require separate bert modelsfor source and target language.
please see sec-tion 4.9 for a detailed comparison..finally, other works also discuss using bertto improve the performance of nmt.
clinchantet al.
(2019) describe initializing the embedding or.
the whole encoder with bert’s parameters.
zhuet al.
(2020) use an attention model to incorporatethe output of bert into encoder and decoder ofnmt.
in our approach, we use bert as an externalenergy-based ranker..4 experiments.
4.1 datasets.
we use german-english (de→en), romanian-english (ro→en) and italian-english (it→en)from iwslt’14 datasets and french-english(fr→en) from iwslt’17 translation tasks.
wealso use iwslt’14 english-german (en→de) toshow that the proposed method can be expandedto translation tasks with a different target language.
all sentences were preprocessed using byte-pair-encoding (sennrich et al., 2016b).
for all languagepairs in iwslt’14 and iwslt’17, we merge thetest datasets tst2010, tst2011, tst2012 and reportbleu on the merged dataset.
we also use german-english (de→en) from the wmt’14 and english-german (en→de) from wmt’16 translation tasks.
finally, we use low-resource translation tasksnepali-english (ne→en) and sinhala-english(si→en) from flores (guzm´an et al., 2019) trans-lation tasks.
we follow dataset distribution and pre-processing steps described in guzm´an et al.
(2019)using the flores implementation.
flores datasetcontains development (dev), devtest and test datasetsimilar to guzm´anfor both language pairs.
et al.
(2019) we use the devtest dataset for all ourevaluations..4.2 base model.
we use the transformer2(vaswani et al., 2017) asour basenmt.
our transformer architecture in-cludes six encoder and six decoder layers, and thenumber of attention heads, embedding dimensionand inner-layer dimension are 8, 512 and 4096, re-spectively.
we use dropout, weight decay, labelsmoothing to regularize our models.
we use layernormalization and early stopping.
models are op-timized using adam (kingma and ba, 2015) withparameters β1 = 0.9, β2 = 0.98, and (cid:15) = 1e−8and we use the same learning rate scheduler as ottet al.
(2019).
we trained our models on 1 nvidiatitanx gpu..2we use the implementation in opennmt (klein et al.,.
2017) and fairseq (ott et al., 2019) toolkits..4531table 1: bleu score comparison for iwslt, flores, and wmt (indicated using *) tasks..de−→en.
fr−→en.
it−→en ro−→en.
si−→en ne−→en.
en−→de de→en*.
en→de*.
basenmt + beambasenmt + samplebasenmt + lmbasenmt + mlmnce-ebrmarginal-ebrshared-ebrconditional-ebm.
oracle.
33.8733.9834.2534.4234.4735.6835.7537.58.
67.54.
31.5031.5931.5632.1332.0033.7733.8035.02.
68.43.
32.0832.2232.5233.6832.8934.0034.1436.05.
71.77.
33.2133.6433.0133.8532.2334.4834.6537.19.
73.95.
7.107.197.117.707.988.6210.2910.47.
14.71.
6.076.446.027.217.367.269.259.82.
11.91.
28.8328.8528.9130.1228.2230.82-30.97.
52.14.
30.1330.2830.3130.6131.4231.65-32.21.
50.89.
28.8428.8928.9328.9829.0329.14-30.23.
45.15.table 2: shared-ebr performance for si→en by train-ing with difference sets of language pairs..basenmt.
+ si→en.
+ de→en.
+ fr→en.
all.
7.10.
8.62.
9.30.
9.76.
10.29.
4.3 marginal-ebm.
to construct the energy network over the sen-language, we use a pre-tences of the targettrained bert (devlin et al., 2019) from hugging-face (wolf et al., 2019) as our pretrained languagemodel and project the hidden state of bert foreach output token into a scalar value and deﬁne theenergy value of the target sentence as the average ofthe scalar values.
we use the bert-base uncasedmodel with 12 encoder layers, 768 hidden statedimension, 12 attention heads and 110m parame-ters.
for the projection layer, we use a 2-layer mlpwith 256 hidden variables.
in our experiments, weonly train the parameters of the projection layerand the rest of bert’s parameters remain frozen.
we use margin weight of α = 10 and tempera-ture t = 1000 for our experiments.
we regularizethe projection layer using l2 regularization.
mod-els are optimized using adam (kingma and ba,2015) with parameters β1 = 0.9, β2 = 0.98, and(cid:15) = 1e−8 and a learning rate of 0.01. we run allexperiments on 1 nvidia tesla m40 gpu..4.4.joint-ebm.
joint-ebm must assign a score to a pair of sen-tences from source and target languages, so to con-struct the joint-ebm, similar to marginal-ebm,we need a joint-bert.
we feed the sentencepairs from source and target languages jointly tobert, thus the name joint-bert.
since joint-bert has not been pre-trained to accept pairs ofsentences from two different languages, we ﬁne-tune it for 12 epochs using the input format of[cls]source[sep]target[sep] with the pairs ofsource and target sentences for each translation.
task.
for ﬁne-tuning, we only mask the tokens ofthe target sentence.
for all translation tasks we usethe bert-base, multilingual cased model with12 encoder layers, 768 hidden state dimension, 12attention heads and 110m parameters.
after ﬁne-tuning joint-bert, we follow the same architec-ture as marginal-ebm for the joint-ebm..4.5 methods.
as the main baseline, we run beam decoding witha beam size of ﬁve over the trained basenmt(basenmt+beam).
we also use the samples drawnfrom the basenmt and report the bleu score ofthe sample with the highest log-probability score onbasenmt (basenmt+sample).
for all methodswe use 100 target samples for each source sentence.
basenmt+lm draws samples from the basenmtand uses log pnmt(y|x) + λ log plm (y) to rankthe samples (λ = 0.01 out of the set of {0.001,0.01, 0.1} results in the best performance)..in our basenmt+lm baseline, we use pre-trained language model to calculate log plm (y).
for the {de, fr, it, ro, si, ne}−→en tasks,we use a pretrained transformer-xl (dai et al.,2019) transfo-xl-wt103 and for the en−→de taskwe use a pretrained xlm (lample and con-neau, 2019) xlm-mlm-ende-1024 from hugging-face (wolf et al., 2019).
basenmt+mlm is simi-lar to basenmt+lm but it uses log pnmt(y|x) +λ log pm lm (y), where pm lm is the averagepseudo-log-probability of sample y calculated us-ing bert.
we use the same architecture of bertas marginal-ebm, but we ﬁne-tuned bert formlm over the target sentences in training sets for10 epochs.
we tuned λ similar to basenmt+lm.
ebr is our method that uses rank-based trainingfor ebms.
we explore ebr with marginal-ebm(marginal-ebr) and joint-ebm (conditional-ebr).
we also use noise-contrastive estimationto train our marginal-ebm, similar to denget al.
(2020), which we refer to as nce-ebr.
next,.
4532we have shared-ebr that trains single marginal-ebm for the tasks with the same target language.
shared-ebr is only trained on iwslt and flo-res tasks with english target.
for this method, weﬁrst sample a translation task and then sample abatch from that task and follow algorithm 1 for thetraining of the marginal-ebm.
finally, as an upperbound for the best achievable result, we also extractthe translations from the sample that are closest tothe gold data (based on bleu score)..4.6 results.
table 1 shows the performance of the describedmethods for iwslt, flores, and wmt transla-tion tasks.3 basenmt+sample achieves a bet-ter score than beam decoding suggesting thatour multinomial sampling supports the modes ofthe distribution deﬁned by the basenmt.
sim-ilarly, oracle values are high,indicating thatthe samples also support the desired distribu-tion.
this satisﬁes the necessary condition forpθ(y|x) ∝ pnmt(y|x) exp(−eθ(y, x)/t ) to becloser to the desired distribution.
re-rankingwith a language model using basenmt+lmimproves over basenmt+sample for de→en,fr→en,it→en, and en→de, butfails onro→en, si→en, and ne→en.
however,in all of these tasks,the difference betweenbasenmt+sample and basenmt+lm is not sub-stantial.
basenmt+mlm is consistently bet-ter than basenmt+lm.
the performance ofbasenmt+mlm is attributable to pll scoring, asthe encoder has the global information over the sen-tence.
marginal-ebr performs considerably bet-ter than basenmt+{beam, sample, lm, mlm}and better than nce-ebr on all tasks except onne→en, where nce-ebr outperforms marginal-ebr.
the main advantage of marginal-ebr overnce-ebr is the use of only sampled data insteadof gold data for training.
see section 4.7 for de-tailed discussion..shared-ebr has a signiﬁcant improvement overthe marginal-ebr, especially it improves the low-resource task of si→en by more than 2 bleupoints.
for this task, we also show that how usingmore language pairs in training improves perfor-mance (table 2)..conditional-ebr outperforms shared-ebr onall tasks.
the performance of conditional-ebr is.
3we use sacrebleu (post, 2018) as a consistent bleu.
implementation for all of our experiments..table 3: the effect of using gold data in the rankingobjective for marginal-ebr..γ.
0.0.
0.25.
0.75.
1.0.de→enfr→en.
35.6833.77.
35.0033.15.
34.2031.65.
33.7530.82.table 4:iwslt’14 de-en.
effect of entropy regularization on.
regularization no regularization.
basenmt + beamconditional-ebr.
oracle.
33.9637.88.
68.21.
33.8737.58.
67.54.due to the use of joint-ebm model, which enablesthe model to deﬁne different energy landscapesfor different source sentences.
therefore, samplesfrom the target language are more separable giventhe source sentence, while marginal-ebm may notdistinguish target sentences for different sourcesentences..the translation improvement of using ebr oniwslt and flores translation tasks are more con-siderable than the improvement of using ebr onwmt tasks.
we believe that pre-trained berthelps low-resource tasks more than large-scaletranslation tasks..4.7 effect of using gold data.
noise-contrastive estimation (nce) trains the en-ergy model using a discriminative training to distin-guish gold data from the sampled data (gutmannand hyv¨arinen, 2010; deng et al., 2020).
in con-trast to the nce-ebr, ebr does not directly usegold data in the training of the ebm, but only ex-ploit it to determine the rank of two points as wellas the margin.
to show that our approach is ef-fective, we introduce parameter γ as the percent-age of the time that we can use gold data as oneof the points (for example, yh in algorithm 1).
table 3 shows the results for both de→en andfr→en tasks using marginal-ebr.
as we increasethe value of γ, the performance of marginal-ebrdrops.
the main reason is that basenmt rarelyproduces the exact correct translation in the sam-ple set, thus learning the ranking with respect tothe gold data is not very informative.
when theγ is zero, the marginal-ebm learns to re-rank thesamples with respect to their distance to the golddata..45334.8 regularized training.
we hypothesize that the performance of ebr im-proves as we increase the support of the base distri-bution toward the mode of the true distribution.
toshow that we add an entropy regularization term tothe likelihood training of basenmt:.
maxθ.
(cid:88).
(cid:88).
(x,y)∈d.
i.log p(yi|y<i, x).
(cid:88).
− β.i.p(yi) log p(yi)..(2).
entropy regularization improves the diversity ofsamples, and as a result, oracle’s score increases by0.67 bleu points.
while basenmt only beneﬁtsless than 0.1 bleu points from the regularization,conditional-ebr improves by 0.3 bleu points(see table 4).
for this study we explored β from{0.01, 0.1}, and reported results use β = 0.01selected based on the validation set.
basenmttrained with β = 0.1 has the oracle score of 65.76on the test set (comparing to the oracle score of68.21 for β = 0.01), which indicates that strongerregularization reduces the sample quality..4.9 using xy-bert for joint-ebm.
to explore the effect of a different way of condi-tioning on the source language, we compare theebm constructed using the joint-bert modelwith ebm constructed using recently introducedxy-bert (guo et al., 2020).
to construct ebmfrom xy-bert, we remove the output layer andproject each hidden-state of the ﬁnal layer to ascalar energy value similar to how we build ebmfrom bert.
we compare these two models oniwslt’14 de→en task.
for xy-bert we usegerman bert for the encoder and english bertfor the decoder, following guo et al.
(2020).
ourjoint-bert uses multilingual bert because wefeed both source and target sentences to bertjointly.
conditional-ebr with xy-bert achieves38.33 bleu score, which is 0.75 bleu pointshigher than conditional-ebr with joint-bertand improves the performance of xy-bert withmask-predict decoding (ghazvininejad et al., 2019)by 1.84 bleu points.4 we believe that the im-provement in conditional-ebr using xy-bert ismostly attributable to using specialized bert mod-els.
moreover, xy-bert has extra trainable mod-ules, so we could ﬁne-tune xy-bert on the trans-.
4guo et al.
(2020) report 36.49 bleu score using xy-.
bert with 10 iterations of mask-predict decoding..lation task for 60 epochs, while keeping the rest ofthe parameters ﬁxed without causing catastrophicforgetting.
joint-bert, on the other hand, doesnot have any extra parameters, so we ﬁne-tunedall parameters for only 15 epochs.
further trainingof joint bert resulted in poor performance.
weleave adding extra modules for better ﬁne-tuningof joint bert for future studies..4.10 maximizing expected score.
as another comparison, we train our models by di-rectly maximizing the expected bleu score (com-pared to rank-based training):.
maxθ.eyp∼pθ(.|x)[bleu(yp, y∗)].
(3).
we use log-trick to calculate the gradient of theabove objective:.
∇θepθ [bleu(yp, y∗)].
= eyp∼pθ [bleu(yp, y∗)[−∇θeθ(yp, x)+ ey(cid:48)∼pθ [∇θe(y(cid:48), x)]]]..(4).
we use self-normalized importance sampling todraw samples from the energy-based model.
weuse one sample to approximate the outer expec-tation and 10 samples to approximate the innerexpectation.
we train both marginal-ebm andjoint-ebm by maximizing the expected bleuscore on iwslt’14 de-en.
the former obtains ascore of 34.20 bleu and the latter achieves 34.77bleu points.
both models underperform rank-based training..4.11.inference time.
we compare the inference latency of ebr vari-ations with basenmt (table 5).
we use100 samples for re-ranking using marginal-ebr,conditional-ebr with joint-bert and condi-tional ebr with xy-bert (guo et al., 2020).
in-ference on marginal-ebr takes on average about170 milliseconds per sentence more than inferencein basenmt as we have to sample 100 sentencesfrom basenmt and evaluate them on the energymodel.
we evaluate the marginal-ebr only on thetarget sentences, while we evaluate conditional-ebr for sequences from both source and target lan-guage, so the input sequence of conditional-ebris longer, thus having higher latency comparingto marginal-ebr.
we also measure the latency ofconditional-ebr when we use xy-bert architec-ture to construct joint-ebm.
in this case, we have.
4534table 5: average inference time per sentence (millisec-onds), baseline transformer uses beam width of 5 andebr uses 100 samples per sentence..method.
de−→en.
en−→de.
base-nmtmarginal-ebrconditional-ebr (joint bert)conditional-ebr (xy-bert).
572749836921.
577756838929.type.
pronoun.
contraction.
rephrase.
tense.
examplen: to us , he meant the freedom .
e: for us , it meant freedom .
n: they are exotic ; they are experimental .
e: they are exotical .
they &apos;re experimental .
n: and it &apos;s our unseen reality .
e: that &apos;s our invisible reality .
n: a new life has been born .
e: and a new life was born ..two separate bert models for source and targetlanguages, increasing the number of parameters by3.3 million and latency by about 90 millisecondsper sentence compared to conditional-ebr thatuses the joint-bert model..5 analysis.
in this section, we study the sentence preference ofmarginal-ebr created by the energy ranking..5.1 qualitative analysis.
we qualitatively investigate how the output ofmarginal-ebr differs from that of basenmtmodel.
on the iwslt’14 test set, we examined200 examples on which marginal-ebr did betterthan nmt and 200 examples where basenmt isbetter.
we ﬁnd that about 30% of the time, themarginal-ebr model chooses a translation withchanged pronoun.
another frequent ‘preference’marginal-ebr makes compared to basenmt isto use the contraction form.
since this iwsltdata set is from ted talk, we conjecture thatthe energy model favors the translations that arein more oral style.
besides, it is also commonfor the marginal-ebr model to prefer rephrases,for example, instead of using ‘will’ as used inbasenmt, marginal-ebr chooses the form ‘amgoing to’.
finally, we ﬁnd, for some pairs,marginal-ebr chooses a different tense comparedto the basenmt model (from map decoding)..table 6 presents quintessential examples weﬁnd after examining 400 examples on iwslt’14de→en test set.
it is worth to mention that exam-ples do not strictly land in only one category.
forexample, the sentences we show in the ‘rephrase‘type will also be counted as the change of pronouns.
with this in mind, we compute statistics over the400 sentences and ﬁnd each of the ‘pronoun’, ‘con-traction’ and ‘rephrase’ appears approximately30% of the time while 10% of the sentences change‘tense’.
the other less frequent types are changingof determiners, prepositions and deletion (compar-ing the map decoding of basenmt and preferred.
table 6: typical examples on iwslt’14 test set, cat-egorized by the difference between basenmt andmarginal-ebr.
‘n’ stands for basenmt and ‘e’stands for marginal-ebr introduced in this paper..table 7: bleu scores by length on iwslt’14 test set.
sentences are divided into 3 groups according to ref-erence length:less than or equal to 5 , in the rangebetween 5 and 10, greater than 10..(0, 5].
(5, 10].
(10, ).
nmt23.78marginal-ebr 26.38.
33.2235.20.
34.7735.68.output by marginal-ebr)..5.2 bleu gains by length.
besides the qualitative analysis, we are also cu-rious to see whether the improvement is affectedby length.
table 7 shows the bleu scores on theiwslt’14 test set, which is divided into three binsaccording to the target length.
shorter sentenceshave the largest increase in bleu, and the gain isdecreasing as length increases.
we reckon that itis easier for ebr to cover larger training space forsentences of shorter length and thus has the largestimprovement in bleu for these sentences..5.3 random sentences.
in the absence of access to the source sentence, theenergy model ranks the outputs purely accordingto the features of target sentences.
we hypothe-size that the energy model is better at differentiat-ing incoherent and coherent sentences and manageto show that through the following analysis.
weapply two kinds of shufﬂe on iwslt’14 test settargets: (1) global shufﬂe: tokens in the sentenceare randomly shufﬂed (2) local shufﬂe: we ﬁrstrandomly select a token and randomly shufﬂe thetokens within a local window of three.
then wecompute the energy scores of these shufﬂed sen-tences as well as the untouched ones.
the energyscores are listed in table 8.
(the energy model as-sign a lower energy to its preference.)
we ﬁnd 87%.
4535table 8: energy scores of randomly shufﬂed sentencesas well as original targets on iwslt’14 de→en testset..deep bidirectional transformers for language under-in proc.
of naacl-hlt, pages 4171–standing.
4186..shufﬂe type average energy scores.
localglobaloriginal.
-0.0130.002-0.037.of the time, the energy model is able to distinguishthe original sentence from a local shufﬂed one, and90.5% from the global shufﬂed one.
this supportsour hypothesis that the energy model is capable ofcapturing the ﬂuency of generated candidates..6 conclusion and future work.
we introduce energy-based re-ranking (ebr) toimprove the performance of autoregressive neuralmachine translation.
despite its superior perfor-mance, ebr suffers from high latency because ofits dependency on sampling from an autoregres-sive model.
directly sampling from the underlyingebm can speed up the inference, which is our fu-ture direction in order to beneﬁt from the power ofenergy-based models for machine translation..references.
dzmitry bahdanau, philemon brakel, kelvin xu,anirudh goyal, ryan lowe, joelle pineau, aaroncourville, and yoshua bengio.
2017. an actor-criticalgorithm for sequence prediction.
in proc.
of iclr..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in proc.
of iclr..leshem choshen, lior fox, zohar aizenbud, and omriabend.
2020. on the weaknesses of reinforcementlearning for neural machine translation.
in proc.
oficlr..stephane clinchant, kweon woo jung, and vassilinanikoulina.
2019. on the use of bert for neural ma-in proceedings of the 3rd work-chine translation.
shop on neural generation and translation, pages108–117..zihang dai, zhilin yang, yiming yang, jaime g. car-bonell, quoc v. le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyonda ﬁxed-length context.
corr, abs/1901.02860..yuntian deng, anton bakhtin, myle ott, arthur szlam,and marc’aurelio ranzato.
2020. residual energy-based models for text generation.
in proc.
of iclr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of.
tobias domhan and felix hieber.
2017. using target-side monolingual data for neural machine translationin proc.
of emnlp,through multi-task learning.
pages 1500–1505..bryan eikema and wilker aziz.
2020. is map decodingall you need?
the inadequacy of the mode in neuralmachine translation.
arxiv:2005.10283..jonas gehring, michael auli, david grangier, denisyarats, and yann n. dauphin.
2017. convolutionalsequence to sequence learning.
in proc.
of icml..marjan ghazvininejad, omer levy, yinhan liu, andluke zettlemoyer.
2019. mask-predict: parallel de-coding of conditional masked language models.
inproceedings of the 2019 conference on empiricalmethods in natural language (emnlp-ijcnlp),pages 6112–6121, hong kong, china.
associationfor computational linguistics..aditya grover, jiaming song, ashish kapoor, kennethtran, alekh agarwal, eric j horvitz, and stefanoermon.
2019. bias correction of learned generativemodels using likelihood-free importance weighting.
in advances in neural information processing sys-tems 32, pages 11058–11070..caglar gulcehre, orhan firat, kelvin xu, kyunghyuncho, and yoshua bengio.
2017. on integrating a lan-guage model into neural machine translation.
com-puter speech & language, 45:137–148..junliang guo, zhirui zhang, linli xu, hao-ran wei,incor-boxing chen, and enhong chen.
2020.porating bert into parallel sequence decoding within advances in neural information pro-adapters.
cessing systems 33..michael gutmann and aapo hyv¨arinen.
2010. noise-contrastive estimation: a new estimation principlefor unnormalized statistical models.
in proc.
of ais-tats, pages 297–304..francisco guzm´an, peng-jen chen, myle ott, juanpino, guillaume lample, philipp koehn, vishravchaudhary, and marc’aurelio ranzato.
2019. twonew evaluation datasets for low-resource machinetranslation: nepali-english and sinhala-english.
inarxiv preprint arxiv:1902.01382..francisco guzm´an, peng-jen chen, myle ott, juanpino, guillaume lample, philipp koehn, vishravchaudhary, and marc’aurelio ranzato.
2019. twonew evaluation datasets for low-resource machinetranslation: nepali-english and sinhala-english.
corr, abs/1902.01382..diederik p. kingma and jimmy ba.
2015. adam:in proc.
of.
a method for stochastic optimization.
iclr..4536information processing systems 32, pages 13522–13532..ronald rosenfeld, stanley f chen, and xiaojin zhu.
2001. whole-sentence exponential language mod-els: a vehicle for linguistic-statistical integration.
computer speech & language, 15(1):55–73..julian salazar, davis liang, toan q. nguyen, andkatrin kirchhoff.
2020. masked language modelin proceedings of the 58th annual meet-scoring.
ing of the association for computational linguis-tics, pages 2699–2712.
association for computa-tional linguistics..rico sennrich, barry haddow, and alexandra birch.
2016a.
improving neural machine translation mod-els with monolingual data.
in proc.
of acl, pages86–96..rico sennrich, barry haddow, and alexandra birch.
2016b.
neural machine translation of rare wordswith subword units.
in proc.
of acl, pages 1715–1725..libin shen, anoop sarkar, and franz josef och.
2004.discriminative reranking for machine translation.
inproc.
of naacl-hlt..shiqi shen, yong cheng, zhongjun he, wei he, huawu, maosong sun, and yang liu.
2016. minimumrisk training for neural machine translation.
in proc.
of acl, pages 1683–1692..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30, pages 5998–6008..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r’emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
arxiv, abs/1910.03771..lijun wu, fei tian, tao qin, jianhuang lai, and tie-yan liu.
2018. a study of reinforcement learningfor neural machine translation.
in proc.
of emnlp..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tieyan liu.
2020.incorporating bert into neural machine translation.
in proc.
of iclr..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander m. rush.
2017. opennmt:open-source toolkit for neural machine translation.
in proc.
of acl..guillaume lample and alexis conneau.
2019. cross-corr,language model pretraining..lingualabs/1901.07291..yann lecun, sumit chopra, raia hadsell, m ranzato,and f huang.
2006. a tutorial on energy-basedlearning.
predicting structured data, 1(0)..tomoya mizumoto and yuji matsumoto.
2016. dis-criminative reranking for grammatical error correc-tion with statistical machine translation.
in proc.
ofnaacl-hlt, pages 1133–1138..nathan ng, kyra yee, alexei baevski, myle ott,michael auli, and sergey edunov.
2019. facebookfair’s wmt19 news translation task submission.
in proceedings of the fourth conference on ma-chine translation (volume 2: shared task papers,day 1), pages 314–319..mohammad norouzi, samy bengio, navdeep jaitly,mike schuster, yonghui wu, dale schuurmans, et al.
2016. reward augmented maximum likelihood forneural structured prediction.
in advances in neuralinformation processing systems, pages 1723–1731..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andmichael auli.
2019.fairseq: a fast, extensibletoolkit for sequence modeling.
in proc.
of naacl-hlt 2019: demonstrations..tetiana parshakova, jean-marc andreoli, and marcdymetman.
2019. global autoregressive models fordata-efﬁcient sequence learning.
in proc.
of conll,pages 900–909..matt post.
2018. a call for clarity in reporting bleuscores.
in proceedings of the third conference onmachine translation: research papers, pages 186–191..marc’aurelio ranzato, sumit chopra, michael auli,and wojciech zaremba.
2016. sequence level train-in proc.
ofing with recurrent neural networks.
iclr..khashayar rohanimanesh, kedar bellare, aron cu-lotta, andrew mccallum, and michael l wick.
2011. samplerank: training factor graphs withatomic gradients.
in proc.
of icml, pages 777–784..amirmohammad rooshenas, aishwarya kamath, andandrew mccallum.
2018. training structured pre-diction energy networks with indirect supervision.
in proc.
of naacl-hlt..amirmohammad rooshenas, dongxu zhang, gopalsharma, and andrew mccallum.
2019.search-guided, lightly-supervised training of structured pre-in advances in neuraldiction energy networks..4537