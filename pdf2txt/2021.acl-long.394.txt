exploring dynamic selection of branch expansion ordersfor code generationhui jiang1,2∗ chulun zhou1,2∗ fandong meng3 biao zhang4 jie zhou3degen huang5 qingqiang wu1,2 jinsong su1,2†1school of informatics, xiamen university2institute of artiﬁcial intelligence, xiamen university3pattern recognition center, wechat ai, tencent inc, china4school of informatics, university of edinburgh 5dalian university of technology.
{hjiang,clzhou}@stu.xmu.edu.cn.
{fandongmeng,withtomzhou}@tencent.com.
b.zhang@ed.ac.uk.
huangdg@dlut.edu.cn.
{wuqq,jssu}@xmu.edu.cn.
abstract.
due to the great potential in facilitating soft-ware development, code generation has at-tracted increasing attention recently.
gener-ally, dominant models are seq2tree models,which convert the input natural language de-scription into a sequence of tree-constructionactions corresponding to the pre-order traver-sal of an abstract syntax tree (ast).
how-ever, such a traversal order may not be suit-able for handling all multi-branch nodes.
inthis paper, we propose to equip the seq2treemodel with a context-based branch selector,which is able to dynamically determine opti-mal expansion orders of branches for multi-branch nodes.
particularly, since the selec-tion of expansion orders is a non-differentiablemulti-step operation, we optimize the selectorthrough reinforcement learning, and formulatethe reward function as the difference of modellosses obtained through different expansion or-ders.
experimental results and in-depth analy-sis on several commonly-used datasets demon-strate the effectiveness and generality of our ap-proach.
we have released our code at https://github.com/deeplearnxmu/cg-rl..1.introduction.
code generation aims at automatically generating asource code snippet given a natural language (nl)description, which has attracted increasing atten-tion recently due to its potential value in simplify-ing programming.
instead of modeling the abstractsyntax tree (ast) of code snippets directly, mostof methods for code generation convert ast intoa sequence of tree-construction actions.
this al-lows for using natural language generation (nlg)models, such as the widely-used encoder-decoder.
joint work with pattern recognition center, wechat ai,.
tencent inc, china..*equal contribution†corresponding author.
models, and obtains great success (ling et al., 2016;dong and lapata, 2016, 2018; rabinovich et al.,2017; yin and neubig, 2017, 2018, 2019; hay-ati et al., 2018; sun et al., 2019, 2020; wei et al.,2019; shin et al., 2019; xu et al., 2020; xie et al.,2021).
speciﬁcally, an encoder is ﬁrst used to learnword-level semantic representations of the input nldescription.
then, a decoder outputs a sequenceof tree-construction actions, with which the cor-responding ast is generated through pre-ordertraversal.
finally, the generated ast is mappedinto surface codes via certain deterministic func-tions..generally, during the generation of dominantseq2tree models based on pre-order traversal,branches of each multi-branch nodes are expandedin a left-to-right order.
figure 1 gives an exam-ple of the nl-to-code conversion conducted by aseq2tree model.
at the timestep t1, the model gen-erates a multi-branch node using the action a1 withthe grammar containing three ﬁelds: type, name,and body.
thus, during the subsequent genera-tion process, the model expands the node of t1 tosequentially generate several branches in a left-to-right order, corresponding to the three ﬁelds of a1.
the left-to-right order is a conventional bias formost human-beings to handle multi-branch nodes,which, however, may not be optimal for expand-ing branches.
alternatively, if we ﬁrst expand theﬁeld name to generate a branch, which can informus the name ‘e’, it will be easier to expand theﬁeld type with a ‘exception’ branch due to the highco-occurrence of ‘e’ and ‘exception’..to verify this conjecture, we choose tranx(yin and neubig, 2018) to construct a variant:tranx-r2l, which conducts depth-ﬁrst gener-ation in a right-to-left manner, and then comparetheir performance on the django dataset.
we ﬁndthat about 93.4% of asts contain multi-branchnodes, and 17.38% of ast nodes are multi-branch.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5076–5085august1–6,2021.©2021associationforcomputationallinguistics5076percentage.
only tranxonly tranx-r2l.
8.477.66.table 1: the percentages of multi-branch nodes, whichcan only be correctly handled by different models.
tranx-r2l is a variant of tranx (yin and neubig,2018), which handles multi-branch nodes in a right-to-left order..ones.
table 1 reports the experimental results.
wecan observe that 8.47% and 7.66% of multi-branchnodes can only be correctly handled by tranxand tranx-r2l, respectively.
therefore, we con-clude that different multi-branch nodes have differ-ent optimal branch expansion orders, which can bedynamically selected based on context to improvethe performance of conventional seq2tree models.
in this paper, we explore dynamic selectionof branch expansion orders for code generation.
speciﬁcally, we propose to equip the conventionalseq2tree model with a context-based branch se-lector, which dynamically quantiﬁes the prioritiesof expanding different branches for multi-branchnodes during ast generations.
however, sucha non-differentiable multi-step operation poses achallenge to the model training.
to deal with this is-sue, we apply reinforcement learning to train the ex-tended seq2tree model.
particularly, we augmentthe conventional training objective with a rewardfunction, which is based on the model training lossbetween different expansion orders of branches.
inthis way, the model is trained to determine opti-mal expansion orders of branches for multi-branchnodes, which will contribute to ast generations.
to summarize, the major contributions of our.
work are three-fold:.
• through in-depth analysis, we point out thatdifferent orders of branch expansion are suit-able for handling different multi-branch astnodes, and thus dynamic selection of branchexpansion orders has the potential to improveconventional seq2tree models..• we propose to incorporate a context-based branch selector into the conventionalseq2tree model and then employ reinforce-ment learning to train the extended model.
tothe best of our knowledge, our work is theﬁrst attempt to explore dynamic selection ofbranch expansion orders for code generation.
• experimental results and in-depth analyses.
figure 1: an example of code generation using theconventional seq2tree model in pre-order traversal..demonstrate the effectiveness and generalityof our model on various datasets..2 background.
as shown in figure 1, the procedure of code gener-ation can be decomposed into three stages.
basedon the learned semantic representations of the in-put nl utterance, the dominant seq2tree model(yin and neubig, 2018) ﬁrst outputs a sequenceof abstract syntax description language (asdl)grammar-based actions.
these actions can thenbe used to construct an ast following the pre-order traversal.
finally, the generated ast ismapped into surface code via a user-speciﬁed func-tion ast to mr(∗)..in the following subsections, we ﬁrst describethe basic asdl grammars of seq2tree models.
then, we introduce the details of tranx (yin andneubig, 2018), which is selected as our basic modeldue to its extensive applications and competitiveperformance (yin and neubig, 2019; shin et al.,2019; xu et al., 2020).
1.
1please note that our approach is also applicable to other.
seq2tree models..5077description:ifexception,renamedtoe,exceptioniscaughtcode:exceptexceptionase:astactionsequence:𝑎(cid:2869) : excepthandler -> excepthandler(expr?
type, expr?
name, stmt* body) 𝑎(cid:2870) : expr -> name(identifier id) 𝑎(cid:2871) : gentoken[exception]𝑎(cid:2872) : expr -> name(identifier id)𝑎(cid:2873) : gentoken[e]𝑎(cid:2874) : reduce𝑎(cid:2873) 𝑡(cid:2870)  𝑡(cid:2872)  𝑡(cid:2873)𝑎(cid:2870) 𝑎(cid:2872)𝑎(cid:2871)  𝑡(cid:2871) 𝑡(cid:2869)𝑎(cid:2869) 𝑎(cid:2874)  𝑡(cid:2874)applyconstrreducegentokentokenconstructor𝑎(cid:2875) 𝑡(cid:2869)  𝑡(cid:2870)  𝑡(cid:2872)𝑎(cid:2869) 𝑎(cid:2870)𝑎(cid:2873)𝑎(cid:2872)  𝑡(cid:2871)  𝑡(cid:2874)𝑎(cid:2874)  𝑡(cid:2871)𝑎(cid:2871)name(empty)typebodynameidexceptionnameeidexcepthandler2.1 asdl grammar.
formally, an asdl grammar contains two com-ponents: type and constructors.
the value of typecan be composite or primitive.
as shown in the‘actionsequence’ and ‘ast z’ parts of figure 1,a constructor speciﬁes a language component of aparticular type using its ﬁelds, e.g., excepthandler(expr?
type, expr?
name, stmt∗ body).
each ﬁeldspeciﬁes the type of its child node and containsa cardinality (single, optional ?
and sequential ∗)indicating the number of child nodes it holds.
forinstance, expr?
name denotes the ﬁeld name hasoptional child node.
the ﬁeld with composite type(e.g.
expr) can be instantiated by constructors ofcorresponding type, while the ﬁeld with primitivetype (e.g.
identiﬁer) directly stores token..there are three kinds of asdl grammar-basedactions that can be used to generate the action se-quence: 1) applyconstr[c].
using this action,a constructor c is applied to the composite ﬁeld ofthe parent node with the same type as c, expand-ing the ﬁeld to generate a branch ending with anast node.
here we denote the ﬁeld of the parentnode as frontier ﬁeld.
2) reduce.
it indicatesthe completion of generating branches for a ﬁeldwith optional or multiple cardinalities.
3) gen-token[v].
it expands a primitive frontier ﬁeld togenerate a token v..obviously, a constructor with multiple ﬁelds canproduce multiple ast branches2, of which gen-eration order has important effect on the modelperformance, as previously mentioned..2.2 seq2tree model.
similar to other nlg models, tranx is trainedto minimize the following objective function:.
lmle(x, a) = −.
log p(at|a<t, x),.
(1).
t(cid:88).
t=1.
where at is the t-th action, and p(at|a<t, x) is mod-eled by an attentional encoder-decoder network(yin and neubig, 2018)..for an nl description x=x1, x2, ..., xn , we usea bilstm encoder to learn its word-level hiddenstates.
likewise, the decoder is also an lstmnetwork.
formally, at the timestep t, the temporaryhidden state ht is updated as.
ht = flstm ([e(at−1) : st−1 : pt] , ht−1) ,.
(2).
2we also note that the ﬁeld with sequential cardinality willbe expanded to multiple branches.
however, in this work, wedo not consider this scenario, which is left as future work..where e(at−1) is the embedding of the previousaction at−1, st−1 is the previous decoder hiddenstate, and pt is a concatenated vector involvingthe embedding of the frontier ﬁeld and the decoderhidden state for the parent node.
furthermore, thedecoder hidden state st is deﬁned as.
st = tanh (w [ht : ct]) ,.
(3).
where ct is the context vector produced from theencoder hidden states and w is a parameter matrix.
here, we calculate the probability of action at.
according to the type of its frontier ﬁeld:.
• composite.
we adopt an applyconstr ac-tion to expand the ﬁeld or a reduce action tocomplete the ﬁeld.3 the probability of usingapplyconstr[c] is deﬁned as follows:.
p (at = applyconstr[c]|a<t, x)e(c)(cid:62)wst.
= softmax.
(cid:16).
(cid:17).
(4).
where e(c) denotes the embedding of the con-structor c..• primitive.
we apply a gentoken action toproduce a token v, which is either generatedfrom the vocabulary or copied from the inputnl description.
formally, the probability ofusing gentoken[v] can be decomposed intotwo parts:.
p (at = gentoken[v]|a<t, x)= p (gen |a<t, x) pgen (v|a<t, x) +.
(1 − p (gen |a<t, x))pcopy (v|a<t, x) ,.
p (gen |a<t, x).
wheresigmoid (wst)..is modeled.
(5)as.
please note that our proposed dynamic selectionof branch expansion orders does not affect otheraspects of the model..3 dynamic selection of branch.
expansion orders.
in this section, we extend the conventionalseq2tree model with a context-based branch se-lector, which dynamically determines optimal ex-pansion orders of branches for multi-branch astnodes.
in the following subsections, we ﬁrst il-lustrate the elaborately-designed branch selectormodule and then introduce how to train the ex-tended seq2tree model via reinforcement learningin detail..3reduce action can be considered as a special apply-.
constr action.
5078figure 2: the reinforced training of the extended tranx model with branch selector.
we ﬁrst fed the informationof ﬁeld and parent node into branch selector.
then, from the policy probability distribution of branch selector, wesample an order o and infer an order ˆo.
finally, we calculate the reward based on the model loss difference betweeno and ˆo, and use the gradients to update parameters of the extended model..3.1 branch selector.
as described in section 2.2, the action predictionat each timestep is mainly affected by its previousaction, frontier ﬁeld and the action of its parentnode.
thus, it is reasonable to construct the branchselector determining optimal expansion orders ofbranches according to these three kinds of informa-tion..speciﬁcally, given a multi-branch node nt attimestep t, where the asdl grammar of action atcontains m ﬁelds [f1, f2, ...fm], we feed the branchselector with three vectors: 1) e(fi): the embed-ding of ﬁeld fi, 2) e(at): the embedding of ac-tion at, 3) st: the decoder hidden state, and thencalculate the priority score of expanding ﬁelds asfollows:.
score(fi) = w2(tanh(w1[st : e(at) : e(fi)])),(6)where w1∈rd1×d2 and w2∈rd2×1 are learnableparameters.4.
afterwards, we normalize priority scores of ex-panding all ﬁelds into a probability distribution:.
pnt = softmax([score(f1) : · · · : score(fm)]).
(7)based on the above probability distribution, wecan sample m times to form a branch expansionorder o = [fo1, ..., fom], of which the policy proba-bility is computed as.
π(o) =.
pnt(foi|fo<i)..(8).
m(cid:89).
i=1.
4we omit the bias term for clarity..it is notable that during the sampling of foi, wemask previously sampled ﬁelds fo<i to ensure thatduplicate ﬁelds will not be sampled..3.2 training with reinforcement learning.
during the generation of asts, with the abovecontext-based branch selector, we deal with multi-branch nodes according to the dynamically deter-mined order instead of the standard left-to-rightorder.
however, the non-differentiability of multi-step expansion order selection and how to deter-mine the optimal expansion order lead to chal-lenges for the model training.
to deal with theseissues, we introduce reinforcement learning to trainthe extended seq2tree model in an end-to-end way.
concretely, we ﬁrst pre-train a conventionalseq2tree model.
then, we employ self-criticaltraining with a reward function that measures lossdifference between different branch expansion or-ders to train the extended seq2tree model..3.2.1 pre-training.
it is known that a well-initialized network isvery important for applying reinforcement learning(kang et al., 2020).
in this work, we require themodel to automatically quantify effects of differentbranch expansion orders on the quality of the gen-erated action sequences.
therefore, we expect thatthe model has the basic ability to generate actionsequences in random order at the beginning.
to dothis, instead of using the pre-order traversal basedaction sequences, we use the randomly-organizedaction sequences to pre-train the seq2tree model.
concretely, for each multi-branch node in anast, we sample a branch expansion order from a.
5079branch selectorrewardtranx 𝑡(cid:2869)?𝑎(cid:2869)?
?𝑓(cid:2869)𝑓(cid:2870)𝑓(cid:2871) 𝑡(cid:2869) 𝑡(cid:2870) 𝑡(cid:2869) 𝑡(cid:2873) 𝑡(cid:2871) 𝑡(cid:2870) 𝑡(cid:2874) 𝑡(cid:2872) 𝑡(cid:2869) 𝑡(cid:2871) 𝑡(cid:2870) 𝑡(cid:2872) 𝑡(cid:2869) 𝑡(cid:2870) 𝑡(cid:2873) 𝑡(cid:2872) 𝑡(cid:2871) 𝑡(cid:2874) 𝑡(cid:2869) 𝑡(cid:2870) 𝑡(cid:2872) 𝑡(cid:2871) 𝑡(cid:2869) 𝑡(cid:2870)update𝜋𝑜=[𝑓(cid:2870), 𝑓(cid:2871), 𝑓(cid:2869)]𝑜(cid:3548)=[𝑓(cid:2871), 𝑓(cid:2869), 𝑓(cid:2870)]uniform distribution, and then reorganize the cor-responding actions according to the sampled order.
we conduct the same operations to all multi-branchnodes of the ast, forming a new training instance.
finally, we use the regenerated training instancesto pre-train our model..in this way, the pre-trained seq2tree model ac-quires the preliminary capability to make predic-tions in any order..3.2.2 self-critical trainingwith the above initialized parameters, we thenperform self-critical training (rennie et al., 2017;kang et al., 2020) to update the seq2tree modelwith branch selector..speciﬁcally, we train the extended seq2treemodel by combining the mle objective and rlobjective together.
formally, given the training in-stance (x, a), we ﬁrst apply the sampling methoddescribed in section 3.1 to all multi-branch nodes,reorganizing the initial action sequence a to form anew action sequence ao, and then deﬁne the modeltraining objective as.
l = lmle(ao|x; θ) +.
lrl(o; θ),.
λ|nmb|.
(cid:88).
n∈nmb.
(9)where lmle(∗) denotes the conventional trainingobjective deﬁned in equation 1, lrl(∗) is the nega-tive expected reward of branch expansion order ofor the multi-branch node n, λ is a balancing hyper-parameter, nmb denotes the set of multi-branchnodes in the training instance, and θ denotes theparameter set of our enhanced model..more speciﬁcally, lrl(∗) is deﬁned as.
lrl(o; θ) = −eo∼π[r(o)]≈ −r(o), o ∼ π,.
(10).
where we approximate the expected reward withthe loss of an order o sampled from the policy π..inspired by successful applications of self-critical training in previous studies (rennie et al.,2017; kang et al., 2020), we propose the rewardr(∗) to accurately measure the effect of any orderon the model performance.
as shown in figure 2,we calculate the reward using two expansion ordersof branches: one is o sampled from the policy π,and the other is ˆo inferred from the policy π withthe maximal generation probability:.
please note that we extend the standard rewardfunction by setting a threshold η to clip the reward,which can prevent the network from being over-conﬁdent in current expansion order of branches.
finally, we apply the reinforce algorithm.
(williams, 1992) to compute the gradient:.
∇θlrl ≈ −r (o) ∇θ log pθ (o) ..(12).
4 experiments.
to investigate the effectiveness and generalizabilityof our model, we carry out experiments on severalcommonly-used datasets..4.1 datasets.
following previous studies (yin and neubig, 2018,2019; xu et al., 2020), we use the following fourdatasets:.
• django (oda et al., 2015).
this datasettotally contains 18,805 lines of python sourcecode, which are extracted from the djangoweb framework, and each line is paired withan nl description..• atis.
this dataset is a set of 5,410 inquiriesof ﬂight information, where the input of eachexample is an nl description and its corre-sponding output is a short piece of code inlambda calculus..• geo.
it is a collection of 880 u.s. geograph-ical questions, with meaning representationsdeﬁned in lambda logical forms like atis.
• conala (yin et al., 2018).
it totally con-sists of 2,879 examples of manually anno-tated nl questions and their python solu-tions on stack overflow.
comparedwith django, the examples of conalacover real-world nl queries issued by pro-grammers with diverse intents, and are signiﬁ-cantly more difﬁcult due to its broad coverageand high compositionality of target meaningrepresentations..4.2 baselines.
to facilitate the descriptions of experimental re-sults, we refer to the enhanced tranx model astranx-rl.
in addition to tranx, we compareour enhanced model with several competitive mod-els:.
r(o) = (lmle(ˆo) − lmle(o)) ∗ (max(η − p(o), 0))..(11).
• tranx (w/ pre-train).
it is an enhancedversion of tranx with pre-training.
we.
5080model.
coarse2fine (dong and lapata, 2018)†tranx (yin and neubig, 2019)†treegen (sun et al., 2020).
tranxtranx (w/ pre-train)tranx-r2ltranx-rand.
tranx-rl (w/o pre-train)tranx-rl.
djangoacc..atisacc..geoacc..conalableu / acc..–77.3 ±0.4–.
77.2 ±0.677.5 ±0.475.9 ±0.874.6 ±1.1.
76.3 ±0.777.9 ±0.5.
87.787.6 ±0.188.1 ±0.6.
87.6 ±0.487.8 ±0.787.5 ±0.986.4 ±1.4.
88.288.8 ±1.0–.
88.8 ±1.088.4±1.186.4 ±1.081.7 ±1.8.
–24.35 ±0.4 / 2.5 ±0.7–.
24.38 ±0.5 / 2.2 ±0.524.57 ±0.5 / 1.4 ±0.324.88 ±0.5 / 2.4 ±0.519.73 ±1.1 / 1.6 ±0.6.
87.2 ±0.889.1 ±0.5.
87.1 ±1.689.5 ±1.2.
23.38 ±0.8 / 2.1 ±0.225.47 ±0.7 / 2.6 ±0.4.
table 2: the performance of our model in comparison with various baselines.
we report the mean performance andstandard deviation over ﬁve random runs.
† indicates the scores are previously reported ones.
note that we onlyreport the result of treegen on atis, since it is the only dataset with released code for preprocessing..compare with it because our model involves apre-training stage..• coarse2fine (dong and lapata, 2018).
this model adopts a two-stage decoding strat-egy to produce the action sequence.
it ﬁrstgenerates a rough sketch of its meaning, andthen ﬁlls in missing detail..• treegen (sun et al., 2020)..it intro-duces the attention mechanism of transformer(vaswani et al., 2017), and a novel ast readerto incorporate grammar and ast structuresinto the network..• tranx-r2l.
it is a variant of the conven-tional tranx model, which deals with multi-branch ast nodes in a right-to-left manner.
• tranx-rand.
it is also a variant of the con-ventional tranx model dealing with multi-branch ast nodes in a random order..• tranx-rl (w/o pre-train).
in this vari-ant of tranx-rl, we train our model fromscratch.
by doing so, we can discuss the effectof pre-training on our model training..to ensure fair comparisons, we use the sameexperimental setup as tranx (yin and neubig,2018).
concretely, the sizes of action embedding,ﬁeld embedding and hidden states are set to 128,128 and 256, respectively.
for decoding, the beamsizes for geo, atis, django and conala are5, 5, 15 and 15, respectively.
we pre-train modelsin 10 epochs for all datasets.
we determine the λsas 1.0 according to the model performance on vali-dation sets.
as in previous studies (alvarez-melisand jaakkola, 2017; yin and neubig, 2018, 2019),we use the exact matching accuracy (acc) as the.
evaluation metric for all datasets.
for conala,we use the corpus-level bleu (yin et al., 2018) asa complementary metric..4.3 main results.
table 2 reports the main experimental results.
over-all, our enhanced model outperforms baselinesacross all datasets.
moreover, we can draw thefollowing conclusions:.
first, our.
reimplemented tranx modelachieves comparable performance to previouslyreported results (yin and neubig, 2019) (tranx).
therefore, we conﬁrm that our reimplementedtranx model are convincing..second, compared with tranx, tranx-r2land tranx-rand, our tranx-rl exhibits bet-ter performance.
this result demonstrates the ad-vantage of dynamically determining branch ex-pansion orders on dealing with multi-branch astnodes..third, the tranx model with pre-training doesnot gain a better performance.
in contrast, remov-ing the model pre-training leads to the performancedegradation of our tranx-rl model.
this re-sult is consistent with the conclusion of previousstudies (wang et al., 2018; kang et al., 2020) thatthe pre-training is very important for the applyingreinforcement learning..4.4 effects of the number of multi-branch.
nodes.
as implemented in related studies on other nlgtasks, such as machine translation (bahdanau et al.,2015), we individually split two relatively large.
5081model.
djangoacc..atisacc..geoacc..conalaacc..tranxtranx-r2ltranx-rl.
77.26±0.876.88±1.078.98±0.9.
94.02±0.893.80±0.394.87±0.5.
89.75±0.889.28±1.190.64±0.9.
25.19±0.624.74±0.726.90±0.6.
table 3: performance of our model in predicting actions for child nodes of multi-branch nodes..tranx tranx-r2l tranx-rl.
012345≥6.
012345≥6.
88.3710010078.9496.9395.6578.75.
98.3090.0085.5066.6654.1628.8812.35.
93.0210010081.5796.9395.2375.00.
91.5290.0084.7063.6048.3326.6612.35.
90.1110010089.4796.9395.6580.63.
97.6790.0086.1767.8157.5028.8812.35.table 4: accuracy on different data groups of atisaccording to the number of multi-branch nodes..tranx tranx-r2l tranx-rl.
table 5: accuracy on different data groups of djangoaccording to the number of multi-branch nodes..datasets (django and atis) into different groupsaccording to the number of multi-branch astnodes, and report the performance of various mod-els on these groups of datasets..tables 4 and 5 show the experimental results.
on most groups, tranx-rl achieves better orequal performance than other models.
therefore,we conﬁrm that our model is general to datasetswith different numbers of multi-branch nodes..4.5 accuracy of action predictions for the.
child nodes.
given a multi-branch node, its child nodes have animportant inﬂuence in the subtree.
therefore, wefocus on the accuracy of action predictions for thechild nodes..for fair comparison, we predict actions with pre-.
vious ground-truth history actions as inputs.
table3 reports the experimental results.
we observe thattranx-rl still achieves higher prediction accu-racy than other baselines on most groups, whichproves the effectiveness of our model again..4.6 case study.
figure 3 shows two examples from django.
inthe ﬁrst example, tranx ﬁrst generates the left-most child node at the timestep t2, incorrectly pre-dicting gentoken[‘gzip’] as reduce action.
bycontrast, tranx-rl puts this child node in thelast position and successfully predict its action,since our model beneﬁts from the previously gen-erated token ‘gzipfile’ of the sibling node, whichfrequently occurs with ‘gzip’..in the second example, tranx incorrectly pre-dicts the second child node at the t10-th timestep,while tranx-rl ﬁrstly predicts it at the timestept6.
we think this error results from the sequentiallygenerated nodes and the errors in early timestepswould accumulatively harm the predictions of latersibling nodes.
by comparison, our model can ﬂexi-bly generate subtrees with shorter lengths, alleviat-ing error accumulation..5 related work.
with the prosperity of deep learning, researchersintroduce neural networks into code generation.
in this aspect, ling et al.
(2016) ﬁrst explore aseq2seq model for code generation.
then, dueto the advantage of tree structure, many attemptsresort to seq2tree models, which represent codesas trees of meaning representations (dong and la-pata, 2016; alvarez-melis and jaakkola, 2017; ra-binovich et al., 2017; yin and neubig, 2017, 2018;sun et al., 2019, 2020)..typically, yin and neubig (2018) proposetranx, which introduces asts as intermediaterepresentations of codes and has become the mostinﬂuential seq2tree model.
then, sun et al.
(2019,2020) respectively explore cnn and transformer.
5082ever, is not suitable to handle all multi-branch astnodes.
different from the above studies that dealwith multi-branch nodes in left-to-right order, ourmodel determines the optimal expansion orders ofbranches for multi-branch nodes..some researchers have also noticed that the se-lection of decoding order has an important impacton the performance of neural code generation mod-els.
for example, alvarez-melis and jaakkola(2017) introduce a doubly rnn model that com-bines width and depth recurrences to traverse eachnode.
dong and lapata (2018) ﬁrstly generatea rough code sketch, and then ﬁll in missing de-tails by considering the input nl description andthe sketch.
gu et al.
(2019a) present an insertion-based seq2seq model that can ﬂexibly generate asequence in an arbitrary order.
in general, theseresearches still deal with multi-branch ast nodesin a left-to-right manner.
thus, these models aretheoretically compatible with our proposed branchselector..finally, it should be noted that have been manynlp studies on exploring other decoding methodsto improve other nlg tasks (zhang et al., 2018; suet al., 2019; zhang et al., 2019; welleck et al., 2019;stern et al., 2019; gu et al., 2019a,b).
however,to the best of our knowledge, our work is the ﬁrstattempt to explore dynamic selection of branchexpansion orders for tree-structured decoding..6 conclusion and future work.
in this work, we ﬁrst point out that the gener-ation of domainant seq2tree models based onpre-order traversal is not optimal for handling allmulti-branch nodes.
then we propose an extendedseq2tree model equipped with a context-basedbranch selector, which is capable of dynamicallydetermining optimal branch expansion orders formulti-branch nodes.
particularly, we adopt rein-forcement learning to train the whole model withan elaborate reward that measures the model lossdifference between different branch expansion or-ders.
extensive experiment results and in-depthanalyses demonstrate the effectiveness and gener-ality of our proposed model on several commonly-used datasets..in the future, we will study how to extend ourbranch selector to deal with indeﬁnite branchescaused by sequential ﬁeld..(a) the ﬁrst example..(b) the second example..figure 3: two django examples produced by differ-ent models..architectures to model code generation.
unlikethese work, shin et al.
(2019) present a seq2treemodel to generate program fragments or tokensinterchangeably at each generation step.
fromanother perspective, xu et al.
(2020) exploit ex-ternal knowledge to enhance neural code genera-tion model.
generally, all these seq2tree modelsgenerate asts in pre-order traversal, which, how-.
5083acknowledgments.
the project was supported by national key re-search and development program of china (grantno.
2020aaa0108004), national natural sciencefoundation of china (grant no.
61672440), natu-ral science foundation of fujian province of china(grant no.
2020j06001), youth innovation fundof xiamen (grant no.
3502z20206059), and thefundamental research funds for the central uni-versities (grant no.
zk20720200077).
we alsothank the reviewers for their insightful comments..references.
david alvarez-melis and tommi s. jaakkola.
2017.tree-structured decoding with doubly-recurrent neu-ral networks.
in proceedings of iclr..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in proceedings oficlr..li dong and mirella lapata.
2016. language to logicalform with neural attention.
in proceedings of acl,pages 33–43..li dong and mirella lapata.
2018. coarse-to-ﬁne de-coding for neural semantic parsing.
in proceedingsof acl, pages 6559–6569..jiatao gu, qi liu, and kyunghyun cho.
2019a.
insertion-based decoding with automatically inferredgeneration order.
trans.
assoc.
comput.
linguistics,pages 661–676..jiatao gu, changhan wang, and junbo zhao.
2019b.
levenshtein transformer.
in proceedings of nips,pages 11179–11189..shirley anugrah hayati, raphael olivier, pravalika av-varu, pengcheng yin, anthony tomasic, and grahamneubig.
2018. retrieval-based neural code genera-tion.
in proceedings of emnlp, pages 925–930..xiaomian kang, yang zhao, jiajun zhang, andchengqing zong.
2020. dynamic context selectionfor document-level neural machine translation viareinforcement learning.
in proceedings of emnlp,pages 2242–2254..wang ling, phil blunsom, edward grefenstette,karl moritz hermann, and andrew senior.
2016.latent predictor networks for code generation.
inproceedings of acl, pages 599–609..yusuke oda, hiroyuki fudaba, graham neubig,hideaki hata, sakriani sakti, tomoki toda, andsatoshi nakamura.
2015. learning to generatepseudo-code from source code using statistical ma-chine translation.
in proceedings of ase, pages 574–584..maxim rabinovich, mitchell stern, and dan klein.
2017. abstract syntax networks for code genera-tion and semantic parsing.
in proceedings of acl,pages 1139–1149..steven j. rennie, etienne marcheret, youssef mroueh,jerret ross, and vaibhava goel.
2017. self-criticalsequence training for image captioning.
in proceed-ings of cvpr, pages 1179–1195..eui chul shin, miltiadis allamanis, marcbrockschmidt, and alex polozov.
2019.pro-gram synthesis and semantic parsing with learnedin proceedings of nips, pagescode idioms.
10825–10835..mitchell stern, william chan, jamie kiros, and jakobuszkoreit.
2019. insertion transformer: flexible se-quence generation via insertion operations.
in pro-ceedings of icml, pages 5976–5985..jinsong su, xiangwen zhang, qian lin, yue qin, jun-feng yao, and yang liu.
2019. exploiting reversetarget-side contexts for neural machine translationvia asynchronous bidirectional decoding.
artiﬁcialintelligence, 277..zeyu sun, qihao zhu, lili mou, yingfei xiong, ge li,and lu zhang.
2019. a grammar-based structuralcnn decoder for code generation.
in proceedings ofaaai, pages 7055–7062..zeyu sun, qihao zhu, yingfei xiong, yican sun, lilimou, and lu zhang.
2020. treegen: a tree-basedtransformer architecture for code generation.
in pro-ceedings of aaai, pages 8984–8991..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of nips, pages 5998–6008..xin wang, fisher yu, zi-yi dou, trevor darrell, andjoseph e. gonzalez.
2018. skipnet: learning dy-namic routing in convolutional networks.
in proceed-ings of eccv, pages 420–436..bolin wei, ge li, xin xia, zhiyi fu, and zhi jin.
2019.code generation as a dual task of code summarization.
in proceedings of nips, pages 6563–6573..sean welleck, kiant´e brantley, hal daum´e iii, andkyunghyun cho.
2019. non-monotonic sequentialtext generation.
in proceedings of icml, pages 6716–6726..ronald j. williams.
1992. simple statistical gradient-following algorithms for connectionist reinforcementlearning.
machine learning, 8:229–256..binbin xie, xiang li, yubin ge, jianwei cui, junfengyao, bin wang, and jinsong su.
2021. improvingtree-structured decoder training for code generationvia mutual learning.
in proceedings of aaai..5084frank f. xu, zhengbao jiang, pengcheng yin, bogdanvasilescu, and graham neubig.
2020. incorporatingexternal knowledge through pre-training for naturallanguage to code generation.
in proceedings of acl,pages 6045–6052..pengcheng yin, bowen deng, edgar chen, bogdanvasilescu, and graham neubig.
2018. learning tomine aligned code and natural language pairs fromstack overﬂow.
in proceedings of msr, pages 476–486..pengcheng yin and graham neubig.
2017. a syntacticneural model for general-purpose code generation.
in proceedings of acl, pages 440–450..pengcheng yin and graham neubig.
2018. tranx: atransition-based neural abstract syntax parser for se-mantic parsing and code generation.
in proceedingsof emnlp, pages 7–12..pengcheng yin and graham neubig.
2019. rerankingfor neural semantic parsing.
in proceedings of acl,pages 4553–4559..biao zhang, deyi xiong, jinsong su, and jiebo luo.
2019. future-aware knowledge distillation for neuralmachine translation.
ieee acm trans.
audio speechlang.
process., pages 2278–2287..xiangwen zhang, jinsong su, yue qin, yang liu, ron-grong ji, and hongji wang.
2018. asynchronousbidirectional decoding for neural machine translation.
in proceedings of aaai, pages 5698–5705..5085