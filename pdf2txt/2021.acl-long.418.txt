enabling lightweight fine-tuning for pre-trained language modelcompression based on matrix product operatorspeiyu liu1,4∗ , ze-feng gao2,1∗ , wayne xin zhao1,4,5† ,z.y.
xie2, zhong-yi lu2† and ji-rong wen1,3,41gaoling school of artiﬁcial intelligence, renmin university of china2department of physics, renmin university of china3 school of information, renmin university of china4beijing key laboratory of big data management and analysis methods5beijing academy of artiﬁcial intelligence, beijing, 100084, china{liupeiyustu,zfgao,qingtaoxie,zlu,jrwen}@ruc.edu.cn, batmanﬂy@gmail.com.
abstract.
this paper presents a novel pre-trained lan-guage models (plm) compression approachbased on the matrix product operator (shortas mpo) from quantum many-body physics.
it can decompose an original matrix into cen-tral tensors (containing the core information)and auxiliary tensors (with only a small pro-portion of parameters).
with the decomposedmpo structure, we propose a novel ﬁne-tuningstrategy by only updating the parameters fromthe auxiliary tensors, and design an optimiza-tion algorithm for mpo-based approximationover stacked network architectures.
our ap-proach can be applied to the original or thecompressed plms in a general way, whichderives a lighter network and signiﬁcantly re-duces the parameters to be ﬁne-tuned.
exten-sive experiments have demonstrated the effec-tiveness of the proposed approach in modelcompression, especially the reduction in ﬁne-tuning parameters (91% reduction on average).
the code to reproduce the results of this pa-per can be found at https://github.com/rucaibox/mpop..1.introduction.
recently, pre-trained language models (plms) (de-vlin et al., 2019; peters et al., 2018; radford et al.,2018) have made signiﬁcant progress in variousnatural language processing tasks.
instead of train-ing a model from scratch, one can ﬁne-tune a plmto solve some speciﬁc task through the paradigmof “pre-training and ﬁne-tuning”..typically, plms are constructed with stackedtransformer layers (vaswani et al., 2017), involv-ing a huge number of parameters to be learned.
though effective, the large model size makes it im-practical for resource-limited devices.
therefore,there is an increasing number of studies focused.
∗authors contributed equally.
† corresponding author..on the parameter reduction or memory reductionof plms (noach and goldberg, 2020), includingparameter sharing (lan et al., 2020), knowledgedistillation (sanh et al., 2019), low-rank approxima-tion (ma et al., 2019) and data quantization (hubaraet al., 2017).
however, these studies mainly applyparameter reduction techniques to plm compres-sion, which may not be intrinsically appropriate forthe learning paradigm and architecture of plms.
the compressed parameters are highly coupled sothat it is difﬁcult to directly manipulate differentparts with speciﬁc strategies.
for example, mostplm compression methods need to ﬁne-tune thewhole network architecture, although only a smallproportion of parameters will signiﬁcantly changeduring ﬁne-tuning (liu et al., 2020)..in this paper, we introduce a novel matrix prod-uct operator (mpo) technique from quantum many-body physics for compressing plms (gao et al.,2020).
the mpo is an algorithm that factorizesa matrix into a sequential product of local tensors(i.e., a multi way array).
here, we call the tensorright in the middle as central tensor and the restas auxiliary tensors.
an important merit of thempo decomposition is structural in terms of infor-mation distribution: the central tensor with mostof the parameters encode the core information ofthe original matrix, while the auxiliary tensors withonly a small proportion of parameters play the roleof complementing the central tensor.
such a prop-erty motivates us to investigate whether such anmpo can be applied to derive a better plm com-pression approach: can we compress the centraltensor for parameter reduction and update the aux-iliary tensors for lightweight ﬁne-tuning?
if thiscould be achieved, we can derive a lighter networkmeanwhile reduce the parameters to be ﬁne-tuned.
to this end, we propose an mpo-based com-pression approach for plms, called mpop.
it isdeveloped based on the mpo decomposition tech-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5388–5398august1–6,2021.©2021associationforcomputationallinguistics5388nique (gao et al., 2020; pirvu et al., 2010).
wehave made two critical technical contributions forcompressing plms with mpo.
first, we introducea new ﬁne-tuning strategy that only focuses on theparameters of auxiliary tensors, so the number ofﬁne-tuning parameters can be largely reduced.
wepresent both theoretical analysis and experimentalveriﬁcation for the effectiveness of the proposedﬁne-tuning strategy.
second, we propose a newoptimization algorithm, called dimension squeez-ing, tailored for stacked neural layers.
since main-stream plms usually consist of multiple trans-former layers, this will produce accumulated re-construction error by directly applying low-rankapproximation with mpo at each layer.
the di-mension squeezing algorithm is able to graduallyperform the dimension truncation in a more sta-ble way so that it can dramatically alleviate theaccumulation error in the stacked architecture..to our knowledge, it is the ﬁrst time that mpois applied to the plm compression, which is wellsuited for both the learning paradigm and the archi-tecture of plms.
we construct experiments to eval-uate the effectiveness of the proposed compressionapproach for albert, bert, distillbert andmobilebert, respectively, on glue benchmark.
extensive experiments have demonstrated the ef-fectiveness of the proposed approach in model com-pression, especially dramatically reducing the ﬁne-tuning parameters (91% reduction on average)..2 related work.
we review the related works in three aspects..pre-trained language model compression.
since the advent of large-scale plms, several vari-ants were proposed to alleviate its memory con-sumption.
for example, distilbert (sanh et al.,2019) and mobilebert (sun et al., 2020c) lever-aged knowledge distillation to reduce the bertnetwork size.
squeezebert (iandola et al., 2020)and q8bert (zafrir et al., 2019) adopted specialtechniques to substitute the operations or quantizeboth weights and activations.
albert (lan et al.,2020) introduced cross-layer parameter sharing andlow-rank approximation to reduce the number ofparameters.
more studies (jiao et al., 2020; houet al., 2020; liu et al., 2020; wang et al., 2020;khetan and karnin, 2020; xin et al., 2020; pappaset al., 2020; sun et al., 2020a) can be found in thecomprehensive survey (ganesh et al., 2020)..tensor-based network compression.
tensor-based methods have been successfully applied toneural network compression.
for example, mpohas been utilized to compress linear layers of deepneural network (gao et al., 2020).
sun et al.
(2020b) used mpo to compress the lstm modelon acoustic data.
novikov et al.
(2015) coinedthe idea of reshaping weights of fully-connectedlayers into high-dimensional tensors and represent-ing them in tensor train (tt) (oseledets, 2011)format, which was extended to other network ar-chitectures (garipov et al., 2016; yu et al., 2017;tjandra et al., 2017; khrulkov et al., 2019).
maet al.
(2019) adopted block-term tensor decomposi-tion to compress transformer layers in plms..lightweight fine-tuning.
in the past, lightweightﬁne-tuning was performed without considering pa-rameter compression.
as a typical approach, train-able modules are inserted into plms.
for example,a “side” network is fused with plm via summationin (zhang et al., 2020), and adapter-tuning insertstask-speciﬁc layers (adapters) between each layerof plms (houlsby et al., 2019; lin et al., 2020;rebufﬁ et al., 2017).
on the contrary, several stud-ies consider removing parameters from plms.
forexample, several model weights are ablated awayby training a binary parameter mask (zhao et al.,2020; radiya-dixit and wang, 2020)..our work is highly built on these studies, whilewe have a new perspective by designing the plmcompression algorithm, which enables lightweightﬁne-tuning.
it is the ﬁrst time that mpo is appliedto plm compression, and we make two major tech-nical contributions for achieving lightweight ﬁne-tuning and stable optimization..3 preliminary.
in this paper, scalars are denoted by lowercase let-ters (e.g., a), vectors are denoted by boldface low-ercase letters (e.g., v), matrices are denoted byboldface capital letters (e.g., m), and high-order(order three or higher) tensors are denoted by bold-face euler script letters (e.g., t ).
an n-order tensorti1,i2,...in can be considered as a multidimensionalarray with n indices {i1, i2, ..., in}..matrix product operator.
originating fromquantum many-body physics, matrix product op-erator (mpo) is a standard algorithm to factorizea matrix into a sequential product of multiple lo-cal tensors (gao et al., 2020; pirvu et al., 2010)..5389figure 1: mpo decomposition for matrix mi×j with ﬁve local tensors, where (cid:81)nak = ik × jk (n = 5 here).
auxiliary tensors ({ai}4respectively.
dash line linking adjacent tensors denotes virtual bonds..k=1 jk = j, andi=1) and central tensor (c) are marked in blue and orange,.
k=1 ik = i, (cid:81)n.formally, given a matrix m ∈ ri×j , its mpo de-composition into a product of n local tensors canbe represented as:.
mpo (m) =.
t(k)[dk−1, ik, jk, dk],.
(1).
n(cid:89).
k=1.
where the t(k)[dk−1, ik, jk, dk] is a 4-order tensorwith size dk−1 × ik × jk × dk in which (cid:81)nk=1 ik =i, (cid:81)nk=1 jk = j and d0 = dn = 1. we use the con-cept of bond to connect two adjacent tensors (pirvuet al., 2010).
the bond dimension dk is deﬁned by:.
mpo-based low-rank approximation.
withthe standard mpo decomposition in eq.
(1), we canexactly reconstruct the original matrix m throughthe product of the derived local tensors.
follow-ing (gao et al., 2020), we can truncate the k-thbond dimension dk (see eq.
(1)) of local tensorsto d(cid:48)k for low-rank approximation: dk > d(cid:48)k. wecan set different values for {dk}nk=1 to control theexpressive capacity of mpo-based reconstruction.
the truncation error induced by the k-th bond di-mension dk is denoted by (cid:15)k (called local trunca-tion error) which can be efﬁciently computed as:.
(cid:18) k(cid:89).
n(cid:89).
(cid:19).
dk = min.
im × jm,.
im × jm.
.
(2).
m=1.
m=k+1.
from eq.
(2), we can see that dk is going to be largein the middle and small on both sides.
we presenta detailed algorithm for mpo decomposition inalgorithm 1. in this case, we refer to the tensorright in the middle as central tensor, and the rest asauxiliary tensor.
figure 1 presents the illustrationof mpo decomposition, and we use n = 5 in thispaper..k=1.
algorithm 1 mpo decomposition for a matrix.
input: matrix m, the number of local tensors noutput : mpo tensor list {t(k)}n1: for k = 1 → n − 1 do2: m[i, j] −→ m[dk−1 × ik × jk, −1]3:4:5:6: m := λv(cid:62)7: end for8: t (n) := m9: normalization10: return {t(k)}n.uλv(cid:62) = svd (m)u[dk−1 × ik × jk, dk] −→ u[dk−1, ik, jk, dk]t (k) := u.k=1.
dk(cid:88).
(cid:15)k =.
λi,.
i=dk−d(cid:48)k.(3).
where {λi}dkm[i1j1...ikjk, ik+1jk+1...injn]..i=1 are the singular values of.
then the total truncation error satisﬁes:.
(cid:107)m − mpo(m)(cid:107)f ≤.
(4).
(cid:118)(cid:117)(cid:117)(cid:116).
n−1(cid:88).
k=1.
(cid:15)2k..the proof can be found in the supplementary ma-terials 1. eq.
(1) indicates that the reconstructionerror is bounded by the sum of the squared localtruncation errors, which is easy to estimate in prac-tice..suppose that we have truncated the dimensionsof local tensors from {dk}nk=1, the com-pression ratio introduced by quantum many-bodyphysics (gao et al., 2020) can be computed as fol-lows:.
k=1 to {d(cid:48).
k}n.ρ =.
(cid:80)n.k=1 d(cid:48)(cid:81)n.k−1ikjkd(cid:48)k...k=1 ikjk.
(5).
1https://github.com/rucaibox/mpop.
5390mpo𝑑!𝑑"#𝑑$𝑑$𝑎$𝑎#𝑑#𝑑#𝑑!𝑎!𝑎%𝑎&𝑑!𝑑%𝑑%truncate dimension𝑑#>𝑑"#𝑑!𝑑%𝑑%𝑑$𝑑$𝑑"#𝑀$×&𝑎$𝑎#𝑎!𝑎%𝑎&𝐴$𝐴#𝐴!𝐴%𝐴%𝐴!𝐴#"𝐴$𝐶"𝐶layers.
(0,1e-4].
(1e-4,1e-3].
(1e-3,∞).
word embeddingfeed-forwardself-attention.
0.660.090.09.
0.260.640.64.
0.090.270.27.table 1: distribution of parameter variations for bertwhen ﬁne-tuned on sst-2 task..the smaller the compression ratio is, the fewer pa-rameters are kept in the mpo representation.
onthe contrary, the larger the compression ratio ρ is,and the more parameters there are, and the smallerthe reconstruction error is.
when ρ > 1, it indi-cates the decomposed tensors have more parame-ters than the original matrix..4 approach.
so far, most of pre-trained language models (plm)are developed based on stacked transformer lay-ers (vaswani et al., 2017).
based on such an archi-tecture, it has become a paradigm to ﬁrst pre-trainplms and then ﬁne-tunes them on task-speciﬁcdata.
the involved parameters of plms can begenerally represented in the matrix format.
hence,it would be natural to apply mpo-based approxi-mation for compressing the parameter matrices inplms by truncating tensor dimensions..in particular, we propose two major improve-ments for mpo-based plm compression, whichcan largely reduce the ﬁne-tuning parameters andeffectively improve the optimization of stacked ar-chitecture, respectively..4.1 lightweight fine-tuning with auxiliary.
tensors.
due to the high coupling of parameters, previousplm compression methods usually need to ﬁne-tune all the parameters.
as a comparison, the mpoapproach decomposes a matrix into a list of localtensors, which makes it potentially possible to con-sider ﬁne-tuning different parts with speciﬁc strate-gies.
next, we study how to perform lightweightﬁne-tuning based on mpo properties..parameter variation from pre-training.
to ap-ply our solution to lightweight ﬁne-tuning, we ﬁrstconduct an empirical experiment to check the varia-tion degree of the parameters before and after ﬁne-tuning.
here, we adopt the standard pre-trainedbert (devlin et al., 2019) and then ﬁne-tune iton the sst-2 task (socher et al., 2013).
we ﬁrstcompute the absolute difference of the variation for.
each parameter value and then compute the ratioof parameters with different variation levels.
thestatistical results are reported in table 1. as wecan see, most of parameters vary little, especiallyfor the word embedding layer.
this ﬁnding hasalso been reported in a previous studies (khetanand karnin, 2020).
as discussed in section 3, aftermpo decomposition, the central tensor containsthe majority of the parameters, while the auxiliarytensors only contain a small proportion of the pa-rameters.
such merit inspires us to consider onlyﬁne-tuning the parameters in the auxiliary tensorswhile keeping the central tensor ﬁxed during ﬁne-tuning.
if this approach was feasible, this willlargely reduce the parameters to be ﬁne-tuned..theoretical analysis.
here we introduce entan-glement entropy from quantum mechanics (cal-abrese and cardy, 2004) as the metric to measurethe information contained in mpo bonds, whichis similar to the entropy in information theory butreplaces probabilities by normalized singular val-ues produced by svd.
this will be more suitablefor measuring the information of a matrix as sin-gular values often correspond to the important in-formation implicitly encoded in the matrix, and theimportance is positively correlated with the magni-tude of the singular values.
following (calabreseand cardy, 2004), the entanglement entropy skcorresponding to the k-th bond can be calculatedby:.
sk = −.
vj ln vj,.
k = 1, 2, ..., n − 1,.
(6).
dk(cid:88).
j=1.
where {vj}dkj=1 denote the normalized svd eigen-values of m[i1j1...ikjk, ik+1jk+1...injn].
the en-tanglement entropy sk is an increasing functionof dimension dk as described in (gao et al., 2020).
based on eq.
(2), the central tensor has the largestbond dimension, corresponding to the largest en-tanglement entropy.
this indicates that most ofthe information in an original matrix will be con-centrated in the central tensor.
furthermore, thelarger a dimension is, the larger the updating effectwill be.
according to (pirvu et al., 2010), it is alsoguaranteed in principle that any change on sometensor will be transmitted to the whole local tensorset.
thus, it would have almost the same effectafter convergence by optimizing the central tensoror the auxiliary tensors for plms..based on the above analysis, we speculate.
5391that the affected information during ﬁne-tuning ismainly encoded on the auxiliary tensors so thatthe overall variations are small.
therefore, forlightweight ﬁne-tuning, we ﬁrst perform the mpodecomposition for a parameter matrix, and thenonly update its auxiliary tensors according to thedownstream task with the central tensor ﬁxed.
ex-perimental results in section 5.2 will demonstratethat such an approach is indeed effective..4.2 dimension squeezing for stackedarchitecture optimization.
most of plms are stacked with multiple trans-former layers.
hence, a major problem with di-rectly applying mpo to compressing plms is thatthe reconstruction error tends to be accumulatedand ampliﬁed exponentially by the number of lay-ers.
it is thus urgent to develop a more stable opti-mization algorithm tailored to the stacked architec-ture..fast reconstruction error estimation.
withoutloss of generality, we can consider a simple casein which each layer contains exactly one param-eter matrix to be compressed.
assume that thereare l layers, so we have l parameter matrices intotal, denoted by {m(l)}ll=1.
let c(l) denote thecorresponding central tensor with a speciﬁc dimen-sion d(l) after decomposing m(l) with mpo.
ouridea is to select a central tensor to reduce its di-mension by one at each time, given the selectioncriterion that this truncation will lead to the least re-construction error.
however, it is time-consumingto evaluate the reconstruction error of the originalmatrix.
according to eq.
(3), we can utilize theerror boundk for a fast estimation of theyielded reconstruction error.
in this case, only one(cid:15)k changes, and it can be efﬁciently computed viathe pre-computed eigenvalues..(cid:113)(cid:80)n−1.
k=1 (cid:15)2.fast performance gap computation.
at eachtime, we compute the performance gap before andafter the dimension reduction (d(l) → d(l) −1) withthe stop criterion.
to obtain the performance ˜p af-ter dimension reduction, we need to ﬁne-tune thetruncated model on the downstream task.
we canalso utilize the lightweight ﬁne-tuning strategy insection 4.1 to obtain ˜p by only tuning the auxil-iary tensors.
if the performance gap (cid:107) p − ˜p (cid:107) issmaller than a threshold ∆ or the iteration numberexceeds the predeﬁned limit, the algorithm will end.
such an optimization algorithm is more stable to.
optimize stacked architectures since it graduallyreduces the dimension considering the reconstruc-tion error and the performance gap.
actually, itis similar to the learning of variable matrix prod-uct states (iblisdir et al., 2007) in physics, whichoptimizes the tensors one by one according to thesequence.
as a comparison, our algorithm dynam-ically selects the matrix to truncate and is moresuitable to plms..algorithm 2 presents a complete procedure forour algorithm.
in practice, there are usually mul-tiple parameter matrices to be optimized at eachlayer.
this can be processed in a similar way: weselect some matrices from one layer to optimizeamong all the considered matrices..algorithm 2 training with dimension squeezing..input: : l layers with corresponding central tensor c(l) anddimension d(l), threshold ∆ and iteration step iter.
find the layer (l∗) with the least reconstruction errorcompress mpo tensor by truncating d(l∗)fine-tuning auxiliary tensors with {c(l)}levaluate loss ˜p = model(inputs)if (cid:107) p − ˜p (cid:107)> ∆ then.
1: evaluate loss p = model(inputs)2: perform mpo decomposition for each layer3: for step = 1 → iter do4:5:6:7:8:9:10:11: end for12: return compressed model.
l=1 ﬁxed.
end if.
break.
4.3 overall compression procedure.
generally speaking, our approach can compressany plms with stacked architectures consisting ofparameter matrices, even the compressed plms.
in other words, it can work with the existing plmcompression methods to further achieve a bettercompression performance.
here, we select al-bert (lan et al., 2020) as a representative com-pressed plm and apply our algorithm to albert.
the procedure can be simply summarized as fol-lows.
first, we obtain the learned albert model(complete) and perform the mpo-decomposition tothe three major parameter matrices, namely wordembedding matrix, self-attention matrix and feed-forward matrix2.
each matrix will be decomposedinto a central tensor and auxiliary tensors.
next,we perform the lightweight ﬁne-tuning to updateauxiliary tensors until convergence on downstreamtasks.
then, we apply the dimension squeezing.
2it introduces a parameter sharing mechanism to keep only.
one copy for both self-attention and feed-forward matrices..5392category method.
inference time.
tucker.
mpo.
tucker(d=1)(cp) o(nmd2)tucker(d>1)mpo(n=2)(svd) o(2md3)o(nmd3)mpo(n>2).
o(nmd + dn).
table 2: inference time complexities of different low-rank approximation methods.
here, n denotes the num-ber of the tensors, m denotes max({ik}nk=1) means thelargest ik in input list, and d denotes max({d(cid:48)k=0)means the largest dimension d(cid:48)k in the truncated dimen-sion list..k}n.optimization algorithm to the three central tensors,i.e., we select one matrix for truncation each time.
after each truncation, we ﬁne-tune the compressedmodel and further stabilize its performance.
thisprocess will repeat until the performance gap or theiteration number exceeds the pre-deﬁned threshold.
in this way, we expect that albert can befurther compressed.
in particular, it can be ﬁne-tuned in a more efﬁcient way, with only a smallamount of parameters to be updated.
section 5.2will demonstrate this..4.4 discussion.
in mathematics, mpo-based approximation can beconsidered as a special low-rank approximationmethod.
now, we compare it with other low-rankapproximation methods, including svd (henryand hofrichter, 1992), cpd (hitchcock, 1927) andtucker decomposition (tucker, 1966)..we present the categorization of these methodsin table 2. for plm compression, low-rank decom-position is only performed once, while it repeatedlyperforms forward propagation computation.
hence,we compare their inference time complexities.
in-deed, all the methods can be tensor-based decom-position (i.e., a list of tensors for factorization) ormatrix decomposition, and we characterize theirtime complexities with common parameters.
in-deed, mpo and tucker represent two categories oflow-rank approximation methods.
generally, thealgorithm capacity is larger with the increase of n(more tensors).
when n > 3, mpo has smallertime complexity than tucker decomposition.
it canbe seen that svd can be considered as a specialcase of mpo when tensor dimension n = 2 andcpd is a special case of tucker when the coretensor is the super-diagonal matrix..the original matrix size.
instead, it is easy to padadditional zero entries to enlarge matrix rows orcolumns, so that we can obtain different mpo de-composition results.
it has demonstrated that dif-ferent decomposition plans always lead to almostthe same results (gao et al., 2020).
in our exper-iments, we adopt an odd number of local tensorsfor mpo decomposition, i.e., ﬁve local tensors (seesupplementary materials).
note that mpo decom-position can work with other compression methods:it can further reduce the parameters from the matri-ces compressed by other methods, and meanwhilelargely reduce the parameters to be ﬁne-tuned..5 experiments.
in this section, we ﬁrst set up the experiments, andthen report the results and analysis..5.1 experimental setup.
datasets.
we evaluate the effectiveness of com-pressing and ﬁne-tuning plms of our approachmpop on the general language understandingevaluation (glue) benchmark (wang et al., 2019).
glue is a collection of 9 datasets for evaluatingnatural language understanding systems.
follow-ing (sanh et al., 2019), we report macro-score (aver-age of individual scores, which is slightly differentfrom ofﬁcial glue score, since spearman correla-tions are reported for sts-b and accuracy scoresare reported for the other tasks) on the developmentsets for each task by ﬁne-tuning mpop..baselines.
our baseline methods include:.
• bert (devlin et al., 2019): the 12-layerbert-base model was pre-trained on wikipediacorpus released by google..• albert (lan et al., 2020): it yields a highlycompressed bert variant with only 11.6m param-eters, while maintains competitive performance,which serves as the major baseline..• distilbert (sanh et al., 2019): it is trained.
via knowledge distillation with 6 layers..• mobilebert (sun et al., 2020c):.
isequipped with bottleneck structures and a carefullydesigned balance between self-attentions and feed-forward networks..it.
all these models are released by huggingface 3.we select these baselines because they are widelyadopted and have a diverse coverage of compres-sion techniques.
note that we do not directly com-.
in practice, we do not need to strictly follow.
3https://huggingface.co/.
5393experiments.
score.
sst-2(acc).
mnli(m_cc).
qnli(acc).
cola(mcc).
sts-b(ρ).
qqp(acc).
mrpc(acc).
rte(acc).
wnli(acc).
avg.
#pr/#to(m).
albertpubalbertrepmpop.
mpopfullmpopfull+lfampopdir.
-78.979.7.
80.380.468.6.
90.390.690.8.
92.293.086.6.
81.684.583.3.
84.484.379.2.
-89.490.5.
91.491.381.9.
-53.454.7.
55.756.015.0.
-88.289.2.
89.289.282.5.
-89.189.4.
89.689.087.0.
-88.589.2.
87.388.074.3.
-71.173.3.
76.978.354.2.
-54.956.3.
56.356.356.3.
11.6/11.611.6/11.61.1/9.
12.7/12.71.2/12.71.1/9.
table 3: performance on glue benchmark obtained by ﬁne-tuning albert and mpop.
“albertpub” and“albertrep” denote the results from the original paper (lan et al., 2020) and reproduced by ours, respectively.
“#pr” and “#to” denote the number (in millions) of pre-trained parameters and total parameters, respectively..pare our approach with other competitive meth-ods (tambe et al., 2020) that require special opti-mization tricks or techniques (e.g., hardware-leveloptimization)..implementation.
the original paper of albertonly reported the results of sst-2 and mnli inglue.
so we reproduce complete results denotedas “albertrep” with the huggingface implemen-tation (wolf et al., 2020).
based on the pre-trainedparameters provided by huggingface, we also re-produce the results of bert, distilbert and mo-bilebert.
to ensure a fair comparison, we adoptthe same network architecture.
for example, thenumber of self-attention heads, the hidden dimen-sion of embedding vectors, and the max lengthof the input sentence are set to 12, 768 and 128,respectively..5.2 experimental results.
note that our focus is to illustrate that our approachcan improve either original (uncompressed) or com-pressed plms.
in our main experiments, we adoptalbert as the major baseline, and report the com-parison results in table 3..comparison with albert.
as shown in table 3,our approach mpop is very competitive in theglue benchmark, and it outperforms albert inall tasks (except mnli) with a higher overall scoreof 79.7. looking at the last column, compared withalbert, mpop reduces total parameters by 22%(#to).
in particular, it results in a signiﬁcant reduc-tion of pre-trained parameters by 91% (#pr).
sucha reduction is remarkable in lightweight ﬁne-tuning,which dramatically improves the ﬁne-tuning efﬁ-ciency.
by zooming in on speciﬁc tasks, the im-provements over albert are larger on cola,rte and wnli tasks.
an interesting explanationis that rte and wnli tasks have small trainingsets (fewer than 4k samples).
the lightweight ﬁne-.
tuning strategy seems to work better with limitedtraining data, which enhances the capacity of plmsand prevents overﬁtting on downstream tasks..ablation results.
our approach has incorporatedtwo novel improvements: lightweight ﬁne-tuningwith auxiliary tensors and optimization with di-mension squeezing.
we continue to study theireffect on the ﬁnal performance.
here we con-sider three variants for comparison: (1) mpopfulland mpopfull+lfa are full-rank mpo representa-tion (without reconstruction error), and ﬁne-tuneall the tensors and only auxiliary tensors, respec-tively.
this comparison is to examine whetheronly ﬁne-tuning auxiliary tensors would lead toa performance decrease.
(2) mpopdir directly opti-mizes the compressed model without the dimensionsqueezing algorithm.
this variant is used to exam-ine whether our optimization algorithm is moresuitable for stacked architecture.
table 3 (last threerows) shows the results when we ablate these.
inparticular, the dimension squeezing algorithm playsa key role in improving our approach (a signiﬁcantperformance decrease for mpopdir), since it is tai-lored to stacked architecture.
comparing mpopfullwith mpopfull+lfa, it is noted that ﬁne-tuning allthe parameters seems to have a negative effect onperformance.
compared with albert, we specu-late that ﬁne-tuning a large model is more likely tooverﬁt on small datasets (e.g., rte and mrpc)..these results show that our approach is able tofurther compress albert with fewer ﬁne-tuningparameters.
especially, it is also helpful to improvethe capacity and robustness of plms..5.3 detailed analysis.
in this section, we perform a series of detailedanalysis experiments for our approach..evaluation with other bert variants.
in gen-eral, our approach can be applied to either uncom-.
5394models.
wnli(acc).
mrpc(acc).
rte(acc).
avg.
#pr/#to(m).
models.
sst-2 mrpc rte.
avg.
#pr(m).
bertmpopb.
distilbertmpopd.
mobilebertmpopm.
56.356.3.
56.356.3.
56.256.2.
85.584.3.
84.184.3.
86.085.3.
70.070.8.
61.461.7.
63.565.7.
110/1107.7/70.4.
66/664.0/43.4.
25.3/25.34.4/15.4.
table 4: evaluation with different bert variants..bert10−12bert11−12bert12.
mpopb.
91.991.791.4.
92.6.
76.575.372.1.
84.3.
67.262.861.4.
70.8.
45.738.631.5.
10.1.table 5: comparison of different ﬁne-tuning strategieson three glue tasks.
the subscript number in bert(·)denotes the index of the layers to be ﬁne-tuned..pressed or compressed plms.
we have evaluatedits performance with albert.
now, we continueto test it with other bert variants, namely origi-nal bert, distilbert and mobilebert.
the lat-ter two bert variants are knowledge distillationbased methods, and the distilled models can also berepresented in the format of parameter matrix.
weapply our approach to the three variants.
table 4presents the comparison of the three variants beforeand after the application of mpop.
as we can see,our approach can substantially reduce the networkparameters, especially the parameters to be ﬁne-tuned.
note that distilbert and mobilebert arehighly compressed models.
these results show thatour approach can further improve other compressedplms..evaluation on different fine-tuning strategies.
experiments have shown that our approach is ableto largely reduce the number of parameters tobe ﬁne-tuned.
here we consider a more simplemethod to reduce the ﬁne-tuning parameters, i.e.,only ﬁne-tune the last layers of bert.
this experi-ment reuses the settings of bert (12 layers) andour approach on bert (i.e., mpopb in table 4).
we ﬁne-tune the last 1-3 layers of bert, and com-pare the performance with our approach mpopb.
from table 5, we can see that such a simple way ismuch worse than our approach, especially on therte task.
our approach provides a more princi-pled way for lightweight ﬁne-tuning.
by updatingauxiliary tensors, it can better adapt to task-speciﬁcloss, and thus achieve better performance..evaluation on low-rank approximation.
asintroduced in section 4.4, mpo is a special low-rank approximation method, and we ﬁrst com-pare its compression capacity with other low-rankapproximation methods.
as shown in table 2,mpo and tucker decomposition represent twomain categories of low-rank approximation meth-ods.
we select cpd (henry and hofrichter, 1992).
(a) cpd v.s.
mpo..(b) # of local tensors..figure 2: comparison of different low-rank approxima-tion variants.
x-axis denotes the compression ratio (ρin eq.
(5)) and y-axis denotes the reconstruction error,measured in the frobenius norm..for comparison because general tucker decompo-sition (tucker, 1966) cannot obtain results withreasonable memory.
our evaluation task is to com-press the word embedding matrix of the released“bert-base-uncased” model4.
as shown in fig-ure 2(a), mpo achieves a smaller reconstructionerror with all compression ratios, which shows thatmpo is superior to cpd.
another hyper-parameterin our mpo decomposition is the number of lo-cal tensors (n).
we further perform the sameevaluation with different numbers of local tensors(n = 3, 5, 7).
from figure 2(b), it can be observedthat our method is relatively stable with respect tothe number of local tensors.
overall, a larger n re-quires a higher time complexity and can yield ﬂexi-ble decomposition.
thus, we set n = 5 for makinga trade-off between ﬂexibility and efﬁciency..6 conclusion.
we proposed an mpo-based plm compressionmethod.
with mpo decomposition, we were ableto reorganize and aggregate information in centraltensors effectively.
inspired by this, we designed anovel ﬁne-tuning strategy that only needs to ﬁne-tune the parameters in auxiliary tensors.
we alsodeveloped a dimension squeezing training algo-rithm for optimizing low-rank approximation over.
4https://huggingface.co/bert-base-uncased.
53950.00.20.40.60.81.0compression ratio050100150reconstruction errorcpdmpo0.00.20.40.60.81.0compression ratio050100150reconstruction errormpo-2mpo-3mpo-5mpo-7stacked network architectures.
extensive experi-ments had demonstrated the effectiveness of ourapproach, especially on the reduction of ﬁne-tuningparameters.
we also empirically found that sucha ﬁne-tuning way was more robust to generalizeon small training datasets.
to our knowledge, it isthe ﬁrst time that mpo decomposition had beenapplied to compress plms.
in future work, we willconsider exploring more decomposition structuresfor mpo..acknowledgments.
this work was partially supported by the nationalnatural science foundation of china under grantsno.
61872369, 61832017 and 11934020, bei-jing academy of artiﬁcial intelligence (baai) un-der grant no.
baai2020zj0301, beijing out-standing young scientist program under grantno.
bjjwzyjh012019100020098, the funda-mental research funds for the central universitiesand the research funds of renmin university ofchina under grant no.
18xnlg22, 19xnq047,20xnlg19 and 21xnh027.
xin zhao and zhong-yi lu are the corresponding authors..references.
pasquale calabrese and john cardy.
2004. entangle-journalment entropy and quantum ﬁeld theory.
of statistical mechanics: theory and experiment,2004(06):p06002..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..prakhar ganesh, yao chen, xin lou, mohammad alikhan, yin yang, deming chen, marianne winslett,hassan sajjad, and preslav nakov.
2020. compress-ing large-scale transformer-based models: a casestudy on bert.
arxiv preprint arxiv:2002.11985..ze-feng gao, song cheng, rong-qiang he, zy xie,hui-hai zhao, zhong-yi lu, and tao xiang.
2020. compressing deep neural networks by ma-trix product operators.
physical review research,2(2):023300..timur garipov, dmitry podoprikhin, alexandernovikov, and dmitry vetrov.
2016. ultimate ten-.
sorization: compressing convolutional and fc layersalike.
arxiv preprint arxiv:1611.03214..er henry and j hofrichter.
1992.
[8] singular value de-composition: application to analysis of experimen-tal data.
methods in enzymology, 210:129–192..frank l hitchcock.
1927. the expression of a tensoror a polyadic as a sum of products.
journal of math-ematics and physics, 6(1-4):164–189..lu hou, zhiqi huang, lifeng shang, xin jiang, xiaochen, and qun liu.
2020. dynabert: dynamicbert with adaptive width and depth.
in advancesin neural information processing systems 33: an-nual conference on neural information processingsystems 2020, neurips 2020, december 6-12, 2020,virtual..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in international conference on machine learning,pages 2790–2799.
pmlr..itay hubara, matthieu courbariaux, daniel soudry,ran el-yaniv, and yoshua bengio.
2017. quantizedneural networks: training neural networks with lowprecision weights and activations.
the journal ofmachine learning research, 18(1):6869–6898..forrest n iandola, albert e shaw, ravi krishna, andkurt w keutzer.
2020. squeezebert: what cancomputer vision teach nlp about efﬁcient neural net-works?
arxiv preprint arxiv:2006.11316..s iblisdir, r orus, and ji latorre.
2007. matrix productstates algorithms and continuous systems.
physicalreview b, 75(10):104305..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-in proceedings of the 2020guage understanding.
conference on empirical methods in natural lan-guage processing: findings, emnlp 2020, onlineevent, 16-20 november 2020, pages 4163–4174.
as-sociation for computational linguistics..ashish khetan and zohar karnin.
2020..schubert:in proceedings ofoptimizing elements of bert.
the 58th annual meeting of the association for com-putational linguistics, pages 2807–2818, online.
association for computational linguistics..valentin khrulkov, oleksii hrinchuk, leyla mir-vakhabova, and ivan oseledets.
2019. tensorizedembedding layers for efﬁcient model compression.
arxiv preprint arxiv:1901.10787..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedin 8th inter-learning of language representations.
national conference on learning representations,.
5396iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..zhaojiang lin, andrea madotto, and pascale fung.
exploring versatile generative language2020.model via parameter-efﬁcient transfer learning.
inproceedings of the 2020 conference on empiricalmethods in natural language processing: findings,emnlp 2020, online event, 16-20 november 2020,pages 441–459.
association for computational lin-guistics..weijie liu, peng zhou, zhiruo wang, zhe zhao,haotang deng, and qi ju.
2020. fastbert: a self-distilling bert with adaptive inference time.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 6035–6044.
associa-tion for computational linguistics..xindian ma, peng zhang, shuai zhang, nan duan,yuexian hou, ming zhou, and dawei song.
2019.a tensorized transformer for language modeling.
inadvances in neural information processing systems32: annual conference on neural information pro-cessing systems 2019, neurips 2019, december8-14, 2019, vancouver, bc, canada, pages 2229–2239..matan ben noach and yoav goldberg.
2020. com-pressing pre-trained language models by matrix de-in proceedings of the 1st confer-composition.
ence of the asia-paciﬁc chapter of the associationfor computational linguistics and the 10th interna-tional joint conference on natural language pro-cessing, pages 884–889..alexander novikov, dmitry podoprikhin, anton os-okin, and dmitry p. vetrov.
2015. tensorizing neu-in advances in neural informationral networks.
processing systems 28: annual conference on neu-ral information processing systems 2015, decem-ber 7-12, 2015, montreal, quebec, canada, pages442–450..ivan v oseledets.
2011. tensor-train decomposition.
siam journal on scientiﬁc computing, 33(5):2295–2317..nikolaos pappas, phoebe mulcaire, and noah a smith.
2020. grounded compositional outputs for adaptivelanguage modeling.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1252–1267..matthew e. peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, naacl-hlt 2018, new or-leans, louisiana, usa, june 1-6, 2018, volume 1(long papers), pages 2227–2237.
association forcomputational linguistics..bogdan pirvu, valentin murg, j ignacio cirac, andfrank verstraete.
2010. matrix product operator rep-resentations.
new journal of physics, 12(2):025012..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training..evani radiya-dixit and xin wang.
2020. how ﬁnecan ﬁne-tuning be?
learning efﬁcient language mod-els.
in international conference on artiﬁcial intelli-gence and statistics, pages 2435–2443.
pmlr..sylvestre-alvise rebufﬁ, hakan bilen, and andreavedaldi.
2017. learning multiple visual domainswith residual adapters.
in advances in neural infor-mation processing systems 30: annual conferenceon neural information processing systems 2017,december 4-9, 2017, long beach, ca, usa, pages506–516..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimenttreebank.
in proceedings of the 2013 conference onempirical methods in natural language processing,pages 1631–1642..siqi sun, zhe gan, yuwei fang, yu cheng, shuohangwang, and jingjing liu.
2020a.
contrastive distil-lation on intermediate representations for languagein proceedings of the 2020model compression.
conference on empirical methods in natural lan-guage processing (emnlp), pages 498–508..xingwei sun, ze-feng gao, zhong-yi lu, junfeng li,and yonghong yan.
2020b.
a model compressionmethod with matrix product operators for speechieee/acm transactions on audio,enhancement.
speech, and language processing, 28:2837–2847..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020c.
mobilebert:a compact task-agnostic bert for resource-limitedin proceedings of the 58th annual meet-devices.
ing of the association for computational linguistics,acl 2020, online, july 5-10, 2020, pages 2158–2170. association for computational linguistics..thierry tambe, coleman hooper, lillian pentecost,en-yu yang, marco donato, victor sanh, alexan-der m rush, david brooks, and gu-yeon wei.
2020.edgebert: optimizing on-chip inference for multi-task nlp.
arxiv preprint arxiv:2011.14203..andros tjandra, sakriani sakti, and satoshi nakamura.
2017. compressing recurrent neural network within 2017 international joint confer-tensor train.
ence on neural networks (ijcnn), pages 4451–4458. ieee..5397ledyard r tucker.
1966. some mathematical notespsychometrika,.
on three-mode factor analysis.
31(3):279–311..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis plat-in 7thform for natural language understanding.
international conference on learning representa-tions, iclr 2019, new orleans, la, usa, may 6-9,2019. openreview.net..sinong wang, belinda li, madian khabsa, hanself-arxiv preprint.
fang, and hao ma.
2020.attention with linear complexity.
arxiv:2006.04768..linformer:.
thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pier-ric cistac, morgan funtowicz, joe davison, samshleifer, et al.
2020. transformers: state-of-the-art natural language processing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing: system demonstrations,pages 38–45..ji xin, raphael tang, jaejun lee, yaoliang yu, andjimmy lin.
2020. deebert: dynamic early exitingin proceedings offor accelerating bert inference.
the 58th annual meeting of the association for com-putational linguistics, pages 2246–2251..rose yu, stephan zheng, anima anandkumar, andyisong yue.
2017. long-term forecasting usingtensor-train rnns.
arxiv..oﬁr zafrir, guy boudoukh, peter izsak, and moshewasserblat.
2019. q8bert: quantized 8bit bert.
arxiv preprint arxiv:1910.06188..tianyi zhang, varsha kishore, felix wu, kilian q.weinberger, and yoav artzi.
2020. bertscore: eval-in 8th inter-uating text generation with bert.
national conference on learning representations,iclr 2020, addis ababa, ethiopia, april 26-30,2020. openreview.net..mengjie zhao, tao lin, fei mi, martin jaggi, and hin-rich schütze.
2020. masking as an efﬁcient alterna-tive to ﬁnetuning for pretrained language models.
inproceedings of the 2020 conference on empiricalmethods in natural language processing, emnlp2020, online, november 16-20, 2020, pages 2226–2241. association for computational linguistics..5398