learning from perturbations: diverse and informativedialogue generation with inverse adversarial training.
wangchunshu zhou∗ † qifei li∗ chenle libeihang university, beijing, chinazhouwangchunshu@buaa.edu.cn.
abstract.
in this paper, we propose inverse adversarialtraining (iat) algorithm for training neuraldialogue systems to avoid generic responsesand model dialogue history better.
in contrastto standard adversarialtraining algorithms,iat encourages the model to be sensitive tothe perturbation in the dialogue history andtherefore learning from perturbations.
by giv-ing higher rewards for responses whose outputprobability reduces more signiﬁcantly whendialogue history is perturbed, the model is en-couraged to generate more diverse and con-sistent responses.
by penalizing the modelwhen generating the same response given per-turbed dialogue history, the model is forcedto better capture dialogue history and gener-ate more informative responses.
experimentalresults on two benchmark datasets show thatour approach can better model dialogue his-tory and generate more diverse and consistentresponses.
in addition, we point out a prob-lem of the widely used maximum mutual in-formation (mmi) based methods for improv-ing the diversity of dialogue response genera-tion models and demonstrate it empirically..1.introduction.
in recent years, neural end-to-end dialogue re-sponse generation models (sordoni et al., 2015;serban et al., 2016; bordes et al., 2016) has gainedincreasing popularity with the recent advance-ments of neural sequence-to-sequence (seq2seq)learning models (sutskever et al., 2014; vaswaniet al., 2017).
while neural dialogue models cangenerate seemingly ﬂuent responses, due to theover-simpliﬁed maximum likelihood estimation(mle) training objective and the high frequencyof generic responses in training corpora, they tendto produce dull and generic responses such as “i.
∗equal contribution.
† corresponding author.
don’t know” much more often than that humansgenerally do (li et al., 2015), which makes dia-logue agents less engaging and ineffective..in addition, recent research on whether neu-ral dialogue systems use dialogue history effec-tively (sankar et al., 2019) shows that most neu-ral dialogue agents fail to take the dialogue his-tory into account when generating responses.
thisproblem makes neural dialogue systems tend togenerate responses irrelevant to the current topicof the conversation and are not consistent with thedialogue history.
this problem may also intensifythe generic response problem, as dull responsesare generally off-topic and irrelevant to the dia-logue history..to address the above issues, in this paper, wepropose inverse adversarial training (iat) al-gorithm for training neural dialogue systems toavoid generic responses and model dialogue his-tory better, thus generating diverse and informa-tive responses.
conventional adversarial trainingmethods generally generate label-preserving ad-versarial inputs with carefully designed methodsand train the model to generate the same output toenhance the model’s robustness.
in contrast, ourapproach perturbs in input dialogue history suchthat a good dialogue model should not generatethe same output if the output is non-generic andrelevant to the dialogue history.
we name our pro-posed method as inverse adversarial training be-cause it is related to conventional adversarial train-ing methods which aim to improve the model’s ad-versarial robustness but our proposed objective ismotivated in the opposite direction.
note that ourwork is not directly related to textgans as wellas their applications on dialogue response genera-tion..speciﬁcally, the proposed inverse adversarialtraining assigns higher rewards to generated re-sponses or ground-truth responses if their likeli-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages694–703august1–6,2021.©2021associationforcomputationallinguistics694hood decreases more when the dialogue history isperturbed, and penalize the model when it gen-erates responses whose likelihood is almost un-changed given either original or perturbed dia-logue history as input.
this encourages the modelto generate more relevant and informative re-sponses and capture dialogue history better.
theproposed iat algorithm can be used in both super-vised and self-supervised fashion (with/withoutreference response), which can be viewed as aform of reward-augmented maximum likelihood(raml) method (norouzi et al., 2016) that im-proves the original mle objective or a rewardingscheme for rl-based text generation algorithms.
the inverse adversariallearning framework isalso conceptually related to self-adversarial learn-ing (zhou et al., 2020) where the the comparisonis made between different checkpoints of the samemodel to provide reward for rl training of thenlg model..in addition, we identify a limitation of thewidely-used maximum mutual information (mmi)based methods for improving the diversity of dia-logue response generation models.
this will bediscussed in detail in section 2.1 and empiricallydemonstrated in section 4.2..we conduct experiments on two dialoguedatasets, opensubtitiles and dailydialog,todemonstrate the effectiveness of the proposed ap-proach.
experimental results show iat helps neu-ral dialogue systems model dialogue history bet-ter and generate more diverse and informative re-sponses..2 related work.
2.1 dull response problem.
neural dialogue models tend to generate genericor dull responses such as i don’t know whichare not engaging for the users (sordoni et al.,2015).
this behavior can be ascribed to thehigh frequency of generic responses in the train-ing corpus and the over-simpliﬁed mle trainingobjective.
how to avoid generic responses andto make the dialogue agent more engaging hasbeen a long-standing problem.
previous work at-tempts to address this problem with different ap-proaches: 1) li et al.
(2015) propose a diversity-promoting objective based on maximum mutualinformation (mmi).
given source s and target t ,their approach ﬁrst generates n-best lists basedon p (t |s) and then rerank the list by combin-.
ing p(t |s) and λp(s|t ); 2) zhang et al.
(2018b)propose to directly optimize p(s|t ) together withp(t |s) with an adversarial information maxi-mization objective; and 3) adversarial learning (liet al., 2017a) and dual adversarial learning (cuiet al., 2019) based on the intuition that real re-sponses are of high diversity, thus can be distin-guished from generated responses which are of-ten dull and generic.
there are also other meth-ods using distributional constraints of the target re-sponses (baheti et al., 2018; cs´aky et al., 2019) orcommonsense knowledge (wu et al., 2020)..while shown to be effective in several datasets,these approaches suffer from several drawbacks.
for the ﬁrst two approaches, while the mmi ob-jective may lead to larger mutual information, itoften does not actually result in more informativeand engaging responses according to our observa-tions.
for example, given a dialog context: “whathave you done with him in the bar last night?”the top response re-ranked by the mmi objectiveis “i have done nothing with him in the bar lastnight.”, which is non-informative and less natu-ral compared with the response “nothing at all.”generated by a standard seq2seq dialogue model.
this is also conﬁrmed in the experiment section.
we suspect this phenomenon is caused by the termp(s|t ) in the mmi objective.
it encourages gen-erating responses that make the last utterance inthe dialogue history have a high likelihood giventhe generated responses.
while a truly informa-tive response may yield a high p(s|t ), the modelcan easily ﬁnd a “shortcut” to cheat this objectiveby simply copying a portion of tokens in the lastutterance, which is likely to have high p(s|t ) aswell as p(t |s).
the adversarial learning based di-alogue model is notoriously hard to train and maysuffer from the problem of mode collapse, whichdecreases the diversity of generated responses..in contrast, our proposed iat approach is basedon the intuition that a diverse, relevant, and consis-tent response should be sensitive to the perturba-tion in the dialogue history, which is from a differ-ent perspective and may be complementary withthe aforementioned approaches..2.2 dialogue history modeling.
recently, sankar et al.
(2019) evaluated whetherexisting neural dialogue systems use dialoguehistory effectively by perturbing dialogue his-tory and observing the variation of model out-.
695put.
they corrupted the dialogue history withboth utterance-level and word-level perturbationand see whether and how much the output perplex-ity decreases.
their experimental results show thatend-to-end neural dialogue systems are generallynon-sensitive to the perturbation of dialogue his-tory, suggesting that they may perform poorly inmodeling dialogue history.
previous work (serbanet al., 2016; zhao et al., 2017) improves the con-text modeling ability with modiﬁcation in modelarchitectures.
in contrast, our approach employsa novel training objective to enhance the dialoguehistory modeling ability, which is orthogonal andmay be complementary with them..3.inverse adversarial training.
in this section, we describe the proposed inverseadversarial training algorithm in detail.
we ﬁrstdescribe how we perturb the dialogue history andthen formally introduce the inverse adversarialtraining algorithm..3.1 perturbation approaches.
following previous study (sankar et al., 2019), weperturb the dialogue history in both utterance andword level and apply them jointly during training..utterance-level perturbations we consider thefollowing operations 1) shuf that shufﬂes the se-quence of utterances in the dialog history, 2) revthat reverses the order of utterances in the his-tory (but maintains word order within each utter-ance) 3) drop that completely drops certain utter-ances, 4) truncate that truncates the dialog historyto contain only the k most recent utterances wherek ≤ n, where n is the length of dialog history, and5) repl that randomly replaces each utterance inthe dialogue history by another utterance in thedataset with a probability of 30%, which resem-bles the negative sampling (mikolov et al., 2013)approach1..word-level perturbations we consider similaroperations but at the word level within every ut-terance 1) word-shufﬂe that randomly shufﬂes thewords within an utterance 2) reverse that reversesthe ordering of words, 3) word-drop that drops30% of the words uniformly 4) noun-drop thatdrops all nouns, 5) verb-drop that drops all verbs,.
1the ﬁrst four kinds of perturbation is originally proposedin (sankar et al., 2019) and the last is proposed in this paper..and 6) word-repl that replace 30% of words with arandom word in the vocabulary uniformly..we explain the role of different perturbationsand their potential effects brieﬂy.
the shuf andrev perturbations change the chronological orderof utterances.
inverse adversarial training withthese kinds of perturbation may help the model tocapture some common-senses about the chrono-logical order of utterances.
the drop and replperturbations may help the model to capture somekinds of casual effects.
finally, the truncate per-turbation may help the model capture long-termand multi-turns dialogue history better..3.2.inverse adversarial training.
in contrast to the adversarial training objectivewhich maximize the likelihood of generating thesame output given perturbed input, the inverse ad-versarial training objective maximizes the reduc-tion of the likelihood of generating the same out-put when the input is perturbed, which is oppositeto the conventional adversarial training..a straightforward approach is to maximize thelikelihood of generating ground-truth responsesgiven original dialogue history while minimizingthis likelihood when given perturbed dialogue his-tory.
however, this approach suffers from severalproblems: first, as a previous study (sankar et al.,2019) has shown, neural dialogue models gener-ally capture the perturbation in the dialogue his-tory poorly, which is suggested by the fact that theoutput embeddings of the encoder are very sim-ilar when given original and perturbed input di-alogue histories.
this results in training the de-coder to simultaneously maximize and minimizethe likelihood of the same output given very simi-lar input, which is undesirable and makes the train-ing ineffective.
the second problem is that thistraining objective does not capture the variationof likelihood and thus treats relevant and engag-ing responses equally with dull and generic re-sponses.
this is undesirable as we only want tomaximize/minimize the likelihood for relevant andengaging responses when conditioning on origi-nal/perturbed dialogue history and dull responsesshould be avoided in both cases..in this paper, we propose a sequence-level ob-jective which is able to capture the variation of thelikelihood of responses given original or perturbedinput.
this makes it possible to model dialoguehistory better and avoid generic response problem.
696figure 1: illustration of iat.
our algorithm assigns high reward and low penalty when the dialogue model generatesrelevant and engaging responses given original and perturbed dialogue history respectively.
the reward and penaltyare respectively decreased and increased when the dialogue model generates dull responses.
note that both dullresponses and engaging responses are gold human-written reference responses.
they are not labeled in the datasetbut automatically detected by the difference of their generation likelihood when given original and perturbeddialogue history.
(best view in color.).
at the same time.
the idea is to evaluate generatedsentences based on the variation of the likelihoodof responses given original or perturbed dialoguehistory and use this variation as rewards for train-ing the dialogue model..given original dialogue history x andperturbed dialogue history x (cid:48),the rewardr(y |x, x (cid:48)) of generating response y , whichis a sequence of n tokens yi, i ∈ 1, 2, ..., n, ismeasured by how much y is more likely tobe generated by the dialogue model given xcompared with that given x (cid:48), which is computedby the difference of negative log-likelihood losses(nll) in two cases, as described below..nllorig = −.
log p (yi|y<i, x).
(1).
gradient of the objective function for dialogue re-sponse generator gθ as:.
(cid:88).
n(cid:88).
y.i=1.
∇θj(θ) =.
∇θ log gθ(yi|y<i, x) · r(y |x, x (cid:48)).
(4)the above training objective encourages the di-alogue model to generate non-generic responsesand model dialogue history better by giving higherrewards when generating good responses based onoriginal dialogue history..similarly, we would also want to penalize thedialogue model when it generates the same re-sponse given perturbed dialogue history to explic-itly force the dialogue system to effectively modelthe dialogue history.
we propose to model thispenalty with a max-margin reward scheme.
givenmargin m, the penalty p(y |x, x (cid:48)) of generatingy is computed by.
nlladv = −.
log p (yi|y<i, x (cid:48)).
(2).
p(y |x, x (cid:48)) = min(0, nlladv − nllorig − m).
(5).
r(y |x, x (cid:48)) = nlladv − nllorig.
(3).
intuitively, the reward r would be high when theresponse y is engaging and relevant to the dia-logue history.
a generic response should be as-signed with a low or even negative reward as itis irrelevant to the dialogue history.
the inverseadversarial training objective is to generate re-sponses to maximize its reward.
with likelihoodratio (sutton et al., 2000), we can formulate the.
the insight behind equation 5 is that when thevariation of likelihood of generating y given xand x (cid:48) is large enough (i.e.
nllorig − nlladv −m > 0), the model should be considered suc-cessfully captured the perturbation in the dialoguehistory and should not be penalized.
in contrast,when the variation is not large enough, we pe-nalize the dialogue agent for generating y givingx (cid:48) because a small variation of likelihood impli-cates: (1) the dialogue agent models dialogue his-tory poorly and (2) the generated responses y may.
n(cid:88).
i=1.
n(cid:88).
i=1.
697be irrelevant to the dialogue history x and thus begeneric and non-informative.
the correspondinggradient can be formulated as:.
(cid:88).
n(cid:88).
y.i=1.
∇θj (cid:48)(θ) =.
∇θ log gθ(yi|y<i, x (cid:48)) · p(y |x, x (cid:48)).
(6)the penalty and reward are combined by di-rectly summing up the gradient in eq (4) and eq(6).
the proposed inverse adversarial training al-gorithm can be applied in both supervised fashionwhere responses y are ground-truth responses inthe dataset and self-supervised fashion where y isgenerated by the dialogue model itself.
the onlydifference between the self-supervised and super-vised version is whether the reference responsesare generated (self-supervised) or ground-truth re-sponses (supervised).
the supervised inverse ad-versarial training can be viewed as a reward func-tion algorithm for raml (norouzi et al., 2016)training that assigns higher rewards for “good”training examples that help our model to gener-ate relevant responses and learn to model dialoguehistory better.
the self-supervised inverse adver-sarial training, in contrast, allows the model to ex-plore freely and train the model with policy gradi-ent (sutton et al., 2000), a reinforcement learningapproach..4 experiments.
to validate the effectiveness of the proposed in-verse adversarial training algorithm, we conductexperiments in order to answer the following tworesearch questions:(1) do inverse adversarial training help neural di-alogue systems model dialogue history better?
(2) do inverse adversarial training help neural di-alogue models generate more diverse, engaging,and informative dialogue responses?.
4.1 experimental settings.
datasets we employ two datasets in our experi-ments.
the ﬁrst dataset is the opensubtitles cor-pus (lison and tiedemann, 2016) which is a large,open-domain dataset containing scripts of moviecharacters.
following previous work, we considereach turn in the dataset as the target response andthe two previous sentences as the dialogue history.
we remove the pairs whose response is shorterthan 5 words and randomly sample 1,800k, 500k,and 12k dialogue turns for training, validation,and testing, respectively..we employ the dailydialog dataset (li et al.,2017b) as the second dataset which consists of di-alogues that resemble daily conversations acrossmultiple topics.
it comprises of 13k dialogues,which is much smaller compared with the open-subtitles dataset.
however, it has an average of7.9 turns per dialog, which is more suitable forevaluating whether the proposed approach is ableto improve the model’s ability of modeling long-term dialogue history..compared models we build dialogue systemswith seq2seq (sutskever et al., 2014) models.
fol-lowing previous work (li et al., 2017a, 2015), weemploy lstm-based seq2seq model for the open-subtitles dataset.
for the dailydialog dataset,we employ the transformer (vaswani et al., 2017)model which yields superior results in prelimi-nary experiments while shown to perform poorlyin modeling dialogue history (sankar et al., 2019).
speciﬁcally, following previous work (xu et al.,2018), we set the hidden size to 256, embeddingsize to 128, vocabulary size to 50k, and batch sizeto 64 for the proposed models and the baselines.
we use the adam optimizer with the initial learn-ing rate 0.1 for model training..we compare the dialogue model trained withthe proposed inverse adversarial learning algo-rithm with the following baseline methods (allcompared models are using the same backbone ar-chitecture):.
• seq2seq: the vanilla seq2seq dialogue.
model trained with mle objective..• seq2seq + mmi: the dialogue model us-ing mutual information method (li et al.,2015), which substracts the score of the targetsequence log p(t |s) by its language modelscore log p(t ) (mmi-anti) or by a backwardgeneration score log p(s|t ) (mmi-bidi) fordecoding..• seq2seq + adversarial learning: a dia-logue model trained with adversarial learn-ing objective (li et al., 2017a).
the model ispretrained with mle objective and then ﬁne-tuned with adversarial learning..• seq2seq + ds: a strong baseline using dis-tributional constraints over the generated re-sponses (baheti et al., 2018)..• cvae: a dialogue response generationmodel using conditional vae (zhao et al.,.
6982017) to improve the discourse-level diver-sity of generated responses..our models are pretrained with the mle ob-jective until the validation perplexity stops de-creasing.
we then apply the inverse adversarialtraining algorithm for continual training.
dur-ing training, reference responses are either gen-erated responses or ground-truth responses inself-supervised and supervised inverse-adversarialtraining respectively.
we combine both supervisedand self-supervised inverse adversarial training byalternatively switching between these two objec-tives for each training iteration..evaluation metrics we employ different au-tomated evaluation metrics to respectively answerthe three research questions introduced at the be-ginning of this section.
to evaluate how well dia-logue systems are able to model dialogue history,we adopt the approach proposed by sankar et al.
(2019), which measures the increases in perplex-ity when the model is fed with perturbed dialoguehistory instead of original dialogue history.
wereport the result in both utterance-level and word-level perturbation..to evaluate if inverse adversarial learning caneffectively reduce the generic response problem,following li et al.
(2015), we evaluate the di-versity of generated responses by calculating thenumber of distinct unigrams, bigrams, and tri-the value isgrams in generated responses.
scaled by the total number of generated tokens toavoid favoring long sentences, which are shownas distinct-1, distinct-2, and distinct-3 in table2. lastly, we compare the percentage of stop-words2 of the responses generated by each model(smaller values that are closer to the distributionof human conversations are preferred).
we alsoreport the token-level overlap between the gener-ated response and the last utterance in the dialoghistory to demonstrate the “shortcut”problem ofmmi-based methods decribed in section 2.1..as our approach is training in an “opposite”direction compared to conventional adversarialtraining employed to enhance the robustness oftrained models, we also conduct experiments toevaluate the robustness of the dialogue responsegeneration models with respectto non label-changing adversarial dialogue history.
similarto the method of evaluating the dialogue his-.
2stopword list from https://www.ranks.
nl/stopwords..we appended punctuations to this list..methodseq2seq- base model- + al- + ds- + cvae- + iat.
dailydialog opensubtitles.
2.71 (1.18)2.76 (1.22)2.79 (1.24)3.25 (1.41)3.69 (1.65).
1.94 (0.33)1.67 (0.41)1.87 (0.44)2.11 (0.49)2.37 (0.42).
table 1: results on the dialogue history modelingability of compared models, which is measured by thedifference between perplexity of gold responses whenreceiving original dialogue history and receiving per-turbed dialogue history.
mean and standard deviationof 5 runs are reported..tory modeling ability, we measure the perplex-ity changes when the model is given a differentbut meaning-preserving dialogue history, which isconstructed by performing word substitution witha bert-based lexical substitution method (zhouet al., 2019) and paraphrase generation (kumaret al., 2020) as word-level and utterance-level per-turbation respectively on the original dialogue his-tory, as the input..in addition, as demonstrated by liu et al.
(2016); zhou and xu (2020), automated metricsare notoriously poor for evaluating dialogue sys-tems.
we thus conduct a human evaluation to bet-ter evaluate the effectiveness of the proposed algo-rithm.
for human evaluation, we invite 20 humanannotators which are all graduate students withgood english proﬁciency to evaluate the quality ofthe model.
following zhang et al.
(2018a), weask human annotators to interact with comparedmodels for 50 utterances with each compared di-alogue system and evaluate the ﬂuency, consis-tency, and diversity of the model (scored between1- 5).
fluency measures how likely the generatedtext is produced by human.
consistency measureshow likely the generated text is related to the in-put dialogue history, which corresponds to the ﬁrstresearch question.
diversity measures how muchthe generated text provides speciﬁc information,rather than “dull” and repeated information, whichcorresponds to the second research question..4.2 experimental results.
results on dialogue history modeling we ﬁrstpresent the results on dialogue history modelingability.
the results are shown in table 1. we cansee that the dialogue model trained with the pro-posed inverse adversarial training algorithm per-.
699method.
seq2seq- base model- + mmi-anti- + mmi-bidi- + al- + ds- + cvae- + iat.
dailydialog.
opensubtitles.
dist-1 dist-2 dist-3.
overlap stop-word dist-1 dist-2 dist-3.
overlap stop-word.
2.324.15∗3.522.253.193.593.72.
6.2811.27∗9.296.017.849.419.81.
9.4319.61∗17.439.3911.6112.9314.93.
15.626.731.516.118.417.715.4∗.
67.462.463.166.861.561.160.9.
1.723.453.52∗2.973.053.353.29.
5.3711.3512.11∗5.446.3010.1310.16.
7.6418.1218.56∗7.4611.5917.0217.30.
22.530.137.823.521.322.520.8∗.
77.874.274.776.471.271.470.9∗.
table 2: results of the diversity of generated responses of compared models.
we report the average value of 5runs on both datasets.
∗ denotes statistically signiﬁcant with p-value < 0.01..dailydialog opensubtitles.
fluency consistency diversity.
methodseq2seq- base model- + mmi- + al- + ds- + iat.
0.75(0.41)-0.83(0.47)0.78(0.44)0.77(0.45).
0.42(0.29)-0.49(0.34)0.46(0.33)0.44(0.31).
table 3: results on the adversarial robustness ofcompared models, which is measured by the differ-ence between perplexity of gold responses when re-ceiving original dialogue history and receiving non-label changing adversarial dialogue history.
al de-notes adversarial learning and iat denotes inverse ad-versarial training..forms signiﬁcantly better than the compared base-lines as the perplexity dramatically increases whenthe input dialogue history is perturbed.
this isnot surprising as our approach is the ﬁrst learn-ing objective which explicitly forces the dialoguesystem to better model dialogue history.
in con-trast, the mmi criterion and the adversarial learn-ing objective do not signiﬁcantly inﬂuence the di-alogue history modeling ability of dialogue sys-tems.
the dialogue model based on cvae modelsdialogue history better than other baselines whilestill under-performs our approach..reults on diversity the results of the diversityof responses generated by compared models areshown in table 2. we can see that both the max-imum mutual information objective and the pro-posed inverse adversarial learning succeed in im-proving the diversity of generated responses.
incontrast, the adversarial learning objective hardlyimproves the diversity, which may be due to the in-stability of adversarial learning on text generation.
while the mmi objective yields slightly larger im-provements on distinct n-gram based metrics, their.
methodseq2seq.
- base model- + mmi-anti- + mmi-bidi- + al- + ds- + cvae- + iat.
2.832.732.802.772.852.933.02∗.
2.692.782.822.692.882.913.05∗.
3.053.103.022.913.123.193.34∗.
table 4: human evaluation results of compared modelon the dailydialog dataset.
the kappa score betweenhuman annotators is 0.73.
∗ denotes statistically signif-icant with p-value < 0.05..approach is used only for re-ranking during infer-ence, which is orthogonal and may be complemen-tary to the proposed approach..in addition, as described in section 2.1,themmi objective may favor non-engaging responsesthat simply repeats the last utterance in the dia-logue history.
this is empirically demonstratedby their high overlap with the last utterance inthe dialog history, as measured by the “overlap”metric.
in contrast, our approach does not sufferfrom this problem and also generate fewer stop-words compared to the mmi-based methods.
inaddition, our approach also outperforms the strongbaselines including that using distributional con-straint and cvae, demonstrating its effectivenessin improving the diversity of generated responses.
results on adversarial robustness we alsoconduct experiments to test the robustness of thedialogue model trained with the proposed inverseadversarial training objective.
the results areshown in table 3. we see that the increase inthe perplexity of ground-truth responses underour model is roughly the same with the baselinetransformer model and the other compared mod-.
700sourceseq2seq+mmi-anti+mmi-bidi+ds+cvae+iatsourceseq2seq+mmi-anti+mmi-bidi+ds+cvae+iatsourceseq2seq+mmi-anti+mmi-bidi+ds+ds+iat.
how long will it take us to drive to london ?
... ok .
you haven’t seen my company car , have you ?
i don’t know about what you are talking about.
i have seen a car in london.
i haven’t seen your company car, have you ?
no, i haven’t seen a car.
no, i think no.
no, i don’t think we can arrive on time.
your throat is inﬂamed .
and your tongue is heavily coated .
you have all the symptoms of inﬂuenza .
i agree with that you have just said.
i want to have all the inﬂuenza.
i do not want to have the inﬂuenza and the throat.
oh that’s too bad.
that’s a sad thing.
oh what should i do for my tougue?
good evening , madam .
can i help you ?
yes .
i bought this sheet here yesterday .
i want to buy a sheet too.
what is the color of the shirt you bought yesterday?
what is wrong with this sheet bought here yesterday?
what problem about the sheet?
what do you think of the sheet?
anything wrong with the sheet?.
table 5: case study of dialogue responses generated by different compared models..els.
this suggests that our proposed iat objectivedoes not harm the adversarial robustness..human evaluation we conduct a human eval-uation of compared models on the dailydialogdataset.
the results are shown in table 4. we cansee that the proposed inverse adversarial trainingobjective substantially improves the consistencyof the dialogue model over all compared baselines,which conﬁrms its ability to train dialogue agentsto model dialogue history better.
as for the diver-sity of generated responses, we ﬁnd that humanannotators do not prefer the responses selected bythe mmi objective over that generated by the base-line model with a large margin.
we ﬁnd that thisis mainly because the mmi objective prefers re-peating tokens which appear in the last utteranceand human annotators ﬁnd it non-informativeness.
in contrast, our approach yields even larger im-provements in the diversity of the generated re-sponses.
we do not ﬁnd the adversarial learningmethod improves the diversity of dialogue mod-els, which may be due to the problem of mode col-lapse in adversarial learning.
the over-all ﬂuencyof compared models is roughly the same, whichmay be because they are all trained or pretrainedwith mle objective..methodours- w/o supervised- w/o self-supervised- w/o reward- w/o penalty- w/o utter pertub- w/o token pertub.
fluency consistency diversity3.022.912.982.882.822.712.84.
3.363.343.313.123.263.233.28.
2.862.682.912.742.882.822.85.table 6: ablation study results of compared model onthe dailydialog dataset.
al denotes adversarial learn-ing and iat denotes inverse adversarial training..analysis of dialogue responses generated by differ-ent compared models.
the samples are presentedin table 5. we can see that the vanilla transformer-based dialogue response generation model tendsto generate irrelevant and generic responses.
ap-plying the mmi objective for re-ranking success-fully avoids those generic responses.
however, itleads to another kind of non-informative responsethat repeats the majority of tokens in the latestutterance, which is also quite unnatural.
in con-trast, dialogue models trained with the proposedinverse adversarial training objective tend to gen-erate more diverse responses which are also morerelevant to the dialogue history..4.3 qualitative analysis.
4.4 ablation study.
to better compare and analyze the inverse adver-sarial training objective, we conduct a qualitative.
to better understand the relative importance of dif-ferent components in the proposed inverse adver-.
701sarial training objective, we conduct an ablationstudy with human evaluation to compare differ-ent model variants against the full model.
the re-sults are shown in table 6. we can ﬁnd that bothsupervised-only and self-supervised-only variantof the proposed inverse adversarial training algo-rithm can improve the consistency and the diver-sity of dialogue models.
however, self-supervisedinverse adversarial training seems to sacriﬁce theﬂuency of generated responses for better diver-sity and consistency as the model trained with-out the self-supervised objective are considered tobe more ﬂuent by human annotators.
the use-fulness of the reward and the penalty objectivesis also demonstrated by human evaluation.
con-cretely, we ﬁnd that the reward described in eq.
(3)contributes more to the diversity of generated re-sponses.
this may be because it assigns highrewards for relevant and speciﬁc responses andin con-negative rewards for generic responses.
trast, the penalty in eq.
(5) helps the dialogue sys-tem model dialogue history better and leads tomore consistent responses by punishing the dia-logue model when generating the same responsesgiven perturbed dialogue history.
as for differ-ent perturbation approaches, we ﬁnd that bothutterance-level and token-level contributes to theperformance improvements.
also, we ﬁnd thatutterance-level perturbation may be more effec-tive for improving the consistency of generated re-sponses.
we suspect this may be because the abil-ity of the dialogue model to distinguish utterance-level perturbation is more important for better di-alogue history modeling..5 conclusion.
in this work, we introduce inverse adversarialtraining (iat) algorithm that is able to simultane-ously reduce the dull response problem and helpneural dialogue systems model dialogue historyiat measures the relevance and consis-better.
tency of responses by the difference of their likeli-hood conditioning on either original and perturbeddialogue history.
in this way, it is able to pre-vent the dialogue system from preferring genericresponses, even they are often of high frequency inthe training corpora.
our method also encouragesthe dialogue agent to model dialogue history bet-ter by penalizing the model when generating thesame responses given perturbed dialogue history.
experimental results on two benchmark datasets.
show that the proposed inverse adversarial trainingalgorithm helps dialogue models capture dialoguehistory better and generate more diverse and con-sistent responses.
we also identify a limitation ofthe widely-used mmi based methods for improv-ing the diversity of dialogue response generationmodels and empirically demonstrate the existenceof this problem through our experimetns..boarder impact.
this work does not involve collection and releaseof data, nor inference of information or judgmentsabout individuals.
however, dialogue systemsmay have a social impact and we believe that mak-ing dialogue agent able to generate more mean-ingful and consistent responses are beneﬁcial.
wealso agree that general control on the bias or un-fairness of neural dialogue agents is important.
we believe this can be done from both the per-spective of data collection and training algorithms.
we believe our proposed training algorithm willlikely not contribute to any ethical concern of chatrobots..acknowledgments.
we thank the anonymous reviewers for their valu-able comments..references.
ashutosh baheti, alan ritter, jiwei li, and bill dolan.
2018. generating more interesting responses inneural conversation models with distributional con-straints.
in proceedings of the 2018 conference onempirical methods in natural language process-ing, pages 3970–3980, brussels, belgium.
associ-ation for computational linguistics..antoine bordes, y-lan boureau, and jason weston.
2016. learning end-to-end goal-oriented dialog.
arxiv preprint arxiv:1605.07683..rich´ard cs´aky, patrik purgai, and g´abor recski.
2019. improving neural conversational models withentropy-based data ﬁltering.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5650–5669, florence,italy.
association for computational linguistics..shaobo cui, rongzhong lian, di jiang, yuanfengsong, siqi bao, and yong jiang.
2019. dal: dualadversarial learning for dialogue generation..ashutosh kumar, kabir ahuja, raghuram vadapalli,syntax-guided con-transactions.
and partha talukdar.
2020.trolled generation of paraphrases..702of the association for computational linguistics,8:330–345..jiwei li, michel galley, chris brockett, jianfeng gao,and bill dolan.
2015. a diversity-promoting objec-tive function for neural conversation models.
arxivpreprint arxiv:1510.03055..jiwei li, will monroe, tianlin shi, s´ebastien jean,alan ritter, and dan jurafsky.
2017a.
adversar-ial learning for neural dialogue generation.
arxivpreprint arxiv:1701.06547..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017b.
dailydialog: a man-arxivually labelled multi-turn dialogue dataset.
preprint arxiv:1710.03957..pierre lison and j¨org tiedemann.
2016. opensub-titles2016: extracting large parallel corpora frommovie and tv subtitles..chia-wei liu, ryan lowe, iulian v serban, michaelnoseworthy, laurent charlin, and joelle pineau.
2016. how not to evaluate your dialogue system:an empirical study of unsupervised evaluation met-rics for dialogue response generation.
arxiv preprintarxiv:1603.08023..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-in advances in neural information processingity.
systems, pages 3111–3119..mohammad norouzi, samy bengio, navdeep jaitly,mike schuster, yonghui wu, dale schuurmans,et al.
2016. reward augmented maximum likeli-hood for neural structured prediction.
in advancesin neural information processing systems, pages1723–1731..chinnadhurai sankar, sandeep subramanian, christo-pher pal, sarath chandar, and yoshua bengio.
2019.do neural dialog systems use the conversation his-tory effectively?
an empirical study.
arxiv preprintarxiv:1906.01603..iulian v serban, alessandro sordoni, yoshua bengio,aaron courville, and joelle pineau.
2016. buildingend-to-end dialogue systems using generative hier-archical neural network models.
in thirtieth aaaiconference on artiﬁcial intelligence..alessandro sordoni, michel galley, michael auli,chris brockett, yangfeng ji, margaret mitchell,jian-yun nie, jianfeng gao, and bill dolan.
2015.a neural network approach to context-sensitive gen-eration of conversational responses.
arxiv preprintarxiv:1506.06714..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural net-works.
in advances in neural information process-ing systems, pages 3104–3112..richard s sutton, david a mcallester, satinder psingh, and yishay mansour.
2000. policy gradi-ent methods for reinforcement learning with func-tion approximation.
in advances in neural informa-tion processing systems, pages 1057–1063..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..sixing wu, ying li, dawei zhang, yang zhou, andzhonghai wu.
2020. diverse and informative di-alogue generation with context-speciﬁc common-sense knowledge awareness.
in acl, pages 5811–5820. association for computational linguistics..jingjing xu, xuancheng ren,.
junyang lin, andxu sun.
2018. diversity-promoting gan: a cross-entropy based generative adversarial network for di-versiﬁed text generation.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, pages 3940–3949..saizheng zhang, emily dinan, jack urbanek, arthurszlam, douwe kiela, and jason weston.
2018a.
personalizing dialogue agents: i have a dog, do youhave pets too?
arxiv preprint arxiv:1801.07243..yizhe zhang, michel galley, jianfeng gao, zhe gan,xiujun li, chris brockett, and bill dolan.
2018b.
generating informative and diverse conversationalresponses via adversarial information maximization.
in s. bengio, h. wallach, h. larochelle, k. grau-man, n. cesa-bianchi, and r. garnett, editors, ad-vances in neural information processing systems31, pages 1810–1820.
curran associates, inc..tiancheng zhao, ran zhao, and maxine eskenazi.
2017. learning discourse-level diversity for neuraldialog models using conditional variational autoen-in proceedings of the 55th annual meet-coders.
ing of the association for computational linguis-tics (volume 1: long papers), pages 654–664, van-couver, canada.
association for computational lin-guistics..wangchunshu zhou, tao ge, ke xu, furu wei, andming zhou.
2019. bert-based lexical substitution.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages3368–3373..wangchunshu zhou, tao ge, ke xu, furu wei, andming zhou.
2020. self-adversarial learning withcomparative discrimination for text generation.
in8th international conference on learning represen-tations, iclr 2020, addis ababa, ethiopia, april26-30, 2020. openreview.net..wangchunshu zhou and ke xu.
2020. learning tocompare for better training and evaluation of opendomain natural language generation models.
inaaai, pages 9717–9724.
aaai press..703