learnda: learnable knowledge-guided data augmentation forevent causality identiﬁcation.
xinyu zuo1,2, pengfei cao1,2, yubo chen1,2, kang liu1,2, jun zhao1,2,weihua peng3 and yuguang chen31national laboratory of pattern recognition, institute of automation, cas, beijing, china2school of artiﬁcial intelligence, university of chinese academy of sciences, beijing, china3beijing baidu netcom science technology co., ltd{xinyu.zuo,pengfei.cao,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn{pengweihua,chenyuguang}@baidu.com.
abstract.
modern models for event causality identiﬁca-tion (eci) are mainly based on supervisedlearning, which are prone to the data lack-ing problem.
unfortunately, the existing nlp-related augmentation methods cannot directlyproduce available data required for this task.
to solve the data lacking problem, we intro-duce a new approach to augment training datafor event causality identiﬁcation, by iterativelygenerating new examples and classifying eventcausality in a dual learning framework.
on theone hand, our approach is knowledge guided,which can leverage existing knowledge basesto generate well-formed new sentences.
onthe other hand, our approach employs a dualmechanism, which is a learnable augmenta-tion framework, and can interactively adjustthe generation process to generate task-relatedsentences.
experimental results on two bench-marks eventstoryline and causal-timebankshow that 1) our method can augment suit-able task-related training data for eci; 2)our method outperforms previous methods oneventstoryline and causal-timebank (+2.5and +2.1 points on f1 value respectively)..1.introduction.
event causality identiﬁcation (eci) aims to iden-tify causal relations between events in texts, whichcan provide crucial clues for nlp tasks, such aslogical reasoning and question answering (girju,2003; oh et al., 2013, 2017).
this task is usuallymodeled as a classiﬁcation problem, i.e.
determin-ing whether there is a causal relation between twoevents in a sentence.
for example in figure 1, aneci system should identify two causal relations intwo sentences: (1) attack cause−→ killed in s1; (2)statement cause−→ protests in s2..most existing methods for eci heavily rely onannotated training data (mirza and tonelli, 2016;.
figure 1: s1 and s2 are causal sentences that containcausal events.
s3 is produced by eda based on s1.
the dotted line indicates the causal relation..riaz and girju, 2014b; hashimoto et al., 2014; huand walker, 2017; gao et al., 2019).
however, ex-isting datasets are relatively small, which impedethe training of the high-performance event causalityreasoning model.
according to our statistics, thelargest widely used dataset eventstoryline corpus(caselli and vossen, 2017) only contains 258 docu-ments, 4316 sentences, and 1770 causal event pairs.
therefore, data lacking is an essential problem thaturgently needs to be addressed for eci..up to now, data augmentation is one of the mosteffective methods to solve the data lacking problem.
however, most of the nlp-related augmentationmethods are a task-independent framework that pro-duces new data at one time (zhang et al., 2015; guoet al., 2019; xie et al., 2019b).
in these frameworks,data augmentation and target task are modeled inde-pendently.
this often leads to a lack of task-relatedcharacteristics in the generated data, such as task-related linguistic expression and knowledge.
forexample, easy data augmentation (eda) (wei andzou, 2019) is the most representative method thatrelies on lexical substitution, deletion, swapping,and insertion to produce new data.
however, solelyrelying on such word operations often generatesnew data that dissatisﬁes task-related qualities.
asshown in figure 1, s3 is produced by eda, it lacksa linguistic expression that expresses the causal se-mantics between kill and attack.
therefore, how to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3558–3571august1–6,2021.©2021associationforcomputationallinguistics3558kimani gray, a young man who likes football, was killed in a police attack shortly after a tight match.in the week following the fatal violence, several protests have erupted because of the official statement.s1:s2:kimani gray, a young man who likes football, was killed in a police attack shortly after a tight match.eda  deletions3:interactively model data augmentation and targettask to generate new data with task-related charac-teristics is a challenging problem on eci..speciﬁc to eci, we argue that an ideal task-related generated causal sentence needs to possesstwo characteristics as follows.
(1) the two eventsin the causal sentence need to have a causal re-lation.
we call such property as causality.
forexample, there is usually a causal relation betweenan attack event and a kill event, while nearly nocausal relation between an attack event and a bornevent.
(2) the linguistic expressions of the causalsentence need to be well-formed to express thecausal semantic of events.
we call such property aswell-formedness, which consists of a) canonicalsentence grammar, b) event-related entities withsemantic roles (e.g.
the attack was carried out by apolice in s1), and c) cohesive words that expressin a and othercomplete causal semantics (e.g.
words except for events and entities in s1)..to this end, we propose a learnable dataaugmentation framework for eci, dubbed aslearnable knowledge-guided data augmentation(learnda).
this framework regards sentence-to-relation mapping (the target task, eci) and relation-to-sentence mapping (the augmentation task, sen-tence generation) as dual tasks and models themutual relation between them via dual learning.
speciﬁcally, learnda can use the duality to gener-ate task-related new sentences learning from iden-tiﬁcation and makes it more accurate to understandthe causal semantic learning from generation.
onthe one hand, learnda is knowledge guided.
itintroduces diverse causal event pairs from kbs toinitialize the dual generation which could ensurethe causality of generated causal sentences.
forexample, the knowledge of judgment cause−→ demon-stration from kbs can be used to construct a novelcausal sentence, which is also helpful to understandthe causal semantic of statement cause−→ protests.
onthe other hand, learnda is learnable.
it employsa constrained generative architecture to generatewell-formed linguistic expressions via iterativelylearning in the dual interaction, which expressesthe causal semantic between given events.
method-ologically, it gradually ﬁlls the remaining missingcohesive words of the complete sentences underthe constraint of given events and related entities..in experiments, we evaluate our model on twobenchmarks.
we ﬁrst concern the standard evalua-tion and show that our model achieves the state-of-.
the-art performance on eci.
then we estimate themain components of learnda.
finally, our learn-able augmentation framework demonstrates deﬁ-nite advantages over other augmentation methodsin generating task-related data for eci..in summary, the contributions as follows:.
• we propose a new learnable data augmenta-tion framework to solve the data lacking prob-lem of eci.
our framework can leverage theduality between identiﬁcation and generationvia dual learning which can learn to generatetask-related sentences for eci..• our framework is knowledge guided andlearnable.
speciﬁcally, we introduce causalevent pairs from kbs to initialize the dual gen-eration, which could ensure the causality ofgenerated causal sentences.
we also employa constrained generative architecture to grad-ually generate well-formed causal linguisticexpressions of generated causal sentences viaiteratively learning in the dual interaction..• experimental results on two benchmarks showthat our model achieves the best performanceon eci.
moreover, it also shows deﬁnite ad-vantages over previous data augmentationmethods..2 related work.
to date, many researches attempt to identify thecausality with linguistic patterns or statistical fea-tures.
for example, some methods rely on syn-tactic and lexical features (riaz and girju, 2013,2014b).
some focus on explicit causal textualpatterns (hashimoto et al., 2014; riaz and girju,2014a, 2010; do et al., 2011; hidey and mckeown,2016).
and some others pay attention on statisti-cal causal association and cues (beamer and girju,2009; hu et al., 2017; hu and walker, 2017)..recently, more attention is paid to the causalitybetween events.
mirza and tonelli (2014) anno-tated causal-timebank of event-causal relationsbased on the tempeval-3 corpus.
mirza et al.
(2014), mirza and tonelli (2016) extracted event-causal relation with a rule-based multi-sieve ap-proach and improved the performance incorporat-ing with event temporal relation.
mostafazadehet al.
(2016) annotated both temporal and causalrelations in 320 short stories.
caselli and vossen(2017) annotated the eventstoryline corpus for.
3559figure 2: overview of the learnable knowledge-guideddual data augmentation for eci..event causality identiﬁcation.
dunietz et al.
(2017)presented because 2.0, a new version of the be-cause corpus (dunietz et al., 2015) of causalrelation and other seven relations.
gao et al.
(2019) modeled document-level structures to iden-tify causality.
liu et al.
(2020) identiﬁed eventcausality with the mention masking generalization.
unlike computer vision, the augmentation of textdata in nlp is pretty rare (chaudhary, 2020).
zuoet al.
(2020) solved the data lacking problem of eciwith the distantly supervised labeled training data.
however, including the distant supervision, mostof the existing data augmentation methods for nlptasks are task-independent frameworks (relatedwork of data augmentation and dual learning aredetailed in appendix b).
inspired by some genera-tive methods which try to generate additional train-ing data while preserving the class label (anaby-tavor et al., 2019; yang et al., 2019; papanikolaouand pierleoni, 2020), we introduce a new learn-able framework for augmenting task-related train-ing data for eci via dual learning enhanced withexternal knowledge..3 methodology.
as shown in figure 2, learnda jointly models aknowledge guided sentence generator (input: eventpair and its causal/non-causal relation, output:causal/non-causal sentence) and an event causalityidentiﬁer (input: event pair and its sentence, out-put: causal/non-causal relation) with dual learning.
learnda iteratively optimizes identiﬁer and gener-ator to generate task-related training data, and thenutilize new data to further train the identiﬁer.
there-fore, we ﬁrst present the main idea of dual learning,which is the architecture of learnable dual augmen-tation, including the states, actions, policies, and.
figure 3: the architecture of learnable dual augmen-tation.
causal and ncausal represent the causal andnon-causal sentence generator respectively.
red partsare the process of <event pair, relation> → sentence→ relation (primal cycle), while blue parts are the pro-cess of <event pair, sentence> → relation → sentence(dual cycle).
solid and dashed lines denote the mainprocess and reward feedback direction respectively..rewards.
then, we brieﬂy introduce the knowledgeguided sentence generator, especially the processesof knowledge guiding and constrained sentencegeneration.
finally, we describe the event causalityidentiﬁer and training processes of learnda..3.1 architecture of learnable dual.
augmentation.
the architecture of learnable dual augmentationis shown in figure 3. speciﬁcally, i denotes theevent causality identiﬁer, and g denotes the sen-tence generator which consists of two independentgenerators.
they produce causal and non-causalsentences on the relation c of input event pair ep..generally, g generates a sentence s(cid:48) which ex-presses the causal or non-causal relation c of theinput event pair ep.
then it receives the rewardr that consists of a semantic alignment rewardrs from itself and a causality reward rc from i(primal cycle).
similarly, i identiﬁes the causalor non-causal relation c(cid:48) of the input event pair epwith its sentence s. then it receives the reward rconsists of a causality reward rc from itself and asemantic alignment reward rs from g (dual cycle).
i and g are optimized interactively with dual re-inforcement learning.
speciﬁcally, for g, an actionis the generation from relation to sentence, a stateis denoted by the representation of input event pairand its relation, a policy is deﬁned by the param-eters of generator.
for i, an action is the identiﬁ-cation from sentence to relation, a state is denotedby the representation of input event pair and its.
3560dual cycleprimal cyclecausal/non-causalrelaitoncausal/non-causalsentenceeventpairannotated    datapre-trained identifierpre-trained generatorlearnable dual augmentation architecturedual-trained  identifierdual augmented        datafull-trained identifierpre-trainingfurthertrainingknowledgerelation->sentence->relationsentence->relation->sentenceidentifierrelation→sentencencausal-generatorcausal-generatorsentence→relationevent pair (ep)causal/non-causal relation (c)ep, s'event pair (ep) sentence (s)ep, c'rsrcrcrsprimal cycledual cyclerrigsentence, a policy is deﬁned by the parameters ofidentiﬁer.
inspired by shen and feng (2020), weutilize a probability distribution over actions givenstates to represent the policys, i.e., the probabilitydistribution of the generation of g and identiﬁca-tion of i. as aforementioned, we introduce tworewards, causality (rc) and semantic alignment(rs) rewards, which encourage g to generate task-related sentences with the feedback from identiﬁer,while further optimize i with the feedback fromgenerator.
deﬁnitions are as following:.
causality reward (rc)if the relation of inputevent pair can be clearly expressed by the gener-ated sentence, it will be easier to be understoodby identiﬁer.
therefore, we use the causal relationclassiﬁcation accuracy as the causality reward toevaluate the causality of generated sentences, whiletune and optimize the identiﬁer itself:p(c(cid:48)|s; θi )−p(c(cid:48)|s; θi ) otherwise,where θi is the parameter of i, p(c(cid:48)|s; θi ) denotesthe probability of relation classiﬁcation, s denotesthe input sentence and c(cid:48) is the classiﬁed relation..correct classiﬁcation.
rc(ep, s) =.
(1).
(cid:40).
semantic alignment reward (rs) we hopethat the semantic of the generated sentence canbe consistent with the relation of the input eventpair.
additionally, if the relation of the input eventpair can be more accurately classiﬁed, the semanticof the new generated sentence can be consideredmore consistent with it.
therefore, we measure thesemantic alignment by means of the probability ofconstructing a sentence with similar semantic tothe input relation, and the reward is:.
where θg is the parameter of g, c is the input re-lation, t is one of the generated tokens ts of thegenerated sentence s(cid:48), and p(t|c; θg) is the gener-ated probability of t. speciﬁcally, there are twoindependent g with different θg.
in detail, θcg isemployed to generated causal sentence when theinput c is causal relation, and non-causal sentenceis generated via θncg when c is non-causal relation..3.2 knowledge guided sentence generator.
as shown in figure 4, knowledge guided sentencegenerator (ksg) ﬁrst introduces diverse causaland non-causal event pairs from kbs for causal-ity.
then, given an event pair and its causal ornon-causal relation, it employs a constrained gen-.
figure 4: flow diagram of the knowledge guided sen-tence generator (ksg).
we take causal sentence gener-ation via lexical knowledge expanding as an example..erative architecture to generate new well-formedcausal/non-causal sentences that contain them..knowledge guiding ksg introduces eventpairs that are probabilistic causal or non-causalfrom multiple knowledge bases in two ways.
(1)lexical knowledge expanding: expanding anno-tated event pairs via external dictionaries, suchas wordnet (miller, 1995) and verbnet (schuler,2005).
(2) connective knowledge introducing: in-troducing event pairs from external event-annotateddocuments (kbp corpus) assisted with framenet(baker et al., 1998) and penn discourse treebank(pdtb2) (group et al., 2008).
as shown in ta-ble 1, we illustrate how to extract event pairs frommultiple knowledge bases.
then, inspired by bor-des et al.
(2013), we ﬁlter the extracted event pairsby converting them into triples <ei, causal/non-causal, ej> and calculating the causal-distance bymaximizing l in a causal representation space:.
(cid:88).
(cid:88).
l =.
[λ + d(e(cid:48).
i, e(cid:48).
j) − d(ei, ej)]+, (3).
where t and t (cid:48) are the causal and non-causaltriples set respectively, and e is the representationof event.
after that, the higher probability of causalrelation, the shorter distance between two events,and we sort event pairs in ascending order by theirdistances.
finally, we keep the top and bottomα% sorted event pairs to obtain the causal and non-causal event pairs sets for generation..constrained sentence generator given anevent pair, constrained sentence generator producesa well-formed sentence that expresses its causal ornon-causal relation in three stages: (1) assigningevent-related entities ensures the logic of the se-mantic roles of events, (2) completing sentencesensures the completeness of causal or non-causal.
rs(ep, c) = p(s(cid:48)|c; θg) =.
p(t|c; θg),.
(2).
(ei,ej )∈t.
(e(cid:48).
i,e(cid:48).
j )∈t (cid:48).
1|ts|.
(cid:88).
t∈ts.
3561ncausal-generatorcausal-generatorevent pair: <hurt,onrush>relation: causalknowledgekimani gray, a young man who likes football, waskilled in a police attack shortly after a tight match.event pair: <killed,attack>relation: causaljohn henderson who is a baseball fanatic,  washurt in a gang onrush before friday’s game.generated  sentence:original sentence:words:eventswords:entitieswords:cohesive             wordsknowledge.
how to extract event pair.
why causal or non-causal.
wordnet.
verbnet.
e.g..framenetpdtb2.
e.g..lexical knowledge expanding.
1) extracting the synonyms and hypernyms from wordnet of each eventin ep.
2) assembling the items from the two groups of two events togenerate causal/non-causal event pairs.
1) extracting the words from verbnet under the same class as each eventin ep.
2) assembling the items from the two groups of two events togenerate causal/non-causal event pairs..items in each group are the synonyms andhypernyms of the annotated causal/non-causal event pairs..items in each group are in the same class ofthe annotated causal/non-causal event pairs..< (killed, attack), causal >=⇒ kill−→ onrush =⇒< (hurt, onrush), causal >original sentence: kimani gray, a young man who likes football, was killed in a police attack shortly after a tight match..−→ hurt, attack.
synonyms.
synonyms.
connective knowledge introducing1) extracting causal/non-causal connectives from framenet1 andpdtb2.
2) extracting any two events connected by causal/non-causalconnectives on kbp corpus to obtain causal/non-causal event pairs andoriginal sentences respectively..introduced event pairs are connected bycausal/non-causal connectives..looting because someone beat up someone, like the travon martin case.
because=⇒ < (loot, beat up), causal >original sentence: looting because someone beat up someone, like the travon martin case..table 1: extracting causal and non-causal event pairs from multiple knowledge bases..semantic expression, (3) ﬁltering sentences ensuresthe quality and diversity of generated sentences..assigning event-related entities.
event relatedentities play different semantic roles of events insentences, which is an important part of event-semantic expression.
hence, as shown in figure 4,given an event pair, we ﬁrstly assign logical entitiesfor input events to guarantee the logic of semanticroles in the new sentences, such as gang is a logicalentity as the body of the event onrush.
logically,entities of the same type play the same semanticroles in similar events.
moreover, as shown in ta-ble 1, there is a corresponding original sentencefor each extracted event pair.
therefore, in newsentence, we assign the most similar entity in thesame type from candidate set2 for each entity inthe original sentence.
for example, we assign gangfor onrush in new sentence which is similar withthe police related to attack in the original sentence.
speciﬁcally, we put the candidate entities in thesame position in the original sentence to obtaintheir bert embeddings.
then we select entitiesvia the cosine similarity between their embeddings:e(ent) = 1w∈ent e(w), where ent is the en-|ent|tity and e(w) is the bert embedding of ent..(cid:80).
completing sentences.
a well-formed sentencerequires a complete linguistic expression to expressthe causal or non-causal semantics.
therefore, wecomplete sentences by ﬁlling the cohesive wordsbetween given events and assigned entities withmasked bert (devlin et al., 2019).
all wordsexcept events and entities are regarded as cohesivewords.
speciﬁcally, we insert a certain numberof the special token [mask] between events and.
entities, and then predict the [mask]3 tokens asnew words.
as shown in figure 4, we ﬁll cohesivetokens via two independent generators to expresscausal and non-causal semantic according to therelation of given events.
for example, in a guidinga causal semantic ﬁlled by the causal generator..(cid:80).
|t (s(cid:48))|.
filtering sentences..inspired by yang et al.
(2019), we design a ﬁlter to select new sentencesthat are balanced between high quality and high di-versity with two key factors: 1) perplexity (ppl):we take the average probability of the ﬁlled cohe-sive words in the new sentence s(cid:48) as its perplexity:p p l(s(cid:48)) = 1t∈t (s(cid:48)) p (t), where t is theset of ﬁlled cohesive words.
2) distance (dis):we calculate the cosine similarity between gener-ated sentence s(cid:48) and annotated data dm as its dis-e(s(cid:48))·e(s)tance: dis(s(cid:48), dm) = 1e(s(cid:48))×e(s) ,where dm is m random selected annotated sen-tences and e is the bert sentence representationof the [cls] token.
a new sentence should haveboth appropriate high ppl which indicates thequality of generation, and appropriate high diswhich indicates the difference from the originalsentences.
therefore, we select the top β% ofthe newly generated sentences according to scorefor the further training of identiﬁer as following:score(s(cid:48)) = µp p l(s(cid:48)) + (1 − µ)dis(s(cid:48), dm)),where the µ is an hyper-parameter..s∈dm.
|dm|.
(cid:80).
3.3 training of learnda for eci.
we brieﬂy describe the training processes oflearnda for eci, including the pre-training of gen-erator and identiﬁer, the dual reinforcement train-ing, and the further training of identiﬁer..2we collect entities from annotated data and kbp corpus..between events and entities in the original sentence..3the inserted [mask] is 1.2 times the number of words.
3562algorithm 1 dual reinforcement training of g i..functions:.
i)..rs.
∇g+ = rs.
i of epi;i of epi;.
primal · ∇θg lg(epi, ci)..computing the stochastic gradient of θg:.
primal = λrs(epi, ci) + (1 − λ)rc(epi, s(cid:48).
end formodel batch updates: θg ← θg + η · ∇g.
for event pair (epi, si, ci) in batch dogenerator generates the sentence s(cid:48)identiﬁer re-predicts the causality c∗computing the reward as:.
require: a set of knowledge guided event pairs {(ep,s,c)}a pre-trained generator g and identiﬁer irepeat: early stop on the development set according to i.
1: loop: primal cycle2:3:4:5:6:7:8:9:10:11: end loop:12:13: loop: dual cycle14:15:16:17:18:19:20:21:22:23: end loop:.
for event pair (epi, si, ci) in batch doidentiﬁer predicts the causality c(cid:48)generator re-generates the sentence s∗computing the reward as:.
end formodel batch updates: θi ← θi + η · ∇i.
dual = γrc(epi, si) + (1 − γ)rs(epi, c(cid:48).
computing the stochastic gradient of θi:.
dual · ∇θi li (epi, si)..∇i+ = rs.
i of epi;.
i of epi;.
rs.
i)..event causality identiﬁer first of all, we for-mulate event causality identiﬁcation as a sentence-level binary classiﬁcation problem.
speciﬁcally,we design a classiﬁer based on bert (devlin et al.,2019) to build our identiﬁer.
the input of the iden-tiﬁer is the event pair ep and its sentence s. next,we take the stitching of manually designed features(same lexical, causal potential, and syntactic fea-tures as gao et al.
(2019)) and two event representa-tions as the input of top mlp classiﬁer.
finally, theoutput is a binary vector to predict the causal/non-causal relation of the input event pair ep..pre-training we pre-train the identiﬁer and gen-erator on labeled data before dual reinforcementtraining.
on the one hand, we train identiﬁer viathe cross-entropy objective function of the relationclassiﬁcation.
on the other hand, for generators, wekeep the events and entities in the input sentences,replace the remaining tokens with a special token[mask], and then train it via the cross-entropyobjective function to re-predict the masked tokens.
speciﬁcally, causal generator and non-causal gen-erator are pre-trained on causal and non-causal la-beled sentences respectively..lg(ep, c) =.
(cid:40).
p(s(cid:48)|c; θg) = 1|ts|p(s(cid:48)|c; θn g) = 1|ts|.
(cid:80).
t∈ts(cid:80).
t∈ts.
p(t|c; θg).
p(t|c; θn g),.
(4).
(5).
li (ep, s) = p(c(cid:48)|s; θi ),.
where θg and θn g is the parameters of causal andnon-causal sentence generators respectively, ts isthe masked tokens.
finally, after dual data aug-mentation, we utilize generated sentences to fur-ther train the dual-trained identiﬁer via the cross-entropy objective function of relation classiﬁcation..4 experiments.
4.1 experimental setup.
dataset and evaluation metrics our experi-ments are conducted on two main benchmarkdatasets, including: eventstoryline v0.9 (esc)(caselli and vossen, 2017) described above; and(2) causal-timebank (causal-tb) (mirza andtonelli, 2014) which contains 184 documents, 6813events, and 318 causal event pairs.
same as pre-vious methods, we use the last two topics of escas the development set for two datasets.
for eval-uation, we adopt precision (p), recall (r), andf1-score (f1) as evaluation metrics.
we conduct5-fold and 10-fold cross-validation on esc andcausal-tb respectively, same as previous meth-ods to ensure comparability.
all the results are theaverage of three independent experiments..parameters settingsin implementations, boththe identiﬁer and generators are implemented onbert-base architecture4, which has 12-layers,768-hiddens, and 12-heads.
we set the learn-ing rate of generator pre-training, identiﬁer pre-training/further training, and dual reinforcementtraining as 1e-5, 1e-5, and 1e-7 respectively.
weset the ratio of the augmented data used for trainingto the labeled data, α, β, µ, λ and γ as 1:2, 30%,50%, 0.2, 0.5 and 0.5 respectively tuned on the de-velopment set.
and we apply early stop and sgdgradient strategy to optimize all models.
we alsoadopt a negative sampling rate of 0.5 for trainingthe identiﬁer, owing to the sparseness of positiveexamples.
(see appendix d for more details.).
compared methods same as previous state-of-the-art work.
for esc, we prefer 1) lstm(cheng and miyao, 2017), a dependency path based.
4https://github.com/google-research/.
dual reinforcement training as shown in al-gorithm 1, we interactively optimize the genera-tor and identiﬁer by dual reinforcement learning.
speciﬁcally, we maximize the following objective.
bert.
3563sequential model that models the context betweenevents to identify causality; 2) seq (choubey andhuang, 2017), a sequence model explores complexhuman designed features for eci; 3) lr+ and ilp(gao et al., 2019), document-level models adoptdocument structures for eci.
for causal-tb, weprefer 1) rb, a rule-based system; 2) dd, a datadriven machine learning based system; 3) vr-c, averb rule based model with data ﬁltering and goldcausal signals enhancement.
these models are de-signed by mirza and tonelli (2014); mirza (2014)for eci..owing to our methods are constructed on bert,we build bert-based methods: 1) bert, a bert-based baseline, our basic proposed event causalityidentiﬁer.
2) mm (liu et al., 2020), the bert-based sota method with mention masking gen-eralization.
3) mm+aug, the further re-trainedmm with our dual augmented data.
4) knowdis(zuo et al., 2020) improved the performance of eciwith the distantly labeled training data.
we com-pare with it to illustrate the quality of our generatedeci-related training data.
5) mm+conceptaug,to make a fair comparison, we introduce causal-related events from conceptnet that employed bymm, and generate new sentences via konwdis andlearnda to further re-train mm (see appendixc for details).
finally, we use learndaf ull in-dicates our full model, which is the dual-trainedidentiﬁer further trained via dual augmented data..4.2 our method vs. state-of-the-art methods.
table 2 shows the results of eci on eventstorylineand causal-timebank.
from the results:.
1) our learndaf ull outperforms all baselinesand achieves the best performance (52.6%/51.9%on f1 value), outperforming the no-bert (ilp/vr-c) and bert (mm/knowdis) state-of-the-art meth-ods by a margin of 7.9%/8.7% and 2.5%/2.1% re-spectively, which justiﬁes its effectiveness.
more-over, bert-based methods demonstrate high recallvalue, which is beneﬁted from more training dataand their event-related guided knowledge..2) comparing knowdis with learndaf ull, wenote that training data generated by learnda ismore helpful to eci than distant supervision withexternal knowledge (+2.9%/+2.1%).
this showsthat learnda can generate more eci-related data.
3) comparing mm+conceptn et with mm,with the same knowledge base, our dual aug-mented data can further improve the performance.
methods.
p.r.f1.
esc.
lstm (cheng and miyao, 2017)seq (choubey and huang, 2017)lr+ (gao et al., 2019)ilp (gao et al., 2019)bertknowdis (zuo et al., 2020)mm (liu et al., 2020)mm+conceptaug (ours)mm+aug (ours)learndaf ull (ours).
causal-tb.
rb (mirza and tonelli, 2014)dd (mirza and tonelli, 2014)vr-c (mirza, 2014)bertmm (liu et al., 2020)knowdis (zuo et al., 2020)mm+conceptaug (ours)mm+aug (ours)learndaf ull (ours).
34.032.737.037.436.139.741.941.241.042.2.
36.867.369.038.536.642.338.839.241.9.
41.544.945.255.856.066.562.566.569.369.8.
12.322.631.543.955.660.559.261.968.0.
37.437.840.744.743.949.750.150.9*51.5*52.6*.
18.433.943.241.044.149.846.9*48.0*51.9*.
table 2: results on event causality identiﬁcation.
* de-notes a signiﬁcant test at the level of 0.05..(+0.8%/+2.8%), which illustrates that learnda canmake more effective use of external knowledge bygenerating task-related training data..4) comparing mm+aug with mm, we note thattraining with our dual augmented data can improvethe performance by 1.4%/3.9%, even though mm isdesigned on bert-large (learnda is constructedon bert-base) and also introduces external knowl-edge.
this indicates that the augmented data gener-ated by our learnda can effectively alleviate theproblem of data lacking on the eci..4.3 effect of learnable dual augmentation.
we analyze the effect of the learnable dual aug-mentation for event causality identiﬁcation.
1) foridentiﬁer.
comparing learndadual with bert intable 3, we note that the performance of the pro-posed identiﬁer is improved (+2.6%) after the dualtraining only with the same labeled data.
this indi-cates that the identiﬁer can learn more informativeexpressions of causal semantic from generationwith dual learning.
2) for generator.
compar-ing bertdualaug with bertaug in table 3, wenote that the dual augmented data is high qualityand more helpful to eci (+2.6%).
this indicatesgenerator can generate more eci task-related datalearned from identiﬁer with dual learning..figure 5 illustrates the learnability of ourlearnda.
speciﬁcally, as the number of trainingrounds of dual learning increases, the generateddata gradually learns task-related information, fur-.
3564methodbert (our basic identiﬁer)bertorgaugbertdualauglearndaduallearndadualaug−w/o.kb−learndadualaug−w/.intro−learndadualaug−w/.verbnet−learndadualaug−w/.wordnetlearndaf ull.
p36.136.637.836.837.539.039.439.642.2.r56.059.765.663.067.066.066.767.669.8.f43.945.4*48.0*46.5*48.1*49.0*49.5*49.9*52.6*.
table 3: ablation results on event causality identiﬁca-tion on esc.
* denotes a signiﬁcant test at the levelof 0.05. bertorgaug and bertdualaug denote thebert is further trained on no-dual and dual augmenteddata respectively; learndadual denotes our identiﬁeris only trained by dual learning without further training;learndadualaug−w/o.kb denotes the learndadualis further trained by dual augmented data withoutknowledge guiding; learndadualaug−w/.<kb> de-notes learndadualis further trained by dual aug-mented data guided with knowledge base kb..figure 5: the impact of the training rounds of duallearning on event causality identiﬁcation on esc.
ineach round, we generate new training data by the gener-ator at the current round.
the performance is achievedby further training the identiﬁer at the current roundwith the aforementioned newly generated data..ther improving the performance accordingly..4.4 effect of knowledge guiding.
table 3 also illustrates the effect of knowledgeguiding on eci depending on different knowl-1) comparing learndaf ull withedge bases.
learndadualaug−w/o.kb, we note that the aug-mented data guided by external knowledge canfurther improve the performance of eci.
2) specif-ically, lexical expanding and connective introduc-ing (sec 3.2) can both make the representation ofcausal relation more generalized, further making iteasier for the identiﬁer to understand the causality.
3) moreover, the expanding is more effective thanthe introducing, because the former brings a widerrange of effective knowledge, thus the guidance of.
methodbert (our identiﬁer)textsurfacebertbacktranslationbertedabertlearndabert.
p36.137.036.836.637.8.r56.057.561.062.465.6.f43.945.0*45.9*46.1*48.0*.
table 4: results of different data augmentation meth-ods on event causality identiﬁcation on esc dataset.
*denotes a signiﬁcant test at the level of 0.05..causalitywell-formednessdiversity (man/auto).
gold3.803.950.0/1.0.
eda3.202.753.08/0.70.
backtrans learnda.
3.703.832.80/0.85.
3.603.643.51/0.66.
table 5: manual (4-score rating (0, 1, 2, 3)) andautomatic (bleu score) evaluation ofthe gener-ated sentences via different methods from causality,well-formedness and diversity.
causality and well-formedness are assessed manually, while diversity isassessed manually and automatically..causal-related knowledge is better..4.5 our augmentation vs. other nlp.
augmentations.
in this section, we conduct a comparison be-tween our augmentation framework and other nlp-related augmentation methods to further illustratethe effectiveness of learnda..effectiveness of our augmentation we trainour identiﬁer with augmented data produced bydifferent nlp-related augmentation methods.
asshown in table 4, the augmented data generated byour learnda is more efﬁcient for eci, which isconsistent with the previous analysis.
the learndacan generate well-formed task-related new sen-tences that contain more event causal knowledge.
speciﬁcally, 1) text surface transformation bringsa slight change to the labeled data, thus it has rel-atively little impact on eci; 2) back translationintroduces limited new causal expressions by trans-lation, thus it slightly increases the recall value oneci; 3) eda can introduce new expressions viasubstitution, but the augmented data is not canon-ical and cannot accurately express the causality,therefore, its impact on eci is also limited..quantitative evaluation of task-relevancewe select ﬁve ph.d. students majoring in nlpto manual score the 100 randomly selectedaugmented sentences given their correspondingoriginal sentences as reference (cohen’s kappa= 0.85).
furthermore, we calculate the bleu(papineni et al., 2002) value to further evaluate the.
3565ate task-related sentences for eci.
moreover, ourframework is knowledge guided and learnable.
ourmethod achieves state-of-the-art performance oneventstoryline and causal-timebank datasets..acknowledgments.
we thank anonymous reviewers for their insight-ful comments and suggestions.
this work is sup-ported by the national key research and devel-opment program of china (no.2018yfb1005100),the national natural science foundation of china(no.u1936207, 61806201).
this work is also sup-ported by beijing academy of artiﬁcial intelli-gence (baai2019qn0301) and the joint projectwith beijing baidu netcom science technologyco., ltd..references.
ateret anaby-tavor, boaz carmeli, esther goldbraich,amir kantor, george kour, segev shlomov, naamatepper, and naama zwerdling.
2019. not enougharxiv,data?
abs/1911.03118..deep learning to the rescue!.
collin f. baker, charles j. fillmore, and john b. lowe.
1998. the berkeley framenet project.
in 36th an-nual meeting of the association for computationallinguistics and 17th international conference oncomputational linguistics, volume 1, pages 86–90,montreal, quebec, canada.
association for compu-tational linguistics..brandon beamer and roxana girju.
2009. using a bi-gram event model to predict causal potential.
in in-ternational conference on intelligent text process-ing and computational linguistics, pages 430–441.
springer..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-relational data.
in advances in neural informationprocessing systems, pages 2787–2795..ruisheng cao, su zhu, chen liu, jieyu li, and kai yu.
2019. semantic parsing with dual learning.
pages51–64..ruisheng cao, su zhu, chenyu yang, chen liu, raoma, yanbin zhao, lu chen, and kai yu.
2020. un-supervised dual paraphrasing for two-stage semanticin proceedings of the 58th annual meet-parsing.
ing of the association for computational linguistics,pages 6806–6817, online.
association for computa-tional linguistics..tommaso caselli and piek vossen.
2017. the eventstoryline corpus: a new benchmark for causal andtemporal relation extraction.
in proceedings of the.
figure 6: the modiﬁcation of dual learning..diversity.
as aforementioned, the task-relevance ofnew sentences on eci is manifested in causalityand well-formedness, while the diversity indicatesthe degree of generalization.
as shown in table5, we note the sentences generated by learndaare equipped with the above three properties thatare close to the labeled sentences.
speciﬁcally,the sentences produced by eda has a certaindegree of causality and diversity due to the lexicalsubstitution assisted by external knowledge.
how-ever, they cannot well express the causality due tothe grammatical irregularities.
correspondingly,new sentences generated via back translation arevery similar to the original sentences, while thediversity is poor..4.6 case study.
we conduct a case study to further investigate theeffectiveness of our learnda.
figure 6 illustratesthe modiﬁcation process of dual learning.
for ex-ample as a), given two causal events, the generatoris expected to generate a causal sentence.
however,the generator without dual learning produces a non-causal sentence.
fortunately, with dual learning,the identiﬁer judges the generated sentence as anon-causal one and guides the generator to producea causal sentence with the feedback.
similarly, asshown in b), given a causal sentence, the identi-ﬁer is expected to output a causal relation, but nodual-trained one cannot do.
correspondingly, thegenerator constructs feedback of low conﬁdence toguide the identiﬁer to output a causal relation..5 conclusion.
this paper proposes a new learnable knowledge-guided data augmentation framework (learnda)to solve the data lacking problem on eci.
ourframework can leverage the duality between gener-ation and identiﬁcation via dual learning to gener-.
3566generatoridentifier<crash, target>causal relationa was crash by b as c targeted ...non-causal relationa was crash by b because c targeted ...generatoridentifier           <order, attack>... a ordered b to attack ...non-causal  relation... a order when    b attack ...causal relation   dual rewardfeedback   dual rewardfeedbacka)b)events and stories in the news workshop, pages 77–86, vancouver, canada.
association for computa-tional linguistics..amit chaudhary.
2020. a visual survey of data aug-.
mentation in nlp..yubo chen, shulin liu, xiang zhang, kang liu, andjun zhao.
2017. automatically labeled data genera-tion for large scale event extraction.
in proceedingsof the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 409–419, vancouver, canada.
association forcomputational linguistics..fei cheng and yusuke miyao.
2017. classifying tem-poral relations by bidirectional lstm over depen-in proceedings of the 55th annualdency paths.
meeting of the association for computational lin-guistics (volume 2: short papers), pages 1–6, van-couver, canada.
association for computational lin-guistics..prafulla kumar choubey and ruihong huang.
2017. asequential model for classifying temporal relationsbetween intra-sentence events.
pages 1796–1802..claude coulombe.
2018..text data augmentationmade simple by leveraging nlp cloud apis.
arxiv,abs/1812.04718..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..quang do, yee seng chan, and dan roth.
2011. min-imally supervised event causality identiﬁcation.
inproceedings of the 2011 conference on empiricalmethods in natural language processing, pages294–303, edinburgh, scotland, uk.
association forcomputational linguistics..jesse dunietz, lori levin, and jaime carbonell.
2015.annotating causal language using corpus lexicogra-phy of constructions.
in proceedings of the 9th lin-guistic annotation workshop, pages 188–196, den-ver, colorado, usa.
association for computationallinguistics..jesse dunietz, lori levin, and jaime carbonell.
2017.the because corpus 2.0: annotating causality andin proceedings of the 11thoverlapping relations.
linguistic annotation workshop, pages 95–104, va-lencia, spain.
association for computational lin-guistics..lei gao, prafulla kumar choubey, and ruihonghuang.
2019. modeling document-level causalstructures for event causal relation identiﬁcation.
inproceedings of the 2019 conference of the north.
american chapter of the association for compu-tational linguistics: human language technolo-gies, volume 1 (long and short papers), pages1808–1817, minneapolis, minnesota.
associationfor computational linguistics..roxana girju.
2003. automatic detection of causalin proceedingsrelations for question answering.
of the acl 2003 workshop on multilingual summa-rization and question answering, pages 76–83, sap-poro, japan.
association for computational linguis-tics..pdtb research group et al.
2008. the pdtb 2.0. an-notation manual.
technical report ircs-08-01, in-stitute for research in cognitive science, universityof pennsylvania..hongyu guo, yongyi mao, and richong zhang.
2019.augmenting data with mixup for sentence classiﬁca-tion: an empirical study.
arxiv, abs/1905.08941..chikara hashimoto, kentaro torisawa, julien kloetzer,motoki sano, istv´an varga, jong-hoon oh, and yu-taka kidawara.
2014. toward future scenario gener-ation: extracting event causality exploiting semanticin pro-relation, context, and association features.
ceedings of the 52nd annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 987–997, baltimore, maryland.
as-sociation for computational linguistics..christopher hidey and kathy mckeown.
2016. identi-fying causal relations using parallel wikipedia arti-cles.
in proceedings of the 54th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1424–1433, berlin, ger-many.
association for computational linguistics..zhichao hu, elahe rahimtoroghi, and marilyn walker.
2017. inference of ﬁne-grained event causality fromblogs and ﬁlms.
pages 52–58..zhichao hu and marilyn walker.
2017. inferring nar-rative causality between event pairs in ﬁlms.
pages342–351..jian liu, yubo chen, and jun zhao.
2020. knowledgeenhanced event causality identiﬁcation with mentionmasking generalizations.
in ijcai-20, pages 3608–3614. international joint conferences on artiﬁcialintelligence organization.
main track..george a miller.
1995. wordnet: a lexical database forenglish.
communications of the acm, 38(11):39–41..paramita mirza.
2014. extracting temporal and causal.
relations between events.
pages 10–17..paramita mirza, rachele sprugnoli, sara tonelli, andmanuela speranza.
2014. annotating causalityin proceedings ofin the tempeval-3 corpus.
the eacl 2014 workshop on computational ap-proaches to causality in language (catocl), pages10–19, gothenburg, sweden.
association for com-putational linguistics..3567paramita mirza and sara tonelli.
2014. an analy-sis of causality between events and its relation toin proceedings of colingtemporal information.
2014, the 25th international conference on compu-tational linguistics: technical papers, pages 2097–2106, dublin, ireland.
dublin city university andassociation for computational linguistics..mehwish riaz and roxana girju.
2014a.
in-depth ex-ploitation of noun and verb semantics to identify cau-in proceedings of thesation in verb-noun pairs.
15th annual meeting of the special interest groupon discourse and dialogue (sigdial), pages 161–170, philadelphia, pa, u.s.a. association for com-putational linguistics..paramita mirza and sara tonelli.
2016. catena:causal and temporal relation extraction from nat-in proceedings of colingural language texts.
2016, the 26th international conference on compu-tational linguistics: technical papers, pages 64–75,osaka, japan.
the coling 2016 organizing com-mittee..nasrin mostafazadeh, alyson grealish, nathanaelchambers, james allen, and lucy vanderwende.
2016.caters: causal and temporal relationscheme for semantic annotation of event structures.
in proceedings of the fourth workshop on events,pages 51–61, san diego, california.
association forcomputational linguistics..jong-hoon oh, kentaro torisawa, chikara hashimoto,motoki sano, stijn de saeger, and kiyonori ohtake.
2013. why-question answering using intra- andin proceedings ofinter-sentential causal relations.
the 51st annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1733–1743, soﬁa, bulgaria.
association forcomputational linguistics..jong-hoon oh, kentaro torisawa, canasai kru-engkrai, ryu iida, and julien kloetzer.
2017.multi-column convolutional neural networks withcausality-attention for why-question answering.
inproceedings of the tenth acm international confer-ence on web search and data mining, pages 415–424. acm..yannis papanikolaou and a. pierleoni.
2020. dare:data augmented relation extraction with gpt-2.
arxiv, abs/2004.13845..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-in proceedings ofuation of machine translation.
the 40th annual meeting of the association for com-putational linguistics, pages 311–318, philadelphia,pennsylvania, usa.
association for computationallinguistics..mehwish riaz and roxana girju.
2010. another lookat causality: discovering scenario-speciﬁc contin-in 2010gency relationships with no supervision.
ieee fourth international conference on semanticcomputing, pages 361–368.
ieee..mehwish riaz and roxana girju.
2013. toward a bet-ter understanding of causality between verbal events:extraction and analysis of the causal power of verb-in proceedings of the sigdialverb associations.
2013 conference, pages 21–30, metz, france.
asso-ciation for computational linguistics..mehwish riaz and roxana girju.
2014b.
recognizingcausality in verb-noun pairs via noun and verb se-in proceedings of the eacl 2014 work-mantics.
shop on computational approaches to causalityin language (catocl), pages 48–57, gothenburg,sweden.
association for computational linguistics..dana ruiter, cristina espa˜na-bonet, and josef vangenabith.
2019. self-supervised neural machinetranslation.
in proceedings of the 57th annual meet-ing of the association for computational linguistics,pages 1828–1834, florence, italy.
association forcomputational linguistics..karin kipper schuler.
2005..verbnet: a broad-.
coverage, comprehensive verb lexicon..lei shen and yang feng.
2020. cdl: curriculumdual learning for emotion-controllable response gen-in proceedings of the 58th annual meet-eration.
ing of the association for computational linguis-tics, pages 556–566, online.
association for com-putational linguistics..shang-yu su, chao-wei huang, and yun-nung chen.
2019. dual supervised learning for natural languageunderstanding and generation.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5472–5477, florence,italy.
association for computational linguistics..shang-yu su, chao-wei huang, and yun-nung chen.
2020. towards unsupervised language understand-in pro-ing and generation by joint dual learning.
ceedings of the 58th annual meeting of the associa-tion for computational linguistics, pages 671–680,online.
association for computational linguistics..mingming sun, xu li, and ping li.
2018. logicianand orator: learning from the duality between lan-guage and knowledge in open domain.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, brussels, belgium.
association for computational linguistics..william yang wang and diyi yang.
2015. that’s so an-noying!!!
: a lexical and frame-semantic embeddingbased data augmentation approach to automatic cat-egorization of annoying behaviors using #petpeevetweets.
in proceedings of the 2015 conference onempirical methods in natural language processing,pages 2557–2563, lisbon, portugal.
association forcomputational linguistics..jason wei and kai zou.
2019. eda: easy data aug-mentation techniques for boosting performance onin proceedings of thetext classiﬁcation tasks..35682019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 6382–6388, hong kong,china.
association for computational linguistics..yingce xia, tao qin, wei chen, jiang bian, nenghaiyu, and tie-yan liu.
2017. dual supervised learn-ing.
in proceedings of the 34th international confer-ence on machine learning-volume 70, pages 3789–3798. jmlr.
org..cihang xie, mingxing tan, boqing gong, jiang wang,alan l. yuille, and quoc v. le.
2019a.
adver-sarial examples improve image recognition.
arxiv,abs/1911.09665..qizhe xie, zihang dai, eduard h. hovy, minh-thangluong, and quoc v. le.
2019b.
unsuperviseddata augmentation for consistency training.
arxiv:learning..sen yang, dawei feng, linbo qiao, zhigang kan,and dongsheng li.
2019. exploring pre-trained lan-guage models for event extraction and generation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages5284–5294, florence, italy.
association for compu-tational linguistics..hai ye, wenjie li, and lu wang.
2019. jointly learn-ing semantic parser and natural language generatorvia dual information maximization.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 2090–2101, flo-rence, italy.
association for computational linguis-tics..a supplementary experiment results.
a.1 statistics of dual augmented data.
annotated data augmented data.
#causal ep.
#causal sent.
#ave sent..117017701.5.
3588104422.9.table 6: statistics of causal event pairs and causal sen-tences in labeled data (esc) and dual augmented data.
(#causal ep.
denotes the number of causal event pairsafter removing duplicates, #causal sent.
denotes thenumber of causal sentences, #ave sent.
denotes the av-erage number of causal sentences containing the samecausal event pair.).
as shown in table 6, our dual augmented data issigniﬁcantly more quantitative than the labeled data.
speciﬁcally, the causal event pairs are increasedby 3.1 times, the causal sentences are increasedby 5.9 times and the average number of causalsentences corresponding to each causal event pairis also increased..a.2 effectiveness of different quantities of.
augmented training data.
ratio1:11:21:31:4.p37.337.837.036.2.r64.765.664.864.2.f147.3*48.0*47.1*46.3*.
xiang zhang, junbo jake zhao, and yann lecun.
2015.character-level convolutional networks for text clas-siﬁcation.
in nips..table 7: performance of identiﬁer (bert) trained withdifferent ratios of labeled data and dual augmented data.
* denotes a signiﬁcant test at the level of 0.05..xinyu zuo, yubo chen, kang liu, and jun zhao.
2020.knowdis: knowledge enhanced data augmentationfor event causality detection via distant supervision.
in proceedings of the 28th international conferenceon computational linguistics, pages 1544–1550,barcelona, spain (online).
international committeeon computational linguistics..we change the quantity of dual augmented datafor training to explore the inﬂuence of augmenta-tion ratio on eci.
as shown in table 7, when theratio is 1:2, the effective knowledge brought bydual augmented data is maximized.
and as the ra-tio increasing, the dual augmented data will bringnoises, which obstructs the model to identify eventcausality and may change the data distribution fromoriginal data (xie et al., 2019a).
this suggests thattoo much augmented data is not better and thatthere is a trade-off between introducing knowledgeand reducing noise..a.3 effectiveness of extracting event pairs.
with different filtering ratios.
table 8 tries to show the effectiveness of extractingevent pairs with different ﬁltering ratios on eci.
with the ratio of retained event pairs increasing,.
3569α.p.30% 37.840% 37.050% 36.2.r65.665.765.0.f1 ∇--0.7-1.5.
48.0*47.3*46.5*.
table 8: performance of identiﬁer (bert) trained withdifferent extracting event pairs ﬁltered in different α.
*denotes a signiﬁcant test at the level of 0.05..the augmented data hurts eci’s performance.
thisproves the effectiveness of ﬁltering, which furtherimproves the causality of the generated sentences..a.4 effectiveness of generated sentenceswith different filtering ratios.
β.p.50% 37.860% 37.370% 36.980% 36.6.r65.665.364.964.5.f1 ∇--0.5-1.0-1.3.
48.0*47.5*47.0*46.7*.
table 9: performance of identiﬁer (bert) trained withnew generated sentences ﬁltered in different β.
* de-notes a signiﬁcant test at the level of 0.05..table 9 tries to show the effectiveness of gener-ated sentences with different ﬁltering ratios.
withthe ratio of retained generated sentences increas-ing, the contribution of ﬁltered generated sentencesfor eci decreases gradually.
this proves the effec-tiveness of ﬁltering, which can balance the overallquality of the sentences against diversity..b supplementary related work.
b.1 dual learning.
for many natural language processing (nlp)tasks, there exist many primal and dual tasks, suchas open information narration (oin) and open in-formation extraction (oie) (sun et al., 2018), nat-ural language understanding (nlu) and naturallanguage generation (nlg) (su et al., 2019, 2020),semantic parsing and natural language generation(ye et al., 2019; cao et al., 2019, 2020), link pre-diction and entailment graph induction (cao et al.,2019), query-to-response and response-to-querygeneration (shen and feng, 2020) and so on.
theduality between the primal task and the dual task isconsidered as a constraint that both problems mustshare the same joint probability mutually.
recently,inspired by xia et al.
(2017) who implemented theduality in a neural-based dual learning system, theabove primal-dual tasks are implemented in twodifferent ways: 1) providing additional labeled sam-ples via bootstrapping, and 2) adding rewards at.
the training stage for each agent.
we observe thatthe event causality identiﬁcation and the sentencegeneration are dual to each other.
therefore, we ap-ply a dual learning framework in the second way tooptimize identiﬁcation and generation interactivelyfor generating eci-related data..b.2 data augmentation for nlp.
the scarcity of annotated data is a thorny problemin machine learning.
unlike computer vision, theaugmentation of text data in nlp is pretty rare.
existing text data augmentation methods for nlptasks are almost task-independent frameworks andcan be roughly summarized into the following cate-gories (chaudhary, 2020): (1) lexical substitutiontries to substitute words without changing the mean-ing (zhang et al., 2015; wei and zou, 2019; wangand yang, 2015; xie et al., 2019b); (2) back trans-lation tries to paraphrase a text while retrainingthe meaning (xie et al., 2019b); (3) text surfacetransformation tries to match transformations us-ing regex (coulombe, 2018); (4) random noiseinjection tries to inject noise in the text to make themodel more robust (wei and zou, 2019); (5) gen-erative method tries to generate additional trainingdata while preserving the class label (anaby-tavoret al., 2019; yang et al., 2019); (6) distantly su-pervision and self-supervision try to introduce newtraining data from unlabeled text (chen et al., 2017;ruiter et al., 2019).
as aforementioned, theseframeworks cannot directly produce new suitabletask-related examples for eci.
however, (1), (3),and (4) cannot guarantee the causality and well-formedness of new examples for eci.
additionally,(2) and (5) are not easy to directly use externalknowledge bases to generalize the event-relatedcausal commonsense.
furthermore, (6) needs todesign proprietary processing methods to generateeci task-related training data.
zuo et al.
(2020)solved the data lacking problem of eci with thedistantly supervised labeled training data.
how-ever, including the distant supervision, most of theexisting text data augmentation methods for nlptasks are task-independent frameworks.
therefore,we introduce a new learnable framework for aug-menting task-related training data for eci via duallearning enhanced with external knowledge..c generation with conceptnet.
to make a fair comparison, we introduce causal-related events from conceptnet based on causal-.
3570related concepts, and obtain the causal sentencevia the method in konwdis (zuo et al., 2020) tofurther re-train mm (liu et al., 2020).
speciﬁcally,ﬁrstly, we obtain triples based on cause-related se-mantic relations from conceptnet, such as causes,hassubevent, hasfirstsubevent, haslastsubevent,motivatedbygoal, and causesdesire relations.
secondly, we assemble any two events from ob-tained causal triples to generate causal event pairsset and ﬁlter them via the ﬁlter of konwdis.
next,we employ ﬁltered causal event pairs to collectpreliminary noisy labeled sentences from externaldocuments via the distantannotator of konwdis.
then, we use the commonfilter of knowdis as-sisted with causal commonsense knowledge to pickout labeled sentences that express causal seman-tics between events.
finally, the reﬁned causalsentences are input into learnda to generated eci-related dual augmented training data and furthertrain the mm to obtain mm+conceptaug..d main experimental environments and.
other parameters settings.
d.1 experimental environments.
we deploy all models on a server with 250gbof memory and 4 titan xp gpus.
speciﬁ-cally, the conﬁguration environment of the server isubuntu 16.04, and our framework mainly dependson python 3.6.0 and pytorch 1.0..d.2 other parameters settings.
all the ﬁnal hyper-parameters for evaluation areaveraged after 3 independent tunings on the devel-opment set.
moreover, the whole dual learningframework which includes event causality identi-ﬁer and knowledge guided sentence generator takesapproximately 5 minutes per epoch when training.
according to the early stop strategy, the trainingrounds for different folds are different, and it takesabout 20-30 rounds..3571