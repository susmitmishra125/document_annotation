prosodic segmentation for parsing spoken dialogue.
elizabeth nielsen mark steedman sharon goldwaterschool of informaticsuniversity of edinburgh, uke.k.nielsen@sms.ed.ac.uk{steedman, sgwater}@inf.ed.ac.uk.
abstract.
parsing spoken dialogue poses unique difﬁ-culties, including disﬂuencies and unmarkedboundaries between sentence-like units.
previ-ous work has shown that prosody can help withparsing disﬂuent speech (tran et al., 2018),but has assumed that the input to the parseris already segmented into sentence-like units(sus), which isn’t true in existing speech ap-plications.
we investigate how prosody af-fects a parser that receives an entire dialogueturn as input (a turn-based model), insteadof gold standard pre-segmented sus (an su-based model).
in experiments on the en-glish switchboard corpus, we ﬁnd that whenusing transcripts alone, the turn-based modelhas trouble segmenting sus, leading to worseparse performance than the su-based model.
however, prosody can effectively replace goldstandard su boundaries: with prosody, theturn-based model performs as well as the su-based model (90.79 vs. 90.65 f1 score, respec-tively), despite performing two tasks (su seg-mentation and parsing) rather than one (pars-ing alone).
analysis shows that pitch and in-tensity features are the most important for thiscorpus, since they allow the model to correctlydistinguish an su boundary from a speech dis-ﬂuency — a distinction that the model other-wise struggles to make..1.introduction.
parsing spoken dialogue poses unique difﬁculties:spontaneous speech is full of disﬂuencies, includ-ing false starts, repetitions, and ﬁlled pauses.
in ad-dition, speech transcripts lack punctuation, whichwould otherwise help signal the boundaries ofsentence-like units (sus).1 because of these difﬁ-culties, current parsers struggle to accurately parse.
1we follow kahn et al.
(2004) in using the term ‘sentence-like units’ rather than ‘sentences’ throughout, since conversa-tional speech doesn’t always consist of syntactically completesentences..english speech transcripts, even when they han-dle other english text well.
however, research hasshown that prosody can help with at least one ofthese problems, improving parsing performance forspeech that contains disﬂuencies (tran et al., 2018,2019).
in this work, we hypothesize that incorpo-rating prosodic features from the speech signal canactually help with both of these problems: not onlyparsing disﬂuent speech, but also parsing speechthat isn’t segmented into sus..other researchers have augmented parsers withprosodic features, but always with the assumptionthat the parser has access to gold su boundaries,which cannot be assumed in a deployed speechapplication.
for example, gregory et al.
(2004);kahn et al.
(2005) and hale et al.
(2006) incor-porated prosody into statistical parsers or parsererankers, with mixed results.
more recently, tranet al.
(2018) and tran et al.
(2019) found thatprosody improved an end-to-end neural parser, withthe most signiﬁcant gains in disﬂuent sentences.
parsing without access to gold su boundaries ismuch more difﬁcult: kahn and ostendorf (2012)showed that parsing quality depends on the qualityof the sentence segmentation.
furthermore, ﬁnd-ing su boundaries is not as simple as ﬁnding longpauses in speech, as we demonstrate below..we hypothesize that access to prosodic featureswill help an english parser that has to both parseand correctly identify su boundaries (which wecall su segmentation).
we test this hypothesisby inputting entire dialog turns to a neural parserwithout gold su boundaries.
we call this the turn-based model, and compare it to an su-based model,which assumes gold su boundaries and parses onesu at a time.
we use turns as our input unit be-cause they resemble the input a dialog agent wouldreceive from a user.
following tran et al.
(2019)and others, we use a human-generated gold tran-script instead of an automatic speech recognition.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages979–992august1–6,2021.©2021associationforcomputationallinguistics979(asr) transcript; we plan to use asr output infuture work..we build on the work of tran et al.
(2018) andtran et al.
(2019), considering two different exper-imental conditions for each model: inputting textfeatures only and inputting both text and prosodicfeatures.
using the switchboard corpus of en-glish conversational dialogue, we ﬁnd that whenonly transcripts are used, the turn-based parser per-forms considerably worse than the su-based parser,which is not surprising given that it needs to per-form two tasks instead of one.
however, whenprosodic features are included, there is no differ-ence in performance between the turn-based andsu-based models, and both models outperform thetext-only counterparts..our primary contributions are:.
• we show that a parser that has access toprosody can perform both su segmentationand parsing as well as a model that only hasto parse..• we show that one difﬁcultly for the prosody-free turn-based model is that it confusesspeech disﬂuencies with su boundaries, asillustrated in figure 1. further analysis indi-cates that adding pitch and intensity featurescan help the model to disambiguate the two,while pause and duration features do not..2 background: prosody and syntax.
prosodic signals divide speech into units (pier-rehumbert, 1980).
the location and type ofthese prosodic units are determined by informationstructure (steedman, 2000), disﬂuencies (shriberg,2001), and to some extent, syntax (cutler et al.,1997).
some psycholinguistic research showsthat in experimental conditions, speakers can useprosody to predict syntax — for example, that en-glish speakers can use prosody to determine whereto attach a modiﬁer or prepositional phrase, orhow to correctly group coordinands (e.g., kjel-gaard and speer (1999); speer et al.
(1996); warrenet al.
(1995)).
however, cutler et al.
(1997) arguesthat english speakers often “fail to exploit” thisprosodic information even when it is present, so itisn’t actually a signal for syntax in practice.
manycomputational linguists have experimented withthis possible link between syntax and prosody byincorporating prosody into syntactic parsers (e.g.,noeth et al.
(2000); gregory et al.
(2004); kahn.
et al.
(2005); tran et al.
(2018)).
these modelshave had mixed success: for example, gregoryet al.
(2004) found that prosody was at best a neu-tral addition to their model, while kahn et al.
(2005)found that prosody helped rerank pcfg output..one possible reason that prosody is only some-what effective in previous research is that prosodicunits below the level of the su do not always coin-cide with traditional syntactic constituents (selkirk,1995, 1984).2 in fact, the only prosodic boundariesthat consistently coincide with syntactic boundariesare the prosodic boundaries at the ends of sus(wagner and watson, 2010).
the prosodic bound-aries at the end of sus are more distinctive (i.e.,tending to correspond to longer pauses and moredistinctive pitch and intensity variations) and lesslikely appear in any other location.
these featuresmake prosody a reliable signal for su boundaries,even though it is an unreliable signal for syntacticstructure below the su level..some researcheres have used this correlation be-tween prosody and su boundaries to help in suboundary detection.
examples of su segmenta-tion models that found prosodic cues were impor-tant include gotoh and renals (2000); kol´aˇr et al.
(2006); kahn et al.
(2004); kahn and ostendorf(2012), who all used traditional statistical models(e.g., hmms, ﬁnite state machines, and decisiontrees), and xu et al.
(2014), who used a neuralmodel.
kahn et al.
(2004) and kahn and ostendorf(2012) also looked at downstream parsing accuracyon the same corpus we use.
like us, kahn andostendorf (2012) don’t use gold su boundaries,but direct comparison is impossible because theyuse asr output instead of human transcriptionsand a different metric for parse performance (spar-seval; roark et al.
(2006)).
however, they showthat having access to gold su boundaries increasesthe sparseval score from 78.5 to 82.3, which showsthat parsing without gold su boundaries is difﬁcult.
however, in some research areas, prosody isless frequently used for su detection.
some asrcorpora and applications segment at relatively ar-bitrary boundaries such as long silences or evenregular intervals (e.g., jain et al.
(2020)).
otherapplications, such as speech translation, do requiresyntactically coherent input, but even there, sys-tems targeting sus have often used only textualfeatures (sridhar et al., 2013; wan et al., 2020)..2we refer here to traditional constituency parsing; ccg(steedman and baldridge, 2011) proposes different syntacticconstituents that coincide with prosodic units..980turn.
sbarq.
s.{how do you} how do you feel.
{th-} that’s of course being facetious.
(a) text+prosody model output.
turn.
edited.
whadvp.
sq.
{how do you}.
how.
do you feel {th-} that’s of course being facetious.
(b) text-only model output.
figure 1: a portion of a turn that contains both disﬂuencies (shown in curly braces) and an su boundary.
asimpliﬁed version of the text+prosody model output is shown in (a), which matches the gold su boundaries.
thetext-only model incorrectly places an su boundary after a disﬂuency (shown in (b))..systems for restoring punctuation from asr out-put must identify su boundaries to correctly insertsentence-ﬁnal punctuation, but these systems aretypically evaluated on rehearsed monologues (suchas ted talks) or read speech, which largely lackdisﬂuencies (e.g., federico et al.
(2012)).
here,we show that prosody is primarily helpful for dis-tinguishing su boundaries from disﬂuencies, soalthough some of these systems have used prosody(e.g., tilk and alum¨ae (2016)), text-only systemsare very competitive (e.g., che et al.
(2016); alamet al.
(2020))..even when su boundaries are already known,other research in parsing conversational speech hasshown that prosody helps identify and correctlyhandle disﬂuencies.
tran et al.
(2018) found thatprosody only modestly affects parsing of ﬂuentsus, but has a marked effect on disﬂuent sus.
this accords with other previous work that hasfound that prosody is helpful in disﬂuency detec-tion (zayats and ostendorf, 2019) we discuss therelationship between prosody and disﬂuencies ingreater detail in section 6, including how prosodyhelps the model not to confuse disﬂuencies and suboundaries, as shown in figure 1 above..3 task and data.
we use the american english corpus switchboardnxt (henceforth swbd-nxt) (calhoun et al.,2010).
we choose this corpus mainly so we cancompare performance with tran et al.
(2018) andtran et al.
(2019), as well as other earlier proba-.
bilistic models such as kahn et al.
(2005).
swbd-nxt comprises 642 dialogues between strangersconducted by telephone.
these dialogues are tran-scribed and hand-annotated with penn treebank-style constituency parses.
we preprocess the tran-scripts to remove punctuation and lower-case allletters, making the input more like an asr tran-script that would be used in a deployed application..the transcript divides the corpus into sus andturns.
since these sus may be sentences or othersyntactically independent units such as sentencefragments, we use the generic term ‘sentence-likeunit’ (su).
a turn is a contiguous span of speechby a single speaker.
turns are hand-annotated inswbd-nxt, but for a deployed dialog agent, aturn is simply whatever contiguous input the usergives.
not all turns in the swbd-nxt containmore than one su: of a total 60.1k turns, 35.8kconsist of a single su.
the remaining 24.3k containmore than one su; the majority (52.4 percent) ofthese contain just two sus.
the average number ofsus per turn is 1.82..we follow the general approach of tran et al.
(2018), but where they parse a single su at a time,we give our parser a single dialog turn at a timefor our turn-based model.
the model returns con-stituency parses for the turn in the form of penntreebank (ptb)-style trees.
in order to keep theoutput in the form of valid ptb trees, we add atop-level constituent, labelled turn, to all turns,however many sus they consist of.
this exampleshows how the two sentences in (1) would be fused.
981into a single turn in (2):.
(1).
(2).
separate sus:a.b..(s (np kim) (vp sings))(s (np sidney) (vp dances)).
merged into a single turn:a..(turn (s (np kim) (vp sings)) (s (npsidney) (vp dances))).
of course, using turns instead of sus leads tolonger inputs.
we experiment with a pipeline ap-proach (ﬁrst segmenting turns into sus, then pars-ing) as well as an end-to-end approach.
in theend-to-end approach, we can’t handle extremelylong inputs since these longer sequences lead tohigh memory usage for transformers.
we still wantto capture the model’s behavior on generally longerinputs, so we ﬁlter out two problematically longturns from the training set (out of 49,294 turns).
we do not have to remove any turns from the devel-opment or test sets.
this leaves the maximum turnlength at 270 tokens.
we also remove any turns forwhich some or all speech features are missing fromthe corpus..3.1 feature extraction.
from the speech signal, we extract features forpauses between words, word duration, pitch, andintensity.
we largely follow the feature extractionprocedure outlined in tran et al.
(2018) and tranet al.
(2019), which we summarize here, noting anydeviations from or additions to their procedure..pause features are extracted from the time-aligned transcript.
each word’s pause feature cor-responds to the pause follows it.
each pause iscategorized into one of six bins by length in sec-onds: p > 1, 0.2 < p ≤ 1, 0.05 < p ≤ 0.2,0 < p ≤ 0.05, p ≤ 0 (see below), and pauseswhere we are missing time-aligned data.
followingtran et al.
(2018), the model learns 32-dimensionalembeddings for each pause category..since we use turns instead of sus, we have todetermine how to handle pauses at the beginningsand endings of turns.
we decide to calculate pausesbased on all words in the transcript, not just thewords for a single speaker at a time.
this meansthat at a turn boundary, we calculate the pause asthe time between the end of one speaker’s turn andthe beginning of the other speaker’s turn.
if onespeaker interrupts another, the pause duration hasa negative value.
we place these negative-valued.
pauses in the same bin as pauses with length 0..duration features are also extracted from thetime-aligned transcript.
we are interested in therelative lengthening or shortening of word tokens,so we normalize the raw duration of each token.
following the code base for tran et al.
(2019), weperform two different types of normalization.
inthe ﬁrst case, we normalize the token’s raw dura-tion by the mean duration of every instance of thatword type.
in the second, we normalize the token’sraw duration by the maximum duration of any wordin the input unit (su or turn).
these two normal-ization methods result in two duration features foreach word token, which are concatenated and inputto the model..pitch features (or more accurately, f0 features)are extracted from the speech signal using kaldi(povey et al., 2011).
these are extracted from25ms frames every 10ms.
three pitch features areextracted: warped normalized cross correlationfunction (nccf); log-pitch with mean subtractionover a 1.5-second window, weighted by probabil-ity of voicing (pov); and the estimated derivativeof the raw log pitch.
for further details on thesefeatures, see ghahremani et al.
(2014)..intensity features are also extracted from thespeech signal using the same software and framesize as we use for pitch features.
starting with40-dimensional mel-frequency ﬁlterbank features,we calculate three features: (1) the log of the totalenergy, normalized by the maximum total energyfor the speaker over the course of the dialog; (2)the log of the total energy in the lower half of the40 mel-frequency bands, normalized by the totalenergy; and (3) the log of the total energy in the up-per half of the 40 mel-frequency bands, normalizedby the total energy..for training, development, and testing, we usethe split described in charniak and johnson (2001),which is a standard split for experiments on swbd-nxt (e.g., kahn et al.
(2005); tran et al.
(2018)).
the training set makes up 90 percent of the data,and the development and testing sets make up 5percent each..4 model.
we use the parser described in tran et al.
(2019), di-rectly extending the code base described in their pa-per.3 the model is a neural end-to-end constituency.
3original: https://github.com/trangham283/prosody nlp;.
our extended code: https://github.com/ekayen/prosody nlp.
982parser based on kitaev and klein (2018)’s text-only parser, with a transformer-based encoder anda chart-style decoder based on stern et al.
(2017)and gaddy et al.
(2018).
this encoder-decoderis augmented with a cnn on the input side thathandles prosodic features (tran et al., 2019).
forfurther description of the model and hyperparame-ters, see appendices a.1 and a.2..the text is encoded using 300-dimensionalglove embeddings (pennington et al., 2014).4 ofthe four types of prosodic features described insection 3, pause and duration features are alreadytoken-level.
however, pitch and intensity featuresare extracted from the speech signal at the framelevel.
in order to map from these frame-level fea-tures to a token-level representation, the pitch andintensity features pass through a cnn, and are thenconcatenated with the token-level pause and dura-tion features..we follow tran et al.
(2019) in training eachmodel 10 times with different random seeds.
forthe development set, we report the mean of these10 models’ performance.
we then select the me-dian model by development set performance, anduse it to calculate test set results.
for any furtherexperiments, such as those discussed in section6, we use the random seed for this median model.
each model is trained for 50 epochs and use theepoch with highest development set performance.
in addition to this end-to-end approach, we alsoreport results for a pipeline approach.
for thepipeline, we ﬁrst segment the speech into sus us-ing a modiﬁed version of the parser architecture:we keep the encoder the same, but we change thedecoder so that it only does sequence labelling, andwe frame the su segmentation task as a sequencelabelling task.
we then use the su-based parserto parse the resulting sus.
we report the model’sperformance with and without prosodic featuresduring the segmentation and parsing steps..5 results.
we compare the turn-based f1 performance ofour parser to a replication of the su-based per-formance described in tran et al.
(2018) and tranet al.
(2019).
table 1 shows the development andtest set results.5 we ﬁnd that the turn-based modelbeneﬁts signiﬁcantly from prosody.
the turn-based.
4see appendix a.3 for results using bert embeddings.
5we use pyevalb to evaluate our parser’s performance,though we modify it so that it behaves identically to evalb:https://github.com/ekayen/pyevalb.
su-based turn-based.
test set:text onlytext+prosodydev.
set:text onlytext+prosody.
90.2990.65.
90.3190.90.
86.5690.79.
86.0890.84.table 1: test and development set f1 of the turn-basedmodel compared to the su-based model.
dev.
setscores are the mean over 10 random seeds.
for the testset, we use the model that has the median dev.
set per-formance out of 10 randomly seeded models..input length (# tokens).
198.3699.180.82.
2–893.0094.911.91.
9–2289.2292.743.52.
23–25584.3089.805.5.text onlytext+pros.
∆.
f1 performance of.
the text-only andtable 2:text+prosody turn-based models on inputs of variouslengths in the development set.
the inputs are dividedinto bins of approximately equal size by token length..model performs equivalently well to the su-basedmodel, despite doing two tasks instead of one.
thesu-based model also improves by 0.36 in f1 scoreon the test set with the addition of prosody.
notethat while prosody has a considerably larger ef-fect on the turn-based model than on the su basedmodel, the exact size of this change will depend onthe corpus.
for example, in a corpus with very fewmulti-su turns, the performance change in the turn-based model might not be as large.
however, ourresults suggest that prosody helps when a modelneeds to both detect su boundaries and parse sus.
the biggest difference between the su- and turn-based models’ performance on this corpus is in thetext-only scenario, where the turn-based parser issubstantially worse.
this is expected for a fewreasons.
first, the text-only turn-based parser en-counters longer inputs.
longer inputs tend to leadto more parse errors simply because there are moreways to parse a longer string.
table 2 shows thiscorrespondence between length and performance.
the median length of turns in the development setis 9 tokens, while the median length of sus is 6tokens.
longer strings are also more likely to con-tain the things that make parsing difﬁcult, namelydisﬂuencies and su boundaries..the turn-based parser’s task is also more com-.
983pipeline.
e2e.
text onlytext+prosodytext onlytext+prosody.
segmentation.
precision recall68.6199.4575.7899.41.
78.8499.9655.0199.41.f173.3199.7163.7499.41.parsingf182.7390.8986.0990.90.table 3: development set performance of the pipeline model on segmentation and parsing as compared to theend-to-end model.
(results are from single models rather than an average as in table 1.).
plex: it has to perform both su segmentation andparsing, rather than parsing alone.
this gives theturn-based parser novel ways to make errors bysplitting a turn into the wrong number of sus.
how-ever, prosody brings the turn-based parser up tothe level of the su-based parser, even though theturn-based model’s task is more complex.
table 5shows how the text-only parser signiﬁcantly over-estimates the number of su boundaries.
withoutprosody, the model achieves an f1 score of 63.74on su prediction on the development set, comparedto 99.41 with prosody (see table 3).
the most com-parable work on swbd is kahn and ostendorf(2012), who achieved 78 f1 using a hidden-eventmodel, where we use a much more powerful trans-former model; however, their model used asrtranscripts as input, so these scores aren’t directlycomparable..we also test the pipeline model described in sec-tion 4, which ﬁrst segments turns into sus andthen parses them, both with and without prosody.
we train just one segmentation model with thesame random seed as the median development setmodel.
we report the development set performanceon segmentation (measured by segmentation f1(makhoul et al., 2000)) and parse f1 in table 3..the text+prosody pipeline model achieves anf1 score of 99.71, which is statistically indistin-guishable from the end-to-end text+prosody model.
in both cases, we see that the addition of prosodyboosts su segmentation accuracy to near-perfectlevels, which explains why the parser performanceis similar (and much better than without prosody)..comparing the two text-only models reveals amore interesting pattern: while the pipeline modelachieves much better segmentation f1, its parsingperformance is worse.
this is unexpected, as pars-ing and segmentation performance are usually cor-related.
this effect seems to arise because the twomodels err in different directions on segmentation:the pipeline model under-segments turns (corre-.
sponding to higher segmentation precision), whilethe end-to-end over-segments (higher recall, sub-stantially lower precision).
when it over-segments,the end-to-end text-only model often splits a wordor short constituent off of an otherwise well-formedsu subtree; by contrast, the pipeline model tendsto leave two or more sus combined and and thento generate many su-internal parsing errors.
thesesu-internal parsing errors include more coordina-tion errors as well as vp, np, and clause attach-ment errors than the end-to-end model.6 however,the pipeline model does as well as the end-to-endmodel at pp attachment and modiﬁer attachment.
overall, these results show that a pipeline modelcan be as effective at parsing as an end-to-end one,but that including prosody is even more importantfor a pipeline model.
since we care about parsingperformance and the end-to-end text-only modeldoes much better at parsing, we use the end-to-endmodel for all remaining analyses..5.1 error types.
we use the berkeley parser analyser (kummer-feld et al., 2012) to determine what types of errorseach of the su-based and end-to-end turn-basedmodels makes.
figure 2 summarizes the output ofthe analyser.
overall, the su-based parser showsonly small effects from prosody, but the turn-basedmodel does signiﬁcantly worse on certain errortypes without prosody.
even for the turn-basedmodel, prosody only affects error types that haveto do with the shape of the tree.
the different labelcategory shows errors where two identically shapedtrees have different constituent labels, and prosodyhas no effect on these..for the turn-based model, poor su segmentationby the text-only model explains some of the dif-ferences between the text+prosody and text-onlymodels.
since 68.8 percent of sus are clauses (i.e.,.
6we use the berkeley parser analyser to analyze types of.
parse error (kummerfeld et al., 2012)..984figure 2: prevalence of various error types in the development set output, given four different experimental condi-tions: su-based, with and without prosody; and turn-based, with and without prosody.
error types are classiﬁedby the berkeley parser analyzer (kummerfeld et al., 2012)..they have a top node of type s, sbar, sq or sinv),an incorrect su segmentation is usually classedas a clause attachment error.
an example of thiskind of attachment error can be seen in appendixa.4.
however, prosody also affects the turn-basedmodel’s rate of np, pp, and modiﬁer attachmenterrors.
since these attachment errors are not ascommon in the text-only su-based model, it seemslikely that they are caused by a cascade effect fromerrors in top-level su segmentation.
prosody alsoaffects the turn-based model’s rate of unary errors(which are errors “involving unary productions thatare not linked to a nearby error such as a matchingextra or missing node”) and single word phraseerrors (which are “a range of node errors that spana single word” but which are not related to othererrors) (kummerfeld et al., 2012).
finally, verymodest differences are seen for two rare error types:np-internal and vp attachment errors..5.2 effect of disﬂuencies.
text onlytext+all prosody∆ from prosodypitch onlyintensity onlyduration onlypause only.
all86.0990.904.8190.7190.2986.2486.21.fluent disﬂuent89.8993.633.7493.4693.3089.9490.09.
84.2589.585.3389.3788.8384.4484.32.table 4: f1 for the text and text+prosody turn-basedmodels when tested on the entire development set, thesubset of the development set consisting of only ﬂuentturns, and the subset of all disﬂuent turns..our turn-based model performs worse overallon disﬂuent turns than on ﬂuent turns, which wasalso true of tran et al.
(2018)’s su-based model.
prosody also leads to a greater gain in f1 for dis-ﬂuent turns than for ﬂuent turns.
these differencesin performance are shown in table 4. the lowerperformance on disﬂuent sentences may be at leastpartially attributable to length differences: the me-dian length of turns with disﬂuencies is 28 tokens,compared to 3 tokens for ﬂuent turns, where wedeﬁne a disﬂuent turn as any turn containing theconstituent tag edited.
as discussed in section 5,longer input generally leads to more parser errors,meaning that disﬂuent sentences are more likelyto cause parser errors.
however, there are otherreasons disﬂuencies are difﬁcult for the turn-basedmodel, as discussed in the following section..6 distinguishing disﬂuencies and su.
boundaries.
one effect of disﬂuencies is that the text-onlymodel tends to confuse certain kinds of disﬂuen-cies for su boundaries, as illustrated in figure 1.table 5 shows that the text+prosody model largelyavoids this confusion, and indeed can do so almostas well using only pitch or intensity features.
how-ever, models using only pause or duration featuresare not good at distinguishing disﬂuencies fromsu boundaries and predict boundaries too often.
these results largely concur with previous workdescribing the similarities and differences betweenprosodic features of disﬂuencies and su bound-aries (shriberg, 2001; wagner and watson, 2010).
in this section, we examine each of the features.
985total.
predicted.
features predicted bound.
bound.
at disf.
goldallpitchintensitydurationpausenone.
2552255225902647343736483516.
241720204225208.table 5: the total number of su boundaries predictedon the dev.
set as compared to the number of su bound-aries predicted to fall at what are actually interruptionpoints within disﬂuencies.
the ﬁrst line shows the tar-get for both values.
we give results for a model with allfour prosodic features, models with only one prosodicfeature at a time, and a model with no prosodic features..more closely with respect to this previous workand our results, highlighting where our results do(and do not) accord with expectations..the disﬂuencies that are relevant to this discus-sion include repetitions and restarts.
examplesof these from swbd-nxt are shown here, withbracketing added for clarity:.
(3).
spurious repetition: it [may] may be atthis pointrestart: [but it’s] but i think it’s relativelyunimportant.
in these examples, the text in square brackets iscalled the reparandum, which is immediately fol-lowed by the interruption point.
disﬂuencies inswbd-nxt are marked in the constituency parseannotation, where the reparandum is marked as aconstituent with the label edited.
the interruptionpoint is the right edge of this constituent..our analysis draws on the work of shriberg(2001), who described the prosodic features of theinterruption point and the reparandum based on ananalysis of three english conversational and task-based dialogue corpora — the switchboard cor-pus (which we use a subset of), atis (hirschman,1992), and amex (kowtko and price, 1989)..pauses.
although pauses may be the most intu-itive potential cue to su boundaries, previous worksuggests that long pauses also characterize interrup-tion points (wagner and watson, 2010; shriberg,2001).
indeed, our analysis shows that longerpauses (> 0.05s) are over-represented in both lo-cations.
if pause types were distributed uniformly,16 percent of both su boundaries and interruption.
points would have a longer pause.
instead, we ﬁndthat 33 percent of sus boundaries and 37 percentof interruption points have such pauses.
this ex-plains why the pause-only model tends to confusesu boundaries and interruption points..duration.
shriberg (2001) found that both in-terruptions and su boundaries are associated withlengthening of the immediately preceding sylla-ble.
lengthening before the interruption point mayoccur even if there are no other prosodic cues tothe disﬂuency, and can be “far greater” than atsu boundaries (shriberg, 2001, 161).
this type oflengthening is captured by our ﬁrst duration feature,which measures the token duration normalized bythe mean duration for its word type.
like shriberg(2001), we ﬁnd that words preceding su bound-aries are lengthened on average (normalized dura-tion: 1.18), and those preceding interruption pointseven more so (normalized duration: 1.41).
in prin-ciple, this extra lengthening could help the duration-only model distinguish su boundaries from inter-ruptions, but in practice the model is nearly as badat distinguishing them as the text-only model..the second duration feature is the token lengthnormalized by the maximum length of any token inthe input, to normalize for speaking rate.
initially,this feature looks helpful: su-ﬁnal words havemean value of 0.86, while words directly beforethe interruption point have a mean of 0.50. how-ever, the feature mainly captures the number ofphones in a word, since words with fewer phones —including english function words — tend to haveshorter normalized duration.
it turns out that func-tion words occur more often before interruptionpoints than before su boundaries: using nltk’sstopwords as a heuristic for function words, only21.9 percent of development set sus end in a func-tion word, while the word before an interrutptionpoint is a function word 51.6 percent of the time(bird and klein, 2009).
since the second durationfeature captures a lexical distinction that is alreadysignalled in the text, it cannot help the duration-only model outperform the text-only model..pitch.
based on previous work, our ﬁnding thatpitch features are useful is not a surprise: the pitchcontour before an interruption point is generally“ﬂat or slowly falling” (shriberg, 2001, 161), whilesu boundaries are characterized by a boundarytone, generally corresponding to a fall or rise.
ourmodel may be able to learn such temporal patterns,but even just looking at static pitch features re-.
986veals differences between boundaries and interrup-tions for two of the three features.
in particular,the mean warped nccf value for pre-interruptionpoint words is signiﬁcantly higher than the valuefor su-ﬁnal words (p < 0.001), though somewhatlower than the overall average value across thedevelopment set.
meanwhile, the log-pitch withpov-weighted mean subtraction is signiﬁcantlylower at interruption points than at su boundaries(p < 0.01).
these differences allow the pitch-onlymodel to distinguish su boundaries and interrup-tion points much better than the pause- or duration-only models can (see table 5).
of these two pitchfeatures, log-pitch is a more direct indicator offundamental frequency (f0), which suggests thataverage perceived pitch is likely lower before dis-ﬂuencies than before su boundaries.
there couldbe several reasons for this difference.
for example,it could be that the “ﬂat or slowly falling” tone ofdisﬂuencies that shriberg (2001) describes has alower average value than su boundaries which canhave either a fall or a rise (e.g., for certain kindsof questions).
however, examining pitch featuresacross the whole corpus obscures more subtle dis-tinctions such as different types of pitch contours..intensity.
we ﬁnd that intensity features aloneare enough to distinguish su boundaries from inter-ruption points, which is interesting because inten-sity has not been previously identiﬁed as an impor-tant cue: shriberg (2001) doesn’t note any partic-ularly distinctive intensity features of the reparan-dum or interruption point, and work by kim et al.
(2006) on the switchboard corpus suggests thatsu boundaries are correlated to lower intensity insome speakers, but that this isn’t consistent acrossspeakers.
the three intensity features correspondto overall energy, energy in the lower half of fre-quencies, and energy in the higher frequencies.
su-ﬁnal words have a signiﬁcantly higher mean valuefor lower-frequency intensity than all other words(p < 0.001), while words before the interruptionpoint do not.
this systematic difference in one in-tensity feature seems to be part of how intensityfeatures allow the model to consistently tell suboundaries apart from disﬂuencies..overall performance.
given our claim that themain issue facing the text-only turn-based parser isdistinguishing disﬂuencies from su boundaries, itis not surprising that the two features that do bestat this, pitch and intensity, also yield the highestoverall performance.
results are shown in table 6..features.
f1.
all featurespitchintensitydurationpauseno prosodic features.
only.
90.9090.71 (ns)90.29 (*)86.24 (*)86.21 (*)86.09 (*).
table 6: results of ablation testing, measured by f1score on the dev.
set.
asterisks indicate a statisticallysigniﬁcant difference (p < 0.001) from the model withall features.
the ﬁrst row shows with all features; thenext four rows show the result with one feature at atime; the ﬁnal row shows the result with no prosody..7 conclusion.
our experiments show that parsing english speechtranscriptions without gold su boundaries is difﬁ-cult for our parser: its f1 score drops by about 4percentage points compared to a model with goldsu boundaries.
incorrect su segmentation causesa large part of this damage, though other errors intree construction also play a role.
we show that wecan undo this damage by giving our parser prosodicinformation.
importantly, prosody helps by allow-ing the parser to distinguish disﬂuencies from suboundaries.
these results argue for giving prosodicinformation to parsers in deployed applications,where no su boundary annotations are available,including dialog agents..furthermore, our experiments show that evenlimited prosodic features help a great deal: for ourenglish data, pitch information alone is not signiﬁ-cantly worse than pitch, intensity, pause, and wordduration information combined.
this means that in-corporating the right kind of prosodic informationcan potentially lead to signiﬁcant gains..acknowledgments.
we are very grateful to trang tran for help answer-ing questions related to her code and to mari osten-dorf for conversations that helped inspire this paper.
we would like to thank korin richmond, the aclreviewers, and members of the agora researchgroup at the university of edinburgh for their feed-back.
this work was supported by funding fromhuawei and the project semantax, which re-ceived funding from the european research coun-cil (erc) under the european union’s horizon2020 research and innovation programme (grantagreement no.
742137)..987references.
tanvirul alam, akib khan, and firoj alam.
2020.punctuation restoration using transformer modelsin proceed-for high-and low-resource languages.
ings of the sixth workshop on noisy user-generatedtext (w-nut 2020), pages 132–142, online.
associ-ation for computational linguistics..edward loper bird, steven and ewan klein.
2009.language processing with python..naturalo’reilly media inc..p. ghahremani, b. babaali, d. povey, k. riedhammer,j. trmal, and s. khudanpur.
2014. a pitch extrac-tion algorithm tuned for automatic speech recogni-in 2014 ieee international conference ontion.
acoustics, speech and signal processing (icassp),pages 2494–2498..yoshihiko gotoh and steve renals.
2000. sentenceboundary detection in broadcast speech transcripts.
in asr2000-automatic speech recognition: chal-lenges for the new millenium isca tutorial and re-search workshop (itrw)..sasha calhoun, jean carletta, jason brenier, neilmayo, dan jurafsky, mark steedman, and davidbeaver.
2010. the nxt-format switchboard cor-pus: a rich resource for investigating the syntax, se-mantics, pragmatics and prosody of dialogue.
lan-guage resources and evaluation, 44:387–419..michelle gregory, mark johnson, and eugene char-niak.
2004. sentence-internal prosody does not helpparsing the way punctuation does.
proceedings ofthe human language technology conference of thenorth american chapter of the association for com-putational linguistic..eugene charniak and mark johnson.
2001. edit detec-tion and parsing for transcribed speech.
in secondmeeting of the north american chapter of the asso-ciation for computational linguistics..xiaoyin che, cheng wang, haojin yang,.
andchristoph meinel.
2016.punctuation predictionfor unsegmented transcript based on word vector.
in proceedings of the tenth international confer-ence on language resources and evaluation (lrec2016), paris, france.
european language resourcesassociation (elra)..anne cutler, delphine dahan, and wilma van donse-laar.
1997. prosody in the comprehension of spo-ken language: a literature review.
language andspeech, 40(2):141–201..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..marcello federico, sebastian st¨uker, luisa bentivogli,michael paul, mauro cettolo, teresa herrmann, janniehues, and giovanni moretti.
2012. the iwslt2011 evaluation campaign on automatic talk transla-tion.
in international conference on language re-sources and evaluation (lrec), pages 3543–3550..david gaddy, mitchell stern, and dan klein.
2018.what’s going on in neural constituency parsers?
ananalysis.
in proceedings of the 2018 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long papers), pages 999–1010,new orleans, louisiana.
association for computa-tional linguistics..john hale, izhak shafran, lisa yung, bonnie j. dorr,mary harper, anna krasnyanskaya, matthew lease,yang liu, brian roark, matthew snover, and robinstewart.
2006. pcfgs with syntactic and prosodicin proceedings ofindicators of speech repairs.
the 21st international conference on computationallinguistics and 44th annual meeting of the associa-tion for computational linguistics, pages 161–168.
association for computational linguistics..lynette hirschman.
1992. multi-site data collectionfor a spoken language corpus.
in proceedings of theworkshop on speech and natural language, hlt’91, page 7–14, usa.
association for computa-tional linguistics..mahaveer jain, gil keren, jay mahadeokar, geoffreyzweig, florian metze, and yatharth saraf.
2020.contextual rnn-t for open domain asr.
in proc.
interspeech 2020, pages 11–15..jeremy g. kahn, matthew lease, eugene charniak,mark johnson, and mari ostendorf.
2005. effectiveuse of prosody in parsing conversational speech.
inproceedings of the conference on human languagetechnology and empirical methods in natural lan-guage processing, hlt ’05, page 233–240, usa.
association for computational linguistics..jeremy g. kahn and mari ostendorf.
2012..jointreranking of parsing and word recognition with au-tomatic segmentation.
computer speech and lan-guage, 26(1):1 – 19..jeremy g. kahn, mari ostendorf, and ciprian chelba.
2004. parsing conversational speech using enhancedsegmentation.
in proceedings of hlt-naacl 2004,pages 125–128, boston, massachusetts, usa.
asso-ciation for computational linguistics..heejin kim, tae jin yoon, jennifer cole, and markhasegawa-johnson.
2006. acoustic differentiationof l- and l-l% in switchboard and radio news speech.
in proceedings of speech prosody 2006..988nikita kitaev and dan klein.
2018. constituency pars-in proceedingsing with a self-attentive encoder.
of the 56th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 2676–2686, melbourne, australia.
associa-tion for computational linguistics..margaret m. kjelgaard and shari r. speer.
1999.prosodic facilitation and interference in the resolu-tion of temporary syntactic closure ambiguity.
jour-nal of memory and language, 40(2):153 – 194..j´achym kol´aˇr, elizabeth shriberg, and yang liu.
2006.using prosody for automatic sentence segmentationin international confer-of multi-party meetings.
ence on text, speech and dialogue, pages 629–636.
springer..jacqueline c. kowtko and patti j. price.
1989. datacollection and analysis in the air travel planningin speech and natural language: pro-domain.
ceedings of a workshop held at cape cod, mas-sachusetts, october 15-18, 1989..jonathan k. kummerfeld, david hall, james r. cur-ran, and dan klein.
2012. parser showdown atthe wall street corral: an empirical investigationin proceedings ofof error types in parser output.
the 2012 joint conference on emnlp and conll,pages 1048–1059, jeju island, south korea..john makhoul, francis kubala, richard schwartz, andralph weischedel.
2000. performance measuresfor information extraction.
proceedings of darpabroadcast news workshop..elmar noeth, anton batliner, andreas kießling, ralfkompe, and heinrich niemann.
2000. verbmobil:the use of prosody in the linguistic components ofa speech understanding system.
ieee transactionson speech and audio processing, 8(5):519–532..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordrepresentation.
in emnlp, pages 1532–1543..janet breckenridge pierrehumbert.
1980. the phonol-ogy and phonetics of english intonation.
ph.d. the-sis, massachusetts institute of technology..daniel povey, arnab ghoshal, gilles boulianne, lukasburget, ondrej glembek, nagendra goel, mirkohannemann, petr motlicek, yanmin qian, petrschwarz, jan silovsky, georg stemmer, and karelvesely.
2011. the kaldi speech recognition toolkit.
in ieee 2011 workshop on automatic speechrecognition and understanding.
ieee signal pro-cessing society..brian roark, mary harper, eugene charniak, bon-nie dorr, mark johnson, jeremy kahn, yang liu,mari ostendorf, john hale, anna krasnyanskaya,matthew lease, izhak shafran, matthew snover,robin stewart, and lisa yung.
2006. sparseval:evaluation metrics for parsing speech.
in proceed-ings of the fifth international conference on lan-guage resources and evaluation (lrec’06)..elisabeth selkirk.
1984. phonology and syntax.
mit.
press, cambridge, ma..elisabeth selkirk.
1995. sentence prosody: intonation,stress, and phrasing.
the handbook of phonologicaltheory, 1:550–569..elizabeth shriberg.
2001. to ’errrr’ is human: ecologyand acoustics of speech disﬂuencies.
journal of theinternational phonetic association, 31:153 – 169..shari speer, margaret kjelgaard, and kathryn dobroth.
1996. the inﬂuence of prosodic structure on the res-olution of temporary syntactic closure ambiguities.
journal of psycholinguistic research, 25:249–71..vivek kumar rangarajan sridhar, john chen, srinivasbangalore, andrej ljolje, and rathinavelu chengal-varayan.
2013. segmentation strategies for stream-ing speech translation.
in proceedings of the 2013conference of the north american chapter of theassociation for computational linguistics, pages230–238, atlanta, georgia.
association for compu-tational linguistics..mark steedman.
2000..information structure andthe syntax-phonology interface.
linguistic inquiry,31(4):649–689..mark steedman and jason baldridge.
2011. combi-natory categorial grammar.
in robert borsley andkersti b¨orjars, editors, non-transformational syn-tax: a guide to current models, pages 181–224.
blackwell, oxford..mitchell stern, jacob andreas, and dan klein.
2017. aminimal span-based neural constituency parser.
inproceedings of the 55th annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 818–827, vancouver, canada.
association for computational linguistics..ottokar tilk and tanel alum¨ae.
2016. bidirectional re-current neural network with attention mechanism forpunctuation restoration.
in interspeech 2016, pages3047–3051..trang tran, shubham toshniwal, mohit bansal, kevingimpel, karen livescu, and mari ostendorf.
2018.parsing speech: a neural approach to integrating lex-ical and acoustic-prosodic information.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics, volume 1 (long papers), pages 69–81, neworleans, louisiana.
association for computationallinguistics..trang tran, jiahong yuan, yang liu, and mari osten-dorf.
2019. on the role of style in parsing speechin proc.
interspeech 2019,with neural models.
pages 4190–4194..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems, pages 5998–6008..989michael wagner and duane g. watson.
2010. ex-perimental and theoretical advances in prosody: areview.
language and cognitive processes, 25(7-9):905–945..david wan, zhengping jiang, chris kedzie, elsbethturcan, peter bell, and kathy mckeown.
2020.subtitles to segmentation: improving low-resourcein proceed-speech-to-texttranslation pipelines.
ings of the workshop on cross-language search andsummarization of text and speech, pages 68–73,marseille, france.
european language resourcesassociation..paul warren, esther grabe, and francis nolan.
1995.prosody, phonology and parsing in closure ambigui-ties.
language and cognitive processes, 10(5):457–486..chenglin xu, lei xie, guangpu huang, xiong xiao,eng siong chng, and haizhou li.
2014. a deepneural network approach for sentence boundary de-tection in broadcast news.
in interspeech-2014,pages 2887–2891..vicky zayats and mari ostendorf.
2019. giving atten-tion to the unexpected: using prosody innovationsin disﬂuency detection.
in proceedings of the 2019conference of the north american chapter of the as-sociation for computational linguistics, volume 1,pages 86–95, minneapolis, minnesota.
associationfor computational linguistics..a appendices.
a.1 model description.
the parser is an encoder-decoder model that takesboth speech and text inputs.
in this appendix, wedescribe the three main model components: thecnn that processes the continuous speech inputsbefore they reach the encoder, the transformer-based encoder, and the chart-style decoder..a.1.1 the speech-processing cnnof the four prosodic features, pause and durationare already discrete at the token level.
pitch andintensity, however, are extracted from frames ev-ery 10 ms in the original speech signal.
if a giventoken is shorter than a ﬁxed number of frames,some frames of left and right context are included;frames from longer tokens are subsampled to re-duce their frame length.
these two frame-basedfeatures features have a different dimensionalitythan the token-level input and they are untenablylong for a sequence model or transformer.
thecnn solves both these problems by producing aﬁxed-length representation for each feature at thetoken level.
this representation can be concate-nated with the other token-level features and inputto the encoder..for a speech input with f frames, the raw fea-tures input to the cnn have dimensions 6 × f ,where 6 is the number of total features for eachframe (3 pitch features and 3 intensity features).
several ﬁlters of different sizes then perform one-dimensional convolution of the input.
these differ-ent ﬁlters allow the cnn to integrate informationon various time scales.
we apply n of each of thesem ﬁlters, for a total of mn ﬁlters.
we use the hy-perparameters described by tran et al.
(2018): n= 32 ﬁlters of widths w = [5, 10, 25, 50], for a totalof mn = 128 ﬁlters.
the output of each ﬁlter isthen max-pooled, which converts the features for agiven token to a uniform dimension..these cnn-processed features are then concate-nated with the token-level prosodic features (pauseand duration) and the text embedding for the token,and then input to the encoder.
the cnn is trainedalong with the encoder-decoder model..a.1.2 the encoder.
the encoder is a standard transformer with eightattention heads, based on the work of kitaev andklein (2018).
for each word of input xi, the trans-former encoder produces a representation of theforward context, −→yi , and the backward context ←−yi .
we represent a given span between indices i andj by subtracting the forward representations andbackward representations and concatenating theresults:.
v(i,j) = [−→yj − −→yi ; ←−yj − ←−yi ].
the next section explains how we use this span rep-resentation v(i,j) to generate scores for constituentsin a tree..a.1.3 the decoder.
the decoder is a chart-style span-based decoder.
its goal is to output the correct tree t for an inputx1, ..., xn.
each tree’s score s(t ) is simply thesum of the scores of its constituents, where eachconstituent is deﬁned by a start index i, an endindex j, and a label l..stree(t ) =.
slabel(i, j, l) + sspan(i, j).
(cid:88).
i,j,label∈t.
as this formula for tree score shows, each con-stituent’s score is made up of a label score and spanscore.
conceptually, the span score corresponds tothe probability that a constituent exists that exactly.
990covers span (i, j) in the input; the label score re-ﬂects the probability that the span (i, j) has a givenconstituent label (e.g., s, np).
the decoder musthave a way of determining the label score and spanscore for each constituent..the label scores are generated by passing thespan representation v(i,j) through a two-layer feed-forward network like the feed-forward networsvaswani et al.
(2017) use:.
this method requires that the grammar be inchomsky-normal form, which the model achievesby collapsing strings of unary rules and usingdummy nodes to make n-ary rules into binary rules.
with this method of generating tree scores fromspan representations, we can then deﬁne the hingeloss for our predicted tree ˆt compared to the goldtree t ∗, where ∆ represents the hamming loss onlabeled spans:.
slabel(i, j) = m2(relu(ln orm(m1v(i,j))+c1))+c2.
the lth element of this vector is the score for the.
a.2 model training details.
label l:.
f f n (x) = w2(relu(w1x + b1)) + b2.
following kitaev and klein (2018), we also includea layer normalization step (ln orm).
this feed-forward network produces a vector for each spanslabel(i, j) whose size is the number of possiblelabels:.
slabel(i, j, l) = [slabel(i, j, )]l.we also need to calculate the span score, butcalculating the score for all spans (i, j) wouldbe prohibitively inefﬁcient.
instead, kitaev andklein (2018), following the approach of stern et al.
(2017) and gaddy et al.
(2018), use a dynamic pro-gramming strategy based on the cky algorithm.
the score for a span (i, j) is calculated in termsof the scores of its subspans, which allows spanscores to be built up recursively from the storedscores of smaller spans.
a given span (i, j) can besplit at any internal point into two subspans, (i, k)and (k, j).
each of these possible splits (i, k, j) isassigned a score, calculated by summing the spanscores of the subspans:.
ssplit(i, k, j) = sspan(i, k) + sspan(k, j).
then, to ﬁnd the best score for this span (i, j), weﬁnd the label and split that maximize the followingsum:.
loss( ˆt , t ∗) =.
max[0, max.
[∆( ˆt , t ∗) + stree( ˆt )].
t.− stree(t ∗)].
we then use this loss function to train ourencoder-decoder, including the cnn input mod-ule for speech..we used the hyperparameters speciﬁed in (tranet al., 2019)’s code base, documented in table 7.each model was trained for 50 epochs on a singlenvidia gtx 1080 gpu, which took approximately7 hours per model.
the text-only models have ap-proximately 23m trainable parameters each, whilethe text+prosody models have approximately 20mtrainable parameters..hyperparameterepochstext embedding dim.
max.
seq.
lengthdropoutnum.
layersnum.
headsmodel dim.
key/value dim..value503002700.348153696.table 7: model hyperparameters.
note that the maxi-mum sequence length for the su-based model is 200tokens..sbest(i, j) = maxl,k.
[slabel(i, j, l) + ssplit(i, k, j)].
a.3.
incorporating bert.
all spans are recursively split into subspans, even-tually arriving at single-word spans.
since thereare no splits possible for a single-word span, thescore for a single word span is simply that word’sbest label score:.
sbest(i, i + 1) = max.
[slabel(i, i + 1, l)].
l.we include here the results for both the su- andturn-based parsers when given bert embeddings(devlin et al., 2019) in place of glove embed-dings (pennington et al., 2014).
we train one modelfor each experimental condition, using the randomseed we used to generate the results shown in ta-ble 1. we see in table 8 that bert improves the.
991turn.
sbar.
although we just moved to california and uh the cost of living ... is ... pathological.
(a) text+prosody model output.
turn.
sbar.
s.although we just moved to california.
and uh the cost of living ... is ... pathological.
(b) text-only model output.
figure 3: an example of a clause attachment error.
the tree shown in (a) is correctly parsed as a single su by thetext+prosody model, whereas the text-only model incorrectly segments this into two sus, as shown in (b).
thisexample is taken from the development set and slightly simpliﬁed for space (shown by ellipses)..performance in all experimental conditions.
thesu-based text+prosody parser does outperform theturn-based parser by a statistically signiﬁcant mar-gin, though this result was obtained on just onemodel instead of 10 randomly seeded models.
how-ever, the turn-based parser’s performance remainsquite close to the su-based parser’s despite havinga more difﬁcult task to perform, and otherwise thebasic pattern from the glove results holds here..su-based turn-based.
text onlytext+prosody.
91.9092.77.
88.0892.12.table 8: development set f1 when using bert embed-dings, comparing the turn-based model to the su-basedmodel..a.4 clause attachment illustration.
figure 3 illustrates an example of an error classiﬁedas a clause attachment error by the berkeley parseranalyser (kummerfeld et al., 2012)..992