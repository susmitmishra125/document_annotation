mask-align: self-supervised neural word alignment.
chi chen1,3,4, maosong sun1,3,4,5, yang liu∗ 1,2,3,4,51department of computer science and technology, tsinghua university, beijing, china2institute for ai industry research, tsinghua university, beijing, china3institute for artiﬁcial intelligence, tsinghua university, beijing, china4beijing national research center for information science and technology5beijing academy of artiﬁcial intelligence.
abstract.
word alignment, which aims to align transla-tionally equivalent words between source andtarget sentences, plays an important role inmany natural language processing tasks.
cur-rent unsupervised neural alignment methodsfocus on inducing alignments from neural ma-chine translation models, which does not lever-age the full context in the target sequence.
inthis paper, we propose mask-align, a self-supervised word alignment model that takesadvantage of the full context on the target side.
our model parallelly masks out each target to-ken and predicts it conditioned on both sourceand the remaining target tokens.
this two-stepprocess is based on the assumption that thesource token contributing most to recoveringthe masked target token should be aligned.
we also introduce an attention variant calledleaky attention, which alleviates the problemof high cross-attention weights on speciﬁc to-kens such as periods.
experiments on four lan-guage pairs show that our model outperformsprevious unsupervised neural aligners and ob-tains new state-of-the-art results.1.
1.introduction.
word alignment is an important task of ﬁndingthe correspondence between words in a sentencepair (brown et al., 1993) and used to be a keycomponent of statistical machine translation (smt)(koehn et al., 2003; dyer et al., 2013).
althoughword alignment is no longer explicitly modeled inneural machine translation (nmt) (bahdanau et al.,2015; vaswani et al., 2017), it is often leveraged toanalyze nmt models (tu et al., 2016; ding et al.,2017).
word alignment is also used in many otherscenarios such as imposing lexical constraints onthe decoding process (arthur et al., 2016; hasler.
∗corresponding author1code can be found at https://github.com/thunlp-mt/.
mask-align..figure 1: an example of inducing an alignment link fortarget token “tokyo” in mask-align.
first, we maskout “tokyo” and predict it with source and other targettokens.
then, the source token “tokio” that contributesmost to recovering the masked word (highlighted inred) is chosen to be aligned to “tokyo”..et al., 2018), improving automatic post-editing (palet al., 2017) , and providing guidance for translatorsin computer-aided translation (dagan et al., 1993).
compared with statistical methods, neural meth-ods can learn representations end-to-end from rawdata and have been successfully applied to super-vised word alignment (yang et al., 2013; tamuraet al., 2014).
for unsupervised word alignment,however, previous neural methods fail to signif-icantly exceed their statistical counterparts suchas fast-align (dyer et al., 2013) and giza++(och and ney, 2003).
recently, there is a surge ofinterest in nmt-based alignment methods whichtake alignments as a by-product of nmt systems(li et al., 2019; garg et al., 2019; zenkel et al.,2019, 2020; chen et al., 2020).
using attentionweights or feature importance measures to inducealignments for to-be-predicted target tokens, thesemethods outperform unsupervised statistical align-ers like giza++ on a variety of language pairs..although nmt-based unsupervised alignershave proven to be effective, they suffer from twomajor limitations.
first, due to the autoregressive.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4781–4791august1–6,2021.©2021associationforcomputationallinguistics4781ich   wurde   1968   in  tokio   geboren  .
source :i   was   born   inin   1968  .target :___tokyoinduced alignment link:    tokio - tokyofigure 2: the architecture of mask-align..property of nmt systems (sutskever et al., 2014),they only leverage part of the target context.
thisinevitably brings noisy alignments when the pre-diction is ambiguous.
consider the target sentencein figure 1. when predicting “tokyo”, an nmtsystem may generate “1968” because future con-text is not observed, leading to a wrong alignmentlink (“1968”, “tokyo”).
second, they have to incor-porate an additional guided alignment loss (chenet al., 2016) to outperform giza++.
this loss re-quires pseudo alignments of the full training datato guide the training of the model.
although thesepseudo alignments can be utilized to partially alle-viate the problem of ignoring future context, theyare computationally expensive to obtain..in this paper, we propose a self-supervisedmodel speciﬁcally designed for the word alignmenttask, namely mask-align.
our model parallellymasks out each target token and recovers it condi-tioned on the source and other target tokens.
fig-ure 1 shows an example where the target token“tokyo” is masked out and re-predicted.
intuitively,as all source tokens except “tokio” can ﬁnd theircounterparts on the target side, “tokio” should bealigned to the masked token.
based on this intu-ition, we assume that the source token contributingmost to recovering a masked target token should bealigned to that target token.
compared with nmt-based methods, mask-align is able to take fulladvantage of bidirectional context on the target sideand hopefully achieves higher alignment quality.
we also introduce an attention variant called leakyattention to reduce the high attention weights onspeciﬁc tokens such as periods.
by encouragingagreement between two directional models both.
for training and inference, our method consistentlyoutperforms the state-of-the-art on four languagepairs without using guided alignment loss..2 approach.
figure 2 shows the architecture of our model.
themodel predicts each target token conditioned on thesource and other target tokens and generates align-ments from the attention weights between sourceand target (section 2.1).
speciﬁcally, our approachintroduces two attention variants, static-kv atten-tion and leaky attention, to efﬁciently obtain atten-tion weights for word alignment.
to better utilizeattention weights from two directions, we encour-age agreement between two unidirectional modelsduring both training (section 2.2) and inference(section 2.3)..2.1 modeling.
conventional unsupervised neural aligners arebased on nmt models (peter et al., 2017; garget al., 2019).
given a source sentence x =x1, .
.
.
, xj and a target sentence y = y1, .
.
.
, yi ,nmt models the probability of the target sentenceconditioned on the source sentence:.
p (y|x; θ) =.
p (yi|y<i, x; θ).
(1).
i(cid:89).
i=1.
where y<i is a partial translation.
one problem ofthis type of approaches is that they fail to exploit thefuture context on the target side, which is probablyhelpful for word alignment..to address this problem, we model the sameconditional probability but predict each target token.
4782w1 + p1w2 + p2w4 + p4encoderlayerw3 + p3h1h2h4h3w1+p1p1w2+p2p2w3+p3p3w4+p4p4t4t3static-kv attentionleaky attentionfeed forward✕ l l ✕(cid:72)(cid:76)(cid:81)(cid:75)(cid:88)(cid:81)(cid:71)(cid:85)(cid:68)(cid:81)(cid:81)(cid:87)(cid:72)(cid:90)(cid:72)(cid:74)(cid:68)(cid:71)(cid:82)(cid:74)(cid:85)(cid:88)(cid:81)(cid:86)(cid:68)(cid:90)(cid:68)(cid:92)(cid:68)t1t2t1t2t3t4h1h2h3h4linear & softmax(cid:71)(cid:82)(cid:74)(cid:85)(cid:88)(cid:81)(cid:86)(cid:68)(cid:90)(cid:68)(cid:92)attention weightsalignment(cid:68)(cid:71)(cid:82)(cid:74)(cid:85)(cid:88)(cid:81)(cid:86)(cid:68)(cid:90)(cid:68)(cid:92)(cid:72)(cid:76)(cid:81)(cid:75)(cid:88)(cid:81)(cid:71)(cid:85)(cid:68)(cid:81)(cid:81)(cid:87)(cid:72)(cid:90)(cid:72)(cid:74)yi conditioned on the source sentence x and theremaining target tokens y\yi:.
p (y|x; θ) =.
p (yi|y\yi, x; θ).
(2).
i(cid:89).
i=1.
this equals to masking out each yi and then recov-ering it.
we build our model on top of transformer(vaswani et al., 2017) which is the state-of-the-artsequence-to-sequence architecture.
next, we willdiscuss in detail the implementation of our model..static-kv attentionas self-attention is fully-connected, directly com-puting (cid:81)ii=1 p (yi|y\yi, x; θ) with a vanilla trans-former requires i separate forward passes, in eachof which only one target token is masked outand predicted.
this is costly and time-consuming.
therefore, how to parallelly mask out and predictall target tokens in a single pass is important..to do so, a major challenge is to avoid the rep-resentation of a masked token getting involved inthe prediction process of itself.
inspired by kasaiet al.
(2020), we modify the self-attention in thetransformer decoder to perform the forward passesconcurrently.
given the word embedding wi andposition embedding pi for target token yi, we ﬁrstseparate the query inputs qi from key ki and valueinputs vi to prevent the to-be-predicted token itselffrom participating in the prediction:.
(3).
(4).
(5).
(6).
(7).
(8).
qi = piwqki = (wi + pi)wkvi = (wi + pi)wv.
where wq, wk and wv are parameter matrices.
the hidden representation hi for yi is computed byattending to keys and values, k(cid:54)=i and v(cid:54)=i, thatcorrespond to the remaining tokens y\yi:.
hi = attention(qi, k(cid:54)=i, v(cid:54)=i).
k(cid:54)=i = concat({km|m (cid:54)= i})v(cid:54)=i = concat({vm|m (cid:54)= i}).
in this way, we ensure that hi is isolated from theword embedding wi in a single decoder layer.
how-ever, there exists a problem of information leakageif we update the key and value inputs for each posi-tion across decoder layers since they will containthe representation of each position from previouslayers.
therefore, we keep the key and value in-puts unchanged and only update the query inputs.
figure 3: an example of inducing alignments from at-tention weights where the source token “.” has high at-tention weights.
the two “in”s in the target sentenceare wrongly aligned to “.” because of the high attentionweights on it..to avoid information leakage:.
i = attention(qlhli = hl−1i wqql.
i, k(cid:54)=i, v(cid:54)=i).
(9).
(10).
i and hl.
where qli denote the query inputs and hiddenstates for yi in the l-th layer, respectively.
h0i is ini-tialized with pi.
we name this variant of attentionthe static-kv attention.
by static-kv, we meanthe keys and values are unchanged across differentlayers in our approach.
our model replaces all self-attention in the decoder with static-kv attention..leaky attention.
extracting alignments from vanilla cross-attentionoften suffers from the high attention weights onsome speciﬁc source tokens such as periods, [eos],or other high frequency tokens (see figure 3).
thisis similar to the “garbage collectors” effect (moore,2004) in statistical aligners, where a source tokenis aligned to too many target tokens.
hereinafter,we will refer to these tokens as collectors.
as aresult of such effect, many target tokens (e.g., the.
4783iwasborninin1968ichwurde1968intokiogeborentokyo..alignmentattention weightsiwasbornintokyoin1968.ichwurde1968intokiogeboren.
deviation to initialize knull and vnull to ensurethat their initial norms are rather small.
when ex-tracting alignments, we only consider the attentionmatrix without the leak position..note that leaky attention is different from addinga special token in the source sequence, which willshare the same high attention weights with the ex-isting collector instead of calibrating it (vig and be-linkov, 2019).
our parameterized method is moreﬂexible than leaky-softmax (sabour et al., 2017)which adds an extra dimension with the value ofzero to the routing logits.
in section 2.2, we willshow that leaky attention is also helpful for apply-ing agreement-based training on two directionalmodels..we remove the cross-attention in all but the lastdecoder layer.
this makes the interaction betweenthe source and target restricted in the last layer.
our experiments demonstrate that this modiﬁca-tion improves alignment results with fewer modelparameters..2.2 training.
to better utilize the attention weights from two di-rections, we apply an agreement loss in the trainingprocess to improve the symmetry of our model,which has proven effective in statistical alignmentmodels (liang et al., 2006; liu et al., 2015).
givena parallel sentence pair (cid:104)x, y(cid:105), we can obtain theattention weights from two different directions, de-noted as w x→y and w y→x.
as alignment is bi-jective, w x→y is supposed to be equal to the trans-pose of w y→x.
we encourage this kind of symme-try through an agreement loss:.
la = mse.
w x→y, w (cid:62).
y→x.
(14).
(cid:16).
(cid:17).
where mse represents the mean squared error..for vanilla attention, la is hardly small becauseof the normalization constraint.
as shown in figure4, due to the use of softmax activation, the minimalvalue of la is 0.25 for vanilla attention.
usingleaky attention, our approach can achieve a loweragreement loss (la = 0.1) by adjusting the weightson the leak position..however, our model may converge to a degen-erate case of zero agreement loss where attentionweights are all zero except for the leak position.
we circumvent this case by introducing an entropy.
figure 4: an illustrative example of the attentionweights from two directional models using vanilla andleaky attention.
leaky attention provides a leak posi-tion “[null]” to collect extra attention weights..two “in”s in figure 3) will be incorrectly alignedto the collectors according to the attention weights.
this phenomenon has been studied in previousworks (clark et al., 2019; kobayashi et al., 2020).
kobayashi et al.
(2020) show that the norms of thevalue vectors for the collectors are usually small,making their inﬂuence on attention outputs actuallylimited.
we conjecture that this phenomenon is dueto the incapability of nmt-based aligners to dealwith tokens that have no counterparts on the otherside because there is no empty (null) token thatis widely used in statistical aligners (brown et al.,1993; och and ney, 2003)..we propose to explicitly model the null to-ken with an attention variant, namely leaky atten-tion.
as shown in figure 4, when calculating cross-attention weights, leaky attention provides an extra“leak” position in addition to the encoder outputs.
acting as the null token, this leak position is ex-pected to address the biased attention weight prob-lem.
to be speciﬁc, we parameterize the key andvalue vectors as knull and vnull for the leak po-sition in the cross-attention, and concatenate themwith the transformed vectors of the encoder outputs.
the attention output zi is computed as follows:.
zi = attention(hli wq, k, v)k = concat(knull, hencwk)v = concat(vnull, hencwv ).
(11).
(12).
(13).
where henc denotes encoder outputs.
2 we use anormal distribution with a mean of 0 and a small.
2a similar attention implementation can be foundhttps://github.com/pytorch/fairseq/blob/master/fairseq/.
inmodules/multihead attention.py..47840.50.5nottruefalsch1.01.0nottruefalschnottruefalsch0.40.40.2[null]0.20.4nottruefalsch0.80.6[null]leakyattention vanilla attentionloss on the attention weights:.
i(cid:88).
j(cid:88).
1i.j=1.
i=1w ijj (w ij.
x→y + λ.x→y + λ).
le,x→y = −.
˜w ij.
x→y log ˜wij.
(15).
˜w ij.
x→y =.
(cid:80).
(16).
where ˜w ijx→y is the renormalized attention weightsand λ is a smoothing hyperparamter.
similarly, wehave le,y→x for the inverse direction..we jointly train two directional models using the.
following loss:.
l = lx→y + ly→x + αla+β(le,x→y + le,y→x).
(17).
where lx→y and ly→x are nll losses, α and βare hyperparameters..2.3.inference.
when extracting alignments, we compute an align-ment score sij for yi and xj as the harmonic meanof attention weights w ijy→x from twodirectional models:.
x→y and w ji.
sij =.
2 w ijw ij.
x→y w jix→y + w ji.
y→x.
y→x.
(18).
we use the harmonic mean because we assume alarge sij requires both w ijy→x to belarge.
word alignments can be induced from thealignment score matrix as follows:.
x→y and w ji.
(zenkel et al., 2019, 2020) and used the preprocess-ing scripts from zenkel et al.
(2019)4. followingding et al.
(2019), we take the last 1000 sentencesof the training data for these three datasets as vali-dation sets.
we used a joint source and target bytepair encoding (bpe) (sennrich et al., 2016) with40k merge operations.
during training, we ﬁlteredout sentences with the length of 1 to ensure thevalidity of the masking process..3.2 settings.
we implemented our model based on the trans-former architecture (vaswani et al., 2017).
the en-coder consists of 6 standard transformer encoderlayers.
the decoder is composed of 6 layers, eachof which contains static-kv attention while onlythe last layer is equipped with leaky attention.
weset the embedding size to 512, the hidden size to1024, and attention heads to 4. the input and outputembeddings are shared for the decoder..we trained the models with a batch size of 36ktokens.
we used early stopping based on the pre-diction accuracy on the validation sets.
we tunedthe hyperparameters via grid search on the chinese-english validation set as it contains gold word align-ments.
in all of our experiments, we set λ = 0.05(eq.
(16)), α = 5, β = 1 (eq.
(17)) and τ = 0.2(eq.
(19)).
the evaluation metric is alignment er-ror rate (aer) (och and ney, 2000)..3.3 baselines.
aij =.
(cid:26) 10.if sij ≥ τotherwise.
(19).
we introduce the following unsupervised neuralbaselines besides two statistical baselines fast-align and giza++:.
where τ is a threshold..3 experiments.
3.1 datasets.
we conducted our experiments on four publicdatasets: german-english (de-en), english-french(en-fr), romanian-english (ro-en) and chinese-english (zh-en).
the chinese-english training setis from the ldc corpus that consists of 1.2m sen-tence pairs.
for validation and testing, we used thechinese-english alignment dataset from liu et al.
(2005)3, which contains 450 sentence pairs for val-idation and 450 for testing.
for other three lan-guage pairs, we followed the experimental setup in.
3http://nlp.csai.tsinghua.edu.cn/∼ly/systems/.
tsinghuaaligner/tsinghuaaligner.html.
• naive-att (garg et al., 2019): a methodthat induces alignments from cross-attentionweights of the best (usually penultimate) de-coder layer in a vanilla tranformer..• naive-att-last: same as naive-att ex-cept that only the last decoder layer performscross-attention..• addsgd (zenkel et al., 2019): a method thatadds an extra alignment layer to repredict theto-be-aligned target token..• mtl-fullc (garg et al., 2019): a methodthat supervises an attention head with sym-metrized naive-att alignments in a multi-task learning framework..4https://github.com/lilt/alignment-scripts.
4785method.
guided de-en en-fr ro-en zh-en.
fast-align (dyer et al., 2013)giza++ (och and ney, 2003).
naive-att (garg et al., 2019)naive-att-lastaddsgd (zenkel et al., 2019)mtl-fullc (garg et al., 2019)bao (zenkel et al., 2020)shift-att (chen et al., 2020).
mtl-fullc-gz (garg et al., 2019)bao-guided (zenkel et al., 2020)shift-aet (chen et al., 2020).
mask-align.
nn.nnnnnn.yyy.n.25.717.8.
31.928.421.220.217.917.9.
16.016.315.4.
14.4.
12.16.1.
18.517.710.07.78.46.6.
4.65.04.7.
4.4.
31.826.0.
32.932.427.626.024.123.9.
23.123.421.2.
19.5.
-18.5.
28.926.4---20.2.
--17.2.
13.8.table 1: alignment error rate (aer) scores on four datasets for different alignment methods.
the lower aer, thebetter.
“guided” denotes whether the guided alignment loss is used during training.
all results are symmetrized.
we highlight the best results for each language pair in bold..• bao (zenkel et al., 2020): an improved ver-sion of addsgd that extracts alignmentswith bidirectional attention optimization.
• shift-att (chen et al., 2020): a method thatinduces alignments when the to-be-alignedtatget token is the decoder input instead of theoutput..we also included three additional baselines withguided training: (1) mtl-fullc-gz (garg et al.,2019) which replaces the alignment labels in mtl-fullc with giza++ results, (2) bao-guided(zenkel et al., 2020) which uses alignments frombao for guided alignment training, (3) shift-aet (chen et al., 2020) which trains an addi-tional alignment module with supervision fromsymmetrized shift-att alignments..3.4 main results.
table 1 shows the results on four datasets.
ourapproach signiﬁcantly outperforms all statisticaland neural baselines.
speciﬁcally, it improves overgiza++ by 1.7-6.5 aer points across differentlanguage pairs without using any guided alignmentloss, making it a good substitute to this commonlyused statistical alignment tool.
compared to shift-att, the best neural methods without guided train-ing, our approach achieves a gain of 2.2-6.4 aerpoints with fewer parameters (as we remove somecross-attention sublayers in the decoder)..when compared with baselines using guidedtraining, we ﬁnd mask-align still achieves sub-.
masked leaky agree aer.
×(cid:88)×××(cid:88)(cid:88).
(cid:88).
××(cid:88)×(cid:88)×(cid:88).
(cid:88).
×××(cid:88)(cid:88)(cid:88)×.
(cid:88).
28.427.228.326.623.417.617.2.
14.4.table 2: ablation study on the german-english dataset.
we use “masked” to denote the masked modeling withstatic-kv attention in section 2.1, “leaky” to denotethe leaky attention in section 2.1 and “agree” to denotethe agreement-based training and inference in sections2.2 and 2.3..stantial improvements over all methods.
for ex-ample, on the romanian-english dataset, it im-proves over shift-aet by 1.7 aer points.
recallthat our method is fully end-to-end, which doesnot require a time-consuming process of obtainingpseudo alignments for full training data..3.5 ablation study.
table 2 shows the ablation results on the german-english dataset.
as we can see, masked modelingseems to play a critical role since removing it willdeteriorate the performance by at least 9.0 aer.
we also ﬁnd that leaky attention and agreement-based training and inference are both important.
removing any of them will signiﬁcantly diminish.
4786(a) vanilla attention.
(b) leaky attention.
figure 5: attention weights from vanilla and leaky attention.
“mr” is short for “menschenrechte”, which means“human rights” in english.
we use “[null]” to denote the leak position..source sentence [null].
vanilla attentionleaky attention.
-1.9.mr.21.128.5.in.
11.717.2.der.
5.218.1.welt.
1995.
\.
1996.
15.020.2.
21.224.2.
17.721.4.
21.823.8.table 3: norms of the transformed value vectors of different source tokens in figure 5. we mark the minimumnorm for each variant of attention with boldface..the performance..3.6 effect of leaky attention.
figure 5 shows the attention weights from vanillaand leaky attention and table 3 presents the normsof the transformed value vectors of each source to-ken for two types of attention.
for vanilla attention,we can see large weights on the high frequencytoken “der” and the small norm of its transformedvalue vector.
as a result, the target token “in” willbe wrongly aligned to “der”.
while for leaky atten-tion, we observe a similar phenomenon on the leakposition “[null]”, and “in” will not be aligned toany source tokens since the weights on all source to-kens are small.
this example shows leaky attentioncan effectively prevent the collector phenomenon..3.7 analysis.
removing end punctuation to further investi-gate the performance of leaky attention, we testedan extraction method that excludes the attentionweights on the end punctuation of a source sentence.
the reason behind this is that when the source sen-tence contains the end punctuation, it will act as thecollector in most cases.
therefore removing it will.
method w/ punc.
w/o punc..vanilla attentionleaky attention.
27.217.2.
17.717.4.table 4: comparison of aer with and without consid-ering the attention weights on end punctuation..alleviate the effect of collectors to a certain extent.
table 4 shows the comparison results.
for vanilla at-tention, removing end punctuation obtains a gain of7.7 aer points.
for leaky attention, however, suchextraction method brings no improvement on align-ment quality.
this suggests that leaky attention caneffectively alleviate the problem of collectors..case study figure 6 shows the attention weightsfrom four different models for the example in fig-ure 1. as we have discussed in section 1, in thisexample, nmt-based methods might fail to resolveambiguity when predicting the target token “tokyo”.
from the attention weight matrices, we can see thatnmt-based methods (figures 6(b) and 6(c)) in-deed put high weights wrongly on “1968” in thesource sentence.
as for mask-align, we can see.
4787humanrightsthroughouttheworldin1995-1996mrinderwelt1995\1996humanrightsthroughouttheworldin1995-1996mrinderwelt1995\1996[null](a) reference.
(b) naive-att-last.
(c) shift-att.
(d) mask-align.
figure 6: attention weights from different models for the example in figure 1. gold alignment is shown in (a).
fortarget token “tokyo”, nmt-based methods naive-att-last (b) and shift-att (c) assign high weights to thewrongly aligned source token “1968”, while mask-align (d) focuses on the correct source token “tokio”..alignment (cpwa) maintains at a low level, indi-cating that our model does not degenerate into atarget masked language model despite the use ofbidirectional target context..4 related work.
our work is closely related to unsupervised neuralword alignment.
while early unsupervised neuralaligners (tamura et al., 2014; alkhouli et al., 2016;peter et al., 2017) failed to outperform their statisti-cal counterparts such as fast-align (dyer et al.,2013) and giza++ (och and ney, 2003), recentstudies have made signiﬁcant progress by inducingalignments from nmt models (garg et al., 2019;zenkel et al., 2019, 2020; chen et al., 2020).
ourwork differs from prior studies in that we design anovel self-supervised model that is capable of uti-lizing more target context than nmt-based modelsto generate high quality alignments without usingguided training..our work is also inspired by the success ofconditional masked language models (cmlms)(ghazvininejad et al., 2019), which have been ap-plied to non-autoregressive machine translation.
the cmlm can leverage both previous and futurecontext on the target side for sequence-to-sequencetasks with the masking mechanism.
kasai et al.
(2020) extend it with a disentangled context trans-former that predicts every target token conditionedon arbitrary context.
by taking the characteristicsof word alignment into consideration, we proposeto use static-kv attention to achieve masking andaligning in parallel.
to the best of our knowledge,this is the ﬁrst work that incorporates a cmlm intoalignment models..figure 7: relations between prediction and alignmentfor different methods..that the attention weights are highly consistent withthe gold alignment, showing that our method cangenerate sparse and accurate attention weights..prediction and alignment we analyzed the rele-vance between the correctness of word-level predic-tion and alignment.
we regard a word as correctlypredicted if any of its subwords are correct and ascorrectly aligned if one of its possible alignmentis matched.
figure 7 shows the results.
we dividetarget tokens into four categories:.
1. cpca: correct prediction & correct alignment;.
2. wpca: wrong prediction & correct alignment;.
3. cpwa: correct prediction & wrong alignment;.
4. wpwa: wrong prediction & wrong alignment..compared with other methods, mask-alignsigniﬁcantly reduces the alignment errors caused bywrong predictions (wpwa).
in addition, the num-ber of the tokens with correct prediction but wrong.
4788iwasbornintokyoin1968.ichwurde1968intokiogeboren.ichwurde1968intokiogeboren.[eos]ichwurde1968intokiogeboren.
[eos]ichwurde1968intokiogeboren.naive-attnaive-att-lastshift-attmask-alignmethod01234567number of tokens (k)cpcawpcacpwawpwa5 conclusion.
we have presented a self-supervised neural align-ment model mask-align.
our model parallellymasks out and predicts each target token.
wepropose static-kv attention and leaky attentionto achieve parallel computation and address the“garbage collectors” problem, respectively.
experi-ments show that mask-align achieves new state-of-the-art results without using the guided align-ment loss.
in the future, we plan to extend ourmethod to directly generate symmetrized align-ments without leveraging the agreement betweentwo unidirectional models..acknowledgments.
this work was supported by the national keyr&d program of china (no.
2017yfb0202204),national natural science foundation of china(no.61925601, no.
61772302) and huawei noah’sark lab.
we thank all anonymous reviewers fortheir valuable comments and suggestions on thiswork..references.
tamer alkhouli, gabriel bretschner, jan-thorsten pe-ter, mohammed hethnawi, andreas guta, and her-mann ney.
2016. alignment-based neural machinetranslation.
in proceedings of the first conferenceon machine translation: volume 1, research papers,pages 54–65, berlin, germany.
association for com-putational linguistics..philip arthur, graham neubig, and satoshi nakamura.
2016.incorporating discrete translation lexiconsinto neural machine translation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 1557–1567, austin,texas.
association for computational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..peter f. brown, stephen a. della pietra, vincent j.della pietra, and robert l. mercer.
1993. the math-ematics of statistical machine translation: parameterestimation.
computational linguistics, 19(2):263–311..wenhu chen, evgeny matusov, shahram khadivi, andjan-thorsten peter.
2016. guided alignment train-ing for topic-aware neural machine translation.
as-sociation for machine translation in the americas,page 121..yun chen, yang liu, guanhua chen, xin jiang, andqun liu.
2020. accurate word alignment inductionfrom neural machine translation.
in proceedings ofthe 2020 conference on empirical methods in natu-ral language processing (emnlp), pages 566–576,online.
association for computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..ido dagan, kenneth church, and willian gale.
1993.robust bilingual word alignment for machine aidedtranslation.
in very large corpora: academic andindustrial perspectives..shuoyang ding, hainan xu, and philipp koehn.
2019.saliency-driven word alignment interpretation forin proceedings of theneural machine translation.
fourth conference on machine translation (volume1: research papers), pages 1–12, florence, italy.
as-sociation for computational linguistics..yanzhuo ding, yang liu, huanbo luan, and maosongsun.
2017. visualizing and understanding neuralmachine translation.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1150–1159, vancouver, canada.
association for computa-tional linguistics..chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameter-in proceedings of theization of ibm model 2.
2013 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, pages 644–648, at-lanta, georgia.
association for computational lin-guistics..sarthak garg, stephan peitz, udhyakumar nallasamy,and matthias paulik.
2019. jointly learning to alignand translate with transformer models.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 4453–4462, hongkong, china.
association for computational lin-guistics..marjan ghazvininejad, omer levy, yinhan liu, andluke zettlemoyer.
2019. mask-predict: parallel de-coding of conditional masked language models.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6112–6121, hong kong, china.
association for computa-tional linguistics..4789eva hasler, adri`a de gispert, gonzalo iglesias, andbill byrne.
2018. neural machine translation decod-ing with terminology constraints.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 506–512, new orleans, louisiana.
as-sociation for computational linguistics..jungo kasai, james cross, marjan ghazvininejad, andjiatao gu.
2020. non-autoregressive machine trans-lation with disentangled context transformer.
in pro-ceedings of the 37th international conference onmachine learning, icml 2020, 13-18 july 2020,virtual event, volume 119 of proceedings of ma-chine learning research, pages 5144–5155.
pmlr..goro kobayashi, tatsuki kuribayashi, sho yokoi, andkentaro inui.
2020. attention module is not only aweight: analyzing transformers with vector norms.
in proceedings of the 2020 conference on empiri-cal methods in natural language processing andthe 10th international joint conference on naturallanguage processing (emnlp-ijcnlp)..philipp koehn, franz j. och, and daniel marcu.
2003.statistical phrase-based translation.
in proceedingsof the 2003 human language technology confer-ence of the north american chapter of the associa-tion for computational linguistics, pages 127–133..xintong li, guanlin li, lemao liu, max meng, andshuming shi.
2019. on the word alignment fromin proceedings of theneural machine translation.
57th annual meeting of the association for com-putational linguistics, pages 1293–1303, florence,italy.
association for computational linguistics..percy liang, ben taskar, and dan klein.
2006. align-ment by agreement.
in proceedings of the humanlanguage technology conference of the naacl,main conference, pages 104–111, new york city,usa.
association for computational linguistics..chunyang liu, yang liu, maosong sun, huanbo luan,and heng yu.
2015. generalized agreement forin proceedings ofbidirectional word alignment.
the 2015 conference on empirical methods in nat-ural language processing, pages 1828–1836, lis-bon, portugal.
association for computational lin-guistics..yang liu, qun liu, and shouxun lin.
2005. log-in proceed-linear models for word alignment.
ings of the 43rd annual meeting of the associationfor computational linguistics (acl’05), pages 459–466, ann arbor, michigan.
association for compu-tational linguistics..robert c. moore.
2004..improving ibm word align-in proceedings of the 42nd an-ment model 1.nual meeting of the association for computationallinguistics (acl-04), pages 518–525, barcelona,spain..franz josef och and hermann ney.
2000..improvedstatistical alignment models.
in proceedings of the38th annual meeting of the association for com-putational linguistics, pages 440–447, hong kong.
association for computational linguistics..franz josef och and hermann ney.
2003. a systematiccomparison of various statistical alignment models.
computational linguistics, 29(1):19–51..santanu pal, sudip kumar naskar, mihaela vela, qunliu, and josef van genabith.
2017. neural auto-matic post-editing using prior alignment and rerank-in proceedings of the 15th conference of theing.
european chapter of the association for computa-tional linguistics: volume 2, short papers, pages349–355, valencia, spain.
association for compu-tational linguistics..jan-thorsten peter, arne nix, and hermann ney.
2017. generating alignments using target fore-sight in attention-based neural machine translation.
the prague bulletin of mathematical linguistics,108(1):27–36..sara sabour, nicholas frosst, and geoffrey e. hinton.
in ad-2017. dynamic routing between capsules.
vances in neural information processing systems30: annual conference on neural information pro-cessing systems 2017, december 4-9, 2017, longbeach, ca, usa, pages 3856–3866..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725, berlin, germany.
association for computa-tional linguistics..ilya sutskever, oriol vinyals, and quoc v. le.
2014.sequence to sequence learning with neural networks.
in advances in neural information processing sys-tems 27: annual conference on neural informa-tion processing systems 2014, december 8-13 2014,montreal, quebec, canada, pages 3104–3112..akihiro tamura, taro watanabe, and eiichiro sumita.
2014. recurrent neural networks for word align-the 52nd an-in proceedings ofment model.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1470–1480, baltimore, maryland.
association for compu-tational linguistics..zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neu-ral machine translation.
in proceedings of the 54thannual meeting of the association for computa-tional linguistics (volume 1: long papers), pages76–85, berlin, germany.
association for computa-tional linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukasz.
4790kaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems 30: annual conference on neuralinformation processing systems 2017, december 4-9, 2017, long beach, ca, usa, pages 5998–6008..jesse vig and yonatan belinkov.
2019. analyzingthe structure of attention in a transformer languagemodel.
in proceedings of the 2019 acl workshopblackboxnlp: analyzing and interpreting neuralnetworks for nlp, pages 63–76, florence, italy.
as-sociation for computational linguistics..nan yang, shujie liu, mu li, ming zhou, and neng-hai yu.
2013. word alignment modeling with con-in proceed-text dependent deep neural network.
ings of the 51st annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 166–175, soﬁa, bulgaria.
associationfor computational linguistics..thomas zenkel, joern wuebker, and john denero.
2019. adding interpretable attention to neural trans-arxivlation models improves word alignment.
preprint arxiv:1901.11359..thomas zenkel, joern wuebker, and john denero.
2020. end-to-end neural word alignment outper-forms giza++.
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 1605–1617, online.
association forcomputational linguistics..4791