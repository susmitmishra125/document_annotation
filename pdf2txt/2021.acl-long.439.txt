dvd: a diagnostic dataset for multi-step reasoningin video grounded dialogue.
hung le‡§∗, chinnadhurai sankar†, seungwhan moon†, ahmad beirami†,alborz geramifard†, satwik kottur††facebook{chinnadhurai, shanemoon, beirami, alborzg, skottur}@fb.com‡singapore management university§institute for infocomm research, a*starhungle.2018@smu.edu.sg.
abstract.
a video-grounded dialogue system is requiredto understand both dialogue, which containssemantic dependencies from turn to turn, andvideo, which contains visual cues of spatialand temporal scene variations.
building suchdialogue systems is a challenging problem, in-volving various reasoning types on both vi-sual and language inputs.
existing bench-marks do not have enough annotations to thor-oughly analyze dialogue systems and under-stand their capabilities and limitations in iso-lation.
these benchmarks are also not ex-plicitly designed to minimise biases that mod-els can exploit without actual reasoning.
toaddress these limitations,in this paper, wepresent dvd, a diagnostic dataset for video-grounded dialogues.
the dataset is designedto contain minimal biases and has detailed an-notations for the different types of reasoningover the spatio-temporal space of video.
dia-logues are synthesized over multiple questionturns, each of which is injected with a setof cross-turn semantic relationships.
we usedvd to analyze existing approaches, provid-ing interesting insights into their abilities andin total, dvd is built from 11klimitations.
cater synthetic videos and contains 10 in-stances of 10-round dialogues for each video,resulting in more than 100k dialogues and 1mquestion-answer pairs.
our code and datasetare publicly available1..1.introduction.
research in visual question answering (vqa) aimsto develop intelligent systems that can reason andanswer questions about visual information.
ear-lier datasets have been introduced to study thisproblem, focusing on images as the visual input(antol et al., 2015; gao et al., 2015; malinowski.
∗ work done when hl was a research intern at facebook.
1github.com/facebookresearch/.
dvdialogues.
figure 1: example dvd dialogue: we demonstrate anexample dialogue in dvd that tests various aspects, in-cluding action recognition, temporal reasoning, spatialreasoning, video interval tracking, and dialogue objecttracking.
qi/ai: question/answer of turn i..and fritz, 2014; zhu et al., 2016) recently, manyqa benchmarks have been proposed to extend thevisual information from the image to video domain(jang et al., 2017; lei et al., 2018; zadeh et al.,2019).
while image qa problems require a sys-tem to learn cross-modality interaction, video qaproblems go beyond and capture visual informationwith temporal variance..as an orthogonal extension from vqa problems,another line of research investigates image/videoqa in a dialogue setting (das et al., 2017; seoet al., 2017; de vries et al., 2017; chattopadhyayet al., 2017; alamri et al., 2019).
in this problem,questions about a given video or image are posi-tioned in a multi-turn dialogue.
in each dialogueturn, a question usually exhibits different types ofcross-turn relations to other questions in prior dia-logue turns, such as object co-reference and topicalignment.
in this work, we investigate the problemof multi-turn video question answering (qa), alsoknown as video-grounded dialogue..numerous approaches to video-grounded dia-logue have shown remarkable performance in build-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5651–5665august1–6,2021.©2021associationforcomputationallinguistics5651q1: until the end of the cube 's rotation , what types of actions does the big thing undertake the most ?
a1: flyingq2: during the same time period , how many sliding objects are there ?
a2: 2q3: among them , there is a ball .
during the whole video , what type of action does it undertake second ?
a3: no actionq4: how about up until now ?
a4: slidingq5: during the red thing 's last slide , how many things are behind the earlier mentioned large object ?
a5: 2q6: how about left of it ?
a6: 0 ...t1t2t3t4t0tobject a:t1t3object b:t2t4t5t6t5t6slidesfliesrotatesing intelligent multimodal systems (hori et al.,2019; schwartz et al., 2019; le et al., 2019; liet al., 2020; le et al., 2020).
however, most ofthese methods exhibit marginal performance gain,and our ability to understand their limitations isimpeded by the complexity of the task.
existingbenchmarks are not designed with enough informa-tion to determine whether current approaches arecapable of sophisticated reasoning and not just ex-ploiting biases, which has been a common concernin vision-language systems (agrawal et al., 2016;goyal et al., 2017; qi et al., 2020)..to address the limitations of existing bench-marks and analyze dialogue systems more efﬁ-ciently, we propose dvd, a diagnostic datasetfor video-grounded dialogues.
we demonstrate anexample dialogue in dvd in figure 1. from scenegraphs and object action annotation of a catervideo (girdhar and ramanan, 2020), we simu-late questions based on reasoning structures, alsoknown as functional programs in clevr (johnsonet al., 2017).
compared to clevr, we introduced17 novel functional modules, designed for videoand dialogue input components.
as illustrated infigure 1, at each dialogue turn, a dvd questiontests dialogue systems to perform different types ofreasoning on videos, such as action recognition andspatio-temporal reasoning.
across turns, we gener-ate questions to be related to each other by incor-porating different types of semantic relationships,including: (1) temporal relation, which requires asystem to learn to localize different temporal seg-ments of the video from turn to turn; (2) objectreference, which requires a system to resolve visualobjects mentioned throughout the dialogue historyin either short-term references (pronouns) or long-term references (e.g.
“the earlier mentioned largeobject”); and (3) topic transfer, which requires asystem to maintain a memory of the last questionturn to solve the question in the current turn..on dvd, we trained a set of baseline methodsand analyzed the results by several aspects of visualand linguistic complexity (section 4).
we foundthat these methods struggle on questions requiringboth video temporal and spatial localization.
theyare also vulnerable to long-term reasoning in bothvideos and dialogues as they are not designed totrack active visual objects or relevant video seg-ments throughout dialogue context.
we hope thedvd dataset will lead to new research avenuesto develop intelligent systems capable of complex.
reasoning on video and dialogue medium (furtherdiscussion in the supplementary material).
thedvd dataset and code will be made public..2 related work.
we compared dvd to existing datasets from thefollowing four angles:1) vision-linguistic.
vision-linguistic understand-ing benchmarks have been proposed, includingcaptioning (farhadi et al., 2010; lin et al., 2014;rohrbach et al., 2015), phrase grounding or objectreference (kazemzadeh et al., 2014; plummer et al.,2015), scene graph learning (krishna et al., 2017),and text-to-clip (anne hendricks et al., 2017).
ourbenchmark, dvd, is more related to vqa in whicha visual input is given and a system is required toanswer a question about this input (antol et al.,2015; zhu et al., 2016; jang et al., 2017; lei et al.,2018).
another related line of research is the re-search of navigation systems in a physical environ-ment (gordon et al., 2018; wijmans et al., 2019).
compared to the prior benchmarks, one major dif-ference of dvd is the extension of single-turn in-teraction to a multi-turn human-machine dialogue.
2) visually-grounded dialogue.
extended fromthe vision-linguistic understanding research, thisline of research focuses on answering questions se-quentially positioned over multiple turns (de vrieset al., 2017; das et al., 2017; chattopadhyay et al.,2017; hori et al., 2019; thomason et al., 2019).
asystem has to understand the dialogue context andresolve cross-turn semantic dependencies.
how-ever, due to the complexity of the tasks, involvingcross-modality and cross-turn information, priorbenchmarks are often subject to bias that modelsoften exploit without actual reasoning (qi et al.,2020).
in this work, we design a diagnostic bench-mark with minimal bias and incorporate a set ofspeciﬁc reasoning requirements.
3) diagnostic.
our work is related to mnist dia-logue (seo et al., 2017) and clevr dialog (kotturet al., 2019).
they involve synthetic images to de-velop image-grounded dialogues.
compared tothem, dvd questions are extended from the imageto the video domain and injected with more diversecross-turn semantics.
as shown in table 1 dvdcontains a higher proportion of unique questionsthan related benchmarks.
dvd is also inspiredby the dialogue state tracking task (dst) (mrkˇsi´cet al., 2017; bordes et al., 2017; kottur et al., 2021;moon et al., 2020).
dst requires a system to detect.
5652split.
dvd-traindvd-valdvd-testdvd-totalclevrclevrervisdialavsdmnist dialogclevr dialog.
#videos/images6,1571,5403,29910,996100k20k123k11.1k50k85k.
#dialogs.
#questions.
61,55115,39632,978109,925n/an/a123k11.1k150k425k.
615,510153,960329,7801,099,2501m305k1.2m101.2k1.5m4.25m.
# uniquequestions360,33499,211200,346620,739854k26.4k380k59k35573k.
table 1: statistics for dvd: compared to syntheticdialogue benchmarks, mnist dialog and clevr di-alog, majority of questions in dvd are unique.
ques-tions are generated from question templates and incor-porated with various cross-turn semantics..all information slots mentioned in dialogue, suchas restaurant name and booking date.
instead, indvd, for each turn, we introduce an object trackingstate, deﬁned as visual objects and their attributesmentioned in dialogue context.
4) multi-step reasoning.
a multi-step reasoningquestion is typically represented by a reasoningstructure, also known as functional programs.
ear-lier efforts (andreas et al., 2016; johnson et al.,2017) designed questions that are expressed as ele-mentary operation programs.
more related to ourwork, song et al.
(2018); yi* et al.
(2020) extendedthe prior work to the video domain with questionsfocusing on the temporal variance of video frames.
a major difference between our work and theseapproaches is the extension of functional programsto a dialogue task with context-based operations,such as object tracking and interval tracking.
thisextension brings a step toward more transparentdialogue systems capable of performing reasoningoperations across question turns..3 the dvd dataset.
our benchmark provides a dataset that can be usedto conduct rich diagnostics to better understand thereasoning capabilities of dialogue systems.
table 1and figure 3 to 6 give an overview of dvd..3.1 objects, spatial relations, and intervals.
objects.
objects are identiﬁed by their attributes,including object shapes, sizes, materials, and col-ors.
one unique characteristic of cater objectsis that each object can move multiple times in asingle video.
from the cater universe, we de-ﬁne 4 types of object actions: “ﬂying”, “rotating”,“sliding”, and “no action” (object being stationary).
another characteristic of cater objects is that one.
figure 2: example spatial relationship: we demon-strate the projection of objects and their movements onthe ground plane.
considering the “left” relationship,“a1 is left of b2” and “a2 is left of b5”..object can be contained by another object, resultingin a visual problem called object containment.
inour experiments, current dialogue systems are stillvulnerable to this problem, making it hard to applyto the open world (see section 4.3).
video intervals.
we deﬁne video intervals as con-tinuous video frames, limited by a start and endpoint, each of which can be the start or end of anobject’s action or the start or end of the whole video.
we formulate two types of video intervals:1) atomic intervals.
in these intervals, all objectshave at most one action and they can be in only oneof the two states: in motion or stationary.
to ﬁndatomic intervals, we simply collate the start and endtimestamps of all object actions in a cater videoand sort them chronologically.
by deﬁnition, anynon-overlapping interval between two timestampsis considered atomic.
this constraint allows us toidentify the relative spatial relationships (“left”,“right”, “behind”, and “front”) between any two ob-jects by using their coordinates at the start and endof the interval.
note that in the cater universe,all actions can be projected either as a straight line(“ﬂying” and “sliding”) or a single point (“rotat-ing” and “no action”).
practically, we focus onspatial reasoning only when one of the two objectsis stationary.
figure 2 demonstrates the “left” spa-tial relation, and figure 3 (top) shows an examplequestion of atomic interval with spatial relation.
2) compositional intervals.
compositional inter-vals are all other intervals that are not atomic.
inthese intervals, an object can have more than oneactions, i.e.
be in more than one states such as“ﬂying” then “no action”.
therefore, its movementprojections are not linear and we do not identifyspatial relations in these cases.
instead, we focus oninformation such as action set and action sequenceto generate questions.
figure 3 (bottom) presentsan example question of compositional interval.
to create dvd questions, we ﬁrst identify all in-.
5653a2a2a2 is not left of b2a2 is not left of b3a2 is left of b4 a1b2a1 is not left of b1a1 is left of b2 b1b4b5b3t1t2t3t4t0tobject a:t1t3object b:t2t4t5t6t5t6slidesfliesrotatesformulation of video intervals  startendstationary object a1moving object a2figure 3: example questions and their functional programs: top: a question of atomic interval with relativespatial relationship.
bottom: a question of compositional interval with action set comparison semantic..figure 4: examples questions positioned in dialogue and their functional programs: each question containedreferences to past dialogue turns, through video temporal relation (tr) or dialogue object reference (or)..tervals in a video (with a minimum duration ofabout 0.5s), then randomly sample one interval,and proceed to create questions based on objectmovements and locations in this interval.
figure5-(a) shows the percentages of dvd questions byvideo interval types.
overall, more than 60% ofquestions are of compositional intervals and amongthe atomic-interval questions, the majority of themcontain a spatial relation.
we still maintain a smallpercentage of temporal-agnostic instances (“none”type) to keep the dialogue ﬂow natural..3.2 question and dialogue generation.
question representation.
we use question tem-plates to materialize questions in natural language.
each template associates with an applicable type ofvideo interval and a functional program.
comparedto clevr functional programs (johnson et al.,2017), we introduce 17 new functional modules, ofwhich 13 are extended for video-based inputs and4 are extended for dialogue-based inputs.
over-all, we utilize 26 question templates for 8 questiontypes.
figure 3 illustrates two sample questionswith corresponding reasoning structures and figure5-(b) shows the statistics of question type distribu-tion.
please refer to the supplementary material forthe full details of functional modules and questiontypes and examples.
dialogue generation.
we generated dialogueswith a ﬁxed length of 10 turns.
in each turn, weadopted a depth first search (dfs) approach, as.
similarly used in clevr (johnson et al., 2017),to instantiate questions by sequentially traversingand executing functional programs.
to generatelinguistic dependencies between dialogue turns, ateach turn, we randomly sample and incorporateone or more of the 3 semantic relations below.
fig-ure 4 and 6 present examples of 2 questions and adialogue with these semantic relations..type i: video temporal relation (tr): this typeof semantic relation tests a system to localize videointervals in relation to past dialogue turns.
werandomly select one of three types of relation: (1)“during” relation reuses the same time interval as thelast dialogue turn, e.g.
the q4 in figure 6; (2) “be-fore” and (3) “after” relations simulate a dialogueﬂow with references to the earlier and subsequentvideo segments.
tr synthesizes scenarios whenhumans either maintain or shift their attention tem-porally from one video segment to a related part.
type ii: dialogue object reference (or): weincorporate object references into a question by re-placing original object phrase, such as “the largerubber cone”, with pronouns, such as “it”, to re-fer to object(s) mentioned in the earlier part of thedialogue.
the distance of reference is one turnand we call this a short-term memory or.
addi-tionally, we simulate long-term memory or byinjecting unique objects mentioned further in thepast dialogue turns.
we simulate this behavior bymaintaining a dialogue object state at each turn.
to choose an object for references, we randomly.
5654query action set<object><action set>action by order<order><object><action><object><objects>same action set<object><objects>same action  seqfilter action<action><objects>find interval<action><object><interval>relate temporal<interval><temporalrelation><objects>during the small cone 's flight , how many objects are in front of the brown shiny thing ?filter materialuniquerelate spatialcountmetalfrontfilter colorbrownrelatetemporalduringfindintervalflightfilter shapeuniqueconefilter sizesmallfunctional program operationsafter the gray rubber thing 's first flight and before its second slide , is there any other object with the same set of activities performed by the block ?filter shapeuniquesame action setexistblockrelateintervalafterfindintervalfirst flightfilter materialuniquerubberfilter colorgrayrelateintervalfindintervalunionintervalexample atomic-interval question example compositional-interval question <interval><interval><interval>union interval<interval><interval><interval>relate spatial<interval><object><spatialrelation><interval>query action seq<object><action sequence><interval><interval><interval><interval>action by frequency<frequency><object><interval><action set>beforesecond slidebefore this time period, how many other things with the same set of activities performed by the aforementioned yellow thing ?
filter coloruniquesame action setcountyellowtrack objectsrelateintervalbeforetrackintervalexample question with ‘before’ tr and long-term or semanticsamong them , there is a red thing .
after this time window, what type of action does it undertake last ?example question with ‘after’ tr and short-term or semanticsuniqueaction by orderfiltercolorredrefer objectlastrelateintervalaftertrackintervalthemtrack object<object tracker><objects><reference><objects>referobjecttrack interval<interval tracker><interval><last turn><reference><interval>referinterval<last turn>figure 5: data analysis of dvd: (a) and (b): questions are distributed by 8 question types and 3 video intervaltypes.
(c): the boxplot displays the distribution of active objects mentioned in each dialogue turn position.
(d): atturn position i, an old object originally mentioned in a prior turn position j might be reused, resulting in referenceof turn distance i − j.
(e): each dialogue turn is incorporated with semantic relations, including tr (temporalrelation), or (object references), and tt (topic transfer).
the dotted line indicates the overall average..sample a past dialogue turn position and samplean object introduced in this turn.
this object thenreplaces the original object phrases in the questionof the current turn.
for example, in question q3in figure 6, “the earlier mentioned small thing” isidentiﬁed from the object originally introduced inq1.
following this method, our dialogue simulatesscenarios in which humans only focus on a sub-set of objects rather than all objects in the videoscene and they can refer to those objects again overmultiple dialogue turns.
figure 5-(c) displays theboxplot of the number of active objects involvedin each turn position.
out of 10 objects (the max-imum number of objects in a cater video), 2to 5 objects are involved on average per dialogue.
figure 5-(d) shows the question distribution by theturn distance of long-term memory or, with themajority of questions containing 2-turn distancereferences..type iii: topic transfer (tt): this relation teststhe model ability to memorize and reuse the contextof the last dialogue turn to the current turn through3 types of topic transfers: (1) attribute transfer and(2) spatial transfer reuse the same question from theprior dialogue turn with a modiﬁcation of object at-tribute or spatial relation (e.g.
q2 and q5 in figure6).
compared to tr, these two types of topic trans-fers focus on human attention shifts in spatial spacerather than temporal space; (3) temporal transferintroduces a unique setting of situated dialogue.
in dvd.
instead of using a ﬁxed video input foreach dialogue instance, at the ﬁrst dialogue turn,we shorten a cater video by a cutoff point, e.g.
t0.
at each later turn, for 30% of time, we up-date the current video input to a new cutoff pointlater than the previous one e.g.
ti+1 > ti.
wedo not update when the cutoff reaches the end ofthe original cater video t i.e.
ti+1 = t .
forinstance, in figure 6, at q7, we reuse the same con-text from q6 but with new extended visual content.
we introduce temporal transfer as a preliminarystep to challenge dialogue systems in a dynamicenvironment with a continuous visual stream..after sampling question templates and semantic de-pendencies, the ground-truth answers are obtainedby executing corresponding functional programs.
for each question template, we discard dominatinginstances to maintain an approximate uniform dis-tribution of answer values, minimizing bias result-ing from question-conditioned data distributions.
additionally, at each turn, we remove any questionthat is ill-posed or becomes redundant when posi-tioned in dialogue.
for instance, the question “howmany red rubber objects are there?” is removed ifin a prior dialogue turn, the question is “how manyred objects are there?” and the answer is already“1”.
to do this, we perform a check at every dia-logue turn to determine whether involving objectsand their attributes are already mentioned in thedialogue object state.
finally, we only keep dia-.
5655(b) question distribution by question type (e) dialogue distribution by the number of turns with semantic relations(d) question distribution by turn distance of object references(a) question distribution by video interval (tr)(or)(tt)(c) number of active objects per dialogue turnfigure 6: dialogue generation: in each dialogue turn, we generate questions with randomly sampled cross-turndependencies: temporal relation (tr), object reference (or), and topic transfers (tt), including attribute (a),spatial (s), and temporal (t) transfer.
we maintain a dialogue object state of active objects which are color-coded..logues that have cross-turn dependencies in 9 outof 10 turns, considering the ﬁrst turn semanticallyindependent.
figure 5-(e) provides the distributionof dialogues by the number of tr, or, and ttrelations.
for more analysis of dvd, please referto the supplementary material..4 dialogue systems on dvd.
k=1.
the video-grounded dialogue task in dvd is de-ﬁned as a turn-based retrieval task from multiple-choice candidate answers.
at each dialogue turni (i = 1, 2, ..., 10), video input vi, the ground-truth dialogue context, including question and an-swer pairs up to the last dialogue turn, ci =(qk, ak)|k=i−1, the question of the current turnqi, are provided.
the system is given a set ofcandidate answers a, predeﬁned as all possible an-swer values for all question types, with |a| = 40in dvd, and is required to select one answer froma. we evaluate models by the accuracy of pre-dicted answers against the ground-truth answers.
for a system denoted as θ, the objective functionis: ˆai = arg maxa p (ai|vi, qi, ci; θ)..4.1 experimental setup.
baselines.
we experimented with a representa-tive set of baseline approaches on dvd, includ-ing: (1) answer prior, which selects the most pop-ular answer option as predicted answers; (2) q-type (random/frequency), which assume knownquestion types and select a random or most popu-lar answer from the corresponding answer space;(3) q-retrieval (tf-idf), which retrieves the mostsimilar question from the training set and use its.
answer as the predicted answer; (4) rnn(q) andhrnn(c+q), which encode dialogue-only compo-nents without seeing visual information to predictanswers; (5) hrnn(c+q)+cnn(v)/ta(v), sameas (4) but with access to visual information whichis encoded by pretrained cnn models and tem-poral attention (ta) (jang et al., 2017; lei et al.,2018; hori et al., 2019); (6) tf(c+q+v), whichuses a transformer-based architecture to encodevisual and language information (schwartz et al.,2019; le et al., 2019; li et al., 2020).
finally, weconducted internal human evaluation on a subsetof the dvd test split.
for each test sample, a hu-man received an input video, dialogue history, andthe question for the current turn.
the human wasrequired to select an answer from the list of 40candidates a to answer the question..experiments.
video-grounded dialogues entaila lot of visio-linguistic and reasoning challengesthat are not easy to be studied in isolation using ex-isting datasets.
to address this issue with dvd, weexploit the rich annotations of dvd in our experi-ments during evaluation.
we designed our experi-ments to systematically analyze model capabilitiesand shortcomings through unique challenges invideo-grounded dialogue systems.
speciﬁcally, insection 4.2, we analyzed the results of all modelsoverall as well as by each question type.
in section4.3, we leverage the spatio-temporal annotationof visual objects to analyze model performanceby related video interval types, spatial reasoning(results by object containment), and temporal rea-soning (results by relative interval length).
in termsof dialogue contextual complexity, in section 4.4,.
5656dialoguedialogue object statetrorttastq1: before the large thing 's first flight , what color is the average thing that is in front of the small thing?
a1: yellowq2: what about its material ?
a2: rubber{obj1: size=large}, {obj2: size=average, color=yellow}, {obj3: size=small}✓✓q3: during the earlier mentioned small thing 's first slide , what shape is the stationary thing to the right of the aforementioned average object?
a3: cube{obj1: size=large}, {obj2: size=average, color=yellow, material=rubber}, {obj3: size=small}✓q4: during the same time period , how many average cyan shiny things are behind the gray object?
a4: 1{obj1: size=large}, {obj2: size=average, color=yellow, material=rubber}, {obj3: size=small}, {obj4: shape=cube}✓q5: how about to the left of it ?
a5: 0{obj1: size=large}, {obj2: size=average, color=yellow, material=rubber}, {obj3: size=small}, {obj4: shape=cube}, {obj5: color=gray}, {obj6: color=cyan, size=average, material=metal}✓✓✓q6: throughout the whole video , does the earlier cube object fly more frequently than the earlier mentioned average object slides ?
a6: true{obj1: size=large}, {obj2: size=average, color=yellow, material=rubber}, {obj3: size=small}, {obj4: shape=cube}, {obj5: color=gray}, {obj6: color=cyan, size=average, material=metal}✓q7: what about up until now ?
a7: false{obj1: size=large}, {obj2: size=average, color=yellow, material=rubber}, {obj3: size=small}, {obj4: shape=cube}, {obj5: color=gray}, {obj6: color=cyan, size=average, material=metal}✓turn 1turn iturn i+1turn jturn 100 to e_00 to e_00 to e_10 to t0 to t.........video inputs:dialogue turns:temporal topic transfer accuracy.
allaction countaction queryattribute querycompare action seqcompare action setcompare action freqobject countobject existnoneatomic (non-spatial)atomic (spatial)compositionaltransfer (attribute)transfer (spatial)transfer (temporal).
21.30.00.00.033.425.148.50.048.90.018.821.222.80.049.828.9.answerprior.
q-type(random).
q-type(freq).
q-retrieval(tf-idf).
rnn(q).
hrnn(c+q).
27.89.312.732.934.128.250.09.149.832.126.327.328.030.742.438.4.
35.323.423.738.737.336.350.523.351.138.331.935.535.445.544.922.6.
32.119.820.639.435.128.244.418.854.439.042.427.632.137.126.43.0.
39.716.325.838.145.532.858.426.266.438.347.236.840.040.829.630.2.
45.828.233.139.252.540.056.938.667.039.547.846.045.845.748.153.5.hrnn(c+q)+cnn(v)49.337.836.743.358.243.062.340.069.243.149.947.550.254.547.762.2.hrnn(c+q)+ta(v)50.236.038.645.157.544.365.240.269.445.150.747.651.457.347.464.6.tf(c+q+v)51.138.839.443.161.645.467.139.969.043.448.947.153.257.748.069.0.human.
89.387.588.198.091.582.988.590.692.399.183.393.987.1100.090.579.8.table 2: experiment results on the dvd test split: models are evaluated by overall accuracy and by questiontypes (top), accuracy by video intervals in question (center), and transferability accuracy (bottom)..we use cross-turn relation annotations to analyzemodel performance by temporal-based attentionshift (tr), dialogue turn distance (or), and short-term transferability (tt)..4.2 results.
from table 2 (top), we observe that “blind” sys-tems that use answers only or questions only,achieve quite poor results up to 39% accuracy.
byselecting the most popular answer option, answerprior only achieves 21% accuracy.
when a “blind”model has access to dialogue history, the perfor-mance increases up to 45%.
this increment showsthat dialogue context contains useful informationfor a dialogue system to infer answers.
we notethat on average there are nearly 3 out of 10 questionturns with a topic transfer per dialogue (see figure5-(e)).
in such cases, a model can randomly makea good guess by just reusing the answer of the lastquestion turn.
when a system is presented withthe visual input, we observe model performanceincreases up to 51%.
however, in the best system,the performance is still far below the human levelwith a performance gap of 38 absolute points..in table 2 (top),.
from the results of q-type(random) per question type, we observed thatanswers are balanced in each question type.
the ta-ble also shows performance drops between pairs ofobject-oriented vs. action-oriented question types.
for instance, tf(c+q+v) achieves 38% accuracyin action count vs. 39% in object count, and 39%accuracy in action query vs. 43% in attributequery.
in comparison-based questions, comparing.
action sets tend to be more challenging than com-paring action sequences.
to compare action sets oftwo objects in a video interval, a system needs toprocess the interval completely.
however, to com-pare action sequences, in most cases, the systemcan determine the answer after the ﬁrst few actionsteps the objects perform.
for more analysis ofquestion types and sub-types, please refer to thesupplementary material..4.3 analysis by visual complexity.
to understand the drive of the performance by vi-sual inputs, we investigated the results by the visualcomplexity in questions.
in table 2 (center), com-pared to hrnn(c+q)+cnn(v), models using at-tention, either through ta(v) or transformer, showmore improvement in compositional interval ques-tions with increments up to 3 absolute points.
inother types of intervals, the performance gains arenot very signiﬁcant.
particularly, in atomic-intervalquestions that require spatial localization, the per-formance does not change when applying attention.
this observation necessitates systems that focus onboth spatial and temporal space of visual inputs..in figure 7 (left), we analyzed model perfor-mance by the number of objects mentioned in ques-tions that are contained in video scenes.
we notedthat current models are vulnerable to visual ob-ject containment, as the accuracy decreases by thenumber of contained objects.
this observation isconsistent with the results of cater action recog-nition tasks (girdhar and ramanan, 2020).
in fig-ure 7 (right), we investigated model performance.
5657figure 7: experiment results by visual properties:left: results by the number of objects mentioned inquestion that are contained in video scenes.
right: re-sults by the relative length of video interval in question..by the relative length of ground-truth video inter-val in question, measured as the percentage of thewhole video length.
to make a fair analysis, weremoved cases in which a question can be solvedcorrectly without localizing the speciﬁc video inter-val but simply using the whole video.
we observedthat model performance decreases as the intervallength increases, demonstrating the challenge oflong-term video understanding in video scenes.
wenoted that there is a drop in performance in the low-est range of interval lengths, 0−10%.
as this rangeoften represents atomic intervals, the majority ofwhich include questions with spatial relations, sys-tems are negatively affected and the curve dropsinitially in this low range..4.4 analysis by cross-turn relations.
we examined model performance in a multi-turnsetting by cross-turn semantic relations.
first, weinvestigated the effect of tr.
in a tr-injected ques-tion, a system is required to learn to retrieve a videosegment related to the last used segment.
however,some questions may be correctly answered withoutlocalizing the correct segments.
for instance, atthe current dialogue turn, a question is of interval(tm, tn) and at the next turn, a question with an“after” tr is of interval (tn, tq) (s.t.
tm < tn < tq)might be solved if the visual context is the samein both intervals.
we separate such question turnsand measured the results of the remaining ques-tions with tr relations “after” and “before”.
fromfigure 8, we observed that current systems arenot optimal to learn to shift attention to relatedintervals, depending on the type of questions.
inaction-based questions (ac, aq, caseq, caset,and caf), the results of “before” and “after” trare lower than those without a tr relation, but inobject-based questions (oc, oe), we observed dif-ferently.
this difference can be explained by thedynamics of actions vs. objects.
between videointervals, information about object actions (e.g.
fre-.
figure 8: experiment results by temporal relations:action count (ac), action query (aq), attribute query(attq), compare action sequence (caseq), compareaction set (caset), compare action frequence (caf),object count (oc), and object exist (oe)..figure 9: experiment results for cross-turn reason-ing: results of action count questions by turn position(left) and by turn distance of object references (right)..quency, types) tends to change more easily thanobjects themselves.
action-based questions chal-lenge systems through cross-turn temporal reason-ing more than object-based questions..secondly, we analyzed the impacts of long-termmemory or.
from figure 9 (left), we noticed thatmodel performance becomes more stable in sys-tems where dialogue history is introduced as aninput.
for instance, compared to rnn(q), the per-formance curve of tf(c+q+v) follows a moregentle downward trend from low to high dialogueturn positions.
to fairly analyze performance byor turn distance, we discard any instances thatdo not require systems to use dialogue context toresolve the references, but simply rely on the inputvideo.
for example, a question with a reference“the earlier mentioned red object” is removed ifthere is indeed only one “red object” in the videoscene.
from results by or turn distance in figure9 (right), we observed all systems are relativelyunstable, even as dialogue history is introducedas an input.
this difference against the results byturn position exhibits a limitation of current sys-tems as they struggle to resolve object referencesby existing dialogue encoding techniques..finally, to analyze the effect of tt relations, weinvestigate a new metric, called transferability, intable 2 (bottom).
when a system is presented witha question turn with a topic transfer, it should learn.
5658(a) tf(c+q+v)(b) hrnn(c+q) + ta(v)(b) action queryto derive the answer in relation to the context ofthe last dialogue turn.
if the last answer is right,an intelligent system should be able to consistentlyanswer in the current turn correctly.
for instance,given a question-answer pair “what is the colorof the sliding cube?
red”, a human can often in-fer the answer to a tt(a)-injected question “whatabout its material?” based on the same visual ob-ject.
we gather questions that precede questionscontaining topic transfers and call this set qttprior.
for each question qttprior that the model answeredcorrectly, we measure the accuracy over the corre-sponding transferred question qtt and average thescores.
we observed a clear performance gain fromrnn(q) to hrnn(c+q) in terms of transferabil-ity metric, demonstrating the impacts of dialoguecontext on tt questions.
a chance-based systemcan achieve approximately 50% transferability byjust recycling answers from prior turns.
the bestsystem results, however, are still far from human-level performance.
this observation necessitatessystems designed with a better contextual memoryto adapt past context in new dialogue turns..5 discussion and conclusion.
we have introduced dvd, a diagnostic dataset de-signed to analyze video-grounded dialogue sys-tems.
dvd dataset is generated with tight controlof data bias through balancing the question and an-swer distribution and questions are built based ona principled approach to reﬂect the complexity invideos and dialogues.
our results have shown thatdvd can provide interesting insights into systemabilities and limitations.
speciﬁcally, our analy-sis has revealed some key shortcomings of currentmodels, including: (1) limited ability to efﬁcientlyintegrate visual information from both spatial andtemporal space; (2) limited ability to recognizeand compile multiple actions in long-ranged videointervals; (3) inconsistent performance across dia-logue turns, especially in cases when systems arerequired to switch attention temporally; and (4) un-stable performance to resolve object co-referencein the dialogue context, especially when the turndistance of the object references increases..these insights provide potential avenues wherewe hope dvd will be a useful benchmark to ex-plore new ideas.
speciﬁcally, we discuss two re-search directions:.
dialogue object tracking.
to further diagnosea dialogue system, we aim to study their long-term.
memory reasoning ability to track objects and theirattributes mentioned in the dialogue context.
weare inspired by research work of dialogue statetracking in task-oriented dialogues (bordes et al.,2017) and propose to use tracking accuracy met-ric in video-grounded dialogue systems.
at eachturn t, a video-grounded dialogue system shouldbe able to track and update a dialogue state st,deﬁned as a set of all mentioned objects oti andi , colors cttheir attributes, including sizes zti, ma-i, and shapes stterials mt1, ot2, ...) =((zt2, mt1), (zt1, st1, mt1, ct2), ...).
we deﬁnetwo tracking metrics, including joint accuracy,measuring the accuracy of prediction of all objectsand attributes as a set, and slot accuracy, measur-ing the accuracy of predicted attributes individually.
the introduction of these evaluation metrics neces-sitates a new learning task, dialogue object track-ing (dot) in video-grounded dialogue systems, tobetter understand current systems’ long-term rea-soning ability..i: st = (ot2, st.2, ct.video interval tracking.
another aspect of di-alogue systems that we want to diagnose is theirability to localize video segments in a multi-turnsetting.
each question turn often focuses on dif-ferent parts of the video as the dialogue extendsover time.
it is important to learn how a system canlocalize the right segments of the video from turnto turn.
similar to dot, we deﬁne a new learningtask for video interval tracking (vit) in a similarnature as text-to-clip tasks (anne hendricks et al.,2017).
the task can be deﬁned as a ranking taskof segment candidates to choose the relevant seg-ments in each question turn.
this task is evaluatedby ranking metrics such as rank@1 or rank@2,and mean intersection over union (miou).
alter-natively, we can adapt grounding, a simple metricused by hudson et al.
(2019) to assess spatial atten-tion of image regions.
in dvd, grounding can beused in temporal attention-based approaches to de-termine model ability to localize the right positionof video intervals in question..finally, we want to emphasize that dvd is de-signed as a synthetic dataset for diagnosis purposesto systematically evaluate model capabilities.
thebenchmark should not be used to replace data ofhuman dialogues but be used to supplement real-world dialogue datasets..5659references.
aishwarya agrawal, dhruv batra, and devi parikh.
2016. analyzing the behavior of visual question an-in proceedings of the 2016 con-swering models.
ference on empirical methods in natural languageprocessing, pages 1955–1960, austin, texas.
asso-ciation for computational linguistics..huda alamri, vincent cartillier, abhishek das, juewang, anoop cherian, irfan essa, dhruv batra,tim k marks, chiori hori, peter anderson, et al.
in pro-2019. audio visual scene-aware dialog.
ceedings of the ieee conference on computer vi-sion and pattern recognition, pages 7558–7567..jacob andreas, marcus rohrbach, trevor darrell, anddan klein.
2016. neural module networks.
in pro-ceedings of the ieee conference on computer visionand pattern recognition, pages 39–48..lisa anne hendricks, oliver wang, eli shechtman,josef sivic, trevor darrell, and bryan russell.
2017.localizing moments in video with natural language.
in proceedings of the ieee international conferenceon computer vision, pages 5803–5812..stanislaw antol, aishwarya agrawal, jiasen lu, mar-garet mitchell, dhruv batra, c lawrence zitnick,and devi parikh.
2015. vqa: visual question an-swering.
in proceedings of the ieee internationalconference on computer vision, pages 2425–2433..antoine bordes, y-lan boureau, and jason weston.
2017. learning end-to-end goal-oriented dialog.
in 5th international conference on learning rep-resentations, iclr 2017, toulon, france, april 24-26, 2017, conference track proceedings.
openre-view.net..prithvijit chattopadhyay, deshraj yadav, viraj prabhu,arjun chandrasekaran, abhishek das, stefan lee,dhruv batra, and devi parikh.
2017. evaluatingvisual conversational agents via cooperative human-in proceedings of the fifth aaai con-ai games.
ference on human computation and crowdsourcing(hcomp)..abhishek das, satwik kottur, khushi gupta, avisingh, deshraj yadav, jos´e mf moura, devi parikh,and dhruv batra.
2017. visual dialog.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 326–335..harm de vries, florian strub, sarath chandar, olivierpietquin, hugo larochelle, and aaron courville.
2017.
guesswhat?!
visual object discovery throughmulti-modal dialogue.
in proceedings of the ieeeconference on computer vision and pattern recog-nition, pages 5503–5512..ali farhadi, mohsen hejrati, mohammad aminsadeghi, peter young, cyrus rashtchian, juliahockenmaier, and david forsyth.
2010. every pic-ture tells a story: generating sentences from images.
in european conference on computer vision, pages15–29.
springer..haoyuan gao, junhua mao, jie zhou, zhiheng huang,lei wang, and wei xu.
2015. are you talking to amachine?
dataset and methods for multilingual im-age question.
advances in neural information pro-cessing systems, 28:2296–2304..rohit girdhar and deva ramanan.
2020. cater: a di-agnostic dataset for compositional actions and tem-in international conference onporal reasoning.
learning representations..daniel gordon, aniruddha kembhavi, mohammadrastegari, joseph redmon, dieter fox, and alifarhadi.
2018. iqa: visual question answering in in-teractive environments.
in proceedings of the ieeeconference on computer vision and pattern recog-nition, pages 4089–4098..yash goyal, tejas khot, douglas summers-stay,dhruv batra, and devi parikh.
2017. making thev in vqa matter: elevating the role of image under-standing in visual question answering.
in proceed-ings of the ieee conference on computer visionand pattern recognition, pages 6904–6913..c. hori, h. alamri, j. wang, g. wichern, t. hori,a. cherian, t. k. marks, v. cartillier, r. g. lopes,a. das, i. essa, d. batra, and d. parikh.
2019. end-to-end audio visual scene-aware dialog using mul-timodal attention-based video features.
in icassp2019 - 2019 ieee international conference onacoustics, speech and signal processing (icassp),pages 2352–2356..drew hudson, a., and christopher d. manning.
2019.gqa: a new dataset for real-world visual reasoningand compositional question answering.
in proceed-ings of the ieee/cvf conference on computer vi-sion and pattern recognition (cvpr)..yunseok jang, yale song, youngjae yu, youngjin kim,and gunhee kim.
2017. tgif-qa: toward spatio-temporal reasoning in visual question answering.
inproceedings of the ieee conference on computervision and pattern recognition, pages 2758–2766..justin johnson, bharath hariharan, laurens van dermaaten, li fei-fei, c lawrence zitnick, and rossgirshick.
2017. clevr: a diagnostic dataset for com-positional language and elementary visual reasoning.
in proceedings of the ieee conference on com-puter vision and pattern recognition, pages 2901–2910..sahar kazemzadeh, vicente ordonez, mark matten,and tamara berg.
2014. referitgame: referringto objects in photographs of natural scenes.
in pro-ceedings of the 2014 conference on empirical meth-ods in natural language processing (emnlp), pages787–798..satwik kottur, seungwhan moon, alborz geramifard,and babak damavandi.
2021. simmc 2.0: a task-oriented dialog dataset for immersive multimodalconversations.
corr, abs/2104.08667..5660satwik kottur, jos´e m. f. moura, devi parikh, dhruvbatra, and marcus rohrbach.
2019. clevr-dialog:a diagnostic dataset for multi-round reasoning inin proceedings of the 2019 confer-visual dialog.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 582–595, minneapolis, minnesota.
as-sociation for computational linguistics..ranjay krishna, yuke zhu, oliver groth, justin john-son, kenji hata, joshua kravitz, stephanie chen,yannis kalantidis, li-jia li, david a shamma, et al.
2017. visual genome: connecting language and vi-sion using crowdsourced dense image annotations.
international journal of computer vision, 123(1):32–73..hung le, doyen sahoo, nancy chen, and steven hoi.
2019. multimodal transformer networks for end-to-end video-grounded dialogue systems.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 5612–5623,florence, italy.
association for computational lin-guistics..hung le, doyen sahoo, nancy chen, and steven c.h.
hoi.
2020. bist: bi-directional spatio-temporal rea-in proceed-soning for video-grounded dialogues.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1846–1859, online.
association for computationallinguistics..jie lei, licheng yu, mohit bansal, and tamara berg.
2018. tvqa: localized, compositional video ques-in proceedings of the 2018 con-tion answering.
ference on empirical methods in natural languageprocessing, pages 1369–1379, brussels, belgium.
association for computational linguistics..zekang li, zongjia li, jinchao zhang, yang feng,cheng niu, and jie zhou.
2020. bridging textand video: a universal multimodal transformer forvideo-audio scene-aware dialog.
dstc workshop@ aaai..tsung-yi lin, michael maire, serge belongie, jameshays, pietro perona, deva ramanan, piotr doll´ar,and c lawrence zitnick.
2014. microsoft coco:in european confer-common objects in context.
ence on computer vision, pages 740–755.
springer..mateusz malinowski and mario fritz.
2014. a multi-world approach to question answering about real-world scenes based on uncertain input.
advancesin neural information processing systems, 27:1682–1690..seungwhan moon, satwik kottur, paul crook, ankitade, shivani poddar, theodore levin, david whit-ney, daniel difranco, ahmad beirami, eunjooncho, rajen subba, and alborz geramifard.
2020.situated and interactive multimodal conversations.
in proceedings of the 28th international conference.
on computational linguistics, pages 1103–1121,barcelona, spain (online).
international committeeon computational linguistics..nikola mrkˇsi´c, diarmuid ´o s´eaghdha, tsung-hsienwen, blaise thomson, and steve young.
2017. neu-ral belief tracker: data-driven dialogue state track-ing.
in proceedings of the 55th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 1777–1788, vancouver,canada.
association for computational linguistics..bryan a plummer, liwei wang, chris m cervantes,juan c caicedo, julia hockenmaier, and svetlanalazebnik.
2015.flickr30k entities: collectingregion-to-phrase correspondences for richer image-in proceedings of the ieeeto-sentence models.
international conference on computer vision, pages2641–2649..jiaxin qi, yulei niu, jianqiang huang, and hanwangzhang.
2020. two causal principles for improvingvisual dialog.
in proceedings of the ieee/cvf con-ference on computer vision and pattern recogni-tion, pages 10860–10869..anna rohrbach, marcus rohrbach, niket tandon, andbernt schiele.
2015. a dataset for movie descrip-tion.
in proceedings of the ieee conference on com-puter vision and pattern recognition, pages 3202–3212..idan schwartz, seunghak yu, tamir hazan, andalexander g schwing.
2019. factor graph attention.
in proceedings of the ieee conference on com-puter vision and pattern recognition, pages 2039–2048..paul hongsuck seo, andreas lehrmann, bohyung han,and leonid sigal.
2017. visual reference resolu-tion using attention memory for visual dialog.
inadvances in neural information processing systems,pages 3719–3729..xiaomeng song, yucheng shi, xin chen, and yahonghan.
2018. explore multi-step reasoning in videoin proceedings of the 26thquestion answering.
acm international conference on multimedia, pages239–247..jesse thomason, michael murray, maya cakmak, andluke zettlemoyer.
2019. vision-and-dialog naviga-tion.
in conference on robot learning (corl)..erik wijmans, samyak datta, oleksandr maksymets,abhishek das, georgia gkioxari, stefan lee, irfanessa, devi parikh, and dhruv batra.
2019. embod-ied question answering in photorealistic environ-ments with point cloud perception.
in proceedingsof the ieee conference on computer vision and pat-tern recognition (cvpr)..kexin yi*, chuang gan*, yunzhu li, pushmeet kohli,jiajun wu, antonio torralba, and joshua b. tenen-baum.
2020. clevrer: collision events for video rep-resentation and reasoning.
in international confer-ence on learning representations..5661amir zadeh, michael chan, paul pu liang, edmundtong, and louis-philippe morency.
2019. social-iq:a question answering benchmark for artiﬁcial socialintelligence.
in proceedings of the ieee conferenceon computer vision and pattern recognition, pages8807–8817..yuke zhu, oliver groth, michael bernstein, and li fei-fei.
2016. visual7w: grounded question answeringin images.
in proceedings of the ieee conferenceon computer vision and pattern recognition, pages4995–5004..a a comparison of dvd to related.
benchmarks.
in table 3, we compare dvd with related bench-marks by 4 aspects: spatial reasoning (sr), tempo-ral reasoning (tr), dialogue object tracking (dot),and video interval tracking (vit).
sr and trare visual-related reasoning types.
sr refers tothe reasoning requirement to localize informationwithin an image.
sr is the most popular reason-ing type, being involved in most vision-languagebenchmarks such as vqa (antol et al., 2015) andtgif-qa (jang et al., 2017).
tr is often presentwhen a video is used as input, which requires sys-tems to localize the relevant temporal location inthe video.
however, tr is not just limited to videounderstanding tasks but also refers to problemswith dynamic visual inputs such as navigation sys-tems or embodied qa.
dot and vit refer to cross-turn semantic relations in a multi-turn dialogueproblem setting.
dot refers to the use of objectreferences, requiring systems to learn to resolvethese references in dialogue context.
dot can beseen clearly in most dialogue benchmarks as ob-ject references are used frequently in traditionaldialogues.
vit is a new reasoning requirement invideo-grounded dialogue tasks.
it requires systemsto localize temporal parts of the video from turn toturn.
vit is less obvious in prior benchmarks asit is challenging to simulate.
it is mostly presentin speciﬁc tasks such as avsd (hori et al., 2019)and cvdn (thomason et al., 2019) where a videoinput is introduced and at each turn, only a speciﬁc.
temporal part of the video is relevant.
comparedto existing benchmarks, dvd is the ﬁrst diagnosticbenchmark that combines all 4 aspects, sr, tr,dot, and vit, together..b dvd functional program modules.
in table 4 and 5, we describe all data types andfunctional program modules in dvd.
in total,there are 20 data types and 32 functional mod-ules.
among the functional modules, comparedto clevr (johnson et al., 2017), we introduced17 novel modules that are designed to be executedon dialogue or video components.
within thesemodules, there are 13 video-based modules (countaction, filter action, same action set, same actionsequence, find interval, union interval, relatespatial, relate temporal, query action set, queryaction sequence, action by frequency, action byorder, equal action) and 4 dialogue-based mod-ules (refer object, track object, refer interval,track interval)..c dvd question types, sub-types, and.
examples.
in table 6, we detail all 8 question types for dvd.
in each question type, we described the types ofvideo intervals applicable, including atomic inter-val, compositional interval, or none.
none type isused in temporal agnostic questions, such as ques-tions to query object attributes or count objects.
ineach question type, we further classify questionsby question sub-types.
figure 10 presents the dis-tribution of questions by question sub-types.
weobserved that per each question type, question sub-types are balanced in most cases.
for instance, thequestion type compare action frequency include 3sub-types: equal, less, and more, and each is about4% of the total questions.
similar observations canbe seen in other question types, including com-pare action sequence, compare action set, andattribute query..5662benchmarks.
diagnosticbenchmark.
visual reasoning language reasoningsr.dot.
vit.
tr.
image/video qa, embodied qavqa (antol et al., 2015), visual7w (zhu et al., 2016)tgif-qa (jang et al., 2017), tv-qa (lei et al., 2018)iqa (gordon et al., 2018), eqa (wijmans et al., 2019)image/video grounded dialogues, navigation dialoguesvisdial (das et al., 2017), guesswhat (de vries et al., 2017)avsd (hori et al., 2019), cvdn (thomason et al., 2019)synthetic image/video qashape (andreas et al., 2016), clevr (johnson et al., 2017)svqa (song et al., 2018), clevrer (yi* et al., 2020)synthetic dialoguesbabi (bordes et al., 2017)mnist dialog (seo et al., 2017), clevr-dialog (kottur et al., 2019)dvd (ours).
(cid:55)(cid:55)(cid:55).
(cid:55)(cid:55).
(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51)(cid:51).
(cid:51)(cid:51).
(cid:51)(cid:51).
(cid:55)(cid:51)(cid:51).
(cid:55)(cid:51)(cid:51).
(cid:55)(cid:51).
(cid:55)(cid:51).
(cid:55)(cid:55)(cid:51).
(cid:55)(cid:55)(cid:55).
(cid:51)(cid:51).
(cid:55)(cid:55).
(cid:51)(cid:51)(cid:51).
(cid:55)(cid:55)(cid:55).
(cid:55)(cid:51).
(cid:55)(cid:55).
(cid:55)(cid:55)(cid:51).
table 3: comparison to related benchmarks: compared to existing datasets for vision-language understanding,dvd is the ﬁrst diagnostic benchmark designed for both spatial reasoning (sr) and temporal reasoning (tr) andexplicit requiring dialogue object tracking (dot) and video interval tracking (vit) in a multi-turn setting..data typeobject.
descriptiona dictionary storing the attributes of an object, including its shape, size, color, andmaterial, and details of its actions, including start and end pointsa list of of objectsa value from the set: “left”, “right”, “front”, and “behind”.
objectsspatial relationtemporal relation a value from the set: “before”, “after”, and “during”reference.
last turnobject tracker.
interval trackerintervalactionaction set.
action sequence.
frequency.
ordercolor.
materialshapesizebinaryinteger.
pronoun, such as “it”, “its”, “them”, “the ﬁrst one”, used to refer to an object or actionmentioned in the last dialogue turnthe last dialogue turn, including the last question and answera list of objects, storing all objects involved and their attributes mentioned so far upto the last dialogue turna list of video intervals mentioned so far up to the last dialogue turna tuple containing the start and end time of a video segmentany value from “sliding”, “ﬂying”, “rotating”, and “no action”any combination of actions, except for “no ation”, without duplication.
a standalone“no action” is acceptable.
any combination of actions, except for “no action”, that can form a sequence.
astandalone “no action” is acceptable.
a positive integer that indicates the number of times an action is undertaken.
fre-quency can also be expressed by superlatives such as “least” or “most”.
an ordinal number that indicates the order of an action during a video interval.
a string that indicates an object color: “gold”, “gray”, “green”, “purple”, “red”,“cyan”, “cylinder”, “blue”, “brown”, “yellow”a string that indicates an object material, including “metal” and “rubber”a string that indicates an object shape, including “cone”, “cube”, “sphere”, “snitch”a string that indicates an object size, including “large”, “medium”, and “small”a binary value, either “false” or “true”an integer value >= 0.table 4: data types in dvd: in total, there are 20 data types, which can be categorized by the following groups(from top to bottom): object-based, relation-based, cross-turn based, action-based, attributes, and binary/integer..5663moduletype.
count.
modulenamecount objectcount action.
exist.
exist.
object-based.
interval-based.
relate.
integer-based.
action-based.
filter colorfiltermaterialfilter shapefilter sizefilteractionsame actionsetsame actionsequenceuniquescenefindintervalunionintervalrelatespatialrelatetemporalgreater thanless thanequalrefer object.
referintervaltrackintervalquery actionsetquery actionsequence.
action byfrequencyaction byorder.
equal action.
multi-turn.
track object.
inputtype.
objects(interval, object,action)objects.
outputtypeintegerinteger.
binary.
(objects, color)objects(objects, material) objects.
(objects, shape)(objects, size)(interval, objects,action)(interval, object).
objectsobjectsobjects.
objects.
(interval, object).
objects.
objects.
(object, action).
objectobjectsinterval.
interval.
interval.
objects.
(interval1,interval2)(interval, object,spatial relation)(interval, temporalrelation)(integer1, integer2) binary(integer1, integer2) binary(integer1, integer2) binaryobjects(reference,turn)object tracker.
objects.
last.
last.
interval.
(reference,turn)interval tracker.
(interval, object).
(interval, object).
(interval, object,frequency)(interval, object,order).
(actionset/sequence, ac-tion set/sequence)objectobject.
interval.
actionsetactionse-quenceactionsetaction.
binary.
module description.
return number of objectsreturn number of times an object undertakesa speciﬁc type of actions during an intervalreturn whether there is at least one resultingobject from the last modulereturn objects of a speciﬁc colorreturn objects of a speciﬁc material.
return objects of a speciﬁc shapereturn objects of a speciﬁc sizereturn objects performing a speciﬁc actionduring an intervalreturn objects performing the same action setas another object during an intervalreturn objects performing the same action se-quence as another object during an intervalreturn the unique object from resulting objectsreturn all objects in the current videoreturn the start and end point of the intervalof an action performed by an objectreturn the overlapping interval from interval1and interval2return objects located in relation to anotherobject during a speciﬁc intervalreturn interval in relation to another interval.
return whether integer1 > integer2return whether integer1 < integer2return whether integer1 = integer2resolve object reference based on the last dia-logue turnreturn all objects mentioned so far in the dia-logueresolve interval reference to an action men-tioned in the last dialogue turnreturn the interval used in the last dialogueturnreturn the set of actions performed by an ob-ject during an intervalreturn the sequence of actions performed byan object during an interval.
return the set of action performed by an objectfor a ﬁxed number of times during an intervalreturn an speciﬁc action performed by an ob-ject during an interval in an ordinal position(e.g.
1st, 2nd)return whether two set of actions are the sameor two sequences of actions are the same.
othermodules.
query colorquerymaterialquery shapequery size.
colormaterial obtain the material of a speciﬁc object.
obtain the color of a speciﬁc object.
objectobject.
shapesize.
obtain the shape of a speciﬁc objectobtain the size of a speciﬁc object.
table 5: details of functional program modules: in total, there are 32 functional program modules, of which 17are modules introduced for video-based and dialogue-based components..5664questiontype.
questioninterval.
compareactionfrequency.
compareactionsequence.
compareactionset.
compositional.
compositional.
compositional.
compositional.
actionquery.
atomic (non-spatial).
atomic (spatial).
all actions.
attributequery.
actioncount.
none.
compositional.
compositional.
atomic (spatial).
objectcount.
objectexist.
atomic (non-spatial).
nonecompositionalatomic (spatial).
atomic (non-spatial).
questionsubtypemore.
equal.
less.
count.
exist.
count.
exist.
by frequency.
by order.
all actionsall actions.
sizecolormaterialshape-.
-.
-.
-.
---.
-.
example.
until the end of the snitch ’s rotation , does the blue thing ﬂy morefrequently than the purple object ﬂies ?
during the whole video , does the sphere rotate as frequently as thecylinder slides ?
after the large thing ’s ﬁrst ﬂight , does the cylinder ﬂy less frequentlythan the green object slides ?
before the large matte thing ’s ﬂight , how many other things perform thesame sequence of activities as the cyan object ?
until the end of the metal sphere ’s slide , is there any other thing withthe same sequence of activities performed by the average purple thing ?
throughout the whole video , how many other things undertake the sametypes of actions as the large block ?
until the end of the cyan shiny thing ’s last slide , is there any other objectthat has the same types of actions as the large rubber object ?
during the gray thing ’s ﬂight , what activities that the big thing performthe least ?
until the end of the average green thing ’s second ﬂight , what is thepurple thing doing ﬁrst ?
during the whole video , what is the brown thing doing ?
after the red cube ’s second slide , what actions does the green sphereundertake ?
during the small thing ’s second rotation , what actions does the averagerubber thing that is in front of the red thing undertake ?
how big is the cylinder ?
what color is the cylinder ?
what material is the brown cone ?
what is the shape of the cyan object ?
throughout the whole video , how many times does the metal cylinderspin in total ?
throughout the whole video , what number of sliding matte cones arethere ?
during the cylinder ’s ﬁrst rotation , what number of objects are in frontof the purple thing ?
before the cylinder ’s slide , how many stationary metallic objects arethere ?
what number of purple things are there ?
throughout the whole video , is there any sliding large rubber cone ?
during the brown thing ’s rotation , is there any cone in front of the purplecube ?
since the start of the big red thing ’s ﬂight , is there a contained small redmetal cylinder ?.
table 6: question types and examples: in total, there are 8 question types, each of which is designed for one ormore types of video intervals (atomic, compositional, or none).
in each question type, we also classify furtherinto question sub-types..figure 10: distribution of questions by question sub-types: for each question type, we classify questions furtherinto corresponding sub-types.
in total, from 8 question types, there are 17 question sub-types..5665