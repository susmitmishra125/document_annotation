factoring statutory reasoning as language understanding challenges.
nils holzenberger and benjamin van durmecenter for language and speech processingjohns hopkins universitybaltimore, maryland, usa.
nilsh@jhu.edu.
vandurme@cs.jhu.edu.
abstract.
statutory reasoning is the task of determin-ing whether a legal statute, stated in naturallanguage, applies to the text description of acase.
prior work introduced a resource that ap-proached statutory reasoning as a monolithictextual entailment problem, with neural base-lines performing nearly at-chance.
to addressthis challenge, we decompose statutory reason-ing into four types of language-understandingchallenge problems, through the introductionof concepts and structure found in prolog pro-grams.
augmenting an existing benchmark,we provide annotations for the four tasks, andbaselines for three of them.
models for statu-tory reasoning are shown to beneﬁt from theadditional structure, improving on prior base-lines.
further, the decomposition into subtasksfacilitates ﬁner-grained model diagnostics andclearer incremental progress..1.introduction.
as more data becomes available, natural languageprocessing (nlp) techniques are increasingly be-ing applied to the legal domain, including for theprediction of case outcomes (xiao et al., 2018;vacek et al., 2019; chalkidis et al., 2019a).
inthe us, cases are decided based on previous caseoutcomes, but also on the legal statutes compiledin the us code.
for our purposes, a case is aset of facts described in natural language, as infigure 1, in blue.
the us code is a set of docu-ments called statutes, themselves decomposed intosubsections.
taken together, subsections can beviewed as a body of interdependent rules speciﬁedin natural language, prescribing how case outcomesare to be determined.
statutory reasoning is thetask of determining whether a given subsection ofa statute applies to a given case, where both areexpressed in natural language.
subsections are im-plicitly framed as predicates, which may be true or.
false of a given case.
holzenberger et al.
(2020) in-troduced sara, a benchmark for the task of statu-tory reasoning, as well as two different approachesto solving this problem.
first, a manually-craftedsymbolic reasoner based on prolog is shown to per-fectly solve the task, at the expense of experts writ-ing the prolog code and translating the natural lan-guage case descriptions into prolog-understandablefacts.
the second approach is based on statisticalmachine learning models.
while these models canbe induced computationally, they perform poorlybecause the complexity of the task far surpassesthe amount of training data available..we posit that statutory reasoning as presentedto statistical models is underspeciﬁed, in that itwas cast as recognizing textual entailment (da-gan et al., 2005) and linear regression.
takinginspiration from the structure of prolog programs,we re-frame statutory reasoning as a sequence offour tasks, prompting us to introduce a novel ex-tension of the sara dataset (section 2), referredto as sara v2.
beyond improving the model’s per-formance, as shown in section 3, the additionalstructure makes it more interpretable, and so moresuitable for practical applications.
we put our re-sults in perspective in section 4 and review relatedwork in section 5..2 sara v2.
the symbolic solver requires experts translating thestatutes and each new case’s description into pro-log.
in contrast, a machine learning-based modelhas the potential to generalize to unseen cases andto changing legislation, a signiﬁcant advantage fora practical application.
in the following, we arguethat legal statutes share features with the symbolicsolver’s ﬁrst-order logic.
we formalize this connec-tion in a series of four challenge tasks, describedin this section, and depicted in figure 1. we hope.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2742–2758august1–6,2021.©2021associationforcomputationallinguistics2742they provide structure to the problem, and a moreefﬁcient inductive bias for machine learning algo-rithms.
the annotations mentioned throughout theremainder of this section were developed by the au-thors, entirely by hand, with regular guidance froma legal scholar1.
examples for each task are givenin appendix a. statistics are shown in figure 2and further detailed in appendix b..argument identiﬁcation this ﬁrst task, in con-junction with the second, aims to identify the ar-guments of the predicate that a given subsectionrepresents.
some terms in a subsection refer tosomething concrete, such as “the united states” or“april 24th, 2017”.
other terms can take a range ofvalues depending on the case at hand, and act asplaceholders.
for example, in the top left box offigure 1, the terms “a taxpayer” and “the taxableyear” can take different values based on the context,while the terms “section 152” and “this paragraph”have concrete, immutable values.
formally, givena sequence of tokens t1, ..., tn, the task is to returna set of start and end indices (s, e) ∈ {1, 2, ..., n}2where each pair represents a span.
we borrow fromthe terminology of predicate argument alignment(roth and frank, 2012; wolfe et al., 2013) andcall these placeholders arguments.
the ﬁrst task,which we call argument identiﬁcation, is taggingwhich parts of a subsection denote such placehold-ers.
we provide annotations for argument identiﬁ-cation as character-level spans representing argu-ments.
since each span is a pointer to the corre-sponding argument, we made each span the short-est meaningful phrase.
figure 2(b) shows corpusstatistics about placeholders..argument coreference some arguments de-tected in the previous task may appear multipletimes within the same subsection.
for instance,in the top left of figure 1, the variable represent-ing the taxpayer in §2(a)(1)(b) is referred to twice.
we refer to the task of resolving this coreferenceproblem at the level of the subsection as argu-ment coreference.
while this coreference can spanacross subsections, as is the case in figure 1, weintentionally leave it to the next task.
keepingthe notation of the above paragraph, given a setof spans {(si, ei)}si=1, the task is to return a ma-trix c ∈ {0, 1}s×s where ci,j = 1 if spans (si, ei)and (sj, ej) denote the same variable, 0 otherwise..1the dataset can be found under https://nlp.jhu..edu/law/.
corpus statistics about argument coreference canbe found in figure 2(a).
after these ﬁrst two tasks,we can extract a set of arguments for every sub-section.
in figure 1, for §2(a)(1)(a), that wouldbe {taxp, taxy, spouse, years}, as shown inthe bottom left of figure 1..structure extraction a prominent feature of le-gal statutes is the presence of references, implicitand explicit, to other parts of the statutes.
re-solving references and their logical connections,and passing arguments appropriately from one sub-section to the other, are major steps in statutoryreasoning.
we refer to this as structure extrac-tion.
this mapping can be trivial, with the taxpayerand taxable year generally staying the same acrosssubsections.
some mappings are more involved,such as the taxpayer from §152(b)(1) becoming thedependent in §152(a).
providing annotations forthis task in general requires expert knowledge, asmany references are implicit, and some must be re-solved using guidance from treasury regulations.
our approach contrasts with recent efforts in break-ing down complex questions into atomic questions,with the possibility of referring to previous answers(wolfson et al., 2020).
statutes contain their ownbreakdown into atomic questions.
in addition, ourstructure is interpretable by a prolog engine..we provide structure extraction annotations forsara in the style of horn clauses (horn, 1951),using common logical operators, as shown in thebottom left of figure 1. we also provide charac-ter offsets for the start and end of each subsection.
argument identiﬁcation and coreference, and struc-ture extraction can be done with the statutes only.
they correspond to extracting a shallow version ofthe symbolic solver of holzenberger et al.
(2020)..argument instantiation we frame legal statutesas a set of predicates speciﬁed in natural language.
each subsection has a number of arguments, pro-vided by the preceding tasks.
given the descrip-tion of a case, each argument may or may not beassociated with a value.
each subsection has an@truth argument, with possible values true orfalse, reﬂecting whether the subsection applies ornot.
concretely, the input is (1) the string represen-tation of the subsection, (2) the annotations fromthe ﬁrst three tasks, and (3) values for some or allof its arguments.
arguments and values are rep-resented as an array of key-value pairs, where thenames of arguments speciﬁed in the structure an-.
2743figure 1: decomposing statutory reasoning into four tasks.
the ﬂowchart on the right indicates the ordering, inputsand outputs of the tasks.
in the statutes in the yellow box, argument placeholders are underlined, and superscriptsindicate argument coreference.
the green box shows the logical structure of the statutes just above it.
in blue aretwo examples of argument instantiation..figure 2: corpus statistics about arguments.
“random statutes” are 9 sections sampled from the us code..notations are used as keys.
in figure 1, comparethe names of arguments in the green box with thekey names in the blue boxes.
the output is val-ues for its arguments, in particular for the @truthargument.
in the example of the top right in fig-ure 1, the input values are taxpayer = alice andtaxable year = 2017, and one expected outputis @truth = true.
we refer to this task as argu-ment instantiation.
values for arguments can befound as spans in the case description, or must bepredicted based on the case description.
the latterhappens often for dollar amounts, where incomesmust be added, or tax must be computed.
figure 1shows two examples of this task, in blue..before determining whether a subsection applies,it may be necessary to infer the values of unspec-iﬁed arguments.
for example, in the top of fig-ure 1, it is necessary to determine who alice’sdeceased spouse and who the dependent mentionedin §2(a)(1)(b) are.
if applicable, we provide valuesfor these arguments, not as inputs, but as additionalsupervision for the model.
we provide manual an-notations for all (subsection, case) pairs in sara.
in addition, we run the prolog solver of holzen-berger et al.
(2020) to generate annotations for allpossible (subsection, case) pairs, to be used as asilver standard, in contrast to the gold manual an-notations.
we exclude from the silver data any(subsection, case) pair where the case is part of.
2744§2(a)(1) (taxp1, taxy2, spouse3, years4, household5, dependent6, deduction7, cost8) :    §2(a)(1)(a) (taxp1, taxy2, spouse3, years4) and    §2(a)(1)(b) (taxp1, taxy2, household5, dependent6, deduction7)§2(a)(1)(b) (taxp1, taxy2, household5, dependent6, deduction7) :    §151(c) (taxp1, taxy2, s24=dependent6)§2.
deﬁnitions and special rules(a) deﬁnition of surviving spouse    (1) in general    for purposes of section 1, the term "surviving spouse" means a taxpayer1-        (a) whose1 spouse3 died during either of the two years4 immediately preceding the taxable year2, and        (b) who maintains as his home a household5 which constitutes for the taxable year2 the principal place of abode (as a member of such household5) of a dependent6 (i) who (within the meaning of section 152) is a son, stepson, daughter, or stepdaughter of the taxpayer1, and (ii) with respect to whom the taxpayer1 is entitled to a deduction7 for the taxable year2 under section 151.    for purposes of this paragraph, an individual1 shall be considered as maintaining a household5 only if over half of the cost8 of maintaining the household5 during the taxable year2 is furnished by such individual1.alice married bob on may 29th, 2008. their son charlie was born october 4th, 2004. bob died october 22nd, 2016. alice's gross income for the year 2016 was $113580.
in 2017, alice's gross income was $567192.
in 2017, alice and charlie lived in a house maintained by alice, and alice was allowed a deduction of $59850 for donating cash to a charity.
charlie had no income in 2017.does section 2(a)(1) apply to alice in 2017?input values.
taxp1 = alice, taxy2 = 2017expected output.
spouse3 = bob, years4 = 2016, household5 = house, dependent6 = charlie, deduction7 = charlie, @truth = truealice employed bob from jan 2nd, 2011 to oct 10, 2019, paying him $1513 in 2019. on oct 10, 2019 bob was diagnosed as disabled and retired.
alice paid bob $298 because she had to terminate their contract due to bob's disability.
in 2019, alice's gross income was $567192.
in 2019, alice lived together with charlie, her father, in a house that she maintains.
charlie had no income in 2019. alice takes the standard deduction in 2019.does section 2(a)(1) apply to alice in 2019?input values.
taxp1 = alice, taxy2 = 2019expected output.
@truth = falseargument identiﬁcationargument coreferencestructure extractionargument instantiation0123456789101112131415160.00.2mentions per subsection0123456789100.00.2arguments per subsection123450.00.5mentions per argumentsara statutesrandom statutes(a) argument coreference0.00.5123456789+in words0.00.50-910-1920-2930-3940-4950+incharacters(b) lengths of argument placeholdersthe test set.
this increases the amount of availabletraining data by a factor of 210..3 baseline models.
we provide baselines for three tasks, omitting struc-ture extraction because it is the one task with thehighest return on human annotation effort2.
in otherwords, if humans could annotate for any of thesefour tasks, structure extraction is where we posittheir involvement would be the most worthwhile.
further, pertierra et al.
(2017) have shown that therelated task of semantic parsing of legal statutes isa difﬁcult task, calling for a complex model..3.1 argument identiﬁcation.
we run the stanford parser (socher et al., 2013) onthe statutes, and extract all noun phrases as spans –speciﬁcally, all nnp, nnps, prp$, np and nmlconstituents.
while de-formatting legal text canboost parser performance (morgenstern, 2014), wefound it made little difference in our case..as an orthogonal approach, we train a bert-based crf model for the task of bio tagging.
withthe 9 sections in the sara v2 statutes, we create7 equally-sized splits by grouping §68, 3301 and7703 into a single split.
we run a 7-fold cross-validation, using 1 split as a dev set, 1 split as a testset, and the remaining as training data.
we embedeach paragraph using bert, classify each contex-tual subword embedding into a 3-dimensional logitwith a linear layer, and run a crf (lafferty et al.,2001).
the model is trained with gradient descentto maximize the log-likelihood of the sequence ofgold tags.
we experiment with using legal bert(holzenberger et al., 2020) and bert-base-cased(devlin et al., 2019) as our bert model.
we freezeits parameters and optionally unfreeze the last layer.
we use a batch size of 32 paragraphs, a learningrate of 10−3 and the adam optimizer (kingma andba, 2015).
based on f1 score measured on the devset, the best model uses legal bert and unfreezesits last layer.
test results are shown in table 1..3.2 argument coreference.
argument coreference differs from the usual coref-erence task (pradhan et al., 2014), even though weare using similar terminology, and frame it in asimilar way.
in argument coreference, it is equally.
2code for the experiments can be found under https:.
//github.com/sgfddttt/sara_v2.
parser-based avg ± stddev macro17.6 ± 4.416.6precision77.9 ± 5.077.3recall28.6 ± 6.2f127.3avg ± stddev macrobert-based64.7 ± 15.065.1precision69.0 ± 24.259.8recall66.2 ± 20.562.4f1.
table 1: argument identiﬁcation results.
average andstandard deviations are computed across test splits..as important to link two coreferent argument men-tions as it is not to link two different arguments.
incontrast, regular coreference emphasizes the pre-diction of links between mentions.
we thus reporta different metric in tables 2 and 4, exact matchcoreference, which gives credit for returning a clus-ter of mentions that corresponds exactly to an argu-ment.
in figure 1, a system would be rewarded forlinking together both mentions of the taxpayer in§2(a)(1)(b), but not if any of the two mentions werelinked to any other mention within §2(a)(1)(b).
this custom metric gives as much credit for cor-rectly linking a single-mention argument (no links),as for a 5-mention argument (10 links)..single mention baseline here, we predict nocoreference links.
under usual coreference met-rics, this system can have low performance..string matching baseline this baseline predictsa coreference link if the placeholder strings of twoarguments are identical, up to the presence of thewords such, a, an, the, any, his and every..single mentionprecisionrecallf1string matchingprecisionrecallf1.
avg ± stddev macro81.7 ± 28.968.286.9 ± 21.882.783.8 ± 26.074.8avg ± stddev macro91.2 ± 20.085.592.8 ± 16.889.491.8 ± 18.687.4.table 2: exact match coreference results.
average andstandard deviations are computed across subsections..we also provide usual coreference metrics in ta-ble 3, using the code associated with pradhan et al.
(2014).
this baseline perfectly resolves corefer-ence for 80.8% of subsections, versus 68.9% forthe single mention baseline..2745single mention0 / 0 / 0.mucceafm 82.5 / 82.5 / 82.5ceafe77.3 / 93.7 / 84.7blanc 50.0 / 50.0 / 50.0.string matching82.1 / 64.0 / 71.992.1 / 92.1 / 92.190.9 / 95.2 / 93.089.3 / 81.0 / 84.7.table 3: argument coreference baselines scored withusual metrics.
results are shown as precision / re-call / f1..in addition, we provide a cascade of the bestmethods for argument identiﬁcation and corefer-ence, and report results in table 4. the cascadeperfectly resolves a subsection’s arguments in only16.4% of cases.
this setting, which groups the ﬁrsttwo tasks together, offers a signiﬁcant challenge..cascadeprecisionrecallf1.
avg ± stddev macro54.5 ± 35.658.053.5 ± 37.252.454.7 ± 33.455.1.table 4: exact match coreference results for bert-based argumentidentiﬁcation followed by stringmatching-based argument coreference.
average andstandard deviations are computed across subsections..3.3 argument instantiation.
argument instantiation takes into account the in-formation provided by previous tasks.
we startby instantiating the arguments of a single subsec-tion, without regard to the structure of the statutes.
we then describe how the structure information isincorporated into the model..algorithm 1 argument instantiation for a singlesubsectionrequire: argument spans with coreference information a,input argument-value pairs d, subsection text s, casedescription c.p ← ∅for a in a \ {@truth} do.
r ← insertvalues(s, a, d, p )y ← bert(c, r)x ← computeattentivereps(y, a)v ← predictvalue(x)p ← p ∪ (a, v).
ensure: output argument-value pairs p1: function arginstantiation(a, d, s, c)2:3:4:5:6:7:8:9:10:11:12:13:14: return p15: end function.
end forr ← insertvalues(s, a, d, p )y ← bert cls(c, r)t ← truthpredictor(y)p ← p ∪ (@truth, t).
single subsection we follow the paradigm ofchen et al.
(2020), where we iteratively modify thetext of the subsection by inserting argument values,and predict values for uninstantiated arguments.
throughout the following, we refer to algorithm 1and to its notation..for each argument whose value is provided, wereplace the argument’s placeholders in subsections by the argument’s value, using insertvalues(line 4).
this yields mostly grammatical sentences,with occasional hiccups.
with §2(a)(1)(a) andthe top right case from figure 1, we obtain “(a)alice spouse died during either of the two yearsimmediately preceding 2017”..we concatenate the text of the case c withthe subsection r, andthe modiﬁed text ofembed it using bert (line 5), yielding asequence of contextual subword embeddingsy = {yi ∈ r768 | i = 1...n}.
keeping with the no-tation of chen et al.
(2020), assume that the em-bedded case is represented by the sequence ofvectors t1, ..., tm and the embedded subsectionby s1, ..., sn.
for a given argument a, computeits attentive representation ˜s1, ..., ˜sm and its aug-mented feature vectors x1, ..., xm.
this operation,described by chen et al.
(2020), is performed bycomputeattentivereps (line 6).
the aug-mented feature vectors x1, ..., xm represent theargument’s placeholder, conditioned on the textof the statute and case..based on the name of the argument span, wepredict its value v either as an integer or a spanfrom the case description, using predictvalue(line 7).
for integers, as part of the model training,we run k-means clustering on the set of all integervalues in the training set, with enough centroidssuch that returning the closest centroid instead ofthe true value yields a numerical accuracy of 1 (seebelow).
for any argument requiring an integer (e.g.
tax), the model returns a weighted average of thecentroids.
the weights are predicted by a linearlayer followed by a softmax, taking as input anaverage-pooling and a maxpooling of x1, ..., xm.
for a span from the case description, we followthe standard procedure for ﬁne-tuning bert onsquad (devlin et al., 2019).
the unnormalizedprobability of the span from tokens i to j is givenby el·xi+r·xj where l, r are learnable parameters.
the predicted value v is added to the set of pre-dictions p (line 8), and will be used in subsequentiterations to replace the argument’s placeholder.
2746in the subsection.
we repeat this process until avalue has been predicted for every argument, ex-cept @truth (lines 3-9).
arguments are processedin order of appearance in the subsection.
finally,we concatenate the case and fully grounded sub-section and embed them with bert (lines 10-11),then use a linear predictor on top of the representa-tion for the [cls] token to predict the value for the@truth argument (line 12)..if q is a subsection and a leaf node thendq ← getargvaluepairs(q)˜s ← getsubsectiontext(q)q ← arginstantiation(a, dq, ˜s, c)else if q is a subsection and not a leaf node then.
algorithm 2 argument instantiation with depen-denciesrequire: argument spans with coreference information a,structure information t , input argument-value pairs d,subsection s, case description censure: output argument-value pairs p1: function arginstantiationfull(a, t, d, s, c)t ← builddependencytree(s, t )2:t ← populateargvalues(t, d)3:q ← depth-ﬁrst traversal of t4:for q in q do5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24: return p25: end function.
dq ← getargvaluepairs(q)x ← getchild(q)dx ← getargvaluepairs(x)dq ← dq ∪ dx˜s ← getsubsectiontext(q)q ← arginstantiation(a, dq, ˜s, c).
end forx ← root(t)p ← getargvaluepairs(x).
c ← getchildren(q)q ← dooperation(c, q).
else if q ∈ {and, or, not} then.
end if.
subsection with dependencies to describe ourprocedure at a high-level, we use the structure ofthe statutes to build out a computational graph,where nodes are either subsections with argument-value pairs, or logical operations.
we resolvenodes one by one, depth ﬁrst.
we treat the single-subsection model described above as a function,taking as input a set of argument-value pairs, astring representation of a subsection, and a stringrepresentation of a case, and returning a set ofargument-value pairs.
algorithm 2 and figure 3summarize the following..we start by building out the subsection’s depen-dency tree, as speciﬁed by the structure annotations(lines 2-4).
first, we build the tree structure usingbuilddependencytree.
then, values for argu-ments are propagated from parent to child, from the.
root down, with populateargvalues.
the treeis optionally capped to a predeﬁned depth.
eachnode is either an input for the single-subsectionfunction or its output, or a logical operation.
wethen traverse the tree depth ﬁrst, performing thefollowing operations, and replacing the node withthe result of the operation:.
• if the node q is a leaf, resolve it using thesingle-subsection function arginstantiation(lines 6-9 in algorithm 2; step 1 in figure 3)..• if the node q is a subsection that is not a leaf,ﬁnd its child node x (getchild, line 12), andcorresponding argument-value pairs other than@truth, dx (getargvaluepairs, line 13).
merge dx with dq, the argument-value pairsof the main node q (line 14).
finally, resolvethe parent node q using the single-subsectionfunction (lines 15-16; step 3 in figure 3..• if node q is a logical operation (line 17), get itschildren c (getchildren, line 18), to whichthe operation will be applied with doopera-tion (line 19) as follows:.
– if q == not, assign the negation of the.
child’s @truth value to q..– if q == or, pick its child with the highest@truth value, and assign its arguments’values to q..– if q == and, transfer the argument-valuepairs from all its children to q. in case ofconﬂicting values, use the value associatedwith the lower @truth value.
this opera-tion can be seen in step 4 of figure 3..this procedure follows the formalism of neu-ral module networks (andreas et al., 2016) and isillustrated in figure 3. reentrancy into the depen-dency tree is not possible, so that a decision madeearlier cannot be backtracked on at a later stage.
one could imagine doing joint inference, or us-ing heuristics for revisiting decisions, for examplewith a limited number of reentrancies.
humans aregenerally able to resolve this task in the order ofthe text, and we assume it should be possible for acomputational model too.
our solution is meant tobe computationally efﬁcient, with the hope of notsacriﬁcing too much performance.
revisiting thisassumption is left for future work..2747figure 3: argument instantiation with the top example from figure 1. at each step, nodes to be processed arein blue, nodes being processed in yellow, and nodes already processed in green.
the last step was omitted, andinvolves determining the truth value of the root node’s @truth argument..metrics and evaluation arguments whose valueneeds to be predicted fall into three categories.
the @truth argument calls for a binary truthvalue, and we score a model’s output using bi-nary accuracy.
the values of some arguments,such as gross income, are dollar amounts.
wescore such values using numerical accuracy, as1 if ∆(y, ˆy) =max(0.1∗y,5000) < 1 else 0, whereˆy is the prediction and y the target.
all other argu-ment values are treated as strings.
in those cases,we compute accuracy as exact match between pre-dicted and gold value.
each of these three metricsdeﬁnes a form of accuracy.
we average the threemetrics, weighted by the number of samples, toobtain a uniﬁed accuracy metric, used to comparethe performance of models..|y−ˆy|.
training based on the type of value expected,we use different loss functions.
for @truth, weuse binary cross-entropy.
for numerical values,we use the hinge loss max(∆(y, ˆy) − 1, 0).
forstrings, let s be all the spans in the case descriptionequal to the expected value.
the loss functionis log((cid:80)i,j∈s el·xi+r·xj )(clark and gardner, 2018).
the model is trainedend-to-end with gradient descent..i≤j el·xi+r·xj ) − log((cid:80).
we start by training models on the silver data,as a pre-training step.
we sweep the values of thelearning rate in {10−2, 10−3, 10−4, 10−5} and thebatch size in {64, 128, 256}.
we try both bert-base-cased and legal bert, allowing updates tothe parameters of its top layer.
we set aside 10%of the silver data as a dev set, and select the bestmodel based on the uniﬁed accuracy on the devset.
training is split up into three stages.
thesingle-subsection model iteratively inserts valuesfor arguments into the text of the subsection.
in.
the ﬁrst stage, regardless of the predicted value,we insert the gold value for the argument, as inteacher forcing (kolen and kremer, 2001).
in thesecond and third stages, we insert the value pre-dicted by the model.
when initializing the modelfrom one stage to the next, we pick the model withthe highest uniﬁed accuracy on the dev set.
inthe ﬁrst two stages, we ignore the structure of thestatutes, which effectively caps the depth of eachdependency tree at 1..picking the best model from this pre-trainingstep, we perform ﬁne-tuning on the gold data.
we take a k-fold cross-validation approach (stone,1974).
we randomly split the sara v2 training setinto 10 splits, taking care to put pairs of cases test-ing the same subsection into the same split.
eachsplit contains nearly exactly the same proportionof binary and numerical cases.
we sweep the val-ues of the learning rate and batch size in the sameranges as above, and optionally allow updates tothe parameters of bert’s top layer.
for a givenset of hyperparameters, we run training on eachsplit, using the dev set and the uniﬁed metric forearly stopping.
we use the performance on thedev set averaged across the 10 splits to evaluatethe performance of a given set of hyperparameters.
using that criterion, we pick the best set of hyper-parameters.
we then pick the ﬁnal model as thatwhich achieves median performance on the dev set,across the 10 splits.
we report the performance ofthat model on the test set..in table 5, we report the relevant argumentinstantiation metrics, under @truth, dollaramount and string.
for comparison, we alsoreport binary and numerical accuracy metrics de-ﬁned in holzenberger et al.
(2020).
the reported.
2748initial treeands2_a_1_a(taxp1=alice, taxy2=2017, spouse3, years4)s2_a_1_b(taxp1=alice, taxy2=2017, household5, dependent6, deduction7)s151_c(taxp1=alice, taxy2=2017, s24=dependent6)andtaxp1=alice, taxy2=2017, spouse3=bob, years4=2016,  @truth=.9s2_a_1_b(taxp1=alice, taxy2=2017, household5, dependent6, deduction7)s151_c(taxp1=alice, taxy2=2017, s24=dependent6)step 1ands2_a_1_b(taxp1=alice, taxy2=2017, household5, dependent6, deduction7)taxp1=alice, taxy2=2017, dependent6=charlie, @truth=.8step 2step  3andtaxp1=alice, taxy2=2017, household5=house, dependent6=charlie, deduction7=charlie, @truth=.7taxp1=alice, taxy2=2017, dependent6=charlie, @truth=.8step 4taxp1=alice, taxy2=2017, spouse3=bob, years4=2016, household5=house, dependent6=charlie, deduction7=charlie, cost8, @truth=.7)…taxp1=alice, taxy2=2017, household5=house, dependent6=charlie, deduction7=charlie, @truth=.7taxp1=alice, taxy2=2017, dependent6=charlie, @truth=.8step 5taxp1=alice, taxy2=2017, spouse3=bob, years4=2016, household5=house, dependent6=charlie, deduction7=charlie, @truth=.7taxp1=alice, taxy2=2017, household5=house, dependent6=charlie, deduction7=charlie, @truth=.7taxp1=alice, taxy2=2017, dependent6=charlie, @truth=.8s2_a_1(taxp1=alice, taxy2=2017, spouse3, years4, household5, dependent6, deduction7, cost8)s2_a_1(taxp1=alice, taxy2=2017, spouse3, years4, household5, dependent6, deduction7, cost8)s2_a_1(taxp1=alice, taxy2=2017, spouse3, years4, household5, dependent6, deduction7, cost8)s2_a_1(taxp1=alice, taxy2=2017, spouse3, years4, household5, dependent6, deduction7, cost8)s2_a_1(taxp1=alice, taxy2=2017, spouse3, years4, household5, dependent6, deduction7, cost8)taxp1=alice, taxy2=2017, spouse3=bob, years4=2016,  @truth=.9taxp1=alice, taxy2=2017, spouse3=bob, years4=2016,  @truth=.9taxp1=alice, taxy2=2017, spouse3=bob, years4=2016,  @truth=.9taxp1=alice, taxy2=2017, spouse3=bob, years4=2016,  @truth=.9@truth58.3 ± 7.558.3 ± 7.559.2 ± 7.557.5 ± 7.565.8 ± 7.260.8 ± 7.4.dollar amount18.2 ± 11.539.4 ± 14.623.5 ± 12.520.6 ± 11.920.6 ± 11.920.6 ± 11.9.string4.4 ± 7.44.4 ± 7.437.5 ± 17.337.5 ± 17.333.3 ± 16.833.3 ± 16.8.uniﬁed43.3 ± 6.247.2 ± 6.249.4 ± 6.247.8 ± 6.252.8 ± 6.249.4 ± 6.2.baseline+ silverbert- pre-training- structure- pre-training,structure.
binary50 ± 8.350 ± 8.351 ± 8.349 ± 8.359 ± 8.253 ± 8.3.numerical30 ± 18.145 ± 19.730 ± 18.130 ± 18.130 ± 18.130 ± 18.1(best results in bold).
table 5: argument instantiation.
we report accuracies, in %, and the 90% conﬁdence interval.
right of the bar areaccuracy metrics proposed with the initial release of the dataset.
blue cells use the silver data, brown cells do not.
“bert” is the model described in section 3.3. ablations to it are marked with a “-” sign..baseline has three parameters.
for @truth, it re-turns the most common value for that argument onthe train set.
for arguments that call for a dollaramount, it returns the one number that minimizesthe dollar amount hinge loss on the trainingset.
for all other arguments, it returns the mostcommon string answer in the training set.
thoseparameters vary depending on whether the trainingset is augmented with the silver data..4 discussion.
our goal in providing the baselines of section 3 isto identify performance bottlenecks in the proposedsequence of tasks.
argument identiﬁcation poses amoderate challenge, with a language model-basedapproach achieving non-trivial f1 score.
the sim-ple parser-based method is not a sufﬁcient solution,but with its high recall could serve as the backboneto a statistical method.
argument coreference is asimpler task, with string matching perfectly resolv-ing nearly 80% of the subsections.
this is in linewith the intuition that legal language is very explicitabout disambiguating coreference.
as reported intable 3, usual coreference metrics seem lower, butonly reﬂect a subset of the full task: coreferencemetrics are only concerned with links, so that ar-guments appearing exactly once bear no weightunder that metric, unless they are wrongly linkedto another argument..argument instantiation is by far the most chal-lenging task, as the model needs strong natural lan-guage understanding capabilities.
simple baselinescan achieve accuracies above 50% for @truth,since for all numerical cases, @truth = true.
wereceive a slight boost in binary accuracy from usingthe proposed paradigm, departing from previousresults on this benchmark.
as compared to the base-line, the models mostly lag behind for the dollar.
amount and numerical accuracies, which can beexplained by the lack of a dedicated numericalsolver, and sparse data.
further, we have madea number of simplifying assumptions, which maybe keeping the model from taking advantage of thestructure information: arguments are instantiatedin order of appearance, forbidding joint prediction;revisiting past predictions is disallowed, forcingthe model to commit to wrong decisions made ear-lier; the depth of the dependency tree is capped at3; and ﬁnally, information is being passed alongthe dependency tree in the form of argument val-ues, as opposed to dense, high-dimensional vectorrepresentations.
the latter limits both the ﬂow ofinformation and the learning signal.
this could alsoexplain why the use of dependencies is detrimentalin some cases.
future work would involve jointprediction (chan et al., 2019), and more carefuluse of structure information..looking at the errors made by the best model intable 5 for binary accuracy, we note that for 39 pos-itive and negative case pairs, it answers each pairidentically, thus yielding 39 correct answers.
in theremaining 11 pairs, there are 10 pairs where it getsboth cases right.
this suggests it may be guessingrandomly on 39 pairs, and understanding 10. thebest bert-based model for dollar amountspredicts the same number for each case, as does thebaseline.
the best models for string argumentsgenerally make predictions that match the categoryof the expected answer (date, person, etc) whilefailing to predict the correct string..performance gains from silver data are notice-able and generally consistent, as can be seen bycomparing brown and blue cells in table 5. thesilver data came from running a human-written pro-log program, which is costly to produce.
a possiblesubstitute is to ﬁnd mentions of applicable statutesin large corpora of legal cases (caselaw, 2019), for.
2749example using high-precision rules (ratner et al.,2017), which has been successful for extractinginformation from cases (boniol et al., 2020)..in this work, each task uses the gold annotationsfrom upstream tasks.
ultimately, the goal is to passthe outputs of models from one task to the next..5 related work.
law-related nlp tasks have ﬂourished in the pastyears, with applications including answering barexam questions (yoshioka et al., 2018; zhong et al.,2020), information extraction (chalkidis et al.,2019b; boniol et al., 2020; lam et al., 2020), man-aging contracts (elwany et al., 2019; liepin¸a et al.,2020; nyarko, 2021) and analyzing court deci-sions (sim et al., 2015; lee and mouritsen, 2017).
case-based reasoning has been approached withexpert systems (popp and schlink, 1974; hellawell,1980; v. d. l. gardner, 1983), high-level hand-annotated features (ashley and br¨uninghaus, 2009)and transformer-based models (rabelo et al., 2019).
closest to our work is saeidi et al.
(2018), wherea dialog agent’s task is to answer a user’s questionabout a set of regulations.
the task relies on a setof questions provided within the dataset..clark et al.
(2019) as well as preceding work(friedland et al., 2004; gunning et al., 2010) tacklea similar problem in the science domain, with thegoal of using the prescriptive knowledge from sci-ence textbooks to answer exam questions.
thecore of their model relies on several nlp and spe-cialized reasoning techniques, with contextualizedlanguage models playing a major role.
clark et al.
(2019) take the route of sorting questions into dif-ferent types, and working on specialized solvers.
incontrast, our approach is to treat each question iden-tically, but to decompose the process of answeringinto a sequence of subtasks..the language of statutes is related to procedu-ral language, which describes steps in a process.
zhang et al.
(2012) collect how-to instructionsin a variety of domains, while wambsganss andfromm (2019) focus on automotive repair instruc-tions.
branavan et al.
(2012) exploit instructions ina game manual to improve an agent’s performance.
dalvi et al.
(2019) and amini et al.
(2020) turn tomodeling textual descriptions of physical and bio-logical mechanisms.
weller et al.
(2020) proposemodels that generalize to new task descriptions..the tasks proposed in this work are germane tostandard nlp tasks, such as named entity recog-.
nition (ratinov and roth, 2009), part-of-speechtagging (petrov et al., 2012; akbik et al., 2018),and coreference resolution (pradhan et al., 2014).
structure extraction is conceptually similar to syn-tactic (socher et al., 2013) and semantic parsing(berant et al., 2013), which pertierra et al.
(2017)attempt for a subsection of tax law..argument instantiation is closest to the task ofaligning predicate argument structures (roth andfrank, 2012; wolfe et al., 2013).
we frame argu-ment instantiation as iteratively completing a state-ment in natural language.
chen et al.
(2020) reﬁnegeneric statements by copying strings from inputtext, with the goal of detecting events.
chan et al.
(2019) extend transformer-based language modelsto permit inserting tokens anywhere in a sequence,thus allowing to modify an existing sequence.
forargument instantiation, we make use of neural mod-ule networks (andreas et al., 2016), which are usedin the visual (yi et al., 2018) and textual domains(gupta et al., 2020).
in that context, arguments andtheir values can be thought of as the hints fromkhot et al.
(2020).
the prolog-based data augmen-tation is related to data augmentation for semanticparsing (campagna et al., 2019; weir et al., 2019)..6 conclusion.
solutions to tackle statutory reasoning may rangefrom high-structure, high-human involvement ex-pert systems,largely self-to less structured,supervised language models.
here, taking inspira-tion from prolog programs, we introduce a novelparadigm, by breaking statutory reasoning downinto a sequence of tasks.
each task can be an-notated for with far less expertise than would berequired to translate legal language into code, andcomes with its own performance metrics.
our con-tribution enables ﬁner-grained scoring and debug-ging of models for statutory reasoning, which fa-cilitates incremental progress and identiﬁcation ofperformance bottlenecks.
in addition, argument in-stantiation and explicit resolution of dependenciesintroduce further interpretability.
this novel ap-proach could possibly inform the design of modelsthat reason with rules speciﬁed in natural language,for the domain of legal nlp and beyond..acknowledgments.
the authors thank andrew blair-stanek for help-ful comments, and ryan culkin for help with theparser-based argument identiﬁcation baseline..2750references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics, coling2018, santa fe, new mexico, usa, august 20-26,2018, pages 1638–1649.
association for computa-tional linguistics..aida amini, antoine bosselut, bhavana dalvi mishra,yejin choi, and hannaneh hajishirzi.
2020. pro-cedural reading comprehension with attribute-awarecontext ﬂow.
in conference on automated knowl-edge base construction, akbc 2020, virtual, june22-24, 2020..jacob andreas, marcus rohrbach, trevor darrell, anddan klein.
2016. neural module networks.
in 2016ieee conference on computer vision and patternrecognition, cvpr 2016, las vegas, nv, usa, june27-30, 2016, pages 39–48.
ieee computer society..kevin d. ashley and stefanie br¨uninghaus.
2009. au-tomatically classifying case texts and predicting out-comes.
artif.
intell.
law, 17(2):125–165..jonathan berant, andrew chou, roy frostig, andpercy liang.
2013. semantic parsing on freebasefrom question-answer pairs.
in proceedings of the2013 conference on empirical methods in naturallanguage processing, emnlp 2013, 18-21 octo-ber 2013, grand hyatt seattle, seattle, washington,usa, a meeting of sigdat, a special interest groupof the acl, pages 1533–1544.
acl..paul boniol, george panagopoulos, christos xy-polopoulos, rajaa el hamdani, david restrepoamariles, and michalis vazirgiannis.
2020. per-formance in the courtroom: automated processingand visualization of appeal court decisions in france.
in proceedings of the natural legal language pro-cessing workshop 2020 co-located with the 26thacm sigkdd international conference on knowl-edge discovery & data mining (kdd 2020), vir-tual workshop, august 24, 2020, volume 2645 ofceur workshop proceedings, pages 11–17.
ceur-ws.org..s.r.k.
branavan, nate kushman, tao lei, and reginabarzilay.
2012. learning high-level planning fromtext.
in proceedings of the 50th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 126–135, jeju island,korea.
association for computational linguistics..giovanni campagna, silei xu, mehrad moradshahi,richard socher, and monica s. lam.
2019. genie:a generator of natural language semantic parsers forin proceedings of thevirtual assistant commands.
40th acm sigplan conference on programminglanguage design and implementation, pldi 2019,phoenix, az, usa, june 22-26, 2019, pages 394–410. acm..caselaw.
2019. caselaw access project..ilias chalkidis, ion androutsopoulos, and nikolaosaletras.
2019a.
neural legal judgment prediction inin proceedings of the 57th annual meet-english.
ing of the association for computational linguis-tics, pages 4317–4323, florence, italy.
associationfor computational linguistics..ilias chalkidis, manos fergadiotis, prodromos malaka-siotis, and ion androutsopoulos.
2019b.
large-scale multi-label text classiﬁcation on eu legislation.
in proceedings of the 57th conference of the as-sociation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume1: long papers, pages 6314–6322.
association forcomputational linguistics..william chan, nikita kitaev, kelvin guu, mitchellstern, and jakob uszkoreit.
2019. kermit: genera-tive insertion-based modeling for sequences.
corr,abs/1906.01604..yunmo chen, tongfei chen, seth ebner, aaron stevenwhite, and benjamin van durme.
2020. readingthe manual: event extraction as deﬁnition compre-hension.
in proceedings of the fourth workshop onstructured prediction for nlp@emnlp 2020, on-line, november 20, 2020, pages 74–83.
associationfor computational linguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics, acl2018, melbourne, australia, july 15-20, 2018, vol-ume 1: long papers, pages 845–855.
associationfor computational linguistics..peter clark, oren etzioni, daniel khashabi, tusharkhot, bhavana dalvi mishra, kyle richardson,ashish sabharwal, carissa schoenick, oyvindtafjord, niket tandon, sumithra bhakthavatsalam,dirk groeneveld, michal guerquin, and michaelschmitz.
2019. from ’f’ to ’a’ on the n.y. regentsscience exams: an overview of the aristo project.
corr, abs/1909.01958..ido dagan, oren glickman, and bernardo magnini.
the pascal recognising textual entail-2005.in machine learning challenges,ment challenge.
evaluating predictive uncertainty, visual objectclassiﬁcation and recognizing textual entailment,first pascal machine learning challenges work-shop, mlcw 2005, southampton, uk, april 11-13, 2005, revised selected papers, volume 3944 oflecture notes in computer science, pages 177–190.
springer..bhavana dalvi, niket tandon, antoine bosselut, wen-tau yih, and peter clark.
2019. everything hap-pens for a reason: discovering the purpose of ac-in proceedings of thetions in procedural text.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 4496–4505, hong kong,china.
association for computational linguistics..2751jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..emad elwany, dave moore, and gaurav oberoi.
2019.bert goes to law school: quantifying the compet-itive advantage of access to large legal corpora incontract understanding.
corr, abs/1911.00473..noah s. friedland, paul g. allen, gavin matthews,michael j. witbrock, david baxter, jon curtis,blake shepard, pierluigi miraglia, j¨urgen angele,steffen staab, eddie m¨onch, henrik oppermann,dirk wenke, david j. israel, vinay k. chaudhri,bruce w. porter, ken barker, james fan, shaw yichaw, peter z. yeh, dan tecuci, and peter clark.
2004. project halo: towards a digital aristotle.
aimag., 25(4):29–48..david gunning, vinay k. chaudhri, peter clark, kenbarker, shaw yi chaw, mark greaves, benjamin n.grosof, alice leung, david d. mcdonald, sunilmishra, john pacheco, bruce w. porter, aaronspaulding, dan tecuci, and jing tien.
2010. projecthalo update - progress toward digital aristotle.
aimag., 31(3):33–58..nitish gupta, kevin lin, dan roth, sameer singh, andmatt gardner.
2020. neural module networks forin 8th international confer-reasoning over text.
ence on learning representations, iclr 2020, ad-dis ababa, ethiopia, april 26-30, 2020. openre-view.net..robert hellawell.
1980. a computer program for le-gal planning and analysis: taxation of stock redemp-tions.
columbia law review, 80(7):1363–1398..nils holzenberger, andrew blair-stanek, and ben-jamin van durme.
2020. a dataset for statutory rea-soning in tax law entailment and question answer-ing.
in proceedings of the natural legal languageprocessing workshop 2020 co-located with the 26thacm sigkdd international conference on knowl-edge discovery & data mining (kdd 2020), vir-tual workshop, august 24, 2020, volume 2645 ofceur workshop proceedings, pages 31–38.
ceur-ws.org..alfred horn.
1951. on sentences which are true ofdirect unions of algebras.
j. symb.
log., 16(1):14–21..tushar khot, daniel khashabi, kyle richardson, peterclark, and ashish sabharwal.
2020. text modularnetworks: learning to decompose tasks in the lan-guage of existing models.
corr, abs/2009.00751..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..john f kolen and stefan c kremer.
2001. a ﬁeldguide to dynamical recurrent networks.
john wiley& sons..anne v. d. l. gardner.
1983. the design of a legal anal-ysis program.
in proceedings of the national con-ference on artiﬁcial intelligence, washington, d.c.,usa, august 22-26, 1983, pages 114–118.
aaaipress..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning (icml2001), williams college, williamstown, ma, usa,june 28 - july 1, 2001, pages 282–289.
morgankaufmann..jason t. lam, david liang, samuel dahan, andfarhana h. zulkernine.
2020. the gap betweendeep learning and law: predicting employment no-tice.
in proceedings of the natural legal languageprocessing workshop 2020 co-located with the 26thacm sigkdd international conference on knowl-edge discovery & data mining (kdd 2020), vir-tual workshop, august 24, 2020, volume 2645 ofceur workshop proceedings, pages 52–56.
ceur-ws.org..thomas r lee and stephen c mouritsen.
2017. judg-.
ing ordinary meaning.
yale lj, 127:788..r¯uta liepin¸a, federico ruggeri, francesca lagioia,marco lippi, kasper drazewski, and paolo tor-roni.
2020. explaining potentially unfair clausesto the consumer with the claudette tool.
inproceedings of the natural legal language pro-cessing workshop 2020 co-located with the 26thacm sigkdd international conference on knowl-edge discovery & data mining (kdd 2020), vir-tual workshop, august 24, 2020, volume 2645 ofceur workshop proceedings, pages 61–64.
ceur-ws.org..leora morgenstern.
2014. toward automated interna-tional law compliance monitoring (tailcm).
techni-cal report, leidos holdings inc reston va..julian nyarko.
2021. stickiness and incomplete con-tracts.
the university of chicago law review, 88..marcos a. pertierra, sarah lawsky, erik hemberg,and una-may o’reilly.
2017. towards formaliz-ing statute law as default logic through automatic se-mantic parsing.
in proceedings of the second work-shop on automated semantic analysis of informa-tion in legal texts co-located with the 16th inter-national conference on artiﬁcial intelligence and.
2752law (icail 2017), london, uk, june 16, 2017, vol-ume 2143 of ceur workshop proceedings.
ceur-ws.org..the twenty-ninth aaai conference on artiﬁcial in-telligence, january 25-30, 2015, austin, texas, usa,pages 2311–2317.
aaai press..slav petrov, dipanjan das, and ryan t. mcdonald.
2012. a universal part-of-speech tagset.
in proceed-ings of the eighth international conference on lan-guage resources and evaluation, lrec 2012, istan-bul, turkey, may 23-25, 2012, pages 2089–2096.
eu-ropean language resources association (elra)..walter g popp and bernhard schlink.
1974. judith, acomputer program to advise lawyers in reasoning acase.
jurimetrics j., 15:303..sameer pradhan, xiaoqiang luo, marta recasens, ed-uard hovy, vincent ng, and michael strube.
2014.scoring coreference partitions of predicted men-in proceed-tions: a reference implementation.
ings of the 52nd annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 30–35, baltimore, maryland.
associa-tion for computational linguistics..juliano rabelo, mi-young kim, and randy goebel.
2019. combining similarity and transformer meth-ods for case law entailment.
in proceedings of theseventeenth international conference on artiﬁcialintelligence and law, icail 2019, montreal, qc,canada, june 17-21, 2019, pages 290–296.
acm..lev-arie ratinov and dan roth.
2009. design chal-lenges and misconceptions in named entity recog-in proceedings of the thirteenth confer-nition.
ence on computational natural language learning,conll 2009, boulder, colorado, usa, june 4-5,2009, pages 147–155.
acl..alexander ratner, stephen h. bach, henry r. ehren-berg, jason alan fries, sen wu, and christopher r´e.
2017. snorkel: rapid training data creation withweak supervision.
proc.
vldb endow., 11(3):269–282..michael roth and anette frank.
2012. aligning predi-cate argument structures in monolingual comparabletexts: a new corpus for a new task.
in proceedingsof the first joint conference on lexical and com-putational semantics, *sem 2012, june 7-8, 2012,montr´eal, canada, pages 218–227.
association forcomputational linguistics..marzieh saeidi, max bartolo, patrick s. h. lewis,sameer singh, tim rockt¨aschel, mike sheldon,guillaume bouchard, and sebastian riedel.
2018.interpretation of natural language rules in conversa-tional machine reading.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, brussels, belgium, october 31 -november 4, 2018, pages 2087–2097.
associationfor computational linguistics..yanchuan sim, bryan r. routledge, and noah a.smith.
2015. the utility of text: the case of ami-cus briefs and the supreme court.
in proceedings of.
richard socher, john bauer, christopher d. manning,and andrew y. ng.
2013. parsing with compo-in proceedings of thesitional vector grammars.
51st annual meeting of the association for computa-tional linguistics, acl 2013, 4-9 august 2013, soﬁa,bulgaria, volume 1: long papers, pages 455–465.
the association for computer linguistics..mervyn stone.
1974. cross-validatory choice and as-sessment of statistical predictions.
journal of theroyal statistical society: series b (methodological),36(2):111–133..thomas vacek, ronald teo, dezhao song, timothynugent, conner cowling, and frank schilder.
2019.litigation analytics: case outcomes extracted fromin proceedings of theus federal court dockets.
natural legal language processing workshop 2019,pages 45–54, minneapolis, minnesota.
associationfor computational linguistics..thiemo wambsganss and hansj¨org fromm.
2019.mining user-generated repair instructions from auto-motive web communities.
in 52nd hawaii interna-tional conference on system sciences, hicss 2019,grand wailea, maui, hawaii, usa, january 8-11,2019, pages 1–10.
scholarspace..nathaniel weir, andrew crotty, alex galakatos, amirilkhechi, shekar ramaswamy, rohin bhushan,ugur c¸ etintemel, prasetya utama, nadja geisler,benjamin h¨attasch, steffen eger, and carsten bin-nig.
2019. dbpal: weak supervision for learninga natural language interface to databases.
corr,abs/1909.06182..orion weller, nicholas lourie, matt gardner, andmatthew e. peters.
2020. learning from task de-scriptions.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 1361–1375.
association for computationallinguistics..jay deyoung,.
justin snyder,.
travis wolfe, benjamin van durme, mark dredze,nicholas andrews, charley beller, chris callison-jonathanburch,weese, tan xu, and xuchen yao.
2013. parma:a predicate argument aligner.
in proceedings of the51st annual meeting of the association for compu-tational linguistics (volume 2: short papers), pages63–68, soﬁa, bulgaria.
association for computa-tional linguistics..tomer wolfson, mor geva, ankit gupta, matt gard-ner, yoav goldberg, daniel deutch, and jonathanberant.
2020. break it down: a question under-standing benchmark.
transactions of the associa-tion for computational linguistics, 8:183–198..2753chaojun xiao, haoxi zhong, zhipeng guo, cunchaotu, zhiyuan liu, maosong sun, yansong feng, xi-anpei han, zhen hu, heng wang, and jianfeng xu.
2018. cail2018: a large-scale legal dataset forjudgment prediction.
corr, abs/1807.02478..kexin yi, jiajun wu, chuang gan, antonio tor-ralba, pushmeet kohli, and josh tenenbaum.
2018.neural-symbolic vqa: disentangling reasoningin ad-from vision and language understanding.
vances in neural information processing systems31: annual conference on neural information pro-cessing systems 2018, neurips 2018, december 3-8,2018, montr´eal, canada, pages 1039–1050..masaharu yoshioka, yoshinobu kano, naoki kiyota,and ken satoh.
2018. overview of japanese statutelaw retrieval and entailment task at coliee-2018.
intwelfth international workshop on juris-informatics(jurisin 2018)..ziqi zhang, philip webster, victoria s. uren, andreavarga, and fabio ciravegna.
2012. automaticallyextracting procedural knowledge from instructionaltexts using natural language processing.
in proceed-ings of the eighth international conference on lan-guage resources and evaluation, lrec 2012, istan-bul, turkey, may 23-25, 2012, pages 520–527.
euro-pean language resources association (elra)..haoxi zhong, chaojun xiao, cunchao tu, tianyangzhang, zhiyuan liu, and maosong sun.
2020. jec-qa: a legal-domain question answering dataset.
inthe thirty-fourth aaai conference on artiﬁcial in-telligence, aaai 2020, the thirty-second innova-tive applications of artiﬁcial intelligence confer-ence, iaai 2020, the tenth aaai symposium on ed-ucational advances in artiﬁcial intelligence, eaai2020, new york, ny, usa, february 7-12, 2020,pages 9701–9708.
aaai press..2754a task examples.
{(15, 26), (35, 51), (62, 88), (92, 99), (122, 134),(155, 168), (173, 182), (188, 210)}.
in the following, we provide several examples foreach of the tasks deﬁned in section 2..output 1.a.1 argument identiﬁcation.
for ease of reading, the spans mentioned in theoutput are underlined in the input..input 1 (§3306(a)(1)(b)).
(b) on each of some 10 days during the calendaryear or during the preceding calendar year, eachday being in a different calendar week, employedat least one individual in employment for someportion of the day..output 1.
{(15, 26), (35, 51), (62, 88), (92, 99), (122, 134),(155, 168), (173, 182), (188, 210)}.
input 2 (§63(c)(5)).
in the case of an individual with respect to whoma deduction under section 151 is allowable to an-other taxpayer for a taxable year beginning in thecalendar year in which the individual’s taxable yearbegins, the basic standard deduction applicable tosuch individual for such individual’s taxable yearshall not exceed the greater of-.
output 2.
{(15, 27), (50, 60), (96, 111), (117, 130),(145, 161), (172, 185), (189, 200), (210, 237),(253, 267), (273, 287), (291, 302), (321, 331)}.
input 3 (§1(d)(iv)).
(iv) $31,172, plus 36% of the excess over $115,000if the taxable income is over $115,000 but not over$250,000;.
output 3.
{(5, 45), (50, 67)}.
a.2 argument coreference.
we report the full matrix c. in addition, for easeof reading, coreference clusters are marked withsuperscripts in the input..input 1 (§3306(a)(1)(b)).
(b) on each of some 10 days1 during the calendaryear2 or during the preceding calendar year3, eachday1 being in a different calendar week4, employedat least one individual5 in employment6 for someportion of the day7...
.
1 0 0 1 0 0 0 00 1 0 0 0 0 0 00 0 1 0 0 0 0 01 0 0 1 0 0 0 00 0 0 0 1 0 0 00 0 0 0 0 1 0 00 0 0 0 0 0 1 00 0 0 0 0 0 0 1.
.
.
input 2 (§63(c)(5))in the case of an individual1 with respect to whoma deduction2 under section 151 is allowable to an-other taxpayer3 for a taxable year4 beginning in thecalendar year5 in which the individual1’s taxableyear6 begins, the basic standard deduction7 appli-cable to such individual1 for such individual1’staxable year6 shall not exceed the greater8 of-{(15, 27), (50, 60), (96, 111), (117, 130),(145, 161), (172, 185), (189, 200), (210, 237),(253, 267), (273, 287), (291, 302), (321, 331)}.
output 2.
.
.
1 0 0 0 0 1 0 0 1 1 0 00 1 0 0 0 0 0 0 0 0 0 00 0 1 0 0 0 0 0 0 0 0 00 0 0 1 0 0 0 0 0 0 0 00 0 0 0 1 0 0 0 0 0 0 01 0 0 0 0 1 0 0 1 1 0 00 0 0 0 0 0 1 0 0 0 1 00 0 0 0 0 0 0 1 0 0 0 01 0 0 0 0 1 0 0 1 1 0 01 0 0 0 0 1 0 0 1 1 0 00 0 0 0 0 0 1 0 0 0 1 00 0 0 0 0 0 0 0 0 0 0 1.
.
.
input 3 (§1(d)(iv)).
(iv) $31,172, plus 36% ofthe excess over$115,0001 if the taxable income2 is over $115,000but not over $250,000;{(5, 45), (50, 67)}.
2755output 3.
(cid:19).
(cid:18) 1 00 1.a.3 structure extraction.
to clarify the link between the input and the output,we are adding superscripts to argument names inthe output.
while the output is represented as plaintext, a graph-based representation would likely beused in a practical system, to facilitate learning andinference.
arguments are keyword based.
for ex-ample, in output 2, the value of the taxp argumentof §63(c)(5) is passed to the spouse argument of§151(b).
if no equal sign is speciﬁed, it meansthe argument names match.
for example, part ofoutput 2 could have been rewritten more explicitlyas §151(b)(spouse=taxp, taxp=s45, taxy=taxy)..input 1 (§3306(a)(1)(b)).
(b) on each of some 10 days1 during the calendaryear2 or during the preceding calendar year3, eachday1 being in a different calendar week4, employedat least one individual5 in employment6 for someportion of the day7.
{(15, 26), (35, 51), (62, 88), (92, 99), (122, 134),(155, 168), (173, 182), (188, 210)}.
.
.
1 0 0 1 0 0 0 00 1 0 0 0 0 0 00 0 1 0 0 0 0 01 0 0 1 0 0 0 00 0 0 0 1 0 0 00 0 0 0 0 1 0 00 0 0 0 0 0 1 00 0 0 0 0 0 0 1.
.
.
.
.
[.
{(15, 27), (50, 60), (96, 111), (117, 130),(145, 161), (172, 185), (189, 200), (210, 237),(253, 267), (273, 287), (291, 302), (321, 331)}.
1 0 0 0 0 1 0 0 1 1 0 00 1 0 0 0 0 0 0 0 0 0 00 0 1 0 0 0 0 0 0 0 0 00 0 0 1 0 0 0 0 0 0 0 00 0 0 0 1 0 0 0 0 0 0 01 0 0 0 0 1 0 0 1 1 0 00 0 0 0 0 0 1 0 0 0 1 00 0 0 0 0 0 0 1 0 0 0 01 0 0 0 0 1 0 0 1 1 0 01 0 0 0 0 1 0 0 1 1 0 00 0 0 0 0 0 1 0 0 0 1 00 0 0 0 0 0 0 0 0 0 0 1.
.
.
output 2.
§63(c)(5)(bassd7, grossinc, s453, taxp1, taxy6,s44b2, s46b4, s475, s488) :-.
§151(b)(spouse=taxp, taxp=s45, taxy) or§151(c)(s24a=taxp, taxp=s45, taxy).]
and§63(c)(5)(a)() and§63(c)(5)(b)(grossinc, taxp)..input 3 (§1(d)(iv)).
(iv) $31,172, plus 36% ofthe excess over$115,0001 if the taxable income2 is over $115,000but not over $250,000;{(5, 45), (50, 67)}.
(cid:19).
(cid:18) 1 00 1.output 3.
§1(d)(iv)(tax1, taxinc2)..output 1.
§3306(a)(1)(b)(caly2, s167, workday1, employment6,.
preccaly3, employee5, s13a4, employer,service) :-.
§3306(c)(employee, employer, service)..input 2 (§63(c)(5)).
in the case of an individual1 with respect to whoma deduction2 under section 151 is allowable to an-other taxpayer3 for a taxable year4 beginning in thecalendar year5 in which the individual1’s taxableyear6 begins, the basic standard deduction7 appli-cable to such individual1 for such individual1’staxable year6 shall not exceed the greater8 of-.
a.4 argument instantiation.
the following are example cases.
in addition tothe case description, subsection to apply and inputargument-value pairs, the agent has access to theoutput of argument identiﬁcation, argument coref-erence and structure extraction, for the entirety ofthe statutes..input 1: case 3306(a)(1)(b)-positive.
case description: alice has employed bob on vari-ous occasions during the year 2017: jan 24, feb 4,mar 3, mar 19, apr 2, may 9, oct 15, oct 25, nov8, nov 22, dec 1, dec 3.subsection to apply: §3306(a)(1)(b).
2756argument-value pairs:caly=“2017”}.
output 1.
{employer=“alice”,.
counts.
{workday=[“jan 24”, “feb 4”, “mar 3”, “mar 19”,“apr 2”, “may 9”, “oct 15”, “oct 25”, “nov 8”,“nov 22”, “dec 1”, “dec 3”], employee=“bob”,employment=“has employed”, “s13a”: [4, 5, 9,11, 13, 19, 41, 43, 45, 47], @truth=true}.
input 2: case §63(c)(5)-negative.
case description: in 2017, alice was paid $33200.
alice and bob have been married since feb 3rd,2017. bob earned $10 in 2017. alice and bob ﬁleseparate returns.
alice is not entitled to a deductionfor bob under section 151..subsection to apply: §63(c)(5).
argument-valuepairs:taxy=“2017”, bassd=500}.
{taxp=“bob”,.
output 2.
{@truth=false}.
input 3: tax case 5.case description: in 2017, alice’s gross incomewas $326332.
alice and bob have been marriedsince feb 3rd, 2017, and have had the same prin-cipal place of abode since 2015. alice was bornmarch 2nd, 1950 and bob was born march 3rd,1955. alice and bob ﬁle separately in 2017. bobhas no gross income that year.
alice takes the stan-dard deduction..pairs:.
{taxy=“2017”,.
subsection to apply: tax.
argument-valuetaxp=“alice”}.
output 3.
{tax=116066, @truth=true}.
b dataset statistics.
b.1 argument identiﬁcation.
012345678910111213141516total.
statisticsaveragestddevmedian.
sara random241621211112136417332002146.
333934321311741052210100194.
3.02.82.
4.03.63.counts.
012345678910total.
statisticsaveragestddevmedian.
sara random242218231814138231146.
334044301510135400161.
2.42.02.
3.12.4.
3.table 6: number of argument placeholders per subsec-tion.
“counts” reports the number of subsections (rightcolumns) containing a speciﬁc number of placeholders(left column).
“random” refers to 9 sections drawn atrandom from the tax code, and annotated..b.2 argument coreference.
in tables 7 and 8, we report statistics on the an-notations for the argument coreference task.
thenumbers in table 7 (resp.
8) were used to plot themiddle (resp.
bottom) histogram in figure 2(a)..table 6 reports statistics on the annotations forthe argument identiﬁcation task.
the numbers inthat table were used to plot the top histogram infigure 2(a)..table 7: number of arguments per subsection.
“counts” reports the number of subsections (rightcolumns) containing a speciﬁc number of arguments(left column).
“random” refers to 9 sections drawn atrandom from the tax code, and annotated..2757table 10: number of arguments-value pairs for the in-put to the argument instantiation task.
“counts” reportsthe number of arguments (right columns) mentioned aspeciﬁc number of times (left column).
“gold” refersto the manually annotated data, and “silver” to the dataproduced automatically through the prolog program..counts012345total.
statisticsaveragestddevmedian.
goldtest813732420120.train7241774152256.
2.10.72.
2.00.81.counts123456789total.
statisticsaveragestddevmedian.
goldtest78334320000120.train1319612781100256.
1.71.01.
1.50.81.silver.
1197548735629327514473275543.
2.30.72.silver.
4124817051871266561573242518275543.
1.81.11.all15372506572376.
2.00.72.all20912916101011.
376.
1.61.01.table 11: number of arguments-value pairs for the out-put to the argument instantiation task.
“counts” reportsthe number of arguments (right columns) mentioned aspeciﬁc number of times (left column).
“gold” refersto the manually annotated data, and “silver” to the dataproduced automatically through the prolog program..counts12345total.
statisticsaveragestddevmedian.
sara random360731661456.
39170660473.
1.20.51.
1.30.61.table 8: number of mentions per argument.
“counts”reports the number of arguments (right columns) men-tioned a speciﬁc number of times (left column).
“ran-dom” refers to 9 sections drawn at random from the taxcode, and annotated..b.3 structure identiﬁcation.
table 9 reports statistics on the annotations forthe structure extraction task.
these numbers forarguments differ from those in table 6, becauseany subsection is allowed to contain the argumentsof any subsections it refers to..counts.
0123456789101112total.
statisticsaveragestddevmedian.
arguments dependencies80422818823711002192.
9134060241314775---192.
3.02.63.
1.02.41.table 9: number of arguments and dependencies ofeach subsection, as represented in the structure annota-tions.
“counts” reports the number of arguments or de-pendencies (right columns) mentioned a speciﬁc num-ber of times (left column)..b.4 argument instantiation.
tables 10 and 11 show statistics for the annotationsfor the argument instantiation task.
in the gold data,we separate training and test data, to show that bothdistributions are close..2758