cosqa: 20,000+ web queries for code search and question answering.
junjie huang1∗, duyu tang2, linjun shou3, ming gong3,ke xu1, daxin jiang3, ming zhou2, nan duan21beihang university3microsoft stc asia2microsoft research asia1huangjunjie@buaa.edu.cn, kexu@nlsde.buaa.edu.cn2,3{dutang,lisho,migon,djiang,nanduan}@microsoft.com.
abstract.
finding codes given natural language query isbeneﬁcial to the productivity of software de-velopers.
future progress towards better se-mantic matching between query and code re-quires richer supervised training resources.
toremedy this, we introduce the cosqa dataset.
it includes 20,604 labels for pairs of naturallanguage queries and codes, each annotated byat least 3 human annotators.
we further intro-duce a contrastive learning method dubbed co-clr to enhance query-code matching, whichworks as a data augmenter to bring more arti-ﬁcially generated training instances.
we showthat evaluated on codexglue with the samecodebert model,training on cosqa im-proves the accuracy of code question answer-ing by 5.1%, and incorporating coclr bringsa further improvement of 10.5%.
1..figure 1: two examples in cosqa.
a pair of a webquery and a python function with documentation is an-notated with “1” or “0”, representing whether the codeanswers the query or not..1.introduction.
with the growing population of software develop-ers, natural language code search, which improvesthe productivity of the development process viaretrieving semantically relevant code given naturallanguage queries, is increasingly important in bothcommunities of software engineering and naturallanguage processing (allamanis et al., 2018; liuet al., 2020a).
the key challenge is how to effec-tively measure the semantic similarity between anatural language query and a code..there are recent attempts to utilize deep neu-ral networks (gu et al., 2018; wan et al., 2019;feng et al., 2020), which embed query and code asdense vectors to perform semantic matching in auniﬁed vector space.
however, these models are.
∗work done during internship at microsoft research asia.
1the cosqa data and leaderboard are availableat https://github.com/microsoft/codexglue/tree/main/text-code/nl-code-search-webquery.
the code is available athttps://github.com/jun-jie-huang/coclr.
mostly trained on pseudo datasets in which a natu-ral language query is either the documentation of afunction or a tedious question from stack overﬂow.
such pseudo queries do not reﬂect the distribu-tion of real user queries that are frequently issuedin search engines.
to the best of our knowledge,datasets that contain real user web queries includelv et al.
(2015), codesearchnet challenge (hu-sain et al., 2019), and codexglue 2 (lu et al.,2021).
these three datasets only have 34, 99, and1,046 queries, respectively, for model testing.
thearea lacks a dataset with a large amount of realuser queries to support the learning of statisticalmodels like deep neural networks for matching thesemantics between natural language web query andcode..to address the aforementioned problems, we in-troduce cosqa, a dataset with 20,604 pairs of webqueries and code for code search and questionanswering, each with a label indicating whether.
2https://github.com/microsoft/codexglue.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5690–5700august1–6,2021.©2021associationforcomputationallinguistics5690python check if path is absolute path or relative pathquery:code:defis_relative_url(url):"""simple method to determine if a urlis relative or absolute"""ifurl.startswith("#"):returnnoneifurl.find("://")>0orurl.startswith("//"):# either 'http(s)://...' or '//cdn...' and therefore absolute returnfalsereturntruelabel:1capitalize letters in string pythonquery:code:defsnake_to_camel(s:str)->str:""" convert string from snake case to camel case.
"""fragments =s.split('_’)returnfragments[0]+''.join(x.title()forx infragments[1:])label:0example 2: example 1: dataset.
size.
natural language.
code.
human-annotated ?.
codesearchnet (husain et al., 2019)gu et al.
(2018)miceli barone and sennrich (2017)staqc (manual) (yao et al., 2018)staqc (auto) (yao et al., 2018)conala (manual) (yin et al., 2018)conala (auto) (yin et al., 2018)so-ds (heyman and cutsem, 2020)nie et al.
(2016)li et al.
(2019)yan et al.
(2020).
functionfunctionfunction.
documentation2.3m18.2m documentation150.4k documentation8.5kstack overﬂow question code blockstack overﬂow question code block268kstatementsstack overﬂow question2.9k598.2k stack overﬂow questionstatements12.1kstack overﬂow question code block312.9k stack overﬂow question code block28752.stack overﬂow questionstack overﬂow question.
functionfunction.
lv et al.
(2015)codesearchnet (husain et al., 2019)codexglue webquerytest 2cosqa (ours).
web query34web query991kweb query20.6k web query.
functionfunctionfunctionfunction.
nononoyesnoyesnononoyesyes.
yesyesyesyes.
table 1: overview of existing datasets on code search and code question answering.
some datasets containingboth unlabelled data and labelled data are listed in separate lines..the code can answer the query or not.
the queriescome from the search logs of the microsoft bingsearch engine, and the code is a function fromgithub3.
to scale up the annotation process onsuch a professional task, we elaborately curate po-tential positive candidate pairs and perform largescale annotation where each pair is annotated by atleast three crowd-sourcing workers.
furthermore,to better leverage the cosqa dataset for query-code matching, we propose a code contrastive learn-ing method (coclr) to produce more artiﬁciallygenerated instances for training..we perform experiments on the task of query-code matching on two tasks: code question answer-ing and code search.
on code question answering,we ﬁnd that the performance of the same code-bert model improves 5.1% after training on thecosqa dataset, and further boosts 10.5% afterincorporating our coclr method.
moreover, ex-periments on code search also demonstrate similarresults..2 related work.
in this part, we describe existing datasets and meth-ods on code search and code question answering..2.1 datasets.
a number of open-sourced datasets with a largeamount of text-code pairs have been proposed forthe purposes of code search (husain et al., 2019;gu et al., 2018; nie et al., 2016) and code ques-tion answering (yao et al., 2018; yin et al., 2018;.
3we study on python in this work, and we plan to extend.
to more programming languages in the future..heyman and cutsem, 2020).
there are also high-quality but small scale testing sets curated for codesearch evaluation (li et al., 2019; yan et al., 2020;lv et al., 2015).
husain et al.
(2019), gu et al.
(2018) and miceli barone and sennrich (2017)collect large-scale unlabelled text-code pairs byleveraging human-leaved comments in code func-tions from github.
yao et al.
(2018) and yin et al.
(2018) automatically mine massive code answersfor stack overﬂow questions with a model trainedon a human-annotated dataset.
nie et al.
(2016)extract the stack overﬂow questions and answerswith most likes to form text-code pairs.
among alltext-code datasets, only those in lv et al.
(2015),codesearchnet challenge (husain et al., 2019) andcodexglue2 contain real user web queries, butthey only have 34, 99 and 1,046 queries for test-ing and do not support training data-driven models.
table 1 illustrates an overview of these datasets..2.2 code search models.
models for code search mainly can be divided intotwo categories: information retrieval based modelsand deep learning based models.
information re-trieval based models match keywords in the querywith code sequence (bajracharya et al., 2006; liuet al., 2020b).
keyword extension by query ex-pansion and reformulation is an effective way toenhance the performance (lv et al., 2015; lu et al.,2015; nie et al., 2016; rahman et al., 2019; rah-man, 2019).
deep learning based models encodequery and code into vectors and utilize vector simi-larities as the metric to retrieve code (sachdev et al.,2018; ye et al., 2016; gu et al., 2018; cambronero.
5691et al., 2019; yao et al., 2019; liu et al., 2019a;feng et al., 2020; zhao and sun, 2020).
there arealso ways to exploit code structures to learn betterrepresentations for code search (wan et al., 2019;haldar et al., 2020; guo et al., 2020)..3 cosqa dataset.
in this section, we introduce the construction ofthe cosqa dataset.
we study python in this work,and we plan to extend to more programming lan-guages in the future.
each instance in cosqa isa pair of natural language query and code, whichis annotated with “1” or “0” to indicate whetherthe code can answer the query.
we ﬁrst describehow to curate web queries, obtain code functions,and get candidate query-code pairs.
after that, wepresent the annotation guidelines and statistics..3.1 data collection.
query curation we use the search logs fromthe microsoft bing search engine as the source ofqueries.
queries without the keyword “python” areremoved.
based on our observation and previouswork (yao et al., 2018; yan et al., 2020), there areseven basic categories of code-related web queries,including: (1) code searching, (2) debugging, (3)conceptual queries, (4) tools usage, (5) program-ming knowledge, (6) vague queries and (7) others.
basically, queries in (2)-(7) categories are not likelyto be answered only by a code function, since theymay need abstract and general explanations in nat-ural language.
therefore, we only target the ﬁrstcategory of web queries that have code searchingintent, i.e., queries that can be answered by a pieceof code..to ﬁlter out queries without code searching in-tent, we manually design heuristic rules based onexact keyword matching.
for example, querieswith the word of beneﬁt or difference are likely toseek a conceptual comparison rather than a codefunction, so we remove all queries with such key-words.
based on the observations, we manually col-lect more than 100 keywords in total.
table 2 dis-plays a part of selected keywords used for removingunqualiﬁed queries and more details can be foundin appendix a. to evaluate the query ﬁltering al-gorithm, we construct a human-annotated testset.
we invite three experienced python programmersto label 250 randomly sampled web queries with abinary label of having/not having searching intent.
then we evaluate the accuracy of intent predictions.
categories.
some keywords.
debugging.
exception, index out of, ignore, stderr, .
.
..conceptualqueries.
vs, versus, difference, advantage, beneﬁt,drawback, how many, what if, why, .
.
..programmingknowledge.
tutorial, advice, argument, suggestion, state-ment, declaration, operator, .
.
..toolsusage.
others.
console, terminal, open python, studio, ide,ipython, jupyter, vscode, vim, .
.
..unicode, python command, “@”, “()”, .
.
..table 2: selected keywords for our heuristic rules toﬁlter out web queries without code search intent in ﬁvecategories.
vague queries are morphologically variable,so we ignore this category..given keyword-based rules and those given by hu-mans.
we ﬁnd the f1 score achieves 67.65, andthe accuracy is up to 82.40. this demonstrates theremarkable effectiveness of our rule-based queryﬁltering algorithm..code collection the selection of code formatis another important issue in constructing query-code matching dataset, which includes a statement(yin et al., 2018), a code snippet/block (yao et al.,2018), a function (husain et al., 2019), etc.
incosqa, we simplify the task and adopt a competepython function with paired documentation to bethe answer to the query for the following reasons.
first, it is complete and independent in functional-ity which may be more prone to answering a query.
second, it is syntactically correct and formally con-sistent which enables parsing syntax structures foradvanced query-code matching.
additionally, acomplete code function is often accompanied withdocumentation wrote by programmers to help un-derstand its functionality and usage, which is bene-ﬁcial for query-code matching (see section 6.4 formore details)..we take the codesearchnet corpus (husainet al., 2019) as the source for code functions, whichis a large-scale open-sourced code corpus allowingmodiﬁcation and redistribution.
the corpus con-tains 2.3 million functions with documentation and4.1 million functions without documentation frompublic github repositories spanning six program-ming languages (go, java, javascript, php, python,and ruby).
in cosqa, we only keep completepython functions with documentation and removethose with non-english documentation or specialtokens (e.g.
“(cid:104)img...(cid:105)” or “http : //”)..5692query.
code.
explanations.
(1) boolean functionto check if variable isa string python.
(2) python check ifargument is list.
(3).
python measuredistance between 2points.
(4) python measuredistance between 2points.
(5) read write in thesame ﬁle python.
(6) python get thevalue in the liststarting with the str.
(7) python check ifsomething is an array.
code can fully satisfy the demandof the query.
therefore the code isa correct answer..code meets the demand of check-ing list type, and the tuple and settypes, which exceeds query’s de-mand.
it is a correct answer..code computes euclidean dis-tance, which is one category ofvector distances.
so it is correct..code computes square distance,which is another category of vectordistances..query asks for reading and writing,but code only implements reading.
the code satisﬁes 50% of the de-mands and is not a correct answer..the query is looking for an ele-ment in the list that starts with aspeciﬁc str, but the code does nothave the function of starting withthe str, and it returns index insteadof value.
there are two unsatisﬁedareas, which is less than 50%..a small part of code is relevent tothe query but is can not answer..table 3: examples and explanations of query-code pairs for correct and incorrect answers..candidate query-code pairs obviously, it isnot possible to annotate all query-code pairs.
toimprove efﬁciency, we wipe off low-conﬁdence in-stances before annotation.
speciﬁcally, we employa codebert-based matching model (feng et al.,2020) to retrieve high-conﬁdence codes for everyquery.
the codebert encoder is ﬁne-tuned on148k automated-minded python stack overﬂowquestion-code pairs (staqc) (yao et al., 2018) withthe default parameters.
a cosine similarity score onthe pooled [cls] embeddings of query and codeis computed to measure the relatedness.
to guar-antee the quality of candidates, we automaticallyremove low-quality query-code pairs according tothe following evaluation metrics..• to ensure the code may answer the query, weonly keep the code with the highest similar-ity to the query and remove the pairs with asimilarity below 0.5..• to increase the code diversity and control the.
code frequency, we restrict the maximum oc-currence of each code to be 10..3.2 data annotation.
annotating such a domain-speciﬁc dataset is dif-ﬁcult since it requires the knowledge of python.
even experienced programmers do not necessarilyunderstand all code snippets.
to ensure the fea-sibility and control annotation quality, we designcomprehensive annotation guidelines and take atwo-step annotation procedure..annotation guidelines our annotation guide-line is developed through several pilots and fur-ther updated with hard cases as the annotation pro-gresses.
annotation participants are asked to makea two-step judgment for each instance: intent anno-tation and answer annotation..in the ﬁrst step of intent annotation, annotatorsare asked to judge whether the query has the intentto search for a code.
they will skip the secondstep if the query is without code search intent.
as.
5693defis_string(val):"""determines whether the passed value is a string,safe for 2/3.
"""try:basestringexceptnameerror:returnisinstance(val,str)returnisinstance(val,basestring)☺defis_listish(obj):""" check if something quacks like a list.
"""ifisinstance(obj,(list,tuple,set)):returntruereturnis_sequence(obj)☺defvector_distance(a,b):""" the euclidean distance between two vectors.
"""a =np.array(a)b =np.array(b)returnnp.linalg.norm(a -b)☺defdist_sq(self,other):""" distance squared to some other point.
"""dx =self.x-other.xdy=self.y-other.yreturndx**2+dy**2☺deffile_read(filename):""" read a file and close it.
returns the file source.
"""fobj=open(filename,'r’);source =fobj.read();fobj.close()returnsourcedefget_list_index(lst,index_or_name):""" return the index of an element in the list.
args: lst(list): the list.
index_or_name(int or str): value of the referenceelement, or directly its numeric index.returns: (int) index of the element in the list.
"""ifisinstance(index_or_name,six.integer_types):returnindex_or_namereturnlst.index(index_or_name)defis_number(obj):""" check if obj is number.
"""returnisinstance(obj,(int,float,np.int_,np.float_))shown in section 3.1, vague queries are hard tobe ﬁltered out by our heuristic intent ﬁltering algo-rithm.
therefore, it is necessary to take this step toremove such queries so that we can focus more onthe matching between query and code rather thanquery discrimination..in the second step of answer annotation, annota-tors are asked to judge whether the code can answerthe query.
they should label the instance with “1”if the code is a correct answer; otherwise, it is la-beled “0”.
in this step, judgment should be madeafter comprehensively considering the relevancebetween query with documentation, query withfunction header, and query with function body..during annotation, it is often the case that a codefunction can completely answer the query, whichmeans that the code can satisfy all the demands inthe query and it is a correct answer.
(case (1) in ta-ble 3.)
but more often, the code can not completelyanswer the query.
it may exceed, partially meetor even totally dissatisfy the demands of the query.
therefore we divide such situations into four cate-gories and give explanations and examples (table3) for each category:.
• if code can answer the query and even exceedthe demand of the query, it is a correct answer.
(case (2) in table 3.).
• if code can meet a certain category of thequery demands, it is also a correct answer.
(case (3) and case (4) in table 3.).
• if code satisﬁes no more than 50% of the querydemands, the code can not correctly answerthe query.
(case (5) and case (6) in table 3.).
• if a small part of the code is relevant to thequery, the code can not be a correct answer.
(case (7) in table 3.).
annotation we ask more than 100 participants,who all have a good grasp of programming knowl-edge, to judge the instances according to the anno-tation guideline.
participants are provided with thefull guidelines and allowed to discuss and searchon the internet during annotation.
when annotationis ﬁnished, each query-code pair has been anno-tated by at least three participants.
we removethe pairs whose inter-annotator agreement (iaa) ispoor, where krippendorff’s alpha coefﬁcient (krip-pendorff, 1980) is used to measure iaa.
we alsoremove pairs with no-code-search-intent queries..finally, 20,604 labels for pairs of web query andcode are retained, and their average krippendorff’salpha coefﬁcient is 0.63. table 4 shows the statis-tics of cosqa..querycode.
# of query20,6046,267.avg.
length6.6071.51.
# of tokens6,78428,254.table 4: statistics of our cosqa dataset..4 tasks.
based on our cosqa dataset, we explore two tasksto study the problem of query-code matching: codesearch and code question answering..the ﬁrst task is natural language code search,where we formulate it as a text retrieval problem.
given a query qi and a collection of codes c ={c1, .
.
.
, ch } as the input, the task is to ﬁnd themost possible code answer c∗.
the task is evaluatedby mean reciprocal rank (mrr)..the second task is code question answering,where we formulate it as a binary classiﬁcationproblem.
given a natural language query q anda code sequence c as the input, the task of codequestion answering predicts a label of “1” or “0”to indicate whether code c answers query q or not.
the task is evaluated by accuracy score..5 methodology.
in this section, we ﬁrst describe the model forquery-code matching and then present our codecontrastive learning method (coclr) to augmentmore training instances..5.1 siamese network with codebert.
the base model we use in this work is a siamesenetwork, which is a kind of neural network withtwo or more identical subnetworks that have thesame architecture and share the same parametersand weights (bromley et al., 1994).
by derivingﬁxed-sized embeddings and computing similari-ties, siamese network systems have proven effec-tive in modeling the relationship between two textsequences (conneau et al., 2017; yang et al., 2018;reimers and gurevych, 2019)..we use a pretrained codebert (feng et al.,2020) as the encoder to map any text sequence to ad-dimensional real-valued vectors.
codebert is abimodal model for natural language and program-ming language which enables high-quality text and.
5694figure 2: the frameworks of the siamese network with codebert (left) and our coclr method (right).
the blueline denotes the original training example.
the red lines and dashed lines denote the augmented examples within-batch augmentation and query-rewritten augmentation, respectively..code embeddings to be derived.
speciﬁcally, itshares exactly the same architecture as roberta(liu et al., 2019b), which is a bidirectional trans-former with 12 layers, 768 dimensional hiddenstates, and 12 attention heads, and is repretrainedby masked language modeling and replaced tokendetection objectives on codesearchnet corpus (hu-sain et al., 2019)..for each query qi and code ci, we concatenate a[cls] token in front of the sequence and a [sep ]token at the end.
then we feed the query and codesequences into the codebert encoder to obtaincontextualized embeddings, respectively.
here weuse the pooled output of [cls] token as the repre-sentations:.
qi = codebert(qi),.
ci = codebert(ci)..(1).
next we perform query-code matching througha multi-layer perceptron.
following chen et al.
(2017) and mou et al.
(2016), we concatenate thequery embedding qi and code embedding ci withthe element-wise difference qi − ci and element-(cid:74) ci, followed by a 1-layer feed-wise product qiforward neural network, to obtain a relation embed-ding:.
r(i,i) = tanh(w1 · [qi, ci, qi − ci, qi.
ci])..(2).
(cid:75).
we expect such an operation can help sharpen thecross information between query and code to cap-ture better matching relationships such as contra-diction..then we put the relation embedding r(i,i) into aﬁnal 1-layer perceptron classiﬁer with a sigmoidoutput layer: s(i,i) = sigmoid(w2 · r(i,i)).
scores(i,i) can be viewed as the similarity of query qiand code ci..to train the base siamese network, we use abinary cross entropy loss as the objective function:.
lb = −[yi · log s(i,i) + (1 − yi) log(1 − s(i,i))],.
(3).
where yi is the label of (qi, ci)..5.2 code contrastive learning.
now we incorporate code contrastive learning intothe siamese network with codebert.
contrastivelearning aims to learn representations by enforcingsimilar objects to be closer while keeping dissimilarobjects further apart.
it is often accompanied withleveraging task-speciﬁc inductive bias to augmentsimilar and dissimilar examples.
in this work, givenan example of query and code (qi, ci), we deﬁneour contrastive learning task on example itself, in-batch augmented examples (qi, cj), and augmentedexample with rewritten query (q(cid:48)i, ci).
hence, theoverall training objective can be formulated as:.
l = lb + lib + lqr..(4).
in-batch augmentation (iba) a straightfor-ward augmentation method is to use in-batch data,where a query and a randomly sampled code areconsidered as dissimilar and forced away by themodels.
speciﬁcally, we randomly sample n exam-ples {(q1, c1), (q2, c2), .
.
.
, (qn, cn)} from a mini-batch.
for (qi, ci), we pair query qi with the othern − 1 codes within the mini-batch and treat then − 1 pairs as dissimilar.
let s(i,j) denote the sim-ilarity of query qi and code cj, the loss function ofthe example with iba is deﬁned as:.
lib = −.
1n − 1.n(cid:88).
j = 1j (cid:54)= i.log(1 − s(i,j)),.
(5).
query-rewritten augmentation (qra) thein-batch augmentation only creates dissimilar pairsfrom the mini-batch, which ignores to augment sim-ilar pairs for learning positive relations.
to remedythis, we propose to augment positive examples byrewriting queries.
inspired by the feature that web.
5695codebertcodebert𝑞𝑢𝑒𝑟𝑦𝑛𝑐𝑜𝑑𝑒𝑛codebert𝑞𝑢𝑒𝑟𝑦𝑛′rewritecodebertcodebert𝑞𝑢𝑒𝑟𝑦1𝑐𝑜𝑑𝑒1codebert𝑞𝑢𝑒𝑟𝑦1′rewrite………codebertcodebert𝑞𝑢𝑒𝑟𝑦𝑖𝑐𝑜𝑑𝑒𝑖queries are often brief and not necessarily grammat-ically correct, we assume that the rewritten querywith minor modiﬁcations shares the same seman-tics as the original one.
therefore, an augmentedpair with a rewritten query from a positive pair canalso be treated as positive..speciﬁcally, given a pair of query qi and codeci with yi = 1, we rewrite qi into q(cid:48)i in one of thethree ways: randomly deleting a word, randomlyswitching the position of two words, and randomlycopying a word.
as shown in section 6.3, switch-ing position best helps increase the performance.
for any augmented positive examples, we alsoapply iba on them.
therefore the loss function forthe example with qra is:.
evaluation metric we use accuracy as the evalu-ation metric on code question answering and meanreciprocal rank (mrr) on code search..implementation details we initialize coclrwith microsoft/codebert-base4 repretrained oncodesearchnet python corpus (husain et al.,2019).
we use the adamw optimizer (loshchilovand hutter, 2019) and set the batch size to 32 onthe two tasks.
on code question answering, weset the learning rate to 1e-5, warm-up rate to 0.1.on code search, we set the learning rate to 1e-6.
all hyper-parameters are tuned to the best on thevalidation set.
all experiments are performed onan nvidia tesla v100 gpu with 16gb memory..lqr = l(cid:48).
b + l(cid:48).
ib,.
(6).
6.2 model comparisons.
where l(cid:48)b and l(cid:48)5 by only change qi to q(cid:48)i..ib can be obtained by eq.
3 and eq..6 experiments.
we experiment on two tasks, including code ques-tion answering and natural language code search.
we report model comparisons and give detailedanalyses from different perspectives..6.1 experiment settings.
we train the models on the cosqa dataset and eval-uate them on two tasks: code question answeringand code search..on code question answering, we randomly splitcosqa into 20,000 training and 604 validationexamples.
as for the test set, we directly use thewebquerytest in codexglue benchmark, whichis a testing set of python code question answer-ing with 1,046 query-code pairs and their expertannotations..on code search, we randomly divide the cosqainto training, validation, and test sets in the numberof 19604:500:500, and restrict the instances forvalidation and testing are all positive.
we ﬁx a codedatabase with 6,267 different codes in cosqa..baseline methods cosqa is a new dataset, andthere are no previous models designed speciﬁcallyfor it.
hence, we simply choose roberta-base(liu et al., 2019b) and codebert (feng et al.,2020) as the baseline methods.
the baseline meth-ods are trained on codesearchnet python corpuswith balanced positive examples.
negative sam-ples consist of a balanced number of instances withrandomly replaced code..table 5 shows the experimental results on the tasksof code question answering and code search.
wecan observe that:.
(1) by leveraging the cosqa dataset, siamesenetwork with codebert achieves overall perfor-mance enhancement on two tasks, especially forcodexglue webquerytest, which is an openchallenge but without direct training data.
the re-sult demonstrates the high-quality of cosqa andits potential to be the training set of webquerytest.
(2) by integrating the code contrastive learningmethod, siamese network with codebert furtherachieves signiﬁcant performance gain on both tasks.
especially on the task of webquerytest, coclrachieves the new state-of-the-art result by increas-ing 15.6%, which shows the effectiveness of ourproposed approach..6.3 analysis: effects of coclr.
to investigate the effects of coclr in query-codematching, we perform ablation study to analyzethe major components in our contrastive loss thatare of importance to help achieve good perfor-mance.
we conduct experiments on the cosqacode search task, using the following settings: (i)ﬁne-tuning with vanilla binary cross-entropy lossonly, (ii) ﬁne-tuning with additional in-batch aug-mentation (iba) loss, (iii) ﬁne-tuning with addi-tional query-rewritten augmentation (qra) loss,(vi) ﬁne-tuning with both additional iba and qraloss.
and for qra loss, we also test the threerewriting methods when applied individually.
theresults are listed in table 6. we can ﬁnd that:.
4https://github.com/microsoft/codebert.
5696model.
data.
code question answering code search.
roberta2csncodebert2csncodebertcsn + cosqacodebert + coclr csn + cosqa.
40.3447.8052.8763.38.
0.1851.2954.4164.66.table 5: evaluation on code question answering and code search.
csn denotes codesearchnet python corpus.
byincorporating coclr method, siamese network with codebert outperforms the existing baseline approaches..augmentations.
no augmentations.
+ query-rewritten (delete)+ query-rewritten (copy)+ query-rewritten (switch)+ in-batch+ in-batch + query-rewritten (delete)+ in-batch + query-rewritten (copy)+ in-batch + query-rewritten (switch).
mrr.
54.4155.2454.8255.6663.5163.4163.9764.66.code component.
complete codew/o headerw/o bodyw/o documentationw/o header & bodyw/o header & documentationw/o body & documentation.
mrr.
64.6662.0159.1158.5452.8943.3542.71.table 6: performance of codebert with different aug-mentations in coclr on code search..table 7: performance of coclr-incorporated code-bert trained and tested with different code compo-nents on code search..(1) both incorporating iba and qra individ-ually or together improve models’ performance.
this indicates the advantage of applying code con-trastive learning for code search..(2) no matter integrating iba or not, the modelwith qra by switching method performs betterthan models with the other two methods.
we at-tribute the phenomenon to the fact that web queriesdo not necessarily have accurate grammar.
soswitching the positions of two words in the querybetter maximizes the agreement between the posi-tive example and the pseudo positive example thanthe other two augmentations, which augments bet-ter examples to learn representations..(3) comparing the two augmentations, addingiba achieves more performance gain than qra(1.25% versus 9.10%).
as the numbers of exam-ples with qra and examples with iba are notequal under two settings, we further evaluate themodel with only one more example with iba.
themrr is 55.52%, which is comparable to the per-formance of adding one more example with qra.
this suggests that there may be no difference be-tween adding examples with iba or examples withqra.
instead, the number of high-quality exam-ples is important for training.
similar ﬁndings arealso reported in sun et al.
(2020), and a theoreticalanalysis is provided in arora et al.
(2019)..6.4 analysis: effects of code components.
to explore the effects of different components ofcode in query-code matching, we evaluate coclron code search and process the codebase by thefollowing operations: (i) removing the functionheader, (ii) removing the natural language docu-mentation, (iii) removing the code statements in thefunction body.
we also combine two of the aboveoperations to see the performance.
from the resultsexhibited in table 7, we can ﬁnd that: by removingcode component, the result of removing documen-tation drops more than those of removing headerand removing function body.
this demonstrates theimportance of natural language documentation incode search.
since documentation shares the samemodality with the query and brieﬂy describes thefunctionality of the code, it may be more semanti-cally related to the query.
besides, it also revealsthe importance of using web queries rather thantreating documentation as queries in code searchdatasets, which liberates models from the matchingbetween documentation with code to the matchingbetween query with documentation and code..7 conclusion.
in this paper, we focus on the matching prob-lem of the web query and code.
we develop alarge-scale human-annotated query-code matchingdataset cosqa, which contains 20,604 pairs ofreal-world web queries and python functions withdocumentation.
we demonstrate that cosqa is an.
5697ideal dataset for code question answering and codesearch.
we also propose a novel code contrastivelearning method, named coclr, to incorporate ar-tiﬁcially generated instances into training.
we ﬁndthat model with coclr outperforms the baselinemodels on code search and code question answer-ing tasks.
we perform detailed analysis to investi-gate the effects of coclr components and codecomponents in query-code matching.
we believeour annotated cosqa dataset will be useful forother tasks that involve aligned text and code, suchas code summarization and code synthesis..acknowledgement.
we thank all anonymous reviewers for their usefulcomments.
we also thank zenan xu, daya guo,shuai lu, wanjun zhong and siyuan wang forvaluable discussions and feedback during the paperwriting process..references.
miltiadis allamanis, earl t. barr, premkumar devanbu,and charles sutton.
2018. a survey of machinelearning for big code and naturalness.
acm com-puting survey..s. arora, hrishikesh khandeparkar, m. khodak,orestis plevrakis, and nikunj saunshi.
2019. a the-oretical analysis of contrastive unsupervised repre-sentation learning.
in icml..s. bajracharya, trung chi ngo, erik linstead, yi-meng dou, paul rigor, p. baldi, and c. lopes.
2006.sourcerer: a search engine for open source code sup-porting structure-based search.
in oopsla ’06..jane bromley, isabelle guyon, yann lecun, eduards¨ackinger, and roopak shah.
1994. signature veri-ﬁcation using a ”siamese” time delay neural network.
in advances in neural information processing sys-tems..jos´e cambronero, hongyu li, s. kim, k. sen, ands. chandra.
2019. when deep learning met codesearch.
proceedings of the 2019 27th acm jointmeeting on european software engineering confer-ence and symposium on the foundations of softwareengineering..qian chen, xiao-dan zhu, zhenhua ling, si wei, huijiang, and d. inkpen.
2017. enhanced lstm for natu-ral language inference.
in acl..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in emnlp..zhangyin feng, daya guo, duyu tang, n. duan,x. feng, ming gong, linjun shou, b. qin, tingliu, daxin jiang, and m. zhou.
2020. codebert: apre-trained model for programming and natural lan-guages.
in findings of the association for computa-tional linguistics: emnlp 2020..xiaodong gu, hongyu zhang, and sunghun kim.
2018..deep code search.
in proceedings of icse..daya guo, shuo ren, shuai lu, zhangyin feng, duyutang, shujie liu, l. zhou, n. duan, jian yin, daxinjiang, and m. zhou.
2020. graphcodebert: pre-training code representations with data ﬂow.
iniclr..rajarshi haldar, lingfei wu, jinjun xiong, and juliahockenmaier.
2020. a multi-perspective architec-in proceedings ofture for semantic code search.
acl..geert heyman and tom van cutsem.
2020. neu-ral code search revisited: enhancing code snippetretrieval through natural language intent.
arxiv,abs/2008.12193..hamel husain, ho-hsiang wu, tiferet gazit, miltiadisallamanis, and marc brockschmidt.
2019. code-searchnet challenge: evaluating the state of seman-tic code search.
arxiv preprint arxiv:1909.09436..k. krippendorff.
1980. krippendorff, klaus, contentanalysis: an introduction to its methodology .
bev-erly hills, ca: sage, 1980..hongyu li, s. kim, and s. chandra.
2019. neural codesearch evaluation dataset.
arxiv, abs/1908.09804..c. liu, xin xia, david lo, cuiyun gao, xiaohu yang,and j. grundy.
2020a.
opportunities and challengesin code search tools.
arxiv, abs/2011.02297..chao liu, xin xia, david lo, zhiwei liu, a. has-san, and shanping li.
2020b.
simplifying deep-arxiv,learning-based model for code search.
abs/2005.14373..jason liu, seohyun kim, vijayaraghavan murali,swarat chaudhuri, and satish chandra.
2019a.
neu-ral query expansion for code search.
in proceedingsof the 3rd acm sigplan international workshopon machine learning and programming languages.
acm..y. liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, m. lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach.
arxiv, abs/1907.11692..i. loshchilov and f. hutter.
2019. decoupled weight.
decay regularization.
in iclr..meili lu, xiaobing sun, s. wang, d. lo, and yucongduan.
2015. query expansion via wordnet for ef-fective code search.
2015 ieee 22nd internationalconference on software analysis, evolution, andreengineering (saner), pages 545–549..5698shuhan yan, hang yu, yuting chen, beijun shen, andlingxiao jiang.
2020. are the code snippets whatwe are searching for?
a benchmark and an empiricalstudy on code search with natural-language queries.
in proceedings of saner..yinfei yang, steve yuan, daniel cer, sheng-yi kong,noah constant, petr pilar, heming ge, yun-hsuansung, brian strope, and ray kurzweil.
2018. learn-ing semantic textual similarity from conversations.
in proceedings of the third workshop on represen-tation learning for nlp..ziyu yao, jayavardhan reddy peddamail, and huansun.
2019. coacor: code annotation for code re-trieval with reinforcement learning.
in proceedingsof www..ziyu yao, daniel s weld, wei-peng chen, andhuan sun.
2018. staqc: a systematically minedquestion-code dataset from stack overﬂow.
in pro-ceedings of www..xin ye, hui shen, xiao ma, razvan c. bunescu, andchang liu.
2016. from word embeddings to docu-ment similarities for improved information retrieval2016 ieee/acm 38thin software engineering.
international conference on software engineering(icse)..pengcheng yin, bowen deng, edgar chen, bogdanvasilescu, and graham neubig.
2018. learning tomine aligned code and natural language pairs fromstack overﬂow.
in international conference on min-ing software repositories..jie zhao and huan sun.
2020. adversarial training forcode retrieval with question-description relevancein findings of the association forregularization.
computational linguistics: emnlp 2020..a heuristics for query filtering.
in this section, we introduce our heuristic rules toﬁlter potential queries without code search intent.
basically, the rules are created from keyword tem-plates and we follow the six categories of querieswithout code search intent to derive the keywords.
note that vague queries are morphologically vari-able so we ignore this categories.
the keywordsare shown in table 8..shuai lu, daya guo, shuo ren, junjie huang, alexeysvyatkovskiy, ambrosio blanco, colin b. clement,dawn drain, daxin jiang, duyu tang, ge li, li-dong zhou, linjun shou, long zhou, michele tu-fano, ming gong, ming zhou, nan duan, neel sun-daresan, shao kun deng, shengyu fu, and shujieliu.
2021. codexglue: a machine learning bench-mark dataset for code understanding and generation.
corr, abs/2102.04664..fei lv, h. zhang, jian-guang lou, s. wang, d. zhang,and jianjun zhao.
2015. codehow: effective codesearch based on api understanding and extendedboolean model (e).
2015 30th ieee/acm interna-tional conference on automated software engineer-ing (ase), pages 260–270..antonio valerio miceli barone and rico sennrich.
2017. a parallel corpus of python functions anddocumentation strings for automated code documen-tation and code generation.
in proceedings of ijc-nlp..lili mou, rui men, ge li, yan xu, lu zhang, rui yan,and zhi jin.
2016. natural language inference bytree-based convolution and heuristic matching.
inacl..liming nie, he jiang, zhilei ren, zeyi sun, and xi-aochen li.
2016. query expansion based on crowdknowledge for code search.
ieee transactions onservices computing, 9:771–783..m. m. rahman.
2019. supporting code search withcontext-aware, analytics-driven, effective query re-2019 ieee/acm 41st internationalformulation.
conference on software engineering: companionproceedings (icse-companion), pages 226–229..m. m. rahman, c. roy, and d. lo.
2019. automaticquery reformulation for code search using crowd-sourced knowledge.
empirical software engineer-ing, pages 1–56..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in emnlp/ijcnlp..saksham sachdev, h. li, sifei luan, s. kim, k. sen,and s. chandra.
2018. retrieval on source code:the 2nda neural code search.
acm sigplan international workshop on machinelearning and programming languages..proceedings of.
s. sun, zhe gan, y. cheng, yuwei fang, shuohangwang, and jing jing liu.
2020. contrastive distil-lation on intermediate representations for languagemodel compression.
in emnlp..yao wan, jingdong shu, yulei sui, guandong xu,zhou zhao, jian wu, and philip s. yu.
2019.multi-modal attention network learning for seman-tic source code retrieval.
2019 34th ieee/acm in-ternational conference on automated software en-gineering (ase), pages 13–25..5699categories.
keywords.
debugging.
exception, index out of, ignore, omit, stderr,try .
.
.
except, debug, no such ﬁle or direc-tory, warning,.
conceptual.
vs, versus, difference, advantage, beneﬁt,drawback, interpret, understand, cannot,can’t, couldn’t, could not, how many, howmuch, too much, too many, more, less,what if, what happens, what is, what are,when, where, which, why, reason, how do.
.
.
work, how .
.
.
works, how does .
.
.
work,need, require, wait, turn .
.
.
on/off, turning.
.
.
on/off,.
programmingknowledge.
tutorial, advice, course, proposal, discuss,suggestion, parameter, argument, statement,class, import, inherit, operator, override,decorator, descriptor, declare, declaration.
toolsusage.
console, terminal, open python, studio, ide,ipython, jupyter, notepad, notebook, vim,pycharm, vscode, eclipse, sublime, emacs,utm, komodo, pyscripter, eric, c#, accesscontrol, pip, install, library, module, launch,version, ip address, ipv, get .
.
.
ip, check.
.
.
ip, valid .
.
.
ip,.
others.
unicode, python command, “()”, “.”, “ ”,“:”, “@”, “=”, “>”, “<”, “-”.
table 8: keywords of queries without code search in-tent in ﬁve categories..5700