ultra-fine entity typing with weak supervision from a maskedlanguage model.
hongliang dai, yangqiu songdepartment of cse, hkust{hdai,yqsong}@cse.ust.hk.
haixun wanginstacarthaixun.wang@instacart.com.
abstract.
recently,there is an effort to extend ﬁne-grained entity typing by using a richertypes, and labelingand ultra-ﬁne set ofnoun phrases including pronouns and nomi-nal nouns instead of just named entity men-tions.
a key challenge for this ultra-ﬁne en-tity typing task is that human annotated dataare extremely scarce, and the annotation abil-ity of existing distant or weak supervisionapproaches is very limited.
to remedy thisproblem, in this paper, we propose to obtaintraining data for ultra-ﬁne entity typing by us-ing a bert masked language model (mlm).
given a mention in a sentence, our approachconstructs an input for the bert mlm so thatit predicts context dependent hypernyms of themention, which can be used as type labels.
ex-perimental results demonstrate that, with thehelp of these automatically generated labels,the performance of an ultra-ﬁne entity typingmodel can be improved substantially.
we alsoshow that our approach can be applied to im-prove traditional ﬁne-grained entity typing af-ter performing simple type mapping..1.introduction.
fine-grained entity typing (ling and weld, 2012)has been long studied in the natural language pro-cessing community as the extracted type informa-tion is useful for downstream tasks such as entitylinking (ling et al., 2015; onoe and durrett, 2020),relation extraction (koch et al., 2014), coreferenceresolution (onoe and durrett, 2020), etc.
recently,ultra-ﬁne entity typing (choi et al., 2018) extendsthe effort to using a richer set of types (e.g., per-son, actor, company, victim) to label noun phrasesincluding not only named entity mentions, but alsopronouns and nominal nouns.
this task directlyuses type words or phrases as tags.
its tag set cancontain more than 10,000 types.
a challenge is thatwith the large type set, it is extremely difﬁcult and.
time-consuming for humans to annotate samples.
as a result, most existing works use weak labelsthat are automatically generated (ling and weld,2012; choi et al., 2018; lee et al., 2020)..there are two main approaches to obtainingweakly labeled training examples.
one approachis to ﬁnd the wikipedia pages that correspond toentity mentions, which can be done by using hyper-links to wikipedia or applying entity linking.
thenthe entity types can be obtained from knowledgebases.
the other approach is to directly use thehead words of nominal mentions as ultra-ﬁne typelabels.
for example, if a nominal mention is “afamous actor,” then the head word “actor” can beused as its type label..several problems exist when using these weaklabels for the ultra-ﬁne typing task.
first, in thedataset created by choi et al.
(2018), on averagethere are fewer than two labels (types) for eachsample annotated through either entity linking orhead word supervision.
on the other hand, a hu-man annotated sample has on average 5.4 labels.
as a result, models trained from the automaticallyobtained labels have a low recall.
second, neitherof the above approaches can create a large numberof training samples for pronoun mentions.
third, itis difﬁcult to obtain types that are highly dependenton the context.
for example, in “i met the moviestar leonardo dicaprio on the plane to l.a.,” thetype passenger is correct for “leonardo dicaprio.”however, this type cannot be obtained by linkingto knowledge bases..in this paper, to alleviate the problems above, wepropose an approach that combines hypernym ex-traction patterns (hearst, 1992; seitner et al., 2016)with a masked language model (mlm), such asbert (devlin et al., 2019), to generate weak la-bels for ultra-ﬁne entity typing.
given a sentencethat contains a mention, our approach adds a shortpiece of text that contains a “[mask]” token into it.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1790–1799august1–6,2021.©2021associationforcomputationallinguistics1790inputin late 2015, [mask] such as leonardo dicaprio starred in therevenant.
at some clinics, they and some other [mask] are told the doctorsdon’t know how to deal with aids, and to go someplace else.
finkelstein says he expects the company to “beneﬁt from some ofthe disruption faced by our competitors and any other [mask] .”.
top words for [mask]actors, stars, actor, directors,ﬁlmmakerspatients, people, doctors, kids,childrencompany, business, companies,group, investors.
table 1: examples of constructed bert mlm inputs for obtaining weak entity typing labels.
entity mentionsare in bold and underlined.
the texts highlighted with blue background are not in the original sentences.
they areinserted to create inputs for bert.
the right column lists the ﬁve most probable words predicted by a pretrainedbert-base-cased mlm..to construct an input to bert.
then, the pretrainedmlm will predict the hypernyms of the mentionas the most probable words for “[mask].” thesewords can then be used as type labels.
for example,consider the ﬁrst example in table 1. the origi-nal sentence is “in late 2015, leonardo dicapriostarred in the revenant.” we construct an inputfor the bert mlm by inserting “[mask] such as”before the mention “leonardo dicaprio.” with thisinput, the pretrained bert mlm predicts “actors,”“stars,” “actor,” “directors,” and “ﬁlmmakers” asthe ﬁve most probable words for “[mask].” mostof them are correct types for the mention after sin-gularization.
this approach can generate labelsfor different kinds of mentions, including namedentity mentions, pronoun mentions, and nominalmentions.
another advantage is that it can producelabels that needs to be inferred from the context.
this allows us to generate more context-dependentlabels for each mention, such as passenger, patient,etc..then, we propose a method to select from theresults obtained through different hypernym extrac-tion patterns to improve the quality of the weak la-bels.
we also use a weighted loss function to makebetter use of the generated labels for model train-ing.
finally, we adopt a self-training step to furtherimprove the performance of the model.
we evalu-ate our approach with the dataset created by choiet al.
(2018), which to the best of our knowledge, isthe only english ultra-ﬁne entity typing dataset cur-rently available.
on this dataset, we achieve morethan 4% absolute f1 improvement over the previ-ously reported best result.
additionally, we alsoapply our approach to a traditional ﬁne-grained en-tity typing dataset: ontonotes (gillick et al., 2014),where it also yields better performance than thestate of the art..our contributions are summarized as follows..• we propose a new way to generate weak labels.
for ultra-ﬁne entity typing..• we propose an approach to make use of thenewly obtained weak labels to improve entitytyping results..• we conduct experiments on both an ultra-ﬁneentity typing dataset and a traditional ﬁne-grained entity typing dataset to verify the ef-fectiveness of our method..our code is available at https://github.com/.
hkust-knowcomp/mlmet..2 related work.
the ultra-ﬁne entity typing task proposed by choiet al.
(2018) uses a large, open type vocabulary toachieve better type coverage than the traditionalﬁne-grained entity typing task (ling and weld,2012) that uses manually designed entity type on-tologies.
there are only limited studies on thisnewly proposed task: a neural model introducedby (onoe and durrett, 2019) ﬁlters samples thatare too noisy to be used and relabels the remainingsamples to get cleaner labels.
a graph propaga-tion layer is introduced by (xiong et al., 2019) toimpose a label-relational bias on entity typing mod-els, so as to implicitly capture type dependencies.
onoe et al.
(2021) use box embeddings to capturelatent type hierarchies.
there is also some work onthe applications of ultra-ﬁne entity typing: onoeand durrett (2020) apply ultra-ﬁne entity typingto learn entity representations for two downstreamtasks: coreference arc prediction and named entitydisambiguation..the traditional ﬁne-grained entity typing task(ling and weld, 2012; yosef et al., 2012) is closely.
1791related to ultra-ﬁne entity typing.
automatic anno-tation (ling and weld, 2012; gillick et al., 2014;dai et al., 2020) is also commonly used in thestudies of this task to produce large size trainingdata.
many different approaches have been pro-posed to improve ﬁne-grained entity typing per-formance.
for example, denoising the automati-cally generated labels (ren et al., 2016), takingadvantage of the entity type hierarchies or typeinter-dependencies (chen et al., 2020; murty et al.,2018; lin and ji, 2019), exploiting external re-sources such as the information of entities providedin knowledge bases (jin et al., 2019; dai et al.,2019; xin et al., 2018), etc..our work is also related to recent studies (petroniet al., 2019; jiang et al., 2020; zhang et al., 2020)that probe pretrained language models to obtainknowledge or results for target tasks.
differentfrom them, we use the predictions produced bybert as intermediate results that are regarded asweak supervision to train better models.
(zhanget al., 2020) also uses hearst patterns to probemasked language models.
however, they targetat the entity set expansion task..3 methodology.
our methodology consists of two main steps.
first,we obtain weak ultra-ﬁne entity typing labels froma bert masked language model.
second, we usethe generated labels in model training to learn bet-ter ultra-ﬁne entity typing models..3.1 labels from bert mlm.
given a sentence and a mention of interest in thesentence, our goal is to derive the hypernym or thetype of the mention using a bert mlm.
to do this,we insert into the sentence a few tokens to createan artiﬁcial hearst pattern (hearst, 1992).
one ofthe inserted tokens is a special “[mask]” token,which serves as the placeholder of the hypernymof the mention.
as the bert mlm predicts the“[mask]” token, we derive the hypernyms of themention..consider the ﬁrst sentence in table 1 as an ex-ample: “in late 2015, leonardo dicaprio starredin the revenant.” to ﬁnd the hypernym or thetype of “leonardo dicaprio”, we insert three to-kens to create a “such as” pattern: “in late 2015,[mask] such as leonardo dicaprio starred inthe revenant.” applying the bert mlm on thesentence, we derive hypernyms such as “actors,”.
patternf1m and any other h25.3m and some other h 24.8h such as m20.7such h as m18.1h including m17.4h especially m11.5.table 2: hypernym extraction patterns.
m denotesthe hyponym; h denotes the hypernym.
the f1 scoreis evaluated with the development set of the ultra-ﬁnedataset (choi et al., 2018) for the labels generated withthe corresponding pattern..“stars,” “directors,” “ﬁlmmakers.” table 1 shows afew more examples..we consider the 63 hearst-like patterns (hearst,1992) presented in (seitner et al., 2016) that expressa hypernym-hypnonym relationship between twoterms.
table 2 lists some of the patterns, whereinh and m denote a hypernym and a hyponym, re-spectively.
for example, “m and some other h”can be used to match “microsoft and some othercompanies.”.
the general procedure to use these patterns tocreate input samples for bert mlm and obtainlabels from its predictions is as follows.
we ﬁrstregard the mention as m .
then, we insert the restof the pattern either before or after the mention,and we replace h with the special “[mask]” to-ken.
after applying the bert mlm on sentenceswith artiﬁcial hearst patterns, we derive top k typelabels from the prediction for “[mask].” to drivethese k labels, we ﬁrst sigularize the most prob-able words that are plural.
then, remove thosethat are not in the type vocabulary of the dataset.
finally, use the most probable k different wordsas k labels.
for example, if we want to obtain 3labels, and the most probable words are “people,”“actors,” “celebrities,” “famous,” “actor,” etc.
thenthe 3 labels should be person, actor, celebrity.
be-cause “actor” is the singluar form of “actors,” and“famous” is not in the type vocabulary..we show the performance of our method forobtaining 10 type labels for each mention withdifferent patterns in table 2. a pre-trained bert-base-cased mlm is used to obtain the results1..for nominal mentions, directly applying the pat-terns that starts with “m ” with the above procedure.
1we use the pretrained model provided in the transform-ers library.
we also tried using bert-large and robertamodels.
however, they do not yield better performance..1792may sometimes be problematic.
for example, con-sider the noun phrase “the factory in thailand” asa mention.
if we use the “m and some other h”pattern and insert “and other [mask]” after themention, the bert mlm will predict the typecountry for thailand instead of for the entire men-tion.
to avoid such errors, while applying patternsthat starts with “m ” for nominal mentions, we re-gard the head word of the mention as m instead.
a more subtle and challenging problem is thatthe quality of the type labels derived from differentpatterns for different mentions can be very different.
for example, for the mention “he” in sentence “hehas won some very tough elections and he’s gover-nor of the largest state,” the pattern “h such as m ”leads to person, election, time, thing, leader as thetop ﬁve types.
but using the pattern “m and anyother h,” we get candidate, politician, man, per-son, governor.
on the other hand, for mention “theal merreikh stadium” in “it was only chaouchi’sthird cap during that unforgettable night in the almerreikh stadium,” the results of using “h such asm ” (the top ﬁve types are venue, place, facility, lo-cation, area) is better than using “m and any otherh” (the top ﬁve types are venue, stadium, game,match, time)..to address the above problem, we do not use asame pattern for all the mentions.
instead, for eachmention, we try to select the best pattern to applyfrom a list of patterns.
this is achieved by usinga baseline ultra-ﬁne entity typing model, bert-ultra-pre, which is trained beforehand without us-ing labels generated with our bert mlm basedapproach.
details of bert-ultra-pre can be foundin section 5.2. denote the pattern list as l. witheach pattern in l, we can apply it on the givenmention to derive a set of labels from the bertmlm.
then, we ﬁnd the set of labels that have themost overlap with the labels predicted by bert-ultra-pre.
finally, the given mention is annotatedwith this set of labels..it is not necessary to use all the patterns in (seit-ner et al., 2016).
to construct l, the list of patternsused for annotation, we perform the following pro-cedure..step 1: initialize l to contain the best performingpattern (i.e., “m and any other h”) only.
step 2: from all the patterns not in l, ﬁnd the onethat may bring the greatest improvement inf1 score if it is added to l..step 3: add the pattern found in step 2 to the l.if the improvement brought by it is largerthan a threshold..step 4: repeat steps 2-3 until no patterns can be.
added..discussion on type coverage since we onlyuse one [mask] token to generate labels, themodel cannot produce multi-word types (e.g., foot-ball player) or single word types that are notpresent in the bert mlm vocabulary.
the bertmlm vocabulary covers about 92% of the labels inthe human annotated dataset constructed by choiet al.
(2018).
type coverage is a known issue withweak supervision, and is tolerable if the generatedlabels can be used to achieve our ﬁnal goal: improv-ing the performance of the ultra-ﬁne entity typingmodel..3.2 training data.
our approach generates type labels for all threetypes of mentions: named entity mentions, pro-noun mentions, and nominal mentions.
for namedentity mentions and nominal mentions, existing au-tomatic annotation approaches can already providesome labels for them by using the entity types inknowledge bases or using the head words as types(ling and weld, 2012; choi et al., 2018).
thus, wecombine these labels with the labels generated byus.
for pronoun mentions, no other labels are used.
besides the automatically annotated samples, wecan also use a small amount of human annotatedsamples provided by the dataset for model training..3.3 model training.
our ultra-ﬁne entity typing model follows thebert-based model in (onoe and durrett, 2019).
given a sentence that contains an entity mention,we form the sequence “[cls] sentence [sep] men-tion string [sep]” as the input to bert.
then, de-noting the ﬁnal hidden vector of the “[cls]” tokenas u, we add a linear classiﬁcation layer on top ofu to model the probability of each type:.
p = σ(w u),.
(1).
where σ is the sigmoid function, w is a trainableweight matrix.
p ∈ rd, where d is the number oftypes used by the dataset.
we assign a type t tothe mention if pt, its corresponding element in p,is larger than 0.5. if no such types are found, weassign the one with the largest predicted probabilityto the mention..1793to make use of the automatically labeled sam-ples, some existing approaches mix them with highquality human annotated samples while trainingmodels (choi et al., 2018; onoe and durrett, 2019).
however, we ﬁnd that better performance can beobtained by pretraining the model on automaticallylabeled samples, then ﬁne-tuning it on human an-notated samples..following (choi et al., 2018), we partition thewhole type vocabulary used by the dataset intothree non-overlapping sets: general, ﬁne, and ultra-ﬁne types, denoted with tg, tf and tu, respectively.
then, we use the following objective for training:.
j (x) = l(x, tg)1(l, tg) + l(x, tf )1(l, tf ).
+ l(x, tu)1(l, tu),.
(2).
where x is a training sample; l denotes the set oftype labels assigned to x through either human orautomatic annotation.
the function 1(l, t ) equals1 when a type in l is in set t and 0 otherwise.
this loss can avoid penalizing some false negativelabels..unlike existing studies, we deﬁne the function ldifferently for human annotated samples and auto-matically labeled samples.
while pretraining withautomatically labeled samples, the labels obtainedthrough entity linking and head word supervisionare usually of higher precision than those obtainedthrough bert mlm.
thus, we propose to assigndifferent weights in the training objective to thelabels generated with different methods:.
l(x, t ) = −.
α(t)[yt · log(pt).
(cid:88).
t∈t.
(3).
+ (1 − yt) · log(1 − pt)],.
where yt equals to 1 if t is annotated as a type forx and 0 otherwise; pt is the probability of whethert should be assigned to x predicted by the model.
the value of α(t) indicates how conﬁdent we areabout the label t for x. speciﬁcally, it equals to apredeﬁned constant value larger than 1 when t is apositive type for x obtained through entity linkingor head word supervision, otherwise, it equals to 1.while ﬁne-tuning with human annotated sam-ples, we directly use the binary cross entropy loss:.
l(x, t ) = −.
[yt·log(pt)+(1−yt)·log(1−pt)]..(cid:88).
t∈t.
3.4 self-training.
denote the ultra-ﬁne entity typing model obtainedafter pretraining on the automatically labeled dataas h, and the model obtained after ﬁne-tuning hwith human annotated data as m. a weakness ofm is that at the ﬁne-tuning stage, it is trained withonly a small number of samples.
thus, we employself-training to remedy this problem..by using m as a teacher model, our self-trainingstep ﬁne-tunes the model h again with a mixture ofthe samples from the automatically labeled data andthe human annotated data.
this time, for the auto-matically annotated samples, we use pseudo labelsgenerated based on the predictions of m instead oftheir original weak labels.
the newly ﬁne-tunedmodel should perform better than m, and is usedfor evaluation..denote the set of human annotated samples ash, the set of automatically labeled samples as a.the training objective at this step is.
jst =.
j (x) + λ.lst (x), (5).
1|h|.
(cid:88).
x∈h.
1|a|.
(cid:88).
x∈a.
where λ is a hyperparameter that controls thestrength of the supervision from the automaticallylabeled data..while computing loss for the samples in a, weonly use the types that are very likely to be positiveor negative.
for a sample x, let pt be the probabilityof it belonging to type t predicted by the model m.we consider a type t very likely to be positive ifpt is larger than a threshold p , or if t is a weaklabel of x and pt is larger than a smaller thresholdpw.
denote the set of such types as ˆy +(x).
weconsider a type t very likely to be negative if pt issmaller than 1 − p .
denote the set of such typesas ˆy −(x).
then we have:.
lst (x) = −.
log(pt).
(cid:88).
t∈ ˆy +(x)(cid:88).
t∈ ˆy −(x).
−.
log(1 − pt)..(6).
thus, we compute the binary cross entropy losswith only the types in ˆy +(x) and ˆy −(x)..4 application to traditional.
fine-grained entity typing.
our approach to generating weak entity type la-bels with bert mlm can also be applied to the.
(4).
1794traditional ﬁne-grained entity typing task.
differ-ent from ultra-ﬁne entity typing, traditional ﬁne-grained entity typing uses a manually designedentity type ontology to annotate mentions.
thetypes in the ontology are organized in an hierar-chical structure.
for example, the ontology usedby the ontonotes dataset contains 89 types includ-ing /organization, /organization/company, /person,/person/politician, etc.
on this dataset, our auto-matic annotation approach can mainly be helpfulto generate better labels for nominal mentions..we still use the same method described in sec-tion 3.1 to create input for bert mlm basedon the given mention.
but with traditional ﬁne-grained entity typing, most mentions are assignedonly one type path (e.g., a company mention willonly be assigned labels {/organization, /organiza-tion/company}, which includes all the types alongthe path of /organization/company).
thus, whilegenerating labels, we only use the most proba-ble word predicted by the bert mlm, which ismapped to the types used by the dataset if possible.
for example, the word “company” and its pluralform are both mapped to /organization/company.
such a mapping from free-form entity type wordsto the types used by the dataset can be createdmanually, which does not require much effort.
wemainly construct the mapping with two ways: 1)check each type used by the dataset, and think of afew words that should belong to it, if possible.
forexample, for the type /person/artist/author, corre-sponding words can be “author,” “writer,” etc.
2)run the bert mlm on a large number of inputsconstructed with unannotated mentions, then try tomap the words that are most frequently predicted asthe most probable word to the entity type ontology..since only the most probable word predicted bythe bert mlm is used to produce labels, we alsoonly use one hypernym relation pattern: “m andany other h.”.
for traditional ﬁne-grained entity typing, we useour approach to generate labels for mentions thatare not previously annotated with other automaticannotation approaches.
while training, all the auto-matically labeled mentions are used together.
thetyping model is the same as the model describedin 3.3. the binary cross entropy loss is directlyemployed as the training objective..5 experiments.
we conduct experiments on our primary task: ultra-ﬁne entity typing.
in addition, we evaluate theperformance of our approach when applied to tra-ditional ﬁne-grained entity typing..5.1 evaluation on ultraﬁne.
for ultra-ﬁne entity typing, we use the dataset cre-ated by choi et al.
(2018).
it uses a type set thatcontains 10,331 types.
these types are partitionedinto three categories: 9 general types, 121 ﬁne-grained types, and 10,201 ultra-ﬁne types.
thereare 5,994 human annotated samples.
they are splitinto train/dev/test with ratio 1:1:1. it also provides5.2m samples weakly labeled through entity link-ing and 20m samples weakly labeled through headword supervision..we compare with the following approaches:.
• ufet (choi et al., 2018).
this approach ob-tains the feature vector for classiﬁcation byusing a bi-lstm, a character level cnn, andpretrained word embeddings..• labelgcn (xiong et al., 2019).
labelgcnuses a graph propagation layer to capture labelcorrelations..• ldet (onoe and durrett, 2019).
ldetlearns a model that performs relabeling andsample ﬁltering to the automatically labeledsamples.
their typing model, which employselmo embeddings and a bi-lstm, is trainwith the denoised labels..• box (onoe et al., 2021).
box represents entitytypes with box embeddings to capture latenttype hierarchies.
their model is bert-based..we use the bert-base-cased version of bertfor both weak label generation and the typingmodel in section 3.3. the hyperparameters aretuned through grid search using f1 on the dev setas criterion.
the value of α(t) in equation (3) isset to 5.0 for positive types obtained through entitylinking or head word supervision.
λ in equation(5) is set to 0.01. p and pw in section 3.4 are set to0.9 and 0.7, respectively.
our approach to generatelabels through bert mlm is applied to each weaksample provided in the original dataset.
in addition,we also use our approach to annotate about 3.7mpronoun mentions, which are extracted throughstring matching from the english gigaword corpus.
1795methodp47.1ufetlabelgcn 50.351.5ldet52.8box53.6ours.
r24.229.233.038.845.3.f132.036.940.244.849.1.table 3: macro-averaged precision, recall, and f1 ofdifferent approaches on the test set..methodbert-ultra-directbert-ultra-preours (single pattern)ours (unweighted loss)ours (no self-train)ours.
p51.050.852.451.553.553.6.r33.839.744.945.842.845.3.f140.744.648.348.547.549.1.table 4: performance of different variants of our ap-proach on the test set.
bert-ultra-direct and bert-ultra-pre are two baseline approaches that do not uselabels generated with our bert mlm based methodin training..(parker et al., 2011).
we generate 10 types for eachsample2.
with the procedure described in sectiton3.1, three hypernym extraction patterns are usedwhile generating labels with bert mlm: “m andany other h,” “h such as m ,” “m and some otherh.” speciﬁcally, adding “h such as m ” and “mand some other h” improves the f1 score from0.253 to 0.274, and from 0.274 to 0.279, respec-tively.
adding any more patterns cannot improvethe f1 score for more than 0.007..following existing work (onoe et al., 2021;onoe and durrett, 2019), we evaluate the macro-averaged precision, recall, and f1 of different ap-proaches on the manually annotated test set.
theresults are in table 3. our approach achieves thebest f1 score.
it obtains more than 4% f1 scoreimprovement over the existing best reported perfor-mance by box in (onoe et al., 2021).
this demon-strates the effectiveness of our approach..uses one pattern: m and any other h; ours (un-weighted loss) removes the α(t) term in equation(3); ours (no self-train) does not perform theself-training step.
we also evaluate two baselineapproaches: bert-ultra-direct uses the samebert based model described in section 3.3, but istrained with only the human annotated training sam-ples; bert-ultra-pre also uses the same bertbased model, but is ﬁrst pretrained with the ex-isting automatically generated training samples inthe dataset provided by choi et al.
(2018), thenﬁne-tuned on the human annotated training data..first, the beneﬁt of using the labels generatedthrough bert mlm can be veriﬁed by comparingours (no self-train) and bert-ultra-pre.
becausethe techniques employed in ours (no self-train),including the use of multiple hypernym extractionpatterns and the weighted loss, are both for bet-ter utilization of our automatic entity type labelgeneration method..the effectiveness of the use of multiple hyper-nym extraction patterns, the weighted loss, andthe self-training step can be veriﬁed by compar-ing ours with ours (single pattern), ours (un-weighted loss) and ours (no self-train), respec-tively.
among them, self-training is most beneﬁ-cial..5.3 evaluation on different kinds of.
mentions.
it is also interesting to see how our approach per-forms on different kinds of mentions.
table 5 liststhe performance of our full approach and two base-line systems on the three kinds of mentions in thedataset: named entity mention, pronoun mentions,and nominal mentions..our approach performs much better than bert-ultra-pre on all three kinds of mentions.
the im-provements in f1 on pronoun and nominal men-tions are relatively more substantial..5.4 case study.
5.2 ablation study.
for ablation study, we verify the effectiveness ofthe different techniques used in our full entity typ-ing approach by evaluating the performance of thefollowing variants: ours (single pattern) only.
2the performance of the trained model is relatively insensi-tive with respect to the number of labels generated with mlm.
the difference between the f1 scores of the models trainedusing 10 and 15 generated types is less than 0.005..table 6 presents several ultra-ﬁne entity typing ex-amples, along with the human annotated labels,and the labels predicted by bert-ultra-pre, bertmlm, and our full approach..in the ﬁrst example, the label prisoner is a typethat depends on the context, and is usually notassigned to humans in knowledge bases.
we thinkthat since we can assign such labels to the trainingsamples with our bert mlm based approach, our.
1796methodbert-ultrabert-ultra-preours.
named entityrp45.158.150.554.754.458.3.f150.852.556.3.pronounr42.946.150.0.f147.448.653.4.p52.951.357.2.nominalr26.933.738.9.f134.338.643.5.p47.445.249.5.table 5: performance on named entity mentions, pronoun mentions, and nominal mentions, respectively..captured in 1795, he was con-ﬁned at dunkirk, escaped, set sailfor india, was wrecked on thefrench coast, and condemned todeath by the decree of the frenchdirectory..sentence.
sentence.
humanbert-ultra-prebert mlm.
ours.
prisoner, personperson, soldier, man, criminalman, prisoner, person, soldier, ofﬁ-cerperson, soldier, man, prisoner.
in.
the.
struck.
morning,alsoa roadside bombapolice patrol on a main road inbaghdad’s northern neighbor-hood of waziriya, damaging apolice vehicle ....humanbert-ultra-prebert mlm.
ours.
bomb, weapon, object, explosiveobject, event, attack, bombweapon, threat, evidence, device, de-brisobject, weapon, bomb.
in october 1917, sutton was pro-moted (temporarily) to the rankof major and appointed ofﬁcercommanding no.7 squadron, aposition he held for the remainedof the war.
soldier, ofﬁcer, male, personperson, politician, maleofﬁcer, pilot, man, unit, aircraftperson, soldier, male, ofﬁcer.
sentence.
humanbert-ultra-prebert mlmours.
table 6: ultra-ﬁne entity typing examples with the cor-responding human annotated labels and predictions ofthree different systems.
entity mentions are in bold andunderlined.
for bert mlm, we list the top ﬁve labels..model is better at predicting them than the baselinemodel..the second and third examples demonstrate thatour model may not only improve the recall by pre-dicting more correct types, but also reduce incor-rect predictions that do not ﬁt the mention or thecontext well..5.5 evaluation on ontonotes.
the ontonotes dataset uses an ontology that con-tains 89 types to label entity mentions.
we use theversion provided by choi et al.
(2018).
it includes.
11,165 manually annotated mentions, which aresplit into a test set that contains 8,963 mentions,and a dev set that contain 2,202 mentions.
it alsoprovides about 3.4m automatically labeled men-tions..since existing annotations for named entity men-tions may be more accurate than the annotationsobtained through our approach, we only apply ourmethod to label nominal mentions.
applying theapproach in section 4, we create 1m new auto-matically labeled mentions with the head word su-pervision samples (such samples contain mostlynominal mentions) in the ultra-ﬁne dataset.
theyare used together with the originally provided 3.4mmentions to train the typing model..on this dataset, we compare with the follow-ing approaches: ufet (choi et al., 2018), ldet(onoe and durrett, 2019), dsam (hu et al.,2020), ltrfet (lin and ji, 2019), bert-direct.
where bert-direct uses the same bert basedmodel as our approach, but trains with only theweak samples provided in the dataset.
ltrfetadopts a hybrid classiﬁcation method to exploittype inter-dependency.
dsam is a diversiﬁed se-mantic attention model with both mention-levelattention and context-level attention..for our approach and bert-direct, we still usethe pretrained bert-base-cased model for initial-ization.
although a very large number of weaklylabeled mentions are provided, not all of them areneeded for training the models.
in our experiments,for both our approach and bert-direct, the per-formance does not increase after training on about0.3m mentions..we report strict accuracy, macro-averaged f1,and micro-averaged f1 (ling and weld, 2012).
theresults are in table 7. as we can see, our approachalso achieves the best performance on this dataset.
comparing it with bert-direct demonstrates thebeneﬁt of the samples automatically labeled withbert mlm.
however,.
less improvement is achieved onontonotes than on the ultra-ﬁne entity typing.
1797methodufetltrfetldetdsambert-directours.
acc macro f1 micro f159.563.864.966.0663.2567.44.
71.877.379.278.1975.9080.35.
76.882.984.583.0780.8485.44.table 7:performance of different approaches onontonotes.
we report strict accuracy, macro-averagedf1, and micro-averaged f1..dataset.
we think there are two main reasons.
first,ontonotes uses a much smaller entity type set(89 types) than the ultra-ﬁne entity typing dataset(10,331 types).
as a result, some ﬁner grainedtypes that can be produced by our approach be-come less beneﬁcial.
second, generating type la-bels that are highly dependent on the context (e.g.,types like criminal, speaker) is an advantage of ourapproach, and the ultra-ﬁne entity typing datasetcontains more such type labels..6 conclusion.
in this work, we propose a new approach to auto-matically generate ultra-ﬁne entity typing labels.
given a sentence that contains a mention, we inserta hypernym extraction pattern with a “[mask]” to-ken in it, so that a pretrained bert mlm may pre-dict hypernyms of the mention for “[mask].” mul-tiple patterns are used to produce better labels foreach mention.
we also propose to use a weightedloss and perform a self-training step to learn betterentity typing models.
experimental results showthat our approach greatly outperforms state-of-the-art systems.
additionally, we also apply our ap-proach to traditional ﬁne-grained entity typing, andverify its effectiveness with experiments..acknowledgments.
this paper was supported by the nsfc grant (no.
u20b2053) from china, the early career scheme(ecs, no.
26206717), the general research fund(grf, no.
16211520), and the research impactfund (rif, no.
r6020-19 and no.
r6021-20) fromthe research grants council (rgc) of hong kong,with special thanks to the wechat-hkust whatlab on artiﬁcial intelligence technology..references.
tongfei chen, yunmo chen, and benjamin van durme.
2020. hierarchical entity typing via multi-levelin proceedings of acl, pageslearning to rank.
8465–8475..eunsol choi, omer levy, yejin choi, and luke zettle-moyer.
2018. ultra-ﬁne entity typing.
in proceed-ings of acl, pages 87–96..hongliang dai, donghong du, xin li, and yangqiuimproving ﬁne-grained entity typingin proceedings of emnlp-.
song.
2019.with entity linking.
ijcnlp, pages 6211–6216..hongliang dai, yangqiu song, and xin li.
2020. ex-ploiting semantic relations for ﬁne-grained entityin automated knowledge base construc-typing.
tion..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-in proceedings of naacl-hlt, pages 4171–ing.
4186..dan gillick, nevena lazic, kuzman ganchev, jessekirchner, and david huynh.
2014.context-dependent ﬁne-grained entity type tagging.
arxivpreprint arxiv:1412.1820..marti a hearst.
1992. automatic acquisition of hy-ponyms from large text corpora.
in proceedings ofcoling..yanfeng hu, xue qiao, xing luo, and chen peng.
2020. diversiﬁed semantic attention model for ﬁne-grained entity typing.
ieee access..zhengbao jiang, frank f xu, jun araki, and grahamneubig.
2020. how can we know what languagemodels know?
transactions of the association forcomputational linguistics, 8:423–438..hailong jin, lei hou, juanzi li, and tiansi dong.
2019. fine-grained entity typing via hierarchicalmulti graph convolutional networks.
in proceedingsof emnlp-ijcnlp, pages 4970–4979..mitchell koch, john gilmer, stephen soderland, anddaniel s. weld.
2014. type-aware distantly super-vised relation extraction with linked arguments.
inproceedings of emnlp, pages 1891–1901..chin lee, hongliang dai, yangqiu song, and xin li.
2020. a chinese corpus for ﬁne-grained entity typ-ing.
in proceedings of lrec, pages 4451–4457..ying lin and heng ji.
2019. an attentive ﬁne-grainedentity typing model with latent type representation.
in proceedings of emnlp-ijcnlp, pages 6198–6203..xiao ling, sameer singh, and daniel s weld.
2015.design challenges for entity linking.
transactionsof the association for computational linguistics,3:315–328..1798xiao ling and daniel s weld.
2012. fine-grained en-tity recognition.
in proceedings of aaai, volume 12,pages 94–100..shikhar murty, patrick verga, luke vilnis,.
irenaradovanovic, and andrew mccallum.
2018. hier-archical losses and new resources for ﬁne-grainedin proceedings of acl,entity typing and linking.
pages 97–109..yasumasa onoe, michael boratko, and greg durrett.
2021. modeling ﬁne-grained entity types with boxembeddings.
arxiv preprint arxiv:2101.00345..yasumasa onoe and greg durrett.
2019. learning todenoise distantly-labeled data for entity typing.
inproceedings of naacl-hlt, pages 2407–2417..yasumasa onoe and greg durrett.
2020. interpretableentity representations through large-scale typing.
inproceedings of emnlp, pages 612–624..robert parker, david graff, junbo kong, ke chen, andkazuaki maeda.
2011. english gigaword ﬁfth edi-tion.
linguistic data consortium..fabio petroni, tim rockt¨aschel, sebastian riedel,patrick lewis, anton bakhtin, yuxiang wu, andalexander miller.
2019. language models as knowl-in proceedings of emnlp-ijcnlp,edge bases?
pages 2463–2473..xiang ren, wenqi he, meng qu, clare r voss, hengji, and jiawei han.
2016. label noise reduction inentity typing by heterogeneous partial-label embed-ding.
in proceedings of acm sigkdd, pages 1825–1834..julian seitner, christian bizer, kai eckert, stefanofaralli, robert meusel, heiko paulheim, and si-mone paolo ponzetto.
2016. a large database ofhypernymy relations extracted from the web.
in pro-ceedings of lrec, pages 360–367..ji xin, yankai lin, zhiyuan liu, and maosong sun.
2018.improving neural ﬁne-grained entity typingwith knowledge attention.
in proceedings of aaai,volume 32..wenhan xiong, jiawei wu, deren lei, mo yu, shiyuchang, xiaoxiao guo, and william yang wang.
imposing label-relational inductive bias for2019.in proceed-extremely ﬁne-grained entity typing.
ings of naacl-hlt, pages 773–784..mohamed amir yosef, sandro bauer, johannes hof-fart, marc spaniol, and gerhard weikum.
2012.hyena: hierarchical type classiﬁcation for entityin proceedings of coling, pages 1361–names.
1370..yunyi zhang, jiaming shen, jingbo shang, and jiaweihan.
2020. empower entity set expansion via lan-guage model probing.
in proceedings of acl, pages8151–8160..1799