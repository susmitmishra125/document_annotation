a large-scale chinese multimodal ner dataset with speech clues.
dianbo sui, zhengkun tian, yubo chen, kang liu, jun zhaonational laboratory of pattern recognition, institute of automation, casschool of artiﬁcial intelligence, university of chinese academy of sciences{dianbo.sui, zhengkun.tian, yubo.chen, kliu, jzhao}@nlpr.ia.ac.cn.
abstract.
in this paper, we aim to explore an unchartedterritory, which is chinese multimodal namedentity recognition (ner) with both textual andacoustic contents.
to achieve this, we con-struct a large-scale human-annotated chinesemultimodal ner dataset, named cnerta.
our corpus totally contains 42,987 anno-tated sentences accompanying by 71 hours ofspeech data.
based on this dataset, we proposea family of strong and representative base-line models, which can leverage textual fea-tures or multimodal features.
upon these base-lines, to capture the natural monotonic align-ment between the textual modality and theacoustic modality, we further propose a sim-ple multimodal multitask model by introduc-ing a speech-to-text alignment auxiliary task.
through extensive experiments, we observethat: (1) progressive performance boosts as wemove from unimodal to multimodal, verifyingthe necessity of integrating speech clues intochinese ner.
(2) our proposed model yieldsstate-of-the-art (sota) results on cnerta,demonstrating its effectiveness.
for further re-search, the annotated dataset is publicly avail-able at http://github.com/dianbowork/cnerta..1.introduction.
“speech is a part of thought.”.
— oliver sacks, seeing voices.
as a fundamental subtask of information extraction,named entity recognition (ner) aims to locate andclassify named entities mentioned in unstructuredtexts into predeﬁned semantic categories, such asperson names, locations and organizations.
nerplays a crucial role in many natural language pro-cessing (nlp) tasks, including relation extraction(zelenko et al., 2003), question answering (moll´aet al., 2006) and summarization (aramaki et al.,2009)..figure 1: the given sentence “南京市长江大桥” canbe segmented into “[南京市] [长江大桥]” or “[南京][市长] [江大桥]”.
only based on textual contents, it isdifﬁcult to infer ner tags.
but the speech waveformsof these two segmentations are radically different..most of the research on ner, such as lam-ple et al.
(2016); ma and hovy (2016); chiu andnichols (2016), only relies on the textual modal-ity to infer tags.
however, when texts are noisyor short, and it is not sufﬁcient to locate and clas-sify named entities accurately only based on tex-tual information (baldwin et al., 2015; lu et al.,2018).
one promising solution is to introduce othermodalities as the supplement of the textual modal-ity.
so far, some studies on multimodal ner, suchas moon et al.
(2018); zhang et al.
(2018); lu et al.
(2018); arshad et al.
(2019); asgari-chenaghluet al.
(2020); yu et al.
(2020); chen et al.
(2020);sun et al.
(2020), have attempted to couple thetextual modality with the visual modality and wit-nessed a stable improvement..in this work, we also focus on multimodal ner.
but differently from previous studies, we pay spe-cial attention to chinese multimodal ner withboth textual and acoustic contents.
the motivationcomes from two aspects:.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2807–2818august1–6,2021.©2021associationforcomputationallinguistics2807南京市南京市(loc)nanjing city长江大桥(loc)yangtze river bridge南京市长江大桥thenanjing yangtze river bridgespeech:sentence:南京市南京(loc)nanjing江大桥(per)daqiao jiang南京市长江大桥nanjing mayor daqiao jiangspeech:sentence:sentence:南京市长江大桥south capital city long  rive  big   bridgefirst, despite much recent success in multimodalner, current studies on this topic are limited inenglish, and totally skirt other languages.
mean-while, previous work on chinese ner, such as xuet al.
(2013); peng and dredze (2016a); zhang andyang (2018); cao et al.
(2018); sui et al.
(2019);gui et al.
(2019); ma et al.
(2020); li et al.
(2020),totally ignores valuable multimodal information.
with around 1.3 billion native speakers and thewide spread of short-form video apps in china, itis necessary and urgent to carry out research onchinese multimodal ner..second, unlike the static visual modality, thetime-varying acoustic modality plays a unique rolein chinese ner, especially in providing preciseword segmentation information.
in detail, differentfrom english, chinese is an ideographic languagefeatured by no word delimiter between words inwritten.
this language characteristic is one of themajor roadblocks in chinese ner, since named en-tity boundaries are usually word boundaries (zhangand yang, 2018).
fortunately, cues contained in theﬂuent acoustic modality, especially pauses betweenadjacent words, are able to aid the ner model indiscovering word boundaries.
a classic exampleshown in figure 1 can perfectly illustrate this point.
in this example, the sentence with ambiguous wordsegmentation would be disambiguated with the aidof the acoustic modality, which would absolutelyassist the model to infer correct ner tags..in this work, we make the following efforts to.
advance multimodal ner:.
first, we construct a large-scale human-annotated chinese ner dataset with textual andacoustic contents, named cnerta.
speciﬁcally,we annotate all occurrences of 3 entity types (per-son name, location and organization) in 42,987 sen-tences originating from the transcripts of aishell-1(bu et al., 2017), a corpus that has been widely em-ployed in mandarin speech recognition research inrecent years (shan et al., 2019; li et al., 2019; tianet al., 2020).
in particular, unlike previous mul-timodal ner datasets (moon et al., 2018; zhanget al., 2018; lu et al., 2018) are all ﬂatly annotated,not only the topmost entities but also nested entitiesare annotated in cnerta..second, based on cnerta, we establish a fam-ily of strong and representative baselines.
in de-tail, we ﬁrst investigate the performance of severalclassic text-only models on our dataset, includingbilstm-crf (lample et al., 2016) and bert-.
crf (devlin et al., 2019).
then, since introduc-ing a lexicon has been proven as an effective wayto incorporate word information in chinese ner(zhang and yang, 2018), we implement severallexicon-enhanced models, such as lattice-lstm(zhang and yang, 2018) and zen (diao et al.,2020), to explore whether the acoustic modality canprovide word information beyond the lexicon.
fi-nally, to verify the effectiveness of introducing theacoustic modality, we test some widely used mul-timodal models, such as cma (tsai et al., 2019)and mmi (yu et al., 2020), on our dataset..third, upon these strong baselines, we furtherpropose a simple multi-modal multi-task model(short for m3t) to make better use of the pauseinformation in the acoustic modality.
speciﬁcally,different from coupling the visual modality withthe textual modality, there is a monotonic align-ment between the acoustic modality and the textualmodality.
armed with such an alignment, the po-sition of each chinese character in the continuousspeech would be determined, which would makeit easy to discover pauses between adjacent words.
therefore, to automatically estimate this desiredalignment, we introduce a speech-to-text alignmentauxiliary task and propose a hybrid ctc/taggingloss.
in the hybrid loss, a masked ctc loss (graveset al., 2006) is designed for enforcing a monotonicalignment between speech and text sequences..the primary contributions of this work can be.
summarized as follows:.
• we construct cnerta,.
the ﬁrst human-annotated chinese multimodal ner dataset,where each annotated sentence is paired withits corresponding speech data.
to our bestknowledge, this dataset is not only the largestmultimodal ner dataset, but also the largestchinese nested ner dataset..• we establish a family of baselines to lever-age textual features or multimodal features.
through various experiments, we observe con-sistent performance boosts originating fromacoustic features, which veriﬁes the signiﬁ-cant merits of integrating acoustic features forchinese ner..• we further propose a multimodal multitaskmethod by introducing a speech-to-text align-ment auxiliary task.
by jointly solving the tag-ging task and the alignment task, the proposedmethod can yield sota results on cnerta..28082 related work.
mutlimodal ner: as multimedia technologyevolves, processing multimodal data is becom-ing a burning issue.
as a basic nlp tool, mul-timodal ner attracts increasing attention in re-cent years.
most of studies on multimodal nerfocus on leveraging the associate images to bet-ter identify the named entities contained in thetext.
speciﬁcally, moon et al.
(2018) propose amultimodal ner network with modality attentionto fuse textual and visual information.
to modelinter-modal interactions and ﬁlter out the noise inthe visual context, zhang et al.
(2018) proposean adaptive co-attention network and a gated vi-sual attention mechanism for multimodal ner.
astransformer-based models (vaswani et al., 2017;devlin et al., 2019) become the mainstream methodin nlp, researchers turn to study how to fuse vi-sual clues in transformers structure.
chen et al.
(2020) use captions to represent images as text andadopt transformer-based sequence labeling mod-els to connect multimodal information.
yu et al.
(2020) propose a multimodal transformer model,which empowers transformer with a multimodalinteraction module to capture the inter-modalitydynamics between words and images.
but differ-ent from them, we aim to explore an unexploredterritory in this work, which is chinese multimodalner with both speech and textual contents..chinese ner: compared with english ner, chi-nese ner is more complicated since the writtentext in chinese is not naturally segmented.
there-fore, how to incorporate word information is thekey challenge in chinese ner.
there are threemain ways to fuse word information in chinesener.
the ﬁrst one is the pipeline method.
inthe pipeline method, chinese word segmentation(cws) is ﬁrst applied and then a word-based nermodel is used.
the second one is to learn cwsand ner tasks jointly (xu et al., 2013; peng anddredze, 2016b; cao et al., 2018; wu et al., 2019).
in such a way, the word boundary information inthe cws task can be transferred to the ner model.
the third one is to resort to an automatically con-structed lexicon (zhang and yang, 2018; ding et al.,2019; liu et al., 2019a; sui et al., 2019; gui et al.,2019; li et al., 2020; ma et al., 2020; xue et al.,2020).
different from all previous studies, we fo-cus on use speech clues to incorporate word infor-mation in chinese ner..audio durationavg sent lenmax sent lenprop nested ent# instance# entity# org# per# loc.
train.
dev.
test.
7.50h19.7744.
56.68h19.6939.
7.59h19.753931.25% 29.50% 28.35%4,4454,44034,1027,2635,88923,8052,7942,1877,0661,0721,1165,8463,3972,58610,893.table 1: the statistics of training, development and testfolds of the annotated corpus.
here, “avg” denotes av-erage, “sent” denotes sentence, “len” denotes length,“prop” denotes proportion, “ent” denotes entity and “#”denotes number..3 dataset acquisition and comparison.
in this work, we aim to explore chinese ner withboth speech and textual clues.
but we are not awareof any such existing corpus, hence we are motivatedin this section, we will discussto collect one.
the data acquisition process, subsequently presentstatistics of the dataset and compare the annotateddataset with other widely-used ner datasets..3.1 dataset acquisition.
the main challenge in data acquisition is to ﬁnda large-scale dataset, which includes texts and thecorresponding speech data.
one possible way is toattach speech data to current existing chinese nerdatasets.
however, it is costly to gather hundredsof participants in the recording.
therefore, we takea different way, manually annotating ner tagson a speech recognition dataset from scratch.
indetail, our annotated dataset is based on aishell-1 (bu et al., 2017) dataset, which is a large-scalemandarin automatic speech recognition dataset.
inthis dataset, text transcriptions are chosen from ﬁvedomains: “finance”, “science and technology”,“sport”, “entertainments” and “news”.
there are400 participants in the recording, and the gender ofparticipants is balanced with 47% male and 53%female.
speech utterances are recorded via threecategories of devices in parallel, which are a highﬁdelity microphone working at 44.1 khz, 16-bit,android phones working at 16 khz, 16-bit, andapple iphones working at 16 khz, 16-bit..to ensure the quality of annotation, we designtwo rounds in the annotation procedure.
in the ﬁrst.
2809dataset.
# train # dev.
# test.
# total language structure.
modality.
msraontonotesweibo nerresume.
geniajnlpbaace-2004ace-2005.
twitter-2015twitter-2017.
46,36415,7241,3503,821.
15,02220,5466,1987,285.
4,0003,373.
-4301271463.
1,669-742968.
1,000723.
4,3654,346270477.
1,8544,2608091,058.
3,257723.
50,72924,3711,8914,761.
18,54524,8067,7499,311.
8,2574,819.chinesechinesechinesechinese.
englishenglishenglishenglish.
englishenglish.
flatflatflatflat.
nestednestednestednested.
flatflat.
texttexttexttext.
texttexttexttext.
text + imagetext + image.
cnerta.
34,102.
4,440.
4,445.
42,987.chinese.
nested.
text + speech.
table 2: a comparison between cnerta and other existing widely-used ner datasets..round, we use brat (stenetorp et al., 2012) as theannotation tool and ask 3 internal annotators (in-cluding the ﬁrst author of this paper) to performannotation, who are very familiar with this task.
they independently identify and classify named en-tities in the transcriptions with more than 17 char-acters.
cohen’s kappa coefﬁcient (cohen, 1960)is used to measure the inter-annotator agreements.
after the ﬁrst round, κ = 0.965, which shows thequality of cnerta is satisfactory.
but there arestill some sentences for which annotators give outdifferent annotations.
for those sentences, the an-notators check the disagreed annotations carefullyand discuss to reach the agreements for all cases.
after we ﬁnish the annotation process, we splitthe dataset into three parts: training, development,and test set.
table 1 shows the high level statisticsof data splits for cnerta..from table 2, we observe that our corpus hasunique value compared with the existing datasets.
the value is reﬂected in the following aspects: (1)cnerta is a large-scale dataset; (2) cnerta isthe ﬁrst chinese multimodal dataset; (3) not onlythe topmost entities but also nested entities areannotated; (4) among these datasets, the acousticmodality is only introduced in cnerta..4 preliminaries.
4.1 task description.
given a text x = x1, x2, ..., xn and its correspond-ing speech s = s1, s2, ..., st, where xi denotesthe i-th chinese character and sj denotes the j-thwaveform frame, the goal of the task is to leveragetextual and speech clues to identify and classify allnamed entities contained in the text..3.2 dataset comparison.
4.2 nested structure linearization.
we compare cnerta with several widely usedner datasets in table 2. speciﬁcally, we ﬁrstcompare our corpus with some chinese nerdatasets, such as msra (levow, 2006), ontonotes(weischedel et al., 2011), weibo ner (peng anddredze, 2016a) and resume (zhang and yang,2018).
then, we compare our corpus with sev-eral widely used nested ner datasets, like ge-nia (kim et al., 2003), jnlpba (collier andkim, 2004), ace-2004 (doddington et al., 2004)and ace-2005 (walker et al., 2004).
finally,multimodal ner datasets, including twitter-2015(zhang et al., 2018) and twitter-2017 (lu et al.,2018), are compared with our corpus..unlike ﬂat ner, named entities may overlap andalso be labeled with more than one label in nestedner.
to solve nested ner, we follow strakov´aet al.
(2019) to encode the nested entity structureinto a conll-like, per-character bio encoding(ramshaw and marcus, 1995).
there are tworules to guide the linearization: (1) entity mentionsstarting earlier have priority over entities startinglater, and (2) for mentions with the same beginning,longer entity mentions have priority over shorterones.
a multilabel for a given chinese character isa concatenation of all intersecting entity mentions,from the highest priority to the lowest.
for moredetails, we refer readers to strakov´a et al.
(2019)..28104.3 acoustic encoder.
the acoustic encoder is used to map raw speech sig-nals into continuous space.
there are three parts inthe proposed acoustic encoder: a speech processinglayer, a convolution front end and a transformer-based encoder..speciﬁcally, in the speech processing layer, aspeech signal ﬁrst goes through a pre-emphasisﬁlter; then gets sliced into frames and a windowfunction is applied to each frame; afterwards, ashort-time fourier transform (kwok and jones,2000) is employed on each frame and the powerspectrum is calculated; and subsequently, the ﬁlterbanks (ravindran et al., 2003) are computed.
then,we use a convolution front end to down-sample thelong acoustic features.
in the convolution front end,following dong et al.
(2018); tian et al.
(2020),two 3×3 cnn layers with stride 2 are stackedfor both time and frequency dimensions.
after-wards, in order to enable the acoustic encoder toattend by relative positions, the positional encod-ing is added to the output of the convolution frontend.
finally, to effectively capture long-term de-pendencies, down-sampled acoustic features ﬂowthrough the transformer-based encoder (vaswaniet al., 2017).
the transformer-based encoder isa stack of 6 identical layers, each of which iscomposed of a self-attention sub-layer and a feed-forward network..5 baselines.
based on the annotated dataset, a family of strongand representative baselines is established, includ-ing (1) text-only models presented in section 5.1,(2) lexicon-enhanced models shown in section 5.2and (3) multimodal models introduced in section5.3..bilstm-crf:featured by a bidirectionallstm (hochreiter and schmidhuber, 1997) asthe textual encoder and conditional random ﬁelds(crf) (lafferty et al., 2001) as the decoder, thewidely used bilstm-crf (lample et al., 2016) isadopted as an important baseline..plm-crf:instead of training a model fromscratch, we also adopt the framework of ﬁne-tuninga pretrained language model (plm) on a down-stream task (radford et al., 2018).
in this frame-work, we adopt bert (devlin et al., 2019) as thetextual encoder and use crf as the decoder.
inaddition to initializing the textual encoder withthe original pretrained bert model, a sota chi-nese pretrained language model, called macbert(cui et al., 2020), is used.
compared with bert,macbert is built upon roberta (liu et al.,2019b) and the original mlm task in bert isreplaced with the mlm as correction task.
formore details, we refer readers to cui et al.
(2020)..5.2 lexicon-enhanced model:.
a drawback of the text-only methods mentionedabove is that explicit word and word sequence infor-mation is not fully exploited, which can be poten-tially useful.
with this consideration, we also adoptlexicon-enhance models to incorporate word lexi-cons.
(1) lattice-lstm (zhang and yang, 2018)is a classic method that can encode a sequence ofinput characters as well as all potential words thatmatch a lexicon.
(2) zen (diao et al., 2020) isa pretrained chinese text encoder enhanced by ann-gram lexicon.
in zen, n-gram contexts are ex-tracted, encoded and integrated with the characterencoder.
for more details about lattice-lstm andzen, we refer readers to zhang and yang (2018)and diao et al.
(2020)..5.1 text-only model.
5.3 multimodal model.
open-source nlp toolkit: many open-sourcenlp toolkits, such as spacy (honnibal et al., 2020)and stanza (qi et al., 2020), support chinese ner.
in spacy, a multitask cnn is employed.
in stanza,a contextualized string representation based taggerfrom akbik et al.
(2018) is adopted.
in both spacyand stanza, the tagger is trained on ontonote(weischedel et al., 2011).
to map the output oftaggers to cnerta’s label space, expert-designedrules are used, such as person → per.
sincethese toolkits are only designed for ﬂat structure,we do not evaluate these toolkits in nested settings..to leverage the acoustic modality, several multi-in these models,modal models are introduced.
fusion modules are built on the top of the acous-tic encoder and the textual encoder, which are de-signed for capturing the interaction between thetextual hidden representations x = [x1, x2, ..., xn];xi ∈ rd and the acoustic representations s =[s1, s2, ..., st(cid:48)]; sj ∈ rd.
we present two repre-sentative fusion modules, which are cross-modalattention (cma) module (tsai et al., 2019) andmultimodal interaction (mmi) module (yu et al.,2020)..2811cross-modal attention module (cma): giventhe textual hidden representations x ∈ rd×n andthe acoustic representations s ∈ rd×t(cid:48), we ﬁrst em-ploy a m-head cross-modal attention mechanism(tsai et al., 2019), by treating x as queries, and sas keys and values:.
cai(x, s) = softmax(.
[wqix]t[wkis](cid:112)d/m.
)[wvis].
mh-ca(x, s) = w(cid:48)[ca1(x, s), ..., cam(x, s)].
where cai refers to the i-th head of cross-modalattention, and {wqi, wki, wvi} ∈ rd/m×d, w(cid:48) ∈rd×d denote the weight matrices for the query, key,value and multi-head attention, respectively.
then,we stack the following sub-layers on top:.
ˆf = ln(x + mh-ca(x, s))f = ln(ˆf + ffn(ˆf)).
(1).
where ln means layer normalization (ba et al.,2016) and ffn means a fully connected feed-forward network, which consists of two linear trans-formation with a relu activation (nair and hin-ton, 2010).
finally, the new textual representationsf ∈ rd×n, which are enhanced by acoustic fea-tures, are fed into the crf decoder to infer nertags..multimodal interaction module (mmi): astack of cross-modal attention layer mentionedabove makes up the multimodal interaction module.
since the architecture of mmi is too complex andis not the core of this paper, we will not introduceit in the main text.
for more details about mmi,we refer readers to yu et al.
(2020)..6 proposed method.
previous multimodal methods ignore a naturalmonotonic alignment between the acoustic modal-ity and the textual modality.
to capture thisalignment, we propose a multimodal multitaskmodel, called m3t.
the framework of the pro-posed method is shown in figure 2..in the m3t model, we adopt the cma moduleto fuse acoustic information into the textual repre-sentations.
besides, a ctc project layer is builtupon the acoustic encoder, and the loss function isa combination of masked ctc loss and crf loss.
speciﬁcally, through the ctc project layer, eachacoustic representation si ∈ rd is ﬁrst mappedto the total size of model units (in this paper, the.
figure 2: overall architecture of the proposed multi-modal multitask model..model unit is the chinese character) and then ispassed through a logit function:.
g = logit(wt.
v s).
(2).
where wv ∈ rd×|v | and |v | is the total size of chi-nese characters.
unlike automatic speech recogni-tion, only the characters in the given text need to bealigned rather than the entire model units.
there-fore, we only keep these rows unchanged, whosecorresponding characters are contained in the giventext, and ﬁll the other rows in g ∈ r|v |×t(cid:48)with thevalue −∞.
the masked tensor g is then fed intoctc loss.
finally, to jointly solve the tagging taskand the alignment task, a hybrid loss of combiningthe masked ctc loss with the crf loss is used:.
l = lcrf + λlctc.
(3).
where λ is a hyperparameter..7 experiments.
in this section, we carry out various experimentsto investigate the effectiveness of introducing theacoustic modality.
in addition, we empirically com-pare the proposed model and these baselines underdifferent settings.
following previous studies inner (zhang and yang, 2018), standard precision(p), recall (r) and f1-score (f1) are used as evalu-ation metrics..28122d conv layeradd & normmulti-headself-attentionfeed forward ctc project layercrf layerpositionalencodingadd & normtextual encoderx2feed forwardadd & normadd & normmulti-head cross-modal attention南 京 市 长 江 大 桥+x6qkv+joint trainingmasked ctc losscrf lossmodel.
resource.
spacystanza.
texttext.
bilstm-crflattice lstmbert-crfmacbert-crfzen-crf.
bilstm-cma-crfbert-cma-crfmacbert-cma-crf.
bilstm-mmi-crfbert-mmi-crfmacbert-mmi-crf.
bilstm-m3tbert-m3tmacbert-m3t.
texttext+lexicontexttexttext+lexicon.
text+speechtext+speechtext+speech.
text+speechtext+speechtext+speech.
text+speechtext+speechtext+speech.
zen-cma-crf text+lexicon+speech.
zen-mmi-crf text+lexicon+speech.
zen-m3t text+lexicon+speech.
flat ner.
nested ner.
p.r.f1.
∆.
f1.
∆.
64.7449.65.
64.4367.1374.4775.1074.85.
66.5675.6775.9477.26.
66.6375.3776.7576.30.
69.8577.7178.7478.66.
23.0127.34.
62.5868.3476.3478.7077.78.
65.2878.4981.3778.07.
64.7879.6280.9179.45.
66.2480.6082.0279.78.
33.9435.27.
63.4967.7375.3976.8676.28.
65.9277.0578.5677.66.
65.8277.4478.7777.84.
68.0079.1380.3579.21.
--.
-----.
↑ 2.43↑ 1.66↑ 1.70↑ 1.38.
↑ 2.33↑ 2.05↑ 1.91↑ 1.56.
↑ 4.51↑ 3.74↑ 3.49↑ 2.93.p.--.
70.7277.9280.0381.2281.18.
75.2881.5081.2081.56.
77.7880.9581.1881.11.
79.1783.4683.9982.99.r.--.
59.1660.8972.3473.6772.12.
61.2574.4876.8074.94.
59.1174.9877.2175.36.
60.3975.8177.4676.41.
--.
64.4368.3675.9977.2676.38.
67.5477.8378.9478.11.
67.1777.8579.1478.13.
68.5279.4580.5979.57.
--.
-----.
↑ 3.11↑ 1.84↑ 1.68↑ 1.73.
↑ 2.74↑ 1.86↑ 1.88↑ 1.75.
↑ 4.09↑ 3.46↑ 3.33↑ 3.19.table 3: precision (%) , recall (%) and f1 score (%) of baselines and our proposed method on cnerta.
∆ meansthe points higher than the corresponding baselines without using the acoustic modality..7.1.implementation details.
7.2 main results.
lstm-based baselines:we use the 50-dimensional character embeddings, which are pre-trained on chinese giga-word * using word2vec(mikolov et al., 2013).
the dimensionality oflstm hidden states is set to 300 and the initiallearning rate is set to 0.001. we train the modelsusing 100 epochs with a batch size of 16..lexicon: the lexicon used in lattice-lstm isthe same as zhang and yang (2018) and the lexi-con used in zen is the same as diao et al.
(2020).
due to low speed in training and inference, we onlyemploy lattice-lstm in unimodal settings..pretrained language model fine-tuning: weuse the base models of bert (devlin et al., 2019),macbert (cui et al., 2020) and zen (diao et al.,2020).
the initial learning rate of pretrained lan-guage model is set to 1 × 10−5.
we ﬁne-tune mod-els using 10 epochs with a batch size of 16..computing infrastructure: all experiments areconducted on an nvidia geforce rtx 2080 ti(11 gb of memory)..*https://catalog.ldc.upenn.edu/.
ldc2011t13.
table 3 shows the results of baselines and our pro-posed model on cnerta.
from the table, we ﬁnd:.
(1) introducing the acoustic modality can sig-niﬁcantly boost the performance of the character-based models, such as bilstm-crf, bert-crfand macbert-crf.
with the simple cma mod-ule to introduce the acoustic modality, there is amore than 1.6% improvement in both ﬂat ner andnested ner.
furthermore, by using the m3t modelto leverage the acoustic modality, a more than 3%improvement can be brought in all cases.
theseexperimental results demonstrate the effectivenessof introducing the acoustic modality in character-based ner models..(2) introducing the acoustic modality can im-prove the performance of lexicon-based models,such as zen-crf.
by introducing the acousticmodes in zen-crf with the cma module, theperformance in ﬂat ner and nested ner can be im-proved by 1.38% and 1.73%, respectively.
armedwith the m3t model, the performance in ﬂat nerand nested ner can be further improved by 2.93%and 3.19%.
although not as signiﬁcant as the im-provement of the character-based models, these.
2813sentence.
gold.
bert-m3t.
bert-crf.
沙特阿拉伯选手马斯拉赫以四十三秒九三预获得预赛第一(maslakh, from saudi arabia, won the ﬁrst place.
in the preliminary contest with 43.93 seconds).
与她在首都机场吃了一碗牛肉面有很大关系(it has a lot to do with a bowl of beef noodles.
eaten at the capital airport).
国际米兰日文官方推特公布了选手抵达时的照片(inter milan’s ofﬁcial japanese twitter released.
photos of the players when they arrived).
沙特阿拉伯(loc)马斯拉赫(per).
沙特阿拉伯(loc)马斯拉赫(per).
沙特(loc)阿拉伯(loc)马斯拉赫(per).
首都机场(loc).
首都机场(loc).
首都(loc).
国际米兰(org) 国际米兰(org) 国际米兰日文(org).
卡巴里罗在毕尔巴鄂掷出了七十米六五的好成绩(kabariro threw a good result of 70.65m in bilbao).
卡巴里罗(per)毕尔巴鄂(loc).
卡巴里罗(per)毕尔(loc)巴鄂(per).
卡巴里罗(per)毕尔巴鄂(loc).
table 4: case studies to illustrate the effectiveness of introducing the acoustic modality.
note that both bert-m3tand bert-crf are trained in ﬂat ner settings..structure.
model.
# typeerror.
# boundaryerror.
total.
flatner.
nestedner.
bert-crf.
bert-m3t.
bert-crf.
bert-m3t.
97(10.68%).
94(11.93%).
125(13.79%).
129(15.21%).
811(89.32%).
694(88.07%).
781(86.20%).
719(84.78%).
908.
788.
906.
848.table 5: the statistics of different errors that occur inthe output of ner models on the development set..results still prove that the acoustic modality canprovide lexicon-based models with some informa-tion that does not contain in the large-scale lexicon.
(3) our proposed method (m3t) can achieve thesota results on cnerta.
compared with cma(tsai et al., 2019) and mmi (yu et al., 2020), thereis a signiﬁcant improvement.
we conjecture thatis due to that the monotonic alignment betweenthe acoustic modality and the textual modality iscaptured by the masked ctc loss and armed withthis alignment, precise word boundary informationcontained in speech is leveraged by the model..7.3 error analysis.
as ner models established here are not yet as ac-curate as one would hope, some analyses of theerrors that occur in the output of ner models are.
performed.
we divide the error into type error andboundary error.
the type error is deﬁned as thatthe boundary of the predicted entity is correct butthe predicted type is wrong, and the other errorsare classiﬁed as boundary errors.
the statistics ofboundary errors and type errors are shown in ta-ble 5. from the table, we ﬁnd that: (1) errors aremainly caused by mistakenly locating boundariesof entities.
therefore, discovering entity bound-aries is the main challenge in chinese ner.
(2)leveraging the acoustic modality can effectivelyreduce boundary errors.
in nested ner, the num-ber of errors decreases from 906 to 848, totallyowning to the reduction of boundary errors, but thenumber of type errors increases, which may be dueto overﬁtting or some random factors..7.4 case studies.
to visually show the effectiveness of introducingthe acoustic modality, case studies on compar-ing the output of bert-crf and bert-m3t arepresent in table 4. from the table, we can observethat: without the acoustic modality, bert-crf isprone to locate some ambiguous entities mistak-enly, such as “沙特阿拉伯” (saudi arabia), “首都机场”(capital airport), “国际米兰” (inter milan).
but armed with the acoustic modality, these entitiesare located with complete accuracy.
in the last case,bert-m3t makes some mistakes.
we listen to thecorresponding audio clip and ﬁnd that there is along pause between “毕尔” and “巴鄂”..28148 conclusion and future work.
in this paper, we explore chinese multimodal nerwith both textual and acoustic contents.
to achievethis, we construct a large-scale manually annotatedmultimodal ner dataset，named cnerta.
basedon this dataset, we establish a family of baselinemodels.
furthermore, we propose a simple multi-modal multitask method by introducing a speech-to-text alignment auxiliary task.
through extensiveexperiments, we prove that chinese ner modelscan beneﬁt from introducing the acoustic modalityand our proposed model is effective..in the future, we are interested in mining otherinformation contained in speech, such as rhythm,emotion, pitch, accent and stress, to boost ner.
meanwhile, we will also work on designing somespeech-text pretraining tasks for building a large-scale pretrained model with multimodal capabili-ties..acknowledgments.
we thank the anonymous reviewers for their in-sightful comments.
we also thank zhixing tian,tao wang and ye bai for helpful suggestions..this work is supported by the national keyresearch and development program of chinathe national2020aaa0106400),(grant no.
natural science foundation of china (grant no.
61922085 and grant no.
61976211) and bei-jing academy of artiﬁcial intelligence (grant no.
baai2019qn0301)..references.
alan akbik, duncan blythe, and roland vollgraf.
2018. contextual string embeddings for sequencein proceedings of the 27th internationallabeling.
conference on computational linguistics..eiji aramaki, yasuhide miura, masatsugu tonoike,tomoko ohkuma, hiroshi mashuichi, and kazuhikoohe.
2009. text2table: medical text summa-rization system based on named entity recognitionin proceedings of theand modality identiﬁcation.
bionlp 2009 workshop.
association for computa-tional linguistics..omer arshad,.
ignazio gallo, shah nawaz, andalessandro calefati.
2019. aiding intra-text repre-sentations with visual context for multimodal namedin 2019 international confer-entity recognition.
ence on document analysis and recognition (ic-dar)..meysam asgari-chenaghlu, m reza feizi-derakhshi,leili farzinvash, and cina motamed.
2020. a.multimodal deep learning approach for named en-tity recognition from social media.
arxiv preprintarxiv:2001.06888..jimmy lei ba, jamie ryan kiros, and geoffrey e hin-arxiv preprint.
ton.
2016. layer normalization.
arxiv:1607.06450..timothy baldwin, marie catherine de marneffe,bo han, young-bum kim, alan ritter, and weixu.
2015. shared tasks of the 2015 workshop onnoisy user-generated text: twitter lexical normaliza-tion and named entity recognition.
in proceedingsof the workshop on noisy user-generated text..hui bu, jiayu du, xingyu na, bengu wu, and haozheng.
2017. aishell-1: an open-source mandarinspeech corpus and a speech recognition baseline.
in2017 20th conference of the oriental chapter of theinternational coordinating committee on speechdatabases and speech i/o systems and assessment(o-cocosda)..pengfei cao, yubo chen, kang liu, jun zhao, andshengping liu.
2018. adversarial transfer learn-ing for chinese named entity recognition with self-in proceedings of the 2018attention mechanism.
conference on empirical methods in natural lan-guage processing..shuguang chen, gustavo aguilar, leonardo neves,and thamar solorio.
2020. a caption is worthinvestigating image captionsa thousand images:arxivfor multimodal named entity recognition.
preprint arxiv:2010.12712..jason p.c.
chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
trans-actions of the association for computational lin-guistics..jacob cohen.
1960. a coefﬁcient of agreement fornominal scales.
educational and psychological mea-surement..nigel collier and jin-dong kim.
2004..introductionto the bio-entity recognition task at jnlpba.
inproceedings of the international joint workshop onnatural language processing in biomedicine and itsapplications (nlpba/bionlp)..yiming cui, wanxiang che, ting liu, bing qin, shi-jin wang, and guoping hu.
2020. revisiting pre-trained models for chinese natural language process-in findings of the association for computa-ing.
tional linguistics: emnlp 2020..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers)..2815shizhe diao, jiaxin bai, yan song, tong zhang, andyonggang wang.
2020. zen: pre-training chinesetext encoder enhanced by n-gram representations.
infindings of the association for computational lin-guistics: emnlp 2020..ruixue ding, pengjun xie, xiaoyan zhang, wei lu,linlin li, and luo si.
2019. a neural multi-digraphmodel for chinese ner with gazetteers.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics..george doddington, alexis mitchell, mark przybocki,lance ramshaw, stephanie strassel, and ralphweischedel.
2004. the automatic content extraction(ace) program – tasks, data, and evaluation.
in pro-ceedings of the fourth international conference onlanguage resources and evaluation (lrec’04)..linhao dong, shuang xu, and bo xu.
2018. speech-transformer: a no-recurrence sequence-to-sequencemodel for speech recognition.
in 2018 ieee interna-tional conference on acoustics, speech and signalprocessing (icassp).
ieee..alex graves, santiago fern´andez, faustino gomez,and j¨urgen schmidhuber.
2006.connectionisttemporal classiﬁcation:labelling unsegmented se-quence data with recurrent neural networks.
in pro-ceedings of the 23rd international conference on ma-chine learning..tao gui, yicheng zou, qi zhang, minlong peng, jin-lan fu, zhongyu wei, and xuanjing huang.
2019.a lexicon-based graph neural network for chinesein proceedings of the 2019 conference onner.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp)..sepp hochreiter and j¨urgen schmidhuber.
1997. long.
short-term memory.
neural computation..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..j-d kim, tomoko ohta, yuka tateisi, and jun’ichi tsu-jii.
2003. genia corpus—a semantically annotatedcorpus for bio-textmining.
bioinformatics..in proceedings of the 2016 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies..gina-anne levow.
2006. the third international chi-nese language processing bakeoff: word segmen-in proceed-tation and named entity recognition.
ings of the fifth sighan workshop on chinese lan-guage processing..mohan li, min liu, and hattori masanori.
2019. end-to-end speech recognition with adaptive computa-in icassp 2019-2019 ieee interna-tion steps.
tional conference on acoustics, speech and signalprocessing (icassp)..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020. flat: chinese ner using ﬂat-latticein proceedings of the 58th annualtransformer.
meeting of the association for computational lin-guistics..wei liu, tongge xu, qinghua xu, jiayu song, andyueran zu.
2019a.
an encoding strategy basedin pro-word-character lstm for chinese ner.
ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019b.
roberta: a robustly optimized bert pretraining ap-proach..di lu, leonardo neves, vitor carvalho, ning zhang,and heng ji.
2018. visual attention model for nametagging in multimodal social media.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics..ruotian ma, minlong peng, qi zhang, zhongyu wei,and xuanjing huang.
2020. simplify the usage ofin proceedings of thelexicon in chinese ner.
58th annual meeting of the association for compu-tational linguistics..xuezhe ma and eduard hovy.
2016..end-to-endsequence labeling via bi-directional lstm-cnns-in proceedings of the 54th annual meetingcrf.
of the association for computational linguistics..henry k kwok and douglas l jones.
2000. improvedinstantaneous frequency estimation using an adap-tive short-time fourier transform.
ieee transactionson signal processing..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems..diego moll´a, menno van zaanen, and daniel smith.
2006. named entity recognition for question an-in proceedings of the australasian lan-swering.
guage technology workshop 2006..guillaume lample, miguel ballesteros, sandeep sub-ramanian, kazuya kawakami, and chris dyer.
2016.neural architectures for named entity recognition..seungwhan moon, leonardo neves, and vitor car-valho.
2018. multimodal named entity recognitionfor short social media posts.
in proceedings of the.
28162018 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long pa-pers)..vinod nair and geoffrey e. hinton.
2010. rectiﬁedlinear units improve restricted boltzmann machines.
in proceedings of the 27th international conferenceon international conference on machine learning..nanyun peng and mark dredze.
2016a..improvingnamed entity recognition for chinese social mediawith word segmentation representation learning.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 2:short papers)..nanyun peng and mark dredze.
2016b..improvingnamed entity recognition for chinese social mediawith word segmentation representation learning.
inproceedings of the 54th annual meeting of the as-sociation for computational linguistics (volume 2:short papers)..peng qi, yuhao zhang, yuhui zhang, jason bolton,and christopher d. manning.
2020.stanza: apython natural language processing toolkit for manyin proceedings of the 58th an-human languages.
nual meeting of the association for computationallinguistics: system demonstrations..alec radford, karthik narasimhan, tim salimans, andimproving language under-.
ilya sutskever.
2018.standing by generative pre-training.
in preprint..for chinese named entity recognition via collabora-tive graph network.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp)..lin sun, jiquan wang, yindu su, fangsheng weng,yuxuan sun, zengwei zheng, and yuanyi chen.
2020. riva: a pre-trained tweet multimodal modelbased on text-image relation for multimodal ner.
in proceedings of the 28th international conferenceon computational linguistics..zhengkun tian, jiangyan yi, ye bai, jianhua tao,shuai zhang, and zhengqi wen.
2020.syn-chronous transformers for end-to-end speech recog-in icassp 2020-2020 ieee internationalnition.
conference on acoustics, speech and signal pro-cessing (icassp)..yao-hung hubert tsai, shaojie bai, paul pu liang,j. zico kolter, louis-philippe morency, and ruslansalakhutdinov.
2019. multimodal transformer forunaligned multimodal language sequences.
in pro-ceedings of the 57th annual meeting of the associa-tion for computational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allin advances in neural information pro-you need.
cessing systems..lance ramshaw and mitch marcus.
1995. text chunk-in third.
ing using transformation-based learning.
workshop on very large corpora..christopher walker, stephanie strassel, julie medero,and kazuaki maeda.
2004. ace 2005 multilingualtraining corpus.
in ldc..sourabh ravindran, c demirogulu, and david v an-derson.
2003. speech recognition using ﬁlter-bankin the thrity-seventh asilomar confer-features.
ence on signals, systems & computers, 2003..changhao shan, chao weng, guangsen wang, dan su,min luo, dong yu, and lei xie.
2019. compo-nent fusion: learning replaceable language modelcomponent for end-to-end speech recognition sys-tem.
in icassp 2019-2019 ieee international con-ference on acoustics, speech and signal processing(icassp)..pontus stenetorp, sampo pyysalo, goran topi´c,tomoko ohta, sophia ananiadou, and jun’ichi tsu-jii.
2012. brat: a web-based tool for nlp-assistedin proceedings of the demonstra-text annotation.
tions session at eacl 2012..jana strakov´a, milan straka, and jan hajic.
2019. neu-ral architectures for nested ner through lineariza-tion.
in proceedings of the 57th annual meeting ofthe association for computational linguistics..dianbo sui, yubo chen, kang liu, jun zhao, andshengping liu.
2019. leverage lexical knowledge.
ralph weischedel, sameer pradhan, lance ramshaw,martha palmer, nianwen xue, mitchell marcus,ann taylor, craig greenberg, eduard hovy, robertontonotes release 4.0.belvin, et al.
2011.ldc2011t03, philadelphia, penn.
: linguistic dataconsortium..fangzhao wu, junxin liu, chuhan wu, yongfenghuang, and xing xie.
2019. neural chinese namedentity recognition via cnn-lstm-crf and joint trainingin the world wide webwith word segmentation.
conference..yan xu, yining wang, tianren liu, jiahua liu, yubofan, yi qian, junichi tsujii, and eric i chang.
2013.joint segmentation and named entity recognition us-ing dual decomposition in chinese discharge sum-maries.
journal of the american medical informat-ics association..mengge xue, bowen yu, tingwen liu, yue zhang,erli meng, and bin wang.
2020. porous latticetransformer encoder for chinese ner.
in proceed-ings of the 28th international conference on com-putational linguistics..2817jianfei yu, jing jiang, li yang, and rui xia.
2020.improving multimodal named entity recognition viaentity span detection with uniﬁed multimodal trans-former.
in proceedings of the 58th annual meetingof the association for computational linguistics..dmitry zelenko, chinatsu aone,.
and anthonyrichardella.
2003. kernel methods for relation ex-traction.
journal of machine learning research..qi zhang, jinlan fu, xiaoyu liu, and xuanjing huang.
2018. adaptive co-attention network for named en-in proceedings of thetity recognition in tweets.
aaai conference on artiﬁcial intelligence..yue zhang and jie yang.
2018. chinese ner usingin proceedings of the 56th annuallattice lstm.
meeting of the association for computational lin-guistics..2818