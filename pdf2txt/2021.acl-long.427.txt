shortformer: better language modeling using shorter inputs.
oﬁr press1,2 noah a. smith1,3 mike lewis2.
1paul g. allen school of computer science & engineering, university of washington2facebook ai research3allen institute for aiofirp@cs.washington.edu.
abstract.
increasing the input length has been a driverof progress in language modeling with trans-formers.
we identify conditions where shorterinputs are not harmful, and achieve perplex-ity and efﬁciency improvements through twonew methods that decrease input length.
first,we show that initially training a model onshort subsequences before moving on to longerones both reduces overall training time and,surprisingly, substantially improves perplex-ity.
second, we show how to improve the ef-ﬁciency of recurrence methods in transform-ers, which let models condition on previouslyprocessed tokens when generating sequencesthat exceed the maximal length the transformercan handle at once.
existing methods re-quire computationally expensive relative posi-tion embeddings; we introduce a simple alter-native of adding absolute position embeddingsto queries and keys instead of to word embed-dings, which efﬁciently produces superior re-sults.
we show that these recurrent modelsalso beneﬁt from short input lengths.
com-bining these techniques speeds up training bya factor of 1.65, reduces memory usage, andsubstantially improves perplexity on wikitext-103, without adding any parameters.1.
1.introduction.
scaling up transformer (vaswani et al., 2017) lan-guage models (radford et al., 2019; lewis et al.,2019; raffel et al., 2019; brown et al., 2020) hasbeen an important driver of progress in nlp.
lan-guage models require data to be segmented intosubsequences for both training and inference: mem-ory constraints limit a language model to handlingat most a few thousand tokens at once, while manytraining and evaluation datasets are much longer..1our code is available at https://github.com/.
ofirpress/shortformer.
recent work focuses on increasing the length of in-put subsequences, which determines the maximumnumber of tokens a model can attend to (baevskiand auli, 2018; sukhbaatar et al., 2019; kitaevet al., 2020; roy et al., 2020)..we challenge the assumption that longer inputsubsequences are always better by showing thatexisting transformers do not always effectively usethem.
we then introduce new methods based onshorter input subsequences that improve runtime,memory efﬁciency, and perplexity..we ﬁrst investigate how input subsequencelength affects transformer language models (§3).
na¨ıve evaluation—where we split a large evalua-tion set into multiple nonoverlapping subsequences,each evaluated independently—initially supportsthe commonly-held belief that models that trainand do inference on longer subsequences achievebetter perplexity (table 1, col. 3)..however, when we evaluate each model witha sliding window (baevski and auli, 2018), out-putting one token at a time using the maximalamount of context, we ﬁnd—surprisingly—thatmodels using subsequences exceeding 1,024 to-kens do not further improve performance (table 1,col. 5)..we conclude that the performance gains (usingna¨ıve evaluation) of models that use longer sub-sequences occur not only because of their bettermodeling ability, but partly because they divide theevaluation set into longer subsequences.
this di-vision helps because of an issue we call the earlytoken curse: by default, early tokens in a subse-quence will have short histories to attend to.
usinglonger subsequences means fewer tokens will suf-fer from the early token curse.
for example, whenusing inputs of length 1,024, about 94% of tokensget to attend to more than 64 preceding tokens.
ifwe use inputs of length 128, only 50% of tokensget to attend to 64 or more preceding tokens..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5493–5505august1–6,2021.©2021associationforcomputationallinguistics5493based on this analysis, we explore how to im-prove models by using shorter inputs.
we introducetwo techniques..for two distinct tasks: generation and evaluation.
inorder to deﬁne these tasks, we ﬁrst deﬁne nonover-lapping and sliding window inference..staged training (§4) first, we show that ini-tially training on shorter subsequences (before mov-ing to longer ones) leads not only to much fasterand more memory-efﬁcient training, but it surpris-ingly also greatly improves perplexity, suggestingthat longer inputs are harmful early in training..position-infused attention (§5) second, weconsider a natural way to avoid the early tokencurse during training and inference: attending tocached representations from the previously evalu-ated subsequence (dai et al., 2019).
this approachinterferes with conventional absolute position em-beddings in a way that forced dai et al.
to use rela-tive position embeddings, which are computation-ally expensive.
we introduce a fast, simple alter-native: instead of adding absolute position embed-dings to word embeddings—thereby entangling aword’s content and positional information—we addthem to the keys and queries in the self-attentionmechanism (but not to the values).
this does notincrease parameter count or runtime.
token repre-sentations can then be cached and reused in subse-quent computations.
we show that when using thismethod, shorter subsequence models outperformlonger ones..finally, we show additive gains from combin-ing staged training and position-infused attention(shortformer, §6), resulting in a model that trainsmuch quicker and achieves better perplexity onwikitext-103.
we also show that these resultstransfer to language modeling on the toronto bookcorpus (§a.5, appendix)..2 background and experimental setup.
transformer language models map a list of tokensxn−l:n−1 to a probability distribution over the nexttoken xn.
we refer to the list of tokens as the cur-rent input subsequence (whose length is l).
causalmasking lets us make l predictions at once, withthe prediction for token i + 1 conditioned on theith token and all previous inputs xn−l:i−1, but noton future inputs.
we deﬁne the number of tokensthe model can attend to at each timestep as its ef-fective context window.
note that l is not to beconfused with the (typically much greater) lengthof a training or evaluation dataset..during inference, language models can be used.
nonoverlapping inference to evaluate a stringlonger than l, we can evaluate each subsequenceof l tokens independently.
this fast approach iscommonly used during training; if used, tokens inone subsequence cannot condition on those in theprevious subsequence, giving rise to the early tokencurse discussed in §1.
see figure 1(a)..sliding window inference an alternative to theabove is to use a sliding window during inference.
here, we choose a stride s between 1 and l − 1and advance the window by s tokens after eachforward pass.2 this means that l − s tokens fromthe previous block are re-encoded, and only s newtokens are outputted.
the advantage is that alloutputs in each subsequence after the ﬁrst haveat least l − s previous tokens to condition on.
however, since tokens must be re-encoded multipletimes, this approach is much slower.
when s = 1,we output one token every inference pass, eachusing the maximal context window, but this is theslowest approach.
see figure 1(b)..minimal and maximal effective context win-dow sizesin the nonoverlapping approach, themin.
and max.
effective context window sizes are1 and l, respectively.
in the sliding window ap-proach, the max.
context window size is still l, butthe min.
context window size is now l − s + 1..evaluation vs. generation in evaluation, amodel assigns a perplexity score to a given se-quence.
evaluation is done using either nonover-lapping inference or with a sliding window of anystride; since we already have the target sequencewe can simultaneously make predictions for multi-ple timesteps using causal masking.
in generation,a model generates a new sequence, as in demonstra-tions of gpt-3 (brown et al., 2020).
generation isdone only with a sliding window with stride s = 1,which we refer to as token-by-token generation.
during generation, we append to the input a singlenew token, get a prediction from the model aboutthe next token (e.g., using beam search or pickingthe token with the highest probability); the processis then repeated.3.
2nonoverlapping inference can be viewed as sliding win-.
dow inference with stride l..3in this paper we do not consider open-ended generation;we generate the dev.
set, and for next-token prediction we.
5494(a).
(b).
(c).
a1 b2 c3.
d1 e2 f3.
a1 b2 c3.
b1 c2 d3.
c1 d2 e3.
d1 e2 f3.
a1 b2 c3.
d4 e5 f6.
figure 1: language model modes for generating or evaluating 6 tokens (a, b, .
.
.
, f ) when subsequence lengthl = 3. the numbers denote the position embeddings (p.e.).
(a) nonoverlapping (§2).
(b) sliding window, strides = 1 .
here, after the ﬁrst inference pass we ignore all outputs other than the last (§2).
(c) caching (§5.2) whereeach subsequence attends to representations of the previous one.
(in the next iteration, tokens d, e and f becomethe cache, with p.e.
1, 2 and 3, the three new tokens get p.e.
4, 5, and 6.).
experimental setup our baseline is the baevskiand auli (2018) model, henceforth b&a, trainedand evaluated on wikitext-103 (merity et al.,2016).
we use this baseline because of its promi-nent role in recent language modeling develop-ments (khandelwal et al., 2020; press et al., 2020).
the training set contains 103.2 million tokens fromenglish wikipedia.
the b&a model has 16 trans-former layers of dimension 1,024, with 8 headsin each self-attention sublayer, and feedforwardsublayers with an inner dimension of 4,096. thismodel ties the word embedding and softmax matri-ces (press and wolf, 2017; inan et al., 2017) anduses sinusoidal position embeddings.
it has a sub-sequence length of 3,072 tokens and achieves aperplexity of 18.65 ± 0.24 (std.
dev.)
on the devel-opment set.
in our experiments, other than varyingthe subsequence length, we modify no other hyper-parameters, including the random seed and numberof training epochs (205)..3 how does context window size affect.
transformers?.
segmenting a corpus into subsequences results indifferent effective context windows for differenttimesteps depending on where they fall in a seg-ment.
subsequence length l is an upper boundon the effective context window at each timestep.
when making the ﬁrst prediction, the model attendsonly to the ﬁrst input token.
when making the sec-ond prediction, the model attends to the ﬁrst twoinputs, and so on, up to the lth timestep where themodel can attend to all input tokens when makingthe lth prediction..3.1 context window size matters.
table 1 explores the effect of subsequence lengthin the b&a model on training runtime and on dev.
set perplexity and runtime.4 we ﬁx the number.
use the ground truth token.
this has the same complexity assampling the token with the highest probability..4for consistency, throughout the paper we run inferencewith a batch size of one.
this causes models shorter than.
train.
inference.
nonoverlapping.
sliding window(token-by-token).
speed ↑ ppl ↓ speed ↑.
ppl ↓.
speed ↑.
28.3k28.5k28.9k28.1k26.1k22.9k18.4k13.9k.
35.3728.0323.8121.4520.1019.1119.0518.65.
2.4k4.8k9.2k14.8k18.1k18.3k17.1k14.7k.
24.9821.4719.7618.8618.4117.9718.1417.92.
746970633718115.subseq.
length.
3264128256512102415363072.table 1: subsequence length’s effects on performanceof the b&a model on the wikitext-103 dev.
set.
thebaseline is the last row.
token-by-token inf.
was com-puted with a sliding window stride s = 1 to outputone token at a time; see §2.
we measure speed intok./sec.
per gpu and use a batch size of 1 for inf..of tokens in each batch to 9,216 but vary the sub-sequence length l and batch size (so the productof the batch size and subsequence length remainsat 9,216).
we report results for both nonoverlap-ping inference and sliding window inference withstride s = 1, which generates only one new tokenper forward pass; it thus has the maximal effec-tive context window for each generated token.
weﬁnd that performance increases as s decreases un-til it reaches a peak and then stops improving (notshown in table 1).5.we derive the following conclusions:training on long sequences is expensive.
models trained on subsequences of length 256 aretwice as fast as models trained on subsequences of3,072 tokens, but gains for even shorter lengths arenegligible (tab.
1, col. 2)..long subsequence lengths can improve re-sults.
when using the na¨ıve approach, nonover-.
l = 512 to run slowly (in n.o.
eval.
), although during batchedn.o.
eval.
they are slightly faster than the l = 512 model..5for example, the l = 3,072 model’s performance peakedat s = 512 (used in baevski and auli (2018)) and thenstopped improving.
thus, the result shown in table 1 forthat model with s = 1 can also be achieved with s = 512even though that runs 500 times faster, at 2.5k tok./sec..5495lapping evaluation, we see a monotonic decrease indev.
perplexity when increasing l (tab.
1, col. 3).
increasing the minimum effective contextwindow size is more important than increasingthe maximum one.
using a sliding window fortoken-by-token evaluation substantially improvesresults for all models (tab.
1, col. 5).
here, wesee negligible improvement between the modelstrained with subsequence lengths of 1,024 and3,072 tokens (0.05 perplexity).
this approach im-proves results by increasing the minimum amountof context available at each timestep which indi-cates that long contexts may not be beneﬁcial totransformer models, but very short contexts areharmful.
however, sliding window inference canbe expensive since each token is encoded manytimes.
for example, token-by-token inference forthe l = 3,072 model is almost 300 times slowerthan nonoverlapping inference..4 training subsequence length.
§3 results show that models trained on shorter sub-sequences can be effective at test time, and aremuch faster to train.
we further explore this below..4.1 staged training.
we propose a two-stage training routine that ini-tially uses short input subsequences followed bylong subsequences.6 this method was previouslyapplied to speed up the training of bert (devlinet al., 2019), but we show that it also improvesperplexity..we use sinusoidal position embeddings; learnedposition embeddings, which we do not consider,create a dependency between the parameterizationand subsequence length.
in our experiments, weneither modify nor reset the state of the optimiza-tion algorithm between the two stages..4.2 experiments.
our experimental setup is described in §2.
wedo not change any hyperparameters other than re-ducing subsequence length while correspondinglyincreasing batch size to keep the number of tokensper batch constant.
as in the baseline, all modelsare trained for 205 epochs..all models are trained in two stages; the secondstage always uses a subsequence length of 3,072,.
32.
512.
128.initial stage subseqence length1024 15362566417.94 17.57 17.58 18.19 18.06 18.20 18.772517.81 17.59 17.52 18.08 18.01 18.14 18.62507517.93 17.61 17.55 18.01 18.05 18.03 18.57100 18.14 17.67 17.62 18.00 18.10 18.00 18.51125 18.61 17.88 17.70 18.00 18.13 17.98 18.49150 19.45 18.37 17.98 18.01 18.15 18.00 18.49175 21.16 19.51 18.57 18.23 18.20 18.08 18.57200 35.38 28.03 23.80 21.45 19.63 18.56 18.84.shcopeegatslaitini.table 2: each model’s perplexity at the end of training(dev.
set, nonoverlapping eval.).
all models have a sub-sequence length of 3,072 tokens at the end of training.
the b&a baseline achieves 18.65 ± 0.24 perplexity..since that lead to the best performance (discussedat end of this subsection)..appendix table 6 shows the time each trainingroutine takes to match the baseline model’s per-formance on the validation set of wikitext-103.7many conﬁgurations match this performance inless than half the time it takes to train the baselineitself; some reach baseline performance in only37% of the time needed to train the baseline..although all models take less time to train thanthe baseline, table 2 shows that many outper-form it.
for example, the best model—trainedwith subsequence length l = 128 until epoch 50—outperforms the baseline by 1.1 perplexity despitecompleting training in 87% of the time the baselinetakes to do so.
the model that trains with l = 128until epoch 100 achieves similarly strong results(17.62 perplexity) and ﬁnishes training in 74% ofthe time it takes the baseline.8.
these results are very robust to the choice ofinitial stage subsequence length and number ofepochs.
table 2 shows that all models with aninitial stage of l = 1,024 tokens or less that switchto the second stage at epoch 125 or before beat thebaseline by a large margin at the end of training.
additionally, appendix table 6 shows that thosemodels match the baseline’s perplexity in at most71% of the time it takes to train the baseline..when we use nonoverlapping evaluation, theb&a baseline obtains 18.65 perplexity on thedevelopment set; our best model obtains 17.52.when we use sliding window evaluation (followingbaevski & auli, we use stride s = 512), our best.
6curriculum learning (bengio et al., 2009) trains on easierinputs before progressing to harder ones.
our approach doesnot change the order in which the training examples are givento the model, but instead modiﬁes their lengths..7table 7 in the appendix shows the epoch at which every.
model matched the baseline’s performance..8table 8 in the appendix shows the total time it took to.
train each model..5496(a).
(b).
figure 2: inputs to the self-attention sublayer, conventionally (left) and with position-infused attention (right), forl = 3, at timestep 3. the numbers denote the position embeddings..model obtains 16.89 perplexity, a large improve-ment on the 17.92 b&a result in that setting.
on thetest set, using the same sliding window evaluation,our model obtains 17.56 perplexity, a substantialgain over the baseline’s 18.70 test-set perplexity.
appendix table 10 shows that our best model usesalmost ﬁve times less memory during the ﬁrst stagethan the baseline..we also found that setting l to less than 3,072tokens in the second stage degraded performance.
(appendix table 9 shows staged training resultswith an initial stage length of 128 for 50 epochs(as in the best model) and varying lengths for thesecond stage.
we found this to also be true for otherinitial stage lengths and epochs.)
unlike results intable 1, where we show that models with l largerthan 1,024 do not substantially improve token-by-token generation perplexity, models trained usingstaged training improve when given longer inputs(appendix table 9).
further, we explored usingmore than two stages (up to six), but this did notoutperform our two-stage curriculum..finally, appendix a.5 shows that staged train-ing substantially improves on the toronto bookcorpus (zhu et al., 2015)..5 repositioning position embeddings.
sliding window inference substantially improvesperformance by increasing the minimum effectivecontext window size.
but it is very slow.
we couldsolve this by letting the model attend to representa-tions of the previous subsequence during inferenceon the current one..in this case, the same token representationswould be used in different positions since a tokengenerated near the end of one subsequence wouldbe cached and reused near the start of the next one.
however, transformer model representations entan-gle positional and content information, so a cached.
token representation would encode an incorrect po-sition when reused in a new position..transformerxl (dai et al., 2019) uses relativeposition embeddings to solve this problem.
how-ever, that approach is slower and uses more param-eters and memory than the baseline transformer.9we solve this using no extra parameters, mem-ory, or runtime.
we also show that our methodcan use much shorter input subsequences and stillachieve superior performance..transformer language models the baselinetransformer lm, given a token list t of length land a tensor p containing the ﬁrst l position em-beddings, produces l next-token predictions usingthe following procedure:.
1. embed each token in t , producing tensor x..2. add the position embedding of each index to.
the token at that index: x = x + p..3. feed x through each transformer layer.
eachthetransformer layer is invoked as follows:self-attention(key=x, query=x, value=x).
self-attention.
sublayer.
in.
4. transform the outputs of the last transformerlayer using the softmax layer, giving l next-token probability distributions..5.1 position-infused attention (pia).
we propose to let the model reuse previous out-puts by making each output contain no explicitpositional information.
to do this, we modify the.
9the self-attention coefﬁcients between q queries and kkeys in transformerxl are the sum of two dot products of sizeq · k; the unmodiﬁed attention sublayer and our pia methodboth compute only one dot product of size q ·k.
we also bench-marked the transformerxl model using its publicly releasedcode and found that their relative position embeddings slowinference by 22% and require 26% more parameters than theirimplementation of the unmodiﬁed self-attention sublayer..5497satkvqself-attentioncatthevqself-attentionthe1cat2sat3kmodel so that it does not add position embeddingsat the beginning of the computation (step 2), butrather adds them to the query and key vectors ateach layer (but not to the value vectors).
the out-puts at each layer are the transformed, weightedsums of the value vectors, and, since the value vec-tors in our model do not contain explicit positionalinformation, the outputs also do not..formally, steps 1 and 4 do not change, step 2is omitted, and step 3 is modiﬁed to invoke theself-attention sublayer as follows:.
sequence tasks such as sentence-level translation,where sequence lengths are short..most language models, including b&a, train ontheir data as nonoverlapping subsequences.
thismeans that training subsequences can be shufﬂed ateach epoch and consumed in random order.
how-ever, when using pia, we would like the cache tocontain the previous subsequence.
we thereforedo not shufﬂe the data, making the cached subse-quence the previously occurring one..figure 1(c) depicts training with a cache that con-tains representations of the previous subsequence..self-attention(key=x+p, query=x+p,value=x).
5.3 experiments.
figure 2 (b) depicts this method..although pia sublayer outputs contain no ex-plicit positioning information, the attention mech-anism can still compute position-dependent out-puts because positional information is added tothe query and key vectors.
our method is imple-mentable in just a few lines of code..5.2 pia enables caching.
in the unmodiﬁed transformer, to generate a stringwhose length exceeds l, it would have to be splitinto separate subsequences, and the model wouldbe unable to attend to the previous subsequencewhen generating the current one..using pia, we can store and attend to represen-tations of the previous subsequence since they nolonger contain any explicit positioning information.
therefore, all our pia models use a cache, whererepresentations from the previous forward pass arestored and attended to in the next forward pass..caching makes generation faster.
the com-plexity of the attention mechanism is o(q·k) whereq is the number of queries (outputs) and k is thenumber of key-value pairs (inputs).
to generatea sequence whose length exceeds l using token-by-token generation in the unmodiﬁed transformer(with subsequence length l), attention takes o(l2)time (since there are l queries and l keys).
usingpia and caching, we can reuse l − 1 of the pre-vious outputs at every layer.
thus, our attentionsublayer takes o(l) time (because now there is asingle query and l keys)..our approach is useful in scenarios where weneed to evaluate or generate sequences that arelonger than the model’s subsequence length.
there-fore, it would not be applicable to sequence-to-.
we use the experimental setup described in §2..the b&a baseline achieves 18.65 on the devel-opment set.
we train two additional baselines, theﬁrst uses pia without caching and the second usescaching but no pia.
if just pia is used (withoutcaching), performance degrades to 19.35 perplex-ity, but the model’s speed and memory usage do notchange.
using caching without pia severely hurtsperformance, obtaining 41.59 perplexity.
disablingdata shufﬂing in the pia-only model achieves simi-lar performance to that model when it does use datashufﬂing, at 19.44 perplexity.
not shufﬂing the datais necessary for recurrent-style training that cachespreviously computed subsequence representations.
our next experiments use the recurrent-styletraining of dai et al.
(2019), where we receive lnew tokens at every training iteration and attend tol(cid:48) cached representations (of the subsequence oftokens that came immediately prior to the l new to-kens).
as before, we output l predictions at everytraining iteration.
this means that the maximal andminimal effective context window sizes are l(cid:48) + land l(cid:48) + 1, respectively..in all our models with pia and caching, we setl(cid:48) = l because a manual exploration of differentmodels where l(cid:48) (cid:54)= l did not yield better results.
table 3 compares the results of our modelsthat use pia and caching to the baseline on thewikitext-103 dev.
set.
evaluation and generationspeeds are shown in the nonoverlapping (n.o.)
andsliding window (s.w., with stride s = 1) speedcolumns, respectively.10 unlike in the baseline,token-by-token evaluation in our model achievesthe same perplexity as nonoverlapping evaluation.
10note that baevski and auli (2018) show that the baselinemodel can also achieve 17.92 during s.w.
evaluation, whens = 512, with a speed of 2.5k tokens per second..5498train.
inference.
speed ↑.
speed ↑.
ppl ↓.
n.o..s.w..22.0k23.8k24.4k23.5k21.5k17.6k16.6k12.9k.
13.9k.
20.5319.0718.3717.9217.8518.1618.1919.11.
18.6517.92.
2.0k4.1k7.9k12.8k14.5k13.8k13.9k7.9k.
14.7k-.
4951504846433934.
-5.subseq.
length.
326412825651276810241536.baseline(3072).
table 3: dev.
perplexity and speed for pia modelstrained with different subsequence lengths (l).
piamodels attend to l new and l cached tokens at each in-ference pass.
n.o.
is nonoverlapping eval.
; s.w.
is slid-ing window eval., where we always use s = 1 (token-by-token) here.
the baseline is evaluated with bothevaluation methods.
we measure speed in tok./sec.
pergpu and use a batch size of 1 for inference..since in both cases, the predictions for each in-put subsequence are conditioned not only on thecurrent input, but also on the previous input, mak-ing the context window the same in both inferencemodes (in both cases, at every timestep, the contextwindow is all tokens up to that timestep)..table 3 shows that as we increase subsequencelength, perplexity improves, peaking at 512 beforestarting to degrade.
our best model obtains 17.85perplexity, which is multiple standard deviationsbetter than the baseline (18.65, n.o.).
table 5 in§6 shows a similar gain on the test set.
the bestmodel runs 1% slower than the baseline during n.o.
eval.
(since caching reduces the speed gain fromsmaller attention matrices in this mode).
table 10(appendix) shows that it uses less than half of thememory the baseline does during training.
our bestmodel trains 55% faster than the baseline..our best model, with subsequence length 512,has attention matrices of size 512 · 1,024 (since wehave 512 queries—one per every new token—and1,024 keys and 1,024 values—one per every newtoken and every cached token).
in the baseline, allattention matrices are of size 3,072 · 3,072..caching previously computed representationslets us do token-by-token generation efﬁcientlywhen generating more than l tokens.
our modelis nine times faster than the baseline at token-by-token generation even as it achieves better perplex-ity and uses much less memory (tab.
3, col. 5)..first stagesubseq.
length.
train.
inference.
speed ↑.
ppl ↓.
3264128256.pia + cache w/ostaged training.
21.6k22.6k22.9k22.5k.
17.6617.5617.4717.50.
21.5k.
17.85.table 4: dev.
perplexity for models that use pia,caching, and staged training (with ﬁnal subseq.
lengthof 512).
we measure speed in tok./sec.
per gpu.
evalu-ation speed is the same for all models, at 14.5k tok./sec..pia and caching also greatly improve perplex-ity on the toronto book corpus; see a.5 in theappendix..6 shortformer results.
to assess whether the gains from staged training,pia and caching are additive, we take our bestcaching pia model, with subsequence length 512,and apply staged training to it, training it with asubsequence length of between 32 to 256 for theﬁrst half of training.11 table 4 shows the results.
as in §4.2, where staged training was applied tothe unmodiﬁed baseline, the results are very robustto the choice of initial stage subsequence length,with all the different choices improving perplexityover the model that does not use staged training..the best model (with initial subsequence length128), which we call shortformer, achieves 17.47dev.
set perplexity and trains 65% faster than thebaseline.
since its attention matrices are of dimen-sion 512 · 1,024 (the baseline’s are 3,072 · 3,072),our model uses less memory (§a.4, appendix).
ithas the same number of parameters as the baseline.
figure 3 (appendix) compares our best modelsusing each method we presented (and their com-bination) to the baseline.
it shows that combin-ing caching, pia and staged training (shortformer)yields the quickest training and best perplexitywhen using nonoverlapping evaluation.
evaluationspeed is similar for all of these models..finally, table 5 compares our best models on the.
test set of wikitext-103 to the state of the art.12.
shortformer is almost twice as fast to train asthe baseline and achieves superior results.
like the.
11we picked 50% of epochs as the length of the ﬁrst stagesince that produced near-optimal results at a fast speed in §4.
12we benchmarked speed, on v100 gpus, for all models.
that had publicly available code..5499train.
inference (test).
model.
param.
↓ speed ↑ mode speed ↑ ppl ↓.
baselinetransformerxl∗sandwich t.compressive t.routing t.knn-lm∗∗.
247m 13.9k.
257m6.0k247m 13.9k329m-.
--.
247m 13.9k.
n.o.
s.w.
n.o.
s.w.
n.o.
n.os.w..14.7k 19.4018.702.5k18.303.2k17.962.5k17.1-15.8-15.79145.pia + cachingstaged trainingshortformer.
247m 21.5k n.o.
247m 17.6ks.w.
247m 22.9k n.o..14.5k2.5k14.5k.
18.5517.5618.15.table 5: comparison of our best models to other stronglms (see text for citations and explanations) evaluatingthe wikitext-103 test set, where s = 512. we mea-sure speed in tok./sec.
per gpu, and use a batch size of1 for inference.
∗transformerxl runs on an older ver-sion of pytorch, which might affect speed.
∗∗knn-lmrequires a 400gb datastore..best model from §5.3, it is nine times faster thanthe baseline for token-by-token generation..since it uses a cache, sliding window evaluationdoes not increase shortformer’s performance.
bytraining the baseline with staged training (and nopia or caching), we obtain a model (our best modelfrom §4.2) that, with sliding window eval., obtainseven better results, but that model is much slowerthan shortformer (table 5, second-to-last row)..shortformer outperforms the baseline’s perplex-ity and performs within a standard deviation ofthe sandwich transformer (press et al., 2020)and transformerxl.
it does not outperform thecompressive transformer (rae et al., 2020), rout-ing transformer (roy et al., 2020) and knn-lm (khandelwal et al., 2020), which make or-thogonal improvements that can be applied to anylanguage model, at the price of slower decoding.
combining them with our approach may yield fur-ther gains.
these results are similar to those weobtain on the toronto book corpus (§a.5 in theappendix)..7 related work.
staged training devlin et al.
(2019) used astaged training routine for bert by performingthe ﬁrst 90% of training on short subsequences (oflength 128) before moving on to longer ones (oflength 512).
they use this method to speed train-ing, but we show that also it improves perplexityand analyze different conﬁgurations of this method.
many recent papers have explored improving.
transformer efﬁciency by reducing the quadraticcost of self-attention, motivated by scaling tolonger sequences (kitaev et al., 2020; roy et al.,2020; tay et al., 2020).
we instead demonstrateimproved results with shorter sequences, whichnaturally also improve efﬁciency..one way to reduce transformer memory usage isto sparsify the attention matrix by letting the modelattend only to a subset of nearby tokens at eachtimestep (child et al., 2019; beltagy et al., 2020;roy et al., 2020).
training on shorter subsequencelengths is much more efﬁcient: we use multiple, butmuch smaller, attention matrices.
since attentionuses memory and computation in a way that scalesquadratically with input size, splitting the inputsinto multiple subsequences each processed indepen-dently lets us use less memory and run faster.
likeour method, beltagy et al.
(2020) attend at eachtimestep to a growing number of neighbors as train-ing progresses, but they use ﬁve stages, which wefound not to be superior to our two-staged method.
the adaptive attention span model of sukhbaataret al.
(2019) learns the maximum effective contextwindow sizes for each head at each layer indepen-dently.
like in our method, context window sizesare smaller at the start of training and lengthenas training progresses.
we show that a simpleapproach of manually choosing two subsequencelengths is highly effective.
in addition, keeping sub-sequence lengths equal across all heads and layerslets us save memory and runtime..position-infusedattention transformerxl(dai et al., 2019) caches and attends to previousrepresentations using an attention sublayer thatuses relative positioning (shaw et al., 2018).
itruns much slower than the unmodiﬁed attentionsublayer, requires extra parameters, and requiresinternally modifying the self-attention sublayer,while our pia method (§5) does not..in parallel with our work, ke et al.
(2020) com-pute attention coefﬁcients by summing two atten-tion matrices, one based on position-position in-teractions and the other on content-content inter-actions.
as in pia, they do not add position em-beddings at the bottom of the model.
they presentresults only for bert, which uses much smallersubsequences than our models..8 conclusion.
our results challenge the conventional wisdom thatlonger subsequences are always better.
by ﬁrst.
5500training on shorter subsequences and then progress-ing to longer ones via staged training, we improveperplexity and reduce training time.
we addition-ally propose position-infused attention, which en-ables caching and efﬁciently attending to previousoutputs; we show that models using this method donot require large input subsequences.
we ﬁnallyshow that these two methods can be combined toproduce a speedier and more accurate model..acknowledgments.
we thank tim dettmers, jungo kasai, gabriel il-harco, hao peng, sewon min, mandar joshi, omerlevy, luke zettlemoyer, julian michael, edwardmisback, soﬁa serrano, nikolaos pappas, jessedodge, myle ott, and sam shleifer for their valu-able feedback and fruitful discussions..references.
alexei baevski and michael auli.
2018. adaptive in-put representations for neural language modeling.
corr, abs/1809.10853..iz beltagy, matthew e. peters, and arman cohan.
2020. longformer: the long-document transformer.
arxiv:2004.05150..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inproceedings of the 26th annual international con-ference on machine learning, icml ’09, page41–48, new york, ny, usa.
association for com-puting machinery..tom b. brown, benjamin mann, nick ryder, melaniesubbiah, jared kaplan, prafulla dhariwal, arvindneelakantan, pranav shyam, girish sastry, amandaaskell, sandhini agarwal, ariel herbert-voss,gretchen krueger, tom henighan, rewon child,aditya ramesh, daniel m. ziegler, jeffrey wu,clemens winter, christopher hesse, mark chen,eric sigler, mateusz litwin, scott gray, benjaminchess, jack clark, christopher berner, sam mc-candlish, alec radford, ilya sutskever, and darioamodei.
2020. language models are few-shot learn-ers..rewon child, scott gray, alec radford,.
ilya sutskever.
2019.quences withsparsehttps://openai.com/blog/sparse-transformers..transformers..andgenerating long se-url.
zihang dai, zhilin yang, yiming yang, jaime g car-bonell, quoc le, and ruslan salakhutdinov.
2019.transformer-xl: attentive language models beyondin proceedings of the 57tha ﬁxed-length context.
annual meeting of the association for computa-tional linguistics, pages 2978–2988..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..hakan inan, khashayar khosravi, and richard socher.
2017. tying word vectors and word classiﬁers: aloss framework for language modeling.
in iclr..guolin ke, di he, and tie-yan liu.
2020. rethinking.
positional encoding in language pre-training..urvashi khandelwal, omer levy, dan jurafsky, lukezettlemoyer, and mike lewis.
2020. generalizationthrough memorization: nearest neighbor languagein international conference on learningmodels.
representations (iclr)..nikita kitaev, lukasz kaiser, and anselm levskaya.
2020. reformer: the efﬁcient transformer.
in inter-national conference on learning representations..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..stephen merity, caiming xiong, james bradbury, andrichard socher.
2016. pointer sentinel mixture mod-els..oﬁr press, noah a. smith, and omer levy.
2020. im-proving transformer models by reordering their sub-in proceedings of the 58th annual meet-layers.
ing of the association for computational linguistics,pages 2996–3005, online.
association for computa-tional linguistics..oﬁr press and lior wolf.
2017. using the output em-in proceed-bedding to improve language models.
ings of the 15th conference of the european chap-ter of the association for computational linguistics:volume 2, short papers, pages 157–163, valencia,spain.
association for computational linguistics..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..jack w. rae, anna potapenko, siddhant m. jayaku-mar, chloe hillier, and timothy p. lillicrap.
2020.compressive transformers for long-range sequencein international conference on learn-modelling.
ing representations..colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j liu.
2019. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
arxiv preprint arxiv:1910.10683..5501aurko roy, mohammad saffar, ashish vaswani, andefﬁcient content-based.
david grangier.
2020.sparse attention with routing transformers..peter shaw, jakob uszkoreit, and ashish vaswani.
2018. self-attention with relative position represen-in proceedings of the 2018 conference oftations.
the north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 2 (short papers), pages 464–468,new orleans, louisiana.
association for computa-tional linguistics..sainbayar sukhbaatar, edouard grave, piotr bo-janowski, and armand joulin.
2019. adaptive at-tention span in transformers.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 331–335, florence, italy.
association for computational linguistics..yi tay, mostafa dehghani, dara bahri, and donaldmetzler.
2020. efﬁcient transformers: a survey..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..yukun zhu, ryan kiros, rich zemel, ruslan salakhut-dinov, raquel urtasun, antonio torralba, and sanjafidler.
2015. aligning books and movies: towardsstory-like visual explanations by watching moviesand reading books.
in proceedings of the ieee inter-national conference on computer vision, pages 19–27..5502a appendix.
a.1 additional staged training results.
table 6 shows the time each staged training modelneeds to match baseline performance, as a fractionof the time it takes to train the baseline.
the fastestthree conﬁgurations each match the baseline’s per-formance in just 37% of the time it takes to trainthe baseline.
this result is very robust to hyperpa-rameter changes, as all models trained with initialsubsequence length of between 64 and 512, thatswitch to the second stage at epoch 50 to 150, man-age to match the baseline’s performance in at most59% of the time it takes to train it..initial stage subsequence length.
64.
128 256 512 1024 1536.
032250.60 0.54 0.53 0.65 0.64 0.71500.53 0.48 0.47 0.54 0.59 0.63 0.810.51 0.43 0.42 0.48 0.53 0.56 0.7975100 0.52 0.40 0.38 0.41 0.47 0.50 0.73125 0.61 0.41 0.37 0.37 0.42 0.46 0.690.48 0.39 0.37 0.40 0.44 0.661500.48 0.43 0.45 0.51 0.70175200.
0.59.shcopeegatslaitini.table 6: time needed to match baseline performance(dev.
set, nonoverlapping eval.)
as a fraction of timeneeded to train the baseline (smaller is better).
mod-els never matching the baseline have empty cells.
allmodels have a subsequence length of 3,072 tokens atthe end of training..initial stage subsequence length.
64 128 256 512 1024 1536.
32136 123 122 146 144 15525135 124 122 136 144 149 17950143 128 125 136 144 145 18175100 158 135 130 136 145 142 175125 190 149 141 140 146 144 174176 160 154 153 151 174150191 178 177 176 189175200.
202.shcopeegats.laitini.table 7: epoch at which each model matches the base-line.
some models never match the baseline, and sothose cells are empty..tables 7 and 8 show the epoch at which eachmodel matched the baseline’s performance and thetotal time it took to train each of our staged trainingmodels..initial stage subsequence length.
64.
128 256 512 1024 1536320.94 0.94 0.94 0.94 0.94 0.95 0.9725500.87 0.87 0.87 0.87 0.88 0.90 0.94750.81 0.81 0.81 0.81 0.82 0.85 0.90100 0.75 0.75 0.74 0.75 0.77 0.80 0.87125 0.68 0.68 0.68 0.69 0.71 0.75 0.84150 0.62 0.62 0.61 0.62 0.65 0.70 0.81175 0.56 0.56 0.55 0.56 0.59 0.66 0.78200 0.49 0.49 0.48 0.50 0.53 0.61 0.75.shcopeegatslaitini.table 8: total time needed to train each model as afraction of the time needed for baseline training..a.2 staged training with shorter final stage.
l.in section 4, all models presented used stagedtraining with a ﬁnal input subsequence length lof 3,072 tokens.
in table 9, we show the resultsof training with a ﬁrst stage with l = 128 for 50epochs, and using varying subsequence lengths forthe second stage.
the best result is obtained whenthe second stage uses l = 3,072. in addition, inall of our other experiments (not presented here)with different l and epoch number values for theﬁrst stage, we observed that using l = 3,072 forthe second stage always achieved the best perplexi-ties.
models trained with staged training and eval-uated with a sliding window sometimes performslightly worse when s is decreased, but this differ-ence is much smaller than the standard deviation.
the l = 1536 and l = 3072 models peaked ats = 512, and then as s was decreased perplexitystarted slightly degrading.13.
a.3 training speed vs. performance.
figure 3 compares the validation performance andtraining speed of the baseline to our models..a.4 memory usage.
to understand how much memory our models andthe baseline use during training, we ﬁnd the largestbatch size that we can load into memory for bothour models and the baseline.
models that can simul-taneously make more predictions are more memoryefﬁcient..13we conjecture that this is because of a train-test mismatchthat occurs since the average effective context length duringtraining is 3,0722 = 1,536 and so the model focuses on learninghow to make predictions for the tokens in the center of theinput, and does not perform as well when making predictionsfor tokens at the end of the input (which is what we use whenusing sliding window evaluation)..5503finalsubseq.
length.
256512102415363072.inference ppl ↓.
nonoverlapping.
sliding windows = 512.s = 1.
21.2619.6918.6418.1017.52.
-19.6917.6017.2816.89.
18.7218.0417.5817.3017.01.table 9: inference perplexity for staged training mod-els trained with an initial stage subsequence length of128 for 50 epochs and varying second stage subse-quence length l (for the second stage’s 155 epochs).
s is stride.
to see how these models perform withoutstaged training, refer to table 1..training.
max.
max.
batch size ↑ predictions ↑.
model.
baseline.
staged training.
pia + caching.
shortformer.
stage 1.stage 2.stage 1.stage 2.
2.
2.
230.
26.
160.
26.
6,144.
29,440.
6,144.
13,312.
20,480.
13,312.table 10: memory usage of the baseline and our mod-els during wikitext-103 training.
for each model weshow the maximal batch size that it could ﬁt on onegpu at once during training.
the max predictions col-umn denotes the number of tokens predicted at eachfeedforward pass, which we calculate by multiplyingbatch size by number of predictions per subsequence(which is equivalent to l).
we benchmarked all mod-els on a v100 gpu, with 32gb of memory.
note thatthe second stage in the staged training model matchesthe performance of the baseline model, because thosearchitectures are identical.
the same is true for the sec-ond stage of the shortformer and the pia + cachingmodel..figure 3: dev.
perplexity vs. training speed for thebaseline and our best staged training model, our bestpia and caching model, and our best combined model(shortformer).
all models are evaluated using nonover-lapping evaluation..table 10 shows the memory usage for the base-line model and our models.
since our shortformermodel has much smaller attention matrices, duringtraining it can make more than double the next-token predictions than the baseline can in eachfeedforward pass.
during inference, we use a batchsize of 1 throughout the paper, following (dai et al.,2019), and in our experiments, the pia + cachingmodel, the ﬁnal staged training model and the base-line all use a similar amount of memory duringnonoverlapping evaluation.
during token-by-tokeninference, the maximum number of predictions forthe baseline model is 7, whereas our model can ﬁta batch size of 39 (so 39 predictions are made dur-ing token-by-token inference), making our modelmore than 5 times more memory efﬁcient than thebaseline.
using a batch size of one is a realisticbenchmarking scenario: in large models such asgpt-3, a batch size of one is used during inference..a.5 toronto book corpus.
to verify that our results transfer to other datasets,we ran our models on the toronto book cor-pus (tbc) (zhu et al., 2015), a 700m tokencollection of books that has previously beenused in the training corpus of bert (alongwith english wikipedia).
we use the sametrain/development/test split as (khandelwal et al.,2020) and (press et al., 2020), as well as their tok-enization, which uses bert’s vocabulary of 29kbpe subwords.
as in (khandelwal et al., 2020)and (press et al., 2020), since the vocabulary ismuch smaller than wikitext-103’s, we use a tiedword embedding and softmax matrix (press andwolf, 2017; inan et al., 2017), instead of using.
550413k15k17k19k21k23ktokenspersecond ()17.017.518.018.519.0perplexity ()shortformer                pia + cache  baselinestaged trainingtable 11 shows that staged training and the short-former improve over the baseline by a wide marginand match the results of the sandwich transformerand the knn-lm.
as noted in section 6, those con-tributions are orthogonal to ours, and combiningthem might yield further gains.
since in table 11the ﬁnal stage of the staged training model (and thebaseline) both have l = 1,024, shortformer lacksa speed advantage in this scenario..table 12 shows results for our staged trainingmodel trained with a ﬁnal stage subsequence lengthof 3,072 tokens, as in our wikitext-103 experi-ments in section 4. this model trains faster thanthe l = 3,072 baseline and also achieves muchbetter perplexity scores (the baseline in this settingachieves a perplexity of 14.52 ± 0.15 (standarddeviation) on the development set).
in addition,note that the shortformer model from table 11achieves better perplexity than even the baselinewith l = 3,072, although shortformer is muchfaster to train and uses much smaller attention ma-trices during inference (of size 512 · 1024; the base-line has attention matrices of size 3,072 · 3,072, asin section 6)..train.
inference.
ppl ↓.
model.
speed↑ mode speed↑ dev.
test.
baseline(l = 3,072).
staged training(l = 3,072).
14.2.
18.1.n.o.
s.w..n.o.
s.w..15.12.5k.
15.12.5k.
14.52 11.6914.14 11.43.
13.19 10.7612.80 10.48.table 12: comparison of the staged training model tothe baseline, when the subsequence length l is set to3,072. in this table, s = 512..the adaptive word embeddings (baevski and auli,2018) as in the wikitext-103 models..to fairly compare our models to the onesfrom (khandelwal et al., 2020) and (press et al.,2020), our initial set of experiments on the tbcuse a maximum subsequence length of 1,024 (forstaged training), train for 59 epochs, and for allother hyperparameters we use the same values asthe ones we used for wikitext-103 (see experi-ment setup in section 2).
in this setting, the base-line achieves a perplexity of 15.38 ± 0.39 (standarddeviation) on the development set..we do not tune the hyperparameters of our meth-ods on the tbc, we simply use the same values asthe best ones that we found on the wikitext-103dataset.
for staged training, our best model trainedfor 50205 % of the epochs with l = 128 and spentthe rest of training with the same subsequence sizeas the baseline.
for the tbc, we again trainedthe staged training model model with l = 128for the ﬁrst 50205 % of training, and then move onto l = 1,024, to match the sandwich trans-former (press et al., 2020) and knn-lm (khan-delwal et al., 2020) which used 1,024 as the subse-quence length..for the pia + caching model, we set l = 512,as we did for our best pia + caching on thewikitext-103 dataset..for the toronto book corpus shortformer, wetrained for the ﬁrst half of training with l = 128before moving on to training with l = 512, as inour wikitext-103 models (section 6)..train.
inference.
ppl ↓.
model.
speed↑ mode speed↑ dev.
test.
baselineknn-lm∗sandwich t..pia + caching.
staged training.
shortformer.
24.0k.
n.o.
s.w.
24.0k s.w.
24.0k s.w..25.5k.
20.5k n.o.
n.o.
s.w.
21.3k n.o..19.2k 15.38 12.7314.75 11.899.6k14.20 10.89-10.839.6k.
-.
15.0k 13.86 11.2019.2k 13.81 11.1813.13 10.729.6k15.5k 13.40 10.88.table 11: comparison of our best models to otherstrong lms trained on the toronto book corpus (tbc).
following khandelwal et al.
(2020) and press et al.
(2020), for the baseline and our staged training model,we set l = 1,024 and s = 512 when using sliding win-dow (s.w.)
evaluation in the tbc dataset.
all modelshave 261m parameters.
∗knn-lm requires a 400gbdatastore..5505