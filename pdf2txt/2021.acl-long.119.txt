a gradually soft multi-task and data-augmented approachto medical question understanding.
khalil mrini1, franck dernoncourt2, seunghyun yoon2, trung bui2,walter chang2, emilia farcas1, and ndapa nakashole1.
1university of california, san diego, la jolla, ca 92093{khalil, efarcas, nnakashole}@ucsd.edu2adobe research, san jose, ca 95110{franck.dernoncourt, syoon, bui, wachang}@adobe.com.
abstract.
users of medical question answering systemsoften submit long and detailed questions, mak-ing it hard to achieve high recall in answer re-trieval.
to alleviate this problem, we proposea novel multi-task learning (mtl) methodwith data augmentation for medical questionunderstanding.
we ﬁrst establish an equiva-lence between the tasks of question summa-rization and recognizing question entailment(rqe) using their deﬁnitions in the medicaldomain.
based on this equivalence, we pro-pose a data augmentation algorithm to use justone dataset to optimize for both tasks, witha weighted mtl loss.
we introduce gradu-ally soft parameter-sharing: a constraint fordecoder parameters to be close, that is gradu-ally loosened as we move to the highest layer.
we show through ablation studies that our pro-posed novelties improve performance.
ourmethod outperforms existing mtl methodsacross 4 datasets of medical question pairs,in rouge scores, rqe accuracy and humanevaluation.
finally, we show that our methodfares better than single-task learning under 4low-resource settings..1.introduction.
in order to retrieve relevant answers, one of thebasic steps in question answering (qa) systems isunderstanding the intent of questions (chen et al.,2012; cai et al., 2017).
this is particularly impor-tant for medical qa systems (wu et al., 2020), asconsumer health questions – questions asked by pa-tients – may use a vocabulary distinct from doctorsto describe similar health concepts (ben abachaand demner-fushman, 2019a).
consumer healthquestions may also contain peripheral informationlike patient history (roberts and demner-fushman,2016), that are not necessary to answer questions.
there is a growing number of approaches to medi-cal question understanding, including query relax-.
figure 1: we highlight the main four aspects of thechq.
our method learns from the task of recogniz-ing question entailment to generate more informativesummaries compared to the baseline..ation (ben abacha and zweigenbaum, 2015; leiet al., 2020), question entailment (ben abacha anddemner-fushman, 2016, 2019b; agrawal et al.,2019), question summarization (ben abacha anddemner-fushman, 2019a), and question similarity(ben abacha and demner-fushman, 2017; yan andli, 2018; mccreery et al., 2019)..medical question summarization is the task ofsummarizing consumer health questions into short,single-sentence questions that capture essential in-formation needed to give a correct answer.
thetask of recognizing question entailment (rqe)is deﬁned by ben abacha and demner-fushman(2016) in the medical domain as a binary classi-ﬁcation task.
for the purpose of this task, a ﬁrstquestion is considered to entail a second one if andonly if every answer to the second question is acorrect, and either full or partial answer to the ﬁrstquestion..we ﬁnd in initial experiments (mrini et al.,2021b) that rqe can teach question summarizersto distinguish salient information from peripheraldetails, and likewise that question summarizationcan beneﬁt rqe classiﬁers.
in our setting, we castthe medical question understanding task as a multi-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1505–1515august1–6,2021.©2021associationforcomputationallinguistics1505source user-written question or consumer health question (chq):subject: morgellon disease.
message: it appears as if i have had this horrible disease for many, many years and it is getting worst.
i am trying to ﬁnd a physician or specialist in the south carolina area who can treat me for this medical/mental disease.
it seems as if this disease has "no" complete treatment and it is more least a disability!reference summarized question or frequently asked question (faq):what are the treatments for morgellon disease, and how can i ﬁnd physician(s) in south carolina who specialize in it?bart trained on summarization loss only (baseline):where can i ﬁnd physician(s) who specialize in morgellon disease?our gradually soft multi-task and data-augmented model:where can i ﬁnd a physician or specialist in south carolina who can treat morgellon disease?
task learning (mtl) problem involving the twotasks of question summarization and recognizingquestion entailment.
we use a simple sum of learn-ing objectives in mrini et al.
(2021b).
in this paper,we introduce a novel, gradually soft multi-task anddata-augmented approach to medical question un-derstanding.1.
previous work on combining summarization andentailment uses at least 2 datasets – 1 from eachtask (pasunuru et al., 2017; guo et al., 2018).
weﬁrst establish an equivalence between both tasks.
this equivalence is the inspiration behind the dataaugmentation schemes introduced in our previouswork (mrini et al., 2021b).
the goal of the dataaugmentation is to use a single dataset for multi-task learning.
we propose to use a weightedloss function to simultaneously optimize for bothtasks.
then, we propose a gradually soft parameter-sharing mtl approach.
we conduct ablation stud-ies to show that our two novelties – data augmen-tation and gradually soft parameter-sharing – im-prove performance in both tasks..our proposed gradually soft multi-task and data-augmented approach outperforms existing single-task and multi-task learning methods on architec-tures achieving state-of-the-art results in abstractivesummarization.
compared to single-task learning,our approach achieves a 12% increase in accuracyon a medical rqe dataset, and an average increaseof 3.5% in rouge-1 f1 scores across 3 medicalquestion summarization datasets.
additionally, weperform human evaluation and ﬁnd our approachgenerates more informative summarized questions.
finally, we ﬁnd that our approach is more efﬁcientat leveraging smaller amounts of data, and yieldsbetter performance under 4 low-resource settings..2 background and related work.
recognizing question entailment(rqe).
ben abacha and demner-fushman (2016) intro-duce the task of rqe.
it is closely related –– butnot exactly similar –– to the task of recognizingtextual entailment (rte) (dagan et al., 2005,2013), and early deﬁnitions of question entailment(groenendijk and stokhof, 1984; roberts, 1996).
the task of rqe is to predict, given two pairsof questions a and b, whether a entails b. rqeconsiders that question a entails question b if every.
1our code is available at:.
https://github.com/khalilmrini/medical-question-understanding.
answer to b is a correct answer to a, and answersa either partially or fully.
it differs from traditionaldeﬁnitions of entailment, where we consider thatthe premise entails the hypothesis if and only if thehypothesis is true only if the premise is true..ben abacha and demner-fushman (2016) deﬁnerqe within the context of medical question an-swering.
the goal is to match a consumer healthquestion (chq) to a frequently asked question(faq), and ultimately match the chq to an expert-written answer.
summarization and entailment.
there is agrowing body of work combining summarizationand entailment (lloret et al., 2008; mehdad et al.,2013; gupta et al., 2014)..falke et al.
(2019) use textual entailment pre-dictions to detect factual errors in abstractive sum-maries generated by state-of-the-art models.
pa-sunuru and bansal (2018) propose an entailmentreward for their reinforced abstractive summarizer,where the entailment score is obtained from apre-trained and frozen natural language inferencemodel..pasunuru et al.
(2017) propose an lstmencoder-decoder model that incorporates entail-ment generation and abstractive summarization.
the authors optimize alternatively between the twotasks, and use separate natural language infer-ence (nli) and abstractive summarization datasets.
only the decoder parameters are shared..li et al.
(2018) closely follow the mtl settingof pasunuru et al.
(2017), and propose a modelwith a shared encoder, an nli classiﬁer and annli-rewarded summarization decoder..guo et al.
(2018) introduce a pointer-generatorsummarization model with coverage loss (see et al.,2017).
they build upon the work of pasunuru et al.
(2017), and add question generation on top of thetwo tasks of abstractive summarization and entail-ment generation.
they also alternate between thethree different objectives.
the authors proposeto share all parameters except the ﬁrst layer ofthe encoder and the last layer of the decoder, andshow that soft parameter-sharing improves overhard parameter-sharing.
their method outperformsthe pointer-generator networks of see et al.
(2017)on the cnn-dailymail news summarization base-line.
here, the authors show performance increasein entailment on some batch sizes and decrease onother batch sizes, and they consider entailment asan auxiliary task..1506transfer learning for medical qa.
bionlp isone of many nlp applications to beneﬁt from lan-guage models that use multi-task learning and trans-fer learning.
there are pretrained language modelsthat are geared towards bionlp applications, thatare based on bert (devlin et al., 2019).
thoseinclude scibert (beltagy et al., 2019) whichhas been ﬁne-tuned using biomedical text frompubmed.
biobert (lee et al., 2020) has been ﬁne-tuned on the pmc dataset, whereas models namedclinicalbert (huang et al., 2019; alsentzer et al.,2019) additionally use the mimic iii dataset (john-son et al., 2016)..transfer learning was a popular approach at the2019 mediqa shared task (ben abacha et al.,2019) on medical nli, rqe and qa.
the questionanswering task involved re-ranking answers, notgenerating them (demner-fushman et al., 2020).
for the rqe task, the best-performing model (zhuet al., 2019) uses transfer learning on nli and en-semble methods..3 methodology.
we consider the multi-task learning of medicalquestion summarization and medical rqe.
theinput to both tasks is a pair of medical questions.
the ﬁrst question is called a consumer healthquestion (chq), and the second question is calleda frequently asked question (faq).
the chqis written by a patient and is usually longer andmore informal, whereas the faq is usually a single-sentence question written by a medical expert.
thepurpose of both tasks is to match a chq to anfaq, and ultimately to an expert-written answerthat matches the faq.
an example pair is shownin figure 1..our novel gradually soft multi-task and data-augmented learning approach to medical questionunderstanding has four main components.
first, weestablish the equivalence between medical questionpairs in question summarization and rqe.
then,we use our equivalence observation to propose ascheme for data augmentation.
third, we show oursimultaneous multi-task learning model architec-ture and learning objective.
finally, we describeour gradually soft parameter-sharing scheme..3.1 equivalence of question summarization.
and rqe.
in the following, we evidence the equivalence be-tween medical question summarization and medi-.
cal rqe.
we ﬁrst consider a pair of medical ques-tions c and f, where c is a chq and f and is anfaq, such that c is longer than f..ben abacha and demner-fushman (2016) deﬁnequestion entailment as: question c entails questionf (c ⇒ f ) if and only if every answer to f isalso a correct answer to c, whether partially orcompletely (1)..according to the guidelines set in the data cre-ation of a medical question summarization datasetby ben abacha and demner-fushman (2019a),doctors were told to grade manually written sum-marized questions (faqs) as perfect, acceptable orincorrect.
the two conditions for a perfect faqare: ﬁrst, an faq should enable to retrieve “com-plete and correct answers” to the original chq,and second, the summarized question should notbe so short that it violates the ﬁrst condition.
theresulting medical question summarization datasetincludes perfect and acceptable faqs.
we assumethat a perfect faq provides complete and correctanswers to the corresponding chq, and that an ac-ceptable faq provides correct answers to the cor-responding chq, whether partially or completely.
we therefore conclude that: f is a good summaryof c, if and only if f enables to retrieve correctanswers to c, whether partially or completely (2).
we have: f enables to retrieve correct answers toc, if and only if answers to f are correct answers toc. therefore, f enables to retrieve correct answersto c, if and only if every answer to f is also a cor-rect answer to c, whether partially or completely.
given the equivalences (1) and (2) above, it followsthat: question f is a good summary of question c,if and only if question c entails question f (3)..3.2 data augmentation.
medical question understanding datasets are scarce,and new high-quality datasets are complex andcostly to create.
we propose in mrini et al.
(2021b)to augment existing datasets in one of the two tasksto create a synthetic dataset of the same size forthe other task.
our two-way data augmentationalgorithm is inspired by the equivalence shown inthe previous subsection, and enables us to train ina simultaneous multi-task setting.
our data aug-mentation method also addresses a weakness inprevious work in multi-task learning, where eachtask involves a distinct dataset, often from a differ-ent domain.
our data augmentation will enable usto use datasets in the same domain, and we hypoth-.
1507esize this can beneﬁt performance in both tasks..for summarization datasets, we create equiva-lent rqe pairs.
for each existing summarizationpair, we ﬁrst choose with equal probability whetherthe equivalent rqe pair is labeled as entailment ornot.
if it is an entailment case, we use the equiva-lence in (3) and create an rqe pair identical to thesummarization pair.
if it is not an entailment case,then we have: (3) ⇔ question f is not a summaryof question c if and only if question c does notentail question f (4).
therefore, to create an equiv-alent rqe pair labeled as not entailement, the rqechq is identical to the chq of the summarizationpair, and the rqe faq is randomly selected froma distinct question pair from the same dataset split.
inversely, for the rqe dataset, we create equiva-lent summarization pairs.
for each existing rqepair, we consider two cases.
if the rqe pair islabeled as entailment, we create an identical sum-marization pair.
if the rqe pair is labeled as notentailment, then following (4), we create a summa-rization pair that is identical to a randomly selectedand distinct rqe pair labeled as entailment fromthe same dataset split..3.3 simultaneous multi-task learning.
previous work on multi-task learning with sum-marization and entailment (pasunuru et al., 2017;guo et al., 2018) optimize for the objectives of thedifferent tasks by alternating between them.
thisalternating multi-task training follows a ratio be-tween the different tasks, that depends on the sizeof the dataset of each task (e.g.
a ratio of 10:1means training for 10 batches on one task, and thenfor 1 batch on the other task).
in our approach, wepropose to optimize simultaneously for the objec-tives of both tasks.
we do not use ratios, as we arenot alternating between objectives and the resultingdatasets from our data augmentation algorithm areof equal size..whereas many previous multi-task settingschose generation tasks (entailment generation andquestion generation), we choose the bart largearchitecture (lewis et al., 2019) as it enables tooptimize for a classiﬁcation task (rqe) and a gen-eration task (summarization) using the same ar-chitecture.
in addition, bart is adequate as itachieves very strong results in benchmark datasetsof recognizing textual entailment and abstractivesummarization.
the input works differently be-tween both tasks.
for summarization, the encoder.
figure 2: overview of the architecture of our proposedgradually soft multi-task and data-augmented model.
the gradually thinning links between decoder layersrepresent the loosening parameter-sharing constraint..takes the chq as input and the decoder takes thefaq as input.
for rqe, both the encoder and de-coder take the entire rqe pair as input.
we add aclassiﬁcation head for rqe, to which we feed thelast decoder output, as it attends over all decoderand encoder positions.
we show an overview ofour architecture in figure 2..we propose to optimize a single loss functionthat combines objectives of both tasks.
our lossfunction is the weighted sum of the negative log-likelihood summarization objective, and the binarycross-entropy classiﬁcation objective of rqe..more formally, given a chq embedding x, thecorresponding faq embedding y, and the entail-ment label lentail ∈ {0, 1}, we optimize the follow-ing multi-task learning loss function:.
lmtl(θ) = − λ ∗ logp(y|x; θ).
+ (1 − λ) ∗ bce ([x; y] , lentail; θ)(1).
where bce is binary cross entropy, and λ is a hy-perparameter between 0 and 1..1508shared encoderdecoderdecoderfaqchq; faqchq; faqchqrecognizing question entailment (rqe)classiﬁcation taskquestion summarizationgeneration taskclassiﬁcation headcross-entropy lossnegative log-likelihood lossgradually soft parameter-sharing loss3.4 gradually soft parameter-sharing.
in multi-task learning, there are two widely usedapproaches:hard parameter-sharing and softparameter-sharing.
guo et al.
(2018) propose softparameter-sharing for all parameters except the ﬁrstlayer of the encoder and last layer of the decoder.
liu et al.
(2019) introduce mt-dnn and show thathard parameter-sharing of all of the transformerencoder layers, and only having task-speciﬁc clas-siﬁcation heads produces results that set a new stateof the art for the glue benchmark (wang et al.,2018)..we propose a hybrid approach, where we applyhard parameter-sharing for the encoder, and a novelgradually soft parameter-sharing approach for thedecoder layers.
we deﬁne gradually soft parameter-sharing as a smooth transition from hard parameter-sharing to task-speciﬁc layers.
it is a soft parameter-sharing approach that is gradually toned down fromthe ﬁrst layer of the decoder to the last layer, whichis entirely task-speciﬁc..in gradually soft parameter-sharing, we con-strain decoder parameters to be close by penalizingtheir l2 distances, and the higher the layer the looserthe constraint.
given a decoder with n layers, thegradually soft parameter-sharing loss term is asfollows:.
lgs(θ) = γ ∗.
n −1(cid:88).
(cid:16).
n=1.
n −nn − 1.e.(cid:17) (cid:13)(cid:13)θqs(cid:13).
dec,n − θrqe.
dec,n.
(cid:13)2(cid:13)(cid:13).
(2)where γ is a hyperparameter, θqsdec,n represents thedecoder parameters for the question summarizationat the n-th layer, and likewise θrqedec,n represents thedecoder parameters for the rqe task at the n-thlayer.
we iterate from the 1st to the (n − 1)-thlayer, as the n -th layer is entirely task-speciﬁc andunconstrained.
we show a high-level representa-tion in figure 2..4 experiments.
4.1 datasets.
we consider 3 medical question summarizationdatasets and 1 medical rqe dataset.
we showdataset statistics in table 1. meqsum andmediqa rqe can be considered low-resource,whereas the other two are far larger.
our datasetsare in the english language.
due to space con-straints, we brieﬂy introduce the datasets and leaveadditional details in the appendix..datasetmeqsumhealthcaremagicicliniqmediqa rqe.
train400181,12224,8518,588.dev10022,6413,105302.test50022,6423,106230.table 1: statistics of the medical dataset splits..the medical question summarization datasetsare meqsum (ben abacha and demner-fushman,2019a), healthcaremagic and icliniq.
we extractin mrini et al.
(2021b) and in mrini et al.
(2021c)the healthcaremagic and icliniq datasets fromthe large-scale meddialog dataset (chen et al.,2020).
whereas meqsum is a high-quality datasetfrom the u.s. national institutes of health (nih),healthcaremagic and icliniq are from onlinehealthcare service platforms.
healthcaremagic’ssummaries are more abstractive and are writtenin a formal style, unlike icliniq’s patient-writtensummaries..the medical rqe dataset is the mediqarqe dataset from the 2019 mediqa shared task(ben abacha et al., 2019).
similarly to meqsum,the question pairs match a longer chq received bythe u.s. national library of medicine (nlm) anda faq from nih institutes.
whereas the train anddev sets have automatically generated chqs, thetest set has manually written chqs.
this results insigniﬁcantly higher dev set results than for test sets,as has been observed during the 2019 mediqashared task..in addition, we use two pretraining datasets.
weuse the xsum dataset (narayan et al., 2018), anabstractive summarization benchmark, for ques-tion summarization.
for the rqe task, we use therecognizing textual entailment (rte) dataset (da-gan et al., 2005; haim et al., 2006; giampiccoloet al., 2007; bentivogli et al., 2009) from the gluebenchmark (wang et al., 2018)..4.2 setup and training settings.
all of our models use the bart large architec-ture.
unless otherwise noted, all experiments onthe 3 question summarization datasets are made us-ing a checkpoint pre-trained on the xsum datasetusing only the summarization objective, and allexperiments on the rqe dataset are made usinga checkpoint pre-trained on the rte dataset, onlyoptimizing the cross-entropy loss..we report rouge f1 scores for the question.
1509r1.
rl.
meqsumr2.
datasetmetricablation of data augmentationgradually soft mtl + existing dataset47.5ablation of gradually soft parameter-sharing47.9hard-shared decoder + data aug.48.9soft-shared decoder + data aug.task-speciﬁc decoder + data aug.45.4our modelgradually soft mtl + data aug..34.035.631.7.
52.053.250.8.
32.3.
37.9.
50.2.
51.3.
54.5.healthcaremagicrlr2r1.
icliniqr2.
rqerl accuracy.
r1.
45.1.
22.9.
40.3.
59.4.
46.0.
54.5.
81.1%.
44.344.846.0.
23.322.825.1.
41.540.943.4.
60.160.761.8.
47.048.347.5.
56.357.856.9.
77.5%79.4%81.8%.
46.9.
24.8.
43.2.
62.3.
48.7.
58.5.
82.1%.
table 2: dev set results for the ablation studies on our two main novelties: our data augmentation algorithm, andour gradually soft parameter-sharing method.
the r1, r2 and rl metrics refer to the f1 scores of rouge-1,rouge-2 and rouge-l (lin, 2004)..the smaller the dataset, the more it beneﬁts fromdata-augmented mtl with rqe..4.4 ablation studies.
we perform two ablation studies to show the addedvalue of our main novelties: our equivalence-inspired data augmentation algorithm and our grad-ually soft parameter-sharing algorithm..data augmentation.
we compare our data aug-mentation algorithm against the following alterna-tive: instead of training using a synthetic dataset forthe auxiliary task, we choose a separate, existingdataset for abstractive summarization or recogniz-ing textual entailment.
this follows the approachtaken by most mtl models.
for the question sum-marization task, we optimize the cross-entropy ob-jective using the rte dataset.
for the rqe task,we optimize the summarization objective using thexsum dataset.
for the sake of fair comparison, weuse the simultaneous mtl objective and the samearchitecture.
results in table 2 show a consistentincrease in performance across all datasets whenusing our data augmentation method, suggestingthat in-domain mtl is more efﬁcient..comparing parameter-sharing conﬁgurations.
we compare our gradually soft parameter-sharingmethod with 3 other parameter-sharing conﬁgura-tions.
for all conﬁgurations, we keep using ourdata augmentation method, and sharing encoderparameters entirely..1. hard-shared decoder: decoder parameters areshared using hard parameter-sharing..2. soft-shared decoder: we apply soft parameter-sharing on decoder parameters across all n layers.
figure 3: dev set performance of multi-task learningas a function of the loss hyperparameter λ. the closerλ is to 0, the more the loss focuses on the rqe ob-jective, and vice-versa for the question summarizationobjective..summarization datasets, and accuracy for the rqedataset, as it is a binary classiﬁcation task with twolabels: entailment and not entailment..the learning rate for rqe experiments is 1 ×10−5 and for the question summarization experi-ments, it is 3 × 10−5.
we use an adam optimizerwhere the betas are 0.9 and 0.999 for summariza-tion, and 0.9 and 0.98 for rqe.
in all experiments,the adam epsilon is 10−8, and the dropout is 0.1.we set the γ hyperparameter to 1 × 10−7..4.3 balancing between the objectives.
our loss function as deﬁned in eq.1 has a hyperpa-rameter λ to balance between the question summa-rization objective and the rqe objective.
we runexperiments where λ varies from 0.1 to 0.9 in 0.1increments.
the results are in figure 3. the best λvalues are 0.5 for meqsum, 0.7 for icliniq, 0.8 forhealthcaremagic and 0.3 for mediqa rqe.
forthe question summarization datasets, we notice that.
1510using the following, unweighted loss term:.
ls(θ) = γ ∗.
(cid:13)(cid:13)θsum.
dec,n − θent.
dec,n.
(cid:13)2(cid:13).
(3).
n(cid:88).
n=1.
3. task-speciﬁc decoder: we train two task-speciﬁcdecoders..our ablation study results in table 2 show thatour gradually soft parameter-sharing method ex-ceeds all 3 of the other parameter-sharing con-ﬁgurations in rqe accuracy, and in the sum ofrouge f1 scores.
these results show our pro-posed smoother parameter-sharing transition be-tween encoder and decoder layers brings abouthigher performance..4.5 results and discussion.
4.5.1 summarization results.
baselines.
we consider three main baselines.
theﬁrst one is bart (lewis et al., 2019), where weonly train on the summarization task.
the secondbaseline trains bart on the same mtl settingsas pasunuru et al.
(2017), using alternative trainingwith entailment generation on the stanford natu-ral language inference (snli) corpus (bowmanet al., 2015) and having a shared decoder and task-speciﬁc encoders.
the third baseline trains barton the same mtl settings as guo et al.
(2018),where, on top of the entailment generation task, weadd the question generation task using the stanfordquestion answering dataset (squad) (rajpurkaret al., 2016), and all parameters are soft-shared,except for the task-speciﬁc ﬁrst encoder layer andlast decoder layer..in addition, we also report the baselines assessedby ben abacha and demner-fushman (2019a)for meqsum.
for data augmentation, they usesemantically-selected relevant question pairs fromthe quora question pairs dataset (iyer et al., 2017).
their results show that coverage loss (see et al.,2017) diminishes the added value of data aug-mentation in pointer-generator networks.
oursummarization-only bart baseline exceeds all ofthe reported meqsum baselines in rouge-1 f1.
summarization results.
we report our summa-rization results in table 3. compared to the single-task bart baseline, our gradually soft multi-taskand data-augmented method performs better acrossall three rouge metrics, and achieves increasesranging from 1.4 to 5.5 points in rouge-1 f1..this differences shows that our method is consis-tently more efﬁcient compared to training only onsummarization..the other two mtl baselines are generally per-forming better than the single-task bart baseline,except for the larger healthcaremagic dataset.
weobserve that the different parameter-sharing con-ﬁgurations and tasks used in the mtl baselinesare scoring about 1 to 4 points below our methodin terms of rouge-1 f1 scores.
this shows thatour choice of tasks, simultaneous mtl loss, dataaugmentation and gradually soft parameter-sharingmethod work consistently better than existing mtlmethods.
human evaluation.
given that rouge is noto-riously unreliable, we hire 2 annotators to judge120 randomly selected summaries from the sum-marization test sets, generated from the single-taskbart baseline and our own method in table 3.we ask the annotators to judge the fluency, co-herence, informativeness and correctness of eachgenerated summary, using best-worst scaling, withthe possibility of ranking both summaries equally.
the annotators are presented with 2 generated sum-maries, in a randomized order at each evaluation,such that they cannot identify which method gener-ated which summary..our human evaluation results are in table 4.scores generally favor our method, more stronglyso in the abstractive datasets – healthcaremagicand meqsum.
however, we note an increase incorrectness for the more extractive icliniq dataset.
on average, our gradually soft multi-task and data-augmented method outputs summarized questionsthat are more ﬂuent and more informative than thesingle-task bart baseline..4.5.2 rqe results and discussion.
baselines.
we compare our method to three base-lines.
the ﬁrst one trains a single-task bart onrqe, with a classiﬁcation head pre-trained on rte.
the second baseline is a feature-based svm fromben abacha and demner-fushman (2016) who in-troduced the mediqa rqe dataset.
the thirdbaseline (zhou et al., 2019) is an adversarial mtlmethod combining medical question answering andrqe.
the architecture consists of a shared trans-former encoder using biobert embeddings (leeet al., 2020), separate classiﬁcation heads for rqeand medical qa, and a task discriminator for ad-versarial training.
a separate dataset is used formedical qa (ben abacha et al., 2019)..1511datasetmetricbaselinesseq2seq attentional model (nallapati et al., 2016)pointer-generator networks (pg) (see et al., 2017)pg + data augmentation (ben abacha and demner-fushman, 2019a)pg + coverage loss (see et al., 2017)pg + coverage loss + data augmentation(ben abacha and demner-fushman, 2019a)models using bartbart (lewis et al., 2019)bart + entailment generation + mtl of pasunuruet al.
(2017)bart + entailment generation & question gener-ation + mtl of guo et al.
(2018)bart + recognizing question entailment + grad-ually soft mtl + data augmentation (ours).
meqsumr2.
r1.
rl.
healthcaremagicrlr2r1.
icliniqr2.
r1.
rl.
24.835.844.2.
13.820.227.6.
24.334.842.8.
39.641.8.
23.124.8.
38.540.5.
---.
--.
---.
--.
---.
--.
---.
--.
---.
--.
---.
--.
45.746.5.
26.827.7.
40.842.3.
44.542.2.
22.320.6.
39.738.1.
48.749.6.
28.029.3.
43.543.8.
47.2.
28.1.
42.0.
44.7.
23.5.
41.9.
51.4.
32.3.
46.5.
49.2.
29.5.
44.8.
45.9.
24.3.
42.9.
54.2.
36.9.
49.1.table 3: test set results on the 3 question summarization datasets..datasetsmeqsumhealthcaremagicicliniq.
fluency coherence+2.50%+11.25%-2.50%+6.25%0%+2.50%.
informative correct+7.50%0%+12.50% +1.25%+3.75% +5.00%.
table 4: human evaluation results on 120 samplesfrom the question summarization datasets.
the percent-ages indicate the added value of our method..methodbart (lewis et al., 2019)feature-based svm (ben abacha and demner-fushman, 2016)biobert + adversarial mtl with medicalqa (zhou et al., 2019)bart + summarization + gradually softmtl + data aug. (ours).
accuracy52.1%54.1%.
63.6%.
64.3%.
table 5: accuracy results on mediqa rqe test set..figure 4: test set 4-run average performance of ourmethod compared to single-task bart in low-resourcesettings.
full dataset results are shown for comparison..rqe results.
we show our rqe results intable 5. we see a 12% increase on the test setcompared to optimizing only on the rqe objective,and 10% increase.
without a separate dataset orembeddings trained on large-scale biomedical data,our method is able to exceed the performance ofzhou et al.
(2019) by 0.7%.
this conﬁrms thestrength of our method, and shows our method canincrease performance in both rqe and questionsummarization in the medical domain..4.6 performance in low-resource settings.
we compare our gradually soft mtl and data-augmented method with the single-task bart base-line on four low-resource settings.
for each dataset,.
we limit the training data to a subset of 50, 100,500 or 1000 datapoints, and keep the same trainingsettings.
to avoid selection bias, we select four ran-dom and distinct subsets per low-resource setting,and show average rouge-1 f1 scores in figure 4..the results show that our approach is able toperform much better in low-resource settings.
wenotice in particular that, on all 4 datasets, the scoresof the single-task bart baseline for 100 and 1000datapoints are lower than or roughly equal to thescores of our method for a training subset of halfthe size (50 and 500 datapoints respectively).
thissuggests that our method’s performance increase isnot only related to additional datapoints, but alsoits gradually soft mtl setting..15125 conclusions.
we propose a novel multi-task learning approachfor medical question understanding.
our approachtrains on the tasks of rqe and question summa-rization in a simultaneous, weighted mtl lossfunction, where we add a loss term to constrainthe decoder layers to be close, and we loosen theconstraint gradually as we move higher up the lay-ers.
we show using the deﬁnitions of both tasks inthe medical domain that we can augment datasets,such that we only need one dataset for mtl.
ourtwo ablation studies show that our gradually softparameter-sharing and our data augmentation al-gorithm each increase performance individually.
we compare our method to single-task learningand existing mtl work, and show improvementsacross 3 medical question summarization datasetsand 1 medical rqe dataset.
finally, we test ourapproach under low-resource settings: we ﬁnd thatit is able to efﬁciently leverage small quantities ofdata, and that these performance increases do notonly depend on additional data from augmentation..acknowledgements.
we gratefully acknowledge the award fromnih/nia grant r56ag067393.
khalil mrini isadditionally supported by adobe research unre-stricted gifts.
this work is part of the voli project(mrini et al., 2021a; johnson et al., 2020).
wethank naba rizvi for the annotation work, and theanonymous reviewers for their feedback..references.
anumeha agrawal, rosa anil george, selvan suntiharavi, sowmya kamath, and anand kumar.
2019.ars_nitk at mediqa 2019: analysing various methodsfor natural language inference, recognising questionentailment and medical question answering system.
in proceedings of the 18th bionlp workshop andshared task, pages 533–540..emily alsentzer, john murphy, william boag, wei-hung weng, di jindi, tristan naumann, andmatthew mcdermott.
2019. publicly available clini-cal bert embeddings.
in proceedings of the 2nd clin-ical natural language processing workshop, pages72–78..guage processing (emnlp-ijcnlp), pages 3606–3611..asma ben abacha and dina demner-fushman.
2016.recognizing question entailment for medical ques-in amia annual symposium pro-tion answering.
ceedings, volume 2016, page 310. american medi-cal informatics association..asma ben abacha and dina demner-fushman.
2017.nlm_nih at semeval-2017 task 3: from question en-tailment to question similarity for community ques-tion answering.
in proceedings of the 11th interna-tional workshop on semantic evaluation (semeval-2017), pages 349–352..asma ben abacha and dina demner-fushman.
2019a.
on the summarization of consumer health questions.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages2228–2234..asma ben abacha and dina demner-fushman.
2019b.
a question-entailment approach to question answer-ing.
bmc bioinformatics, 20(1):511..asma ben abacha, chaitanya shivade, and dinademner-fushman.
2019. overview of the mediqa2019 shared task on textual inference, question en-tailment and question answering.
in proceedings ofthe 18th bionlp workshop and shared task, pages370–379..asma ben abacha and pierre zweigenbaum.
2015.means: a medical question-answering system com-bining nlp techniques and semantic web technolo-information processing & management,gies.
51(5):570–594..luisa bentivogli, peter clark, ido dagan, and danilogiampiccolo.
2009. the ﬁfth pascal recognizing tex-tual entailment challenge.
in tac..samuel bowman, gabor angeli, christopher potts, andchristopher d manning.
2015. a large annotatedcorpus for learning natural language inference.
inproceedings of the 2015 conference on empiricalmethods in natural language processing, pages632–642..ruichu cai, binjun zhu, lei ji, tianyong hao, junyan, and wenyin liu.
2017. an cnn-lstm atten-tion approach to understanding user query intentfrom online health communities.
in 2017 ieee in-ternational conference on data mining workshops(icdmw), pages 430–437.
ieee..long chen, dell zhang, and levene mark.
2012. un-derstanding user intent in community question an-in proceedings of the 21st internationalswering.
conference on world wide web, pages 823–828..iz beltagy, kyle lo, and arman cohan.
2019. scibert:a pretrained language model for scientiﬁc text.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-.
shu chen, zeqian ju, xiangyu dong, hongchao fang,sicheng wang, yue yang, jiaqi zeng, ruisi zhang,ruoyu zhang, meng zhou, penghui zhu, and peng-tao xie.
2020. meddialog: a large-scale medical di-alogue dataset.
arxiv preprint arxiv:2004.03329..1513ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in machine learning challenges work-shop, pages 177–190.
springer..kexin huang, jaan altosaar, and rajesh ranganath.
2019. clinicalbert: modeling clinical notes andarxiv preprintpredicting hospital readmission.
arxiv:1904.05342..ido dagan, dan roth, mark sammons, and fabio mas-simo zanzotto.
2013. recognizing textual entail-ment: models and applications.
synthesis lectureson human language technologies, 6(4):1–220..dina demner-fushman, yassine mrabet, and asmaben abacha.
2020. consumer health informationand question answering: helping consumers ﬁndanswers to their health-related information needs.
journal of the american medical informatics asso-ciation, 27(2):194–201..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages4171–4186..tobias falke, leonardo fr ribeiro, prasetya ajieutama,ido dagan, and iryna gurevych.
2019.ranking generated summaries by correctness: an in-teresting but challenging application for natural lan-guage inference.
in proceedings of the 57th annualmeeting of the association for computational lin-guistics, pages 2214–2220..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recognizingtextual entailment challenge.
in proceedings of theacl-pascal workshop on textual entailment andparaphrasing, pages 1–9..jeroen antonius gerardus groenendijk and martin jo-han bastiaan stokhof.
1984. studies on the seman-tics of questions and the pragmatics of answers.
ph.d. thesis, univ.
amsterdam..han guo, ramakanth pasunuru, and mohit bansal.
2018. soft layer-speciﬁc multi-task summarizationin pro-with entailment and question generation.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 687–697..anand gupta, manpreet kaur, shachar mirkin, adarshsingh, and aseem goyal.
2014. text summarizationthrough entailment-based minimum vertex cover.
inproceedings of the third joint conference on lex-ical and computational semantics (* sem 2014),pages 75–80..r bar haim, ido dagan, bill dolan, lisa ferro, danilogiampiccolo, bernardo magnini, and idan szpektor.
2006. the second pascal recognising textual entail-ment challenge.
in proceedings of the second pas-cal challenges workshop on recognising textualentailment..shankar iyer, nikhil dandekar, and kornél csernai.
2017. first quora dataset release: question pairs..alistair ew johnson, tom j pollard, lu shen,h lehman li-wei, mengling feng, moham-mad ghassemi, benjamin moody, peter szolovits,leo anthony celi, and roger g mark.
2016. mimic-iii, a freely accessible critical care database.
scien-tiﬁc data, 3(1):1–9..janet johnson, khalil mrini, allison moore, emiliafarkas, ndapa nkashole, michael hogarth, andnadir weibel.
2020. voice-based conversationalagents for older adults.
in proceedings of the chi2020 workshop on conversational agents for healthand wellbeing, honolulu, hawaii..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so, andjaewoo kang.
2020. biobert: a pre-trained biomed-ical language representation model for biomedicaltext mining.
bioinformatics, 36(4):1234–1240..chuan lei, vasilis efthymiou, rebecca geis, andfatma ozcan.
2020. expanding query answers onin edbt, pages 567–medical knowledge bases.
578..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, ves stoyanov, and luke zettlemoyer.
2019.bart: denoising sequence-to-sequence pre-trainingfor natural language generation,translation, andcomprehension.
arxiv preprint arxiv:1910.13461..haoran li, junnan zhu, jiajun zhang, and chengqingzong.
2018. ensure the correctness of the summary:incorporate entailment knowledge into abstractivesentence summarization.
in proceedings of the 27thinternational conference on computational linguis-tics, pages 1430–1441..chin-yew lin.
2004. rouge: a package for automaticin text summarization.
evaluation of summaries.
branches out, pages 74–81..xiaodong liu, pengcheng he, weizhu chen, and jian-feng gao.
2019. multi-task deep neural networksfor natural language understanding.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4487–4496..elena lloret, oscar ferrández, rafael munoz, andmanuel palomar.
2008. a text summarization ap-proach under the inﬂuence of textual entailment.
innlpcs, pages 22–31..clara mccreery, namit katariya, anitha kannan, man-ish chablani, and xavier amatriain.
2019. domain-relevant embeddings for medical question similarity.
arxiv preprint arxiv:1910.04192..1514yashar mehdad, giuseppe carenini, frank tompa, andraymond ng.
2013. abstractive meeting summa-in proceed-rization with entailment and fusion.
ings of the 14th european workshop on natural lan-guage generation, pages 136–146..kirk roberts and dina demner-fushman.
2016..in-teractive use of online health resources: a compari-son of consumer and professional questions.
jour-nal of the american medical informatics associa-tion, 23(4):802–811..abigail see, peter j liu, and christopher d manning.
2017. get to the point: summarization with pointer-generator networks.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1073–1083..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel bowman.
2018. glue:a multi-task benchmark and analysis platform forin proceedingsnatural language understanding.
of the 2018 emnlp workshop blackboxnlp: an-alyzing and interpreting neural networks for nlp,pages 353–355..chaochen wu, guan luo, chao guo, yin ren, annizheng, and cheng yang.
2020. an attention-basedmulti-task model for named entity recognition andintent analysis of chinese online medical questions.
journal of biomedical informatics, 108:103511..guokai yan and jianqiang li.
2018. medical questionsimilarity calculation based on weighted domain dic-in proceedings of the 2018 internationaltionary.
conference on big data and computing, pages 104–107..huiwei zhou, xuefei li, weihong yao, chengkunlang, and shixian ning.
2019. dut-nlp at mediqa2019: an adversarial multi-task network to jointlymodel recognizing question entailment and questionin proceedings of the 18th bionlpanswering.
workshop and shared task, pages 437–445..wei zhu, xiaofeng zhou, keqiang wang, xun luo,xiepeng li, yuan ni, and guotong xie.
2019. panlpat mediqa 2019: pre-trained language models, trans-fer learning and knowledge distillation.
in proceed-ings of the 18th bionlp workshop and shared task,pages 380–388..khalil mrini, chen chen, ndapa nakashole, nadirweibel, and emilia farcas.
2021a.
medical questionunderstanding and answering for older adults.
the3rd southern california (socal) nlp symposium..khalil mrini, franck dernoncourt, walter chang,emilia farcas, and ndapa nakashole.
2021b.
jointsummarization-entailment optimization for con-in proceed-sumer health question understanding.
ings of the second workshop on natural languageprocessing for medical conversations, pages 58–65,online.
association for computational linguistics..khalil mrini, franck dernoncourt, seunghyun yoon,trung bui, walter chang, emilias farcas, andndapa nakashole.
2021c.
ucsd-adobe atmediqa 2021: transfer learning and answer sen-tence selection for medical summarization.
in pro-ceedings of the 20th workshop on biomedical lan-guage processing, pages 257–262, online.
associa-tion for computational linguistics..ramesh nallapati, bowen zhou, cicero dos santos,ça˘glar guì‡lçehre, and bing xiang.
2016. abstrac-tive text summarization using sequence-to-sequencein proceedings of the 20thrnns and beyond.
signll conference on computational natural lan-guage learning, pages 280–290, berlin, germany.
association for computational linguistics..shashi narayan, shay b. cohen, and mirella lapata.
2018. don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-in proceedings of the 2018treme summarization.
conference on empirical methods in natural lan-guage processing, brussels, belgium..ramakanth pasunuru and mohit bansal.
2018. multi-reward reinforced summarization with saliency andentailment.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 2 (short papers), pages 646–653..ramakanth pasunuru, han guo, and mohit bansal.
2017. towards improving abstractive summariza-in proceedings oftion via entailment generation.
the workshop on new frontiers in summarization,pages 27–32..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392..craige roberts.
1996..information structure in dis-course: towards an integrated formal theory of prag-matics..1515