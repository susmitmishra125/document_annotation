multi-timeline summarization (mtls): improving timelinesummarization by generating multiple summaries.
yi yu1, adam jatowt2, antoine doucet3kazunari sugiyama1, masatoshi yoshikawa11kyoto university, japan2university of innsbruck, austria, 3university of la rochelle, franceyuyi@db.soc.i.kyoto-u.ac.jpadam.jatowt@uibk.ac.at, antoine.doucet@univ-lr.fr{kaz.sugiyama, yoshikawa}@i.kyoto-u.ac.jp.
abstract.
in this paper, we address a novel task, multi-ple timeline summarization (mtls), whichextends the ﬂexibility and versatility of time-line summarization (tls).
given any collec-tion of time-stamped news articles, mtls au-tomatically discovers important yet differentstories and generates a corresponding timelinefor each story.
to achieve this, we propose anovel unsupervised summarization frameworkbased on the two-stage afﬁnity propagationprocess.
we also introduce a quantitative eval-uation measure for mtls based on the previ-ous tls evaluation methods.
experimental re-sults show that our mtls framework demon-strates high effectiveness and mtls task canprovide better results than tls..1.introduction.
nowadays, online news articles are one of the mostpopular web documents.
however, due to a hugeamount of news articles available online, it is get-ting difﬁcult for users to effectively search, under-stand, and track the entire news stories.
to solvethis problem, a research area of timeline sum-marization (tls) has been established, which canalleviate the redundancy and complexity inherentin news article collections thereby helping usersbetter understand the news landscape..after the inﬂuential work on temporal sum-maries by swan and allan (2000), tls has at-tracted researchers’ attention.
most of works ontls (martschat and markert, 2018; steen andmarkert, 2019; gholipour ghalandari and ifrim,2020) have focused on improving the performanceof summarization.
however, their drawbacks are asfollows: (a) the methods work essentially on a ho-mogeneous type of datasets such as ones compiledfrom the search results of an unambiguous query(e.g., “bp oil spill”).
the requirements imposedon the input dataset make it hard for tls systems.
to generalize; (b) the output is usually a single time-line regardless of the size and the complexity of theinput dataset..we propose here the multiple timeline summa-rization (mtls) task that enhances and further gen-eralizes tls.
mtls automatically generates a setof timelines that summarize disparate yet importantstories, rather than always generating a single time-line as is in the case of tls.
an effective mtlsframework should: (a) detect key events includingboth short- and long-term events, (b) link eventsrelated to the same story and separate events be-longing to other stories, and (c) provide informativesummaries of constituent events to be incorporatedinto the generated timelines..mtls can also help to deal with the ambiguity,which is common in information retrieval.
for ex-ample, suppose that a user wants to get an overviewof news about a basketball player, michael jordan,from a large collection of news articles.
however,when a search engine over such a collection takes“michael jordan” as a query, it would likely returndocuments constituting a mixture of news about dif-ferent persons having the same name.
then, howcan a typical tls system return meaningful resultsif only a single timeline can be generated?
simi-larly, ambiguous queries such as “apple”, “ama-zon”, “java” require mtls solutions to producehigh quality results..to address this task, we further propose a two-stage afﬁnity propagation summarization frame-work (2saps).
it uses temporal information em-bedded in sentences to discover important events,and their linking information latent in news articlesto construct timelines.
2saps has several advan-tages: ﬁrstly, it is entirely unsupervised which isespecially suited to tls-related tasks as there arevery few gold summaries available for training su-pervised systems; secondly, both the number ofevents and the number of generated timelines are.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages377–387august1–6,2021.©2021associationforcomputationallinguistics377self-determined.
this allows our framework to bedependent only on the input document collection,instead of on human efforts..furthermore, the current tls evaluation mea-sures allow only 1-to-1 comparison (system- tohuman-generated timeline), which is not suitablefor mtls task where multiple timelines must becompared to (typically) multiple ground-truth time-lines.
therefore, we also propose a quantitativeevaluation measure for mtls based on the adapta-tion of the previous tls evaluation framework..given these points, our contributions in this work.
are summarized as follows:.
1. we propose a novel task (mtls), which auto-matically generates multiple, informative, anddiverse timelines from an input time-stampeddocument collection..2. we introduce a superior mtls model that out-performs all tls-adapted mtls baselines..3. we design an evaluation measure for mtlssystems by extending the original tls evalu-ation framework..2 related work.
2.1 timeline summarization.
since the ﬁrst work on timeline summariza-tion (swan and allan, 2000; allan et al., 2001),this topic has received much attention over theyears (alonso et al., 2009; yan et al., 2011a; zhaoet al., 2013; tran et al., 2013; li and li, 2013;suzuki and kobayashi, 2014; wang et al., 2016;takamura et al., 2011; pasquali et al., 2019, 2021).
in the following, we review the major approaches.
chieu and lee (2004) constructed timeline bydirectly selecting the top ranked sentences based onthe summed similarities within n-day long window.
yan et al.
(2011b) proposed evolutionary timelinesummarization (ets) to return the evolution tra-jectory along the timeline, consisting of individualbut correlated summaries of each date.
shahaf et al.
(2012) created information maps (maps) to helpusers understand domain-speciﬁc knowledge.
how-ever, the output consists of a set of storylines thathave intersections or overlaps, which is not appro-priate for a dataset that may contain quite differenttopics.
nguyen et al.
(2014) proposed a pipelineto generate timelines consisting of date selection,sentence clustering and sentence ranking..recently, martschat and markert (2018) adapteda submodular function model for tls task, which.
is originally used for multi-document summariza-tion (mds).
duan et al.
(2020) introduced the taskof comparative timeline summarization (cts),which captures important comparative aspects ofevolutionary trajectories in two input sets of docu-ments.
the output of the cts system is, however,always two timelines generated in a contrastiveway.
then, gholipour ghalandari and ifrim (2020)examined different tls strategies and categorizedtls frameworks into the following three types:direct summarization approaches, date-wise ap-proaches, and event detection approaches..to the best of our knowledge, the idea of multi-ple timeline summarization has not been formallyproposed yet.
table 1 compares the related tasks..2.2 timeline evaluation.
some works (yan et al., 2011b; chen et al., 2019;duan et al., 2020) evaluate timeline by only com-puting rouge scores (lin, 2004).
this way ig-nores the temporal aspect of a timeline, which isimportant in timeline summarization.
martschatand markert (2017) then proposed a framework,called tilse, to assess timelines from both textualand temporal aspects.
subsequently, tls works(steen and markert, 2019; gholipour ghalandariand ifrim, 2020; born et al., 2020) have followedthis framework to evaluate their models.
someresearches (tran et al., 2015; shahaf et al., 2012;alonso and shiells, 2013) also involved user stud-ies, in which users are required to score system-generated timelines based on varying criteria suchas relevance and understandability.
in section 5,we will adapt the tilse framework to mtls task..3 problem deﬁnition.
we formulate mtls task as follows:.
input: a time-stamped news article collectiond = {d1, d2, ..., d|d|}.
the collection can be stan-dalone or compiled from search results returned bya news search engine..t.output:.
timelines,.
a set of.
={t1, t2, .
.
.
, tk} is generated based on d, so thateach timeline ti includes a sequence of time/date11 , stiand summary pairs (ttil ) wherestij (i = 1, .
.
.
, k) are the summary sentences forthe time tti(j = 1, .
.
.
, l) and l is the length ofjti.
each timeline in t should be consistent andcoherent, yet different from other timelines..1 ), .
.
.
, (tti.
, sti.
l.1in this paper, time and date are used as synonyms..378tasks.
tls (most of which in section 2.1)cts (duan et al., 2020)ets (yan et al., 2011b)maps (shahaf et al., 2012)mtls (proposed task).
output1 timeline(cid:88).
(cid:88).
(cid:88).
output≥ 2 timelines.
automaticallydetermine k.input heterogeneouscollection.
(cid:88) (always 2).
(cid:88)(cid:88).
(cid:88).
(cid:88)(cid:88).
quantitativelyevaluate(cid:88)(cid:88)(cid:88).
(cid:88).
table 1: comparison between different tls related tasks (k is the number of generated timelines)..we note that while the traditional tls taskis limited as a document collection for itistypically coherent and homogeneous, mtls ismore ﬂexible as the input news collection canbe diverse.
for example, the input collectioncan be generated using a search query q com-posed of multiple entities or concepts like q ={“egypt”, “h1n1”, “iraq”} or by using an am-biguous query like q = {“michael”, “jordan”},or it can also consist of news articles crawled overa certain time span from multiple news sources.
generally, the more heterogeneous d is, the moretimelines could be produced.
the intuition behindthis idea is that users will need more structuredinformation to help them understand a relativelycomplex document collection..4 framework.
next, we present two key components of our frame-work: event generation module (sec.
4.1) and time-line generation module (sec.
4.2)..we ﬁrst make the following two assumptions:assumption 1: news articles sometimes retrospec-tively mention past events for providing necessarycontext to the target event, for underlying continu-ation, causality, etc.
assumption 2: sentences mentioning similardates have higher probability to refer to the sameevent than sentences with different dates..4.1 event generation module.
in this module, we extract important historicalevents from a document collection.
gholipour gha-landari and ifrim (2020) constructed events by sim-ply grouping articles with close publication datesinto clusters, resulting in lower accuracy.
note thatassumption 1 implies that a single news articlemay contain multiple events.
accordingly, in ourwork, the concept of event is more ﬁne-grained.
we deﬁne event as a set of sentences that describethe same real-world occurrence, typically using thesame identifying information (e.g., actions, enti-ties, locations).
this information is captured bysentence-bert (reimers and gurevych, 2019): a.pre-trained model on a transformer network wheresimilar meanings are positioned nearby in semanticvector space.
we then employ afﬁnity propaga-tion (ap) (frey and dueck, 2007) following steenand markert (2019) for clustering similar sentences.
ap algorithm groups data points by selecting a setof exemplars along with their followers due to mes-sage passing.
it operates over an afﬁnity matrixs, where s(i, j) denotes similarity between datapoints xi and xj..we observe that high semantic similarity doesnot always guarantee that sentences refer to thesame event.
especially, for some periodic events,similar happenings might have occurred severaltimes.
for example, a news article could includesentences reporting that brazil won the gold medalin the world cup (in 2002) while some other sen-tences in this document could recall that brazil haswon the ﬁrst place in the world cup in 1994. itis clear that those sentences describe two distinctevents, which would be grouped into one event ifonly semantic similarity is considered..therefore, based on assumption 2, we introduceanother key factor, temporal similarity, which en-hances the conﬁdence of how likely two sentenceswill refer to the same event.
we deﬁne each ele-ment s1(vi, vj) of afﬁnity matrix s1 as follows:.
s1(vi, vj) = α1 ·sdate(ti, tj)+(1 − α1)·scos(vi, vj),.
(1)where vi and vj denote different sentences, and tiand tj denote dates mentioned by vi and vj, respec-tively.2 in addition, sdate and scos denote the tem-poral and semantic similarities, respectively.
whilewe employ cosine similarity for the semantic simi-larity, we deﬁne temporal similarity sdate(i, j) toquantify how similar two dates are using equation(2):.
sdate(ti, tj) =.
1expγ·|ti−tj | ,.
(2).
where γ3 is the decay rate of the exponential func-.
2.we use heideltime (strötgen and gertz, 2013) for resolving temporalexpressions.
if a sentence does not explicitly mention any date, we assume itrefers to the publication date of the article.
we set γ = 0.05 in the experiments..3.
379(3).
(4).
tion.
the larger the time gap between two dates,the smaller the value of sdate..by passing messages of both semantic and tem-poral information between sentences, clusters con-sisting of exemplar and non-exemplar sentencesare constructed to form the candidate event set e.each cluster represents an event..event selection.
in a timeline, it is not neces-sary to show all events of a story as users usuallycare about the most important events only.
we de-sign an event selection step that is helpful for han-dling excessive number of events.
the selectionrelies on two measures: salience and consistencydeﬁned by equations (3) and (4), respectively:.
salience(e) =.
log(| e |)log(| d |).
,.
consistency(e) =.
(cid:80).
vi∈e,vi(cid:54)=ve.
| e | −1.
scos(vi,ve),.
where ve is the exemplar sentence in event e; | e |and | d | denote the number of sentences in e anddocument collection d, respectively..intuitively, important historical events would of-ten be mentioned by future news reports.
salienceof event is used to evaluate such importance andis computed as the relative frequency of sentencesabout that event compared with all sentences in thecollection.
on the other hand, consistency ensureshigh quality of events.
we then rank all candi-date events based on the weighted summed scoreof these two measures.
hereafter, we denote theweight of event salience as ζ1 and that of eventconsistency as 1 − ζ1..we select the top-scored events obtaining a newevent set e∗ by setting a threshold.
to avoid tuningits value, we set the value to one standard deviationfrom the mean (lower end)..4.2 timeline generation module.
while tls systems directly link all the identiﬁedevents, mtls requires their deeper understand-ing.
as described in section 1, an effective mtlsframework should link events related to the samestory and separate other unrelated events to differ-ent timelines.
to achieve this, we explain the fol-lowing steps in this module: event linking, time-line selection, and timeline summarizing..event linking.
according to assumption 1,current events can refer to related past events.
wethus deﬁne a reference matrix r, in which eachelement r(ei, ej) denotes the degree of reference.
between two events ei and ej.
as events in ourwork are represented by sentences and a sentencebelongs to a single event, r(ei, ej) can be reﬂectedby counting patterns of sentence co-occurrences indocuments.
formally, r(vi, vj) represents the casewhere two sentences vj and vi refer to each otheras deﬁned by equation (5):.
r(vi, vj) =.
(cid:26) 10.vi,vj ∈ d ∧vi ∈ ek,vj ∈ el, ek (cid:54)= elotherwise,.
(5).
where d is an article, ek and el are elements in e∗.
the degree of reference between ei and ej is.
then deﬁned as follows:.
r(ei, ej) =.
(cid:80).
(cid:80).
v1∈ei.
v2∈ej| ei | · | ej |.
r(v1, v2).
,.
(6).
where |ei| and |ej| are sizes of ei, ej, respectively.
we then construct a graph of events where eachnode is an e ∈ e∗, and the value of an edge reﬂectsthe connection degree between a pair of two events.
we reuse ap algorithm to detect the communityof events over the afﬁnity matrix s2 deﬁned byequation (7):.
s2(ei, ej) = α2 · r(ei, ej) + (1 − α2) · scos(ei, ej),.
(7).
where scos(ei, ej) denotes cosine similarity be-tween ei and ej to capture semantic similarity.
based on the afﬁnity matrix s2, ap ﬁnally gen-erates clusters, i.e., the initial timeline set, t ..timeline selection.
in order to ensure the qual-ity of constructed timelines, we deﬁne criteria toselect high-quality timelines from t .
similar toevent selection described in section 4.1, we alsouse two indicators to evaluate the quality of a time-line.
we deﬁne timeline salience as the averagescore of event salience of all events within thetimeline, and timeline coherence as the averageof semantic similarity scores between any chrono-logically4 adjacent events deﬁned by equation (8):.
coherence(t ) =.
(cid:80).
ei,ei+1∈t scos(ei, ei+1)| t | −1.
,.
(8).
where | t | is the size of a timeline, i.e., the numberof events in this timeline..intuitively, important timelines, which reﬂect im-portant stories in the document collection, are morelikely to be preferred by users.
timeline saliencecaptures this importance by passing the importanceof its components (i.e., events), while timeline co-herence ensures that the story expressed by thetimeline is consistent..4the time of an event e is given by its exemplar sentence..380we rank timelines based on a weighted sum oftimeline salience and timeline coherence.
theweight of timeline salience is denoted as ζ2; thusthe weight of timeline coherence is 1−ζ2.
we thenselect the top-scored elements from the timeline sett based on a threshold.
same as before, we set thevalue to one standard deviation from the mean..timeline summarizing.
by previous steps, wehave now obtained multiple timelines {t1, t2, ...},where t is a list of events {e1, e2, ...}.
however,it is not feasible to show all contents of each e asit usually contains many sentences.
we use onlythe exemplar sentence in event since exemplar isthe most typical and representative member in thegroup..in addition, it is possible that two events ei andej occur on the same day.
in this case, we concate-nate their exemplar sentences..timeline tagging.
this step is an add-on tomtls systems.
to better understand the storiesof constructed timelines, we believe that it shouldbe helpful for users to also obtain a label for eachtimeline.
as described in section 1, the input doc-ument collection may be composed of differenttopics or of one topic discussed through differentaspects.
for example, among the timelines gener-ated based on the topic syria, one timeline mightsummarize the story about syrian civil war whileanother might be about syrian political elections.
alabel should then help people understand the storyof the timeline.
we simply select the 3 most fre-quent words among events (excluding stopwords)for each timeline as its label..5 evaluation framework.
5.1 tls evaluation.
tls evaluation relies on rouge score and itsvariants as follows:.
concatenation-based rouge (concat).
itconsiders only the textual overlap between concate-nated system summaries and ground-truth, whileignoring all date information of timeline (yan et al.,2011b; nguyen et al., 2014; wang et al., 2016)..date-agreement rouge (agreement).
it mea-sures both textual and temporal information over-lap by computing rouge score only when thedate in the system-generated timelines matches theone of the ground-truth timeline (tran et al., 2013).
otherwise, its value is 0..alignment-based rouge.
it linearly penal-izes the rouge score by the distances of datesor/and summary contents.
martschat and markert(2017) proposed three types of this metric: align,align+, align+m:1 (align by date, align by date andcontents, align by date and contents where the mapfunction is non-injective, respectively)..date selection (d-select).
it evaluates how wellthe model works in selecting correct dates in theground-truth (martschat and markert, 2018)..5.2 mtls evaluation.
the evaluation methods for tls cannot directly as-sess the performance of mtls systems as there aremultiple output timelines and multiple ground-truthtimelines.
concretely, given an input collectiond, corresponding ground-truth timeline set g ={g1, g2, ...gk1} (k1 ≥ 1), and system-generatedtimeline set t = {t1, t2, ..., tk2} (k2 ≥ 1), eval-uation metrics need information to automatically“match” the ground-truth timeline when evaluatingti.
therefore, we make the system ﬁnd the closestground-truth g∗ to timeline t as follows:.
g∗ = arg max.
fm(t, g),.
g∈g.
(9).
where fm is the tls evaluation function to com-pute the score between t and g based on metricm, which can be either concat, agreement, align,align+, align+m:1, or d-select.
then, the overallperformance of the mtls models is computed bytaking the average of all the members in t ..6 experimental setup.
the goal of our experiments is to answer the fol-lowing research questions (rqs):.
rq1: do mtls models produce more mean-.
ingful output than tls models?.
rq2: how does 2saps framework perform onmtls task compared with other mtls baselines?
rq3: how effective are the components of themodules in 2saps?
how do parameter changes inthe model affect the results?.
6.1 datasets.
we note that there is no available dataset for mtlstask, thus we construct mtls datasets5 extendingexisting tls datasets.
tran et al.
released time-line17 (binh tran et al., 2013) and crisis (tranet al., 2015) datasets for tls over news articles..5the datasets are now available at.
https://yiyualt.github.io/mtlsdata/..381nametimeline17crisis.
#topics94.
#groundtruth avg.timespan.
1722.
250 days343 days.
#docs.
4,6509,242.
#sents.
183,782331,044.table 2: statistics on tls datasets..segments) by partition/division algorithms; thenadopt tls techniques to generate a timeline foreach sub-dataset (segment).
we now describe thechoices for each step..l=1 d1:egypt d2:ﬁnan d3:haiti d4:h1n1 d5:libya.
dataset division approaches:.
table 3: mtls datasets used for our experiments..tls approaches:.
l=2.
l=3.
l=4.
l=5.
d6:egypt+libya d7:haiti+iraqd8:h1n1+haiti d9:ﬁnan+mjd10:egypt+mjd11:egypt+h1n1+iraq d12:ﬁnan+iraq+syriad13:egypt+ iraq+mj d14:ﬁnan+h1n1+mjd15:ﬁnan+libya+mjd16:egypt+ﬁnan+haiti+iraq d17:ﬁnan+h1n1+iraq+mj d18:h1n1+haiti+iraq+mj d19:ﬁnan+h1n1+haiti+mj d20:egypt+haiti+iraq+mjd21:ﬁnan+h1n1+haiti+iraq+mjd22:h1n1+haiti+iraq+mj+syriad23:egypt+ﬁnan+haiti+mj+syriad24:egypt+ﬁnan+ iraq+mj+syriad25:egypt+ﬁnan+h1n1+haiti+mj.
table 2 shows their statistics.
to assure high com-plexity of data, we generate multiple datasets fromtls datasets by varying degree of story mixtures.
we construct mtls datasets based on combiningtls datasets, according to the following procedure:(1) set the number of topics l used to generatea new dataset; (2) from tls datasets, randomlychoose l topics, then merge their document collec-tions into a new dataset d along with grouping theirassociated ground-truth timelines into g.6 (3) re-peat steps (1) and (2).
here, the value of l reﬂectsthe complexity of the dataset.
the more topics thedataset contains, the more complex it is..we repeated the steps (1)~(3) on timeline177and ﬁnally created 25 datasets as shown in ta-ble 3. timeline17 contains 9 document collec-tions, covering the following topics: “bp oil spill”(bpoil), “inﬂuenza h1n1” (h1n1), “michael jack-son death” (mj), “libyan war” (libya), “egyptianprotest” (egypt), “financial crisis” (ﬁnan), “haitiearthquake” (haiti), “iraq war” (iraq), “syrian cri-sis” (syria)..6.2 baselines.
• random.
we randomly decide the number ofsegments from 1 to 10. then, we assign anews article to a random segment..• lda (latent dirichlet allocation) (blei et al.,2003).
given a dataset, we ﬁrst use lda todetect the main topics in the dataset.
then, weassign each news article to its dominant topic..• k-means (macqueen et al., 1967).
we use.
k-means algorithm in scikit-learn.8.
• chieu2004 (chieu and lee, 2004): it is afrequently used unsupervised tls baselinewhich selects the top-ranked sentences basedon summed similaries within n-day window..• martschat2018 (martschat and markert,2018): it is one of the state-of-the-art tlsmodels and is also the ﬁrst work to establishformal experimental settings for tls task.
weuse the implementation given by the authors.9.
• ghalandari2020 (gholipour ghalandariand ifrim, 2020): it constructs timeline by ﬁrstpredicting the important dates via a simple re-gression model and then selecting importantsentences for each date.10.
we combine the above 3 dataset division ap-proaches and 3 tls approaches and thus yield 9baselines..6.3 experimental settings.
concerning the characteristics of mtls task andour datasets, the experimental settings differ fromthe tls settings applied in martschat and markert(2018).
in particular, the settings are:.
as there are no ready models for mtls task, wedesign the baselines as “divide-and-summarize” ap-proaches.
the underlying idea is: ﬁrst segment theinput dataset into sub-datasets (subsequently called.
6if a topic has multiple ground-truth timelines, we pick onethat has length closest to the average length of the timelinesfor that topic..7we note that crisis contains only 4 topics, resulting in.
few possible combinations, so we ﬁnally decided to skip it..• when generating timelines, none of the com-pared models knows the actual value of l(i.e., l is not an input data).
the stratiﬁcationgiven in table 3 is shown only for the readerto explain the datasets’ construction method..8https://scikit-learn.org/9https://github.com/smartschat/tilse.
10https://github.com/complementizer/.
news-tls..382• for the dataset-division algorithms, lda andk-means, we use different techniques to ﬁndoptimal number of segments.
for lda, weevaluate topic coherence measure (cv score)(röder et al., 2015) for topic numbers rangingfrom 1 to 10, and then choose the optimalnumber.
for k-means, we use silhouette value(rousseeuw, 1987) to determine the optimalnumber of segments..• all the compared methods do not take theinformation of the ground-truth as input.
thatis, the number of dates, the average number ofsummary sentences per date, the total numberof summary sentences, the ground-truth startdates, and end dates are all unknown..• we set the length of timelines to 20 and sum-.
mary length to 2 sentences per date..7 results and discussion.
7.2 performance of 2saps.
we now investigate the performance of our frame-work to answer rq2.
table 5 shows the over-all performance of mtls systems.
we observethat 2saps achieves the best performance interms of all rouge metrics.
in particular, whencompared with chieu2004, martschat2018 andghalandari2020 in terms of concat rouge-1 score, it outperforms them by 52.9%, 12.2%,and 16.4%, respectively.
we also observe thatghalandari2020 method still achieves the bestperformance among baselines except for concatrouge-1.
furthermore, it is worth noticing that k-means works best in dividing datasets.
on average,k-means outperforms random and lda by 15%and 7.2%, respectively, in terms of concat rouge-1. finally, compared with the best-performing base-line, k-means-ghalandari2020, our 2saps out-performs it by 9.9%, 15.1%, 0%, 10%, 4.7%, 3.6%,19.1%, in terms of concat (rouge-1,rouge-2), align+m:1 (rouge-1,rouge-2), agreement(rouge-1,rouge-2) and d-select, respectively..7.1 mtls vs. tls.
7.3 ablation study.
we ﬁrst address rq1 to show the necessity ofmtls and to demonstrate that tls performspoorly when an input dataset contains mixture ofdocuments on different stories.
to achieve this, wecompare results of mtls baselines with a standardtls approach.
table 4 shows the performancecomparison between tls and mtls baselinesbased on martchat2018.
for fair comparisonin this ﬁrst experiment, we select only one time-line from mtls outputs that is most similar to thetimeline generated by tls.
we observe that whenl = 1, 2, mtls underperforms tls by 15.1%,4.8% in terms of align+m:1 rouge-1, respec-tively.
however, it outperforms tls by 150%,117.1%, and 94.7% when l equals 3,4,5, respec-tively.
this indicates that as the complexity of inputdocument collection increases (higher l values),tls systems do not produce good results whencompared to mtls ones.
in real world scenarios,it is rather rare that the input dataset is clean enoughto contain only a single topic.
thus, these resultssuggest that mtls approach should in practice bemore useful than tls.
the results for the other twotls algorithms introduced in section 6.2 show asimilar trend, too.
furthermore, the example out-puts of tls and mtls systems are also availableas supplementary materials..we turn to the ﬁrst part of rq3.
we conduct ab-lation tests on event selection (es) and timelineselection (ts) components.
table 6 shows thechanges of different models.
we observe that with-out es, d-select and align+m:1 rouge-2 scoresdecrease 14.6% and 42.2% compared with 2saps.
the plausible reason is that without es, many unim-portant dates and events are included in a timeline,resulting in low recall of correct dates.
on the otherhand, without ts component, the generated time-line set tends to contain noisy timelines, causinglow rouge-1 as the performance drops by 18.8%..7.4 parameter impact.
we now analyze the impact of key parameters, α1,α2, ζ1, ζ2.
α1 and α2 directly inﬂuence the qualityof generated events and timelines, while ζ1 andζ2 indirectly affect the model’s performance bycontrolling the selection steps.
figure 1 shows theperformance of 2saps under concat rouge-1,align+m:1 rouge-1, and agreement rouge-1.
in particular, we observe that: a smaller valueof α1 (from 0.1 to 0.4) gives better results thana larger value (figure 1a).
when α1 turns to 1,ap algorithm does not converge, and the valuesof all measures become 0. the plausible reasonfor this could be that when sentence dates are very.
383model.
tls (martschat2018).
mtls (k-means-martschat2018).
mtls (lda-martschat2018).
metricconcat (rouge-1)concat (rouge-2)align+m:1 (rouge-1)align+m:1 (rouge-2)concat (rouge-1)concat (rouge-2)align+m:1 (rouge-1)align+m:1 (rouge-2)concat (rouge-1)concat (rouge-2)align+m:1 (rouge-1)align+m:1 (rouge-2).
l=10.2870.0610.0530.0110.2720.0560.0460.0090.2740.0540.0430.007.l=20.3100.0690.0630.0170.3640.0840.0630.0140.3320.0740.0570.009.l=30.2140.0380.0320.0110.3620.0850.0820.0260.3630.0890.0780.027.l=40.2610.0440.0410.0070.4000.1000.0970.0340.3350.0790.0800.024.l=50.2020.0350.0380.0070.3900.0840.0820.0240.2730.0590.0650.018.table 4: performance comparison between tls and mtls systems.
for fair comparisons, we compare the singletimeline generated by tls model with the most related timeline generated by mtls models..concat.
align+m:1.agreement.
d-select.
rouge-1 rouge-2 rouge-1 rouge-2 rouge-1 rouge-2.
f1.
randomldak-means.
randomldak-means.
randomldak-means.
0.1910.1920.229.
0.2540.2890.291.
0.2530.2680.284.
0.0270.0350.046.
0.0490.0680.071.
0.0480.0620.073.
0.0190.0230.027.
0.0440.0620.061.
0.0680.0850.096.
0.0040.0050.006.
0.0090.0170.017.
0.0150.0250.030.
0.0100.0130.014.
0.0370.0520.051.
0.0580.0760.085.
0.0020.0040.004.
0.0070.0150.015.
0.0130.0240.028.
0.0750.0890.096.
0.3520.3870.376.
0.4140.4400.467.mtls methods.
baselines.
chieu2004.
martschat2018.
ghalandari2020.
our method.
2saps.
table 5: overall performance obtained by the baselines and the proposed methods over d1 ~d25 datasets..0.312.
0.084.
0.096.
0.033.
0.089.
0.029.
0.556.
2saps w/o es2saps w/o ts2saps.
d-select rouge-1 rouge-20.4750.5020.556.
0.0850.0780.096.
0.0190.0230.033.table 6: ablation results of 2saps model, showingchanges of align+m:1 rouge and d-select f1 scores..close, the elements of transition matrix differ onlyslightly, resulting in non-convergence..figure 1b shows the impact of the reference re-lation in linking events.
the values of all metricsincrease as α2 increases.
it makes sense that ref-erence relation exerts an important role in linkingevents into timelines, thus a higher value is nec-essary.
however, when α2 is over 0.9, the perfor-mance drops because when news articles providefew contextual events (e.g., background events, re-lated events, etc.
), then the reference relation be-tween events becomes unreliable..ζ1 controls the impact of event salience de-scribed in section 4.1. another corresponding fac-tor is event consistency, which is weighted by 1-ζ1.
figure 1c shows that the model with larger valuesof ζ1 underperforms the ones with relatively smallvalues of ζ1 (from 0.2 to 0.4), indicating that con-.
(a) α1: temporal similarity.
(b) α2: reference relation.
(c) ζ1: event salience.
(d) ζ2: timeline salience.
figure 1: impact of parameters on f1 score..sistency of event matters more than its salience inselecting high-quality events.
finally, in figure 1d,we observe that along with the increase of ζ2, theperformance of all metrics decrease, suggestingthat the coherence of timeline is more effectivethan salience in selecting good timelines..7.5 limitations.
our 2saps model works essentially on the unitof sentences and constructs a graph where eachsentence is a node and edge is the relation between.
384sentences.
it has then a complexity of o(n2).
fu-ture work could address this by simplifying graphstructure and providing approximate solutions tocover also the cases of processing large datasets.
another solution is to select only important sen-tences from news articles using the combination ofclassiﬁcation, summarization or ﬁltering..8 conclusions.
we introduced mtls task to generalize the time-line summarization problem.
mtls improves theperformance of timeline summarization by gener-ating multiple summaries.
we conducted exper-iments to ﬁrst show that given a heterogeneoustime-stamped news article collection, tls usuallydoes not produce satisfactory result.
we further pro-posed 2saps, a two-stage clustering-based frame-work, to effectively solve mtls task.
further-more, we extended tls datasets to mtls datasets,as well as introduced a novel evaluation measurefor mtls.
experimental results show that 2sapsoutperforms mtls baselines which follow the“divide-and-summarize” strategy.
our work sig-niﬁcantly improves the generalization ability oftimeline summarization and can provide users witheasier access to news collections.
as an unsuper-vised approach that does not require costly trainingdata, it can be applied to any potential datasets andlanguages..in future work, we plan to test our approach onadditional mtls datasets.
we will also investigatescenarios in which mtls can enhance informationretrieval systems operating over news article col-lections.
for users searching over large temporalcollections, structuring the returned results into aseries of timelines could prove beneﬁcial, insteadof returning a usual list of interwoven documentsthat relate to different stories or periods..acknowledgments.
we greatly appreciate the authors in conll’18 pa-per (martschat and markert, 2018) for making theirdata public.
in particular, we wish to thank sebas-tian martschat for his great support in discussionsabout the experiment setup and reproduction.
wealso want to thank anonymous reviewers for theirinvaluable feedback..references.
james allan, rahul gupta, and vikas khandelwal.
2001. temporal summaries of new topics.
in pro-ceedings of the 24th annual international acm si-gir conference on research and development ininformation retrieval (sigir ’01), pages 10–18..omar alonso, michael gertz, and ricardo baeza-yates.
2009. clustering and exploring search re-in proceed-sults using timeline constructions.
ings of the 18th acm conference on informationand knowledge management (cikm ’09), pages 97–106..omar alonso and kyle shiells.
2013. timelines asin pro-summaries of popular scheduled events.
ceedings of the 22nd international conference onworld wide web (www ’13), pages 1037–1044..giang binh tran, mohammad alrifai, and datquoc nguyen.
2013.predicting relevant newsevents for timeline summaries.
in proceedings ofthe 22nd international conference on world wideweb (www ’13), pages 91–92..david m blei, andrew y ng, and michael i jordan.
2003. latent dirichlet allocation.
journal of ma-chine learning research, 3(jan):993–1022..leo born, maximilian bacher, and katja markert.
2020. dataset reproducibility and ir methods inin proceedings of thetimeline summarization.
12th language resources and evaluation confer-ence (lrec’20), pages 1763–1771..xiuying chen, zhangming chan, shen gao, meng-hsuan yu, dongyan zhao, and rui yan.
2019.learning towards abstractive timeline summariza-the 28th internationaltion.
joint conference on artiﬁcial intelligence (ijcai-19), pages 4939–4945..in proceedings of.
hai leong chieu and yoong keok lee.
2004. querybased event extraction along a timeline.
in pro-ceedings of the 27th annual international acm si-gir conference on research and development ininformation retrieval (sigir ’04), pages 425–432..yijun duan, adam jatowt, and masatoshi yoshikawa.
2020. comparative timeline summarization via dy-in pro-namic afﬁnity-preserving random walk.
ceedings of the 24th european conference on artiﬁ-cial intelligence (ecai’20), pages 1778–1785..brendan j frey and delbert dueck.
2007. clusteringby passing messages between data points.
science,315(5814):972–976..demian gholipour ghalandari and georgiana ifrim.
examining the state-of-the-art in news2020.in proceedings of thetimeline summarization.
58th annual meeting of the association for compu-tational linguistics (acl’20), pages 1322–1334..385jiwei li and sujian li.
2013. evolutionary hierarchi-cal dirichlet process for timeline summarization.
in proceedings of the 51st annual meeting of the as-sociation for computational linguistics (acl’13),pages 556–560..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in proceedings ofthe 42th annual meeting of the association for com-putational linguistics (acl’04), pages 74–81..james macqueen et al.
1967. some methods for clas-siﬁcation and analysis of multivariate observations.
in proceedings of the 5th berkeley symposium onmathematical statistics and probability, pages 281–297..sebastian martschat and katja markert.
2017. improv-ing rouge for timeline summarization.
in proceed-ings of the 15th conference of the european chap-ter of the association for computational linguistics(eacl’17), pages 285–290..sebastian martschat and katja markert.
2018. atemporally sensitive submodularity framework forin proceedings of thetimeline summarization.
22nd conference on computational natural lan-guage learning (conll’18), pages 230–240..kiem-hieu nguyen, xavier tannier, and véroniquemoriceau.
2014. ranking multidocument event de-scriptions for building thematic timelines.
in pro-ceedings of the 25th international conference oncomputational linguistics (coling 2014), pages1208–1217..dafna shahaf, carlos guestrin, and eric horvitz.
2012.trains of thought: generating information maps.
in proceedings of the 21st international conferenceon world wide web (www ’12), pages 899–908..julius steen and katja markert.
2019. abstractivetimeline summarization.
in proceedings of the 2ndworkshop on new frontiers in summarization (new-sum’19), pages 21–31..jannik strötgen and michael gertz.
2013. multilingualand cross-domain temporal tagging.
languageresources and evaluation, 47(2):269–298..satoko suzuki and ichiro kobayashi.
2014. on-linesummarization of time-series documents using agraph-based algorithm.
in proceedings of the 28thpaciﬁc asia conference on language, informationand computing (paclic’14), pages 470–478..russell swan and james allan.
2000. automatic gen-in proceedings oferation of overview timelines.
the 23rd annual international acm sigir confer-ence on research and development in informationretrieval (sigir ’00), pages 49–56..hiroya takamura, hikaru yokono, and manabu oku-mura.
2011. summarizing a document stream.
inproceedings of the 33rd european conference on in-formation retrieval (ecir 2011), pages 177–188..giang tran, mohammad alrifai, and eelco herder.
timeline summarization from relevant2015.in proceedings of the 37th europeanheadlines.
conference on information retrieval (ecir 2015),pages 245–256..arian pasquali, ricardo campos, alexandre ribeiro,brenda santana, alípio jorge, and adam jatowt.
2021. tls-covid19: a new annotated corpusfor timeline summarization.
in proceedings of the43rd european conference on information retrieval(ecir 2021), pages 497 – 512..giang binh tran, tuan a tran, nam-khanh tran,mohammad alrifai, and nattiya kanhabua.
2013.leveraging learning to rank in an optimizationin pro-framework for timeline summarization.
ceedings of sigir 2013 workshop on time-awareinformation access (#taia’13)..arian pasquali, vítor mangaravite, ricardo campos,alípio mário jorge, and adam jatowt.
2019. inter-active system for automatically generating tempo-ral narratives.
in proceedings of the 41st europeanconference on information retrieval (ecir 2019),pages 251–255..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conferenceon empirical methods in natural language process-ing (emnlp-ijcnlp 2019), pages 3982–3992..michael röder, andreas both, and alexander hinneb-urg.
2015. exploring the space of topic coherencemeasures.
in proceedings of the 8th acm interna-tional conference on web search and data mining(wsdm ’15), pages 399–408..peter j rousseeuw.
1987. silhouettes: a graphical aidto the interpretation and validation of cluster analy-sis.
journal of computational and applied mathe-matics, 20:53–65..william yang wang, yashar mehdad, dragomir radev,and amanda stent.
2016. a low-rank approxi-mation approach to learning joint embeddings ofnews stories and images for timeline summariza-tion.
in proceedings of the 15th conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies (naacl-hlt 2016), pages 58–68..rui yan, liang kong, congrui huang, xiaojun wan,xiaoming li, and yan zhang.
2011a.
timeline gen-eration through evolutionary trans-temporal sum-marization.
in proceedings of the 2011 conferenceon empirical methods in natural language process-ing (emnlp ’11), pages 433–443..rui yan, xiaojun wan, jahna otterbacher, liang kong,xiaoming li, and yan zhang.
2011b.
evolutionarytimeline summarization: a balanced optimizationin proceed-framework via iterative substitution.
ings of the 34th international acm sigir confer-ence on research and development in informationretrieval (sigir ’11), pages 745–754..386xin wayne zhao, yanwei guo, rui yan, yulan he,and xiaoming li.
2013. timeline generation withsocial attention.
in proceedings of the 36th inter-national acm sigir conference on research anddevelopment in information retrieval (sigir ’13),pages 1061–1064..387