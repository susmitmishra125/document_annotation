subsequence based deep active learningfor named entity recognition.
puria radmard1,2,3pr450@cam.ac.uk.
yassir fathullah2yf286@cam.ac.uk.
aldo lipani1,3aldo.lipani@ucl.ac.uk.
1university college london2university of cambridge3vector ai.
abstract.
active learning (al) has been successfullyapplied to deep learning in order to drasti-cally reduce the amount of data required toachieve high performance.
previous workshave shown that lightweight architectures fornamed entity recognition (ner) can achieveoptimal performance with only 25% of theoriginal training data.
however, these meth-ods do not exploit the sequential nature oflanguage and the heterogeneity of uncertaintywithin each instance, requiring the labelling ofwhole sentences.
additionally, this standardmethod requires that the annotator has accessto the full sentence when labelling.
in thiswork, we overcome these limitations by allow-ing the al algorithm to query subsequenceswithin sentences, and propagate their labels toother sentences.
we achieve highly efﬁcientresults on ontonotes 5.0, only requiring 13%of the original training data, and conll 2003,requiring only 27%.
this is an improvement of39% and 37% compared to querying full sen-tences..1.introduction.
the availability of large datasets has been key to thesuccess of deep learning in natural language pro-cessing (nlp).
this has galvanized the creation oflarger datasets in order to train larger deep learningmodels.
however, creating high quality datasets isexpensive due to the sparsity of natural language,our inability to label it efﬁciently compared to otherforms of data, and the amount of prior knowledgerequired to solve certain annotation tasks.
sucha problem has motivated the development of newactive learning (al) strategies which aim to efﬁ-ciently train models, by automatically identifyingthe best training examples from large amounts of.
code is made available on: https://github.com/.
puria-radmard/rfl-sbdalner.
unlabeled data (wei et al., 2015; wang et al., 2017;tong and koller, 2002).
this tremendously reduceshuman annotation effort as much fewer instancesneed to be labeled manually..to minimise the amount of data needed to traina model, al algorithms iterate between training amodel, and querying information rich instances tohuman annotators from a pool of unlabelled data(huang et al., 2014).
this has been shown to workwell when the queries are ‘atomic’—a single an-notation requires a unit labour, and describes en-tirely the instance to be annotated.
conversely,each instance of structured data, such as sequences,require multiple annotations.
hence, such query se-lection methods can result in a waste of annotationbudget (settles, 2011)..for example,.
in named entity recognition(ner), each sentence is usually considered an in-stance.
however, because each token has a sepa-rate label, annotation budgeting is typically doneon a token basis (shen et al., 2017).
budget wast-ing may therefore arise from the heterogeneity ofuncertainty across each sentence; a sentence cancontain multiple subsequences (of tokens) of whichthe model is certain on some and uncertain on oth-ers.
by making the selection at a sentence level,although some budget is spent on annotating uncer-tain subsequences, the remaining budget may bewasted on annotating subsequences for which anannotation is not needed..it can therefore be desirable for annotators tolabel subsequences rather than the full sentences.
this gives a greater ﬂexibility to al strategies tolocate information rich parts of the input with im-proved efﬁciency – and reduces the cognitive de-mands required of annotators.
annotators may infact perform better if they are asked to annotateshorter sequences, because longer sentences cancause boredom, fatigue, and inaccuracies (rzeszo-tarski et al., 2013)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4310–4321august1–6,2021.©2021associationforcomputationallinguistics4310in this work, we aim to improve upon the ef-ﬁciency of al for ner by querying for subse-quences within each sentence, and propagatinglabels to unseen, identical subsequences in thedataset.
this strategy simulates a setup in whichannotators are presented with these subsequences,and do not have access to the full context, ensuringthat their focus is centred on the tokens of interest.
we show that al algorithms for ner tasks thatuse subsequences, allowing training on partiallylabelled sentences, are more efﬁcient in terms ofbudget than those that only query full sentences.
this improvement is furthered by generalising ex-isting acquisition functions (§ 4.1) for use withsequential data.
we test our approaches on twoner datasets, ontonotes 5.0 and conll 2003.on ontonotes 5.0, shen et al.
(2017) achieve state-of-the-art performance with 25% of the originaldataset querying full sentences, while we requireonly 13% of the dataset querying subsequences.
on conll 2003, we show that the al strategyof shen et al.
(2017) requires 50% of the datasetto achieve the same results as training on the fulldataset, while ours requires only 27%..contributions of this paper are:.
1. improving the efﬁciency of al for ner byallowing querying of subsequences over fullsentences;.
2. an entity based analysis demonstrating thatsubsequence querying al strategies tend toquery more relevant tokens (i.e., tokens be-longing to entities);.
3. an uncertainty analysis of the queries madeby both full sentence and subsequence query-ing methods, demonstrating that querying fullsentences leads to selecting more tokens towhich the model is already certain..2 related work.
al algorithms aim to query information rich datapoints to annotators in order to improve the perfor-mance of the model in a data efﬁcient way.
tradi-tionally these algorithms choose data points whichlie close to decision boundaries (pinsler et al.,2019), where uncertainty is high, in order for themodel to learn more useful information.
this mea-sure of uncertainty, measured through acquisitionfunctions, are therefore vital to al.
key functionsinclude predictive entropy (maxent) (gal et al.,2017), mutual information between model poste-rior and predictions (bald) (houlsby et al., 2011;.
gal et al., 2017), or the certainty of the modelwhen making label predictions (here called lc)(mingkun li and sethi, 2006).
these techniquesensure all instances used for training, painstak-ingly labelled by experts, have maximum impacton model performance.
there has been explorationof uncertainty and deep learning based al for ner(chen et al., 2015; shen et al., 2017; settles andcraven, 2008; fang et al., 2017).
these approacheshowever, treat each sentence as a single query in-stead of a collection of individually labelled to-kens.
in these methods, the acquisition functionsthat score sentences aggregate token-wise scores(through summation or averaging)..other works forgo this aggregation, queryingsingle tokens at a time (tomanek and hahn, 2009;wanvarie et al., 2011; marcheggiani and arti`eres,2014).
these works show that al for ner can beimproved by taking the single token as a unit query,and use semi-supervision (reddy et al., 2018; is-cen et al., 2019) for training on partially labelledsentences (muslea et al., 2002).
however, queryingsingle-tokens is inapplicable in practise because,either a) annotators have access to the full sentencewhen queried but can only label one token, whichwould lead to frustration as they are asked to readthe full sentence but only annotate a single token, orb) annotators only have access to the token of inter-est, which means that they would not have enoughinformation to label tokens differently based ontheir context, leading to annotators labeling anyunique token with the same label.
moreover, if thelatter approach was somehow possible, we wouldbe able to reduce the annotation effort to the annota-tion of only the unique tokens forming the dataset,its dictionary.
furthermore, all of these past worksuse conditional random fields (crfs) (laffertyet al., 2001), which have since been surpassed asthe state-of-the-art for ner (and most nlp tasks)by deep learning models (devlin et al., 2019)..in this work we follow the approach where anno-tators only have access to subsequences of multipletokens.
however, instead of making use of singletokens, we will query more than one token, provid-ing enough context to the annotators.
this allowsthe propagation of these annotations to identicalsubsequences in the dataset, further reducing thetotal annotation effort..43113 background.
3.1 active learning algorithms.
most al strategies are based on a repeating score,query and ﬁne-tune cycle.
after initially training anner model with a small pool of labelled examples,the following is repeated: (1) score all unlabelledinstances, (2) query the highest scoring instancesand add them to training set, and, (3) ﬁne-tune themodel using the updated training set (huang et al.,2014)..to describe this further, notation and proposedtraining process is introduced, with details in fol-lowing sections.
first, the sequence tagging dataset,denoted by d = {(x(n), y(n))}nn=1, consists of acollection of sentence and ground truth labels.
thei-th token of the n-th sentence (y(n)) has a labely(n)i = c with c belonging to c = {c1, ..., ck}.
we also differentiate between the labelled and un-labelled datasets, dl and du , which initially areempty and equal to d. finally, we ﬁx a as the totalnumber of tokens queried in each iteration..i.
3.2 acquisition functions.
instances in the unlabelled pool are queried us-ing an acquisition function.
this function aims toquantify the uncertainty of the model when generat-ing predictive probabilities over possible labels foreach instance.
instances with the highest predictiveuncertainty are deemed as the most informative formodel training.
previously used acquisition func-tions such as least conﬁdence (lc) and maximumnormalized log-probability (mnlp) (shen et al.,2017; chen et al., 2015) are generalised for vari-able length sequences.
letting ˆy(n)<i be the historyof predictions prior to the i-th input, the next outputprobability will be p(n)i = c| ˆy(n)<i , x(n)).
then, we deﬁne the token-wise lc score as:.
i,c = p (ˆy(n).
lc(n).
i = − maxc∈c.
log p(n)i,c ..(1).
the lc acquisition function for sequences is thendeﬁned as:.
(cid:16).
lc.
1 , ..., x(n)x(n).
(cid:96).
(cid:17).
=.
lc(n)j.,.
(2).
and, for mnlp as:.
(cid:16).
mnlp.
1 , ..., x(n)x(n).
(cid:96).
(cid:17).
=.
lc(n)j.
..(3).
(cid:96)(cid:88).
j=1.
1(cid:96).
(cid:96)(cid:88).
j=1.
note that this is similar to lc except for the nor-malization factor 1/(cid:96).
the formulation above canbe applied to other types of commonly used acquisi-tion functions such as maximum entropy (maxent)(gal et al., 2017) by simply deﬁning:.
me(n).
i = −.
i,c log p(n)p(n)i,c ,.
(4).
(cid:88).
c∈c.
as the token score.
given the task of quantifyinguncertainty amongst the unlabelled pool of data,both of these metrics - lc and maxent - provideintuitive interpretations.
eq.
(1) scores highly to-kens for which the predicted label has lowest conﬁ-dence, while eq.
(4) scores highly tokens for whichthe whole probability mass function has higher en-tropy.
both of these therefore score more highlyuniform predictive distributions, which indicatesunderlying uncertainty..finally, given the similarity of performance be-tween mnlp and bayesian active learning bydisagreement (bald) (houlsby et al., 2011) inner tasks (shen et al., 2017), and the computa-tional complexity required to calculate bald withrespect to the other activation functions, we willnot compare against bald..4 subsequence acquisition.
in this section we describe how we build on pastworks, and the core contribution of this paper.
ourwork forms a more ﬂexible al algorithm that oper-ates on subsequences, as opposed to full sentences(shen et al., 2017).
this is achieved by generalisingacquisition functions for subsequences (§ 4.1) scor-ing and querying subsequences within sentences(§ 4.2), and performing label propagation on un-seen sentences to avoid the multiple annotations ofrepeated subsequences (§ 4.3)..4.1 subsequence acquisition functions.
since this work focuses on the querying of sub-sequences, from the previously deﬁned lc andmnlp we generalize them to deﬁne a family ofacquisition functions applicable for both full sen-tences and subsequences:.
(cid:16).
lcα.
i+1, ..., x(n)x(n).
i+(cid:96).
(cid:17).
=.
1(cid:96)α.i+(cid:96)(cid:88).
j=i+1.
lc(n)j.
..(5).
special cases are when α = 0 and α = 1 whichreturn the original deﬁnitions of lc in eq.
(2) andmnlp in eq.
(3).
as noted by shen et al.
(2017),.
4312lc for sequences biases acquisition towards longersentences.
the tuneable normalisation factor ineq.
(5) over the sequence of scores mediates the bal-ance of shorter and longer subsequences selected.
this generalisation can be applied to other typesof commonly used acquisition functions such asmaxent and bald by modifying the token-wisescore..4.2 subsequence selection.
i.j., ..., x(n).
each sentence x(n) can be broken into a set of sub-sequences s (n) = {(x(n)) | ∀i < j} whereall elements s ∈ s (n) can be efﬁciently scored byﬁrst computing the token scores, then aggregat-ing as required.
once this has been done for allsentences in du , a query set sq ⊂ ∪ns (n) ofnon-overlapping (mutually disjoint) subsequencesis found.
the requirement of non-overlapping sub-sequences avoids the problem of relabelling tokens,but disallows simply choosing the highest scoringsubsequences (since these can overlap).
insteadat each round of querying, we perform a greedyselection, repeatedly choosing the highest scoringsubsequence that does not overlap with previouslyselected subsequences.
adjustments can be madeto reﬂect practical needs, such as restricting thelength (cid:96) of the viable subsequences to [(cid:96)min, (cid:96)max].
this is because longer subsequences are easier tolabel, while shorter subsequences are more efﬁcientin querying uncertain tokens, and so the selectionis only allowed to operate within these bounds..additionally, it is easy to imagine a scenario inwhich a greedy selection method does not select themaximum total score that can be generated froma sentence.
this scenario is illustrated in table 1where lengths are restricted to (cid:96)min = (cid:96)max = 3for simplicity.
note that tokens can become unse-lectable in future rounds because they are not insidea span of unlabelled tokens of at least size (cid:96)min.
when the algorithm has queried all subsequencesof this size range, it starts to query shorter subse-quences by relaxing the length constraint.
howeverin practise, model performance on the validation setconverges before all subsequences of valid rangehave been exhausted.
nonetheless, when choosingsubsequences of size [(cid:96)min, (cid:96)max] = [4, 7] thesewill be exhausted when roughly 90% and 80% oftokens have been labelled for the ontonotes 5.0and conll 2003 datasets..4.3 subsequence label propagation.
since a subsequence querying algorithm can resultin partially labelled sentences, it raises the questionof how unlabelled tokens should be handled.
in pre-vious work based on the use of crfs (tomanek andhahn, 2009; wanvarie et al., 2011; marcheggianiand arti`eres, 2014) this was solved by using semi-supervision on tokens for which the model showedlow uncertainty.
however, for neural networks, theuse of model generated labels could lead to themodel becoming over-conﬁdent, harming perfor-mance and biasing (arazo et al., 2020) uncertaintyscores.
hence, we ensure that backpropagationonly occurs from labelled tokens..our ﬁnal contribution to the al algorithm is theuse of another semi-supervision strategy where wepropagate uniquely labelled subsequences in or-der to minimise the number of annotations needed.
when queried for a subsequence, the annotator (inthis case an oracle) is not given the contextual to-kens in the remainder of the sentence.
for this rea-son, given an identical subsequence, a consistentannotator will provide the same labels.
therefore,the proposed algorithm maintains a dictionary thatmaps previously queried subsequences to their pro-vided labels.
once a queried subsequence and itslabel are added to the dictionary, all other matchingsubsequences in the unlabelled pool are given thesame, but temporary, labels..the tokens retain these temporary labels untilthey are queried themselves.
after scoring andranking members of s, the algorithm will disre-gard sequences that match exactly members of thisdictionary, which is updated during the queryinground.
however, if tokens belonging to these pre-viously seen subsequences are encountered in adifferent context, meaning as part of a differentsubsequence, they may also be queried.
for ex-ample, in table 1, if the subsequence “shop tobuy” had been previously queried elsewhere in thedataset, the red subsequence will not be consideredfor querying, as it retains its temporary labels.
in-stead, the green subsequence could be queried, inwhich case the temporary labels of tokens 6 and 7will be overwritten by new, permanent labels..therefore, the value of (cid:96)min becomes a trade-offbetween the improved resolution of the acquisitionfunction, and the erroneous propagation of shorter,more frequent label subsequences to identical onesin different contexts..4313yassir is going to the shop.
buy shoes.
jx(n)jy(n)jlc(n)j.
1.x.
3.22.
2.o.
-.
3.o.
-.
4.o.
-.
5.x.
6.x.
7to.
x.
8.x.
9.x.
10..x.
0.41.
0.78lc1 = 0.67.
0.83.
0.60.
0.27lc1 = 0.46.
0.50.lc1 = 0.74.lc1 = 0.57.table 1: this shows the subsequences from a sentence using (cid:96)min = (cid:96)max = 3, α = 1. besides the token index j,the top three rows show the tokens, labels, and the token-wise scores.
if y(n)j = x, then the corresponding token isunlabelled, hence the score is considered when selecting the next query.
after this, the subsequences constitutings (n) are displayed with their lc1 scores.
in this case “shop to buy” will be chosen since it maximises lc1,but ‘traps’ its surrounding tokens until (cid:96)min is lowered to 2 and “shoes .” may be considered..4.4 subsequence active learning algorithm.
finally, we summarise the al algorithm proposed.
given a set of unlabelled data du , we initiallyrandomly select a proportion of sentences fromdu , label them, and add these to dl.
a dictionaryb is also initialised.
using these labelled sentenceswe train a model.
then, the following proposedtraining cycle is repeated until du is empty (or anearly stopping condition is reached):.
1. find all consecutive unlabelled subsequencesin du , and score them using a pre-deﬁnedacquisition function..2. select the top scoring non-overlapping sub-sequences sq that do not appear in b, suchthat the number of tokens in sq is a, andquery them to the annotators.
update dl anddu .
as each sequence is selected, add it to b,mapping it to its true labels..3. provide all occurrences of the keys of b indu with their corresponding temporary labels.
these will not be included in dl as these aretemporary..4. finetune the model on sentences with any la-.
bel, temporary and permanent..repeat this process until convergence..5 experimental setup.
5.1 datasets.
as in previous works (shen et al., 2017), we usethe two following ner datasets:.
ontonotes 5.0. this is a dataset used to compareresults with the full sentence querying baseline(weischedel, ralph et al., 2013), and comprising of.
text coming from: news, conversational telephonespeech, weblogs, usenet newsgroups, broadcast,and talk shows.
this is a bio formatted datasetwith a total of k = 37 classes and 99,333 trainingsentences, with an average sentence length of 17.8tokens in its training set..conll 2003. this is a dataset, also in bio for-mat, with only 4 entity types (loc, misc, per,org) resulting in k = 9 labels (tjong kim sangand de meulder, 2003).
this dataset is made froma collection of news wire articles from the reuterscorpus (lewis et al., 2004).
the average sentencelength is 12.6 tokens in its training set..a full list of class types and entity lengths andfrequencies for both datasets can be found in theappendix..5.2 ner model.
following the work of shen et al.
(2017), a cnn-cnn-lstm model for combined letter- and token-level embeddings was used; see appendix for anoverview of the model and hyperparameters settingand validation.
furthermore, the al algorithmused in (shen et al., 2017) will serve as one ofthe baselines following the same procedure.
thisrepresents an equivalent algorithm to that proposed,but which can only query full sentences, and doesnot use label propagation..5.3 model training and evaluation.
as the evaluation measure we use the f1 score.
af-ter the ﬁrst round of random subsequence selection,the model is trained.
after subsequent selectionsthe model is ﬁnetuned - training is resumed fromthe previous round’s parameters.
in all cases, themodel training was stopped either after 30 epochswere completed, or if the f1 score for the valida-.
4314(a) lcα for ontonotes 5.0 ner dataset.
(b) lcα for conll 2003 ner dataset.
figure 1: f1 score on test set achieved each round using round-optimal model parameters.
all subsequenceexperiments here use (cid:96)min = 4, (cid:96)max = 7. each curve is averaged over 10 runs..tion set had monotonically decreased for 2 epochs.
this validation set is made up of a randomly se-lected 1% of sentences of the original training set.
after ﬁnetuning, the model reloads its parametersfrom the round-optimal epoch, and its performanceis evaluated on the test set.
furthermore, the alalgorithms were also stopped after all hyperparam-eter variations using that dataset and acquisitionfunction family had converged to the same best f1,which we denote with f ∗1 .
for the ontonotes 5.0dataset, f ∗1 value was achieved after 30% of thetraining set was labelled, and for the conll 2003dataset after 40%..5.4 active learning setup & evaluation.
we choose (cid:96)min = 4 to give a realistic contextto the annotator, and to avoid a signiﬁcant prop-agation of common subsequences.
the upperbound of (cid:96)max = 7 was chosen to ensure subse-quences were properly utilised, since the averagesentence length of both datasets is roughly twicethis size.
for the ontonotes 5.0 dataset, everyround a = 10, 000 tokens are queried, whereas forthe conll 2003 dataset a = 2, 000 tokens.
theserepresent roughly 0.5% and 1% of the availabletraining set..we evaluate the efﬁcacy and efﬁciency of thetested al strategies in three ways.
first, modelperformance over the course of the algorithm wasevaluated using end of round f1 score on the testset.
we compare the proportion of the dataset’s to-kens labelled when the model achieves 99% of the1 score ( ˆf1f ∗1 ).
we also quantify therate of improvement of model performance duringtraining using the normalised area under the curve(auc) score of each f1 test curve.
the normalisa-tion ensures that the resulting auc score is in the.
= 0.99 × f ∗.
∗.
range [0, 1], and it is achieved by dividing the aucscore by the size of the dataset.
this implies thatmethods that converge faster to their best perfor-mance will have a higher normalized auc.
second,we consider how quickly the algorithms can locateand query relevant tokens (named entities).
third,we ﬁnally evaluate their ability to extract the mostuncertain tokens from the unlabelled pool..6 results & discussion.
6.1 active learning performance.
figure 1 shows the lcα performance curves forα = 0, α = 1 and the best performing value foreach acquisition class (based on the normalisedauc score, table 3) for full sentence querying(fs), and only the best performing α values for sub-sequence querying (sub).
the ﬁgure also showsthe performance of training on the complete train-ing set (no al), and when the both sentencesand subsequences are random selected by the ac-quisition function.
the equivalent ﬁgures formaxentα are available in appendix, and followsimilar trends.
then, the performance of eachcurve, quantiﬁed in terms of the normalised aucis summarised in table 3..table 2 shows further analysis of the best resultsin figure 1, with best referring to acquisition func-tion and optimal α. these results ﬁrst show thatsubsequence querying methods are more efﬁcientthan querying full sentences, achieving their ﬁnalf1 with substantially less annotated data, and withhigher normalised auc scores.
for ontonotes 5.0,querying subsequences reduces ﬁnal proportion re-quired by 38.8%.
for conll 2003, this reductionis 36.6%.
altogether, subsequence querying holdsimproved efﬁciency over the full sentence queryingbaseline..4315051015202530percentage of tokens manually labelled0.450.500.550.600.650.700.750.800.850.90f1fs, α=1fs, α=0fs, α=0.7sub, α=0.1fs, randomsub, randomno al0510152025303540percentage of tokens manually labelled0.30.40.50.60.70.80.91.0f1fs, α=1fs, α=0fs, α=0.7sub, α=1fs, randomsub, randomno alf1score100% al0.8430.8290.9380.930.final frac.
of the datasetsubfs13%22%27%42%.
on 5.0conll.
table 2: summary of the results of the al strategiesfrom figure 1, when the models are trained using 100%of the training set and active learning (al), with thebest hyperparameter setting of the acquisition functionwith for full sentence and subsequence, based on nor-malised auc score..as a point of interest, full sentence querying canbe easily improved by optimising α alone.
forthe ontonotes 5.0 dataset, using lc1, 24.2% oftokens are required to achieve f ∗1 .
this however,can be improved by 9.33% to only requiring 22.0%by choosing α = 0.7. for conll 2003, usinglc1 for full sentences, 50.0% of the dataset wasrequired, but when using lc0.7, it was 40.7% ofthe tokens..6.2 entity recall.
this section and the next aim to understand someof the underlying mechanisms that allow the sub-sequence querying methods to achieve resultssubstantially better than a full sentence baseline.
namely, the ability of the different methods to ex-tract the tokens for which the model is the mostuncertain about.
given that the majority of tokensin both datasets have the same label - “o”, signi-fying no entity - it is likely that tokens belongingto entities, particularly rarer classes, trigger highermodel uncertainty.
querying full sentences at atime, the al algorithm will spend much of its tokenbudget for that round labelling non-entity tokenswhile attempting to locate the more informativeentities.
subsequence querying methods, not facedwith this wasteful behaviour, allow the al algo-rithm to query entity tokens quicker, locating andlabelling the majority of entity tokens faster overthe course of training..the proportion of tokens belonging to entitiesthat the al algorithm has queried against the roundnumber is plotted in figure 2 for ontonotes 5.0.for both datasets, the random querying methodscontain a distribution of token classes that reﬂectthe dataset at large, producing roughly linear curvesfor this ﬁgure.
curves for all methods that employ.
figure 2: proportion of tokens that belong to entitieslabelled, against the round number..an uncertainty based acquisition function are con-cave, and the auc reﬂects the ranking of modelperformance for each querying method.
this rela-tion suggests that shortly after initialisation, betterperforming algorithm variations query entity to-kens faster.
in later stages of ﬁnetuning this rateis reduced, likely because after labelling a largeproportion of them, the remaining entity tokenscause little uncertainty for the model.
in a practicalsetting where querying may have to be stopped be-fore model performance has converged (i.e.
due toaccumulated cost of annotations), it is greatly ben-eﬁcial to ensure that the model is exposed to a highnumber of relevant tokens, because this increasesthe likelihood of locating entity tokens belongingto underrepresented classes at an early stage..6.3 uncertainty score analysis.
finally, this section compares the scores of tokensin the queried set sq for each querying method.
comparing the distribution and development ofthese scores provides a direct insight to the core as-sumptions of why full sentence querying is outper-formed.
figure 3 shows the difference in score dis-tributions for sentence versus subsequence query-ing, against querying round number, for roundspreceding model performance convergence.
first,it is seen that decreasing the individual query size(full sentence to subsequence) increases the medianuncertainty extracted at the earlier rounds.
second,figure 3 provides evidence for the mechanism sug-gested earlier: aggregating the token scores acrossfull sentences means querying both the highly un-certain tokens, and the tokens that provide little un-certainty.
querying high scoring sentences like thiscan cause a distribution with two peaks as seen in.
4316dataset.
ontonotes5.0.conll2003.acquisitionfunction.
full sentenceα = 0 α = 1 optimal (α).
subsequenceα = 0 α = 1 optimal (α).
lcαmaxentαrandomlcαmaxentαrandom.
0.7940.791.
0.8020.803.
0.804 (0.7)0.803 (1.0).
0.8170.815.
0.8120.813.
0.818 † (0.1)0.816 † (0.5).
0.8570.841.
0.8750.882.
0.879 (0.7)0.882 (1.0).
0.8850.881.
0.8830.883.
0.892 † (1.0)0.891 † (0.9).
0.734.
0.824.
0.769.
0.859.table 3: normalised auc scores for model performance (f1 score on test set) for α = 0, 1, and its optimal value ineach case.
each pair of differences between the optimized acquisition function for full sentences and subsequences(indicated by a †) are signiﬁcantly different (two-sided unpaired t-test, with p-value < 0.05)..figure 3: distributions of the queried lc scores for theontonotes 5.0 dataset, made on the 1st, 5th, 10th, and15th scoring rounds.
this corresponds to scores aftertraining on 1%, 3.2%, 6.1%, and 9.0% of the utilisedtraining set..the ﬁgure.
as the model becomes increasingly cer-tain about its predictions, high scores are localisedwithin smaller subsequences, and the coarse sen-sitivity of full sentence querying means it forfeitsall the higher scoring tokens.
these differenceswere also observed when comparing subsequencequerying methods with sub-optimal α..this ﬁgure only analyses behaviour of up to 9%of the training set’s tokens have been queried.
in-stead, figure 4 show how the mean of token-wisescores evolve for different querying methods forthe ontonotes 5.0 dataset until convergence.
thisclearly shows that subsequence querying methodsconverge faster over the full course of the algo-rithm compared to full sentence querying.
thisis consistent with figure 1 in terms of initial rateand ﬁnal time of model performance convergence,namely that model performance plateaus alongsidethe uncertainty score..keeping track of query scores like this is also areasonable idea in industrial applications.
when.
figure 4: average value of lc for all tokens in sq withconﬁdence intervals, against round number.
score val-ues are averaged over all tested values of α.training on a very semantically speciﬁc corpus,there may not be enough fully labelled sentencesto build a test set.
in that case, observing the rateprogress of score convergence can be used as anearly stopping method for the al algorithm (zhuet al., 2010)..7 conclusion & future work.
in this study we have employed subsequencequerying methods for improving the efﬁciency ofal for ner tasks.
we have seen that these meth-ods outperform full sentence querying in termsof annotations required for optimal model perfor-mance, requiring 38.8% and 36.6% fewer tokensfor the ontonotes 5.0 and conll 2003 datasets.
optimal results for subsequence querying (and fullsentence querying) were achieved by generalisingpreviously used al acquisition functions, deﬁninga larger family of acquisition functions for sequen-tial data..the analysis of § 6.3 suggests that a full sentencequerying causes noisy acquisition functions due tothe tokens in the queried sentences that were not.
4317151015round number0.00.51.01.52.02.53.0token-wise scorefssubhighly scored.
this added noise reduces the bud-get efﬁciency, and a subsequence querying methodeliminates a large part of this effect.
this efﬁciencyalso translated into a faster recall of named entitiesin the dataset to be queried (§ 6.2)..limitations and future work: limitations of thisstudy are largely centred on the use of an oracle toprovide tokens with their labels.
with human anno-tators, the cropped context of subsequence queriesmay make them produce more inaccuracies thanwhen annotating full sentences.
such studies willhelp reveal how context affects label accuracy, howthis, in turn, affects optimal hyperparameters inthe subsequence selection process (such as optimalquery length), further accommodations that mustbe made to effectively optimise worker efﬁciency,and how to deal with unreliable labels.
we leave tofuture work the evaluation of these querying meth-ods with human annotators.
there are also ways to incorporate model generatedlabelling methods for more robust semi-supervisioninto our framework that we leave to future work.
finally, there are examples of other tasks for struc-tured data, such as audio, video, and image segmen-tation, where the part of an instance may be queried.
a generalisation of the strategy demonstrated forthe ner case may allow for more efﬁcient activelearning querying methods for these other types ofdata..references.
eric arazo, diego ortego, paul albert, noel e.o’connor, and kevin mcguinness.
2020. pseudo-in deep semi-labeling and conﬁrmation biasin 2020 international jointsupervised learning.
conference on neural networks (ijcnn), pages 1–8..yukun chen, thomas a. lasko, qiaozhu mei,joshua c. denny, and hua xu.
2015. a study of ac-tive learning methods for named entity recognitionin clinical text.
journal of biomedical informatics,58:11 – 18..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..approach.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 595–605, copenhagen, denmark.
associationfor computational linguistics..yarin gal, riashat islam, and zoubin ghahramani.
2017. deep bayesian active learning with imagein proceedings of the 34th internationaldata.
conference on machine learning, volume 70 ofproceedings of machine learning research, pages1183–1192, international convention centre, syd-ney, australia.
pmlr..neil houlsby, ferenc husz´ar, zoubin ghahramani, andm´at´e lengyel.
2011. bayesian active learning forclassiﬁcation and preference learning..s. huang, r. jin, and z. zhou.
2014. active learn-ing by querying informative and representative ex-amples.
ieee transactions on pattern analysis andmachine intelligence, 36(10):1936–1949..ahmet iscen, giorgos tolias, yannis avrithis, andondrej chum.
2019. label propagation for deepin proceedings of thesemi-supervised learning.
ieee/cvf conference on computer vision and pat-tern recognition (cvpr)..john lafferty, andrew mccallum, and fernando cnpereira.
2001. conditional random ﬁelds: prob-abilistic models for segmenting and labeling se-quence data..yann lecun and yoshua bengio.
1998. convolutionalnetworks for images, speech, and time series, page255–258.
mit press, cambridge, ma, usa..david d lewis, yiming yang, tony g rose, and fanli.
2004. rcv1: a new benchmark collection forjournal of machinetext categorization research.
learning research, 5(apr):361–397..wang ling, chris dyer, alan w. black, and isabeltwo/too simple adaptations oftrancoso.
2015.word2vec for syntax problems.
in proceedings ofthe 2015 conference of the north american chap-ter of the association for computational linguis-tics: human language technologies, pages 1299–1304, denver, colorado.
association for computa-tional linguistics..diego marcheggiani and thierry arti`eres.
2014. anexperimental comparison of active learning strate-gies for partially labeled sequences.
in proceedingsof the 2014 conference on empirical methods innatural language processing (emnlp), pages 898–906, doha, qatar.
association for computationallinguistics..mingkun li and i. k. sethi.
2006. conﬁdence-basedactive learning.
ieee transactions on pattern anal-ysis and machine intelligence, 28(8):1251–1261..meng fang, yuan li, and trevor cohn.
2017. learninghow to active learn: a deep reinforcement learning.
ion muslea, steven minton, and craig a. knoblock.
2002. active + semi-supervised learning = robust.
4318in proceedings of the nine-multi-view learning.
teenth international conference on machine learn-ing, icml ’02, page 435–442, san francisco, ca,usa.
morgan kaufmann publishers inc..dittaya wanvarie, hiroya takamura, and manabu oku-mura.
2011. active learning with subsequence sam-pling strategy for sequence labeling tasks.
journalof natural language processing, 18(2):153–173..robert pinsler, jonathan gordon, eric nalisnick, andjos´e miguel hern´andez-lobato.
2019. bayesianbatch active learning as sparse subset approximation.
in advances in neural information processing sys-tems, volume 32, pages 6359–6370.
curran asso-ciates, inc..kai wei, rishabh iyer, and jeff bilmes.
2015. submod-ularity in data subset selection and active learning.
in proceedings of the 32nd international conferenceon machine learning, volume 37 of proceedingsof machine learning research, pages 1954–1963,lille, france.
pmlr..weischedel, ralph, palmer, martha, marcus, mitchell,hovy, eduard, pradhan, sameer, ramshaw, lance,xue, nianwen, taylor, ann, kaufman, jeff, fran-chini, michelle, el-bachouti, mohammed, belvin,robert, and houston, ann.
2013. ontonotes release5.0..jingbo zhu, huizhen wang, eduard hovy, andmatthew ma.
2010. conﬁdence-based stopping cri-teria for active learning for data annotation.
acmtrans.
speech lang.
process., 6(3)..y reddy, viswanath pulabaigari, and eswara b.
2018.interna-.
semi-supervised learning: a brief review.
tional journal of engineering technology, 7:81..jeffrey rzeszotarski, ed chi, praveen paritosh, andpeng dai.
2013. inserting micro-breaks into crowd-sourcing workﬂows.
proceedings of the aaai con-ference on human computation and crowdsourcing,1(1)..burr settles.
2011. from theories to queries: ac-in active learning andtive learning in practice.
experimental design workshop in conjunction withaistats 2010, volume 16 of proceedings of ma-chine learning research, pages 1–18, sardinia,italy.
jmlr workshop and conference proceedings..burr settles and mark craven.
2008. an analysisof active learning strategies for sequence labelingin proceedings of the conference on em-tasks.
pirical methods in natural language processing,emnlp ’08, page 1070–1079, usa.
associationfor computational linguistics..yanyao shen, hyokun yun, zachary lipton, yakovkronrod,and animashree anandkumar.
2017.deep active learning for named entity recognition.
in proceedings of the 2nd workshop on representa-tion learning for nlp, pages 252–256, vancouver,canada.
association for computational linguistics..erik f. tjong kim sang and fien de meulder.
2003. introduction to the conll-2003 shared task:language-independent named entity recognition.
inproceedings of the seventh conference on natu-ral language learning at hlt-naacl 2003, pages142–147..katrin tomanek and udo hahn.
2009..semi-supervised active learning for sequence labeling.
inproceedings of the joint conference of the 47th an-nual meeting of the acl and the 4th internationaljoint conference on natural language processingof the afnlp, pages 1039–1047, suntec, singapore.
association for computational linguistics..simon tong and daphne koller.
2002. support vec-tor machine active learning with applications to textclassiﬁcation.
j. mach.
learn.
res., 2:45–66..k. wang, d. zhang, y. li, r. zhang, and l. lin.
2017.cost-effective active learning for deep image classi-ﬁcation.
ieee transactions on circuits and systemsfor video technology, 27(12):2591–2600..4319a model architecture.
j.j.the model architecture is built of three sections.
the character-level convolutional neural network(cnn) (lecun and bengio, 1998) character-levelencoder extracts character level features, wwordforeach token x(i)j.in a sentence.
then, a latent token embedding wemb.
corre-sponding to that token is generated.
the fullrepresentation of the token is the concatentationof the two vectors: wfull).
thetoken-label embeddings, wemb, are initialised us-ing word2vec (ling et al., 2015), and updatedduring training and ﬁnetuning, as per the base-line paper.
a second, token-level cnn encoder}(cid:96)iis used to generate {htokenj=1, given the token-j }(cid:96)ilevel representations {wfullj=1.
the ﬁnal token-level encoding is deﬁned by another concatentation:hencj.:= (htokenj.:= (wchar.
, wembj., wfull.
j )..j.j.j.finally, a tag decoder is used to generate thetoken-level pmfs over the c possible token classes:j }(cid:96)i{hencj=1..lstm−−−→ {ˆy(i).
j }(cid:96)i.j=1.
b model & training parameters.
table 4 lists the hyperparameter values used to trainthe ner model.
note that while dropout is usedduring training, it is turned off when generating theprobabilities that contribute to the scoring of theacquisition function.
model was developed usingpytorch, and trained on a titan rtx..hyperparameter value.
batch sizedropout rate for convolutional layersdropout rate for embedding layersgradient clipping magnitudecharacter- and token-level cnn kernel sizelayers in character- and token-level cnnscharacter embedding vector sizenumber of ﬁlters per character-level cnn layernumber of ﬁlters per token-level cnn layeroptimiser typeoptimiser learning rate.
320.50.250.35335050300sgd1.0.table 4: values of model and training hyperparametersused throughout the investigation..c dataset analysis.
here, we cluster similar labels in the bio format,reducing the total k classes to the k(r) = (k +k(r).
therefore, c(r)1 , ..., c(r)1)/2 class groups c(r)corresponds exactly to c1, the empty label, whilec(r)k , k > 1 groups the raw labels c2k−2 and c2k−1..1.figures 5 and 7 show the distribution of theseclass groups for the ontonotes 5.0 and conll2003 datasets respectively.
for the former, countsrange from 199 tokens for the ’language’ to46698 tokens for the ’org’ class.
the full avail-able training set totals 1766955 tokens in 99333sentences; this is partitioned into a train and vali-dation set during experimentation.
a further testset comprises of 146253 tokens in 8057 sentences.
the latter’s training set contains 172210 tokens in13689 sentences, and its test set has 42141 tokensin 3091 sentences sentences..figure 5: composition of token classes in the onto-notes 5.0 english ner training set..figure 6: lengths of entities in the onto-notes 5.0training set in number of tokens, again omitting theempty class ’o’.
4320figure 7: composition of token classes in the conll2003 ner training set..(a) lcα for ontonotes5.0 ner dataset.
(b) maxentα for ontonotes5.0 ner dataset.
(c) lcα for conll 2003 ner dataset.
figure 8: lengths of entities in the conll 2003 train-ing set in number of tokens, again omitting the emptyclass ’o’.
d active learning results for both.
datasets.
in figure 9 we show the model performance plot-ted against the percentage of the tokens used as atraining set for all the combinations of acquisitionfunctions..(d) maxentα for conll 2003 ner datasetfigure 9: f1 score on test set achieved each round (top)and against time (bottom in each case) using round-optimal model parameters.
all subsequence experi-ments here use (cid:96)min = 3, (cid:96)max = 6..4321051015202530percentage of tokens manually labelled0.450.500.550.600.650.700.750.800.850.90f1fs, α=1fs, α=0fs, α=0.7sub, α=0.1fs, randomsub, randomno al0510152025303540percentage of tokens manually labelled0.30.40.50.60.70.80.91.0f1fs, α=1fs, α=0sub, α=0.9fs, randomsub, randomno al0510152025303540percentage of tokens manually labelled0.30.40.50.60.70.80.91.0f1fs, α=1fs, α=0fs, α=0.7sub, α=1fs, randomsub, randomno al0510152025303540percentage of tokens manually labelled0.30.40.50.60.70.80.91.0f1fs, α=1fs, α=0sub, α=0.9fs, randomsub, randomno al