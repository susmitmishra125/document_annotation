multi-label few-shot learning for aspect category detection.
shiwan zhao2∗ honglei guo2 chao xue3†mengting hu1hang gao1 tiegang gao1 renhong cheng1 zhong su2.
1 nankai university.
2 ibm research - china.
3 jd explore academy.
{mthu, knimet}@mail.nankai.edu.cn, {zhaosw, guohl, suzhong}@cn.ibm.comxuechao19@jd.com, {gaotiegang, chengrh}@nankai.edu.cn.
abstract.
aspect category detection (acd) in sentimentanalysis aims to identify the aspect categoriesin this paper, wementioned in a sentence.
formulate acd in the few-shot learning sce-nario.
however, existing few-shot learning ap-proaches mainly focus on single-label predic-tions.
these methods can not work well forthe acd task since a sentence may containmultiple aspect categories.
therefore, we pro-pose a multi-label few-shot learning methodbased on the prototypical network.
to allevi-ate the noise, we design two effective attentionmechanisms.
the support-set attention aimsto extract better prototypes by removing irrel-evant aspects.
the query-set attention com-putes multiple prototype-speciﬁc representa-tions for each query instance, which are thenused to compute accurate distances with thecorresponding prototypes.
to achieve multi-label inference, we further learn a dynamicthreshold per instance by a policy network.
ex-tensive experimental results on three datasetsdemonstrate that the proposed method signiﬁ-cantly outperforms strong baselines..1.introduction.
aspect category detection (acd) (pontiki et al.,2014, 2015) is an important task in sentiment anal-ysis.
it aims to identify the aspect categories men-tioned in a given sentence from a predeﬁned setof aspect categories.
for example, in the sen-tence “the cheesecake is tasty and the staffs arefriendly”, two aspect categories, i.e.
food and ser-vice, are mentioned.
the performance of existingapproaches for the acd task (zhou et al., 2015;schouten et al., 2018; hu et al., 2019) relies heavilyon the scale of the labeled dataset.
they usuallysuffer from limited data and fail to generalize wellto novel aspect categories with only a few labeled.
∗ shiwan zhao is the corresponding author.
† the work was (partially) done in ibm..figure 1: example meta-task in a 3-way 2-shot sce-nario.
the words in gray background describe the tar-get aspects of interest, while the words marked by therectangle are irrelevant aspects, which tend to be noisefor this meta-task..instances.
on the one hand, it is time-consumingand labor-intensive to annotate large-scale datasets.
on the other hand, given a large dataset, manylong-tail aspects still suffer from data sparsity..few-shot learning (fsl) provides a solution toaddress the above challenges.
fsl learns like ahuman, identifying novel classes with limited su-pervised information by exploiting prior knowl-edge.
many efforts have been devoted to fsl(ravi and larochelle, 2017; finn et al., 2017; snellet al., 2017; wang et al., 2018; gao et al., 2019).
among these methods, the prototypical network(snell et al., 2017) is a promising approach, whichis simple but effective.
it follows the meta-learningparadigm by building a collection of n -way k-shot meta-tasks.
a meta-task aims to infer a queryset with the help of a small labeled support set.
itﬁrst learns a prototype for each class in the sup-port set.
then the query instance is predicted bymeasuring the distance with n prototypes in theembedding space..in this paper, we formulate acd in the fsl.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6330–6340august1–6,2021.©2021associationforcomputationallinguistics6330(a)  room_cleanlinesssupport set(b) staff_ownerquery set(a) and (c)(b)(b)(c) hotel1)okay, so it is a cute chain hotel.
2)i really don’t see how people are giving this hotel such high ratings.1)hotel is just plain dirty.
2)the owners are extremely smart and worldly 3)not a typical customer service response, especially from the owner !1)i think the salon has problems starting with the owner.
2)the owner is very nice.1)cleanliness was great, and the food was really good.
2)people have mentioned, bed bugs on yelp !!
scenario, which aims to detect aspect categories ac-curately with limited training instances.
however,acd is a multi-label classiﬁcation problem sincea sentence may contain multiple aspect categories.
most fsl works learn a single-label classiﬁer andcan not work well to address the acd task.
thereasons are two-fold.
firstly, the sentences of eachclass (i.e., aspect category) in the support set are di-verse and contain noise from irrelevant aspects.
asdisplayed in figure 1, there are three classes in thesupport set, and each class has two instances.
theaspect categories food and salon tend to be noise forthis meta-task, making it hard to learn a good pro-totype for each class in the support set.
secondly,the query set is also noisy.
figure 1 demonstratesthree different cases.
the ﬁrst sentence mentionstwo aspects hotel and room cleanliness out of thesupport set.
we need to detect both aspects accu-rately as multi-label classiﬁcation.
when detectingeach of them, the other aspect acts as noise andmakes the task hard.
the second sentence is aneasy case with a single aspect staff owner.
thethird sentence mentions the aspect staff owner outof the support set, while the aspect service is noisefor this meta-task.
in summary, the noise from boththe support set and query set makes the few-shotacd a challenging task..to this end, we propose a multi-label fslmethod based on the prototypical network (snellet al., 2017).
we alleviate the noise in the supportset and query set by two effective attention mecha-nisms.
concretely, the support-set attention tries toextract the common aspect of each class.
by remov-ing the noise (i.e., irrelevant aspects), the support-set attention can yield better prototypes.
then for aquery instance, the query-set attention utilizes theprototypes to compute multiple prototype-speciﬁcquery representations, in which the irrelevant as-pects are removed.
given the better prototypes andthe corresponding prototype-speciﬁc query repre-sentations, we can compute accurate distances be-tween the query instance and the prototypes in theembedding space.
we detect the aspect categoriesin the query instance by ranking the distances.
toselect the positive aspects from the ranking, wedesign a policy network (williams, 1992) to learn adynamic threshold for each instance.
the thresholdis modeled as the action of the policy network withcontinuous action space..the main contributions of our work are as fol-.
lows:.
• we formulate acd as a multi-label fsl prob-lem and design a multi-label fsl methodbased on the prototypical network to solvethe problem.
to the best of our knowledge,we are the ﬁrst to address acd in the few-shotscenario..• to alleviate the noise from the support set andquery set, we design two effective attentionmechanisms, i.e., support-set attention andquery-set attention..• experimental results on the three datasetsdemonstrate that our method outperformsstrong baselines signiﬁcantly..2 related work.
aspect category detection previous works foracd can mainly be divided into two types: unsu-pervised and supervised methods.
unsupervisedapproaches extract aspects by mining semantic as-sociation (su et al., 2006) or co-occurrence fre-quency (hai et al., 2011; schouten et al., 2018).
these methods require a large corpus to mine as-pect knowledge and have limited performance.
su-pervised methods address this task via hand-craftedfeatures (kiritchenko et al., 2014), automaticallylearning useful representations (zhou et al., 2015),multi-task learning (xue et al., 2017; hu et al.,2019), or topic-attention model (movahedi et al.,2019).
the above methods detect aspect categoriesout of a pre-deﬁned set, which cannot handle theunseen classes.
these challenges motivate us toinvestigate this task in the few-shot scenario.
few-shot learningfew-shot learning (fsl)(fe-fei et al., 2003; fei-fei et al., 2006) is closeto real artiﬁcial intelligence, which borrows thelearning process from the human.
by incorporatingthe prior knowledge, it obtains new knowledge fastwith limited supervised information.
many workshave been proposed for fsl, which can be mainlydivided into four research directions..one promising direction is distance-based meth-ods.
these methods measure the distance betweeninstances in the feature embedding space.
thesiamese network (koch et al., 2015) infers the sim-ilarity score between an instance pair.
others com-pare the cosine similarity (vinyals et al., 2016) oreuclidean distance (snell et al., 2017).
the rela-tion network (sung et al., 2018) exploits a neuralnetwork to learn the distance metric.
afterward,.
6331figure 2: the left part depicts the main network for an example n -way k-shot meta-task with a query instance(n = 3, k = 2).
each small cube of the instance symbolizes an aspect category.
the colored cubes indicate thetarget aspects of interest while the white cubes indicate the noisy aspects.
the right part shows the details of thesupport-set attention..garcia and bruna (2018) utilize graph convolu-tion network to extract the structural informationof classes.
the second direction focuses on theoptimization of networks.
model-agnostic meta-learning (maml) algorithm (finn et al., 2017)learns a good initialization of the model and up-dates the model by a few labeled examples.
metanetworks (munkhdalai and yu, 2017) achieve rapidgeneralization via fast parameterization.
the thirdtype is based on hallucination (wang et al., 2018; liet al., 2020).
this research line directly deals withdata deﬁciency by “learning to augment”, whichdesigns a generator on the base classes and thenhallucinates novel class data to augment few-shotsamples.
the last direction introduces a weight gen-erator to predict classiﬁcation weight given a fewnovel class samples, either based on attention mech-anism (gidaris and komodakis, 2018) or gaussiandistribution (guo and cheung, 2020)..a recent work proto-hatt (gao et al., 2019) issimilar to ours.
proto-hatt is based on the pro-totypical network (snell et al., 2017), which dealswith the text noise in the relation classiﬁcation taskby employing hybrid attention at both the instance-level and the feature-level.
this method is designedfor single-label fsl.
compared with it, our methoddesigns two attention mechanisms to alleviate thenoise on the support set and query set, respectively.
the collaboration of two attentions helps computeaccurate distances between the query instance andprototypes, and then improves multi-label fsl..multi-label few-shot learning.
compared.
with single-label fsl, the multi-label fsl has beenunderexplored.
previous works focus on image syn-thesis (alfassy et al., 2019) and signal processing(cheng et al., 2019).
rios and kavuluru (2018)develop few-shot and zero-shot methods for multi-label text classiﬁcation when there is a known struc-ture over the label space.
their approach relies onlabel descriptors and the hierarchical structure ofthe label spaces, which limits its application inpractice.
hou et al.
(2020) propose to address themulti-label intent detection task in the fsl sce-nario.
it calibrates the threshold by kernel regres-sion.
different from this work, we learn a dynamicthreshold per instance in a reinforced manner..3 methodology.
in the few-shot acd scenario, each meta-task con-tains a support set s and a query set q. the meta-task is to assign the query instance to the class(es)of the support set.
an instance may be a multi-aspect sentence.
thus a query sentence may de-scribe more than one class out of the support set1.
therefore, we deﬁne the few-shot acd as a multi-label few-shot classiﬁcation problem..3.1 overview.
suppose in an n -way k-shot meta-task, the sup-i=1, where each xiport set is s = {(xi.
k), yi}n.1, ...xi.
1we found that the probability of a query instance belong-ing to more than one class is around 4.5% in the acd dataset,i.e.
fewasp, by randomly sampling 10,000 5-way 5-shotmeta-tasks with 5 query sentences for each class..6332saequerysoftmaxlabel of querymse loss1100.50.50normalizesupport-set attentionsaesaeeqaqaqahi1support setaspect category 1aspect category 2aspect category 3hi2hi1hi2hi1hi2i=1i=2i=3hqhi1hi2viattention matrixwiaspect-wise attentionri1ri2esaqaencodersupport-set attentionquery-set attentioneuclidean  distanceprototyperik-shot instances of aspect icommon aspect vector1, ..., xi.
is a sentence and (xik) all contain the aspectcategory yi.
a query instance is (xq, yq), whereyq is a binary label vector indicating the aspects inxq out of n classes..figure 2 presents the main network by an exam-ple 3-way 2-shot meta-task.
it is composed of threemodules, i.e., encoder, support-set attention (sa)and query-set attention (qa).
each class in the sup-port set contains k instances, which are fed intothe encoder to obtain k encoded sequences.
next,sa module extracts a prototype for this class fromthe encoded sequences.
after obtaining n proto-types, we feed a query instance into the qa moduleto compute multiple prototype-speciﬁc query rep-resentations, which are then used to compute theeuclidean distances with the corresponding proto-types.
finally, we normalize the negative distancesto obtain the ranking of prototypes and then selectthe positive predictions (i.e., aspect categories) bya dynamic threshold.
next, we will introduce themodules of our method in detail..3.2 encoder.
given an input sentence x = {w1, w2, ..., wn},we ﬁrst map itinto an embedding sequence{e1, e2, ..., en} by looking up the pre-trainedglove embeddings (pennington et al., 2014).
thenwe encode the embedding sequence by a convolu-tional neural network (cnn) (zeng et al., 2014;gao et al., 2019).
the convolution kernel slideswith the window size m over the embedding se-quence.
we gain the contextual sequence h ={h1, h2, ..., hn}, h ∈ rn×d:.
hi = cnn(ei− m−1.
, ..., ei+ m−1.
).
2.
2.
(1).
where cnn(·) is a convolution operation.
the ad-vantages of cnn are two-fold: ﬁrst, the convo-lution kernel can extract n-gram features on thereceptive ﬁeld.
for example, the bi-gram feature ofhot dog could help detect the aspect category food;second, cnn enables parallel computing over in-puts, which is more efﬁcient (xue and li, 2018)..3.3 support-set attention (sa).
in each class of the support set, the k-shot in-stances describe a common aspect, i.e., the targetaspect of interest2.
as shown in figure 1, two.
2in almost all cases, there is only one common aspect inthe k instances.
we randomly sample 10,000 5-way 5-shotmeta-tasks, and found that the probability of containing morethan one common aspect in each class is less than 0.086%.
the probability will be much lower in the 10-way scenario..sentences, “cleanliness was great, and the foodwas really good” and “people have mentioned,bed bugs on yelp!
!”, share the common aspectroom cleanliness.
the former contains two as-pect categories room cleanliness and food.
in thisexample meta-task, it is an instance of the classroom cleanliness.
however, when sampling othermeta-tasks, the instance may be used to representthe class food.
this leads to confusion and makeslearning a good prototype difﬁcult.
to deal with theissue brought by multi-aspect sentences, we ﬁrstneed to identify the common aspect.
as depicted inthe right part of figure 2, we compute the commonaspect vector by the combination of the k-shotinstances.
we then regard the vector as a conditionand inject it into the attention mechanism to makeour attention mechanism aspect-wise.
common aspect vector the encoded k-shot in-stances of a class contain one common aspect andsome irrelevant aspects.
among these aspects, thecommon aspect is the majority.
thus, we simplyconduct a word-level average to extract the com-mon aspect vector vi ∈ rd..vi = avg(h i.
1, h i.
2, ..., h i.k).
(2).
the average operation highlights the common as-pect, but cannot completely eliminate noisy aspects.
to further reduce the noise of irrelevant aspects ineach instance, we use the common aspect as thecondition in the attention mechanism.
aspect-wise attention to make the attentionmechanism adapt to the condition, we have twodesigns.
first, we directly use the common aspectvector to compute the attention with each instance(see eq.
4), which ﬁlters out the irrelevant aspectsof each instance to some extent.
second, we exploitthe idea of dynamic conditional network, which hasbeen demonstrated effective in fsl (zhao et al.,2018).
by predicting a dynamic attention matrixwith the common aspect vector, our attention mech-anism can further adapt to the condition, i.e., thecommon aspect vector of the class.
speciﬁcally, welearn different perspectives of the condition by sim-ply repeating the common aspect vector (vaswaniet al., 2017).
then it is fed into a linear layer toobtain the attention matrix w i for class i..w i = w (vi ⊗ em ) + b.
(3).
where (vi ⊗ em ) ∈ rem ×d is the operation repeat-edly concatenating vi for em times.
the linearlayer has parameter matrix w ∈ rd×em and bias.
6333b ∈ rd.
this layer is shared in the classes of allmeta-tasks, which is learned to be class-agnostic.
thus in the testing phase, it can generate aspect-wise attention for a novel class..then in class i of the support set, we exploitthe common aspect vector and attention matrix tocalculate a denoised representation for every in-stance.
the denoised representation rij for the j-thinstance is computed as below..datasetfewasp(single)fewasp(multi)fewasp.
#cls.
100100100.
#inst./cls.
200400630.
#inst.
200004000063000.table 1: statistics of three datasets.
#cls.
denotes thenumber of classes.
#inst./cls.
denotes the number ofinstances per class.
#inst.
denotes the total number ofinstances..β = softmax(vitanh(h ij = βh irij.jw i)).
(4).
(mse) loss:.
the training objective is the mean square error.
in this way, the support-set attention is adaptedto the condition and is also class-speciﬁc.
thus ittends to focus on the correct aspect even for a multi-aspect sentence representing different classes..finally, the average of denoised representationsfor k-shot instances is the prototype of this class..ri = avg(ri.
1, ri.
2, ..., ri.
k).
(5).
after processing all classes in the support set,.
we obtain n prototypes {r1, r2, ..., rn }..3.4 query-set attention (qa).
a query instance may also contain multiple aspects,making the sentence noisy.
to deal with the noisein a query instance, we select the relevant aspectsfrom the query instance by the qa module.
specif-ically, we ﬁrst process the query instance by theencoder and obtain the encoded instance hq.
thenwe feed hq into the qa module to obtain multipleprototype-speciﬁc query representations riq by then prototypes..ρi = softmax(ritanh(hq))q = ρihqri.
the qa module tries to focus on the aspect cat-egory which is similar to the prototype.
in eq.
6,the attention is non-parametric.
it can reduce thedependence on parameters and can accelerate theadaptation to unseen classes..3.5 training objective.
for a query instance, we compute the euclideandistance (ed) between each prototype and itsprototype-speciﬁc query representation, and we ob-tain n distances.
next, we normalize the negativedistances as the ﬁnal prediction, which is a rankingof the prototypes..ˆy = softmax(−ed(ri, ri.
q)),.
i ∈ [1, n ].
(7).
(cid:88).
l =.
(ˆy − yq)2.
(8).
where yq is the ground-truth.
we also normalizeyq to ensure the consistency between the predictionand the ground-truth.
learning dynamic threshold (dt) to selectthe positive aspects from the ranking (see eq.
7)for a query instance, we further learn a dynamicthreshold.
the threshold is modeled by a policynetwork (williams, 1992), which has a continu-ous action space following beta distribution (chouet al., 2017).
given a query instance, we deﬁnethe state as [(r1 − r1q )2; ˆy].
wefeed the state into the policy network and obtainthe parameters a and b of a beta distribution.
thenwe sample a threshold τ from beta(τ |a, b).
thereward score is the f1 score for this instance basedon τ .
we also introduce a reference score∗, whichis the f1 score based on a baseline action, i.e., thea−1mode of beta(τ |a, b):a+b−2 .
the training objec-tive is deﬁned as below to minimize the negativeexpected reward..q )2; ...; (rn − rn.
(6).
lt = −(score − score∗)logp (τ ).
(9).
where p (τ ) is the probability of τ in the beta dis-tribution.
during inference, we select the positiveaspects in ˆy with the baseline action..4 experiments.
4.1 datasets.
we construct three few-shot acd datasets fromyelp aspect (bauman et al., 2017), which is a large-scale multi-domain dataset for aspect recommenda-tion.
we group all instances by aspects and choose100 aspect categories.
following han et al.
(2018),we split the 100 aspects without intersection into64 aspects for training, 16 aspects for validation,and 20 aspects for testing..6334models.
relation networkmatching networkgraph networkprototypical networkimpproto-hattproto-awatt (ours).
5-way 5-shotf175.7981.8981.4583.3083.6983.3386.71†‡.
auc0.93310.97050.96540.96490.96650.96450.9756†‡.
5-way 10-shotf1auc72.020.908684.620.974985.040.974686.290.975386.140.974786.710.976288.54†‡0.9796.
10-way 5-shotf1auc63.780.918170.950.963070.750.954574.230.959773.800.960073.420.957180.28†‡0.9701†‡.
10-way 10-shotf1auc61.150.905473.280.967277.840.969776.830.967177.090.969177.650.970082.97†‡0.9755†‡.
table 2: evaluation results in terms of auc and macro-f1 (%) on fewasp(single).
all results are the average of5 runs.
the marker † refers to p-value<0.05 of the t-test when comparing with prototypical network.
the marker‡ refers to p-value<0.05 when comparing with proto-hatt..models.
relation networkmatching networkgraph networkprototypical networkimpproto-hattproto-awatt (ours).
5-way 5-shotf1auc58.380.849165.700.895459.250.879767.880.896768.860.901269.150.911071.72†0.9145†.
5-way 10-shotf1auc61.370.862169.020.913864.630.904572.320.916073.510.922973.910.930377.19†‡0.9389†.
10-way 5-shotf1auc43.710.842250.860.882845.420.860552.720.880153.960.88710.904455.3458.89†‡0.8980†.
10-way 10-shotf1auc44.850.847254.420.899448.490.884458.920.906859.860.91100.923860.2166.76†‡0.9234†.
table 3: evaluation results in terms of auc and macro-f1 (%) on fewasp(multi)..according to the sentence type, i.e., single-aspect or multi-aspect3, we sample different typesof sentences from each group and construct threedatasets: fewasp(single), fewasp(multi), and fe-wasp, which are composed of single-aspect, multi-aspect, and both types of sentences, respectively.
note that fewasp is randomly sampled from theoriginal set of each class, which can better reﬂectthe data distribution in real applications.
the statis-tics of the three datasets are shown in table 1..4.2 experimental settings.
evaluation metrics previous single-label fsl(snell et al., 2017) usually evaluates performanceby accuracy.
in the multi-label setting, we chooseauc (area under curve) and macro-f1 as the eval-uation metrics.
auc is utilized for model selectionand macro-f1 is computed with a threshold.
in ourexperiments, we found that for all methods in threedatasets, the overall best thresholds are 0.3 in the5-way setting and 0.2 in the 10-way setting.
thuswe choose them for evaluating the baselines.
training details we ﬁrst train the main networkwith mse loss l (eq.
8).
then we initializethe main network with the learned parameters andjointly train the policy network with lt (eq.
9).
the implementation details are described in theappendix..3a sentence contains a single aspect or multiple aspects..4.3 compared methods.
our approach is named as proto-awatt (aspect-wise attention).
we validate the effectiveness of theproposed method by comparing with the followingpopular approaches..• matching network (vinyals et al., 2016): itis a metric-based attention method, where dis-tance is measured by cosine similarity..• prototypical network (snell et al., 2017): itcomputes the average of embedded supportexamples for each class as the prototype, andthen measures the distance between the em-bedded query instance and each prototype..• relation network (sung et al., 2018): it uti-lizes a neural network to learn the relationmetric..• graph network (garcia and bruna, 2018):it casts fsl as a supervised message passingtask by graph neural network..• imp (allen et al., 2019): it proposes inﬁnitemixture prototypes to represent each class bya set of clusters, with the number of clustersdetermined directly from the data..• proto-hatt (gao et al., 2019): it is based onthe prototypical network, which deals with the.
6335models.
relation networkmatching networkgraph networkprototypical networkimpproto-hattproto-awatt (ours).
5-way 5-shotf159.5267.1461.4966.9668.9670.2675.37†‡.
auc0.85560.90760.89480.88880.89950.91540.9335†‡.
5-way 10-shotf1auc62.780.869870.090.923969.890.923573.270.917774.130.923075.240.934380.16†‡0.9528†‡.
10-way 5-shotf1auc45.620.849451.270.884447.910.873552.060.873554.140.885057.260.906365.65†‡0.9206†.
10-way 10-shotf1auc44.700.837754.610.899056.060.901959.030.901359.840.908161.510.928669.70†‡0.9342†.
table 4: evaluation results in terms of auc and macro-f1 (%) on fewasp..models.
proto-awatt (ours)w/o saw/o attention matrix w iw/o qaw/o dtw/o dt w/ krw/o dt w/ ms.fewasp.
auc0.92060.88900.91280.88860.91610.91590.9163.f165.6554.3461.6851.1964.4864.0664.00.models.
proto-hattf1auc57.26glove + cnn0.906359.46glove + lstm 0.913757.330.8971bert59.570.9067distilbert.
proto-awattf1auc65.650.920666.860.935770.090.945970.230.9451.table 6: ablation study of using different encoders inthe 10-way 5-shot scenario on fewasp..table 5: ablation study of the 10-way 5-shot scenarioon fewasp..noise with hybrid instance-level and feature-level attention mechanisms..4.4 experimental analysis.
we report the experimental results of various meth-ods in table 2, table 3, table 4 and table 5. thebest scores on each metric are marked in bold.
theexperimental results demonstrate the effectivenessof our method.
overall performance auc and macro-f1 scoresof all the methods are shown in table 2, table 3and table 4. firstly, we observe that our methodproto-awatt achieves the best results on almostall evaluation metrics of the three datasets.
thisreveals the effectiveness of the proposed method.
secondly, compared to proto-hatt, proto-awattachieves signiﬁcant improvement.
it is worth not-ing that the average improvement of macro-f1 onthree datasets is 4.99%.
this exhibits that the saand qa modules successfully reduce noise for few-shot acd.
meanwhile, accurate distance measure-ment between prototypes and the prototype-speciﬁcquery representations can facilitate the detection ofmultiple aspects in the query instance..then we found that all methods on fe-wasp(multi) perform consistently worse than thecounterparts on fewasp(single) and fewasp.
thisis because more aspects increase the complexityof the dataset.
on fewasp(multi), proto-awattstill outperforms other methods in most settings,.
which demonstrates the robustness of our model onvarious data distributions..in general, the 10-way scenario contains muchmore noise than the 5-way.
we observe thatcompared to proto-hatt, proto-awatt achievesmore signiﬁcant improvements in the 10-way sce-nario than the 5-way.
the results further indicatethat proto-awatt can really alleviate the noise.
ablation study table 5 depicts the results of ab-lation study.
firstly, without the sa module, theperformances of proto-awatt drop a lot.
in par-ticular, auc drops by 3.43%, and macro-f1 dropsby 17.23% relatively.
this veriﬁes that the sa mod-ule helps reduce noise and extract better prototypes.
we can also see that without attention matrix w iin sa causes consistent decreases on all metrics.
this suggests that predicting dynamic attention ma-trix for each class is effective, which makes thesa module extract better prototypes.
then wefound that without the qa module, proto-awattsigniﬁcantly performs worse.
this validates thatfor a query instance, computing multiple prototype-speciﬁc query representations helps obtain accuratedistances for ranking, which facilitates the multi-label predictions..finally, when removing dt and using a staticthreshold (τ = 0.2 in the 10-way setting), it causesa slight decrease.
this shows that learning dynamicthreshold is effective.
we further compare dt withtwo alternative dynamic threshold methods: (1)ms (mean ± standard deviation of the thresholdby cross-validation); (2) a kernel regression (kr).
6336figure 3: macro-f1 scores for different thresholds on10-way 5-shot setting of fewasp..approach which is proposed by hou et al.
(2020)to calibrate the threshold.
comparing with ms andkr, our method slightly outperforms them.
this isbecause dt beneﬁts from reinforcement learningand directly optimizes the evaluation metrics.
different encoders we also compare the perfor-mances of our method with a strong baseline proto-hatt when using different encoders to obtain thecontextual sequence h. the results are reported intable 6. the output of pre-trained encoders, i.e.,bert (devlin et al., 2019) or distilbert (sanhet al., 2019), are directly used as the contextualsequence.
we observe that proto-awatt signiﬁ-cantly outperforms the strong baseline proto-hatton all encoders.
effects of thresholds as depicted in figure 3,we analyze the impact of different thresholds onthe macro-f1 score during inference.
we can seethat proto-awatt without dt consistently outper-forms proto-hatt in various thresholds.
macro-f1scores of the two methods are getting worse asτ grows.
however, the declines in proto-hattare more signiﬁcant.
at τ = 0.9, the macro-f1 ofproto-hatt drops nearly to 0. proto-awatt with-out dt still achieves much higher macro-f1.
thisindicates that the proposed two attention mecha-nisms help extract an accurate ranking of proto-types.
the ranking is less sensitive to the threshold,which makes our method robust and stable.
wealso found that learning threshold by dt beneﬁtsfrom a reinforced way, which slightly outperformskr and the best static threshold..4.5 visualizations.
we further analyze proto-awatt by visualizingthe extracted representations from the support set.
(a) proto-hatt.
(b) proto-awatt.
figure 4: visualization of extracted prototypes for thesupport set..(a) proto-hatt.
(b) proto-awatt.
figure 5: visualization of extracted representations forthe query set..and query set, respectively.
the representations arevisualized by t-sne (maaten and hinton, 2008).
toobserve the performance in a challenging situation,we choose the testing set from fewasp(multi) asan example.
support set figure 4 presents the visualizationof extracted prototypes from two methods.
we ran-domly sample 5 classes and then sample 50 times of5-way 5-shot meta-tasks for the ﬁve classes.
thenfor each class, we have 50 prototype vectors.
weobserve that prototype vectors from our approachare more separable than those from proto-hatt.
this further indicates that the sa module can alle-viate noise and thus yield better prototypes.
query set we randomly sample 5 classes andthen sample 20 times of 5-way 5-shot meta-tasksfor these classes.
each meta-task has 5 query in-stances per class.
thus we have 25 × 20 = 500query instances.
it is worth noting that our modellearns n prototype-speciﬁc query representationsfor each query instance.
we choose the represen-tations according to the ground-truth label.
how-ever, proto-hatt only outputs a single representa-tion for a query instance.
as depicted in figure 5,we can see that the representations learned by ourmethod are obviously more separable than thoseby proto-hatt.
this further reveals that proto-awatt can obtain accurate prototype-speciﬁcquery representations, which contributes to com-.
63370.10.20.30.40.50.60.70.80.9threshold 0102030405060macro f1proto-hattproto-awatt w/o dtproto-awattproto-awatt w/o dt w/ krhatt_protomy_rlhatt_querymy_rl_queryputing accurate distances..5 conclusion.
in this paper, we formulate the aspect category de-tection (acd) task in the few-shot learning (fsl)scenario.
existing fsl methods mainly focus onsingle-label predictions.
they can not work wellfor the acd task since a sentence may contain mul-tiple aspect categories.
therefore, we propose amulti-label fsl method based on the prototypicalnetwork.
speciﬁcally, we design two effective at-tention mechanisms for the support set and queryset to alleviate the noise from both sets.
to achievemulti-label inference, we further learn a dynamicthreshold per instance by a policy network with con-tinuous action space.
extensive experimental re-sults in three datasets demonstrate that our methodoutperforms strong baselines signiﬁcantly..acknowledgements.
we sincerely thank all the anonymous reviewersfor providing valuable feedback.
this work is sup-ported by the national science and technology ma-jor project, china (grant no.
2018yfb0204304)..references.
amit alfassy, leonid karlinsky, amit aides, josephshtok, sivan harary, rogerio feris, raja giryes,and alex m bronstein.
2019. laso: label-set op-erations networks for multi-label few-shot learning.
in (cvpr), pages 6548–6557..kelsey allen, evan shelhamer, hanul shin, and joshuainﬁnite mixture prototypes for.
tenenbaum.
2019.few-shot learning.
in (icml), pages 232–241..konstantin bauman, bing liu, and alexander tuzhilin.
2017. aspect based recommendations: recom-mending items with the most valuable aspects basedon user reviews.
in (kdd), page 717–725..kai-hsiang cheng, szu-yu chou, and yi-hsuan yang.
2019. multi-label few-shot learning for sound eventrecognition.
in 2019 ieee 21st international work-shop on multimedia signal processing (mmsp),pages 1–5.
ieee..po-wei chou, daniel maturana, and sebastian scherer.
2017. improving stochastic policy gradients in con-tinuous control with deep reinforcement learning us-ing the beta distribution.
in (icml), pages 834–843..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in (naacl-hlt), pages 4171–4186..li fe-fei et al.
2003. a bayesian approach to unsu-pervised one-shot learning of object categories.
in(iccv), pages 1134–1141..li fei-fei, rob fergus, and pietro perona.
2006.
(tpami),.
one-shot learning of object categories.
28(4):594–611..chelsea finn, pieter abbeel, and sergey levine.
2017.model-agnostic meta-learning for fast adaptation ofdeep networks.
in (icml), pages 1126–1135..tianyu gao, xu han, zhiyuan liu, and maosong sun.
2019. hybrid attention-based prototypical networksfor noisy few-shot relation classiﬁcation.
in (aaai),pages 6407–6414..victor garcia and joan bruna.
2018. few-shot learning.
with graph neural networks.
in (iclr)..spyros gidaris and nikos komodakis.
2018. dy-namic few-shot visual learning without forgetting.
in (cvpr), pages 4367–4375..yiluan guo and ngai-man cheung.
2020. attentiveweights generation for few shot learning via informa-tion maximization.
in (cvpr), pages 13499–13508..zhen hai, kuiyu chang, and jung-jae kim.
2011. im-plicit feature identiﬁcation via co-occurrence associ-in international conference onation rule mining.
intelligent text processing and computational lin-guistics, pages 393–404..xu han, hao zhu, pengfei yu, ziyun wang, yuan yao,zhiyuan liu, and maosong sun.
2018. fewrel:a large-scale supervised few-shot relation classiﬁ-cation dataset with state-of-the-art evaluation.
in(emnlp), pages 4803–4809..yutai hou, yongkui lai, yushan wu, wanxianglearningarxiv preprint.
che, and ting liu.
2020.for multi-labelarxiv:2010.05256..intent detection..few-shot.
mengting hu, shiwan zhao, li zhang, keke cai,zhong su, renhong cheng, and xiaowei shen.
2019. can: constrained attention networks forin (emnlp-multi-aspect sentiment analysis.
ijcnlp), pages 4601–4610..svetlana kiritchenko, xiaodan zhu, colin cherry, andsaif mohammad.
2014. nrc-canada-2014: detect-ing aspects and sentiment in customer reviews.
inproceedings of the 8th international workshop on se-mantic evaluation (semeval 2014), pages 437–442..gregory koch, richard zemel, and ruslan salakhutdi-nov. 2015. siamese neural networks for one-shot im-age recognition.
in icml deep learning workshop..kai li, yulun zhang, kunpeng li, and yun fu.
2020.adversarial feature hallucination networks for few-shot learning.
in (cvpr), pages 13470–13479..6338laurens van der maaten and geoffrey hinton.
2008.journal of machine.
visualizing data using t-sne.
learning research, 9(nov):2579–2605..oriol vinyals, charles blundell, timothy lillicrap,daan wierstra, et al.
2016. matching networks forone shot learning.
in (neurips), pages 3630–3638..sajad movahedi, erfan ghadery, heshaam faili, andazadeh shakery.
2019. aspect category detec-arxiv preprinttion via topic-attention network.
arxiv:1901.01183..tsendsuren munkhdalai and hong yu.
2017. meta net-.
works.
in (icml), pages 2554–2563..yu-xiong wang, ross girshick, martial hebert, andbharath hariharan.
2018. low-shot learning fromimaginary data.
in (cvpr), pages 7278–7286..ronald j williams.
1992. simple statistical gradient-following algorithms for connectionist reinforce-ment learning.
machine learning, 8(3-4):229–256..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for word rep-resentation.
in (emnlp), pages 1532–1543..wei xue and tao li.
2018. aspect based sentimentin.
analysis with gated convolutional networks.
(acl), pages 2514–2523..wei xue, wubai zhou, tao li, and qing wang.
2017.mtna: a neural multi-task model for aspect cat-egory classiﬁcation and aspect term extraction onrestaurant reviews.
in (ijcnlp), pages 151–156..daojian zeng, kang liu, siwei lai, guangyou zhou,and jun zhao.
2014. relation classiﬁcation viain (coling),convolutional deep neural network.
pages 2335–2344..fang zhao, jian zhao, shuicheng yan, and jiashi feng.
2018. dynamic conditional networks for few-shotlearning.
in (eccv), pages 19–35..xinjie zhou, xiaojun wan, and jianguo xiao.
2015.representation learning for aspect category detec-tion in online reviews.
in (aaai), page 417–423..maria pontiki, dimitris galanis, haris papageorgiou,suresh manandhar, and ion androutsopoulos.
2015.semeval-2015 task 12: aspect based sentimentanalysis.
in (semeval 2015), pages 486–495..maria pontiki, dimitris galanis, john pavlopoulos,harris papageorgiou,ion androutsopoulos, andsuresh manandhar.
2014. semeval-2014 task 4: as-pect based sentiment analysis.
in (semeval 2014),pages 27–35..sachin ravi and hugo larochelle.
2017. optimizationas a model for few-shot learning.
in (iclr), pages1–11..anthony rios and ramakanth kavuluru.
2018. few-shot and zero-shot multi-label learning for structuredlabel spaces.
in (emnlp), pages 3132–3142..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..kim schouten, onne van der weijde, flavius frasin-car, and rommert dekker.
2018. supervised and un-supervised aspect category detection for sentimentieee transac-analysis with co-occurrence data.
tions on cybernetics, 48(4):1263–1275..jake snell, kevin swersky, and richard zemel.
2017.in.
prototypical networks for few-shot learning.
(neurips), pages 4077–4087..qi su, kun xiang, houfeng wang, bin sun, and shi-wen yu.
2006. using pointwise mutual informationto identify implicit features in customer reviews.
ininternational conference on computer processingof oriental languages, pages 22–30..flood sung, yongxin yang, li zhang, tao xiang,philip hs torr, and timothy m hospedales.
2018.learning to compare: relation network for few-shotlearning.
in (cvpr), pages 1199–1208..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in (neurips), pages 5998–6008..6339models.
proto-awatt (ours)w/o saw/o attention matrix w iw/o qaw/o dtw/o dt w/ krw/o dt w/ ms.fewasp(single)f1auc80.280.970163.630.93040.970378.0869.610.954179.460.968979.410.969578.730.9681.fewasp(multi)f1auc58.890.898053.020.885457.420.895951.510.892059.400.89700.900659.1359.010.8976.fewasp.
auc0.92060.88900.91280.88860.91610.91590.9163.f165.6554.3461.6851.1964.4864.0664.00.table 7: ablation study of the 10-way 5-shot scenario on three datasets..a implementation details.
hyperparameters all baselines and our modelare implemented by pytorch.
we initialize wordembeddings with 50-dimension glove vectors andﬁne-tune them during the training.
all other pa-rameters are initialized by sampling from a nor-mal distribution n (0, 0.1).
the dimension of thehidden state d is 50. the convolutional windowsize m is set as 3. the optimizer is adam with alearning rate 10−3.
when jointly training the pol-icy network, the learning rate is set to 10−4.
ineach dataset, we construct four fsl tasks, wheren = 5, 10 and k = 5, 10. and the number ofquery instances per class is 5. for example, in a5-way 10-shot meta-task, there are 5 × 10 = 50 in-stances in the support set and 5 × 5 = 25 instancesin the query set.
dynamic threshold (dt)in this module, weﬁrst map the state into a vector representationthrough linear layers.
then the vector is mappedinto two separate linear layers with softplus as theactivation function.
we obtain the parameters ofbeta distribution, i.e.
a and b, respectively.
whentraining the policy network, a reward is computedbased on the softmax output (i.e.
ranking of pro-totypes).
however, the softmax output is narrowand highly conﬁdent, resulting in sparse rewards.
therefore, we exploit a temperature t = 2 to makethe softmax output more smooth.
in addition, two-stage training is also designed to deal with thesparse rewards.
we ﬁrst train the main networkto obtain accurate rankings.
then when learningthe policy network, we can gain more meaningfulrewards.
training detailsin every epoch, we randomlysample 800 meta-tasks for training.
the number ofmeta-tasks during validation and testing are bothset as 600. the average score of meta-tasks areused for evaluation.
we employ an early stop strat-egy if the auc score of the validation set is notimproved in 3 epochs, and the best model is chosen.
figure 6: effects of attention matrix on 5-way 5-shotsetting of fewasp..for testing.
for all baselines and our model, we re-port the average testing results from 5 runs, wherethe seeds are set to [5, 10, 15, 20, 25].
all modelsare trained on one tesla p100 gpu with 16gb ofram..b experimental results.
ablation study we display the results of ablationstudy on three datasets in table 7.effects of attention matrix to explore the ef-fects of the condition on the attention matrix, wecompare the performances of proto-awatt by set-ting different repeat times em in eq.
3. the resultsare displayed in figure 6. we can see that by re-peating more times of the common aspect vector,the auc and macro-f1 score both outperform theresults of setting em = 1. as em grows, the per-formances are improved.
however, when settingem as 25 or even 50, the performances decline.
apossible reason is that the model tends to overﬁtthe training classes..634012345672550repeat times0.9150.9250.9350.945aucauc0.730.750.770.79macro-f1macro-f1