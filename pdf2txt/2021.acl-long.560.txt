language embeddings for typology and cross-lingual transfer learning.
dian yu∗, taiqi he∗ and kenji sagae.
university of california, davis{dianyu, tqhe, sagae}@ucdavis.edu.
abstract.
cross-lingual language tasks typically requirea substantial amount of annotated data or par-allel translation data.
we explore whetherlanguage representations that capture relation-ships among languages can be learned andsubsequently leveraged in cross-lingual taskswithout the use of parallel data.
we gener-ate dense embeddings for 29 languages usinga denoising autoencoder, and evaluate the em-beddings using the world atlas of languagestructures (wals) and two extrinsic tasks ina zero-shot setting: cross-lingual dependencyparsing and cross-lingual natural language in-ference1..1.introduction.
recent efforts to leverage multilingual datasets inlanguage modeling (conneau and lample, 2019;devlin et al., 2019) and machine translation (john-son et al., 2017; lu et al., 2018) highlight the po-tential of multilingual models that can performwell across various languages, including ones forwhich training sets are scarce.
most of the currentmultilingual research focuses on learning invariantrepresentations or removing language-speciﬁc fea-tures after training (libovický et al., 2020; bjervaand augenstein, 2021).
despite recent advances,there are still limitations.
previous work has shownthat similar languages can beneﬁt from sharing pa-rameters, but less similar languages do not help(zoph et al., 2016; pires et al., 2019).
however,in spite of some interests in typology (ponti et al.,2019), identifying similar languages is nontrivial,especially for less studied ones.
in addition, aszhao et al.
(2019) suggest, learning invariant rep-resentations can actually harm model performance..∗ equal contribution.
1our learned language embeddings and code avail-able at https://github.com/diandyu/language_embeddings.
therefore, in order to leverage language agnosticand language speciﬁc information effectively, wepropose to generate language representations andexamine the interactions among different languagerepresentations..one way to represent language identity withina multilingual model is the use of language codes,or dense vectors representing language embed-dings.
if languages are represented with vectorsthat capture cross-lingual similarities and differ-ences across different dimensions, this informationcan guide a multilingual model regarding what andhow much of the information in the model shouldbe shared among speciﬁc languages.
much of theprevious research focused on generating languageembeddings using prior knowledge such as word or-der (ammar et al., 2016; littell et al., 2017), usinga parallel corpus (bjerva et al., 2019b; östling andtiedemann, 2017), and using language codes as anindicator to distinguish input and output words in ashared vocabulary into different languages (john-son et al., 2017; conneau and lample, 2019).
incontrast, our work focuses on generating and us-ing language embeddings more effectively as soft-sharing (de lhoneux et al., 2018) of parametersamong various languages in a single model.
fur-thermore, we are motivated by a more difﬁcult set-ting where the properties of each language are notknown in advance, and no parallel data is available.
we investigate whether we can generate lan-guage embeddings to represent typological infor-mation derived solely from corpora in each lan-guage without the use of any parallel text, trans-inspired bylation models, or prior knowledge.
the ﬁndings that structural similarity, especiallyword ordering, is crucial in large pretrained multi-lingual language models (k et al., 2020), we pro-pose an unsupervised method leveraging denois-ing autoencoders (vincent et al., 2008) to gener-ate language embeddings.
we show that our ap-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7210–7225august1–6,2021.©2021associationforcomputationallinguistics7210proach captures typological information by com-paring the information in our language embeddingsto language-speciﬁc information in the world at-las of language structures (wals, dryer andin addition, to address thehaspelmath, 2013).
question of whether the learned language embed-dings can help in downstream language tasks, weplug-in the language embeddings to cross-lingualdependency parsing and natural language inference(xnli, conneau et al., 2018) in a zero-shot learn-ing setting, obtaining performance improvements..2 related work.
previous related research approached language rep-resentations by using prior knowledge, dense lan-guage embeddings with multilingual parallel data,or no prior knowledge about languages but hav-ing language embeddings primarily as a signal toidentify different languages..2.1 feature-based language representations.
an intuitive method to represent language informa-tion is through explicit information such as knownword order patterns (ammar et al., 2016; little,2017), part-of-speech tag sequences (wang andeisner, 2017), and syntactic dependencies (östling,2015).
littell et al.
(2017) propose sparse vec-tors using pre-deﬁned language features such asknown typological and geographical informationfor a given language.
however, linguistic featuresmay not be available for less studied languages.
our proposed approach assumes no prior knowl-edge about each language, deriving typological in-formation from plain text alone.
once a vector fora target language is created, it contains many typo-logical features of the target language, and can beused for transfer learning in downstream tasks..2.2 dense representation with parallel data.
other previous work has also explored dense con-tinuous representations of languages.
one methodis to append a language token to the beginning ofa source sentence and train the language embed-dings with a many-to-one neural machine trans-lation model (malaviya et al., 2017; tan et al.,2019).
another method is to concatenate languageembedding vectors to a character level languagemodel (östling and tiedemann, 2017; bjerva andaugenstein, 2018; bjerva et al., 2019a).
thesetwo methods require parallel translation data suchas bible and ted talk.
rabinovich et al.
(2017).
derive typological information in the form of phylo-genetic trees from translation of different languagesinto english and french using the european par-liament speech corpus (koehn, 2005), based onthe assumption that unique language properties arepresent in translations (baker et al., 1993; toury,1995).
bjerva et al.
(2019b) abstract the translatedsentences from other languages to english withpart-of-speech tags, function words, dependencyrelation tags, and constituent tags, and train theembedding vectors by concatenating a languagerepresentation with a symbol representation.
incomparison, we generate our language embeddingsusing no parallel corpora or linguistic annotation,which is suitable for a wider variety of languages,including in situations where no parallel data orprior knowledge is available..2.3 language vectors without parallel data.
the approach that is closest to ours is xlm (con-neau and lample, 2019), which adds languageembeddings to each byte pair embedding usingwikipedia data in various languages with a maskedlanguage modeling objective.
however, similarto johnson et al.
(2017), the trained language em-beddings only serve as an indicator to the encoderand decoder to identify input and output words inthe vocabulary as belonging to different languages.
in fact, in a follow up paper, xlm-r (conneauet al., 2020), language embeddings are removedfrom the model for better code-switching, whichsuggests that the learned language embeddings maynot be optimal for cross-lingual tasks.
in this paper,following the ﬁnding that structural similarity iscritical in multilingual language models (k et al.,2020), we generate language embeddings from adenoising autoencoder objective and demonstratethat they can be effectively used in cross-lingualzero-shot learning..3 generating language embeddings.
we ﬁrst present the data used to generate languageembeddings, then introduce our approach inspiredby denoising autoencoders (vincent et al., 2008)..3.1 data and preprocessing.
to train our multilingual model, we use the com-moncrawl dataset from the conll 2017 sharedtask (ginter et al., 2017) to obtain monolingualplain text in various languages.
to represent wordsacross different languages in a shared space, we.
7211use the unsupervised pretrained aligned word em-beddings from muse (lample et al., 2018).
wechoose the 29 languages from the conll 2017monolingual text dataset for which muse pre-trained embeddings are available.2 a subset of200k sentences are selected randomly for eachlanguage.
the languages we use are: english,french, romanian, arabic, german, russian, bul-garian, greek, slovak, catalan, hebrew, slovene,croatian, hungarian, spanish, czech, indonesian,swedish, danish, italian, turkish, dutch, norwe-gian bokmål, ukrainian, estonian, polish, viet-namese, finnish, and portuguese, which cover tenlanguage genera..we experiment with two types of word repre-sentations in training language embeddings.
themost straightforward way is to use the pretrainedmuse embedding for each speciﬁc language (werefer to this setting as spe.).
we also experimentedwith mapping word embeddings from different lan-guages into one language (english in our experi-ments because it is used as the pivot language inmuse embeddings, eng.)
for three reasons.
first,because muse is mainly trained by an orthogonalrotation matrix and the distances among words ineach language are still maintained thereafter, lan-guage identities can potentially be revealed.
the re-sult is that the learned language embeddings reﬂectthe features incorporated in the unsupervised wordmapping methods instead of the intrinsic languagefeatures.
second, we hypothesize that mapping toa single language space requires the model to en-code more information in language embeddings astheir language identities instead of relying on theirrevealed ones.
finally, using shared word embed-dings can reduce the vocabulary size for memoryconcerns by effectively reducing both the lookuptable size and the output softmax dimension size.
for eng.
word embedding mapping, we alignwords from different languages to english embed-dings using cross-domain similarity local scaling(csls, lample et al., 2018).
the vocabulary ofour model is restricted to the words in the englishmuse embeddings, and all unknown words arereplaced with a special unknown token.
althoughimperfect mapping from each language to englishtokens may introduce noise (see scores in appendixd) and result in a coarse approximation of the orig-inal sentences, crucial syntactic and semantic infor-.
2https://github.com/facebookresearch/.
muse.
mation should still be present..in our experiments, a language code is appendedto each token according to the original language ofthe sentence.
for instance, the german sentence“er hat den roten hund nicht gesehen" would berepresented in our spe.
condition as.
er_de hat_de den_de roten_de hund_de nicht_de gesehen_de.
and in the eng.
condition as.
he_de has_de the_de red_de dog_de not_de seen_de.
intuitively, the idea is to have the words themselvesbe the same across languages (either through thealigned muse embeddings or by direct mappingto english words), and let the additional languagecode provide to the model the information thatwould explain the structural differences observedacross languages in the training data..3.2 denoising autoencoder.
given a multilingual plain text corpus with sen-tences in each language (and no parallel text), weﬁrst perturb each sentence to create a noisy ver-sion of the sentence where its words are randomlyshufﬂed.
the training objective is to recover theoriginal sentences, which requires the model tolearn how to order words in each language.
we hy-pothesize that compared to language modeling, thiswill encourage the language embeddings to learnmore structural information instead of relying ontopics or word co-occurrence to generate meaning-ful training sentences.
we implement our multilin-gual denoising autoencoder with an lstm-based(hochreiter and schmidhuber, 1997) sequence-to-sequence model (sutskever et al., 2014).
the in-put strings are perturbed sentences and the outputstrings are the original sentences.
see appendixa.1 for implementation details..after preprocessing the data, we concatenate alanguage embedding vector initialized from normaldistribution as a language identity feature (the lan-guage code mentioned in section 3.1) to each ofthe pretrained word embeddings.
since certain lan-guages are more similar to, or more different from,each other, the model will learn how to reorder asequence of words depending on the speciﬁc lan-guage.
for example, reordering an italian sentenceshould be more similar to reordering a spanish sen-tence than it is to reordering a german sentence.
because the decoder captures the actual word orderof the sentences in each target language, whereas.
7212the language codes in the encoder are meant tocapture only language identity and no word orderinformation, we use the extracted language em-beddings from the decoder in our experiments.3each word is represented with a pretrained 300-dimensional vector, and each language embeddingis represented with a 50-dimensional vector4.
theinput token is thus a 350-dimensional vector fromthe concatenation..4 experiments.
to examine the quality of the typological infor-mation captured by the language embeddings, weperform intrinsic and extrinsic evaluations.
ourintrinsic evaluation consists of predicting linguistictypology and language features from the worldatlas of language structures (wals, dryer andhaspelmath, 2013).
our extrinsic evaluations arebased on cross-lingual dependency parsing andcross-lingual natural language inference (xnli,conneau et al., 2018) in a zero-shot learningsetting, where a trained model makes predictionson a language not seen during training, but forwhich a language embedding has been learnedfrom plain monolingual text.
in contrast withprevious research which applies learned typologyto cluster similar languages and train machinetranslation tasks in clusters (tan et al., 2019), weexplore if we can apply the learned embeddingsdirectly into downstream tasks.
we compare threedifferent sets of embeddings based on our approachwith three sets of embeddings from previous work:lang_emb represents language embed-dings from our proposed denoising autoencodertrained with language speciﬁc muse embeddings,using commoncrawl text..spe..eng..lang_emb represents language embed-dings trained with english muse embeddingsafter mapping words from different languages toenglish, using commoncrawl text..wiki lang_emb represents language embed-dings trained with english muse embeddingsusing wikipedia.
we use the same data selectionand preprocessing process as detailed in section3.1. we use these embeddings to show the.
3to conﬁrm our assumption about the embeddings for thelanguage codes in the encoder and the decoder, we also per-formed experiments using the encoder language embeddings.
as expected, the results obtained with embeddings from theencoder were inferior in every case tested..4we experimented with different dimensions for language.
embedding and did not observe performance difference..impact of training data.
in addition, we use theseembeddings to compare with xlm embeddingstrained with wikipedia..malaviya represents language embeddings frommalaviya et al.
(2017), trained with a many-to-onemachine translation model using bible paralleldata.
it has 26 languages in common with our 29languages except english, hebrew, and norwegian.
we use these embeddings to represent previousmethods of learning language representations fromparallel data.5.
xlm mono represents language embeddingstrained with xlm model using the same monolin-gual data as wiki lang_emb on 29 languages..xlm parallel represents language embeddingstrained with xlm using monolingual and paralleldata from 15 xnli languages.
we extract theembeddings from the publicly available model..4.1 linguistic typology prediction.
we ﬁrst inspect the language embeddings qualita-tively through principle component analysis (pca)visualization.
we also use spectral clustering torecover the language genus (language family sub-group) information from the embeddings.
to com-pare the quality of the clusterings quantitatively,we calculate the adjusted rand index (hubert andarabie, 1985) between the generated clusters andthe actual language genera..4.2 wals feature prediction.
we evaluate the language embeddings on predict-ing language features in wals.
each wals fea-ture describes a characteristic of languages, such asthe order of subject, object, and verb.
we considerthe features for which information is available formore than 50% of the languages we use and casteach feature prediction as a multi-class classiﬁca-tion task.
we then classify the features into thefollowing categories (see details in appendix b)..• lexicon:.
usage of speciﬁc words, e.g.
whether the language has separate words for“hand” and “arm”, etc.;.
• syntax: mostly related to the relative ordersbetween various types of constituents, includ-ing order of subject, object and verb, adpo-.
5we do not evaluate the embeddings from malaviya et al.
(2017) on parsing and xnli because they do not include en-glish embeddings, which are necessary for a direct comparison.
in xnli, in particular, there is only training data for english..7213sitions and noun phrases, and also featuresrelated to syntactic constructions;.
• partially morphological (part.
morph.
):features that mainly concern syntax or seman-tics but either usually relate to morphology(such as inﬂectional morphemes), or have mor-phological information coded in the values ofthe features, e.g.
gender systems, order ofnegative morphemes and verbs;.
• non-learnable: features that mainly concernmorphology, phonology, or phonotactics, andare not learnable from reordering plain text..the categories make it easier to evaluate whatthe language embeddings capture.
we train linearclassiﬁers to predict wals results.
for each fea-ture, we hold out one language and train a classiﬁeron the language embeddings of the rest of the lan-guages to predict the corresponding feature valueson the held-out language embedding, in a leave-one-out cross-validation scheme.
we then averagethe accuracy of the features within each categoryto report the results.
in addition to comparing dif-ferent language embeddings, we also compare totwo baselines: a random baseline, and a majoritybaseline (which predicts the most common valuefor each feature).
we repeat this procedure 100times while randomly permuting the orders of theinput vectors to the classiﬁers to eliminate possibleeffects due to initial states and report the averageand signiﬁcant scores..compared to a recent shared task where theinput is some features of a language (e.g.
lan-guage family and various wals features), withoptionally pre-computed language embeddings todevelop models to predict other features (bjervaet al., 2020), we investigate if trained languageembeddings alone can be used to predict walsfeatures.
in addition, we showed that our languageembeddings outperformed a frequency baselineamong other baselines (see section 5.2) comparedto bjerva et al.
(2020)..4.3 cross-lingual dependency parsing.
since our language embeddings are trained usinga word ordering task, we hypothesize that theycapture syntactic information.
to verify that mean-ingful syntactic information is captured in the lan-guage embeddings, we use a dependency parsingtask where sentences for each target language areparsed with a model trained with treebanks from.
other languages, but no training data for the targetlanguage.
this can be seen as a form of cross-lingual parsing or zero-shot parsing, where multi-ple source languages are used to train a model fora new target language.
without annotated trainingdata for parsing a target language, the model is ex-pected to leverage treebanks from other languagesthrough language embeddings..we use 16 languages from universal dependen-cies v2.6 (zeman et al., 2020), representing ﬁvedistinct language genera (table 2).
we modiﬁed yuzhang’s implementation6 of biafﬁne dependencyin speciﬁc,parser (dozat and manning, 2017).
we freeze word embeddings, concatenate a 50-dimensional embedding (either the correspondingeng.
language embedding or a random embedding)to the embedding of each token, and not use part-of-speech information (since we are assuming noannotated data is available for the target language).
the goal of this evaluation is not to obtain state-of-the-art attachment scores, but to ﬁnd whethera model that uses our language embeddings pro-duces higher attachment scores than a model thatinstead uses random embeddings of the same size7.
while our embeddings should capture syntactic ty-pology, random embeddings would simply indicateto the model the language for each sentence withno information about how languages are related..4.4 xnli.
natural language inference (nli) is a language un-derstanding task where the goal is to predict textualentailment between a premise and a hypothesis asa three-way classiﬁcation: neutral, contradiction,and entailment.
the xnli dataset (conneau et al.,2018) translates english nli validation and testdata into 14 other languages.
we evaluate on tenof the xnli languages which we trained languageembeddings with..state-of-the-art models on xnli are transform-ers (vaswani et al., 2017) pretrained on large cor-pora (hu et al., 2020).
to evaluate if our learnedlanguage embeddings (from an lstm model) canbe plugged off-the-shelf into other architecturessuch as transformer, we compare with two strongtransformer-based baselines, xlm (conneau andlample, 2019. l = 12, h = 1024, 250m params).
6https://github.com/yzhangcs/parser7random embeddings are used to eliminate the effect ofdifferent dimensionality.
in our preliminary experiments, wefound that adding a random embedding performs better thannot adding any embedding..7214figure 1: two-dimensional pca projection of the 50-dimensional language embeddings.
shapes represent auto-matically derived clusters, and colors represent language genera..and xlm-r (conneau et al., 2020. xlm-rbase:l = 12, h = 768, 270m params; xlm-rlarge: l= 24, h = 1024, 550m params).
xlm adds lan-guage embeddings together with each word embed-ding and position embedding as the input embed-ding in training masked language modeling (mlm,with monolingual data) and/or a translation lan-guage modeling (tlm, with translation paralleldata).
in comparison, xlm-r removes languageembedding and is pretrained with mlm on muchmore data.
we train our model on the englishmultinli (williams et al., 2018) dataset, and di-rectly evaluate the trained model on the other lan-guages without language-speciﬁc ﬁne-tuning, in azero-shot cross-lingual setting.
to select the bestcheckpoint for test set evaluation, we follow con-neau et al.
(2020) by evaluating on the develop-in addition, we alsoment set of all languages.
experiment with a fully zero-shot transfer settingwhere we select the best checkpoint by evaluatingon the english development set.
we run the se-lected checkpoint on the test set of each languageand report the accuracy scores.
we use the pub-lic available xlm model pretrained on 15 xnlilanguages with mlm and tlm objectives, andxlm-r pretrained on 100 languages.
in order toadd our learned language embeddings into xlmand xlm-r models, we normalize our embeddingsto have the same variance as the xlm language em-beddings, and we learn a simple linear projection.
layer to map our 50-dimension embeddings (whichis frozen during training) to the hidden dimensionof corresponding models.
we report all results av-eraged over three random seeds.
see appendix a.2for implementation details..5 results and analysis.
we show results of our proposed language embed-dings in comparison to the baselines and languagevectors generated from previous work on linguistictypology, wals, cross-lingual parsing, and xnli.
we report results with eng.
language embeddings.
detailed comparison to other language embeddingson each task can be found in appendix c..5.1 linguistic typology.
n features.
randommajority.
malaviyaxlm mono.
spe.
eng.
wiki.
lexicon syntax part.
morph.
non-learn.
rand46.
20.
14.
2.
-.
0.560.640.66∗0.41.
0.640.85∗0.87∗.
0.610.75.
0.740.750.78∗0.79∗0.81∗.
0.520.69.
0.660.66.
0.680.71∗0.70∗.
0.520.68.
0.660.68.
0.660.660.68.
--.
0.130.12.
0.530.580.51.table 1: wals prediction and linguistic typologyclustering results on 26 in-common languages across10 language genera.
∗indicates statistical signiﬁcance(p < 0.01) over the majority baseline..figure 1 shows a two-dimensional pca projec-.
72150.40.20.00.20.40.60.40.20.00.20.40.6englisharabicbulgariancatalancroatianczechdanishdutchestonianfinnishfrenchgermangreekhebrewhungarianindonesianitaliannorwegianpolishportugueseromanianrussianslovakslovenespanishswedishturkishukrainianvietnamesegermanicsemiticslavicromancefinnicgreekugricmalayo-sumbawanturkicviet-muongtion of the learned language embeddings.
due tospace limitations, we only show the projection ofthe language embeddings using words mapped toenglish embeddings; using language-speciﬁc em-beddings produces similar results.
we can clearlysee the clustering of slavic languages on the lowerleft, romance on the right, and germanic on theupper left.
our dataset also contains two finniclanguages, which appear right above the slaviclanguages, and two semitic languages, which ap-pear on the lower right.
the other languages, viet-namese, indonesian, turkish, and greek, are fromlanguage groups underrepresented in our dataset,and appear either mixed with the germanic lan-guages (in the case of hungarian, turkish andgreek), or far on the lower right corner (viet-namese, indonesian).
romanian, a romance lan-guage, appears miscategorized by our languageembeddings.
while it is close to the cluster of ro-mance languages, it appears closer to the singletonlanguages in the dataset and to the two semiticlanguages..in addition to actual language relationships rep-resented by color, we also present the result ofspectral clustering with four categories through dif-ferent shapes.
results illustrate that our languageembeddings can capture similarities and dissimi-larities among language families.
in comparison,language embeddings generated by malaviya et al.
(2017) do not capture clearly visible language rela-tionships (see appendix c.3).
quantitatively, clus-ters from our learned language embeddings (eng.)
achieve a much higher rand score (0.58) comparedto previous language embeddings, as shown in ta-ble 1 (last column).
this indicates that our clustersclosely align with true language families..5.2 wals predictions.
table 1 shows the prediction accuracy for walsfeatures, averaged within each category.
unlike thelanguage representations generated by bjerva et al.
(2019b), which do not outperform the majoritybaseline without ﬁnetuning, our derived languageembeddings perform signiﬁcantly better than thebaselines and previous methods in lexicon, syntax,and partially morphological categories.
note thateven though the training objective of the denois-ing autoencoder is to recover a language-speciﬁcword order, the model does not use linguistic fea-tures such as grammatical relation labels or subject-verb-object order information.
instead, it derives.
typological information from text alone through theword reordering task.
the language embeddingsgenerated with words mapped to english embed-dings (wiki and eng.)
generally produce moreaccurate predictions, with the models trained fromwikipedia producing slightly better results likelydue to cleaner training data.
we show wals re-sults comparison on 29 languages and comparisonto xlm parallel in appendix c.1.
results fromdifferent settings show that we do not need cleandata (e.g.
wiki) to generate language embeddings..language.
baseline language emb..finnic.
estonianfinnishgermanicdanishenglishgermannorwegiansloveneromancecatalanfrenchspanishportuguese.
semitic.
arabichebrew.
slavic.
bulgarianczechrussian.
average.
56.1959.59.
63.3174.5164.3677.1967.92.
72.4168.7574.4271.11.
48.4441.87.
62.9165.6262.10.
64.61.
61.68 (+5.49)62.91 (+3.32).
69.62 (+6.31)74.08 (-0.43)65.67 (+1.31)78.20 (+1.01)67.91 (-0.01).
80.76 (+8.35)79.37 (+10.62)81.74 (+7.32)79.57 (+8.46).
52.51 (+4.07)33.66 (-8.21).
67.00 (+1.09)66.98 (+1.36)66.45 (+4.35).
68.01 (+3.40).
table 2: zero-shot parsing results (uas), where eachof 16 languages are parsed using annotated languagein the language emb.
from the other 15 languages.
column, results were obtained by concatenating the lan-guage embedding to each token’s muse embedding.
in the baseline column, results were obtained using arandom embedding instead.
boldface indicates a statis-tically signiﬁcant difference (p < 0.05)..5.3 cross-lingual dependency parsing.
the cross-lingual dependency parsing results in ta-ble 2 indicate that our language embeddings are infact effective in allowing a parsing model to lever-age information from different languages to parse a.
7216fr.
es.
de.
el.
bg.
ru.
tr.
ar.
vi.
avg..selected with english development set.
xlmxlm + lang_emb.
77.378.3.
77.979.0.
75.976.5.
74.375.6.
75.376.6.
73.874.8.
70.471.3.
70.972.3.
73.274.4.
74.375.4.selected with averaged development set.
xlmxlm + lang_emb.
xlm-rbasexlm-rbase + lang_emb.
xlm-rlargexlm-rlarge + lang_emb.
77.478.5.
77.978.8.
83.683.9.
78.279.0.
78.779.4.
84.684.8.
76.176.7.
76.977.4.
83.083.7.
75.475.9.
76.076.2.
82.482.8.
76.376.8.
77.978.2.
83.384.2.
74.475.3.
75.976.1.
80.381.1.
70.371.5.
72.473.2.
79.180.3.
71.772.4.
72.272.6.
79.079.4.
73.574.8.
74.875.4.
80.080.3.
74.875.7.
75.976.4.
81.782.3.table 3: results on xnli test set with zero-shot prediction.
the results show that adding language embeddingsoutperforms the baselines in all settings..new language.
substantial accuracy improvementswere observed for 13 of the 16 languages used inthe experiment, while accuracy degradation wasobserved for two languages.
notably, there werelarge improvements for each of the four romancelanguages used (ranging from 7.32 to 10.62 ab-solute points), and a steep drop in accuracy forhebrew (-8.21).
although a sizeable improvementwas observed for the only other language from thesame genus in our experiment (arabic, with a 4.07improvement), accuracy for the two semitic lan-guages was far lower than the accuracy for the othergenera.
this is likely due to the over-representationof indo-european languages in our dataset, and thelower quality of the muse word alignments forthese languages (appendix d)..while our accuracy results are well below cur-rent results obtained with supervised methods (i.e.
using training data for each target language), theaverage accuracy improvement of 3.4 over the base-line, which uses the exact same parsing setup butwithout language embeddings, shows that our lan-guage embeddings encode actionable syntactic in-formation, corroborating our results using wals..5.4 xnli prediction.
the xnli results in table 3 indicate that our lan-guage embeddings, which capture relationshipsbetween each test language and the training lan-guage (english), are also effective in tasks involv-ing higher-level semantic information.
we observeconsistent performance gains over very strong base-lines in all settings and models for each language.
speciﬁcally, in the fully zero-shot setting where.
we select the best model based on the english de-velopment data, adding our learned language em-beddings increases 1.1 absolute points on averagefor xlm.
the same trend holds for xlm-r re-sults, not shown due to space limits.
on the otherhand, if we select the best model on the averageddevelopment set following conneau et al.
(2020),we observe averaged performance gain of 0.9, 0.5,and 0.6 absolute points for xlm, xlm-rbase, andxlm-rlarge, respectively.
we conjecture that thelower improvement on xlm-r models comparedto xlm is due to that xlm-r was pretrained with-out language embeddings.
when we add our lan-guage embeddings to the original word and po-sitional embeddings, the distribution of the over-all input embedding such as variance is changed.
hence, the language embeddings can be consideredas noise at the beginning, making it hard to learnand incorporate additional information.
however,the improvement is consistent over all strong base-lines, suggesting that our language embeddings,which are not optimized towards any speciﬁc task,can be leveraged off-the-shelf in large pretrainedmodels and achieve better zero-shot transfer abilityin downstream tasks..5.5 discussion.
our results in each of the intrinsic and extrinsicevaluation settings demonstrate that our denoisingautoencoder objective, which has been shown tobe effective in various language model pre-trainingtasks (lewis et al., 2020; raffel et al., 2020), iseffective for learning language embeddings thatcapture typological information and can be used to.
7217improve cross-lingual inference.
even though re-constructing the original sentence from a randomlyordered string is the direct training objective, ourevaluation of the resulting embeddings is not basedsimply on word order..the grammar of a language is of course an im-portant factor in determining the order of words ina sentence in that language, although it is not theonly factor.
the syntax area features in our walsevaluation, which are largely related to relative or-ders of constituents and syntactic constructions andtherefore clearly relevant to our training objective,conﬁrm that part of what our embeddings captureis in fact related to word ordering.
however, ourresults on the lexicon and morphology areas in-dicate that language-speciﬁc information capturein our embeddings goes beyond ordering informa-tion.
although it may seem that the model only hasaccess to information about word ordering duringtraining, text in the various languages also providesinformation about word usage, co-occurrence, andto some extent even inﬂection through the wordembeddings.
as a result, language embeddingstrained with our approach capture interpretable anduseful typological information beyond word order.
because language embeddings are the only sig-nal to the model indicating what each of the lan-guages that are mixed within the training data readslike, we conjecture that our denoising autoencoderobjective encourages the embeddings to encodelanguage-speciﬁc information necessary to distin-guish each language from the others..6 conclusion.
language embeddings have the potential to con-tribute to our understanding of language and lin-guistic typology, and to improve the performanceof downstream multilingual nlp applications.
ourproposed method to generate dense vectors to cap-ture language features is relatively simple, basedon the idea of denoising autoencoders.
the modeldoes not require any labeled or parallel data, whichmakes it promising for cross-lingual learning insituations where no task-speciﬁc training datasetsare available..we showed that the trained language embed-dings represent typological information, and canalso beneﬁt the downstream tasks in a zero-shotlearning setting.
this is an encouraging result thatindicates that task-speciﬁc annotated data for var-ious languages can be leveraged more effectively.
for improved task performance in situations wherelanguage-speciﬁc resources may be scarce.
at thesame time, our results indicate that the effective-ness of our approach is sensitive to the set of lan-guages used, highlighting the importance of usinga more balanced variety of languages than is cur-rent practice, our work included.
we will pursuean investigation of the impact of language selectionin multilingual and cross-lingual models as futurework, to our understanding of these methods andtheir broader applicability..acknowledgments.
we thank the anonymous reviewers for their con-structive suggestions.
this work was supported bythe national science foundation under grant no.
1840191. any opinions, ﬁndings, and conclusionsor recommendations expressed are those of the au-thors and do not necessarily reﬂect the views of thensf..ethical consideration.
our motivation to learn language embeddings with-out parallel data is to understand how languagerelationships and typology can be generated with-out any human annotation.
we also explore howour learned language embeddings can be appliedto downstream tasks.
we hope that our proposedmethod can inspire future research on generatingand utilizing typology in cross-lingual settings be-cause we may not have a large amount of trans-lation data for each language, which has beenwidely used in past research on data-driven mod-eling of linguistic typology.
since our proposedmethod can be easily adapted to different archi-tectures and pre-trained models with minimal cost(in terms of both data annotation cost and compu-tation cost), it can reduce resources needed whenapplying language embeddings for zero-shot cross-lingual downstream tasks.
we run all our exper-iments on two titan rtx gpus and two rtx2080ti gpus.
we compare our language embed-dings to baselines in the standard settings in litera-ture..references.
waleed ammar, george mulcaire, miguel ballesteros,chris dyer, and noah a. smith.
2016. many lan-guages, one parser.
transactions of the associationfor computational linguistics, 4:431–444..7218mona baker, gill francis, and elena tognini-bonelli.
’corpus linguistics and translation stud-1993.ies: implications and applications’.
john benjaminspublishing company, netherlands..johannes bjerva and isabelle augenstein.
2018. fromphonology to syntax: unsupervised linguistic typol-ogy at different levels with language embeddings.
inproceedings of the 2018 conference of the northamerican chapter of the association for compu-tational linguistics: human language technolo-gies, volume 1 (long papers), pages 907–916, neworleans, louisiana.
association for computationallinguistics..johannes bjerva and isabelle augenstein.
2021. doestypological blinding impede cross-lingual sharing?
in proceedings of the 16th conference of the euro-pean chapter of the association for computationallinguistics: main volume, pages 480–486, online.
association for computational linguistics..johannes bjerva, yova kementchedjhieva, ryan cot-terell, and isabelle augenstein.
2019a.
a probabilis-tic generative model of linguistic typology.
in pro-ceedings of the 2019 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long and short papers), pages 1529–1540,minneapolis, minnesota.
association for computa-tional linguistics..johannes bjerva, robert östling, maria han veiga,jörg tiedemann, and isabelle augenstein.
2019b.
what do language representations really represent?
computational linguistics, 45(2):381–389..in natural language processing, pages 2475–2485,brussels, belgium.
association for computationallinguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-in 5th international conference on learninging.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..matthew s. dryer and martin haspelmath, editors.
2013. wals online.
max planck institute for evo-lutionary anthropology, leipzig..filip ginter, jan hajiˇc, juhani luotolahti, milan straka,and daniel zeman.
2017. conll 2017 shared task- automatically annotated raw texts and word embed-dings.
lindat/clarin digital library at the in-stitute of formal and applied linguistics (úfal),faculty of mathematics and physics, charles uni-versity..sepp hochreiter and jürgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..johannes bjerva, elizabeth salesky, sabrina j. mielke,aditi chaudhary, celano giuseppe, edoardo mariaponti, ekaterina vylomova, ryan cotterell, and is-abelle augenstein.
2020. sigtyp 2020 shared task:in proceedingsprediction of typological features.
of the second workshop on computational researchin linguistic typology, pages 1–11, online.
associ-ation for computational linguistics..junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th internationalconference on machine learning, volume 119 ofproceedings of machine learning research, pages4411–4421.
pmlr..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..alexis conneau and guillaume lample.
2019. cross-lingual language model pretraining.
in advances inneural information processing systems, volume 32,pages 7059–7069.
curran associates, inc..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methods.
lawrence hubert and phipps arabie.
1985. compar-ing partitions.
journal of classiﬁcation, 2(1):193–218..melvin johnson, mike schuster, quoc v. le, maximkrikun, yonghui wu, zhifeng chen, nikhil thorat,fernanda viégas, martin wattenberg, greg corrado,macduff hughes, and jeffrey dean.
2017. google’smultilingual neural machine translation system: en-abling zero-shot translation.
transactions of the as-sociation for computational linguistics, 5:339–351..karthikeyan k, zihan wang, stephen mayhew, anddan roth.
2020. cross-lingual ability of multilin-gual bert: an empirical study.
in international con-ference on learning representations..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,.
7219iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander rush.
2017. opennmt: open-source toolkit for neural machine translation.
inproceedings of acl 2017, system demonstrations,pages 67–72, vancouver, canada.
association forcomputational linguistics..philipp koehn.
2005. europarl: a parallel corpusfor statistical machine translation.
in conferenceproceedings: the tenth machine translation summit,pages 79–86, phuket, thailand.
aamt, aamt..guillaume lample, alexis conneau, marc’aurelioranzato, ludovic denoyer, and hervé jégou.
2018.word translation without parallel data.
in 6th inter-national conference on learning representations,iclr 2018, vancouver, bc, canada, april 30 - may3, 2018, conference track proceedings.
openre-view.net..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..miryam de lhoneux, johannes bjerva, isabelle au-genstein, and anders søgaard.
2018. parametersharing between dependency parsers for related lan-guages.
in proceedings of the 2018 conference onempirical methods in natural language processing,pages 4992–4997, brussels, belgium.
associationfor computational linguistics..jindˇrich libovický, rudolf rosa, and alexander fraser.
2020. on the language neutrality of pre-trained mul-tilingual representations.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 1663–1674, online.
association for computa-tional linguistics..patrick littell, david r. mortensen, ke lin, kather-ine kairis, carlisle turner, and lori levin.
2017.uriel and lang2vec: representing languages astypological, geographical, and phylogenetic vectors.
in proceedings of the 15th conference of the euro-pean chapter of the association for computationallinguistics: volume 2, short papers, pages 8–14,valencia, spain.
association for computational lin-guistics..alexa n. little.
2017. connecting documentation andrevitalization: a new approach to language apps.
inproceedings of the 2nd workshop on the use of com-putational methods in the study of endangered lan-guages, pages 151–155, honolulu.
association forcomputational linguistics..yichao lu, phillip keung, faisal ladhak, vikas bhard-waj, shaonan zhang, and jason sun.
2018. a neu-ral interlingua for multilingual machine translation.
in proceedings of the third conference on machinetranslation: research papers, pages 84–92, brus-sels, belgium.
association for computational lin-guistics..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in natu-ral language processing, pages 1412–1421, lis-bon, portugal.
association for computational lin-guistics..chaitanya malaviya, graham neubig, and patrick lit-tell.
2017. learning language representations forin proceedings of the 2017typology prediction.
conference on empirical methods in natural lan-guage processing, pages 2529–2535, copenhagen,denmark.
association for computational linguis-tics..robert östling.
2015. word order typology throughmultilingual word alignment.
in proceedings of the53rd annual meeting of the association for compu-tational linguistics and the 7th international jointconference on natural language processing (vol-ume 2: short papers), pages 205–211, beijing,china.
association for computational linguistics..robert östling and jörg tiedemann.
2017. continuousin proceed-multilinguality with language vectors.
ings of the 15th conference of the european chap-ter of the association for computational linguistics:volume 2, short papers, pages 644–649, valencia,spain.
association for computational linguistics..telmo pires, eva schlinger, and dan garrette.
2019.in pro-how multilingual is multilingual bert?
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4996–5001, florence, italy.
association for computa-tional linguistics..edoardo maria ponti, helen o’horan, yevgeni berzak,ivan vuli´c, roi reichart, thierry poibeau, ekaterinashutova, and anna korhonen.
2019. modeling lan-guage variation and universals: a survey on typo-logical linguistics for natural language processing.
computational linguistics, 45(3):559–601..ella rabinovich, noam ordan, and shuly wintner.
2017. found in translation: reconstructing phylo-genetic language trees from translations.
in proceed-ings of the 55th annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 530–540, vancouver, canada.
associa-tion for computational linguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploring.
7220the limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..ilya sutskever, oriol vinyals, and quoc v le.
2014.sequence to sequence learning with neural networks.
in z. ghahramani, m. welling, c. cortes, n. d.lawrence, and k. q. weinberger, editors, advancesin neural information processing systems 27, pages3104–3112.
curran associates, inc..xu tan, jiale chen, di he, yingce xia, tao qin, andtie-yan liu.
2019. multilingual neural machinetranslation with language clustering.
in proceedingsof the 2019 conference on empirical methods innatural language processing and the 9th interna-tional joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 963–973, hongkong, china.
association for computational lin-guistics..gideon toury.
1995. descriptive translation studiesand beyond.
john benjamins, amsterdam /philadel-phia..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..pascal vincent, hugo larochelle, yoshua bengio, andpierre-antoine manzagol.
2008. extracting andcomposing robust features with denoising autoen-in proceedings of the 25th internationalcoders.
conference on machine learning, pages 1096–1103.
acm..dingquan wang and jason eisner.
2017. fine-grainedprediction of syntactic typology: discovering la-tent structure with supervised learning.
transac-tions of the association for computational linguis-tics, 5:147–161..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:.
system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..daniel zeman, joakim nivre, mitchell abrams, eliaackermann, noëmi aepli, željko agi´c, lars ahren-berg, chika kennedy ajede, gabriel˙e aleksan-draviˇci¯ut˙e, lene antonsen, katya aplonova, an-gelina aquino, maria jesus aranzabe, gashaw aru-tie, masayuki asahara, luma ateyah, furkan at-maca, mohammed attia, aitziber atutxa, lies-beth augustinus, elena badmaeva, miguel balles-teros, esha banerjee, sebastian bank, verginicabarbu mititelu, victoria basmov, colin batchelor,john bauer, kepa bengoetxea, yevgeni berzak,irshad ahmad bhat, riyaz ahmad bhat, ericabiagetti, eckhard bick, agn˙e bielinskien˙e, ro-gier blokland, victoria bobicev, loïc boizou,emanuel borges völker, carl börstell, cristinabosco, gosse bouma, sam bowman, adriane boyd,kristina brokait˙e, aljoscha burchardt, marie can-dito, bernard caron, gauthier caron, tatiana cav-alcanti, gül¸sen cebiro˘glu eryi˘git, flavio massimil-iano cecchini, giuseppe g. a. celano, slavomír ˇcé-plö, savas cetin, fabricio chalub, ethan chi, jinhochoi, yongseok cho, jayeol chun, alessandra t.cignarella, silvie cinková, aurélie collomb, ça˘grıçöltekin, miriam connor, marine courtin, eliza-beth davidson, marie-catherine de marneffe, vale-ria de paiva, elvis de souza, arantza diaz de ilar-raza, carly dickerson, bamba dione, peter dirix,kaja dobrovoljc, timothy dozat, kira droganova,puneet dwivedi, hanne eckhoff, marhaba eli, alielkahky, binyam ephrem, olga erina, tomaž er-javec, aline etienne, wograine evelyn, richárdfarkas, hector fernandez alcalde, jennifer fos-ter, cláudia freitas, kazunori fujita, katarína gaj-došová, daniel galbraith, marcos garcia, moagärdenfors, sebastian garza, kim gerdes, filipginter, iakes goenaga, koldo gojenola, memduhgökırmak, yoav goldberg, xavier gómez guino-vart, berta gonzález saavedra, bernadeta grici¯ut˙e,matias grioni, loïc grobol, normunds gr¯uz¯ıtis,bruno guillaume, céline guillot-barbance, tungagüngör, nizar habash, jan hajiˇc, jan hajiˇc jr.,mika hämäläinen, linh hà m˜y, na-rae han,kim harris, dag haug, johannes heinecke, oliverhellwig, felix hennig, barbora hladká, jaroslavahlaváˇcová, florinel hociung, petter hohle, jenahwang, takumi ikeda, radu ion, elena irimia,o. lájídé ishola, tomáš jelínek, anders johannsen,hildur jónsdóttir, fredrik jørgensen, markus juu-tinen, hüner ka¸sıkara, andre kaasen, nadezhdakabaeva, sylvain kahane, hiroshi kanayama,jenna kanerva, boris katz, tolga kayadelen, jes-sica kenney, václava kettnerová, jesse kirchner,elena klementieva, arne köhn, abdullatif kök-sal, kamil kopacewicz, timo korkiakangas, nataliakotsyba, jolanta kovalevskait˙e, simon krek, sooky-oung kwak, veronika laippala, lorenzo lam-bertino, lucia lam, tatiana lando, septina dianlarasati, alexei lavrentiev, john lee, phươnglê h`ông, alessandro lenci, saran lertpradit, her-man leung, maria levina, cheuk ying li, josieli, keying li, kyungtae lim, yuan li, nikola.
7221imilan wendt, paul widmer, seyi williams, matswirén, christian wittern, tsegay woldemariam,tak-sum wong, alina wróblewska, mary yako,kayo yamashita, naoki yamazaki, chunxiao yan,koichi yasuoka, marat m. yavrumyan, zhuoran yu,zdenˇek žabokrtský, amir zeldes, hanzhi zhu, andanna zhuravleva.
2020. universal dependencies 2.6.lindat/clariah-cz digital library at the insti-tute of formal and applied linguistics (úfal), fac-ulty of mathematics and physics, charles univer-sity..han zhao, remi tachet des combes, kun zhang, andgeoffrey gordon.
2019. on learning invariant rep-resentations for domain adaptation.
in proceedingsof the 36th international conference on machinelearning, volume 97 of proceedings of machinelearning research, pages 7523–7532.
pmlr..barret zoph, deniz yuret, jonathan may, and kevinknight.
2016. transfer learning for low-resourcein proceedings of theneural machine translation.
2016 conference on empirical methods in natu-ral language processing, pages 1568–1575, austin,texas.
association for computational linguistics..ljubeši´c, olga loginova, olga lyashevskaya,teresa lynn, vivien macketanz, aibek makazhanov,michael mandl, christopher manning, ruli ma-nurung, c˘at˘alina m˘ar˘anduc, david mareˇcek, ka-trin marheinecke, héctor martínez alonso, andrémartins, jan mašek, hiroshi matsuda, yuji mat-sumoto, ryan mcdonald, sarah mcguinness, gus-tavo mendonça, niko miekka, margarita misir-pashayeva, anna missilä, c˘at˘alin mititelu, mariamitrofan, yusuke miyao, simonetta montemagni,amir more, laura moreno romero, keiko sophiemori, tomohiko morioka, shinsuke mori, shigekimoro, bjartur mortensen, bohdan moskalevskyi,kadri muischnek, robert munro, yugo murawaki,kaili müürisep, pinkey nainwani,juan igna-cio navarro horñiacek, anna nedoluzhko, guntanešpore-b¯erzkalne, lương nguy˜ên thi., huy`ênnguy˜ên thi.
minh, yoshihiro nikaido, vitaly niko-laev, rattima nitisaroj, hanna nurmi, stina ojala,atul kr.
ojha, adédayo.
olúòkun, mai omura,emeka onwuegbuzia, petya osenova, robertöstling, lilja øvrelid, ¸saziye betül özate¸s, arzu-can özgür, balkız öztürk ba¸saran, niko partanen,elena pascual, marco passarotti, agnieszka pate-juk, guilherme paulino-passos, angelika peljak-łapi´nska, siyao peng, cenel-augusto perez, guyperrier, daria petrova, slav petrov, jason phelan,jussi piitulainen, tommi a pirinen, emily pitler,barbara plank, thierry poibeau, larisa ponomareva,martin popel, lauma pretkalnin, a, sophie prévost,prokopis prokopidis, adam przepiórkowski, ti-ina puolakainen, sampo pyysalo, peng qi, an-driela rääbis, alexandre rademaker, loganathanramasamy, taraka rama, carlos ramisch, vinitravishankar, livy real, petru rebeja, siva reddy,georg rehm,ivan riabov, michael rießler,erika rimkut˙e, larissa rinaldi, laura rituma,luisa rocha, mykhailo romanenko, rudolf rosa,valentin ros, ca, davide rovati, olga rudina, jackrueter, shoval sadde, benoît sagot, shadi saleh,alessio salomoni, tanja samardži´c, stephaniesamson, manuela sanguinetti, dage särg, baibasaul¯ıte, yanin sawanakunanon, salvatore scarlata,nathan schneider, sebastian schuster, djamé sed-dah, wolfgang seeker, mojgan seraji, mo shen,atsuko shimada, hiroyuki shirasu, muh shohibus-sirri, dmitry sichinava, aline silveira, natalia sil-veira, maria simi, radu simionescu, katalin simkó,mária šimková, kiril simov, maria skachedubova,aaron smith, isabela soares-bastos, carolyn spa-dine, antonio stella, milan straka, jana strnadová,alane suhr, umut sulubacak, shingo suzuki, zsoltszántó, dima taji, yuta takahashi, fabio tam-burini, takaaki tanaka, samson tella,isabelletellier, guillaume thomas, liisi torga, marsidatoska, trond trosterud, anna trukhina, reut tsar-faty, utku türk, francis tyers, sumire uematsu,roman untilov, zdeˇnka urešová, larraitz uria,hans uszkoreit, andrius utka, sowmya vajjala,daniel van niekerk, gertjan van noord, viktorvarga, eric villemonte de la clergerie, veronikavincze, aya wakasa, lars wallin, abigail walsh,jing xian wang, jonathan north washington, max-.
7222a implementation details.
b wals categories.
a.1 denoising autoencoder.
we use the commoncrawl dataset from the conll2017 shared task (ginter et al., 2017): http://hdl.handle.net/11234/1-1989.
we implementthe denoising autoencoder with a two-layer lstmwith 500 hidden units and global attention (luonget al., 2015) using a modiﬁed version of open-nmt (klein et al., 2017).
we use a batch size of16 and adam optimization (kingma and ba, 2015)for training with initial learning rate of 1, 0.85 de-cay applied every 25,000 steps after the ﬁrst 10,000steps.
the word embedding size if 300 pretrainedfrom muse and the language embedding size is 50.we apply global attention (lu et al., 2018) betweenthe decoder and the encoder..for experiment with xlm (conneau and lam-ple, 2019), we use the provided code base 8 fol-lowing the suggested preprocessing processes andtraining details..a.2 xnli.
for xnli experiments with both xlm and xlm-r, we follow the hyper-parameter tuning sugges-tions in the code base and author response.
we tunethe hyper-parameters on the english developmentset to match the scores reported in the correspond-ing papers, and use the same hyper-parameters forall runs..speciﬁcally,.
for xlm, we ﬁne-tuned themlm_tlm_xnli15_1024 model with the imple-mentation from the xlm code base (conneau andlample, 2019).
we use a learning rate of 5e-6(from a suggested range of [5e-6, 2.5e-5, 1.25e-4]),a batch size of 8 (from suggested range of [4, 8]),and run 150 epochs (with early stopping if the val-idation accuracy does not improve for 5 epochs)where each epoch size is 20000 examples, taking510s on a single titan rtx gpu..for xlm-r, we modiﬁed the huggingface im-plementation (wolf et al., 2020).
we use a learningrate of 7.5e-6, accumulated batch size of 128, andrun 10 full epochs (with early stopping).
we evalu-ate on the development set every 720 training steps.
for each epoch, xlm-r base takes 6468s on a sin-gle rtx 2080ti gpu, and xlm-r takes 18306son a single titan rtx gpu..8https://github.com/facebookresearch/.
xlm.
lexicon:.
129a hand and arm, 138a tea;syntax: 81a order of subject, object and verb,82a order of subject and verb, 83a order of ob-ject and verb, 84a order of object, oblique, andverb, 85a order of adposition and noun phrase,86a order of genitive and noun, 87a order of ad-jective and noun, 88a order of demonstrative andnoun, 92a position of polar question particles,93a position of interrogative phrases in contentquestions, 95a relationship between the orderof object and verb and the order of adpositionand noun phrase, 96a relationship between theorder of object and verb and the order of rela-tive clause and noun, 97a relationship betweenthe order of object and verb and the order of ad-jective and noun, 106a reciprocal constructions,110a periphrastic causative constructions, 113asymmetric and asymmetric standard negation,114a subtypes of asymmetric standard negation,121a comparative constructions, 122a relativiza-tion on subjects, 125a purpose clauses, 126a’when’ clauses, 127a reason clauses, 128autterance complement clauses, 144b positionof negative words relative to beginning and endof clause and with respect to adjacency to verb;partially morphological: 30a number of gen-ders, 31a sex-based and non-sex-based gendersystems, 32a systems of gender assignment, 34aoccurrence of nominal plurality, 35a plurality inindependent personal pronouns, 36a the asso-ciative plural, 37a deﬁnite articles, 38a indeﬁ-nite articles, 41a distance contrasts in demon-stratives, 43a third person pronouns and demon-stratives, 44a gender distinctions in independentpersonal pronouns, 45a politeness distinctionsin pronouns, 46a indeﬁnite pronouns, 47a in-tensiﬁers and reﬂexive pronouns, 48a personmarking on adpositions, 49a number of cases,50a asymmetrical case-marking, 51a position ofcase afﬁxes, 52a comitatives and instrumentals,53a ordinal numerals, 54a distributive numer-als, 57a position of pronominal possessive afﬁxes,62a action nominal constructions, 65a perfec-tive/imperfective aspect, 66a the past tense, 67athe future tense, 68a the perfect, 71a the pro-hibitive, 72a imperative-hortative systems, 74asituational possibility, 75a epistemic possibility,76a overlap between situational and epistemicmodal marking, 77a semantic distinctions of evi-dentiality, 78a coding of evidentiality, 98a align-.
7223ment of case marking of full noun phrases, 101aexpression of pronominal subjects, 102a verbalperson marking, 103a third person zero of verbalperson marking, 111a nonperiphrastic causativeconstructions, 112a negative morphemes, 115anegative indeﬁnite pronouns and predicate nega-tion, 116a polar questions, 117a predicative pos-session, 118a predicative adjectives, 119a nomi-nal and locational predication, 120a zero copulafor predicate nominals, 124a ’want’ complementsubjects, 143a order of negative morpheme andverb, 143f postverbal negative morphemes, 144aposition of negative word with respect to sub-ject, object, and verb, 144d the position of nega-tive morphemes in svo languages, 144i snegvoorder, 144j svnego order, 144k svoneg or-non-learnable: 1a consonant invento-der;ries, 2a vowel quality inventories, 3a consonant-vowel ratio, 4a voicing in plosives and frica-tives, 5a voicing and gaps in plosive systems,6a uvular consonants, 9a the velar nasal, 11afront rounded vowels, 12a syllable structure,14a fixed stress locations, 15a weight-sensitivestress, 16a weight factors in weight-sensitivestress systems, 17a rhythm types, 19a presenceof uncommon consonants, 21a exponence of se-lected inﬂectional formatives, 21b exponence oftense-aspect-mood inﬂection, 22a inﬂectionalsynthesis of the verb, 23a locus of marking inthe clause, 24a locus of marking in possessivenoun phrases, 25a locus of marking: whole-language typology, 26a preﬁxing vs. sufﬁxingin inﬂectional morphology, 27a reduplication,29a syncretism in verbal person/number mark-ing, 69a position of tense-aspect afﬁxes, 70athe morphological imperative, 79a suppletionaccording to tense and aspect, 79b suppletion inimperatives and hortatives, 104a order of personmarkers on the verb, 136a m-t pronouns, 142apara-linguistic usages of clicks..c additional results.
c.1 wals.
additional comparison on wals with differentlanguage embedding baselines..c.2 cross-lingual dependency parsing.
part.
morph.
non-learn.
rand.
n features.
randommajority.
xlm mono.
spe.
cceng.
ccwiki.
lexicon2.syntax13.
0.560.68.
0.68.
0.660.86∗0.82∗.
0.610.76.
0.760.77∗0.80∗0.80∗.
46.
0.520.68.
0.64.
0.670.72∗0.70∗.
21.
0.520.68.
0.67.
0.670.70∗0.70∗.
-.
--.
0.11.
0.580.620.62.table 4: results on the wals prediction task and lin-guistic typology on 29 languages across 10 languagegenera.
∗indicates statistical signiﬁcance (p < 0.01)over the majority baseline..n features.
lexicon1.syntax13.eng.
xlm parallel.
0.710.28.
0.610.57.part.
morph.
non-learn..38.
0.530.56.
25.
0.510.50.table 5: results on the wals prediction task andlinguistic typology on 10 languages in comparison toxlm language embeddings trained from xnli lan-guage parallel data (mlm + tlm objectives)..language.
spe.
wiki.
xlm mono.
finnic.
estonianfinnishgermanicdanishenglishgermannorwegiansloveneromancecatalanfrenchspanishportuguese.
semitic.
arabichebrew.
slavic.
bulgarianczechrussian.
60.2762.49.
68.8173.0063.6477.7671.17.
81.7278.5783.3879.65.
52.5838.93.
66.2267.3162.92.
61.3662.32.
70.3769.6865.4975.4271.19.
81.9579.2782.0980.09.
51.2633.01.
64.5759.1562.08.average.
68.02.
66.83.
53.5155.65.
66.4870.9664.0174.2464.29.
79.1372.5281.4080.40.
50.5847.56.
62.0964.6662.04.
65.60.table 6: zero-shot parsing results (uas) comparingspe., wiki, and xlm mono language embeddings.
re-sults show that using language embeddings can im-prove parsing performance, and our methods outper-form previous methods by a large margin (2.4 absolutepoints)..7224c.3 linguistic typology.
figure 2: two-dimensional pca projection of the language embeddings from malaviya et al.
(2017)..c.4 xnli.
fr.
es.
de.
el.
bg.
ru.
tr.
ar.
vi.
avg..selected with averaged development set.
xlm-rbase + spe..xlm-rbase + wiki.
xlm-rbase + xlm mono.
xlm-rbase + xlm parallel.
78.9.
79.0.
78.2.
78.3.
79.8.
78.7.
78.4.
78.9.
77.2.
76.4.
76.7.
76.5.
76.0.
76.1.
76.4.
76.6.
78.2.
78.3.
78.1.
77.6.
76.2.
75.9.
75.9.
75.4.
73.8.
73.8.
73.6.
73.2.
72.5.
71.8.
72.3.
72.4.
75.3.
75.5.
75.1.
75.0.
76.4.
76.2.
76.1.
76.0.table 7: results on xnli test set with zero-shot prediction comparing different language embeddings on xlm-rbase.
we cannot compare directly to machine translation-based methods such as malaviya et al.
(2017) becausethere is no english embeddings.
the results show that on xlm-rbase, our language embeddings perform betterthan language embeddings from previous research..d muse word translation accuracy.
en.
fr.
es.
de.
el.
bg.
ru.
tr.
he.
ar.
vi.
en.
100.00.
83.00.
83.85.
77.07.
59.81.
60.86.
67.51.
61.48.
58.69.
52.04.
54.57.table 8: precision at k = 1 word translation to english for the most frequent 50,000 words in each language usingcsls for the generated dictionary..72250.40.20.00.20.40.60.40.20.00.20.4arabicbulgariancatalancroatianczechdanishdutchestonianfinnishfrenchgermangreekhungarianindonesianitalianpolishportugueseromanianrussianslovakslovenespanishswedishturkishukrainianvietnamesesemiticslavicromancegermanicfinnicgreekugricmalayo-sumbawanturkicviet-muong