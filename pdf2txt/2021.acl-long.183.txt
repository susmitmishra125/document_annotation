excar: event graph knowledge enhanced explainable causalreasoning.
li du, xiao ding∗, kai xiong, ting liu, and bing qinresearch center for social computing and information retrievalharbin institute of technology, china{ldu, xding, kxiong, tliu,qinb}@ir.hit.edu.cn.
abstract.
prior work infers the causation between eventsmainly based on the knowledge induced fromthe annotated causal event pairs.
however, ad-ditional evidence information intermediate tothe cause and effect remains unexploited.
byincorporating such information, the logical lawbehind the causality can be unveiled, and theinterpretability and stability of the causal rea-soning system can be improved.
to facilitatethis, we present an event graph knowledge en-hanced explainable causal reasoning frame-work (excar).
excar ﬁrst acquires addi-tional evidence information from a large-scalecausal event graph as logical rules for causalreasoning.
to learn the conditional probabilis-tic of logical rules, we propose the conditionalmarkov neural logic network (cmnln) thatcombines the representation learning and struc-ture learning of logical rules in an end-to-enddifferentiable manner.
experimental resultsdemonstrate that excar outperforms previ-ous state-of-the-art methods.
adversarial eval-uation shows the improved stability of ex-car over baseline systems.
human evaluationshows that excar can achieve a promisingexplainable performance..1.introduction.
causal reasoning aims at understanding the gen-eral causal dependency between the cause and ef-fect (luo et al., 2016).
causality is commonly ex-pressed by humans in the text of natural language,and is of great value for various artiﬁcial intelli-gence applications, such as question answering (ohet al., 2013), event prediction (li et al., 2018), anddecision making (sun et al., 2018)..previous work mainly learns causal knowledgefrom manually annotated causal event pairs, andachieves promising performances (luo et al., 2016;.
∗corresponding author.
figure 1: (a) without the evidence event i, we can hardlyreveal the implicit causation between a and b.
(b) theabsent of evidence events may restrict the performanceof event-pair based methods.
(c) given an event pair,the excar framework obtains evidence events froman event graph and conducts causal reasoning using theadditional evidence events.
(d) the causal strength (cs)of the same rule can vary with different antecedents.
wedeﬁne such phenomenon as superimposed causal effect..xie and mu, 2019a; li et al., 2019).
however, re-cent works have questioned the seemingly superbperformance for some of these studies (mccoyet al., 2019; poliak et al., 2018; gururangan et al.,2018).
speciﬁcally, training data may contain ex-ploitable superﬁcial cues that are correlative of theexpected output.
the main concern is that theseworks have not learned the underlying mechanismof causation so that their inference models are notstable enough and their results are not explainable..while we notice that there is plentiful evidenceinformation outside the given corpus that can pro-vide more clues for understanding the logical lawof the causality.
figure 1 (a) exempliﬁes two cluesi1 : excess liquidity and i2: invest demand in-crease for explaining how a: quantitative easinggradually leads to b: house price increases..without these important evidence information,on the other hand, as illustrated in figure 1 (b),.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2354–2363august1–6,2021.©2021associationforcomputationallinguistics2354the causal relationship between (cid:104)a, d(cid:105) and between(cid:104)c, b(cid:105) could not be deducted from the known causa-tion between (cid:104)a, b(cid:105) and between (cid:104)c, d(cid:105).
in contrast,with intermediate event i in hand, according tothe transitivity of causality (hall, 2000), the logicchain of (cid:104)a ⇒ i ⇒ d(cid:105) and (cid:104)c ⇒ i ⇒ b(cid:105) couldbe naturally derived from the observed logic chain(cid:104)a ⇒ i ⇒ b(cid:105) and (cid:104)c ⇒ i ⇒ d(cid:105)..to fully exploit the potential of the evidenceinformation, we present an event graph knowl-edge enhanced explainable causal reasoning(excar) framework.
in particular, as illustratedin figure 1 (c), given an input event pair (cid:104)c, e(cid:105),excar ﬁrstly retrieves external evidence eventssuch as i1, i2 from a large-scale causal event graph(ceg, a causal knowledge base constructed by us),and deﬁnes the causation between c, i1, i2, e asa set of logical rules (e.g., ri = (ei ⇒ ii)), whichrules are useful representations for the causal rea-soning task because they are interpretable and canprovide insight to inference results..pearl (2001) pointed out that the underlyinglogic of causality is a probabilistic logic.
the ad-vantage of using a probabilistic logic is that byequipping logical rules with probability, one canbetter model statistically complex and noisy data.
however, learning such probabilistic logical rulesin the causal reasoning scenario is quite difﬁcult—- it requires modeling the superimposed causaleffect for each logical rule.
different from ﬁrst-order logical rules induced from some knowledgegraphs, the probability of the logical rule (i.e.
thecausal strength of the cause-effect pair) in causalreasoning is uncertain, which varies with differentantecedents.
for example, as shown in figure 1 (d),with the antecedent a: catch a cold, a fever canhardly lead to life danger.
while if fever is causedby the antecedent b: septicemia, it can result inlife danger with a high probability..to address this issue, we further proposea conditional markov neural logic network(cmnln) for learning the conditional causal de-pendency of logical rules in an end-to-end fashion.
speciﬁcally, cmnln ﬁrst decomposes the logicalrules set derived from the ceg into several distinctlogic chains and learns a distributed representationfor each logic chain in an embedding space.
subse-quently, cmnln estimates the conditional proba-bility of each logical rule by an antecedent-awarepotential function.
then cmnln computes theprobability of each logic chain by multiplying the.
probabilities of logical rules in the chain.
finally,cmnln predicts the causality score of the inputevent pair based on the disjunction of chain-levelcausality information..experimental results show that our approach caneffectively utilize the event graph information toimprove the accuracy of causal reasoning by morethan 5%.
adversarial evaluation and human eval-uation show that excar can achieve stable andexplainable performance.
the code is released athttps://github.com/sjcfr/excar..2 background.
2.1 task formalization.
in this paper, both the copa (luo et al., 2016) andthe c-copa causal reasoning task are deﬁned as amultiple-choice task.
speciﬁcally, as the followingexample shows, given a premise event, one needsto choose a more plausible cause (effect) from twohypothesis events.
example:.
premise: the company lost money.
ask-for: causehypothesis 1: its products received favorable comments.
hypothesis 2: some of its products were defective.
therefore, the causal reasoning task could beformalized as a prediction problem: given a cause-effect event pair (cid:104)c, e(cid:105) composed by the premiseevent and one of the hypothesis events, the predic-tion model is required to predict a score measuringthe causality of the event pair..2.2 causal event graph.
ceg is a large-scale causal knowledge base con-structed by us, from which we can retrieve a set ofadditional evidences for a given cause-effect eventpair (cid:104)c, e(cid:105).
formally, ceg is a directed acyclicgraph and can be denoted as g = {v, r}, wherev is the node set, r is the edge set.
each nodevi ∈ v corresponds to an event, while each edgerij ∈ r denotes that there is a causal relationshipbetween the ith event and jth event..2.3 rule-based reasoning using markov.
logic network.
in this paper, to enhance the explainability and sta-bility of causal reasoning, we cast the causal reason-ing problem as a rule based reasoning task.
specif-ically, given an input causal event pair (cid:104)c, e(cid:105), weretrieve a set of evidence events from the ceg.
theevidence events together with c and e further form.
2355figure 2: illustration of the excar framework and the architecture of cmnln..into a set of causal logical rules, where a rule de-scribes the causal relationship between two events.
formally, a rule ri = (ei1 ⇒ ei2), where ⇒ is alogical connective indicating the causal relation-ship between two events ei1 and ei2.
with regardto these causal logical rules, the causal mechanismcan be revealed and the causal reasoning can beconducted in an explainable way..however, the underlying logic is a probabilis-tic logic.
markov logic network (mln) (pearl,1988) can model such uncertainty by assigningeach causal rule a causal strength, which mea-sures the probability that this rule holds true.
letp (ri) denote the causal strength of rule ri.
mlnestimates p (ri) using a potential function φ(ri).
thereafter, the causality score y is predicted bysimply multiplying the causal strength of obtainedrules:.
p (y ) =.
p (ri) =.
φ(ri),.
(1).
1z.
(cid:89).
i.
1z.
(cid:89).
i.where 1.z is a normalization constant..however, there still remains two challenges forrule-based causal reasoning using mln: 1) mlndeﬁnes potential functions as linear combinationsof some hand-crafted features; 2) mln cannotmodel the inﬂuence of antecedents of rules.
differ-ent from mln, in this paper, we propose a condi-tional markov neural logic network, which workson the embedding space of logic rules to model theconditional causal strength of rules..3 method.
as shown in figure 2, excar consists of twocomponents.
given an event pair (cid:104)c, e(cid:105), excaremploys an evidence retrieval module to retrievesevidence events from a prebuilt causal event graphto generate a set of logical rules.
then excar con-ducts causal reasoning based on the logical rules us-ing a conditional markov neural logic network..3.1 evidence events retrieval.
given an event pair (cid:104)c, e(cid:105) outside the causal eventgraph, to obtain the evidences from the ceg, weﬁrst locate the cause and effect in the ceg.
intu-itively, semantically similar events would have sim-ilar causes and effects, and share similar locationsin the ceg.
to this end, we employ a pretrainedlanguage model elmo (peters et al., 2018) to de-rive the semantic representation for events in theceg, as well as the cause and effect event.
thenevents in the ceg which are semantically similar tothe input cause and effect event can be found usingcosine similarity of the semantic representations.
these events can serve as anchors for locating thecause and effect event.
then as figure 2 shows,taking the anchors of the cause event as start points,and taking the anchors of the effect event as endpoints, the evidence events can be retrieved by abreadth first search (bfs) algorithm..after the retrieving process, the cause, effect andevidence events constitute a causal logical graph(clg) g∗ = {v ∗, r∗}, where v ∗ and r∗ is thenode set and edge set, respectively.
each node eiwithin v ∗ is an event, each edge rj within r∗ de-scribes the causal relationship between two events.
taking g∗ as the input, the following causal reason-ing process is equipped with a set of logical rulesfor revealing the behind causal mechanism..3.2 conditional markov neural logic.
network.
3.2.1 overview.
given the clg, we can derive a set of causallogical rules for supporting the causal reasoningprocess.
however, as figure 1 (d) shows, thecausal strength of a rule may vary with differentantecedents, where the antecedent can be an event,a simple rule or a complex of single rules.
forclarity, we denote the antecedent of a rule ri asantei.
inﬂuenced by a certain antecedent, the.
2356causal strength of a rule can be described by a con-ditional probability p (ri|antei)..as shown in figure 2, a single rule derived fromthe clg can have multiple antecedents, and eachof these antecedents can have its own inﬂuenceon the causal strength of the rule.
to addressthis issue by exploiting the effectiveness of neuralmodels in representation learning, we propose thecmnln that works on the embeddings of logicalrules.
to model the superimposed causal effect ofrules, cmnln regards the clg as a compositionof distinct causal logic chains {ρ1, · · · , ρm}, andpredicts causality score through combining infor-mation of each causal logic chain.
hence, withineach causal logic chain, we can estimate a chain-speciﬁc causal strength for each rule rjk ∈ ρj, us-ing an antecedent-aware potential function.
thencmnln aggregates the intra-chain causation in-formation and inter-chain causation information toderive the causality score..3.2.2 logic chain generation.
1∧, · · · , ∧rjlj.
for supporting the following reasoning process, weﬁrst explore the clg to generate all possible causallogic chains {ρ1, · · · , ρm}.
as shown in figure 2,ρj = {rj} describes a serial of transitivecausal logical rules starting from the cause event cand ending at the effect event e.considering that each rule rjk−1 and ej.
k ∈ ρj is com-posed by two events ejk , a causal logicchain ρj with lj rules contains totally lj + 1 events{ej0 and ejlj are the cause eventc and the effect event e, respectively.
taking cand e as the start and end point respectively, wecan enumerate all distinct causal logic chains in theclg using a depth first searching algorithm..} , where ej.
0, · · · , ejlj.
3.2.3 event encoding.
0 · · · [cls] ej.
a bert-based encoder (devlin et al., 2019) is em-ployed to encode all events within each causal logicchain into chain-speciﬁc distributed embeddings.
speciﬁcally, for a causal logic chain ρj con-taining lj+1 events {ej0, · · · , ej}, we ﬁrst pro-ljcess the event sequence into the form of:k · · · [cls] ej[cls] ejljafter that, the processed event sequence is fedinto bert.
we deﬁne the ﬁnal hidden state of the[cls] token before each event as the representa-tion of the corresponding event.
in this way, weobtain an event embedding set h = {hj},where hjk ∈ rdis the embedding of the kth eventwithin the causal logic chain ρj.
note that, hj0 is.
0, · · · , hjlj.
..the representation of the cause event c, and hjljthe representation of the effect event e..is.
3.2.4 chain-speciﬁc conditional causal.
strength estimation.
logic chains ρj =given one of the causal(rj1∧, · · · , ∧rj) and corresponding event represen-ljtations h = {hj}, cmnln estimates thechain-speciﬁc causal strength for each rule usingan antecedent-aware potential function..0, · · · , hjlj.
for a rule rjk ∈ ρj, we deﬁne the chain-wiseantecedent of rjk as (rjk−1) , and denoteit as antejk, wecan derive the chain-speciﬁc causal strength usingan antecedent-aware potential function as:.
2∧, · · · , ∧rjk. therefore, with regard to antej.
1∧rj.
p (rj.
k|antej.
k) = φa(rj.
k, antej.
k)..(2).
considering that each logical rule rjk−1and ej.
kis composedk, the input of φa(·) is thek, and the embed-k. we denote the representationk, and describe the speciﬁc process for.
of two events ejdistributed representation of antejding of ejof antejderiving sjgiven sj.
k in the following section.
k, hj.
k, to model the inﬂuencek, we ﬁrst derive antecedent-aware repre-.
k−1and ejkas sj.
k−1 and hj.
of antejsentations of ej.
k−1 and ej.
k using an mlp:.
h(cid:48)j.k−1 =tanh(wc[sjh(cid:48)jk =tanh(we[sj.
k||hjk||hj.
k−1] + bc),k] + bc),.
(3).
(4).
k−1 and ej.
resentations h(cid:48)jk−1 and h(cid:48)jtional causal strength of rj.
where ·||· is the concatenate operation, and wc,we ∈ rd×2d are two different weight matrix model-k on ejing the inﬂuence of sjk, respectively.
then based on the antecedent-aware event rep-k, we calculate the condi-k as:k) = σ(h(cid:48)jwhere wcs ∈ rd×d are trainable parameters, and σis a sigmoid function..k−1wcsh(cid:48)j.k, antej.
φa(rj.
k),.
(5).
0 with hj.
antecedent representation along with the es-timation of conditional causal strength, the repre-sentation of antecedents are also recursively up-dated.
speciﬁcally, at the ﬁrst reasoning step, weinitialize sj0. at the kth reasoning step, sjis obtained based on sjk−1, the conditional causalstrength p (rjk), and the embedding of eventswithin rjk :k = tanh(p (rjsj.
k]) + sjwhere wu ∈ rd×2d is a parameter matrix..k)wu[hj.
k|antej.
k|antej.
k−1||hj.
k−1,.
(6).
k.23573.2.5.intra-chain information aggregation.
we aggregate the intra-chain causality informationto derive a distributed representation and a chain-level causal strength for each causal logic chain..we notice that, in the conditional causal strengthestimation process, at the ljth reasoning step,antejlj +1 actually includes all the rules within ρj.
hence, we utilize the representation of antejlj +1 asthe representation of ρj, which we denote as sj..given the chain-speciﬁc conditional causalstrength for each rule within ρj, we can calculatea chain-level causal strength csj for ρj by multi-plying the conditional causal strength of the rules:.
csj =.
p (rj.
k|antej.
k) =.
φa(rj.
k, antej.
k)..(7).
lj(cid:89).
k=1.
lj(cid:89).
k=1.
then we normalize the chain-level causal.
strengths as:.
ality of causality with the transitivity of causalityto generate false rules with more complex patterns(e.g.
: if e1 ⇒ e2 ⇒ e3, then we can induce arf = (e3 ⇒ e1)).
by sampling false rules andtraining the potential functions of these false rulesφa(rf , antef ) to be zero, the reliability of condi-tional causal strength estimation can be enhanced.
with regard to the causal logic driven negativesampling process, the loss function of cmnln isdeﬁned as:.
l = lcausality score + λlconditional cs,.
(11).
where both lcausality score and lconditional cs arecross entropy loss, measuring the difference be-tween the predicted and ground truth causalityscore, and between the predicted and the ideal con-ditional causal strength, respectively; λ is a balancecoefﬁcient..ˆcsj = softmaxj(csj)..(8).
4 experiments.
3.2.6 aggregating chain-level informationfor predicting causality score.
finally, we obtain the disjunction of chain-levelcausality information to predict the causality scorey .
intuitively, a causal logic chain with highercausal strength should have a stronger inﬂuence ony .
therefore, we aggregate the chain-level infor-mation through calculating a linear combination oflogic chain representations {s1 · · · , sm} using thenormalized causal strengths { ˆcs1, · · · , ˆcsm}:.
u = σj ˆcsj · sj(9)where u ∈ r1×d is a ﬁnal state carrying informa-tion from the disjunction of {ρ1, · · · , ρm}..the causality score y is predicted based on u:.
y = softmax(wyu + by),.
(10).
where wy and by are trainable parameters..3.3 training.
in the training process, we introduce a causal logicdriven negative sampling to improve the reliabil-ity of conditional causal strength estimation.
inparticular, if there exists a rule ri = (ei1 ⇒ ei2)within the clg, due to the unidirectionality ofcausality, we can derive a corresponding false rulerf = (ei2 ⇒ ei1).
from the clg, we can also gen-erate a wrong antecedent for the false rule throughrandom sampling.
hence, ideally, the conditionalcausal strength of these false rules should equal0. in addition, we also combine the unidirection-.
4.1 construction of c-copa dataset.
to evaluate the robustness of the excar frame-work, we build an additional chinese common-sense causal reasoning dataset c-copa..the c-copa dataset is built upon a large-scaleweb news corpus sogoucs (wang et al., 2008) byhuman annotation.
we start the annotation processfrom manually extracting causal event pairs fromraw texts within the corpus.
given a causal eventpair, we ﬁrst randomly generate an ask-for indica-tor, where ask-for ∈ [“effect”, “cause”].
then theask-for indicator are used to decide whether thecause or effect event to be the premise or plausiblehypothesis.
given the premise, an implausible ef-fect (cause) events is generated by a human annota-tor.
as a result, the same as the copa dataset, eachinstance within the c-copa consists a premiseevent p, a plausible and an implausible hypothesisevent h+ and h−, and an ask-for indicator a..three chinese volunteers are enlisted for validat-ing the dataset.
agreement between volunteers ishigh (cohen’s k = 0.923).
instances with divergedresults between volunteers are removed from thedataset.
after the annotation process, a total of3,258 instances are left and we randomly split theseinstances into two equal-sized parts as the develop-ment set and the test set, respectively..4.2 construction of causal event graph.
before constructing the ceg, we have to collecta sufﬁcient number of causal event pairs.
to this.
2358end, we harvest english causal event pairs fromthe causalbank corpus (li et al., 2020), whichcontains 314 million commonsense causal eventpairs in total.
while the chinese causal event pairsare collected from a raw web text corpus crawledfrom multiple websites date from 2018 to 2019,and ﬁltered with keywords.
more details could befound in the appendix..then an english and a chinese ceg are buildbased on the corresponding causal event pair cor-pus.
to balance the computation burden and cov-erage of the event graph, we build the english andthe chinese ceg based on 1,500,000 chinese and1,5000,000 english causal event pairs randomlysampled from the whole corpus, respectively..4.3 experimental settings.
given a cause or effect event, we ﬁnd three mosttextually similar events from the causal event graph,and employ them as the anchors.
in the evidence re-trieving process, we limit the maximum searchingdepth of bfs to 3, and restrict the size of evidenceevent set to be no more than 8. we employ thepre-trained bert-base model as the event encoder,which encodes each input event to a 768-dimensionvector.
on both datasets, for each instance, 5 nega-tive rules are sampled to facilitate the estimation ofconditional causal strength.
model is trained withthe balance coefﬁcient λ of 0.1..4.4 baselines.
statistical-based methodsthese methods estimate words or phrase levelcausality from large-scale corpora.
then the causal-ity of an input event pair could be obtained throughsynthesizing the word or phrase level causality..• pmi (jabeen et al., 2014) measures the word-.
level causality using point mutual information..• pmi ex (gordon et al., 2011) is an asymmet-ric word-level pmi which takes the directionalityof causal inference into consideration..• cs (luo et al., 2016) measures word-levelcausality through integrating both the necessitycausality and sufﬁciency causality..• cs mwp (sasaki et al., 2017) measures thecausality between words and prepositional phrasesusing the cs score..pre-trained-model-based methods• bert wang et al.
[2019a] and li et al.
[2019]ﬁnetune bertbase with different hyper parametersto predict the causality of each (cid:104)c, e(cid:105) pair..excar-based methods.
copa c-copa.
methodspmi (jabeen et al., 2014)pmi ex (gordon et al., 2011)cs (luo et al., 2016)cs mwp (sasaki et al., 2017)bert (wang et al., 2019a)bert (li et al., 2019)excar (with cmnln)-w/ mln-w/ ﬁxed-cs-concat.
58.865.470.271.270.473.478.876.375.075.4.
56.262.368.9-72.874.581.578.076.977.1.table 1: accuracy (%) of causal reasoning on the testset of copa and c-copa..we replace the cmnln layer of excar frame-.
work with different reasoning modules and get:.
• excar-w/ mln refers to substitute thecmnln layer by a classical markov logic net-work layer..• excar-w/ ﬁx-cs arbitrarily assign a ﬁxed.
causal strength 0.5 for each logical rule..• excar-concat ﬂattens the causal logical graphinto a single event sequence and takes the eventsequence as input..4.5 quantitative analysis.
we list the results on both the copa dataset andc-copa dataset in table 1. we ﬁnd that:.
(1) statistical-based methods, such as cs (luoet al., 2016) and cs mwp (sasaki et al., 2017)achieve comparable performances with bert-based methods, this is mainly because they har-vest causal knowledge with elaborate patterns fromlarge-scale corpus sized up to 10tb.
trainingbert with such causal knowledge may providepotential space for improvement, which is left forfuture work..(2) compared to causal pair based bert, ex-car related methods show improved performance.
this indicates that incorporating additional evi-dences from the event graph can be helpful forrevealing the causal decision mechanism and thenimprove the accuracy of causal reasoning..(3) excar-w/ mln and excar -w/ cmnlnoutperforms excar-concat, which ﬂats the clginto an event sequence.
this shows that exploitingthe complex causal correlation patterns betweenlogical rules can be helpful for the causal reasoningtask..(4) excar-w/ mln and excar -w/ cmnlnshows improved performance compared to excar-w/ ﬁxed-cs.
this conﬁrms that neuralizing rules toaccount for the uncertainty of the logical rules ishelpful for the causal reasoning task..2359methodsbert (li et al.,2019)excar-w/ cmnln-w/ mln-w/ ﬁxed-cs-concat.
copa61.5 (∆ = −9.9).
c-copa62.7 (∆ = −10.1).
78.2 (∆ = −0.6)76.1 (∆ = −1.8)74.3 (∆ = −0.7)73.9 (∆ = −1.5).
80.7 (∆ = −0.8)76.4 (∆ = −1.6)75.9 (∆ = −1.0)76.0 (∆ = −1.1).
table 2: prediction accuracy (%) after adversary attack..avg.
explainability score.
fixed-cs mln cmnln1.250.95.
1.43.table 3: average explainability score of cmnln, mlnand uniﬁed causal strength on c-copa..(5) excar-w/ cmnln further improves theprediction accuracy compared to excar-w/ mln,suggesting that by incorporating the antecedent-aware potential function cmnln can model theconditional causal strength of logical rules forcausal reasoning..4.6 stability analysis.
in this paper, we propose to enhance the stabilityof our approach through introducing additional ev-idence information.
we investigate the speciﬁcinﬂuence of these evidences on the stability ofour approach through an adversarial evaluation.
following bekoulis et al.
[2018] and yasunagaet al.
[2018], we attack the reasoning systems byadding a perturbation term on the word embeddingof inputs.
the perturbation term is derived using agradient-based method fgm (miyato et al., 2016).
table 2 shows the prediction accuracy after ad-versary attack, and ∆ denotes the change of per-formance brought by adversary attack.
for exam-ple, ∆ = -9.9 means a 9.9% decrease of predictionaccuracy after the adversary attack.
we ﬁnd that,compared with event pair based bert, excar cansigniﬁcantly improve the stability of the predictionaccuracy.
these results show that by incorporatingadditional evidence events, excar could revealthe behind causal mechanism to increase the stabil-ity of prediction results..4.7 human evaluation for explainability.
we analyze the explainability of our approach quan-titatively through human evaluations.
in particu-lar, we randomly sample 200 instances from thetest set of c-copa and make prediction using ex-car.
then we employ three experts to give anexplainability score belonging to {0, 1, 2} to evalu-ate whether the causality strengths derived by our.
figure 3: example of causal reasoning result made byexcar..approach are reasonable, where 0 stands for un-explainable, 1 stands for moderately explainableand 2 stands for explainable.
for comparison, wefurther introduce two baselines: (1) markov logicnetwork (mln); (2) fixed-cs..the average explainability scores are shown intable 3, from which we can observe that: (1) theaverage explainability scores of cmnln and mlnare higher than that of ﬁxed-cs.
this is because,through neuralizing the logical rules and equippingthe logical rules with probability, cmnln andmln can better model the potential noise in theretrieved evidences, as well as the uncertainty ofrules.
(2) the explainability score of cmnln isfurther higher than that of mln.
this indicates that,cmnln can model the conditional causal strengthof logical rules using the antecedent-aware poten-tial function, and then increase the reasonability ofcausal strength estimation..4.8 case study.
figure 3 provides an example of causal reasoningmade by excar on c-copa.
given a cause eventreduction of grain production, e: rise of inﬂationrate is more likely to be the effect of the cause.
however, it is difﬁcult to directly infer the effect e:rise of inﬂation rate directly from the cause eventc:reduction of grain production.
correspondingly,given c and e, excar can obtain evidence eventssuch as i1: food prices increase and i2: grainprices out of control from the causal event graph.
these results show that excar can obtain relevantevidences and hence choose the correct effect eventin an explainable manner..we also examined the estimated causal strengths.
as shown in figure 3, the causal strength betweeni1 and e is higher in the logic chain ρ2 comparedto ρ1.
intuitively, with the additional antecedenti2: grain prices out of control, i1: food pricesincrease could be more likely to lead to e: rise ofinﬂation rate.
these results indicate that cmnlncan model the conditional causal strength of rules..2360figure 4: reasoning accuracy of excar with differentnumber of evidence events on the test set of c-copa..4.9 effect of the number of evidence events.
we compare the reasoning accuracy of excar onsamples with different numbers of evidence events.
experiments are conducted on the test set of c-copa.
results are shown in figure 4. we can ﬁndthat, when the evidence events number increasesfrom 0 to 3, the reasoning accuracy increases ingeneral, since sufﬁcient evidences are helpful forthe reasoning task.
however, the accuracy starts todecrease when evidence number exceeds 4. this in-dicates that noisy evidence events may be obtained.
the inclusion of noisy evidence events emphasisthe necessity of neutralizing the logical rules, as thesymbolic logic based systems cannot accommodatefor the noise in the rules..5 related work.
5.1 causal reasoning.
causal reasoning remains a challenging problemfor today’s ai systems.
statistical-based methodscan provide strong baselines, as they can ﬁnd someuseful cues from large-scale causal corpus.
for ex-ample, gordon et al.
(2011) measured the causalitybetween words using pmi, and estimated the pmibased on a personal story corpus.
while luo et al.
(2016) and sasaki et al.
(2017) further introduceddirection information into a causal strength index.
then through synthesizing the word-level causality,the causality between events could be inferred..compared to statistical-based methods, deepneural networks enable models to learn the causal-ity between events considering the semantics ofevents.
to this end, xie and mu (2019b) devisedattention-based models to capture the word-levelcausal relationships.
while wang et al.
and li et al.
(2019) ﬁnetuned the pretrained language modelbert on causal event pairs corpus to learn thepairwise causality knowledge between events..in this paper, we argue that in addition to theevent pair itself, causal reasoning also needs toinvolve more evidence information.
to addressthis issue, we propose a novel inference frameworkexcar, which is able to incorporate the additional.
evidence events from an event graph for supportingthe causal reasoning task..5.2 explainable textual inference.
explainability has been a long-pursued goal fortextual inference systems, as it can help to unveilthe decision making mechanism of black-box mod-els and enhance the stability of reasoning, whichcan be crucial for applications in various domains,such as medical and ﬁnancial domains.
to intro-duce interpretability in textural inference process,previous studies can be mainly divided into twocategories: generating explainable information anddevising self-explaining mechanism..beyond the task related information, automatedgenerated textual explanations are helpful for justi-fying the reliability of models.
for example, cam-buru et al.
(2018) and nie et al.
(2019) train mul-titask learning models to learn to generate expla-nations for textual entailment inference.
on theother hand, the incorporation of relevant externalknowledge can not only increase the model perfor-mance compared to purely data-driven approaches,but also can be helpful for understanding the modelbehavior (niu et al., 2019; wang et al., 2019b)..another line of work designs self-explainingmodels to reveal the reasoning process of mod-els.
attention mechanism was devised to explicitlymeasure the relative importance of input textualfeatures.
hence, it has been widely employed toenhance the interpretability of deep neural models.
in this paper, to conduct causal reasoning in anexplainable manner, we propose to induce a set oflogic rules from a pre-built causal event graph, andexplicitly model the conditional causal strength ofeach logical rule.
the probabilistic logical rulescan provide clues to explain the prediction results..6 conclusion.
we devise a novel explainable causal reasoningframework excar.
given an event pair, excaris able to obtain logical rules from a large-scalecausal event graph to provide insight to inferenceresults.
to learn the conditional probabilistic of log-ical rules, we propose a conditional markov neurallogic network that combines the strengths of rule-based and neural models.
empirically, our methodoutperforms prior work on two causal reasoningdatasets, including copa and c-copa.
further-more, excar is interpretable by providing expla-nations in terms of probabilistic logical rules..23617 acknowledgments.
we thank the anonymous reviewers for their con-structive comments, and gratefully acknowledgethe support of the technological innovation “2030megaproject” - new generation artiﬁcial intel-ligence of china (2018aaa0101901), and thenational natural science foundation of china(61976073)..references.
giannis bekoulis, johannes deleu, thomas demeester,and chris develder.
2018. adversarial training formulti-context joint entity and relation extraction.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2830–2836..oana-maria camburu, tim rockt¨aschel, thomaslukasiewicz, and phil blunsom.
2018. e-snli: natu-ral language inference with natural language explana-tions.
in advances in neural information processingsystems, pages 9539–9549..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naccl 2019, pages 4171–4186..andrew s gordon, bejan, cosmin a , and kenji sagae.
2011. commonsense causal reasoning using millionsof personal stories.
in ijcai..suchin gururangan, swabha swayamdipta, omer levy,roy schwartz, samuel r bowman, and noah asmith.
2018. annotation artifacts in natural languageinference data.
arxiv preprint arxiv:1803.02324..ned hall.
2000. causation and the price of transitivity..the journal of philosophy, 97(4):198–222..shahida jabeen, xiaoying gao, and peter andreae.
2014. using asymmetric associations for common-sense causality detection.
in pricai..zhongyang li, , xiao , ding, and ting liu.
2018. con-structing narrative event evolutionary graph for scriptevent prediction.
in proceedings of the 27th inter-national joint conference on artiﬁcial intelligence,pages 4201–4207..zhongyang li, tongfei chen,.
,van durme.
2019. learning to rank for plausibleplausibility.
arxiv preprint arxiv:1906.02079.., and benjamin.
zhongyang li, xiao ding, ting liu, j edward hu, andbenjamin van durme.
2020. guided generation ofcause and effect..zhiyi luo, yuchen sha, kenny q zhu, seung-wonhwang, and zhongyuan wang.
2016. commonsensecausal reasoning between short texts.
in fifteenthinternational conference on the principles of knowl-edge representation and reasoning..r thomas mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntac-tic heuristics in natural language inference.
arxivpreprint arxiv:1902.01007..takeru miyato, andrew m dai, and ian goodfel-training methods forlow.
2016.semi-supervised text classiﬁcation.
arxiv preprintarxiv:1605.07725..adversarial.
yixin nie, adina williams, emily dinan, mohit bansal,jason weston, and douwe kiela.
2019. adversarialnli: a new benchmark for natural language under-standing.
arxiv preprint arxiv:1910.14599..zheng-yu niu, hua wu, haifeng wang, et al.
2019.knowledge aware conversation generation with ex-plainable reasoning over augmented graphs.
in pro-ceedings of the 2019 conference on empirical meth-ods in natural language processing and the 9th in-ternational joint conference on natural languageprocessing (emnlp-ijcnlp), pages 1782–1792..jong-hoon oh, kentaro torisawa, chikara hashimoto,motoki sano, stijn de saeger, and kiyonori ohtake.
2013. why-question answering using intra-and inter-sentential causal relations.
in proceedings of the 51stannual meeting of the association for computationallinguistics (volume 1: long papers), pages 1733–1743..judea pearl.
1988. probabilistic reasoning in intelligentsystems; networks of plausible inference.
technicalreport..judea pearl.
2001. direct and indirect effects.
in pro-ceedings of the seventeenth conference on uncer-tainty in artiﬁcial intelligence, pages 411–420..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of naacl-hlt, pages2227–2237..adam poliak, aparajita haldar, rachel rudinger, j ed-ward hu, ellie pavlick, aaron steven white, and ben-jamin van durme.
2018. collecting diverse naturallanguage inference problems for sentence representa-tion evaluation.
arxiv preprint arxiv:1804.08207..shota sasaki, sho takase, naoya inoue, naoakiokazaki, and kentaro inui.
2017. handling mul-tiword expressions in causality estimation.
in iwcs2017..yawei sun, cheng, gong , and yuzhong qu.
2018.reading comprehension with graph-based temporal-casual reasoning.
in proceedings of the 27th inter-national conference on computational linguistics,pages 806–817..alex wang, yada pruksachatkun, nikita nangia, aman-preet singh, julian michael, felix hill, omer levy,and samuel bowman.
2019a.
superglue: a stickier.
2362benchmark for general-purpose language understand-in advances in neural informationing systems.
processing systems, pages 3266–3280..canhui wang, min zhang, shaoping ma, and liyunru.
2008. automatic online news issue constructionin proceedings of the 17thin web environment.
international conference on world wide web, pages457–466..xiang wang, dingxian wang, canran xu, xiangnan he,yixin cao, and tat-seng chua.
2019b.
explainablereasoning over knowledge graphs for recommenda-in proceedings of the aaai conference ontion.
artiﬁcial intelligence, volume 33, pages 5329–5336..zhipeng xie and feiteng mu.
2019a.
boosting causalembeddings via potential verb-mediated causal pat-terns.
in ijcai, pages 1921–1927.
i press..zhipeng xie and feiteng mu.
2019b.
distributed rep-resentation of words in cause and effect spaces.
inijcai, volume 33, pages 7330–7337..michihiro yasunaga, jungo kasai, and dragomir radev.
2018. robust multilingual part-of-speech tagging viaadversarial training.
in naacl..2363