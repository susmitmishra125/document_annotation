coreference reasoning in machine reading comprehension.
mingzhu wu1, naﬁse sadat moosavi1, dan roth2, iryna gurevych1.
1ukp lab, technische universitat darmstadt2department of computer and information science, upenn1https://www.ukp.tu-darmstadt.de2 https://www.seas.upenn.edu/˜danroth/.
abstract.
coreference resolution is essential for natu-ral language understanding and has been longstudied in nlp.
in recent years, as the for-mat of question answering (qa) became astandard for machine reading comprehension(mrc), there have been data collection efforts,e.g., dasigi et al.
(2019), that attempt to evalu-ate the ability of mrc models to reason aboutcoreference.
however, as we show, coref-erence reasoning in mrc is a greater chal-lenge than earlier thought; mrc datasets donot reﬂect the natural distribution and, conse-quently, the challenges of coreference reason-ing.
speciﬁcally, success on these datasetsdoes not reﬂect a model’s proﬁciency in coref-erence reasoning.
we propose a methodol-ogy for creating mrc datasets that better re-ﬂect the challenges of coreference reasoningand use it to create a sample evaluation set.
the results on our dataset show that state-of-the-art models still struggle with these phe-nomena.
furthermore, we develop an effec-tive way to use naturally occurring corefer-ence phenomena from existing coreference res-olution datasets when training mrc models.
this allows us to show an improvement in thecoreference reasoning abilities of state-of-the-art models.1.
1.introduction.
machine reading comprehension is the ability toread and understand the given passages and answerquestions about them.
coreference resolution isthe task of ﬁnding different expressions that referto the same real-world entity.
the tasks of corefer-ence resolution and machine reading comprehen-sion have moved closer to each other.
convertingcoreference-related datasets into an mrc format.
1the.
codeat.
and the.
resulting dataset.
avail-https://github.com/ukplab/.
are.
ablecoref-reasoning-in-qa..improves the performance on some coreference-related datasets (wu et al., 2020b; aralikatte et al.,2019).
there are also various datasets for the taskof reading comprehension on which the modelrequires to perform coreference reasoning to an-swer some of the questions, e.g., drop (duaet al., 2019), duorc (saha et al., 2018), multirc(khashabi et al., 2018), etc..quoref (dasigi et al., 2019) is a dataset that isparticularly designed for evaluating coreferenceunderstanding of mrc models.
figure 1 shows aqa sample from quoref in which the model needsto resolve the coreference relation between “his”and “john motteux” to answer the question..figure 1: a sample from the quoref dataset..recent.
large pre-trained language modelsreached high performance on quoref.
however,our results and analyses suggest that this datasetcontains artifacts and does not reﬂect the naturaldistribution and, therefore, the challenges of coref-erence reasoning.
as a result, high performanceson quoref do not necessarily reﬂect the coreferencereasoning capabilities of the examined models andanswering questions that require coreference rea-soning might be a greater challenge than currentscores suggest..in this paper, we propose two solutions to ad-dress this issue.
first, we propose a methodologyfor creating mrc datasets that better reﬂect thecoreference reasoning challenge.
we release a sam-ple challenging evaluation set containing 200 exam-ples by asking an annotator to create new question-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5768–5781august1–6,2021.©2021associationforcomputationallinguistics5768answer pairs using our methodology and based onexisting passages in quoref.
we show that thisdataset contains fewer annotation artifacts, and itsdistribution of biases is closer to a coreference reso-lution dataset.
the performance of state-of-the-artmodels on quoref considerably drops on our evalu-ation set suggesting that (1) coreference reasoningis still an open problem for mrc models, and (2)our methodology opens a promising direction tocreate future challenging mrc datasets..second, we propose to directly use coreferenceresolution datasets for training mrc models toimprove their coreference reasoning.
we automati-cally create a question whose answer is a corefer-ring expression m1 using the bart model (lewiset al., 2020).
we then consider this question, m1’santecedent, and the corresponding document as anew (question, answer, context) tuple.
this datahelps the model learning to resolve the coreferencerelation between m1 and its antecedent to answerthe question.
we show that incorporating theseadditional data improves the performance of thestate-of-the-art models on our new evaluation set..our main contributions are as follows:.
• we show that quoref does not reﬂect the natu-ral challenges of coreference reasoning and pro-pose a methodology for creating mrc datasetsthat better reﬂect this challenge..• we release a sample challenging dataset thatis manually created by an annotator using ourmethodology.
the results of state-of-the-artmrc models on our evaluation set show that,despite the high performance of mrc modelson quoref, answering questions based on coref-erence reasoning is still an open challenge.
• we propose an approach to use existing coref-erence resolution datasets for training mrcmodels.
we show that, while coreference reso-lution and mrc datasets are independent andbelong to different domains, our approach im-proves the coreference reasoning of state-of-the-art mrc models..2 related work.
2.1 artifacts in nlp datasets.
one of the known drawbacks of many nlp datasetsis that they contain artifacts.2 models tend to ex-.
2i.e., the conditional distribution of the target label basedon speciﬁc attributes of the training domain diverges whiletesting on other domains..ploit these easy-to-learn patterns in the early stagesof training (arpit et al., 2017; liu et al., 2020;utama et al., 2020b), and therefore, they may notfocus on learning harder patterns of the data thatare useful for solving the underlying task.
as aresult, overﬁtting to dataset-speciﬁc artifacts limitsthe robustness and generalization of nlp models.
there are two general approaches to tackle suchartifacts: (1) adversarial ﬁltering of biased exam-ples, i.e., examples that contain artifacts, and (2) de-biasing methods.
in the ﬁrst approach, potentiallybiased examples are discarded from the dataset, ei-ther after the dataset creation (zellers et al., 2018;yang et al., 2018a; le bras et al., 2020; bartoloet al., 2020), or while creating the dataset (duaet al., 2019; chen et al., 2019; nie et al., 2020)..in the second approach, they ﬁrst recognize ex-amples that contain artifacts, and use this knowl-edge in the training objective to either skip or down-weight biased examples (he et al., 2019; clarket al., 2019a), or to regularize the conﬁdence ofthe model on those examples (utama et al., 2020a).
the use of this information in the training objectiveimproves the robustness of the model on adversarialdatasets (he et al., 2019; clark et al., 2019a; utamaet al., 2020a), i.e., datasets that contain counterex-amples in which relying on the bias results in an in-correct prediction.
in addition, it can also improvein-domain performances as well as generalizationacross various datasets that represent the same task(wu et al., 2020a; utama et al., 2020b)..while there is an emerging trend of includingadversarial models in data collection, their effec-tiveness is not yet compared with using debias-ing methods, e.g., whether they are still beneﬁcialwhen we use debiasing methods or vice versa..2.2.joint qa and coreference reasoning.
there are a few studies on the joint understand-ing of coreference relations and reading compre-hension.
wu et al.
(2020b) propose to formulatecoreference resolution as a span-prediction task bygenerating a query for each mention using the sur-rounding context, thus converting coreference reso-lution to a reading comprehension problem.
theyleverage the plethora of existing mrc datasets fordata augmentation and improve the generalizationof the coreference model.
in parallel to wu et al.
(2020b), aralikatte et al.
(2019) also cast ellipsisand coreference resolution as reading comprehen-sion tasks.
they leverage the existing neural archi-.
5769tectures designed for mrc for ellipsis resolutionand outperform the previous best results.
in a sim-ilar direction, hou (2020) propose to cast bridg-ing anaphora resolution as question answering andpresent a question answering framework for thistask.
however, none of the above works investigatethe impact of using coreference data on qa..dua et al.
(2020) use amazon mechanical turk-ers to annotate the corresponding coreferencechains of the answers in the passages of quoreffor 2,000 qa pairs.
they then use this additionalcoreference annotation for training a model onquoref.
they show that including these additionalcoreference annotations improves the overall per-formance on quoref.
the proposed method by duaet al.
(2020) requires annotating additional corefer-ence relations on every new coreference-aware qadataset.
contrary to this, our approach uses exist-ing coreference resolution datasets, and therefore,applies to any new qa dataset without introducingany additional cost..3 how well quoref presents coreference.
reasoning?.
for creating the quoref dataset, annotators ﬁrstidentify coreferring expressions and then ask ques-tions that connect the two coreferring expressions.
dasigi et al.
(2019) use a bert-base model (de-vlin et al., 2019) that is ﬁne-tuned on the squaddataset (rajpurkar et al., 2016) as an adversarialmodel to exclude qa samples that the adversarialmodel can already answer.
the goal of using thisadversarial model is to avoid including question-answer pairs that can be solved using surface cues.
they claim that most examples in quoref cannotbe answered without coreference reasoning..if we ﬁne-tune a roberta-large model onquoref, it achieves 78 f1 score while the estimatedhuman performance is around 93 f1 score (dasigiet al., 2019).
this high performance, given thatroberta can only predict continuous span an-swers while quoref also contains discontinuousanswers, indicates that either (1) quoref presentscoreference-aware qa very well so that the modelcan properly learn coreference reasoning fromthe training data, (2) pretrained transformer-basedmodels have already learned coreference reason-ing during their pre-training, e.g., as suggested bytenney et al.
(2019) and clark et al.
(2019b), or (3)coreference reasoning is not necessarily requiredfor solving most examples..in this section, we investigate whether quorefcontains the known artifacts of qa datasets, andtherefore, models can solve some of the qa pairswithout performing coreference reasoning.
figure2 shows such an example where simple lexical cuesare enough to answer the question despite the factthat coreference expressions “frankie” and “his”were included in the corresponding context..figure 2: a qa example that relies on simple lexicaloverlap without requiring coreference reasoning..we investigate ﬁve artifacts (biases) as follows:.
• random named entity: the majority of answersin quoref are person names.
to evaluate thisartifact, we randomly select a person namedentity from the context as the answer.3.
• wh-word (weissenborn et al., 2017): to rec-ognize the qa pairs that can be answered byonly using the interrogative adverbs from thequestion, we train a model on a variation of thetraining dataset in which questions only containinterrogative adverbs..• empty question (sugawara et al., 2020):.
torecognize qa pairs that are answerable withoutconsidering the question,4 we train a qa modelonly on the contexts and without questions.
• semantic overlap (jia and liang, 2017): forthis artifact, we report the ratio of the qa pairswhose answers lie in the sentence of the con-text that has the highest semantic similarity tothe question.
we use sentence-bert (reimersand gurevych, 2019) to ﬁnd the most similarsentence..• short distance reasoning: for this bias, we traina model only using the sentence of the con-text that is the most similar to the question,instead of the whole context.
we exclude thequestion-answer pairs in which the most simi-lar sentence does not contain the answer.
thismodel will not learn to perform coreference rea-soning when the related coreferring pairs arenot in the same sentence..3we use spacy (honnibal and johnson, 2015) for ner.
4e.g., this can indicate the bias of the model to select the.
most frequent named entity in the context as the answer..5770for wh-word, empty question, and short distancereasoning, we use the tase model (segal et al.,2020) to learn the bias.
biased examples are thenthose that can be correctly solved by these models.
we only change the training data for biased exam-ple detection, if necessary, and the developmentset is unchanged.
the quoref column in table 1reports the proportion of biased examples in thequoref development set..bias.
quoref conllbart.
random named entitywh-wordempty questionsemantic overlapshort-distance reasoning.
9.3922.9921.5128.6650.70.
1.5213.1211.6021.389.86.table 1: the proportion of examples in the quoref de-velopment set and conll-2012 coreference resolutiondataset that contain each of the examined biases..we also investigate whether these biases havesimilar ratios in a coreference resolution dataset.
we use the conll-2012 coreference resolutiondataset (pradhan et al., 2012a) and convert it toa reading comprehension format, i.e., conllbartin section 5.5 this data contains question-answerpairs in which the question is created based on acoreferring expression in conll-2012, and theanswer is its closest antecedent.
we split this datainto training and test sets and train bias modelson the training split.
the conllbart column intable 1 shows the bias proportions on this data..as we see, the short distance reasoning is themost prominent bias in the quoref dataset.
how-ever, the ratio of such biased examples is onlyaround 10% in conll-2012.
therefore, apartfrom the examples that can be solved without coref-erence reasoning,6 the difﬁculty of the requiredcoreference reasoning in the remaining examplesis also not comparable with naturally occurringcoreference relations in a coreference resolutiondataset..as a result, high performance on quoref doesnot necessarily indicate that the model is adept atperforming coreference reasoning..5we report the bias ratios of conlldec in section 5 in the.
appendix..
6e.g., about 20% of examples can be answered without.
considering the question..4 creating an mrc dataset that better.
reﬂects coreference reasoning.
there is a growing trend in using adversarial mod-els for data creation to make the dataset more chal-lenging or discard examples that can be solvedusing surface cues (bartolo et al., 2020; nie et al.,2020; yang et al., 2018a; zellers et al., 2018; yanget al., 2018b; dua et al., 2019; chen et al., 2019;dasigi et al., 2019)..quoref is also created using an adversarial datacollection method to discard examples that can besolved using simple lexical cues.
the assumption isthat it is hard to avoid simple lexical cues by whichthe model can answer questions without corefer-ence reasoning.
therefore, an adversarial model(a) is used to discard examples that contain suchlexical cues.
while this adversarial ﬁltering re-moves examples that are easy to solve by a, itdoes not ensure that the remaining examples donot contain shortcuts that are not explored by a.first, the adversarial model in quoref is trained onanother dataset, i.e., squad.
thus, the failure ofa on quoref examples may be due to (1) quorefhaving different lexical cues than those in squad,or (2) domain shift.
second, and more importantly,as argued by dunietz et al.
(2020), making the taskchallenging by focusing on examples that are moredifﬁcult for existing models is not a solution formore useful reading comprehension.7.
we instead propose a methodology for creating.
question-answer pairs as follows:.
• annotators should create a question that con-nects the referring expression m1 to its an-tecedent m2 so that (1) m2 is more informa-tive than m1,8 and (2) m1 and m2 reside in adifferent sentence..• candidate passages for creating qa pairs areselected according to their number of namedentities and pronouns.
the number of distinctnamed entities is an indicator of the number ofentities in the text.
therefore, there would bemore candidate entities for resolving referringexpressions.
the number of pronouns indicatesthat we have enough candidate m1s that havemore informative antecedents..we provide this guideline to a student from the.
7as put by them: “the dominant mrc research paradigmis like trying to become a professional sprinter by glancingaround the gym and adopting any exercises that look hard”..8proper names are more informative than common nouns,and they are more informative than pronouns (lee et al., 2013)..5771context snippet.
question.
gold answer.
”diamonds” was certiﬁed sextuple platinum by therecording industry association of america (riaa).
incanada, the song debuted at number nine on the cana-dian hot 100 for the issue dated october 13, 2012 [...] itremained atop of it for four consecutive weeks [...].
the ever-winding path of john frusciante’s solo careeris a confusing one to say the least [...] the album of thesame name is frusciante’s ﬁrst experimenting with theacid house genre.
he previously released an ep, sect insgt under this alias in 2012..what is the full name of the chart ofwhich diamonds remained atop for fourconsecutive weeks?.
canadian hot 10.who did release an ep called sect insgt?.
john frusciante.
table 2: examples from our dataset.
the context is cropped to only show the relevant parts..computer science department for generating newqa pairs from the existing passages in the quorefdevelopment set.
we use quoref passages to ensurethat the source of performance differences on ourdataset vs. quoref is not due to domain differences.
this results in 200 new qa pairs.
table 2 presentsexamples from our dataset..table 3 shows the results of the examined bi-ases on our dataset.
by comparing table 3 andtable 1, we observe that the examined biases areless strong in our dataset, and their distribution iscloser to those in conll-2012.
as we will see intable 5, the performance of state-of-the-art modelson quoref drops more than 10 points, i.e., 13-18points, on our challenge dataset.9.
bias.
random named entitywh-wordempty questionsemantic overlapshort-distance reasoning.
ours.
3.0313.6411.6224.5035.35.table 3: proportion of biased examples in our dataset..5.improving coreference reasoning.
while we do not have access to many coreferenceannotations for the task of coreference-aware mrc,there are various datasets for the task of coreferenceresolution.
coreference resolution datasets containthe annotation of expressions that refer to the sameentity.
in this paper, we hypothesize that we candirectly use coreference resolution corpora to im-prove the coreference reasoning of mrc models.
we propose an effective approach to convert coref-erence annotations into qa pairs so that modelslearn to perform coreference resolution by answer-ing those questions.
in our experiments, we use the.
9we examine 50 randomly selected examples from our.
challenge set, and they were all answerable by a human..conll-2012 dataset (pradhan et al., 2012b) thatis the largest annotated dataset with coreferenceinformation..5.1 coreference-to-qa conversion.
the existing approach to convert coreference an-notations into (question, context, answer) tuples,which is used to improve coreference resolutionperformance (wu et al., 2020b; aralikatte et al.,2019), is to use the sentence of the anaphor asa declarative query, and its closest antecedent asthe answer.
the format of these queries is notcompatible with questions in mrc datasets, andtherefore, the impact of this data on mrc modelsmay be limited.
in this work, we instead generatequestions from those declarative queries using anautomatic question generation model.
we use thebart model (lewis et al., 2020) that is one of thestate-of-the-art text generation models.
below weexplain the details of each of these two approachesfor creating qa data from conll-2012.
table 4shows examples from both approaches..conlldec: wu et al.
(2020b) and aralikatteet al.
(2019) choose a sentence that contains ananaphor as a declarative query, the closest non-pronominal antecedent of that anaphor as the an-swer, and the corresponding document of the ex-pressions as the context.10 we remove the tuplesin which the anaphor and its antecedent are identi-cal.
the reason is that (1) quoref already containsmany examples in which the coreference relationis between two mentions with the same string, and(2) even after removing such examples, conlldeccontains around four times more qa pairs than thequoref training data..conllbart: we use a ﬁne-tuned bart model(lewis et al., 2020) released by durmus et al..10we use the code provided by aralikatte et al.
(2019)..5772passage in conll.
mention cluster.
conlldec quesion.
conllbart question.
gold answer.
my mother was thelma wahl [...] shewas a very good mother.
she was athuntingdon because she needed care[...].
the angel also held a large chain in hishand [...] the angel tied the dragon withthe chain for 1000 years..[my mother, she, she, she].
she was at hunting-don because <ref> she</ref> needed care..who was at hunting-don because she neededcare?.
my mother.
[a large chain, the chain].
a large chain.
angel.
thetied thedragon with <ref> thechain </ref> for 1000years..what did the angel tiethe dragon with for1000 years?.
table 4: coreference-to-qa conversion examples using conlldec and conllbart approaches..(2020) for question generation and apply it onthe declarative queries in conlldec.
the bartmodel speciﬁes potential answers by masking nounphrases or named entities in the query and thengenerates questions for each masked text span.
weonly keep questions whose answer, i.e., the maskedexpression, is a coreferring expression and replacethat answer with its closest non-pronominal an-tecedent.
we only keep questions in which themasked expression and its antecedent are not iden-tical.
such qa pairs enforce the model to resolvethe coreference relation between the two corefer-ring expressions to answer generated questions..5.2 experimental setups.
we use two recent models from the quoref leader-board: roberta (liu et al., 2019) and tase (se-gal et al., 2020), from which tase has the state-of-the-art results.
we use roberta-large from hug-gingface (wolf et al., 2020).
tase casts mrc as asequence tagging problem to handle questions withmulti-span answers.
it assigns a tag to every tokenof the context indicating whether the token is a partof the answer.
we use the taseio+sse setup thatis a combination of their multi-span architectureand single-span extraction with io tagging.we usethe same conﬁguration and hyper-parameters fortaseio+sse as described in segal et al.
(2020).
we train all models for two epochs in all experi-ments.11 we use the f1 score that calculates thenumber of shared words between predictions andgold answers for evaluation..training strategies.
to include the additionaltraining data that we create from conll-2012using coreference-to-conll conversion methods,we use two different strategies:.
• joint: we concatenate the training examplesfrom quoref and conll-to-qa converted.
datasets.
therefore, the model is jointly trainedon the examples from both datasets..• transfer: since the conll-to-qa data is auto-matically created and is noisy, we also examinea sequential ﬁne-tuning setting in which weﬁrst train the model on the conll-to-qa con-verted data, and then ﬁne-tune it on quoref..5.3 data.
we evaluate all the models on four different qadatasets..• quoref : the ofﬁcial development and test setsof quoref, i.e., quorefdev and quoreftest, re-spectively..• our challenge set: our new evaluation set de-.
scribed in section 4..• contrast set: the evaluation set by gardneret al.
(2020) that is created based on the of-ﬁcial quoref test set.
for creating this eval-uation set, the authors manually performedsmall but meaningful perturbations to the testexamples in a way that it changes the goldlabel.
this dataset is constructed to evaluatewhether models decision boundaries align totrue decision boundaries when they are mea-sured around the same point..• multirc: multi-sentence reading compre-hension set (khashabi et al., 2018) is createdin a way that answering questions requires amore complex understanding from multiplesentences.
therefore, coreference reasoningcan be one of the sources for improving theperformance on this dataset.
note that mul-tirc is from a different domain than the restof evaluation sets.12.
11the only difference of tase in our experiments and thereported results in segal et al.
(2020) is the number of trainingepochs.
for a fair comparison, we train all models for thesame number of iterations..12to use the multirc development set, which is in a multi-choice answer selection format, we convert it to a readingcomprehension format by removing qa pairs whose answerscannot be extracted from the context..5773model.
training setup.
quorefdev quoreftest ours contrast set multirc.
tase.
roberta.
baselineconllbartjoint-conlldectransfer-conlldecjoint-conllbarttransfer-conllbarttransfer-squad.
baselineconllbartjoint-conlldectransfer-conlldecjoint-conllbarttransfer-conllbarttransfer-squad.
84.0534.9584.3685.0084.3085.1384.70.
79.6428.8275.1574.1078.7078.2280.18.
84.7135.7685.1485.8885.9385.9887.02.
79.6929.1074.8373.6579.5978.3379.82.
66.4839.5565.9273.0769.3773.0167.99.
64.3529.0056.9460.0967.0766.6264.88.
73.4426.2474.8875.6974.0077.4078.28.
69.9517.3657.7858.9566.7866.5869.46.
51.8326.5144.7150.1848.2651.9653.51.
37.1214.8129.9730.9335.4336.8438.26.table 5: impact of incorporating coreference data in mrc using conlldec and conllbart conversion methodson roberta-large and tase models.
the baseline and conllbart rows show the results when models are trainedon the quoref training data and the conllbart data, respectively.
joint refers to the setting in which the model isjointly trained on quoref and the converted conll data.
transfer refers to the setting in which the model is ﬁrsttrained on the converted conll data and ﬁne-tuned on quoref.
transfer-squad shows the impact of trainingthe model on additional qa data from a similar domain.
results are reported based on f1 scores.
the highest f1scores for each model are boldfaced and scores lower than the baseline are marked in gray..the contrast set and multirc datasets are notdesigned to explicitly evaluate coreference reason-ing.
however, we include them among our evalua-tion sets to have a broader view about the impactof using our coreference data in qa..training.
examples.
test.
examples.
quoref trainconlldecconllbartsquad.
19399 quoref dev89403 ours1890686588 multirc.
contrast set.
2418200700389.table 6: number of examples in each dataset..table 6 reports the statistics of these qa datasets.
in addition, it reports the number of examples inconlldec and conllbart datasets that we cre-ate by converting the conll-2012 training datainto qa examples.
since the question generationmodel cannot generate a standard question for ev-ery declarative sentence, conllbart contains asmaller number of examples.
we also include thestatistics of squad in table 6 as we use it forinvestigating whether the resulting performancechanges are due to using more training data or us-ing coreference-aware additional data..the language of all the datasets is english..5.4 results.
table 5 presents the results of evaluating the im-pact of using coreference annotations to improvecoreference reasoning in mrc.
we report the re-.
sults for both of the examined state-of-the-art mod-els, i.e., tase and roberta-large, using bothtraining settings: (1) training the model jointly onquoref and conll-to-qa converted data (joint),and (2) pre-training the model on conll-to-qadata ﬁrst and ﬁne-tuning it on quoref (transfer).
baseline represents the results of the examinedmodels that are only trained on quoref.
conllbartrepresents the results of the models that are onlytrained on the conllbart data.
transfer-squadreports the results of the sequential training whenthe model is ﬁrst trained on the squad trainingdataset (rajpurkar et al., 2016) and is then ﬁne-tuned on quoref..based on the results of table 5, we make the.
following observations..first, the most successful setting for improvingcoreference reasoning, i.e., improving the perfor-mance on our challenge evaluation set, is transfer-conllbart.
pre-training the tase model onconllbart improves its performance on all of theexamined evaluation sets.
however, it only im-proves the performance of roberta on our chal-lenge set..second, squad contains well-formed qa pairswhile conllbart and conlldec contain noisy qa.
also, squad and quoref are both created basedon wikipedia articles, and therefore, have similardomains.
however, the genres of the documents inconll-2012 include newswire, broadcast news,broadcast conversations, telephone conversations,.
5774model.
semantic overlap ¬semantic overlap.
short reasoning ¬short reasoning.
tase baselinejoint-conlldecjoint-conllbarttransfer-conlldectransfer-conllbarttransfer-squad.
roberta baselinejoint-conlldecjoint-conllbarttransfer-conlldectransfer-conllbarttransfer-squad.
dev.
81.69+2.07+0.86+1.29+1.74+0.84.
78.09-5.550.00-4.20-1.02+1.32.
ours.
77.2-5.80-3.00+8.56+1.23+0.58.
67.39-10.04+1.94+1.79-0.55-1.08.dev.
84.86-0.30+0.03+0.83+0.85+1.19.
80.19-4.15-1.28-6.02-1.58+0.25.
ours.
62.96+1.19+4.82+5.94+8.26+0.10.
63.36-6.55+2.97-6.27+3.18+1.05.
dev.
94.84+0.94+0.64+1.07+1.54-0.91.
90.04-2.53-0.48-3.52-0.95+0.45.
ours.
89.04-1.65+1.20+8.82+4.70+2.3.
84.23-7.32-1.36-7.00-5.06-9.46.dev.
72.95-0.34-0.16+0.83+0.60+0.33.
68.97-6.54-1.43-7.65-1.94+0.6.
ours.
54.15+0.03+3.80+5.37+7.51+2.15.
53.48-7.46+4.95-2.77+6.27+5.99.
table 7: f1 score differences of various tase and roberta models on the quorefdev and our dataset splits thatare created based on the semantic overlap and short distance reasoning biases.
for instance, ours in the ¬semanticoverlap column shows the performance differences of the examined models on the split of our dataset in whichexamples do not contain the semantic overlap bias.
negative differences are marked in gray..weblogs, magazines, and bible, which are very dif-ferent from those in quoref.
as a result, pretrainingon squad has a positive impact on the majority ofdatasets.
however, this impact is less pronouncedon our challenge dataset, as it requires coreferencereasoning while this skill is not present in squadexamples..finally, while using the sentence of coreferringmentions as a declarative query (conlldec) is thecommon method for converting coreference reso-lution datasets into qa format in previous studies,our results show using conllbart has a more posi-tive impact compared to using conlldec..5.5 analysis.
to analyze what kind of examples beneﬁt morefrom incorporating the coreference data, we splitquorefdev and our dataset into different subsetsbased on the semantic overlap and short distancereasoning biases, which are the most commontypes of biases in both datasets..the semantic overlap column in table 7 repre-sents the results on the subset of the data in whichanswers reside in the most similar sentence of thecontext, and the ¬semantic overlap column con-tains the rest of the examples in each of the exam-ined datasets.
the short reasoning column presentsthe results on the subset of the data containing ex-amples that can be solved by the short distance rea-soning bias model, and ¬ short reasoning presentsthe results on the rest of the examples..table 7 shows the performance differences of thetase and roberta models on these four subsetsfor each of the two datasets..surprisingly, the performance of the baselinemodels is lower on the semantic overlap subsetcompared to ¬semantic overlap on quorefdev.
thiscan indicate that examples in the ¬semantic over-lap subset of quorefdev contain other types of bi-ases that make qa less challenging on this subset.
the addition of the coreference resolution an-notations in all four training settings reduces theperformance gap of the tase model on the se-mantic overlap and ¬semantic overlap subsets forboth datasets.
incorporating coreference data forroberta, on the other hand, has a positive impactusing the conllbart data and on the harder sub-sets of our challenge evaluation set, i.e., ¬semanticoverlap and ¬short reasoning..finally, there is still a large performance gap be-tween short reasoning and ¬ short reasoning sub-sets.
in our coreference-to-qa conversion methods,we consider the closest antecedent of each anaphoras the answer.
a promising direction for futurework is to also create qa pairs based on longerdistance coreferring expressions, e.g., to create twoqa pairs based on each anaphor, one in which theanswer is the closest antecedent, and the other withthe ﬁrst mention of the entity in the text as theanswer..6 conclusions.
we show that the high performance of recent mod-els on the quoref dataset does not necessarily indi-cate that they are adept at performing coreferencereasoning, and that qa based on coreference rea-soning is a greater challenge than current scores.
5775suggest.
we then propose a methodology for creat-ing a dataset that better presents the coreference rea-soning challenge for mrc.
we provide our method-ology to an annotator and create a sample dataset.
our analysis shows that our dataset contains fewerbiases compared to quoref, and the performance ofstate-of-the-art quoref models drops considerablyon this evaluation set..to improve the coreference reasoning of qamodels, we propose to use coreference resolu-tion datasets to train mrc models.
we proposea method to convert coreference annotations intoan mrc format.
we examine the impact of in-corporating this coreference data on improving thecoreference reasoning of qa models using two top-performing qa systems from the quoref leader-board.
we show that using coreference datasetsimproves the performance of both examined mod-els on our evaluation set, indicating their improvedcoreference reasoning.
the results on our evalua-tion set suggest that there is still room for improve-ment, and reading comprehension with coreferenceunderstanding remains a challenge for existing qamodels, especially if the coreference relation isbetween two distant expressions..acknowledgments.
this work has been supported by the german re-search foundation (dfg) as part of the qasciinfproject (grant gu 798/18-3), and the german fed-eral ministry of education and research and thehessian ministry of higher education, research,science and the arts within their joint supportof the national research center for applied cy-bersecurity athene.
dan roth’s work is partlysupported by contract fa8750-19-2-1004 with theus defense advanced research projects agency(darpa).
the authors would like to thank michaelbugert, max glockner, yevgeniy puzikov, nilsreimers, andreas r¨uckl´e, and the anonymous re-viewers for their valuable feedback..references.
rahul aralikatte, matthew lamm, daniel hardt, andanders søgaard.
2019. a simple transfer learn-ing baseline for ellipsis resolution.
arxiv preprintarxiv:1908.11141..devansh arpit, stanisław jastrzundeﬁnedbski, nicolasballas, david krueger, emmanuel bengio, maxin-der s. kanwal, tegan maharaj, asja fischer, aaroncourville, yoshua bengio, and simon lacoste-julien.
2017. a closer look at memorization indeep networks.
in proceedings of the 34th interna-tional conference on machine learning - volume 70,icml’17, pages 233–242.
jmlr.org..max bartolo, alastair roberts, johannes welbl, sebas-tian riedel, and pontus stenetorp.
2020. beat theai: investigating adversarial human annotation forreading comprehension.
transactions of the associ-ation for computational linguistics, 8:662–678..michael chen, mike d’arcy, alisa liu, jared fer-nandez, and doug downey.
2019. codah: anadversarially-authored question answering datasetfor common sense.
in proceedings of the 3rd work-shop on evaluating vector space representationsfor nlp, pages 63–69, minneapolis, usa.
associ-ation for computational linguistics..christopher clark, mark yatskar, and luke zettle-moyer.
2019a.
don’t take the easy way out: en-semble based methods for avoiding known datasetin proceedings of the 2019 conference onbiases.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4067–4080, hong kong, china.
association forcomputational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019b.
what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..pradeep dasigi, nelson f. liu, ana marasovi´c,noah a. smith, and matt gardner.
2019. quoref:a reading comprehension dataset with questions re-in proceedings ofquiring coreferential reasoning.
the 2019 conference on empirical methods in nat-ural language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5925–5932, hong kong,china.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..5776dheeru dua, sameer singh, and matt gardner.
2020.beneﬁts of intermediate annotations in reading com-prehension.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5627–5634, online.
association for computa-tional linguistics..dheeru dua, yizhong wang, pradeep dasigi, gabrielstanovsky, sameer singh, and matt gardner.
2019.drop: a reading comprehension benchmark requir-ing discrete reasoning over paragraphs.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 2368–2378, min-neapolis, minnesota.
association for computationallinguistics..jesse dunietz, greg burnham, akash bharadwaj,owen rambow, jennifer chu-carroll, and dave fer-rucci.
2020. to test machine comprehension, startby deﬁning comprehension.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 7839–7859, online.
as-sociation for computational linguistics..esin durmus, he he, and mona diab.
2020. feqa: aquestion answering evaluation framework for faith-fulness assessment in abstractive summarization.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5055–5070, online.
association for computational lin-guistics..matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameersingh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesin findings of the associationvia contrast sets.
for computational linguistics: emnlp 2020, pages1307–1323, online.
association for computationallinguistics..he he, sheng zha, and haohan wang.
2019. unlearndataset bias in natural language inference by ﬁttingthe residual.
in proceedings of the 2nd workshop ondeep learning approaches for low-resource nlp(deeplo 2019), pages 132–142, hong kong, china.
association for computational linguistics..matthew honnibal and mark johnson.
2015. an im-proved non-monotonic transition system for depen-in proceedings of the 2015 con-dency parsing.
ference on empirical methods in natural languageprocessing, pages 1373–1378, lisbon, portugal.
as-sociation for computational linguistics..yufang hou.
2020. bridging anaphora resolution asquestion answering.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 1428–1438, online.
associationfor computational linguistics..robin jia and percy liang.
2017. adversarial exam-ples for evaluating reading comprehension systems.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages2021–2031, copenhagen, denmark.
association forcomputational linguistics..daniel khashabi, snigdha chaturvedi, michael roth,shyam upadhyay, and dan roth.
2018. looking be-yond the surface: a challenge set for reading com-prehension over multiple sentences.
in proceedingsof the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (long pa-pers), pages 252–262, new orleans, louisiana.
as-sociation for computational linguistics..ronan le bras, swabha swayamdipta, chandra bhaga-vatula, rowan zellers, matthew peters, ashish sab-harwal, and yejin choi.
2020. adversarial ﬁlters ofdataset biases.
in international conference on ma-chine learning, pages 1078–1088.
pmlr..heeyoung lee, angel chang, yves peirsman,nathanael chambers, mihai surdeanu, and danjurafsky.
2013. deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.
computational linguistics, 39(4):885–916..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..sheng liu, jonathan niles-weed, narges razavian,and carlos fernandez-granda.
2020. early-learningregularization prevents memorization of noisy labels.
in advances in neural information processing sys-tems 33 pre-proceedings (neurips 2020)..y. liu, myle ott, naman goyal, jingfei du, mandarjoshi, danqi chen, omer levy, m. lewis, lukezettlemoyer, and veselin stoyanov.
2019. roberta:a robustly optimized bert pretraining approach.
arxiv, abs/1907.11692..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..5777sameer pradhan, alessandro moschitti, nianwen xue,olga uryupina, and yuchen zhang.
2012a.
conll-2012 shared task: modeling multilingual unre-stricted coreference in ontonotes.
in joint confer-ence on emnlp and conll - shared task, pages1–40, jeju island, korea.
association for computa-tional linguistics..prasetya ajie utama, naﬁse sadat moosavi, and irynagurevych.
2020a.
mind the trade-off: debiasingnlu models without degrading the in-distributionin proceedings of the 58th annualperformance.
meeting of the association for computational lin-guistics, pages 8717–8729, online.
association forcomputational linguistics..sameer pradhan, alessandro moschitti, nianwen xue,olga uryupina, and yuchen zhang.
2012b.
conll-2012 shared task: modeling multilingual unre-stricted coreference in ontonotes.
in joint confer-ence on emnlp and conll - shared task, pages1–40, jeju island, korea.
association for computa-tional linguistics..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..amrita saha, rahul aralikatte, mitesh m. khapra, andkarthik sankaranarayanan.
2018. duorc: towardscomplex language understanding with paraphrasedreading comprehension.
in proceedings of the 56thannual meeting of the association for computa-tional linguistics (volume 1: long papers), pages1683–1693, melbourne, australia.
association forcomputational linguistics..elad segal, avia efrat, mor shoham, amir globerson,and jonathan berant.
2020. a simple and effec-tive model for answering multi-span questions.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3074–3080, online.
association for computa-tional linguistics..saku sugawara, pontus stenetorp, kentaro inui, andakiko aizawa.
2020. assessing the benchmark-ing capacity of machine reading comprehensionin proceedings of the 34th aaai confer-datasets.
ence on artiﬁcial intelligence (aaai 2020).
associ-ation for the advancement of artiﬁcial intelligence..prasetya ajie utama, naﬁse sadat moosavi, and irynagurevych.
2020b.
towards debiasing nlu modelsfrom unknown biases.
in proceedings of the 2020conference on empirical methods in natural lan-guage processing (emnlp), pages 7597–7610, on-line.
association for computational linguistics..dirk weissenborn, georg wiese, and laura seiffe.
2017. making neural qa as simple as possiblebut not simpler.
in proceedings of the 21st confer-ence on computational natural language learning(conll 2017), pages 271–280, vancouver, canada.
association for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, rmi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander m. rush.
2020.transformers: state-of-the-art natural language pro-cessing.
in proceedings of the 2020 conference onempirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..mingzhu wu, naﬁse sadat moosavi, andreas r¨uckl´e,and iryna gurevych.
2020a.
improving qa general-ization by concurrent modeling of multiple biases.
in findings of the association for computationallinguistics: emnlp 2020, pages 839–853, online.
association for computational linguistics..wei wu, fei wang, arianna yuan, fei wu, and ji-wei li.
2020b.
corefqa: coreference resolution asquery-based span prediction.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 6953–6963, online.
as-sociation for computational linguistics..zhilin yang, peng qi, saizheng zhang, yoshua bengio,william cohen, ruslan salakhutdinov, and christo-pher d. manning.
2018a.
hotpotqa: a datasetfor diverse, explainable multi-hop question answer-ing.
in proceedings of the 2018 conference on em-pirical methods in natural language processing,pages 2369–2380, brussels, belgium.
associationfor computational linguistics..ian tenney, dipanjan das, and ellie pavlick.
2019.bert rediscovers the classical nlp pipeline.
inproceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..zhilin yang, saizheng zhang, jack urbanek, willfeng, alexander h miller, arthur szlam, douwekiela, and jason weston.
2018b.
mastering the dun-geon: grounded language learning by mechanicalturker descent.
in proceedings of 6th internationalconference on learning representations (iclr)..5778rowan zellers, yonatan bisk, roy schwartz, andyejin choi.
2018. swag: a large-scale adversar-ial dataset for grounded commonsense inference.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages 93–104, brussels, belgium.
association for computa-tional linguistics..a additional statistics about biased.
examples.
table 8 shows the proportion of biased examplesin the conlldec set.
we can see that the resultsare similar to that of the conllbart set..to compare the ratio of biased examples betweenquorefdev and our challenge set when consider-ing the same number of examples in both datasets,we randomly sample 10 different subsets fromquorefdev and our challenge set with 100 samplesin each subset and compute the rations in each sub-set.
figure 3 shows the results.
as we see, in thissetting the ratio of all bias types in our evaluationset is still lower than those in quorefdev..b additional experiments.
table 9 shows additional experiments for pre-training the examined models on coreference data.
we examine an additional setting for pre-trainingon both conlldec and conllbart by ﬁrst trainingthe models on conlldec, then on conllbart, andﬁnally on quoref (transfer-conllbart+dec).
bycomparing the results of transfer-conllbart+decwith transfer-conllbart from table 5, we observethat pre-training the models on both conlldecand conllbart does not result in any additionaladvantage compared to only using conllbart..c additional examples.
table 10 presents more examples from conlldecand conllbart..bias.
conlldec.
random named entitywh-wordempty questionsemantic overlapshort-distance reasoning.
2.1112.9711.2435.3221.05.table 8: proportion of biased examples in conlldecdataset..5779figure 3: the average, upper and lower bounds of the ratio of biased examples in quorefdev and our challenge setfor the randomly sampled 10 subsets..model.
training setup.
quorefdev quoreftest ours contrast set multirc.
tase.
roberta.
baselinetransfer-conllbart+dec.
baselinetransfer-conllbart+dec.
84.0585.01.
79.6473.29.
84.7185.73.
79.6973.73.
66.4868.06.
64.3558.19.
73.4476.54.
69.9557.18.
51.8349.61.
37.1231.50.table 9: additional experiments on using the conlldec and conllbart data for pre-training roberta-large andtase models.
transfer-conllbart+dec refers to the setting in which the model is ﬁrst trained on conlldec, thenon conllbart, and ﬁnally on quoref..5780proportion of biased examples0204060random named entitywh-wordempty questionsemantic overlapshort-distance dev averagedev lower bounddev upper boundours averageours lower boundours upper boundpassage in conll.
mention cluster.
conlldec quesion.
conllbart question.
gold answer.
[bill clinton, mr. clinton, his, he, he].
bill clinton.
after george w. bush is sworn in, billclinton will head to new york.
mr.clinton will also spend time at his presi-dential library in arkansas.
he says hewill come to washington, ’every nowand then’.
paul had already decided not to stop atephesus.
he did not want to stay toolong in asia.
he was hurrying becausehe wanted to be in jerusalem on the dayof pentecost if possible..the kmt vice chairman arrived at partyheadquarters to meet with kmt chair-man lien chan on the afternoon ofpw...he said that he will follow lienchan as a lifelong volunteer.....it also includes a lot of sheep, goodclean - living, healthy sheep, and an ital-ian entrepreneur has an idea about howto make a little money of them...so thisguy came up with the idea of havingpeople adopting sheep by an internet..george w. bush has met with al gore inwashington.
the two men met for just15 minutes at the vice president’s ofﬁ-cial residence...bush went into the talkswith his defeated rival after meeting withpresident clinton earlier today..meanwhile prime minister ehud baraktold israeli television he doubts a peacedeal can be reached before israel’sfebruary 6th election.
he said he willnow focus on suppressing palestinian vi-olence..[paul, he, he, he].
paul.
[the kmt vice chairman, he, he].
the kmt vice chairman.
[an italian entrepreneur, this guy].
an italian entrepreneur.
[al gore, his defeated rival].
al gore.
says <ref> hehe</ref> will come towashington, ‘every nowand then.’.
was.
hehurryingbecause <ref> he</ref> wanted to be injerusalem on the day ofpentecost if possible..he said that <ref> he</ref> will follow lienchan as a lifelong vol-unteer..so <ref> this guy</ref> came up withtheidea of havingpeople adopting sheepby an internet..bush went into the talkswith <ref> his defeatedrival </ref> after meet-ing with president clin-ton earlier today..who says he will cometo washington, ’everynow and then’?.
who was hurrying be-cause they wanted to bein jerusalem on the dayof pentecost if possible?.
who said that he will fol-low lien chan as a life-long volunteer?.
who came up with theidea of having peopleadopting sheep by an in-ternet?.
who did bush go intothe talks with after meet-ing with president clin-ton earlier today?.
said <ref> hehe</ref> will now fo-cussuppressingpalestinian violence..on.
who said he will nowfocus on suppressingpalestinian violence?.
[prime minister ehud barak, he, he].
prime minister ehud barak.
table 10: more examples from coreference-to-qa conversion using conlldec and conllbart approaches..5781