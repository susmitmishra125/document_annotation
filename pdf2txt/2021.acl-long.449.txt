adapting unsupervised syntactic parsing methodology for discoursedependency parsingliwen zhang1,2,3,4, ge wang1,2,3,4, wenjuan han5, kewei tu1∗1school of information science and technology, shanghaitech university2shanghai engineering research center of intelligent vision and imaging3shanghai institute of microsystem and information technology4university of chinese academy of sciences5beijing institute for general artiﬁcial intelligence, beijing, china{zhanglw1, wangge, tukw}@shanghaitech.edu.cnhanwenjuan@bigai.ai.
abstract.
one of the main bottlenecks in developing dis-course dependency parsers is the lack of an-notated training data.
a potential solution isto utilize abundant unlabeled data by usingunsupervised techniques, but there is so farlittle research in unsupervised discourse de-pendency parsing.
fortunately, unsupervisedsyntactic dependency parsing has been stud-ied for decades, which could potentially beadapted for discourse parsing.
in this paper,we propose a simple yet effective method toadapt unsupervised syntactic dependency pars-ing methodology for unsupervised discoursedependency parsing.
we apply the methodto adapt two state-of-the-art unsupervised syn-tactic dependency parsing methods.
exper-imental results demonstrate that our adapta-tion is effective.
moreover, we extend theadapted methods to the semi-supervised andsupervised setting and surprisingly, we ﬁndthat they outperform previous methods spe-cially designed for supervised discourse pars-ing.
further analysis shows our adaptations re-sult in superiority not only in parsing accuracybut also in time and space efﬁciency..1.introduction.
discourse parsing, aiming to ﬁnd how the textspans in a document relate to each other, beneﬁtsvarious down-stream tasks, such as machine trans-lation evaluation (guzm´an et al., 2014; joty et al.,2014), summarization (marcu, 2000; hirao et al.,2013), sentiment analysis (bhatia et al., 2015; hu-ber and carenini, 2020) and automated essay scor-ing (miltsakaki and kukich, 2004; burstein et al.,2013).
researchers have made impressive progresson discourse parsing from the constituency per-spective, which presents discourse structures asconstituency trees (ji and eisenstein, 2014; fengand hirst, 2014; joty et al., 2015; nishida and.
*corresponding author..nakayama, 2020).
however, as demonstrated bymorey et al.
(2018), discourse structure can also beformulated as a dependency structure.
besides that,there might exist ambiguous parsing in terms of theconstituency perspective (morey et al., 2018).
allof these suggest that dependency discourse pars-ing is a different promising approach for discourseparsing..one of the main bottlenecks in developing dis-course dependency parsing methods is the lackof annotated training data since the labeling ef-fort is labor-intensive and time-consuming, andneeds well-trained experts with linguistic knowl-edge (marcu et al., 1999).
this problem canbe tackled by employing unsupervised and semi-supervised methods that can utilize unlabeled data.
however, while unsupervised methodology hasbeen studied for decades in syntactic dependencyparsing, there is little attention paid to the counter-part in discourse dependency parsing.
consider-ing the similarity between syntactic and discoursedependency parsing, it is natural to suggest suchmethodology can be adapted from the former to thelatter..in this paper, we propose a simple yet effectiveadaptation method that can be readily applied to dif-ferent unsupervised syntactic dependency parsingapproaches.
adaptation from syntactic dependencyparsing to discourse dependency parsing has twochallenges.
first, unlike syntactic parsing whichhas a ﬁnite vocabulary, in discourse parsing, thenumber of elementary discourse units (edus) isunlimited.
this makes it difﬁcult if not impossi-ble to directly apply syntactic approaches requiringenumeration of words or word categories to dis-course parsing.
second, in a discourse dependencyparse tree, the dependencies within a sentence ora paragraph often form a complete subtree.
thereis no correspondence to this constraint in syntacticparsing approaches.
to address these two chal-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5782–5794august1–6,2021.©2021associationforcomputationallinguistics5782lenges, we cluster the edus to produce clustersresembling part-of-speech (pos) tags in syntacticparsing and we introduce the hierarchical eisneralgorithm that ﬁnds the optimal parse tree conform-ing to the constraint..we applied our adaptation method to two state-of-the-art unsupervised syntactic dependency pars-ing models: neural conditional random field au-toencoder (ncrfae, li and tu (2020)) and vari-ational variant of discriminative neural depen-dency model with valences (v-dndmv, han et al.
(2019)).
in our experiments, the adapted modelsperforms better than the baseline on both rst dis-course treebank (rst-dt, carlson et al.
(2001))and scidtb (yang and li, 2018) in the unsuper-vised setting.
when we extend the two models tothe semi-supervised and supervised setting, we ﬁndthey can outperform previous methods specially de-signed for supervised discourse parsing..further analysis indicates that the hierarchicaleisner algorithm shows superiority not only in pars-ing accuracy but also in time and space efﬁciency.
its empirical time and space complexity is close too(n2) with n being the number of edus, while theunconstrained algorithm adopted by most previouswork has a complexity of o(n3).
the code andtrained models can be found at: https://github.
com/ehaschia/discoursedependencyparsing..2 related work.
unsupervised syntactic dependency parsingunsupervised syntactic dependency parsing is thetask to ﬁnd syntactic dependency relations betweenwords in sentences without guidance from annota-tions.
the most popular approaches to this task aredependency model with valences (dmv, kleinand manning (2004)), a generative model learn-ing the grammar from pos tags for dependencypredictions, and its extensions.
jiang et al.
(2016)employ neural networks to capture the similaritiesbetween pos tags ignored by vanilla dmv andhan et al.
(2019) further amend the former withdiscriminative information obtained from an addi-tional encoding network.
besides, there are alsosome discriminative approaches modeling the con-ditional probability or score of the dependency treegiven the sentence, such as the crf autoencodermethod proposed by cai et al.
(2017)..discourse dependency parsing there is limitedwork focusing on discourse dependency parsing.
li et al.
(2014) proposes an algorithm to convert.
constituency rst tree to dependency structure.
intheir algorithm, each non-terminal is assigned witha head edu, which is the head edu of its left-most nucleus child.
then, a dependency relation iscreated for each non-terminal from its head to itsdependent, in a procedure similar to those designedfor syntactic parsing.
hirao et al.
(2013) proposesanother method that differs from the previous onein the processing of multinuclear relations.
yoshidaet al.
(2014) proposes a dependency parser builtaround a maximum spanning tree decoder andtrains on dependency trees converted from rst-dt.
their parser achieved better performance onthe summarization task than a similar constituency-based parser.
morey et al.
(2018) reviews the rstdiscourse parsing from the dependency perspec-tive.
they adapt the the best discourse constituencyparsing models until 2018 to the dependency task.
yang and li (2018) constructs a discourse depen-dency treebank scidtb for scientiﬁc abstracts.
tothe best of our knowledge, we are the ﬁrst to inves-tigate unsupervised and semi-supervised discoursedependency parsing..unsupervised constituent discourse parsingkobayashi et al.
(2019) propose two unsupervisedmethods that build unlabeled constituent discoursetrees by using the cky dynamic programming al-gorithm.
their methods build the optimal tree interms of a similarity (dissimilarity) score functionthat is deﬁned for merging (splitting) text spansinto larger (smaller) ones.
nishida et al.
(2020) useviterbi em with a margin-based criterion to train aspan-based neural unsupervised constituency dis-course parser.
the performance of these unsuper-vised methods is close to that of previous super-vised parsers..3 adaptation.
we propose an adaptation method that can be read-ily integrated with different unsupervised syntacticdependency parsing approaches.
first, we clus-ter the element discourse units (edu) to produceclusters resembling pos tags or words used in syn-tactic parsing.
this is necessary because manyunsupervised syntactic parsers require enumerationof words or word categories, typically in model-ing multinomial distributions as we shall see insection 4. while edus, which are sequencesof words, cannot be enumerated, its clusters can.
during parsing, we apply the hierarchical eisneralgorithm used for parse tree, a novel modiﬁed ver-.
5783algorithm 1 eisner algorithm.
1: inputs:.
2: initialize:.
score matrix s ∈ rn×n.
c = {}, i = {},ci→i = 0,3: for l = 1, ..., n do4:.
for i = 1, ...n − l do.
i = 1, .
.
.
, n.(cid:46) span length(cid:46) span start index(cid:46) span end index(sij + ci→k + ck+1←j).
(sji + ci→k + ck+1←j).
(ii→k + ck→j).
(ck→i + ij→k).
j = i + lii→j = maxi≤k≤jii←j = maxi≤k≤jci→j = maxi≤k≤jci←j = maxi≤k≤j.
end for.
10:11: end for.
ratiorst-dtscidtb.
train dev.
test3.0-2.60.140.140.12.table 1: the percentage of dependencies violating theconstraint that each sentence or paragraph correspondsto a subtree..demonstrate structural characteristics not taken intoaccount by the eisner algorithm.
speciﬁcally, adocument has a hierarchical structure which dividesthe document into paragraphs, each paragraph intosentences, and ﬁnally each sentence into edus,and the discourse parse tree should be consistentwith this hierarchical structure.
equivalently, in adiscourse parse tree, every sentence or paragraphshould be exactly covered by a complete subtree,like figure 1. we empirically ﬁnd that this con-straint is satisﬁed by most of the gold discourseparses in the rst discourse treebank (rst-dt,carlson et al.
(2001)) and scidtb (yang and li,2018) datasets (table 1)..we therefore propose the hierarchical eisneralgorithm, a novel modiﬁcation to the eisner algo-rithm that incorporates the constraint.
our new al-gorithm has almost the same state transition formu-las as the eisner algorithm except for a few changesbrought by the hierarchical constraint.
concretely,our algorithm ﬁnds the optimal parse tree in abottom-up way and divides the process into 3 steps:intra-sentence parsing, intra-paragraph parsing, andintra-document parsing.
in the intra-sentence pars-ing step, we run the original eisner algorithm, ex-cept that we need not to form a tree.
then in the.
1:.
[the.
figurefinancial account-ing standards board’s coming rule ondisclosure]e1 [involving ﬁnancialinstruments]e2 [willbe effective for ﬁnancialstatements with ﬁscalyears]e3 [ending after june 15, 1990.
]e4 [the date wasmisstated in friday’s edition .
]e5 [(see: ”fasb plansrule on financial risk of instruments”]e6[–wsj oct.27, 1989)]e7.
5:.
6:.
7:.
8:.
9:.
sion of the classic eisner algorithm, used for parsetree to produce discourse dependency parse treesthat conform to the constraint that every sentence orparagraph should correspond to a complete subtree..3.1 clustering.
given an input document represented as an edusequence x1, x2, .
.
.
, xn, we can use word embed-ding or context sensitive word embedding to getthe vector representation xi of the i-th edu xi.
speciﬁcally, we use bert (devlin et al., 2019)to encode each word.
let wi be the encoding ofthe i-th word in the document.
for an edu xispanning from word position b to e, we followtoshniwal et al.
(2020) and concatenate the en-coding of the endpoints to form its representation:xi = [wb; we].
with the representations of alledus from the whole training corpus obtained, weuse k-means (lloyd, 1982) to cluster them.
let cibe the cluster label of xi..3.2 hierarchical eisner algorithm.
the eisner algorithm (eisner, 1996) is a dynamicprogramming algorithm widely used to ﬁnd theoptimal syntactic dependency parse tree.
the basicidea of it is to parse the left and right dependentsof an token independently and combine them at alater stage.
algorithm 1 shows the pseudo-codeof the eisner algorithm.
here ci→j represents acomplete span, which consists of a head token iand all of its descendants on one side, and ii→jrepresent an incomplete span, which consists of ahead i and its partial descendants on one side andcan be extended by adding more descendants tothat side..discourse dependency parse trees, however,.
5784algorithm 2 modiﬁcation to algorithm 1(sij + ci→k + ck+1←j).
6: ii→j = maxi≤k≤j7: ii←j = maxi≤k≤j8: ci→j = maxi≤k≤jj∈e.
(sji + ci→k + ck+1←j).
(ii→k + ck→j).
(cid:46) here.
e is a set of the index of the end boundary ofsentences.
9: ci←j = maxi≤k≤ji∈b.
(ci←k + ik←j).
(cid:46) here b.is a set of the index of the begin boundary ofsentences..intra-paragraph step, we combine all intra-sentencespans in the paragraph.
under the constraint thatthere can only be one edu in every sentence whosehead is not belong to this sentence.
to achieve that,we modify the state transition equations (step 6-9in algorithm 1) to prune invalid arcs.
figure 2shows some cases during merge across sentencespans.
case 1 are valid because the constraint issatisﬁed.
case 2 is invalid because the head ofedu e6 can not be e4 or e5 hence the constraintis violated.
from these cases, we can ﬁnd thatfor incomplete span ii→k and complete span ck→jacross sentences, we only merge them when j isat the end boundary of a sentence as algorithm 2shows.
after the intra-paragraph step, we move tothe intra-document step to combine paragraph-levelspans following the same procedure as in the intra-paragraph step and form the ﬁnal document-leveltree..our method has lower time complexity than theoriginal eisner algorithm.
suppose a document haskp paragraphs, each paragraph has ks sentencesand each sentence has ke edus.
the time complex-s k3ity of the original eisner algorithm is o(k3e )while the time complexity of our hierarchical eis-s k3ner algorithm is o(k2.
pk3.
pk3.
e )..4 model.
we adapt two current state-of-the-art models inunsupervised syntactic dependency parsing for dis-course parsing.
one is neural crf autoencoder(ncrfae, li and tu (2020); cai et al.
(2017)),a discriminative model, and the other is : varia-tional variant of dndmv (v-dndmv, han et al.
(2019)), a generative model..figure 2: cases of span merging in discourse parsing.
e1-e6 are edus.
red e1-e3 make up a sentence and e4-e6 make up another sentence.
complete spans are de-picted as triangles and incomplete spans as trapezoids..4.1 neural crf autoencoder.
a crf autoencoder (ammar et al., 2014) consistsof an encoder and a decoder.
the encoder predictsa hidden structure, such as a discourse dependencytree in our task, from the input and the decodertries to reconstruct the input from the hidden struc-ture.
in a neuralized crf autoencoder, we employneural networks as the encoder and/or decoder..we use the widely used biafﬁne dependencyparser (dozat and manning, 2017) as the encoder tocompute the hidden structure distribution pφ(y|x),parameterized with φ. here y represents the hid-den structure and x is input document.
we feedthe input document x into a bi-lstm network toproduce the contextual representation of each edusegmentation ri, and then feed ri to two mlp net-works to produce two continuous vectors v(head)and v(dep), representing i-th edu segmentationibeing used as dependency head and dependent re-spectively..i.a biafﬁne function is used to compute the scorematrix s. each matrix element sij , the score for adependency arc pointing from xi to xj, is computedas follows:.
sij = v(head)(cid:62)i.wv(dep)i.
+ b.
(1).
where w is the parameter matrix and b is the.
bias..following dozat and manning (2017) we formu-late pφ(y|x) as a head selection problem processthat selects the dependency head of each edu in-.
5785e1e4e2e3e5e6e1e4e2e3e5e6dependently:.
pφ(y|x) =.
p (hi|x).
(2).
(cid:89).
i.where hi is the index of the head of edu xi andp (hi|x) is computed by softmax function withscore sij:.
for the unlabelled data where the gold parses are.
unknown, the unlabelled loss is:.
lu(u) = −.
(cid:88).
x∈u.
maxy∈y(x).
log pφ,λ(ˆx, y|x).
(9).
we optimize the encoder parameter φ and de-coder parameter λ together with gradient descentmethods..p (hi = j|x) =.
esjik=1 eski.
(cid:80)n.(3).
4.2 variational variant of dndmv.
the decoder parameterized with λ computespλ(ˆx|y), the probability of the reconstructed docu-ment ˆx given the parse tree y. following cai et al.
(2017) and li and tu (2020), we independentlypredict each edu ˆxi from its head speciﬁed by y.since edus cannot be enumerated, we reformu-late the process as predicting the edu cluster ˆcigiven its dependency head cluster chi.
our decodersimply speciﬁes a categorical distribution p (ˆci|chi)for each possible edu cluster and compute the re-construction probability as follows:.
pλ(ˆx|y) =.
p (ˆci|chi).
(4).
(cid:89).
i.we achieve the ﬁnal reconstruction distributionby cascading the encoder and decoder distribution:.
pφ,λ(ˆx, y|x) = pλ(ˆx|y)pφ(y|x).
(5).
the best parsing is obtained by maximizingpφ,λ(ˆx, y|x):.
y∗ = arg max.
pφ,λ(ˆx, y|x).
(6).
y.we consider the general case of training thecrf autoencoder with dataset d containing bothlabelled data l and unlabelled data u. purely su-pervised or unsupervised learning can be seen asspecial cases of this setting.
the loss function l(d)consists of a labelled loss ll(l) and an unlabelledloss lu(u):.
l(d) = αll(l) + (1 − α)lu(u).
(7).
where α is the hyperparameter weighting the im-portance of the two parts..for the labelled data, where the gold parse trees.
y∗ are known, labelled loss is:.
ll(l) = −.
log pφ,λ(ˆx, y∗|x).
(8).
(cid:88).
x∈l.
v-dndmv is a variational autoencoder modelcomposed of both an encoder and a decoder.
theencoder is a bi-lstm that takes the input docu-ment and produces parameters of a gaussian distri-bution from which a continuous vector s summa-rizing the document sampled..the decoder models the joint probability of thedocument and its discourse dependency tree condi-tion on s with a generative grammar.
the grammaris deﬁned on a ﬁnite set of discrete symbols, soin our adapted model, input documents are rep-resented by edu clusters instead of edus thatare inﬁnite in number.
there are three types ofgrammar rules, each associated with a set of proba-bilistic distributions: root,child and decision.
to generate a document, we ﬁrstly sample from theroot distribution proot(chd|s) to determine thecluster label of the head edu of the document andthen recursively decide whether to generate a newchild edu cluster and what child edu cluster togenerate by sampling from the decision distribu-tion pdecision(dec|h, dir, val, s) and child distri-bution pchild(chd|h, dir, val, s).
dir denotes thegeneration direction (i.e, left or right), val is a bi-nary variable denoting whether the current edualready has a child in the direction dir or not.
decis a binary variable indicating whether to continuegenerating a child edu, and h and chd denotethe parent and child edu cluster respectively.
weuse neural networks to calculate these distributions.
the input of the networks is the continuous vectoror matrix representations of grammar rule compo-nents such as h, chd, val and dir as well as docu-ment vector s produced by the encoder..the training objective for learning the model isthe probability of the training data.
the interme-diate continuous vector s and the hidden variablerepresenting the dependency tree are both marginal-ized.
since the marginalized probability cannot becalculated exactly, v-dndmv maximizes the ev-idence lower bound (elbo), a lower bound ofthe marginalized probability.
elbo consists of.
5786the conditional likelihood of the training data andan regularisation term given by the kl divergencebetween pθ(s|x) and p (s) (which is a standardgaussian).
the conditional likelihood is shown asfollows:.
l(θ) =.
1n.n(cid:88).
(cid:88).
i=1.
y(i)∈y(x(i)).
log pθ(x(i), y(i)|s(i)).
hyper-parameter for our ncrfae model, weadopt the hyper-parameters of li and tu (2020).
for our v-ndnmv model we adopt the hyper-parameters of han et al.
(2019).
we use adam(kingma and ba, 2015) to optimize our objectivefunctions.
experimental details are provided inappendix a..(10).
5.2 main result.
here n is the number of training samples, y isthe dependency tree and y(x) is the set of all pos-sible dependency tree in x. θ is the parameters ofthe neural networks.
we can rewrite the conditionalprobability as following:.
pθ(x, y|s) =.
p (r|s).
(11).
(cid:89).
r∈(x,y).
where r is the grammar rule involved in generatingx along with y..we optimize elbo using the expectation-maximization (em) algorithm, alternating the e-step and the m-step.
in the e-step, we ﬁx rule pa-rameters and use our hierarchical eisner algorithmto compute the expectation of possible dependencytree y, which gives the expected count of rules usedin the training samples.
in the m-step, expectedcount of rules computed in the e-step is used totrain the prediction neural networks with gradientdescent methods.
the regularisation term is alsooptimized using gradient descent methods in them-step.
after training, the parsing result y∗of anew test case x is obtained as:.
y∗ = arg maxy∈y(x).
pθ(x, y|s).
(12).
5 experiment.
5.1 setting.
data we evaluate the performance of our modelson the rst discourse treebank* (rst-dt, carlsonet al.
(2001)) and scidtb† (yang and li, 2018).
rst-dt consists of wall street journal articlesmanually annotated with rst structures (mann andthompson, 1988).
we use the method proposedby li et al.
(2014) to convert the rst structuresamples into dependency structures.
scidtb con-sists of scientiﬁc abstracts from acl anthologyannotated with dependency structures..*https://catalog.ldc.upenn.edu/.
ldc2002t07.
we compared our methods with the following base-lines:.
right branching (rb) is a rule based method.
given a sequence of elements (i.e., edus or sub-trees), rb generates a left to right chain struc-ture, like x1 → x2, x2 → x3 · · · .
in order todevelop a strong baseline, we include the hierar-chical constraint introduced in section 3.2 in thisprocedure.
that is, we ﬁrst build sentence-leveldiscourse trees using the right branching methodbased on sentence segmentation.
then we buildparagraph-level trees using the right branchingmethod to form a left to right chain of sentence-level subtrees.
finally we obtain document-leveltrees in the same way.
since this method has threestages, we call it “rb rb rb”.
this simple pro-cedure forms a strong baseline in terms of perfor-mance.
as nishida and nakayama (2020) reports,the unlabeled f1 score of constituent structures ofrb rb rb reaches 79.9 on rst-dt.
correspond-ingly, the performance of the supervised methodproposed by (joty et al., 2015) is 82.5..nishida20 is a neural model for unsuper-vised discourse constituency parsing proposed bynishida and nakayama (2020).
this model runs acky parser that uses a bi-lstm model to learnrepresentations of text spans, complemented withlexical, syntactic and structural features.
we con-vert its result to dependency structure using thesame conversation method of li et al.
(2014).
tomake a fair comparison, we use rb rb rb to ini-tialize their model instead of rb∗ rb rb as intheir paper, where rb∗ means using predicted syn-tactic structures for initialization at the sentencelevel..compared with baselines , our two adapted mod-els ncrfae and v-dndmv both achieve betterperformance on the two datasets.
results also showthat the generative model v-dndmv is better thanthe discriminatve model ncrfae in the unsuper-vised setting..†https://github.com/pku-tangent/scidtb.
we also investigate the semi-supervised setting.
5787scidtb rst-dt.
rb rb rbnishida20adapted v-dndmvadapted ncrfae.
52.5-54.453.3.
43.941.944.244.0.table 2: unsupervised discourse dependency parsingresults on rst-dt and scidtb.
the evaluation metricis the unlabeled attachment score (uas)..figure 3:semi-supervised discourse dependencyparsing results on scidtb.
the v-dndmv-s andncrfae-s mean these two model are trained on la-beled data only.
the x-axis represents the ratio of la-beled/unlabeled data used for training.
the y-axis rep-resents the uas score..on the scidtb dataset of our adapted models withvaried ratios of labeled/unlabeled data.
experimen-tal results are shown in figure 3, which indicatethat ncrfae outperforms v-dndmv for all theratios.
even when trained with only a few labeleddata (0.01 of labeled data in scidtb, only about 7samples), the discriminative model already outper-forms the generative model signiﬁcantly.
besidesthat, we also ﬁnd our semi-supervised methodsreach higher uas scores than their supervised ver-sions (trained with labeled data only) for all thelabeled/unlabeled data ratios..inspired by the promising results in the semi-supervised setting, we also investigate the perfor-mance of our adapted ncrfae and v-dndmv inthe fully supervised setting.
the results are shownin table 3. we evaluate our models on the rst-dt and scidtb datasets and compare them witheight models.
nivre04 (nivre et al., 2004) andwang17 (wang et al., 2017) are two transition-based models for dependency parsing.
yang andli (2018) adapts them to discourse dependencyparsing.
feng14 (feng and hirst, 2014), ji14.
‡we correct their evaluation metrics, so the result is differ-.
ent from the original paper (li et al., 2014)..rst-dt.
scidtb.
-nivre0448.7‡li1465.6feng1466.9ji1464.4joty1566.1braud17-wang1766.4morey18adapted v-dndmv 63.570.2adapted ncrfae.
uas las uas las53.542.5----54.5--65.0.
70.257.6----70.2-73.479.1.
--48.551.748.049.9-48.7-51.8.table 3: supervised discourse dependency parsing re-sults on rst-dt and scidtb.
the uas is unlabeledattachment score and las is labeled attachmentscore..(ji and eisenstein, 2014), joty15 (joty et al.,2015) and braud17 (braud et al., 2017) aremethods for discourse constituent parsing and theyare adapted for discourse dependency parsing bymorey et al.
(2018).
li14 (li et al., 2014) andmorey18 (morey et al., 2018) are graph-basedand transition-based methods specially designedfor discourse dependency parsing, respectively.
these models are statistical or simple neural mod-els, and they do not use pretrained language models(like bert, elmo (peters et al., 2018)) to extractfeatures..as table 3 shows, the performance of our ncr-fae is signiﬁcantly better than the baseline models.
especially, the uas and las of ncrfae are 8.9points and 11.5 points higher than the best baselinemodels on the scidtb dataset, respectively.
be-sides that, we ﬁnd that v-dndmv also beats base-lines on the scidtb dataset and reaches compara-ble results on rst-dt.
we also test our approacheswithout using bert and ﬁnd that they still outper-form the baselines.
for example, the performanceof ncrfae with glove (pennington et al., 2014)on scidtb averaged over 5 runs is: uas: 73.9 las:55.5. these results again give evidence for oursuccess in adapting unsupervised syntactic depen-dency parsing methods for discourse dependencyparsing as the adapted methods not only work in theunsupervised setting, but also reach state-of-the-artin the supervised setting..as for the performance gap between v-dndmvand ncrfae, we believe that the main reason istheir different abilities to extract contextual featuresfrom the input text for the parsing task.
as a gen-erative model, the decoder of v-dndmv follows.
5788figure 4: analysis of time and space cost in runningour hierarchical eisner and traditional eisner algorithmon rst-dt dataset against document length.left: timecost.
right: space cost..a strong assumption that each token in the inputtext is generated independently, which preventsthe contextual features from being directly used.
instead, contextual features are mixed with otherinformation in the document representation whichacts as the condition of the generation process inthe model.
ncrfae, on the other hand, employsa discriminative parser to leverage contextual fea-tures for dependency structure prediction directly.
thus, as long as there is sufﬁcient labeled data,ncrfae can achieve much better results than v-dndmv.
we have observed a similar phenomenonin syntactic parsing..signiﬁcance test we investigate the signiﬁcanceof the performance improvement in every setting.
for unsupervised parsing, we perform a t-test be-tween the strongest baseline rb rb rb and v-dndmv.
the t-value and p-value calculated on10 runs are 2.86 and 0.00104, which shows thesigniﬁcance of the improvement.
for the semi-supervised results, we also perform signiﬁcancetests between the semi-supervised and supervised-only results.
the results show that our semi-supervised method signiﬁcantly outperforms thesupervised-only method.
for example, on the0.5:0.5 setting, the t-value is 2.13 and the p-valueis 0.04767. for the fully supervised setting, due toa lack of code from previous work, it is currentlydifﬁcult for us to carry out a signiﬁcance analysis.
instead, we show that our models are very stableand consistently outperform the baselines by run-ning our models for 10-times.
for example, ourncrfae uas score is 78.95±0.29 on the scidtbdataset..6 analysis.
clusters.
10uas 52.7.
3053.9.
5054.6.
10053.5.table 4: uas with different cluster numbers on the de-velopment set of scidtb..randomk-meansnice.
mutual information0.007§0.1060.096.table 5: mutual information.
the experiments are run on servers equipped withnvidia titan v gpus.
we can observe clearlythat the curve of the hierarchical eisner algorithmalways stays far below that of the eisner algorithm,which veriﬁes our theoretical analysis on the timecomplexity of the hierarchical eisner algorithm insection 3.2..the right part of figure 4 demonstrates a similarphenomenon where we illustrate the memory usageof the hierarchical and traditional eisner algorithmsagainst the training document length in the samecomputing environment.
from the curves of thesetwo ﬁgures we can conclude that our hierarchicaleisner algorithm has advantage over the traditionalone in both time and space efﬁciencies..besides the superiority in computational efﬁ-ciency, our experiments also indicate that our hi-erarchical eisner algorithm can achieve better per-formance than the traditional one.
with other con-ditions ﬁxed, the uas produced by hierarchicaleisner is 79.1 in the task of supervised discourseparsing on the scidtb dataset while the corre-sponding result of the eisner algorithm is 78.6..6.2 number of clusters.
to explore the suitable number of clusters of edus,we evaluate our ncrfae model with differentcluster numbers from 10 to 100. as table 4 shows,there is an upward trend while the number of clus-ters increases from 10 to 50. after reaching thepeak, the uas decreases as the number of clustercontinues to increase.
we thus choose 50 for ourexperiments..6.3 label analysis.
6.1 eisner vs. hierarchical eisner.
in the left part of figure 4 we show the curves of thetime cost of the hierarchical and traditional eisneralgorithms against the rst-dt document length..in order to inspect if there exist any coherent re-lations between the clusters of edus obtained for.
§this is the actual evaluation result and the theoretical.
result should be 0.0.
5789figure 5: heat-maps of probabilities that relations use different label as dependency head (left) or child (right)..adaptation in discourse parsing and the labels ofdependency arcs, similar to that between pos tagsand syntactic dependency labels, we compute theco-appearance distribution of cluster labels and de-pendency arc labels.
in figure 5, we show theprobabilities of the clusters being used as headsphead(ck|rm) and children pchild(ck|rm) given dif-ferent dependency types respectively.
here ck andrm represent different type of clusters and relations.
we cluster edus to 10 clusters and only show asubset of them.
detailed heat-map can be found inappendix b..by observing the two heat-maps, we notice ob-vious trends that for each dependency arc label,the co-appearance probabilities are concentratedat certain cluster labels.
for example, when thecluster is used as dependency heads, more than60% of the co-appearance probability for arc labelcomparison and same-unit is concentratedat cluster type 9 and 6 respectively; when the clus-ter is used as dependency children, cluster type1 receives more than 40% of the co-appearanceprobability for certain arc labels.
the property dis-played by the adaptation clusters is very similarto that of pos tags, which justiﬁes our clusteringstrategy adopted for discourse parsing..to further quantify the coherence between theadaptation clusters and dependency arcs, we eval-uate the mutual information between two discreterandom variables in the training set of scidtb: oneis the tuple consists of two cluster labels for a pairof edus in the training sample, representing de-pendency head and child respectively; and the otheris the binary random variable indicating whetherthere exists a dependency arc between a edu pair.
in the training data.
besides our adaptation clusters,we also evaluate this metric for two other clusteringstrategies, random clustering and nice proposedby he et al.
(2018), for comparison and show theresults in table 5. we see that measured by mutualinformation, clusters produced by our clusteringstrategy is much more coherent with dependenciesthan the other strategies..7 conclusion.
in this paper, we propose a method to adapt unsu-pervised syntactic parsing methods for discoursedependency parsing.
first, we cluster the elementdiscourse units (edu) to produce clusters resem-bling pos tags.
second, we modify the eisneralgorithm used for ﬁnding the optimal parse treewith hierarchical constraint.
we apply the adap-tations to two unsupervised syntactic dependencyparsing methods.
experimental results show thatour method successfully adapts the two models fordiscourse dependency parsing, which demonstrateadvantages in both parsing accuracy and runningefﬁciency..acknowledgment.
this work was supported by the national naturalscience foundation of china (61976139)..references.
waleed ammar, chris dyer, and noah a smith.
2014.conditional random ﬁeld autoencoders for unsuper-vised structured prediction.
in advances in neuralinformation processing systems, volume 27, pages3311–3319.
curran associates, inc..5790parminder bhatia, yangfeng ji, and jacob eisenstein.
2015. better document-level sentiment analysisin proceedings offrom rst discourse parsing.
the 2015 conference on empirical methods in nat-ural language processing, pages 2212–2218, lis-bon, portugal.
association for computational lin-guistics..francisco guzm´an, shaﬁq joty, llu´ıs m`arquez, andpreslav nakov.
2014. using discourse structure im-proves machine translation evaluation.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 1: long pa-pers), pages 687–698, baltimore, maryland.
associ-ation for computational linguistics..chlo´e braud, maximin coavoux, and anders søgaard.
2017. cross-lingual rst discourse parsing.
in pro-ceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics: volume 1, long papers, pages 292–304,valencia, spain.
association for computational lin-guistics..jill burstein, joel tetreault, and martin chodorow.
2013.holistic discourse coherence annotationfor noisy essay writing.
dialogue & discourse,4(2):34–52..jiong cai, yong jiang, and kewei tu.
2017. crfautoencoder for unsupervised dependency parsing.
in proceedings of the 2017 conference on empiri-cal methods in natural language processing, pages1638–1643, copenhagen, denmark.
association forcomputational linguistics..lynn carlson, daniel marcu,.
and mary ellenokurovsky.
2001. building a discourse-tagged cor-pus in the framework of rhetorical structure theory.
in proceedings of the second sigdial workshop ondiscourse and dialogue..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..timothy dozat and christopher d. manning.
2017.deep biafﬁne attention for neural dependency pars-in 5th international conference on learninging.
representations, iclr 2017, toulon, france, april24-26, 2017, conference track proceedings.
open-review.net..jason m. eisner.
1996. three new probabilistic modelsin col-for dependency parsing: an exploration.
ing 1996 volume 1: the 16th international confer-ence on computational linguistics..vanessa wei feng and graeme hirst.
2014. a linear-time bottom-up discourse parser with constraintsin proceedings of the 52nd an-and post-editing.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 511–521, baltimore, maryland.
association for compu-tational linguistics..wenjuan han, yong jiang, and kewei tu.
2019. en-hancing unsupervised generative dependency parserwith contextual information.
in proceedings of the57th annual meeting of the association for com-putational linguistics, pages 5315–5325, florence,italy.
association for computational linguistics..junxian he, graham neubig,.
and taylor berg-kirkpatrick.
2018. unsupervised learning of syn-tactic structure with invertible neural projections.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 1292–1302, brussels, belgium.
associationfor computational linguistics..tsutomu hirao, yasuhisa yoshida, masaaki nishino,norihito yasuda, and masaaki nagata.
2013. single-document summarization as a tree knapsack prob-in proceedings of the 2013 conference onlem.
empirical methods in natural language processing,pages 1515–1520, seattle, washington, usa.
asso-ciation for computational linguistics..patrick huber and giuseppe carenini.
2020. unsuper-vised learning of discourse structures using a tree au-toencoder.
arxiv preprint arxiv:2012.09446..yangfeng ji and jacob eisenstein.
2014. representa-tion learning for text-level discourse parsing.
in pro-ceedings of the 52nd annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 13–24, baltimore, maryland.
associ-ation for computational linguistics..yong jiang, wenjuan han, and kewei tu.
2016. un-supervised neural dependency parsing.
in proceed-ings of the 2016 conference on empirical methodsin natural language processing, pages 763–771,austin, texas.
association for computational lin-guistics..shaﬁq joty, giuseppe carenini, and raymond t. ng.
2015. codra: a novel discriminative frameworkfor rhetorical analysis.
computational linguistics,41(3):385–435..shaﬁq joty, francisco guzm´an, llu´ıs m`arquez, andpreslav nakov.
2014. discotk: using discoursestructure for machine translation evaluation.
in pro-ceedings of the ninth workshop on statistical ma-chine translation, pages 402–408, baltimore, mary-land, usa.
association for computational linguis-tics..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,.
5791iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..dan klein and christopher manning.
2004. corpus-based induction of syntactic structure: models of de-in proceedings of thependency and constituency.
42nd annual meeting of the association for com-putational linguistics (acl-04), pages 478–485,barcelona, spain..naoki kobayashi, tsutomu hirao, kengo naka-mura, hidetaka kamigaito, manabu okumura, andmasaaki nagata.
2019. split or merge: which isbetter for unsupervised rst parsing?
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 5797–5802, hongkong, china.
association for computational lin-guistics..sujian li, liang wang, ziqiang cao, and wenjie li.
2014. text-level discourse dependency parsing.
inproceedings of the 52nd annual meeting of the as-sociation for computational linguistics (volume 1:long papers), pages 25–35, baltimore, maryland.
association for computational linguistics..zhao li and kewei tu.
2020. unsupervised cross-lingual adaptation of dependency parsers using crfin findings of the association forautoencoders.
computational linguistics: emnlp 2020, pages2127–2133, online.
association for computationallinguistics..stuart lloyd.
1982. least squares quantization inieee transactions on information theory,.
pcm.
28(2):129–137..william c mann and sandra a thompson.
1988.rhetorical structure theory: toward a functional the-ory of text organization.
text, 8(3):243–281..daniel marcu.
2000. the theory and practice of dis-course parsing and summarization.
mit press..daniel marcu, estibaliz amorrortu, and magdalenaromera.
1999. experiments in constructing a cor-in towards standards andpus of discourse trees.
tools for discourse tagging..eleni miltsakaki and karen kukich.
2004. evaluationof text coherence for electronic essay scoring sys-tems.
natural language engineering, 10(1):25..mathieu morey, philippe muller, and nicholas asher.
2018. a dependency perspective on rst discourseparsing and evaluation.
computational linguistics,44(2):197–235..kosuke nishida, kyosuke nishida,.
itsumi saito,hisako asano, and junji tomita.
2020. unsuper-vised domain adaptation of language models forreading comprehension.
in proceedings of the 12thlanguage resources and evaluation conference,pages 5392–5399, marseille, france.
european lan-guage resources association..noriki nishida and hideki nakayama.
2020. unsuper-vised discourse constituency parsing using viterbiem.
transactions of the association for computa-tional linguistics, 8:215–230..joakim nivre, johan hall, and jens nilsson.
2004.memory-based dependency parsing.
in proceedingsof the eighth conference on computational naturallanguage learning (conll-2004) at hlt-naacl2004, pages 49–56, boston, massachusetts, usa.
association for computational linguistics..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..shubham toshniwal, haoyue shi, bowen shi, lingyugao, karen livescu, and kevin gimpel.
2020. across-task analysis of text span representations.
inproceedings of the 5th workshop on representationlearning for nlp, pages 166–176, online.
associa-tion for computational linguistics..yizhong wang, sujian li, and houfeng wang.
2017.a two-stage parsing method for text-level discourseanalysis.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 2: short papers), pages 184–188, vancou-ver, canada.
association for computational linguis-tics..an yang and sujian li.
2018. scidtb: discourse de-pendency treebank for scientiﬁc abstracts.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 2: shortpapers), pages 444–449, melbourne, australia.
as-sociation for computational linguistics..yasuhisa yoshida, jun suzuki, tsutomu hirao, andmasaaki nagata.
2014. dependency-based dis-course parser for single-document summarization.
in proceedings ofthe 2014 conference on em-pirical methods in natural language processing(emnlp), pages 1834–1839, doha, qatar.
associ-ation for computational linguistics..5792a experimental details for ourncrfae and v-dndmv.
we implement our ncrfae and v-dndmv mod-els by pytorch 1.6 and python 3.8.3. we run ourexperiments on a server with intel(r) xeon(r)gold 5115 cpu and nvidia titan v gpu.
basedon these software and hardware environments,our ncrfae and v-dndmv models trained onthe scidtb dataset use about 30 and 45 min-utes, respectively.
moreover, our ncrfae andv-dndmv models trained on the rst-dt datasetuse about 4 and 18 hours, respectively.
the numberof parameters in ncrfae is about 8.26 million,and the number of parameters in v-dndmv is0.47 million.
the hyperparameter conﬁgurationsof the result report in our paper are shown in table 6.we choose the hyperparameter conﬁgurations bymanual tuning and the uas score on the develop-ment dataset is used to select among them.
due tothe lack of development set of rst-dt, we preparea development set with 20 instances randomly sam-pled from the training set.
the size of each datasetis shown in table 7..ncrfae v-dndmv.
50.
50.clustercluster numberhidden layeredu embeddingcluster embeddingvalence embeddingfnn(embedding)bi-lstmlstm dropoutfnn(head)fnn(dep)fnn dropoutoptimizer & losslearning rateadam beta 1adam beta 2l2reg.
1536--1*2001*4000.331*5001*2000.33.
2e-30.90.91e-4.
153620201*2001*320.0--0.3.
1e-30.90.9990.0.table 6: hyper-parameters for our ncrfae and v-dndmv..usage doc..edu.
traintesttraindev.
test.
34738742152151.
1944323461046720182013.relationtype.
19.
17.rst-dt.
scidtb.
table 7: size of rst-dt and scidtb.
here the relationtype is coarse-grained relation..b full heat-maps.
5793(a) head.
figure 8: heat-maps of probabilities that relations use different label as dependency head (a) or child (b)..(b) child.
5794