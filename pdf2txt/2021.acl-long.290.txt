surprisal estimators for human reading times need character models.
byung-doh oh.
christian clarkdepartment of linguisticsthe ohio state university{oh.531, clark.3664, schuler.77}@osu.edu.
william schuler.
abstract.
while the use of character models has beenpopular in nlp applications, it has not beenexplored much in the context of psycholin-guistic modeling.
this paper presents a char-acter model that can be applied to a struc-tural parser-based processing model to calcu-late word generation probabilities.
experimen-tal results show that surprisal estimates froma structural processing model using this char-acter model deliver substantially better ﬁts toself-paced reading, eye-tracking, and fmridata than those from large-scale language mod-els trained on much more data.
this may sug-gest that the proposed processing model pro-vides a more humanlike account of sentenceprocessing, which assumes a larger role ofmorphology, phonotactics, and orthographiccomplexity than was previously thought..1.introduction and related work.
expectation-based theories of sentence processing(hale, 2001; levy, 2008) posit that processing dif-ﬁculty is determined by predictability in context.
in support of this position, predictability quantiﬁedthrough surprisal has been shown to correlate withbehavioral measures of word processing difﬁculty(goodkind and bicknell, 2018; hale, 2001; levy,2008; shain, 2019; smith and levy, 2013).
how-ever, surprisal itself makes no representational as-sumptions about sentence processing, leaving openthe question of how best to estimate its underlyingprobability model..in natural language processing (nlp) applica-tions, the use of character models has been popularfor several years (al-rfou et al., 2019; kim et al.,2016; lee et al., 2017).
character models havebeen shown not only to alleviate problems without-of-vocabulary words but also to embody mor-phological information available at the subwordlevel.
for this reason, they have been extensively.
used to model morphological processes (elsneret al., 2019; kann and schütze, 2016) or incor-porate morphological information into models ofsyntactic acquisition (jin et al., 2019).
nonethe-less, the use of character models has been slow tocatch on in psycholinguistic surprisal estimation,which has recently focused on evaluating large-scale language models that make predictions at theword level (e.g.
futrell et al.
2019; goodkind andbicknell 2018; hale et al.
2018; hao et al.
2020).
this raises the question of whether incorporatingcharacter-level information into an incremental pro-cessing model will result in surprisal estimates thatbetter characterize predictability in context..to answer this question, this paper presents acharacter model that can be used to estimate wordgeneration probabilities in a structural parser-basedprocessing model.1 the proposed model deﬁnes aprocess of generating a word from an underlyinglemma and a morphological rule, which allows theprocessing model to capture the predictability of agiven word form in a ﬁne-grained manner.
regres-sion analyses on self-paced reading, eye-tracking,and fmri data demonstrate that surprisal estimatescalculated from this character-based structural pro-cessing model contribute to substantially better ﬁtscompared to those calculated from large-scale lan-guage models, despite the fact that these other mod-els are trained on much more data and show lowerperplexities on test data.
this ﬁnding deviates fromthe monotonic relationship between test perplexityand predictive power observed in previous studies(goodkind and bicknell, 2018; wilcox et al., 2020).
furthermore, it suggests that the character-basedstructural processing model may provide a morehumanlike account of processing difﬁculty and maysuggest a larger role of morphology, phonotactics,and orthographic complexity than was previously.
1code for model and experiments is available at https:.
//github.com/byungdoh/acl21_semproc..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3746–3757august1–6,2021.©2021associationforcomputationallinguistics3746thought..2 background.
the experiments presented in this paper use sur-prisal predictors (shannon, 1948) calculated byan incremental processing model based on a left-corner parser (johnson-laird, 1983; van schijndelet al., 2013).
this incremental processing modelprovides a probabilistic account of sentence pro-cessing by making a single lexical attachment deci-sion and a single grammatical attachment decisionfor each input word..surprisal.
surprisal can be deﬁned as the neg-ative log ratio of preﬁx probabilities of word se-quences w1..t at consecutive time steps t − 1 and t:.
s(wt).
def= − log.
p(w1..t)p(w1..t−1).
(1).
these preﬁx probabilities can be calculated bymarginalizing over the hidden states qt of the for-ward probabilities of an incremental processingmodel:.
p(w1..t) =.
p(w1..t qt).
(2).
(cid:88).
qt.
these forward probabilities are in turn deﬁned re-cursively using a transition model:.
p(w1..t qt).
def=.
p(wt qt | qt−1) · p(w1..t−1 qt−1).
(cid:88).
qt−1.
(3)left-corner parsing.
the transition model pre-sented in this paper is based on a probabilistic left-corner parser (johnson-laird, 1983; van schijndelet al., 2013).
left-corner parsers have been usedto model human sentence processing because theydeﬁne a ﬁxed number of decisions at every timestep and also require only a bounded amount ofworking memory, in keeping with experimental ob-servations of human memory limits (miller andisard, 1963).
the transition model maintains adistribution over possible working memory storestates qt at every time step t, each of which con-sists of a bounded number d of nested derivationfragments adt .
each derivation fragment spansa part of a derivation tree from some apex node adtlacking a base node bdt yet to come.
previous workhas shown that large annotated corpora such as thepenn treebank (marcus et al., 1993) do not requiremore than d = 4 of such fragments (schuler et al.,2010)..t /bd.
at each time step, a left-corner parsing modelgenerates a new word wt and a new store state qt.
in two phases (see figure 1).
first, it makes alexical decision (cid:96)t regarding whether to use theword to complete the most recent derivation frag-ment (match), or to use the word to create a newpreterminal node a(cid:96)t (no-match).
subsequently, themodel makes a grammatical decision gt regardingwhether to use a predicted grammar rule to combinethe node constructed in the lexical phase a(cid:96)t withthe next most recent derivation fragment (match),or to use the grammar rule to convert this node intoa new derivation fragment agt /bgt (no-match):2(cid:88).
p(wt qt | qt−1) =.
(cid:96)t,gt.
p((cid:96)t | qt−1) ·p(wt | qt−1 (cid:96)t) ·p(gt | qt−1 (cid:96)t wt) ·p(qt | qt−1 (cid:96)t wt gt).
(4).
thus, the parser creates a hierarchically organizedsequence of derivation fragments and joins thesefragments up whenever expectations are satisﬁed.
in order to update the store state based on thelexical and grammatical decisions, derivation frag-ments above the most recent nonterminal node arecarried forward, and derivation fragments below itare set to null (⊥):.
.
t.t.t.(cid:75).
(cid:75).
t−1.
def=.
d(cid:48)=1.
d(cid:89).
.
p(qt | .
.
.).
t , bd(cid:48)ad(cid:48)(cid:74)ad(cid:48)t , bd(cid:48)(cid:74)t , bd(cid:48)ad(cid:48)(cid:74).
t−1, bd(cid:48)= ad(cid:48)if d(cid:48) < d= agt , bgtif d(cid:48) = d= ⊥, ⊥if d(cid:48) > d(cid:75)(5)= 1 if ϕ is truewhere the indicator functionand 0 otherwise, and d = argmaxd(cid:48){ad(cid:48)(cid:44)⊥} + 1 −t−1m(cid:96)t − mgt .
together, these probabilistic decisionsgenerate the n unary branches and n − 1 binarybranches of a parse tree in chomsky normal formfor an n-word sentence..ϕ(cid:75)(cid:74).
3 model.
3.1 processing model.
the processing model extends the above left-cornerparser to maintain lemmatized predicate informa-tion by augmenting each preterminal, apex, andbase node to consist not only of a syntactic cate-gory label cpt , cad, but also of a binary pred-∈ {0, 1}k+v·k,icate context vector hpt , hadwhere k is the size of the set of predicate contextsand v is the maximum valence of any syntactic.
, or hbd.
, or cbd.
t.t.t.t.2johnson-laird (1983) refers to lexical and grammatical.
decisions as ‘shift’ and ‘predict’ respectively..3747bd−m(cid:96)tt−1.
bgt.
(7).
(8).
a) lexical decision (cid:96)t.b) grammatical decision gt.
a(cid:96)t.adt−1.
wt.
⇒.
bdt−1.
adt−1.
wt.
⇒.
bdt−1.
adt−1.
a(cid:96)t.bdt−1.
ad−m(cid:96)tt−1.
agt.
ad−m(cid:96)tt−1.
ad−m(cid:96)tt−1.
⇒bd−m(cid:96)tt−1.
a(cid:96)t.⇒bd−m(cid:96)tt−1.
bgt.
a(cid:96)t.agt.
m(cid:96)t.= 1.m(cid:96)t.= 0.mgt.
= 1.mgt.
= 0.figure 1: left-corner parser operations: a) lexical match (m(cid:96)t=1) and no-match (mgtapex a(cid:96)t , and b) grammatical match (mgt.
=1) and no-match (m(cid:96)t=0) operations, creating new=0) operations, creating new apex agt and base bgt ..category.3 each 0 or 1 element of this vector rep-resents a unique predicate context, which consistsof a (cid:104)predicate, role(cid:105) pair that speciﬁes the con-tent constraints of a node in a predicate-argumentstructure.
these predicate contexts are obtained byreannotating the training corpus using a general-ized categorial grammar of english (nguyen et al.,2012),4 which is sensitive to syntactic valence andnon-local dependencies..lexical decisions.
each lexical decision of theparser includes a match decision m(cid:96)t and decisionsabout a syntactic category c(cid:96)t and a predicate con-text vector h(cid:96)t that together specify a preterminalnode p(cid:96)t .
the probability of generating the matchdecision and the predicate context vector dependson the base node bdt−1 of the previous derivationfragment (i.e.
its syntactic category and predicatecontext vector).
the ﬁrst term of equation 4 cantherefore be decomposed into the following:.
p((cid:96)t | qt−1) = softmax.
( ffθl[δd.
m(cid:96)t h(cid:96)t.(cid:62), [δ(cid:62)cbdt−1.
, h(cid:62)bdt−1.]
el] ) ·.
p(c(cid:96)t | qt−1 m(cid:96)t h(cid:96)t ).
(6).
where ff is a feedforward neural network, andδi is a kronecker delta vector consisting of aone at element i and zeros elsewhere.
depthd = argmaxd(cid:48){ad(cid:48)(cid:44)⊥} is the number of non-nullt−1derivation fragments at the previous time step, andel is a matrix of jointly trained dense embeddingsfor each syntactic category and predicate context.
the syntactic category and predicate context vector.
3the valence of a category is the number of unsatisﬁedsyntactic arguments it has.
separate vectors for syntacticarguments are needed in order to correctly model cases such aspassives where syntactic arguments do not align with predicatearguments..4the predicates in this annotation scheme come fromwords that have been lemmatized by a set of rules that havebeen manually written and corrected in order to account forcommon irregular inﬂections..together deﬁne a complete preterminal node p(cid:96)t foruse in the word generation model:.
def=.
p(cid:96)t..
, hbd.
t−1.
cbdc(cid:96)t , h(cid:96)t.t−1.
+ h(cid:96)t.if m(cid:96)tif m(cid:96)t.= 1= 0.and a new apex node a(cid:96)t for use in the grammaticaldecision model:.
def=.
a(cid:96)t..
adt−1p(cid:96)t.if m(cid:96)tif m(cid:96)t.= 1= 0.grammatical decisions.
each grammatical de-cision includes a match decision mgt and decisionsabout a pair of syntactic category labels cgt and c(cid:48)gt ,as well as a predicate context composition oper-ator ogt , which governs how the newly generatedpredicate context vector h(cid:96)t is propagated throughits new derivation fragment agt /bgt .
the probabilityof generating the match decision and the compo-sition operators depends on the base node bd−m(cid:96)tof the previous derivation fragment and the apexnode a(cid:96)t from the current lexical decision (i.e.
theirsyntactic categories and predicate context vectors).
the third term of equation 4 can accordingly bedecomposed into the following:p(gt | qt−1 (cid:96)t wt) =( ffθg[δdsoftmaxmgt ogt.
(cid:62), [δ(cid:62)cb.]
eg] ) ·.
, δ(cid:62)ca(cid:96)t., h(cid:62)a(cid:96)t., h(cid:62)d−m(cid:96)tbt−1.
t−1.
d−m(cid:96)tt−1.
p(cgt | qt−1 (cid:96)t wt mgt ogt ) ·p(c(cid:48)gt.
| qt−1 (cid:96)t wt mgt ogt cgt ).
(9).
where eg is a matrix of jointly trained dense em-beddings for each syntactic category and predicatecontext.
the composition operators are associatedwith sparse composition matrices aogt which canbe used to compose predicate context vectors asso-ciated with the apex node agt :.
def=.
agt.
ad−m(cid:96)tt−1cgt , aogt ha(cid:96)t.if mgtif mgt.
= 1= 0.
(10).
3748and sparse composition matrices bogt which can beused to compose predicate context vectors associ-ated with the base node bgt :.
def=.
bgt.
.
c(cid:48)gt , bogt [hd−m(cid:96)tbt−1gt , bogt [0(cid:62), ha(cid:96)tc(cid:48).
(cid:62), ha(cid:96)t(cid:62)](cid:62).
(cid:62)](cid:62) if mgt.
=1.
if mgt.
=0.
(11).
3.2 character-based word model.
|the baseline version of the word model p(wtqt−1 (cid:96)t) uses relative frequency estimation withbackoff probabilities for out-of-vocabulary wordstrained using hapax legomena.
a character-basedtest version of this model instead applies a mor-phological rule rt to a lemma xt to generate aninﬂected form wt.
the set of rules model afﬁxa-tion through string substitution and are inverses oflemmatization rules that are used to derive predi-cates in the generalized categorial grammar anno-tation (nguyen et al., 2012).
for example, the rule%ay→%aid can apply to the word say to deriveits past tense form said.
there are around 600 suchrules that account for inﬂection in sections 02 to21 of the wall street journal corpus of the penntreebank (marcus et al., 1993), which includes anidentity rule for words in bare form and a ‘no se-mantics’ rule for generating certain function words.
for an observed input word wt, the model ﬁrstgenerates a list of (cid:104)xt, rt(cid:105) pairs that deterministicallygenerate wt.
this allows the model to capture mor-phological regularity and estimate how expected aword form is given its predicted syntactic categoryand predicate context, which have been generatedas part of the preceding lexical decision.
in addi-tion, this lets the model hypothesize the underly-ing morphological structure of out-of-vocabularywords and assign probabilities to them.
the secondterm of equation 4 can thus be decomposed intothe following:.
p(wt | qt−1 (cid:96)t) =.
(cid:88).
xt,rt.
p(xt | qt−1 (cid:96)t) ·p(rt | qt−1 (cid:96)t xt) ·p(wt | qt−1 (cid:96)t xt rt).
(12).
the probability of generating the lemma sequencedepends on the syntactic category cp(cid:96)t and predicatecontext h(cid:96)t resulting from the preceding lexicaldecision (cid:96)t:.
p(xt | qt−1 (cid:96)t) =.
softmaxxt,i.
( wx xt,i + bx ).
(cid:89).
i.
(13).
where xt,1, xt,2, ..., xt,i is the character sequence oflemma xt, with xt,1 = (cid:104)s(cid:105) and xt,i = (cid:104)e(cid:105) as specialstart and end characters.
wx and bx are respec-tively a weight matrix and bias vector of a softmaxclassiﬁer.
a recurrent neural network (rnn) calcu-lates a hidden state xt,i for each character from aninput vector at that time step and the hidden stateafter the previous character xt,i−1:.
xt,i = rnnθx( [δ(cid:62)cp(cid:96)t., h(cid:62)(cid:96)t., δ(cid:62).
xt,i] ex, x(cid:62).
t,i−1 ).
(14).
where ex is a matrix of jointly trained dense em-beddings for each syntactic category, predicate con-text, and character..subsequently, the probability of applying a par-ticular morphological rule to the generated lemmadepends on the syntactic category cp(cid:96)t and predi-cate context h(cid:96)t from the preceding lexical decisionas well as the character sequence of the lemma:.
p(rt | qt−1 (cid:96)t xt) = softmax.
( wr rt,i + br ) (15).
rt.
where wr and br are respectively a weight matrixand bias vector of a softmax classiﬁer.
rt,i is thelast hidden state of an rnn that takes as input thesyntactic category, predicate context, and charactersequence of the lemma xt,2, xt,3, ..., xt,i−1 withoutthe special start and end characters:.
rt,i = rnnθr( [δ(cid:62)cp(cid:96)t., h(cid:62)(cid:96)t., δ(cid:62).
xt,i] er, r(cid:62).
t,i−1 ).
(16).
where er is a matrix of jointly trained dense em-beddings for each syntactic category, predicate con-text, and character..finally, as the model calculates probabilitiesonly for (cid:104)xt, rt(cid:105) pairs that deterministically gener-ate wt, the word probability conditioned on thesevariables p(wt | qt−1 (cid:96)t xt rt) is deterministic..4 experiment 1: effect of character.
model.
in order to assess the inﬂuence of the character-based word generation model over the baselineword generation model on the predictive qualityof surprisal estimates, linear mixed-effects modelscontaining common baseline predictors and one ormore surprisal predictors were ﬁtted to self-pacedreading times.
subsequently, a series of likelihoodratio tests were conducted in order to evaluate therelative contribution of each surprisal predictor toregression model ﬁt..37494.1 response data.
the ﬁrst experiment described in this paper usedthe natural stories corpus (futrell et al., 2018),which contains self-paced reading times from 181subjects that read 10 naturalistic stories consist-ing of 10,245 tokens.
the data were ﬁltered toexclude observations corresponding to sentence-initial and sentence-ﬁnal words, observations fromsubjects who answered fewer than four compre-hension questions correctly, and observations withdurations shorter than 100 ms or longer than 3000ms. this resulted in a total of 768,584 observa-tions, which were subsequently partitioned intoan exploratory set of 383,906 observations and aheld-out set of 384,678 observations.
the partition-ing allows model selection (e.g.
making decisionsabout predictors and random effects structure) tobe conducted on the exploratory set and a singlehypothesis test to be conducted on the held-out set,thus eliminating the need for multiple trials correc-tion.
all observations were log-transformed priorto model ﬁtting..4.2 predictors.
the baseline predictors commonly included in allregression models are word length measured incharacters and index of word position within eachsentence.5 in addition to the baseline predictors,surprisal predictors were calculated from two vari-ants of the processing model in which word gen-eration probabilities p(wt | qt−1 (cid:96)t) are calculatedusing relative frequency estimation (freqwsurp)and using the character-based model described insection 3.2 (charwsurp).
both variants of theprocessing model were trained on a generalizedcategorial grammar (nguyen et al., 2012) reannota-tion of sections 02 to 21 of the wall street journal(wsj) corpus of the penn treebank (marcus et al.,1993).
beam search decoding with a beam size of5,000 was used to estimate preﬁx probabilities andsurprisal predictors for both variants..to account for the time the brain takes to pro-cess and respond to linguistic input, it is standardpractice in psycholinguistic modeling to include‘spillover’ variants of predictors from precedingwords (rayner et al., 1983; vasishth, 2006).
how-ever, as including multiple spillover variants ofpredictors leads to identiﬁability issues in mixed-.
5although unigram surprisal or 5-gram surprisal is alsocommonly included as a baseline predictor, it was not includedin this experiment due to convergence issues..model comparison χ2.
full vs. no charwsurpfull vs. no freqwsurp.
204.480.024.p-value0.0001∗∗∗0.8779.table 1: likelihood ratio test evaluating the contribu-tion of charwsurp and freqwsurp in predicting self-paced reading times from the natural stories corpus..effects modeling (shain and schuler, 2019), char-wsurp and freqwsurp were both spilled over byone position.
all predictors were centered andscaled prior to model ﬁtting, and all regressionmodels included by-subject random slopes for allﬁxed effects as well as random intercepts for eachword and subject-sentence interaction, followingthe convention of keeping the random effects struc-ture maximal in psycholinguistic modeling (barret al., 2013)..4.3 likelihood ratio testing.
a total of three linear mixed-effects models wereﬁtted to reading times in the held-out set usinglme4 (bates et al., 2015); the full model includedthe ﬁxed effects of both charwsurp and freqw-surp, and the two ablated models included the ﬁxedeffect of either charwsurp or freqwsurp.
this re-sulted in two pairs of nested models whose ﬁt couldbe compared through a likelihood ratio test (lrt).
the ﬁrst lrt tested the contribution of charwsurpby comparing the ﬁt of the full regression modelto that of the regression model without the ﬁxedeffect of charwsurp.
similarly, the second lrttested the contribution of freqwsurp by comparingthe ﬁt of the full regression model to that of theregression model without its ﬁxed effect..4.4 results.
the results in table 1 show that the contribution ofcharwsurp in predicting reading times is statisti-cally signiﬁcant over and above that of freqwsurp(p < 0.0001), while the converse is not signiﬁcant(p = 0.8779).
this demonstrates that incorporat-ing a character-based word generation model tothe structural processing model better captures pre-dictability in context, subsuming the effects of theprocessing model without it..5 experiment 2: comparison to other.
models.
to further examine the impact of the character-based word generation model, charwsurp and fre-.
3750qwsurp were evaluated against surprisal predictorscalculated from a number of other large-scale pre-trained language models and smaller parser-basedmodels.
to compare the predictive power of sur-prisal estimates from different language models onequal footing, we calculated the increase in log-likelihood (∆ll) to a baseline regression model asa result of including a surprisal predictor, followingrecent work (goodkind and bicknell, 2018; haoet al., 2020)..5.1 surprisal estimates from other models.
a total of three pretrained language models wereused to calculate surprisal estimates at each word.6.
• glstmsurp (gulordava et al., 2018): a two-layer lstm model trained on ∼80m tokens ofthe english wikipedia..• jlstmsurp (jozefowicz et al., 2016): a two-layer lstm model with cnn character inputstrained on ∼800m tokens of the 1b word bench-mark (chelba et al., 2014)..• gpt2surp (radford et al., 2019): gpt-2 xl, a48-layer decoder-only transformer model trainedon the webtext dataset (∼8m web documents)..in addition, three incremental parsing models.
were used to calculate surprisal estimates:.
• rnngsurp (hale et al., 2018; dyer et al., 2016):an lstm-based model with explicit phrasestructure, trained on sections 02 to 21 of thewsj corpus..• vslcsurp (van schijndel et al., 2013): a left-corner parser based on a pcfg with subcatego-rized syntactic categories (petrov et al., 2006),trained on a generalized categorial grammar rean-notation of sections 02 to 21 of the wsj corpus..• jlcsurp (jin and schuler, 2020): a neural left-corner parser based on stack lstms (dyer et al.,2015), trained on sections 02 to 21 of the wsjcorpus..5.2 procedures.
the set of self-paced reading times from the nat-ural stories corpus after applying the same dataexclusion criteria as experiment 1 provided theresponse variable for the regression models.
in ad-dition to the full dataset, regression models were.
6please refer to the appendix for surprisal calculation, out-.
of-vocabulary handling, and re-initialization procedures..also ﬁtted to a ‘no out-of-vocabulary (no-oov)’version of the dataset, in which observations cor-responding to out-of-vocabulary words for thelstm language model with the smallest vocab-ulary (i.e.
gulordava et al., 2018) were also ex-cluded.
this exclusion criterion was included inorder to avoid putting the lstm language mod-els that may have unreliable surprisal estimates forout-of-vocabulary words at an unfair disadvantage.
this resulted in a total of 744,607 observationsin the no-oov dataset, which were subsequentlypartitioned into an exploratory set of 371,937 obser-vations and a held-out set of 372,670 observations.
all models were ﬁtted to the held-out set, and allobservations were log-transformed prior to modelﬁtting..the predictors included in the baseline linearmixed-effects model were word length, word posi-tion in sentence, and unigram surprisal.
unigramsurprisal was calculated using the kenlm toolkit(heaﬁeld et al., 2013) with parameters trained onthe gigaword 4 corpus (parker et al., 2009).
inorder to calculate the increase in log-likelihood(∆ll) attributable to each surprisal predictor, a‘full’ linear-mixed effects model, which includesone surprisal predictor on top of the baseline model,was ﬁtted for each surprisal predictor.
as with ex-periment 1, the surprisal predictors were spilledover by one position.
all predictors were centeredand scaled prior to model ﬁtting, and all regressionmodels included by-subject random slopes for allﬁxed effects and random intercepts for each wordand subject-sentence interaction..additionally, in order to examine whether any ofthe models fail to generalize across domains, theirperplexity on the entire natural stories corpus wasalso calculated..5.3 results.
the results show that surprisal from the character-based structural model (charwsurp) made thebiggest contribution to model ﬁt compared to sur-prisal from other models on both full and no-oovsets of self-paced reading times (figure 2; the dif-ference between the model with charwsurp andother models is signiﬁcant with p < 0.001 by apaired permutation test using by-item errors).
theexclusion of oov words did not make a notabledifference in the overall trend of ∆ll across mod-els.
this ﬁnding, despite the fact that the pre-trained language models were trained on much.
3751(a) baseline ll: -20445.4.
(b) baseline ll: -17485.2.figure 2: perplexity measures from each model, andimprovements in regression model log-likelihood fromincluding each surprisal estimate on natural storiesself-paced reading data..figure 3: residual error from the regression modelwith gpt2surp and change in error from the regres-sion model with charwsurp.
circle widths show thefrequency of each syntactic category in the natural sto-ries self-paced reading data..larger datasets and also show lower perplexitieson test data,7 suggests that this model may providea more humanlike account of processing difﬁculty.
in other words, accurately predicting the next wordalone does not fully explain humanlike processingcosts that manifest in self-paced reading times.
theanalysis of residuals grouped by the lowest basecategory of the previous time step (cbd) from man-ual annotations (shain et al., 2018) shows that theimprovement of charwsurp over gpt2surp wasbroad-based across categories (see figure 3)..t−1.
6 experiment 3: eye-tracking data.
in order to examine whether these results general-ize to other latency-based measures, linear-mixedeffects models were ﬁtted on the dundee eye-tracking corpus (kennedy et al., 2003) to test thecontribution of each surprisal predictor, followingsimilar procedures to experiment 2..6.1 procedures.
the set of go-past durations from the dundee cor-pus (kennedy et al., 2003) provided the response.
7perplexity of the parsing models is higher partly because.
they optimize for a joint distribution over words and trees..variable for the regression models.
the dundeecorpus contains gaze durations from 10 subjectsthat read 20 newspaper editorials consisting of51,502 tokens.
the data were ﬁltered to excludeunﬁxated words, words following saccades longerthan four words, and words at starts and ends ofsentences, screens, documents, and lines.
thisresulted in the full set with a total of 195,296 obser-vations, which were subsequently partitioned intoan exploratory set of 97,391 observations and aheld-out set of 97,905 observations.
as with exper-iment 2, regression models were also ﬁtted to a nooov version of the dataset, in which observationscorresponding to out-of-vocabulary words for thegulordava et al.
(2018) model were also excluded.
this resulted in a subset with a total of 184,894 ob-servations (exploratory set of 92,272 observations,held-out set of 92,622 observations).
all modelswere ﬁtted to the held-out set, and all observationswere log-transformed prior to model ﬁtting..the predictors included in the baseline linearmixed-effects models were word length, word po-sition, and saccade length.
in order to calculatethe increase in log-likelihood from including eachsurprisal predictor, a full model including one sur-.
3752(a) baseline ll: -65100.6.
(b) baseline ll: -60807.5.figure 4: perplexity measures from each model,and improvements in regression model log-likelihoodfrom including each surprisal estimate on dundee eye-tracking data..figure 5: residual error from the regression modelwith gpt2surp and change in error from the regres-sion model with charwsurp.
circle widths show thefrequency of each syntactic category in the dundee eye-tracking data..prisal predictor on top of the baseline model wasﬁtted for each surprisal predictor.
all surprisal pre-dictors were spilled over by one position, and allpredictors were centered and scaled prior to modelﬁtting.
all regression models included by-subjectrandom slopes for all ﬁxed effects and random in-tercepts for each word and sentence..6.2 results.
the results in figure 4 show that as with experi-ment 2, surprisal from the character-based struc-tural model (charwsurp) made the biggest contri-bution to model ﬁt on both full and no-oov setsof go-past durations (the difference between modelwith charwsurp and other models is signiﬁcantwith p < 0.001 by a paired permutation test us-ing by-item errors).
in contrast to natural stories,surprisal from the two left-corner parsing models(i.e.
vslcsurp and jlcsurp) did not contribute toas much model ﬁt compared to other models.
theexclusion of oov words again did not make a no-table difference in the general trend across differentmodels, although it led to an increase in ∆ll forglstmsurp and rnngsurp.
residuals groupedby the lowest base category from the previous time.
step show that, similarly to natural stories, theimprovement of charwsurp over gpt2surp wasbroad-based across different categories (see fig-ure 5).
these results provide further support for theobservation that language models that are trainedto predict the next word accurately do not fully ex-plain processing cost in the form of latency-basedmeasures..7 experiment 4: fmri data.
finally, to examine whether a similar tendencyis observed in brain responses, we analyzed thetime series of blood oxygenation level-dependent(bold) signals in the language network, whichwere identiﬁed using functional magnetic reso-nance imaging (fmri).
to this end, the novel sta-tistical framework of continuous-time deconvolu-tional regression (cdr; shain and schuler, 2019)was employed.
as cdr allows the data-driven es-timation of continuous impulse response functionsfrom variably spaced linguistic input, it is moreappropriate for modeling fmri responses, whichare typically measured in ﬁxed time intervals.
sim-ilarly to the previous experiments, the increase incdr model log-likelihood as a result of including a.
3753surprisal predictor on top of a baseline cdr modelwas calculated for evaluation..7.1 procedures.
this experiment used the same fmri data usedby shain et al.
(2019), which were collected from78 subjects that listened to a recorded version ofthe natural stories corpus.
the functional regionsof interest (froi) corresponding to the domain-speciﬁc language network were identiﬁed for eachsubject based on the results of a localizer task thatthey conducted.
this resulted in a total of 202,295observations, which were subsequently partitionedinto an exploratory set of 100,325 observations anda held-out set of 101,970 observations by assigningalternate 60-second intervals of bold series todifferent partitions for each participant.
all modelswere ﬁtted to the bold signals in the held-out set.
the predictors included in the baseline cdrmodel were the index of current fmri samplewithin the current scan, unigram surprisal, andthe deconvolutional intercept which captures theinﬂuence of stimulus timing.
following shainet al.
(2019), the cdr models assumed the two-parameter hrf based on the double-gamma canon-ical hrf (lindquist et al., 2009).
furthermore, thetwo parameters of the hrf were tied across pre-dictors, modeling the assumption that the shape ofthe blood oxygenation response to neural activityis identical in a given region.
however, to allow thehrfs to have differing amplitudes, a coefﬁcientthat rescales the hrf was estimated for each pre-dictor.
the models also included a by-froi randomeffect for the amplitude coefﬁcient and a by-subjectrandom intercept..to calculate the increase in log-likelihood fromincluding each predictor, a full cdr model includ-ing the ﬁxed effects of one surprisal predictor wasalso ﬁtted for each surprisal predictor.
all surprisalpredictors were included without spillover,8 and allpredictors were centered prior to model ﬁtting..7.2 results.
the results in figure 6 show that surprisal fromgpt-2 (gpt2surp) made the biggest contributionto model ﬁt in comparison to surprisal from othermodels (difference between model with gpt2surpand other models signiﬁcant with p < 0.001 by apaired permutation test using by-item errors).
most.
8as cdr estimates continuous hrfs from variably spacedlinguistic input, consideration of spillover variants of surprisalpredictors was not necessary..(a) baseline ll: -269825.1.figure 6: perplexity measures from each model, andimprovements in regression model log-likelihood fromincluding each surprisal estimate on natural storiesfmri data..notably, in contrast to self-paced reading times andeye-gaze durations, charwsurp did not contributeas much to model ﬁt on fmri data, with a ∆lllower than those of the lstm language models.
this differential contribution of charwsurp acrossdatasets suggests that latency-based measures andblood oxygenation levels may capture different as-pects of online processing difﬁculty..8 conclusion.
this paper presents a character model that canbe used to estimate word generation probabili-ties in a structural parser-based processing model.
experiments demonstrate that surprisal estimatescalculated from this processing model generallycontribute to substantially better ﬁts to human re-sponse data than those calculated from large-scalepretrained language models or other incrementalparsers.
these results add a new nuance to the rela-tionship between perplexity and predictive powerreported in previous work (goodkind and bicknell,2018; wilcox et al., 2020).
in addition, they sug-gest that structural parser-based processing modelsmay provide a more humanlike account of sen-tence processing, and may suggest a larger role ofmorphology, phonotactics, and orthographic com-plexity than was previously thought..acknowledgments.
the authors would like to thank the anonymousreviewers for their helpful comments.
this workwas supported by the national science foundationgrant #1816891. all views expressed are those ofthe authors and do not necessarily reﬂect the viewsof the national science foundation..3754ethical considerations.
experiments presented in this work used datasetsfrom previously published research (futrell et al.,2018; kennedy et al., 2003; marcus et al., 1993;shain et al., 2019), in which the procedures for datacollection and validation are outlined..references.
rami al-rfou, do kook choe, noah constant, mandyguo, and llion jones.
2019. character-level lan-guage modeling with deeper self-attention.
in pro-ceedings of the thirty-third aaai conference on ar-tiﬁcial intelligence, pages 3159–3166..dale j. barr, roger levy, christoph scheepers, andharry j. tily.
2013. random effects structure forconﬁrmatory hypothesis testing: keep it maximal.
journal of memory and language, 68:255–278..douglas bates, martin mächler, ben bolker, and stevewalker.
2015. fitting linear mixed-effects modelsusing lme4.
journal of statistical software, 67(1):1–48..ciprian chelba, tomas mikolov, mike schuster, qi ge,thorsten brants, and phillipp koehn.
2014. one bil-lion word benchmark for measuring progress in sta-in proceedings of in-tistical language modeling.
terspeech, pages 2635–2639..chris dyer, miguel ballesteros, wang ling, austinmatthews, and noah a. smith.
2015. transition-based dependency parsing with stack long short-in proceedings of the 53rd annualterm memory.
meeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing, pages 334–343..chris dyer, adhiguna kuncoro, miguel ballesteros,and noah a. smith.
2016. recurrent neural networkgrammars.
in proceedings of the 2016 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, pages 199–209..micha elsner, andrea d. sims, alexander erd-mann, antonio hernandez, evan jaffe, lifeng jin,martha booker johnson, shuan karim, david l.king, luana lamberti nunes, byung-doh oh,nathan rasmussen, cory shain, stephanie an-tetomaso, kendra v. dickinson, noah diewald,michelle mckenzie, and symon stevens-guille.
2019. modeling morphologicaltypol-ogy, and change: what can the neural sequence-to-journal of lan-sequence framework contribute?
guage modelling, 7(1):53–98..learning,.
richard futrell, edward gibson, harry j. tily, idanblank, anastasia vishnevetsky, steven piantadosi,and evelina fedorenko.
2018. the natural stories.
in proceedings of the eleventh interna-corpus.
tional conference on language resources and eval-uation, pages 76–82..richard futrell, ethan wilcox, takashi morita, pengqian, miguel ballesteros, and roger levy.
2019.neural language models as psycholinguistic sub-jects: representations of syntactic state.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, pages 32–42..adam goodkind and klinton bicknell.
2018. predic-tive power of word surprisal for reading times is ain pro-linear function of language model quality.
ceedings of the 8th workshop on cognitive modelingand computational linguistics, pages 10–18..kristina gulordava, piotr bojanowski, edouard grave,tal linzen, and marco baroni.
2018. colorlessgreen recurrent networks dream hierarchically.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1195–1205..john hale.
2001. a probabilistic earley parser as a psy-cholinguistic model.
in proceedings of the secondmeeting of the north american chapter of the asso-ciation for computational linguistics on languagetechnologies, pages 1–8..john hale, chris dyer, adhiguna kuncoro, andjonathan brennan.
2018. finding syntax in humanencephalography with beam search.
in proceedingsof the 56th annual meeting of the association forcomputational linguistics, pages 2727–2736..yiding hao, simon mendelsohn, rachel sterneck,randi martinez, and robert frank.
2020. probabilis-tic predictions of people perusing: evaluating met-rics of language model performance for psycholin-guistic modeling.
in proceedings of the 10th work-shop on cognitive modeling and computational lin-guistics, pages 75–86..kenneth heaﬁeld,.
ivan pouzyrevsky, jonathan h.clark, and philipp koehn.
2013. scalable modiﬁedkneser-ney language model estimation.
in proceed-ings of the 51st annual meeting of the associationfor computational linguistics, pages 690–696..lifeng jin, finale doshi-velez, timothy miller, laneschwartz, and william schuler.
2019. unsupervisedin pro-learning of pcfgs with normalizing ﬂow.
ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 2442–2452..lifeng jin and william schuler.
2020. memory-bounded neural incremental parsing for psycholin-guistic prediction.
in proceedings of the 16th inter-national conference on parsing technologies andthe iwpt 2020 shared task on parsing into en-hanced universal dependencies, pages 48–61..3755philip n. johnson-laird.
1983. mental models: to-wards a cognitive science of language, inference,and consciousness.
harvard university press, cam-bridge, ma..alec radford, jeff wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaitechnical report..rafal jozefowicz, oriol vinyals, mike schuster, noamshazeer, and yonghui wu.
2016. exploring the lim-its of language modeling.
arxiv..katharina kann and hinrich schütze.
2016. med: thelmu system for the sigmorphon 2016 sharedtask on morphological reinﬂection.
in proceedingsof the 14th sigmorphon workshop on computa-tional research in phonetics, phonology, and mor-phology, pages 62–70..alan kennedy, robin hill, and joël pynte.
2003. thein proceedings of the 12th euro-.
dundee corpus.
pean conference on eye movement..yoon kim, yacine jernite, david sontag, and alexan-der m. rush.
2016. character-aware neural lan-guage models.
in proceedings of the thirtieth aaaiconference on artiﬁcial intelligence, pages 2741–2749..jason lee, kyunghyun cho, and thomas hofmann.
2017. fully character-level neural machine trans-lation without explicit segmentation.
transactionsof the association for computational linguistics,5:365–378..roger levy.
2008. expectation-based syntactic com-.
prehension.
cognition, 106(3):1126–1177..martin a. lindquist, ji meng loh, lauren y. atlas, andtor d. wager.
2009. modeling the hemodynamicresponse function in fmri: efﬁciency, bias and mis-modeling.
neuroimage, 45(1, supplement 1):s187–s198..mitchell p. marcus, beatrice santorini, and mary annmarcinkiewicz.
1993. building a large annotatedcorpus of english: the penn treebank.
computa-tional linguistics, 19(2):313–330..george a. miller and stephen isard.
1963. some per-journalceptual consequences of linguistic rules.
of verbal learning and verbal behavior, 2(3):217–228..luan nguyen, marten van schijndel, and williamschuler.
2012. accurate unbounded dependency re-covery using generalized categorial grammars.
inproceedings of the 24th international conference oncomputational linguistics, pages 2125–2140..robert parker, david graff, junbo kong, ke chen,english gigaword.
and kazuaki maeda.
2009.ldc2009t13..slav petrov, leon barrett, romain thibaux, and danklein.
2006. learning accurate, compact, and inter-pretable tree annotation.
in proceedings of the 21stinternational conference on computational linguis-tics and 44th annual meeting of the association forcomputational linguistics, pages 433–440..keith rayner, marcia carlson, and lyn frazier.
1983.the interaction of syntax and semantics during sen-tence processing: eye movements in the analysisof semantically biased sentences.
journal of verballearning and verbal behavior, 22(3):358–374..marten van schijndel, andy exley, and williamschuler.
2013. a model of language processing ashierarchic sequential prediction.
topics in cognitivescience, 5(3):522–540..william schuler, samir abdelrahman, tim miller, andlane schwartz.
2010. broad-coverage incrementalparsing using human-like memory constraints.
com-putational linguistics, 36(1):1–30..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, pages 1715–1725..cory shain.
2019. a large-scale study of the effectsof word frequency and predictability in naturalisticreading.
in proceedings of the annual conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 4086–4094..cory shain, idan asher blank, marten van schijn-del, william schuler, and evelina fedorenko.
2019.fmri reveals language-speciﬁc predictive codingduring naturalistic sentence comprehension.
neu-ropsychologia, 138..cory shain, marten van schijndel, and williamschuler.
2018.deep syntactic annotations forbroad-coverage psycholinguistic modeling.
in work-shop on linguistic and neuro-cognitive resources(lrec 2018)..cory shain and william schuler.
2019. continuous-time deconvolutional regression for psycholinguis-tic modeling.
psyarxiv..claude elwood shannon.
1948. a mathematical theoryof communication.
bell system technical journal,27:379–423..nathaniel j. smith and roger levy.
2013. the effectof word predictability on reading time is logarithmic.
cognition, 128:302–319..mitchell stern, daniel fried, and dan klein.
2017. ef-fective inference for generative neural parsing.
inproceedings of the 2017 conference on empiricalmethods in natural language processing, pages1695–1700..3756c procedures for hidden state.
re-initialization.
• glstmsurp, jlstmsurp, gpt2surp: the hid-den states of these models were re-initialized atthe end of every article before making predic-tions on the next article..• rnngsurp, vslcsurp, jlcsurp: since thesemodels predict parsing operations while makingword predictions, their hidden states were re-initialized after each sentence..shravan vasishth.
2006. on the proper treatment ofspillover in real-time reading studies: consequencesfor psycholinguistic theories.
in proceedings of theinternational conference on linguistic evidence,pages 96–100..ethan gotlieb wilcox, jon gauthier, jennifer hu, pengqian, and roger p. levy.
2020. on the predictivepower of neural language models for human real-time comprehension behavior.
in proceedings of the42nd annual meeting of the cognitive science soci-ety, pages 1707–1713..a procedures for surprisal calculation.
• glstmsurp, jlstmsurp: these models di-rectly estimate p(wt | w1..t−1), which can be usedto calculate s(wt) = − log p(wt | w1..t−1)..• gpt2surp: since gpt-2 relies on byte-pair en-coding (sennrich et al., 2016), negative log prob-abilities of word pieces corresponding to wt wereadded together to calculate s(wt) = − log p(wt |w1..t−1)..• rnngsurp: since the generative rnng modeldeﬁnes a joint distribution over words andtrees, we marginalize over trees to calculate| w1..t−1).
to keep this tractable, a word-p(wtsynchronous beam search (stern et al., 2017)was used with beam size 100, fast-track beamsize 5, and word beam size 10..• vslcsurp, jlcsurp: beam search decodingwith a beam size of 5,000 and 2,000 respectivelywas used to estimate preﬁx probabilities and sur-prisal predictors..b procedures for out-of-vocabulary.
handling.
• glstmsurp, jlstmsurp, jlcsurp: out-of-vocabulary (oov) words in the test corpus werereplaced with a corresponding “unk” symbolprior to surprisal estimation..• gpt2surp: special oov handling was not nec-essary because gpt-2 uses byte-pair encoding(sennrich et al., 2016)..• rnngsurp, vslcsurp: mapping rules fromthe berkeley parser9 were used to replace oovwords with a set of unknown word classes(e.g.
“unk-lc-ing”)..9https://github.com/slavpetrov/.
berkeleyparser.
3757