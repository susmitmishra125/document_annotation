verb knowledge injection for multilingual event processing.
olga majewska1.
ivan vuli´c1 goran glavaš2 edoardo m. ponti1,3 anna korhonen1.
1language technology lab, tal, university of cambridge, uk2 data and web science group, university of mannheim, germany3 mila – quebec ai institute, montreal, canada1{om304,iv250,ep490,alk23}@cam.ac.uk2goran@informatik.uni-mannheim.de.
abstract.
linguistic probing of pretrained transformer-based language models (lms) revealed thatthey encode a range of syntactic and semanticproperties of a language.
however, they arestill prone to fall back on superﬁcial cues andsimple heuristics to solve downstream tasks,rather than leverage deeper linguistic informa-tion.
in this paper, we target a speciﬁc facetof linguistic knowledge, the interplay betweenverb meaning and argument structure.
we in-vestigate whether injecting explicit informa-tion on verbs’ semantic-syntactic behaviourimproves the performance of pretrained lmsin event extraction tasks, where accurate verbprocessing is paramount.
concretely, we im-part the verb knowledge from curated lexi-cal resources into dedicated adapter modules(verb adapters), allowing it to complement, indownstream tasks, the language knowledge ob-tained during lm-pretraining.
we ﬁrst demon-strate that injecting verb knowledge leads toperformance gains in english event extraction.
we then explore the utility of verb adapters forevent extraction in other languages: we investi-gate 1) zero-shot language transfer with multi-lingual transformers and 2) transfer via (noisyautomatic) translation of english verb-basedlexical knowledge.
our results show that thebeneﬁts of verb knowledge injection indeed ex-tend to other languages, even when relying onnoisily translated lexical knowledge..1.introduction.
large transformer-based encoders, pretrained withself-supervised language modeling (lm) objec-tives, form the backbone of state-of-the-art modelsfor most nlp tasks (devlin et al., 2019; yang et al.,2019b; liu et al., 2019).
recent probes showedthat they implicitly extract a non-negligible amountof linguistic knowledge from text corpora in anunsupervised fashion (hewitt and manning, 2019;vuli´c et al., 2020; rogers et al., 2020, inter alia)..in downstream tasks, however, they often rely onspurious correlations and superﬁcial cues (nivenand kao, 2019) rather than a deep understandingof language meaning (bender and koller, 2020),which is detrimental to both generalisation and in-terpretability (mccoy et al., 2019)..in this work, we focus on a speciﬁc facet of lin-guistic knowledge: reasoning about events.1 iden-tifying tokens in the text that mention events andclassifying the temporal and causal relations amongthem is crucial to understand the structure of a storyor dialogue (carlson et al., 2002; miltsakaki et al.,2004) and to ground a text in real-world facts..verbs (with their arguments) are prominentlyused for expressing events (with their participants).
thus, ﬁne-grained knowledge about verbs, e.g., thesyntactic patterns in which they partake and thesemantic frames, may help pretrained encoders toachieve a deeper understanding of text and improvetheir performance in event-oriented downstreamtasks.
there already exist some expert-curatedcomputational resources that organise verbs intoclasses based on their syntactic-semantic properties(jackendoff, 1992; levin, 1993).
in particular, herewe consider english verbnet and framenet as richsources of verb knowledge..expanding a line of research on injecting ex-ternal linguistic knowledge into pretrained lms(peters et al., 2019; levine et al., 2020; lauscheret al., 2020b), we integrate verb knowledge into thelms for the ﬁrst time.
we devise a new method todistil verb knowledge into dedicated adapter mod-ules (pfeiffer et al., 2020b), which reduce the riskof (catastrophic) forgetting of and allow seamlessmodular integration with distributional knowledge..1for instance, in the sentence “stately, plump buck mulli-gan came from the stairhead, bearing a bowl of lather (...)”, anevent of coming occurs in the past, with buck mulliganas a participant, simultaneously to an event of bearing withan additional participant, a bowl..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6952–6969august1–6,2021.©2021associationforcomputationallinguistics6952we hypothesise that complementing pretrainedlms with verb knowledge should beneﬁt modelperformance in downstream tasks that involve eventextraction and processing.
we ﬁrst put this hypoth-esis to the test in english monolingual event identi-ﬁcation and classiﬁcation tasks from the tempeval(uzzaman et al., 2013) and ace (doddington et al.,2004) datasets.
we report modest but consistentimprovements in the former, and signiﬁcant per-formance boosts in the latter, thus verifying thatverb knowledge is indeed paramount for a deeperunderstanding of events and their structure..moreover, expert-curated resources are not avail-able for most of the languages spoken worldwide.
therefore, we also investigate the effectiveness oftransferring verb knowledge across languages; inparticular, from english to spanish, arabic andchinese.
the results demonstrate the success ofthe transfer techniques, and also shed some light onan important linguistic question: to what extent canverb classes (and predicate–argument structures)be considered cross-lingually universal, rather thanvarying across languages (hartmann et al., 2013)?
overall, our main contributions consist in 1) mit-igating the limitations of pretrained encoders re-garding event understanding by supplying externalverb knowledge; 2) proposing a new method to doso in a modular way through verb adapters; 3) ex-ploring techniques to transfer verb knowledge toresource-poor languages.
the performance gainsacross four diverse languages and several eventprocessing tasks and datasets validate that comple-menting distributional knowledge with curated verbknowledge is both beneﬁcial and cost-effective..2 verb knowledge for event processing.
figure 1 illustrates our framework for injectingverb knowledge from verbnet or framenet andleveraging it in downstream event processing tasks.
first, we inject the external verb knowledge, formu-lated as the so-called lexical constraints (mrkši´cet al., 2017; ponti et al., 2019) (in our case – verbpairs, see §2.1), into a (small) additional set ofadapter parameters (§2.2) (houlsby et al., 2019).
second (§2.3), we combine the language knowl-edge encoded in the original lm parameters andthe verb knowledge from verb adapters for eventprocessing tasks.
to this end, we either a) ﬁne-tuneboth sets of parameters (1. pretrained lm; 2. verbadapters) or b) freeze both sets of parameters andinsert an additional set of task-speciﬁc adapter pa-.
figure 1: injecting verb knowledge into a pretrainedtransformer-based lm.
1) dedicated verb adapterstrained to recognise pairs of verbs from the same verb-net (vn) class or framenet (fn) frame; 2) fine-tuning for an event processing task: a) full ﬁne-tuning– lm’s original parameters and verb adapters both ﬁne-tuned for the task; b) task adapter (ta) ﬁne-tuning –additional task adapter is mounted on top of the verbadapter and tuned for the task.
for simplicity, we showonly a single transformer layer.
snowﬂakes denotefrozen parameters in the respective training step..rameters.
in both cases, the task-speciﬁc training isinformed both by the general language knowledgecaptured in the pretrained lm, and the specialisedverb knowledge, captured in the verb adapters..2.1 sources of verb lexical knowledge.
given the inter-connectedness between verbs’meaning and syntactic behaviour (levin, 1993;kipper schuler, 2005), we assume that reﬁninglatent representation spaces with verb knowledgewould have a positive effect on event extractiontasks that strongly revolve around verbs.
lexicalclasses, deﬁned in terms of verbs’ shared semantic-syntactic properties, provide a mapping betweenthe verbs’ senses and the morpho-syntactic realisa-tion of their arguments (jackendoff, 1992; levin,1993).
the potential of verb classiﬁcations lies intheir predictive power: for any given verb, a set ofrich semantic-syntactic properties can be inferredbased on its class membership.
in this work, weexplicitly harness this rich linguistic knowledge toaid pretrained lms in capturing regularities in theproperties of verbs and their arguments..we select two major english lexical databases– verbnet (kipper schuler, 2005) and framenet(baker et al., 1998) – as sources of verb knowledgeat the semantic-syntactic interface, each represent-ing a different lexical framework..6953multi-head attentionadd & normalizefeed-forwardadd & normalizeadd & normalizeverb (vn/fn)adapterverb-pair classifierlverbor[convert, transform, true][separate,  split,     false]ltaskmulti-head attentionadd & normalizefeed-forwardadd & normalizeadd & normalizeverb (vn/fn)adapter...it also[affects]statesmall businesses, which[pay]occurrencepremiums...event task classifiermulti-head attentionadd & normalizefeed-forwardadd & normalizeadd & normalizeverb (vn/fn)adaptertask adapter1)                                    2a)                                     2b)   event task classifierltask...it also[affects]statesmall businesses, which[pay]occurrencepremiums...verbnet (vn) (kipper schuler, 2005; kipper et al.,2006), the largest available verb-focused lexicon,organises verbs into classes based on the overlap intheir semantic properties and syntactic behaviour;it builds on the premise that a verb’s predicate-argument structure informs its meaning (levin,1993).
each entry provides a set of thematic rolesand selectional preferences for the verbs’ argu-ments; it also lists the syntactic contexts character-istic for the class members.
its hierarchical classiﬁ-cation starts from broader classes and spans severalgranularity levels where each subclass further re-ﬁnes the semantic-syntactic properties inheritedfrom its parent class.2 the vn class member-ship is english-speciﬁc, but the underlying verbclass construction principles are thought to applycross-lingually (jackendoff, 1992; levin, 1993); itstranslatability has been indicated in previous work(vuli´c et al., 2017; majewska et al., 2018).
thecurrent english vn contains 329 main classes..framenet (fn) (baker et al., 1998) is more se-mantically oriented than vn.
grounded in thetheory of frame semantics (fillmore, 1976, 1977,1982), it organises concepts according to semanticframes, i.e., schematic representations of situationsand events, which they evoke, each characterisedby a set of typical roles assumed by its participants.
the word senses associated with each frame (fn’slexical units) are similar in terms of their semanticcontent, as well as their typical argument structures.
currently, english fn covers 1,224 frames and itsannotations illustrate the typical syntactic realisa-tions of the frame elements.
frames themselvesare, however, semantically deﬁned: this means thatthey may be shared even across languages withdifferent syntactic properties.3.
2.2 training verb adapters.
training task and data generation.
in order toinject external verb knowledge into pretrained lms,we devise an intermediary training task: we train.
2for example, within a top-level class ‘free-80’, whichincludes verbs like liberate, discharge, and exonerate whichparticipate in a np v np pp.theme frame (e.g., it freed him ofguilt), there exists a subset of verbs participating in a syntacticframe np v np s_ing (‘free-80-1’), within which thereexists an even more constrained subset of verbs appearing withprepositional phrases headed speciﬁcally by the prepositionfrom (e.g., the scientist puriﬁed the water from bacteria)..3for instance, descriptions of transactions will include thesame frame elements buyer, seller, goods, money in mostlanguages.
indeed, english fn has inspired similar projectsin other languages: e.g., spanish (subirats and sato, 2004),japanese (ohara, 2012), and danish (bick, 2011)..a dedicated vn-/fn-knowledge adapter (hereaftervn-adapter and fn-adapter).
we frame the taskas binary word-pair classiﬁcation: we predict if twoverbs belong to the same vn class or fn frame.
we extract training instances from fn and vn in-dependently.
this allows for a separate analysis ofthe impact of verb knowledge from each resource.
we generate positive training instances by ex-tracting all unique verb pairings from the set ofmembers of each main vn class/fn frame (e.g.,walk–march), resulting in 181,882 instances cre-ated from vn and 57,335 from fn.
we then gener-ate k = 3 negative examples per positive exampleby combining controlled and random sampling.
incontrolled sampling, we follow prior work on se-mantic specialisation (wieting et al., 2015; glavašand vuli´c, 2018b; lauscher et al., 2020b).
foreach positive example p = (w1, w2) in the trainingbatch b, we create two negatives ˆp1 = ( ˆw1, w2)and ˆp2 = (w1, ˆw2); ˆw1 is the verb from batch bother than w1 that is closest to w2 in terms of theircosine similarity in an auxiliary static word embed-ding space xaux ∈ rd; conversely, ˆw2 is the verbfrom b other than w2 closest to w1.
we addition-ally create one negative instance ˆp3 = ( ˆw1, ˆw2) byrandomly sampling ˆw1 and ˆw2 from batch b, notconsidering w1 and w2.
we ensure that the nega-tives are not present in the global set of all positiveverb pairs..similar to lauscher et al.
(2020b), we tokeniseeach (positive and negative) training instance intowordpiece tokens, prepended with sequence starttoken [cls], and with [sep] tokens in betweenthe verbs and at the end of the input sequence.
weuse the representation of the [cls] token xcls ∈rh (with h as the hidden state size of the trans-former) from the last transformer layer as the latentrepresentation of the verb pair, and feed it to a sim-ple binary classiﬁer:4 ˆy = softmax(xclswcl +bcl),with wcl ∈ rh×2 and bcl ∈ r2 as classiﬁer’strainable parameters.
we train by minimising thestandard cross-entropy loss (lverb in figure 1)..adapter architecture.
instead of directly ﬁne-tuning all parameters of the pretrained transformer,we opt for storing verb knowledge in a separate setof adapter parameters, keeping the verb knowledge.
4we also experimented with sentence-level tasks: we fed(a) pairs of sentence examples from vn/fn in a binary clas-siﬁcation setup (e.g., jackie leads rose to the store.
– jackieescorts rose.
); and (b) individual sentences in a multi-classclassiﬁcation setup (predicting the correct vn class/fn frame).
these variants, however, led to weaker performance..6954separate from the general language knowledge ac-quired in pretraining.
this (1) allows downstreamtraining to ﬂexibly combine the two sources ofknowledge, and (2) bypasses the issues with catas-trophic forgetting and interference (hashimotoet al., 2017; de masson d'autume et al., 2019)..we adopt the standard efﬁcient adapter archi-tecture of pfeiffer et al.
(2020a,c).
in eachtransformer layer l, we insert a single adapter(adapterl) after the feed-forward sub-layer.
theadapter itself is a two-layer feed-forward neuralnetwork with a residual connection, consisting ofa down-projection d ∈ rh×m, a gelu activa-tion (hendrycks and gimpel, 2016), and an up-projection u ∈ rm×h, where h is the hiddensize of the transformer model and m is the di-mensionality of the adapter: adapterl(hl, rl) =ul(gelu(dl(hl))) + rl; where rl is the resid-ual connection, output of the transformer’s feed-forward layer, and hl is the transformer hiddenstate, output of the subsequent layer normalisation..2.3 downstream fine-tuning for event tasks.
the next step is downstream ﬁne-tuning for eventprocessing tasks.
we experiment with (1) token-level event trigger identiﬁcation and classiﬁcationand (2) span extraction for event triggers and ar-guments (a sequence labeling task); see §3.
forthe former, we mount a classiﬁcation head – a sim-ple single-layer feed-forward softmax regressionclassiﬁer – on top of the transformer augmentedwith vn-/fn-adapters.
for the latter, we followthe architecture from prior work (m’hamdi et al.,2019; wang et al., 2019) and add a crf layer (laf-ferty et al., 2001) on top of the sequence of trans-former’s outputs (for subword tokens)..for all tasks, we propose and evaluate two differ-ent ﬁne-tuning regimes: (1) full ﬁne-tuning, wherewe update both the original transformer’s parame-ters and vn-/fn-adapters (see 2a in figure 1); and(2) task-adapter (ta) ﬁne-tuning, where we keepboth transformer’s original parameters and vn-/fn-adapters frozen, while stacking a new train-able task adapter on top of the vn-/fn-adapter ineach transformer layer (see 2b in figure 1)..2.4 cross-lingual transfer.
creation of curated resources like vn or fn takesyears of expert linguistic labour.
consequently,such resources do not exist for a vast majorityof languages.
given the inherent cross-lingualnature of verb classes and semantic frames (see.
english (en)spanish (es)chinese (zh)arabic (ar).
verbnet.
181,88296,30060,36570,278.framenet.
57,33536,62321,81524,551.table 1: number of positive verb pairs in english, andin each target language obtained via vtrans (§2.4)..§2.1), we investigate the potential for verb knowl-edge transfer from english to target languages,without any manual target-language adjustments.
massively multilingual lms, such as multilingualbert (mbert) (devlin et al., 2019) or xlm-r (conneau et al., 2020) have become the defacto standard mechanisms for zero-shot (zs) cross-lingual transfer.
in our ﬁrst transfer approach: weﬁne-tune mbert ﬁrst on the english verb knowl-edge, then on english task data, and then simplymake task predictions for the target language input.
the second approach, dubbed vtrans, is in-spired by the work on cross-lingual transfer of se-mantic specialisation for static word embeddings(glava´s et al., 2019; ponti et al., 2019; wang et al.,in brief (with full details in appendix2020b).
c), starting from a set of positive pairs from en-glish vn/fn, vtrans involves three steps: (1)automatic translation of verbs in each pair intothe target language, (2) ﬁltering of the noisy targetlanguage pairs by means of a transferred relationprediction model trained on the english examples,and (3) training the verb adapters injected intothe pretrained model, now with the translated andﬁltered target-language verb pairs.
for the mono-lingual target-language fn-/vn-adapter training,we follow the protocol used for english, see §2.2..3 experimental setup.
event processing tasks and data.
in event pro-cessing tasks, systems are tasked with detectingthat something happened, identifying what type ofoccurrence took place, as well as what entities wereinvolved.
verbs typically act as the organisationalcore of each such event schema, carrying a lot of se-mantic and structural weight.
therefore, a model’sgrasp of verbs’ properties should have a bearingon ﬁnal task performance.
based on this assump-tion, we select event extraction and classiﬁcationas suitable tasks to proﬁle the methods from §2..these tasks and the corresponding data are basedon the two prominent frameworks for annotatingevent expressions: timeml (pustejovsky et al.,2003, 2005) and the automatic content extraction.
6955(ace) (doddington et al., 2004).
first, we relyon the timeml-annotated corpus from tempevaltasks (verhagen et al., 2010; uzzaman et al., 2013),which targets automatic identiﬁcation of temporalexpressions and relations, and events.
second, weuse the ace dataset: it provides annotations forentities, the relations between them, and for eventsin which they participate in newswire text.5.
task 1: trigger identiﬁcation and classiﬁca-tion (tempeval).
we frame the ﬁrst event pro-cessing task as a token-level classiﬁcation problem,predicting whether a token triggers an event andassigning it to one of the following event types: oc-currence (e.g., died, attacks), state (e.g., share,assigned), reporting (e.g., announced, said), i-action (e.g., agreed, trying), i-state (e.g., under-stands, wants, consider), aspectual (e.g., ending,began), and perception (e.g., watched, spotted).6we use the tempeval-3 data for english and span-ish (uzzaman et al., 2013), and the tempeval-2data for chinese (verhagen et al., 2010) (see table6 in the appendix for exact dataset sizes)..task 2: trigger and argument identiﬁcationand classiﬁcation (ace).
in this sequence la-beling task, we detect and label event triggers andtheir arguments, with four individually scored sub-tasks: (i) trigger identiﬁcation, where we identifythe key word conveying the nature of the event,and (ii) trigger classiﬁcation, where we classifythe trigger word into one of the predeﬁned cate-gories; (iii) argument identiﬁcation, where we pre-dict whether an entity mention is an argument ofthe event identiﬁed in (i), and (iv) argument classiﬁ-cation, where the correct role needs to be assignedto the identiﬁed event arguments.
we use the acedata available for english, chinese, and arabic.7.
event extraction as speciﬁed in these two frame-works is a challenging, highly context-sensitiveproblem, where different words (most often verbs)may trigger the same type of event, and con-versely, the same word (verb) can evoke differ-.
5we provide more details about the frameworks and their.
corresponding annotation schemes in appendix a..
6e.g., in the sentence: “the rules can also affect smallbusinesses, which sometimes pay premiums tied to employees’health status and claims history.”, affect and pay are eventtriggers of type state and occurrence, respectively..7the ace annotations distinguish 34 trigger types (e.g.,business:merge-org, justice:trial-hearing, conﬂict:attack)and 35 argument roles.
following previous work (hsi et al.,2016), we conﬂate eight time-related argument roles - e.g.,‘time-at-end’, ‘time-before’, ‘time-at-beginning’ - into asingle ‘time’ role in order to alleviate training data sparsity..ent types of event schemata depending on the con-text.
adopting these tasks for evaluation thus testswhether leveraging ﬁne-grained curated knowledgeof verbs’ semantic-syntactic behaviour can improvepretrained lms’ reasoning about event-triggeringpredicates and their arguments..model conﬁgurations.
for each task, we com-pare the performance of the underlying “vanilla”bert-based model (see §2.3) against its variantwith an added vn-adapter or fn-adapter8 (see§2.2) in two regimes: (a) full ﬁne-tuning, and (b)task adapter (ta) ﬁne-tuning (see figure 1).
toensure that any performance gains are not merelydue to increased parameter capacity offered by theadapters, we also evaluate a variant where we re-place the verb adapter with a randomly initialisedadapter of the same size (+random).
additionally,we examine the impact of increasing the capacityof the trainable task adapter by replacing it with a‘double task adapter’ (2ta), i.e., a task adapterwith double the number of trainable parameterscompared to the base architecture from §2.2.
fi-nally, we compare the vn/fn-adapter approachwith a computationally more expensive alternativemethod of injecting external verb knowledge, se-quential ﬁne-tuning, where the full bert is ﬁrstﬁne-tuned on the fn/vn data (as in 2.2) and thenon the task (see appendix d for details)..training details: verb adapters.
we experi-mented with k ∈ {2, 3, 4} negative examples andthe following combinations of controlled (c) andrandomly (r) sampled negatives (see §2.2): k = 2[cc], k = 3 [ccr], k = 4 [ccrr].
in our preliminaryexperiments we found k = 3 [ccr] to yield best-performing adapters.
the evaluation and analysispresented in §4 are thus based on this setup.
ourvn- and fn-adapters are injected into the bertbase cased model: the details on adapter trainingand hyperparameter search are in appendix b..downstream task fine-tuning.
in downstreamﬁne-tuning on tempeval, we train for 10 epochs inbatches of size 32, with a learning rate 1e − 4 andmaximum input sequence length of t = 128 word-piece tokens.
for ace, in light of a greater datasparsity,9 we search for optimal hyperparameters.
8we also experimented with inserting both verb adapterssimultaneously; however, this resulted in weaker downstreamperformance than adding each separately, a likely product ofthe partly redundant, partly conﬂicting information encodedin these adapters (see §2.1 for comparison of vn and fn)..9most event types (≈ 70%) have fewer than 100 labeled.
instances, and three have fewer than 10 (liu et al., 2018)..6956for each language and evaluation setup from thefollowing grid: learning rate l ∈ {1e − 5, 1e − 6},epochs n ∈ {3, 5, 10, 25, 50}, batch b ∈ {8, 16}(maximum input sequence length t = 128)..transfer experiments in zero-shot (zs) setups arebased on mbert, to which we add the vn- or fn-adapter trained on the english vn/fn data.
wetrain the model on english training data availablefor each task, and evaluate it on the target-languagetest set.
for the vtrans approach (§2.4), weuse language-speciﬁc bert models available forour target languages, and leverage target-languageadapters trained on translated and automaticallyreﬁned verb pairs.
the model, with or without thetarget-language vn-/fn-adapter, is trained andevaluated on the training and test data available inthe language.
we carry out the procedure for threetarget languages (see table 1).
we use the samenegative sampling parameter conﬁguration provenstrongest in our english experiments (k = 3 [ccr])..4 results and discussion.
english event processing.
table 2 shows the per-formance on english task 1 (tempeval) and task2 (ace).
first, we note that the computationallymore efﬁcient setup with a dedicated task adapter(ta) yields higher absolute scores compared tofull ﬁne-tuning (fft) on tempeval.
when theunderlying bert is frozen along with the addedfn-/vn-adapter, the ta is enforced to encodeadditional task-speciﬁc knowledge into its parame-ters, beyond what is provided in the verb adapter.
this yields two strongest results overall from the+fn/vn setups.
on ace, the primacy of ta-basedtraining is overturned in favour of fft.
encourag-ingly, boosts provided by verb adapters are visibleregardless of the chosen task ﬁne-tuning regime..we notice consistent statistically signiﬁcant10improvements in the +vn setup, although the per-formance of the ta-based setups clearly suffers inargument (arg) tasks due to decreased trainableparameter capacity.
lack of visible improvementsfrom the random adapter supports the interpre-tation that performance gains indeed stem fromthe added useful ‘non-random’ signal in the verbadapters.
in addition, we verify how our principalsetup with added adapter modules compares to analternative established approach, sequential ﬁne-tuning (+fn/vnseq).
in tempeval, we note that.
10we test signiﬁcance with the student’s t-test with a sig-.
niﬁcance value set at α = 0.05 for sets of model f1 scores..ﬁne-tuning all model parameters on vn/fn dataallows retrieving more additional verb knowledgebeneﬁcial for task performance than adding smallerpre-trained adapters on top of the underlying model.
however, fn/vnseq scores are still inferior to theresults achieved in the ta-based +fn/vn setup.
in ace, the fn/vnseq results in trigger tasks areweaker than those achieved through the addition ofself-contained knowledge adapters, however, theyoffer additional boosts in argument tasks..multilingual event processing.
table 3 com-pares the performance of zero-shot (zs) transferand monolingual target training (via vtrans) ontempeval in spanish and chinese.
for both, theaddition of the fn-adapter in the ta-based setupboosts zs transfer.
the beneﬁts extend to the fftsetup in chinese, achieving the top score overall..in monolingual evaluation, we observe consis-tent gains from the added transferred knowledgevia vtrans in spanish.
in chinese performanceboosts come from the transferred vn-style classmembership information (+vn).
this suggests thateven the noisily translated verb pairs carry enoughuseful signal through to the target language.
totease apart the contribution of the language-speciﬁcencoders and transferred verb knowledge, we carryout an additional monolingual evaluation substi-tuting the target-language bert with mbert,trained on (noisy) target language verb signal (es-mbert/zh-mbert).
although mbert scoresare lower than monolingual berts in absoluteterms, the use of the transferred verb knowledgehelps reduce the gap between the models, withgains achieved over the baselines in spanish.11.
in ace, the top scores are achieved in the mono-lingual fft setting; as with english, keeping thefull capacity of bert parameters unfrozen notice-ably helps performance.12 in arabic, fn knowl-edge provides performance boosts across the fourtasks and with both the zero-shot (zs) and mono-lingual (vtrans) transfer approaches, whereas theaddition of the vn adapter boosts scores in argtasks.
the usefulness of fn knowledge extendsto zero-shot transfer in chinese, and both adaptersbeneﬁt the arg tasks in the monolingual (vtrans).
11due to analogous patterns in relative scores of mbertand monolingual berts in monolingual ace evaluation, weshow the vtrans mbert results in ace in appendix e..12this is especially the case in arg tasks, where the ta-based setup fails to achieve meaningful improvements overzero, even with extended training up to 100 epochs.
due tothe computational burden of such long training, the results inthis setup are limited to trigger tasks (after 50 epochs)..6957tempeval.
t-ident&class.
ace.
t-identt-classarg-identarg-class.
fft.
73.6.
69.365.333.831.6.
+rand +fn.
+vn.
+fnseq +vnseq ta.
+rand +fn.
+vn.
73.5.
69.665.533.531.6.
73.6.
70.866.734.232.2.
73.6.
70.366.234.632.8.
74.2.
70.065.436.334.3.
73.9.
69.865.436.233.9.
74.5.
65.158.02.10.6.
74.4.
65.058.51.90.6.
75.0.
65.759.52.30.8.
75.2.
66.460.22.50.8.table 2: results on english tempeval and ace test sets for full ﬁne-tuning (fft) and the task adapter (ta) setup.
provided are average f1 scores over 10 runs.
statistically signiﬁcant (paired t-test; p < 0.05) improvements overboth baselines marked in bold; the same labeling is also used in all subsequent tables..spanish mbert-zs.
chinese mbert-zs.
es-bertes-mbert.
zh-bertzh-mbert.
fft.
37.277.773.5.
49.982.080.2.
37.277.173.6.
49.981.680.1.
+random +fn.
+random +fn.
+vn.
36.677.474.1.
47.981.880.0.ta.
38.070.065.3.
49.276.271.8.
38.070.065.4.
49.576.371.8.
38.670.765.8.
50.175.972.1.
+vn.
36.570.666.2.
48.276.971.9.table 3: results on spanish and chinese tempeval test sets for full ﬁne-tuning (fft) and the task adapter (ta) set-up, for zero-shot (zs) transfer with mbert and monolingual target language evaluation with language-speciﬁcbert (es-bert / zh-bert) or mbert (es-mbert / zh-mbert), with fn/vn adapters trained onvtrans-translated verb pairs (see §2.4).
f1 scores are averaged over 10 runs..fft.
+random +fn.
+vn.
+random +fn.
+vn.
arabic.
mbert-zs.
chinese mbert-zs.
ar-bert.
zh-bert.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
15.814.21.20.9.
68.863.631.728.4.
36.927.94.33.9.
75.567.927.325.8.
17.216.12.11.5.
70.264.434.030.3.
42.130.95.54.9.
74.568.029.828.2.
16.315.62.71.9.
68.662.833.429.7.
36.829.86.15.2.
74.968.628.827.2.ta.
29.425.62.01.2.
24.022.0––.
47.838.65.13.5.
69.858.4––.
30.326.33.31.6.
21.319.5––.
49.440.16.04.7.
69.357.5––.
32.927.83.31.6.
24.623.1––.
55.043.57.65.7.
70.059.9––.
32.428.43.61.3.
23.522.3––.
55.444.98.47.1.
70.260.0––.
37.077.674.4.
50.581.879.9.
13.512.20.60.4.
68.962.829.326.7.
36.725.23.12.7.
74.968.226.125.2.table 4: results on arabic and chinese ace test sets for full ﬁne-tuning (fft) and the task adapter (ta) setup,for zero-shot (zs) transfer with mbert and vtrans transfer approach with language-speciﬁc bert (ar-bert/ zh-bert) and fn/vn adapters trained on noisily translated verb pairs (§2.4).
f1 scores averaged over 5 runs..transfer setup.
notably, in zero-shot transfer, weobserve that the highest scores are achieved in thetask adapter (ta) ﬁne-tuning, where the inclusionof the verb adapters offers additional gains.
overall,however, the argument tasks elude the restricted ca-pacity of the ta-based setup, with very low scores..additionally, in appendix e we show the resultswith sequential ﬁne-tuning.
similarly to our en re-sults (table 2), we observe advantages of using thefull capacity of bert parameters to encode verbknowledge in most setups in tempeval, while thecomparison to the adapter-based approach is lessclear-cut on ace.
in sum, sequential ﬁne-tuningis a strong verb knowledge injection variant; how-ever, it is computationally more expensive and less.
portable.
the modular and efﬁcient adapter-basedapproach therefore presents an attractive alterna-tive, while offering competitive task performance.
crucially, the strong results from the sequentialsetup further corroborate our core ﬁnding that ex-ternal lexical verb information is indeed beneﬁcialfor event processing tasks across the board..zero-shot transfer vs monolingual training.
the results reveal a considerable gap between theperformance of zs transfer versus monolingual ﬁne-tuning.
the event extraction tasks pose a signif-icant challenge to zero-shot transfer via mbert;however, mbert exhibits much more robust per-formance in the monolingual setup, with availabletarget-language training data for event tasks.
in.
6958the latter, mbert trails language-speciﬁc bertsby less than 5 points (table 3).
this is encourag-ing, given that monolingual pretrained lms cur-rently exist only for a small set of high-resourcelanguages.
for all other languages – should therebe language-speciﬁc event task data – one can lever-age mbert.
moreover, mbert’s performance isfurther improved by the inclusion of transferredverb knowledge via vtrans: in spanish, whereits typological closeness to english renders di-rect transfer of semantic-syntactic information vi-able, the addition of vtrans-based verb adaptersyields signiﬁcant gains both in the fft and the tasetup.13 these results conﬁrm the effectiveness oflexical knowledge transfer suggested previously inthe work on semantic specialisation of static wordvectors (ponti et al., 2019; wang et al., 2020b)..double task adapter.
promisingly, we see intable 5 that the relative performance gains fromfn/vn adapters are preserved regardless of theadded trainable task adapter capacity.
as expected,the increased task adapter size helps argument tasksin ace, where verb adapters produce additionalgains.
overall, this suggests that verb adaptersindeed encode additional, non-redundant informa-tion beyond what is offered by the pretrained modelalone, and boost the dedicated task adapter..cleanliness of verb knowledge.
despite thepromising results with the vtrans approach, thereare still fundamental limitations: (1) noisy trans-lation based on cross-lingual semantic similaritymay already break the verbnet class membershipalignment; and (2) the language-speciﬁcity of verbclasses due to which they cannot be directly portedto another language without adjustments.14.
the ﬁne-grained class divisions and exact classmembership in vn may be too english-speciﬁc toallow direct automatic translation.
on the contrary,semantically-driven framenet lends itself betterto cross-lingual transfer: we report higher averagegains in cross-lingual setups with the fn-adapter.
to quickly verify if the noisy direct transfercurbs the usefulness of injected knowledge, weevaluate the injection of clean verb knowledgefrom a small lexical resource available in spanish:we train an es fn-adapter on top of es-bert on.
13we noted analogous positive effects on performance of.
the more powerful xlm-r large model (appendix e)..14this is in contrast to the proven cross-lingual portabilityof synonymy and antonymy relations shown in previous workon semantic specialisation transfer (mrkši´c et al., 2017; pontiet al., 2019), which rely on semantics alone..(a) tempeval.
english.
spanish.
chinese.
en-bert.
mbert-zses-bert.
mbert-zszh-bert.
2ta.
74.5.
37.773.1.
49.178.1.
+fn.
74.8.
38.373.6.
50.178.1.
2ta.
+fn.
+vn.
(b) ace.
en en-bert.
ar mbert-zs.
ar-bert.
zh mbert-zs.
zh-bert.
t-idt-clarg-idarg-cl.
t-idt-clarg-idarg-cl.
t-idt-clarg-idarg-cl.
t-idt-clarg-idarg-cl.
t-idt-clarg-idarg-cl.
67.561.66.23.9.
31.226.35.93.9.
40.636.9––.
54.645.69.28.0.
72.359.62.62.3.
68.162.68.96.7.
32.627.16.04.1.
42.338.1––.
56.346.210.88.5.
73.163.02.82.6.
+vn.
74.8.
37.173.6.
48.878.6.
68.962.77.15.0.
31.729.36.94.3.
43.039.5––.
58.146.911.39.9.
72.061.33.32.9.table 5: results on (a) tempeval and (b) ace for thedouble task adapter-based approaches (2ta)..2,866 verb pairs derived from its framenet (subi-rats and sato, 2004).
the results (appendix e)reveal that, despite having 12 times fewer positiveexamples for training the verb adapter compared tovtrans, the ‘native’ es fn-adapter offers gainsbetween +0.2 and +0.4 points over vtrans, com-pensating the limited coverage with gold standardaccuracy.
this suggests that work on optimisingand accelerating resource creation merits futureresearch efforts on a par with modeling work..5 related work.
event extraction.
the cost and complexity ofevent annotation requires robust transfer solutionscapable of making ﬁne-grained predictions in theface of data scarcity.
traditional event extractionmethods relied on hand-crafted, language-speciﬁcfeatures (ahn, 2006; gupta and ji, 2009; llorenset al., 2010; hong et al., 2011; li et al., 2013;glavaš and šnajder, 2015) (e.g., pos tags, entityknowledge), which limited their generalisation abil-ity and effectively prevented language transfer..more recent approaches commonly resorted toword embedding input and neural text encoderssuch as recurrent nets (nguyen et al., 2016; duanet al., 2017; sha et al., 2018) and convolutional nets(chen et al., 2015; nguyen and grishman, 2015),.
6959as well as graph neural networks (nguyen and gr-ishman, 2018; yan et al., 2019) and adversarial net-works (hong et al., 2018; zhang et al., 2019).
mostrecent empirical advancements in event trigger andargument extraction tasks stem from ﬁne-tuning oflm-pretrained transformer networks (yang et al.,2019a; wang et al., 2019; m’hamdi et al., 2019;wadden et al., 2019; liu et al., 2020)..limited training data nonetheless remains an ob-stacle, especially when facing previously unseenevent types.
the alleviation of such data scarcity is-sues was attempted through data augmentation – au-tomatic data annotation (chen et al., 2017; zheng,2018; araki and mitamura, 2018) and bootstrap-ping for training data generation (ferguson et al.,2018; wang et al., 2019).
the recent release ofthe large english event detection dataset maven(wang et al., 2020c), with annotations of eventtriggers only, partially remedies for english datascarcity.
maven also demonstrates that even thestate-of-the-art transformer models fail to yieldsatisfying event detection performance in the gen-eral domain.
the fact that it is unlikely to expectdatasets of similar size for other event extractiontasks and especially for other languages only em-phasises the need for external event-related knowl-edge and transfer learning approaches, such as theones introduced in this work..semantic specialisation.
representation spacesinduced through self-supervised objectives fromlarge corpora, be it the word embedding spaces(mikolov et al., 2013; bojanowski et al., 2017)or those spanned by lm-pretrained transform-ers (devlin et al., 2019; liu et al., 2019), encodeonly distributional knowledge.
a large body ofwork focused on semantic specialisation of suchdistributional spaces by injecting lexico-semanticknowledge from external resources (e.g., wordnet(fellbaum, 1998), babelnet (navigli and ponzetto,2010) or conceptnet (liu and singh, 2004)) in theform of lexical constraints (faruqui et al., 2015;mrkši´c et al., 2017; glavaš and vuli´c, 2018b; ka-math et al., 2019; vuli´c et al., 2021)..joint specialisation models (yu and dredze,2014; lauscher et al., 2020b; levine et al., 2020,inter alia) train the representation space fromscratch on the large corpus, but augment the self-supervised training objective with an additionalobjective based on external lexical constraints.
lauscher et al.
(2020b) add to the masked lm(mlm) and next sentence prediction (nsp) pre-.
training objectives of bert (devlin et al., 2019)an objective that predicts pairs of (near-)synonyms,aiming to improve word-level semantic similarityin bert’s representation space.
in a similar vein,levine et al.
(2020) add the objective that predictswordnet supersenses.
while joint specialisationmodels allow the external knowledge to shape therepresentation space from the very beginning ofthe distributional training, this also means that anychange in lexical constraints implies a new, compu-tationally expensive pretraining from scratch..retroﬁtting and post-specialisation methods(faruqui et al., 2015; mrkši´c et al., 2017; vuli´cet al., 2018; ponti et al., 2018; glavaš and vuli´c,2019; lauscher et al., 2020a; wang et al., 2020a),in contrast, start from a pretrained representa-tion space (word embedding space or a pretrainedencoder) and ﬁne-tune it using external lexico-semantic knowledge.
wang et al.
(2020a) ﬁne-tunethe pre-trained roberta (liu et al., 2019) withlexical constraints obtained automatically via de-pendency parsing, whereas lauscher et al.
(2020a)use lexical constraints derived from conceptnet toinject knowledge into bert: both adopt adapter-based ﬁne-tuning, storing the external knowledgein a separate set of parameters.
our work adopts asimilar adapter-based specialisation approach, how-ever, focusing on event-oriented downstream tasks,and knowledge from verbnet and framenet..6 conclusion.
we investigated the potential of leveraging knowl-edge about semantic-syntactic behaviour of verbsto improve the capacity of large pretrained mod-els to reason about events in diverse languages.
we proposed an auxiliary pretraining task to in-ject verbnet- and framenet-based lexical verbknowledge into dedicated verb adapter modules.
we demonstrated that state-of-the-art pretrainedmodels still beneﬁt from the gold standard lin-guistic knowledge stored in lexical resources, eventhose with limited coverage.
crucially, we showedthat the beneﬁts of the knowledge from resource-rich languages can be extended to other, resource-leaner languages through translation-based transferof verb class/frame membership information..acknowledgements.
this work is supportedby the erc consolidator grant lexical (no648909) awarded to ak.
the work of gg issupported by the baden-württemberg stiftung(eliteprogramm, agree grant)..6960references.
david ahn.
2006. the stages of event extraction.
inproceedings of the workshop on annotating andreasoning about time and events, pages 1–8, syd-ney, australia.
association for computational lin-guistics..jun araki and teruko mitamura.
2018. open-domainin pro-event detection using distant supervision.
ceedings of the 27th international conference oncomputational linguistics, pages 878–891, santafe, new mexico, usa.
association for computa-tional linguistics..cyprien de masson d'autume, sebastian ruder, ling-peng kong, and dani yogatama.
2019. episodicin ad-memory in lifelong language learning.
vances in neural information processing systems,volume 32, pages 13143–13152, vancouver, canada.
curran associates, inc..collin f. baker, charles j. fillmore, and john b. lowe.
1998. the berkeley framenet project.
in proceed-ings of coling, pages 86–90, montreal, quebec,canada..emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-in proceedings of thestanding in the age of data.
58th annual meeting of the association for compu-tational linguistics, pages 5185–5198, online.
as-sociation for computational linguistics..eckhard bick.
2011. a framenet for danish.
in pro-ceedings of the 18th nordic conference of compu-tational linguistics (nodalida 2011), pages 34–41, riga, latvia.
northern european association forlanguage technology (nealt)..piotr bojanowski, edouard grave, armand joulin, andtomas mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..lynn carlson, daniel marcu,.
and mary ellenokurowski.
2002.rst discourse treebankldc2002t07.
technical report, philadelphia: lin-guistic data consortium..yubo chen, shulin liu, xiang zhang, kang liu, andjun zhao.
2017. automatically labeled data genera-tion for large scale event extraction.
in proceedingsof the 55th annual meeting of the association forcomputational linguistics (volume 1: long papers),pages 409–419, vancouver, canada.
association forcomputational linguistics..yubo chen, liheng xu, kang liu, daojian zeng, andjun zhao.
2015. event extraction via dynamic multi-pooling convolutional neural networks.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing (volume 1: long papers), pages 167–176,beijing, china.
association for computational lin-guistics..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzmán, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020. unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 8440–8451, online.
association for computational lin-guistics..leon r.a. derczynski.
2017. automatically ordering.
events and times in text.
springer, berlin..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..george doddington, alexis mitchell, mark przybocki,lance ramshaw, stephanie strassel, and ralphweischedel.
2004. the automatic content extraction(ace) program – tasks, data, and evaluation.
inproceedings of the fourth international conferenceon language resources and evaluation (lrec’04),pages 837–840, lisbon, portugal.
european lan-guage resources association (elra)..shaoyang duan, ruifang he, and wenli zhao.
2017.exploiting document level information to improveinevent detection via recurrent neural networks.
proceedings of the eighth international joint con-ference on natural language processing (volume 1:long papers), pages 352–361, taipei, taiwan.
asianfederation of natural language processing..manaal faruqui, jesse dodge, sujay kumar jauhar,chris dyer, eduard hovy, and noah a. smith.
2015.retroﬁtting word vectors to semantic lexicons.
inproceedings of the 2015 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 1606–1615, denver, colorado.
associationfor computational linguistics..christiane fellbaum, editor.
1998. wordnet: an elec-.
tronic lexical database.
mit press..james ferguson, colin lockard, daniel weld, and han-naneh hajishirzi.
2018. semi-supervised event ex-traction with paraphrase clusters.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 359–364, new orleans, louisiana.
as-sociation for computational linguistics..charles j. fillmore.
1976. frame semantics and thein annals of the new yorknature of language.
academy of sciences: conference on the origin anddevelopment of language and speech, volume 280,pages 20–32, new york, new york..6961charles j. fillmore.
1977. the need for a frame seman-tics in linguistics.
in statistical methods in linguis-tics, pages 5–29.
ed.
hans karlgren.
scriptor..charles j. fillmore.
1982. frame semantics..in lin-guistics in the morning calm, pages 111–137.
ed.
the linguistic society of korea.
hanshin publish-ing co..goran glava´s, edoardo maria ponti, and ivan vuli´c.
2019. semantic specialization of distributional wordvectors.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on nat-ural language processing (emnlp-ijcnlp): tuto-rial abstracts, hong kong, china.
association forcomputational linguistics..goran glavaš and jan šnajder.
2015. construction andevaluation of event graphs.
natural language engi-neering, 21(4):607–652..goran glavaš and ivan vuli´c.
2018a.
discriminatingbetween lexico-semantic relations with the special-in proceedings of the 2018ization tensor model.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 181–187, new orleans, louisiana.
associa-tion for computational linguistics..goran glavaš and ivan vuli´c.
2018b..explicitin pro-retroﬁtting of distributional word vectors.
ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 34–45, melbourne, australia.
asso-ciation for computational linguistics..goran glavaš and ivan vuli´c.
2019. generalized tun-ing of distributional word vectors for monolingualand cross-lingual lexical entailment.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4824–4830, flo-rence, italy.
association for computational linguis-tics..prashant gupta and heng ji.
2009. predicting un-known time arguments based on cross-event prop-in proceedings of the acl-ijcnlp 2009agation.
conference short papers, pages 369–372, suntec,singapore.
association for computational linguis-tics..iren hartmann, martin haspelmath, and bradley tay-lor, editors.
2013. valency patterns leipzig.
maxplanck institute for evolutionary anthropology,leipzig..dan hendrycks and kevin gimpel.
2016. gaussian er-ror linear units (gelus).
corr, abs/1606.08415..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..yu hong, jianfeng zhang, bin ma, jianmin yao,guodong zhou, and qiaoming zhu.
2011. usingcross-entity inference to improve event extraction.
in proceedings of the 49th annual meeting of theassociation for computational linguistics: humanlanguage technologies, pages 1127–1136, portland,oregon, usa.
association for computational lin-guistics..yu hong, wenxuan zhou, jingli zhang, guodongzhou, and qiaoming zhu.
2018. self-regulation:employing a generative adversarial network to im-in proceedings of the 56thprove event detection.
annual meeting of the association for computa-tional linguistics (volume 1: long papers), pages515–526, melbourne, australia.
association forcomputational linguistics..neil houlsby, andrei giurgiu, stanislaw jastrzebski,bruna morrone, quentin de laroussilhe, andreagesmundo, mona attariyan, and sylvain gelly.
2019. parameter-efﬁcient transfer learning for nlp.
in proceedings of the 36th international conferenceon machine learning, volume 97 of proceedingsof machine learning research, pages 2790–2799,long beach, california, usa.
pmlr..andrew hsi, yiming yang, jaime carbonell, andruochen xu.
2016. leveraging multilingual train-in pro-ing for limited resource event extraction.
ceedings of coling 2016, the 26th internationalconference on computational linguistics: techni-cal papers, pages 1201–1210, osaka, japan.
thecoling 2016 organizing committee..ray jackendoff.
1992. semantic structures, volume 18..mit press..armand joulin, piotr bojanowski, tomas mikolov,hervé jégou, and edouard grave.
2018. loss intranslation: learning bilingual word mapping within proceedings of the 2018a retrieval criterion.
conference on empirical methods in natural lan-guage processing, pages 2979–2984, brussels, bel-gium.
association for computational linguistics..kazuma hashimoto, caiming xiong, yoshimasa tsu-ruoka, and richard socher.
2017. a joint many-taskmodel: growing a neural network for multiple nlpin proceedings of the 2017 conference ontasks.
empirical methods in natural language processing,pages 1923–1933, copenhagen, denmark.
associa-tion for computational linguistics..aishwarya kamath, jonas pfeiffer, edoardo mariaponti, goran glavaš, and ivan vuli´c.
2019. spe-cializing distributional vectors of all words for lex-ical entailment.
in proceedings of the 4th workshopon representation learning for nlp (repl4nlp-2019), pages 72–83, florence, italy.
association forcomputational linguistics..6962diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof international conference on learning represen-tations (iclr), san diego, ca, usa..karin kipper, anna korhonen, neville ryant, andextending verbnet withmartha palmer.
2006.in proceedings of the fifth in-novel verb classes.
ternational conference on language resources andevaluation (lrec’06), pages 1027–1032, genoa,italy.
european language resources association(elra)..karin kipper schuler.
2005..verbnet: a broad-coverage, comprehensive verb lexicon.
ph.d. thesis,university of pennsylvania..john lafferty, andrew mccallum, and fernando cnpereira.
2001. conditional random ﬁelds: prob-abilistic models for segmenting and labeling se-quence data.
in proceedings of international con-ference on machine learning (icml-2001), pages282–289, williams college, williamstown, ma,usa..anne lauscher, olga majewska, leonardo f. r.ribeiro, iryna gurevych, nikolai rozanov, andgoran glavaš.
2020a.
common sense or worldknowledge?
investigating adapter-based knowledgein proceed-injection into pretrained transformers.
ings of deep learning inside out (deelio): thefirst workshop on knowledge extraction and inte-gration for deep learning architectures, pages 43–49, online.
association for computational linguis-tics..anne lauscher, ivan vuli´c, edoardo maria ponti, annakorhonen, and goran glavaš.
2020b.
specializingunsupervised pretraining models for word-level se-mantic similarity.
in proceedings of the 28th inter-national conference on computational linguistics,pages 1371–1383, barcelona, spain (online).
inter-national committee on computational linguistics..beth levin.
1993. english verb classes and alterna-tions: a preliminary investigation.
university ofchicago press..yoav levine, barak lenz, or dagan, ori ram, danpadnos, or sharir, shai shalev-shwartz, amnonshashua, and yoav shoham.
2020. sensebert:in proceedingsdriving some sense into bert.
of the 58th annual meeting of the association forcomputational linguistics, pages 4656–4667, on-line.
association for computational linguistics..qi li, heng ji, and liang huang.
2013. joint eventextraction via structured prediction with global fea-tures.
in proceedings of the 51st annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 73–82, soﬁa, bulgaria.
association for computational linguistics..hugo liu and push singh.
2004. conceptnet – a prac-tical commonsense reasoning tool-kit.
bt technol-ogy journal, 22(4):211–226..jian liu, yubo chen, kang liu, wei bi, and xiaojiangliu.
2020. event extraction as machine reading com-prehension.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 1641–1651, online.
associa-tion for computational linguistics..jian liu, yubo chen, kang liu, and jun zhao.
2018.event detection via gated multilingual attentionmechanism.
in proceedings of the 32nd aaai con-ference on artiﬁcial intelligence, pages 4865–4872,new orleans, louisiana, usa..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrainingapproach.
arxiv preprint arxiv:1907.11692..hector llorens, estela saquete, and borja navarro.
2010. tipsem (english and spanish): evaluatingin pro-crfs and semantic roles in tempeval-2.
ceedings of the 5th international workshop on se-mantic evaluation, pages 284–291, uppsala, swe-den.
association for computational linguistics..olga majewska, ivan vuli´c, diana mccarthy, yanhuang, akira murakami, veronika laippala, andanna korhonen.
2018.investigating the cross-lingual translatability of verbnet-style classiﬁcation.
language resources and evaluation, 52(3):771–799..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448,florence, italy.
association for computational lin-guistics..meryem m’hamdi, marjorie freedman, and jonathanmay.
2019. contextualized cross-lingual event trig-ger extraction with minimal resources.
in proceed-ings of the 23rd conference on computational nat-ural language learning (conll), pages 656–665,hong kong, china.
association for computationallinguistics..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-ity.
in advances in neural information processingsystems, volume 26, pages 3111–3119, lake tahoe,nevada, usa.
curran associates, inc..eleni miltsakaki, rashmi prasad, aravind joshi, andbonnie webber.
2004. the penn discourse tree-in proceedings of the fourth internationalbank.
conference on language resources and evaluation(lrec’04), pages 2237–2240, lisbon, portugal.
eu-ropean language resources association (elra)..nikola mrkši´c, ivan vuli´c, diarmuid ó séaghdha, iraleviant, roi reichart, milica gaši´c, anna korho-nen, and steve young.
2017. semantic specializa-tion of distributional word vector spaces using mono-lingual and cross-lingual constraints.
transactions.
6963of the association for computational linguistics,5:309–324..roberto navigli and simone paolo ponzetto.
2010. ba-belnet: building a very large multilingual semanticnetwork.
in proceedings of the 48th annual meet-ing of the association for computational linguistics,pages 216–225, uppsala, sweden.
association forcomputational linguistics..thien huu nguyen, kyunghyun cho, and ralph gr-ishman.
2016. joint event extraction via recurrentin proceedings of the 2016 con-neural networks.
ference of the north american chapter of the as-sociation for computational linguistics: humanlanguage technologies, pages 300–309, san diego,california.
association for computational linguis-tics..thien huu nguyen and ralph grishman.
2015. eventdetection and domain adaptation with convolutionalneural networks.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing (volume 2: shortpapers), pages 365–371, beijing, china.
associa-tion for computational linguistics..thien huu nguyen and ralph grishman.
2018. graphconvolutional networks with argument-aware pool-in proceedings of theing for event detection.
32nd aaai conference on artiﬁcial intelligence, vol-ume 18, pages 5900–5907, new orleans, louisiana,usa..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 4658–4664, florence, italy.
associationfor computational linguistics..kyoko ohara.
2012. semantic annotations in japaneseframenet: comparing frames in japanese and en-in proceedings of the eighth internationalglish.
conference on language resources and evaluation(lrec’12), pages 1559–1562, istanbul, turkey.
eu-ropean language resources association (elra)..matthew e. peters, mark neumann, robert l. lo-gan iv, roy schwartz, vidur joshi, sameer singh,and noah a. smith.
2019. knowledge enhancedcontextual word representations.
in proceedings ofthe 2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language process-ing (emnlp-ijcnlp), pages 43–54, hong kong,china..jonas pfeiffer, aishwarya kamath, andreas rücklé,and iryna gurevych.
2020a.
kyunghyun cho,adapterfusion: non-destructive task composi-arxiv preprinttion forlearning.
transferarxiv:2005.00247..jonas pfeiffer, andreas rücklé, clifton poth, aish-ivan vuli´c, sebastian ruder,warya kamath,kyunghyun cho, and iryna gurevych.
2020b.
adapterhub: a framework for adapting transform-ers.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 46–54, online.
asso-ciation for computational linguistics..jonas pfeiffer, ivan vuli´c, iryna gurevych, and se-bastian ruder.
2020c.
mad-x: an adapter-basedframework for multi-task cross-lingual transfer.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7654–7673, online.
association for computa-tional linguistics..edoardo maria ponti, ivan vuli´c, goran glavaš, nikolamrkši´c, and anna korhonen.
2018. adversarialpropagation and zero-shot cross-lingual transfer ofin proceedings of theword vector specialization.
2018 conference on empirical methods in naturallanguage processing, pages 282–293, brussels, bel-gium..edoardo maria ponti, ivan vuli´c, goran glavaš, roireichart, and anna korhonen.
2019. cross-lingualsemantic specialization via lexical relation induction.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2206–2217, hong kong, china.
association for computa-tional linguistics..james pustejovsky, josé m. castano, robert ingria,roser sauri, robert j. gaizauskas, andrea set-zer, graham katz, and dragomir r. radev.
2003.timeml: robust speciﬁcation of event and tempo-ral expressions in text.
new directions in questionanswering, 3:28–34..james pustejovsky, robert ingria, roser sauri, josé m.castaño, jessica littman, robert j. gaizauskas, an-drea setzer, graham katz, and inderjeet mani.
2005.in the lan-the speciﬁcation language timeml.
guage of time: a reader, pages 545–557.
citeseer..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we know abouthow bert works.
transactions of the associationfor computational linguistics, 8:842–866..lei sha, feng qian, baobao chang, and zhifang sui.
2018.jointly extracting event triggers and argu-ments by dependency-bridge rnn and tensor-basedin proceedings of the 32ndargument interaction.
aaai conference on artiﬁcial intelligence, pages5916–5923, new orleans, louisiana, usa..carlos subirats and hiroaki sato.
2004..spanishin 4th internationalframenet and framesql.
conference on language resources and evalua-tion.
workshop on building lexical resources fromsemantically annotated corpora, lisbon, portugal.
citeseer..6964naushad uzzaman, hector llorens, leon derczyn-ski, james allen, marc verhagen, and james puste-jovsky.
2013. semeval-2013 task 1: tempeval-3:evaluating time expressions, events, and temporalin second joint conference on lexicalrelations.
and computational semantics (*sem), volume 2:proceedings of the seventh international workshopon semantic evaluation (semeval 2013), pages 1–9, atlanta, georgia, usa.
association for computa-tional linguistics..marc verhagen and james pustejovsky.
2008. tempo-ral processing with the tarsqi toolkit.
in coling2008: companion volume: demonstrations, pages189–192, manchester, uk.
coling 2008 organizingcommittee..marc verhagen, roser saurí, tommaso caselli, andjames pustejovsky.
2010. semeval-2010 task 13:in proceedings of the 5th interna-tempeval-2.
tional workshop on semantic evaluation, pages 57–62, uppsala, sweden.
association for computa-tional linguistics..ivan vuli´c, goran glavaš, nikola mrkši´c, and annakorhonen.
2018. post-specialisation: retroﬁttingvectors of words unseen in lexical resources.
in pro-ceedings of the 2018 conference of the north amer-ican chapter of the association for computationallinguistics: human language technologies, vol-ume 1 (long papers), pages 516–527, new orleans,louisiana.
association for computational linguis-tics..ivan vuli´c, nikola mrkši´c, and anna korhonen.
2017.cross-lingual induction and transfer of verb classesbased on word vector space specialisation.
in pro-ceedings of the 2017 conference on empirical meth-ods in natural language processing, pages 2546–2558, copenhagen, denmark.
association for com-putational linguistics..ivan vuli´c, edoardo maria ponti, anna korhonen, andgoran glavaš.
2021. lexfit: lexical ﬁne-tuning ofpretrained language models.
in proceedings of the59th annual meeting of the association for compu-tational linguistics, online.
association for compu-tational linguistics..ivan vuli´c, edoardo maria ponti, robert litschko,goran glavaš, and anna korhonen.
2020. probingpretrained language models for lexical semantics.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 7222–7240, online.
association for computa-tional linguistics..david wadden, ulme wennberg, yi luan, and han-naneh hajishirzi.
2019. entity, relation, and eventextraction with contextualized span representations.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 5784–5789, hong kong, china.
association for computa-tional linguistics..ruize wang, duyu tang, nan duan, zhongyu wei,xuanjing huang, cuihong cao, daxin jiang, mingzhou, et al.
2020a.
k-adapter:infusing knowl-edge into pre-trained models with adapters.
arxivpreprint arxiv:2002.01808..shike wang, yuchen fan, xiangying luo, and dongyu.
2020b.
shikeblcu at semeval-2020 task 2:an external knowledge-enhanced matrix for multi-lingual and cross-lingual lexical entailment.
in pro-ceedings of the fourteenth workshop on semanticevaluation, pages 255–262, barcelona (online).
in-ternational committee for computational linguis-tics..xiaozhi wang, xu han, zhiyuan liu, maosong sun,and peng li.
2019. adversarial training for weaklyin proceedings of thesupervised event detection.
2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 998–1008, minneapolis, min-nesota.
association for computational linguistics..xiaozhi wang, ziqi wang, xu han, wangyi jiang,rong han, zhiyuan liu, juanzi li, peng li, yankailin, and jie zhou.
2020c.
maven: a massive gen-eral domain event detection dataset.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1652–1671, online.
association for computationallinguistics..john wieting, mohit bansal, kevin gimpel, and karenlivescu.
2015. from paraphrase database to compo-sitional paraphrase model and back.
transactionsof the association for computational linguistics,3:345–358..haoran yan, xiaolong jin, xiangbin meng, jiafengguo, and xueqi cheng.
2019. event detection withmulti-order graph convolution and aggregated atten-in proceedings of the 2019 conference ontion.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5766–5770, hong kong, china.
association forcomputational linguistics..sen yang, dawei feng, linbo qiao, zhigang kan, anddongsheng li.
2019a.
exploring pre-trained lan-guage models for event extraction and generation.
in proceedings of the 57th annual meeting of theassociation for computational linguistics, pages5284–5294, florence, italy.
association for compu-tational linguistics..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019b.
xlnet: generalized autoregressive pretrain-ing for language understanding.
in advances in neu-ral information processing systems, pages 5753–5763, vancouver, canada..6965mo yu and mark dredze.
2014..improving lexicalembeddings with semantic knowledge.
in proceed-ings of the 52nd annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 545–550, baltimore, maryland.
associ-ation for computational linguistics..tongtao zhang, heng ji, and avirup sil.
2019. jointentity and event extraction with generative adversar-ial imitation learning.
data intelligence, 1(2):99–120..fanghua zheng.
2018. a corpus-based multidimen-sional analysis of linguistic features of truth and de-in proceedings of the 32nd paciﬁc asiaception.
conference on language, information and compu-tation, pages 841–848, hong kong.
association forcomputational linguistics..6966tempeval.
ace.
englishspanishchineseenglishchinesearabic.
train.
830,00551,51123,180529573356.test.
7,1745,4665,313404327.table 6: number of tokens (tempeval) and documents(ace) in the training and test sets..a frameworks for annotating event.
expressions.
two prominent frameworks for annotating eventexpressions are timeml (pustejovsky et al., 2003,2005) and the automatic content extraction (ace)(doddington et al., 2004).
timeml was developedas a rich markup language for annotating event andtemporal expressions, addressing the problems ofidentifying event predicates and anchoring them intime, determining their relative ordering and tempo-ral persistence (i.e., how long the consequences ofan event last), as well as tackling contextually un-derspeciﬁed temporal expressions (e.g., last month,two days ago).
currently available english corporaannotated based on the timeml scheme includethe timebank corpus (pustejovsky et al., 2003), ahuman annotated collection of 183 newswire texts(including 7,935 annotated events, comprisingboth punctual occurrences and states which ex-tend over time) and the aquaint corpus, with80 newswire documents grouped by their coveredstories, which allows tracing progress of eventsthrough time (derczynski, 2017).
both corpora,supplemented with a large, automatically timeml-annotated training corpus are used in the tempeval-3 task (verhagen and pustejovsky, 2008; uzzamanet al., 2013), which targets automatic identiﬁca-tion of temporal expressions, events, and temporalrelations..the ace dataset provides annotations for enti-ties, the relations between them, and for events inwhich they participate in newspaper and newswiretext.
for each event, it identiﬁes its lexical instanti-ation, i.e., the trigger, and its participants, i.e., thearguments, and the roles they play in the event.
forexample, an event type “conﬂict:attack” (“it couldswell to as much as $500 billion if we go to war iniraq.”), triggered by the noun “war”, involves twoarguments, the “attacker” (“we”) and the “place”(“iraq”), each of which is annotated with an entitylabel (“gpe:nation”)..b adapter training and.
hyperparameter search.
following pfeiffer et al.
(2020a), we train theadapters for 30 epochs using the adam algorithm(kingma and ba, 2015), a learning rate of 1e − 4and the adapter reduction factor of 16 (pfeifferet al., 2020a), i.e., d = 48. our batch size is 64,comprising 16 positive examples and 3 × 16 = 48negative examples (since k = 3)..we experimented with n ∈ {10, 15, 20, 30}training epochs, as well as an early stopping ap-proach using validation loss on a small held-outvalidation set as the stopping criterion, with a pa-tience argument p ∈ {2, 5}; we found the adapterstrained for the full 30 epochs to perform most con-sistently across tasks..the size of the training batch varies based on thevalue of k negative examples generated from thestarting batch b of positive pairs: e.g., by generat-ing k = 3 negative examples for each of 8 positiveexamples in the starting batch we end up with atraining batch of total size 8+3∗8 = 32. we exper-imented with starting batches of size b ∈ {8, 16}and found the conﬁguration k = 3, b = 16 toyield the strongest results (reported in this paper)..c vtrans: technical details.
first, we automatically translate the verbs by re-trieving their nearest neighbour in the target lan-guage from the shared cross-lingual embeddingspace, aligned using the relaxed cross-domainsimilarity local scaling (rcsls) model of joulinet al.
(2018).
such translation procedure is liableto error due to an imperfect cross-lingual embed-ding space as well as polysemy and out-of-contextword translation.
we dwarf these issues in thesecond step, where we purify the set of noisilytranslated target language verb pairs by means of aneural lexico-semantic relation prediction model,the specialization tensor model (glavaš and vuli´c,2018a), here adjusted for binary classiﬁcation.
wetrain the stm for the same task as verb adaptersduring verb knowledge injection (§2.2): to distin-guish (positive) verb pairs from the same englishvn class/fn frame from those from different vnclasses/fn frames.
in training, the input to stmare static word embeddings of english verbs takenfrom a shared cross-lingual word embedding space.
we then make predictions in the target languageby feeding vectors of target language verbs (fromnoisily translated verb pairs), taken from the same.
6967cross-lingual word embedding space, as input forstm.
we provide more details on stm training inwhat follows..stm training details.
we train the stm usingthe sets of english positive examples from eachlexical resource (table 1).
negative examples aregenerated using controlled sampling (see §2.2),using a k = 2 [cc] conﬁguration, ensuring thatgenerated negatives do not constitute positive con-straints in the global set.
we use the pre-trained300-dimensional static distributional word vectorscomputed on wikipedia data using the fasttextmodel (bojanowski et al., 2017), cross-linguallyaligned using the rcsls model of joulin et al.
(2018), to induce the shared cross-lingual embed-ding space for each source-target language pair.
the stm is trained using the adam optimizer(kingma and ba, 2015), a learning rate l = 1e − 4,a batch size of 32 (positive and negative) trainingexamples, for a maximum of 10 iterations.
we setthe values of other training hyperparameters as inponti et al.
(2019), i.e., the number of specialisationtensor slices k = 5 and the size of the specialisedvectors h = 300..d sequential fine-tuning details.
in the sequential ﬁne-tuning setup, we ﬁrst trainthe full cased variant of the bert-based modelon the vn/fn data.
we generate negative exam-ples using the strongest performing conﬁgurationof sampling parameters: k = 3 [ccr].
we trainthe model for 4 epochs using the adam algorithm(kingma and ba, 2015), a learning rate of 2e − 5with 1000 warmup steps and a batch size of 64.next, we ﬁne-tune the vn/fn-pretrained model onthe two downstream tasks.
for task 1, we train for10 epochs in batches of 32 and a learning rate of1e−4 and a maximum input sequence t = 128. intask 2, we ﬁnd an optimal hyperparameter conﬁg-uration for each language-setup combination fromthe grid: learning rate l ∈ {1e − 5, 1e − 6}, epochsn ∈ {3, 5, 10, 25, 50}, batch size b ∈ {8, 16}, withmaximum input sequence length of t = 128..e additional results.
table 9 presents the results of monolingual eval-uation substituting the monolingual target lan-guage bert with the massively multilingualencoder, with or without the fn/vn adapterstrained on (noisy) target language verb signal (ar-mbert/zh-mbert).
table 10 provides addi-.
+fnseq.
+vnseq.
ar mbert-zs.
ar-bert.
ar-mbert.
zh mbert-zs.
zh-bert.
zh-mbert.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
16.115.11.21.0.
70.565.032.929.5.
64.655.624.620.5.
41.629.64.64.0.
75.669.026.825.9.
72.664.127.025.8.
15.214.11.11.0.
69.163.730.227.6.
65.557.123.419.9.
39.927.87.66.4.
75.768.526.125.0.
72.662.224.923.9.table 7: results on arabic and chinese ace test setsfor the sequential ﬁne-tuning setup for zero-shot (zs)transfer with mbert and the vtrans transfer ap-proach with language-speciﬁc bert (ar-bert / zh-bert) or mbert, on noisily translated fn/vn data(§2.4).
f1 scores averaged over 5 runs; signiﬁcant im-provements (paired t-test; p < 0.05) over both base-lines marked in bold..tional results for spanish task 1 (tempeval) usingan alternative multilingual encoder, xlm-r (large)(conneau et al., 2020), as the underlying model(trained with the following hyperparameters: learn-ing rate l = 2e − 5, batch size b ∈ {16, 32}).
tables 8 and 7 include the results for the sequentialﬁne-tuning setup for task 1 (tempeval) and task 2(ace), respectively.
table 11 shows the results onspanish tempeval for different conﬁgurations ofspanish bert with an added spanish fn-adaptertrained on spanish framenet data..+fnseq.
+vnseq.
spanish mbert-zs.
es-bertes-mbert.
zh-bertzh-mbert.
37.377.873.3.
51.482.380.1.chinese mbert-zs.
37.477.673.2.
50.082.279.1.table 8: results on spanish and chinese tempeval testsets for monolingual sequential ﬁne-tuning.
signiﬁcantimprovements over the baselines in table 3 in bold..6968arabic.
ar-mbert.
chinese.
zh-mbert.
t-identt-classarg-identarg-class.
t-identt-classarg-identarg-class.
64.956.225.421.3.
74.162.926.224.8.
65.256.525.421.9.
74.462.926.325.3.fft.
+random +fn.
+vn.
+random +fn.
+vn.
65.157.427.223.0.
74.064.327.226.2.
65.656.224.619.9.
73.363.628.026.4.ta.
20.714.4––.
62.252.4––.
18.014.0––.
62.652.2––.
23.216.5––.
63.854.3––.
19.514.5––.
62.554.0––.
table 9: results on arabic and chinese ace test sets for full monolingual ﬁne-tuning (fft) and the task adapter(ta) setup with underlying mbert and vtrans fn/vn adapters.
f1 scores averaged over 5 runs; signiﬁcantimprovements (paired t-test; p < 0.05) over both baselines marked in bold..spanish xlm-r-zses-xlm-r.fft.
42.277.1.
+random +fn.
42.377.1.
42.777.6.
+vn.
42.077.6.ta.
40.774.8.
+random +fn.
40.373.9.
40.974.5.
+vn.
40.875.4.table 10: results on spanish tempeval test sets for full ﬁne-tuning (fft) and the task adapter (ta) setup, forzero-shot (zs) transfer and monolingual target language evaluation with xlm-r large, with fn/vn adapterstrained on vtrans-translated verb pairs (see §2.4).
f1 scores are averaged over 10 runs; signiﬁcant improvements(paired t-test; p < 0.05) over both baselines marked in bold..fft+fnes.
ta+fnes.
2ta+fnes.
es-bert.
78.0 (+0.4).
70.9 (+0.2).
73.8 (+0.2).
table 11: results (f1 scores) on spanish tempeval fordifferent conﬁgurations of spanish bert with addedspanish fn-adapter (fnes), trained on clean spanishfn constraints.
numbers in brackets indicate relativeperformance w.r.t.
the corresponding setup with fn-adapter trained on (a larger set of) noisy spanish con-straints obtained through automatic translation of verbpairs from english fn (vtrans approach)..6969