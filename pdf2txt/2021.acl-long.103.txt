attention calibration for transformer in neural machine translation.
yu lu1,2∗, jiali zeng3, jiajun zhang1,2†, shuangzhi wu3 and mu li31 national laboratory of pattern recognition, institute of automation, cas, beijing, china2 school of artiﬁcial intelligence, university of chinese academy of sciences, beijing, china3 tencent cloud xiaowei, beijing, china{yu.lu, jjzhang}@nlpr.ia.ac.cn{lemonzeng, frostwu, ethanlli}@tencent.com.
abstract.
attention mechanisms have achieved substan-tial improvements in neural machine transla-tion by dynamically selecting relevant inputsfor different predictions.
however, recent stud-ies have questioned the attention mechanisms’capability for discovering decisive inputs.
inthis paper, we propose to calibrate the attentionweights by introducing a mask perturbationmodel that automatically evaluates each in-put’s contribution to the model outputs.
we in-crease the attention weights assigned to the in-dispensable tokens, whose removal leads to adramatic performance decrease.
the extensiveexperiments on the transformer-based transla-tion have demonstrated the effectiveness of ourmodel.
we further ﬁnd that the calibrated at-tention weights are more uniform at lower lay-ers to collect multiple information while moreconcentrated on the speciﬁc inputs at higherlayers.
detailed analyses also show a greatneed for calibration in the attention weightswith high entropy where the model is uncon-ﬁdent about its decision1..figure 1: examples of the attention weights before andafter calibration.
“in ” denotes the timestep after theprediction “in”.
the dashed boxes indicate the inputswhich should receive more attention measured by ourmask perturbation model..1.introduction.
attention mechanisms have been ubiquitous in neu-ral machine translation (nmt) (bahdanau et al.,2015; vaswani et al., 2017).
it dynamically en-codes source-side information by inducing a con-ditional distribution over inputs, where the onesthat are most relevant to the current translation areexpected to receive more attention..however, many studies doubt whether highly-attended inputs have a large impact on the modeloutputs.
on the one hand, erasing the representa-tions accorded high attention weights do not neces-sarily lead to a performance decrease (serrano and.
∗work done while the author was an intern at tencent.
† corresponding author.
1https://github.com/yulu-dada/attention-calibration-.
nmt.
smith, 2019), which can be attributed to that unim-portant words (e.g., punctuations) are frequentlyassigned with high attention weights (mohanku-mar et al., 2020).
on the other hand, jain and wal-lace (2019) state that attention weights are incon-sistent with other feature importance metrics in textclassiﬁcation tasks.
it further proves that attentionmechanisms are incapable of precisely identifyingdecisive inputs for each prediction, which wouldresult in wrong-translation or over-translation innmt (tu et al., 2016).
we take figure 1 as anexample.
after producing the target-side word“deaths”, attention mechanisms wrongly attributemost attention to the “(cid:104)eos(cid:105)”, making parts ofthe source sentence untranslated..in this paper, we propose to calibrate the vanillaattention mechanism by focusing more on key in-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1288–1298august1–6,2021.©2021associationforcomputationallinguistics1288远郊连日大雪多人死亡交通中断<eos>远郊连日大雪多人死亡交通中断heavy snow in countryside caused many deathssrc:base:deaths _ [before]deaths _ [after]deaths _ [mask]in _[before]in _[after]in _[mask]heavy snow incountrysidehas caused many deathsandtraffic interruptionours:days of heavy snow in countryside left many deaths and transportation disruptedref:远郊连日大雪多人死亡交通中断<eos>puts.
to test what inputs affect the model predictionmost, we tend to observe how the model decisionchanges as perturbing parts of inputs.
we deﬁnethe perturbation operation as applying a learnablemask to scale each attention weight.
then, we per-form a “deletion game”, which aims to ﬁnd thesmallest perturbation extents that cause the signif-icant quality degradation.
in this manner, we canﬁnd the most informative inputs for the prediction.
based on the results detected by the mask pertur-bation model, we further calibrate attention weightsby reallocating more attention to informative inputs.
we design three fusion methods to incorporate thecalibrated attention weights into original attentionweights: (1) ﬁxed weighted sum, (2) annealinglearning, and (3) gating mechanism.
the mask per-turbation model and nmt model are jointly trained,while the attention weights in nmt are correctedbased on the actual contributions measured by themask perturbation model..recall the example in figure 1. after producingthe target word “in”, our mask perturbation modelﬁnds that the source word “远郊 [countryside]”with a high attention weight is exactly the decisiveinput for the prediction.
therefore, we strengthenthe corresponding attention weight of “远郊 [coun-tryside]”.
however, after the prediction “deaths”,the highly-attended “(cid:104)eos(cid:105)” is not the decisive in-put at the current step.
we redistribute the attentionweights to the source words (“交通 [trafﬁc]” and“中断 [interruption]”) which receive little attentionbut are important for the subsequent translation dis-covered by our mask perturbation model.
aftercalibration, the missing source information “trafﬁcinterruption” is well-translated..we conduct extensive experiments to verify ourmethod’s effectiveness on transformer-based trans-lation (nist zh⇒en, wmt14 en⇒de, wmt16en⇔ro, wmt17 en⇔fi, and en⇔lv).
experi-mental results show that our calibration methodscan signiﬁcantly boost performance.
we further vi-sualize calibrated attention weights and investigatewhen attention weights need to be corrected..the contributions of this paper are three-fold:.
• we propose a mask perturbation model to au-tomatically assess each input’s contributionfor translation, which is simple yet effective..• detailed analyses show that calibrated atten-tion weights are more uniform at lower layerswhile more focused at the higher layers.
high-entropy attention weights are found to havegreat needs for calibration at all layers..2 background.
in this section, we ﬁrst brieﬂy introduce the frame-work of transformer (vaswani et al., 2017) with afocus on the multi-head attention (mha).
then wepresent an analysis of the learned attention weights,the correlation with feature importance measures,which motivates our ideas discussed afterward..2.1 transformer architecture.
the transformer is an encoder-decoder frameworkwith stacking layers of attention blocks.
the en-coder ﬁrst transforms an input x = {x1, x2, ...xn}to a sequence of continues representations h ={h1, h2, ...hn}, from which the decoder gener-ates an output sequence y = {y1, y2, ...ym}..multi-head attention between encoder and de-coder enables each prediction to attend overallinputs from different representation subspacesjointly.
for the single head, we ﬁrst project h ={h1, h2, ...hn} to keys k and values v usingdifferent linear projections.
at the t-th position,we project the hidden state of the previous decoderlayer to the query vector qt.
then we multiply qtby keys k to obtain an attention at, which is usedto calculate a weighted sum of values v ..attn (qt, k, v ) = at ∗ v(cid:19).
at = softmax.
(cid:18) qtkt√dk.
(1).
where dk is the dimension of the keys.
for mha,we use different projections to obtain the queries,keys, and values representations for each head..it is noted that transformer (base model) per-forms n = 6 cross-lingual attention layers and em-ploys h = 8 parallel attention heads for each time.
thus we implement our methods on n × h atten-tion operations separately.
for simplicity, we nextdenote the query, keys, and values as qt, k, v re-gardless of what layers and heads they come from..• we design three methods to calibrate originalattention weights by highlighting the informa-tive inputs, which are experimentally provedto outperform strong baselines..2.2 disagreement between attention weights.
and feature importance metrics.
attention mechanisms provide a distribution overthe context representations of inputs, which are.
12893.1 mask perturbation model.
to search the source-side inputs that the modelrelies on to produce the output, we can observehow the model prediction changes as perturbingdifferent parts of the input sentence.
we apply amask to scale each input’s attention weight, whichsimulates the process of perturbation..formally, let mt be a mask at t-th step.
the.
perturbed attention weight ap.
t is calculated as:.
(3).
apt = mt (cid:12) at + (1 − mt) (cid:12) µ0where µ0 is a uniform distribution (an average vec-tor of 1n ) and (cid:12) denotes element-wise multiplica-tion.
the mask mt is obtained based on the hiddenstate in the decoder qt and keys k:(cid:32).
(cid:33).
mt = σ.qtw q(kw k)t√dk.
(4).
here, σ(·) is the sigmoid function.
a smaller valueof mt means a larger perturbation extent on orig-inal attention weights.
considering the structureof multi-head attention in transformer, w q andw k differ among layers and heads..to test the effect of perturbing distinct regions ofinputs, we borrow the idea “deletion game” to ﬁndthe smallest perturbation extent, which leads to asigniﬁcant performance decrease.
the objectivefunction of mask perturbation model is:.
l (θm) = −lnmt (ap.
t , θ) + αlc (θm).
(5).
where θ denotes the parameters of the originaltransformer.
lnmt (apt , θ) is the cross-entropyloss of the translation model when using perturbedattention weights apt .
θm = {w q, w k} repre-sents the parameters of mask perturbation model.
the ﬁrst term indicates that the perturbation op-eration aims to harm the translation quality.
thesecond one serves as a penalty term to encouragemost of the mask to be turned off (perturb inputsas few as possible)..(6).
lc (θm) = (cid:107)1 − mt(cid:107)2the perturbation extent is determined by the hyper-parameter α. notably, earlier studies employ masksand “deletion game” as the analytical tools to ex-plore the importance of each attention head (fongand vedaldi, 2017) or the contributions of the pix-els in the ﬁgure to the model outputs (voita et al.,2019).
however, we extend to probing the inputs’contributions to the model prediction in nmt andfurther use the masks to calibrate the attentionmechanisms based on the analytical results..figure 2: the mean kendall-τ correlation between at-tention weights (a) and gradient importance metrics(τit) on zh⇒en translation..often presented as communicating the relative im-portance of inputs.
however, recent work has cau-tioned against whether the inputs accorded highattention weights decide the model outputs (jainand wallace, 2019).
our analysis examines thecorrelation with attention weights and feature im-portance metrics in nmt to test if the attentionmechanisms focus on the decisive inputs.
we ap-ply gradient-based methods (simonyan et al., 2014;li et al., 2016) to measure the importance of eachcontextual representation hi for model output yt:.
τit = |∇hip(yt|x1:n)|.
(2).
we train a baseline transformer model on nistzh⇒en dataset and extract the averaged attentionweights over heads..figure 2 reports the statistics of kendall-τ corre-lation for each attention layer, where the observedcorrelation is all modest (0 indicates no correlation,while 1 implies perfect concordance).
the inconsis-tency with feature importance metrics reveals thatthe high-attention inputs are not always responsiblefor the model prediction.
it further motivates uswhether we can calibrate the attention weights tofocus more on the decisive inputs to achieve bettertranslation..3 our method.
we aim to make the attention mechanism more fo-cused on the informative inputs.
the ﬁrst step is todiscover what inputs are essential for the model pre-diction.
as shown in figure 3, we design a maskperturbation model to worsen the performancewith limited perturbation on the original attentionweights.
by doing this, we can automatically detectwhat inputs decide the model outputs.
then, wedesign an attention calibration network (acn)to correct the original attention weights, highlight-ing the decisive inputs based on what inputs areperturbed by the mask perturbation model..1290123456layer0.1650.1700.1750.180kendall- correlationwith it0.17680.16850.16830.16650.16910.1697figure 3: the overview of the framework.
the mask perturbation model is trained to perturb the attention weightsof decisive inputs to harm the performance.
acn looks for what inputs are perturbed and enhance the correspond-ing attention weights..3.2 attention calibration network.
as aforementioned, our mask perturbation modelremoves the most informative input to deterioratethe translation by setting the corresponding masksto zero.
in other words, a smaller mask means alarger perturbation, namely a more signiﬁcant im-pact on the prediction.
we propose to calibrate theoriginal attention weights in nmt by highlightingthe essential inputs for each model prediction..formally, the calibrated attention weight act.can be designed as:.
t = at (cid:12) e1−mtac.
(7).
we increase the attention weights of key inputswhich suffer large perturbation extents.
the atten-tion weights of other less-informative inputs arecorrespondingly decreased.
we design three meth-ods to incorporate act into the original one at toobtain combined attention weights acomb.
:.
t.• fixed weighed sum.
in this method, the cal-ibrated attention weights are added to the orig-inal attention weights of ﬁxed ratio λ as:.
acombt.= softmax(at + λ ∗ act ).
(8).
• annealing learning.
considering the maskperturbation model is not well-trained at theearly stage, we expect the effect of act to besmaller at ﬁrst and gradually grow with thetraining step s. to this end, we use annealinglearning to control the ratio of ac.
t as:.
acombt.= γ(s) ∗ at + (1 − γ(s)) ∗ actγ(s) = e−s/105.
(9).
• gating mechanism.
we propose a calibra-tion gate to dynamically select the amount of.
the information from the perturbation modelin the decoding process..acombt.= gt ∗ at + (1 − gt) ∗ act.gt = σ(qtw g + bg).
(10).
where w g and bg are trainable parametersvary among different layers and heads..3.3 training.
our mask perturbation model and nmt model arejointly optimized.
as shown in figure 3, the maskperturbation model is trained to worsen the per-formance by limited perturbation on the attentionweights (equation 5).
given what inputs are per-turbed, we can ﬁgure out the decisive inputs foreach model prediction and calibrate the originalattention weights in the nmt model by acn.
withthe calibrated attention weights, the nmt model isﬁnally optimized by:m(cid:88).
logp(yt|y<t, x; acomb.
, θ).
lnmt (θ) = −.
t.t=1.
(11)during testing, the mask perturbation model alsohelps identify the informative inputs based on thehidden state in the decoder at each step (as seenin equation 4).
the nmt model decodes with thecalibrated attention weights.
moreover, our methodcan provide the saliency map between inputs andoutputs based on the generated mask, an accessiblemeasurement of the inputs’ contributions to themodel predictions..4 experiments.
we evaluate our method in ldc chinese-english(zh⇒en), wmt14 english-german (en⇒de),wmt16 english-romanian (en⇔ro), wmt17english-finnish (en⇔fi) and english-latvian(en⇔lv)..1291attentionmechanismsattention calibration network (acn)mask perturbation model original attention weightsmaskcalibrated attention weightsperturbed attention weightskqnmt decoderworseperformance after perturbationbetterperformanceafter calibration（𝑎𝑐）（𝑎𝑝）（𝑎）（𝑚）sourceldc1wmt142.
wmt173.
lang.
zh⇒enen⇒deen⇒lvlv⇒enen⇒fifi⇒enen⇒roro⇒en.
traindev.
2.09m 8784.54m 3000.test47893003.vocab.
32k37k.
4.46m 2003.
2001.
2.63m 3000.
3002.
37k.
32k.
32k.
wmt164.
0.61m 1999.
1999.table 1: statistics of the datasets..4.1 dataset.
we tokenize the corpora using a script frommoses (koehn et al., 2007).
byte pair encoding(bpe) (sennrich et al., 2016) is applied to all lan-guage pairs to construct a join vocabulary exceptfor zh⇒en where the source and target languagesare separately encoded..for zh⇒en, we remove the sentences of morethan 50 words.
we use nist 2002 as validationset, nist 2003-2006 as the testbed.
for en⇒de,newstest2013 and newstest2014 are set as valida-tion and test sets.
we use the standard 4-grambleu (papineni et al., 2002) on the true-case out-put to score the performance.
for en⇔ro, weuse newsdev2016 and newstest2016 as develop-ment and test sets.
for en⇔lv and en⇔fi, news-dev2017 and newstest2017 are validation set andtest set.
see table 1 for statistics of the data..4.2 settingswe implement the described models with fairseq5toolkit for training and evaluating.
we experimentwith transformer base (vaswani et al., 2017): hid-den size dmodel = 512, 6 encoder and decoder lay-ers, 8 attention heads and 2048 feed-forward inner-layer dimension.
the dropout rate of the residualconnection is 0.1 except for zh⇒en (0.3).
duringtraining, we use label smoothing of value (cid:15)ls = 0.1and employ the adam (β1 = 0.9, β2 = 0.998) forparameter optimization with a scheduled learningrate of 4,000 warm-up steps.
all the experimentslast for 150k steps except for small-scale en⇔rotranslation tasks (100k).
for evaluation, we aver-age the last ten checkpoints and use beam search.
1the corpora includes ldc2000t50, ldc2002t01,ldc2002e18, ldc2003e07, ldc2003e14, ldc2003t17and ldc2004t07.
following previous work, we use case-insensitive tokenized bleu to evaluate the performance.
2http://www.statmt.org/wmt14/translation-task.html3http://www.statmt.org/wmt17/translation-task.html4http://www.statmt.org/wmt16/translation-task.html5https://github.com/pytorch/fairseq.
modelgnmt (wu et al., 2016)‡conv (gehring et al., 2017)‡attisall (vaswani et al., 2017)‡(feng et al., 2020)‡(weng et al., 2020)‡our implemented baseline.
ours.
fixedannealgate.
test24.6125.1627.327.5527.727.3727.3828.1∗27.75.table 2: the comparison of our model, transformerbaselines and related work on the wmt14 en⇒de us-ing case-sensitive bleu.
results with ‡ mark are takenfrom the corresponding papers.
“∗” indicates the gainsare statistically signiﬁcant than baselines with p<0.05..(beam size 4, length penalty 0.6) for inference..besides, the hyperparameter λ in equation 8 de-cides how much the calibrated attention weights areincorporated in the fixed weighted sum method.
we set λ = 0.1 in all experiments for comparison..4.3 main results.
to comprehensively compare with the existingbaselines and similar work, we report the resultsof some competitive models including gnmt (wuet al., 2016), conv (gehring et al., 2017) and at-tisall (vaswani et al., 2017) on wmt14 en⇒detranslation task.
besides, we also compare ourmethod against related researches about introduc-ing word alignment information to guide transla-tion (weng et al., 2020; feng et al., 2020).
aspresented in table 2, our method exhibits betterperformance than the above models.
unlike su-pervised attention with external word alignment,our model yields a signiﬁcant gain by looking intowhat inputs affect the model’s internal training..table 3 shows the translation quality measuredin bleu score for nist zh⇒en.
our proposedmodel signiﬁcantly outperforms the baseline by0.96 (mt02), 0.84 (mt03), 0.58 (mt04), 1.02(mt05) and 0.76 (mt06), respectively..we also conduct our experiments on wmt17en⇔fi and en⇔lv.
as shown in table 4, ourmethods improve the performance over baselineby 0.54 bleu (en⇒fi), 0.6 bleu (fi⇒en), 0.57bleu (en⇒lv) and 0.95 bleu (lv⇒en).
forthe small-scale wmt16 en⇔ro, our methodsachieve a substantial improvement of 1.44 morebleu (en⇒ro) and 0.95 bleu (ro⇒en).
com-.
1292modelbaseline.
ours.
fixedannealgate.
dev mt03 mt04 mt05 mt0647.2248.5848.5647.8948.5648.4247.4948.8548.2247.98∗49.16∗49.52∗.
49.9550.3250.97∗50.78∗.
49.5849.4149.7350.42∗.
ave48.2448.4448.7449.00∗.
table 3: evaluation of translation quality for zh⇒en translation using case-insensitive bleu score.
“∗” indicatesthe gains are statistically signiﬁcant than baselines with p<0.05..modelbaseline.
ours.
fixedannealgate.
en⇒lv lv⇒en en⇒fi fi⇒en en⇒ro ro⇒en16.2616.5416.3516.83∗.
17.7618.45∗18.1218.71∗.
22.5623.123.27∗24.00∗.
26.0726.226.3926.67∗.
27.5328.0228.2∗28.48∗.
22.0122.4222.422.55∗.
table 4: evaluation of translation quality for wmt17 en⇔fi, wmt17 en⇔lv and wmt16 en⇔ro usingcase-insensitive bleu score.
“∗” indicates the gains are statistically signiﬁcant than baselines with p<0.05..figure 4: experimental results on the validation set andthe averaged value of generated masks with respect todifferent hyperparameter α on zh⇒en translation task(gate mechanism)..figure 5: the mean kendall-τ correlation between at-tention weights (a), the masks (m) generated by ourmask perturbation model and gradient importance mea-sures (τit) on zh⇒en..pared to the large-scale dataset, the insufﬁcienttraining data make it harder to learn the relation-ship between inputs and outputs, leaving a greaterneed for calibrating attention weights..overall, our proposed model signiﬁcantly out-performs the strong baselines, especially for thesmall-scale dataset.
more importantly, the parame-ter size is tiny (6m), which cannot add much costto the training and inference process..effect of fusion methods for three fusionmethods, the ﬁxed weighted sum has a limited gain.
annealing learning is comparatively more stable,which reduces the impact of acn when the maskperturbation model is not well-trained at the initialstage.
but it is challenging to design an annealingstrategy that can be applied to all language pairs.
gate mechanism mostly achieves the best perfor-mance for dynamically controlling the proportionsof original and calibrated attention weights..effect of hyperparameter the hyperparameterα in the loss function of the mask perturbationmodel (as in equation 5) decides how much maskswould turn on to perturb the original attentionweights.
figure 4 exhibits the average value ofgenerated masks across heads as the function ofthe setting of α. a larger α forces the model toturn off most masks, which makes the value of themask closer to 1, resulting in a smaller perturbationextent on the attention weights..correlation with feature importance metricsfigure 5 reports the correlation between our gener-ated mask (m) and the gradient-based importancemeasures6 (τit).
we ﬁnd that the masks are rela-tively closer to the gradient-based importance mea-sures than the original attention weights, which.
6though these measures are insufﬁcient for telling whatinputs are important (kindermans et al., 2019), they do pro-vide measures of individual feature importance with knownsemantics (ross et al., 2017)..12930.0200.0250.0300.0350.0400.0454647484950bleu47.7349.149.5248.9148.7947.740.300.350.400.45averaged value of masksbleuaveraged value of masks123456layer0.160.180.200.220.240.26kendall- correlation with it0.17680.16850.16830.16650.16910.16970.22060.22170.22610.21950.21870.2211attention weightsgenerated masks(a) 1-st layer.
figure 6: the jsd between attention weights beforeand after calibration at different layers on zh⇒en anden⇒de translation.
note that the overall jsd for eachlanguage pair is decided by the hyperparameter α, butthe calibration extents of layers are learned by acn..prove the effectiveness of our mask perturbationmodel to discover decisive inputs..(b) 6-th layer.
5 analysis.
in this section, we explain how our proposedmethod helps produce better translation by investi-gating: (1) what attention weights need to calibrateand (2) calibrated attention weights are more fo-cused or more uniform.
speciﬁcally, we delve intothe differences between layers, which give insightsinto the attention mechanism’s inner working.
weconduct analyses on zh⇒en nist03 and en⇒denewstest2014 to understand our model from differ-ent perspectives..we apply jensen-shannon divergence (jsd) be-tween attention weights before and after calibrationto measure the calibration extent:.
jsd (a1, a2) =.
kl[a1(cid:107)a] +.
kl[a2(cid:107)a] (12).
12.
12.
2.where a = a1+a2.
a high jsd means the cali-brated attention weights are distant from the orig-inal one.
besides, we use the entropy changes ofattention weights to test whether the calibrated at-tention weights become more uniform or focused..(cid:52)ent (a1, a2) = ent (a1) − ent (a2).
(13).
where ent (a) = − (cid:80)mscribe the uncertainty of the distribution..i=1 ailogai, a metric to de-.
5.1 what attention weights need to calibrate?.
figure 7: the jsd between attention weights beforeand after calibration with respect to the entropy of orig-inal attention distributions..attention layers are not well-trained in the originalnmt model and have an urgent need to calibrate.
figure 6 depicts the jsd between original and cal-ibrated attention weights.
we ﬁnd high jsd forhigh layers and low jsd for low layers in zh⇒entask.
however, a different pattern is observed inen⇒de task, where jsd in the high layer is lowerthan in the low layers.
we speculate that the dif-ference is due to the language discrepancy and wewill explore this phenomenon in our future work..high or low entropy?
more focused contribu-tions of inputs suggest that the model is more con-ﬁdent about the choice of important tokens (voitaet al., 2020).
we attempt to validate whether theattention weights are more likely to be calibratedwhen the nmt model is uncertain about its de-cision.
figure 7 shows the positive relationshipbetween calibration extent and the entropy of at-tention weights.
take the 6-th attention layer inzh⇒en translation as an example (as seen in fig-ure 7(b)).
the averaged jsd is 0.0084 for theattention weights in rang [0,0.8], while the valueis 0.0324 for the attention weights where the en-tropy is larger than 3.2. these ﬁndings can also beobserved at different attention layers and languagepairs..high or low layers?
concerning the roles of dif-ferent attention layers, one natural question is what.
we infer that a higher entropy indicates the nmtmodel relies on multiple inputs to generate the.
12941234560.0240.0260.0280.030jsd0.02490.02680.02430.02960.02950.0276zhen123456layer0.040.060.08jsd0.07840.06950.07080.05810.04280.0548ende0-0.80.8-1.61.6-2.42.4-3.23.2-400.020.040.060.080.1jsdthe entropy of the original attention weightszhen0-0.50.5-0.750.75-11-22-40.000.020.040.060.080.100.12ende0-0.80.8-1.61.6-2.42.4-3.23.2-400.020.040.060.080.1jsdthe entropy of the original attention weightszhen0-0.50.5-0.750.75-11-22-40.000.020.040.060.080.100.120.14endelayer123456all.
zh⇒en+ 0.0203- 0.011- 0.0023- 0.0224- 0.0303- 0.0083- 0.0336.en⇒de+ 0.1846+ 0.0762+ 0.0207- 0.0336- 0.0595- 0.01- 0.0224.table 5: entropy differences ((cid:52)ent) between the orig-inal and calibrated attention weights.
“+” means thecalibrated attention weights are more disperse.
“-” in-dicates attention weights are sharper after calibration..translation, which increases the probability of infor-mation redundancy or error signals.
our proposedmodel is more likely to calibrate these attentionweights to makes the nmt model pay more atten-tion to the informative inputs..5.2 calibrated attention weights are more.
dispersed or focused?.
there are multiple reasons why the calibrated atten-tion weights can boost performance.
section 4.3states that our generated masks are much closerto the gradient-based feature importance measurescompared with attention weights.
on the otherhand, we present the entropy differences of theoriginal and calibrated attention weights in table 5where the entropy of attention weights are overallsmaller after calibration.
however, the changesvary across layers.
for en⇒de translation, the cal-ibrated attention weights are more uniform at 1-3layers and more focused at 4-6 layers, while the at-tention weights become more focused for all layersexcept the 1-st layer on zh⇒en task.
these ﬁnd-ings prove that each attention layer plays a differentrole in the decoding process.
the low layers gener-ally grasp information from various inputs, whilethe high layers look for some particular words tiedto the model predictions..6 related work.
the attention mechanism is ﬁrst introduced to aug-ment vanilla recurrent network (bahdanau et al.,2015; luong et al., 2015), which are then the back-bone of state-of-the-art transformer (vaswani et al.,2017) for nmt.
it yields better performance andprovides a window into how a model is operat-ing (belinkov and glass, 2019; du et al., 2020).
this section reviews the recent researches on ana-lyzing and improving attention mechanisms..the attention debate many recent studies havespawned interest in whether attention weights faith-fully represent each input token’s responsibilityfor model prediction.
serrano and smith ﬂipthe model’s decision by permuting some small at-tention weights, with high-weighted componentsnot being the reason for the decision.
somework (jain and wallace, 2019; vashishth et al.,2019) ﬁnd a weak correlation between attentionscores and other well-ground feature importancemetrics, specially gradient-based and leave-one-outmethods, in various text classiﬁcation tasks.
wealso present the correlation analysis in the less-discussed transformer-based nmt and reach asimilar conclusion.
as opposed to the critiques ofregarding attention weights as explanation, wiegr-effe and pinter claim that the trained attentionmechanisms do learn something meaningful aboutthe relationship between inputs and outputs, suchas syntactic information (raganato and tiedemann,2018; vig and belinkov, 2019; pham et al., 2019)..can attention be improved?
there is plenty ofwork on supervising attention weights with lexi-cal probabilities (arthur et al., 2016), word align-ment (chen et al., 2016; liu et al., 2016; mi et al.,2016; cohn et al., 2016; garg et al., 2019; fenget al., 2020), human rationales (strout et al., 2019)and sparsity regularization (zhang et al., 2019).
un-like them, we never introduce any external knowl-edge but highlight the inputs whose removal wouldsigniﬁcantly decrease transformer’s performance.
another work line aims to make attention betterindicative of the inputs’ importance (kitada andiyatomi, 2020; tutek and snajder, 2020; mohanku-mar et al., 2020) which is designed for analysiswith no signiﬁcant performance gain, while ourmethods incorporate the analytical results to en-hance the nmt performance..7 conclusion.
in this paper, we present a mask perturbation modelto automatically discover the decisive inputs for themodel prediction.
we propose three methods to cal-ibrate the attention mechanism by focusing on thediscovered vital inputs.
extensive experimental re-sults show that our approaches obtain signiﬁcantimprovements over the state-of-the-art system.
an-alytical results indicate that our proposed meth-ods make the low layer’s attention weights moredispersed to grasp multiple information.
in con-trast, high-layer attention weights become more.
1295focused on speciﬁc essential inputs.
we furtherﬁnd a greater need for calibration in the originalattention weights with high entropy.
our workprovides insights on future work about learningmore useful information via attention mechanismsin other attention-based frameworks..acknowledgments.
the research work has been funded by the natu-ral science foundation of china under grant no.
u1836221 and the national key research anddevelopment program of china under grant no.
2018yfc0823404.
the research work in this paperhas also been supported by beijing academy ofartiﬁcial intelligence (baai2019qn0504).
thiswork is also supported by youth innovation promo-tion association cas no.
2017172..references.
philip arthur, graham neubig, and satoshi nakamura.
2016.incorporating discrete translation lexiconsinto neural machine translation.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 1557–1567, austin,texas.
association for computational linguistics..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlyin 3rd inter-learning to align and translate.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..yonatan belinkov and james r. glass.
2019. analysismethods in neural language processing: a survey.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages3348–3354.
association for computational linguis-tics..wenhu chen, evgeny matusov, shahram khadivi,and jan-thorsten peter.
2016. guided alignmenttraining for topic-aware neural machine translation.
corr, abs/1607.01628..trevor cohn, cong duy vu hoang, ekaterina vy-molova, kaisheng yao, chris dyer, and gholamrezaincorporating structural alignmenthaffari.
2016.biases into an attentional neural translation model.
in naacl hlt 2016, the 2016 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, san diego california, usa, june 12-17, 2016,pages 876–885.
the association for computationallinguistics..mengnan du, ninghao liu, and xia hu.
2020. tech-niques for interpretable machine learning.
commun.
acm, 63(1):68–77..yang feng, wanying xie, shuhao gu, chenze shao,wen zhang, zhengxin yang, and dong yu.
2020.modeling ﬂuency and faithfulness for diverse neu-ral machine translation.
in the thirty-fourth aaaiconference on artiﬁcial intelligence, aaai 2020,the thirty-second innovative applications of arti-ﬁcial intelligence conference, iaai 2020, the tenthaaai symposium on educational advances in arti-ﬁcial intelligence, eaai 2020, new york, ny, usa,february 7-12, 2020, pages 59–66.
aaai press..ruth c fong and andrea vedaldi.
2017. interpretableexplanations of black boxes by meaningful perturba-tion.
in proceedings of the ieee international con-ference on computer vision, pages 3429–3437..sarthak garg, stephan peitz, udhyakumar nallasamy,and matthias paulik.
2019. jointly learning to alignand translate with transformer models.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing, emnlp-ijcnlp 2019, hong kong, china,november 3-7, 2019, pages 4452–4461.
associationfor computational linguistics..jonas gehring, michael auli, david grangier, de-nis yarats, and yann n dauphin.
2017. convolu-in proceed-tional sequence to sequence learning.
ings of the 34th international conference on ma-chine learning, volume 70, pages 1243–1252..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556.
association for computa-tional linguistics..pieter-jan kindermans, sara hooker,.
julius ade-bayo, maximilian alber, kristof t. sch¨utt, svend¨ahne, dumitru erhan, and been kim.
2019.in woj-the (un)reliability of saliency methods.
ciech samek, gr´egoire montavon, andrea vedaldi,lars kai hansen, and klaus-robert m¨uller, edi-tors, explainable ai: interpreting, explaining andvisualizing deep learning, volume 11700 of lec-ture notes in computer science, pages 267–280.
springer..shunsuke kitada and hitoshi iyatomi.
2020. at-tention meets perturbations: robust and inter-pretable attention with adversarial training.
corr,abs/2009.12064..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondrej bojar, alexandraconstantin, and evan herbst.
2007. moses: open.
1296source toolkit for statistical machine translation.
inacl 2007, proceedings of the 45th annual meet-ing of the association for computational linguistics,june 23-30, 2007, prague, czech republic.
the as-sociation for computational linguistics..jiwei li, xinlei chen, eduard hovy, and dan jurafsky.
2016. visualizing and understanding neural modelsin nlp.
in proceedings of the 2016 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 681–691.
association for computa-tional linguistics..lemao liu, masao utiyama, andrew finch, and ei-ichiro sumita.
2016. neural machine translationwith supervised attention.
in proceedings of col-ing 2016, the 26th international conference oncomputational linguistics: technical papers, pages3093–3102.
the coling 2016 organizing com-mittee..thang luong, hieu pham, and christopher d. man-ning.
2015. effective approaches to attention-basedin proceedings of theneural machine translation.
2015 conference on empirical methods in naturallanguage processing, emnlp 2015, lisbon, portu-gal, september 17-21, 2015, pages 1412–1421.
as-sociation for computational linguistics..haitao mi, zhiguo wang, and abe ittycheriah.
2016.supervised attentions for neural machine translation.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages2283–2288.
association for computational linguis-tics..akash kumar mohankumar, preksha nema, sharannarasimhan, mitesh m. khapra, balaji vasan srini-vasan, and balaraman ravindran.
2020. towardstransparent and explainable attention models.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4206–4216. association for computational linguistics..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in proceedings of the40th annual meeting of the association for compu-tational linguistics, pages 311–318.
association forcomputational linguistics..thuong-hai pham, dominik mach´acek, and ondrejbojar.
2019. promoting the knowledge of sourcesyntax in transformer nmt is not needed.
com-putaci´on y sistemas, 23(3)..alessandro raganato and j¨org tiedemann.
2018. ananalysis of encoder representations in transformer-in proceedings of thebased machine translation.
2018 emnlp workshop blackboxnlp: analyzingand interpreting neural networks for nlp, pages287–297.
association for computational linguis-tics..andrew slavin ross, michael c. hughes, and finaledoshi-velez.
2017. right for the right reasons:training differentiable models by constraining theirin proceedings of the twenty-sixthexplanations.
international joint conference on artiﬁcial intelli-gence, ijcai-17, pages 2662–2670..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics (volume 1: long papers), pages 1715–1725.
association for computational linguistics..soﬁa serrano and noah a. smith.
2019. is attentionin proceedings of the 57th annualinterpretable?
meeting of the association for computational lin-guistics, pages 2931–2951.
association for compu-tational linguistics..karen simonyan, andrea vedaldi, and andrew zisser-man.
2014. deep inside convolutional networks: vi-sualising image classiﬁcation models and saliencyin 2nd international conference on learn-maps.
ing representations, iclr 2014, banff, ab, canada,april 14-16, 2014, workshop track proceedings..julia strout, ye zhang, and raymond mooney.
2019.do human rationales improve machine explana-in proceedings of the 2019 acl work-tions?
shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 56–62.
association forcomputational linguistics..zhaopeng tu, zhengdong lu, yang liu, xiaohua liu,and hang li.
2016. modeling coverage for neuralmachine translation.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics, acl 2016, august 7-12, 2016, berlin,germany, volume 1: long papers.
the associationfor computer linguistics..martin tutek and jan snajder.
2020. staying true toyour word: (how) can attention become explanation?
in proceedings of the 5th workshop on representa-tion learning for nlp, pages 131–142.
associationfor computational linguistics..shikhar vashishth, shyam upadhyay, gaurav singhatten-tomar,and manaal faruqui.
2019.corr,tion interpretability across nlp tasks.
abs/1909.11218..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, undeﬁne-dukasz kaiser, and illia polosukhin.
2017. attentionis all you need.
in proceedings of the 31st interna-tional conference on neural information processingsystems, pages 6000–6010.
curran associates inc..jesse vig and yonatan belinkov.
2019. analyzingthe structure of attention in a transformer languagein proceedings of the 2019 acl work-model.
shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 63–76.
association forcomputational linguistics..1297elena voita, rico sennrich, and ivan titov.
2020.analyzing the source and target contributions topredictions in neural machine translation.
corr,abs/2010.10907..elena voita, david talbot, fedor moiseev, rico sen-nrich, and ivan titov.
2019. analyzing multi-headself-attention: specialized heads do the heavy lift-in proceedings of theing, the rest can be pruned.
57th conference of the association for computa-tional linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages5797–5808.
association for computational linguis-tics..rongxiang weng, heng yu, xiangpeng wei, and wei-hua luo.
2020. towards enhancing faithfulness forin proceedings of theneural machine translation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 2675–2684,online.
association for computational linguistics..sarah wiegreffe and yuval pinter.
2019. attention isnot not explanation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 11–20.
association for computa-tional linguistics..yonghui wu, mike schuster, zhifeng chen, quoc vle, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, et al.
2016. google’s neural machinetranslation system: bridging the gap between hu-arxiv preprintman and machine translation.
arxiv:1609.08144..jiajun zhang, yang zhao, haoran li, and chengqingzong.
2019. attention with sparsity regularizationfor neural machine translation and summarization.
ieee acm trans.
audio speech lang.
process.,27(3):507–518..1298