a sequence-to-sequence approach to dialogue state tracking.
yue feng† ∗ yang wang‡ hang li‡†university college london, london, uk‡bytedance ai lab, beijing, china† yue.feng.20@ucl.ac.uk‡{wangyang.127, lihang.lh}@bytedance.com.
abstract.
this paper is concerned with dialogue statetracking (dst) in a task-oriented dialogue sys-tem.
building a dst module that is highlyeffective is still a challenging issue, althoughsigniﬁcant progresses have been made recently.
this paper proposes a new approach to dia-logue state tracking, referred to as seq2seq-du, which formalizes dst as a sequence-to-sequence problem.
seq2seq-du employs twobert-based encoders to respectively encodethe utterances in the dialogue and the descrip-tions of schemas, an attender to calculate atten-tions between the utterance embeddings andthe schema embeddings, and a decoder to gen-erate pointers to represent the current state ofdialogue.
seq2seq-du has the following ad-vantages.
it can jointly model intents, slots,and slot values; it can leverage the rich rep-resentations of utterances and schemas basedon bert; it can effectively deal with cate-gorical and non-categorical slots, and unseenschemas.
in addition, seq2seq-du can also beused in the nlu (natural language understand-ing) module of a dialogue system.
experimen-tal results on benchmark datasets in differentsettings (sgd, multiwoz2.2, multiwoz2.1,woz2.0, dstc2, m2m, snips, and atis)show that seq2seq-du outperforms the exist-ing methods..1.introduction.
a task-oriented dialogue system usually consistsof several modules: natural language understand-ing (nlu), dialogue state tracking (dst), dialoguepolicy (policy), and natural language generation(nlg).
we consider dst and also nlu in thispaper.
in nlu, a semantic frame representing thecontent of user utterance is created in each turn.
∗the work was done when the ﬁrst author was an intern at.
bytedance ai lab..figure 1: an example of dialogue state tracking.
givena dialogue history that contains user utterances and sys-tem utterances, and descriptions of schema that containall possible intents and slot-value pairs, a dialogue statefor the current turn is created which is represented byintents and slot-value pairs.
there are slot values ob-tained from the schema (categorical) as well as slot val-ues extracted from the utterances (non-categorical).
#4,#6, etc denote pointers..of dialogue.
in dst, several semantic frames rep-resenting the ‘states’ of dialogue are created andupdated in multiple turns of dialogue.
domainknowledge in dialogues is represented by a repre-sentation referred to as schema, which consists ofpossible intents, slots, and slot values.
slot valuescan be in a pre-deﬁned set, with the correspondingslot being referred to as categorical slot, and theycan also be from an open set, with the correspond-ing slot being referred to as non-categorical slot.
figure 1 shows an example of dst..we think that a dst module (and an nlu mod-ule) should have the following abilities.
(1) global,the model can jointly represent intents, slots, andslot values.
(2) represenable, it has strong capa-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1714–1725august1–6,2021.©2021associationforcomputationallinguistics1714<start>, “findflight”, <sep>, “depart”, #4, #4, <sep>, “arrive”, #6, #7,<sep>, “seating_class”, #economy, <sep>, “depart_date”, #17,#18, <end><start>, “findflight”, <sep>, “depart”, #4, #4, <sep>, “arrive”, #6, #7, <sep>, “seating_class”, #economy, <end>slots:“depart”:starting city for the trip.“arrive”:ending city for the trip.“seating_class”: seating class for the booking.possible_values: ["economy", "business", "first class”]“depart_date”: start date for the trip.intents:“findflight”:search for one-way flights to a destination.“reserveflight”:reserve a one-way flight.findeconomyflightsfrombeijingtolosangeles.sure,whatdatesareyoulookingfor?flyingonmay2.ok,ifoundadeltaflightfor3500dollars.usersystemstateservice:“flight”:find your next flight.schemability to represent knowledge for the task, on topof a pre-trained language model like bert.
(3)scalable, the model can deal with categorical andnon-categorical slots and unseen schemas..many methods have been proposed for dst(wu et al., 2019; zhong et al., 2018; mrkˇsi´c et al.,2017; goo et al., 2018).
there are two lines ofrelevant research.
(1) to enhance the scalability ofdst, a problem formulation, referred to as schema-guided dialogue, is proposed.
in the setting, it isassumed that descriptions on schemas in naturallanguage across multiple domains are given andutilized.
consequently, a number of methods aredeveloped to make use of schema descriptions to in-crease the scalability of dst (rastogi et al., 2019;zang et al., 2020; noroozi et al., 2020).
the meth-ods regard dst as a classiﬁcation and/or an ex-traction problem and independently infer the intentand slot value pairs for the current turn.
therefore,the proposed models are generally representableand scalable, but not global.
(2) there are also afew methods which view dst as a sequence to se-quence problem.
some methods sequentially inferthe intent and slot value pairs for the current turnon the basis of dialogue history and usually employa hierarchical structure (not based on bert) forthe inference (lei et al., 2018; ren et al., 2019;chen et al., 2020b).
recently, a new approach isproposed which formalizes the tasks in dialogueas sequence prediction problems using a uniﬁedlanguage model (based on gpt-2) (hosseini-aslet al., 2020).
the method cannot deal with un-seen schemas and intents, however, and thus is notscalable..we propose a novel approach to dst, referredto as seq2seq-du (sequence-to-sequence for dia-logue understanding), which combines the advan-tages of the existing approaches.
to the best ofour knowledge, there was no previous work whichstudied the approach.
we think that dst shouldbe formalized as a sequence to sequence or ‘trans-lation’ problem in which the utterances in the dia-logue are transformed into semantic frames.
in thisway, the intents, slots, and slot values can be jointlymodeled.
moreover, nlu can also be viewed asa special case of dst and thus seq2seq-du canalso be applied to nlu.
we note that very recentlythe effectiveness of the sequence to sequence ap-proach has also been veriﬁed in other languageunderstanding tasks (paolini et al., 2021)..to encode the utterances in the dialogue, a bertbased encoder to encode the schema descriptions,an attender to calculate attentions between the utter-ance embeddings and schema embeddings, and adecoder to generate pointers of items representingthe intents and slots-value pairs of state..seq2seq-du has the following advantages.
(1)global: it relies on the sequence to sequence frame-work to simultaneously model the intents, slots,and slot-values.
(2) representable: it employsbert (devlin et al., 2019) to learn and utilize bet-ter representations of not only the current utterancebut also the previous utterances in the dialogue.
ifschema descriptions are available, it also employsbert for the learning and utilization of their repre-sentations.
(3) scalable: it uses the pointer genera-tion mechanism, as in the pointer network (vinyalset al., 2015), to create representations of intents,slots, and slot-values, no matter whether the slotsare categorical or non-categorical, and whether theschemas are unseen or not..experimental results on benchmark datasetsshow that seq2seq-du1 performs much better thanthe baselines on sgd, multiwoz2.2, and multi-woz2.1 in multi-turn dialogue with schema de-scriptions, is superior to bert-dst on woz2.0,dstc2, and m2m, in multi-turn dialogue with-out schema descriptions, and works equally wellas joint bert on atis and snips in single turndialogue (in fact, it degenerates to joint bert)..2 related work.
there has been a large amount of work ontask-oriented dialogue, especially dialogue statetracking and naturallanguage understanding(eg., (zhang et al., 2020; huang et al., 2020; chenet al., 2017)).
table 1 makes a summary of existingmethods on dst.
we also indicate the methods onwhich we make comparison in our experiments..2.1 dialogue state tracking.
previous approaches mainly focus on encodingof the dialogue context and employ deep neu-ral networks such as cnn, rnn, and lstm-rnn to independently infer the values of slotsin dst (mrkˇsi´c et al., 2017; xu and hu, 2018;zhong et al., 2018; ren et al., 2018; rastogi et al.,2017; ramadan et al., 2018; wu et al., 2019; zhanget al., 2019; heck et al., 2020).
the approaches.
1the code is available at https://github.com/.
seq2seq-du comprises a bert-based encoder.
sweetalyssum/seq2seq-du..1715data setssgd.
comparisonyes.
modelfastsgd (noroozi et al., 2020).
sgd baseline (rastogi et al., 2019).
trippy (heck et al., 2020).
trade (wu et al., 2019).
ds-dst (zhang et al., 2019).
bert-dst (chao and lane, 2019).
statenet (ren et al., 2018).
glad (zhong et al., 2018).
belief tracking (ramadan et al., 2018).
dst+lu (rastogi et al., 2018)joint bert (chen et al., 2019)slot-gated (goo et al., 2018).
atten.-birnn (liu and lane, 2016).
rnn-lstm (hakkani-t¨ur et al., 2016).
sequicity (lei et al., 2018).
comer (ren et al., 2019).
credit (chen et al., 2020b).
simpletod (hosseini-asl et al., 2020).
characteristicsbert-based model, employs two carry-over procedures and multi-head atten-tions to model schema descriptions.
bert-based model, predictions are made over a dynamic set of intents andslots, using their descriptions.
bert-based model, make use of various copy mechanisms to ﬁll slots withvalues.
generate dialogue states from utterances using a copy mechanism, facilitatingknowledge transfer for new schema elements.
bert-based model, to classify over a candidate list or ﬁnd values from textspans.
use bert as dialogue context encoder and makes parameter sharing acrossslots.
independent of number of values, shares parameters across slots and usespre-trained word vectors.
use global modules to share parameters across slots and uses local modules toretrain slot-speciﬁc parameters.
utilize semantic similarity between dialogue utterances and ontology, andinformation is shared across domains..sentations of user utterance and dialogue context.
select candidates for each slot, while candidates are generated by nlu.
a joint intent classiﬁcation and slot ﬁlling model based on bert.
use a slot gate, models relation between intent and slot vectors to createsemantic frames.
attention-based model, explores several strategies for alignment between intentclassiﬁcation and slot labeling.
use rnn with lstm cells to create complete semantic frames from userutterances.
two-stage sequence-to-sequence model based on copynet, conducts bothdialogue state tracking and response generation.
bert-based hierarchical encoder-decoder model, generates state sequencebased on user utterancehierarchical encoder-decoder model, views dst as a sequence generationproblem.
a uniﬁed sequence-to-sequence model based on gpt-2, conducts dialogue statetracking, dialogue action prediction, and response generation..sgd and multiwoz2.2.
multiwoz2.2.
multiwoz2.2.
multiwoz2.2.
dstc2, woz2.0, and m2m.
dstc2 and woz2.0.
dstc2 and woz2.0.
dstc2 and woz2.0.
m2matis and snipsatis and snips.
atis and snips.
atis and snips.
camrest676 and kvret.
woz2.0 and multiwoz2.0.
multiwoz2.0 and multiwoz2.1.
multiwoz2.0 and multiwoz2.1.
neural belief tracker (mrkˇsi´c et al., 2017) conduct reasoning on pre-trained word vectors, and combines them into repre-.
dstc2 and woz2.0.
yes.
yes.
yes.
yes.
yes.
yes.
yes.
yes.
yes.
yesyesyes.
yes.
yes.
no.
yes.
no.
yes.
table 1: summary of existing methods on dst..cannot deal with unseen schemas in new domains,however.
to cope with the problem, a new direc-tion called schema-guided dialogue is proposedrecently, which assumes that natural language de-scriptions of schemas are provided and can be usedto help transfer knowledge across domains.
assuch, a number of methods are developed in therecent dialogue competition sgd (rastogi et al.,2019; zang et al., 2020; noroozi et al., 2020; chenet al., 2020a).
our work is partially motivated bythe sgd initiative.
our model seq2seq-du isunique in that it formalizes schema-guided dst asa sequence-to-sequence problem using bert andpointer generation..in fact, sequence-to-sequence models are alsoutilized in dst.
sequicity (lei et al., 2018) is atwo-step sequence to sequence model which ﬁrstencodes the dialogue history and generates a be-lief span, and then generates a language responsefrom the belief span.
comer (ren et al., 2019)and credit (chen et al., 2020b) are hierarchi-cal sequence-to-sequence models which representthe intents and slot-value pairs in a hierarchicalway, and employ a multi-stage decoder.
simple-tod (hosseini-asl et al., 2020) is a uniﬁed ap-proach to task-oriented dialogue which employs.
a single and causal language model to performsequence prediction in dst, policy, and nlg.
our proposed approach also uses a sequence-to-sequence model.
there are signiﬁcant differencesbetween our model seq2seq-du and the existingmodels.
first, there is no hierarchy in decodingof seq2seq-du.
a ﬂat structure on top of bertappears to be sufﬁcient for jointly capturing theintents, slots, and values.
second, the decoder inseq2seq-du generates pointers instead of tokens,and thus can easily and effectively handle categor-ical slots, non-categorical slots, as well as unseenschemas..2.2 natural language understanding.
traditionally the problem of nlu is decomposedinto two independent issues, namely classiﬁca-tion of intents and sequence labeling of slot-valuepairs (liu and lane, 2016; hakkani-t¨ur et al.,2016).
for example, deep neural network com-bined with conditional random ﬁeld is employedfor the task (yao et al., 2014).
recently the pre-trained language model bert (chen et al., 2019) isexploited to further enhance the accuracy.
methodsare also proposed which can jointly train and utilizeclassiﬁcation and sequence labeling models (chen.
1716figure 2: the architecture of seq2seq-du, containing utterance encoder, schema encoder, utterance-schema atten-der, and state decoder..et al., 2019; goo et al., 2018).
in this paper, weview nlu as special case of dst and employ ourmodel seq2seq-du to perform nlu.
seq2seq-ducan degenerate to a bert based nlu model..3 our approach.
our approach seq2seq-du formalizes dialoguestate tracking as a sequence to sequence problemusing bert and pointer generation.
as shownin figure 2, seq2seq-du consists of an utteranceencoder, a schema encoder, an utterance schemaattender, and a state decoder.
in each turn of dia-logue, the utterance encoder transforms the currentuser utterance and the previous utterances in thedialogue into a sequence of utterance embeddingsusing bert; the schema encoder transforms theschema descriptions into a set of schema embed-dings also using bert; the utterance schema at-tender calculates attentions between the utteranceembeddings and the schema embeddings to cre-ate attended utterance and schema representations;ﬁnally, the state decoder sequentially generates astate representation on the basis of the attended rep-resentations using lstm and pointer generation..3.1 utterance encoder.
the utterance encoder takes the current user utter-ance as well as the previous utterances (user andsystem utterances) in the dialogue (a sequence oftokens) as input and employs bert to construct asequence of utterance embeddings.
the relationsbetween the current utterance and the previous ut-terances are captured by the encoder..the input of the encoder is a sequence of tokenswith length n , denoted as x = (x1, ..., xn ).
the.
ﬁrst token x1 is [cls], followed by the tokens ofthe current user utterance and the tokens of the pre-vious utterances, separated by [sep].
the outputis a sequence of embeddings also with length n ,denoted as d = (d1, ..., dn ) and referred to as ut-terance embeddings, with one embedding for eachtoken..3.2 schema encoder.
the schema encoder takes the descriptions of in-tents, slots, and categorical slot values (a set of com-bined sequences of tokens) as input and employsbert to construct a set of schema embeddings..schemaintentslotvalue.
sequence 1service descriptionservice descriptionslot description.
sequence 2intent descriptionslot descriptionvalue.
table 2: descriptions for a dialogue schema.
two com-bined descriptions are used for describing an intent, aslot, or a value in the schema..suppose that there are i intents, s slots, andv categorical slot values in the schemas.
eachschema element is described by two descriptions asoutlined in table 2. the input is a set of combinedsequences of tokens, denoted as y = {y1, ..., ym }.
note that m = i + s + v .
each combined se-quence starts with [cls], followed by the tokens ofthe two descriptions with [sep] as a separator.
theﬁnal representation of [cls] is used as the embed-ding of the input intent, slot, or slot value.
the out-put is a set of embeddings, and all the embeddingsare called schema embeddings e = {e1, ..., em }..1717utterance-schema attenderd1d2d3dn…e1e2e3em…da1da2da3dan…ea1ea2ea3eam…dedaeabert-based utterance encodercurrent user utteranceprevious utterancesclsx1x2sepx3x4dialogue…d1d2d3dn-1dnbert-based schema encoderintentslotslot descriptionvalueclsx1x2sepx3x4valueservice descriptionslot descriptionclsx1x2sepx3x4service descriptionintent descriptionclsx1x2sepx3x4ejutterance encoderschema encoderlstmlstmlstmlstmlstmlstm<intent><slot1><value1><slot2><value2><eos><bos>ℎ"ℎ#ℎ$ℎ%ℎ&ℎ'("(#($(%(&()*+,-state decoder<intent><slot1><value1><slot2><value2>."/".#/#.$/$.%/%.&/&.
'/'the schema encoder in fact adopts the same ap-proach of schema encoding as in (rastogi et al.,2019).
there are two advantages with the ap-proach.
first, the encoder can be trained acrossdifferent domains.
schema descriptions in differ-ent domains can be utilized together.
second, oncethe encoder is ﬁne-tuned, it can be used to processunseen schemas with new intents, slots, and slotvalues..3.3 utterance-schema attender.
the utterance-schema attender takes the sequenceof utterance embeddings and the set of schema em-beddings as input and calculates schema-attendedutterance representations and utterance-attendedin this way, informa-schema representations.
tion from the utterances and information from theschemas are fused..first, the attender constructs an attention matrix,indicating the similarities between utterance em-beddings and schema embeddings.
given the i-thutterance token embedding di and j-th schema em-bedding ej, it calculates the similarity as follows,.
a(i, j) = r(cid:124)tanh(w1di + w2ej),where r, w1, w2 are trainable parameters..(1).
the attender then normalizes each row of matrixa as a probability distribution, to obtain matrixa. each row represents the attention weights ofschema elements with respect to an utterance to-ken.
then the schema-attended utterance represen-(cid:124)tations are calculated as da = ea.
the attenderalso normalizes each column of matrix a as a prob-ability distribution, to obtain matrix (cid:101)a. each col-umn represents the attention weights of utterancetokens with respect to a schema element.
thenthe utterance-attended schema representations arecalculated as ea = d (cid:101)a..3.4 state decoder.
the state decoder sequentially generates a state rep-resentation (semantic frame) for the current turn,which is represented as a sequence of pointers to el-ements of the schemas and tokens of the utterances(cf., figure 1).
the sequence can then be eitherre-formalized as a semantic frame in dialogue statetracking2,.
[intent; (slot1, value1); (slot2, value2); ...],.
2for simplicity, we assume here that there is only onesemantic frame in each turn.
in principle, there can be multipleframes..or a sequence of labels in nlu (intent-labelingand slot-ﬁlling).
the pointers point to the elementsof intents, slots, and slot values in the schema de-scriptions (categorical slot values), as well as thetokens in the utterances (non-categorical slot val-ues).
the elements in the schemas can be eitherwords or phrases, and the tokens in the utterancesform spans for extraction of slot values..is.
the.
state decoder.
an lstm usingpointer (vinyals et al., 2015) and attention (bah-danau et al., 2015).
it takes the two representationsda and ea as input.
at each decode step t, thedecoder receives the embedding of the previousitem wt−1, the utterance context vector ut, theschema context vector st, and the previous hiddenstate ht−1, and produces the current hidden stateht:.
ht = lstm(wt−1, ht−1, ut, st)..(2).
we adopt the attention function in (bahdanauet al., 2015) to calculate the context vectors as fol-lows,.
ut = attend(ht−1, da, da),st = attend(ht−1, ea, ea)..(3).
(4).
the decoder then generates a pointer from the setof pointers in the schema elements and the tokensof the utterances on the basis of the hidden stateht.
speciﬁcally, it generates a pointer of item waccording to the following distribution,.
zw = q(cid:124)tanh(u1ht + u2kw),p (#w) = softmax(zw),.
(5).
(6).
where #w is the pointer of item w, kw is the repre-sentation of item w either in the utterance represen-tations da or in the schema representations ea, q,u1, and u2 are trainable parameters, and softmaxis calculated over all possible pointers..during decoding, the decoder employs beamsearch to ﬁnd the best sequences of pointers interms of probability of sequence..3.5 training.
the training of seq2seq-du follows the standardprocedure of sequence-to-sequence.
the only dif-ference is that it is always conditioned on theschema descriptions.
each instance in trainingconsists of the current utterance and the previousutterances, and the state representation (sequenceof pointers) for the current turn.
two pre-trained.
1718characteristicsno.
of domainsno.
of dialoguestotal no.
of turnsavg.
turns per dialogueavg.
tokens per turnno.
of categorical slotsno.
of non-categorical slotshave schema descriptionhave unseen schemas in test set.
sgd multiwoz2.2 multiwoz2.1 woz2.0 dstc2 m2m atis1616,142329,96420.449.7553162yesyes.
78438113,55613.4613.38370yesno.
88,438113,55613.4613.132140yesno.
11,61223,35414.498.5430nono.
21,50014,7969.868.24014nono.
-4,4784,478111.280120nono.
16004,4727.4511.2430nono.
snips-13,08413,08419.09072nono.
table 3: statistics of datasets in experiments.
numbers are those of training datasets..bert models are used for representations of utter-ances and schema descriptions respectively.
thebert models are then ﬁne-tuned in the trainingprocess.
cross-entropy loss is utilized to measurethe loss of generating a sequence..4 experiments.
4.1 datasets.
we conduct experiments using the benchmarkdatasets on task-oriented dialogue.
sgd (rastogiet al., 2019) and multiwoz2.2 (zang et al., 2020)are datasets for dst; they include schemas withcategorical slots and non-categorical slots in multi-ple domains and natural language descriptions onthe schemas, as shown in table 2. in particular,sgd includes unseen schemas in the test set.
mul-tiwoz2.1 (eric et al., 2020) is the previous versionof multiwoz2.2, which only has categorical slotsin multiple domains.
woz2.0 (wen et al., 2017)and dstc2 (henderson et al., 2014) are datasetsfor dst; they contain schemas with only categor-ical slots in a single domain.
m2m (shah et al.,2018) is a dataset for dst and it has span annota-tions for slot values in multiple domains.
atis (turet al., 2010) and snips (coucke et al., 2018) aredatasets for nlu in single-turn dialogues in a sin-gle domain.
table 3 gives the statics of datasets inthe experiments..4.2 baselines and variants.
we make comparison between our approach andthe state-of-the-art methods on the datasets.
sgd, multiwoz2.2 and multiwoz2.1: wecompare seq2seqdu with six state-of-the-art meth-ods on sgd, multiwoz2.2 and multiwoz2.1,which utilize schema descriptions, span-basedand candidate-based methods, uniﬁed seq2seqmodel and bert: fastsgt (noroozi et al., 2020),sgdbaseline (rastogi et al., 2019), trippy (hecket al., 2020), simpletod (hosseini-asl et al.,.
2020), trade (wu et al., 2019), and ds-dst (zhang et al., 2019).
woz2.0 and dstc2: our approach is comparedagainst the state-of-the-art methods on woz2.0and dstc2, including those using a hierarchi-cal seq2seq model and bert: comer (renet al., 2019), bert-dst (chao and lane, 2019),statenet (ren et al., 2018), glad (zhong et al.,2018), belief tracking (ramadan et al., 2018), andneural belief tracker (mrkˇsi´c et al., 2017).
m2m: we evaluate our approach and the state-of-the-art methods on m2m, which respectivelyemploy a bert-based architecture and a jointly-trained language understanding model, bert-dst (chao and lane, 2019) and dst+lu (rastogiet al., 2018).
atis and snips: we make comparison betweenour approach and the state-of-the-art methods onatis and snips for nlu within the sequence la-beling framework, including joint bert (chenet al., 2019), slot-gated (goo et al., 2018),atten.-birnn (liu and lane, 2016), and rnn-lstm (hakkani-t¨ur et al., 2016)..we also include two variants of seq2seq-du.
the differences are whether to use the schema de-scriptions, and the formation of dialogue state.
seq2seq-du-w/oschema: it is used for datasetsthat do not have schema descriptions.
it only con-tains utterance encoder and state decoder.
seq2seq-du-seqlabel: it is used for nlu in asingle-turn dialogue.
it views the problem as se-quence labeling, and only contains the utteranceencoder and state decoder..4.3 evaluation measures.
we make use of the following metrics in evaluation.
intent accuracy: percentage of turns in dialoguefor which the intent is correctly identiﬁed.
joint goal accuracy: percentage of turns forwhich all the slots are correctly identiﬁed.
for non-.
1719categorical slots, a fuzzy matching score is used onsgd and exact match are used on the other datasetsto keep the numbers comparable with other works.
slot f1: f1 score to evaluate accuracy of slot se-quence labeling..4.4 training.
the.
use.
wepre-trained bert model([bert-base, uncased]), which has 12 hid-den layers of 768 units and 12 self-attention headsto encode utterances and schema descriptions.
the hidden size of lstm decoder is also 768.the dropout probability is 0.1. we also use beamsearch for decoding, with a beam size of 5. thebatch size is set to 8. adam (kingma and ba,2014) is used for optimization with an initiallearning rate of 1e-4.
hyper parameters are chosenusing the validation dataset in all cases..the training curves of all models are shown in.
appendix a..4.5 experimental results.
tables 4, 5, 6, and 7 show the results.
one can seethat seq2seq-du performs signiﬁcantly better thanthe baselines in dst and performs equally well asthe baselines in nlu..dst is carried out in different settings in sgd,multiwoz2.2, multiwoz2.1, woz2.0, dstc2,and m2m.
in all cases, seq2seq-du works sig-niﬁcantly better than the baselines.
the resultsindicate that seq2seq-du is really a general andeffective model for dst, which can be applied tomultiple settings.
speciﬁcally, seq2seq-du canleverage the schema descriptions for dst whenthey are available (sgd and multiwoz2.2, multi-woz2.1)3. it can work well in zero-shot learningto deal with unseen schemas (sgd).
it can alsoeffectively handle categorical slots (multiwoz2.1,woz2.0 and dstc2) and non-categorical slots(m2m).
it appears that the success of seq2seq-du is due to its suitable architecture design witha sequence-to-sequence framework, bert-basedencoders, utterance-schema attender, and pointergeneration decoder..nlu is formalized as sequence labeling inatis and snips.
seq2seq-du is degenerated toseq2seq-du-seqlabel, which is equivalent to thebaseline of joint bert.
the results suggest that it is.
3there are better performing systems in the sgd competi-tion.
the systems are not based on single methods and thusare not directly comparable with our method..the case.
specially, the performances of seq2seq-du are comparable with joint bert, indicatingthat seq2seq-du can also be employed in nlu..model.
sgd-baselinetradeds-dstfastsgtsimpletodtrippyseq2seq-du.
sgdjoint ga int acc0.906--0.903--0.910.
0.254--0.292--0.301.multiwoz2.2joint ga int acc.
multiwoz2.1joint ga int acc.
0.4200.4540.517--0.5350.544.
------0.909.
0.4340.4600.512-0.5140.5530.561.
------0.911.table 4: accuracies of seq2seq-du and baselineson sgd, multiwoz2.2 and multiwoz2.1 datasets.
seq2seq-du outperforms baselines in terms of all met-rics..model.
woz2.0dstc2joint ga joint ga.neural belief trackerbelief trackinggladstatenetbert-dstcomerseq2seq-du-w/oschema.
0.8420.8550.8810.8890.8770.8860.912.
0.734-0.7450.7550.693-0.850.table 5: accuracies of seq2seq-du and baselineson woz2.0 and dstc2 datasets.
seq2seq-du-w/oschema performs signiﬁcantly better than the base-lines..model.
m2mjoint ga int acc.
dst+lubert-dstseq2seq-du-w/oschema.
0.7670.8690.909.
--0.997.table 6: accuracies of seq2seq-du and baselines onm2m dataset.
seq2seq-du-w/oschema signiﬁcantlyoutperforms the baselines..model.
rnn-lstmatten.-birnnslot-gatedjoint bertseq2seq-du-seqlabel.
atis.
snips.
slot f10.9430.9420.9520.9610.955.int acc slot f10.8730.9260.8780.9110.8880.9410.9700.9750.9680.965.int acc0.9690.9670.9700.9860.990.table 7: accuracies of seq2seq-du and baselines onatis and snips datasets.
seq2seq-du-seqlabel per-forms comparably with joint bert..17204.6 ablation study.
we also conduct ablation study on seq2seq-du.
we validate the effects of three factors: bert-based encoder, utterance-schema attention, andpointer generation decoder.
the results indicatethat all the components of seq2seq-du are indis-pensable..effect of bert.
to investigate the effectiveness of using bert inthe utterance encoder and schema encoder, we re-place bert with bi-directional lstm and run themodel on sgd and multiwoz2.2.
as shown infigure 3, the performance of the bilstm-basedmodel seq2seq-du-w/obert in terms of joint gaand int.
acc decreases signiﬁcantly compared withseq2seq-du.
it indicates that the bert-based en-coders can create and utilize more accurate repre-sentations for dialogue understanding..effect of attention.
to investigate the effectiveness of using atten-tion, we compare seq2seq-du with seq2seq-du-w/oattention which eliminates the attention mecha-nism, seq2seq-du-w/schemaatt which only con-tains the utterance-attended schema representa-tions, and seq2seq-du-w/utteranceatt which onlycontains the schema-attended utterance represen-tations.
figure 3 shows the results on sgd andmultiwoz2.2 in terms of joint ga and int.
acc.
one can observe that without attention the perfor-mances deteriorate considerably.
in addition, theperformances of unidirectional attentions are infe-rior to the performance of bidirectional attention.
thus, utilization of bidirectional attention betweenutterances and schema descriptions is desriable..effect of pointer generation.
to investigate the effectiveness of the pointer gen-eration mechanism, we directly generate wordsfrom the vocabulary instead of generating pointersin the decoding process.
figure 3 also shows theresults of seq2seq-du-w/opointer on sgd andmultiwoz2.2 in terms of joint ga and int.
acc.
from the results we can see that pointer gener-ation is crucial for coping with unseen schemas.
in sgd which contains a large number of unseenschemas in the test set, there is signiﬁcant perfor-mance degradation without pointer generation.
theresults on multiwoz2.2, which does not have un-seen schemas in the test set, show pointer gener-ation can also make signiﬁcant improvement on.
figure 3: ablation study results of seq2seq-du withrespect to bert, attention, and pointer generation onsgd and multiwoz2.2..already seen schemas by making full use of schemadescriptions..4.7 discussions.
case studywe make qualitative analysis on the results ofseq2seq-du and sgd-baseline on sgd and mul-tiwoz2.2.
we ﬁnd that seq2seq-du can makemore accurate inference of dialogue states by lever-aging the relations existing in the utterances andschema descriptions.
for example, in the ﬁrst casein table 8, the user wants to ﬁnd a cheap guest-house.
seq2seq-du can correctly infer that thehotel type is “guesthouse” by referring to the rela-tion between “hotel-pricerange” and “hotel-type”.
in the second case, the user wants to rent a roomwith in-unit laundry.
in the dataset, a user who in-tends to rent a room will care more about the laun-dry property.
seq2seq-du can effectively extractthe relation between “intent” and “in-unit-laundry”,yielding a correct result.
in contrast, sgd-baselinedoes not model the relations in the schemas, andthus it cannot properly infer the values of “hotel-type” and “in-unit-laundry”..dealing with unseen schemaswe analyze the zero-shotlearning ability ofseq2seq-du.
table 9 presents the accuracies ofseq2seq-du in different domains on sgd.
(notethat only sgd has unseen schemas in test set.)
weobserve that the best performances can be obtained.
1721id dialogue utterance.
dialogue state.
1.
2.user: i wanna rent a place insys: how manycampbell.
baths?
user: one bath is ﬁne.
sys: how many bedrooms?
user:one bedroom is ﬁne.
it alsoneeds in-unit laundry.
user: the location isn’t reallyimportant.
it does need to becheap though, and preferably aguesthouse..campbell;“area”:true;“in-unit-laundry”:“intent”:rent; “number-of-baths”: 1; “number-of-beds”: 1; “active-intent”:findhomebyarea;“hotel-area”: dontcare ;“hotel-pricerange”: cheap;“ hotel-type”: guesthouse;“active intent”: ﬁnd-hotel;.
of.
predictions.
statesgd-baseline“area”: campbell; “in-unit-laundry”: – ; “intent”:rent; “number-of-baths”:1; “number-of-beds”: 1;“active-intent”: findhome-byarea;“hotel-area”: dontcare ;“hotel-pricerange”: cheap;“hotel-type”: hotel; “activeintent”: ﬁnd-hotel;.
of.
predictions.
stateseq2seq-du“area”: campbell;“in-unit-laundry”: true; “in-tent”:rent; “number-of-baths”: 1; “number-of-beds”: 1; “active-intent”:findhomebyarea;“hotel-area”:dontcare;“hotel-pricerange”: cheap;“hotel-type”: guesthouse;“active intent”: ﬁnd-hotel;.
table 8: case study on seq2seq-du and sgd-baseline on sgd and multiwoz2.2.
the ﬁrst example is fromsgd and the second is from multiwoz2.2.
the underlined slot-value pairs represent the ground truth states.
theslot-value pairs in blue are correctly predicted ones, while the slot-value pairs in red are incorrectly predicted ones..in the domains with all seen schemas.
the domainsthat have more partially seen schemas achievehigher accuracies, such as ”hotels”, ”movies”,”services”.
the accuracies decline in the domainswith more unseen schemas, such as ”messaging”and ”rentalcars”.
we conclude that seq2seq-ducan perform zero-shot learning across domains.
however, the ability still needs enhancement..domainmessaging*rentalcars*payment*music*restaurants*flights*trains*buses*homes.
joint ga int acc0.35100.79010.58350.94380.96270.96490.92570.88050.9081.
0.04890.06250.07190.12340.12950.15890.16830.16840.2275.domainmedia*events*hotels**movies**travelservices**alarm*weatherridesharing.
joint ga int acc0.90650.93270.98910.78360.99660.98420.57680.99650.9991.
0.23070.31860.33960.43860.44900.47740.55670.57920.6702.table 9: accuracy of seq2seq-du in each domain onsgd test set.
domains marked with ‘*’ are those forwhich the schemas in the test set are not present inthe training set.
domains marked with ‘**’ have boththe unseen and seen schemas.
for other domains, theschemas in the test set are also seen in the training set..categorical slots and non-categorical slots.
table 10 shows the accuracies of seq2seq-du andthe baselines with respect to categorical and non-categorical slots on sgd and multiwoz2.2.
(wedid not compare with fastsgt on sgd datasetdue to unavailability of the codes.)
one can seethat seq2seq-du can effectively deal with bothcategorical and non-categorical slots.
furthermore,seq2seq-du demonstrates higher accuracies oncategorical slots than non-categorical slots.
weconjecture that it is due to the co-occurrences ofcategorical slot values in both the dialogue historyand the schema descriptions.
the utterance-schema.
attention can more easily capture the relations be-tween the values..model.
sgd.
multiwoz2.2.
categorical-joint-ga-0.513-not available-.
tradesgd-baselineds-dstfastsgttrippyseq2seq-du 0.578.noncategorical-joint-ga-0.361-not available-0.393.categorical-joint-ga0.6280.5700.706-0.6840.758.noncategorical-joint-ga0.6660.6610.701-0.7330.711.table 10: accuracies of seq2seq-du and baselineswith respect to categorical and non-categorical slots onsgd and multiwoz2.2..5 conclusion.
we have proposed a new approach to dialogue statetracking.
the approach, referred to as seq2seq-du, takes dialogue state tracking (dst) as a prob-lem of transforming all the utterances in a dia-logue into semantic frames (state representations)on the basis of schema descriptions.
seq2seq-duis unique in that within the sequence to sequenceframework it employs bert in encoding of ut-terances and schema descriptions respectively andgenerates pointers in decoding of dialogue state.
seq2seq-du is a global, reprentable, and scalablemodel for dst as well as nlu (natural languageunderstanding).
experimental results show thatseq2seq-du signiﬁcantly outperforms the state-of-the-arts methods in dst on the benchmark datasetsof sgd, multiwoz2.2, multiwoz2.1, woz2.0,dstc2, m2m, and performs as well as the state-of-the-arts in nlu on the benchmark datasets ofatis and snips..1722references.
dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in iclr..guan-lin chao and ian lane.
2019. bert-dst: scal-able end-to-end dialogue state tracking with bidirec-tional encoder representations from transformer.
ininterspeech..hongshen chen, xiaorui liu, dawei yin, and jil-iang tang.
2017. a survey on dialogue systems:recent advances and new frontiers.
acm sigkddexplorations newsletter..lu chen, boer lv, chi wang, su zhu, bowen tan, andkai yu.
2020a.
schema-guided multi-domain dia-logue state tracking with graph attention neural net-works.
in aaai..qian chen, zhu zhuo, and wen wang.
2019. bertfor joint intent classiﬁcation and slot ﬁlling.
arxivpreprint arxiv:1902.10909..zhi chen, lu chen, zihan xu, yanbin zhao, su zhu,and kai yu.
2020b.
credit: coarse-to-ﬁne se-quence generation for dialogue state tracking.
arxivpreprint arxiv:2009.10435..alice coucke, alaa saade, adrien ball, th´eodorebluche, alexandre caulier, david leroy, cl´ementdoumouro, thibault gisselbrecht, francesco calt-agirone, thibaut lavril, et al.
2018. snips voiceplatform: an embedded spoken language understand-ing system for private-by-design voice interfaces.
incorr..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing.
in naacl..mihail eric, rahul goel, shachi paul, abhishek sethi,sanchit agarwal, shuyang gao, adarsh kumar,anuj goyal, peter ku, and dilek hakkani-tur.
2020.multiwoz 2.1: a consolidated multi-domain dia-logue dataset with state corrections and state track-ing baselines.
in lrec..chih-wen goo, guang gao, yun-kai hsu, chih-lihuo, tsung-chieh chen, keng-wei hsu, and yun-nung chen.
2018. slot-gated modeling for joint slotﬁlling and intent prediction.
in naacl..dilek hakkani-t¨ur, g¨okhan t¨ur, asli celikyilmaz,yun-nung chen, jianfeng gao, li deng, and ye-yi wang.
2016. multi-domain joint semanticinframe parsing using bi-directional rnn-lstm.
interspeech..michael heck, carel van niekerk, nurul lubis, chris-tian geishauser, hsien-chin lin, marco moresi, andmilica gaˇsi´c.
2020. trippy: a triple copy strategyfor value independent neural dialog state tracking.
in acl..matthew henderson, blaise thomson, and jason dwilliams.
2014. the second dialog state trackingchallenge.
in sigdial..ehsan hosseini-asl, bryan mccann, chien-sheng wu,semih yavuz, and richard socher.
2020. a simplelanguage model for task-oriented dialogue.
arxivpreprint arxiv:2005.00796..minlie huang, xiaoyan zhu, and jianfeng gao.
2020.challenges in building intelligent open-domain di-alog systems.
acm transactions on informationsystems (tois)..diederik p kingma and jimmy ba.
2014. adam: a.method for stochastic optimization.
in corr..wenqiang lei, xisen jin, min-yen kan, zhaochunren, xiangnan he, and dawei yin.
2018. sequic-ity: simplifying task-oriented dialogue systems withsingle sequence-to-sequence architectures.
in acl..bing liu and ian lane.
2016. attention-based recur-rent neural network models for joint intent detectionand slot ﬁlling.
in interspeech..nikola mrkˇsi´c, diarmuid o s´eaghdha, tsung-hsienwen, blaise thomson, and steve young.
2017. neu-ral belief tracker: data-driven dialogue state track-ing.
in acl..vahid noroozi, yang zhang, evelina bakhturina, andtomasz kornuta.
2020. a fast and robust bert-baseddialogue state tracker for schema-guided dialoguedataset.
arxiv preprint arxiv:2008.12335..giovanni paolini, ben athiwaratkun, jason krone,jie ma, alessandro achille, rishita anubhai, ci-cero nogueira dos santos, bing xiang, and stefanosoatto.
2021. structured prediction as translation be-tween augmented natural languages.
in iclr..osman ramadan, paweł budzianowski, and milicagaˇsi´c.
2018. large-scale multi-domain belief track-ing with knowledge sharing.
in acl..abhinav rastogi, raghav gupta, and dilek hakkani-tur.
2018. multi-task learning for joint languageinunderstanding and dialogue state tracking.
sigdial..abhinav rastogi, dilek hakkani-t¨ur, and larry heck.
2017. scalable multi-domain dialogue state track-ing.
in asru..abhinav rastogi, xiaoxue zang, srinivas sunkara,to-raghav gupta, and pranav khaitan.
2019.wards scalable multi-domain conversational agents:the schema-guided dialogue dataset.
arxiv preprintarxiv:1909.05855..liliang ren, jianmo ni, and julian mcauley.
2019.scalable and accurate dialogue state tracking via hi-erarchical sequence generation.
in emnlp..1723liliang ren, kaige xie, lu chen, and kai yu.
2018.in.
towards universal dialogue state tracking.
emnlp..pararth shah, dilek hakkani-t¨ur, gokhan t¨ur, ab-hinav rastogi, ankur bapna, neha nayak, andlarry heck.
2018. building a conversational agentovernight with dialogue self-play.
arxiv preprintarxiv:1801.04871..gokhan tur, dilek hakkani-t¨ur, and larry heck.
2010..what is left to be understood in atis?
in slt..oriol vinyals, meire fortunato, and navdeep jaitly..2015. pointer networks.
in nips..tsung-hsien wen, david vandyke, nikola mrksic,milica gasic, lina m rojas-barahona, pei-hao su,stefan ultes, and steve young.
2017. a network-based end-to-end trainable task-oriented dialoguesystem.
in eacl..chien-sheng wu, andrea madotto, ehsan hosseini-asl, caiming xiong, richard socher, and pascalefung.
2019. transferable multi-domain state gener-ator for task-oriented dialogue systems.
in acl..puyang xu and qi hu.
2018. an end-to-end approachfor handling unknown slot values in dialogue statetracking.
in acl..kaisheng yao, baolin peng, geoffrey zweig, dong yu,xiaolong li, and feng gao.
2014. recurrent condi-tional random ﬁeld for language understanding.
inicassp..xiaoxue zang, abhinav rastogi, srinivas sunkara,raghav gupta, jianguo zhang, and jindong chen.
2020. multiwoz 2.2: a dialogue dataset with addi-tional annotation corrections and state tracking base-lines.
in acl..jian-guo zhang, kazuma hashimoto, chien-shengwu, yao wan, philip s yu, richard socher, andcaiming xiong.
2019. find or classify?
dual strat-egy for slot-value predictions on multi-domain dia-log state tracking.
arxiv preprint arxiv:1910.03544..zheng zhang, ryuichi takanobu, qi zhu, minliehuang, and xiaoyan zhu.
2020.recent ad-vances and challenges in task-oriented dialog sys-tems.
science china technological sciences..victor zhong, caiming xiong, and richard socher.
2018. global-locally self-attentive encoder for di-alogue state tracking.
in acl..1724a training curves.
figure 4 shows the training losses of seq2seq-duon the training datasets, while figure 5 shows theaccuracies of seq2seq-du on the test sets duringtraining.
we regard training convergence when theﬂuctuation of loss is less than 0.01 for consecutive20 thousand steps.
seq2seq-du converges at the180k-th step on sgd, multiwoz2.2, and multi-woz2.1.
seq2seq-du-w/oschema converges atthe 150k-th step on woz2.0 and at the 140k-th stepon dstc2, and m2m.
furthermore, seq2seq-du-seqlabel converges at the 130k-th step on atisand snips.
these are consistent with the generaltrends in machine learning that more complex mod-els are more difﬁcult to train..figure 4: training losses of seq2seq-du on all train-ing datasets..figure 5: accuracies of seq2seq-du in terms of joinga / slot f1 on all test sets..172520k40k60k80k100k120k140k160k180k200ktraining step0.00.51.01.52.02.53.03.5losssgdmultiwoz2.2woz2.0dstc2m2msnipsatismultiwoz2.120k40k60k80k100k120k140k160k180k200ktraining step0.00.20.40.60.81.0joint ga / slot f1sgdmultiwoz2.2woz2.0dstc2m2msnipsatismultiwoz2.1