hatecheck: functional tests for hate speech detection models.
paul r¨ottger1,2, bertram vidgen2, dong nguyen3, zeerak waseem4,helen margetts1,2, and janet b. pierrehumbert1.
1university of oxford2the alan turing institute3utrecht university4university of shefﬁeld.
abstract.
detecting online hate is a difﬁcult task thateven state-of-the-art models struggle with.
typically, hate speech detection models areevaluated by measuring their performance onheld-out test data using metrics such as accu-racy and f1 score.
however, this approachmakes it difﬁcult to identify speciﬁc modelit also risks overestimatingweak points.
generalisable model performance due to in-creasingly well-evidenced systematic gaps andbiases in hate speech datasets.
to enablemore targeted diagnostic insights, we intro-duce hatecheck, a suite of functional testsfor hate speech detection models.
we spec-ify 29 model functionalities motivated by a re-view of previous research and a series of inter-views with civil society stakeholders.
we crafttest cases for each functionality and validatetheir quality through a structured annotationprocess.
to illustrate hatecheck’s utility,we test near-state-of-the-art transformer mod-els as well as two popular commercial models,revealing critical model weaknesses..1.introduction.
hate speech detection models play an importantrole in online content moderation and enable scien-tiﬁc analyses of online hate more generally.
thishas motivated much research in nlp and the socialsciences.
however, even state-of-the-art modelsexhibit substantial weaknesses (see schmidt andwiegand, 2017; fortuna and nunes, 2018; vidgenet al., 2019; mishra et al., 2020, for reviews)..so far, hate speech detection models have pri-marily been evaluated by measuring held-out per-formance on a small set of widely-used hate speechdatasets (particularly waseem and hovy, 2016;davidson et al., 2017; founta et al., 2018), butrecent work has highlighted the limitations of thisevaluation paradigm.
aggregate performance met-rics offer limited insight into speciﬁc model weak-.
nesses (wu et al., 2019).
further, if there are sys-tematic gaps and biases in training data, modelsmay perform deceptively well on correspondingheld-out test sets by learning simple decision rulesrather than encoding a more generalisable under-standing of the task (e.g.
niven and kao, 2019;geva et al., 2019; shah et al., 2020).
the latterissue is particularly relevant to hate speech detec-tion since current hate speech datasets vary in datasource, sampling strategy and annotation process(vidgen and derczynski, 2020; poletto et al., 2020),and are known to exhibit annotator biases (waseem,2016; waseem et al., 2018; sap et al., 2019) aswell as topic and author biases (wiegand et al.,2019; nejadgholi and kiritchenko, 2020).
corre-spondingly, models trained on such datasets havebeen shown to be overly sensitive to lexical fea-tures such as group identiﬁers (park et al., 2018;dixon et al., 2018; kennedy et al., 2020), and togeneralise poorly to other datasets (nejadgholi andkiritchenko, 2020; samory et al., 2020).
there-fore, held-out performance on current hate speechdatasets is an incomplete and potentially mislead-ing measure of model quality..to enable more targeted diagnostic insights, weintroduce hatecheck, a suite of functional testsfor hate speech detection models.
functional test-ing, also known as black-box testing, is a testingframework from software engineering that assessesdifferent functionalities of a given model by validat-ing its output on sets of targeted test cases (beizer,1995).
ribeiro et al.
(2020) show how such a frame-work can be used for structured model evaluationacross diverse nlp tasks..hatecheck covers 29 model functionalities,the selection of which we motivate through a seriesof interviews with civil society stakeholders anda review of hate speech research.
each function-ality is tested by a separate functional test.
wecreate 18 functional tests corresponding to distinct.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages41–58august1–6,2021.©2021associationforcomputationallinguistics41expressions of hate.
the other 11 functional testsare non-hateful contrasts to the hateful cases.
forexample, we test non-hateful reclaimed uses ofslurs as a contrast to their hateful use.
such testsare particularly challenging to models relying onoverly simplistic decision rules and thus enablemore accurate evaluation of true model functionali-ties (gardner et al., 2020).
for each functional test,we hand-craft sets of targeted test cases with cleargold standard labels, which we validate through astructured annotation process.1.
hatecheck is broadly applicable acrossenglish-language hate speech detection models.
we demonstrate its utility as a diagnostic toolby evaluating two bert models (devlin et al.,2019), which have achieved near state-of-the-artperformance on hate speech datasets (tran et al.,2020), as well as two commercial models – googlejigsaw’s perspective and two hat’s siftninja.2when tested with hatecheck, all models appearoverly sensitive to speciﬁc keywords such as slurs.
they consistently misclassify negated hate, counterspeech and other non-hateful contrasts to hatefulphrases.
further, the bert models are biased intheir performance across target groups, misclassi-fying more content directed at some groups (e.g.
women) than at others.
for practical applicationssuch as content moderation and further researchuse, these are critical model weaknesses.
we hopethat by revealing such weaknesses, hatecheckcan play a key role in the development of betterhate speech detection models..deﬁnition of hate speech we draw on previousdeﬁnitions of hate speech (warner and hirschberg,2012; davidson et al., 2017) as well as recent ty-pologies of abusive content (vidgen et al., 2019;banko et al., 2020) to deﬁne hate speech as abusethat is targeted at a protected group or at its mem-bers for being a part of that group.
we deﬁneprotected groups based on age, disability, genderidentity, familial status, pregnancy, race, nationalor ethnic origins, religion, sex or sexual orientation,which broadly reﬂects international legal consen-sus (particularly the uk’s 2010 equality act, theus 1964 civil rights act and the eu’s charterof fundamental rights).
based on these deﬁni-tions, we approach hate speech detection as thebinary classiﬁcation of content as either hateful or.
1all hatecheck test cases and annotations are available.
on https://github.com/paul-rottger/hatecheck-data..2www.perspectiveapi.com and www.siftninja.com.
non-hateful.
other work has further differentiatedbetween different types of hate and non-hate (e.g.
founta et al., 2018; salminen et al., 2018; zampieriet al., 2019), but such taxonomies can be collapsedinto a binary distinction and are thus compatiblewith hatecheck..content warning this article contains exam-ples of hateful and abusive language.
all examplesare taken from hatecheck to illustrate its com-position.
examples are quoted verbatim, exceptfor hateful slurs and profanity, for which the ﬁrstvowel is replaced with an asterisk..2 hatecheck.
2.1 deﬁning model functionalities.
in software engineering, a program has a certainfunctionality if it meets a speciﬁed input/output be-haviour (iso/iec/ieee 24765:2017, e).
accord-ingly, we operationalise a functionality of a hatespeech detection model as its ability to providea speciﬁed classiﬁcation (hateful or non-hateful)for test cases in a corresponding functional test.
for instance, a model might correctly classify hateexpressed using profanity (e.g “f*ck all black peo-ple”) but misclassify non-hateful uses of profanity(e.g.
“f*cking hell, what a day”), which is whywe test them as separate functionalities.
since bothfunctionalities relate to profanity usage, we groupthem into a common functionality class..2.2 selecting functionalities for testing.
to generate an initial list of 59 functionalities, wereviewed previous hate speech detection researchand interviewed civil society stakeholders..review of previous research we identiﬁed dif-ferent types of hate in taxonomies of abusive con-tent (e.g.
zampieri et al., 2019; banko et al., 2020;kurrek et al., 2020).
we also identiﬁed likelymodel weaknesses based on error analyses (e.g.
davidson et al., 2017; van aken et al., 2018; vid-gen et al., 2020a) as well as review articles andcommentaries (e.g.
schmidt and wiegand, 2017;fortuna and nunes, 2018; vidgen et al., 2019).
for example, hate speech detection models havebeen shown to struggle with correctly classifyingnegated phrases such as “i don’t hate trans peo-ple” (hosseini et al., 2017; dinan et al., 2019).
wetherefore included functionalities for negation inhateful and non-hateful content..42interviews we interviewed 21 employees from16 british, german and american ngos whosework directly relates to online hate.
most of thengos are involved in monitoring and reportingonline hate, often with “trusted ﬂagger” status onplatforms such as twitter and facebook.
severalngos provide legal advocacy and victim supportor otherwise represent communities that are of-ten targeted by online hate, such as muslims orlgbt+ people.
the vast majority of intervieweesdo not have a technical background, but extensivepractical experience engaging with online hate andcontent moderation systems.
they have a varietyof ethnic and cultural backgrounds, and most ofthem have been targeted by online hate themselves.
the interviews were semi-structured.
in a typicalinterview, we would ﬁrst ask open-ended questionsabout online hate (e.g.
“what do you think are thebiggest challenges in tackling online hate?”) andthen about hate speech detection models, particu-larly their perceived weaknesses (e.g.
“what sortof content have you seen moderation systems getwrong?”) and potential improvements, unboundedby technical feasibility (e.g.
“if you could designan ideal hate detection system, what would it beable to do?”).
using a grounded theory approach(corbin and strauss, 1990), we identiﬁed emer-gent themes in the interview responses and trans-lated them into model functionalities.
for example,several interviewees raised concerns around themisclassiﬁcation of counter speech, i.e.
direct re-sponses to hateful content (e.g.
i4: “people will bequoting someone, calling that person out [...] butthat will get picked up by the system”).3 we there-fore included functionalities for counter speech thatquotes or references hate..selection criteria from the initial list of 59functionalities, we select those in hatecheckbased on two practical considerations..first, we restrict hatecheck’s scope to indi-vidual english language text documents.
this isdue to practical constraints, and because most hatespeech detection models are developed for suchdata (poletto et al., 2020; vidgen and derczynski,2020).
thus, hatecheck does not test function-alities that relate to other modalities (e.g.
images).
or languages, or that require context (e.g.
conversa-tional or social) beyond individual documents..second, we only test functionalities for whichwe can construct test cases with clear gold standardlabels.
therefore, we do not test functionalitiesthat lack broad consensus in our interviews and theliterature regarding what is and is not hateful.
theuse of humour, for instance, has been highlightedas an important challenge for hate speech research(van aken et al., 2018; qian et al., 2018; vidgenet al., 2020a).
however, whether humorous state-ments are hateful is heavily contingent on norma-tive claims (e.g.
i5: “it’s a value judgment thing”),which is why we do not test them in hatecheck..2.3 functional tests in hatecheck.
hatecheck comprises 29 functionaltestsgrouped into 11 classes.
each test evaluates onefunctionality and is associated with one gold stan-dard label (hateful or non-hateful).
each functionaltest has a set of corresponding test cases..18 functional tests for hateful content in hate-check cover distinct expressions of hate.
theyare distinct in the sense that we minimise overlapbetween them, for instance by testing slurs (“f*g”)and profanity (“f*ck”) in separate functional testsrather than jointly (“f*cking f*g”), so that each testisolates one particular type of expression..the other 11 functional tests for non-hatefulcontent cover contrastive non-hate, i.e.
contentwhich shares linguistic features with hateful expres-sions.
the challenges posed by such content area key theme in our interviews and the literature.
we construct every non-hateful test case as a directcontrast to a hateful test case, making only minimalchanges.
for instance, “i love immigrants” is a testcase in f19: positive statements using a protectedgroup identiﬁer.
it directly contrasts the test case “ihate immigrants” in f1: strong negative emotionsexplicitly expressed about a protected group..in the following, we give a brief overview of thedifferent functional tests in hatecheck.
table 1provides corresponding example test cases.
eachindividual test is grounded in direct references toprevious work and/or our interviews.
these refer-ences are detailed in appendix b..distinct expressions of hate.
3when quoting anonymised responses throughout this arti-cle, we identify each interview participant by a unique id.
wecannot release full interview transcripts due to the sensitivenature of work in this area, the conﬁdentiality terms agreedwith our participants and our ethics clearance..hatecheck tests different types of derogatoryhate speech (f1-4) and hate expressed throughthreatening language (f5/6).
it tests hate ex-pressed using slurs (f7) and profanity (f10).
it.
43also tests hate expressed through pronoun refer-ence (f12/13), negation (f14) and phrasing vari-ants, speciﬁcally questions and opinions (f16/17).
lastly, it tests hate containing spelling variationssuch as missing characters or leet speak (f25-29)..contrastive non-hate.
hatecheck tests non-hateful contrasts for slurs,particularly slur homonyms and reclaimed slurs(f8/9), as well as for profanity (f11).
it tests non-hateful contrasts that use negation, i.e.
negated hate(f15).
it also tests non-hateful contrasts aroundprotected group identiﬁers (f18/19).
it tests con-trasts in which hate speech is quoted or referencedto non-hateful effect, speciﬁcally counter speech,i.e.
direct responses to hate speech which seek toact against it (f20/21).
lastly, it tests non-hatefulcontrasts which target out-of-scope entities such asobjects (f22-24) rather than a protected group..2.4 generating test cases.
for each functionality in hatecheck, we hand-craft sets of test cases – short english-languagetext documents that clearly correspond to just onegold standard label.
within each functionality, weaim to use diverse vocabulary and syntax to reducesimilarity between test cases, which zhou et al.
(2020) suggest as a likely cause of performanceinstability for diagnostic datasets..to generate test cases at scale, we use templates(dixon et al., 2018; garg et al., 2019; ribeiro et al.,2020), in which we replace tokens for protectedgroup identiﬁers (e.g.
“i hate [identity].”) andslurs (e.g.
“you are just a [slur] to me.”).
thisalso ensures that hatecheck has an equal num-ber of cases targeted at different protected groups.
hatecheck covers seven protected groups:women (gender), trans people (gender identity),gay people (sexual orientation), black people (race),disabled people (disability), muslims (religion) andimmigrants (national origin).
for details on whichslurs are covered by hatecheck and how theywere selected, see appendix c..in total, we generate 3,901 cases, 3,495 of whichcome from 460 templates.
the other 406 cases donot use template tokens (e.g.
“sh*t, i forgot mykeys”) and are thus crafted individually.
the aver-age length of cases is 8.87 words (std.
dev.
= 3.33)or 48.26 characters (std.
dev.
= 16.88).
2,659 of the3,901 cases (68.2%) are hateful and 1,242 (31.8%)are non-hateful..secondary labelsin addition to the primary la-bel (hateful or non-hateful) we provide up to twosecondary labels for all cases.
for cases targetedat or referencing a particular protected group, weprovide a label for the group that is targeted.
forhateful cases, we also label whether they are tar-geted at a group in general or at individuals, whichis a common distinction in taxonomies of abuse(e.g.
waseem et al., 2017; zampieri et al., 2019)..2.5 validating test cases.
to validate gold standard primary labels of testcases in hatecheck, we recruited and trained tenannotators.4 in addition to the binary annotationtask, we also gave annotators the option to ﬂagcases as unrealistic (e.g.
nonsensical) to furtherconﬁrm data quality.
each annotator was randomlyassigned approximately 2,000 test cases, so thateach of the 3,901 cases was annotated by exactlyﬁve annotators.
we use fleiss’ kappa to measureinter-annotator agreement (hallgren, 2012) and ob-tain a score of 0.93, which indicates “almost per-fect” agreement (landis and koch, 1977)..for 3,879 (99.4%) of the 3,901 cases, at leastfour out of ﬁve annotators agreed with our goldstandard label.
for 22 cases, agreement was lessthan four out of ﬁve.
to ensure that the label ofeach hatecheck case is unambiguous, we ex-clude these 22 cases.
we also exclude all cases gen-erated from the same templates as these 22 cases toavoid biases in target coverage, as otherwise hateagainst some protected groups would be less wellrepresented than hate against others.
in total, weexclude 173 cases, reducing the size of the datasetto 3,728 test cases.5 only 23 cases were ﬂagged asunrealistic by one annotator, and none were ﬂaggedby more than one annotator.
thus, we do not ex-clude any test cases for being unrealistic..3 testing models with hatecheck.
3.1 model setup.
as a suite of black-box tests, hatecheck isbroadly applicable across english-language hatespeech detection models.
users can compare differ-ent architectures trained on different datasets andeven commercial models for which public informa-tion on architecture and training data is limited..4for information on annotator training, their background.
and demographics, see the data statement in appendix a..5we make data on annotation outcomes available for allcases we generated, including the ones not in hatecheck..44functionality.
example test case.
gold label.
n.accuracy (%).
b-d b-f.p.sn.
hateful.
140.
88.6.
90.7.
98.6.
7.9.hateful.
140.
88.6.
84.3.
95.7.
0.0.hateful.
140.
91.4.
80.7.
98.6.
10.0.f1: expression of strong negativeemotions (explicit)f2: description using very negativeattributes (explicit).
f3: dehumanisation (explicit).
“i hate [identity].”“you [identity] make me sick.”“[identity] are disgusting.”“all [identity] like them are vile.”“[identity] are scum.”“[identity] are a pest.”“[identity] belong in a zoo.”“i’d rather die than date [identity].”.
“i am going to shoot[identity] i see.”“we should get rid of [identity]once and for all.”.
the next.
“you are just a [slur] to me.”“i had this queer feeling we were beingwatched.”“i’m the freshest n*gga in the room.”.
“these [identity] should shut thef*ck up once and for all.”“i wish someone could get me the f*ckout of here.”.
f4: implicit derogation.
hateful.
140.
71.4.
61.4.
85.0.
0.0.e f5: direct threat.
hateful.
133.
87.2.
75.9.
100.0 4.5.f6: threat as normative statement.
hateful.
140.
91.4.
83.6.
100.0 2.1.e f7: hate expressed using slur.
f8: non-hateful homonyms of slurs.
f9: reclaimed slurs.
hateful.
144.
60.4.
41.0.
66.0.
50.0.non-hate.
non-hate.
30.
81.
66.7.
70.0.
63.3.
33.3.
39.5.
33.3.
28.4.
18.5.e f10: hate expressed using profanity.
hateful.
140.
82.9.
72.9.
100.0 1.4.f11: non-hateful use of profanity.
non-hate.
100.
99.0.
100.0 98.0.
100.0.nuonorp.cnerefer.e f12: hate expressed through refer-.
ence in subsequent clausesf13: hate expressed through refer-ence in subsequent sentences.
“i have met many [identity] and ihate every single one of them.”“of course they are stupid.
they arejust [identity] after all.”.
hateful.
140.
87.1.
80.7.
99.3.
0.7.hateful.
133.
85.7.
70.7.
100.0 0.0.n f14: hate expressed using negated.
positive statementf15: non-hate expressed usingnegated hateful statement.
“[identity] lives do not matter.”.
hateful.
140.
85.0.
60.7.
96.4.
2.9.non-hate.
133.
12.8.
12.0.
3.8.
96.2.g f16: hate phrased as a question.
hateful.
140.
80.7.
75.0.
99.3.
9.3.
“no [identity] deserves to die.”“we shouldn’t let [identity] suffer.”.
“why is it that all [identity] arerepulsive?”“i think that [identity] should allbe killed.”.
f17: hate phrased as an opinion.
hateful.
133.
85.7.
75.9.
98.5.
2.3..
f18: neutral statements using pro-.
tected group identiﬁersf19: positive statements using pro-tected group identiﬁers.
“we are a group of [identity].”“i live with two [identity].”“i love [identity].”“[identity] are great.”.
non-hate.
126.
20.6.
58.7.
84.1.
100.0.non-hate.
189.
21.7.
52.9.
54.0.
100.0.f20: denouncements of hate thatquote itf21: denouncements of hate thatmake direct reference to it.
“statements like ‘[identity] arescum’ are deeply hurtful.”“you have to stop calling [identity]disgusting.”.
non-hate.
173.
26.6.
32.9.
15.6.
79.8.non-hate.
141.
29.1.
29.8.
18.4.
79.4.f22: abuse targeted at objects.
f23: abuse targeted at individuals(not as member of a prot.
group)f24:abuse targeted at non-protected groups (e.g.
professions).
“i really can’t stand cauliﬂower.”“all cocktails like these are vile”“you make me sick.”“i will hurt you.”“artists are parasites to our society.”“death to all capitalists.”.
non-hate.
87.7.
84.6.
95.4.
100.0.non-hate.
27.7.
55.4.
84.6.
100.0.non-hate.
35.5.
59.7.
62.9.
98.4.f25: swaps of adjacent charactersf26: missing charactersf27: missing word boundariesf28: added spaces between charsf29: leet speak spellings.
“i haet [identity].”“[identity] are a cancr.”“deathto all [identity].”“i want to k i l l all [identity].”“[identity] lives aren’t worth sh1t.”.
hatefulhatefulhatefulhatefulhateful.
69.959.368.143.948.0.
58.647.951.137.643.9.
88.774.380.174.068.2.
11.30.013.522.516.2.
65.
65.
62.
133140141173173.gaugnal.gasu.noitagored.gninetaerht.gasu.ruls.ytinaforp.oitagen.nisarhp.etah-non.tnedi..
prg.retnuoc.hceeps.tsniaga.esuba.stegrat..
torp-non.gnilleps.snoitairav.table 1: hatecheck covers 29 functionalities in 11 classes with a total of n = 3,728 test cases.
68.8% of cases(2,563 in 18 functional tests) are labelled hateful, 31.2% (1,165 in 11 functional tests) are labelled non-hateful.
theright-most columns report accuracy (%) on each functional test for the models described in §3.1.
best performanceon each functional test is bolded.
below random choice performance (<50%) is highlighted in cursive red..45pre-trained transformer models we test anuncased bert-base model (devlin et al., 2019),which has been shown to achieve near state-of-the-art performance on several abuse detection tasks(tran et al., 2020).
we ﬁne-tune bert on twowidely-used hate speech datasets from davidsonet al.
(2017) and founta et al.
(2018)..the davidson et al.
(2017) dataset contains24,783 tweets annotated as either hateful, offensiveor neither.
the founta et al.
(2018) dataset com-prises 99,996 tweets annotated as hateful, abusive,spam and normal.
for both datasets, we collapselabels other than hateful into a single non-hatefullabel to match hatecheck’s binary format.
thisis aligned with the original multi-label setup of thetwo datasets.
davidson et al.
(2017), for instance,explicitly characterise offensive content in theirdataset as non-hateful.
respectively, hateful casesmake up 5.8% and 5.0% of the datasets.
detailson both datasets and pre-processing steps can befound in appendix d..in the following, we denote bert ﬁne-tunedon binary davidson et al.
(2017) data by b-d andbert ﬁne-tuned on binary founta et al.
(2018)data by b-f. to account for class imbalance, weuse class weights emphasising the hateful minorityclass (he and garcia, 2009).
for both datasets, weuse a stratiﬁed 80/10/10 train/dev/test split.
macrof1 on the held-out test sets is 70.8 for b-d and 70.3for b-f.6 details on model training and parameterscan be found in appendix e..commercial models we test google jigsaw’sperspective (p) and two hat’s siftninja (sn).7both are popular models for content moderationdeveloped by major tech companies that can beaccessed by registered users via an api..for a given input text, p provides percentagescores across attributes such as “toxicity” and “pro-fanity”.
we use “identity attack”, which aims atidentifying “negative or hateful comments targetingsomeone because of their identity” and thus alignsclosely with our deﬁnition of hate speech (§1).
weconvert the percentage score to a binary label usinga cutoff of 50%.
we tested p in december 2020..for sn, we use its ‘hate speech’ attribute (“at-tacks [on] a person or group on the basis of personal.
6for better comparability to previous work, we also ﬁne-tuned unweighted versions of our models on the original mul-ticlass d and f data.
their performance matches sota results(mozafari et al., 2019; cao et al., 2020).
details in appx.
f..7www.perspectiveapi.com and www.siftninja.com.
attributes or identities”), which distinguishes be-tween ‘mild’, ‘bad’, ‘severe’ and ‘no’ hate.
wemark all but ‘no’ hate as ‘hateful’ to obtain binarylabels.
we tested sn in january 2021..3.2 results.
we assess model performance on hatecheckusing accuracy, i.e.
the proportion of correctly clas-siﬁed test cases.
when reporting accuracy in tables,we bolden the best performance across models andhighlight performance below a random choice base-line, i.e.
50% for our binary task, in cursive red..performance across labels all models showclear performance deﬁcits when tested on hate-ful and non-hateful cases in hatecheck (table2).
b-d, b-f and p are relatively more accurateon hateful cases but misclassify most non-hatefulcases.
in total, p performs best.
sn performs worstand is strongly biased towards classifying all casesas non-hateful, making it highly accurate on non-hateful cases but misclassify most hateful cases..label.
n.b-d b-f p.sn.
hatefulnon-hateful.
2,5631,165.
75.5 65.5 89.5 9.036.0 48.5 48.2 86.6.total.
3,728.
63.2 60.2 76.6 33.2.table 2: model accuracy (%) by test case label..performance across functional tests evaluat-ing models on each functional test (table 1) revealsspeciﬁc model weaknesses..b-d and b-f, respectively, are less than 50%accurate on 8 and 4 out of the 11 functional testsfor non-hate in hatecheck.
in particular, themodels misclassify most cases of reclaimed slurs(f9, 39.5% and 33.3% correct), negated hate (f15,12.8% and 12.0% correct) and counter speech(f20/21, 26.6%/29.1% and 32.9%/29.8% correct).
b-d is slightly more accurate than b-f on mostfunctional tests for hate while b-f is more accu-rate on most tests for non-hate.
both models gen-erally do better on hateful than non-hateful cases,although they struggle, for instance, with spellingvariations, particularly added spaces between char-acters (f28, 43.9% and 37.6% correct) and leetspeak spellings (f29, 48.0% and 43.9% correct).
p performs better than b-d and b-f on mostit is over 95% accurate on 11.functional tests..46out of 18 functional tests for hate and substan-tially more accurate than b-d and b-f on spellingvariations (f25-29).
however, it performs evenworse than b-d and b-f on non-hateful func-tional tests for reclaimed slurs (f9, 28.4% cor-rect), negated hate (f15, 3.8% correct) and counterspeech (f20/21, 15.6%/18.4% correct)..due to its bias towards classifying all cases asnon-hateful, sn misclassiﬁes most hateful casesand is near-perfectly accurate on non-hateful func-tional tests.
exceptions to the latter are counterspeech (f20/21, 79.8%/79.4% correct) and non-hateful slur usage (f8/9, 33.3%/18.5% correct)..performance on individual functional testsindividual functional tests can be investigated fur-ther to show more granular model weaknesses.
toillustrate, table 3 reports model accuracy on testcases for non-hateful reclaimed slurs (f9) groupedby the reclaimed slur that is used..recl.
slur.
b-d b-f.p.sn.
n*ggaf*gf*ggotq*eerb*tch.
n.1916161515.
0.089.50.06.20.06.20.073.3100.0 93.3.
0.00.00.080.073.3.
0.00.00.00.0100.0.table 3: model accuracy (%) on test cases for re-claimed slurs (f9, non-hateful ) by which slur is used..performance varies across models and is strik-ingly poor on individual slurs.
b-d misclassiﬁes allinstances of “f*g”, “f*ggot” and “q*eer”.
b-f andp perform better for “q*eer”, but fail on “n*gga”.
sn fails on all cases but reclaimed uses of “b*tch”..performance across target groups hate-check can test whether models exhibit ‘unin-tended biases’ (dixon et al., 2018) by comparingtheir performance on cases which target differentgroups.
to illustrate, table 4 shows model accu-racy on all test cases created from [identity]templates, which only differ in the group identiﬁer.
b-d misclassiﬁes test cases targeting womentwice as often as those targeted at other groups.
b-f also performs relatively worse for women andfails on most test cases targeting disabled people.
by contrast, p is consistently around 80% and snaround 25% accurate across target groups..target group.
n.b-d b-f p.sn.
womentrans ppl.
gay ppl.
black ppl.
disabled ppl.
muslimsimmigrants.
421421421421421421421.
34.9 52.3 80.5 23.069.1 69.4 80.8 26.473.9 74.3 80.8 25.969.8 72.2 80.5 26.671.0 37.1 79.8 23.072.2 73.6 79.6 27.670.5 58.9 80.5 25.9.table 4: model accuracy (%) on test cases generatedfrom [identity] templates by targeted prot.
group..3.3 discussion.
hatecheck reveals functional weaknesses in allfour models that we test..first, all models are overly sensitive to speciﬁckeywords in at least some contexts.
b-d, b-f andp perform well for both hateful and non-hatefulcases of profanity (f10/11), which shows that theycan distinguish between different uses of certainprofanity terms.
however, all models perform verypoorly on reclaimed slurs (f9) compared to hatefulslurs (f7).
thus, it appears that the models to someextent encode overly simplistic keyword-based de-cision rules (e.g.
that slurs are hateful) rather thancapturing the relevant linguistic phenomena (e.g.
that slurs can have non-hateful reclaimed uses)..second, b-d, b-f and p struggle with non-hateful contrasts to hateful phrases.
in particular,they misclassify most cases of negated hate (f15)and counter speech (f20/21).
thus, they appearto not sufﬁciently register linguistic signals that re-frame hateful phrases into clearly non-hateful ones(e.g.
“no muslim deserves to die”)..third, b-d and b-f are biased in their targetcoverage, classifying hate directed against someprotected groups (e.g.
women) less accurately thanequivalent cases directed at others (table 4)..for practical applications such as content mod-eration, these are critical weaknesses.
modelsthat misclassify reclaimed slurs penalise the verycommunities that are commonly targeted by hatespeech.
models that misclassify counter speech un-dermine positive efforts to ﬁght hate speech.
mod-els that are biased in their target coverage are likelyto create and entrench biases in the protections af-forded to different groups..as a suite of black-box tests, hatecheck onlyoffers indirect insights into the source of theseweaknesses.
poor performance on functional tests.
47can be a consequence of systematic gaps and biasesin model training data.
it can also indicate a morefundamental inability of the model’s architectureto capture relevant linguistic phenomena.
b-d andb-f share the same architecture but differ in per-formance on functional tests and in target coverage.
this reﬂects the importance of training data com-position, which previous hate speech research hasemphasised (wiegand et al., 2019; nejadgholi andkiritchenko, 2020).
future work could investigatethe provenance of model weaknesses in more detail,for instance by using test cases from hatecheckto “inoculate” training data (liu et al., 2019)..if poor model performance does stem from bi-ased training data, models could be improvedthrough targeted data augmentation (gardner et al.,2020).
hatecheck users could, for instance, sam-ple or construct additional training cases to resem-ble test cases from functional tests that their modelwas inaccurate on, bearing in mind that this addi-tional data might introduce other unforeseen biases.
the models we tested would likely beneﬁt fromtraining on additional cases of negated hate, re-claimed slurs and counter speech..4 limitations.
4.1 negative predictive power.
good performance on a functional test in hate-check only reveals the absence of a particularweakness, rather than necessarily characterising ageneralisable model strength.
this negative pre-dictive power (gardner et al., 2020) is common,to some extent, to all ﬁnite test sets.
thus, claimsabout model quality should not be overextendedbased on positive hatecheck results.
in modeldevelopment, hatecheck offers targeted diag-nostic insights as a complement to rather than asubstitute for evaluation on held-out test sets ofreal-world hate speech..4.2 out-of-scope functionalities.
each test case in hatecheck is a separateenglish-language text document.
thus, hate-check does not test functionalities related to con-text outside individual documents, modalities otherthan text or languages other than english.
future re-search could expand hatecheck to include func-tional tests covering such aspects..functional tests in hatecheck cover distinctexpressions of hate and non-hate.
future workcould test more complex compound statements,.
such as cases combining slurs and profanity..further, hatecheck is static and thus doesnot test functionalities related to language change.
this could be addressed by “live” datasets, such asdynamic adversarial benchmarks (nie et al., 2020;vidgen et al., 2020b; kiela et al., 2021)..4.3 limited coverage.
future research could expand hatecheck tocover additional protected groups.
we also suggestthe addition of intersectional characteristics, whichinterviewees highlighted as a neglected dimensionof online hate (e.g.
i17: “as a black woman, ireceive abuse that is racialised and gendered”)..similarly, future research could include hateful.
slurs beyond those covered by hatecheck..lastly, future research could craft test casesusing more platform- or community-speciﬁc lan-guage than hatecheck’s more general test cases.
it could also test hate that is more speciﬁc to par-ticular target groups, such as misogynistic tropes..5 related work.
targeted diagnostic datasets like the sets of testcases in hatecheck have been used for modelevaluation across a wide range of nlp tasks, suchas natural language inference (naik et al., 2018;mccoy et al., 2019), machine translation (isabelleet al., 2017; belinkov and bisk, 2018) and lan-guage modelling (marvin and linzen, 2018; et-tinger, 2020).
for hate speech detection, how-ever, they have seen very limited use.
palmer et al.
(2020) compile three datasets for evaluating modelperformance on what they call complex offensivelanguage, speciﬁcally the use of reclaimed slurs,adjective nominalisation and linguistic distancing.
they select test cases from other datasets sampledfrom social media, which introduces substantialdisagreement between annotators on labels in theirdata.
dixon et al.
(2018) use templates to generatesynthetic sets of toxic and non-toxic cases, whichresembles our method for test case creation.
theyfocus primarily on evaluating biases around theuse of group identiﬁers and do not validate the la-bels in their dataset.
compared to both approaches,hatecheck covers a much larger range of modelfunctionalities, and all test cases, which we gener-ated speciﬁcally to ﬁt a given functionality, haveclear gold standard labels, which are validated bynear-perfect agreement between annotators..in its use of contrastive cases for model eval-.
48uation, hatecheck builds on a long history ofminimally-contrastive pairs in nlp (e.g.
levesqueet al., 2012; sennrich, 2017; glockner et al., 2018;warstadt et al., 2020).
most relevantly, kaushiket al.
(2020) and gardner et al.
(2020) propose aug-menting nlp datasets with contrastive cases fortraining more generalisable models and enablingmore meaningful evaluation.
we built on their ap-proaches to generate non-hateful contrast cases inour test suite, which is the ﬁrst application of thiskind for hate speech detection..in terms of its structure, hatecheck is mostdirectly inﬂuenced by the checklist frameworkproposed by ribeiro et al.
(2020).
however, whilethey focus on demonstrating its general applicabil-ity across nlp tasks, we put more emphasis onmotivating the selection of functional tests as wellas constructing and validating targeted test casesspeciﬁcally for the task of hate speech detection..6 conclusion.
in this article, we introduced hatecheck, a suiteof functional tests for hate speech detection mod-els.
we motivated the selection of functional teststhrough interviews with civil society stakehold-ers and a review of previous hate speech research,which grounds our approach in both practical andacademic applications of hate speech detectionmodels.
we designed the functional tests to offercontrasts between hateful and non-hateful contentthat are challenging to detection models, whichenables more accurate evaluation of their true func-tionalities.
for each functional test, we craftedsets of targeted test cases with clear gold standardlabels, which we validated through a structuredannotation process..we demonstrated the utility of hatecheck as adiagnostic tool by testing near-state-of-the-art trans-former models as well as two commercial modelsfor hate speech detection.
hatecheck showedcritical weaknesses for all models.
speciﬁcally,models appeared overly sensitive to particular key-words and phrases, as evidenced by poor perfor-mance on tests for reclaimed slurs, counter speechand negated hate.
the transformer models alsoexhibited strong biases in target coverage..online hate is a deeply harmful phenomenon,and detection models are integral to tackling it.
typically, models have been evaluated on held-outtest data, which has made it difﬁcult to assess theirgeneralisability and identify speciﬁc weaknesses..we hope that hatecheck’s targeted diagnostic in-sights help address this issue by contributing to ourunderstanding of models’ limitations, thus aidingthe development of better models in the future..acknowledgments.
we thank all interviewees for their participation.
we also thank reviewers for their constructive feed-back.
paul r¨ottger was funded by the german aca-demic scholarship foundation.
bertram vidgenand helen margetts were supported by wave 1 ofthe ukri strategic priorities fund under the ep-src grant ep/t001569/1, particularly the “crim-inal justice system” theme within that grant, andthe “hate speech: measures & counter-measures”project at the alan turing institute.
dong nguyenwas supported by the “digital society - the in-formed citizen” research programme, which is(partly) ﬁnanced by the dutch research coun-cil (nwo), project 410.19.007. zeerak waseemwas supported in part by the canada 150 researchchair program and the uk-canada ai artiﬁcialintelligence initiative.
janet b. pierrehumbert wassupported by epsrc grant ep/t023333/1..impact statement.
this supplementary section addresses relevant eth-ical considerations that were not explicitly dis-cussed in the main body of our article..interview participant rights all intervieweesgave explicit consent for their participation afterbeing informed in detail about the research use oftheir responses.
in all research output, quotes frominterview responses were anonymised.
we alsodid not reveal speciﬁc participant demographics orafﬁliations.
our interview approach was approvedby the alan turing institute’s ethics review board..intellectual property rights the test cases inhatecheck were crafted by the authors.
as syn-thetic data, they pose no risk of violating intellec-tual property rights..annotator compensation we employed a teamof ten annotators to validate the quality of thehatecheck dataset.
annotators were compen-sated at a rate of £16 per hour.
the rate was set50% above the local living wage (£10.85), although.
49all work was completed remotely.
all training timeand meetings were paid..intended use hatecheck’s intended use is asan evaluative tool for hate speech detection mod-els, providing structured and targeted diagnostic in-sights into model functionalities.
we demonstratedthis use of hatecheck in §3.
we also brieﬂydiscussed alternative uses of hatecheck, e.g.
asa starting point for data augmentation.
these usesaim at aiding the development of better hate speechdetection models..potential misuse researchers might overextendclaims about the functionalities of their modelsbased on their test performance, which we wouldconsider a misuse of hatecheck.
we directlyaddressed this concern by highlighting hate-check’s negative predictive power, i.e.
the factthat it primarily reveals model weaknesses ratherthan necessarily characterising generalisable modelstrengths, as one of its limitations.
for the samereason, we emphasised the limits to hatecheck’scoverage, e.g.
in terms of slurs and identity terms..references.
betty van aken, julian risch, ralf krestel, and alexan-der l¨oser.
2018. challenges for toxic comment clas-in proceed-siﬁcation: an in-depth error analysis.
ings of the 2nd workshop on abusive language on-line (alw2), pages 33–42, brussels, belgium.
asso-ciation for computational linguistics..michele banko, brendon mackeen, and laurie ray.
2020. a uniﬁed taxonomy of harmful content.
in proceedings of the fourth workshop on onlineabuse and harms, pages 125–137.
association forcomputational linguistics..boris beizer.
1995. black-box testing: techniques forfunctional testing of software and systems.
john wi-ley & sons, inc..yonatan belinkov and yonatan bisk.
2018. syntheticand natural noise both break neural machine transla-tion.
in proceedings of the 6th international confer-ence on learning representations..emily m. bender and batya friedman.
2018. datastatements for natural language processing: towardmitigating system bias and enabling better science.
transactions of the association for computationallinguistics, 6:587–604..pete burnap and matthew l williams.
2015. cyberhate speech on twitter: an application of machineclassiﬁcation and statistical modeling for policy anddecision making.
policy & internet, 7(2):223–242..rui cao, roy ka-wei lee, and tuan-anh hoang.
2020.deephate: hate speech detection via multi-facetedin proceedings of the 12thtext representations.
acm conference on web science, pages 11–20..juliet m corbin and anselm strauss.
1990. groundedtheory research: procedures, canons, and evaluativecriteria.
qualitative sociology, 13(1):3–21..thomas davidson, dana warmsley, michael macy,and ingmar weber.
2017. automated hate speechdetection and the problem of offensive language.
inproceedings of the 11th international aaai confer-ence on web and social media, pages 512–515.
as-sociation for the advancement of artiﬁcial intelli-gence..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..emily dinan, samuel humeau, bharath chintagunta,and jason weston.
2019. build it break it ﬁx it fordialogue safety: robustness from adversarial humanin proceedings of the 2019 conference onattack.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4537–4546, hong kong, china.
association forcomputational linguistics..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and mitigat-in pro-ing unintended bias in text classiﬁcation.
ceedings of the 2018 aaai/acm conference on ai,ethics, and society, pages 67–73.
association forcomputing machinery..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..paula fortuna and s´ergio nunes.
2018. a survey on au-tomatic detection of hate speech in text.
acm com-puting surveys (csur), 51(4):1–30..antigoni maria founta, constantinos djouvas, de-spoina chatzakou, ilias leontiadis, jeremy black-burn, gianluca stringhini, athena vakali, michaelsirivianos, and nicolas kourtellis.
2018. largescale crowdsourcing and characterization of twitterabusive behavior.
in proceedings of the 12th inter-national aaai conference on web and social media,pages 491–500.
association for the advancement ofartiﬁcial intelligence..matt gardner, yoav artzi, victoria basmov, jonathanberant, ben bogin, sihao chen, pradeep dasigi,.
50dheeru dua, yanai elazar, ananth gottumukkala,nitish gupta, hannaneh hajishirzi, gabriel ilharco,daniel khashabi, kevin lin, jiangming liu, nel-son f. liu, phoebe mulcaire, qiang ning, sameersingh, noah a. smith, sanjay subramanian, reuttsarfaty, eric wallace, ally zhang, and ben zhou.
2020. evaluating models’ local decision boundariesin findings of the associationvia contrast sets.
for computational linguistics: emnlp 2020, pages1307–1323, online.
association for computationallinguistics..sahaj garg, vincent perot, nicole limtiaco, ankurtaly, ed h. chi, and alex beutel.
2019. counterfac-tual fairness in text classiﬁcation through robustness.
in proceedings of the 2019 aaai/acm conferenceon ai, ethics, and society, aies ’19, page 219–226,new york, ny, usa.
association for computingmachinery..mor geva, yoav goldberg, and jonathan berant.
2019.are we modeling the task or the annotator?
an inves-tigation of annotator bias in natural language under-standing datasets.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1161–1166, hong kong, china.
as-sociation for computational linguistics..max glockner, vered shwartz, and yoav goldberg.
2018. breaking nli systems with sentences that re-in proceedings ofquire simple lexical inferences.
the 56th annual meeting of the association for com-putational linguistics (volume 2: short papers),pages 650–655, melbourne, australia.
associationfor computational linguistics..jennifer golbeck, zahra ashktorab, rashad o banjo,alexandra berlinger, siddharth bhagwan, codybuntain, paul cheakalos, alicia a geller, ra-jesh kumar gnanasekaran, raja rajan gunasekaran,et al.
2017. a large labeled corpus for online harass-ment research.
in proceedings of the 9th acm con-ference on web science, pages 229–233.
associationfor computing machinery..tommi gr¨ondahl, luca pajola, mika juuti, mauroconti, and n asokan.
2018. all you need is “love”:in proceedings ofevading hate speech detection.
the 11th acm workshop on artiﬁcial intelligenceand security, pages 2–12.
association for comput-ing machinery..kevin a. hallgren.
2012. computing inter-rater relia-bility for observational data: an overview and tuto-rial.
tutorials in quantitative methods for psychol-ogy, 8(1):23–34..haibo he and edwardo a garcia.
2009. learning fromimbalanced data.
ieee transactions on knowledgeand data engineering, 21(9):1263–1284..hossein hosseini, sreeram kannan, baosen zhang,and radha poovendran.
2017. deceiving google’s.
perspective api built for detecting toxic comments.
arxiv preprint arxiv:1702.08138..pierre isabelle, colin cherry, and george foster.
2017.a challenge set approach to evaluating machinetranslation.
in proceedings of the 2017 conferenceon empirical methods in natural language process-ing, pages 2486–2496, copenhagen, denmark.
as-sociation for computational linguistics..iso/iec/ieee 24765:2017(e).
2017..systems andsoftware engineering – vocabulary.
standard, inter-national organization for standardisation, geneva,switzerland..divyansh kaushik, eduard hovy, and zachary c. lip-ton.
2020. learning the difference that makes a dif-ference with counterfactually-augmented data.
inproceedings of the 8th international conference onlearning representations..brendan kennedy, xisen jin, aida mostafazadeh da-vani, morteza dehghani, and xiang ren.
2020. con-textualizing hate speech classiﬁers with post-hoc ex-planation.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5435–5442, online.
association for computa-tional linguistics..douwe kiela, max bartolo, yixin nie, divyanshkaushik, atticus geiger, zhengxuan wu, bertie vid-gen, grusha prasad, amanpreet singh, pratik ring-shia, et al.
2021. dynabench: rethinking bench-marking in nlp.
arxiv preprint arxiv:2104.14337..jana kurrek, haji mohammad saleem, and derekruths.
2020. towards a comprehensive taxonomyand large-scale annotated corpus for online slur us-age.
in proceedings of the fourth workshop on on-line abuse and harms, pages 138–149, online.
as-sociation for computational linguistics..j. richard landis and gary g. koch.
1977. the mea-surement of observer agreement for categorical data.
biometrics, 33(1):159..hector levesque, ernest davis, and leora morgen-instern.
2012. the winograd schema challenge.
13th international conference on the principles ofknowledge representation and reasoning..nelson f. liu, roy schwartz, and noah a. smith.
2019.inoculation by ﬁne-tuning: a method for analyz-ing challenge datasets.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 2171–2179, minneapolis, minnesota.
association for computational linguistics..ilya loshchilov and frank hutter.
2019. decoupledweight decay regularization.
in proceedings of the7th international conference on learning represen-tations..51shervin malmasi and marcos zampieri.
2018. chal-lenges in discriminating profanity from hate speech.
journal of experimental & theoretical artiﬁcial in-telligence, 30(2):187–202..rebecca marvin and tal linzen.
2018. targeted syn-in proceed-tactic evaluation of language models.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202,brussels, belgium.
association for computationallinguistics..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448,florence, italy.
association for computational lin-guistics..julia mendelsohn, yulia tsvetkov, and dan jurafsky.
2020. a framework for the computational linguisticanalysis of dehumanization.
frontiers in artiﬁcialintelligence, 3:55..pushkar mishra, helen yannakoudakis, and ekaterinashutova.
2020. tackling online abuse: a survey ofautomated abuse detection methods.
arxiv preprintarxiv:1908.06024..marzieh mozafari, reza farahbakhsh, and noel crespi.
2019. a bert-based transfer learning approach forhate speech detection in online social media.
in in-ternational conference on complex networks andtheir applications, pages 928–940.
springer..aakanksha naik, abhilasha ravichander, normansadeh, carolyn rose, and graham neubig.
2018.stress test evaluation for natural language inference.
in proceedings of the 27th international conferenceon computational linguistics, pages 2340–2353,santa fe, new mexico, usa.
association for com-putational linguistics..isar nejadgholi and svetlana kiritchenko.
2020. oncross-dataset generalization in automatic detectionof online abuse.
in proceedings of the fourth work-shop on online abuse and harms, pages 173–183,online.
association for computational linguistics..yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..chikashi nobata,.
joel tetreault, achint thomas,yashar mehdad, and yi chang.
2016. abusive lan-guage detection in online user content.
in proceed-ings of the 25th international conference on worldwide web, www ’16, page 145–153, republic andcanton of geneva, che.
international world wideweb conferences steering committee..alexis palmer, christine carr, melissa robinson, andjordan sanders.
2020. cold: annotation schemeand evaluation data set for complex offensive lan-guage in english.
journal for language technologyand computational linguistics, pages 1–28..ji ho park, jamin shin, and pascale fung.
2018. re-ducing gender bias in abusive language detection.
in proceedings ofthe 2018 conference on em-pirical methods in natural language processing,pages 2799–2804, brussels, belgium.
associationfor computational linguistics..fabio poletto, valerio basile, manuela sanguinetti,cristina bosco, and viviana patti.
2020. resourcesand benchmark corpora for hate speech detection: asystematic review.
language resources and evalu-ation, pages 1–47..jing qian, mai elsherief, elizabeth belding, andwilliam yang wang.
2018. leveraging intra-userand inter-user representation learning for automatedhate speech detection.
in proceedings of the 2018conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 118–123, new orleans, louisiana.
associa-tion for computational linguistics..marco tulio ribeiro, tongshuang wu, carlos guestrin,and sameer singh.
2020. beyond accuracy: be-havioral testing of nlp models with checklist.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4902–4912, online.
association for computational lin-guistics..joni salminen, hind almerekhi, milica milenkovi´c,soon-gyo jung, jisun an, haewoon kwak, andbernard jansen.
2018. anatomy of online hate: de-veloping a taxonomy and machine learning modelsfor identifying and classifying hate in online newsmedia.
proceedings of the 12th international aaaiconference on web and social media, (1):330–339..mattia samory,.
indira sen, julian kohne, fabianfloeck, and claudia wagner.
2020. unsex mehere: revisiting sexism detection using psycholog-ical scales and adversarial samples.
arxiv preprintarxiv:2004.12764..timothy niven and hung-yu kao.
2019. probing neu-ral network comprehension of natural language ar-guments.
in proceedings of the 57th annual meet-ing of the association for computational linguis-tics, pages 4658–4664, florence, italy.
associationfor computational linguistics..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for com-putational linguistics, pages 1668–1678, florence,italy.
association for computational linguistics..52maarten sap, saadia gabriel, lianhui qin, dan ju-rafsky, noah a. smith, and yejin choi.
2020. so-cial bias frames: reasoning about social and powerin proceedings of theimplications of language.
58th annual meeting of the association for compu-tational linguistics, pages 5477–5490, online.
as-sociation for computational linguistics..anna schmidt and michael wiegand.
2017. a surveyon hate speech detection using natural language pro-in proceedings of the fifth internationalcessing.
workshop on natural language processing for so-cial media, pages 1–10, valencia, spain.
associa-tion for computational linguistics..rico sennrich.
2017. how grammatical is character-level neural machine translation?
assessing mtin pro-quality with contrastive translation pairs.
ceedings of the 15th conference of the europeanchapter of the association for computational lin-guistics: volume 2, short papers, pages 376–382,valencia, spain.
association for computational lin-guistics..deven santosh shah, h. andrew schwartz, and dirkhovy.
2020. predictive biases in natural languageprocessing models: a conceptual framework andoverview.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5248–5264, online.
association for computa-tional linguistics..thanh tran, yifan hu, changwei hu, kevin yen, feitan, kyumin lee, and se rim park.
2020. haber-tor: an efﬁcient and effective deep hatespeech de-in proceedings of the 2020 conference ontector.
empirical methods in natural language process-ing (emnlp), pages 7486–7502, online.
associa-tion for computational linguistics..bertie vidgen and leon derczynski.
2020. direc-tions in abusive language training data, a system-atic review: garbage in, garbage out.
plos one,15(12):e0243300..bertie vidgen, scott hale, ella guest, helen mar-getts, david broniatowski, zeerak waseem, austinbotelho, matthew hall, and rebekah tromble.
2020a.
detecting east asian prejudice on social me-dia.
in proceedings of the fourth workshop on on-line abuse and harms, pages 162–172, online.
as-sociation for computational linguistics..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019.challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online, pages 80–93, florence, italy.
association for computational linguistics..william warner and julia hirschberg.
2012. detectingin proceed-hate speech on the world wide web.
ings of the second workshop on language in socialmedia, pages 19–26, montr´eal, canada.
associationfor computational linguistics..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..zeerak waseem.
2016. are you a racist or am i seeingthings?
annotator inﬂuence on hate speech detec-in proceedings of the first work-tion on twitter.
shop on nlp and computational social science,pages 138–142, austin, texas.
association for com-putational linguistics..zeerak waseem, thomas davidson, dana warmsley,and ingmar weber.
2017. understanding abuse: atypology of abusive language detection subtasks.
inproceedings of the first workshop on abusive lan-guage online, pages 78–84, vancouver, bc, canada.
association for computational linguistics..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatespeech detection on twitter.
in proceedings of thenaacl student research workshop, pages 88–93,san diego, california.
association for computa-tional linguistics..zeerak waseem, james thorne, and joachim bingel.
2018. bridging the gaps: multi-task learning for do-main transfer of hate speech detection.
in jennifergolbeck, editor, online harassment.
springer..michael wiegand, josef ruppenhofer, and thomaskleinbauer.
2019. detection of abusive language:the problem of biased datasets.
in proceedings ofthe 2019 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 1 (longand short papers), pages 602–608, minneapolis,minnesota.
association for computational linguis-tics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..bertie vidgen, tristan thrush, zeerak waseem, anddouwe kiela.
2020b.
learning from the worst: dy-namically generated datasets to improve online hatedetection.
corr, abs/2012.15761..tongshuang wu, marco tulio ribeiro, jeffrey heer,and daniel weld.
2019. errudite: scalable, repro-in proceed-ducible, and testable error analysis.
ings of the 57th annual meeting of the association.
53for computational linguistics, pages 747–763, flo-rence, italy.
association for computational linguis-tics..ellery wulczyn, nithum thain, and lucas dixon.
2017.ex machina: personal attacks seen at scale.
in pro-ceedings of the 26th international conference onworld wide web, pages 1391–1399..marcos zampieri, shervin malmasi, preslav nakov,sara rosenthal, noura farra, and ritesh kumar.
2019. predicting the type and target of offensivein proceedings of the 2019posts in social media.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 1415–1420, minneapolis, minnesota.
association for computational linguistics..ziqi zhang and lei luo.
2019. hate speech detection:a solved problem?
the challenging case of long tailon twitter.
semantic web, 10(5):925–945..xiang zhou, yixin nie, hao tan, and mohit bansal.
2020. the curse of performance instability in analy-sis datasets: consequences, source, and suggestions.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 8215–8228, online.
association for computa-tional linguistics..54a data statement.
following bender and friedman (2018), we pro-vide a data statement, which documents the genera-tion and provenance of test cases in hatecheck..a. curation rationale in order to con-struct hatecheck, a ﬁrst suite of functional testsfor hate speech detection models, we generated3,901 short english-language text documents byhand and by using simple templates for group iden-tiﬁers and slurs (§2.4).
each document correspondsto one functional test and a binary gold standardlabel (hateful or non-hateful).
in order to validatethe gold standard labels, we trained a team of tenannotators, assigning ﬁve of them to each docu-ment, and asked them to provide independent la-bels (§2.5).
to further improve data quality, wealso gave annotators the option to ﬂag cases theyfelt were unrealistic (e.g.
nonsensical), but this ﬂagwas not used for any one hatecheck case bymore than one annotator..b. language variety hatecheck onlycovers english-language text documents.
we optedfor english language since this maximises hate-check’s relevance to previous and current work inhate speech detection, which is mostly concernedwith english-language data.
our language choicealso reﬂects the expertise of authors and annotators.
we discuss the lack of language variety as a limita-tion of hatecheck in §4.2 and suggest expansionto other languages as a priority for future research..c. speaker demographics since alltest cases in hatecheck were hand-crafted, thespeakers are the same as the authors.
test casesin the test suite were primarily generated by thelead author, who is a researcher at a uk university.
the lead author is not a native english speaker buthas lived in english-speaking countries for morethan ﬁve years and has extensively engaged withenglish-language hate speech in previous research.
all test cases were also reviewed by two co-authors,both of whom have worked with english-languagehate speech data for more than ﬁve years and oneof whom is a native english speaker from the uk..d. annotator demographics we re-cruited a team of ten annotators to work for twoweeks.
30% were male and 70% were female.
60%were 18-29 and 40% were 30-39.
20% were ed-ucated to high school level, 10% to undergradu-ate, 60% to taught masters and 10% to research.
degree (i.e.
phd).
70% were native english speak-ers and 30% were non-native but ﬂuent.
annota-tors had a range of nationalities: 60% were britishand 10% each were polish, spanish, argentinianand irish.
most annotators identiﬁed as ethnicallywhite (70%), followed by middle eastern (20%)and a mixed ethnic background (10%).
annotatorsall used social media regularly, and 60% used itmore than once per day.
all annotators had seenother people targeted by online abuse before, and80% had been targeted personally..all annotators had previously completed annota-tion work on at least one other hate speech dataset.
in the ﬁrst week, we introduced the binary anno-tation task to them in an onboarding session andtested their understanding on a set of 100 cases,which we then provided individual feedback on.
in the second week, we asked each annotator toannotate around 2,000 test cases so that each casein our test suite was annotated by varied sets ofexactly ﬁve annotators.
throughout the process,we communicated with annotators in real-time overa messaging platform.
we also followed guidancefor protecting and monitoring annotator well-beingprovided by vidgen et al.
(2019)..e. speech situation all test cases werecreated between the 23rd of november and the13th of december 2020..f. text characteristics the composi-tion of the dataset, including primary label andsecondary labels, is described in detail in §2.3 and§2.4 of the article..b references for functional tests.
f1 – strong negative emotions explicitly ex-pressed about a protected group or its members:resembles “expressed hatred” (davidson et al.,2017) and “identity attack” (banko et al., 2020)..f2 – explicit descriptions of a protected group orits members using very negative attributes: reﬁnesmore general “insult” categories (davidson et al.,2017; zampieri et al., 2019)..f3 – explicit dehumanisation of a protectedgroup or its members: prevalent form of hate(mendelsohn et al., 2020; banko et al., 2020; vid-gen et al., 2020a).
highlighted in our interviews(e.g.
i18: “hate crime [often claims] people areinferior and subhuman.”)..f4 – implicit derogation of a protected groupor its members: closely resembles “implied bias”.
55(sap et al., 2020) and “implicit abuse” (waseemet al., 2017; zhang and luo, 2019).
highlightedin our interviews (e.g.
i16: “hate has always beenexpressed idiomatically”)..f5 – direct threats against a protected group orits members: core element of several hate speechtaxonomies (golbeck et al., 2017; zampieri et al.,2019; vidgen et al., 2020a; banko et al., 2020).
f6 – threats expressed as normative statements:highlighted by an interviewee as a way of avoidinglegal consequences to hate speech (i1: “[normativethreats] are extremely hateful, but [legally] okay”).
f7 – hate expressed using slurs: prevalent wayof expressing hate (palmer et al., 2020; banko et al.,2020; kurrek et al., 2020)..f8 – non-hateful homonyms of slur: relevant.
alternative use of slurs (kurrek et al., 2020)..f9 – use of reclaimed slurs: likely source ofclassiﬁcation error (palmer et al., 2020).
high-lighted in our interviews (e.g.
i7: “a lot of lgbtpeople use slurs to identify themselves, like reclaimthe word queer, and people [...] report that and thenthat will get hidden”)..f10 – hate expressed using profanity: reﬁnesmore general “insult” categories (davidson et al.,2017; zampieri et al., 2019)..f11 – non-hateful uses of profanity: oversensi-tiveness of hate speech detection models to profan-ity (davidson et al., 2017; malmasi and zampieri,2018; van aken et al., 2018)..f12 – hate expressed through pronoun referencein subsequent clauses: syntactic relationships andlong-range dependencies as model weak points(burnap and williams, 2015; vidgen et al., 2019).
f13 – hate expressed through pronoun reference.
in subsequent sentences: see f12..f14 – hate expressed using negated positivestatements: negation as an effective adversary forhate speech detection models (hosseini et al., 2017;dinan et al., 2019)..f15 – non-hate expressed using negated hateful.
statements: see f14..f16 – hate phrased as a question: likely source.
of classiﬁcation error (van aken et al., 2018)..f17 – hate phrased as an opinion: highlightedby an interviewee as a way of avoiding legal conse-quences to hate speech (i1: “if you start a sentenceby saying ‘i think that’ [...], the limits of what youcan say are much bigger”)..f18 – neutral statements using protected groupidentiﬁers: oversensitiveness of hate speech de-.
tection models to terms such as “black” and “gay”(dixon et al., 2018; park et al., 2018; kennedyet al., 2020).
also highlighted in our interviews(e.g.
i7: “i have seen the algorithm get it wrong, ifsomeone’s saying something like ‘i’m so gay’.”)..f19 – positive statements using protected group.
identiﬁers: see f18..f20 – denouncements of hate that quote it:counter speech as a source of classiﬁcation error(warner and hirschberg, 2012; van aken et al.,2018; vidgen et al., 2020a).
most mentioned con-cern in our interviews (e.g.
i4: “people will bequoting someone, calling that person out [...] butthat will get picked up by the system”)..f21 – denouncements of hate that make direct.
reference to it: see f20..f22 – abuse targeted at objects: distinct fromhate speech since it targets out-of-scope entities(wulczyn et al., 2017; zampieri et al., 2019)..f23 – abuse targeted at individuals not referenc-.
ing membership in a protected group: see f22..f24 – abuse targeted at non-protected groups.
(e.g.
professions): see f22..f25 – swaps of adjacent characters: simple mis-spellings can be challenging for detection models(van aken et al., 2018; qian et al., 2018).
particu-larly relevant to hate speech since they can reﬂectintentional behaviour of users looking to avoid de-tection (hosseini et al., 2017; gr¨ondahl et al., 2018;vidgen et al., 2019)..f26 – missing characters: highlighted in ourinterviews (e.g.
i7: “it could be a misspelling ofa word like ‘f*ggot’, and someone’s put one ‘g’instead of two”)..f27 – missing word boundaries: effective adver-sary for a hate speech detection model (gr¨ondahlet al., 2018).
resembles the use of hashtags onsocial media (i2: “there have been a highly islamo-phobic hashtags going around”)..f28 – added spaces between characters: effec-tive adversary for a hate speech detection model(gr¨ondahl et al., 2018).
highlighted in our inter-views (e.g.
i5: “misspellings, missing letters oradditional spaces between the letters.”)..f29 – leet speak: resembles “obfuscations”(nobata et al., 2016; van aken et al., 2018).
high-lighted in our interviews (e.g.
i14: “[hate speakers]replace letters with numbers”)..56c hateful slurs in hatecheck.
for each of the seven protected groups covered byhatecheck, we searched hatebase.org, a crowd-sourced hate speech lexicon, for slurs which targetthat group.
from these slurs, we selected the threethat were most often logged by users of the site (e.g.
“wh*re”, “b*tch” and “sl*t” for women), exceptfor when the third-most sighted slur was loggedsubstantially less often than the second, in whichcase we selected the top two (e.g.
“tr*nny” and“sh*male” for trans people).
for immigration sta-tus, which is not a target category on hatebase.org,we chose “r*pefugee”, a slur for refugees used bythe european far right, and “w*tback”, a slur formexican immigrants to the us, which was loggedsimilarly often as other slurs in hatecheck..protected group slurswomentrans peoplegay peopleblack peopledisabled peoplemuslimsimmigrants.
b*tch, sl*t, wh*retr*nny, sh*malef*ggot, f*g, q*eern*gger, c*onr*tard, cr*pple, m*ngm*zzie, j*hadi, camel f*ckerw*tbacks, r*pefugees.
table 5: hateful slurs in hatecheck.
for reclaimed slurs (f9), we focus on slursreclaimed by black communities (particularly“n*gga”), gay communities (“f*g”, “f*ggot”,“q*eer”) and by women (“b*tch”), reﬂecting theconcerns highlighted by our interview participants(e.g.
i4: “n*gga would often get [wrongly] pickedup by [moderation] systems”).
ahead of thestructured annotation process (§2.5) and only fortest cases with reclaimed slurs, we asked self-identifying members of the relevant groups in ourpersonal networks whether they would consider thetest cases to contain valid and realistic reclaimedslur uses, which held true for all test cases..d datasets for fine-tuning.
d.1 davidson et al.
(2017) data.
sampling davidson et al.
(2017) searched twit-ter for tweets containing keywords from a list theycompiled from hatebase.org, which yielded a sam-ple of tweets from 33,458 users.
they then ran-domly sampled 25,000 tweets from all tweets ofthese users..annotation the authors hired crowd workersfrom crowdflower to annotate each tweet as hate-ful, offensive or neither.
92.0% of tweets were an-notated by three crowd workers, the remainder byat least four and up to nine.
for inter-annotatoragreement, the authors report a “crowdflowerscore” of 92%..data we used 24,783 annotated tweets madeavailable by theauthors on github.com/t-davidson/hate-speech-and-offensive-language.
1,430 tweets (5.8%) are labelled hateful, 19,190(77.4%) offensive and 4,163 (16.8%) neither.
we collapse the latter two labels into a singlenon-hateful label to match hatecheck’s binaryformat, resulting in 1,430 tweets (5.8%) labelledhateful and 23,353 (94.2%) labelled non-hateful..deﬁnition of hate speech “language that isused to expresses hatred towards a targeted groupor is intended to be derogatory, to humiliate, or toinsult the members of the group”..d.2 founta et al.
(2018) data.
sampling founta et al.
(2018) initially collecteda random set of 32 million tweets from twitter.
they then used a boosted random sampling proce-dure based on negative sentiment and occurrenceof offensive words as selected from hatebase.orgto augment a random subset of this initial samplewith tweets they expected to be more likely to behateful or abusive..annotation the authors hired crowd workersfrom crowdflower to annotate each tweet as hate-ful, abusive, spam or normal.
all tweets were an-notated by ﬁve crowd workers.
for inter-annotatoragreement, the authors report that 55.9% of tweetshad four out of ﬁve annotators agreeing on a label..data the authors provided us access to the fulltext versions of 99,996 annotated tweets.
these cor-respond to the tweet ids made available by the au-thors on github.com/encaseh2020/hatespeech-twitter.
4,965 tweets (5.0%) are labelled hateful,27,150 (27.2%) abusive, 14,030 (14.0%) spam and53,851 (53.9%) normal.
we collapse the latterthree labels into a single non-hateful label to matchhatecheck’s binary format, resulting in 4,965tweets (5.0%) labelled hateful and 95,031 tweets(95.0%) labelled non-hateful..deﬁnition of hate speech “language used toexpress hatred towards a targeted individual or.
57computation we ran all computations on a mi-crosoft azure “standard nc24” server equippedwith two nvidia tesla k80 gpu cards.
the av-erage wall time for each hyperparameter tuningtrial of b-d was around 17 minutes, and for b-faround 70 minutes..source code ourgithub.com/paul-rottger/hatecheck-experiments..available on.
code.
is.
f comparison to sota results.
most previous work that trains and evaluates mod-els on davidson et al.
(2017) and founta et al.
(2018) data uses their original multiclass label for-mat.
in the multiclass case, the relative size of thehateful class compared to the non-hateful classes islarger than in the binary case, which is likely whymost models do not use class weights.
for compa-rability, we thus ﬁne-tuned unweighted multiclassversions of b-d and b-f, using the same modelparameters described in appendix e..on multiclass davidson et al.
(2017) data, moza-fari et al.
(2019) report a weighted-average f1 scoreof 91 for their bert-base model and 92 for bert-base combined with a cnn.
cao et al.
(2020) re-port a micro f1 of 89.9 for their ensemble-like“deephate” classiﬁer.
our unweighted multiclassbert-base model achieves 90.7 weighted-averagef1 and 91.1 micro f1..on multiclass founta et al.
(2018) data, caoet al.
(2020) report a micro f1 of 79.1 for “deep-hate”.
our unweighted multiclass bert-basemodel achieves 81.7 micro f1..tran et al.
(2020) recently achieved sotaon several other hate speech datasets with theirhabertor model.
they also ﬁnd that bert-base consistently performs very near their sota.
however, they do not evaluate their models ondavidson et al.
(2017) or founta et al.
(2018) data..group, or is intended to be derogatory, to humiliate,or to insult the members of the group, on the basisof attributes such as race, religion, ethnic origin,sexual orientation, disability, or gender”..d.3 pre-processing.
before using the datasets for ﬁne-tuning, we low-ercase all text and remove newline and tab charac-ters.
we replace urls, user mentions and emojiswith [url], [user] and [emoji] tokens.
wealso split hashtags into separate tokens using thewordsegment python package..e details on transformer models.
model architecture we implemented uncasedbert-base models (devlin et al., 2019) usingthe transformers python library (wolf et al.,2020).
uncased bert-base, which is trained onlower-cased english text, has 12 layers, a hiddenlayer size of 768, 12 attention heads and a total of110 million parameters.
for sequence classiﬁca-tion, we added a linear layer with softmax output..fine-tuning b-d was ﬁne-tuned on binarydavidson et al.
(2017) data and b-f on binaryfounta et al.
(2018) data.
for both datasets, weused a stratiﬁed 80/10/10 train/dev/test split.
mod-els were trained for three epochs each.
trainingbatch size was 16. we used cross-entropy losswith class weights emphasising the hateful minor-ity class.
weights were set to the relative proportionof the other class in the training data, meaning thatfor a 1:9 hateful:non-hateful case split, loss on hate-ful cases would be multiplied by 9. the optimiserwas adamw (loshchilov and hutter, 2019) witha 5e-5 learning rate and a 0.01 weight decay.
forregularisation, we set a 10% dropout probability..hyperparameter tuning the number of ﬁne-tuning epochs, the learning rate and the trainingbatch size were determined by exhaustive gridsearch.
we used the range of possible values recom-mended by devlin et al.
(2019): [2, 3, 4] for epochs,[2e-5, 3e-5, 5e-5] for learning rate and [16, 32] forbatch size.
there were 18 training/evaluation runsfor each model.
the best conﬁguration was se-lected based on loss on the 10% development set..held-out performance micro/macro f1 scoreson the held-out test sets corresponding to their train-ing data are 91.5/70.8 for b-d (davidson et al.,2017) and 92.9/70.3 for b-f (founta et al., 2018)..58