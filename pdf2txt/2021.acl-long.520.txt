question answering over temporal knowledge graphs.
apoorv saxenaindian institute of sciencebangaloreapoorvsaxena@iisc.ac.in.
soumen chakrabartiindian institute of technologybombaysoumen@cse.iitb.ac.in.
partha talukdargoogle researchindiapartha@google.com.
abstract.
temporal knowledge graphs (temporal kgs)extend regular knowledge graphs by provid-ing temporal scopes (e.g., start and end times)on each edge in the kg.
while question an-swering over kg (kgqa) has received someattention from the research community, qaover temporal kgs (temporal kgqa) is arelatively unexplored area.
lack of broad-coverage datasets has been another factor lim-iting progress in this area.
we address thischallenge by presenting cronquestions,the largest known temporal kgqa dataset,clearly stratiﬁed into buckets of structural com-plexity.
cronquestions expands the onlyknown previous dataset by a factor of 340×.
we ﬁnd that various state-of-the-art kgqamethods fall far short of the desired perfor-mance on this new dataset.
in response,we also propose cronkgqa, a transformer-based solution that exploits recent advances intemporal kg embeddings, and achieves per-formance superior to all baselines, with an in-crease of 120% in accuracy over the next bestperforming method.
through extensive experi-ments, we give detailed insights into the work-ings of cronkgqa, as well as situationswhere signiﬁcant further improvements appearpossible.
in addition to the dataset, we have re-leased our code as well..1.introduction.
temporal knowledge graphs (temporal kgs) aremulti-relational graph where each edge is associ-ated with a time duration.
this is in contrast to aregular kg where no time annotation is present.
for example, a regular kg may contain a factsuch as (barack obama, held position, presidentof usa), while a temporal kg would contain thestart and end time as well — (barack obama, heldposition, president of usa, 2008, 2016).
edgesmay be associated with a set of non-contiguous.
time intervals as well.
these temporal scopes onfacts can be either automatically estimated (taluk-dar et al., 2012) or user contributed.
several suchtemporal kgs have been proposed in the literature,where the focus is on kg completion (dasguptaet al.
2018; garc´ıa-dur´an et al.
2018; leetaru andschrodt 2013; lacroix et al.
2020; jain et al.
2020)..the task of knowledge graph question answer-ing (kgqa) is to answer natural language ques-tions using a kg as the knowledge base.
this isin contrast to reading comprehension-based ques-tion answering, where typically the question is ac-companied by a context (e.g., text passage) andthe answer is either one of multiple choices (ra-jpurkar et al., 2016) or a piece of text from thecontext (yang et al., 2018).
in kgqa, the an-swer is usually an entity (node) in the kg, and thereasoning required to answer questions is eithersingle-fact based (bordes et al., 2015), multi-hop(yih et al.
2015, zhang et al.
2017) or conjunc-tion/comparison based reasoning (talmor and be-rant, 2018).
temporal kgqa takes this a stepfurther where:.
1. the underlying kg is a temporal kg.
2. the answer is either an entity or time duration.
3. complex temporal reasoning might be needed..kg embeddings are low-dimensional dense vec-tor representations of entities and relations in a kg.
several methods have been proposed in the litera-ture to embed kgs (bordes et al.
2013, trouillonet al.
2016, vashishth et al.
2020).
these embed-dings were originally proposed for the task of kgcompletion i.e., predicting missing edges in thekg, since most real world kgs are incomplete.
recently, however, they have also been applied tothe task of kgqa where they have been shown toincrease performance the settings of both of com-plete and incomplete kgs (saxena et al.
2020; sunet al.
2020)..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6663–6676august1–6,2021.©2021associationforcomputationallinguistics6663dataset.
kg.
freebasesimplequestionsmetaqa kgmetaqafreebasewebquestionsfreebasecomplexwebquestionstempquestionsfreebasecronquestions (ours) wikidata.
temporalfacts(cid:55)(cid:55)(cid:55)(cid:55)(cid:55)(cid:51).
(cid:55)(cid:55)(cid:55)(cid:51)(cid:51)(cid:51).
(cid:55)(cid:51)(cid:51)(cid:51)(cid:51)(cid:51).
0%0%<16%-100%100%.
108k400k5,81035k1,271410k.
question typesmulti-entity multi-relation temporal.
# questions.
table 1: kgqa dataset comparison.
statistics about percentage of temporal questions for webquestions are takenfrom jia et al.
(2018a).
we do not have an explicit number of temporal questions for complexwebquestions, butsince it is constructed automatically using questions from webquestions, we expect the percentage to be similarto webquestions (16%).
please refer to section 2.1 for details..temporal kg embeddings are another upcomingarea where entities, relations and timestamps in atemporal kg are embedded in a low-dimensionalvector space (dasgupta et al.
2018, lacroix et al.
2020, jain et al.
2020, goel et al.
2019).
here too,the main application so far has been temporal kgcompletion.
in our work, we investigate whethertemporal kg embeddings can be applied to thetask of temporal kgqa, and how they fare com-pared to non-temporal embeddings or off-the-shelfmethods without any kg embeddings..in this paper we propose cronquestions, anew dataset for temporal kgqa.
cronques-tions consists of both a temporal kg and accom-panying natural language questions.
there werethree main guiding principles while creating thisdataset:1. the associated kg must provide temporal an-.
notations..reasoning..2. questions must involve an element of temporal.
3. the number of labeled instances must be largeenough that it can be used for training models,rather than for evaluation alone..guided by the above principles, we present adataset consisting of a temporal kg with 125kentities and 328k facts, along with a set of 410knatural language questions that require temporalreasoning..on this new dataset, we apply approaches basedon deep language models (lm) alone, such as t5(raffel et al., 2020), bert (devlin et al., 2019),and knowbert (peters et al., 2019), and alsohybrid lm+kg embedding approaches, such asentities-as-experts (f´evry et al., 2020) and em-bedkgqa (saxena et al., 2020).
we ﬁnd thatthese baselines are not suited to temporal reason-ing.
in response, we propose cronkgqa, anenhancement of embedkgqa, which outperforms.
baselines across all question types.
cronkgqaachieves very high accuracy on simple temporalreasoning questions, but falls short when it comesto questions requiring more complex reasoning.
thus, although we get promising early results,cronquestions leaves ample scope to improvecomplex temporal kgqa.
our source code alongwith the cronquestions dataset can be found athttps://github.com/apoorvumang/cronkgqa..2 related work.
2.1 temporal qa data sets.
there have been several kgqa datasets proposedin the literature (table 1).
in simplequestions (bor-des et al., 2015) one needs to extract just a singlefact from the kg to answer a question.
metaqa(zhang et al., 2017) and webquestionssp (yihet al., 2015) require multi-hop reasoning, whereone must traverse over multiple edges in the kgto reach the answer.
complexwebquestions (tal-mor and berant, 2018) contains both multi-hop andconjunction/comparison type questions.
however,none of these are aimed at temporal reasoning, andthe kg they are based on is non-temporal..temporal qa datasets have mostly been studiedin the area of reading comprehension.
one suchdataset is torque (ning et al., 2020), where thesystem is given a question along with some context(a text passage) and is asked to answer a multiplechoice question with ﬁve choices.
this is in con-trast to kgqa, where there is no context, and theanswer is one of potentially hundreds of thousandsof entities..tempquestions (jia et al., 2018a) is a kgqadataset speciﬁcally aimed at temporal qa.
it con-sists of a subset of questions from webquestions,free917 (cai and yates, 2013) and complex-questions (bao et al., 2016) that are temporal in.
6664example template.
reasoningsimple time when did {head} hold the position of {tail}simple entity which award did {head} receive in {time}before/after who was the {tail} {type} {head}first/lasttime join.
example questionwhen did obama hold the position of president of usawhich award did brad pitt receive in 2001who was the president of usa before obamawhen did messi play their ﬁrst game.
when did {head} play their {adj} gamewho held the position of {tail} during {event} who held the position of president of usa during wwii.
table 2: example questions for different types of temporal reasoning.
{head}, {tail} and {time} correspond toentities/timestamps in facts of the form (head, relation, tail, timestamp).
{event} corresponds to entities in eventfacts eg.
wwii.
{type} can be one of before/after and {adj} can be one of ﬁrst/last.
please refer to section 3.2 fordetails..nature.
they gave a deﬁnition for “temporal ques-tion” and used certain trigger words (for example‘before’, ‘after’) along with other constraints toﬁlter out questions from these datasets that fell un-der this deﬁnition.
however, this dataset containsonly 1271 questions — useful only for evaluation— and the kg on which it is based (a subset offreebase (bollacker et al., 2008)) is not a temporalkg.
another drawback is that freebase has notbeen under active development since 2015, there-fore some information stored in it is outdated andthis is a potential source of inaccuracy..2.2 temporal qa algorithms.
to the best of our knowledge, recent kgqa al-gorithms (miller et al.
2016; sun et al.
2019; co-hen et al.
2020; sun et al.
2020) work with non-temporal kgs, i.e., kgs containing facts of theform (subject, relation, object).
extending these totemporal kgs containing facts of the form (subject,relation, object, start time, end time) is a non-trivialtask.
tequila (jia et al., 2018b) is one methodaimed speciﬁcally at temporal kgqa.
tequiladecomposes and rewrites the question into non-temporal sub-questions and temporal constraints.
answers to sub-questions are then retrieved usingany kgqa engine.
finally, tequila uses con-straint reasoning on temporal intervals to computeﬁnal answers to the full question.
a major draw-back of this approach is the use of pre-speciﬁedtemplates for decomposition, as well as the as-sumption of having temporal constraints on entities.
also, since it is made for non-temporal kgs, thereis no direct way of applying it to temporal kgswhere facts are temporally scoped..3 cronquestions: the new temporal.
kgqa dataset.
cronquestions, our temporal kgqa datasetconsists of two parts: a kg with temporal anno-tations, and a set of natural language questions.
requiring temporal reasoning..3.1 temporal kg.
to prepare our temporal kg, we started by takingall facts with temporal annotations from the wiki-data subset proposed by lacroix et al.
(2020).
weremoved some instances of the predicate “memberof sports team” in order to balance out the kgsince this predicate constituted over 50 percent ofthe facts.
timestamps were discretized to years.
this resulted in a kg with 323k facts, 125k entitiesand 203 relations..however, this ﬁltering of facts misses out on im-portant world events.
for example, the kg subsetcreated using the aforementioned technique con-tains the entity world war ii but no associated factthat tells us when world war ii started or ended.
this knowledge is needed to answer questions suchas “who was the president of the usa during worldwar ii?.” to overcome this shortcoming, we ﬁrstextracted entities from wikidata that have a “starttime” and “end time” annotation.
from this set,we then removed entities which were game shows,movies or television series (since these are not im-portant world events, but do have a start and endtime annotation), and then removed entities withless than 50 associated facts.
this ﬁnal set of enti-tities was then added as facts in the format (wwii,signiﬁcant event, occurred, 1939, 1945).
the ﬁnaltemporal kg consisted of 328k facts out of which5k are event-facts..3.2 temporal questions.
to generate the qa dataset, we started with a setof templates for temporal reasoning.
these weremade using the ﬁve most frequent relations fromour wikidata subset, namely• member of sports team• position held• award received• spouse.
6665templateseed qn.
humanparaphrases.
machineparaphrases.
when did {head} play in {tail}when did messi play in fc barcelonawhen was messi playing in fc barcelonawhich years did messi play in fc barcelonawhen did fc barcelona have messi in their teamwhat time did messi play in fc barcelonawhen did messi play for fc barcelonawhen did messi play at fc barcelonawhen has messi played at fc barcelona.
table 3: slot-ﬁlled paraphrases generated by humansand machine.
please refer to section 3.2 for details..simple entitysimple timebefore/afterfirst/lasttime joinentity answertime answertotal.
train90,65161,47123,869118,55655,453225,672124,328350,000.dev7,7455,1971,98211,1983,87819,36210,63830,000.test7,8125,0462,15111,1593,83219,52410,47630,000.table 4: number of questions in our dataset across dif-ferent types of reasoning required and different answertypes.
please refer to section 3.2.1 for details..• employer.
this resulted in 30 unique seed templates overﬁve relations and ﬁve different reasoning structures(please see table 2 for some examples).
each ofthese templates has a corresponding procedure thatcould be executed over the temporal kg to extractall possible answers for that template.
however,similar to zhang et al.
(2017), we chose not tomake this procedure a part of the dataset, to removeunwelcome dependence of qa systems on suchformal candidate collection methods.
this alsoallows easy augmentation of the dataset, since onlyquestion-answer pairs are needed..in the same spirit as complexwebquestions,we then asked human annotators to paraphrasethese templates in order to generate more linguisticdiversity.
annotators were given slot-ﬁlled tem-plates with dummy entities and times, and askedto rephrase the question such that the dummy en-tities/times were present in the paraphrase and thequestion meaning did not change.
this resulted in246 unique templates..we then used the monolingual paraphraser de-veloped by hu et al.
(2019) to automatically gen-erate paraphrases using these 246 templates.
afterverifying their correctness through annotators, weended up with 654 templates.
these templates were.
then ﬁlled using entity aliases from wikidata togenerate 410k unique question-answer pairs..finally, while splitting the data into train/test.
folds, we ensured that1. paraphrases of train questions are not present in.
test questions..2. there is no entity overlap between test questionsand train questions.
event overlap is allowed.
the second requirement implies that, if the ques-tion “who was president before obama” is presentin the train set, the test set cannot contain any ques-tion that mentions the entity ‘obama’.
while thispolicy may appear like an overabundance of cau-tion, it ensures that models are doing temporal rea-soning rather than guessing from entities seen dur-ing training.
lewis et al.
(2020) noticed an issue inwebquestions where they found that almost 30%of test questions overlapped with training ques-tions.
the issue has been seen in the metaqadataset as well, where there is signiﬁcant overlapbetween test/train entities and test/train questionparaphrases, leading to suspiciously high perfor-mance on baseline methods even with partial kgdata (saxena et al., 2020), which suggests that mod-els that apparently perform well are not necessarilyperforming the desired reasoning over the kg..a drawback of our data creation protocol isthat question/answer pairs are generated automat-ically.
therefore, the question distribution is ar-tiﬁcial from a semantic perspective.
(complex-webquestions has a similar limitation.)
however,since developing models that are capable of tempo-ral reasoning is an important direction for naturallanguage understanding, we feel that our datasetprovides an opportunity to both train and evaluatekgqa models because of its large size, notwith-standing its lower-than-natural linguistic variety.
insection 6.4, we show the effect that training datasize has on model performance..summarizing, each of our examples contains.
1. a paraphrased natural language question.
2. a set of entities/times in the question.
3. a set of ‘gold’ answers (entity or time)..the entities are speciﬁed as wikidata ids (e.g.,q219237), and times are years (e.g., 1991).
weinclude the set of entities/times in the test ques-tions as well since similar to other kgqa datasets(metaqa, webquestions, complexwebquestions)and methods that use these datasets (pullnet,emql), entity linking is considered as a sepa-rate problem and complete entity linking is as-.
6666sumed.
we also include the seed template andhead/tail/time annotation in the train fold, but omitthese from the test fold..3.2.1 question categorizationin order to aid analysis, we categorize questionsinto “simple reasoning” and “complex reasoning”questions (please refer to table 4 for the distribu-tion statistics).
simple reasoning: these questions require a sin-gle fact to answer, where the answer can be ei-ther an entity or a time instance.
for example thequestion “who was the president of the unitedstates in 2008?” requires a single fact to answerthe question, namely (barack obama, held posi-tion, president of usa, 2008, 2016)complex reasoning: these questions.
requiremultiple facts to answer and can be more varied.
for example “who was the ﬁrst president ofthe united states?” this requires reasoningover multiple facts pertaining to the entity“president of the united states”.
in our dataset,all questions that are not “simple reasoning”questions are considered complex questions.
these are further categorized into the types“before/after‘’, “ﬁrst/last” and “time join” —please refer table 2 for examples of thesequestions..4 temporal kg embeddings.
we investigate how we can use kg embeddings,both temporal and non-temporal, along with pre-trained language models to perform temporalkgqa.
we will ﬁrst brieﬂy describe the speciﬁckg embedding models we use, and then go on toshow how we use them in our qa models.
in allcases, the scores are turned into suitable losses withregard to positive and negative tuples in an incom-plete kg, and these losses minimized to train theentity, time and relation representations..4.1 complex.
complex (trouillon et al., 2016) represents eachentity e as a complex vector ue ∈ cd.
each rela-tion r is represented as a complex vector vr ∈ cdas well.
the score φ of a claimed fact (s, r, o) is.
φ(s, r, o) = (cid:60)((cid:104)us, vr, u(cid:63).
o(cid:105)).
= (cid:60)(cid:0) (cid:80)d.d=1 us[d]vr[d]uo[d](cid:63)(cid:1).
(1)where (cid:60)(·) denotes the real part and c(cid:63) is thecomplex conjugate.
despite further developments,complex, along with reﬁned training protocols.
(lacroix et al., 2018) remains among the strongestkb embedding approaches (rufﬁnelli et al., 2020)..4.2 tcomplex, tntcomplex.
lacroix et al.
(2020) took an early step to extendcomplex with time.
each timestamp t is also rep-resented as a complex vector wt ∈ cd.
for aclaimed fact (s, r, o, t), their tcomplex scoringfunction is.
φ(s, r, o, t) = (cid:60)((cid:104)us, vr, u(cid:63).
o, wt(cid:105)).
(2).
their tntcomplex scoring function uses two rep-resentations of relations r: vtr , which is sensitive totime, and vr, which is not.
the scoring function isthe sum of a time-sensitive and a time-insensitivepart: (cid:60)((cid:104)us, vt.o, wt(cid:105) + (cid:104)us, vr, u(cid:63).
o, 1(cid:105))..r , u(cid:63).
4.3 timeplex.
timeplex (jain et al., 2020) augmented com-plex with embeddings ut ∈ cd for discretizedtime instants t. to incorporate time, timeplexuses three representations for each relation r, viz.,(vsor , vstr ) and writes the base score of a tuple(s, r, o, t) as.
r , vot.
φ(s, r, o, t) = (cid:104)us, vsor , u(cid:63)+ β (cid:104)uo, vot.
r , u(cid:63).
r , u(cid:63)o(cid:105) + α (cid:104)us, vstt (cid:105)t (cid:105) + γ (cid:104)us, uo, u(cid:63)t (cid:105),(3).
where α, β, γ are hyperparameters..5 cronkgqa: our proposed method.
we start with a temporal kg, apply a time-agnosticor time-sensitive kg embedding algorithm (com-plex, tcomplex, or timeplex) to it, and obtainentity, relation, and timestamp embeddings for thetemporal kg.
we will use the following notation.
• e is the matrix of entity embeddings• t is the matrix of timestamp embeddings• e.t is the concatenation of e and t matrices.
this is used for scoring answers, since the answercan be either an entity or timestamp..in case entity/timestamp embeddings are complexvalued vectors in cd, we expand them to real val-ued vectors of size 2d, where the ﬁrst half is thereal part and the second half is the complex part ofthe original vector..we ﬁrst apply embedkgqa (saxena et al.,2020) directly to the task of temporal kgqa.
in itsoriginal implementation, embedkgqa uses com-plex (section 4.1) embeddings and can only dealwith non-temporal kgs and single entity questions.
in order to apply it to cronquestions, we setthe ﬁrst entity encountered in the question as the.
6667figure 1: the cronkgqa method.
(i) a temporal kg embedding model (section 4) is used to generate em-beddings for each timestamp and entity in the temporal knowledge graph (ii) bert is used to get two questionembeddings: qeent and qetime.
(iii) embeddings of entity/time mentions in the question are combined with ques-tion embeddings using equations 4 and 5 to get score vectors for entity and time prediction.
(iv) score vectors areconcatenated and softmax is used get answer probabilities.
please refer to section 5 for details..“head entity” needed by embedkgqa.
along withthis, we set the entity embedding matrix e to be thecomplex embedding of our kg entities, and initial-ize t to a random learnable matrix.
embedkgqathen performs prediction over e.t ..next, we modify embedkgqa so that it canuse temporal kg embeddings.
we use tcomplex(section 4.2) for getting entity and timestamp em-beddings.
cronkgqa (figure 1) utilizes twoscoring functions, one for predicting entity andone for predicting time.
using a pre-trained lm(bert in our case) cronkgqa ﬁnds a questionembedding qe.
this is then projected to get twoembeddings, qeent and qetime, which are questionembeddings for entity and time prediction respec-tively.
entity scoring function: we extract a subject en-tity s and a timestamp t from the question.
ifeither is missing, we use a dummy entity/time.
then, using the scoring function φ(s, r, o, t) fromequation 2, we calculate a score for each entitye ∈ e as.
φent(e) = (cid:60)((cid:104)us, qeent, u(cid:63).
e, wt(cid:105)).
(4).
where e is the set of entities in the kg.
thisgives us a score for each entity being an answer.
time scoring function: similarly, we extract asubject entity s and object entity o from the ques-tion, using dummy entities if none are present.
then, using 2, we calculate a score for each times-.
tamp t ∈ t as.
φtime(t) = (cid:60)((cid:104)us, qetime, u(cid:63).
o, wt(cid:105)).
(5).
the scores for all entities and times are concate-nated, and softmax is used to calculate answerprobabilities over this combined score vector.
themodel is trained using cross entropy loss..6 experiments and diagnostics.
in this section, we aim to answer the followingquestions:1. how do baselines and cronkgqa performon the cronquestions task?
(section 6.2.)
2. do some methods perform better than others on.
speciﬁc reasoning tasks?
(section 6.3.).
3. how much does the training dataset size (num-ber of questions) affect the performance of amodel?
(section 6.4.).
4. do temporal kg embeddings confer any advan-tage over non-temporal kg embeddings?
(sec-tion 6.5.).
6.1 other methods compared.
it has been shown by petroni et al.
(2019) and raf-fel et al.
(2020) that large lms, such as bertand its variants, capture real world knowledge (col-lected from their massive, encyclopedic trainingcorpus) and can directly be applied to tasks suchas qa.
in these baselines, we do not speciﬁcallyfeed our version of the temporal kg to the model —.
6668bert-2.1, 30.2, ...-3.1, -50, ...harry truman[cls] who was the president of usaafter world war iipresidentof usaworld war iioccuredposition heldposition held2008 - 20161945 - 1953significantevent1939 - 1945barackobamaharrytrumanq11696: president               of usaq11613: harry               trumanq362: world            war iitemporal kg embeddings19441945<empty>temporalkge  modelmodel.
overall.
hits@1question type.
complex0.0860.0860.0830.0730.2860.2570.2570.392.
0.0710.070.070.0810.2880.2780.2880.647.answer typesimple entity time0.060.0480.0480.0670.0570.2130.2310.549.
0.0520.050.0510.0910.290.3060.3290.987.
0.0770.0820.0810.0880.4110.3130.3180.699.hits@10question type.
overall.
0.2130.2020.201-0.6720.6630.6780.884.complex0.2050.1920.189-0.6320.6140.6230.802.answer typesimple entity time0.2530.2310.23-0.3410.6650.6980.857.
0.1920.1860.185-0.850.6620.6680.898.
0.2250.2150.217-0.7250.7290.7530.992.bertrobertaknowbertt5-3bembedkgqat-eae-addt-eae-replacecronkgqa.
table 5: performance of baselines and our methods on the cronquestions dataset.
methods above the midruledo not use any kg embeddings, while the ones below use either temporal or non-temporal kg embeddings.
hits@10 are not available for t5-3b since it is a text-to-text model and makes a single prediction.
please refer tosection 6.2 for details..we instead expect the model to have the real worldknowledge to compute the answer.
bert: we experiment with bert, roberta(liu et al., 2019) and knowbert (peters et al.,2019) which is a variant of bert where informa-tion from knowledge bases such as wikidata andwordnet has been injected into bert.
we add aprediction head on top of the [cls] token of theﬁnal layer and do a softmax over it to predict theanswer probabilities..t5: in order to apply t5 (raffel et al., 2020)to temporal qa, we transform each questionin our dataset to the form ‘temporal question:(cid:104)question(cid:105)?’.
for evaluation there are two cases:1. time answer: we do exact string matching.
between t5 output and correct answer..2. entity answer: we compare the system outputto the aliases of all entities in the kg.
theentity having an alias with the smallest editdistance (levenshtein, 1966) to the predictedtext output is taken as the predicted entity.
entities as experts: f´evry et al.
(2020) proposedeae, a model which aims to integrate entityknowledge into a transformer-based languagemodel.
for temporal kgqa on cronques-tions, we assume that all grounded entity andtime mention spans are marked in the question1.
we will refer to this model as t-eae-add.
we tryanother variant of eae, t-eae-replace, whereinstead of adding the entity/time and bert tokenembeddings, we replace the bert embeddingswith the entity/time embeddings for entity/timementions.2.
1this assumption can be removed by using eae’s early.
transformer stages as ne spotters and disambiguators..2appendix a.1 gives details of our eae implementation..6.2 main results.
table 5 shows the results of various methods onour dataset.
we see that methods based on largepre-trained lms alone (bert, roberta, t5), aswell as knowbert, perform signiﬁcantly worsethan methods that are augmented with kg embed-dings (temporal or non-temporal).
this is probablybecause having kg embeddings speciﬁc to ourtemporal kg helps the model to focus on thoseentities/timestamps.
in our experiments, bert per-forms slightly better than knowbert, even thoughknowbert has entity knowledge in its parameters.
t5-3b performs the best among the lms we tested,possibly because of the large number of parametersand pre-training..even among methods that use kg embeddings,cronkgqa performs the best on all metrics,followed by t-eae-replace.
since embedkgqahas non-temporal embeddings, its performance onquestions where the answer is a time is very low —comparable to bert — which is the lm used inour embedkgqa implementation..another.
interesting thing to note is.
theperformance on simple reasoning questions.
cronkgqa far outperforms baselines for simplequestions, achieving close to 0.99 hits@1, whichis much lower for t-eae (0.329).
we believe theremight be a few reasons that contribute to this:1. there is the inductive bias of combining em-beddings using tcomplex scoring function incronkgqa, which is the same one used increating the entity and time embeddings, thusmaking the simple questions straightforward toanswer.
however, not relying on a scoring func-tion means that t-eae can be extended to anykg embedding, whereas cronkgqa cannot..666901@.
sti.h.1.
0.8.
0.6.
0.4.
0.2.
0.cronkgqa simplecronkgqa complext-eae-add simplet-eae-add complex.
20.
40.
60.
80.
100.train dataset size (%).
figure 2: model performance (hits@10) vs. trainingdataset size (percentage) for cronkgqa and t-eae-add.
solid line is for simple reasoning and dashedline is for complex reasoning type questions.
foreach dataset size, models were trained until validationhits@10 did not increase for 10 epochs.
please refer tosection 6.4 for details..2. another contributing reason could be thatthere are fewer parameters to be trained incronkgqa while a 6-layer transformer en-coder needs to be trained from scratch in t-eae.
transformers typically require large amounts ofvaried data to train successfully..6.3 performance across question types.
table 6 shows the performance of kg embeddingbased models across different types of reasoning.
as stated above in section 6.2, cronkgqa per-forms very well on simple reasoning questions(simple entity, simple time).
among complex ques-tion types, all models (except embedkgqa) per-form the best on time join questions (e.g., ‘whoplayed with roberto dinamite on the brazil na-tional football team’).
this is because such ques-tions typically have multiple answers (such as allthe players when roberto dinamite was playingfor brazil), which makes it easier for the model tomake a correct prediction.
in the other two ques-tion types, the answer is always a single entity/time.
before/after questions seem most challenging forall methods, with the best method achieving only0.288 hits@1..6.4 effect of training dataset size.
figure 2 shows the effect of training dataset size onmodel performance.
as we can see, for t-eae-add,.
increasing the training dataset size from 10% to100% steadily increases its performance for bothsimple and complex reasoning type questions.
thiseffect is somewhat present in cronkgqa forcomplex reasoning, but not so for simple reasoningtype questions.
we hypothesize that this is becauset-eae has more trainable parameters — it has a6-layer transformer that needs to be trained fromscratch — in contrast to cronkgqa that needsto merely ﬁne tune bert and train some shallowprojection layers.
these results afﬁrm our hypothe-sis that having a large, even if synthetic, dataset isuseful for training temporal reasoning models..6.5 temporal vs. non-temporal kg.
embeddings.
we conducted further experiments to study theeffect of temporal vs. non-temporal kg embed-dings.
we replaced the temporal entity embeddingsin t-eae-replace with complex embeddings, andtreated timestamps as regular tokens (not associ-ated with any entity/time mentions).
cronkgqa-cx is the same as embedkgqa.
the results canbe seen in table 7. as we can see, for bothcronkgqa and t-eae-replace, using temporalkge (tcomplex) gives a signiﬁcant boost in per-formance compared to non-temporal kge (com-plex).
cronkgqa receives a much larger boostin performance compared to t-eae-replace, proba-bly because the scoring function has been modeledafter tcomplex and not complex, while thereis no such embedding-speciﬁc engineering in t-eae-replace.
another observation is that ques-tions having temporal answers achieve very lowaccuracy (0.057 and 0.062 respectively) in bothcronkgqa-cx and t-eae-replace-cx, whichis much lower than what these models achieve withtcomplex.
this shows that having temporal kgembeddings is essential for achieving good perfor-mance for kg embedding-based methods..7 conclusion.
in this paper we introduce cronquestions, anew dataset for temporal knowledge graph ques-tion answering.
while there exist some temporalkgqa datasets, they are all based on non-temporalkgs (e.g., freebase) and have relatively few ques-tions.
our dataset consists of both a temporal kgas well as a large set of temporal questions requir-ing various structures of reasoning.
in order todevelop such a large dataset, we used a synthetic.
6670before/after0.1990.2560.2560.288.first/last0.3240.2850.2880.371.timejoin0.2230.1750.1680.511.simpleentity0.4210.2960.3180.988.simpletime0.0870.3210.3460.985.all.
0.2880.2780.2880.647.embedkgqat-eae-addt-eae-replacecronkgqa.
table 6: hits@1 for different reasoning type questions.
‘simple entity’ and ‘simple time’ correspond to simplequestion type in table 5 while the others correspond to complex question type.
please refer to section 6.3 for moredetails..questiontype.
simplecomplexentity answertime answeroverall.
cronkgqa t-eae-replacetcxcx0.3290.290.2570.2860.3180.4110.2310.0570.2880.288.tcx0.9870.3920.6990.5490.647.cx0.2480.2470.3470.0620.247.table 7: hits@1 for cronkgqa and t-eae-replaceusing complex(cx) and tcomplex(tcx) kg embed-dings.
please refer to section 6.5 for more details..generation procedure, leading to a question distri-bution that is artiﬁcial from a semantic perspective.
however, having a large dataset provides an op-portunity to train models, rather than just evaluatethem.
we experimentally show that increasing thetraining dataset size steadily improves the perfor-mance of certain methods on the tkgqa task..we ﬁrst apply large pre-trained lm based qamethods on our new dataset.
then we inject kgembeddings, both temporal and non-temporal, intothese lms and observe signiﬁcant improvementin performance.
we also propose a new method,cronkgqa, that is able to leverage temporalkg embeddings to perform tkgqa.
in our ex-periments, cronkgqa outperforms all baselines.
these results suggest that kg embeddings can beeffectively used to perform temporal kgqa, al-though there remains signiﬁcant scope for improve-ment when it comes to complex reasoning ques-tions..acknowledgements.
we would like to thank the anonymous reviewersfor their constructive feedback, and pat verga andwilliam cohen from google research for their in-sightful comments.
we would also like to thankchitrank gupta (iit bombay) for his help in de-bugging the source code and dataset.
this work issupported in part by a gift from google research,india and a jagadish bose fellowship..references.
junwei bao, nan duan, zhao yan, ming zhou, andtiejun zhao.
2016. constraint-based question an-in proceedings ofswering with knowledge graph.
coling 2016, the 26th international conferenceon computational linguistics: technical papers,pages 2503–2514, osaka, japan.
the coling 2016organizing committee..kurt bollacker, colin evans, praveen paritosh, timsturge, and jamie taylor.
2008. freebase: a collab-oratively created graph database for structuring hu-man knowledge.
in proceedings of the 2008 acmsigmod international conference on managementof data, sigmod ’08, page 1247–1250, new york,ny, usa.
association for computing machinery..antoine bordes, nicolas usunier, sumit chopra, andjason weston.
2015. large-scale simple questionanswering with memory networks.
arxiv preprintarxiv:1506.02075..antoine bordes, nicolas usunier, alberto garcia-duran,jason weston, and oksana yakhnenko.
2013. translating embeddings for modeling multi-in neural information processingrelational data.
systems (nips), pages 1–9..qingqing cai and alexander yates.
2013. large-scalesemantic parsing via schema matching and lexiconextension.
in proceedings of the 51st annual meet-ing of the association for computational linguis-tics (volume 1: long papers), pages 423–433, soﬁa,bulgaria.
association for computational linguis-tics..william w. cohen, haitian sun, r. alex hofer, andmatthew siegler.
2020. scalable neural methods forreasoning with a symbolic knowledge base..shib sankar dasgupta, swayambhu nath ray, andpartha talukdar.
2018. hyte: hyperplane-basedtemporally aware knowledge graph embedding.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2001–2011, brussels, belgium.
association forcomputational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training of deepbidirectional transformers for language understand-ing..6671thibault f´evry, livio baldini soares, nicholas fitzger-ald, eunsol choi, and tom kwiatkowski.
2020. en-tities as experts: sparse memory access with entitysupervision.
arxiv preprint arxiv:2004.07202..alberto garc´ıa-dur´an, sebastijan dumanˇci´c, andmathias niepert.
2018. learning sequence encodersfor temporal knowledge graph completion.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 4816–4821, brussels, belgium.
association for computa-tional linguistics..rishab goel, seyed mehran kazemi, marcus brubaker,and pascal poupart.
2019. diachronic embeddingfor temporal knowledge graph completion..j. edward hu, huda khayrallah, ryan culkin, patrickxia, tongfei chen, matt post, and benjaminvan durme.
2019.improved lexically constraineddecoding for translation and monolingual rewriting.
in proceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), minneapolis, minnesota.
association for computational linguistics..prachi jain, sushant rathi, mausam, and soumenchakrabarti.
2020. temporal knowledge base com-pletion: new algorithms and evaluation protocols.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 3733–3747, online.
association for computa-tional linguistics..zhen jia, abdalghani abujabal, rishiraj saha roy, jan-nik str¨otgen, and gerhard weikum.
2018a.
tem-pquestions: a benchmark for temporal question an-swering.
in companion proceedings of the the webconference 2018, www ’18, page 1057–1062, re-public and canton of geneva, che.
internationalworld wide web conferences steering committee..zhen jia, abdalghani abujabal, rishiraj saha roy, jan-nik str¨otgen, and gerhard weikum.
2018b.
tequila.
proceedings of the 27th acm international confer-ence on information and knowledge management..timoth´ee lacroix, guillaume obozinski, and nico-las usunier.
2020. tensor decompositions for tem-poral knowledge base completion.
arxiv preprintarxiv:2004.04926..timoth´ee lacroix, nicolas usunier, and guillaumeobozinski.
2018. canonical tensor decompositionarxiv preprintfor knowledge base completion.
arxiv:1806.07297..kalev leetaru and philip a schrodt.
2013. gdelt:global data on events, location, and tone, 1979–in isa annual convention, volume 2, pages2012.
1–49.
citeseer..patrick lewis, pontus stenetorp, and sebastian riedel.
2020. question and answer test-train overlap inopen-domain question answering datasets..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach..alexander miller, adam fisch, jesse dodge, amir-hossein karimi, antoine bordes, and jason weston.
2016. key-value memory networks for directly read-ing documents.
arxiv preprint arxiv:1606.03126..qiang ning, hao wu, rujun han, nanyun peng, mattgardner, and dan roth.
2020. torque: a readingcomprehension dataset of temporal ordering ques-tions..matthew e. peters, mark neumann, robert l. lo-gan iv au2, roy schwartz, vidur joshi, sameersingh, and noah a. smith.
2019. knowledge en-hanced contextual word representations..fabio petroni, tim rockt¨aschel, patrick lewis, antonbakhtin, yuxiang wu, alexander h. miller, and se-bastian riedel.
2019. language models as knowl-edge bases?.
colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former..pranav rajpurkar, jian zhang, konstantin lopyrev, andsquad: 100,000+ questionspercy liang.
2016.for machine comprehension of text.
arxiv preprintarxiv:1606.05250..daniel rufﬁnelli, samuel broscheit, and rainergemulla.
2020. you can teach an old dog newtricks!
on training knowledge graph embeddings.
in international conference on learning represen-tations..apoorv saxena, aditay tripathi, and partha taluk-dar.
2020. improving multi-hop question answeringover knowledge graphs using knowledge base em-beddings.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4498–4507, online.
association for computa-tional linguistics..haitian sun, andrew o. arnold, tania bedrax-weiss,fernando pereira, and william w. cohen.
2020.faithful embeddings for knowledge base queries..vladimir i levenshtein.
1966. binary codes capableof correcting deletions, insertions, and reversals.
insoviet physics doklady, volume 10 (8), pages 707–710. soviet union..haitian sun, tania bedrax-weiss, and william w. co-hen.
2019. pullnet: open domain question answer-ing with iterative retrieval on knowledge bases andtext..6672alon talmor and jonathan berant.
2018. the webas a knowledge-base for answering complex ques-tions.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 1 (long papers), pages 641–651, neworleans, louisiana.
association for computationallinguistics..partha pratim talukdar, derry wijaya, and tommitchell.
2012. coupled temporal scoping of rela-tional facts.
in proceedings of wsdm 2012..th´eo trouillon, johannes welbl, sebastian riedel, ´ericgaussier, and guillaume bouchard.
2016. complexembeddings for simple link prediction.
in interna-tional conference on machine learning (icml)..shikhar vashishth, soumya sanyal, vikram nitin,in-nilesh agrawal, and partha talukdar.
2020.teracte:improving convolution-based knowledgegraph embeddings by increasing feature interactions.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 34 (03), pages 3009–3016..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need..zhilin yang, peng qi, saizheng zhang, yoshua ben-gio, william w cohen, ruslan salakhutdinov, andchristopher d manning.
2018. hotpotqa: a datasetfor diverse, explainable multi-hop question answer-ing.
arxiv preprint arxiv:1809.09600..wen-tau yih, ming-wei chang, xiaodong he, andjianfeng gao.
2015. semantic parsing via stagedquery graph generation: question answering withknowledge base.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing (volume 1: longpapers), pages 1321–1331, beijing, china.
associa-tion for computational linguistics..yuyu zhang, hanjun dai, zornitsa kozareva, alexan-der j. smola, and le song.
2017. variational reason-ing for question answering with knowledge graph..a appendix.
a.1 entities as experts (eae).
the model architecture follows transformer(vaswani et al., 2017) interleaved with an entitymemory layer.
it has two embedding matrices, fortokens and entities.
it works on the input sequence.
x as follows.
x 0 = tokenembed(x)x 1 = transformer0(x 0, num layers = l0)x 2 = entitymemory(x 1)x 3 = layernorm(x 2 + x 1)x 4 = transformer1(x 3, num layers = l1)x 5 = taskspeciﬁcheads(x 4)the whole model (transformers, token and entityembeddings, and task-speciﬁc heads) is trained endto end using losses for entity linking, mention de-tection and masked language modeling..(6).
a.2 eae for temporal kgqa.
cronquestions does not provide a text cor-pus forthere-training language models.
fore, we use bert (devlin et al., 2019) fortransformer0 as well as tokenembed (eqn.
6).
for entitymemory, we use tcomplex/timeplexembeddings of entities and timestamps that havebeen pre-trained using the cronquestions kg(please refer to section 4 for details on kg embed-dings).
the modiﬁed model is as follows:.
x 1 = bert(x)x 2 = entitytimeembedding(x 1)x 3 = layernorm(x 2 + x 1)x 4 = transformer1(x 3, num layers = 6)x 5 = predictionhead(x 4).
(7).
for simplicity, we assume that all grounded entityand time mention spans are marked in the ques-tion, i.e., for each token, we know.
which entity ortimestamp it belongs to (or if it doesn’t belong toany).
thus, for each token xi in the input x,• x 1[i] contains the contextual bert embedding.
of xi.
• for x 2[i] there are 3 cases..– xi is a mention of entity e. then x 2[i] = e[e].
– xi is a mention of timestamp t. then x 2[i] =.
– xi is not a mention.
then x 2[i] is the zero.
t [t]..vector..predictionhead takes the ﬁnal outputfromtransformer1 of the token corresponding to the[cls] token of bert as the predicted answer em-bedding.
this answer embedding is scored againste.t using dot product to get a score for each possi-ble answer, and softmax is taken to get answerprobabilities.
the model is trained on the qadataset using cross-entropy loss.
we will refer.
6673to this model as t-eae-add since we are takingelement-wise sum of bert and entity/time embed-dings..t-eae-replace instead of adding entity/timeand bert embeddings, we replace the bert em-beddings with the entity/time embeddings for en-tity/time mentions.
speciﬁcally, before feeding totransformer1 in step 4 of eqn.
7,1. if xi is not an entity or time mention, x 3[i] =.
bert(x 1[i]).
2. if xi is an entity or time mention, x 3[i] =.
entitytimeembedding(x 1[i])the rest of the model remains the same..a.3 examples.
tables 8 to 12 contain some example questionsfrom the validation set of cronquestions, alongwith the top 5 predictions of the models we experi-mented with.
t5-3b has a single prediction sinceit is a text-to-text model..6674question.
who held the position of prime minister of sweden before 2nd world war.
question typegold answer(s) per albin hansson.
before/after.
bert.
knowbert.
t5-3bembedkgqat-eae-addt-eae-replacecronkgqa.
emil stang, sr., sigurd ibsen, johan nygaardsvold, laila freivalds, j. s. woodsworthbenito mussolini, ¨osten und´en, hans-dietrich genscher, winston churchill,lutz graf schwerin von krosigkbo osten undenper albin hansson, tage erlander, carl gustaf ekman, arvid lindman, hjalmar brantingper albin hansson, manuel roxas, arthur sauv´e, konstantinos demertzis, karl rennerper albin hansson, tage erlander, arvid lindman, val`ere bernard, vladko maˇcekper albin hansson, tage erlander, arvid lindman, carl gustaf ekman, hjalmar branting.
table 8: before/after reasoning type question..question.
when did man on wire receive oscar for best documentary feature.
question typegold answer(s).
simple time2008.bertknowbertt5-3bembedkgqat-eae-addt-eae-replacecronkgqa.
1995, 1993, 1999, 1991, 19871993, 1996, 1994, 2006, 199519972017, 2008, 2016, 2013, 20042008, 2009, 2005, 1999, 20072009, 2008, 2005, 2006, 20072008, 2007, 2009, 2002, 1945.table 9: simple reasoning question with time answer..question.
who did john alan lasseter work with while employed at pixar.
question typegold answer(s) floyd norman.
time join.
bert.
knowbertt5-3bembedkgqa.
t-eae-add.
t-eae-replace.
cronkgqa.
tim cook, eleanor winsor leach, david r. williams, robert m. boynton,jules steeg1994, 1997, walt disney animation studios, christiane kubrick, 1989john alan lasseterjohn lasseter, floyd norman, duncan marjoribanks, glen keane, theodore tyjohn lasseter, anne marie bardwell, will finn, floyd norman,rejean bourdagesjohn lasseter, will finn, floyd norman, nik ranieri, ken duncanjohn lasseter, floyd norman, duncan marjoribanks, david pruiksma,theodore ty.
table 10: time join type question..6675question.
where did john hubley work before working for industrial films.
question typegold answer(s) the walt disney studios.
before/after.
bert.
knowbert.
t5-3b.
embedkgqa.
t-eae-add.
t-eae-replace.
cronkgqa.
the walt disney studios, warner bros. cartoons, pixar, microsoft, united states navy´ecole polytechnique, piti´e-salpˆetri`ere hospital, the walt disney studios,elisabeth buddenbrook, yale universitylondon ﬁlm schoolthe walt disney studios, coll`ege de france, warner bros. cartoons,university of naples federico ii, eth zurichthe walt disney studios, fleischer studios, upa, walter lantz productions,wellesley collegethe walt disney studios, city college of new york, upa,yale university, indiana universitythe walt disney studios, upa, saint petersburg state university,warner bros. cartoons, coll`ege de france.
table 11: before/after reasoning type question..question.
the last person that naomi foner gyllenhaal was married to was.
question typegold answer(s) stephen gyllenhaal.
first/last.
bertknowbertt5-3b.
embedkgqa.
t-eae-add.
t-eae-replace.
cronkgqa.
1928, jennifer lash, stephen mallory, martin landau, bayerische verfassungsmedaille in goldnadia benois, eugenia zukerman, germany national football team, talulah riley, lola landaugyllenhaalstephen gyllenhaal, naomi foner gyllenhaal, wolfhard von boeselager,heinrich schweiger, bruce paltrowstephen gyllenhaal, marianne zoff, cotter smith, douglas wilder, gerd vespermannstephen gyllenhaal, hetty broedelet-henkes, naomi foner gyllenhaal,miles copeland, jr., member of the chamber of representatives of colombiastephen gyllenhaal, antonia fraser, bruce paltrow,naomi foner gyllenhaal, wolfhard von boeselager.
table 12: first/last reasoning type question..6676