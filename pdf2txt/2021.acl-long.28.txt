multimodal sentiment detection based on multi-channelgraph neural networks.
xiaocui yang, shi feng, yifei zhang, daling wangschool of computer science and engineering, northeastern university, chinayangxiaocui@stumail.neu.edu.cn,{fengshi, wangdaling, zhangyifei}@cse.neu.edu.cn.
abstract.
with the popularity of smartphones, we havewitnessed the rapid proliferation of multi-modal posts on various social media platforms.
we observe thatthe multimodal sentimentexpression has speciﬁc global characteristics,such as the interdependencies of objects orscenes within the image.
however, most pre-vious studies only considered the representa-tion of a single image-text post and failed tocapture the global co-occurrence characteris-tics of the dataset.
in this paper, we proposemulti-channel graph neural networks withsentiment-awareness (mgnns) for image-text sentiment detection.
speciﬁcally, we ﬁrstencode different modalities to capture hiddenrepresentations.
then, we introduce multi-channel graph neural networks to learn mul-timodal representations based on the globalcharacteristics of the dataset.
finally, we im-plement multimodal in-depth fusion with themulti-head attention mechanism to predict thesentiment of image-text pairs.
extensive exper-iments conducted on three publicly availabledatasets demonstrate the effectiveness of ourapproach for multimodal sentiment detection..1.introduction.
the tasks of extracting and analyzing sentimentsembedded in data have attracted substantial atten-tion from both academic and industrial communi-ties (zhang et al., 2018; yue et al., 2018).
withthe increased use of smartphones and the bloom ofsocial media such as twitter, tumblr and weibo,users can post multimodal tweets (e.g., text, im-age, and video) about diverse events and topics toconvey their feelings and emotions.
therefore, mul-timodal sentiment analysis has become a popularresearch topic in recent years (kaur and kautish,2019; soleymani et al., 2017).
as shown in fig.
1,sentiment is no longer expressed by a pure modalityin the multimodal scenario but rather by the com-.
(a) we have a fun day onthe beach!
(positive).
(b) we have a nice day ona deserted beach.
(posi-tive).
figure 1: multimodal posts with global characteristics.
two posts express the user’s positive sentiment frommultimodal data that has global characteristics, includ-ing the “have a fun/nice day” phrase, the ocean scene,and the beach scene..bined expressions of multiple modalities (e.g., text,image, etc.).
in contrast to unimodal data, multi-modal data consist of more information and makethe user’s expression more vivid and interesting..we focus on multimodal sentiment detection forimage-text pairs in social media posts.
the problemof image-text mismatch and ﬂaws in social mediadata, such as informality, typos, and a lack of punc-tuation, pose a fundamental challenge for the effec-tive representation of multimodal data for the senti-ment detection task.
to tackle this challenge, xu etal.
(2017; 2017) constructed different networks formultimodal sentiment analysis, such as a hierar-chical semantic attentional network (hsan) anda multimodal deep semantic network (mdsn).
xu et al.
(2018) and yang et al.
(2020) proposed aco-memory network (co-mem) and a multi-viewattentional network (mvan) models, respectively,introducing memory networks to realize the inter-action between modalities..the above methods treat each image-text postin the dataset as a single instance, and feature de-pendencies across instances are neglected or mod-eled implicitly.
in fact, social media posts havespeciﬁc global co-occurring characteristics, i.e., co-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages328–339august1–6,2021.©2021associationforcomputationallinguistics328occurring words, objects, or scenes, which tendto share similar sentiment orientations and emo-tions.
for example, the co-occurrences of thewords “have a fun/nice day” and of the brightscenes “ocean/beach” in the two images in fig.
1imply a strong relationship between these featuresand positive sentiment.
how to more effectivelymake use of the feature co-occurrences across in-stances and capture the global characteristics of thedata remain a great challenge..we propose a multi-channel graph neural net-works model with sentiment-awareness (mgnns)for multimodal sentiment analysis that consists ofthree stages..(i) feature extraction.
for text modality, weencode the text and obtain a text memory bank; forimage modality, we ﬁrst extract objects and scenesand then capture the image’ semantic features froma multiview perspective..(ii) feature representation.
we employ agraph neural network (gnn) for text modalitybased on the global shared matrices, i.e., one textgraph based on word co-occurrence is built basedon the whole dataset.
speciﬁcally, we ﬁrst connectword nodes within an appropriate small windowin the text.
after that, we update the node repre-sentation by itself as well as neighbor nodes.
forimage modality, it is believed that different viewsof an image, such as the beach (scene view) andperson (object view) in fig.
1(a), can reﬂect auser’s emotions (xu and mao, 2017).
the existingliterature usually models the relationship betweenthe scenes and objects within an image, failing tocapture the rich co-occurrence information fromthe perspective of the whole dataset.
in contrast,we explicitly build two graphs for scenes and ob-jects according to the co-occurrences in the datasetsand propose graph convolutional network (gcn)models over the two graphs to represent the images.
in general, to tackle the isolated feature problem,we build multiple graphs for different modalities,with each gnn acting as a channel, and proposea multi-channel graph neural networks (multi-gnn) module to capture the in-depth global char-acteristics of the data.
this multi-channel basedmethod can provide complementary representationfrom different sources (george and marcel, 2021;george et al., 2019; islam et al., 2019)..(iii) feature fusion.
previous studies usually di-rectly connect multimodal representations, withoutconsidering multimodal interactions (wang et al.,.
2020a; xu, 2017; xu and mao, 2017).
in this stage,we realize the pairwise interaction of text and im-age modalities from different channels through theuse of the multimodal multi-head attention in-teraction (mmai) module and obtain the fusionrepresentation..our main contributions are summarized as fol-.
lows:.
• we propose a novel mgnns framework thatmodels the global characteristics of the datasetto handle the multimodal sentiment detectiontask.
to the best of our knowledge, we arethe ﬁrst to apply gnn to the image-text mul-timodal sentiment detection task..• we construct the mmai module from differ-ent channels to realize in-depth multimodalinteraction..• we conduct extensive experiments on threepublicly available datasets, and the resultsshow that our model outperforms the state-of-the-art methods..2 related work.
2.1 multimodal sentiment analysis.
for convenience, multimodal polarity analysis andemotion analysis are uniﬁed to form multimodalsentiment analysis.
traditional machine learningmethods are adopted to address the multimodal sen-timent analysis task (p´erez-rosas et al., 2013; youet al., 2016).
recently, deep learning models havealso achieved promising results for this task.
forthe video dataset, wang et al.
(2020b) proposed anovel method, transmodality, to fuse multimodalfeatures with end-to-end translation models; zhanget al.
(2020) leveraged semi-supervised varia-tional autoencoders to mine more information fromunlabeled data; and hazarika et al.
(2020) con-structed a novel framework, misa, which projectseach modality to two distinct subspaces: modality-invariant and modality-speciﬁc subspaces.
thereis a massive amount image-text data on social plat-forms, and thus, image-text multimodal sentimentanalysis has attracted the attention of many re-searchers.
xu et al.
constructed different networksfor multimodal sentiment analysis—hsan (2017),mdsn (2017) and co-mem (2018).
yang et al.
(2020) built an image-text emotion dataset, namedtumemo, and further proposed mvan for multi-modal emotion analysis..3292.2 graph neural network.
the graph neural network has achieved promisingresults for text classiﬁcation, multi-label recogni-tion, and multimodal tasks.
for text classiﬁcation,a novel neural network called graph neural net-work (gnn), and its variants have been rapidlydeveloped, and their performance is better than thatof traditional methods, such as text gcn (yaoet al., 2019), tensorgcn (liu et al., 2020), andtextlevelgnn (huang et al., 2019).
the gcn isalso introduced in the multi-label image recogni-tion task to model the label dependencies (chenet al., 2019)..recently, graph convolutional network hasbeen applied in different multimodal tasks, such asvisual dialog (guo et al., 2020; khademi, 2020),multimodal fake news detection (wang et al.,2020a), and visual question answering (vqa)(hudson and manning, 2019; khademi, 2020).
jiang et al.
(2020) applied a novel knowledge-bridge graph network (kbgn) in modeling therelations among the visual dialogue cross-modalinformation in ﬁne granularity.
wang et al.
(2020a)proposed a novel knowledge-driven multimodalgraph convolutional network (kmgcn) to modelsemantic representations for fake news detection.
however, the kmgcn extracted visual words asvisual information and did not make full use of theglobal information of the image.
khademi (2020)introduced a new neural network architecture, amultimodal neural graph memory network (mn-gmn), for vqa, which model constructed a visualgraph network based on the bounding-boxes, whichproduced overlapping parts that might provide re-dundant information..for the image-text dataset, we found that certainwords often appear in a text post simultaneously,and different objects or scenes within an imagehave speciﬁc co-occurrences that indicate certainsentiments.
we explicitly model these global char-acteristics of the dataset through the use of a multi-channel gnn..3 proposed model.
fig.
2 illustrates the overall architecture of our pro-posed mgnns model for multimodal sentimentdetection that consists of three modules: the en-coding module, the multi-gnn module, and themultimodal interaction module.
we ﬁrst encodetext and image input into hidden representations.
then, we introduce gnn from different channels.
to learn multiple modal representations.
in thispaper, the channels are the text-gnn (tg) mod-ule, the image-gcn-scene (igs) module, and theimage-gcn-object (igo) module.
finally, werealize the in-depth interactions between differentmodalities by multimodal multi-head attention..3.1 problem formalization.
is the text modality and vi.
is to identify whichthe goal of our modelsentiment is expressed by an image-text post.
given a set of multimodal posts from socialmedia, p = {(t1, v1), ..., (tn , vn )}, wheretiis the corre-sponding visual information, n represents thenumber of posts.
we need to learn the modelf : p → l to classify each post (ti, vi) into thepredeﬁned categories li.
for polarity classiﬁca-tion, li ∈ {p ositive, n eutral, n egative};∈for{angry, bored, calm, f ear, happy, love,sad }..classiﬁcation,.
emotion.
li.
3.2 encoding.
for text modality, we ﬁrst encode words by glove(pennington et al., 2014) to obtain the embeddingvector and then obtain the text memory bank, m t,by bigru (cho et al., 2014):.
m t = fbigru (embedding(t )), m t ∈ rlt×2dt,(1)where t is a text sequence, lt is the maximumlength of a padded text sequence, and dt is thedimension of hidden units in the bigru..for image modality, we extract image featuresfrom both the object and scene views to capture suf-ﬁcient information.
we believe that there are inter-dependencies between different objects or scenes inan image.
to explicitly model this co-occurrence,we ﬁrst extract objects o = {o1, ..., olo} byyolov3 (farhadi and redmon, 2018), and ex-tract scenes s = {s1, ..., sls} by vgg-place (zhouet al., 2017).
finally, we obtain the object andscene memory banks with the pretrained resnet(he et al., 2016).
thus, if an input image v has a448×448 resolution and is split into 14×14 = 196visual blocks of the same size, then each block isrepresented by a 2,048-dimensional vector..m x = f x.resn et(v ), m x ∈ rlx×dx.
,.
(2).
where x ∈ {object, scene}, lx = 196, and dx =2, 048..330figure 2: the framework of the proposed multi-channel graph neural networks with sentiment-awareness(mgnns) for multimodal sentiment detection.
the channels are text-gnn (tg) for text modality, image-gcn-scene (igs) for image scene modality, and image-gcn-object (igo) for image object modality.
note that wedelete the stopwords during data preprocessing so that the words “a” and “the” do not have connections..3.3 multi-channel graph neural networks.
in this subsection, we present our proposed multi-gnn module.
as fig.
2 shows, this module con-sists of the tg channel (middle), the igo channel(right), and the igs channel (left)..text gnn: as shown in the middle of fig.
2,motivated by (huang et al., 2019), we learn textrepresentation through the text level gnn.
fortext with lt words t = {w1, ..., wk, ..., wlt}, wherethe kth word, wk, is initialized by glove embeddingk ∈ rd, d = 300. we build the graph of the text-rtbased vocabulary of the training dataset, which isdeﬁned as follows:.
n t = {wk|k ∈ [1, lt]}..(3).
we build edges between wk and wj when the num-ber of co-occurrences of two words is not less than2..et = {et.
k,j|wk ∈ [w1, wlt]; wj ∈ [wk−ws, wk+ws]},.
(4)where n t and et are the set of nodes and edges ofthe text graph, respectively.
the word representa-tions in n t and the edge weights in et are takenfrom global shared matrices built based on vocab-ulary and the edge set of the dataset, respectively.
that is, the representations of the same nodes andweights of the edges are shared globally.
etk,j is.
initialized by point-wise mutual information (pmi)(wang et al., 2020a) and is learned in the trainingprocess.
ws is the hyperparameter sliding windowsize, which indicates how many adjacent nodes areconnected to each word in the text graph..then, we update the node representation basedon its original representations and neighboringnodes by the message passing mechanism (mpm)(gilmer et al., 2017), which is deﬁned as follows:.
at.
k = maxj∈n ws.
k.kjrtetk,.
(5).
(cid:48).
(6).
rtk.= αrt.
k + (1 − α)atk,k ∈ rd is the aggregated information fromwhere atneighboring nodes from node k−ws to k+ws, andmax is the reduction function.
α is the trainablevariable that indicates how much original informa-∈ rd istion of the node should be kept, and rtkthe updated representation of node k..(cid:48).
finally, we can calculate the new representation.
of text t as follows:.
(cid:48).
t.=.
lt(cid:88).
k=1.
(cid:48).
rtk.(7).
image gcn: in this module, we explicitlymodel interdependence within lx scenes or objectsby igx, as shown on the left and right sides of fig..331we have a fun day on the beach!glove embeddingwehavefundaybeachtheonaoceanhappythereanlifetext_gcnbi-grutext memory bankscene_resnetextractscenesbridgebeachoceancoastcastleimage-scene memory bank…𝐶!"#$#=365𝐷!
"#$#×sentiment-awareness scene featurek-vemotion-aware multihead-attentionqsentiment embedding matrixextractobjectsobject_resnetpersonbuildingboatbuscastleimage-object memory bank…𝐶%&’#"(=80𝐷%&’#"(×emotion-aware multihead-attentionk-vqsentimentembedding matrixmultimodal multi-head attention interactionemotion label predicting ×𝑳𝒐text feature×𝑳𝒔image-gcn-scene (igs)text-gnn (tg) sentiment-awareness scene featureimage-gcn-object (igo)………………2, respectively.
the graph of the image is deﬁnedas follows:.
n x = {xp|p ∈ [1, lx]},.
(8).
where n x ∈ rcxis the set of nodes of igx;x or x ∈ {object, scene}, cx = 80 whenx = object, and cx = 365 when x = scene..to build the edges of igx, we ﬁrst build theglobal shared co-occurrence matrix-based dataset:.
ex = {ex.
p,q|p ∈ [1, lx] , q ∈ [1, lx]},.
(9).
where ex ∈ rcx×cxedge weight exof xp and xq in the dataset..is the co-occurrence matrix;p,q indicates the co-occurrence times.
then, we calculate the conditional probability.
for node p as follows:.
p,q = exp x.p,q/n x.p , when q (cid:54)= p.(10).
where n xthe dataset.
note that p x.p denotes the occurrence times of xp inp,q (cid:54)= p x.q,p..as mentioned by (chen et al., 2019), the simplecorrelation above may suffer several drawbacks.
we further build the binary co-occurrence matrix:.
bx.
p,q =.
(cid:40).
1, if p x0, if p x.p,q ≥ βp,q ≤ β.,.
(11).
where β is the hyperparameter used to ﬁlter noisyedges..it is obvious that the role of the central node isdifferent from that of neighboring nodes, so weneed to further calculate the weight of the edge:.
(cid:40).
rx.
p,q =.
1 − γ, if p = qγ/(cid:80)cxq=1bx.
p,q, if p (cid:54)= q.,.
(12).
where rx ∈ rcx×cxis the weighted co-occurrence matrix, and hyperparameter γ indicatesthe importance of neighboring nodes..finally, we input node n x and edge rx of theimage into the graph convolutional network.
likein (kipf and welling, 2016), every layer can becalculated as follows:.
h x.l+1 = h( (cid:99)rxh x.
(13).
lw xl),l+1 ∈ rcx×dx(cid:48).
, h x.l ∈ rcx×dx, and (cid:99)rx ∈ rcx×cx.
where h xl ∈rdx×dx(cid:48)is the normalizedrepresentation of rx; h(·) is a non-linear operation.
when l = 1, h x1 is the word-embedding vector ofn x.., w x.figure 3: the mmai module illustrates the processof multimodal interaction from four channels, x ∈{object, scene}.
we take the interaction process be-tween text and image scene channels as an example todemonstrate this for convenience.
the dotted arrowsare the outputs of the other two channels after the inter-actions..by stacking multiple gcn layers, we can explic-itly learn and model the complex interdependenceof the nodes.
then, we obtain the image represen-tation with objects or scenes dependencies:.
l+1)t, i x ∈ rcx.
i x = m axp ooling(m x)(h x..
(14)but, we cannot capture the relationship betweennodes and sentiments.
therefore, we learn thesentiment-awareness image representation throughmulti-head attention (vaswani et al., 2017)..att = sof tmax( qktdk.
√.
)v,.
(15).
ei x = m h(q, k, v ).
h , v w v.= concat(head1, ..., headh )w owhere headh = att(qw qh , kw k.h ),(16)where m h(·) is multi-head attention; w qh ∈h ∈ rdmodel×dv ,h ∈ rdmodel×dk , w vrd×dk , w kand w o ∈ rhdv×d; and h = 5, dmodel =300, dk = dv = 60. q ∈ rls×d is a senti-ment embedding matrix built based on the labelset ls = 3 for polarity classiﬁcation and ls = 7 foremotion classiﬁcation; k = v = i xw i , w i ∈rcx×dmodel, k, v ∈ rdmodel..3.4 multimodal interaction.
motivated by the transformer (vaswani et al.,2017) prototype, we design a multimodal multi-head attention interaction (mmai) module thatcan effectively learn the interaction between text.
332k-vtext-guided image scene attentionqtext featureimage-x memory bankadd & normfeed forwardadd & normtext memory bankk-vimage scene -guided text attentionadd & normfeed forwardadd & normqsentiment-awareness x featurefused feature𝑁!"#𝑁#"!
modality and image modality by multiple channels,as shown in fig.
3..we employ the mmai to obtain the text guidedimage-x representations and image-x guided textrepresentations, x ∈ {object, scene}.
for thetext-guided image-x attention,.
n +1 = ln (m h(q = h t gxot gx.
n , k = v = m x).
+ h t gx.
n ),.
h t gx.
n +1 = ln (f f n (ot gx.
n +1) + ot gx.
n +1),.
where ln (·) is layer normalization, and f f n (·)is the feed-forward network.
when n = 1,h t gx1for the image-x-guided text attention,.
= t (cid:48), as in eq.
7..n +1 = ln (m h(q = h xgtoxgt.
n , k = v = m t).
+ h xgt.
n ),.
(17).
(18).
(19).
(20).
h xgt.
n +1 = ln (f f n (oxgt.
n +1) + oxgt.
n +1),.
1.when n = 1, h xgt= ei x, as in eq.
16. form h, h = 4, dmodel = 512, dk = dv = 128.the fused multimodal representation is as follows:rm = [h t gon ⊕ h ogtn ⊕ h t gsn ], where⊕ is a concatenation operation..n ⊕ h sgt.
3.5 sentiment detection.
finally, we feed the above fused representation,rm, into the top fully connected layer and employthe softmax function for sentiment detection..lm = sof tmax(wsrm + bs), lm ∈ rls.
,.
(21).
where ws and bs are the parameters of the fullyconnected layer..4 experiments.
we conduct experiments on three multimodal senti-ment datasets from social media platforms, mvsa-single, mvsa-multiple (niu et al., 2016), andtumemo (yang et al., 2020), and compare ourmgnns model with a number of unimodal andmultimodal approaches..4.1 datasets.
mvsa-single and mvsa-multiple are two dif-ferent scale image-text sentiment datasets crawledfrom twitter1.
tumemo is a multimodal weak-supervision emotion dataset containing a large.
dataset.
train.
val.
test.
all.
mvsa-s3,608mvsa-m 13,618tumemo156,204.
4,5114514521,703 1,70317,02419,525 19,536 195,265.table 1: statistics of the different datasets..amount of image-text data crawled from tumblr2.
the statistics of these datasets are given in ap-pendix a; and for a fair comparison, we adopt thesame data preprocessing method as that of yang(yang et al., 2020).
the corresponding details areshown in appendix b..4.2 experimental setup.
parameter.
mvsa-∗.
tumemo.
learning ratewsobject-βscene-βγlxn t gxn xgt.
4e − 540.40.30.2211.
5e − 550.40.50.2211.table 2: parameter settings of the different datasets..we adopt the cross-entropy loss function andadam optimizer.
in the process of extracting ob-jects and scenes, we reserve the objects with theprobability greater than 0.5 and the top-5 scenes,respectively.
the other parameters are listed in ta-ble 2, ∗ ∈ {single, m ultiple}.
we use accuracy(acc) and f1-score (f1) as evaluation metrics.
allmodels are implemented with pytorch..4.3 baselines.
we compare our model with multimodal sentimentmodels with the same modalities and the unimodalbaseline models..unimodal baselines: for text modality, cnn(kim, 2014) and bi-lstm (zhou et al., 2016) arewell-known models for text classiﬁcation tasks, andbiacnn (lai et al., 2015) incorporates the cnnand bilstm models with an attention mechanismfor text sentiment analysis.
tgnn (huang et al.,2019) is a text-level graph neural network for textclassiﬁcation.
for image modality, osda (yang.
1https://twitter.com.
2http://tumblr.com.
333modality model.
mvsa-singlef1acc.
mvsa-multiplef1acc.
tumemo.
acc.
f1.
text.
image.
image-text.
cnnbilstmbiacnntgnn.
osdasgnognduig.
hsanmdsnco-memmvan‡mgnns.
0.68190.70120.70360.7034.
0.66750.66200.66590.6822.
0.69880.69840.70510.7298‡0.7377.
0.55900.65060.69160.6594.
0.66510.62480.61910.6538.
0.66900.69630.70010.7139‡0.7270.
0.65640.67900.68470.6967.
0.66620.67650.67430.6819.
0.67960.68860.69920.7183‡0.7249.
0.57660.67900.63190.6180.
0.66230.58640.60100.6081.
0.67760.68110.69830.7038‡0.6934.
0.61540.61880.62120.6379.
0.47700.43530.45640.4636.
0.63090.64180.64260.6553‡0.6672.
0.47740.51260.50160.6362.
0.34380.42320.44460.4561.
0.53980.56920.59090.6543‡0.6669.table 3: experiment results of acc and f1 on three datasets.
‡ represents the reproductive operation..et al., 2020) is an image sentiment analysis modelbased on multiple views.
note that the sgn, ogn,and duig are variants of our model and rely onlyon image modality.
sgn and ogn are the im-age graph convolutional neural networks based onscenes and objects for image sentiment analysis, re-spectively.
duig is the image graph convolutionalneural network with dual views, e.g., object andscene..muiltimodal baselines: hsan (xu, 2017) is ahierarchical semantic attentional network based onimage captions for multimodal sentiment analysis.
mdsn (xu and mao, 2017) is a deep semantic net-work with attention for multimodal sentiment anal-ysis.
co-mem (xu et al., 2018) is a co-memorynetwork for iteratively modeling the interactionsbetween multiple modalities.
mvan (yang et al.,2020) is a multi-view attentional network that uti-lizes a memory network for multimodal emotionanalysis.
this model achieves state-of-the-art per-formance on image-text multimodal sentiment clas-siﬁcation tasks..4.4 experimental results and analysis.
the experimental results of the baseline meth-ods and our model are shown in table 3, wheremgnns denotes that our model is based on multi-channel graph neural networks3..we can make the following observations.
first,.
3the source codes are available for use at https://.
github.com/yangxiaocui1215/mgnns..our model (mgnns) is competitive with the otherstrong baseline models on the three datasets.
notethat the data distribution of mvsa-∗ is extremelyunbalanced.
thus, we reproduce the mvan modelwith acc and weighted-f1 metrics instead of themicro-f1 metric used in the original paper, whichis more realistic.
second, the multimodal senti-ment analysis models perform better than mostof the unimodal sentiment analysis models on allthree datasets.
moreover, the segmental indictorsare difﬁcult to capture for images owing to the lowinformation density, and the sentiment analysis onthe image modality achieves the worst results.
fi-nally, the tgnn unimodal model outperforms thehsan multimodal model, indicating that the gnnhas excellent performance in sentiment analysis..4.5 ablation experiments.
we conduct ablation experiments on the mgnnsmodel to demonstrate the effectiveness of differentmodules.
table 4 shows that the whole mgnnsmodel achieves the best performance among allmodels.
to show the performance of the multi-gnn module, we replace the text-gnn with thecnn, as well as the image-gcn with the pre-trained resnet.
the removal of the mmai mod-ule (w/o mmai) and multi-gnn module (w/omgnn) adversely affect the model results, whichindicates that these modules are useful for multi-modal sentiment analysis.
by replacing the mmaimodule with the coatt (lu et al., 2016) module.
334datasets.
model.
acc.
f1.
mvsa-single.
mvsa-multiple.
tumemo.
w/o mgnnw/o mmai+coattw/o scenew/o objectmgnns.
w/o mgnnw/o mmai+coattw/o scenew/o objectmgnns.
w/o mgnnw/o mmai+coattw/o scenew/o objectmgnns.
0.7010 0.68470.7108 0.68790.7255 0.69860.7304 0.69880.7034 0.69000.7377 0.7270.
0.7019 0.67520.7128 0.67920.7210 0.68490.7170 0.67970.7110 0.68480.7249 0.6934.
0.6553 0.65470.6370 0.63470.6624 0.66060.6618 0.65930.6592 0.65840.6672 0.6669.table 4: ablation experiment results..(+coatt), the model performance is found to beslightly worse than that of the mgnns module.
this further illustrates the importance of multi-modal interactions and the superiority of the mmaimodule.
when one of the object views (w/o ob-ject) or scene views (w/o scene) is removed, theperformance of the model declines, which indicatesthat both views of the image are effective for multi-modal sentiment analysis..4.6 transferability experiment.
in the multi-gnn module, we build multiplegraphs for different modalities based on the dataset.
for different datasets, the graphs built by the uni-modal model are different.
however, can graphcapture from one dataset (e.g., mvsa-single) havepositive effects on other datasets (e.g., tumemo)?
in this subsection, we will verify the transferabilityof the model through experiments..as table 5 shows, the following conclusions canbe drawn: (i) regardless of the modality, such astext or image, compared to introducing the graphconstructed based on own dataset, the experimen-tal results calculated based on graphs transferredfrom other datasets are worse.
this is mainly be-cause each dataset has unique global characteris-tics, the experimental results based on transferredgraphs are slightly worse.
(ii) however, due to.
the commonality of datasets when expressing thesame emotions, the results of the transferred mod-els are not completely worse.
for example, thesame scenes and objects can appear in differentimages in different datasets simultaneously for im-age modalities.
therefore, graphs from differentdatasets have transferability and can be used forother datasets.
(iii) for different datasets, the exper-imental results of “x2y-text” are worse than thoseof “x2y-image”.
that is, the text graph has worsetransferability.
the reason for this may be that textgraphs with various nodes are created based on thevocabulary of different datasets.
two situationsin the transferred text graph will seriously affectthe results: fewer nodes will lose information, andmore nodes will provide redundant information.
(iv) when the dataset gap is relatively wide, thetransferability of text graphs is worse.
for exam-ple, from the larger datasets transfer to the smallestdataset, including t2s-text and m2s-text, exper-imental results show a drop of 2.45% and 2.69%,respectively; from the smaller datasets transfer tothe most largest dataset, including s2t-text andm2t-text, experimental results show a signiﬁcantdrop of 4.81% and 4.09%, respectively..4.7 hyperparameter settings.
hyperparameter ws: to obtain adequate infor-mation from neighboring nodes in the tgnn, weconduct experiments under different settings forhyperparameter ws in eq.
4, the related results ofwhich are shown in fig.
4. the best ws selectionvaries among different datasets since the averagetext length of tumemo is longer compared to otherdata.
the tgnn cannot obtain sufﬁcient informa-tion from neighboring nodes with ws values thatare too small, while larger values may degrade theperformance due to the redundant information pro-vided by neighboring nodes..(a) comparisons on mvsa-∗ (b) comparisons on tumemo.
figure 4: acc comparisons with different values of ws.
ms is mvsa-single, mm is mvsa-multiple, and tis tumemo..335      : 6                               $ f f 0 6 0 0       : 6                               $ f f 7model.
mvsa-singleacc.
f1.
model.
mvsa-multiple.
acc.
f1.
model.
tumemof1.
acc.
m2s-textt2s-textm2s-imaget2s-imagemgnns.
0.69850.69390.69010.7027.
0.71320.71080.72060.72550.7377 0.7270 mgnns.
s2m-textt2m-texts2m-imaget2m-image.
0.71460.71100.71770.71830.7249.
0.6912s2t-text0.6752m2t-texts2t-image0.67950.6848 m2t-image0.6934.mgnns.
0.6191 0.62020.6263 0.62390.6635 0.66110.6625 0.66150.6672 0.6669.table 5: transferability experiment results of acc and f1 on different datasets.
s, m and t denote mvsa-single,mvsa-multiple, and tumemo, respectively.
for “z” modality, “x2y-z” represents that the graph that is builtbased on the “x” dataset is transfered to the “y” dataset, where z ∈ {text, image}, x ∈ {mvsa-single, mvsa-multiple, tumemo}, and y ∈ {mvsa-single, mvsa-multiple, tumemo}.
for example, “m2s-text” representsthat the text graph that is built based on the mvsa-multiple dataset is transferred to the mvsa-single dataset..lized..hyperparameter β: we vary the values ofhyperparameter β in eq.
11 for the binary co-occurrence matrix from different views, the resultsof which are shown in fig.
5. we ﬁnd that thebest β value is different for different views in dif-ferent datasets.
for mvsa-∗, the smaller β valuecan reserve more edges to capture more informa-tion since the scene co-occurrence matrix is sparserthan that in the object view.
for tumemo with alarge amount of data, preserving the top-5 scenesproduces many noise edges, so the value of scene-βis greater than that of mvsa-∗..(a) comparisons of cbjectview on mvsa-∗.
(b) comparisons ofview on mvsa-∗.
scene.
(a) comparisons on mvsa-∗ (b) comparisons on tumemo.
figure 6: acc comparisons with different γ values..5 conclusions.
this paper proposes a novel model, mgnns, thatis built based on the global characteristics of thedataset for multimodal sentiment detection tasks.
as far as we know, this is the ﬁrst application ofgraph neural networks in image-text multimodalsentiment analysis.
the experimental results onpublicly available datasets demonstrated that ourproposed model is competitive with strong baselinemodels..in future work, we plan to construct a modelthat adopts the advantages of the gnn and pre-trained models such as bert, visualbert, andetc.
we want to design a reasonable algorithm tocharacterize the quality of the objects and scenesselected from the image and further improve therepresentation ability of the model..(c) comparisons of objectview on tumemo.
(d) comparisons ofview on tumemo.
scene.
figure 5: acc comparisons with different β values..acknowledgments.
hyperparameter γ: as fig.
6 shows, themodel receives the best performance for the threedatasets when γ is 0.2. when γ is smaller, theneighboring nodes do not receive enough attention;in contrast, their own information is not fully uti-.
the project is supported by the national key r&dprogram of china (2018yfb1004700) and by thenational natural science foundation of china(61772122, 61872074, u1811261)..336                2 e m h f w β                               $ f f 0 6  2 0 0  2                6 f h q h β                               $ f f 0 6  6 0 0  6                   2 e m h f w β                               $ f f 7  2                   6 f h q h β                               $ f f 7  6                  γ                               $ f f 0 6 0 0                  γ                               $ f f 7references.
zhao-min chen, xiu-shen wei, peng wang, and yan-wen guo.
2019. multi-label image recognition withgraph convolutional networks.
in proceedings of theieee conference on computer vision and patternrecognition, pages 5177–5186..kyunghyun cho, bart van merrienboer, caglar gul-cehre, dzmitry bahdanau, fethi bougares, holgerlearningschwenk, and yoshua bengio.
2014.phrase representations using rnn encoder–decoderfor statistical machine translation.
in proceedings ofthe 2014 conference on empirical methods in nat-ural language processing (emnlp), pages 1724–1734..ali farhadi and joseph redmon.
2018. yolov3: an in-cremental improvement.
computer vision and pat-tern recognition, cite as..anjith george and sebastien marcel.
2021. learningone class representations for face presentation attackdetection using multi-channel convolutional neuralnetworks.
ieee transactions on information foren-sics and security, 16:361–375..anjith george, zohreh mostaani, david geissenbuhler,olegs nikisins, andr´e anjos, and s´ebastien mar-cel.
2019. biometric face presentation attack de-tection with multi-channel convolutional neural net-work.
ieee transactions on information forensicsand security, 15:42–55..justin gilmer, samuel s. schoenholz, patrick f. riley,oriol vinyals, and george e. dahl.
2017. neuralin pro-message passing for quantum chemistry.
ceedings of the 34th international conference onmachine learning - volume 70, pages 1263–1272..dan guo, hui wang, hanwang zhang, zheng-juniterative context-zha, and meng wang.
2020.in 2020aware graph inference for visual dialog.
ieee/cvf conference on computer vision and pat-tern recognition (cvpr), pages 10055–10064..devamanyu hazarika, roger zimmermann, and sou-janya poria.
2020. misa: modality-invariant and-speciﬁc representations for multimodal sentimentanalysis.
in proceedings of the 28th acm interna-tional conference on multimedia, pages 1122–1131..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in proceedings of the ieee conference onnition.
computer vision and pattern recognition, pages 770–778..lianzhe huang, dehong ma, sujian li, xiaodongzhang, and houfeng wang.
2019. text level graphin proceed-neural network for text classiﬁcation.
ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 3442–3448..drew a. hudson and christopher d. manning.
2019.gqa: a new dataset for real-world visual reason-ing and compositional question answering.
in 2019ieee/cvf conference on computer vision and pat-tern recognition (cvpr), pages 6700–6709..jumayel islam, robert e mercer, and lu xiao.
2019.multi-channel convolutional neural network for twit-ter emotion and sentiment recognition.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 1355–1365..xiaoze jiang, siyi du, zengchang qin, yajing sun,and jing yu.
2020. kbgn: knowledge-bridge graphnetwork for adaptive vision-text reasoning in visualdialogue.
in proceedings of the 28th acm interna-tional conference on multimedia, pages 1265–1273..ramandeep kaur and sandeep kautish.
2019. mul-timodal sentiment analysis: a survey and compari-son.
international journal of service science, man-agement, engineering, and technology (ijssmet),10(2):38–58..mahmoud khademi.
2020. multimodal neural graphmemory networks for visual question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 7177–7188..yoon kim.
2014. convolutional neural networks forsentence classiﬁcation.
in proceedings of the 2014conference on empirical methods in natural lan-guage processing (emnlp), pages 1746–1751..thomas n. kipf and max welling.
2016..semi-supervised classiﬁcation with graph convolutionalnetworks.
in iclr (poster)..siwei lai, liheng xu, kang liu, and jun zhao.
2015.recurrent convolutional neural networks for textclassiﬁcation.
in twenty-ninth aaai conference onartiﬁcial intelligence..xien liu, xinxin you, xiao zhang, ji wu, and pinglv.
2020. tensor graph convolutional networks fortext classiﬁcation.
in proceedings of the aaai con-ference on artiﬁcial intelligence, volume 34, pages8409–8416..jiasen lu, jianwei yang, dhruv batra, and devi parikh.
2016. hierarchical question-image co-attention forin advances in neuralvisual question answering.
information processing systems, volume 29, pages289–297..teng niu, shiai zhu, lei pang, and abdulmotalebel saddik.
2016. sentiment analysis on multi-viewin international conference on multi-social data.
media modeling, pages 15–27.
springer..jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for word rep-resentation.
in proceedings of the 2014 conference.
337on empirical methods in natural language process-ing (emnlp), pages 1532–1543..ver´onica p´erez-rosas, rada mihalcea, and louis-philippe morency.
2013. utterance-level multi-in proceedings of themodal sentiment analysis.
51st annual meeting of the association for compu-tational linguistics, pages 973–982..mohammad soleymani, david garcia, brendan jou,bj¨orn schuller, shih-fu chang, and maja pantic.
2017. a survey of multimodal sentiment analysis.
image and vision computing, 65:3–14..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in proceedings of the 31st internationalconference on neural information processing sys-tems, volume 30, pages 5998–6008..youze wang, shengsheng qian, jun hu, quan fang,and changsheng xu.
2020a.
fake news detectionvia knowledge-driven multimodal graph convolu-in proceedings of the 2020 inter-tional networks.
national conference on multimedia retrieval, pages540–547..zilong wang, zhaohong wan, and xiaojun wan.
2020b.
transmodality: an end2end fusion methodwith transformer for multimodal sentiment analysis.
in proceedings of the web conference 2020, pages2514–2520..nan xu.
2017. analyzing multimodal public sentimentbased on hierarchical semantic attentional network.
in 2017 ieee international conference on intelli-gence and security informatics (isi), pages 152–154.
ieee..nan xu and wenji mao.
2017. multisentinet: a deepsemantic network for multimodal sentiment analy-sis.
in proceedings of the 2017 acm on conferenceon information and knowledge management, pages2399–2402.
acm..nan xu, wenji mao, and guandan chen.
2018. a co-memory network for multimodal sentiment analysis.
in the 41st international acm sigir conferenceon research & development in information retrieval,pages 929–932..xiaocui yang, shi feng, daling wang, and yifeizhang.
2020. image-text multimodal emotion clas-siﬁcation via multi-view attentional network.
ieeetransactions on multimedia..liang yao, chengsheng mao, and yuan luo.
2019.graph convolutional networks for text classiﬁcation.
in proceedings of the aaai conference on artiﬁcialintelligence, volume 33, pages 7370–7377..quanzeng you, jiebo luo, hailin jin, and jianchaoyang.
2016. cross-modality consistent regressionfor joint visual-textual sentiment analysis of social.
multimedia.
in proceedings of the ninth acm inter-national conference on web search and data mining,pages 13–22.
acm..lin yue, weitong chen, xue li, wanli zuo, and ming-hao yin.
2018. a survey of sentiment analysis insocial media.
knowledge and information systems,pages 1–47..dong zhang, shoushan li, qiaoming zhu, andguodong zhou.
2020. multi-modal sentiment clas-siﬁcation with independent and interactive knowl-ieee access,edge via semi-supervised learning.
8:22945–22954..lei zhang, shuai wang, and bing liu.
2018. deeplearning for sentiment analysis: a survey.
wileyinterdisciplinary reviews: data mining and knowl-edge discovery, 8(4):e1253..bolei zhou, agata lapedriza, aditya khosla, audeoliva, and antonio torralba.
2017. places: a 10million image database for scene recognition.
ieeetransactions on pattern analysis and machine intelli-gence, 40(6):1452–1464..peng zhou, wei shi, jun tian, zhenyu qi, bingchen li,hongwei hao, and bo xu.
2016. attention-basedbidirectional long short-term memory networks forin proceedings of the 54threlation classiﬁcation.
annual meeting of the association for computationallinguistics (volume 2: short papers), pages 207–212..a dataset.
a.1 mvsa-single and mvsa-multiple.
the statistics for the mvsa-simple and mvsa-multiple datasets are listed in table 1, showingthat the various categories are highly unbalanced.
mvsa-single and mvsa-multiple have differentdata distributions..dataset sentiment train val.
test all.
mvsa-simple.
mvsa-multiple.
positiveneutralnegativeall.
positiveneutralnegativeall.
2,146 268376471,086 1363,608 451.
26947136452.
2,6834701,3584,511.
9,054 1,132 1,132 11,3184,4083,526 4411,038 1301,29813,618 1,703 1,703 17,024.
441130.table 6: number of instances for each sentiment onthe mvsa-∗ dataset..338emotion train.
val.
test.
all.
angryboredcalmfearfulhappylovingsadall.
1,45411,6353,22825,8261,81114,4872,02616,2115,02740,2143,45127,6092,52820,222156,204 19,525.
1,4553,2291,8112,0275,0263,4512,52719,536.
14,54432,28318,10920,26450,26734,51125,277195,265.table 7: number of instances of each emotion on thetumemo dataset..a.2 tumemo.
the statistics for the tumemo dataset are listed intable 2, containing a large number of image-textposts labeled by emotion..b preprocessing data.
the text data contain many useless characters forsentiment analysis, such as urls, stopwords, andpunctuation.
we need to preprocess text data toenhance the effectiveness of multimodal emotiondetection.
we perform data preprocessing as fol-lows:.
• remove the “url”, as in“http://...”;.
• remove the stopwords, such as “a, an, the, and.
etc.
”;.
• remove the useless punctuation, including pe-.
riods, commas, semicolons, etc;.
• remove the hashtag and its content (#content);in particular, the tumemo dataset uses #emo-tion as a weakly supervised label..• remove the posts for which the text length is.
less than 3..339