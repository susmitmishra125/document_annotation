a uniﬁed approach to sentence segmentationof punctuated text in many languages.
rachel wicks1 and matt post1,21center for language and speech processing2human language technology center of excellencejohns hopkins universityrewicks@jhu.edu, post@cs.jhu.edu.
abstract.
examples of ambiguity in punctuated contexts.
the sentence is a fundamental unit of text pro-cessing.
yet sentences in the wild are com-monly encountered not in isolation, but un-segmented within larger paragraphs and doc-uments.
therefore, the ﬁrst step in many nlppipelines is sentence segmentation.
despiteits importance, this step is the subject of rel-atively little research.
there are no standardtest sets or even methods for evaluation, leav-ing researchers and engineers without a clearfooting for evaluating and selecting models forthe task.
existing tools have relatively smalllanguage coverage, and efforts to extend themto other languages are often ad hoc..we introduce a modern context-based mod-eling approach that provides a solution tothe problem of segmenting punctuated textin many languages, and show how it can betrained on noisily-annotated data.
we also es-tablish a new 23-language multilingual eval-uation set.
our approach exceeds high base-lines set by existing methods on prior englishcorpora (wsj and brown corpora), and alsoperforms well on average on our new evalua-tion set.
we release our tool, ersatz, as opensource..1.introduction.
in many ways, the sentence is the fundamentalunit of text in natural language processing (nlp).
from the user perspective, tasks such as sentimentanalysis, pos tagging, or machine translation con-sume sentences and emit classiﬁcations, annota-tions, or transductions of those inputs.
even tasksthat operate at the paragraph or document level,such as coreference resolution or summarization,often make use of sentences internally.
yet atthe same time, sentences in the wild rarely ex-ist with marked sentence boundaries.
for manylanguages, punctuation serves as a cue for these.
en.
cs.
ro.
... in the u.s. ⊗ house of representatives ......in the u.s. (cid:88) most mexican spanish ....... podnikanie s.r.o.
⊗ a hlavním investorem... a systémy s.r.o.
(cid:88) v roce 2017 ....... w. pauli s, .a.
⊗ constituie direc¸tii ...... de robles s, .a.
(cid:88) a jucat în ....table 1: examples of ambiguous full stop punctu-ation in english, czech, and romanian from wikipedia.
(cid:88) denotes a sentence boundary while ⊗ denotes an am-biguous sentence-internal position..boundaries, but this punctuation is ambiguous—as we might see with acronyms or abbreviationsin english.
when segmented sentences are re-quired, they must be split using a sentence seg-mentation technique that can resolve these ambi-guities.
despite its importance and early positionin the nlp pipeline, sentence segmentation is thesubject of relatively little research.
widely-usedtools such as that in moses (koehn et al., 2007)are implemented with ad-hoc, manually-designed,language-speciﬁc rules, leaving them vulnerable tothe long tail of languages and language phenomena.
the little comparative work that does exist gener-ally focuses on techniques that work in english orother indo-european languages (palmer and hearst,1997; gillick, 2009)..secondly, there is not a well-understood method-ology for training segmenters that do not makenarrow assumptions about the features or charac-teristics of the languages they support.
at theheart of this is the lack of labeled training data.
manually-split datasets that accompany annotationprojects tend to be small, and larger datasets aretypically (imperfectly) segmented by the very toolswhose performance is under question.
tools such.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3995–4007august1–6,2021.©2021associationforcomputationallinguistics3995as nltk (bird and loper, 2004), which packagespunkt (kiss and strunk, 2006), provide an unsuper-vised method to train a model, but it is unclear whatthe effect is when switching to non-latin-script lan-guages, or how a more supervised approach wouldhandle such noisy data..finally, and perhaps most importantly, there areno standard test sets or even metrics for evaluatingsegmenter performance, leaving researchers and en-gineers with no objective way to determine whichone is best..the work described in this paper is aimedatthese problems.
we propose a simplewindow-based model and semi-supervised train-ing paradigm for the segmentation of punctuatedtext (§3).
we frame the task as binary classiﬁcationapplied to a set of candidate punctuation locationsdeﬁned by a regular expression.
leveraging thesimilarity of the task across languages (table 1),we show that our model is able to successfullybootstrap from multilingual data that has been im-perfectly segmented.
we deﬁne a common metricthat works across different tools (§4), and assem-ble a multilingual test suite by semi-automaticallysplitting existing (undersegmented) test sets (§5),providing a basis for proper comparison.
we re-lease these data splits along with our tool, ersatz,as open source.1.
2 background.
a sentence is a sequence of grammatically linkedwords that conveys a complete thought.
the termcan be difﬁcult to deﬁne in a precise manner thatwill not admit any exceptions, and in applicationslike machine translation, there are many timeswhere the basic input unit is not a sentence, buta sentence fragment, such as a headline or an itemfrom a list.
in this work, we skirt these complexi-ties, choosing instead to focus on the most commonscenario, in which we are dealing with standardwritten language.
for this, we adopt a functionaldeﬁnition: a sentence is a group of words that endswith a sentence-ending punctuation mark, such as(for many languages) a period, question mark, orexclamation point.
since punctuation is often usedfor non-sentence-ending purposes as well, the pri-mary challenge for sentence segmentation is resolv-ing this ambiguity for each segmentation candidate..1https://github.com/rewicks/ersatz.
or.
pip install ersatz..research in sentence segmentation2 has beenlimited in scope.
prior work either introduces meth-ods that work under a set of assumptions uniqueto latin-script languages (the existence and impor-tance of casing, word length, or whitespace), ortackles new languages ad hoc, making adaptationto new languages and domains difﬁcult..statistical methods use text-based features suchas casing, punctuation, or length of surroundingwords to make decisions around punctuation.
theearliest work we found (riley, 1989) consideredall sentence boundaries and used decision treesbased on these features.
gillick (2009) trainedtwo statistical models in the form of an svm andnaive bayes classiﬁer.
palmer and hearst (1997)introduced satz and shifted the approach by onlyfocusing potential sentence boundaries being nearsentence-ending punctuation, using part-of-speechdistribution vectors as input to a feed-forward neu-ral network and additionally applied their techniqueto german and french..in order to work without labeled data, kiss andstrunk (2006) used heuristics to craft scores basedon likelihood values of occurrences of tokens, punc-tuation, casing and token length, and then manu-ally tune a threshold of score to indicate a sentenceboundary.
this work expanded the most multilin-gually, considering 10 indo-european languages aswell as estonian and turkish..other work has focused on speciﬁc non-englishlanguages.
xue and yang (2011) study chinese anddissect the theoretical reasons behind segmentingchinese sentences to match their english equiv-alents.
to segment thai, which lacks punctua-tion, zhou et al.
(2016) use pos-taggers.
somework has tackled the problem of domains.
sanchez(2019) approaches the problem of legal text, whichhas a set structure without punctuation; other ap-proaches (wang et al., 2019; rehbein et al., 2020)have investigated speech, which lacks both punctu-ation and written textual structure..a popular splitter is packaged in the mosestoolkit (koehn et al., 2007),3 which works by split-ting on all sentence-ﬁnal punctuation unless thepreceding context is a “non-breaking preﬁx”—ahand-built, language-speciﬁc list of acronyms andabbreviations.
this approach cannot resolve theambiguity where punctuation legitimately exists atthe end of a sentence and is indifferent to novel.
2alternately called sentence boundary detection.
3we use the repackaged python module at https://.
pypi.org/project/sentence-splitter/..3996abbreviations at inference time.
it produces a con-servative segmenter that is high precision (unlikelyto oversegment) but low recall (prone to underseg-menting).
this raises the question of what effectreliance on this tool has had on construction of re-cent massive bitexts, such as ccmatrix (schwenket al., 2019b, §4.3).
gillick (2009) credit a 0.75%increase in accuracy to reduction of summarizationerror by a factor of four.
errors in segmentationmay therefore affect the top matches for a sentencewhen doing bitext construction.
another popularsplitter is spacy, which has not been described orevaluated anywhere, as far as we could tell..with sentence splitting being a crucial piece ofmodern corpus creation for machine translationand other tasks, the lack of approaches and rig-orous comparisons between tools limits the ﬁeld.
additionally, the research ﬁeld moving towards (of-ten massively) multilingual settings, the need tobuild multilingual tools compare them in a properscientiﬁc framework is both important and evident..3 approach.
our general approach is to treat sentence segmen-tation as a binary classiﬁcation problem, predictingsentence-internal (⊗) or sentence-ending ((cid:88)) po-sitions.
the input to the model (§3.1), shown infigure 1, is the concatenated left and right tokencontexts, as depicted in table 1. predictions forboth training and inference are done only at prede-ﬁned candidate sites, which are determined by aregular expression (§3.2).
we then train in a semi-supervised setting where many of the labels maybe missing (§3.3)..3.1 models.
our basic model is depicted in figure 1. the en-coder is a two-layer transformer (vaswani et al.,2017).
our hyperparameter search incorporates vo-cabulary size (v ), embedding size (e), and left andright context sizes (l and r).
we also experimentwith simpler architectures (§8.4), including singleblocks of fully-connected linear layers with a tanhactivation.4 these simpler models typically tradedincreased throughput for slight degradations in f1.
our training objective is binary cross-entropy loss..4we initially experimented with various functions and lay-ers (sigmoid, relu, pooling layers, etc) but found that tanhperforms best..figure 1: model architecture.
a binary predictor is con-structed from token embeddings from the left and rightcontext.
arrows denote output dimensions: v is thevocabulary, l and r the left and right context windowsizes, and e the model/embedding size..3.2 candidate sites.
our model works with segmentation candidatesites for both training and inference.
this can bedone in a fairly general, language-agnostic way.
let p be the set of all punctuation, and pe ⊂ p bethe set of sentence-ending punctuation.
for a giveninput, we examine every character boundary andmatch based on two regular expressions for the leftand right context, respectively:.
• (.
*pep *) : the left context ends withsentence-ﬁnal punctuation, optionally fol-lowed by any amount of punctuation; and.
• ([^0-9].
*) : the right context does not.
start with a number..raw text examples can be found in table 1 andtokenized examples with ﬁxed context sizes areshown in table 2..input to the model is in the form of documents.
a linear pass over the data identiﬁes all candidatessites and assembles them into a batch, with theirassociated left and right contexts.
at training time,instances are extracted with their labels: ⊗ for line-internal sites, and (cid:88) for sites that occur betweeninput lines.
at inference time, the trained classiﬁeris applied, and newlines are inserted where (cid:88) ispredicted..this general deﬁnition carries beneﬁts and risks.
on the positive side, it allows us to work with manylanguages without having to develop language-.
3997encoder!in !the !u.s.
!mr.
!rogers……linear + softmax⊗ ✓0.640.46embeddings(1)(2c, e)(2c, e)(v)(v)(v)(v)(v)(v)label.
left context right context.
⊗⊗(cid:88)(cid:88).
_the _ p .
k .
s o on er ? "
b .
a .
t .
n er s . "
).
_ s h t_ h e __ " w e_ i _ st.table 2: candidate site examples with their labels.
left context-size (6) and right context-size (4) occursin text, ‘_’ is subwordafter subword tokenization.
beginning-of-word character..speciﬁc rules.
it also speeds up training and in-ference, boosting both training speed and inferenceperformance.
on the downside, this loose deﬁni-tion can permit oversegmentation, since it permits,for example, word-internal segmentation in englishand other languages.
the criteria for identifyingcandidate sites can be easily altered to be moreconstrained or more general depending upon usecase, and the list of punctuation to support morelanguages, if necessary.
our default list coversmany languages.5.
3.3 training data.
as noted in our motivation, sentences in the wildare often not segmented but are part of paragraphsand documents.
it is therefore unsurprising to ﬁndmany segmentation errors in existing corpora.
aparticular problem one can observe is that of under-segmentation, perhaps resulting from applicationof conservative segmentation tools.
this meansthe raw training data may contain many false nega-tives ((cid:88) sites mistakenly labeled as ⊗).
training asentence segmentation model therefore presents achicken-and-egg problem.
we aim to train directlyon existing data created for mt purposes, despiteits having been either segmented by imperfect seg-menters, or never segmented..while some data is undersegmented, the vast ma-jority of the end-of-line contexts should be correct,since they are either (a) natural existing bound-aries at the end of a paragraph or document or (b)the result of applying a conservative segmenter.
we therefore hope to train classiﬁers even despitethis noise.
because we are considering a binaryclassiﬁcation problem (and using the associated bi-nary cross entropy loss), we additionally consider.
5our punctuation set (by unicode name): full stop, ques-tion mark, exclamation mark, ellipsis, ideographic full stop,devanagari danda, arabic question mark, arabic full stop,khmer sign khan.
adding a weighted λ value to the (cid:88) class in orderto give more credence to these contexts.6.
for punctuation at the end of a line, the right-context is taken from the tokens at the beginningof the next sentence.
in section §7.3, we look intowhether it matters if this right context is the truedocument context, or whether a random sentencewill serve..4 evaluation: metric.
for evaluation, we begin by removing sentencesthat do not end in punctuation, since none of thetools are able to segment these.
we then concate-nate the test set into a single line, joining sentenceswith a space..evaluation among different tools contains sub-tle complexities.
first, some tools normalize ortokenize the input text, complicating alignment be-tween the input and the output.
second, differenttools may attempt to segment at different subsetsof input string locations, which might unfairly biasthe results in favor of conservative tools.
finally, ifwe permit segmentation at any point of the input,there is a large class imbalance between ⊗ and (cid:88).
the class imbalance advocates for f1 as a naturalmetric.
the use of f1 also addresses the secondissue, since only the gold positive class ((cid:88)) factorsinto the score.
the ﬁrst two issues also requirethat we align a segmenter’s output with the goldstandard segmented text.
since the texts are largelysimilar, we can do this efﬁciently using a modiﬁedlevenshtein distance7 that only considers a ﬁxedmaximum distance between any two characters.
once the text is aligned, we compute f1 againstthe set of (cid:88) symbols in the gold text.
an exampleis depicted in figure 2..5 evaluation: data.
we have noted the difﬁculty with making use ofimperfect training data, and how we hope to workaround it (§3.3).
unfortunately, this workaroundcannot be used for evaluation, where we need gold-standard data..we construct test sets from the wmt newstranslation test sets (barrault et al., 2020), which.
6generally, we ﬁnd no weight (λ = 1.0) is sufﬁcientin punctuated english, but increasing the weight (λ = 20)improved performance in some languages and the multilingualsetting where the data is noisier..7while the distance itself can also be considered in com-paring tools, we do not report these distances, and instead usethe technique to align text within the window..3998figure 2: input text formatted as gold-standard data with two system outputs.
gold positive labels are marked with(cid:88).
for scoring, system outputs are independently aligned to the gold text, which accounts for text transformationsmade by some tools and allows precision and recall to be computed..provides for decent-size test sets in many lan-guages.
we manually corrected all sentence seg-mentations.
while some sets were already well-segmented, some more recent years were extremelyunder-segmented.
in table 5, we show the testsets’ line counts before and after manual correc-tion.8 additionally, we report the % of candidatesites with a true (cid:88) label, which provides a mea-sure of the ambiguity of the punctuation.
many⊗ positions occur in acronyms, such as “u.s.a.
",embedded quotes, ellipsis, or in company namessuch as “yahoo!"..
6 experimental setup.
we consider three language settings: (i) mono-lingual english, (ii) a multilingual setting that in-cludes the set of recent wmt languages plus ara-bic, and (iii) a much larger multilingual setting thatincludes the previous languages plus all languageswith at least 10k lines in the wikimatrix (schwenket al., 2019a) dataset..starting with the english setting, we investigatethe performance of a basic model and vary param-eters such as context size, embedding size, andvocabulary size.
after ﬁnding an optimal setting,we expand to the ﬁrst multilingual setting and re-peat.
we train a single multilingual model that isagnostic of language and does not need languagespeciﬁcation as input.
similar to the monolingualsetting, we vary the aforementioned parameters,and compare the best model to baselines (§6.3).
inorder to test expandability, we then train with thesame parameters on the largest set of languages (us-ing the additional wikimatrix data), and compareto the previous model’s performance..while we do not widely experiment with addi-tional monolingual settings, we train monolingualmodels in each language to compare against themultilingual models’ performance.
we report the.
8iu was left uncorrected due to the fact that availablebitext often aligned “sentences" with singular or compoundsentences in english and a lack of automatic translation corre-sponding to sentences..comparison of these three settings to baselines intable 5..6.1 datasetswe train our english model on a subset of the wsj9and the english news commentary datasets pro-vided by wmt.10.
to expand to a multilingual setting, we considerthe set of all wmt task languages and arabic (23in total) allowing us to leverage the various mono-lingual datasets (joanis et al., 2020) released as partof the wmt workshops—often using news com-mentary datasets, as well as wikimatrix (schwenket al., 2019a), ccmatrix (schwenk et al., 2019b),and global voices (nguyen and daumé iii, 2019).
for validation data, we use wmt test sets whenavailable, and iwslt (cettolo et al., 2017) forarabic..we experimented with (i) balancing the data soeach language has equal amounts of data, (ii) nor-malizing the amount of data per language basedon the relative ambiguity (measured by percent ofcandidate sites labeled as true (cid:88)), and (iii) usingall available data.
we ﬁnd that the third methodperforms the best and thus report under this setting.
in the larger multilingual setting, we consider allwikimatrix languages with more than 10k uniquelines (64 additional languages) and do not expandthe validation set.
for a complete list of datasets,please see table 7 in appendix a..6.2 training.
for each vocabulary size, we train a sentencepiece(kudo and richardson, 2018) model over the train-ing data..we use a binary cross-entropy loss over the la-bels, adam optimizer with a learning rate of 0.0001,and a λ of 1.0 (english) and 20.0 (multilingual).
9sections 1-2, 7-23 for training; section 24 for validation,and sections 03-06 for test in order to mirror the splits in birdand loper (2004).
10http://data.statmt.org/.
news-commentary/v15/.
3999… him.
he added: “mr.
rogers” …him.✓headded: “ mr. rogershim.✓headded: " mr.✓rogershim.✓headded:✓''mr.✓rogerstextgoldsys1sys2p      r       f1–––0.51.00.670.31.00.5on the (cid:88) class (with the exception of the exper-iments in §7.4).
we use a batch size of 25k in-stances, and compute f1 over the validation dataevery 500 batches, saving the model with the high-est inference-time f1 score.
this is the collectivef1 score across all languages in the multilingualsettings.
if the model has not improved in 15 vali-dations, training terminates..the models were trained on a tesla v100 gpu.
the monolingual models took approximately 2hours to train while the multilingual models tookapproximately 10-15 hours..6.3 baselines.
we use the following existing tools as baselines:.
alwaysas a lower-bound for our precision metric..split on every candidate site.
this serves.
splitta (gillick, 2009) ships with both svm andnaive bayes models.
it targets english texts.
wefound similar performance and only report thenaive bayes scores..nltk punkt kiss and strunk (2006) introducean unsupervised training method for this task whichuses frequency of occurences of input featuressuch as casing, punctuation, and length in orderto segment.
pretrained models for 18 languages(labeled as punkt in table 5) are packaged withnltk.
nltk additionally provides the frameworkto train a new model.
we use this to train an ad-ditional model on all data (to simulate a multilin-gual model) and report the results in table 5 aspunktml.
punkt (and thus punktml) does notsegment around non-latin punctuation..moses sentence splitter uses a list of prede-ﬁned acronyms and abbreviations for each lan-guage.
if left token is in this list, it does not split.
this circumvents the whole point behind the ambi-guity "in the u.s.".
spacy sentencizeris a “rule-based system"without speciﬁc details and varies from languageto language..7 monolingual experiments.
we ﬁrst explore common questions and concernswhile focusing on english data and results.
wehave three main parameters to study: context size,embedding size, and vocabulary size.
we addition-ally consider how the training data affects results–both in relative noise in class labels in addition to.
figure 3: heat map showing the change in f1 with re-spect to context size in a linear model.
embedding sizeand vocabulary size are kept constant at 32 and 125 re-spectively..training on shufﬂed sentences instead of documents.
in general, we ﬁnd our technique creates a mono-lingual english model (table 3) that outperformsthe baselines..f1 precision recall.
always.
splittapunktmosesspacy.
86.9.
99.398.698.888.0.our tool.
99.8.
76.9.
99.698.899.786.3.
99.8.
100.0.
99.198.498.089.7.
99.8.table 3: scores on english wsj 03-06 test data.
thecandidate set is determined the original english punc-tuation contexts as described in 3.2.
7.1 exploring context size.
starting with a minimal model with an embeddingsize of 32, and a vocabulary size of 125, we in-vestigate whether such a small model can solvethis problem.
our method is rooted in a contextualencoding of the subword tokens inside its contextwindows, and may beneﬁt from increasing the sizeof these windows.
at the operating point with avery small embedding and vocabulary size, the win-dow size is the determining factor on performance.
the results on english in figure 3 show that a min-imal amount of left and right context is necessary;however, left context is more beneﬁcial than right.
400012345right context size12345678left context size92.0892.4592.7892.7592.6998.0498.7999.0299.0099.1598.7999.4299.3499.2899.4498.9999.4299.4799.4499.5299.1399.4299.5099.3699.5099.1399.5099.4799.5599.4499.0799.4499.3999.4799.5599.2099.4799.5099.4799.50f1 on english devtest98.698.899.099.299.4context..7.2 how large of a model is necessary?.
we consider whether increasing the size of themodel by doubling the embedding size and qua-drupling the vocabulary size can produce betterresults.
while varying the context windows (asseen in figure 3) can result in increasingly higherscores, varying embedding size and vocabulary sizedid not produce the same effect.
keeping a ﬁxedcontext window, we ﬁnd that any given change inembedding size or vocabulary size increases f1score by no more than 0.6%.
while necessary toﬁnd the optimal model, it is clear that the contextsize is more important to experimentation.
we notethat a vocabulary size of 2000 tends to performworse than smaller sizes while vocabulary sizesof 125 and 500 perform equally well when pairedwith any embedding size.
each of our monolingualmodels reported in table 5 is the result of a gridsearch over various vocab sizes, and lambda weight(§3.3).
we keep context sizes of left (6) and right(4) and embedding size (128) constant..7.3.is document context necessary?.
because released monolingual data is often cleanedwith sentences being removed and shufﬂed, it isunreasonable to assume that a set of consecutivesentences will always be available for training..in order to justify using this data, we repeat asubset of the previous english experiment—testingcontext and embedding sizes by training the modelon the same data that has been shufﬂed.
we test onthe same validation data that has not been shufﬂedand retain its document order.
in table 4, we showthat shufﬂing the training data has little impact onperformance and document context is unnecessaryin this punctuated setting..f1 precision recall.
originalshufﬂedundersegmented.
99.899.697.5.
99.799.695.2.
99.999.699.8.table 4: scores on english wsj 03-06 test data.
orig-inal is the best model trained on original english mono-lingual news commentary data.
shufﬂed is trained onshufﬂed data, described in §7.3.
undersegmented istrained on raw wikipedia, described in §7.4..7.4 can we train on undersegmented data?.
uncleaned, unﬁltered wikipedia dumps do nothave sentence boundaries in them.
the smallestunit is the paragraph.
data scraped from internetsites is likely to have a similar form and much ofour monolingual data is not guaranteed to be seg-mented.
in order to justify that this approach workswithout already having segmented data, we showthat we can achieve similar results as our previousenglish results in this setting.
we train on one mil-lion randomly-selected paragraphs from an englishwikipedia dump.
while many ⊗ labels are nowincorrect due to paragraphs being unsegmented, weassume the (cid:88) class is relatively noise-free..because we already established that shufﬂing thedata does not affect performance in this setting, therandom selection is sufﬁcient.
while maintainingpreviously chosen hyper-parameters—such as con-text sizes, learning rate, and dropout—we searchamong potential λ values to use as a weight forthe (cid:88) label.
we ﬁnd that increasing the λ valueto 200.0 achieves the highest f1 of 97.5. an un-weighted model performs poorly.
while still dis-tant from the cleanly-trained models, it performssigniﬁcantly better than the poorer baselines.
com-parison to our other english models can be seen intable 4..8 multilingual experiments.
after outperforming current baselines in a monolin-gual english setting, we generalized our approachto work multilingually.
the multilingual model cansegment text irrespective of input language..in parallel to the monolingual conditions, wetrain two-layer transformer models with 6 tokensof left context, and 4 tokens of right context with128 embedding size.
while we did experiment withscaling these for the multilingual model, we foundlittle effect.
we additionally scale the vocabularysize to 12,000 to accommodate the larger charactersets in chinese and japanese.
because more of theadditional languages have undersegmented data,we searched over potential lambda weights for the(cid:88) class and report the best conﬁguration (λ =20.0) in table 5..8.1 discussion.
results of ersatz and baselines can be foundin table 5. in all cases, ersatz is at least com-petitive with baselines, if not outperforming them.
although most differences are small it outperforms.
4001# orig..# corr.
% (cid:88) punkt.
punktml.
always.
spacy moses.
ersatzm.
ersatz.
ersatzwm.
arcsdeenesetﬁfrguhiiujakkkmltlvplpsrorutatrzh.
all.
146066478577063000200019961619101625072971993100023201000200110012719199999199730002000.
45k.
1504172619657706306420171996165510182521297110721002236110002017100527262000991100530092003.
84.980.190.248.686.578.295.095.092.368.659.189.492.296.359.276.470.796.489.188.466.167.585.1.
-99.899.798.699.199.399.799.5--------98.3--98.8---.
92.799.699.587.798.999.499.799.6100.014.491.30.299.62.094.799.494.899.498.798.192.395.8-.
90.385.391.488.083.684.097.990.43.890.6-93.7--76.688.678.9-90.991.389.385.196.6.
-99.799.998.898.799.599.899.799.715.1----98.699.692.8-98.599.493.899.5-.
98.299.899.999.898.899.899.999.799.898.586.199.999.799.798.899.793.499.399.399.398.199.6100.0.
98.099.899.898.798.699.799.999.6100.099.193.799.999.899.798.899.599.199.399.399.496.999.5100.0.
93.596.397.977.096.590.698.998.297.983.763.998.197.199.185.590.390.199.197.096.489.685.299.2.
89.0.
98.099.899.899.198.699.899.999.4100.098.693.699.999.999.798.999.699.299.399.299.596.699.5100.0.
98.9.
48k.
73.3.
-.
87.6.
-.
-.
-.
98.9.table 5: test set statistics (left block) and f1 scores (right block) on our test data.
% (cid:88) denotes the number ofcandidate sites with a true (cid:88) label.
punktml denotes punkt model trained on our data.
lack of a score means themodel was not available for that language.
ersatzm denotes monolingual models, ersatz the wmt-languagesmultilingual model, and ersatzwm the model trained with additional wikimatrix languages..spacy in all languages and often outperforms bothpunkt and moses..the moses splitter is an interesting case.
it iden-tiﬁes split points via a mix of general and language-speciﬁc regular expressions, which are then ﬁlteredagainst a curated list of “non-breaking preﬁxes”.
this results in a conservative segmenter that willnot (for example) allow a sentence to end withthe token u.s.. as such, its high performance isnotable.
however, the comparison is likely unfair,since it was likely built and reﬁned against the newsdatasets that constitute our wmt test sets.
thisapproach is therefore effective in this domain, butmay not generalize.
our single multilingual model,trained on noisy data, performs nearly identically..8.2 performance across languages.
sentence segmentation is not equally difﬁcult in alllanguages or with respect to all punctuation.
the ‘.’.
is by far the most ambiguous form of punctuationand is frequently used as an abbreviation marker.
other scripts using their own punctuation, suchas hindi, have speciﬁed a particular marker (thedevanagari danda) as a sentence-ending punctua-tion that is rarely used sentence-internally.
in thesecases, ambiguity is introduced when alternativepunctuation (such as ‘.’ or ‘...’) is used.
addi-tionally, even languages with the same scripts maynot have the same level of ambiguity.
french hasthe smallest number of punctuated contexts occur-ring sentence-internally within our test set, whileenglish has the most..we note that the multilinguality of our modelhurts the near-perfect performance that we see inthe monolingual english models.
we additionallynote that some monolingual models perform worsethan the multilingual model (see pl in table 5).
we hypothesize that this may be due to a lack of.
4002data, and the additional languages contain similarcontexts, so the model may learn more about cas-ing, punctuation, and length with additional data..9 summary.
8.3 scaling to more languages.
while we note that it is difﬁcult to evaluate manyof the world’s languages due to a lack of gold stan-dard test data, we test for scalability by includingadditional languages (as described in §6) duringtraining and noting any changes in performance onthe evaluable languages.
we include 64 additionallanguages (see table 7 in the appendix for compre-hensive list) to bring us to a total of 87 languages.
table 5 also includes scores from a larger multilin-gual model (ersatzwm) that was built with these64 additional languages.
overall, we ﬁnd very lit-tle change between these two settings.
with en,we actually see some improvement in performancefrom the smaller multilingual model.
generally,there is not signiﬁcant degradation of scores, im-plying this technique can generalize to additionallanguages..8.4 how does size affect the speed?.
with our context construction method, we beneﬁtfrom batching to decrease runtime, since the deci-sion at each candidate point is dependent only onits immediate window.
we benchmark our modelsas well as the baselines (table 6).
while our mod-els are slower than some baselines, we ﬁnd thatincreasing the size of the model does not dramat-ically increase the runtime.
additionally, the rate(in tokens per second) is roughly constant..layer (# layers).
# params time (s) f1.
linear (x1)linear (x2)transformer (x1)transformer (x2).
spacymosespunkt.
1.7m1.7m2.3m2.9m.
---.
333574172.
13.811643.2.
97.598.098.798.7.
88.098.898.6.table 6: time in seconds for 1 million english tokensin input ﬁle.
f1 is score on english test set.
we showvarious size encoders for our method.
linear is a linearlayer with tanh activation..as one of the earliest steps in nlp pipelines, sen-tence segmentation is an important task.
however,it has not to this date received proper experimentalattention, relying instead on ad hoc methods.
itis a good time to correct this oversight, as nlpmoves to the use of larger and larger corpora cov-ering more and more languages.
even as the ﬁeldmoves towards processing text at the paragraph ordocument level directly, it is likely that sentenceprocessing will be with us for some time..we show here that a simple context-based modelcan produce state-of-the-art results with a modesthyperparameter search, trained on noisy annota-tions from imperfectly-segmented data.
togetherwith a straightforward multilingual approach toidentifying candidate split points and training onnoisy segmented data, our single model performswell across a range of languages.
more fundamen-tally, we have deﬁned an experimental frameworkfor benchmarking and future comparative work..missing from our paper is an evaluation of theeffect of these tools on downstream tasks.
an ob-vious candidate for future work is to conduct thisevaluation.
it is possible that some tasks will notbe affected by small differences among the bestperforming models, but this work at least shedslight on those differences.
another obvious direc-tion is to look at approaches that would work forunpunctuated text (e.g., wang et al.
(2019)).
thiswould expand the functionality of segmenters intoother important areas, such as speech translation,and to languages, like thai, that do not mark endsof sentences..acknowledgments.
the authors wish to thank elizabeth salesky, carlosaguirre, jacob bremerman and the anonymous re-viewers for helpful technical discussions and feed-back..references.
loïc barrault, magdalena biesialska, ondˇrej bojar,marta r. costa-jussà, christian federmann, yvettegraham, roman grundkiewicz, barry haddow,matthias huck, eric joanis, tom kocmi, philippkoehn, chi-kiu lo, nikola ljubeši´c, christofmonz, makoto morishita, masaaki nagata, toshi-aki nakazawa, santanu pal, matt post, and marcoszampieri.
2020. findings of the 2020 conference onin proceedings ofmachine translation (wmt20)..4003the fifth conference on machine translation, pages1–55, online.
association for computational lin-guistics..david d. palmer and marti a. hearst.
1997. adap-tive multilingual sentence boundary disambiguation.
computational linguistics, 23(2):241–267..steven bird and edward loper.
2004. nltk: the nat-ural language toolkit.
in proceedings of the acl in-teractive poster and demonstration sessions, pages214–217, barcelona, spain.
association for compu-tational linguistics..mauro cettolo, marcello federico, luisa bentivogli,jan niehues, sebastian stüker, katsuitho sudoh,koichiro yoshino, and christian federmann.
2017.overview of the iwslt 2017 evaluation campaign.
in14th international workshop on spoken languagetranslation, pages 2–14, tokyo, japan..dan gillick.
2009. sentence boundary detection andthe problem with the u.s. in proceedings of humanlanguage technologies: the 2009 annual confer-ence of the north american chapter of the associa-tion for computational linguistics, companion vol-ume: short papers, pages 241–244, boulder, col-orado.
association for computational linguistics..eric joanis, rebecca knowles, roland kuhn, samuellarkin, patrick littell, chi-kiu lo, darlene stew-art, and jeffrey micher.
2020. the nunavut hansardinuktitut–english parallel corpus 3.0 with prelimi-nary machine translation results.
in proceedings ofthe 12th language resources and evaluation con-ference, pages 2562–2572, marseille, france.
euro-pean language resources association..tibor kiss and jan strunk.
2006. unsupervised mul-tilingual sentence boundary detection.
computa-tional linguistics, 32(4):485–525..philipp koehn, hieu hoang, alexandra birch, chriscallison-burch, marcello federico, nicola bertoldi,brooke cowan, wade shen, christine moran,richard zens, chris dyer, ondˇrej bojar, alexandraconstantin, and evan herbst.
2007. moses: opensource toolkit for statistical machine translation.
inproceedings of the 45th annual meeting of the as-sociation for computational linguistics companionvolume proceedings of the demo and poster ses-sions, pages 177–180, prague, czech republic.
as-sociation for computational linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing: systemdemonstrations, pages 66–71, brussels, belgium.
association for computational linguistics..khanh nguyen and hal daumé iii.
2019. globalvoices: crossing borders in automatic news sum-in proceedings of the 2nd workshopmarization.
on new frontiers in summarization, pages 90–97,hong kong, china.
association for computationallinguistics..ines rehbein,.
josef ruppenhofer,.
and thomasschmidt.
2020.improving sentence boundary de-tection for spoken language transcripts.
in proceed-ings of the 12th language resources and evaluationconference, pages 7102–7111, marseille, france.
european language resources association..michael d. riley.
1989. some applications of tree-based modelling to speech and language.
in speechand natural language: proceedings of a workshopheld at cape cod, massachusetts, october 15-18,1989..george sanchez.
2019. sentence boundary detectionin proceedings of the natural legalin legal text.
language processing workshop 2019, pages 31–38,minneapolis, minnesota.
association for computa-tional linguistics..holger schwenk, vishrav chaudhary, shuo sun,hongyu gong, and francisco guzmán.
2019a.
wikimatrix: mining 135m parallel sentences incorr,1620 language pairs from wikipedia.
abs/1907.05791..holger schwenk, guillaume wenzek, sergey edunov,edouard grave, and armand joulin.
2019b.
cc-matrix: mining billions of high-quality parallel sen-tences on the web.
corr, abs/1911.04944..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..xiaolin wang, masao utiyama, and eiichiro sumita.
2019. online sentence segmentation for simultane-ous interpretation using multi-shifted recurrent neu-ral network.
in proceedings of machine translationsummit xvii volume 1: research track, pages 1–11,dublin, ireland.
european association for machinetranslation..nianwen xue and yaqin yang.
2011. chinese sen-tence segmentation as comma classiﬁcation.
in pro-ceedings of the 49th annual meeting of the associ-ation for computational linguistics: human lan-guage technologies, pages 631–635, portland, ore-gon, usa.
association for computational linguis-tics..nina zhou, aiti aw, nattadaporn lertcheva, and xu-ancong wang.
2016. a word labeling approach tothai sentence boundary detection and pos tagging.
in proceedings of coling 2016, the 26th inter-national conference on computational linguistics:technical papers, pages 319–327, osaka, japan.
thecoling 2016 organizing committee..a appendix.
4004dataset.
# lines.
# tokens.
dataset.
# lines.
# tokens.
et news crawlwikimatrix.
1.6m152k.
22m ps news crawl5.1m.
ar news comm.
v15wikimatrix.
cs news comm.
v15wikimatrix.
de news comm.
v15wikimatrix.
en news comm.
v15.
wsj (sec 00-02;07-23).
es news comm.
v15.
ﬁ news crawlwikimatrix.
fr news comm.
v15wikimatrix.
gu news crawl.
common crawl.
hi news comm.
v15wikimatrixnews crawl.
iu n.h.i 3.0.ja news comm..news crawl.
181k774k.
277k429k.
422k1m.
609k40k.
465k.
4.7m207k.
415k2.2m.
283k164k.
78151.1m135k.
1.3m.
29833.4m.
10m kk news comm.
v1516m.
news crawl.
5.2m km jw corpus7.3m.
common crawl.
8.9m19m.
13m819k.
12m.
lt news crawlwikimatrix.
lv news crawl.
pl global voiceswikipedianews crawl.
sadasystrantranstac.
50m ro global voices2.6m.
wikimatrixnews crawl.
10m ru news comm.
v1550m.
wikimatrix.
ta news crawlwikimatrix.
tr global voiceswikimatrixnews crawl.
8.0m zh news comm.
v15.
wikimatrix.
3.8m1.3m.
213k20m3.0m.
43906.9m.
16.4k1.1m.
107k343k.
2.5m84.8k.
1.8m.
58k405k3.0m.
64.0k132k196k75k.
4043223k6.9m.
377k2.2m.
501k61.0k.
6529304k7.9m.
445k492k.
280k14m.
4.6m2.0m.
37m1.1m.
29m.
890k6.6m44m.
1.8m4.1m5.1m1.2m.
76k4.7m140m.
7.3m37m.
5.3m532k.
80k4.5m108m.
772k890k.
table 7: multilingual datasets line count and token count..4005dataset.
# lines.
# tokens.
dataset.
# lines.
# tokens.
ar news comm v15iwslt 2017.cs wmt18 testwmt20 test.
de wmt19 testwmt20 test.
en news commentary.
wmt20 test (en-cs)wsj 03-06; 24.es wmt11 test.
wmt13 test set.
et news commentary.
wmt18 test set.
ﬁ wmt18 test setwmt19 test set.
fr wmt15 test set.
gu news commentary.
wmt19 test set.
hi news commentary.
wmt14 test set.
iu n.h.i 3.0 devn.h.i 3.0 dev.
ja news commentary.
wmt20 test set.
wmt20 test set (fr-de).
16371504.
30081726.
20091965.
3000.
10k.
30133064.
30002017.
30311996.
15021655.
30001018.
30002521.
30283028.
30001072.
277k.
38k20k.
47k26k.
31k31k.
56k.
69k62k.
41k30k.
38k21k.
25k33k.
40k14k.
56k57k.
27k27k.
5.9k1888.kk news comm v15wmt19 test.
km wmt wikidev.
wmt20 test.
lt news comm v15wmt19 test.
lv news commentarywmt17 test.
pl news commentary v15wmt20 test set.
ps wmt wiki dev.
wmt20 test set.
ro news commentary v15wmt16 test set.
ru wmt18 test setwmt20 test set.
ta news commentary.
wmt20 test set.
tr wmt16 test setwmt18 test set.
zh wmt18 test setwmt20 test set.
30001002.
26092361.
30001000.
30002017.
30001005.
31662726.
30002000.
3000991.
30001005.
30113009.
40972003.
38k30k.
14k15k.
44k17k.
49k33k.
16k16k.
64k55k.
60k43k.
52k15k.
32k13k.
44k46k.
5.8k3.7k.
table 8: dev and test data.
test data is bolded.
all news and wikipedia sets come from wmt news translationtasks except ar.
all test sets are lang-en unless otherwise noted.
nhi is the nunavut hansard inuktitut englishparallel corpus-3.0.
when news commentary was used, the bottom n lines were taken..4006# lines.
# tokens.
# lines.
# tokens.
anarzasazbarbabebgbnbrbscacebdaeleoeufafofyglgomhehrhuhyidisitjvkako.
52k35k16k164k40k101k164k454k360k43k502k459k80k453k454k454k305k427k38k56k453k22k458k455k456k23k456k124k469k27k42k454k.
la93klb57k15klmo170k mg49k mk112k ml223k mr523k mwl452k53k831k417k188k494k555k554k369k818k46k84k512k19k387k551k353k52k468k160k386k40k81k wuu395k.
nds-nlndsnenlnoocptshsimplesiskslsqsrsvswtetgtlttukvi.
45k43k10k12k452k150k216k32k14k95k70k456k457k171k460k454k465k182k453k451k262k452k452k70k213k17k122k78k466k456k46k.
50k59k16k18k672k130k225k78k22k145k81k348k472k389k367k582k666k281k539k624k523k520k388k118k170k20k237k80k313k646k7k.
table 9: additional wikimatrix languages with line and token counts for training data.
language code based onwikipedia codes..4007