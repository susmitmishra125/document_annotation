from paraphrasing to semantic parsing: unsupervised semantic parsingvia synchronous semantic decodingshan wu1,3, bo chen1, chunlei xin1,3, xianpei han1,2,∗, le sun1,2,∗,weipeng zhang4, jiansong chen4, fan yang4, xunliang cai41chinese information processing laboratory 2state key laboratory of computer scienceinstitute of software, chinese academy of sciences, beijing, china3university of chinese academy of sciences, beijing, china 4meituan{wushan2018,chenbo,xianpei,sunle}@iscas.ac.cn,xinchunlei20@mails.ucas.ac.cn,{zhangweipeng02,chenjiansong,yangfan79,caixunliang}@meituan.com.
abstract.
semantic parsing is challenging due to thestructure gap and the semantic gap betweenin this paper,utterances and logical forms.
we propose an unsupervised semantic pars-ing method – synchronous semantic decod-ing (ssd), which can simultaneously resolvethe semantic gap and the structure gap byjointly leveraging paraphrasing and grammar-constrained decoding.
speciﬁcally, we refor-mulate semantic parsing as a constrained para-phrasing problem: given an utterance, ourmodel synchronously generates its canonicalutterance1 and meaning representation.
dur-ing synchronous decoding: the utterance para-phrasing is constrained by the structure of thelogical form, therefore the canonical utterancecan be paraphrased controlledly; the seman-tic decoding is guided by the semantics of thecanonical utterance, therefore its logical formcan be generated unsupervisedly.
experimen-tal results show that ssd is a promising ap-proach and can achieve competitive unsuper-vised semantic parsing performance on multi-ple datasets..1.introduction.
semantic parsing aims to translate natural lan-guage utterances to their formal meaning repre-sentations, such as lambda calculus (zettlemoyerand collins, 2005; wong and mooney, 2007),funql (kate et al., 2005; lu et al., 2008), andsql queries.
currently, most neural semanticparsers (dong and lapata, 2016; chen et al.,2018b; zhao et al., 2020; shao et al., 2020) modelsemantic parsing as a sequence to sequence trans-lation task via encoder-decoder framework..∗corresponding author1canonical utterances are pseudo-language representa-tions of logical forms, which have the synchronous structureof logical forms.
(berant and liang, 2014; xiao et al., 2016;su and yan, 2017; cao et al., 2020).
figure 1: different from previous staged methods (in-dicated by gray lines), our method generates canonical ut-terance and logical form synchronously.
the semantic gapand the structure gap are simultaneously resolved by jointlyleveraging paraphrasing and grammar-constrained decoding.
thus, our synchronous decoding employs both the semanticand the structure constraints to solve unsupervised semanticparsing..semantic parsing is a challenging task due tothe structure gap and the semantic gap betweenlanguage utterances and logical forms.
naturalfor structure gap, because utterances are usu-ally word sequences and logical forms are usuallytrees/graphs constrained by speciﬁc grammars, asemantic parser needs to learn the complex struc-ture transformation rules between them.
for se-mantic gap, because the ﬂexibility of natural lan-guages, the same meaning can be expressed usingvery different utterances, a semantic parser needsbe able to map various expressions to their seman-tic form.
to address the structure gap and the se-mantic gap, current semantic parsers usually relyon a large amount of labeled data, often resultingin data bottleneck problem..previous studies have found that the structuregap and the semantic gap can be alleviated byleveraging external resources,therefore the re-liance on data can be reduced.
for structure gap,previous studies found that constrained decodingcan effectively constrain the output structure byinjecting grammars of logical forms and facts in.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5110–5121august1–6,2021.©2021associationforcomputationallinguistics5110paraphrase modelsemantic parsinggrammarparaphrasingsynchronous semanticdecoding(our method)structureconstraintssemanticconsistencylogical form: answer ( count ( river ( traverse_2 ( state0 ) ) ) )canonical utterance: what is the number of river traverse state0utterance: how many rivers run through state0semanticconsistencystructureconstraintsknowledge bases during inference.
for example,the grammar-based neural semantic parsers (xiaoet al., 2016; yin and neubig, 2017) and the con-strained decoding algorithm (krishnamurthy et al.,2017).
for semantic gap, previous studies havefound that paraphrasing is an effective techniquefor resolving the diversity of natural expressions.
using paraphrasing, semantic parsers can han-dle the different expressions of the same mean-ing, therefore can reduce the requirement of la-beled data.
for example, supervised methods (be-rant and liang, 2014; su and yan, 2017) use theparaphrasing scores between canonical utterancesand sentences to re-rank logical forms; two-stage(cao et al., 2020) rewrites utterances to canonicalutterances which can be easily parsed.
the maindrawback of these studies is that they use con-strained decoding and paraphrasing independentlyand separately, therefore they can only alleviate ei-ther semantic gap or structure gap..in this paper, we propose an unsupervised se-mantic parsing method – synchronous seman-tic decoding (ssd), which can simultaneouslyresolve the structure gap and the semantic gapby jointly leveraging paraphrasing and grammar-constrained decoding.
speciﬁcally, we model se-mantic parsing as a constrained paraphrasing task:given an utterance, we synchronously decode itscanonical utterance and its logical form using ageneral paraphrase model, where the canonical ut-terance and the logical form share the same un-derlying structure.
based on the synchronous de-coding, the canonical utterance generation can beconstrained by the structure of logical form, andthe logical form generation can be guided by thesemantics of canonical form.
by modeling theinterdependency between canonical utterance andlogical form, and exploiting them through syn-chronous decoding, our method can perform ef-fective unsupervised semantic parsing using onlypretrained general paraphrasing model – no anno-tated data for semantic parsing is needed..we.
results.
conduct.
experimentsexperimental.
on geo andovernight.
showthat our method is promising, which can achievecompetitive unsupervised semantic parsing per-formance, and can be further improved withexternal resources.
the main contributions of thispaper are:.
coding , which can simultaneously re-solve the semantic gap and the structuregap by jointly leveraging paraphrasing andgrammar-constrained semantic decoding..• we design two effective synchronous seman-tic decoding algorithms – rule-level inferenceand word-level inference, which can generateparaphrases under the grammar constraintsand synchronously decode meaning repre-sentations..• our model achieves competitive unsuper-vised semantic parsing performance on geoand overnight datasets..2 model overview.
we now present overview of our synchronoussemantic decoding algorithm, which can jointlyleverage paraphrasing and grammar-constraineddecoding for unsupervised semantic parsing.
given an utterance, ssd reformulates seman-tic parsing as a constrained paraphrasing prob-lem, and synchronously generates its canonicalutterance and logical form.
for example infig.
2, given “how many rivers run throughtexas”, ssd generates “what is the number ofriver traverse state0” as its canonical form andanswer(count(river(traverse 2(state0)))) as its logical form.
during syn-chronous decoding: the utterance paraphrase gen-eration is constrained by the grammar of logicalforms, therefore the canonical utterance can begenerated controlledly; the logical form is gener-ated synchronously with the canonical utterancevia synchronous grammar.
logical form genera-tion is controlled by the semantic constraints fromparaphrasing and structure constraints from gram-mars and database schemas.
therefore the logicalform can be generated unsupervisedly..to this end, ssd needs to address two chal-lenges.
firstly, we need to design paraphrasing-based decoding algorithms which can effectivelyimpose grammar constraints on inference.
sec-ondly, current paraphrasing models are trained onnatural language sentences, which are differentfrom the unnatural canonical utterances.
there-fore ssd needs to resolve this style bias for effec-tive canonical utterance generation..• we propose an unsupervised semantic pars-ing method – synchronous semantic de-.
speciﬁcally, we ﬁrst propose two inference al-gorithms for constrained paraphrasing based syn-.
5111figure 2: overview of our approach.
the sentence is paraphrased to canonical utterance and parsed to logical form syn-chronously.
when decoding “traverse”, the paraphrase model tends to generate the words such as “run”, “ﬂow”, “traverse”to preserve semantics.
and synchronous grammar limits the next words of the canonical utterance to follow the candidateproduction rules.
then it is easy to discard “run” and “ﬂow”, and select the most likely word “traverse” with its production rulefrom the candidates.
in this parper, we propose rule-level and word-level inference methods to decode words and productionrules synchronously..rule-level.
chronous semantic decoding:infer-ence and word-level inference.
then we resolvethe style bias of paraphrase model via adaptiveﬁne-tuning and utterance reranking, where adap-tive ﬁne-tuning can adjust the paraphrase modelto generate canonical utterances, and utterancereranking resolves the style bias by focusing moreon semantic coherence.
in sections 3-5, we pro-vide the details of our implementation..a non-terminal, and α and β are sequence of termi-nal and non-terminal symbols.
each non-terminalsymbol in α is aligned to the same non-terminalsymbol in β, and vice versa.
therefore, an scfgdeﬁnes a set of joint derivations of aligned pairs ofutterances and logical forms..scfgs can provide useful constraints for se-mantic decoding by restricting the decoding spaceand exploiting the semantic knowledge:.
3 synchronous semantic decoding.
given an utterance x, we turn semantic pars-ing into a constrained paraphrasing task.
con-cretely, we use synchronous context-free gram-mar as our synchronous grammar, which pro-vides a one-to-one mapping from a logical formy to its canonical utterance cy.
the parsing taskpparse(y|x) is then transferred toˆy = arg maxy∈ypparaphrase(cy|x).
instead of directlyˆy = arg maxy∈yparsing utterance into its logical form, ssd gen-erates its canonical utterance and obtains its log-ical form based on the one-to-one mapping rela-tion.
in following we ﬁrst introduce the grammarconstraints in decoding, and then present two in-ference algorithms for generating paraphrases un-der the grammar constraints..3.1 grammar constraints in decoding.
synchronous context-free grammar(scfg) is em-ployed as our synchronous grammar, which iswidely used to convert a meaning representationinto an unique canonical utterance (wang et al.,2015; jia and liang, 2016).
an scfg consists ofa set of production rules: n → (cid:104)α, β(cid:105), where n is.
grammar constraints the grammarsen-sure the generated utterances/logical forms aregrammar-legal.
in this way the search space canbe greatly reduced.
for example, when expandingthe non-terminal $r in fig 2 we don’t need toconsider the words “run” and “ﬂow”, becausethey are not in the candidate grammar rules..semantic constraints like the type checkingin wang et al.
(2015), the constraints of knowl-edge base schema can be integrated to further re-ﬁne the grammar.
the semantic constraints ensurethe generated utterances/logical forms will be se-mantically valid..3.2 decoding.
3.2.1 rule-level inferenceone strategy to generate paraphrase under thegrammar constraint is taking the grammar ruleas the decoding unit.
grammar-based decodershave been proposed to output sequences of gram-mar rules instead of words(yin and neubig, 2017).
like them, our rule-level inference method takesthe grammar rule as the decoding unit.
figure 3(a) shows an example of our rule level inferencemethod..5112$r → root →flow...how many rivers run through texas                      (                             (                         (      traverse_2 ( ... ) ) ) )what isthe number ofrivertraverse...what is $eanswer($e)the number of $ccount($c)river $rriver($r)paraphrasingrunlocatednexttraverse $straverse_2($s)located in $sloc_1($s)...synchronous decodinganswercountriver ...$e →$c → $r → scfgrules:(a) rule-level inference.
(b) word-level inference.
figure 3: from the utterance “which state is city0 in”, two inference methods generate its canonical utterance“what is state that city0 located in” and its logical form answer(state(loc 1(city0))).
the ways theyhandle non-terminal $c which is not at the end of utterance-side production rule are represented by purple lines..algorithm 1: rule-level inferenceinput.
: input utterance x, paraphrasing model.
p ara, beam size b, maximum output length l,scfg rules r, maximum search depth k;.
1 beam0 ← {(cid:104)s(cid:105)}2 outputs ← {}3 for t = 1 to l do4.for hypothesis c in beamt−1 dofor r in expand rules for c do.
if all non-terminals in rβ are on the right then.
c(cid:48) ← expand(c, r)beamt ← beamt ∪ {c(cid:48)}.
else.
c(cid:48) ← expand(c, r)beamc(cid:48)for k = 1 to k do.
t ← {c(cid:48)}.
for hypothesis h in beamc(cid:48).
t+k−1 do.
rh ← expand rules for h’s ﬁrst.
non-terminal.
beamc(cid:48).
t+k ← beamc(cid:48)beamc(cid:48)t+k ← nbest(beamc(cid:48)move utterances from beamc(cid:48).
t+k∪ expand(h, rh)t+k, b).
t+k to beamt+k,.
if non-terminals are on the right of theutterances..beamt ← nbest(beamt,b − |outputs|)1819 move full utterances from beamt to outputs20.if beamt is empty then.
return outputs.
2122 return outputs.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.when the non-terminal in the utterance-sideproduction rule is at the end of the rule (e.g., $e →(cid:104)state $s, state($s)(cid:105)), denoting the utterance-side production rule as rβ = [w1, w2, ..., wlr , n ],we can simply expand non-terminals in canonicalutterances by this rule, and generate the canoni-cal utterances from left to right with probabilitiescomputed by:.
lr(cid:89).
i=1.
p (cy≤t |x) = p (cy<t |x).
pparaphrase(wi|x, cy<t , w<i).
(1)otherwise, we generate the next production rulesto expand this rule (i.e., rule with purple line), until.
there is no non-terminal on the left of words, or thegenerating step reaches the depth of k. we usebeam search during the inference.
the inferencedetails are described in algorithm 1..3.2.2 word-level inference.
except for rule-level inference, we also proposea word-level inference algorithm, which generatesparaphrases word by word under the scfg con-straints..firstly, we construct a deterministic automatonusing lr(1) parser (knuth, 1965) from the cfgin utterance side.
the automaton can transit fromone state to another in response to an input.
theinputs of the automaton are words and the statesof it are utterance/logical form segments.
lr(1)parser peeks ahead one lookahead input symbol,and the state transition table describes the accept-able inputs and the next states..then, in each decoding step we generate a wordwith a new state which is transited from previ-ous state.
an example is shown in figure 3 (b).
only the acceptable words in the current state canbe generated, and the end-of-sentence symbol canonly be generated when reaching the ﬁnal state.
beam search is also used in this inference..4 adaptive fine-tuning.
the above decoding algorithms only rely ona paraphrase generation model , which gener-ates canonical utterance and logical form syn-chronously for semantic parsing.
we can directlyuse general paraphrase generation models suchas gpt-2(radford et al., 2019), t5(raffel et al.,2020) for ssd.
however, as described in above,there exists a style bias between natural languagesentences and canonical utterances, which hurtsthe performance of unsupervised semantic par-.
5113locatedinwhat thatstateislargestcitycity0lake0thelocatedlocatedanswer(state($s))answer(state(loc_1(city0)))answer(state(loc_1(lake0)))answer(state(loc_1(largest(city))))answer($e)$e → $s → root →what is $eanswer($e)state $sstate($s)that $c located in loc_1($c)$c → city0city0root → <what is $e, answer($e)>$e → <state $s, state($s)>$s → <that $c located in, loc_1($c)>$c → <city0, city0>$e → $s → root →what is $eanswer($e)state $sstate($s)that $c located in loc_1($c)$c → city0city0locatedinwhat thatstateislargestcitycity0lake0thelocatedlocatedanswer(state($s))answer(state(loc_1(city0)))answer(state(loc_1(lake0)))answer(state(loc_1(largest(city))))answer($e)$e → $s → root →what is $eanswer($e)state $sstate($s)that $c located in loc_1($c)$c → city0city0ing.
in this section, we describe how to allevi-ate this bias via adaptive ﬁne-tuning.
given atext generation model, after pretraining it usingparaphrase corpus, we ﬁne-tune it using synthe-sized (cid:104) sentence, canonical utterance (cid:105)pairs..previous studies have shown that.
the pre-training on synthesized data can signiﬁcantly im-prove the performance of semantic parsing (xuet al., 2020a; marzoev et al., 2020; yu et al., 2020;xu et al., 2020b).
speciﬁcally, we design threedata synthesis algorithms:1) cus we sample cus from scfgs, and pre-serve executable ones.
as we do not have thepaired sentences, we only ﬁne-tune the languagemodel of the plms on cus.
2) self paras we use the trained paraphrase modelto get the natural language paraphrases of the sam-pled canonical utterances to form (cid:104) sentence,canonical utterance (cid:105) pairs.
3) external paras we also use external para-phrase methods such as back translation to get thepairs..5 utterance reranking.
adaptive ﬁne-tuning resolves the style bias prob-lem by ﬁtting a better paraphrase model.
in thissection, we propose an utterance reranking algo-rithm to further alleviate the style bias by rerank-ing and selecting the best canonical form..given the utterance x and top-n parsing re-sults (yn, cn), n = 1, 2, ..., n , we rerank all candi-dates by focusing on semantic similarities betweenx and cn, so that canonical utterances can be ef-fectively selected.
reranking for semantic pars-ing has been exploited in many previous studies(berant and liang, 2014; yin and neubig, 2019).
these works employ reranking for canonical ut-terances selection.
differently, our re-ranker doesnot need labeled data.
formally, we measure twosimilarities between x and cn and the ﬁnal rerank-ing score is calculated by:.
score(x, c) = log p(c|x) + srec(x, c).
(2).
+ sasso(x, c).
reconstruction score the reconstruction scoremeasures the coherence and adequacy of thecanonical utterances, using the probability of re-producing the original input sentence x from cwith the trained paraphrasing model: srec(x, c) =log ppr(x|c).
association score the association score mea-sures whether x and c contain words that are likelyto be paraphrases.
we calculate it as:.
sasso(x, c) = log.
p (ci|xj) a(j|i).
|c|(cid:89).
|x|(cid:88).
i=1.
j=0.
|x|(cid:89).
|c|(cid:88).
j=1.
i=0.
+ log.
p (xj|ci) a(i|j).
(3).
in which, p (ci|xj) means the paraphrase proba-bility from xj to ci, and a(j|i) means the align-ment probability.
the paraphrase probability andalignment are trained and inferred as the transla-tion model in smt ibm model 2..6 experiments.
6.1 experimental settings.
datasets we conduct experiments on threedatasets: overnight(λ-dcs), geo(funql),and geogranno, which use different meaningrepresentations and on different domains.
our im-plementations are public available2..overnight this is a multi-domain dataset,language paraphraseswhich contains naturalpaired with lambda dcs logical forms acrosseight domains.
we use the same train/test splitsas wang et al.
(2015)..geo(funql) this is a semantic parsingbenchmark about u.s. geography (zelle andmooney, 1996) using the variable-free semanticrepresentation funql (kate et al., 2005).
we ex-tend the funql grammar to scfg for this dataset.
we follow the standard 600/280 train/test splits..geogranno this is another version ofgeo (herzig and berant, 2019), in which lambdadcs logical forms paired with canonical utter-ances are produced from scfg.
instead of para-phrasing sentences, crowd workers are required toselect the correct canonical utterance from can-didate list.
we follow the split (train/valid/test487/59/278) in original paper..paraphrase model we obtain the paraphrasemodel by training t5 and gpt2.0 on wikianswerparaphrase3, we train 10 epochs with learning rateas 1e-5.
follow li et al.
(2019), we sample 500kpairs of sentences in wikianswer corpus as train-ing set and 6k as dev set.
we generate adap-tive ﬁne-tuning datasets proportional to their la-beled datasets, and back-translation(from english.
2https://github.com/lingowu/ssd3http://knowitall.cs.washington.edu/ paralex.
5114supervisedrecombination (jia and liang, 2016)crossdomain (su and yan, 2017)seq2action (chen et al., 2018b)dual (cao et al., 2019)two-stage (cao et al., 2020)ssd (word-level)ssd (grammar-level)unsupervised (with nonparallel data)two-stage (cao et al., 2020)wmdsamples (cao et al., 2020)ssd-samples (word-level)ssd-samples (grammar-level)unsupervisedcross-domain zero shotgenovernightsynth-seq2seqsynthpara-seq2seqssd (word-level)ssd (grammar-level).
bas.
blo.
cal.
hou.
pub.
rec.
res.
soc.
avg..85.286.288.287.587.286.286.2.
64.731.971.771.3.
-15.616.128.468.368.8.
58.160.261.463.765.764.964.9.
53.429.058.758.8.
28.327.723.637.354.958.1.
78.079.881.579.880.481.781.7.
58.336.160.160.6.
53.617.316.133.951.256.5.
71.471.474.173.075.772.772.7.
59.347.961.762.2.
52.445.930.238.155.056.1.
76.478.980.781.480.182.382.3.
60.334.257.658.8.
55.346.736.639.154.757.8.
79.684.782.981.586.181.781.7.
68.141.064.365.4.
60.226.326.941.760.259.3.
76.281.680.781.682.881.581.5.
73.253.870.971.1.
61.761.343.162.765.466.9.
81.482.982.183.082.782.782.7.
48.435.846.049.1.
-9.79.223.333.637.1.
75.878.279.078.980.179.279.0.
60.738.761.462.2.
-31.325.238.155.457.6.table 1: overall results on overnight..to chinese then translate back) is used to obtainexternal paraphrases data.
on average, we sam-ple 423 cus per domain, and synthesize 847 in-stances per domain in self paras and 1252 in ex-ternal paras..unsupervised settingsin unsupervised set-tings, we do not use any annotated semanticparsing data.
the paraphrase generation mod-els are ﬁxed after the paraphrasing pre-trainingand the adaptive ﬁne-tuning.
the models are em-ployed to generate canonical utterances and mrssynchronously via rule-level or word-level infer-ence.
in rule-level inference, the leftmost non-terminators are eliminated by cyclically expandedand the maximum depth k is set to 5, the beamsize is set to 20. ssd uses t5 as the pre-trainedlanguage model in all the proposed components,including adaptive ﬁne-tuning, reranking and thetwo decoding constraints.
ablation experimentsare conducted over all components with rule-levelinference..unsupervised settings (with external nonparal-lel data) cao et al.
(2020) have shown that exter-nal nonparallel data (including nonparallel naturallanguage utterances and canonical utterances) canbe used to build unsupervised semantic parsers.
for fair comparison, we also conduct unsuper-vised experiments with external unparallel data.
speciﬁcally, we enhance the original ssd usingthe samples methods (cao et al., 2020): we label.
each input sentences with the most possible out-puts in the nonparallel corpus and use these sam-ples as peusdo training data – we denote this set-ting as ssd-samples..supervised settings our ssd method can befurther enhanced using annotated training in-stances.
speciﬁcally, given the annotated(cid:104)utterance, logical form(cid:105) instances, we ﬁrst trans-form logical form to its canonical form, then usethem to further ﬁne-tune our paraphrase modelsafter unsupervised pre-training..baselines we compare our method with the fol-lowing unsupervised baselines: 1) cross-domainzero shot(herzig and berant, 2018), which trainson other source domains and then generalizesto target domains in overnight and 2) gen-overnight(wang et al., 2015) in which modelsare trained on synthesized (cid:104)cu, mr(cid:105) pairs; 3) wealso implement seq2seq baseline on the synthe-sized data as synth-seq2seq.
4) synthpara-seq2seq is trained on the synthesized data and(cid:104)cu paraphrase, mr(cid:105) pairs, the paraphrases areobtained in the same way in section 4..6.2 experimental results.
6.2.1 overall results.
the overall results of different baselines and ourmethod are shown in table 1 and table 3 (we alsodemonstrate several cases in appendix).
for our.
5115bas.
blo.
cal.
hou.
pub.
rec.
res.
soc..68.8.
58.1.
56.5.
56.1.
57.8.
59.3.
66.9.
37.1.
57.6.overn.
avg..geogranno58.5.geo(funql)63.2.completemodelconstraints.
- semantic- grammar.
adaptive fine-tuning- external paras- paras (only cus)- fine-tuning.
reranking.
65.563.9.
54.452.1.
52.447.6.
51.950.3.
56.545.3.
57.452.3.
62.056.3.
35.729.9.
66.864.563.9.
56.655.453.6.
51.650.048.4.
54.051.347.6.
48.447.844.9.
56.955.653.2.
63.362.062.5.
32.431.731.4.
- rerankingoracle (r@20).
58.271.2pretrained language models67.058.3.gpt-2rand.
57.283.2.
56.386.9.
50.276.9.
55.782.7.
58.086.9.
62.988.1.
37.762.8.
54.648.6.
53.745.8.
55.747.6.
56.150.3.
58.955.6.
66.454.5.
32.730.3.table 2: albation results of our model with different settings on the three datasets..54.549.7.
53.852.350.7.
54.679.8.
55.648.9.
56.550.7.
56.554.351.8.
57.270.9.
58.351.4.
61.153.9.
61.159.655.4.
62.581.4.
62.154.3.geogranno.
geo(funql).
unsupervised (with nonparallel data).
supervised.
depht (jie and lu, 2018)copynet (herzig and berant, 2019)one-stage (cao et al., 2020)two-stage (cao et al., 2020)seq2seq (guo et al., 2020)ssd (word-level)ssd (grammar-level).
two-stage (cao et al., 2020)wmdsamples (cao et al., 2020)ssd-samples (word-level)ssd-samples (grammar-level).
unsupervised.
synth-seq2seqsynthpara-seq2seqssd (word-level)ssd (grammar-level).
table 3: overallgeo(funql)..-72.071.971.6-72.972.0.
63.735.364.064.4.
32.741.457.258.5.
89.3---87.188.387.9.
--64.365.0.
36.145.462.863.2.results on geogranno and.
method, we report its performances on three set-tings.
we can see that:.
1. by synchronously decoding canonicalutterances and meaning representations, ssdachieves competitive unsupervised semanticparsing performance.
in all datasets, our methodoutperforms other baselines in the unsupervisedsettings.
these results demonstrate that unsu-pervised semantic parsers can be effectively builtby simultaneously exploit semantic and structuralconstraints, without the need of labeled data..2. our model can achieve competitive per-formance on different datasets with differ-ent settings.
in supervised settings, our modelcan achieve competitive performance with sota.
with nonparallel data, our model can outperformtwo-stage.
on geo(funql) our model also ob-.
tains a signiﬁcant improvement compared withbaselines, which also veriﬁes that our method isnot limited to speciﬁc datasets (i.e., overnightand geogranno, which are constructed withscfg and paraphrasing.).
3. both rule-level inference and word-levelinference can effectively generate paraphrasesunder the grammar constraints.
the rule-levelinference can achieve better performance, we be-lieve this is because rule-level inference is morecompact than word-level inference, therefore therule-level inference can search wider space andbeneﬁt beam search more..6.2.2 detailed analysis.
effect of decoding constraints to analyze theeffect of decoding constraints, we conduct ab-lation experiments with different constraint set--tings and the results are shown in table 2:semantic denotes removing the semantic con-straint, -grammar denotes all constraints are re-moved at the same time, the decoding is unre-stricted.
we can see that the constrained decod-ing is critical for our paraphrasing-based semanticparsing, and both grammar constraints and seman-tic constraints contribute to the improvement..effect of adaptive fine-tuning to analyze theeffect of adaptive ﬁne-tuning, we show the re-sults with different settings by ablating a ﬁne-tuning corpus at a time (see table 2).
we cansee that adaptive ﬁne-tuning can signiﬁcantly im-prove the performance.
and the paraphrase gen-eration model can be effectively ﬁne-tuned onlyusing cus or self paras, which can be easily con-structed..5116semantic parsing is also proposed(chen et al.,2018a).
such as variational auto-encoding (yinet al., 2018), dual learning framework for seman-tic parsing (cao et al., 2019), dual informationmaximization method (ye et al., 2019), and back-translation (sun et al., 2019).
one other strat-egy is to generate data for semantic parsing, e.g.,wang et al.
(2015) construct a semantic parsingdataset from grammar rules and crowdsourcingparaphrase.
guo et al.
(2018) produce pseudo-labeled data.
jia and liang (2016) create new“recombinant” training examples with scfg.
thedomain transfer techniques are also used to reducethe cost of data collecting for the unseen domain(su and yan, 2017; herzig and berant, 2018; luet al., 2019; zhong et al., 2020).
goldwasser et al.
(2011); poon and domingos (2009); schmitt et al.
(2020) leverage external resources or techniquesfor unsupervised learning..constrained decoding.
after neural parsersmodel semantic parsing as a sentence to logicalform translation task (yih et al., 2015; krishna-murthy et al., 2017; iyyer et al., 2017; jie and lu,2018; lindemann et al., 2020), many constraineddecoding algorithms are also proposed, such astype constraint-based illegal token ﬁltering (kr-ishnamurthy et al., 2017); lisp interpreter-basedmethod (liang et al., 2017); type constraints forgenerating valid actions (iyyer et al., 2017)..paraphrasing in semantic parsing.
para-phrase models have been widely used in semanticparsing.
parasempre (berant and liang, 2014)use paraphrase model to rerank candidate logicalforms.
wang et al.
(2015) employ scfg grammarrules to produce mr and canonical utterancepairs, and construct overnight dataset byparaphrasing utterances.
dong et al.
(2017) useparaphrasing to expand the expressions of querysentences.
compared with these methods, wecombine paraphrasing with grammar-constrainedreducedecoding,the requirement oflabeled data and achieveunsupervised semantic parsing..therefore ssd can further.
8 conclusions.
we propose an unsupervised semantic parsingmethod – synchronous semantic decoding, whichleverages paraphrasing and grammar-constraineddecoding to simultaneously resolve the semanticgap and the structure gap.
speciﬁcally, we design.
figure 4: semi-supervised results of different ratios oflabeled data on overnight..effect of reranking to analyze the effect ofreranking, we compare the settings with/withoutreranking and its upper bound – oracle, whichcan always select the correct logical form if it iswithin the beam.
experimental results show thatreranking can improve the semantic parsing per-formance.
moreover, there is still a large marginbetween our method and oracle, i.e., the unsuper-vised semantic parsing can be signiﬁcantly pro-moted by designing better reranking algorithms..effect of adding labeled data to investigatethe effect of adding labeled data, we test ourmethod by varying the size of the labeled data onovernight from 0% to 100%.
in fig.
4, we cansee that our method can outperform baselines us-ing the same labeled data.
and a small amountof data can produce a good performance using ourmethod..effect of pretrained language models to an-alyze the effect of plms, we show the results withdifferent plm settings: instead of t5 we use gpt-2 or randomly initialized transformers to constructparaphrasing models.
experimental results showthat powerful plms can improve the performance.
powered by the language generation models to dosemantic parsing, our method can beneﬁt from therapid development of plms..7 related work.
data scarcity in semantic parsing.
witnessedthe labeled data bottleneck problem, many tech-niques have been proposed to reduce the demandfor labeled logical forms.
many weakly super-vised learning are proposed (artzi and zettle-moyer, 2013; berant et al., 2013; reddy et al.,2014; agrawal et al., 2019; chen et al., 2020),such as denotation-base learning (pasupat andliang, 2016; goldman et al., 2018),iterativesearching (dasigi et al., 2019).
semi-supervised.
511701020304050607080900510153050100accuracy (%)ssdseq2seqsyth-seq2seq01020304050607080900510153050100accuracy (%)ssdseq2seqsythpara-seq2seqtwo synchronous semantic decoding algorithmsfor paraphrasing under grammar constraints, andexploit adaptive ﬁne-tuning and utterance rerank-ing to alleviate the style bias in semantic parsing.
experimental results show that our approach canachieve competitive performance in unsupervisedsettings..acknowledgments.
we sincerely thank the reviewers for their insight-ful comments and valuable suggestions.
more-over, this work is supported by the national keyresearch and development program of china(no.
2020aaa0106400), the national natural sciencefoundation of china under grants no.
61906182and 62076233, and in part by the youth innovationpromotion association cas(2018141)..references.
priyanka agrawal, ayushi dalmia, parag jain, ab-hishek bansal, ashish r. mittal, and karthiksankaranarayanan.
2019. uniﬁed semantic pars-in proceedings of theing with weak supervision.
57th conference of the association for computa-tional linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages4801–4810..yoav artzi and luke zettlemoyer.
2013. weakly su-pervised learning of semantic parsers for mappinginstructions to actions.
tacl, 1:49–62..jonathan berant, andrew chou, roy frostig, and percyliang.
2013. semantic parsing on freebase fromquestion-answer pairs.
in proceedings of the 2013conference on empirical methods in natural lan-guage processing, emnlp 2013, 18-21 october2013, grand hyatt seattle, seattle, washington,usa, a meeting of sigdat, a special interest groupof the acl, pages 1533–1544..of the association for computational linguistics,acl 2020, online, july 5-10, 2020, pages 6806–6817. association for computational linguistics..bo chen, bo an, le sun, and xianpei han.
2018a.
semi-supervised lexicon learning for wide-coveragesemantic parsing.
in proceedings of the 27th inter-national conference on computational linguistics,coling 2018, santa fe, new mexico, usa, august20-26, 2018, pages 892–904.
association for com-putational linguistics..bo chen, xianpei han, ben he, and le sun.
2020.learning to map frequent phrases to sub-structuresof meaning representation for neural semantic pars-ing.
in the thirty-fourth aaai conference on ar-tiﬁcial intelligence, aaai 2020, the thirty-secondinnovative applications of artiﬁcial intelligenceconference, iaai 2020, the tenth aaai symposiumon educational advances in artiﬁcial intelligence,eaai 2020, new york, ny, usa, february 7-12,2020, pages 7546–7553.
aaai press..bo chen, le sun, and xianpei han.
2018b.
sequence-to-action: end-to-end semantic graph generation forin proceedings of the 56th an-semantic parsing.
nual meeting of the association for computationallinguistics, acl 2018, melbourne, australia, july15-20, 2018, volume 1: long papers, pages 766–777..pradeep dasigi, matt gardner, shikhar murty, lukezettlemoyer, and eduard h. hovy.
2019.iterativesearch for weakly supervised semantic parsing.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,naacl-hlt 2019, minneapolis, mn, usa, june 2-7, 2019, volume 1 (long and short papers), pages2669–2680..li dong and mirella lapata.
2016. language to logi-cal form with neural attention.
in proceedings of the54th annual meeting of the association for compu-tational linguistics, acl 2016, august 7-12, 2016,berlin, germany, volume 1: long papers..jonathan berant and percy liang.
2014. semanticin proceedings of theparsing via paraphrasing.
52nd annual meeting of the association for com-putational linguistics, acl 2014, june 22-27, 2014,baltimore, md, usa, volume 1: long papers, pages1415–1425..li dong, jonathan mallinson, siva reddy, and mirellalapata.
2017. learning to paraphrase for ques-in proceedings of the 2017 con-tion answering.
ference on empirical methods in natural languageprocessing, emnlp 2017, copenhagen, denmark,september 9-11, 2017, pages 875–886..ruisheng cao, su zhu, chen liu, jieyu li, and kai yu.
2019. semantic parsing with dual learning.
in pro-ceedings of the 57th conference of the associationfor computational linguistics, acl 2019, florence,italy, july 28- august 2, 2019, volume 1: long pa-pers, pages 51–64..ruisheng cao, su zhu, chenyu yang, chen liu, raoma, yanbin zhao, lu chen, and kai yu.
2020. un-supervised dual paraphrasing for two-stage semanticparsing.
in proceedings of the 58th annual meeting.
omer goldman, veronica latcinnik, ehud nave, amirgloberson, and jonathan berant.
2018. weakly su-pervised semantic parsing with abstract examples.
in proceedings of the 56th annual meeting of the as-sociation for computational linguistics, acl 2018,melbourne, australia, july 15-20, 2018, volume 1:long papers, pages 1809–1819..dan goldwasser, roi reichart, james clarke, and danroth.
2011. conﬁdence driven unsupervised seman-tic parsing.
in the 49th annual meeting of the asso-.
5118ciation for computational linguistics: human lan-guage technologies, proceedings of the conference,19-24 june, 2011, portland, oregon, usa, pages1486–1495..daya guo, yibo sun, duyu tang, nan duan, jian yin,hong chi, james cao, peng chen, and ming zhou.
2018. question generation from sql queries im-proves neural semantic parsing.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, brussels, belgium, oc-tober 31 - november 4, 2018, pages 1597–1607..jiaqi guo, qian liu, jian-guang lou, zhenwen li,xueqing liu, tao xie, and ting liu.
2020. bench-marking meaning representations in neural semanticparsing.
in emnlp..jonathan herzig and jonathan berant.
2018. decou-pling structure and lexicon for zero-shot semanticparsing.
in proceedings of the 2018 conference onempirical methods in natural language process-ing, brussels, belgium, october 31 - november 4,2018, pages 1619–1629..jonathan herzig and jonathan berant.
2019. don’tparaphrase, detect!
rapid and effective data collec-in proceedings of thetion for semantic parsing.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing,emnlp-ijcnlp 2019, hong kong, china, novem-ber 3-7, 2019, pages 3808–3818.
association forcomputational linguistics..mohit iyyer, wen-tau yih, and ming-wei chang.
2017.search-based neural structured learning for sequen-tial question answering.
in proceedings of the 55thannual meeting of the association for computa-tional linguistics, acl 2017, vancouver, canada,july 30 - august 4, volume 1: long papers, pages1821–1831..robin jia and percy liang.
2016. data recombinationfor neural semantic parsing.
in proceedings of the54th annual meeting of the association for compu-tational linguistics, acl 2016, august 7-12, 2016,berlin, germany, volume 1: long papers..zhanming jie and wei lu.
2018. dependency-basedhybrid trees for semantic parsing.
in proceedings ofthe 2018 conference on empirical methods in natu-ral language processing, brussels, belgium, octo-ber 31 - november 4, 2018, pages 2431–2441..rohit j. kate, yuk wah wong, and raymond j.mooney.
2005. learning to transform natural to for-mal languages.
in proceedings, the twentieth na-tional conference on artiﬁcial intelligence and theseventeenth innovative applications of artiﬁcial in-telligence conference, july 9-13, 2005, pittsburgh,pennsylvania, usa, pages 1062–1068..jayant krishnamurthy, pradeep dasigi, and matt gard-ner.
2017. neural semantic parsing with type con-straints for semi-structured tables.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing, emnlp 2017, copen-hagen, denmark, september 9-11, 2017, pages1516–1526..zichao li, xin jiang, lifeng shang, and qun liu.
2019. decomposable neural paraphrase generation.
in proceedings of the 57th conference of the as-sociation for computational linguistics, acl 2019,florence, italy, july 28- august 2, 2019, volume 1:long papers, pages 3403–3414..chen liang, jonathan berant, quoc le, kenneth d.forbus, and ni lao.
2017. neural symbolic ma-chines: learning semantic parsers on freebase within proceedings of the 55th an-weak supervision.
nual meeting of the association for computationallinguistics, acl 2017, vancouver, canada, july 30- august 4, volume 1: long papers, pages 23–33..matthias lindemann, jonas groschwitz, and alexan-der koller.
2020. fast semantic parsing with well-in proceedings of the 2020typedness guarantees.
conference on empirical methods in natural lan-guage processing, emnlp 2020, online, novem-ber 16-20, 2020, pages 3929–3951.
association forcomputational linguistics..wei lu, hwee tou ng, wee sun lee, and luke s.zettlemoyer.
2008. a generative model for pars-ing natural language to meaning representations.
in2008 conference on empirical methods in naturallanguage processing, emnlp 2008, proceedingsof the conference, 25-27 october 2008, honolulu,hawaii, usa, a meeting of sigdat, a special in-terest group of the acl, pages 783–792..zhichu lu, forough arabshahi, igor labutov, andtom m. mitchell.
2019. look-up and adapt: a one-shot semantic parser.
corr, abs/1910.12197..alana marzoev, samuel madden, m. frans kaashoek,michael j. cafarella, and jacob andreas.
2020. un-natural language processing: bridging the gap be-tween synthetic and natural language data.
corr,abs/2004.13645..panupong pasupat and percy liang.
2016..inferringin proceedingslogical forms from denotations.
of the 54th annual meeting of the association forcomputational linguistics, acl 2016, august 7-12,2016, berlin, germany, volume 1: long papers..hoifung poon and pedro m. domingos.
2009. unsu-in proceedings of thepervised semantic parsing.
2009 conference on empirical methods in natu-ral language processing, emnlp 2009, 6-7 august2009, singapore, a meeting of sigdat, a specialinterest group of the acl, pages 1–10..donald e. knuth.
1965. on the translation of lan-inf.
control., 8(6):607–.
guages from left to right.
639..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..5119colin raffel, noam shazeer, adam roberts, katherinelee, sharan narang, michael matena, yanqi zhou,wei li, and peter j. liu.
2020. exploring the limitsof transfer learning with a uniﬁed text-to-text trans-former.
j. mach.
learn.
res., 21:140:1–140:67..siva reddy, mirella lapata, and mark steedman.
2014.large-scale semantic parsing without question-answer pairs.
transactions of the association forcomputational linguistics, 2:377–392..martin schmitt, sahand sharifzadeh, volker tresp, andhinrich sch¨utze.
2020. an unsupervised joint sys-tem for text generation from knowledge graphs andsemantic parsing.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing, emnlp 2020, online, november 16-20,2020, pages 7117–7130.
association for computa-tional linguistics..bo shao, yeyun gong, weizhen qi, nan duan, andxiaola lin.
2020. multi-level alignment pretrainingfor multi-lingual semantic parsing.
in proceedingsof the 28th international conference on computa-tional linguistics, coling 2020, barcelona, spain(online), december 8-13, 2020, pages 3246–3256.
international committee on computational linguis-tics..yu su and xifeng yan.
2017. cross-domain seman-tic parsing via paraphrasing.
in proceedings of the2017 conference on empirical methods in naturallanguage processing, emnlp 2017, copenhagen,denmark, september 9-11, 2017, pages 1235–1246..yibo sun, duyu tang, nan duan, yeyun gong, xi-aocheng feng, bing qin, and daxin jiang.
2019.neural semantic parsing in low-resource settingswith back-translation and meta-learning.
corr,abs/1909.05438..yushi wang, jonathan berant, and percy liang.
2015.building a semantic parser overnight.
in proceed-ings of the 53rd annual meeting of the associationfor computational linguistics and the 7th interna-tional joint conference on natural language pro-cessing of the asian federation of natural languageprocessing, acl 2015, july 26-31, 2015, beijing,china, volume 1: long papers, pages 1332–1342..yuk wah wong and raymond j. mooney.
2007. learn-ing synchronous grammars for semantic parsingwith lambda calculus.
in acl 2007, proceedings ofthe 45th annual meeting of the association for com-putational linguistics, june 23-30, 2007, prague,czech republic..chunyang xiao, marc dymetman, and claire gardent.
2016. sequence-based structured prediction for se-mantic parsing.
in proceedings of the 54th annualmeeting of the association for computational lin-guistics, acl 2016, august 7-12, 2016, berlin, ger-many, volume 1: long papers..dongqin xu, junhui li, muhua zhu, min zhang, andimproving amr parsingguodong zhou.
2020a.
in pro-with sequence-to-sequence pre-training.
ceedings of the 2020 conference on empirical meth-ods in natural language processing, emnlp 2020,online, november 16-20, 2020, pages 2501–2511.
association for computational linguistics..silei xu, sina j. semnani, giovanni campagna, andmonica s. lam.
2020b.
autoqa: from databasesto qa semantic parsers with only synthetic train-ing data.
in proceedings of the 2020 conference onempirical methods in natural language process-ing, emnlp 2020, online, november 16-20, 2020,pages 422–434.
association for computational lin-guistics..hai ye, wenjie li, and lu wang.
2019. jointly learn-ing semantic parser and natural language generatorvia dual information maximization.
in proceedingsof the 57th conference of the association for com-putational linguistics, acl 2019, florence, italy,july 28- august 2, 2019, volume 1: long papers,pages 2090–2101..wen-tau yih, ming-wei chang, xiaodong he, andjianfeng gao.
2015. semantic parsing via stagedquery graph generation: question answering withknowledge base.
in proceedings of the 53rd annualmeeting of the association for computational lin-guistics and the 7th international joint conferenceon natural language processing of the asian fed-eration of natural language processing, acl 2015,july 26-31, 2015, beijing, china, volume 1: longpapers, pages 1321–1331..pengcheng yin and graham neubig.
2017. a syntacticneural model for general-purpose code generation.
in acl 2017, pages 440–450.
acl 2017..pengcheng yin and graham neubig.
2019. rerank-ing for neural semantic parsing.
in proceedings ofthe 57th conference of the association for compu-tational linguistics, acl 2019, florence, italy, july28- august 2, 2019, volume 1: long papers, pages4553–4559..pengcheng yin, chunting zhou, junxian he, and gra-ham neubig.
2018. structvae: tree-structured latentvariable models for semi-supervised semantic pars-ing.
in proceedings of the 56th annual meeting ofthe association for computational linguistics, acl2018, melbourne, australia, july 15-20, 2018, vol-ume 1: long papers, pages 754–765..tao yu, chien-sheng wu, xi victoria lin, bailinwang, yi chern tan, xinyi yang, dragomir r.radev, richard socher, and caiming xiong.
2020.grappa: grammar-augmented pre-training for tablesemantic parsing.
corr, abs/2009.13845..john m. zelle and raymond j. mooney.
1996. learn-ing to parse database queries using inductive logicin proceedings of the thirteenthprogramming.
national conference on artiﬁcial intelligence and.
5120dataset, we can still efﬁciently generate the utter-ances containing it, which shows our constrained-paraphrasing based semantic parser has the gener-alization ability on unseen words.
we found thatthe parser maintains high recall, covering the cor-rect canonical utterances in our n-best list of pre-dictions.
as case 2 shows the designed utterancereranking score can select the best canonical ut-terances by focusing on coherence and adequacy.
with adaptive ﬁne-tuning (case 3), our model cangenerate the utterances focusing more on seman-tics to alleviate the style bias..eighth innovative applications of artiﬁcial intelli-gence conference, aaai 96, iaai 96, portland, ore-gon, usa, august 4-8, 1996, volume 2., pages 1050–1055..luke s. zettlemoyer and michael collins.
2005.learning to map sentences to logical form: struc-tured classiﬁcation with probabilistic categorialgrammars.
in uai ’05, proceedings of the 21st con-ference in uncertainty in artiﬁcial intelligence, ed-inburgh, scotland, july 26-29, 2005, pages 658–666..yuanyuan zhao, weiwei sun, junjie cao, and xiaojunwan.
2020. semantic parsing for english as a sec-in proceedings of the 58th annualond language.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages6783–6794.
association for computational linguis-tics..victor zhong, mike lewis, sida i. wang, and lukezettlemoyer.
2020. grounded adaptation for zero-shot executable semantic parsing.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing, emnlp 2020, online,november 16-20, 2020, pages 6869–6882.
associa-tion for computational linguistics..a appendix.
a.1 case study.
in table 4, we present the cases generated fromssd.
cases show that ssd can output semantics-similar and grammar-legal utterances.
in casein paraphrase1, “take-out” does not appear.
cases.
x → clogp (c|x).
x ← csrec(x, c).
x ↔ csasso(x, c).
overallrerankingscore.
input: restaurants that accept credit cards and offer takeoutoutputs: restaurant that takes credit cards and that has take-outoutputs: restaurant that has take-out and that takes credit cardsoutputs: restaurant that takes credit cardsoutputs: restaurant that takes credit cards and that takes credit cards.
input: meetings held in the same place as the weekly standup meetingoutputs: meeting whose date is date of weekly standupoutputs: meeting whose location is location of weekly standupoutputs: meeting whose location is location that is location of weekly standupoutputs: meeting whose date is at most date of weekly standup.
input: meetings held in the same place as the weekly standup meetingoutputs: meeting whose location is location of weekly standupoutputs: meeting whose location is location that is location of weekly standupoutputs: location that is location of weekly standupoutputs: meeting whose date is date of weekly standup.
-54.2-72.0-77.2-84.2.
-62.2-62.7-65.0-67.2.
-2.7-6.0-18.0-31.2.
-3.1-8.8-22.4-26.7.
-40.2-22.1-21.1-35.8.
-22.1-21.1-32.6-40.2.
-20.3-20.3-31.9-28.1.
-67.1-62.4-63.5-73.3.
-62.4-63.5-62.8-67.1.
-77.6-101.1-131.5-139.0.
-169.5-147.2149.6-176.3.
-86.2-90.6-113.4-138.5.table 4: output cases from ssd on overnight.
the outputs are sorted by the generation scorelogp (c|x).
the underlined canonical utterances are correct.
adaptive ﬁne-tuning is employed in thethird case, and not employed in the ﬁrst two cases.
accuracies on overnight.
in genovernightwang et al.
(2015), all the canonical utterances are also generated without manual annotation.
theprevious methods with superscript ∗ means they use different unsupervised settings..5121