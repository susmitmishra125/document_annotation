assessing the representations of idiomaticity in vector models with anoun compound dataset labeled at type and token levelsmarcos garcia∗1, tiago kramer vieira∗2,carolina scarton3, marco a. p. idiart4, aline villavicencio2,31 citius research centre, universidade de santiago de compostela, galiza (spain)2 institute of informatics, federal university of rio grande do sul (brazil)3 department of computer science, university of shefﬁeld (uk)4 institute of physics, federal university of rio grande do sul (brazil)marcos.garcia.gonzalez@udc.gal, tiagokv@hotmail.com,{c.scarton, a.villavicencio}@sheffield.ac.uk,marco.idiart@gmail.com.
abstract.
accurate assessment of the ability of embed-ding models to capture idiomaticity may re-quire evaluation at token rather than type level,to account for degrees of idiomaticity and pos-sible ambiguity between literal and idiomaticusages.
however, most existing resourceswith annotation of idiomaticity include ratingsonly at type level.
this paper presents thenoun compound type and token idiomatic-ity (nctti) dataset, with human annotationsfor 280 noun compounds in english and 180in portuguese at both type and token level.
we compiled 8,725 and 5,091 token level an-notations for english and portuguese, respec-tively, which are strongly correlated with thecorresponding scores obtained at type level.
the nctti dataset is used to explore howvector space models reﬂect the variability ofseveral ex-idiomaticity across sentences.
periments using state-of-the-art contextualisedmodels suggest that their representations arenot capturing the noun compounds idiomatic-ity as human annotators.
this new multilin-gual resource also contains suggestions forparaphrases of the noun compounds both attype and token levels, with uses for lexical sub-stitution or disambiguation in context..1.introduction.
multiword expressions (mwes) such as nouncompounds (ncs), have been considered a chal-lenge for nlp (sag et al., 2002).
this is partly dueto the wide range of idiomaticity that they display,from more literal to idiomatic combinations (oliveoil vs. shrinking violet).
the task of identifying thedegree of idiomaticity of mwes has been investi-gated at type level, to determine the potential of anmwe to be idiomatic in general.
some of theseapproaches are based on the assumption that the.
* equal contribution..distance between the representation of an mwe asa unit and the representation of the compositionalcombination of its components is an indication ofthe degree of idiomaticity: they are closer if themwe is more compositional.
good performancesare obtained even with non-contextualised wordembeddings like word2vec (mikolov et al., 2013),and vector operations like addition and multipli-cation (mitchell and lapata, 2010; reddy et al.,2011; cordeiro et al., 2019).
additionally, forsome mwes, there is a potential ambiguity be-tween an idiomatic and a literal sense, like in thepotentially idiomatic mwe brass ring which canbe ambiguous between the more literal meaning aring made of brass and the more idiomatic senseof a prize.
considering that these mwes can haveboth idiomatic and literal senses, a related task oftoken-level identiﬁcation evaluates whether in aparticular context an mwe is idiomatic or not.
forthis task, models that incorporate the context inwhich an mwe occurs tend to be better equippedto distinguish idiomatic from literal occurrences(sporleder and li, 2009; king and cook, 2018;salton et al., 2016)..contextualised embedding models, like bert(devlin et al., 2019), brought signiﬁcant advancesto a variety of downstream tasks (e.g.
zhu et al.
(2020) for machine translation and jiang andde marneffe (2019) for natural language inference).
they also seem to beneﬁt tasks like idiomatic-ity and metaphor identiﬁcation (gao et al., 2018),since their interpretation is often dependent on con-textual clues.
nonetheless, previous work foundthat non-contextualised models seem to still bringinformative clues for these tasks (king and cook,2018), and their combination with contextualisedmodels could improve results (e.g.
for metaphoridentiﬁcation (mao et al., 2019)).
this comple-mentarity between non-contextualised and contex-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2730–2741august1–6,2021.©2021associationforcomputationallinguistics2730tualised models may be an indication that enoughcore idiomatic information may already be avail-able at type level.
moreover, type-based compo-sitionality prediction measures that perform wellwith static embeddings may also perform well fortoken-based prediction with contextualised models.
to address these questions, in this paper, wepresent the noun compound type and token id-iomaticity (nctti) dataset, containing 280 ncs inenglish and 180 in portuguese, annotated with thedegree of idiomaticity perceived by human anno-tators, at type and token level.1 nctti contains atotal of 8,725 annotations in 840 different sentencesin english, and 5,091 annotations in 540 sentencesin portuguese.
moreover, nctti has several para-phrases for each nc which are classiﬁed as eithertype level or token level equivalents.
to control forthe level of idiomaticity, the nctti dataset has abalanced amount of compositional, partly compo-sitional and idiomatic items.
as the importance ofcontext to determine interpretation may be relatedto factors like the degree of idiomaticity, associa-tion strength or the frequency of an nc, we presentan illustrative analysis of their impact for the perfor-mance of different models in capturing idiomaticity.
we also examine how the performance obtainedfor human idiomaticity judgments per type differsfrom the performance obtained per token..our contributions can be summarised as: (1)building the nctti dataset with information abouttype and token idiomaticity for ncs in two lan-guages, (2) evaluating to what extent models areable to detect idiomaticity at type and token level,analysing different levels of contextualisation and(3) proposing two new measures of idiomaticity.
moreover, the paraphrases provided for each nc attype and token level make nctti a useful resourcefor enhancing paraphrase datasets (e.g.
ppdb(ganitkevitch et al., 2013)), for tasks involving lex-ical substitution (mccarthy and navigli, 2007; mi-halcea et al., 2010), or for improving the results ofdownstream tasks, such as text simpliﬁcation (paet-zold, 2016; alva-manchego et al., 2020).
suchparaphrases may also be useful for improving thetask of machine translation, avoiding the need forparallel mwe corpora (zaninello and birch, 2020).
section 2 gives an overview of existing id-iomaticity datasets.
section 3 presents the ncttidataset and the annotations, and section 4 discusses.
the evaluation of the performance of different wordembeddings in detecting idiomaticity..2 related work.
datasets with type-level annotations are availablefor ncs in english (farahmand et al., 2015; reddyet al., 2011; ramisch et al., 2016; kruszewski andbaroni, 2014), german (roller et al., 2013; schulteim walde et al., 2016), french (cordeiro et al.,2019) and portuguese (cordeiro et al., 2019).
how-ever, datasets with idiomatic information at tokenlevel are scarce, e.g., the vnc-tokens (cook et al.,2008), containing almost 3k annotations for 53verb-noun combinations in english..regarding the use of contextualised embeddingsto model idiomaticity, nandakumar et al.
(2019)compared different static and contextualised em-beddings to predict the ncs compositionality, ob-taining better results with static vectors learnt indi-vidually for each nc.
shwartz and dagan (2019)train various classiﬁers initialised with static andcontextualised embeddings for different composi-tional tasks, achieving the best results with bertembeddings.
yu and ettinger (2020), using par-tially idiomatic expressions of the bird dataset(asaadi et al., 2019), show that contextualised em-beddings from language models heavily rely onword content, missing additional information pro-vided by compositional operations..in this paper we take advantage of the ncttidataset to observe whether vector representationsobtained with different strategies correlate withhuman annotations at both type and token levels..3 the noun compound type and token.
idiomaticity dataset.
this section describes the procedure to create thenctti dataset and its main characteristics.2.
3.1 source data.
we used as basis the english and portuguese sub-sets of the nc compositionality dataset (cordeiroet al., 2019), which contain compositionality scoresfor 280 two-word ncs in english (90 of whichcame from reddy et al.
(2011)), and 180 in por-tuguese, all of them labeled at type level: i.e., theannotators provided a compositionality value fora compound (from 0 –fully idiomatic– to 5, fully.
1type level annotations come from cordeiro et al.
(2019),.
the dataset used as source for the nctti..2the nccti dataset can be downloaded from the follow-.
ing url: https://github.com/marcospln/nctti..2731compositional) after reading various sentences withthis nc..to obtain more ﬁne-grained compatible token-level annotations about the impact of different con-texts in the interpretation of ncs, we used the sameoriginal sentences as in the source dataset (threesentences per compound with the same sense wereselected from reddy et al.
(2011) dataset).3.language experts classiﬁed each noun com-pound regarding their semantic compositionalityas idiomatic (e.g., gravy train), partially idiomatic(e.g., grandfather clock), or compositional (e.g.,research project).
for english, this resulted in 103,88, and 89 idiomatic, partially idiomatic, and com-positional compounds.
for portuguese, each classhas 60 compounds, as the selection had been bal-anced when the source dataset was created..3.2 annotation procedure.
we used the same protocol as reddy et al.
(2011)and cordeiro et al.
(2019), asking each participantto give 0 to 5 scores for an nc and its componentsin a speciﬁc sentence (e.g., glass ceiling in “womenare continuing to slowly break through the glassceiling of uk business [.
.
.
]”).
in particular, weasked participants for: (i) the contribution of thehead to the meaning of the nc (e.g., is a glassceiling literally a ceiling?
); (ii) the contributionof the modiﬁer to the meaning of the nc (e.g.,is a glass ceiling literally of glass?
); and (iii) thedegree of compositionality of the compound (i.e.,to what extent the meaning of the nc can be seenas a combination of its parts).
additionally, weasked for up to three synonyms of the nc in thatparticular sentence (e.g., synonyms at token level).
we used amazon mechanical turk to obtainthe annotations for english, and a dedicated onlineplatform for the questionnaire in portuguese,4 aswe could not ﬁnd a suitable number of annotatorsfor this language in amt.5 taking this into account,the numbers of the portuguese annotations are ingeneral lower to those obtained for english..for each language, we have included the threesentences of every compound in the dataset (840sentences in english, and 540 in portuguese),which were randomly submitted to the annotators..3some contexts are spans of tokens instead of sentences,.
but usually enough to interpret the meaning of the nc..4the platform was provided by cordeiro et al.
(2019).
5the annotation process was approved by the ethics com-mittee of the university of shefﬁeld.
this is a thorough evalu-ation process peer-reviewed by three ethical reviewers.
themonetary compensation was deemed appropriate for the task..for english, we compiled at least 10 annotationsper sentence, resulting in 8,725 annotations (10.4annotations per sentence on average).
a total of412 annotators have taken part in the process, andon average, each participant labeled 21 instances.
for portuguese we set the threshold in 5 annota-tions per sentence: we got 5,091 annotations by33 participants, so that each sentence has a meanof 9.4 annotations and each annotator labeled onaverage 154 sentences..3.3 results.
inter-annotator agreement: we computed theinter-annotator agreements for two and three an-notators with the largest number of sentences incommon (table 1).
for english, we obtained krip-pendorff’s α (krippendorff, 2011) values of 0.30for two annotators (199 sentences) and 0.22 forthree annotators (76 sentences).
the α values forportuguese were of 0.52 for two annotators (131sentences) and 0.44 for three annotators (60 sen-tences).
overall, and using the divisions proposedby landis and koch (1977), the agreement resultscan be classiﬁed as ‘fair’ (for english), and ‘mod-erate’ (for portuguese)..data.
ncheadmodiﬁer.
english320.220.300.380.330.420.45.portuguese.
20.520.660.56.
30.440.530.48.table 1: krippendorff’s α inter-annotator agreementfor the nc, head, and modiﬁers for 2 and 3 annotators..english portuguese.
dataallidiomaticpartialcompositional.
0.920.710.780.66.
0.900.820.780.91.table 2: spearman ρ correlations between the averagecompositionality values per compound of the nctti,and the original scores of the nc compositionalitydataset (p < 0.01 in all cases).
all values were calcu-lated with the all compounds for each language, whileidiomatic, partial, and compositional were computedon the three compositionality levels..correlation token vs.then, wecalculated the correlations (spearman ρ) betweenthe average compositionality scores of the nctti.
type scores:.
2732noun compound.
head.
data.
english.
portuguese.
english.
portuguese.
english.
idiom.
partialcomp..mean0.952.344.13.std mean1.520.582.461.013.610.67.std mean1.530.813.340.914.230.94.std mean1.831.373.651.414.200.66.std mean1.691.072.751.034.340.93.modiﬁer.
portuguesestd1.181.150.87.std mean2.021.292.671.263.900.66.table 3: mean compositionality scores for each class in english and portuguese (from 0, fully idiomatic, to 5,fully compositional), and standard deviations.
left columns contain the scores for the whole compound, while thevalues for the head and modiﬁer are in the middle and right columns, respectively.
the type averages for the ncsreported by cordeiro et al.
(2019) are 1.1, 2.4, and 4.2 for english and 1.3, 2.5, and 3.9 for portuguese..dataset and those of the original resource (nc com-positionality dataset).
table 2 contains the correla-tion results for each language and compositionalityclass.
the strong to very strong signiﬁcant corre-lations conﬁrm the robustness between type-leveland token-level human compositionality annota-tions for these two datasets.6.
idiomaticity values: with regards to the id-iomaticity values of each class, table 3 displaysboth the average scores and the standard deviationin both languages.
as expected, for the wholecompounds, partially idiomatic ncs are those withhigher standard deviations, and their mean com-positionality values are in the middle of the scale(2.34 and 2.46).
in english, the results of both id-iomatic and compositional compounds are morehomogeneous, as they are clearly located on themargins of the scale (< 1 and > 4, respectively)with lower deviations.
this is not the case in por-tuguese, where the average values are > 1 and < 4for idiomatic and compositional ncs, respectively,placing even the idiomatic cases closer towards themiddle of the scale.
with respect to the averagevalues for the heads and modiﬁers, we can high-light the following observations: ﬁrst, both headand modiﬁer scores are consistently higher than themeans for the whole compound in every scenarioalso suggesting at least a partial compositionality intheir token occurrences.
second, for idiomatic ncs,the scores of the modiﬁers are higher than those ofthe heads, while for partially compositional ncsthe results are the opposite.7 finally, regarding thecompositional level, the modiﬁer values are higherin english, while in portuguese the heads seem tocontribute more to the meaning of the nc..6removing annotators with low agreement (spearman ρ <.
0.2, and ρ < 0.4) resulted in almost identical correlations..7the results for partially idiomatic compounds are ex-pected to some extent as the head tends to bear more semanticload about the whole expression (e.g., as in collocations)..observing the variability across the annotations,we found some divergence in a few compounds(e.g., brass ring labeled as idiomatic for a compo-sitional occurrence “three drawers, each with abrass ring pull, provide plenty of storage whateveryou use it for.”), which hints at possible interferencefrom a salient meaning (giora, 1999).
however,further investigation is needed..paraphrases:as mentioned, we asked the partic-ipants to provide synonyms or paraphrases for thenoun compounds in each particular context.
in thisrespect, it is worth noting that while some sugges-tions may be applicable across all the sentences foran nc (e.g.
spun sugar for cotton candy, consid-ered as a type level synonym), others are more de-pendent on context and differ for speciﬁc sentences(e.g.
ﬂight recorder and unknown process, for blackbox, which can be considered as token level para-phrases).
we have classiﬁed the paraphrases astype or token level using the following procedure:to organise the large set of paraphrases provided bythe annotators (see below), we performed an auto-matic classiﬁcation as follows: we labeled as typelevel synonyms those paraphrases proposed for thethree sentences of each compound, and those sug-gested for two sentences with a frequency >= 3;token level synonyms are those proposed only forone sentence with a frequency >= 2..in english, 9,690 different paraphrases were pro-posed by the annotators (average 34.60 per nc),and 3,554 were suggested by at least 5 participants(average of 12.70 per nc).
out of them, 1,506 wereclassiﬁed as type level (5.4 synonyms per nc, onaverage), and 353 at token level (0.42 per sentence,1.3 per nc).
overall, 118 ncs have token levelsynonyms for one sentence, 69 for two sentences,and 16 for the three sentences..for portuguese, the annotators suggested a totalof 6,579 paraphrases (314 by at least 5 participants.
2733sentencekeri enjoys music and has turned into a skilled disc jockey.
quality wedding disc jockey equipment comes at a cost.
let one of our high energy disc jockeys entertain your next party.
idiomaticity score at the type-level: 1.25. most common (type-level) paraphrase: dj..record playerbroadcasterannouncer.
mean paraphrase.
1.22.51.7.table 4: annotation example of the english nc disc jockey.
each row includes a sentence with the target nctogether with the mean idiomaticity score and a token-level paraphrase.
bottom row shows the most common(type-level) paraphrase and the mean idiomaticity score from the original dataset (also at the type-level)..and 764 by >= 3, average of 4.2 per nc).
743synonyms were proposed for the 180 compounds(an average of 4.1 per nc), being classiﬁed as typelevel.
concerning token level synonyms, we havecollected 192 synonyms (1.1 per nc, on average).
in this case the total number of annotations waslower, and the ﬁnal resource contains 61 ncs withtoken level synonyms for one sentence, 38 for twosentences, and 6 compounds have token level syn-onyms for the three sentences..the collection of paraphrases included in thenctti make this dataset a valuable resource fordifferent evaluations, such as lexical substitutiontasks and assessments of the performance of em-bedding models to correctly identify contextualisedsynonyms of ncs with different degrees of id-iomaticity..table 4 shows an annotation example for the ncdisc jockey, in english.
it includes the three sen-tences together with the average idiomaticity scoreand both token-level and type-level paraphrases..4 experiments.
this section displays some of the comparative anal-yses for the relevance of type and token annota-tion for idiomaticity detection.
first, we adapt thetype level compositionality prediction approachesused on static word vectors (mitchell and lapata,2010) to contextualised models (nandakumar et al.,2019), here computing the correlation also at tokenlevel.
in particular, the assumption is that com-positionality can be approximated as the distancebetween the representation for an nc and the repre-sentation for the compositional combination of itsindividual components.
then, we measure whetherthe vector representations reﬂect the variability ofthe human annotators, who capture different nu-ances of the ncs depending on the sentences inwhich they occur.
similarly, in a third experimentwe use the standard deviations of the idiomatic-ity scores in the three contexts to observe how the.
interpretation of the ncs varies across sentences,and whether this correlates with the contextualisedrepresentations produced by various models.
morespeciﬁcally, we assume that, if models adequatelyincorporate contextual information, the standarddeviations of the similarities between the ncs indifferent contexts should be correlated with thoseof the human annotators..4.1 models.
we evaluate four contextualised models:threebert variants, based on the transformers archi-tecture (vaswani et al., 2017), and elmo, whichlearns word vectors using bidirectional lstms(peters et al., 2018).
for english we used theelmo small model provided by peters et al.
(2018),bert-large uncased (devlin et al., 2019), distil-bert (sanh et al., 2019), based on bert-baseand distilled on squad dataset, and sentence-bert (reimers and gurevych, 2019), trainedon bert-large and both multinli and snli.8for portuguese we selected the elmo pre-trainedweights provided by quinta de castro et al.
(2018)and the multilingual versions of the models usedfor english, namely mbert (base cased), andboth multilingual distilbert and sentence-bert(reimers and gurevych, 2020).
as a static non-contextualised baseline we used glove (penning-ton et al., 2014) (the english ofﬁcial models with300 dimensions and trained on 840 billion tokens,and the equivalent portuguese model released byhartmann et al.
(2017)).
the vector representationswere obtained with the ﬂairnlp framework (ak-bik et al., 2019) using the models provided by thetransformers library (wolf et al., 2020)..the representations of ncs (and their sentences)were obtained by averaging the word (or subword,if adopted by the model) embeddings.
we used theconcatenation of the three layers for elmo and of.
8https://www.nyu.edu/projects/bowman/.
multinli/https://nlp.stanford.edu/projects/snli/.
2734the last four hidden layers for the bert models.
in glove, words which are not in the vocabularywere skipped..4.2 experiment 1: compositionality.
prediction.
unsupervised type idiomaticity identiﬁcation withstatic non-contextualised word embeddings oftenassumes that the similarity between the nc em-bedding and the compositional embedding of thecomponent words (e.g.
police car vs. police andcar) is an indication of idiomaticity (mitchell andlapata, 2010): the more similar they are the morecompositional the nc is.
to approximate thiswith contextualised models, we calculate the co-sine similarities between the contextualised vectorof the nc in each sentence with two types of non-contextualised vectors.
the ﬁrst evaluates if evenin the absence of an informative sentence context,each of the component words would be enough ofa trigger to cue the nc meaning (e.g.
eager foreager beaver).
this is implemented as the vectorfor the nc out of context, obtained by feeding themodel only with the compound, dubbed nc out.9the second non-contextualised vector evaluates ifthe representations for the individual words haveenough information to reconstruct the meaning ofthe nc in the absence of context and of the col-located component.
it is implemented as the sumof the individual vectors of the nc components,where each nc component is fed individually tothe model as a sentence, referred to as nc outcomp.
on each case, we calculate two spearman correla-tions with human judgments: at token level, usingall the sentences for each language; and at typelevel, comparing the average cosine similaritiesof each nc with their compositionality scores attype level.
we also compute correlations betweenthe similarities and frequency-based data, namelythe nc raw frequency, and the ppmi (church andhanks, 1990) between its component words, to ver-ify whether they have any impact in these measuresof idiomaticity.
the frequency data were obtainedfrom ukwac, with 2.25b tokens in english (baroniet al., 2009), and brwac, containing 2.7b tokensin portuguese (wagner filho et al., 2018)..the results by cordeiro et al.
(2019) suggestedthat if the two components of an nc are processedas a single token unit (for instance, by explic-.
9this representation equivalent to the avg phrase used by.
yu and ettinger (2020)..itly linking them with an underscore) the result-ing static representation captures the nc idiomaticmeaning.
this is not surprising since by linking thetwo components we create a new word that wouldbe treated by the model as completely independentof the preexisting component words.
but such pre-processing may not be desirable or even feasible.
in this sense the contextualised models would be agood promise, since we expected that by process-ing a sentence with an idiomatic nc, the contextwould be enough to lead the model into linking thecomponent words and assigning the correspond-ing idiomatic meaning.
figuratively speaking, thecontextualised models would put the underscorefor us.
therefore, if contextualised models cap-ture idiomaticity, the similarity between nc andnc outcomp (or nc out) should have strong corre-lations with the idiomaticity scores of the ncs..table 5 shows the signiﬁcant correlations in en-glish (top rows) and portuguese (bottom).
theseresults indicate at best weak (nc outcomp) to mod-erate (nc out) correlations between models’ pre-dictions and human judgments, both at type andtoken levels.
moreover, the correlations obtainedare much smaller than those found by the staticmodels used by cordeiro et al.
(2019).
for english,the best correlations (0.37) were obtained by bert,while elmo and sentence-bert achieved the bestperformance in portuguese (0.27 and 0.26, respec-tively).
in both languages, the lower values werethose of distilbert.
it is worth noting that a directcomparison between the bert models in both lan-guages should not be done, as they are monolingual(for english) and multilingual (for portuguese)..for ppmi, only weak positive correlations werefound for elmo and distilbert, indicating thatfor them higher cosine values weakly imply ncswith stronger association scores.
moreover, weakto moderate negative correlations with frequencywere found for the bert models, suggesting thatcosine similarity is higher for less frequent ncs.
the differences between nc out and nc outcompindicate the importance of some degree of contex-tualisation (also found by yu and ettinger (2020)),even if only as one component contextualising theother in nc out, which may not be retrievable fromthe combination of the context-independent vectorsof the components (nc outcomp).
this is in linewith the original strategy used with static embed-dings, which learns the distribution of the ncspre-identiﬁed as single tokens in corpora and that.
2735resulted in signiﬁcantly better correlations per typethan any of the contextualised models (cordeiroet al., 2019)..to make a fairer comparison between both ap-proaches, we injected into the bert models sin-gle representations for the ncs, learnt from thereferred ukwac and brwac corpora.
we ﬁrst an-notated as single tokens in the corpus those ncspresent in the dataset, and used attentive mim-icking with one-token-approximation (schick andsch¨utze, 2019, 2020b) to learn up to 500 contextsfor each compound.
after that, we injected thesetype level vectors into the bert models usingbertram (schick and sch¨utze, 2020a).
for en-glish, these new representations obtained lower re-sults than the original bert in nc out (e.g., 0.37vs. 0.28 at type level), but higher in nc outcomp(0.16 vs. 0.33 at type level).
for portuguese, in-cluding single representations for the ncs in bertimproved the correlations in three of the four sce-narios (except for nc out at token level), but thebest results were almost identical to those of elmo(see the full results in the bottom rows of table 5).
regarding the results reported by nandakumaret al.
(2019), for english, our experiments yieldedhigher correlations for bert and lower for elmo(≈ 0.3 in both cases, depending on the setting),which may be due to differences in how the vectorsare generated (e.g., the use of different input sen-tences, hidden layers or compositional operations).
in sum, the results of these evaluations suggestthat the use of a straightforward adaptation of acompositionality prediction approach that led togood performance with static models was not assuccessful with contextualised models..4.3 experiment 2: investigating idiomaticity.
with word embedding models.
we analyse whether models are able to capture dif-ferences in idiomaticity perceived by human anno-tators across the sentences in which an nc occurs.
that is, if an nc is found to be more idiomatic inone sentence than in others.
for that, we createdan annotator’s vector for each sentence, combin-ing the human scores to create a three dimensionalvector representation, where the ﬁrst dimension isthe average nc compositionality, and the secondand third are the average scores of the contributionsof the head and of the modiﬁer.
for representingthe sentence we obtain an embedding by averag-ing their (sub)words.
we calculated the euclidean.
distances between (i) the annotators’ vectors and(ii) the cosine similarities between sentence em-beddings of each of the possible combinations ofthe three sentences associated to each nc.
then,we measured the correlations between these valuesusing spearman ρ. we aim to assess if annotationsand models indicate the same relative differences.10the results were averaged for the 280 (english) and180 (portuguese) ncs..table 6 shows the results for the whole datasetsand divided by compositionality level.
as we com-pare euclidean distances with cosine similaritiesnegative values are actually positive correlationsand vice versa.
the average ρ is close to 0 suggest-ing that the embedding models do not capture thenuances in idiomaticity perceived by the annotatorsbetween the different sentences per nc..4.4 experiment 3: nc idiomaticity across.
sentences.
we also analysed the similarity among the annota-tions for each nc in the three sentences, computingthe standard deviations of the average composition-ality scores given by the annotators.
in contrast tothe previous experiment, here we represent the hu-man annotations using only the idiomaticity scoresof the whole ncs and the models’ output as the con-textualised embedding of the ncs in each sentence.
at token level most compounds (85.7% in englishand 91.1% in portuguese) have mean idiomatic-ity scores with less than 0.6 of standard deviation.
very few ncs have deviations higher than 1: ﬁve inenglish and four in portuguese.
looking at the con-texts in which they occur, the variability seems tobe due to the different topics to which the sentencesrefer.
for instance, the annotators have identiﬁedtwo senses of ﬁring line: one, more idiomatic, re-ferring to a position in which someone is criticised(mean score of 1.25), and a second one (partiallycompositional, with an average of 2.7) referringto a speciﬁc position in an armed conﬂict.
in por-tuguese, c´eu aberto (‘open-air’, lit.
‘open-sky’)was interpreted as less compositional (1.2) whendescribing urban settings (e.g., open-air shoppingcenters) than when referring to wild places (e.g.,lobas que lutavam a c´eu aberto, ‘wolves ﬁghtingin the open’), with a mean idiomaticity score of 3..10spearman ρ is not used here as a statistical test but asa measure to evaluate if the sentence comparisons with twodifferent metrics yield the same relative differences.
as thereare only three sentences to compare, ρ assumes only fourvalues ±0.5 or ±1..2736english.
nc out.
nc outcomp.
model.
bert.
dbert.
predpred0.160.360.07––0.20–0.120.33bertram 0.16cordeiro et al.
(2019) best prediction result at type-level (word2vec skip-gram): 0.73.freq-0.11-0.26-0.20–0.15.freq–-0.33-0.22–0.23.freq-0.26-0.27-0.30––.
pred0.37–0.19–0.28.pred0.20––0.070.20.sbertelmo.
token levelpp–––0.22–.
token levelpp–0.13–0.18–.
type levelpp–0.15–0.25–.
type levelpp–––0.29–.
freq-0.34-0.31-0.33––.
portuguese.
nc out.
nc outcomp.
model.
bert.
dbert.
predpred0.190.160.190.160.260.240.270.260.21bertram 0.14cordeiro et al.
(2019) best prediction result at type-level (ppmi model): 0.60.freq-0.120.120.210.150.09.pred–0.190.160.270.24.freq-0.11-0.19–-0.19–.
freq–0.160.230.17–.
sbertelmo.
token levelpp0.230.460.140.17–.
token levelpp0.210.190.150.17–.
type levelpp0.240.240.160.22–.
type levelpp0.270.500.150.21–.
freq–-0.20–-0.120.17.pred–0.170.190.270.27.table 5: spearman ρ correlations of contextualised models at token and type level (with the best type-level resultsfrom cordeiro et al.
(2019) for comparison).
nc out (left) refers to the results of the non-compositional approach,while nc outcomp are those of the compositional one (right).
pred are the results of the compositionality predictionmeasures proposed.
pp and freq mean ppmi and frequency, respectively.
correlations have p < 0.01 except forvalues in italic (p <= 0.05).
non-signiﬁcant results are omitted..total.
idiomatic.
composit..model.
bert.
dbert.
sbert.
elmo.
glove.
model.
bert.
dbert.
sbert.
elmo.
glove.
ave. ρ-0.066-0.0320.0110.0060.016.ave. ρ0.0060.0310.001-0.008-0.006.english.
stdev0.720.710.730.700.69.stdev0.700.720.720.710.72.ave. ρ-0.0580.0470.0150.0050.044.ave. ρ0.0830.050-0.025-0.017-0.017.stdev0.710.710.740.700.74.stdev0.710.750.720.750.77.portuguese.
part.
comp.
stdevave. ρ0.74-0.0280.69-0.1190.700.0570.670.0000.66-0.063.part.
comp.
stdevave. ρ0.71-0.0500.710.0830.720.0080.720.0420.66-0.058.ave. ρ-0.111-0.036-0.0380.0450.030.ave. ρ-0.017-0.0580.036-0.0500.058.stdev0.700.740.740.710.71.stdev0.690.700.720.670.73.total.
idiomatic.
composit..table 6: average correlations (spearman ρ) and standard deviations (stdev) on the whole dataset (total) and inthe three classes: idiomatic, partially compositional, and compositional noun compounds.
negative values arepositive correlations and vice versa..to observe whether language models capturethese differences across sentences, we calculatedthe cosine similarities between the ncs in the three.
sentences and the standard deviation of these threevalues.
we then computed the spearman correla-tions between these deviations obtained from the.
2737models’ representations and those of the humanannotations: all correlations were very low and notsigniﬁcant, suggesting that the vector representa-tions do not capture the variability perceived bythe annotators.
finally, we have also selected twoncs in english with a combination of idiomaticand compositional meanings (brick wall, and goldmine).
in these examples, we found that for bert(our best model) the cosine similarities betweenthe idiomatic meanings were higher (0.83 in bothcases) than between idiomatic and compositionalsenses (0.68 and 0.7, respectively), suggesting thatthey are somehow identifying the different senses.
however, since the highest standard deviationswere achieved with ncs representing the samesense in all contexts (e.g., big wig and grass root),further analysis is needed..as neither the cosine similarities obtained withbert-based models nor the standard deviationsbetween them were correlated with the variation inthe human scores, these analyses suggest that state-of-the-art contextualised models still do not modelsemantic compositionality as human annotators do.
the experiments performed in this section haveshown, on the one hand, some of the possibilitiesof a multilingual dataset labeled at type and tokenlevel; on the other hand, the results also suggestthat capturing idiomaticity is a hard task for cur-rent language models, as only some of them showmoderate correlations with human annotations insome scenarios..5 conclusions and future work.
this paper presented the nctti, a dataset of ncsin english and portuguese annotated at type andtoken level with human judgments about idiomatic-ity, and with suggestions of paraphrases.
the verystrong correlations found between type and tokenjudgments conﬁrm the robustness of the scores,while the paraphrases provide further validation ofthe interpretation of the ncs..moreover, evaluations involving embeddingmodels with different levels of contextualisationsuggest that they are still far from providing ac-curate estimates of nc idiomaticity, at least usingthe measures proposed and analysed in the paper.
mwes are still a pain in the neck for nlp, anddatasets like the nctti can contribute towardsﬁnding better representations for them and bettermeasures for idiomaticity identiﬁcation..future work includes using these ncs as seeds.
in cross-lingual representations for enriching thedataset with nc equivalents in different languages.
besides, we also plan to enlarge the datasets in-cluding a subset of sentences with ambiguous ncshaving idiomatic and compositional interpretationsdepending on the context..acknowledgments.
aline villavicencio and carolina scarton arefunded by the epsrc project mia: modeling id-iomaticity in human and artiﬁcial language pro-cessing (ep/t02450x/1).
marcos garcia is fundedby the conseller´ıa de cultura, educaci´on e orde-naci´on universitaria of the galician government(erdf 2014-2020: call ed431g 2019/04), andby a ram´on y cajal grant (ryc2019-028473-i)..references.
alan akbik, tanja bergmann, duncan blythe, kashifrasul, stefan schweter, and roland vollgraf.
2019.flair: an easy-to-use framework for state-of-the-art nlp.
in proceedings of the 2019 conference ofthe north american chapter of the association forcomputational linguistics (demonstrations), pages54–59, minneapolis, minnesota.
association forcomputational linguistics..fernando alva-manchego, carolina scarton, and lu-cia specia.
2020. data-driven sentence simpliﬁca-tion: survey and benchmark.
computational lin-guistics, 46(1):135–187..shima asaadi, saif mohammad, and svetlana kir-itchenko.
2019. big bird: a large, ﬁne-grained,bigram relatedness dataset for examining semanticin proceedings of the 2019 confer-composition.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 505–516, minneapolis, minnesota.
as-sociation for computational linguistics..marco baroni, silvia bernardini, adriano ferraresi,and eros zanchetta.
2009. the wacky wide web: acollection of very large linguistically processed web-crawled corpora.
language resources and evalua-tion, 43(3):209–226..pedro vitor quinta de castro, n´adia f´elix felipe dasilva, and anderson da silva soares.
2018. por-tuguese named entity recognition using lstm-crf.
in proceedings of the 13th international con-ference on the computational processing of the por-tuguese language (propor 2018), pages 83–92,canela–rs, brazil.
springer, cham..kenneth ward church and patrick hanks.
1990. wordassociation norms, mutual information, and lexicog-raphy.
computational linguistics, 16(1):22–29..2738paul cook, afsaneh fazly, and suzanne stevenson.
2008. the vnc-tokens dataset.
in proceedings ofthe lrec workshop towards a shared task for mul-tiword expressions (mwe 2008), pages 19–22..silvio cordeiro, aline villavicencio, marco idiart, andcarlos ramisch.
2019. unsupervised composition-ality prediction of nominal compounds.
computa-tional linguistics, 45(1):1–57..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..meghdad farahmand, aaron smith, and joakim nivre.
2015. a multiword expression data set: anno-tating non-compositionality and conventionalizationin proceedings offor english noun compounds.
the 11th workshop on multiword expressions, pages29–33, denver, colorado.
association for computa-tional linguistics..juri ganitkevitch, benjamin van durme, and chrisppdb: the paraphrasecallison-burch.
2013.database.
in proceedings of the 2013 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 758–764, atlanta, georgia.
associa-tion for computational linguistics..ge gao, eunsol choi, yejin choi, and luke zettle-moyer.
2018. neural metaphor detection in context.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages607–613, brussels, belgium.
association for com-putational linguistics..rachel giora.
1999. on the priority of salient mean-ings: studies of literal and ﬁgurative language.
jour-nal of pragmatics, 31(7):919–929..nathan hartmann, erick fonseca, christopher shulby,marcos treviso, j´essica silva, and sandra alu´ısio.
2017. portuguese word embeddings: evaluatingon word analogies and natural language tasks.
inproceedings of the 11th brazilian symposium in in-formation and human language technology, pages122–131, uberlˆandia, brazil.
sociedade brasileirade computac¸ ˜ao..nanjiang jiang and marie-catherine de marneffe.
2019. evaluating bert for natural language infer-ence: a case study on the commitmentbank.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 6086–6091, hong kong, china.
association for computa-tional linguistics..milton king and paul cook.
2018. leveraging dis-tributed representations and lexico-syntactic ﬁxed-ness for token-level prediction of the idiomaticityin proceed-of english verb-noun combinations.
ings of the 56th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 345–350, melbourne, australia.
asso-ciation for computational linguistics..computing krippen-klaus krippendorff.
2011.postprint version.
re-dorff’s alpha-reliability.
trieved from http://repository.upenn.edu/asc_papers/43..germ´an kruszewski and marco baroni.
2014. deadparrots make bad pets: exploring modiﬁer effects innoun phrases.
in proceedings of the third joint con-ference on lexical and computational semantics(*sem 2014), pages 171–181, dublin, ireland.
as-sociation for computational linguistics and dublincity university..j richard landis and gary g koch.
1977. the mea-surement of observer agreement for categoricaldata.
biometrics, 33:159–174..rui mao, chenghua lin, and frank guerin.
2019. end-to-end sequential metaphor identiﬁcation inspired byin proceedings of the 57th an-linguistic theories.
nual meeting of the association for computationallinguistics, pages 3888–3898, florence, italy.
asso-ciation for computational linguistics..diana mccarthy and roberto navigli.
2007. semeval-2007 task 10: english lexical substitution task.
inproceedings of the fourth international workshopon semantic evaluations (semeval-2007), pages 48–53, prague, czech republic.
association for compu-tational linguistics..rada mihalcea, ravi sinha, and diana mccarthy.
2010. semeval-2010 task 2: cross-lingual lexicalsubstitution.
in proceedings of the 5th internationalworkshop on semantic evaluation, pages 9–14, up-psala, sweden.
association for computational lin-guistics..tomas mikolov, ilya sutskever, kai chen, greg cor-rado, and jeffrey dean.
2013. distributed represen-tations of words and phrases and their composition-ality.
in proceedings of the 26th international con-ference on neural information processing systems -volume 2, nips’13, pages 3111–3119, usa.
curranassociates inc..jeff mitchell and mirella lapata.
2010. compositionin distributional models of semantics.
cognitive sci-ence, 34(8):1388–1429..navnita nandakumar, timothy baldwin, and baharsalehi.
2019. how well do embedding models cap-ture non-compositionality?
a view from multiwordexpressions.
in proceedings of the 3rd workshop onevaluating vector space representations for nlp,pages 27–34, minneapolis, usa.
association forcomputational linguistics..2739gustavo h. paetzold.
2016. lexical simpliﬁcation fornon-native english speakers.
ph.d. thesis, theuniversity of shefﬁeld, shefﬁeld, uk..computational linguistics and intelligent text pro-cessing (cicling 2002), pages 1–15, mexico city,mexico.
springer, berlin, heidelberg..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-in proceedings of the 2018 confer-resentations.
ence of the north american chapter of the associ-ation for computational linguistics: human lan-guage technologies, volume 1 (long papers), pages2227–2237, new orleans, louisiana.
associationfor computational linguistics..carlos ramisch, silvio cordeiro, leonardo zilio,marco idiart, and aline villavicencio.
2016. hownaked is the naked truth?
a multilingual lexicon ofin proceed-nominal compound compositionality.
ings of the 54th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 156–161, berlin, germany.
associationfor computational linguistics..siva reddy, diana mccarthy, and suresh manand-har.
2011. an empirical study on compositional-in proceedings of 5th in-ity in compound nouns.
ternational joint conference on natural languageprocessing, pages 210–218, chiang mai, thailand.
asian federation of natural language processing..nils reimers and iryna gurevych.
2019. sentence-bert: sentence embeddings using siamese bert-networks.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3982–3992, hong kong, china.
association forcomputational linguistics..nils reimers and iryna gurevych.
2020. makingmonolingual sentence embeddings multilingual us-in proceedings of theing knowledge distillation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 4512–4525,online.
association for computational linguistics..stephen roller, sabine schulte im walde, and silkescheible.
2013. the (un)expected effects of apply-ing standard cleansing models to human ratings onin proceedings of the 9th work-compositionality.
shop on multiword expressions, pages 32–41, at-lanta, georgia, usa.
association for computationallinguistics..giancarlo salton, robert ross, and john kelleher.
2016.idiom token classiﬁcation using sententialin proceedings of the 54thdistributed semantics.
annual meeting of the association for computa-tional linguistics (volume 1: long papers), pages194–204, berlin, germany.
association for compu-tational linguistics..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2019. distilbert, a distilled versionof bert: smaller, faster, cheaper and lighter.
arxivpreprint arxiv:1910.01108..timo schick and hinrich sch¨utze.
2019. attentivemimicking: better word embeddings by attendingto informative contexts.
in proceedings of the 2019conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 1 (long and shortpapers), pages 489–494, minneapolis, minnesota.
association for computational linguistics..timo schick and hinrich sch¨utze.
2020a.
bertram:improved word embeddings have big impact on con-in proceedings oftextualized model performance.
the 58th annual meeting of the association for com-putational linguistics, pages 3996–4007, online.
association for computational linguistics..timo schick and hinrich sch¨utze.
2020b.
rare words:a major problem for contextualized embeddings andhow to ﬁx it by attentive mimicking.
in proceedingsof the thirty-fourth aaai conference on artiﬁcialintelligence, pages 8766–8774..sabine schulte im walde, anna h¨atty, stefan bott,and nana khvtisavrishvili.
2016. ghost-nn: arepresentative gold standard of german noun-nounin proceedings of the tenth inter-compounds.
national conference on language resources andevaluation (lrec’16), pages 2285–2292, portoroˇz,slovenia.
european language resources associa-tion (elra)..vered shwartz and ido dagan.
2019. still a pain in theneck: evaluating text representations on lexical com-position.
transactions of the association for com-putational linguistics, 7:403–419..caroline sporleder and linlin li.
2009. unsupervisedrecognition of literal and non-literal use of idiomaticin proceedings of the 12th confer-expressions.
ence of the european chapter of the acl (eacl2009), pages 754–762, athens, greece.
associationfor computational linguistics..ivan a. sag, timothy baldwin, francis bond, anncopestake, and dan flickinger.
2002. multiwordin pro-expressions: a pain in the neck for nlp.
ceedings of the third international conference on.
ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
arxiv preprint arxiv:1706.03762..2740jorge a. wagner filho, rodrigo wilkens, marco idiart,and aline villavicencio.
2018. the brwac corpus:a new open resource for brazilian portuguese.
inproceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, remi louf, morgan funtow-icz, joe davison, sam shleifer, patrick von platen,clara ma, yacine jernite, julien plu, canwen xu,teven le scao, sylvain gugger, mariama drame,quentin lhoest, and alexander rush.
2020. trans-formers: state-of-the-art natural language process-ing.
in proceedings of the 2020 conference on em-pirical methods in natural language processing:system demonstrations, pages 38–45, online.
asso-ciation for computational linguistics..lang yu and allyson ettinger.
2020. assessing phrasalrepresentation and composition in transformers.
inproceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4896–4907, online.
association for computa-tional linguistics..andrea zaninello and alexandra birch.
2020. multi-word expression aware neural machine translation.
in proceedings of the 12th language resourcesand evaluation conference, pages 3816–3825, mar-seille, france.
european language resources asso-ciation..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
incorporating bert into neural machine2020.in proceedings of the eighth interna-translation.
tional conference on learning representations, ad-dis ababa, ethiopia..2741