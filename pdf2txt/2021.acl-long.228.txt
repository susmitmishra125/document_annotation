marginal utility diminishes: exploring the minimum knowledge forbert knowledge distillation.
yuanxin liu1,2∗, fandong meng3, zheng lin1†, weiping wang1, jie zhou31institute of information engineering, chinese academy of sciences, beijing, china2school of cyber security, university of chinese academy of sciences, beijing, china3pattern recognition center, wechat ai, tencent inc, china{liuyuanxin,linzheng,wangweiping}@iie.ac.cn, {fandongmeng,withtomzhou}@tencent.com.
abstract.
recently, knowledge distillation (kd) hasshown great success in bert compression.
in-stead of only learning from the teacher’s softlabel as in conventional kd, researchers ﬁndthat the rich information contained in the hid-den layers of bert is conducive to the stu-dent’s performance.
to better exploit the hid-den knowledge, a common practice is to forcethe student to deeply mimic the teacher’s hid-den states of all the tokens in a layer-wise man-ner.
in this paper, however, we observe thatalthough distilling the teacher’s hidden stateknowledge (hsk) is helpful, the performancegain (marginal utility) diminishes quickly asmore hsk is distilled.
to understand this ef-fect, we conduct a series of analysis.
speciﬁ-cally, we divide the hsk of bert into threedimensions, namely depth, length and width.
we ﬁrst investigate a variety of strategies toextract crucial knowledge for each single di-mension and then jointly compress the threedimensions.
in this way, we show that 1) thestudent’s performance can be improved by ex-tracting and distilling the crucial hsk, and 2)using a tiny fraction of hsk can achieve thesame performance as extensive hsk distilla-tion.
based on the second ﬁnding, we fur-ther propose an efﬁcient kd paradigm to com-press bert, which does not require loadingthe teacher during the training of student.
fortwo kinds of student models and computing de-vices, the proposed kd paradigm gives rise totraining speedup of 2.7× ∼3.4×..1.introduction.
since the launch of bert (devlin et al., 2019),pre-trained language models (plms) have beenadvancing the state-of-the arts (sotas) in a widerange of nlp tasks.
at the same time, the growing.
∗ work was done when yuanxin liu was an intern atpattern recognition center, wechat ai, tencent inc, china..† zheng lin is the corresponding author..figure 1: the acc variation of rosita (liu et al.,2021) and tinybert (jiao et al., 2020) on qnli withthe increase of hsk..size of plms has inspired a wave of research inter-est in model compression (han et al., 2016) in thenlp community, which aims to facilitate the de-ployment of the powerful plms to resource-limitedscenarios..knowledge distillation (kd) (hinton et al.,2015) is an effective technique in model com-pression.
in conventional kd, the student modelis trained to imitate the teacher’s prediction overclasses, i.e., the soft labels.
subsequently, romeroet al.
(2015) ﬁnd that the intermediate representa-tions in the teacher’s hidden layers can also serve asa useful source of knowledge.
as an initial attemptto introduce this idea to bert compression, pkd(sun et al., 2019) proposed to distill representationsof the [cls] token in bert’s hidden layers, andlater studies (jiao et al., 2020; sun et al., 2020; houet al., 2020; liu et al., 2021) extend the distillationof hidden state knowledge (hsk) to all the tokens.
in contrast to the previous work that attempts toincrease the amount of hsk, in this paper we ex-plore towards the opposite direction to “compress”hsk.
we make the observation that although distill-ing hsk is helpful, the marginal utility diminishesquickly as the amount of hsk increases.
to under-stand this effect, we conduct a series of analysis.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages2928–2941august1–6,2021.©2021associationforcomputationallinguistics2928010−310−210−1100amount of hsk (norma i(ed)85.085.586.086.587.087.588.0qnli accrosita6tinybert4by compressing the hsk from three dimensions,namely depth, length and width (see section 2.3 fordetailed description).
we ﬁrst compress each singledimension and compare a variety of strategies to ex-tract crucial knowledge.
then, we jointly compressthe three dimensions using a set of compressionconﬁgurations, which specify the amount of hskassigned to each dimension.
figure 1 shows theresults on qnli dataset.
we can ﬁnd that 1) per-ceivable performance improvement can be obtainedby extracting and distilling the crucial hsk, and 2)with only a tiny fraction of hsk the students canachieve the same performance as extensive hskdistillation..based on the second ﬁnding, we further proposean efﬁcient paradigm to distill hsk.
concretely,we run bert over the training set to obtain andstore a subset of hsk.
this can be done on clouddevices with sufﬁcient computational capability.
given a target device with limited resource, wecan compress bert and select the amount of hskaccordingly.
then, the compressed model can per-form kd on either the cloud or directly on thetarget device using the selected hsk and the origi-nal training data, dispensing with the need to loadthe teacher model..in summary, our maojor contributions are:.
• we observe the marginal utility diminishingeffect of hsk in bert kd.
to our knowl-edge, we are the ﬁrst attempt to systematicallystudy knowledge compression in bert kd..• we conduct exploratory studies on how toextract the crucial knowledge in hsk, basedon which we obtain perceivable improvementsover a widely-used hsk distillation strategy..• we propose an efﬁcient kd paradigm basedon the empirical ﬁndings.
experiments onthe glue benchmark for nlu (wang et al.,2019) show that, the proposal gives rise totraining speedup of 2.7× ∼3.4× for tiny-bert and rosita on gpu and cpu1..2 preliminaries.
2.1 bert architecture.
the backbone of bert consists of an embeddinglayer and l identical transformer (vaswani et al.,2017) layers.
the input to the embedding layer is a.
1the code is available at https://github.com/.
llyx97/marginal-utility-diminishes.
text sequence x tokenized by wordpiece (wu et al.,2016).
there are two special tokens in x: [cls]is inserted in the left-most position to aggregatethe sequence representation and [sep] is used toseparate text segments.
by summing up the tokenembedding, the position embedding and the seg-ment embedding, the embedding layer outputs a se-quence of vectors e = (cid:2)e1, · · · , e|x|(cid:3) ∈ r|x|×dh ,where dh is the hidden size of the model..then, e passes through the stacked transformer.
layers, which can be formulated as:.
hl = trml (hl−1) , l ∈ [1, l].
(1).
where hl = (cid:2)hl,1, · · · , hl,|x|(cid:3) ∈ r|x|×dh is theoutputs of the lth layer and h0 = e. eachtransformer layer is composed of two sub-layers:the multi-head self-attention layer and the feed-forward network (ffn).
each sub-layer is followedby a sequence of dropout (srivastava et al., 2014),residual connection (he et al., 2016) and layer nor-malization (ba et al., 2016)..finally, for the tasks of nlu, a task-speciﬁcclassiﬁer is employed by taking as input the repre-sentation of [cls] in the lth layer..2.2 bert compression with kd.
knowledge distillation is a widely-used techniquein model compression, where the compressedmodel (student) is trained under the guidance ofthe original model (teacher).
this is achieved byminimizing the difference between the features pro-duced by the teacher and the student:.
lkd =.
(cid:88).
l (cid:0)f s(x), f t (x)(cid:1).
(2).
(f s ,f t ).
where (cid:0)f s, f t (cid:1) is a pair of features from studentand teacher respectively.
l is the loss functionand x is a data sample.
in terms of bert com-pression, the predicted probability over classes, theintermediate representations and the self-attentiondistributions can be used as the features to transfer.
in this paper, we focus on the intermediate rep-resentations {hl}ll=0 (i.e., the hsk), which haveshown to be a useful source of knowledge in bertcompression.
the loss function is computed as themean squared error (mse) in a layer-wise way:.
lhsk =.
(cid:16).
mse.
hs.
l w, ht.
g(l).
(cid:17).
(3).
(cid:48)l(cid:88).
l=0.
2929where l(cid:48)is the student’s layer number and g(l) isthe layer mapping function to select teacher lay-ers.
w ∈ rdsh is the linear transformationto project the student’s representations hsto thelsame size as the teacher’s representation htl ..h ×dt.
2.3 hsk compression.
(cid:48).
(cid:105).
g(l(cid:48) ).
+1)×|x|×dt.
g(0), · · · , ht.
according to equation 3,the hsk fromteacher can be stacked into a tensor (cid:98)ht =(cid:104)∈ r(lhth , whichconsists of three structural dimensions, namelydepth, length and width.
for the depth dimension,(cid:98)ht can be compressed by eliminating entire layers.
by dropping the representations corresponding toparticular tokens, we compress the length dimen-sion.
when it comes to the width dimension, we setthe eliminated activations to zero.
we will discussthe strategies to compress each dimension later insection 4..3 experimental setups.
3.1 datasets.
we perform experiments on seven tasks fromthe general language understanding evaluation(glue) benchmark (wang et al., 2019): cola(linguistic acceptability), sst-2 (sentiment analy-sis), rte, qnli, mnli-m and mnli-mm (naturallanguage inference), mrpc and sts-b (semanticmatching/similarity).
due to space limitation, weonly report results on cola, sst-2, qnli andmnli for single-dimension hsk compression insection 4, and results on the other three tasks arepresented in appendix e..3.2 evaluation.
following (devlin et al., 2019), for the dev set,we use matthew’s correlation and spearman cor-relation to evaluate the performance on cola andsts-b respectively.
for the other tasks, we reportthe classiﬁcation accuracy.
we use the dev set toconduct our exploratory studies and the test set re-sults are reported to compare hsk compressionwith the existing distillation strategy.
for the testset of mrpc, we report the results of f1 score..3.3.implementation details.
we take two representative kd-based methods, i.e.,tinybert (jiao et al., 2020) and rosita (liuet al., 2021), as examples to conduct our analysis.
tinybert is a compact version of bert that israndomly initialized.
it is trained with two-stage.
kd: ﬁrst on the unlabeled general domain data andthen on the task-speciﬁc training data.
rositareplaces the ﬁrst stage kd with structured pruningand matrix factorization, which can be seen as a di-rect transfer of bert’s knowledge from the modelparameters..we focus on kd with the task-speciﬁc train-ing data and do not use any data augmentation.
for tinybert, the student model is initializedwith the 4-layer general distillation model pro-vided by jiao et al.
(2020) (denoted as tinybert4).
for rosita, we ﬁrst ﬁne-tune bertbase onthe downstream task and then compress it fol-lowing liu et al.
(2021) to obtain a 6-layer stu-dent model (denoted as rosita6).
the ﬁne-tunedbertbase is used as the shared teacher for tiny-bert and rosita.
following jiao et al.
(2020),we ﬁrst conduct hsk distillation as in equation 3(w/o distilling the self-attention distribution) andthen distill the teacher’s predictions using cross-entropy loss.
all the results are averaged over threeruns with different random seeds.
the model ar-chitecture of the students and the hyperparametersettings can be seen in appendix a and appendixb respectively..4 single-dimension knowledge.
compression.
researches on model pruning have shown that thestructural units in a model are of different levelsof importance, and the unimportant ones can bedropped without affecting the performance.
in thissection, we investigate whether the same law holdsfor hsk compression in kd.
we study the threedimensions separately and compare a variety ofstrategies to extract the crucial knowledge.
whena certain dimension is compressed, the other twodimensions are kept to full scale..4.1 depth compression.
4.1.1 compression strategies.
from the layer point of view, hsk compres-sion can be divided into two steps.
first, thelayer mapping function g(l) selects one of theteacher layers for each student layer.
this pro-duces l(cid:48) + 1 pairs of teacher-student features:(cid:104)(hsl(cid:48) , htsecond, asubset of these feature pairs are selected to performhsk distillation..g(0)), · · · , (hs.
0 , ht.
g(l(cid:48) ).
(cid:105)).
..for the ﬁrst step, a simple but effective strategy.
2930figure 2: illustration of the redesigned uniform layermapping strategy.
left: ltop is the top teacher layer.
right: ltop is the second-top teacher layer..is the uniform mapping function:.
g(l) = l ×.)
= 0.
(4).
ll(cid:48) , mod(l, l.(cid:48).
in this way, the teacher layers are divided into l(cid:48)blocks and the top layer of each block serves asthe guidance in kd.
recently, wang et al.
(2020a)empirically show that the upper-middle layers ofbert, as compared with the top layer, are a betterchoice to guide the top layer of student in self-attention distillation.
inspired by this, we redesignequation 4 to allow the top student layer to dis-till knowledge from an upper-middle teacher layer,and the lower layers follow the uniform mappingprinciple.
this function can be formulated as:.
g(l, ltop) = l × round(.
(5).
ltopl(cid:48) ).
(cid:48).
..l=l(cid:48) −n d+1.
g(l,ltop))}l.where ltop is the teacher layer corresponding tothe top student layer and round() is the rounding-off operation.
figure 2 gives an illustration ofg(l, ltop) with a 6-layer teacher and a 3-layer stu-dent.
speciﬁcally, for the 12-layer bertbaseteacher, we select ltop from {8, 10, 12}.
for thesecond step, we simply keep the top n d featurepairs: {(hs.
l , ht4.1.2 results and analysisfigure 3 presents the results of depth compressionwith different layer mapping functions.
we can ﬁndthat: 1) for the g(l, 12) mapping function (the greylines), depth compression generally has a negativeimpact on the students’ performance.
specially,the performance of rosita6 declines drasticallywhen the number of layers is reduced to 1 ∼ 3.
2)in terms of the g(l, 10) and g(l, 8) mapping func-tions (the blue and orange lines), hsk distillationwith only one or two layers can achieve compa-rable performance as using all the l(cid:48) + 1 layers..figure 3: results of depth compression on cola, sst-2, qnli and mnli.
each color denotes a layer map-ping function.
the number of layers in hsk includesthe embedding layer.
full results on seven tasks areshown in appendix e.1..on the qnli and mnli datasets, the performancecan even be improved by eliminating the lowerlayers.
3) in general, the student achieves betterresults with the redesigned layer mapping functionin equation 5 across the four tasks.
this demon-strates that, like the self-attention knowledge, themost crucial hsk does not necessarily reside in thetop bert layer, which reveals a potential way toimprove hsk distillation of bert.
4) comparedwith g(l, 8), the improvement brought by g(l, 10) ismore stable across different tasks and student mod-els.
therefore, we use the g(l, 10) layer mappingfunction when investigating the other two dimen-sions..4.2 length compression.
4.2.1 compression strategies.
to compress the length dimension, we design amethod to measure the tokens’ importance by us-ing the teacher’s self-attention distribution.
theintuition is that self-attention controls the informa-tion ﬂow among tokens across layers, and thus therepresentations of the most attended tokens maycontain crucial information..(cid:110).
(cid:111)ah.
l =.
atl,a.
assuming that the teacher has ah attentionheads, and the attention weights in the lth layerl,a ∈ r|x|×|x| is theis at, where atattention matrix of the ath head.
each row of atl,ais the attention distribution of a particular token toall the tokens.
in our length compression strategy,the importance score of the tokens is the attentiondistribution of the [cls] token (i.e., the ﬁrst row in.
a=1.
2931embeddingtext sequencetrm1trm2trm3trm4trm5trm6embeddingtext sequencetrm1trm2trm3studentteacherembeddingtext sequencetrm1trm2trm3trm4trm5trm6embeddingtext sequencetrm1trm2trm3studentteacher1234567152025303540cola mccrosita6 g(l,8)rosita6 g(l,10)rosita6 g(l,12)tinybert4 g(l,8)tinybert4 g(l,10)tinybert4 g(l,12)1234567868788899091sst-2 acc1234567hsk depth (# of layers)86.2586.5086.7587.0087.2587.5087.75qnli acc1234567hsk depth (# of layers)79.079.580.080.581.081.5mnli-m accfigure 4: length compression results of rosita6 oncola, sst-2, qnli and mnli.
the horizontal axisrepresents the compressed hsk length normalized byfull length.
the left-most points in each plot mean com-pressing the length to one token.
full results on seventasks are shown in appendix e.2..at.
l,a) averaged over the ah heads:.
sl =.
1ah.
ah(cid:88).
a=1.
at.
l,a,1, sl ∈ r|x|.
(6).
to match the depth of the student, we employ thelayer mapping function in equation 5 to selectsg(l,ltop) for the lth student layer..the length compression strategies examined in.
this section are summarized as:.
attis the attention-based strategy as describedabove.
the layer mapping function to select s isthe same as the one to select hsk, i.e., g(l, 10)..att w/o [sep]excludes the hsk of the specialtoken [sep].
the rationality of this operation willbe explained in the following analysis..att (ltop = 12) w/o [sep]w/o [sep] in that it utilizes g(l, 12) to select s..is different from att.
leftis a naive baseline that discards tokens fromthe tail of the text sequence.
when the token num-ber is reduced to 1, the student only distills thehsk from the [cls] token..4.2.2 results and analysisthe length compression results are shown in fig-ure 4 and figure 5. we can derive the follow-ing observations: 1) for all strategies, signiﬁcantperformance decline can only be observed whenhsk length is compressed heavily (to less than0.05 ∼ 0.30).
in some cases, using a subset oftokens’ representation even leads to perceivable.
figure 5: length compression results of tinybert4on cola, sst-2, qnli and mnli.
the axes, pointsand lines are deﬁned in the same way as figure 4. fullresults on seven tasks are shown in appendix e.2..figure 6: the proportion of the data samples in which[sep] is among the top1 and top3 attended tokens.
wepresent the results over the 12 layers of the bertbaseﬁne-tuned on cola, sst-2, qnli and mnli.
full re-sults on seven tasks are shown in appendix e.2..improvement over the full length (e.g., rosita6on cola and tinybert4 on sst-2 and qnli).
2)the performance of att is not satisfactory.
whenbeing applied to rosita6, the att strategy under-performs the left baseline.
the results of att intinybert4, though better than those in rosita6,still lag behind the other strategies at the left-mostpoints.
3) excluding [sep] in the att strategy al-leviates the drop in performance, especially whenhsk length is compressed to less than 0.05.
4) asa general trend, further improvement over att w/o[sep] can be obtained by using g(l, 12) in the se-lection of s, which produces the most robust resultsamong the four strategies..to explain why the att strategy performs poorly,we inspect into the tokens that receive the highestimportance scores under equation 6. we ﬁnd thatthe special token [sep] is dominant in most hiddenlayers.
as shown in figure 6, from the 4th ∼ 10th.
29320.050.100.150.200.250.3025.027.530.032.535.037.540.042.5cola mccleftattatt w/o [sep]att (ltop=12) w/o [sep]full length0.050.100.150.200.250.3085868788899091sst-2 acc0.000.050.100.150.20hsk length (normalized)85.085.586.086.587.087.5qnli acc0.000.050.100.150.20hsk length (normalized)79.079.580.080.581.081.5mnli-m acc0.050.100.150.200.250.30242628303234cola mccleftattatt w/o [sep]att (ltop=12) w/o [sep]full length0.00.10.20.30.488.088.589.089.590.090.5sst-2 acc0.000.050.100.150.20hsk length (normalized)85.7586.0086.2586.5086.7587.0087.25qnli acc0.000.050.100.150.20hsk length (normalized)79.5079.7580.0080.2580.5080.7581.0081.25mnli-m acc24681012020406080100% of training samplescola[sep] in top1[sep] in top324681012020406080100sst-224681012layer index020406080100% of training samplesqnli24681012layer index020406080100mnlimally, the mask m is deﬁned as:.
mi =.
(cid:26) 1,0,.i ∈ iotherwise.
(7).
(cid:110).
where i =roundof the remained n w activations..i × dthn w.i=1.
(cid:17)(cid:111)n w.(cid:16).
is the indices.
mag mask masks out the activations with lowmagnitude.
therefore, this mask is dynamic, i.e.,every ht.
l,i(∀i, l) has its own m..4.3.2 results and analysisthe width compression results can be seen in fig-ure 7, from which we can obtain two ﬁndings.
first,the masks reveal different patterns when combinedwith different student models.
for rosita6, theperformance of rand mask and uniform maskdecreases sharply at 20% hsk width.
in compari-son, the performance change is not that signiﬁcantwhen it comes to tinybert4.
this suggests thattinybert4 is more robust to hsk width compres-sion than rosita6.
second, the magnitude-basedmasking strategy obviously outperforms randmask and uniform mask.
as we compress thenonzero activations in hsk from 100% to 20%, theperformance drop of mag mask is only marginal,indicating that there exists considerable knowledgeredundancy in the width dimension..5 three-dimension joint knowledge.
compression.
with the ﬁndings in single-dimension compression,we are now at a position to investigate joint hskcompression from the three dimensions..5.1 measuring the amount of hsk.
for every single dimension, measuring the amountof hsk is straightforward: using the number oflayers, tokens and activations for depth, length andwidth respectively.
in order to quantify the totalamount of hsk (denoted as ahsk), we deﬁneone unit of ahsk as the amount of hsk in anyhtl,i(∀l ∈ [0, l], i ∈ [1, |x|]).
in other words, theahsk of (cid:98)ht equals to (l(cid:48) + 1) × |x|.
when hskis compressed to n d layers, n l tokens and n wactivations, the ahsk is n d × n l × n wdth...5.2 compression conﬁgurations & strategiesformally, the triplet (n d, n l, n w ) deﬁnes asearch space ∈ r(lh of the conﬁgu-.
+1)×|x|×dt.
(cid:48).
figure 7: results of width compression with differentmasking strategies on cola, sst-2, qnli and mnli.
full results on seven tasks are shown in appendix e.3..layers, [sep] is the most attended token for almostall training samples.
meanwhile, [sep] frequentlyappears in the top three positions across all thelayers.
similar phenomenon was found in clarket al.
(2019), where [sep] receives high attentionscores from itself and other tokens in the middlelayers.
combining this phenomenon and the resultsin figure 4 and figure 5, it can be inferred that therepresentations of [sep] is not a desirable sourceof knowledge for rosita and tinybert.
we con-jecture that this is because there exists some trivialpatterns in the representations of [sep], which pre-vents the student to extract the informative featuresthat are more relevant to the task..4.3 width compression.
4.3.1 compression strategies.
as discussed in section 2.3, the width dimension iscompressed by setting some activations in the inter-mediate representations to zero.
practically, we ap-ply a binary mask m ∈ rdth to the vectors in htl ,(cid:105)l,1, · · · , m (cid:12) htm (cid:12) htwhich gives rise to,where (cid:12) denotes the element-wise product.
onthis basis, we introduce and compare three mask-ing designs for width compression:.
l,|x|.
(cid:104).
rand mask randomly set the values in m tozero, where the total number of “0” is controlled bythe compression ratio.
this mask is static, i.e.,htl,i(∀i, l) for all the training samples share thesame mask..uniform mask is also a static mask.
it is con-structed by distributing “0” in a uniform way.
for-.
29330.20.40.60.81.02025303540cola mcc0.20.40.60.81.088.088.589.089.590.090.591.0sst-2 acc0.20.40.60.81.0hsk width (normalized)6570758085qnli accrosita6 mag maskrosita6 rand maskrosita6 uniform masktinybert4 mag masktinybert4 rand masktinybert4 uniform mask0.20.40.60.81.0hsk width (normalized)7778798081mnli-m accfigure 8: results of 3d hsk compression for rosita6.
the horizontal axis represents the remained ahsknormalized by the ahsk of (cid:98)ht .
the left-most points (grey stars) in each plot correspond to only distilling theteacher’s predictions.
the mapping function used in depth compression is shown in the title of each plot, and thered stars denote the results of using g(l, 12).
we show the averaged and best results of the conﬁgurations with thesame ahsk.
the error bars of “avg” denote the standard deviation..figure 9: results of 3d hsk compression for tinybert4.
the axes, points, titles and lines are deﬁned in thesame way as figure 8..1±10% 3±5% 5±5% 10±5% 50±5%.
can be seen in appendix c..ahsk /unit.
rosita6tinybert4.
1313.
2018.
3126.
3630.
2113.table 1: the number of sampled conﬁgurations for dif-ferent ahsk.
each ahsk is extended to ±5% or±10% to include more conﬁgurations..rations for three-dimension (3d) hsk compres-sion, and we could have multiple combinations of(n d, n l, n w ) that satisfy a particular ahsk.
inpractice, we reconstruct the search space as:.
n d ∈ [1, l(cid:48) + 1], n l ∈ [1, 50], n wdth.∈ {0.1 × i}10i=1(8)to study the student’s performance with differentamounts of hsk, we sample a set of conﬁgurationsfor a range of ahsk, the statistics of which is sum-marized in table 1. details of the conﬁgurations.
to compress each single dimension in joint hskcompression, we utilize the most advantageousstrategies that we found in section 4. speciﬁcally,att (ltop = 12) w/o [sep] is used to compresslength, mag mask is used to compress width andthe g(l, ltop) for depth compression is selected ac-cording to the performance of depth compression..5.3 results and analysis.
the results of 3d joint hsk compression are pre-sented in figure 8 and figure 9. as we can see,introducing hsk in kd brings consistent improve-ment to the conventional prediction distillationmethod.
however, the marginal beneﬁt quicklydiminishes as more hsk is included.
typically,with less than 1% of hsk, the student models canachieve the same or better result as full-scale hskdistillation.
over a certain threshold of ahsk, the.
29342025303540mcccola, g(l,8)8788899091accsst-2, g(l,10)76788082accmnli-m, g(l,10)65666768accrte, g(l,8)010−310021001100amo−nt of hsk (normalized)85868788accqnli, g(l,10)bes,g(l,12))re   is,illavg0100310021001100am(−n, (f hsk (n(rmalize )7778798081accmrpc, g(l,10)0100310021001100am(−n, (f hsk (n(rmalize )84858687s)earmanrsts-b, g(l,8)25.027.530.032.535.037.5mcccola, g(l,8)888990accsst-2, g(l,8)bestg(l,12)pred distillavg798081accmnli-m, g(l,12)63646566accrte, g(l,10)010−310121011100amoun− of hsk (normalized)85868788accqnli, g(l,10)0101310121011100am)u(− )f hsk (n)rmali0ed)818283a  mrpc, g(l,8)0101310121011100am)u(− )f hsk (n)rmali0ed)85.0085.2585.5085.7586.00spearmanrsts-b, g(l,10)method.
bertbase(t).
dev.
tinybert4.
w/ hsk compression.
rosita6.
w/ hsk compression.
bertbase(g).
test.
tinybert4.
w/ hsk compression.
rosita6.
w/ hsk compression.
cola sst-2 qnli mnli-m/mm mrpc rte.
sts-b avg.
60.1.
29.837.530.643.0.
52.1.
28.230.628.135.3.
93.5.
89.790.690.191.6.
93.5.
90.990.690.591.3.
91.5.
87.288.187.688.2.
90.5.
86.487.387.086.7.
84.7/84.7.
81.0/81.481.5/81.781.2/81.581.8/82.0.
84.6/83.4.
81.0/80.381.5/80.881.5/80.481.9/80.9.
86.0.
82.483.380.780.9.
88.9.
85.685.483.084.5.
67.5.
64.766.364.968.0.
66.4.
61.561.761.761.7.
88.5.
85.186.183.487.2.
85.8.
76.879.073.979.9.
82.1.
75.276.975.077.8.
80.7.
73.874.673.375.3.table 2: dev and test set performance of bertbase and kd-based bert compression methods.
(g) and (t)denote the results of bertbase from devlin et al.
(2019) and the results of our teacher model, respectively..performance begins to decrease.
among differenttasks and student models, the gap between the bestresults (peaks on the blue lines) and full-scale hskdistillation varies from 0.3 (rosita6 on mnliand sts-b) to 5.3 (tinybert4 on cola).
theresults also suggest that existing bert distillationmethod (i.e., g(l, 12)) can be improved by simplycompressing hsk: numerous points of differentconﬁgurations lie over the red stars..table 2 presents the results of different kd-based bert compression methods.
for fair com-parison, we do not include other methods describedin section 7, because they either distill differenttype of knowledge or use different student modelstructure.
here, we focus on comparing the perfor-mance with or without hsk compression given thesame student model.
we can see that except for theresults of a few tasks on the test sets, hsk com-pression consistently promotes the performance ofthe baseline methods..6.improving training efﬁciency.
existing bert compression methods mostly fo-cus on improving the inference efﬁciency.
how-ever, the teacher model is used to extract featuresthroughout the training process, which suggeststhat the training efﬁciency still has room for im-provement.
as shown in figure 10, the compressedmodels achieve considerable inference speedup,while the increase in training speed is relativelysmall.
moreover, for students with different sizesor architectures, the teacher should be deployedevery time when training a new student.
intuitively,we can run the teacher once and reuse the featuresfor all the students.
in this way, we do not need toload the teacher model while training the student,and thereby increasing the training speed.
we refer.
figure 10: training time (left) and inference time(right) with different devices and models on mnli.
on-line means the teacher is loaded during training andofﬂine is the proposed kd paradigm.
please refer toappendix d for the experimental setups..to this strategy as ofﬂine hsk distillation 2..to evaluate the training efﬁciency of the pro-posed kd paradigm, we compute the training timeon the mnli dataset.
the results are presented inthe left plots of figure 10. as we can see, ofﬂinehsk distillation increases the training speed of thestudent models, as compared with online distilla-tion.
the speedup is consistent for different studentmodels and devices..despite the training speedup, however, loadingand storing hsk increases the memory consump-tion.
the full set of hsk can take up a large amountof space, especially for the pre-trained languagemodels like bert.
fortunately, our ﬁndings in theprevious sections suggest that the student only re-quires a tiny fraction of hsk..table 3 summarizes the actual memory consump-.
2in the literature (gou et al., 2020), “ofﬂine distillation”also means the teacher parameters are ﬁxed during kd, whichis different from our deﬁnition here..29352.2x faster2.0x3.1x2.8x9.2x faster6.9x7.7x faster4.0x2.8x faster2.5x3.4x2.7x(n d, n l, n w ) ahsk feature size (gb) mag mask size (gb).
(1, 9, 0.1)(5, 2, 0.3)(3, 8, 0.2)full rosita6.
0.934.8896.
1.03.45.41011.
2.52.86.80.table 3: memory consumption of different ahsk onmnli training set.
the last row is the full set of hskfor rosita6..tion of four conﬁgurations with different ahsk.
as we can see, the full set of hsk for rosita6takes up approximately 1 tb of memory space,which is only applicable to some high-end cloudservers.
compressing the hsk can reduce the sizeto gb level, which enables training on devices likepersonal computers.
it is worth noticing that stor-ing the dynamic mag mask is consuming, whichtypically accounts for more space than hsk.
how-ever, the binary masks can be further compressedusing some data compression algorithms..based on the above results and analysis, we sum-marize our paradigm for efﬁcient hsk distillationas: first, the teacher bert runs on the trainingdata to obtain and store the features of hsk andpredictions.
this can be done on devices that havesufﬁcient computing and memory resources.
then,according to the target application and device, wedecide the student’s structure and the amount ofhsk to distill.
finally, kd can be performed on acloud server or directly on the target device..7 related work.
kd is widely studied in bert compression.
inaddition to distilling the teacher’s predictions as inhinton et al.
(2015), researches have shown thatthe student’s performance can be improved by us-ing the representations from intermediate bertlayers (sun et al., 2019; liu et al., 2021; hou et al.,2020) and the self-attention distributions (jiao et al.,2020; sun et al., 2020).
typically, the knowledgeis extensively distilled in a layer-wise manner.
tofully utilize bert’s knowledge, some recent workalso proposed to combine multiple teacher layers inbert kd (passban et al., 2021; li et al., 2020) orkd on transformer-based nmt models (wu et al.,2020).
in contrast to these studies that attempt to in-crease the amount knowledge, we study bert kdfrom the compression point of view.
similar ideacan be found in minilms (wang et al., 2020a,b),which only use the teacher’s knowledge to guidethe last layer of student.
however, they only con-.
sider knowledge from the layer dimension, whilewe investigate the three dimensions of hsk..we explore a variety of strategies to determinefeature importance for each single dimension.
thisis related to a line of studies called the attributionmethods, which attempt to attribute a neural net-work’s prediction to the input features.
the atten-tion weights have also been investigated as an at-tribution method.
however, prior work (wiegreffeand pinter, 2019; serrano and smith, 2019; brun-ner et al., 2020; hao et al., 2020) ﬁnds that attentionweights usually fail to correlate well with their con-tributions to the ﬁnal prediction.
this echoes withour ﬁnding that the original att strategy performspoorly in length compression.
however, the atten-tion weights may play different roles in attributionand hsk distillation.
whether the ﬁndings in attri-bution are transferable to hsk distillation is still aproblem that needs further investigation..8 conclusions and future work.
in this paper, we investigate the compression ofhsk in bert kd.
we divide the hsk of bertinto three dimensions and explore a range of com-pression strategies for each single dimension.
onthis basis, we jointly compress the three dimen-sions and ﬁnd that, with a tiny fraction of hsk, thestudent can achieve the same or even better perfor-mance as distilling the full-scale knowledge.
basedon this ﬁnding, we propose a new paradigm to im-prove the training efﬁciency in bert kd, whichdoes not require loading the teacher model duringtraining.
the experiments show that the trainingspeed can be increased by 2.7× ∼ 3.4× for twokinds of student models and two types of cpu andgpu devices..most of the compression strategies investigatedin this study are heuristic, which still have roomfor improvement.
therefore, a future directionof our work could be designing more advancedalgorithm to search for the most useful hsk inbert kd.
additionally, since hsk distillationin the pre-training stage is orders of magnitudetime-consuming than task-speciﬁc distillation, themarginal utility diminishing effect in pre-trainingdistillation is also a problem worth studying..acknowledgments.
this work was supported by national natural sci-ence foundation of china (no.
61976207, no.
61906187)..2936references.
lei jimmy ba, jamie ryan kiros, and geoffrey e.corr,.
layer normalization..hinton.
2016.abs/1607.06450..gino brunner, yang liu, damian pascual, oliverrichter, massimiliano ciaramita, and roger watten-hofer.
2020. on identiﬁability in transformers.
iniclr.
openreview.net..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin black-look at?
an analysis of bert’s attention.
boxnlp@acl..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in naacl-hlt (1), pages 4171–4186.
as-sociation for computational linguistics..jianping gou, baosheng yu, stephen john maybank,and dacheng tao.
2020. knowledge distillation: asurvey.
corr, abs/2006.05525..song han, huizi mao, and william j. dally.
2016.deep compression: compressing deep neural net-work with pruning, trained quantization and huff-man coding.
in iclr..yaru hao, li dong, furu wei, and ke xu.
2020. self-attention attribution: interpreting information inter-actions inside transformer.
corr, abs/2004.11207..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-in cvpr, pages 770–778.
ieee computernition.
society..geoffrey e. hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
corr, abs/1503.02531..lu hou, zhiqi huang, lifeng shang, xin jiang, xiaochen, and qun liu.
2020. dynabert: dynamicbert with adaptive width and depth.
in neurips..xiaoqi jiao, yichun yin, lifeng shang, xin jiang,xiao chen, linlin li, fang wang, and qun liu.
2020. tinybert: distilling bert for natural lan-guage understanding.
in emnlp (findings), pages4163–4174.
association for computational linguis-tics..jianquan li, xiaokang liu, honghong zhao, ruifengxu, min yang, and yaohong jin.
2020. bert-emd:many-to-many layer mapping for bert compres-in emnlp (1),sion with earth mover’s distance.
pages 3009–3018.
association for computationallinguistics..yuanxin liu, zheng lin, and fengcheng yuan.
2021.rosita: reﬁned bert compression with integratedtechniques.
in aaai..peyman passban, yimeng wu, mehdi rezagholizadeh,and qun liu.
2021. alp-kd: attention-based layerprojection for knowledge distillation.
in aaai..adriana romero, nicolas ballas, samira ebrahimi ka-hou, antoine chassang, carlo gatta, and yoshuabengio.
2015. fitnets: hints for thin deep nets.
iniclr (poster)..soﬁa serrano and noah a. smith.
2019. is attentioninterpretable?
in acl (1), pages 2931–2951.
asso-ciation for computational linguistics..nitish srivastava, geoffrey e. hinton, alexkrizhevsky, ilya sutskever, and ruslan salakhutdi-nov. 2014. dropout: a simple way to prevent neuralj. mach.
learn.
res.,networks from overﬁtting.
15(1):1929–1958..siqi sun, yu cheng, zhe gan, and jingjing liu.
2019.patient knowledge distillation for bert model com-pression.
in emnlp/ijcnlp (1), pages 4322–4331.
association for computational linguistics..zhiqing sun, hongkun yu, xiaodan song, renjie liu,yiming yang, and denny zhou.
2020. mobilebert:a compact task-agnostic bert for resource-limiteddevices.
in acl, pages 2158–2170.
association forcomputational linguistics..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips, pages 5998–6008..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r. bowman.
2019.glue: a multi-task benchmark and analysis plat-in iclrform for natural language understanding.
(poster).
openreview.net..wenhui wang, hangbo bao, shaohan huang, li dong,and furu wei.
2020a.
minilmv2: multi-head self-attention relation distillation for compressing pre-trained transformers.
corr, abs/2012.15828..wenhui wang, furu wei, li dong, hangbo bao, nanyang, and ming zhou.
2020b.
minilm: deep self-attention distillation for task-agnostic compressionof pre-trained transformers.
in neurips..sarah wiegreffe and yuval pinter.
2019. attention isnot not explanation.
in emnlp/ijcnlp (1), pages11–20.
association for computational linguistics..yimeng wu, peyman passban, mehdi rezagholizadeh,and qun liu.
2020. why skip if you can combine:a simple knowledge distillation technique for inter-in emnlp (1), pages 1016–1021.
mediate layers.
association for computational linguistics..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klausmacherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, lukasz kaiser, stephan gouws,.
2937yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,and jeffrey dean.
2016. google’s neural machinetranslation system: bridging the gap between humanand machine translation.
corr, abs/1609.08144..a architecture of student models.
tinybert (jiao et al., 2020) rescales the structureof bert from the number of layers, the dimensionof the transformer layer outputs, and the hiddendimension of feed-forward networks.
we use the4-layer version (14.5m parameters) of tinybertthat is released by jiao et al.
(2020)..rosita (liu et al., 2021) compresses bertfrom four structural dimensions, namely the layer,attention heads, the hidden dimension of the feed-forward network and the rank of svd to com-press the embedding matrix.
in practice, we scalethe four dimensions to construct a 6-layer modelrosita6 that has approximately the same size astinybert4.
rosita6 has 6 layers and 2 atten-tion heads, and the ffn dimension and embeddingmatrix rank are 768 and 128 respectively..b hyperparameters.
following jiao et al.
(2020), we ﬁrst distill hskand then distill the teacher’s predictions.
the hy-perparamers for hsk distillation basically followjiao et al.
(2020), except that the training epoch ofcola is changed from 50 to 30, the training epochof qnli is changed from 10 to 5, and the batchsize for mnli and qnli is changed from 256 to64. for prediction distillation, we use the lineardecaying learning rate schedule.
for each modeland dataset, we tune the number of epoch (from{5, 10}) and learning rate (from {2e−5, 5e−5}) forthe baseline method that use the uniform layer-wisestrategy g(l, 12), and the hyperparameters are usedfor all the results with compressed hsk.
table 4summarizes the hyperparameters..c conﬁgurations of 3d compression.
strategy.
as described in the paper, for each ahsk we canobtain a number of conﬁgurations.
speciﬁcally,when we use the rosita6 there are 13, 21, 45,75, 112 conﬁgurations for ahsk = 1 ± 10%, 3 ±10%, 5 ± 10%, 10 ± 10%, 50 ± 10% respectively.
we randomly sample subsets of conﬁgurations inour experiments, the statistics of which is shown in.
table 1. the conﬁgurations that exceed the layerconstrain are excluded for tinybert4.
the de-tailed conﬁgurations for different ahsk are sum-marized in table 5..d experimental settings for efﬁciency.
evaluation.
in figure 9, we show the training and inferencetime of two models on two devices.
the trainingtime is computed as the time to run 500 trainingsteps (i.e, batches of data).
when it comes to in-ference, we run the models on the entire trainingset and dev set for gpu and cpu respectively.
fortraining, the batch size is set to 64 and 16 for gpuand cpu respectively.
for inference, we set thebatch size to 128 and 1 for gpu and cpu respec-tively.
the maximum sequence length is 128 forall the settings.
for ofﬂine distillation, we use theconﬁguration (1, 9, 0.1)..e more experimental results.
e.1 full results of depth compression.
depth compression results on all seven tasks arepresented in figure 11. like the results on cola,sst-2, qnli and mnli, the results on mrpc,rte and sts-b also suggest that the redesignedmapping functions (i.e., g(l, 8) and g(l, 10)) gen-erally outperforms the original uniform mappingfunction g(l, 12), especially when hsk is com-pressed to one layer..e.2 full results of length compression.
length compression results on all seven tasks arepresented in figure 12 and figure 13. as we cansee, the general trends on mrpc, rte and sts-bare in accordance with the other four tasks, whoseresults are discussed in section 4.2.2. signiﬁcantperformance drop only occurs when length is com-pressed to less than 0.05 on mrpc, rte and sts-b. the strategy base on original attentinon weights(i.e., att) performs poorly with small hsk length.
in comparison, att w/o [sep] and att (ltop = 12)w/o [sep] reduce the performance drop caused bylength compression..figure 14 shows the proportion of data sampleswhere [sep] is the top1 and top3 most attended to-ken.
we can see that for most data samples, [sep]is among the top3 tokens and frequently appears asthe top1 from the 4th ∼ 10th layers.
this patternis consistent across the seven tasks..2938dataset.
hsk distillation.
learning rate (constant)batch sizemax sequence length# epoch.
prediction distillation.
learning rate (linear decay)batch sizemax sequence length# epoch.
cola sst-2 qnli mnli mrpc.
rte.
sts-b.
5e−5326430.
2e−532645.
5e−5326410.
2e−532645.
5e−5641285.
2e−5641285.
5e−5641285.
5e−5641285.
5e−53212820.
2e−5321285.
5e−53212820.
2e−5321285.
5e−53212820.
2e−5321285.table 4: hyperparameters for hsk distillation and prediction distillation..e.3 full results of width compression.
figure 15 shows the full results of width com-pression on all seven tasks.
we can see that thegap between compression strategies is larger forrosita6, as compared with tinybert4.
amongthe three strategies, mag mask clearly outper-forms rand mask and uniform mask..1±10%.
3±5%.
5±5%.
10±5%.
50±5%.
(2, 1, 0.5)(5, 2, 0.1)(1, 9, 0.1)(1, 2, 0.5)(1, 5, 0.2)(1, 1, 1.0)(3, 3, 0.1)(3, 1, 0.3)(1, 10, 0.1)(2, 5, 0.1)(1, 1, 0.9)(5, 1, 0.2)(1, 3, 0.3).
(6, 1, 0.5)(6, 5, 0.1)(1, 29, 0.1)(1, 10, 0.3)(1, 5, 0.6)(2, 15, 0.1)(5, 2, 0.3)(5, 1, 0.6)(5, 6, 0.1)(3, 5, 0.2)(1, 30, 0.1)(5, 3, 0.2)(1, 6, 0.5)(1, 15, 0.2)(2, 3, 0.5)(1, 3, 1.0)(2, 5, 0.3)(3, 10, 0.1)(3, 2, 0.5)(3, 1, 1.0).
(6, 10, 0.8)(2, 36, 0.7)(3, 25, 0.7)(2, 27, 0.9)(6, 9, 0.9)(6, 12, 0.7)(6, 27, 0.3)(7, 24, 0.3)(4, 18, 0.7)(3, 21, 0.8)(5, 10, 1.0)(7, 23, 0.3)(5, 25, 0.4)(4, 26, 0.5)(6, 17, 0.5)(2, 26, 1.0)(6, 8, 1.0)(4, 12, 1.0)(2, 43, 0.6)(5, 16, 0.6)(4, 25, 0.5).
(1, 13, 0.4)(6, 2, 0.4)(5, 5, 0.2)(2, 24, 0.1)(1, 25, 0.2)(1, 6, 0.8)(7, 7, 0.1)(6, 8, 0.1)(4, 4, 0.3)(2, 6, 0.4)(2, 3, 0.8)(3, 8, 0.2)(4, 12, 0.1)(1, 7, 0.7)(1, 49, 0.1)(2, 4, 0.6)(5, 10, 0.1)(1, 5, 1.0)(3, 4, 0.4)(1, 10, 0.5)(2, 13, 0.2)(3, 2, 0.8)(4, 3, 0.4)(1, 8, 0.6)(5, 2, 0.5)(2, 5, 0.5)(6, 1, 0.8)(4, 6, 0.2)(1, 16, 0.3)(4, 13, 0.1)(6, 4, 0.2).
(7, 3, 0.5)(1, 49, 0.2)(3, 8, 0.4)(3, 5, 0.7)(1, 25, 0.4)(2, 7, 0.7)(3, 16, 0.2)(7, 14, 0.1)(2, 8, 0.6)(5, 3, 0.7)(3, 7, 0.5)(4, 8, 0.3)(5, 7, 0.3)(6, 8, 0.2)(5, 5, 0.4)(6, 2, 0.8)(5, 4, 0.5)(4, 5, 0.5)(1, 13, 0.8)(1, 32, 0.3)(3, 33, 0.1)(3, 34, 0.1)(4, 25, 0.1)(6, 4, 0.4)(5, 10, 0.2)(2, 26, 0.2)(7, 15, 0.1)(1, 11, 0.9)(4, 6, 0.4)(2, 6, 0.8)(2, 10, 0.5)(1, 10, 1.0)(4, 26, 0.1)(5, 2, 1.0)(3, 4, 0.8)(4, 3, 0.8).
table 5: conﬁgurations (n d, n l, n w ) of 3d hskcompression for different ahsk.
the conﬁgurationsin bold font are not used for tinybert4..2939figure 11: results of depth compression on seven tasks.
each color denotes a layer mapping function..figure 12: length compression results of rosita6 on seven tasks.
the horizontal axis represents the compressedhsk length normalized by full length.
the left-most points in each plot mean compressing the length to one token..figure 13: length compression results of tinybert4 on seven tasks.
the horizontal axis represents the com-pressed hsk length normalized by full length.
the left-most points in each plot mean compressing the length toone token..29401234567152025303540cola mccrosita6 g(l,8)rosita6 g(l,10)rosita6 g(l,12)tinybert4 g(l,8)tinybert4 g(l,10)tinybert4 g(l,12)1234567868788899091sst-2 acc123456779.079.580.080.581.081.5mnli-m acc12345676465666768rte acc1234567hsk depth (# of layers)86.2586.5086.7587.0087.2587.5087.75qnli acc1234567hsk depth (# of layers)76788082mrpc acc1234567hsk depth (# of layers)828384858687sts-b spearmanr0.050.100.150.200.250.3025.027.530.032.535.037.540.042.5cola mccleftattatt w/o [sep]att (ltop=12) w/o [sep]full length0.050.100.150.200.250.3085868788899091sst-2 acc0.000.050.100.150.2079.079.580.080.581.081.5mnli-m acc0.000.050.100.150.2050556065rte acc0.000.050.100.150.20hsk length (normalized)85.085.586.086.587.087.5qnli acc0.000.050.100.150.20hsk length (normalized)7274767880mrpc acc0.000.050.100.150.20hsk length (normalized)304050607080sts-b spearmanr0.050.100.150.200.250.30242628303234cola mccleftattatt w/o [sep]att (ltop=12) w/o [sep]full length0.00.10.20.30.488.088.589.089.590.090.5sst-2 acc0.000.050.100.150.2079.5079.7580.0080.2580.5080.7581.0081.25mnli-m acc0.000.050.100.150.2052.555.057.560.062.565.0rte acc0.000.050.100.150.20hsk length (normalized)85.7586.0086.2586.5086.7587.0087.25qnli acc0.000.050.100.150.20hsk length (normalized)777879808182mrpc acc0.000.050.100.150.20hsk length (normalized)767880828486sts-b spearmanrfigure 14: the proportion of the data samples in which [sep] is among the top1 and top3 attended tokens.
wepresent the results over the 12 layers of the bertbase ﬁne-tuned on seven tasks..figure 15: results of width compression with different masking strategies on seven tasks..294124681012020406080100% of training samplescola[sep] in top1[sep] in top324681012020406080100sst-224681012020406080100mnli24681012020406080100rte24681012layer index020406080100% of training samplesqnli24681012layer index020406080100mrpc24681012layer index020406080100sts-b0.20.40.60.81.02025303540cola mcc0.20.40.60.81.088.088.589.089.590.090.591.0sst-2 acc0.20.40.60.81.07778798081mnli-m acc0.20.40.60.81.050556065rte acc0.20.40.60.81.0hsk width (normalized)6570758085qnli accrosita6 mag maskrosita6 rand maskrosita6 uniform masktinybert4 mag masktinybert4 rand masktinybert4 uniform mask0.20.40.60.81.0hsk width (normalized)70727476788082mrpc acc0.20.40.60.81.0hsk width (normalized)304050607080sts-b spearmanr