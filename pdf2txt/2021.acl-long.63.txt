discontinuous named entity recognition as maximal clique discovery.
yucheng wang∗, bowen yu∗, hongsong zhutingwen liu†, nan yu, limin suninstitute of information engineering, chinese academy of sciencesschool of cyber security, university of chinese academy of sciences{wangyucheng,yubowen,zhuhongsong}@iie.ac.cn{liutingwen,yunan,sunlimin}@iie.ac.cn.
abstract.
named entity recognition (ner) remains chal-lenging when entity mentions can be discontin-uous.
existing methods break the recognitionprocess into several sequential steps.
in train-ing, they predict conditioned on the goldenintermediate results, while at inference rely-ing on the model output of the previous steps,which introduces exposure bias.
to solve thisproblem, we ﬁrst construct a segment graphfor each sentence, in which each node denotesa segment (a continuous entity on its own, ora part of discontinuous entities), and an edgelinks two nodes that belong to the same en-tity.
the nodes and edges can be generatedrespectively in one stage with a grid taggingscheme and learned jointly using a novel ar-chitecture named mac.
then discontinuousner can be reformulated as a non-parametricprocess of discovering maximal cliques in thegraph and concatenating the spans in eachclique.
experiments on three benchmarksshow that our method outperforms the state-of-the-art (sota) results, with up to 3.5 percent-age points improvement on f1, and achieves5x speedup over the sota model.1.
1.introduction.
named entity recognition (ner) is the task ofdetecting mentions of real-world entities from textand classifying them into predeﬁned types.
nerbeneﬁts many natural language processing applica-tions (e.g., information retrieval (berger and laf-ferty, 2017), relation extraction (yu et al., 2019),and question answering (khalid et al., 2008))..ner methods have been extensively investigatedand researchers have proposed effective ones.
mostprior approaches (huang et al., 2015; chiu and.
∗ the two authors contribute equally.
† corresponding author.
1the source code is available at https://github..com/131250208/infextraction.
figure 1: an example involving discontinuous men-tions.
entities are highlighted with colored underlines..nichols, 2016; gridach, 2017; zhang and yang,2018; gui et al., 2019; xue et al., 2020) cast thistask as a sequence labeling problem where eachtoken is assigned a label that represents its entitytype.
their underlying assumption is that an entitymention should be a short span of text (muis andlu, 2016), and should not overlap with each other.
while such assumption is valid for most cases, itdoes not always hold, especially in clinical cor-pus (pradhan et al., 2015).
for example, figure 1shows two discontiguous entity mentions with over-lapping segments.
thus, there is a need to movebeyond continuous entities and devise methods toextract discontinuous ones..towards this goal, current state-of-the-art(sota) models can be categorized into twoclasses: combination-based and transition-based.
combination-based models ﬁrst detect all the over-lapping spans and then learn to combine these seg-ments with a separate classiﬁer (wang and lu,2019); transition-based models incrementally la-bel the discontinuous spans through a sequence ofshift-reduce actions (dai et al., 2020b).
althoughthese methods have achieved reasonable perfor-mance, they continue to have difﬁculty with thesame problem: exposure bias (zhang et al., 2019).
speciﬁcally, combination-based methods use thegold segments to guide the classiﬁer during thetraining process while at inference the input seg-ments are given by a trained model, leading to agap between training and inference (wang and lu,2019).
for transition-based models, at trainingtime, the current action relies on the golden previ-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages764–774august1–6,2021.©2021associationforcomputationallinguistics764productivecoughwithwhiteorbloodysputume1e1e1e2e2e2ous actions, while in the testing phase, the entireaction sequence is generated by the model.
as aresult, a skewed prediction will further deviate thepredictions of the follow-up actions.
such accumu-lated discrepancy may hurt the performance..in order to overcome the limitation of such priorworks, we propose mac, a maximal clique discov-ery based discontinuous ner model.
the coreinsight behind mac is that all (potentially discon-tinuous) entity mentions in the sentence can nat-urally form a segment graph by interpreting theircontained continuous segments as nodes, and con-necting segments of the same entity to each other asedges.
then the discontinuous ner task is equiva-lent to ﬁnding the maximal cliques from the graph,which is a well-studied problem in graph theory.
so, the question that remains is how to constructsuch a segment graph.
we decompose it into twouncoupled subtasks, segment extraction (se) andedge prediction (ep) in mac.
typically, given an n-token sentence, two n × n tag tables are formed forse and ep respectively where each entry capturesthe interaction between two individual tokens.
seis then regarded as a labeling problem where tagsare assigned to distinguish the boundary tokens ofeach segment, which have beneﬁts in identifyingoverlapping segments.
ep is converted as the prob-lem of aligning the boundary tokens of segmentscontained in the same entity.
overall, the tag tablesof se and ep are generated independently, and willbe consumed together by a maximum clique search-ing algorithm to recover desired entities from them,thus immune from the exposure bias problem..we conducted experiments on three standard dis-continuous ner benchmarks.
experiments showthat mac can effectively recognize discontinuousentity mentions without sacriﬁcing the accuracy oncontinuous mentions.
this leads to a new state-of-the-art (sota) on this task, with substantial gainsof up to 3.5% absolute percentage points over previ-ous best reported result.
lastly, we show that in theruntime experiments on gpu environments, macis about ﬁve times faster than the sota model..2 related work.
discontinuous ner requires to identify all entitymentions that have discontinuous structures.
toachieve this end, several researchers introducednew position indicators into the traditional biotagging scheme so that the sequential labeling mod-els can be employed (tang et al., 2013; metke-.
jimenez and karimi, 2016; dai et al., 2017; tanget al., 2018).
however, this model suffers from thelabel ambiguity problem due to the limited ﬂexi-bility of the extended tag set.
as the improvement,muis and lu (2016) used hyper-graphs to repre-sent entity spans and their combinations, but didnot completely resolve the ambiguity issue (daiet al., 2020b).
wang and lu (2019) presented apipeline framework which ﬁrst detects all the can-didate spans of entities and then merges them intoentities.
by decomposing the task into two inter-dependency steps, this approach does not have theambiguity issue, but meanwhile being susceptibleto exposure bias.
recently, dai et al.
(2020b) con-structed a transition action sequence for recogniz-ing discontinuous and overlapping structure.
attraining time, it predicts with the ground truth pre-vious actions as condition while at inference it hasto select the current action based on the results ofprevious steps, leading to exposure bias.
in thispaper, for the ﬁrst time we propose a one-stagemethod to address discontinuous ner while with-out suffering from the ambiguity issue, realizingthe consistency of training and inference..joint extraction aims to detect entity pairsalong with their relations using a single model (yuet al., 2020).
discontinuous ner is related tojoint extraction where the discontiguous entitiescan be viewed as relation links between seg-ments (wang and lu, 2019).
our model is mo-tivated by tplinker (wang et al., 2020), whichformulates joint extraction as a token pair link-ing problem by aligning the boundary tokens ofentity pairs.
the main differences between ourmodel and tplinker are two-fold: (1) we proposea tailor-designed tagging scheme for recognizingdiscontinuous segments; (2) the maximal cliquediscovery algorithm is introduced into our modelto accurately merge the discontinuous segments..maximal clique discovery is to ﬁnd a clique ofmaximum size in a given graph (dutta and lauri,2019).
here, a clique is a subset of the vertices allof which are pairwise adjacent.
maximal clique dis-covery ﬁnds extensive application across diversedomains (stix, 2004; boginski et al., 2005; im-biriba et al., 2017).
in this paper, we reformu-late discontinuous ner as the task of maximalclique discovery by constructing a segment graphand leveraging the classic b-k backtracking algo-rithm (bron and kerbosch, 1973) to ﬁnd all themaximum cliques as the entities..765figure 2: an example of the extraction process..3 methodology.
in graph theory, a clique is a vertex subset of anundirected graph where every two vertices in theclique are adjacent, while a maximal clique is theone that cannot be extended by including one moreadjacent vertex.
that means each vertex in themaximal clique has close relations with each other,and no other vertex can be added, which is similarto the relations between segments in a discontin-uous entity.
based on this insight, we claim thatdiscontinuous ner can be equivalently interpretedas discovering maximal cliques from a segmentgraph, where nodes represent segments that eitherform entities on their own or present as parts of adiscontinuous entity, and edges connect segmentsthat belong to the same entity mention..considering the maximum clique searching pro-cess is usually non-parametric (bron and kerbosch,1973), discontinuous ner is actually decomposedinto two subtasks: segment extraction and edge pre-diction, to respectively create the nodes and edgesof the segment graph.
their prediction results canbe generated independently with our proposed gridtagging scheme, and will be consumed togetherto construct a segment graph, so that the maxi-mal clique discovery algorithm can be applied torecover desired entities.
the overall extraction pro-cess is depicted in figure 2. next, we will ﬁrstintroduce our grid tagging scheme and its decodingworkﬂow.
then we will detail the mac, a maximalclique discovery based discontinuous ner modelbased on this tagging scheme..3.1 grid tagging scheme.
inspired by wang et al.
(2020), we implementsingle-stage segment extraction and edge predic-tion based on a novel grid tagging scheme.
givenan n-token sentence, our scheme constructs an.
figure 3: a tagging example for segment extraction..n × n tag table by enumerating all possible tokenpairs and giving each token pair the tag(s) based ontheir relation(s).
note that one token pair may havemultiple tags according to the pre-deﬁned tag set..3.1.1 segment extraction.
2.as demonstrated in figure 1, entity mentions couldoverlap with each other.
to make our model capa-ble of extracting such overlapping segments, weconstruct a two-dimensional tag table.
figure 3provides an example.
a pair of tokens (ti, tj) willbe assigned with a set of labels if a segment from tito tj belongs to the corresponding categories.
con-sidering j ≥ i, we discard the lower triangle regionof the tag table, so n2+ngrids are actually gener-ated for an n-token sentence.
in practice, the bistagging scheme is adopted to represent if a segmentis a continuous entity mention (x-s) or locates atthe beginning (x-b) or inside (x-i) of a discontin-uous entity of type x. for example, (upper, body)is assigned with the tag pob-s since “upper body”is a continuous entity of type part of body (pob).
and the tag of (sever, joint) is ade-b as “severjoint” is a beginning segment of the discontinuousmention “sever joint pain” of type adverse drugevent (ade).
meanwhile, “joint” is also recog-nized as an entity since there is a pob-s tag in theplace of (joint, joint), thus the overlapping segmentextraction problem is solved..3.1.2 edge prediction.
edge prediction is to construct the links betweensegments of the same entity mention by aligningtheir boundary tokens.
the tagging scheme is de-ﬁned as follows: (1) head to head (x-h2h) indi-cates it locates in a place (ti, tj) where ti and tjare respectively the beginning tokens of two seg-ments which constitute the same entity of type x;(2) tail to tail (x-t2t) is similar to x-h2h, but fo-cusing on the ending token.
as shown in figure 4,“sever” has the ade-h2h and ade-t2t relations.
766severjoint,shoulderandupperbodypainseverjoint,shoulderandupperbodypainseverjoint,shoulderandupperbodypainseverjoint,shoulderandupperbodypainseverjoint,shoulderandupperbodypainseverjoint,shoulderandupperbodypainsever joint painjointshoulderupper bodysever shoulder painsever upper body painade-b:severade-b:sever jointade-i:shoulderade-i:painade-i:upper body painpob-s:jointpob-s:shoulderpob-s:upper bodypobadesegmentgraphgraphconstructionmaximal clique discoveryseverjoint,shoulderandupperbodypainseverade-bade-bjointpob-s,shoulderade-ipob-sandupperpob-sade-ibodypainade-ialgorithm 1 decoding procedureinput: the segment tagging results s and edge tagging re-sults e of sentence t. s(ti, tj) and e(ti, tj) respectivelydenote the tag set of token pair (ti, tj) in two schemes..output: r = {(ek, tk)}m.k=1, ek, tk are respectively the text.
and the type of the k-th entity..1: initialize the edge set a and entity set r with ∅2: obtain the segment set n by decoding s.3: for segment s ∈ n do4:5:6:.
deﬁne type ← the entity type of s or gif type-h2h ∈ e(s.start, g.start) & type-t2t ∈.
for segment g ∈ n do.
e(s.end, g.end) then.
end if.
end for.
add (s, g) to a.
7:8:9:10: end for11: construct the segment graph g based on n and a12: find the maximal cliques c in g with the b-k algorithm13: for clique c ∈ c do14:15:16:17: end for18: return r.deﬁne t ← the entity type of a random segment in cconcat the segments of c with their order in t as eadd (e, t) to r.add r to c.algorithm 2 b-k backtracking algorithminput: the graph goutput: the set of all maximal cliques: c.1: initialize c and two vertex sets r, x with ∅2: deﬁne p ← the node set of g3: function bronker(r, p, x)if p = ∅ & x= ∅ then4:5:6:7:8:9:10:11:12:13: end function14: bronker(r, p, x)15: return c.deﬁne n(v) ← the neighbor set of vbronker( r ∪ n(v), p ∩ n(v), x ∩ n(v))p ← p \ vx ← x ∪ v.end iffor v ∈ p do.
end for.
// call the bronker function.
erate two representations, hsi , as the task-speciﬁc features for the segment extractor and theedge predictor, respectively:.
i and he.
hsi = wsi = wehe.
h · hi + bsh,h · hi + beh,.
(1).
(2).
where w∗vector to be learned during training..h is a parameter matrix and b∗.
h is a bias.
3.3.2 segment extractorthe probability that a pair of tokens are the bound-ary tokens of a segment can be represented as:.
p (ti, tj) = p (e = tj|b = ti)p (b = ti),.
(3).
where b and e denotes the beginning token andending token.
in our tagging scheme (figure 3), we.
figure 4: a tagging example for edge prediction..figure 5: the overall structure of the mac model..to “shoulder” and “pain”, because the type of thediscontinuous entity mention “sever shoulder pain”is adverse drug event .
the same logic goes forother tags in the matrix..3.2 decoding workﬂow.
formally, the decoding procedure is summarized inalgorithm 1. the segment tagging table s and edgetagging table e of a sentence t serve as the inputs.
firstly, we extract all the typed segments throughdecoding s. then we construct a segment graph g,in which segments that belong to the same entity(decoded from e) have edges with each other.
fig-ure 2 gives an example.
correspondingly, we canyield a continuous entity mention from the single-vertex clique directly, and concatenate segments ineach multiple-vertex clique following their origi-nal sequential order in t to recover discontinuousentity mentions.
we choose the classic b-k back-tracking algorithm (bron and kerbosch, 1973)for ﬁnding the maximal cliques in g, which takesm3 ) time, where m is the number of nodes.
o(3.
3.3 model structure.
with the grid tagging scheme, we propose an end-to-end neural architecture named mac.
figure 5reveals the overview structure..3.3.1 token representationgiven an n-token sentence [t1, · · · , tn], we ﬁrstmap each token ti into a low-dimensional contex-tual vector hi with a basic encoder.
then we gen-.
767severjoint,shoulderandupperbodypainseverade-h2h  ade-t2tade-h2h ade-h2h  ade-t2tjointade-t2t,shoulderade-h2hade-t2t  ade-h2h  ade-t2tandupperade-h2hbodypainade-h2h  ade-t2tade-t2tade-h2h  ade-t2tseverjoint,shoulderandupperbodypainencodersegmentextractoredgepredictorhave a ﬁxed beginning token ti at the i-th row, andtake the given beginning token as the condition tolabel the corresponding ending token, so p (b = ti)in the i-th row is always 1. hence, all we need todo is to calculate p (e = tj|b = ti)..inspired by su (2019) and yu et al.
(2021), welevderage the conditional layer normalization(cln) mechanism to model the conditional proba-bility.
that is, a conditional vector is introduced asextra contextual information to generate the gainparameter γ and bias λ of the well known layer nor-malization mechanism (ba et al., 2016) as follows:.
x − µσ.
(cid:118)(cid:117)(cid:117)(cid:116).
1d.d(cid:88).
i=1.
cln(c, x) = γc (cid:12) (.)
+ λc,.
(4).
µ =.
xi, σ =.
(xi − µ)2,.
(5).
1d.d(cid:88).
i=1.
γc = wαc + bα, λc = wβc + bβ..(6).
where c and x are the conditional vector andinput vector respectively.
xi denotes the i-th el-ement of x, µ and σ are the mean and standarddeviation taken across the elements of x, respec-tively.
x is ﬁrstly normalized by ﬁxing the meanand variance and then scaled and shifted by γc andλc respectively.
based on the cln mechanism, therepresentation of token pair (ti, tj) being a segmentboundary can be deﬁned as:.
i,j = cln(hshsb.
i , hs.
j)..(7).
in this way, for different ti, different ln pa-rameters are generated, which results in effectivelyadapting hj to be more ti-speciﬁc..furthermore, besides the features of boundarytokens, we also consider inner tokens and segmentlength to learn a better segment representation.
speciﬁcally, we deploy a lstm network (hochre-iter and schmidhuber, 1997) to compute the hiddenstates of inner tokens, and use a looking-up table toembed the segment length.
since the ending tokenis always behind the beginning one, in each rowri, only the tokens behind ti will be fed into thelstm.
we take the hidden state outputted at eachtime step tj as the inner token representation of thesegment si:j. then the representation of a segmentfrom ti to tj can be deﬁned as follows:.
i , ..., hs.
i:j = lstm(hshineleni:j = emb(j − i), j ≥ i,i:j + eleni,j + hini:j = hsbhsi:j ..j), j ≥ i,.
(8).
(9).
(10).
3.3.3 edge predictoredge prediction is similar with segment extractionsince they all need to learn the representation ofeach token pair.
the key differences are summa-rized in the following two aspects: (1) the distancebetween segments is usually not informative, sothe length embedding eleni:j is valueless in edge pre-diction; (2) encoding the tokens between segmentsmay carry noisy semantics for correlation taggingand aggravate the burden of training, so no hini:j isrequired.
under such considerations, we representeach token pair for edge prediction as:.
i,j = cln(hehe3.4 training and inference.
i , he.
j)..(11).
in practical, our grid tagging scheme aims to tagmost relevant labels for each token pair, so it can beseen as a multi-label classiﬁcation problem.
oncehaving the comprehensive token pair representa-tions (hsi:j), we can build the multi-labelclassiﬁer via a fully connected network.
mathe-matically, the predicted probability of each tag for(ti, tj) can be estimated via:.
i:j and he.
i,j = sigmoid(wi · hipi.
i,j + bi),.
(12).
where i ∈ {s, e} is the symbol of subtask indicator,denoting segment extraction and edge prediction re-spectively, and each dimension of pii,j denotes theprobability of a tag between ti and tj.
the sigmoidfunction is used to transfer the projected value intoa probability, in this case, the cross-entropy losscan be used as the loss function which has beenproved suitable for multi-label classiﬁcation task:.
n(cid:88).
n(cid:88).
ki(cid:88).
li = −.
(yi.
i,j[k]log(pi.
i,j[k]).
(13).
i=1j=si+ (1 − yi.
k=1i,j[k])log(1 − pi.
i,j[k])),.
where ki is the number of pre-deﬁned tags ini, pii,j[k] ∈ [0, 1] is the predicted probability of(ti, tj) along the k-th tag, and yii,j[k] ∈ {0, 1} isthe corresponding ground truth.
si equals to 1 ifi = e or i if i = s. then, the losses from segmentextraction and edge prediction are aggregated toform the training objective j (θ):j (θ) = ls + le..(14).
at inference, the probability vector pi.
i,j needsthresholding to be converted to tags.
we enumer-ate several values in the range (0, 1) and pick theone that maximizes the evaluation metrics on thevalidation (dev) set as the threshold..768cadec.
share 13.train dev.
test.
train dev.
test.
train.
share 14dev.
test.
model.
cadec.
share 13prec.
rec.
f1 prec.
rec.
f1 prec.
rec.
f1.
share 14.s 5,340 1,097 1,160 8,508 1,250 9,009 17,407 1,361 15,850990 5,146 669 5,333 10,354 771 7,922m 4,430 89856694d 491947.19.5p 11.1 10.5.
436 1,0048.2.
5817111.3 10.6.
8010.4.
9.7.table 1: statistics of datasets.
s, m, and d respectivelyrepresent the number of sentences, total mentions, anddiscontinuous mentions.
p denotes the percentage ofdiscontinuous mentions in total mentions..68.7 66.1 67.4 77.0 72.9 74.9 74.9 78.5 76.6bioe72.1 48.4 58.0 83.9 60.4 70.3 79.1 70.7 74.7graphcombb 69.8 68.7 69.2 80.1 73.9 76.9 76.5 82.3 79.3transe 68.9 69.0 69.0 80.5 75.0 77.7 78.1 81.2 79.6transb 68.8 67.3 68.0 77.3 72.9 75.0 76.0 78.6 77.3.mac.
70.5 72.5 71.5 84.3 78.2 81.2 78.2 84.7 81.3.table 2: main results on three benchmark datasets.
bold marks highest number among all models..4 evaluation.
4.1 datasets.
following previous work (dai et al., 2020b), weconduct experiments on three benchmark datasetsfrom the biomedical domain: (1) cadec (karimiet al., 2015) is sourced from askapatient: an onlineforum where patients can discuss their experienceswith medications.
we use the dataset pre-processedby dai et al.
(2020b) which selected adverse drugevent (ade) annotations from the original datasetbecause only the ades involve discontinuous an-notations.
(2) share 13 (pradhan et al., 2013) and(3) share 14 (mowery et al., 2014) focus on theidentiﬁcation of disorder mentions in clinical notes,including discharge summaries, electrocardiogram,echocardiogram, and radiology reports.
around10% of mentions in these three data sets are discon-tinuous.
the descriptive statistics of the datasetsare reported in table 1..4.2.implementation details.
we implement our model upon the in-ﬁeld bertbase model: yelp bert (dai et al., 2020a) forcadec, and clinical bert (alsentzer et al.,2019) for share 13 and 14. the network parame-ters are optimized by adam (kingma and ba, 2014)with a learning rate of 1e-5.
the batch size is ﬁxedto 12. the threshold for converting probabilityto tag is set as 0.5. all the hyper-parameters aretuned on the dev set.
we run our experiments on anvidia tesla v100 gpu for at most 300 epochs,and choose the model with the best performanceon the dev set to output results on the test set.
wereport the test score of the run with the median devscore among 5 randomly initialized runs..4.3 comparison models.
for comparison, we employ the following mod-els as baselines: (1) bioe (metke-jimenez and.
karimi, 2016) expands the bio tagging schemewith additional tags to represent discontinuous en-tity; (2) graph (muis and lu, 2016) uses hyper-graphs to organize entity spans and their combi-nations; (3) comb (wang and lu, 2019) ﬁrst de-tects entity spans, then deploys a classiﬁer to mergethem.
for fair comparison, we re-implement combbased on the in-ﬁld bert backbone called combb;(4) transe (dai et al., 2020b) is the current bestdiscontinuous ner method, which generates a se-quence of actions with the aid of buffer and stackstructure to detect entity; note that the originaltranse model is based on elmo.
for fair compari-son with our model, we also implement the in-ﬁeldbert-based trans models, namely transb..4.4 main results.
table 2 reports the results of our models againstother baseline methods.
we have the following ob-servations.
(1) our method, mac, signiﬁcantly out-performs all other methods and achieves the sotaf1 score on all three datasets.
(2) bert-basedtrans model achieves poorer results than its elmo-based counterpart, which is in line with the claimin the original paper.
(3) over the sota methodtranse, mac achieves substantial improvements of2.6% in f1 score on three datasets averagely.
more-over, the wilcoxon’s test shows that a signiﬁcantdifference (p < 0.05) exists between our modeland transe.
we consider that it is because transeis inherently a multi-stage method as it introducesseveral dependent actions, thus suffering from theexposure bias problem.
while for our mac method,it elegantly decomposes the discontinuous nertask into two independent subtasks and learns themtogether with a joint model, realizing the consis-tency of training and inference.
(4) combb can beapproximately seen as the pipeline version of ourmethod, their performance gap again conﬁrms theeffectiveness of our one-stage learning framework..769model.
prec..cadecrec..f1.
prec..f1.
prec..share 13rec..share 14rec..f1.
52.0/ 1.0.
68.3/ 5.837.9/ 6.051.8/ 39.7 39.5/ 12.3 44.8/ 18.8bioe69.5/ 60.8 43.2/ 14.8 53.3/ 23.9 82.3/ 78.4 47.4/ 36.6 60.2/ 50.0 60.0/ 42.7 52.8/ 39.5 56.2/ 41.1graphcombb 63.9/ 44.0 57.8/ 23.4 60.7/ 30.6 59.7/ 65.5 49.8/ 29.6 54.3/ 40.8 52.9/ 51.2 52.8/ 35.0 52.9/ 41.666.5/ 41.2 64.3/ 35.1 65.4/ 37.9 70.5/ 78.5 56.8/ 39.4 62.9/ 52.5 61.9/ 56.1 64.5/ 43.8 63.1/ 49.2transe69.1/ 39.5 64.4/ 34.0 66.7/ 36.6 68.2/ 65.9 55.4/ 39.0 61.1/ 49.0 55.5/ 52.0 55.6/ 37.8 55.6/ 43.8transb.
38.4/ 4.5.
57.3/ 1.8.
37.5/ 8.8.mac.
74.7/ 52.9 65.5/ 38.3 69.8/ 44.4 77.9/ 66.1 60.5/ 48.4 68.1/ 55.9 69.3/ 51.0 70.2/ 57.6 69.7/ 54.1.table 3: results on discontinuous entity mentions.
in the table, two scores are reported and separated by a slash(“/”).
the former is the score on sentences with at least one discontinuous entity mention.
the latter is the scoreonly considering discontinuous entity mentions..model.
f1 dis f1 dis f1(cid:63).
78.7mac– tag b and s78.2– segment length embedding78.176.8– cln mechanism– segment inner representation 72.9.
56.455.855.752.755.6.
46.646.146.244.446.3.table 4: an ablation study on the share 13 dev set.
f1, dis f1, and dis f1(cid:63) respectively denote the overallf1 score, f1 score on sentences with at least one dis-continuous mention, and on discontinuous mentions..as shown in table 1, only around 10% mentionsare discontinuous in all three datasets, which is farless than the continuous entity mentions.
to evalu-ate the effectiveness of our proposed model on rec-ognizing discontinuous mentions, following muisand lu (2016), we report the results on sentencesthat include at least one discontinuous mention.
wealso report the evaluation results when only discon-tinuous mentions are considered.
the scores inthese two settings are separated by a slash in ta-ble 3. comparing table 2 and 3, we can see thatthe bioe model performs better than the graphwhen testing on the full dataset but far worse ondiscontinuous mentions.
consistently, our modelagain defeat the baseline models in terms of f1score.
even though some models outperform macon precision or recall, they greatly sacriﬁce anotherscore, which results in lower f1 score than mac..4.5 model ablation study.
to verify the effectiveness of each component, weablate one component at a time to understand itsimpact on the performance.
concretely, we investi-gated the tagging scheme of segments, the segmentlength embedding, the cln mechanism (by re-placing it with the vector concatenation), and thesegment inner token representation..from these ablations shown in table 4, we ﬁnd.
figure 6: examples of the overlapping patterns.
pattern.
cadectrain dev test.
share 13train dev test.
share 14train dev test.
noleftrightmulti..5727011351.
9541615.
16 34841 16748231814.
41 193 53511 200 3529735192080.
39 24630 238675156.table 5: statistics of overlapping patterns..that: (1) when we take b, i and s tags in segmentextraction as one class, the score slightly drops by0.5%, which indicates the segments in differentpositions of entities may have different semanticfeatures, so distinguishing them can reduce theconfusion in the process of model recognition; (2)when we remove the segment length embedding(formula 9), the overall f1 score drops by 0.6%,showing that it is necessary to let segment extractoraware of the token pair distance information toﬁlter out impossible segments by implicit distanceconstraint; (3) compared with concatenating, it isa better choice to use cln (formula 7 and 11)to fuse the features of two tokens, which brings1.9% improvement; (4) removing segment innerfeatures (formula 8) results in a remarkable dropon the overall f1 score while little drop on thescores of discontinuous mentions, which suggeststhat the information of inner tokens is essential torecognize continuous entity mentions.
overall, wecan conclude that the improvement of grid encoderbrings signiﬁcant performance gains..770(a) cadec.
(b) share 13.
(c) share 14.figure 7: performance on different overlapping patterns..4.6 performance analysis.
4.6.1.impact of overlapping structure.
as discussed in the introduction, overlap is verycommon in discontinuous entity mentions.
to eval-uate the capability of our model on extraction over-lapping structures, as suggested in (dai et al.,2020b), we divide the test set into four categories:(1) no overlap; (2) left overlap; (3) right overlap;and (4) multiple overlap.
figure 6 gives examplesfor each overlapping pattern.
as illustrated in fig-ure 7, mac outperforms transe on all the overlap-ping patterns.
transe gets zero scores on some pat-terns.
it might result from insufﬁcient training sincethese overlapping patterns have relatively fewersamples in the training sets (see table 5), whilethe sequential action structure of transition-basedmodel is a bit data hungry.
by contrast, mac ismore resilient to overlapping patterns, we attributethe performance gains to two design choices: (1)the grid tagging scheme has strong power in accu-rately identifying overlapping segments and assem-bling them into a segment graph; (2) based on thegraph, the maximal clique discovery algorithm caneffectively recover all the candidate overlappingentity mentions..4.6.2.impact of interval and span length.
intervals between segments usually make the totallength of a discontinuous mention longer than con-tinuous one.
considering the involved segments,the whole span is even longer.
that is, differentwords of a discontinuous mention may be distant toeach other, which makes discontinuous ner harderthan the conventional ner task.
to further evalu-ate the robustness of mac in different settings, weanalyse the results of test sets on different intervaland span lengths.
the interval length refers to the.
length.
cadectrain dev test.
share 13train dev test.
share 14train dev test.
362175668363048.
8421414439.
89654 21512 102468484253495.
15 125 22726 118 322184911261163924643812380288.
10 10733 14620 120433616312586.table 6: statistics of interval length..length.
cadectrain dev test.
share 13train dev test.
share 14train dev test.
109567915753118.
323131315918.
30424 10815 15716 12565927106916.
124937251907117 115 259165513120615421441043110.
74615 11327 14065127610333657.table 7: statistics of span length..= 1= 2= 3= 4= 5= 6≥ 7.
= 3= 4= 5= 6= 7= 8≥ 9.number of words between discontinuous segments.
the span length refers to the number of words ofthe whole span.
for example, for the entity mention“sever shoulder pain” in “sever joint, shoulder andupper body pain.”, the interval length is 5, and thespan length is 8. such phenomenon requires mod-els to have the ability of capturing the semanticdependency between distant segments..for the convenience of analysis, we report alldatasets’ distribution on interval and span length intable 6 and 7, respectively.
and figure 8 shows thef1 scores of transe and mac on different intervaland span lengths.
as we can see, mac outperformstranse in most setting.
even though mac is de-feated in some cases, the sample number in thosecases is too small to disprove the superiority of.
771 q r o h i w u l j k w p x o w l              )   v f r u h                        7 u d q ve 0 d f q r o h i w u l j k w p x o w l              )   v f r u h                              7 u d q ve 0 d f q r o h i w u l j k w p x o w l              )   v f r u h                              7 u d q ve 0 d ffigure 8: performance on different interval length..(a) cadec.
(b) cadec.
(c) share 13.
(d) share 13.
(e) share 14.
(f) share 14.mac.
for example, on cadec, transe outper-forms mac when span length is 8, but the samplenumber in the test set is only 10..we ﬁgure out an interesting phenomenon: bothmac and transe show poor performance when in-terval length is 1 and span length is 3, even thoughthe corresponding training samples are sufﬁcientenough (see length = 1 in table 6 and length =3 in table 72).
this might result from two folds:(1) even though the training samples are sufﬁcient,their features and context are different from theones in the test set; (2) discontinuous mentionswith interval length equal to 1 are harder cases thanthe others, since only one word to separate the seg-ments makes these discontinuous mentions verysimilar to the continuous ones, which confuse themodel to treat them as a continuous mention.
weleave this problem to our future work..4.6.3 analysis on running speed.
table 8 shows the comparison of computational ef-ﬁciency between the sota model transe, transb,and our proposed mac.
all of these models are im-.
2for discontinuous mentions, when span length is 3, the.
interval length can only be 1..model.
cadec.
share 13.share 14.transbtransemac.
33.9 sen/s33.4 sen/s29.1 sen/s36.3 sen/s40.3 sen/s40.6 sen/s193.3 sen/s 200.2 sen/s 198.1 sen/s.
table 8: comparison on running speed.
sen/s refers tothe number of sentences can be processed per second..plemented by pytorch and ran on a single teslav100 gpu environment.
as we can see, theprediction speed of mac is around 5 times fasterthan transe.
since the transition-based model em-ploys a stack to store partially processed spans anda buffer to store unprocessed tokens (dai et al.,2020b), it is difﬁcult to utilize gpu parallel com-puting to speed up the extraction process.
in theofﬁcial implementation, transe is restricted to pro-cesses one token at a time, which means it is se-riously inefﬁcient and difﬁcult to deploy in realdevelopment environment.
by contrast, mac is ca-pable of handling data in batch mode because it isa single-stage sequence labeling model in essence..5 conclusion.
in this paper, we reformulate discontinuous neras the task of discovering maximal cliques in a seg-ment graph, and propose a novel mac architecture.
it decomposes the construction of segment graphas two independent 2-d grid tagging problems, andsolves them jointly in one stage, addressing theexposure bias issue in previous studies.
extensiveexperiments on three benchmark datasets show thatmac beats the previous sota method by as muchas 3.5 pts in f1, while being 5 times faster.
furtheranalysis demonstrates the ability of our model inrecognizing discontinuous and overlapping entitymentions.
in the future, we would like to exploresimilar formulation in other information extractiontasks, such as event extraction and nested ner..acknowledgments.
we thank the reviewers for their insightful sug-gestions.
this work is supported by the nationalkey research and development program of chinathe guangdong(grant no.2017yfb0802804),province key area research and development pro-gram of china (grant no.2019b010137004), theyouth innovation promotion association of chi-nese academy of sciences (grant no.2021153),and the key program of national natural sciencefoundation of china (grant no.u1766215)..7721234567+interval length0102030405060f1 scoretransemac3456789+span length010203040506070f1 scoretransemac1234567+interval length20304050607080f1 scoretransemac3456789+span length1020304050607080f1 scoretransemac1234567+interval length304050607080f1 scoretransemac3456789+span length3040506070f1 scoretransemacreferences.
emily alsentzer, john r murphy, willie boag, wei-hung weng, di jin, tristan naumann, and matthewmcdermott.
2019. publicly available clinical bertembeddings..jimmy lei ba, jamie ryan kiros, and geoffrey e hin-arxiv preprint.
ton.
2016. layer normalization.
arxiv:1607.06450..sarvnaz karimi, alejandro metke-jimenez, madonnakemp, and chen wang.
2015. cadec: a corpus ofadverse drug event annotations.
journal of biomedi-cal informatics..mahboob alam khalid, valentin jijkoun, and maartende rijke.
2008. the impact of named entity normal-ization on information retrieval for question answer-ing.
in proceedings of ecir..adam berger and john lafferty.
2017. information re-in proceedings of.
trieval as statistical translation.
acm sigir..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..vladimir boginski, sergiy butenko, and panos mpardalos.
2005. statistical analysis of ﬁnancial net-works.
computational statistics & data analysis..coen bron and joep kerbosch.
1973. algorithm 457:ﬁnding all cliques of an undirected graph.
commu-nications of the acm..jason pc chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
transac-tions of the association for computational linguis-tics..xiang dai, sarvnaz karimi, ben hachey, and cecileparis.
2020a.
cost-effective selection of pretrainingdata: a case study of pretraining bert on social me-dia.
in proceedings of emnlp: findings..xiang dai, sarvnaz karimi, ben hachey, and cecileparis.
2020b.
an effective transition-based modelfor discontinuous ner.
in proceedings of acl..xiang dai, sarvnaz karimi, and cecile paris.
2017.medication and adverse event extraction from noisytext.
in proceedings of the australasian languagetechnology association workshop..sourav dutta and juho lauri.
2019. finding a max-imum clique in dense graphs via χ2 statistics.
inproceedings of cikm..mourad gridach.
2017. character-level neural networkfor biomedical named entity recognition.
journal ofbiomedical informatics..tao gui, ruotian ma, qi zhang, lujun zhao, yu-gangjiang, and xuanjing huang.
2019. cnn-based chi-nese ner with lexicon rethinking.
in proceedings ofijcai..sepp hochreiter and j¨urgen schmidhuber.
1997. long.
short-term memory.
neural computation..zhiheng huang, wei xu, and kai yu.
2015. bidirec-tional lstm-crf models for sequence tagging.
arxivpreprint arxiv:1508.01991..alejandro metke-jimenez and sarvnaz karimi.
2016.concept identiﬁcation and normalisation for adversedrug event discovery in medical forums.
in proceed-ings of iswc..danielle l mowery, sumithra velupillai, brett r south,lee christensen, david martinez, liadh kelly, lor-raine goeuriot, noemie elhadad, sameer pradhan,guergana savova, et al.
2014. task 2: share/clefin proceedings ofehealth evaluation lab 2014.clef..aldrian obaja muis and wei lu.
2016. learning torecognize discontiguous entities.
in proceedings ofemnlp..sameer pradhan, no´emie elhadad, brett r south,david martinez, lee christensen, amy vogel,hanna suominen, wendy w chapman, and guer-gana savova.
2015. evaluating the state of the art indisorder recognition and normalization of the clini-cal narrative.
journal of the american medical in-formatics association..sameer pradhan, noemie elhadad, brett r south,david martinez, lee m christensen, amy vogel,hanna suominen, wendy w chapman, and guer-gana k savova.
2013. task 1: share/clef ehealthevaluation lab 2013. in proceedings of clef..volker stix.
2004. finding all maximal cliques in dy-namic graphs.
computational optimization and ap-plications..jianlin su.
2019. conditional text generation based on.
conditional layer normalization..buzhou tang, hongxin cao, yonghui wu, min jiang,and hua xu.
2013. recognizing clinical entities inhospital discharge summaries using structural sup-port vector machines with word representation fea-in bmc medical informatics and decisiontures.
making.
springer..tales imbiriba, jos´e carlos moreira bermudez, andcedric richard.
2017. band selection for nonlin-ear unmixing of hyperspectral images as a maximalclique problem.
ieee transactions on image pro-cessing..buzhou tang, jianglu hu, xiaolong wang, and qing-cai chen.
2018. recognizing continuous and discon-tinuous adverse drug reaction mentions from socialmedia using lstm-crf.
wireless communications andmobile computing..773bailin wang and wei lu.
2019. combining spansinto entities: a neural two-stage approach for rec-in proceedings ofognizing discontiguous entities.
emnlp..yucheng wang, bowen yu, yueyang zhang, tingwenliu, hongsong zhu, and limin sun.
2020. tplinker:single-stage joint extraction of entities and relationsthrough token pair linking.
in proceedings of col-ing..mengge xue, bowen yu, zhenyu zhang, tingwen liu,yue zhang, and bin wang.
2020. coarse-to-ﬁne pre-in proceed-training for named entity recognition.
ings of emnlp..bowen yu, zhenyu zhang, tingwen liu, bin wang,sujian li, and quangang li.
2019. beyond wordattention: using segment attention in neural relationextraction.
in proceedings of ijcai..bowen yu, zhenyu zhang, jiawei sheng, tingwen liu,yubin wang, yucheng wang, and bin wang.
2021.in proceedingssemi-open information extraction.
of the web conference..bowen yu, zhenyu zhang, and jianlin su.
2020. jointextraction of entities and relations based on a noveldecomposition strategy.
in proceedings of ecai..wen zhang, yang feng, fandong meng, di you, andqun liu.
2019. bridging the gap between trainingand inference for neural machine translation.
in pro-ceedings of acl..yue zhang and jie yang.
2018. chinese ner using lat-.
tice lstm.
in proceedings of acl..774