a survey of race, racism, and anti-racism in nlp.
anjalie fieldcarnegie mellon universityanjalief@cs.cmu.edu.
su lin blodgettmicrosoft researchsulin.blodgett@microsoft.com.
zeerak waseemuniversity of shefﬁeldz.w.butt@sheffield.ac.uk.
yulia tsvetkovuniversity of washingtonyuliats@cs.washington.edu.
abstract.
despite inextricable ties between race and lan-guage, little work has considered race in nlpresearch and development.
in this work, wesurvey 79 papers from the acl anthology thatmention race.
these papers reveal varioustypes of race-related bias in all stages of nlpmodel development, highlighting the need forproactive consideration of how nlp systemscan uphold racial hierarchies.
however, per-sistent gaps in research on race and nlp re-main: race has been siloed as a niche topicand remains ignored in many nlp tasks; mostwork operationalizes race as a ﬁxed single-dimensional variable with a ground-truth label,which risks reinforcing differences producedby historical racism; and the voices of histor-ically marginalized people are nearly absent innlp literature.
by identifying where and hownlp literature has and has not considered race,especially in comparison to related ﬁelds, ourwork calls for inclusion and racial justice innlp research practices..1.introduction.
race and language are tied in complicated ways.
raciolinguistics scholars have studied how they aremutually constructed: historically, colonial pow-ers construct linguistic and racial hierarchies tojustify violence, and currently, beliefs about theinferiority of racialized people’s language practicescontinue to justify social and economic exclusion(rosa and flores, 2017).1 furthermore, languageis the primary means through which stereotypesand prejudices are communicated and perpetuated(hamilton and trolier, 1986; bar-tal et al., 2013).
however, questions of race and racial biashave been minimally explored in nlp literature..1we use racialization to refer the process of “ascribing andprescribing a racial category or classiﬁcation to an individualor group of people .
.
.
based on racial attributes including butnot limited to cultural and social history, physical features,and skin color” (hudley, 2017)..while researchers and activists have increasinglydrawn attention to racism in computer science andacademia, frequently-cited examples of racial biasin ai are often drawn from disciplines other thannlp, such as computer vision (facial recognition)(buolamwini and gebru, 2018) or machine learn-ing (recidivism risk prediction) (angwin et al.,2016).
even the presence of racial biases in searchengines like google (sweeney, 2013; noble, 2018)has prompted little investigation in the acl com-munity.
work on nlp and race remains sparse,particularly in contrast to concerns about genderbias, which have led to surveys, workshops, andshared tasks (sun et al., 2019; webster et al., 2019).
in this work, we conduct a comprehensive sur-vey of how nlp literature and research practicesengage with race.
we ﬁrst examine 79 papers fromthe acl anthology that mention the words ‘race’,‘racial’, or ‘racism’ and highlight examples of howracial biases manifest at all stages of nlp modelpipelines (§3).
we then describe some of the limi-tations of current work (§4), speciﬁcally showingthat nlp research has only examined race in a nar-row range of tasks with limited or no social context.
finally, in §5, we revisit the nlp pipeline with a fo-cus on how people generate data, build models, andare affected by deployed systems, and we highlightcurrent failures to engage with people traditionallyunderrepresented in stem and academia..while little work has examined the role of racein nlp speciﬁcally, prior work has discussed racein related ﬁelds, including human-computer in-teraction (hci) (ogbonnaya-ogburu et al., 2020;rankin and thomas, 2019; schlesinger et al.,2017), fairness in machine learning (hanna et al.,2020), and linguistics (hudley et al., 2020; motha,2020).
we draw comparisons and guidance fromthis work and show its relevance to nlp research.
our work differs from nlp-focused related workon gender bias (sun et al., 2019), ‘bias’ generally.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages1905–1925august1–6,2021.©2021associationforcomputationallinguistics1905(blodgett et al., 2020), and the adverse impacts oflanguage models (bender et al., 2021) in its explicitfocus on race and racism..in surveying research in nlp and related ﬁelds,we ultimately ﬁnd that nlp systems and researchpractices produce differences along racialized lines.
our work calls for nlp researchers to considerthe social hierarchies upheld and exacerbated bynlp research and to shift the ﬁeld toward “greaterinclusion and racial justice” (hudley et al., 2020)..2 what is race?.
it has been widely accepted by social scientists thatrace is a social construct, meaning it “was broughtinto existence or shaped by historical events, socialforces, political power, and/or colonial conquest”rather than reﬂecting biological or ‘natural’ differ-ences (hanna et al., 2020).
more recent work hascriticized the “social construction” theory as circu-lar and rooted in academic discourse, and insteadreferred to race as “colonial constituted practices”,including “an inherited western, modern-colonialpractice of violence, assemblage, superordination,exploitation and segregation” (saucier et al., 2016).
the term race is also multi-dimensional andcan refer to a variety of different perspectives, in-cluding racial identity (how you self-identify), ob-served race (the race others perceive you to be),and reﬂected race (the race you believe others per-ceive you to be) (roth, 2016; hanna et al., 2020;ogbonnaya-ogburu et al., 2020).
racial catego-rizations often differ across dimensions and dependon the deﬁned categorization schema.
for exam-ple, the united states census considers hispanican ethnicity, not a race, but surveys suggest that2/3 of people who identify as hispanic considerit a part of their racial background.2 similarly,the census does not consider ‘jewish’ a race, butsome nlp work considers anti-semitism a formof racism (hasanuzzaman et al., 2017).
race de-pends on historical and social context—there areno ‘ground truth’ labels or categories (roth, 2016).
as the work we survey primarily focuses on theunited states, our analysis similarly focuses on theu.s. however, as race and racism are global con-structs, some aspects of our analysis are applicableto other contexts.
we suggest that future studieson racialization in nlp ground their analysis in theappropriate geo-cultural context, which may result.
2https://bit.ly/3r9j1fo, https://pewrsr..ch/36vluel.
in ﬁndings or analyses that differ from our work..3 survey of nlp literature on race.
3.1 acl anthology papers about race.
in this section, we introduce our primary surveydata—papers from the acl anthology3—and wedescribe some of their major ﬁndings to empha-size that nlp systems encode racial biases.
wesearched the anthology for papers containing theterms ‘racial’, ‘racism’, or ‘race’, discarding onesthat only mentioned race in the references sectionor in data examples and adding related papers citedby the initial set if they were also in the acl an-thology.
in using keyword searches, we focus onpapers that explicitly mention race and considerpapers that use euphemistic terms to not have sub-stantial engagement on this topic.
as our focusis on nlp and the acl community, we do not in-clude nlp-related papers published in other venuesin the reported metrics (e.g.
table 1), but we dodraw from them throughout our analysis..our initial search identiﬁed 165 papers.
how-ever, reviewing all of them revealed that many donot deeply engage on the topic.
for example, 37papers mention ‘racism’ as a form of abusive lan-guage or use ‘racist’ as an offensive/hate speechlabel without further engagement.
30 papers onlymention race as future work, related work, or mo-tivation, e.g.
in a survey about gender bias, “non-binary genders as well as racial biases have largelybeen ignored in nlp” (sun et al., 2019).
afterdiscarding these types of papers, our ﬁnal analysisset consists of 79 papers.4.
table 1 provides an overview of the 79 papers,manually coded for each paper’s primary nlp taskand its focal goal or contribution.
we determinedtask/application labels through an iterative process:listing the main focus of each paper and then col-lapsing similar categories.
in cases where paperscould rightfully be included in multiple categories,we assign them to the best-matching one based onstated contributions and the percentage of the paperdevoted to each possible category.
in the appendixwe provide additional categorizations of the papers.
3the acl anthology includes papers from all ofﬁcialacl venues and some non-acl events listed in appendix a,as of december 2020 it included 6, 200 papers.
4we do not discard all papers about abusive language, onlyones that exclusively use racism/racist as a classiﬁcation label.
we retain papers with further engagement, e.g.
discussionsof how to deﬁne racism or identiﬁcation of racial bias in hatespeech classiﬁers..1906abusive languagesocial science/social mediatext representations (lms, embeddings)text generation (dialogue, image captions, story gen. )sector-speciﬁc nlp applications (edu., law, health)ethics/task-independent biascore nlp applications (parsing, nli, ie)total.
suproc.tcelloc.62--11111.suprocezylana.
4102-2--18.ledompoleved.26-1-1111.noitisop/yevrus.21-132-9.saibed.2-211118.saib.tceted.5195-1122.latot.212013876479.table 1: 79 papers on race or racism from the acl anthology, categorized by nlp application and focal task..according to publication year, venue, and racialcategories used, as well as the full list of 79 papers..3.2 nlp systems encode racial bias.
next, we present examples that identify racial biasin nlp models, focusing on 5 parts of a standardnlp pipeline: data, data labels, models, model out-puts, and social analyses of outputs.
we includepapers described in table 1 and also relevant liter-ature beyond the acl anthology (e.g.
neurips,pnas, science).
these examples are not intendedto be exhaustive, and in §4 we describe some of theways that nlp literature has failed to engage withrace, but nevertheless, we present them as evidencethat nlp systems perpetuate harmful biases alongracialized lines..data a substantial amount of prior work has al-ready shown how nlp systems, especially wordembeddings and language models, can absorb andamplify social biases in data sets (bolukbasi et al.,2016; zhao et al., 2017).
while most work focuseson gender bias, some work has made similar ob-servations about racial bias (rudinger et al., 2017;garg et al., 2018; kurita et al., 2019).
these studiesfocus on how training data might describe racialminorities in biased ways, for example, by exam-ining words associated with terms like ‘black’ ortraditionally european/african american names(caliskan et al., 2017; manzini et al., 2019).
somestudies additionally capture who is described, re-vealing under-representation in training data, some-times tangentially to primary research questions:rudinger et al.
(2017) suggest that gender bias maybe easier to identify than racial or ethnic bias innatural language inference data sets because of.
data sparsity, and caliskan et al.
(2017) alter theimplicit association test stimuli that they use tomeasure biases in word embeddings because someafrican american names were not frequent enoughin their corpora..an equally important consideration, in additionto whom the data describes is who authored thedata.
for example, blodgett et al.
(2018) showthat parsing systems trained on white mainstreamamerican english perform poorly on africanamerican english (aae).5 in a more general exam-ple, wikipedia has become a popular data sourcefor many nlp tasks.
however, surveys suggestthat wikipedia editors are primarily from white-majority countries,6 and several initiatives havepointed out systemic racial biases in wikipediacoverage (adams et al., 2019; field et al., 2021).7models trained on these data only learn to processthe type of text generated by these users, and fur-ther, only learn information about the topics theseusers are interested in.
the representativeness ofdata sets is a well-discussed issue in social-orientedtasks, like inferring public opinion (olteanu et al.,2019), but this issue is also an important considera-tion in ‘neutral’ tasks like parsing (waseem et al.,2021).
the type of data that researchers chooseto train their models on does not just affect whatdata the models perform well for, it affects whatpeople the models work for.
nlp researchers can-not assume models will be useful or function formarginalized people unless they are trained on data.
5we note that conceptualizations of aae and the accom-panying terminology for the variety have shifted considerablyin the last half century; see king (2020) for an overview..6https://bit.ly/2yv07il7https://bit.ly/3j2weza.
1907generated by them..data labels although model biases are oftenblamed on raw data, several of the papers we surveyidentify biases in the way researchers categorize orobtain data annotations.
for example:.
• annotation schema returning to blodgettet al.
(2018), this work deﬁnes new parsingstandards for formalisms common in aae,demonstrating how parsing labels themselveswere not designed for racialized language va-rieties..• annotation instructions sap et al.
(2019)show that annotators are less likely to labeltweets using aae as offensive if they aretold the likely language varieties of the tweets.
thus, how annotation schemes are designed(e.g.
what contextual information is provided)can impact annotators’ decisions, and fail-ing to provide sufﬁcient context can resultin racial biases..• annotator selection waseem (2016) showthat feminist/anti-racist activists assign differ-ent offensive language labels to tweets thanﬁgure-eight workers, demonstrating that an-notators’ lived experiences affect data annota-tions..models some papers have found evidence thatmodel instances or architectures can change theracial biases of outputs produced by the model.
sommerauer and fokkens (2019) ﬁnd that the wordembedding associations around words like ‘race’and ‘racial’ change not only depending on themodel architecture used to train embeddings, butalso on the speciﬁc model instance used to extractthem, perhaps because of differing random seeds.
kiritchenko and mohammad (2018) examine gen-der and race biases in 200 sentiment analysis sys-tems submitted to a shared task and ﬁnd differentlevels of bias in different systems.
as the train-ing data for the shared task was standardized, allmodels were trained on the same data.
however,participants could have used external training dataor pre-trained embeddings, so a more detailed in-vestigation of results is needed to ascertain whichfactors most contribute to disparate performance..model outputs several papers focus on modeloutcomes, and how nlp systems could perpetuateand amplify bias if they are deployed:.
• classiﬁers trained on common abusive lan-guage data sets are more likely to label tweets.
containing characteristics of aae as offensive(davidson et al., 2019; sap et al., 2019).
• classiﬁers for abusive language are morelikely to label text containing identity termslike ‘black’ as offensive (dixon et al., 2018).
• gpt outputs text with more negative senti-ment when prompted with aae -like inputs(groenwold et al., 2020)..social analyses of outputs while the examplesin this section primarily focus on racial biases intrained nlp systems, other work (e.g.
includedin ‘social science/social media’ in table 1) usesnlp tools to analyze race in society.
examples in-clude examining how commentators describe foot-ball players of different races (merullo et al., 2019)or how words like ‘prejudice’ have changed mean-ing over time (vylomova et al., 2019)..while differing in goals, this work is often sus-ceptible to the same pitfalls as other nlp tasks.
one area requiring particular caution is in the in-terpretation of results produced by analysis models.
for example, while word embeddings have becomea common way to measure semantic change or es-timate word meanings (garg et al., 2018), josephand morgan (2020) show that embedding associ-ations do not always correlate with human opin-ions; in particular, correlations are stronger for be-liefs about gender than race.
relatedly, in hci,the recognition that authors’ own biases can affecttheir interpretations of results has caused some au-thors to provide self-disclosures (schlesinger et al.,2017), but this practice is uncommon in nlp..we conclude this section by observing that whenresearchers have looked for racial biases in nlpsystems, they have usually found them.
this litera-ture calls for proactive approaches in consideringhow data is collected, annotated, used, and inter-preted to prevent nlp systems from exacerbatinghistorical racial hierarchies..4 limitations in where and how nlp.
operationalizes race.
while §3 demonstrates ways that nlp systems en-code racial biases, we next identify gaps and limi-tations in how these works have examined racism,focusing on how and in what tasks researchers haveconsidered race.
we ultimately conclude that priornlp literature has marginalized research on raceand encourage deeper engagement with other ﬁelds,critical views of simpliﬁed classiﬁcation schema,.
1908and broader application scope in future work (blod-gett et al., 2020; hanna et al., 2020)..4.1 common data sets are narrow in scope.
the papers we surveyed suggest that research onrace in nlp has used a very limited range ofdata sets, which fails to account for the multi-dimensionality of race and simpliﬁcations inher-ent in classiﬁcation.
we identiﬁed 3 common datasources:8.
• 9 papers use a set of tweets with inferred prob-abilistic topic labels based on alignment withu.s. census race/ethnicity groups (or the pro-vided inference model) (blodgett et al., 2016).
• 11 papers use lists of names drawn fromsweeney (2013), caliskan et al.
(2017), orgarg et al.
(2018).
most commonly, 6 pa-pers use african/european american namesfrom the word embedding association test(weat) (caliskan et al., 2017), which in turndraws data from greenwald et al.
(1998) andbertrand and mullainathan (2004)..• 10 papers use explicit keywords like ‘blackwoman’, often placed in templates like “i am a” to test if model performance remains.
the same for different identity terms..while these commonly-used data sets can iden-tify performance disparities, they only capture anarrow subset of the multiple dimensions of race(§2).
for example, none of them capture self-identiﬁed race.
while observed race is often appro-priate for examining discrimination and some typesof disparities, it is impossible to assess potentialharms and beneﬁts of nlp systems without assess-ing their performance over text generated by anddirected to people of different races.
the corpusfrom blodgett et al.
(2016) does serve as a start-ing point and forms the basis of most current workassessing performance gaps in nlp models (sapet al., 2019; blodgett et al., 2018; xia et al., 2020;xu et al., 2019; groenwold et al., 2020), but eventhis corpus is explicitly not intended to infer race.
furthermore, names and hand-selected iden-tity terms are not sufﬁcient for uncovering modelbias.
de-arteaga et al.
(2019) show this in ex-amining gender bias in occupation classiﬁcation:when overt indicators like names and pronouns arescrubbed from the data, performance gaps and po-tential allocational harms still remain.
names also.
8we provide further counts of what racial categories papers.
use and how they operationalize them in appendix b..generalize poorly.
while identity terms can be ex-amined across languages (van miltenburg et al.,2017), differences in naming conventions often donot translate, leading some studies to omit examin-ing racial bias in non-english languages (lauscherand glavaˇs, 2019).
even within english, names of-ten fail to generalize across domains, geographies,and time.
for example, names drawn from theu.s. census generalize poorly to twitter (wood-doughty et al., 2018), and names common amongblack and white children were not distinctly differ-ent prior to the 1970s (fryer jr and levitt, 2004;sweeney, 2013)..we focus on these 3 data sets as they weremost common in the papers we surveyed, butwe note that others exist.
preot¸iuc-pietro andungar (2018) provide a data set of tweets withself-identiﬁed race of their authors, though it islittle used in subsequent work and focused ondemographic prediction, rather than evaluatingmodel performance gaps.
two recently-releaseddata sets (nadeem et al., 2020; nangia et al.,2020) provide crowd-sourced pairs of more- andless-stereotypical text.
more work is needed tounderstand any privacy concerns and the strengthsand limitations of these data (blodgett et al., 2021).
additionally, some papers collect domain-speciﬁcdata, such as self-reported race in an online com-munity (loveys et al., 2018), or crowd-sourcedannotations of perceived race of football players(merullo et al., 2019).
while these works offerclear contextualization, it is difﬁcult to use thesedata sets to address other research questions..4.2 classiﬁcation schemes operationalizerace as a ﬁxed, single-dimensionalu.s.-census label.
work that uses the same few data sets inevitablyalso uses the same few classiﬁcation schemes, oftenwithout justiﬁcation.
the most common explicitlystated source of racial categories is the u.s. census,which reﬂects the general trend of u.s.-centrismin nlp research (the vast majority of work we sur-veyed also focused on english).
while census cate-gories are sometimes appropriate, repeated use ofclassiﬁcation schemes and accompanying data setswithout considering who deﬁned these schemesand whether or not they are appropriate for the cur-rent context risks perpetuating the misconceptionthat race is ‘natural’ across geo-cultural contexts.
we refer to hanna et al.
(2020) for a more thorough.
1909overview of the harms of “widespread uncriticaladoption of racial categories,” which “can in turnre-entrench systems of racial stratiﬁcation whichgive rise to real health and social inequalities.” atbest, the way race has been operationalized in nlpresearch is only capable of examining a narrow sub-set of potential harms.
at worst, it risks reinforcingracism by presenting racial divisions as natural,rather than the product of social and historical con-text (bowker and star, 2000)..as an example of questioning who devised racialcategories and for what purpose, we consider thepattern of re-using names from greenwald et al.
(1998), who describe their data as sets of names“judged by introductory psychology students to bemore likely to belong to white americans than toblack americans” or vice versa.
when incorpo-rating this data into weat, caliskan et al.
(2017)discard some judged african american names astoo infrequent in their embedding data.
work sub-sequently drawing from weat makes no mentionof the discarded names nor contains much discus-sion of how the data was generated and whether ornot names judged to be white or black by introduc-tory psychology students in 1998 are an appropriatebenchmark for the studied task.
while gatheringdata to examine race in nlp is challenging, and inthis work we ourselves draw from examples thatuse greenwald et al.
(1998), it is difﬁcult to inter-pret what implications arise when models exhibitdisparities over this data and to what extent modelswithout disparities can be considered ‘debiased’..finally, almost all of the work we examined con-ducts single-dimensional analyses, e.g.
focus onrace or gender but not both simultaneously.
thisfocus contrasts with the concept of intersection-ality, which has shown that examining discrim-ination along a single axis fails to capture theexperiences of people who face marginalizationalong multiple axes.
for example, considerationof race often emphasizes the experience of gender-privileged people (e.g.
black men), while consid-eration of gender emphasizes the experience ofrace-privileged people (e.g.
white women).
nei-ther reﬂect the experience of people who face dis-crimination along both axes (e.g.
black women)(crenshaw, 1989).
a small selection of papers haveexamined intersectional biases in embeddings orword co-occurrences (herbelot et al., 2012; mayet al., 2019; tan and celis, 2019; lepori, 2020), butwe did not identify mentions of intersectionality in.
any other nlp research areas.
further, several ofthese papers use nlp technology to examine or val-idate theories on intersectionality; they do not drawfrom theory on intersectionality to critically exam-ine nlp models.
these omissions can mask harms:jiang and fellbaum (2020) provide an example us-ing word embeddings of how failing to consider in-tersectionality can render invisible people marginal-ized in multiple ways.
numerous directions remainfor exploration, such as how ‘debiasing’ modelsalong one social dimension affects other dimen-sions.
surveys in hci offer further frameworkson how to incorporate identity and intersectional-ity into computational research (schlesinger et al.,2017; rankin and thomas, 2019)..4.3 nlp research on race is restricted tospeciﬁc tasks and applications.
finally, table 1 reveals many common nlp appli-cations where race has not been examined, such asmachine translation, summarization, or question an-swering.9 while some tasks seem inherently morerelevant to social context than others (a claim wedispute in this work, particularly in §5), research onrace is compartmentalized to limited areas of nlpeven in comparison with work on ‘bias’.
for exam-ple, blodgett et al.
(2020) identify 20 papers thatexamine bias in co-reference resolution systemsand 8 in machine translation, whereas we identify0 papers in either that consider race.
instead, raceis most often mentioned in nlp papers in the con-text of abusive language, and work on detecting orremoving bias in nlp models has focused on wordembeddings..overall, our survey identiﬁes a need for the ex-amination of race in a broader range of nlp tasks,the development of multi-dimensional data sets,and careful consideration of context and appropri-in general, race isateness of racial categories.
difﬁcult to operationalize, but nlp researchers donot need to start from scratch, and can instead drawfrom relevant work in other ﬁelds..5 nlp propagates marginalization of.
racialized people.
while in §4 we primarily discuss race as a topic ora construct, in this section, we consider the role, ormore pointedly, the absence, of traditionally under-represented people in nlp research..9we identiﬁed only 8 relevant papers on text generation,which focus on other areas including chat bots, gpt-2/3, hu-mor generation, and story generation..19105.1 people create data.
as discussed in §3.2, data and annotations are gen-erated by people, and failure to consider who cre-ated data can lead to harms.
in §3.2 we identifya need for diverse training data in order to ensuremodels work for a diverse set of people, and in §4we describe a similar need for diversity in data thatis used to assess algorithmic fairness.
however,gathering this type of data without consideration ofthe people who generated it can introduce privacyviolations and risks of demographic proﬁling..as an example, in 2019, partially in responseto research showing that facial recognition al-gorithms perform worse on darker-skinned thanlighter-skinned people (buolamwini and gebru,2018; raji and buolamwini, 2019), researchersat ibm created the “diversity in faces” data set,which consists of 1 million photos sampled fromthe the publicly available yfcc-100m data set andannotated with “craniofacial distances, areas andratios, facial symmetry and contrast, skin color,age and gender predictions” (merler et al., 2019).
while this data set aimed to improve the fairnessof facial recognition technology, it included pho-tos collected from a flickr, a photo-sharing web-site whose users did not explicitly consent for thisuse of their photos.
some of these users ﬁled alawsuit against ibm, in part for “subjecting themto increased surveillance, stalking, identity theft,and other invasions of privacy and fraud.”10 nlpresearchers could easily repeat this incident, forexample, by using demographic proﬁling of socialmedia users to create more diverse data sets.
whileobtaining diverse, representative, real-world datasets is important for building models, data mustbe collected with consideration for the people whogenerated it, such as obtaining informed consent,setting limits of uses, and preserving privacy, aswell as recognizing that some communities maynot want their data used for nlp at all (paullada,2020)..5.2 people build models.
research is additionally carried out by people whodetermine what projects to pursue and how toapproach them.
while statistics on acl confer-ences and publications have focused on geographic.
10https://bit.ly/3r3luik.
https://nbcnews.to/3j5hi39 ibm has since re-moved the “diversity in faces” data set as well as their “detectfaces” public api and stopped their use of and research onfacial recognition.
https://bit.ly/3j2jv4i.
representation rather than race, they do highlightunder-representation.
out of 2, 695 author afﬁli-ations associated with papers in the acl anthol-ogy for 5 major conferences held in 2018, only 5(0.2%) were from africa, compared with 1, 114from north america (41.3%).11 statistics pub-lished for 2017 conference attendees and acl fel-lows similarly reveal a much higher percentageof people from “north, central and south amer-ica” (55% attendees / 74% fellows) than from “eu-rope, middle east and africa” (19%/13%) or “asia-paciﬁc” (23%/13%).12 these broad regional cate-gories likely mask further under-representation, e.g.
percentage of attendees and fellows from africaas compared to europe.
according to an nsf re-port that includes racial statistics rather than na-tionality, 14% of doctorate degrees in computerscience awarded by u.s. institutions to u.s. cit-izens and permanent residents were awarded toasian students, < 4% to black or african ameri-can students, and 0% to american indian or alaskanative students (national center for science andengineering statistics, 2019).13.it is difﬁcult to envision reducing or eliminatingracial differences in nlp systems without changesin the researchers building these systems.
onetheory that exempliﬁes this challenge is interestconvergence, which suggests that people in posi-tions of power only take action against systematicproblems like racism when it also advances theirown interests (bell jr, 1980).
ogbonnaya-ogburuet al.
(2020) identify instances of interest conver-gence in the hci community, primarily in diversityinitiatives that beneﬁt institutions’ images ratherthan underrepresented people.
in a research setting,interest convergence can encourage studies of incre-mental and surface-level biases while discouragingresearch that might be perceived as controversialand force fundamental changes in the ﬁeld..demographic statistics are not sufﬁcient foravoiding pitfalls like interest convergence, as theyfail to capture the lived experiences of researchers.
ogbonnaya-ogburu et al.
(2020) provide severalexamples of challenges that non-white hci re-searchers have faced, including the invisible laborof representing ‘diversity’, everyday microaggres-.
11http://www.marekrei.com/blog/.
geographic-diversity-of-nlp-conferences/.
12https://www.aclweb.org/portal/content/acl-diversity-statistics.
13results exclude respondents who did not report race orethnicity or were native hawaiian or other paciﬁc islander..1911sions, and altering their research directions in ac-cordance with their advisors’ interests.
rankin andthomas (2019) further discuss how research con-ducted by people of different races is perceived dif-ferently: “black women in academia who conductresearch about the intersections of race, gender,class, and so on are perceived as ‘doing service,’whereas white colleagues who conduct the same re-search are perceived as doing cutting-edge researchthat demands attention and recognition.” while wedraw examples about race from hci in the absenceof published work on these topics in nlp, the lackof linguistic diversity in nlp research similarlydemonstrates how representation does not neces-sarily imply inclusion.
although researchers fromvarious parts of the world (asia, in particular) dohave some numerical representation among aclauthors, attendees, and fellows, nlp research over-whelmingly favors a small set of languages, witha heavy skew towards european languages (joshiet al., 2020) and ‘standard’ language varieties (ku-mar et al., 2021)..5.3 people use models.
finally, nlp research produces technology that isused by people, and even work without direct ap-plications is typically intended for incorporationinto application-based systems.
with the recogni-tion that technology ultimately affects people, re-searchers on ethics in nlp have increasingly calledfor considerations of whom technology might harmand suggested that there are some nlp technolo-gies that should not be built at all.
in the context ofperpetuating racism, examples include criticism oftools for predicting demographic information (tat-man, 2020) and automatic prison term prediction(leins et al., 2020), motivated by the history ofusing technology to police racial minorities and re-lated criticism in other ﬁelds (browne, 2015; buo-lamwini and gebru, 2018; mcilwain, 2019).
incases where potential harms are less direct, theyare often unaddressed entirely.
for example, whilelow-resource nlp is a large area of research, apaper on machine translation of white americanand european languages is unlikely to discuss howcontinual model improvements in these settings in-crease technological inequality.
little work on low-resource nlp has focused on the realities of struc-tural racism or differences in lived experience andhow they might affect the way technology shouldbe designed..detection of abusive language offers an infor-mative case study on the danger of failing to con-sider people affected by technology.
work on abu-sive language often aims to detect racism for con-tent moderation (waseem and hovy, 2016).
how-ever, more recent work has show that existing hatespeech classiﬁers are likely to falsely label text con-taining identity terms like ‘black’ or text containinglinguistic markers of aae as toxic (dixon et al.,2018; sap et al., 2019; davidson et al., 2019; xiaet al., 2020).
deploying these models could censorthe posts of the very people they purport to help..in other areas of statistics and machine learning,focus on participatory design has sought to am-plify the voices of people affected by technologyand its development.
an icml 2020 workshoptitled “participatory approaches to machine learn-ing” highlights a number of papers in this area(kulynych et al., 2020; brown et al., 2019).
afew related examples exist in nlp, e.g.
gupta et al.
(2020) gather data for an interactive dialogue agentintended to provide more accessible informationabout heart failure to hispanic/latinx and africanamerican patients.
the authors engage with health-care providers and doctors, though they leave focalgroups with patients for future work.
while nlpresearchers may not be best situated to examinehow people interact with deployed technology, theycould instead draw motivation from ﬁelds that havestronger histories of participatory design, such ashci.
however, we did not identify citing participa-tory design studies conducted by others as commonpractice in the work we surveyed.
as in the caseof researcher demographics, participatory design isnot an end-all solution.
sloane et al.
(2020) providea discussion of how participatory design can col-lapse to ‘participation-washing’ and how such workmust be context-speciﬁc, long-term, and genuine..6 discussion.
we conclude by synthesizing some of the obser-vations made in the preceding sections into moreactionable items.
first, nlp research needs toexplicitly incorporate race.
we quote benjamin(2019): “[technical systems and social codes] op-erate within powerful systems of meaning that ren-der some things visible, others invisible, and createa vast array of distortions and dangers.”.
in the context of nlp research, this philosophyimplies that all technology we build works in ser-vice of some ideas or relations, either by upholding.
1912them or dismantling them.
any research that isnot actively combating prevalent social systemslike racism risks perpetuating or exacerbating them.
our work identiﬁes several ways in which nlpresearch upholds racism:.
• systems contain representational harms andperformance gaps throughout nlp pipelines• research on race is restricted to a narrow sub-set of tasks and deﬁnitions of race, which canmask harms and falsely reify race as ‘natural’• traditionally underrepresented people are ex-cluded from the research process, both as con-sumers and producers of technology.
furthermore, while we focus on race, whichwe note has received substantially less attentionthan gender, many of the observations in this workhold for social characteristics that have receivedeven less attention in nlp research, such as so-cioeconomic class, disability, or sexual orientation(mendelsohn et al., 2020; hutchinson et al., 2020).
nevertheless, none of these challenges can be ad-dressed without direct engagement with marginal-ized communities of color.
nlp researchers candraw on precedents for this type of engagementfrom other ﬁelds, such as participatory design andvalue sensitive design models (friedman et al.,2013).
additionally, numerous organizations al-ready exist that serve as starting points for partner-ships, such as black in ai, masakhane, data forblack lives, and the algorithmic justice league.
finally, race and language are complicated, andwhile readers may look for clearer recommenda-tions, no one data set, model, or set of guidelinescan ‘solve’ racism in nlp.
for instance, whilewe draw from linguistics, hudley et al.
(2020) inturn call on linguists to draw models of racial jus-tice from anthropology, sociology, and psychol-ogy.
relatedly, there are numerous racialized ef-fects that nlp research can have that we do notaddress in this work; for example, bender et al.
(2021) and strubell et al.
(2019) discuss the envi-ronmental costs of training large language models,and how global warming disproportionately affectsmarginalized communities.
we suggest that read-ers use our work as one starting point for bringinginclusion and racial justice into nlp..acknowledgements.
been supported in part by the canada 150 researchchair program and the uk-canada artiﬁcial intel-ligence initiative.
a.f.
has been supported in partby a google phd fellowship and a grfp undergrant no.
dge1745016.
this material is basedupon work supported in part by the national sci-ence foundation under grants no.
iis2040926 andiis2007960.
any opinions, ﬁndings, and conclu-sions or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreﬂect the views of the nsf..7 ethical considerations.
we, the authors of this work, are situated in thecultural contexts of the united states of americaand the united kingdom/europe, and some of usidentify as people of color.
we all identify as nlpresearchers, and we acknowledge that we are situ-ated within the traditionally exclusionary practicesof academic research.
these perspectives have im-pacted our work, and there are viewpoints outsideof our institutions and experiences that our workmay not fully represent..references.
julia adams, hannah br¨uckner, and cambria naslund.
2019. who counts as a notable sociologist onwikipedia?
gender, race, and the “professor test”.
socius, 5..silvio amir, mark dredze, and john w. ayers.
2019.mental health surveillance over social media within proceedings of the sixth work-digital cohorts.
shop on computational linguistics and clinical psy-chology, pages 114–120, minneapolis, minnesota.
association for computational linguistics..julia angwin, jeff larson, surya mattu, and laurenkirchner.
2016. machine bias: there’s softwareused across the country to predict future criminalsand it’s biased against blacks.
propublica..stavros assimakopoulos, rebecca vella muskat, lon-neke van der plas, and albert gatt.
2020. annotat-ing for hate speech: the maneco corpus and somein proceed-input from critical discourse analysis.
ings of the 12th language resources and evaluationconference, pages 5088–5097, marseille, france.
european language resources association..daniel bar-tal, carl f graumann, arie w kruglanski,and wolfgang stroebe.
2013. stereotyping and prej-udice: changing conceptions.
springer science &business media..we gratefully thank hanna kim, kartik goyal, ar-tidoro pagnoni, qinlan shen, and michael milleryoder for their feedback on this work.
z.w.
has.
francesco barbieri and jose camacho-collados.
2018.how gender and skin tone modiﬁers affect emoji se-in proceedings of the seventhmantics in twitter..1913joint conference on lexical and computational se-mantics, pages 101–106, new orleans, louisiana.
association for computational linguistics..derrick a bell jr. 1980. brown v. board of educationand the interest-convergence dilemma.
harvard lawreview, pages 518–533..emily bender, timnit gebru, angelina mcmillan-major, and shmargaret shmitchell.
2021. on thedangers of stochastic parrots: can language models.
in proceedings of the 2021 confer-be too big?
ence on fairness, accountability, and transparency,page 610–623, new york, ny, usa.
association forcomputing machinery..ruha benjamin.
2019. race after technology: aboli-.
tionist tools for the new jim code.
wiley..improving.
shane bergsma, mark dredze, benjamin van durme,and david yarowsky.
2013.theresa wilson,broadlyviausercommunication-based name and location clus-in proceedings of the 2013tering on twitter.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, pages 1010–1019, atlanta,georgia.
association for computational linguistics..classiﬁcation.
marianne bertrand and sendhil mullainathan.
2004.are emily and greg more employable than lak-isha and jamal?
a ﬁeld experiment on labor mar-ket discrimination.
american economic review,94(4):991–1013..su lin blodgett, solon barocas, hal daum´e iii, andhanna wallach.
2020. language (technology) ispower: a critical survey of “bias” in nlp.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5454–5476, online.
association for computational lin-guistics..su lin blodgett, lisa green, and brendan o’connor.
2016. demographic dialectal variation in socialmedia: a case study of african-american english.
in proceedings of the 2016 conference on empiri-cal methods in natural language processing, pages1119–1130, austin, texas.
association for compu-tational linguistics..su lin blodgett, gilsinia lopez, alexandra olteanu,robert sim, and hanna wallach.
2021. stereotyp-ing norwegian salmon: an inventory of pitfalls infairness benchmark datasets.
in proceedings of thejoint conference of the 59th annual meeting of theassociation for computational linguistics and the11th international joint conference on natural lan-guage processing, online.
association for compu-tational linguistics..su lin blodgett, johnny wei, and brendan o’connor.
2018. twitter universal dependency parsing forafrican-american and mainstream american en-glish.
in proceedings of the 56th annual meeting of.
the association for computational linguistics (vol-ume 1: long papers), pages 1415–1425, melbourne,australia.
association for computational linguis-tics..tolga bolukbasi, kai-wei chang,.
james zou,and adam kalai.
2016.venkatesh saligrama,man is to computer programmer as woman is tohomemaker?
debiasing word embeddings.
inproceedings ofthe 30th international confer-ence on neural information processing systems,page 4356–4364, red hook, ny, usa.
curranassociates inc..rishi bommasani, kelly davis, and claire cardie.
2020. interpreting pretrained contextualized repre-sentations via reductions to static embeddings.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 4758–4781, online.
association for computational lin-guistics..geoffrey c. bowker and susan leigh star.
2000.sorting things out: classiﬁcation and its conse-quences.
inside technology.
mit press..anna brown, alexandra chouldechova, emily putnam-hornstein, andrew tobin, and rhema vaithianathan.
2019. toward algorithmic accountability in pub-lic services: a qualitative study of affected commu-nity perspectives on algorithmic decision-making inchild welfare services.
in proceedings of the 2019chi conference on human factors in computingsystems, chi ’19, page 1–12, new york, ny, usa.
association for computing machinery..simone browne.
2015. dark matters: on the surveil-.
lance of blackness.
duke university press..joy buolamwini and timnit gebru.
2018. gendershades: intersectional accuracy disparities in com-in proceedings ofmercial gender classiﬁcation.
the 1st conference on fairness, accountability andtransparency, pages 77–91, new york, ny, usa.
pmlr..aylin caliskan,.
and arvindjoanna j. bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356(6334):183–186..michael castelle.
2018. the linguistic ideologies ofin proceed-deep abusive language classiﬁcation.
ings of the 2nd workshop on abusive language on-line (alw2), pages 160–170, brussels, belgium.
as-sociation for computational linguistics..bharathi raja chakravarthi.
2020. hopeedi: a mul-tilingual hope speech detection dataset for equality,diversity, and inclusion.
in proceedings of the thirdworkshop on computational modeling of people’sopinions, personality, and emotion’s in social me-dia, pages 41–53, barcelona, spain (online).
asso-ciation for computational linguistics..1914isobelle clarke and jack grieve.
2017. dimensions ofabusive language on twitter.
in proceedings of thefirst workshop on abusive language online, pages1–10, vancouver, bc, canada.
association for com-putational linguistics..kimberl´e crenshaw.
1989. demarginalizing the inter-section of race and sex: a black feminist critique ofantidiscrimination doctrine, feminist theory and an-tiracist politics.
university of chicago legal forum,1989(8)..thomas davidson, debasmita bhattacharya, and ing-mar weber.
2019. racial bias in hate speech andabusive language detection datasets.
in proceedingsof the third workshop on abusive language online,pages 25–35, florence, italy.
association for com-putational linguistics..maria de-arteaga, alexey romanov, hanna wal-lach, jennifer chayes, christian borgs, alexandrachouldechova, sahin geyik, krishnaram kentha-padi, and adam tauman kalai.
2019. bias in bios:a case study of semantic representation bias in ain proceedings of the confer-high-stakes setting.
ence on fairness, accountability, and transparency,page 120–128, new york, ny, usa.
association forcomputing machinery..dorottya demszky, nikhil garg, rob voigt, jameszou, jesse shapiro, matthew gentzkow, and dan ju-rafsky.
2019. analyzing polarization in social me-dia: method and application to tweets on 21 massshootings.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 2970–3005, minneapolis, minnesota.
associ-ation for computational linguistics..lucas dixon, john li, jeffrey sorensen, nithum thain,and lucy vasserman.
2018. measuring and mitigat-in pro-ing unintended bias in text classiﬁcation.
ceedings of the 2018 aaai/acm conference on ai,ethics, and society, page 67–73, new york, ny,usa.
association for computing machinery..jacob eisenstein, noah a. smith, and eric p. xing.
2011. discovering sociolinguistic associations withstructured sparsity.
in proceedings of the 49th an-nual meeting of the association for computationallinguistics: human language technologies, pages1365–1374, portland, oregon, usa.
association forcomputational linguistics..yanai elazar and yoav goldberg.
2018. adversarialremoval of demographic attributes from text data.
in proceedings of the 2018 conference on empiri-cal methods in natural language processing, pages11–21, brussels, belgium.
association for computa-tional linguistics..anjalie field, chan young park, and yulia tsvetkov.
2021.controlled analyses of social biases inwikipedia bios.
computing research repository,arxiv:2101.00078. version 1..batya friedman, peter kahn, alan borning, and alinahuldtgren.
2013. value sensitive design and infor-mation systems.
in neelke doorn, daan schuur-biers, ibo van de poel, and michael gorman, editors,early engagement and new technologies: openingup the laboratory, volume 16. springer, dordrecht..roland g fryer jr and steven d levitt.
2004. thecauses and consequences of distinctively blackthe quarterly journal of economics,names.
119(3):767–805..ryan j. gallagher, kyle reing, david kale, and gregver steeg.
2017. anchored correlation explanation:topic modeling with minimal domain knowledge.
transactions of the association for computationallinguistics, 5:529–542..nikhil garg, londa schiebinger, dan jurafsky, andjames zou.
2018. word embeddings quantify100 years of gender and ethnic stereotypes.
pro-ceedings ofthe national academy of sciences,115(16):e3635–e3644..ona de gibert, naiara perez, aitor garc´ıa-pablos, andmontse cuadros.
2018. hate speech dataset fromin proceedings of thea white supremacy forum.
2nd workshop on abusive language online (alw2),pages 11–20, brussels, belgium.
association forcomputational linguistics..nabeel gillani and roger levy.
2019. simple dynamicword embeddings for mapping perceptions in thein proceedings of the third work-public sphere.
shop on natural language processing and compu-tational social science, pages 94–99, minneapolis,minnesota.
association for computational linguis-tics..anthony g greenwald, debbie e mcghee, and jor-dan lk schwartz.
1998. measuring individual dif-ferences in implicit cognition: the implicit associa-tion test.
journal of personality and social psychol-ogy, 74(6):1464..sophie groenwold, lily ou, aesha parekh, samhitaandhonnavalli, sharon levy, diba mirza,investigating african-william yang wang.
2020.american vernacular english in transformer-basedtext generation.
in proceedings of the 2020 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 5877–5883, online.
association for computational linguistics..itika gupta, barbara di eugenio, devika salunke, an-drew boyd, paula allen-meares, carolyn dickens,and olga garcia.
2020. heart failure educationof african american and hispanic/latino patients:data collection and analysis.
in proceedings of thefirst workshop on natural language processing formedical conversations, pages 41–46, online.
asso-ciation for computational linguistics..david l hamilton and tina k trolier.
1986. stereo-types and stereotyping: an overview of the cogni-.
1915tive approach.
in j. f. dovidiom and s. l. gaert-ner, editors, prejudice, discrimination, and racism,pages 127––163.
academic press..alex hanna, emily denton, andrew smart, and jamilasmith-loud.
2020. towards a critical race method-ology in algorithmic fairness.
in proceedings of the2020 conference on fairness, accountability, andtransparency, page 501–512, new york, ny, usa.
association for computing machinery..mohammed hasanuzzaman, ga¨el dias, and andyway.
2017. demographic word embeddings forin proceedings ofracism detection on twitter.
the eighth international joint conference on natu-ral language processing (volume 1: long papers),pages 926–936, taipei, taiwan.
asian federation ofnatural language processing..aur´elie herbelot, eva von redecker, and johannam¨uller.
2012. distributional techniques for philo-in proceedings of the 6th work-sophical enquiry.
shop on language technology for cultural heritage,social sciences, and humanities, pages 45–54, avi-gnon, france.
association for computational lin-guistics..xiaolei huang, linzi xing, franck dernoncourt, andmichael j. paul.
2020. multilingual twitter cor-pus and baselines for evaluating demographic biasin proceedings of thein hate speech recognition.
12th language resources and evaluation confer-ence, pages 1440–1448, marseille, france.
euro-pean language resources association..anne h. charity hudley.
2017. language and racial-ization.
in ofelia garc´ıa, nelson flores, and mas-similiano spotti, editors, the oxford handbook oflanguage and society, pages 381–402.
oxford uni-versity press..anne h charity hudley, christine mallinson, and marybucholtz.
2020. toward racial justice in linguis-tics: interdisciplinary insights into theorizing race inthe discipline and diversifying the profession.
lan-guage, 96(4):e200–e235..ben hutchinson, vinodkumar prabhakaran, emilydenton, kellie webster, yu zhong, and stephen de-nuyl.
2020. social biases in nlp models as barriersfor persons with disabilities.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 5491–5501, online.
as-sociation for computational linguistics..may jiang and christiane fellbaum.
2020. interdepen-dencies of gender and race in contextualized wordin proceedings of the second work-embeddings.
shop on gender bias in natural language process-ing, pages 17–25, barcelona, spain (online).
asso-ciation for computational linguistics..linguistics, pages 4392–4415, online.
associationfor computational linguistics..pratik joshi, sebastin santy, amar budhiraja, kalikabali, and monojit choudhury.
2020. the state andfate of linguistic diversity and inclusion in the nlpin proceedings of the 58th annual meet-world.
ing of the association for computational linguistics,pages 6282–6293, online.
association for computa-tional linguistics..david jurgens, libby hemphill, and eshwar chan-drasekharan.
2019. a just and comprehensive strat-egy for using nlp to address online abuse.
in pro-ceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 3658–3666, florence, italy.
association for computa-tional linguistics..saket karve, lyle ungar, and jo˜ao sedoc.
2019. con-ceptor debiasing of word representations evaluatedin proceedings of the first workshopon weat.
on gender bias in natural language processing,pages 40–48, florence, italy.
association for com-putational linguistics..anna kasunic and geoff kaufman.
2018. learning tolisten: critically considering the role of ai in humanstorytelling and character creation.
in proceedingsof the first workshop on storytelling, pages 1–13,new orleans, louisiana.
association for computa-tional linguistics..brendan kennedy, xisen jin, aida mostafazadeh da-vani, morteza dehghani, and xiang ren.
2020. con-textualizing hate speech classiﬁers with post-hoc ex-planation.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5435–5442, online.
association for computa-tional linguistics..sharese king.
2020. from african american vernac-ular english to african american language: re-thinking the study of race and language in africanamericans’ speech.
annual review of linguistics,6(1):285–300..svetlana kiritchenko and saif mohammad.
2018. ex-amining gender and race bias in two hundred sen-in proceedings of thetiment analysis systems.
seventh joint conference on lexical and compu-tational semantics, pages 43–53, new orleans,louisiana.
association for computational linguis-tics..bogdan kulynych, david madras, smitha milli, in-ioluwa deborah raji, angela zhou, and richardzemel.
2020. participatory approaches to machinelearning.
international conference on machinelearning workshop..kenneth joseph and jonathan morgan.
2020. when doword embeddings accurately reﬂect surveys on ourbeliefs about people?
in proceedings of the 58th an-nual meeting of the association for computational.
sachin kumar, antonios anastasopoulos, shuly wint-ner, and yulia tsvetkov.
2021. machine translationin proceed-into low-resource language varieties.
ings of the 59th annual meeting of the association.
1916for computational linguistics.
association for com-putational linguistics..keita kurita, nidhi vyas, ayush pareek, alan w black,and yulia tsvetkov.
2019. measuring bias in contex-tualized word representations.
in proceedings of thefirst workshop on gender bias in natural languageprocessing, pages 166–172, florence, italy.
associ-ation for computational linguistics..jana kurrek, haji mohammad saleem, and derekruths.
2020. towards a comprehensive taxonomyand large-scale annotated corpus for online slur us-age.
in proceedings of the fourth workshop on on-line abuse and harms, pages 138–149, online.
as-sociation for computational linguistics..anne lauscher and goran glavaˇs.
2019. are we con-sistently biased?
multidimensional analysis of bi-ases in distributional word vectors.
in proceedingsof the eighth joint conference on lexical and com-putational semantics (*sem 2019), pages 85–91,minneapolis, minnesota.
association for computa-tional linguistics..nayeon lee, andrea madotto, and pascale fung.
2019.exploring social bias in chatbots using stereotypein proceedings of the 2019 workshopknowledge.
on widening nlp, pages 177–180, florence, italy.
association for computational linguistics..kobi leins, jey han lau, and timothy baldwin.
2020.give me convenience and give her death: whoshould decide what uses of nlp are appropriate, andon what basis?
in proceedings of the 58th annualmeeting of the association for computational lin-guistics, pages 2908–2913, online.
association forcomputational linguistics..michael lepori.
2020. unequal representations: ana-lyzing intersectional biases in word embeddings us-ing representational similarity analysis.
in proceed-ings of the 28th international conference on com-putational linguistics, pages 1720–1728, barcelona,spain (online).
international committee on compu-tational linguistics..haochen liu, jamell dacon, wenqi fan, hui liu, zitaoliu, and jiliang tang.
2020. does gender matter?
in proceed-towards fairness in dialogue systems.
ings of the 28th international conference on com-putational linguistics, pages 4403–4416, barcelona,spain (online).
international committee on compu-tational linguistics..siyi liu, lei guo, kate mays, margrit betke, andderry tanti wijaya.
2019. detecting frames in newsheadlines and its application to analyzing news fram-in pro-ing trends surrounding u.s. gun violence.
ceedings of the 23rd conference on computationalnatural language learning (conll), pages 504–514, hong kong, china.
association for computa-tional linguistics..kate loveys, jonathan torrez, alex fine, glen mori-arty, and glen coppersmith.
2018. cross-culturaldifferences in language markers of depression on-in proceedings of the fifth workshop online.
computational linguistics and clinical psychology:from keyboard to clinic, pages 78–87, new or-leans, la.
association for computational linguis-tics..thomas manzini, lim yao chong, alan w black,and yulia tsvetkov.
2019. black is to criminalas caucasian is to police: detecting and removingin proceed-multiclass bias in word embeddings.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 615–621, minneapo-lis, minnesota.
association for computational lin-guistics..chandler may, alex wang, shikha bordia, samuel r.bowman, and rachel rudinger.
2019. on measur-ing social biases in sentence encoders.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 622–628, minneapo-lis, minnesota.
association for computational lin-guistics..elijah mayﬁeld, michael madaio, shrimai prab-humoye, david gerritsen, brittany mclaughlin,ezekiel dixon-rom´an, and alan w black.
2019.equity beyond bias in language technologies for ed-ucation.
in proceedings of the fourteenth workshopon innovative use of nlp for building educationalapplications, pages 444–460, florence, italy.
asso-ciation for computational linguistics..charlton d. mcilwain.
2019. black software: the in-ternet and racial justice, from the afronet to blacklives matter.
oxford university press, incorpo-rated..j. a. meaney.
2020. crossing the line: where do de-mographic variables ﬁt into humor detection?
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics: student re-search workshop, pages 176–181, online.
associa-tion for computational linguistics..julia mendelsohn, yulia tsvetkov, and dan jurafsky.
2020. a framework for the computational linguisticanalysis of dehumanization.
frontiers in artiﬁcialintelligence, 3:55..michele merler, nalini ratha, rogerio s feris, andjohn r smith.
2019. diversity in faces.
computingresearch repository, arxiv:1901.10436. version 6..jack merullo, luke yeh, abram handler, alvin gris-som ii, brendan o’connor, and mohit iyyer.
2019.investigating sports commentator bias within a largecorpus of american football broadcasts.
in proceed-ings of the 2019 conference on empirical methods.
1917in natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 6355–6361, hongkong, china.
association for computational lin-guistics..emiel van miltenburg, desmond elliott, and piekvossen.
2017. cross-linguistic differences and simi-larities in image descriptions.
in proceedings of the10th international conference on natural languagegeneration, pages 21–30, santiago de compostela,spain.
association for computational linguistics..ehsan mohammady and aron culotta.
2014. usingcounty demographics to infer attributes of twitterusers.
in proceedings of the joint workshop on so-cial dynamics and personal attributes in social me-dia, pages 7–16, baltimore, maryland.
associationfor computational linguistics..aida mostafazadeh davani, leigh yeh, mohammadatari, brendan kennedy, gwenyth portillo wight-man, elaine gonzalez, natalie delong, rhea bha-tia, arineh mirinjian, xiang ren, and morteza de-hghani.
2019. reporting the unreported: event ex-traction for analyzing the local representation of hatecrimes.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages5753–5757, hong kong, china.
association forcomputational linguistics..suhanthie motha.
2020. is an antiracist and decoloniz-ing applied linguistics possible?
annual review ofapplied linguistics, 40:128–133..moin nadeem, anna bethke, and siva reddy.
2020.stereoset: measuring stereotypical bias in pre-computing researchtrained language models.
repository, arxiv:2004.09456. version 1..nikita nangia, clara vania, rasika bhalerao, andsamuel r. bowman.
2020. crows-pairs: a chal-lenge dataset for measuring social biases in maskedlanguage models.
in proceedings of the 2020 con-ference on empirical methods in natural languageprocessing (emnlp), pages 1953–1967, online.
as-sociation for computational linguistics..national center for science and engineering statistics.
2019. doctorate recipients from u.s. universities.
national science foundation..saﬁya u. noble.
2018. algorithms of oppression:how search engines reinforce racism.
nyu press..ihudiya finda ogbonnaya-ogburu, angela d.r.
smith,alexandra to, and kentaro toyama.
2020. criticalin proceedings of the 2020race theory for hci.
chi conference on human factors in computingsystems, chi ’20, page 1–16, new york, ny, usa.
association for computing machinery..alexandra olteanu, carlos castillo, fernando diaz,social data: bi-and emre kiciman.
2019.ases, methodological pitfalls, and ethical boundaries.
frontiers in big data, 2:13..julia parish-morris.
2019. computational linguisticsfor enhancing scientiﬁc reproducibility and reducingin proceedings of the sixthhealthcare inequities.
workshop on computational linguistics and clini-cal psychology, pages 94–102, minneapolis, min-nesota.
association for computational linguistics..amandalynne paullada.
2020. how does machinetranslation shift power?
in proceedings of the firstworkshop on resistance ai..ellie pavlick, heng ji, xiaoman pan, and chriscallison-burch.
2016. the gun violence database:a new task and data set for nlp.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 1018–1024, austin,texas.
association for computational linguistics..daniel preot¸iuc-pietro and lyle ungar.
2018. user-level race and ethnicity predictors from twitter text.
in proceedings of the 27th international conferenceon computational linguistics, pages 1534–1545,santa fe, new mexico, usa.
association for com-putational linguistics..inioluwa deborah raji and joy buolamwini.
2019. ac-tionable auditing: investigating the impact of pub-licly naming biased performance results of com-in proceedings of the 2019mercial ai products.
aaai/acm conference on ai, ethics, and society,pages 429–435..anil ramakrishna, victor r. mart´ınez, nikolaos ma-landrakis, karan singla, and shrikanth narayanan.
2017. linguistic analysis of differences in portrayalof movie characters.
in proceedings of the 55th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1669–1678, vancouver, canada.
association for computa-tional linguistics..yolanda a. rankin and jakita o. thomas.
2019.straighten up and ﬂy right: rethinking intersection-ality in hci research.
interactions, 26(6):64–68..alexey romanov, maria de-arteaga, hanna wal-lach, jennifer chayes, christian borgs, alexan-dra chouldechova, sahin geyik, krishnaram ken-thapadi, anna rumshisky, and adam kalai.
2019.what’s in a name?
reducing bias in bios withoutaccess to protected attributes.
in proceedings of the2019 conference of the north american chapter ofthe association for computational linguistics: hu-man language technologies, volume 1 (long andshort papers), pages 4187–4195, minneapolis, min-nesota.
association for computational linguistics..jonathan rosa and nelson flores.
2017. unsettlingrace and language: toward a raciolinguistic perspec-tive.
language in society, 46(5):621–647..1918wendy d roth.
2016. the multiple dimensions of race..ethnic and racial studies, 39(8):1310–1338..shamik roy and dan goldwasser.
2020. weakly su-pervised learning of nuanced frames for analyzingin proceedings of thepolarization in news media.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 7698–7716,online.
association for computational linguistics..rachel rudinger, chandler may,.
and benjaminvan durme.
2017. social bias in elicited natural lan-guage inferences.
in proceedings of the first aclworkshop on ethics in natural language process-ing, pages 74–79, valencia, spain.
association forcomputational linguistics..wesley santos and ivandr´e paraboni.
2019. moralstance recognition and polarity classiﬁcation fromtwitter and elicited text.
in proceedings of the inter-national conference on recent advances in naturallanguage processing (ranlp 2019), pages 1069–1075, varna, bulgaria.
incoma ltd..maarten sap, dallas card, saadia gabriel, yejin choi,and noah a. smith.
2019. the risk of racial biasin proceedings of thein hate speech detection.
57th annual meeting of the association for com-putational linguistics, pages 1668–1678, florence,italy.
association for computational linguistics..maarten sap, saadia gabriel, lianhui qin, dan ju-rafsky, noah a. smith, and yejin choi.
2020. so-cial bias frames: reasoning about social and powerin proceedings of theimplications of language.
58th annual meeting of the association for compu-tational linguistics, pages 5477–5490, online.
as-sociation for computational linguistics..p.k.
saucier, t.p.
woods, p. douglass, b. hesse, t.k.
nopper, g. thomas, and c. wun.
2016. concep-tual aphasia in black: displacing racial formation.
critical africana studies.
lexington books..ari schlesinger, w. keith edwards, and rebecca e.grinter.
2017.intersectional hci: engaging iden-tity through gender, race, and class.
in proceedingsof the 2017 chi conference on human factors incomputing systems, chi ’17, page 5412–5427, newyork, ny, usa.
association for computing machin-ery..tyler schnoebelen.
2017. goal-oriented design for eth-ical machine learning and nlp.
in proceedings ofthe first acl workshop on ethics in natural lan-guage processing, pages 88–93, valencia, spain.
as-sociation for computational linguistics..deven santosh shah, h. andrew schwartz, and dirkhovy.
2020. predictive biases in natural languageprocessing models: a conceptual framework andoverview.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 5248–5264, online.
association for computa-tional linguistics..usman shahid, barbara di eugenio, andrew rojecki,and elena zheleva.
2020. detecting and understand-ing moral biases in news.
in proceedings of the firstjoint workshop on narrative understanding, story-lines, and events, pages 120–125, online.
associa-tion for computational linguistics..sima shariﬁrad and stan matwin.
2019..usingattention-based bidirectional lstm to identify dif-ferent categories of offensive language directed to-ward female celebrities.
in proceedings of the 2019workshop on widening nlp, pages 46–48, florence,italy.
association for computational linguistics..emily sheng, kai-wei chang, premkumar natarajan,and nanyun peng.
2019. the woman worked asa babysitter: on biases in language generation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 3407–3412, hong kong, china.
association for computa-tional linguistics..m sloane, e moss, o awomolo, and l forlano.
2020.participation is not a design ﬁx for machine learning.
computing research repository, arxiv:2007.02423.version 3..harold somers.
2006. language engineering and thepathway to healthcare: a user-oriented view.
inproceedings of the first international workshopon medical speech translation, pages 28–35, newyork, new york.
association for computational lin-guistics..pia sommerauer and antske fokkens.
2019. concep-tual change and distributional semantic models: aninexploratory study on pitfalls and possibilities.
proceedings of the 1st international workshop oncomputational approaches to historical languagechange, pages 223–233, florence, italy.
associa-tion for computational linguistics..emma strubell, ananya ganesh, and andrew mccal-lum.
2019. energy and policy considerations forin proceedings of the 57thdeep learning in nlp.
annual meeting of the association for computa-tional linguistics, pages 3645–3650, florence, italy.
association for computational linguistics..tony sun, andrew gaut, shirlyn tang, yuxin huang,mai elsherief, jieyu zhao, diba mirza, elizabethbelding, kai-wei chang, and william yang wang.
2019. mitigating gender bias in natural languagein proceedings ofprocessing: literature review.
the 57th annual meeting of the association for com-putational linguistics, pages 1630–1640, florence,italy.
association for computational linguistics..latanya sweeney.
2013. discrimination in online addelivery: google ads, black names and white names,racial discrimination, and click advertising.
queue,11(3):10–29..1919samson tan, shaﬁq joty, min-yen kan, and richardit’s morphin’ time!
combatingsocher.
2020.linguistic discrimination with inﬂectional perturba-in proceedings of the 58th annual meet-tions.
ing of the association for computational linguistics,pages 2920–2935, online.
association for computa-tional linguistics..yi chern tan and l elisa celis.
2019. assessing socialand intersectional biases in contextualized word rep-resentations.
in proceedings of the 2019 conferenceon advances in neural information processing sys-tems, volume 32, pages 13230–13241.
curran asso-ciates, inc..rachael tatman.
2020. what i won’t build.
workshop.
on widening nlp..rocco tripodi, massimo warglien, simon levis sul-lam, and deborah paci.
2019. tracing antisemiticlanguage through diachronic embedding projections:france 1789-1914. in proceedings of the 1st inter-national workshop on computational approaches tohistorical language change, pages 115–125, flo-rence, italy.
association for computational linguis-tics..ekaterina vylomova, sean murphy, and nicholashaslam.
2019. evaluation of semantic change ofin proceed-harm-related concepts in psychology.
ings of the 1st international workshop on computa-tional approaches to historical language change,pages 29–34, florence, italy.
association for com-putational linguistics..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial trig-gers for attacking and analyzing nlp.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2153–2162, hongkong, china.
association for computational lin-guistics..william warner and julia hirschberg.
2012. detectinghate speech on the world wide web.
in proceedingsof the second workshop on language in social me-dia, pages 19–26, montr´eal, canada.
association forcomputational linguistics..zeerak waseem.
2016. are you a racist or am i seeingthings?
annotator inﬂuence on hate speech detectionon twitter.
in proceedings of the first workshop onnlp and computational social science, pages 138–142, austin, texas.
association for computationallinguistics..zeerak waseem, thomas davidson, dana warmsley,and ingmar weber.
2017. understanding abuse: atypology of abusive language detection subtasks.
inproceedings of the first workshop on abusive lan-guage online, pages 78–84, vancouver, bc, canada.
association for computational linguistics..zeerak waseem and dirk hovy.
2016. hateful sym-bols or hateful people?
predictive features for hatespeech detection on twitter.
in proceedings of thenaacl student research workshop, pages 88–93,san diego, california.
association for computa-tional linguistics..zeerak waseem, smarika lulz, and isabelle bingel,joachim augenstein.
2021. disembodied machinelearning: on the illusion of objectivity in nlp.
computing research repository, arxiv:2101.11974.version 1..kellie webster, marta r. costa-juss`a, christian hard-meier, and will radford.
2019. gendered ambigu-ous pronoun (gap) shared task at the gender biasin nlp workshop 2019. in proceedings of the firstworkshop on gender bias in natural language pro-cessing, pages 1–7, florence, italy.
association forcomputational linguistics..michael wojatzki, saif mohammad, torsten zesch,and svetlana kiritchenko.
2018. quantifying qual-itative data for understanding controversial issues.
in proceedings of the eleventh international confer-ence on language resources and evaluation (lrec2018), miyazaki, japan.
european language re-sources association (elra)..zach wood-doughty, nicholas andrews, rebeccamarvin, and mark dredze.
2018. predicting twit-in pro-ter user demographics from names alone.
ceedings of the second workshop on computationalmodeling of people’s opinions, personality, andemotions in social media, pages 105–111, new or-leans, louisiana, usa.
association for computa-tional linguistics..zach wood-doughty, michael smith, david bronia-towski, and mark dredze.
2017. how does twitteruser behavior vary across demographic groups?
inproceedings of the second workshop on nlp andcomputational social science, pages 83–89, van-couver, canada.
association for computational lin-guistics..lucas wright, derek ruths, kelly p dillon, haji mo-hammad saleem, and susan benesch.
2017. vec-in proceedingstors for counterspeech on twitter.
of the first workshop on abusive language online,pages 57–62, vancouver, bc, canada.
associationfor computational linguistics..mengzhou xia, anjalie field, and yulia tsvetkov.
2020. demoting racial bias in hate speech detection.
in proceedings of the eighth international work-shop on natural language processing for social me-dia, pages 7–14, online.
association for computa-tional linguistics..qiongkai xu, lizhen qu, chenchen xu, and ran cui.
in proceed-2019. privacy-aware text rewriting.
ings of the 12th international conference on nat-ural language generation, pages 247–257, tokyo,japan.
association for computational linguistics..1920guanhua zhang, bing bai, junqi zhang, kun bai, con-ghui zhu, and tiejun zhao.
2020. demographicsshould not be the reason of toxicity: mitigatingdiscrimination in text classiﬁcations with instanceweighting.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 4134–4145, online.
association for computa-tional linguistics..jieyu zhao and kai-wei chang.
2020. logan: lo-cal group bias detection by clustering.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1968–1977, online.
association for computationallinguistics..jieyu zhao, tianlu wang, mark yatskar, vicente or-donez, and kai-wei chang.
2017. men also likeshopping: reducing gender bias ampliﬁcation usingcorpus-level constraints.
in proceedings of the 2017conference on empirical methods in natural lan-guage processing, pages 2979–2989, copenhagen,denmark.
association for computational linguis-tics..1921sdrowyekticilpxe.52.
3.
10.decruos-dworc.112.cilbup/lanretxe.21.
1.
4.dengila-susnec.711.
9.detroper-fles.51.
6.detciderp.113.
5.seman.8.
2111.total132048247.
4+bwbwah{bwah}w/non-wtotal.
table 2: racial categories used by acl anthologypapers.
bwah stand for black, white, asian, andhispanic.
{bwah} denotes any incomplete subset ofbwah other than bw (e.g.
black and hispanic).
4+denotes that the paper used ≥ 4 racial categories, oftenincluding “other”, “mixed”, or an open-ended text box.
papers with multiple schema are counted as separatedata points..venues.
the majority of papers were published inworkshops, which is consist with the large num-ber of workshop papers.
in 2019, approximately2,038 papers were published in workshops14 and1,680 papers were published in conferences (acl,emnlp, naacl, conll, cicling), meaning54.8% were published in workshops.
in our dataset, 46.8% of papers surveyed were published inworkshops.
the most number of papers were pub-lished in the largest conferences: acl and emnlp.
thus, while table 1 suggests that discussions ofrace have been siloed to particular nlp applica-tions, figure 2 does not show evidence that theyhave been siloed to particular venues..in table 2, for all papers that use categorizationschema to classify race, we show what racial cate-gories they use.
if a paper uses multiple schemes(e.g.
collects crowd-sourced annotations of stereo-types associated with different races and also asksannotators to self-report their race), we report eachscheme as a separate data point.
this table does notinclude papers that do not specify racial categories(e.g.
examine “racist language” without specifyingtargeted people or analyze semantic change of top-ics like “racism” and “prejudice”).
finally, we mapterms used by papers to the ones in table 2, e.g.
pa-pers examining african american vs. europeanamerican names are included in bw..the majority of papers focus on binary.
14https://www.aclweb.org/anthology/.
venues/ws/.
figure 1: year of publication of 79 papers that mention“racial” or “racism”.
more papers have been publishedin recent years (2019-2020)..figure 2: venue of publication of 79 papers that men-tion “racial” or “racism”.
about half (46.8%) were pub-lished in workshops..a acl anthology venues.
acl events: aacl, acl, anlp, cl, conll,eacl, emnlp, findings, naacl, semeval,*sem, tacl, wmt, workshops, special interestgroups.
non-acl events: alta, amta, ccl, col-ing, eamt, hlt, ijcnlp, jep/taln/recital,lilt, lrec, muc, paclic, ranlp, ro-cling/ijclclp, tinlap, tipster.
b additional survey metrics.
we show three additional breakdowns of the dataset: figure 1 shows the number of papers publishedeach year, figure 2 shows the number of paperspublished in each venue, and table 2 shows howpapers have operationalized race.
as expected,given the growth of nlp research in general andthe increasing focus on social issues (e.g.
“ethicsand nlp” track was added to acl in 2020) morework has been published on race in more recentyears (2019, 2020).
in figure 2, we consider ifwork on race has been siloed into or out of speciﬁc.
192205101520252006201120122013201420162017201820192020010203040workshopaclemnlpnaaclcolinglrec*seminlgconllijcnlpranlptaclblack/white racial categories.
while many papersdraw deﬁnitions from the u.s. census, very few pa-pers consider less-commonly-selected census cat-egories like native american or paciﬁc islander.
the most common method for identifying people’srace uses ﬁrst or last names (10 papers) or explicitkeywords like “black” and “white” (10 papers)..1923c full list of surveyed papers.
assimakopoulos et al.
(2020).
bommasani et al.
(2020).
chakravarthi (2020).
groenwold et al.
(2020).
gupta et al.
(2020).
huang et al.
(2020).
jiang and fellbaum (2020).
joseph and morgan (2020).
kennedy et al.
(2020).
kurrek et al.
(2020).
lepori (2020).
liu et al.
(2020).
meaney (2020).
nangia et al.
(2020).
roy and goldwasser (2020).
sap et al.
(2020).
shah et al.
(2020).
shahid et al.
(2020).
tan et al.
(2020).
xia et al.
(2020).
zhang et al.
(2020).
zhao and chang (2020).
amir et al.
(2019).
davidson et al.
(2019).
demszky et al.
(2019).
gillani and levy (2019).
jurgens et al.
(2019).
karve et al.
(2019).
kurita et al.
(2019).
lauscher and glavaˇs (2019).
lee et al.
(2019).
liu et al.
(2019).
manzini et al.
(2019).
may et al.
(2019).
mayﬁeld et al.
(2019).
merullo et al.
(2019).
mostafazadeh davani et al.
(2019).
parish-morris (2019).
romanov et al.
(2019).
santos and paraboni (2019).
sap et al.
(2019).
shariﬁrad and matwin (2019).
sommerauer and fokkens (2019).
tripodi et al.
(2019).
vylomova et al.
(2019).
wallace et al.
(2019).
xu et al.
(2019).
barbieri and camacho-collados (2018).
acl.
lrec.
aclacl.
venuelrecacl.
year202020202020 workshop2020emnlp2020 workshop20202020 workshop202020202020 workshop2020 coling2020 coling2020 workshopemnlp2020emnlp2020acl2020acl20202020 workshop20202020 workshopacl20202020emnlp2019 workshop2019 workshop2019naacl2019 workshop20192019 workshop2019 workshop2019 workshop2019 workshopconll2019naacl20192019acl2019 workshopemnlp20192019emnlp2019 workshopnaacl2019ranlp20192019acl2019 workshop2019 workshop2019 workshop2019 workshopemnlp2019inlg2019*sem2018.acl.
task typenlp taskcollect corpusabusive languagedetect biastext representationscollect corpusabusive languagedetect biastext generationcollect corpussector-spec.
nlp apps.
detect biasabusive languagedetect biastext representationsdetect biastext representationsdebiasabusive languagecollect corpusabusive languagedetect biastext representationsdebiastext generationsurvey/positionsocial science/mediadetect biastext representationsanalyze corpussocial science/mediacollect corpusabusive languagesurvey/positionethics/task-indep.
biassocial science/mediaanalyze corpusethics/task-indep.
bias develop model.
abusive languageabusive languageethics/task-indep.
biassector-spec.
nlp apps.
analyze corpus.
debiasdetect biasdetect bias.
detect biasabusive languageanalyze corpussocial science/mediaanalyze corpustext representationssurvey/positionabusive languagedebiastext representationsdetect biastext representationsdetect biastext representationsdetect biastext generationdevelop modelsocial science/mediadebiastext representationsdetect biastext representationssurvey/positionsector-spec.
nlp apps.
social science/mediaanalyze corpuscore nlp applications develop modelsurvey/positionsector-spec.
nlp apps.
debiassector-spec.
nlp apps.
collect corpussocial science/mediaabusive languagedetect biasanalyze corpusabusive languagetext representationsdetect biasanalyze corpustext representationsanalyze corpussocial science/mediadetect biastext generationdevelop modeltext generationanalyze corpussocial science/media.
1924blodgett et al.
(2018).
castelle (2018).
de gibert et al.
(2018).
elazar and goldberg (2018).
kasunic and kaufman (2018).
kiritchenko and mohammad (2018).
loveys et al.
(2018).
preot¸iuc-pietro and ungar (2018).
sheng et al.
(2019).
wojatzki et al.
(2018).
wood-doughty et al.
(2018).
clarke and grieve (2017).
gallagher et al.
(2017).
hasanuzzaman et al.
(2017).
ramakrishna et al.
(2017).
rudinger et al.
(2017).
schnoebelen (2017).
van miltenburg et al.
(2017).
waseem et al.
(2017).
wood-doughty et al.
(2017).
wright et al.
(2017).
blodgett et al.
(2016).
pavlick et al.
(2016).
waseem (2016).
waseem and hovy (2016).
mohammady and culotta (2014).
bergsma et al.
(2013).
herbelot et al.
(2012).
warner and hirschberg (2012).
eisenstein et al.
(2011).
somers (2006).
acl.
*sem.
taclijcnlpacl.
debiasanalyze corpuscollect corpusdebiassurvey/positiondetect bias.
core nlp applications2018abusive language2018 workshopabusive language2018 workshopethics/task-indep.
bias2018emnlptext generation2018 workshopsocial science/media2018sector-spec.
nlp apps.
analyze corpus2018 workshopdevelop modelsocial science/media2018 colingdetect biastext generationemnlp2018collect corpussocial science/media2018lrecdevelop modelsocial science/media2018 workshopanalyze corpusabusive language2017 workshopdevelop modelsocial science/media2017develop modelabusive language2017analyze corpus2017social science/mediadetect bias2017 workshop core nlp applicationssurvey/position2017 workshop ethics/task-indep.
bias2017detect biassurvey/position2017 workshopanalyze corpus2017 workshop2017 workshopanalyze corpusethics/task-indep.
bias collect corpusemnlp2016collect corpuscore nlp applications2016emnlpdetect biasabusive language2016 workshopcollect corpusabusive language2016 workshopdevelop modelsocial science/media2014 workshopdevelop modelsocial science/medianaacl2013analyze corpussocial science/media2012 workshopdevelop modelabusive language2012 workshopanalyze corpussocial science/media2011survey/positionsector-spec.
nlp apps.
2006 workshop.
image processingabusive languagesocial science/mediaabusive language.
inlg.
acl.
1925