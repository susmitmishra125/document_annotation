on sample based explanation methods for nlp:efﬁciency, faithfulness, and semantic evaluationziming huang ∗sogou incbeijing, chinahzmyouxiang@gmail.com.
wei zhang ∗wayfairboston ma, usawzhang5@wayfair.com.
yada zhumit-ibm watson ai labibm research, ny, usayzhu@us.ibm.com.
guangnan yeibm researchnew york, usagye@us.ibm.comabstract.
xiaodong cuiibm researchnew york, usaxcui@us.ibm.com.
fan zhangibm data and ailittleton ma, usafzhang@us.ibm.com.
in the recent advances of natural language pro-cessing, the scale of the state-of-the-art modelsand datasets is usually extensive, which chal-lenges the application of sample-based expla-nation methods in many aspects, such as ex-planation interpretability, efﬁciency, and faith-fulness.
in this work, for the ﬁrst time, wecan improve the interpretability of explana-tions by allowing arbitrary text sequences asthe explanation unit.
on top of this, we im-plement a hessian-free method with a modelfaithfulness guarantee.
finally, to compareour method with the others, we propose asemantic-based evaluation metric that can bet-ter align with humans’ judgment of explana-tions than the widely adopted diagnostic or re-training measures.
the empirical results onmultiple real data sets demonstrate the pro-posed method’s superior performance to pop-ular explanation techniques such as inﬂuencefunction or tracin on semantic evaluation..1.introduction.
as complex nlp models such as the transformersfamily (vaswani et al., 2017; devlin et al., 2019)become an indispensable tool in many applications,there are growing interests to explain the workingmechanism of these “black-box” models.
amongthe vast of existing techniques for explaining ma-chine learning models, inﬂuence functions (ham-pel, 1974; koh and liang, 2017) that uses traininginstances as explanations to a model’s behaviorhave gained popularity in nlp very recently.
dif-ferent from other methods such as using input era-sure (li et al., 2016), saliency maps or attention ma-trices (serrano and smith, 2019; jain and wallace,2019; wiegreffe and pinter, 2019) that only look at.
∗equal contribution.
wei zhang did the work while beinga research scientist at ibm t.j. watson research center atyorktown heights, ny, usa; ziming huang was a researchscientist at ibm research lab at beijing, china..how a speciﬁc input or input sequence impacts themodel decision, explaining with training instancescan cast light on the knowledge a model has en-coded about a problem, by answering questionslike ’what knowledge did the model capture fromwhich training instances so that it makes decisionin such a manner during test?’.
very recently, themethod has been applied to explain bert-based(devlin et al., 2019) text classiﬁcation (han et al.,2020; meng et al., 2020b) and natural languageinference (han et al., 2020) models, as well as toaid text generation for data augmentation (yanget al., 2020a) using gpt-2 (radford et al., 2019).
although useful, inﬂuence function may not beentirely bullet-proof for nlp applications..first, following the original formulation (kohand liang, 2017), the majority of existing worksuse entire training instances as explanations.
how-ever, for long natural language texts that are com-mon in many high-impact application domains(e.g., healthcare, ﬁnance, or security), it may bedifﬁcult, if not impossible, to comprehend an entireinstance as an explanation.
for example, a model’sdecision may depend only on a speciﬁc part of along training instance..second, for modern nlp models and large-scaledatasets, the application of inﬂuence functions canlead to prohibitive computing costs due to inversehessian matrix approximation.
although hessian-free inﬂuence score such as tracin (pruthi et al.,2020b) was introduced very recently, it may notbe faithful to the model in question and can resultin spurious explanations for the involvement ofsub-optimal checkpoints..last, the evaluation of explanation methods, inparticular, for the training-instance-based ones, re-mains an open question.
previous evaluation iseither under an over-simpliﬁed assumption on theagreement of labels between training and test in-stances (hanawa et al., 2020; han et al., 2020) or.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5399–5411august1–6,2021.©2021associationforcomputationallinguistics5399is based on indirect or manual inspection (hookeret al., 2019; meng et al., 2020b; han et al., 2020;pruthi et al., 2020a).
a method to automaticallymeasure the semantic relations at scale and thathighly correlates to human judgment is still miss-ing in the evaluation toolset..to address the above problems, we propose aframework to explain model behavior that includesboth a set of new methods and a new metric thatcan measure the semantic relations between thetest instance and its explanations.
the new methodallows for arbitrary text spans as the explanationunit and is hessian-free while being faithful to theﬁnal model.
our contributions are:.
1. we propose a new explanation framework thatcan use arbitrary explanation units as explana-tions and be hessian-free and faithful at thesame time;.
2. a new metric to measure the semantic related-ness between a test instance and its explana-tion for bert-based deep models..2 preliminaries.
suppose a model parameterized by ˆθ is trainedon classiﬁcation dataset d = {dtrain, dtest} byempirical risk minimization over dtrain.
let z =(x, y) ∈ dtrain and z(cid:48) = (x(cid:48), y(cid:48)) ∈ dtest denote atraining and a test instance respectively, where xis a token sequence, and y is a scalar.
the goal oftraining instance based explanation is to provide fora given test z(cid:48) an ordered list of training instancesas explanation.
two notable methods to calculatethe inﬂuence score are if and tracin:.
if (koh and liang, 2017) assumes the inﬂu-ence of z can be measured by perturbing the lossfunction l with a fraction of the loss on z, andobtain.
ipert,loss(z, z(cid:48); ˆθ).
= −∇θl(z(cid:48), ˆθ)h −1ˆθ.
∇θl(z, ˆθ),.
(1).
where h is the hessian matrix calculated on theentire training dataset, a potential computation bot-tleneck for large dataset d and complex modelwith high dimensional ˆθ..tracin (pruthi et al., 2020b) instead assumesthe inﬂuence of a training instance z is the sum ofits contribution to the overall loss all through the.
entire training history, and conveniently it leads to.
tracin(z, z(cid:48)) =.
ηi∇ˆθi.
l(ˆθi, z)∇ˆθi.
l(ˆθi, z(cid:48)),.
(2).
(cid:88).
i.where i iterates through the checkpoints saved atdifferent training steps and ηi is a weight for eachcheckpoint.
tracin does not involve hessian ma-trix and more efﬁcient to compute.
we can summa-rize the key differences between them according tothe following desiderata of an explanation method:.
efﬁciency for each z(cid:48), tracin requires o(cg)where c is the number of models and g is thetime spent for gradient calculation; whereas ifneeds o(n 2g) where n is the number of traininginstances, and n >> c in general.
1.faithfulness if is faithful to ˆθ since all its calcu-lation is based on a single ﬁnal model, yet tracinmay be less faithful to ˆθ since it obtains gradientsfrom a set of checkpoints 2..interpretability both methods use the entiretraining instance as an explanation.
explanationswith a ﬁner-grained unit, e.g., phrases, may be eas-ier to interpret in many applications where the textsare lengthy..3 proposed method.
to improve on the above desiderata, a new methodshould be able to: 1) use any appropriate granu-larity of span(s) as the explanation unit; 2) avoidthe need of hessian while maintaining faithfulness.
we discuss the solutions for both in section 3.1and 3.2, and combine them into one formation insection 3.3 followed by critical implementationdetails..3.1.improved interpretability with spans.
to achieve 1), we ﬁrst start with inﬂuence functions(koh and liang, 2017) and consider an arbitraryspan of training sequence x to be evaluated for thequaliﬁcation as explanation 3. our core idea is tosee how the model loss on test instance z(cid:48) changes.
1some approximation such as hessian-inverse-vector-product (baydin et al., 2016) may improve efﬁciency too(n sg) where s is the approximation step and s < n.2we may say tracin is faithful to the data rather than tothe model.
and in the case where checkpoint averaging canbe used as model prediction, the number of checkpoints maybe too few to justify eq.
2..3the method can be trivially generalized to multiple spans.
5400with the training span’s importance.
the moreimportant a training span is to z(cid:48), the greater thisinﬂuence score should be.
we derive it in threefollowing steps..to token j.to be xij,.
first, we deﬁne the training span fromand the=.
token isequence with xij masked is x−ij[x0, ..., xi−1, [mask], ..., [mask], xj+1, ...]and its corresponding training data is z−ij.
we use logit difference (li et al., 2020) asimportance score based on the empirical-risk-estimated parameter ˆθ obtained from dtrain as:imp(xij|z, ˆθ) = logity(x; ˆθ) − logity(x−ij; ˆθ),where every term in the right hand side (rhs) isthe logit output evaluated at a model prediction yfrom model ˆθ right before applying the softmaxfunction.
this equation tells us how importanta training span is.
it is equivalent to the lossdifference.
imp(xij|z; ˆθ) = l(z−ij; ˆθ) − l(z; ˆθ),.
(3).
when the− (cid:80)yi.
crossi(y = yi)logityi.
entropy loss l(z; θ) =.
(x; θ) is applied..then, we measure xij’s inﬂuence on modelˆθ by adding a fraction of imp(xij|z; ˆθ) scaledby a small value (cid:15) to the overall loss and ob-tain ˆθ(cid:15),xij |z:= argminθezi∈dtrain[l(zi, θ)] +(cid:15)l(z−ij; θ) − (cid:15)l(z; θ).
applying the classical re-sult in (cook and weisberg, 1982; koh and liang,2017), the inﬂuence of up-weighing the importanceof xij on ˆθ is.
dˆθ(cid:15),xij |zd(cid:15).
(cid:12)(cid:12)(cid:12)(cid:15)=0.
=.
h −1ˆθ.
(∇ˆθl(z; ˆθ) − ∇ˆθl(z−ij; ˆθ))..finally, applying the above equation and thechain rule, we obtain the inﬂuence of xij to z(cid:48)as:if+(xij|z, z(cid:48); ˆθ) := ∇(cid:15)l(z(cid:48); ˆθ(cid:15),xij |z)|(cid:15)=0= ∇θl(z(cid:48); ˆθ)h −1ˆθ.
(∇θl(z; ˆθ) − ∇θl(z−ij; ˆθ))..if+ measures the inﬂuence of a training span onan entire test sequence.
similarly, we also measurethe inﬂuence of a training span to a test span x(cid:48)klby applying eq.
3 and obtainkl|z(cid:48); ˆθ).
if++(xij|z, x(cid:48).
:=∇(cid:15)l(z(cid:48)=(∇θl(z(cid:48)h −1ˆθ.
−kl; ˆθ(cid:15),xij |z) − ∇(cid:15)l(z(cid:48); ˆθ(cid:15),xij |z)|(cid:15)=0−kl; ˆθ) − ∇θl(z(cid:48); ˆθ))(∇θl(z; ˆθ) − ∇θl(z−ij; ˆθ))..the complete derivation can be found in appendix..on the choice of spans theoretically, if+ andif++ can be applied to any text classiﬁcation prob-lem and dataset with an appropriate choice of thespan.
if no information about valid span is avail-able, shallow parsing tools or sentence split-toolscan be used to shatter an entire text sequence intochunks, and each chunk can be used as span can-didates.
in this situation, the algorithm can workin two steps: 1) using masking method (li et al.,2020) to determine the important test spans; and2) for each span we apply if++ to ﬁnd traininginstances/spans as explanations..usually, we can choose top-k test spans, andeven can choose k=1 in some cases.
in this work,we look at the later case without loss of gener-ality, and adopt two aspect-based sentiment anal-ysis datasets that can conveniently identify a de-terministic span in each text sequence, and framethe span selection task as a reading comprehen-sion task (rajpurkar et al., 2016).
we discussthe details in section 5. note that the discus-sion can be trivially generalized to the case wherek>1 using bayesian approach such as imp(xij) =kl)[imp(xij|xkl)(cid:48)] which can be explored inep (x(cid:48)future work..3.2 faithful & hessian-free explanations.
to achieve 2), we would start with the method oftracin (pruthi et al., 2020b) described in eq.
2which is hessian free by design.
tracin deﬁnesthe contribution of a training instance to be thesum of its contribution (loss) throughout the entiretraining life cycle, which eradicated the need forhessian.
however, this assumption is drasticallydifferent from if’s where the contribution of z isobtained solely from the ﬁnal model ˆθ.
by nature,if is a faithful method, and its explanation is faith-ful to ˆθ, and tracin in its vanilla form is arguablynot a faithful method..proposed treatment based on the assumptionthat the inﬂuence of z on ˆθ is the sum of inﬂu-ences of all variants close to ˆθ, we deﬁne a setof “faithful” variants satisfying the constraint of{ˆθi|1 > δ >> ||ˆθi − ˆθ||2}, namely δ-faithful toˆθ.
the smaller δ is, the more faithful the explana-tion method is.
instead, the δ for tracin can bearbitrary large without faithfulness guarantees, assome checkpoints can be far from the ﬁnal ˆθ.
thus,we construct a δ-faithful explanation method that.
5401mirrors tracin as:.
tracinf(z, z(cid:48)) =(cid:88).
i.
∇ˆθ+δi.
l(ˆθ + δi, z)∇ˆθ+δi.
l(ˆθ + δi, z(cid:48))..the difference between tracin and tracinf isthat the checkpoints used in tracin are correlatedin time whereas all variants of tracinf are con-ditionally independent.
finding a proper δi can betricky.
if ill-chosen, δi may diverge ˆθ so much thathurts gradient estimation.
in practice, we estimateδi = ηig(zi|ˆθ) obtained from a single-step gradientdescent g(zi|ˆθ) with some training instance zi onmodel ˆθ, scaled by an i-speciﬁc weighting parame-ter ηi, which in the simplest case is uniform for alli. usually ηi should be small enough so that ˆθ + δican stay close to ˆθ.
in this paper we set η as themodel learning rate for proof of concept.
is tracinf faithful?
first, any ˆθ + δi is closeto ˆθ.
under the assumption of lipschitz continuity,there exists a k ∈ r+ such that ∇l(ˆθ + δi, z)is bounded around ∇l(ˆθ, z) by k|ηig2(zi|ˆθ)|,the second derivative, because |∇l(ˆθ + δi, z) −∇l(ˆθ, z)| < k|ηig2(zi|ˆθ)|.
a proper ηi can be cho-sen so that the right hand side (rhs) is sufﬁcientlysmall to bound the loss within a small range.
thus,the gradient of loss, and in turn the tracinf scorecan stay δ-faithful to ˆθ for an sufﬁciently small δ,which tracin can not guarantee..3.3 the combined method.
by combining the insights from section 3.1 and3.2, we obtain a ﬁnal form named tracin++:.
tracin++(x(cid:48)(cid:88).
(cid:2)∇l(ˆθ + δi, z(cid:48).
kl|z(cid:48), xij|z; ˆθ) =.
−kl) − ∇l(ˆθ + δi, z(cid:48))(cid:3).
i.
(cid:2)∇l(ˆθ + δi, z) − ∇l(ˆθ + δi, z−ij)(cid:3)..this ultimate form mirrors the if++ method, andit satisﬁes all of our desiderata on an improvedexplanability method.
similarly, tracin+ thatmirrors if+ is.
tracin+(z(cid:48), xij|z; ˆθ) =.
∇l(z(cid:48); ˆθ + δi).
(cid:88).
i.we discuss the computation challenge.
follow-ing (koh and liang, 2017), we adopt the vector-hessian-inverse-product (vhp) with stochasticestimation (baydin et al., 2016).
the seriesof stochastic updates, one for each training in-stance, is performed by the vhp() function in thetorch.autograd.functional package andthe update stops until convergence.
unfortunately,we found that naively applying this approach leadsto vhp explosion due to large parameter size.
tobe speciﬁc, in our case, the parameters are the lasttwo layers of roberta-large (liu et al., 2019)plus the output head, a total of 12m parameters pergradient vector.
to stabilize the process, we takethree approaches: 1) applying gradient clipping (setto 100) to avoid accumulating the extreme gradi-ent values; 2) adopting early termination when thenorm of vhp stabilizes (usually < 1000 traininginstances, i.e., the depth); and 3) slowly decayingthe accumulated vhp with a factor of 0.99 (i.e.,the damp) and update with a new vhp() estimatewith a small learning rate (i.e., the scale) of 0.004.please refer to our code for more details.
onceobtained, the vhp is ﬁrst cached and then retrievedto perform the dot-product with the last term.
thecomplexity for each test instance is o(dt) whered is the depth of estimation and t is the time spenton each vhp() operation.
the time complexity ofdifferent if methods only vary on a constant factorof two.
for.
tracin+ andtracin++, we need to create multiple modelvariants.
for tracin, we save three checkpointsof the most recent training epochs; for tracin+or tracin++, we start with the same checkpointand randomly sample a mini-batch 3 times andperform one-step training (learning rate 1e-4) foreach selection to obtain three variants.
we do notover-tune those hyper-parameters for replicabilityconcerns..of tracin,.
each.
4 evaluation metrics.
this section introduces our semantic evaluationmethod, followed by a description of two otherpopular metrics for comparison..(cid:2)∇l(ˆθ + δi, z) − ∇l(ˆθ + δi, z−ij)(cid:3)..4.1 semantic agreement (sag).
3.4 additional detailssince the rhs of if, if+ and if++ equationsall involve the inverse of hessian matrix, here.
intuitively, a rational explanation method shouldrank explanations that are semantically related tothe given test instance relatively higher than theless relevant ones.
our idea is to ﬁrst deﬁne the.
5402semantic representation of a training span xij of zand measure its similarity to that of a test span x(cid:48)klof z(cid:48).
since our method uses bert family as thebase model, we obtain the embedding of a trainingspan by the difference of x and its span-maskedversion xij as.
where ˆθ(cid:48)is the model re-trained by the setdtrain/{z}|k1 .
notice the re-training uses thesame set of hyper-parameter settings as training(section 6.1).
to obtain {z}|k1 , we combine theexplanation lists for all test instances (by score ad-dition) and then remove the top-k from this list..emb(xij) = emb(x) − emb(x−ij),.
(4).
5 data.
where emb is obtained from the embedding of sen-tence start token such as “[cls]” in bert (devlinet al., 2019) at the last embedding layer.
to obtainembedding of the entire sequence we can simplyuse the emb(x) without the last term in eq.
4.thus, all spans are embedded in the same semanticspace and the geometric quantities such as cosineor dot-product can measure the similarities of em-beddings.
we deﬁne the semantic agreement sagas:.
1 ) =.
sag(z(cid:48), {z}|k1k.(cid:88).
z.cos(emb(xij|z), emb(x(cid:48).
kl|z(cid:48))),.
(5).
intuitively, the metric measures the degree to whichtop-k training spans align with a test span on se-mantics..4.2 other metrics.
label.
(lag)label agreementagreement(hanawa et al., 2020) assumes that the label ofan explanation z should agree with that of thetext case z(cid:48).
accordingly, we retrieve the top-ktraining instances from the ordered explanationlist and calculate the label agreement (lag) asfollows:.
lag(z(cid:48), {z}|n.
1 ) =.
i(y(cid:48) == yk),.
1k.(cid:88).
k∈[1,k].
where i(·) is an indicator function.
lag measuresthe degree to which the top-ranked z agree with z(cid:48)on class label, e.g., if the sentiment of the test z(cid:48)and explanation z agree..re-training accuracy loss (ral) ral mea-sures the loss of test accuracy after removing thetop-k most inﬂuential explanations identiﬁed by anexplanation method (hanawa et al., 2020; hookeret al., 2019; han et al., 2020).
the assumption isthat the higher the loss the better the explanationmethod is.
formally,.
ral(f, ˆθ) = acc(ˆθ) − acc(ˆθ(cid:48)),.
our criteria for dataset selection are two folds: 1.the dataset should have relatively high classiﬁ-cation accuracy so that the trained model can be-have rationally; and 2. the dataset should allowfor easy identiﬁcation of critical/useful text spansto compare span-based explanation methods.
wechose two aspect-based sentiment analysis (absa)datasets; one is atsa, a subset of mams (jianget al., 2019) for product reviews, where aspectsare the terms in the text.
the other is sentihood(saeidi et al., 2016) of location reviews.
we canidentify the relevant span of an aspect term semi-automatically and train models with high classiﬁca-tion accuracy in both datasets.
(see section 6.1 fordetails).
data statistics and instances are in table 1and 2..mamssentihood.
train111862977.dev1332747.test13361491.table 1: data statistics.
note that we regard each train-ing instance as aspect-speciﬁc, i.e., the concatenationof aspect term and the text x as model input..automatic span annotation as shown in thecolored text in table 2, we extract the spans foreach term to serve as explanation units for if+,if++, tracin+ and tracin++.
to reduce an-notation effort, we convert span extraction into aquestion answering task (rajpurkar et al., 2016)where we use aspect terms to formulate questionssuch as “how is the service?” which concatenateswith the text before being fed into pre-trained ma-chine reading comprehension (rc) models.
theoutput answer is used as the span.
when the rcmodel fails, we use heuristics to extract words be-fore and after the term word, up to the closest sen-tence boundary.
see appendix for more details.
wesampled a subset of 100 annotations and found thatthe rc model has about 70% of exact match (ra-jpurkar et al., 2016) and the overall annotation hasa high recall of over 90% but low em due to theinvolvement of heuristics..5403dataset.
mams.
sentihood.
textthe service was impeccable, the menu traditional but inven-tive and presentation for the most part excellent but the fooditself came up short.
i live in location2 and i love it location1 just stay away fromlocation1 lol..aspectservicemenufoodlocation1location2.
sentiment++--+.
table 2: dataset instances.
in text, each aspect has a supporting span which we annotate semi-automatically.
wechoose a subset where test instances.
the annotation error(not) mitigatingwrongly-annotated spans may confusetheexplanation methods.
for example, as shown in2, if the span of location2 is annotated as “i loveit”, span-based explanation methods will use itto ﬁnd wrong examples for explanation.
thustest instances with incorrectly annotated spansare omitted, i.e., no tolerance to annotation errorfor test instances.
to the contrary, for traininginstances, we do not correct the annotation error.
the major reason is the explanation methods havea chance to rank the wrongly annotated spanslower (its importance score imp() of eq.
3 can belower and in turn for its inﬂuence scores.)
also, itis labor-intensive to do so..6 experiments.
6.1 model training details.
we train two separate models for mams andsentihood.
the model’s input is the concatena-tion of the aspect term and the entire text, andthe output is a sentiment label.
the two mod-els share similar settings: 1.they both useroberta-large (liu et al., 2019) from hug-gingface (wolf et al., 2019) which is fed into thebertforsequenceclassification func-tion for initialization.
we ﬁne-tune the parametersof the last two layers and the output head using abatch size of 200 for atsa and 100 for sentihoodand max epochs of 100. we use adamw opti-mizer (loshchilov and hutter, 2019) with weightdecay 0.01 and learning rate 1e-4.
both modelsare written in pytorch and are trained on a singletesla v100 gpu and took less than 2 hours foreach model to train.
the models are selected ondev set performance, and both trained models arestate-of-the-art: 88.3% on mams and 97.6% forsentihood at the time of writing..6.2 comparing explanation methods.
we compare the six explanation methods on twodatasets and three evaluation metrics in table 3from which we can draw the following conclusions:1) tracin family outperforms if family ac-cording to sag and lag metrics.
we see that bothmetrics are robust against the choice of k. it itworth noting that tracin family methods are notonly efﬁcient, but also effective for extracting ex-planations compared to if family as per sag andlag..2) span-based methods (with +) outperformvanilla methods (w/o +).
it is good news becausean explanation can be much easier to comprehend ifwe can highlight essential spans in text, and if++and tracin++ shows us that such highlightingcan be justiﬁed by their superiority on the evalua-tion of sag and lag..3) sag and lag shows a consistent trend oftracin++ and if++ being superior to the restof the methods, while ral results are inconclusive,which resonates with the ﬁndings in (hooker et al.,2019) where they also observed randomness af-ter removing examples under different explanationmethods.
this suggests that the re-training methodmay not be a reliable metric due to the random-ness and intricate details involved in the re-trainingprocess..4) the sag measures tracin+ differently thanlag shows that lag may be an over-simplisticmeasure by assuming that label y can represent theentire semantics of x, which may be problematic.
but sag looks into the x for semantics and canproperly reﬂect and align with humans judgments..the impact of k on metrics one critical param-eter for evaluation metrics is the choice of k forsag and lag (we do not discuss k for ral dueto its randomness).
here we use 200 mams testinstances as subjects to study the inﬂuence of k, asshown in figure 1..5404if(cid:88)(cid:55)interpretable explanations?
(cid:55).
faithful to ˆθ?
hessian-free?.
if++ tracinf tracin+ tracin++(cid:88)(cid:88)(cid:88)(cid:55)(cid:88)(cid:88)(cid:88).
(cid:88)(cid:88)(cid:88)(cid:88).
(cid:88)(cid:88)(cid:55).
mams.
sentihood.
sag(k=10)sag(k=100)lag(k=10)lag(k=100)ral(- top 20%)ral(- top 50%)sag(k=10)sag(k=50)lag(k=10)lag(k=50)ral(- top 20%)ral(- top 50%).
14.2214.6521.6326.0709.8028.5504.6903.5653.0056.3810.5616.21.
21.7419.8365.4162.5203.5518.1422.5422.2161.9663.1606.9111.05.
15.8915.9738.2043.1909.8022.3003.0701.7855.9159.6609.2327.83.
22.6519.5408.6006.2711.8905.6400.9801.6118.2217.4906.919.23.
23.9221.3278.0375.0216.0518.1426.2123.4366.6566.7209.234.58.if+(cid:88)(cid:55)(cid:88).
17.1715.1025.6625.6605.6401.4704.7507.8241.9144.0516.2118.53.table 3: performance of difference explanation methods on 200 test cases on each dataset.
for sag and lagwe set k ∈ {10, 100}; for ral we set k ∈ {20%, 50%}, and ral we consider removing the top 20% or 50%from the ordered training instance list.
computation time for if family is about 20 minutes per test instance withrecursion depth 1000 (the minimal value to guarantee convergence) on a tesla v100 gpu.
the time for tracinfamily only depends on gradient calculation, which is trivial compared to if family..we found that as k increases, all methods, ex-cept for if and tracinf, decrease on sag andlag.
the decrease is favorable because the expla-nation method is putting useful training instancesbefore less useful ones.
in contrast, the increasesuggests the explanation method fails to rank use-ful ones on top.
this again conﬁrms that span-based explanation can take into account the usefulinformation in x and reduce the impact of noisyinformation involved in if and tracinf..6.3 comparing faithfulnesshow faithful our proposed tracin++ to ˆθ?
toanswer this question, we ﬁrst deﬁne the notion ofstrictly faithful explanation and then test an ex-planation method’s faithfulness against it.
notethat none of the discussed methods is strictlyfaithful, since if++ used approximated inverse-hessian and tracin++ is a δ away from beingstrictly faithful.
to obtain ground truth, we mod-ify tracin++ to use a single checkpoint ˆθ as the“ultimately faithful” explanation method 4. then,we obtain an explanation list for each test instanceand compute its spearman correlation with the listobtained from the ground truth.
the higher thecorrelation, the more faithful the method is..in table 4 we discovered that tracin++ hassimilar mean as if++ but has a much lower vari-ance, showing its stability over if++.
this alignswith the ﬁnding of basu et al.
(2021) which ar-gues that in deep non-convex networks, inﬂuencefunction usually is non-stable across test instances.
tracin family arguably may be a promising di-rection to stability.
both methods are more faithfulto ground truth than control that uses checkpoints,.
4the choice of ground truth can also be the exact computa-tion of inverse-hessian in if (our future work).
faithfulnessdoes not equal to correctness; there is no guarantee the groundtruth is a valid explanation method, but it can be a valid bench-mark for faithfulness.
figure 1: sag and lag v.s.
k values on 200 mamstest instances..54050.10.120.140.160.180.20.220.240.26102030405060708090100mamsonsagifif+if++tracinftracin+tracin++00.10.20.30.40.50.60.70.80.9102030405060708090100mamsonlagifif+if++tracinftracin+tracin++methodcontrol.
spearmanmean var.
4.8455.11tracin++ 60.143.5720.5059.37.if++.
table 4: comparison of correlation with ground truth.
the experiment is run 5 times each; “control” is onlydifferent from tracin++ on the models used: “con-trol” uses three checkpoints of the latest epochs, buttracin++ uses three δ-faithful model variants..showing that the model “ensemble” around ˆθ maybe a better choice than “checkpoint averaging” formodel explanations.
further explorations may beneeded since there are many variables in this com-parison..7 a case study.
table 5 demonstrate the differences of explanationmethods.
in action, tracin++ shows both thetest span and explanation span to a user; tracin+shows only the training span, and tracin doesnot show spans.
interestingly we can observe thetop-1 explanation found by tracin++ is moresemantically related than others in the example, acommon pattern among the test cases..8 related work.
popular explanation methods include gradient-based (sundararajan et al., 2017), attention-based(clark et al., 2019; jain and wallace, 2019; wiegr-effe and pinter, 2019), as well as sample-based(koh and liang, 2017; yeh et al., 2018; pruthiet al., 2020b) methods..major progress on sample-based explanationmethods there have been a series of recent ef-forts to explain black-box deep neural nets (dnn),such as lime (ribeiro et al., 2016) that approxi-mates the behavior of dnn with an interpretablemodel learned from local samples around predic-tion, inﬂuence functions (koh and liang, 2017;koh et al., 2019) that picks training samples asexplanation via its impact on the overall loss, andexemplar points (yeh et al., 2018) that can assignweights to training samples.
tracin (pruthi et al.,2020b) is the latest breakthrough that overcomesthe computational bottleneck of inﬂuence func-tions with the cost of faithfulness..the discussion of explanation faithfulness innlp the issue of faithfulness of explanationswas primarily discussed under the explanation gen-eration context (camburu et al., 2018) where thereis no guarantee that a generated explanation wouldbe faithful to a model’s inner-workings (jacovi andgoldberg, 2020).
in this work, we discuss faithful-ness in the sample-based explanations framework.
the faithfulness to model either can be guaranteedonly in theory but not in practice (koh and liang,2017) or can not be guaranteed at all (pruthi et al.,2020b)..sample-based explanation methods for nlphan et al.
(2020) applied if for sentiment analysisand natural language inference and also studiedits utility on detecting data artefacts (gururanganet al., 2019).
yang et al.
(2020b) used inﬂuencefunctions to ﬁlter the generated texts.
the oneclosest to our work is (meng et al., 2020a) where asingle word is used as the explanation unit.
theirformation uses gradient-based methods for singlewords, while ours can be applied to any text unitgranularity using text masking..explanation of nlp models by input erasureinput erasure has been a popular trick for measur-ing input impact for nlp models by replacing inputby zero vector (li et al., 2016) or by marginaliza-tion of all possible candidate tokens (kim et al.,2020) that arguably dealt with the out of distribu-tion issue introduced by using zero as input mask.
similar to (kim et al., 2020; li et al., 2020; jacoviand goldberg, 2021) we also use “[mask]” to-ken, with the difference that we allow masking ofarbitrary length of an input sequence..evaluations of sample-based methods abenchmark of evaluating sample-based explanationmethods has not been agreed upon.
for diagnosticpurposes, koh et al.
(2017) proposed a self-explanation method that uses the training instances(2020)to explain themselves; hanawa et al.
proposed the label and instance consistency as away of model sanity check.
on the non-diagnosticsetting, sample removal and re-training (han et al.,2020; hooker et al., 2019) assumes that removinguseful training instances can cause signiﬁcantaccuracy loss; input enhancement method assumesuseful explanations can also improve model’sdecision making at model input side (hao, 2020),and manual inspections (han et al., 2020; menget al., 2020a) were also used to examine if the.
5406test case.
tracin++.
tracin+.
tracinf.
if++.
if+.
if.
been here a few times and food has always been good but service really sufferswhen it gets crowded.
expected there to be more options for tapas the food was mediocre but theservice was pretty good.
decor is simple yet functional and although the staff are not the most attentivein the world, ...this place is the tourist fav of chinese food in the city, the service was fast, butthe taste of the food is average, too much starch ...... the host was rude to us as we walked in, we stayed because the decor ischarming and we wanted french food.
the scene a dark refurbished dining car hosts plenty of hipsters in carefullyselected thrift-store clothing.
an unpretentious sexy atmosphere lends itself to the above average wine-listand a menu that can stand-up to any other restaurant ....+.
+.
+.
0.
+.
+.
+.
table 5: showcasing top-1 explanations.
aspect terms are in blue, and the spans are in bold font.
tracinfdo not highlight either training or testing span; tracin+ highlights training span; tracin++ highlights bothtraining and test spans.
tracin++ and if++ can help users understand which span of z inﬂuenced which spanof z(cid:48), which tracinf and if do not provide..meanings of explanations align with that of the testinstance.
in this paper, we automate this semanticexamination using the embedding similarities..oana-maria camburu, tim rockt¨aschel, thomaslukasiewicz, and phil blunsom.
2018. e-snli: nat-ural language inference with natural language expla-nations.
in nips..9 future work.
tracin++ opens some new questions: 1) howcan we generalize tracin++ to cases where testspans are unknown?
2) can we understand the con-nection between if and tracin which may sparkdiscoveries on sample-based explanation methods?
3) how can we apply tracin++ to understandsequence generation models?.
acknowledgement.
this work is supported by the mit-ibm watson ailab.
the views and conclusions are those of the au-thors and should not be interpreted as representingthe ofﬁcial policies of the funding agencies.
wethank anonymous reviewers for their valuable feed-back.
we also thank your family for the supportduring this special time..references.
samyadeep basu, philip pope, and soheil feizi.
2021.inﬂuence functions in deep learning are fragile.
iclr..atılım g¨unes¸ baydin, barak a pearlmutter, and jef-frey mark siskind.
2016. tricks from deep learning.
arxiv preprint arxiv:1611.03777..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertan analysis of bert’s attention.
black-look at?
boxnlp, abs/1906.04341..r dennis cook and sanford weisberg.
1982. residu-als and inﬂuence in regression.
new york: chap-man and hall..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..suchin gururangan, tam dang, dallas card, andnoah a. smith.
2019. variational pretraining forsemi-supervised text classiﬁcation.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 5880–5894, flo-rence, italy.
association for computational linguis-tics..frank r hampel.
1974. the inﬂuence curve and itsrole in robust estimation.
journal of the americanstatistical association, 69(346):383–393..xiaochuang han, byron c. wallace, and yuliatsvetkov.
2020. explaining black box predictionsand unveiling data artifacts through inﬂuence func-tions.
in acl..5407kazuaki hanawa, sho yokoi, satoshi hara, andevaluation criteria forarxiv preprint.
kentaro inui.
2020.instance-based explanation.
arxiv:2006.04528..yiding hao.
2020. evaluating attribution methods us-ing white-box lstms.
in proceedings of the thirdblackboxnlp workshop on analyzing and interpret-ing neural networks for nlp, pages 300–313, on-line.
association for computational linguistics..sara hooker, dumitru erhan, pieter-jan kindermans,and been kim.
2019. a benchmark for interpretabil-ity methods in deep neural networks.
in advances inneural information processing systems, volume 32.curran associates, inc..alon jacovi and yoav goldberg.
2020. towards faith-fully interpretable nlp systems: how should we de-ﬁne and evaluate faithfulness?
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4198–4205, online.
as-sociation for computational linguistics..alon jacovi and yoav goldberg.
2021. aligningfaithful interpretations with their social attribution.
transactions of the association for computationallinguistics, 9:294–310..sarthak jain and byron c. wallace.
2019. attention isin proceedings of the 2019 con-not explanation.
ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, volume 1 (long and short pa-pers), pages 3543–3556, minneapolis, minnesota.
association for computational linguistics..qingnan jiang, lei chen, ruifeng xu, xiang ao, andmin yang.
2019. a challenge dataset and effec-tive models for aspect-based sentiment analysis.
inemnlp-ijcnlp, pages 6281–6286..siwon kim, jihun yi, eunji kim, and sungroh yoon.
2020. interpretation of nlp models through inputin proceedings of the 2020 con-marginalization.
ference on empirical methods in natural languageprocessing (emnlp), pages 3154–3167, online.
as-sociation for computational linguistics..pang wei koh, kai-siang ang, hubert h. k. teo,and percy liang.
2019. on the accuracy of inﬂu-ence functions for measuring group effects.
corr,abs/1905.13289..pang wei koh and percy liang.
2017. understand-ing black-box predictions via inﬂuence functions.
inicml, pages 1885–1894..jiwei li, will monroe, and dan jurafsky.
2016. un-derstanding neural networks through representationerasure.
arxiv preprint arxiv:1612.08220..linyang li, ruotian ma, qipeng guo, xiangyang xue,and xipeng qiu.
2020. bert-attack: adversarial at-tack against bert using bert.
emnlp..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
corr..ilya loshchilov and frank hutter.
2019. decoupled.
weight decay regularization.
in iclr..fanyu meng, junlan feng, danping yin, si chen, andmin hu.
2020a.
a structure-enhanced graph con-volutional network for sentiment analysis.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 586–595, online.
associ-ation for computational linguistics..yuxian meng, chun fan, zijun sun, eduard hovy, feiwu, and jiwei li.
2020b.
pair the dots: jointly ex-amining training history and test stimuli for modelinterpretability.
arxiv preprint arxiv:2010.06943..phiyodr..2020..roberta-large-ﬁnetuned-squad2..https://huggingface.co/phiyodr/bart-large-finetuned-squad2.
accessed 19-dec-2020]..[online;.
danish pruthi, bhuwan dhingra, livio baldini soares,michael collins, zachary c lipton, graham neubig,and william w cohen.
2020a.
evaluating explana-tions: how much do explanations from the teacheraid students?
arxiv preprint arxiv:2012.00893..garima pruthi, frederick liu, mukund sundararajan,and satyen kale.
2020b.
estimating training datainﬂuence by tracking gradient descent.
in nips..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in proceedings ofthe 2016 conference on empirical methods in natu-ral language processing, pages 2383–2392, austin,texas.
association for computational linguistics..marco tulio ribeiro, sameer singh, and carlosguestrin.
2016.
”why should i trust you?”: explain-ing the predictions of any classiﬁer.
in proceedingsof the 22nd acm sigkdd international conferenceon knowledge discovery and data mining, kdd’16, page 1135–1144, new york, ny, usa.
asso-ciation for computing machinery..marzieh saeidi, guillaume bouchard, maria liakata,and sebastian riedel.
2016. sentihood: targetedaspect based sentiment analysis dataset for urbanneighbourhoods.
in coling..soﬁa serrano and noah a. smith.
2019. is attentionin proceedings of the 57th annualinterpretable?
meeting of the association for computational lin-guistics, pages 2931–2951, florence, italy.
associa-tion for computational linguistics..5408mukund sundararajan, ankur taly, and qiqi yan.
2017.axiomatic attribution for deep networks.
corr,abs/1703.01365..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips..sarah wiegreffe and yuval pinter.
2019. attention isnot not explanation.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 11–20, hong kong, china.
associ-ation for computational linguistics..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r’emi louf, morgan funtow-icz, and jamie brew.
2019. huggingface’s trans-formers: state-of-the-art natural language process-ing.
arxiv preprint arxiv:1910.03771..yiben yang, chaitanya malaviya, jared fernandez,swabha swayamdipta, ronan le bras, ji-pingwang, chandra bhagavatula, yejin choi, and dougdowney.
2020a.
g-daug: generative data augmen-tation for commonsense reasoning.
arxiv preprintarxiv:2004.11546..yiben yang, chaitanya malaviya, jared fernandez,swabha swayamdipta, ronan le bras, ji-pingwang, chandra bhagavatula, yejin choi, and dougdowney.
2020b.
generative data augmentation forcommonsense reasoning.
in findings of the associ-ation for computational linguistics: emnlp 2020,pages 1008–1025, online.
association for computa-tional linguistics..chih-kuan yeh, joon sik kim, ian e.h. yen, andpradeep ravikumar.
2018. representer point selec-tion for explaining deep neural networks.
in nips..5409a span extraction details.
the model we apply the huggingface (wolf et al., 2019) pre-trained rc model “phiyodr/roberta-large-ﬁnetuned-squad2” (phiyodr, 2020) which is chosen based on our comparison to a set of similar modelson squad 2.0 dataset.
we use the squad 2.0-trained model instead of 1.0 because the data is morechallenging since it involves multiple passages, and the model has to compare valid and invalid passagesfor answer span extraction, a case similar to the dataset we use.
templates we used are: the heuristics.
how is the x?
how was the x?
how are the x?
how were the x?
how do you rate the x?
how would you rate the x?
how do you think of the x?
what do you think about the x?
what do you say about the x?
what happened to the x?
what did the x do?.
table 6: templates for rc model.
when the rc model fails: 1) we consider rc model fails when no span is extracted, or the entire text isreturned as an answer.
2) we identify the location of the term in the text and expand the scope from thelocation both on the left and on the right, and when sentence boundary is found, we stop and return thespan as the span for the term.
note that we do ﬁnd cases where the words around a term do not necessarilytalk about the term.
however, we found such a case to be extremely rare..b derivation of if++.
ipert,loss(xij, z−kl; ˆθ).
(cid:12)(cid:12):= ∇(cid:15)imp(xij|x; ˆθ(cid:15),z−ij ,z)(cid:12)(cid:12)(cid:12)(cid:15)=0(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:15)=0.
dimp(xij|x; ˆθ)dˆθ.
dˆθ(cid:15),z−kl,zd(cid:15).
=.
(.
).
= (∇θoy(x, ˆθ) − ∇θoy(x−ij, ˆθ))(.
dˆθ(cid:15),z−kl,zd(cid:15)= −(∇θoy(x, ˆθ) − ∇θoy(x−ij, ˆθ))h −1ˆθ.
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:15)=0.
).
(∇θl(z−kl, ˆθ) − ∇θl(z, ˆθ)).
c derivation of tracin+ and tracin++.
similar to if(koh and liang, 2017) and tracin(pruthi et al., 2020b), we start from the taylor expansionon point ˆθt around z(cid:48) and z(cid:48).
−ij as.
l(ˆθt+1, z(cid:48)) ∼ l(ˆθt, z(cid:48)) + ∇l(ˆθt, z(cid:48))(ˆθt+1 − ˆθt)−ij)(ˆθt+1 − ˆθt)−ij) + ∇l(ˆθt, z(cid:48).
−ij) ∼ l(ˆθt, z(cid:48).
l(ˆθt+1, z(cid:48).
if sgd is assumed for optimization for simplicity, (ˆθt+1 − ˆθt) = λ∇l(ˆθt, z).
thus, putting it in aboveequations and perform subtraction, we obtain.
l(ˆθt+1, z(cid:48)) − l(ˆθt+1, z(cid:48).
−ij) ∼ l(ˆθt, z(cid:48).
−ij) − l(ˆθt, z(cid:48)) + [∇l(ˆθt, z(cid:48)) − ∇l(ˆθt, z(cid:48).
−ij)]λ∇l(ˆθt, z).
5410and,.
imp(x(cid:48).
ij|z(cid:48); ˆθt+1) − imp(x(cid:48).
ij|z(cid:48); ˆθt) ∼ [∇l(ˆθt, z(cid:48).
−ij) − ∇l(ˆθt, z(cid:48))]λ∇l(ˆθt, z).
so, the left term is the change of importance by parameter change; we can interpret it as the change ofimportance score of span xij w.r.t the parameter of networks.
then, we integrate over all the contributionsfrom different points in the training process and obtain.
tracin+(x(cid:48).
ij|z(cid:48), z) =.
[∇l(ˆθt, z(cid:48).
−ij) − ∇l(ˆθt, z(cid:48))]λ∇l(ˆθt, z).
(cid:88).
t.(cid:88).
t.(cid:88).
t.the above formation is very similar to tracinwhere a single training instance z is evaluated as a whole.
but we are interested in the case where an meaning unit xkl in z can be evaluated for inﬂuence.
thus, weapply the same logic of the above equation to z−kl, the perturbed training instance where token k to l ismasked, as.
tracin+(x(cid:48).
ij|z(cid:48), z−kl) =.
[∇l(ˆθt, z(cid:48).
−ij) − ∇l(ˆθt, z(cid:48))]λ∇l(ˆθt, z−kl).
then, the difference tracin+(x(cid:48)training span xkl on test span x(cid:48).
ij|z(cid:48), z) − tracin+(x(cid:48).
ij|z(cid:48), z−kl) can indicate how much impact a.ij.
formally, the inﬂuence of xkl on x(cid:48).
ij is.
tracin++(x(cid:48).
ij, x−kl|z(cid:48), z) = λ.
[∇l(ˆθt, z(cid:48).
−ij) − ∇l(ˆθt, z(cid:48))][∇l(ˆθt, z) − ∇l(ˆθt, z−kl)].
we denote that such a form is very easy to implement, since each item in summation requires only four(4) gradient estimates..5411