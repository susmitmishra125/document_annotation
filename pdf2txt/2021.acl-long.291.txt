cogalign: learning to align textual neural representations to cognitivelanguage processing signals.
yuqi ren and deyi xiong ∗college of intelligence and computing, tianjin university, tianjin, china{ryq20, dyxiong}@tju.edu.cn.
abstract.
most previous studies integrate cognitive lan-guage processing signals (e.g., eye-trackingor eeg data) into neural models of naturallanguage processing (nlp) just by directlyconcatenating word embeddings with cogni-tive features, ignoring the gap between thetwo modalities (i.e.,cognitive)textual vs.in this pa-and noise in cognitive features.
per, we propose a cogalign approach to theseissues, which learns to align textual neuralrepresentations to cognitive features.
in co-galign, we use a shared encoder equippedwith a modality discriminator to alternativelyencode textual and cognitive inputs to capturetheir differences and commonalities.
addition-ally, a text-aware attention mechanism is pro-posed to detect task-related information and toavoid using noise in cognitive features.
ex-perimental results on three nlp tasks, namelynamed entity recognition, sentiment analysisand relation extraction, show that cogalignachieves signiﬁcant improvements with mul-tiple cognitive features over state-of-the-artmodels on public datasets.
moreover, ourmodel is able to transfer cognitive informationto other datasets that do not have any cogni-tive processing signals.
the source code forcogalign is available at https://github.
com/tjunlp-lab/cogalign.git..1.introduction.
cognitive neuroscience, from a perspective of lan-guage processing, studies the biological and cogni-tive processes and aspects that underlie the mentallanguage processing procedures in human brainswhile natural language processing (nlp) teachesmachines to read, analyze, translate and generatehuman language sequences (muttenthaler et al.,2020).
the commonality of language process-ing shared by these two areas forms the base of.
∗corresponding author.
cognitively-inspired nlp, which uses cognitivelanguage processing signals generated by humanbrains to enhance or probe neural models in solvinga variety of nlp tasks, such as sentiment analysis(mishra et al., 2017; barrett et al., 2018), namedentity recognition (ner) (hollenstein and zhang,2019), dependency parsing (strzyz et al., 2019),relation extraction (hollenstein et al., 2019a), etc.
in spite of the success of cognitively-inspirednlp in some tasks, there are some issues in theuse of cognitive features in nlp.
first, for the inte-gration of cognitive processing signals into neuralmodels of nlp tasks, most previous studies havejust directly concatenated word embeddings withcognitive features from eye-tracking or eeg, ignor-ing the huge differences between these two typesof representations.
word embeddings are usuallylearned as static or contextualized representationsof words in large-scale spoken or written texts gen-erated by humans.
in contrast, cognitive languageprocessing signals are collected by speciﬁc medi-cal equipments, which record the activity of humanbrains during the cognitive process of languageprocessing.
these cognitive processing signals areusually assumed to represent psycholinguistic in-formation (mathias et al., 2020) or cognitive load(antonenko et al., 2010).
intuitively, information inthese two types of features (i.e., word embeddingsand cognitive features) is not directly comparableto each other.
as a result, directly concatenatingthem could be not optimal for neural models tosolve nlp tasks..the second issue with the incorporation of cogni-tive processing signals into neural models of nlp isthat not all information in cognitive processing sig-nals is useful for nlp.
the recorded signals containinformation covering a wide variety of cognitiveprocesses, particularly for eeg (williams et al.,2019; eugster et al., 2014).
for different tasks, wemay need to detect elements in the recorded signals,.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3758–3769august1–6,2021.©2021associationforcomputationallinguistics3758figure 1: neural architecture of the proposed cogalign.
for inference, only the components in the red dashedbox are used..which are closely related to speciﬁc nlp tasks, andneglect features that are noisy to the tasks..in order to address the two issues, we proposecogalign, a multi-task neural network that learnsto align neural representations of texts to cogni-tive processing signals, for several nlp tasks.
asshown in figure 1, instead of simply concatenatingcognitive features with word embeddings, we usetwo private encoders to separately encode cognitiveprocessing signals and word embeddings.
the twoencoders will learn task-speciﬁc representationsfor cognitive and textual inputs in two disentan-gled spaces.
to align the representations of neuralnetwork with cognitive processing signals, we fur-ther introduce an additional encoder that is sharedby both data sources.
we alternatively feed cog-nitive and textual inputs into the shared encoderand force it to minimize an adversarial loss of thediscriminator stacked over the shared encoder.
thediscriminator is task-agnostic so that it can focuson learning both differences and deep commonal-ities between neural representations of cognitiveand textual features in the shared encoder.
we wantthe shared encoder to be able to transfer knowledgeof cognitive language processing signals to otherdatasets even if cognitive processing signals are notavailable for those datasets.
therefore, cogaligndoes not require cognitive processing signals asinputs during inference..partially inspired by the attentive pooling net-work (santos et al., 2016), we propose a text-awareattention mechanism to further align textual inputsand cognitive processing signals at the word level..the attention network learns a compatibility matrixof textual inputs to cognitive processing signals.
the learned text-aware representations of cognitiveprocessing signals also help the model to detecttask-related information and to avoid using othernoisy information contained in cognitive process-ing signals..in a nutshell, our contributions are listed as fol-.
lows:.
• we present cogalign that learns to align neu-ral representations of natural language to cog-nitive processing signals at both word and sen-tence level.
our analyses show that it canlearn task-related speciﬁc cognitive process-ing signals..• we propose a text-aware attention mechanismthat extracts useful cognitive information viaa compatibility matrix..• with the adversarially trained shared encoder,cogalign is capable of transferring cognitiveknowledge into other datasets for the sametask, where no recorded cognitive processingsignals are available..• we conduct experiments on incorporating eye-tracking and eeg signals into 3 different nlptasks: ner, sentiment analysis and relationextraction, which show cogalign achievesnew state-of-the-art results and signiﬁcant im-provements over strong baselines..37592 related work.
eye-tracking for nlp.
eye-tracking data haveproved to be associated with language comprehen-sion activity in human brains by numerous researchin neuroscience (rayner, 1998; henderson and fer-reira, 1993).
in cognitively motivated nlp, severalstudies have investigated the impact of eye-trackingdata on nlp tasks.
in early works, these signalshave been used in machine learning approaches tonlp tasks, such as part-of-speech tagging (barrettet al., 2016), multiword expression extraction (ro-hanian et al., 2017), syntactic category prediction(barrett and søgaard, 2015).
in neural models, eye-tracking data are combined with word embeddingsto improve various nlp tasks, such as sentimentanalysis (mishra et al., 2017) and ner (hollen-stein and zhang, 2019).
eye-tracking data havealso been used to enhance or constrain neural atten-tion in (barrett et al., 2018; sood et al., 2020b,a;takmaz et al., 2020)..eeg for nlp.
electroencephalography (eeg)measures potentials ﬂuctuations caused by the ac-tivity of neurons in cerebral cortex.
the explo-ration of eeg data in nlp tasks is relatively lim-ited.
chen et al.
(2012) improve the performanceof automatic speech recognition (asr) by usingeeg signals to classify the speaker’s mental state.
hollenstein et al.
(2019a) incorporate eeg signalsinto nlp tasks, including ner, relation extractionand sentiment analysis.
additionally, muttenthaleret al.
(2020) leverage eeg features to regularizeattention on relation extraction..adversarial learning.
the concept of adversar-ial training originates from the generative adver-sarial nets (gan) (goodfellow et al., 2014) in com-puter vision.
since then, it has been also appliedin nlp (denton et al., 2015; ganin et al., 2016).
recently, a great variety of studies attempt to intro-duce adversarial training into multi-task learningin nlp tasks, such as chinese ner (cao et al.,2018), crowdsourcing learning (yang et al., 2018),cross-lingual transfer learning (chen et al., 2018;kim et al., 2017), just name a few.
different fromthese studies, we use adversarial learning to deeplyalign cognitive modality to textual modality at thesentence level..3 cogalign.
cogalign is a general framework for incorporat-ing cognitive processing signals into various nlp.
tasks.
the target task can be speciﬁed at the predic-tor layer with corresponding task-speciﬁc neuralnetwork.
cogalign focuses on aligning cognitiveprocessing signals to textual features at the wordand encoder level.
the text-aware attention aims atlearning task-related useful cognitive information(thus ﬁltering out noises) while the shared encoderand discriminator collectively learns to align repre-sentations of cognitive processing signals to thoseof textual inputs in a uniﬁed semantic space.
thematched neural representations can be transferredto another datasets of the target task even thoughcognitive processing signals is not present.
theneural architecture of cogalign is visualized infigure 1. we will elaborate the components ofmodel in the following subsections..3.1.input layer.
the inputs to our model include textual word em-beddings and cognitive processing signals..i.word embeddings.
for a given word xi fromthe dataset of a target nlp task (e.g., ner), we ob-tain the vector representation hwordby looking up apre-trained embedding matrix.
the obtained wordembeddings are ﬁxed during training.
for ner,previous studies have shown that character-levelfeatures can improve the performance of sequencelabeling (lin et al., 2018).
we therefore apply acharacter-level cnn framework (chiu and nichols,2016; ma and hovy, 2016) to capture the character-level embedding.
the word representation of wordxi in ner task is the concatenation of word em-bedding and character-level embedding..cognitive processing signals.
for cognitive in-puts, we can obtain word-level eye-tracking andeeg via data preprocessing (see details in section5.1).
thus, for each word xi, we employ two cog-and heegnitive processing signals heye.
the cogni-tive input hcogcan be either a single type of signalor a concatenation of different cognitive processingsignals..i.i.i.
3.2 text-aware attention.
as not all information contained in cognitive pro-cessing signals is useful for the target nlp task,we propose a text-aware attention mechanism to as-sign text sensitive weights to cognitive processingsignals.
the main process of attention mechanismconsists of learning a compatibility matrix betweenword embeddings h word ∈ rdw×n and cogni-tive representations h cog ∈ rdc×n from the input.
3760layer and preforming cognitive-wise max-poolingoperation over the matrix.
the compatibility matrixg ∈ rdw×dc can be computed as follows:.
state of modality k, we use a self-attention mecha-nism to ﬁrst reduce the dimension of the output ofthe shared bi-lstm h s.k ∈ rdh×n :.
g = tanh(h wordu h cogt.
).
(1).
where dw and dc are the dimension of word embed-dings and cognitive representations, respectively,n is the length of the input, and u ∈ rn ×n is atrainable parameter matrix..we then obtain a vector gcog ∈ rdc, which iscomputed as the importance score for each elementin the cognitive processing signals with regard tothe word embeddings, by row-wise max-poolingover g. finally, we compute attention weights andthe text-aware representation of cognitive process-ing signals h cog (cid:48).
as follows:.
αcog = softmax(gcog).
h cog (cid:48).
= αcogh cog.
(2).
(3).
3.3 encoder layer.
we adopt bi-lstms to encode both cognitive andtextual inputs following previous works (hollen-stein and zhang, 2019; hollenstein et al., 2019a).
in this work, we employ two private bi-lstmsand one shared bi-lstm as shown in figure 1,where private bi-lstms are used to encode cogni-tive and textual inputs respectively and the sharedbi-lstm is used for learning shared semantics ofboth types of inputs.
we concatenate the outputs ofprivate bi-lstms and shared bi-lstm as inputto the task-speciﬁc predictors of subsequent nlptasks.
the hidden states of the shared bi-lstmare also fed into the discriminator..3.4 modality discriminator.
we alternatively feed cognitive and textual inputsinto the shared bi-lstm encoder.
our goal is thatthe shared encoder is able to map the representa-tions of the two different sources of inputs intothe same semantic space so as to learn the deepcommonalities of two modalities (cognitive andtextual).
for this, we use a self-supervised discrim-inator to provide supervision for training the sharedencoder..particularly, the discriminator is acted as a clas-siﬁer to categorize the alternatively fed inputs intoeither the textual or cognitive input.
for the hidden.
α = softmax(vt tanh(wsh s.k + bs)).
(4).
n(cid:88).
hsk =.
αih ski.
(5).
i=1where ws ∈ rdh×dh, bs ∈ rdh, v ∈ rdh aretrainable parameters in the model, hsk is the outputof self-attention mechanism.
then we predict thecategory of the input by softmax function:.
d(hs.
k) = softmax(wdhs.
k + bd).
(6).
where d(hscoder is encoding an input with modality k..k) is the probability that the shared en-.
3.5 predictor layer.
given a sample x, the ﬁnal cognitively augmentedrepresentation after the encoder layer can be for-mulated as h (cid:48) = [h p; h s] ∈ r2dh×n .
h p andh s are the result of private bi-lstm and sharedbi-lstm, respectively..for sequence labeling tasks like ner, we em-ploy the conditional random ﬁeld (crf) (laffertyet al., 2001) as the predictor as bi-lstm-crf iswidely used in many sequence labeling tasks (maand hovy, 2016; luo et al., 2018) due to the excel-lent performance and also in cognitively inspirednlp (hollenstein and zhang, 2019; hollensteinet al., 2019a).
firstly, we project the feature repre-sentation h (cid:48)onto another space of which dimen-sion is equal to the number of ner tags as follows:.
oi = wnh.
i + bn.
(cid:48).
(7).
we then compute the score of a predicted tag.
sequence y for the given sample x:.
score(x, y) =.
(oi,yi + tyi−1,yi).
(8).
n(cid:88).
i=1.
where t is a transition score matrix which deﬁnesthe transition probability of two successive labels.
sentiment analysis and relation extraction canbe regarded as multi-class classiﬁcation tasks, with3 and 11 classes, respectively.
for these two tasks,we use a self attention mechanism to reduce thedimension of h (cid:48)and obtain the probability of apredicted class via the softmax function..37614 training and inference.
4.1 adversarial learning.
in order to learn the deep interaction between cog-nitive and textual modalities in the same semanticspace, we want the shared bi-lstm encoder tooutput representations that can fool the discrimi-nator.
therefore we adopt the adversarial learningstrategy.
particularly, the shared encoder acts as thegenerator that tries to align the textual and cogni-tive modalities as close as possible so as to misleadthe discriminator.
the shared encoder and discrim-inator works in an adversarial way..additionally, to further increase the difﬁcultyfor the discriminator to distinguish modalities, weadd a gradient reversal layer (grl) (ganin andlempitsky, 2015) in between the encoder layerand predictor layer.
the gradient reversal layerdoes nothing in the forward pass but reverses thegradients and passes them to the preceding layerduring the backward pass.
that is, gradients withrespect to the adversarial loss ∂ladvare replacedwith − ∂ladv.
after going through grl..∂θ.
∂θ.
4.2 training objective.
cogalign is established on a multi-task learningframework, where the ﬁnal training objective iscomposed of the adversarial loss ladv and the lossof the target task lt ask.
for ner, we exploitthe negative log-likelihood objective as the lossfunction.
given t training examples (x i; yi)1,lt ask is deﬁned as follows:.
lt ask = −.
logp(yi|x i).
(9).
t(cid:88).
i=1.
where y denotes the ground-truth tag sequence.
the probability of y is computed by the softmaxfunction:.
p(y|x) =.
(cid:80).
escore(x,y)(cid:101)y∈y escore(x,(cid:101)y).
(10).
for sentiment analysis and relation extractiontasks, the task objective is similar to that of ner.
the only difference is that the label of the task ischanged from a tag sequence to a single class.
the adversarial loss ladv is deﬁned as:.
ladv = minθs.
(maxθd.
k(cid:88).
tk(cid:88).
k=1.
i=1.
logd(s(x i.k))) (11).
1x can be either textual or cognitive input as we alterna-tively feed word embeddings and cognitive processing signalsinto cogalign..where θs and θd denote the parameters of the sharedbi-lstm encoders s and modality discriminatord, respectively, x ik is the representation of sen-tence i in a modality k. the joint loss of cogalignis therefore deﬁned as:.
l = lt ask + ladv.
(12).
4.3.inference.
after training, the shared encoder learns a uniﬁedsemantic space for representations of both cog-nitive and textual modality.
we believe that theshared space embeds knowledge from cognitiveprocessing signals.
for inference, we thereforeonly use the textual part and the shared encoder(components in the red dashed box in figure 1).
the private encoder outputs textual-modality-onlyrepresentations while the shared encoder generatescognitive-augmented representations.
the two rep-resentations are concatenated to feed into the pre-dictor layer of the target task.
this indicates thatwe do not need cognitive processing signals for theinference of the target task.
it also means that wecan pretrain cogalign with cognitive processingsignals and then transfer it to other datasets wherecognitive processing signals are not available forthe same target task..5 experiments.
we conducted experiments on three nlp tasks,namely ner, sentiment analysis and relation ex-traction with two types of cognitive processing sig-nals (eye-tracking and eeg) to validate the effec-tiveness of the proposed cogalign..5.1 dataset and cognitive processing signalswe chose a dataset2 with multiple cognitive pro-cessing signals: zurich cognitive language pro-cessing corpus (zuco) (hollenstein et al., 2018).
this corpus contains simultaneous eye-trackingand eeg signals collected when 12 native en-glish speakers are reading 1,100 english sentences.
word-level signals can be divided by the durationof each word..the dataset includes two reading paradigms: nor-mal reading and task-speciﬁc reading where sub-jects exercise some speciﬁc task.
in this work,we only used the data of normal reading, sincethis paradigm accords with human natural read-ing.
the materials for normal reading paradigm.
2the data is available here: https://osf.io/q3zws/.
3762early.
late.
context.
ﬁrst ﬁxation duration (ffd)ﬁrst pass duration (fpd)number of ﬁxations (nfix)ﬁxation probability (fp)mean ﬁxation duration (mfd)total ﬁxation duration (tfd)n re-ﬁxations (nr)re-read probability (rrp)total regression-from duration (trd)w-2 ﬁxation probability (w-2 fp)w-1 ﬁxation probability (w-1 fp)w+1 ﬁxation probability (w+1 fp)w+2 ﬁxation probability (w+2 fp)w-2 ﬁxation duration (w-2 fd)w-1 ﬁxation duration (w-1 fd)w+1 ﬁxation duration (w+1 fd)w+2 ﬁxation duration (w+2 fd).
the duration of word w that is ﬁrst ﬁxatedthe sum of the ﬁxations before eyes leave the word wthe number of times word w that is ﬁxatedthe probability that word w is ﬁxatedthe average ﬁxation durations for word wthe total duration of word w that is ﬁxatedthe number of times word w that is ﬁxated after the ﬁrst ﬁxationthe probability of word w that is ﬁxated more than oncethe total duration of regressions from word wthe ﬁxation probability of the word w-2the ﬁxation probability of the word w-1the ﬁxation probability of the word w+1the ﬁxation probability of the word w+2the ﬁxation duration of the word w-2the ﬁxation duration of the word w-1the ﬁxation duration of the word w+1the ﬁxation duration of the word w+2.
table 1: eye-tracking features used in the ner task..consist of two datasets: 400 movie reviews fromstanford sentiment treebank (socher et al., 2013)with manually annotated sentiment labels, includ-ing 123 neutral, 137 negative and 140 positive sen-tences; 300 paragraphs about famous people fromwikipedia relation extraction corpus (culotta et al.,2006) labeled with 11 relationship types, such asaward, education..we also tested our model on ner task.
for ner,the selected 700 sentences in the above two tasksare annotated with three types of entities: person,organization, and location.
all annotateddatasets3 are publicly available.
the cognitive pro-cessing signals and textual features used for eachtask in this work are the same as (hollenstein et al.,2019a)..eye-tracking features.
eye-tracking signalsrecord human gaze behavior while reading.
theeye-tracking data of zuco are collected by an in-frared video-based eye tracker eyelink 1000 pluswith a sampling rate of 500 hz.
for ner, weused 17 eye-tracking features that cover all stagesof gaze behaviors and the effect of context.
ac-cording to the reading process, these features aredivided into three groups: early, the gaze behav-ior when a word is ﬁxated for the ﬁrst time; late,the gaze behavior over a word that is ﬁxated manytimes; context, the eye-tracking features overneighboring words of the current word.
the 17 eye-tracking features used in the ner task are shownin the table 1. in the other two tasks, we employed5 gaze behaviors, including the ﬁrst ﬁxation dura-tion (ffd), the number of ﬁxations (nfix), thetotal ﬁxation duration (tfd), the ﬁrst pass duration.
3https://github.com/ds3lab/zuco-nlp/.
(fpd), the gaze duration (gd) that is the durationof the ﬁrst time eyes move to the current word untileyes leave the word..eeg features.
eeg signals record the brain’selectrical activity in the cerebral cortex by plac-ing electrodes on the scalp of the subject.
in thedatasets we used, eeg signals are recorded by a128-channel eeg geodesic hydrocel system (elec-trical geodesics, eugene, oregon) at a samplingrate of 500 hz with a bandpass of 0.1 to 100 hz.
the original eeg signals recorded are of 128 di-mensions.
among them, 23 eeg signals are re-moved during preprocessing since they are not re-lated to the cognitive processing (hollenstein et al.,2018).
after preprocessing, we obtained 105 eegsignals.
the left eeg signals are divided into 8frequency bands by the frequency of brain’s electri-cal signals: theta1 (t1, 4-6 hz), theta2 (t2, 6.5-8hz), alpha1 (a1, 8.5-10 hz), alpha2 (a2, 10.5-13hz), beta1 (b1, 13.5-18 hz), beta2 (b2, 18.5-30hz), gamma1 (g1, 30.5-40 hz) and gamma2 (g2,40-49.5 hz).
the frequency bands reﬂects the dif-ferent functions of brain cognitive processing.
forner, we used 8 eeg features that are obtained byaveraging the 105 eeg signals at each frequencyband.
for the other two tasks, eeg features wereobtained by averaging the 105 signals over all fre-quency bands.
all used eeg features are obtainedby averaging over all subjects and normalization..5.2 settings.
we evaluated three nlp tasks in terms of precision,recall and f1 in our experiments.
word embed-dings of all nlp tasks were initialized with thepublicly available pretrained glove (pennington.
3763eye.
signals modelbase∗(hollenstein et al., 2019a)basebase+tacogalign(hollenstein et al., 2019a)basebase+tacogalign(hollenstein et al., 2019a)basebase+tacogalign.
eye+eeg.
eeg.
p (%)89.3486.290.5690.7590.7686.789.8289.5489.8785.189.7090.7591.28.nerr (%)78.6084.381.0581.7782.5281.580.5582.2283.0883.281.1182.9483.02.f1 (%)83.4885.185.43∗85.93∗86.41∗83.984.76∗85.62∗86.21∗84.085.11∗86.31∗86.79∗.
sentiment analysisr (%)59.4261.961.9662.7164.1064.860.2962.1965.3859.361.4963.8865.94.f1 (%)58.2762.061.19∗61.41∗62.30∗65.159.79∗60.91∗62.81∗60.860.84∗63.23∗65.40∗.
p (%)59.4765.164.2664.6362.8668.364.0962.2063.1166.362.8665.2265.11.relation extractionr (%)75.6761.778.2376.4782.0660.277.1678.4682.6060.076.5277.5382.07.f1 (%)75.2561.577.95∗78.04∗78.56∗60.377.61∗77.81∗78.66∗59.877.72∗78.12∗78.93∗.
p (%)79.5261.482.0183.2678.3360.582.7980.8377.9459.879.0082.2478.66.table 2: results of cogalign and other methods on the three nlp tasks augmented with eye-tracking features (eye),eeg features (eeg), and both (eye+eeg).
‘base∗’ denotes that the model does not use any cognitive processingsignals.
‘base’ is a neural model that consist of a textual private encoder and textual predictor, and combinescognitive processing signals with word embeddings via direct concatenation, similar to previous works.
‘base+ta’is a neural model where direct concatenation in the base model is replaced by the text-aware attention mechanism.
signiﬁcance is indicated with the asterisks: * = p<0.01..et al., 2014) vectors of 300 dimensions.
for ner,we used 30-dimensional randomly initialized char-acter embeddings.
we set the dimension of hiddenstates of lstm to 50 for both the private bi-lstmand shared bi-lstm.
we performed 10-fold crossvalidation for ner and sentiment analysis and 5-fold cross validation for relation extraction..5.3 baselines.
we compared our model with previous state-of-the-art methods on zuco dataset.
the method byhollenstein et al.
(2019a) incorporates cognitiveprocessing signals into their model via direct con-catenation mentioned before..5.4 results.
results of cogalign on the three nlp tasks areshown in table 2. from the table, we observe that:.
• by just simply concatenating word embed-dings with cognitive processing signals, thebase model is better than the model withoutusing any cognitive processing signals, indi-cating that cognitive processing signals (eithereye-tracking or eeg signals) can improve allthree nlp tasks.
notably, the improvementsgained by eye-tracking features are larger thanthose obtained by eeg signals while the com-bination of both does not improve over onlyusing one of them.
we conjecture that thismay be due to the low signal-to-noise ratio ofeeg signals, which further decreases whentwo signals are combined together..• compared with the base model, the base+taachieves better results on all nlp tasks.
the.
text-aware attention gains an absolute im-provement of 0.88, 2.04, 0.17 f1 on ner,sentiment analysis, and relation extraction, re-spectively.
with base+ta, the best results formost tasks are obtained by the combination ofeye-tracking and eeg signals.
this suggeststhat the proposed text-aware attention mayhave alleviated the noise problem of cognitiveprocessing signals..• the proposed cogalign achieves the highestf1 over all three tasks, with improvements of0.48, 2.17 and 0.87 f1 over base+ta on ner,sentiment analysis and relation extraction, re-spectively, which demonstrates the effective-ness of our proposed model.
in addition, co-galign with both cognitive processing signalsobtains new state-of-the-art performance inall nlp tasks.
this suggests that cogalignis able to effectively augment neural modelswith cognitive processing signals..5.5 ablation study.
to take a deep look into the improvements con-tributed by each part of our model, we performablation study on all three nlp tasks with twocognitive processing signals.
the ablation test in-cludes: (1) w/o text-aware attention, removingtext-aware attention mechanism; (2) w/o cognitiveloss, discarding the loss of the cognitive predic-tor whose inputs are cognitive processing signals;(3) w/o modality discriminator, removing the dis-criminator to train parameters with the task loss.
table 3 reports the ablation study results..3764model.
cogalign (eye+eeg)- text-aware attention- cognitive loss- modality discriminator.
nerp (%) r (%)83.0291.2882.4590.5181.1190.2083.6689.63.sentiment analysis.
relation extraction.
f1 (%)86.79∗86.19∗85.45∗86.09∗.
p (%) r (%)65.9465.1165.3064.7565.4264.4866.2464.11.f1 (%)65.40∗63.90∗63.77∗63.28∗.
p (%) r (%)82.0778.6683.1477.6781.2477.7980.7178.61.f1 (%)78.93∗78.68∗77.75∗78.46∗.
table 3: ablation study on the three nlp tasks.
signiﬁcance is indicated with the asterisks: * = p<0.01..(a) without adv.
(b) with adv.
figure 2: the visualization of hidden states from theshared bi-lstm layer.
‘adv’ denotes the adversariallearning.
red dots are the hidden representations ofcognitive processing signals while blue dots hidden rep-resentations of textual inputs.
both are at the word levelvia t-sne (van der maaten and hinton, 2008)..the absence of the text-aware attention, cogni-tive loss and modality discriminator results in asigniﬁcant drop in performance.
this demonstratesthat these components all contribute to the effectiveincorporation of cognitive processing signals intoneural models of the three target tasks.
cogalignoutperforms both (2) w/o cognitive loss and (3)w/o modality discriminator by a great margin, in-dicating that the cognitive features can signiﬁcantlyenhance neural models..furthermore, we visualize the distribution of hid-den states learned by the shared bi-lstm to give amore intuitive demonstration of the effect of adver-sarial learning.
in figure 2, clearly, the modalitydiscriminator with adversarial learning forces theshared bi-lstm encoder to align textual inputs tocognitive processing signals in the same space..6 analysis.
6.1 text-aware attention analysis.
in addition to denoising the cognitive processingsignals, the text-aware attention mechanism alsoobtains the task-speciﬁc features.
to have a clearview of the role that the text-aware attention mecha-nism plays in cogalign, we randomly choose sam-ples and visualize the average attention weightsover each signal in figure 3..for eye-tracking, signals reﬂecting the late syn-.
(a) eye-tracking.
(b) eeg.
figure 3: the visualization of attention weights overcognitive processing signals by the text-aware attentionin the three nlp tasks.
darker colors represent higherattention weights..tactic processing, such as ‘nfix’ (number of ﬁx-ation), ‘tfd’ (total ﬁxation duration), play an im-portant role in the three tasks.
these results areconsistent with ﬁndings in cognitive neuroscience.
in cognitive neuroscience, researchers have shownthat readers tend to gaze at nouns repeatedly (furt-ner et al., 2009) (related to the eye-tracking signalnfix, the number of ﬁxations) and there is a de-pendency relationship between regression featuresand sentence syntactic structures (lopopolo et al.,2019).
in other nlp tasks that infused eye-trackingfeatures, the late gaze features have also proved tobe more important than early gaze features, such asmultiword expression extraction (rohanian et al.,2017).
moreover, from the additional eye-trackingused in ner, we can ﬁnd that the cognitive featuresfrom the neighboring words are helpful to identifyentity, such as ‘w-2 fp’ (w-2 ﬁxation probability),‘w+1 fp’ (w+1 ﬁxation probability)..since a single eeg signal has no practical mean-ing, we only visualize the attention weights overeeg signals used in the ner task.
obviously,attentions to ‘t1’ (theta1) and ‘a2’ (alpha2) arestronger than other signals, suggesting that low fre-quency electric activities in the brain are obviouswhen we recognize an entity..3765model.
baselinebaseline (two encoders)cogalign (eye)cogalign (eeg)cogalign (eye+eeg).
wikigold.
p (%) r (%)70.6780.7073.3980.1672.5980.3971.9180.5474.1781.71.f1 (%)75.1975.7376.1775.9377.76.sstp (%) r (%)57.5856.6758.0556.7659.6958.0558.3457.2558.3358.60.f1 (%)56.4056.8957.2757.1058.32.table 4: results of cogalign in transfer learning to other datasets without cognitive processing signals.
‘baseline’is a model trained and tested with one encoder for textual inputs.
‘baseline (+zuco text)’ is the baseline trainedwith both zuco textual data and target dataset (i.e., wikigold or sst).
‘baseline (two encoders)’ is the same ascogalign (the inference version), where cognitive processing signals are replaced by textual inputs..6.2 transfer learning analysis.
the cognitively-inspired nlp is limited by the col-lection of cognitive processing signals.
thus, wefurther investigate whether our model can transfercognitive features to other datasets without cogni-tive processing signals for the same task.
we enabletransfer learning in cogalign with a method similarto the alternating training approach (luong et al.,2016) that optimizes each task for a ﬁxed numberof mini-batches before shifting to the next task.
inour case, we alternately feed instances from thezuco dataset and those from other datasets builtfor the same target task but without cognitive pro-cessing signals into cogalign.
since cogalign isa multi-task learning framework, model parame-ters can be updated either by data with cognitiveprocessing signals or by data without such signals,where task-speciﬁc loss is used in both situations.
please notice that only textual inputs are fed intotrained cogalign for inference..to evaluate the capacity of cogalign in trans-ferring cognitive features, we select benchmarkdatasets for ner and sentiment analysis: wikigold(balasuriya et al., 2009) and stanford sentimenttreebank (socher et al., 2013).
since no otherdatasets use the same set of relation types as thatin zuco dataset, we do not test the relation extrac-tion task for transfer learning.
to ensure that thesame textual data are used for comparison, we adda new baseline model (baseline (+zuco text)) that istrained on the combination of textual data in zucoand benchmark dataset.
additionally, as cogalignuses two encoders for inference (i.e., the textualencoder and shared encoder), for a fair comparison,we setup another baseline (baseline (two encoders))that also uses two encoders fed with the same tex-tual inputs.
the experimental setup is the same asmentioned before..results are shown in the table 4. we can ob-serve that cogalign consistently outperforms the.
two baselines.
it indicates that cogalign is ableto effectively transfer cognitive knowledge (eithereye-tracking or eeg) from zuco to other datasets.
results show that the best performance is achievedby transferring both eye-tracking and eeg signalsat the same time..7 conclusions.
in this paper, we have presented cogalign, a frame-work that can effectively fuse cognitive processingsignals into neural models of various nlp tasks bylearning to align the textual and cognitive modal-ity at both word and sentence level.
experimentsdemonstrate that cogalign achieves new state-of-the-art results on three nlp tasks on the zucodataset.
analyses suggest that the text-aware at-tention in cogalign can learn task-related cogni-tive processing signals by attention weights whilethe modality discriminator with adversarial learn-ing forces cogalign to learn cognitive and textualrepresentations in the uniﬁed space.
further ex-periments exhibit that cogalign is able to transfercognitive information from zuco to other datasetswithout cognitive processing signals..acknowledgments.
the present research was partially supported bythe national key research and development pro-gram of china (grant no.
2019qy1802) andnatural science foundation of tianjin (grant no.
19jczdjc31400).
we would like to thank theanonymous reviewers for their insightful com-ments..references.
pavlo antonenko, fred paas, roland grabner, andtamara van gog.
2010. using electroencephalog-raphy to measure cognitive load.
educational psy-chology review, 22(4):425–438..3766dominic balasuriya, nicky ringland, joel nothman,tara murphy, and james r. curran.
2009. namedin proceedingsentity recognition in wikipedia.
of the 1st 2009 workshop on the people’s webmeets nlp: collaboratively constructed semanticresources@ijcnlp 2009, suntec, singapore, au-gust 7, 2009, pages 10–18.
association for compu-tational linguistics..manuel j. a. eugster, tuukka ruotsalo, michielm. a. spap´e, ilkka kosunen, oswald barral, niklasravaja, giulio jacucci, and samuel kaski.
2014.predicting term-relevance from brain signals.
in the37th international acm sigir conference on re-search and development in information retrieval,sigir ’14, gold coast , qld, australia - july 06- 11, 2014, pages 425–434.
acm..maria barrett, joachim bingel, nora hollenstein,marek rei, and anders søgaard.
2018. sequenceclassiﬁcation with human attention.
in proceedingsof the 22nd conference on computational naturallanguage learning, pages 302–312..marco r. furtner, john f. rauthmann, and pierresachse.
2009. nomen est omen: investigating thedominance of nouns in word comprehension witheye movement analyses.
advances in cognitive psy-chology, 5..maria barrett, joachim bingel, frank keller, and an-ders søgaard.
2016. weakly supervised part-of-speech tagging using eye-tracking data.
in proceed-ings of the 54th annual meeting of the associationfor computational linguistics (volume 2: short pa-pers), pages 579–584..maria barrett and anders søgaard.
2015. reading be-havior predicts syntactic categories.
in proceedingsof the 19th conference on computational naturallanguage learning, conll 2015, beijing, china,july 30-31, 2015, pages 345–349.
acl..pengfei cao, yubo chen, kang liu, jun zhao, andshengping liu.
2018. adversarial transfer learn-ing for chinese named entity recognition with self-in proceedings of the 2018attention mechanism.
conference on empirical methods in natural lan-guage processing, pages 182–192..xilun chen, yu sun, ben athiwaratkun, claire cardie,and kilian weinberger.
2018. adversarial deep av-eraging networks for cross-lingual sentiment classi-ﬁcation.
transactions of the association for compu-tational linguistics, 6:557–570..yun-nung chen, kai-min chang, and jack mostow.
2012. towards using eeg to improve asr accu-in human language technologies: confer-racy.
ence of the north american chapter of the asso-ciation of computational linguistics, proceedings,june 3-8, 2012, montr´eal, canada, pages 382–385.
the association for computational linguistics..jason pc chiu and eric nichols.
2016. named entityrecognition with bidirectional lstm-cnns.
trans-actions of the association for computational lin-guistics, 4:357–370..aron culotta, andrew mccallum, and jonathan betz.
2006.integrating probabilistic extraction modelsand data mining to discover relations and patterns intext.
in proceedings of the human language tech-nology conference of the naacl, main conference,pages 296–303..emily l denton, soumith chintala, rob fergus, et al.
2015. deep generative image models using a lapla-cian pyramid of adversarial networks.
advancesin neural information processing systems, 28:1486–1494..yaroslav ganin and victor s. lempitsky.
2015. unsu-pervised domain adaptation by backpropagation.
inproceedings of the 32nd international conferenceon machine learning, icml 2015, lille, france,6-11 july 2015, volume 37 of jmlr workshopand conference proceedings, pages 1180–1189.
jmlr.org..yaroslav ganin, evgeniya ustinova, hana ajakan,pascal germain, hugo larochelle, franc¸ois lavi-olette, mario marchand, and victor lempitsky.
2016. domain-adversarial training of neural net-works.
the journal of machine learning research,17(1):2096–2030..ian goodfellow, jean pouget-abadie, mehdi mirza,bing xu, david warde-farley, sherjil ozair, aaroncourville, and yoshua bengio.
2014. generative ad-versarial nets.
advances in neural information pro-cessing systems, 27:2672–2680..john m henderson and fernanda ferreira.
1993. eyemovement control during reading: fixation mea-sures reﬂect foveal but not parafoveal process-ing difﬁculty.
canadian journal of experimen-tal psychology/revue canadienne de psychologieexp´erimentale, 47(2):201..nora hollenstein, maria barrett, marius troen-dle, francesco bigiolli, nicolas langer,andce zhang.
2019a.
advancing nlp with cogni-arxiv preprinttive language processing signals.
arxiv:1904.02682..nora hollenstein, jonathan rotsztejn, marius troen-dle, andreas pedroni, ce zhang, and nicolas langer.
2018. zuco, a simultaneous eeg and eye-trackingscientiﬁcresource for natural sentence reading.
data, 5(1):1–13..nora hollenstein and ce zhang.
2019. entity recog-nition at ﬁrst sight: improving ner with eye move-ment information.
in proceedings of the 2019 con-ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies, naacl-hlt 2019, minneapo-lis, mn, usa, june 2-7, 2019, volume 1 (long andshort papers), pages 1–10.
association for compu-tational linguistics..3767joo-kyung kim, young-bum kim, ruhi sarikaya, anderic fosler-lussier.
2017. cross-lingual transferlearning for pos tagging without cross-lingual re-sources.
in proceedings of the 2017 conference onempirical methods in natural language processing,pages 2832–2838..john d. lafferty, andrew mccallum, and fernandoc. n. pereira.
2001. conditional random ﬁelds:probabilistic models for segmenting and labeling se-quence data.
in proceedings of the eighteenth inter-national conference on machine learning (icml2001), williams college, williamstown, ma, usa,june 28 - july 1, 2001, pages 282–289.
morgankaufmann..ying lin, shengqi yang, veselin stoyanov, and hengji.
2018. a multi-lingual multi-task architecture forlow-resource sequence labeling.
in proceedings ofthe 56th annual meeting of the association for com-putational linguistics, acl 2018, melbourne, aus-tralia, july 15-20, 2018, volume 1: long papers,pages 799–809.
association for computational lin-guistics..alessandro lopopolo, stefan l. frank, antal van denbosch, and roel willems.
2019. dependency pars-ing with your eyes: dependency structure predictsin proceedings ofeye regressions during reading.
the workshop on cognitive modeling and computa-tional linguistics..ling luo, zhihao yang, pei yang, yin zhang,lei wang, hongfei lin, and jian wang.
2018.an attention-based bilstm-crf approach todocument-level chemical named entity recognition.
bioinform., 34(8):1381–1388..minh-thang luong, quoc v. le,.
ilya sutskever,oriol vinyals, and lukasz kaiser.
2016. multi-in 4th inter-task sequence to sequence learning.
national conference on learning representations,iclr 2016, san juan, puerto rico, may 2-4, 2016,conference track proceedings..xuezhe ma and eduard h. hovy.
2016. end-to-endsequence labeling via bi-directional lstm-cnns-crf.
in proceedings of the 54th annual meeting ofthe association for computational linguistics, acl2016, august 7-12, 2016, berlin, germany, volume1: long papers.
the association for computer lin-guistics..laurens van der maaten and geoffrey hinton.
2008.visualizing data using t-sne.
journal of machinelearning research, 9(11)..sandeep mathias, diptesh kanojia, abhijit mishra,and pushpak bhattacharya.
2020. a survey on us-ing gaze behaviour for natural language processing.
in twenty-ninth international joint conference onartiﬁcial intelligence and seventeenth paciﬁc riminternational conference on artiﬁcial intelligenceijcai-pricai-20..abhijit mishra, diptesh kanojia, seema nagar, kuntaldey, and pushpak bhattacharyya.
2017. leverag-ing cognitive features for sentiment analysis.
arxivpreprint arxiv:1701.05581..lukas muttenthaler, nora hollenstein, and maria bar-rett.
2020. human brain activity for machine atten-tion.
corr, abs/2006.05113..jeffrey pennington, richard socher, and christopher d.manning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing, emnlp 2014, october 25-29, 2014,doha, qatar, a meeting of sigdat, a special inter-est group of the acl, pages 1532–1543.
acl..keith rayner.
1998. eye movements in reading andinformation processing: 20 years of research.
psy-chological bulletin, 124(3):372..omid rohanian, shiva taslimipoor, victoria yaneva,and le an ha.
2017. using gaze data to predict mul-in proceedings of the interna-tiword expressions.
tional conference recent advances in natural lan-guage processing, ranlp 2017, varna, bulgaria,september 2 - 8, 2017, pages 601–609.
incomaltd..cicero dos santos, ming tan, bing xiang, and bowenzhou.
2016. attentive pooling networks.
arxivpreprint arxiv:1602.03609..richard socher, alex perelygin, jean wu, jasonchuang, christopher d manning, andrew y ng,and christopher potts.
2013. recursive deep mod-els for semantic compositionality over a sentimenttreebank.
in proceedings of the 2013 conference onempirical methods in natural language processing,pages 1631–1642..ekta sood, simon tannert, diego frassinelli, andreasinterpret-bulling, and ngoc thang vu.
2020a.
ing attention models with human visual attentionin machine reading comprehension.
arxiv preprintarxiv:2010.06396..ekta sood, simon tannert, philipp m¨uller, and an-dreas bulling.
2020b.
improving natural languageprocessing tasks with human gaze-guided neural at-tention.
arxiv preprint arxiv:2010.07891..michalina strzyz, david vilares, and carlos g´omez-rodr´ıguez.
2019. towards making a dependencyparser see.
arxiv preprint arxiv:1909.01053..ece takmaz, sandro pezzelle, lisa beinborn, andraquel fern´andez.
2020. generating image descrip-tions via sequential cross-modal alignment guidedin proceedings of the 2020 con-by human gaze.
ference on empirical methods in natural languageprocessing, emnlp 2020, online, november 16-20,2020, pages 4664–4677.
association for computa-tional linguistics..3768chad c. williams, mitchel kappen, cameron d. has-sall, bruce wright, and olave e. krigolson.
2019.thinking theta and alpha: mechanisms of intuitiveand analytical reasoning.
neuroimage, 189:574–580..yaosheng yang, meishan zhang, wenliang chen, weizhang, haofen wang, and min zhang.
2018. ad-versarial learning for chinese ner from crowd an-in proceedings of the thirty-secondnotations.
aaai conference on artiﬁcial intelligence, (aaai-18), the 30th innovative applications of artiﬁcial in-telligence (iaai-18), and the 8th aaai symposiumon educational advances in artiﬁcial intelligence(eaai-18), new orleans, louisiana, usa, february2-7, 2018, pages 1627–1635.
aaai press..3769