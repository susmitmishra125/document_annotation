competence-based multimodal curriculum learning formedical report generation.
fenglin liu1, shen ge2, xian wu2∗1adsplab, school of ece, peking university2tencent medical ai lab, beijing, chinafenglinliu98@pku.edu.cn; {shenge, kevinxwu}@tencent.com.
abstract.
medical report generation task, which targetsto produce long and coherent descriptionsof medical images, has attracted growing re-search interests recently.
different from thegeneral image captioning tasks, medical reportgeneration is more challenging for data-drivenneural models.
this is mainly due to 1) the se-rious data bias and 2) the limited medical data.
to alleviate the data bias and make best useof available data, we propose a competence-based multimodal curriculum learning frame-work (cmcl).
speciﬁcally, cmcl simulatesthe learning process of radiologists and op-timizes the model in a step by step manner.
firstly, cmcl estimates the difﬁculty of eachtraining instance and evaluates the competenceof current model; secondly, cmcl selects themost suitable batch of training instances con-sidering current model competence.
by iter-ating above two steps, cmcl can graduallyimprove the model’s performance.
the ex-periments on the public iu-xray and mimic-cxr datasets show that cmcl can be incor-porated into existing models to improve theirperformance..figure 1: two examples of ground truth reports andreports generated by a state-of-the-art approach co-attention (jing et al., 2018) and our approach.
thered bounding boxes and red colored text indicatethe abnormalities in images and reports, respectively.
the blue colored text stands for the similar sentencesused to describe the normalities in ground truth reports.
there are notable visual and textual data biases andthe co-attention (jing et al., 2018) fails to depict therare but important abnormalities and generates some er-ror sentences (underlined text) and repeated sentences(italic text)..1.introduction.
medical images, e.g., radiology and pathology im-ages, and their corresponding reports, which de-scribe the observations in details of both normaland abnormal regions, are widely-used for diag-nosis and treatment (delrue et al., 2011; goergenet al., 2013).
in clinical practice, writing a medi-cal report can be time-consuming and tedious forexperienced radiologists, and error-prone for inex-perienced radiologists.
therefore, automaticallygenerating medical reports can assist radiologistsin clinical decision-making and emerge as a promi-nent attractive research direction in both artiﬁcial.
∗corresponding author..intelligence and clinical medicine (jing et al., 2018,2019; li et al., 2018, 2019; wang et al., 2018; xueet al., 2018; yuan et al., 2019; zhang et al., 2020a;chen et al., 2020; liu et al., 2021a,b, 2019c)..many existing medical report generation modelsadopt the standard image captioning approaches: acnn-based image encoder followed by a lstm-based report decoder, e.g., cnn-hlstm (jinget al., 2018; liang et al., 2017).
however, directlyapplying image captioning approaches to medicalimages has the following problems: 1) visual databias: the normal images dominate the dataset overthe abnormal ones (shin et al., 2016).
further-more, for each abnormal image, the normal regionsdominate the image over the abnormal ones.
as.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3001–3012august1–6,2021.©2021associationforcomputationallinguistics3001lungs are clear.
no pleural effusions or pneumothoraces.
heart and mediastinum of normal size and contour.
1scoliosis.no acute cardiopulmonary abnormality.no focal airspace consolidation.
clear lungs.
there is no pneumothorax or pleural effusion.
1scoliosis is present.
no acute bonyabnormalities.
no pneumothorax or pleural effusion.
the heart is normal in size.
the lungs are clear.
the hilar and mediastinal contours are normal.
no evidence of pneumothorax.
ground truth:co-attention:ours:the heart is enlarged.
there is no pneumothorax.no acute bony abnormality.
there is a moderate right pleural effusion with associated atelectasis.
the left lung is clear.
no pneumothorax is seen.the heart and mediastinum are normal.
the lungs are clear.
1there is mild blunting of the right costophrenic xxxx.
there is no infiltrate, mass or pneumothorax.
the right internal jugular catheter has been removed.1blunting of right costophrenic.
heart size is normal.no acute bony abnormality.
there is no pleural effusion.
no visualized pneumothorax.
the lungs are clear.ground truth:co-attention:ours:shown in figure 1, abnormal regions (red bound-ing boxes) only occupy a small part of the entireimage; 2) textual data bias: as shown in figure 1,in a medical report, radiologists tend to describeall the items in an image, making the descriptionsof normal regions dominate the entire report.
be-sides, many similar sentences are used to describethe same normal regions.
3) training efﬁciency:during training, most existing works treat all thesamples equally without considering their difﬁcul-ties.
as a result, the visual and textual biases couldmislead the model training (jing et al., 2019; xueet al., 2018; yuan et al., 2019; liu et al., 2021a,b;li et al., 2018).
as shown in figure 1, even a state-of-the-art model (jing et al., 2018) still generatessome repeated sentences of normalities and fails todepict the rare but important abnormalities..to this end, we propose a novel competence-based multimodal curriculum learning framework(cmcl) which progressively learns medical re-ports following an easy-to-hard fashion.
such astep by step process is similar to the learning curveof radiologists: (1) ﬁrst start from simple and easy-written reports; (2) and then attempt to consumeharder reports, which consist of rare and diverseabnormalities.
in order to model the above gradualworking patterns, cmcl ﬁrst assesses the difﬁcultyof each training instance from multiple perspec-tives (i.e., the visual complexity and textual com-plexity) and then automatically selects the mostrewarding training samples according to the cur-rent competence of the model.
in this way, once theeasy and simple samples are well-learned, cmclincreases the chance of learning difﬁcult and com-plex samples, preventing the models from gettingstuck in bad local optima1, which is obviously abetter solution than the common approaches ofuniformly sampling training examples from thelimited medical data.
as a result, cmcl could bet-ter utilize the limited medical data to alleviate thedata bias.
we evaluate the effectiveness of the pro-posed cmcl on two public datasets, i.e., iu-xray(demner-fushman et al., 2016) and mimic-cxr(johnson et al., 2019)..overall, the main contributions of this work are:.
• we introduce the curriculum learning in medi-cal report generation, which enables the mod-els to gradually proceed from easy samples to.
1current models tend to generate plausible general reportswith no prominent abnormal narratives (jing et al., 2019; liet al., 2018; yuan et al., 2019; liu et al., 2021a,b).
more complex ones in training, helping exist-ing models better utilize the limited medicaldata to alleviate the data bias..• we assess the difﬁculty of each training in-stance from multiple perspectives and proposea competence-based multimodal curriculumlearning framework (cmcl) to consider mul-tiple difﬁculties simultaneously..• we evaluate our proposed approach on twopublic datasets.
after equipping our proposedcmcl, which doesn’t introduce additionalparameters and only requires a small modiﬁ-cation to the training data pipelines, perfor-mances of the existing baseline models can beimproved on most metrics.
moreover, we con-duct human evaluations to measure the effec-tiveness in terms of its usefulness for clinicalpractice..2 related work.
the related works are introduced from: 1) imagecaptioning and paragraph generation; 2) medicalreport generation and 3) curriculum learning..image captioning and paragraph generationthe task of image captioning (chen et al., 2015;vinyals et al., 2015), which aims to generate a sen-tence to describe the given image, has receivedextensive research interests (anderson et al., 2018;rennie et al., 2017; liu et al., 2019a, 2020a).
theseapproaches mainly adoptthe encoder-decoderframework which translates the image to a sin-gle descriptive sentence.
such an encoder-decoderframework have achieved great success in advanc-ing the state-of-the-arts (vinyals et al., 2015; luet al., 2017; xu et al., 2015; liu et al., 2018, 2019b).
speciﬁcally,the encoder network (krizhevskyet al., 2012; he et al., 2016) computes visual rep-resentations for the visual contents and the de-coder network (hochreiter and schmidhuber, 1997;vaswani et al., 2017) generates a target sentencebased on the visual representations.
in contrastto the image captioning, image paragraph genera-tion, which aims to produce a long and semantic-coherent paragraph to describe the input image,has recently attracted growing research interests(krause et al., 2017; liang et al., 2017; yu et al.,2016).
to perform the image paragraph genera-tion, a hierarchical lstm (hlstm) (krause et al.,2017; liang et al., 2017) is proposed as the decoderto well generate long paragraphs..3002medical report generation the medical re-ports are expected to 1) cover contents of key med-ical ﬁndings such as heart size, lung opacity, andbone structure; 2) correctly capture any abnormal-ities and support with details such as the locationand shape of the abnormality; 3) correctly describepotential diseases such as effusion, pneumothoraxand consolidation (delrue et al., 2011; goergenet al., 2013; li et al., 2018; liu et al., 2021a,b).
therefore, correctly describing the abnormalitiesbecome the most urgent goal and the core valueof this task.
similar to image paragraph genera-tion, most existing medical report generation works(jing et al., 2018, 2019; li et al., 2018; wang et al.,2018; xue et al., 2018; yuan et al., 2019; zhanget al., 2020a,b; miura et al., 2021; lovelace andmortazavi, 2020; liu et al., 2021b, 2019c) attemptto adopt a cnn-hlstm based model to automat-ically generate a ﬂuent report.
however, due tothe data bias and the limited medical data, thesemodels are biased towards generating plausible butgeneral reports without prominent abnormal narra-tives (jing et al., 2019; li et al., 2018; yuan et al.,2019; liu et al., 2021a,b)..curriculum learning in recent years, curricu-lum learning (bengio et al., 2009), which enablesthe models to gradually proceed from easy samplesto more complex ones in training (elman, 1993),has received growing research interests in natu-ral language processing ﬁeld, e.g., neural machinetranslation (platanios et al., 2019; kumar et al.,2019; zhao et al., 2020; liu et al., 2020b; zhanget al., 2018; kocmi and bojar, 2017; xu et al.,2020) and computer vision ﬁeld, e.g., image clas-siﬁcation (weinshall et al., 2018), human attributeanalysis(wang et al., 2019) and visual question an-swering (li et al., 2020).
for example, in neuralmachine translation, platanios et al.
(2019) pro-posed to utilize the training samples in order ofeasy-to-hard and to describe the “difﬁculty” of atraining sample using the sentence length or therarity of the words appearing in it (zhao et al.,2020).
however, these methods (platanios et al.,2019; liu et al., 2020b; xu et al., 2020) are singledifﬁculty-based and unimodal curriculum learningapproaches.
it is obviously not applicable to med-ical report generation task, which involves multi-modal data, i.e., visual medical images and textualreports, resulting in multi-modal complexities, i.e.,the visual complexity and the textual complexity.
therefore, it is hard to design one single metric to.
estimate the overall difﬁculty of medical report gen-eration.
to this end, based on the work of platanioset al.
(2019), we propose a competence-based mul-timodal curriculum learning approach with multi-ple difﬁculty metrics..3 framework.
in this section, we brieﬂy describe typical medicalreport generation approaches and introduce the pro-posed competence-based multimodal curriculumlearning (cmcl)..as shown in the top of figure 2, many medi-cal report generation models adopt the encoder-decoder manner.
firstly, the visual features areextracted from the input medical image via a cnnmodel.
then the visual features are fed into a se-quence generation model, like lstm to producethe medical report.
in the training phase, all train-ing instances are randomly shufﬂed and groupedinto batches for training.
in other words, all train-ing instances are treated equally.
different fromtypical medical report generation models, cmclbuilds the training batch in a selective manner.
themiddle part of figure 2 displays the framework ofcmcl equipped with one single difﬁculty metric.
cmcl ﬁrst ranks all training instances accordingto this difﬁculty metric and then gradually enlargesthe range of training instances that the batch is se-lected.
in this manner, cmcl can train the modelsfrom easy to difﬁcult instances..since medical report generation involves multi-modal data, like visual medical images and textualreports, it is hard to design one single metric toestimate the overall difﬁculty.
therefore, we alsopropose a cmcl with multiple difﬁculty metrics.
as shown in the bottom of figure 2, the traininginstances are ranked by multiple metrics indepen-dently.
at each step, cmcl generates one batchfor each difﬁculty metric and then calculates theperplexity of each batch based on current model.
the batch with highest perplexity is selected totrain the model.
it can be understood that cmclsets multiple syllabus in parallel, and the model isoptimized towards the one with lowest competence..4 difﬁculty metrics.
in this section, we deﬁne the difﬁculty metrics usedby cmcl.
as stated in section 2, the key challengeof medical report generation is to accurately cap-ture and describe the abnormalities (delrue et al.,2011; goergen et al., 2013; li et al., 2018).
there-.
3003figure 2: the top illustrates the typical encoder-decoder approach; the middle illustrates the single difﬁculty-based curriculum learning, where only one difﬁculty metric is used; the bottom illustrates the multiple difﬁculty-based curriculum learning, where multiple difﬁculty metrics are introduced..fore, we assess the difﬁculty of instances based onthe difﬁculty of accurately capturing and describingthe abnormalities..as follows:.
4.1 visual difﬁculty.
we deﬁne both a heuristic metric and a model-based metric to estimate the visual difﬁculty..heuristic metric d1if a medical image containscomplex visual contents, it is more likely to containmore abnormalities, which increases the difﬁcultyto accurately capture them.
to measure such visualdifﬁculty, we adopt the widely-used resnet-50 (heet al., 2016) pre-trained on imagenet (deng et al.,2009) and ﬁne-tuned on chexpert dataset (irvinet al., 2019), which consists of 224,316 x-ray im-ages with each image labeled with occurrences of14 common radiographic observations.
speciﬁcally,we ﬁrst extract the normal image embeddings ofall normal training images from the last averagepooling layer of resnet-50.
then, given an in-put image, we again use the resnet-50 to obtainthe image embedding.
at last, the average cosinesimilarity between the input image and normal im-ages is adopted as the heuristic metric of visualdifﬁculty..model conﬁdence d2 we also introduce amodel-based metric.
we adopt the above resnet-50 to conduct the abnormality classiﬁcation task.
we ﬁrst adoptthe resnet-50 to acquire theclassiﬁcation probability distribution p (i) ={p1(i), p2(i), .
.
.
, p14(i)} among the 14 commondiseases for each image i in the training dataset,where pn(i) ∈ [0, 1].
then, we employ the entropyvalue h(i) of the probability distribution, deﬁned.
h(i) = −.
(pn(i) log (pn(i)) +.
(1).
14(cid:88).
n=1.
(1 − pn(i)) log (1 − pn(i))).
we employ the entropy value h(i) as the modelconﬁdence measure, indicating whether an imageis easy to be classiﬁed or not..4.2 textual difﬁculty.
we also deﬁne a heuristic metric and a model-basedmetric to estimate the textual difﬁculty..heuristic metric d3 a serious problem for med-ical report generation models is the tendency togenerate plausible general reports with no promi-nent abnormal narratives (jing et al., 2019; li et al.,2018; yuan et al., 2019).
the normal sentences areeasy to learn, but are less informative, while mostabnormal sentences, consisting of more rare anddiverse abnormalities, are relatively more difﬁcultto learn, especially at the initial learning stage.
tothis end, we adopt the number of abnormal sen-tences in a report to deﬁne the difﬁculty of a report.
following jing et al.
(2018), we consider sentenceswhich contain “no”, “normal”, “clear”, “stable” asnormal sentences, the rest sentences are consideras abnormal sentences..model conﬁdence d4 similar to visual difﬁ-culty, we further introduce a model conﬁdence as ametric.
to this end, we deﬁne the difﬁculty usingthe negative log-likelihood loss values (xu et al.,2020; zhang et al., 2018) of training samples.
toacquire the negative log-likelihood loss values, we.
3004random shufflebatch sampled uniformlydatasetranking by single difficulty metricsbatch sampled by model competencedatasetranking by difficulty metrics 1ranking by difficulty metrics 2ranking by difficulty metrics 3ranking by difficulty metrics nbatch sampled by model perplexity and competence……datasetencoderdecodermodelencoderdecodermodelencoderdecodermodelperplexity updatemultiple difficulty-based curriculum learningsingle difficulty-based curriculum learningbaselinealgorithm 1 single difﬁculty-based curriculum learning(platanios et al., 2019)..input: the training set dtrain.
output: a model with single difﬁculty-based curriculum.
learning..1: compute difﬁculty d for each training sample in dtrain;2: sort dtrain based d to acquire dtrain3: at t = 0, initialize the model competence c(0) by eq.
(2);uniformly sample a data batch, b(0), from the top c(0)portions of dtrain.
;.
;.
1.
1.
4: repeat5:6:7:.
train the model with the b(t);t ← t + 1;estimate the model competence, c(t), by eq.
(2);uniformly sample a data batch, b(t), from the top c(t)portions of dtrain.
;.
1.
8: until model converge..adopt the widely-used and classic cnn-hlstm(jing et al., 2018), in which the cnn is imple-mented with resnet-50, trained on the downstreamdataset used for evaluation with a cross-entropyloss..it is worth noticing that since we focus on themedical report generation and design the metricsbased on the difﬁculty of accurately capturing anddescribing the abnormalities, we do not considersome language difﬁculty metrics used in neural ma-chine translation, e.g., the sentence length (platan-ios et al., 2019), the n-gram rarity together withnamed entity recognition (ner) and parts ofspeech (pos) taggings (zhao et al., 2020)..5 approach.
in this section, we ﬁrst brieﬂy introduce the con-ventional single difﬁculty-based curriculum (pla-tanios et al., 2019).
then we propose the multipledifﬁculty-based curriculum learning for medicalreport generation..5.1 single difﬁculty-based curriculum.
learning.
platanios et al.
(2019) proposed a competence-based and single difﬁculty-based curriculum learn-ing framework (see algorithm 1), which ﬁrst sortseach instance in the training dataset dtrain accord-ing to a single difﬁculty metric d, and then deﬁnesthe model competence c(t) ∈ (0, 1] at training stept by following functional forms:.
c(t) = min.
(cid:32).
(cid:114)1, pt.1 − c(0)pt.(cid:33).
+ c(0)p.(2).
algorithm 2 multiple difﬁculty-based curriculum learn-ing.
the red colored text denotes the differences from algo-rithm 1.input: the training set dtrain, i ∈ {1, 2, 3, 4}.
output: a model with multiple difﬁculty-based curriculum.
learning..dtrain;.
1: compute four difﬁculties, di, for each training sample in.
2: sort dtrain based each difﬁculty of every sample, resulting.
in dtraini.
(i.e., dtrain3: for i = 1, 2, 3, 4 do4:.
1., dtrain2., dtrain3., dtrain4.
);.
ti = 0; initialize the model competence from ithperspective, ci(0), by eq.
(2); uniformly sample a databatch, bi(0), from the top ci(0) portions of dtraincompute the perplexity (ppl) on bi(0), ppl(bi(0));.
;.
i.j = arg max.
(ppl(bi(ti)));.
i.
9:10:11:.
train the model with the bj(tj);tj ← tj + 1;estimate the model competence from jth perspective,cj(tj), by eq.
(2); uniformly sample a data batch,bj(tj), from the top cj(tj) portions of dtraincompute the perplexity (ppl) of model on bj(tj),ppl(bj(tj));13: until model converge..12:.
;.
j.
5:6: end for7: repeat8:.
the duration of curriculum learning and determinesthe length of the curriculum.
in implementations,at training time step t, the top c(t) portions of thesorted training dataset are selected to sample a train-ing batch to train the model.
in this way, the modelis able to gradually proceed from easy samples tomore complex ones in training, resulting in ﬁrststarting to utilize the simple and easy-written re-ports for training, and then attempting to utilizeharder reports for training..5.2 multiple difﬁculty-based curriculum.
learning.
the training instances of medical report generationtask are pairs of medical images and correspondingreports which is a multi-modal data.
it’s hard toestimate the difﬁculty with only one metric.
in ad-dition, the experimental results (see table 4) showthat directly fusing multiple difﬁculty metrics asone (d1 + d2 + d3 + d4) is obviously inappropriate,which is also veriﬁed in platanios et al.
(2019).
tothis end, we extend the single difﬁculty-based cur-riculum learning into the multiple difﬁculty-basedcurriculum learning, where we provide the medicalreport generation models with four different difﬁ-culty metrics, i.e., d1, d2, d3, d4 (see section 4)..where c(0) is the initial competence and usuallyset to 0.01, p is the coefﬁcient to control the cur-riculum schedule and is usually set to 2, and t is.
a simple and natural way is to randomly or se-quentially choose a curricula to train the model,i.e., 1→2→3→4→1.
however, a better approach.
3005is to adaptively select the most appropriate curric-ula for each training step, which follows the com-mon practice of human learning behavior: whenwe have learned some curricula well, we tend tochoose the under-learned curricula to learn.
algo-rithm 2 summarizes the overall learning processof the proposed framework and figure 3 illustratesthe process of algorithm 2. in implementations,similarly, we ﬁrst sort the training dataset based onthe four difﬁculty metrics and acquire four sortedtraining datasets in line 1-2. then, based on themodel competence, we acquire the training samplesfor each curricula, in line 4. in line 5, we furtherestimate the perplexity (ppl) of model on differenttraining samples bi(ti) corresponding to differentcurricula, deﬁned as:.
ppl(bi(ti)) =.
(cid:88).
rk∈bi(ti).
(cid:118)(cid:117)(cid:117)n(cid:116).
n(cid:89).
m=1.
1.p (wk.
m|wk.
1 , .
.
.
, wk.
m−1).
1 , wk.
where rk = {wk2 , .
.
.
, wkn } denotes the k-threport in bi(ti).
the perplexity (ppl) measureshow many bits on average would be needed to en-code each word of the report given the model, sothe current curricula with higher ppl means thatthe model is not well-learned for this curricula andneed to be improved.
therefore, the ppl can beused to determine the curricula at each trainingstep dynamically.
speciﬁcally, in line 8-9, we se-lect the under-learned curricula, i.e., the curriculawith maximum ppl, to train the current model.
af-ter that, we again estimate the model competencein the selected curricula in line 11 and compute theppl of model on the training samples correspond-ing to the selected curricula in line 12..6 experiment.
we ﬁrstly describe two public datasets as well asthe widely-used metrics, baselines and settings.
then we present the evaluation of our cmcl..6.1 datasets.
we conduct experiments on two public datasets,i.e., a widely-used benchmark iu-xray (demner-fushman et al., 2016) and a recently released large-scale mimic-cxr (johnson et al., 2019)..• iu-xray2 is collected by indiana universityand is widely-used to evaluate the perfor-mance of medical report generation methods..2https://openi.nlm.nih.gov/.
mimic-cxr/2.0.0/.
figure 3: illustration of algorithm 2..it contains 7,470 chest x-ray images asso-ciated with 3,955 radiology reports sourcedfrom indiana network for patient care..• mimic-cxr3.
is the recently releasedlargest dataset to date and consists of 377,110chest x-ray images and 227,835 radiology re-ports from 64,588 patients of the beth israeldeaconess medical center..for iu-xray dataset, following previous works(chen et al., 2020; jing et al., 2019; li et al., 2019,2018), we randomly split the dataset into 70%-10%-20% training-validation-testing splits.
at last,we preprocess the reports by tokenizing, convert-ing to lower-cases and removing non-alpha tokens.
for mimic-cxr, following chen et al.
(2020);liu et al.
(2021a,b), we use the ofﬁcial splits toreport our results, resulting in 368,960 samples inthe training set, 2,991 samples in the validation setand 5,159 samples in the test set.
we convert all to-kens of reports to lower-cases and ﬁlter tokens thatoccur less than 10 times in the corpus, resulting ina vocabulary of around 4,000 tokens..6.2 baselines.
we tested three representative baselines that wereoriginally designed for image captioning and three.
3https://physionet.org/content/.
3006𝐷1train𝐷2train𝐷3train𝐷4train𝐵1(𝑡1)𝐵2(𝑡2)𝐵3(𝑡3)𝐵4(𝑡4)𝑗=argmax𝑖𝑃𝑃𝐿(𝐵𝑖(𝑡𝑖))𝑃𝑃𝐿(𝐵1(𝑡1))𝑃𝑃𝐿(𝐵2(𝑡2))𝑃𝑃𝐿(𝐵3(𝑡3))𝑃𝑃𝐿(𝐵4(𝑡4))for example, j=3𝑡3←𝑡3+1updateperplexity𝐷1train𝐷2train𝐷3train𝐷4trainupdatecompetenceperplexitytrain model with 𝐵3(𝑡3)competitive baselines that were originally designedfor medical report generation..6.2.1.image captioning baselines.
• nic: vinyals et al.
(2015) proposed theencoder-decoder network, which employs acnn-based encoder to extract image featuresand a rnn-based decoder to generate the tar-get sentence, for image captioning..• spatial-attention: lu et al.
(2017) proposedthe visual attention, which is calculated on thehidden states, to help the model to focus onthe most relevant image regions instead of thewhole image..• adaptive-attention: considering that thedecoder tends to require little or no visualinformation from the image to predict the non-visual words such as “the” and “of”, lu et al.
(2017) designed an adaptive attention modelto decide when to employ the visual attention..6.2.2 medical report generation baselines.
• cnn-hlstm: jing et al.
(2018) introducedthe hierarchical lstm structure (hlstm),which contains the paragraph lstm and thesentence lstm.
hlstm ﬁrst uses the para-graph lstm to generate a series of high-leveltopic vectors representing the sentences, andthen utilizes the sentence lstm to generate asentence based on each topic vector..• hlstm+att+dual: harzig et al.
(2019) pro-posed a hierarchical lstm with the atten-tion mechanism and further introduced twolstms, i.e., normal lstm and abnormallstm, to help the model to generate moreaccurate normal and abnormal sentences..• co-attention:.
jing et al.
(2018) proposedthe co-attention model, which combines themerits of visual attention and semantic atten-tion, to attend to both images and predictedsemantic tags4 simultaneously, exploring thesynergistic effects of visual and semantic in-formation..6.3 metrics and settings.
we adopt the widely-used bleu (papineni et al.,2002), meteor (banerjee and lavie, 2005) androuge-l (lin, 2004), which are reported by the.
evaluation toolkit (chen et al., 2015)5, to test theperformance.
speciﬁcally, rouge-l is proposedfor automatic evaluation of the extracted text sum-marization.
meteor and bleu are originallydesigned for machine translation evaluation..for all baselines, since our focus is to change thetraining paradigm, which improves existing base-lines by efﬁciently utilizing the limited medicaldata, we keep the inner structure of the baselinesuntouched and preserve the original parameter set-ting.
for our curriculum learning framework, fol-lowing previous work (platanios et al., 2019), thec(0) and p are set to 0.01 and 2, respectively.
fordifferent baselines, we ﬁrst re-implement the base-lines without using any curriculum.
when equip-ping baselines with curriculum, following platan-ios et al.
(2019), we set t in eq.
(2) to a quarterof the number of training steps that the baselinemodel takes to reach approximately 90% of its ﬁ-nal bleu-4 score.
to boost the performance, wefurther incorporate the batching method (xu et al.,2020), which batches the samples with similar dif-ﬁculty in the curriculum learning framework.
tore-implement the baselines and our approach, fol-lowing common practice (jing et al., 2019; li et al.,2019, 2018; liu et al., 2021a,b), we extract imagefeatures for both dataset used for evaluation froma resnet-50 (he et al., 2016), which is pretrainedon imagenet (deng et al., 2009) and ﬁne-tunedon public available chexpert dataset (irvin et al.,2019).
to ensure consistency with the experimentsettings of previous works (chen et al., 2020), foriu-xray, we utilize paired images of a patient asthe input; for mimic-cxr, we use single imageas the input.
for parameter optimization, we useadam optimizer (kingma and ba, 2014) with abatch size of 16 and a learning rate of 1e-4..6.4 automatic evaluation.
as shown in table 1, for two datasets, all baselinesequipped with our approach receive performancegains over most metrics.
the results prove theeffectiveness and the compatibility of our cmclin promoting the performance of existing modelsby better utilizing the limited medical data.
be-sides, in table 2, we further select six existingstate-of-the-art models, i.e., hrgr-agent (li et al.,2018), cmas-rl (jing et al., 2019), sentsat +kg (zhang et al., 2020a), up-down (andersonet al., 2018), transformer (chen et al., 2020) and.
4https://ii.nlm.nih.gov/mti/.
5https://github.com/tylin/coco-caption.
3007dataset: mimic-cxr (johnson et al., 2019).
b-1.
b-2.
b-3.
b-4.
m.dataset: iu-xray (demner-fushman et al., 2016)r-lb-1.
b-2.
b-3.
b-4.
m.methods.
nic (vinyals et al., 2015)†w/ cmclspatial-attention (lu et al., 2017)†w/ cmcladaptive-attention (lu et al., 2017)†w/ cmclcnn-hlstm (krause et al., 2017)†w/ cmclhlstm+att+dual (harzig et al., 2019)†w/ cmclco-attention (jing et al., 2018)†w/ cmcl.
0.2900.301.
0.3020.312.
0.3070.302.
0.3210.337.
0.3280.330.
0.3290.344.
0.1820.189.
0.1890.200.
0.1920.192.
0.2030.210.
0.2040.206.
0.2060.217.
0.1190.123.
0.1220.125.
0.1240.129.
0.1290.136.
0.1270.133.
0.1330.140.
0.0810.085.
0.0820.087.
0.0840.091.
0.0920.097.
0.0900.088.
0.0950.097.
0.1120.119.
0.1200.118.
0.1190.125.
0.1250.131.
0.1220.119.
0.1290.133.r-l.0.2490.241.
0.2590.258.
0.2620.264.
0.2700.274.
0.2670.272.
0.2730.281.
0.3520.358.
0.3740.381.
0.4330.437.
0.4350.462.
0.4470.461.
0.4630.473.
0.2270.223.
0.2350.246.
0.2850.281.
0.2800.293.
0.2890.298.
0.2930.305.
0.1540.160.
0.1580.164.
0.1940.196.
0.1870.207.
0.1920.201.
0.2070.217.
0.1090.114.
0.1200.123.
0.1370.140.
0.1310.155.
0.1440.150.
0.1550.162.
0.1330.137.
0.1460.153.
0.1660.174.
0.1730.179.
0.1750.173.
0.1780.186.
0.3130.317.
0.3220.327.
0.3490.338.
0.3460.360.
0.3580.359.
0.3650.378.table 1: performance of automatic evaluations on the test sets of the mimic-cxr and the iu-xray datasets.
cmcl denotes the competence-based multimodal curriculum learning framework.
b-n, m and r-l are short forbleu-n, meteor and rouge-l, respectively.
higher is better in all columns.
† denotes our re-implementation.
as we can see, all baseline models enjoy comfortable improvements in most metrics with our cmcl..methods.
hrgr-agent (li et al., 2018)cmas-rl (jing et al., 2019)sentsat + kg (zhang et al., 2020a)up-down (anderson et al., 2018)transformer (chen et al., 2020)r2gen (chen et al., 2020).
cmcl (ours).
dataset: mimic-cxr (johnson et al., 2019).
b-1.
---0.3170.3140.353.
0.344.b-2.
---0.1950.1920.218.
0.217.b-3.
---0.1300.1270.145.
0.140.b-4.
---0.0920.0900.103.
0.097.m.---0.1280.1250.142.
0.133.r-l.---0.2670.2650.277.
0.281.dataset: iu-xray (demner-fushman et al., 2016)r-lb-1.
b-3.
b-4.
b-2.
m.0.4380.4640.441-0.3960.470.
0.473.
0.2980.3010.291-0.2540.304.
0.305.
0.2080.2100.203-0.1790.219.
0.217.
0.1510.1540.147-0.1350.165.
0.162.
----0.1640.187.
0.186.
0.3220.3620.367-0.3420.371.
0.378.table 2: comparison with existing state-of-the-art methods on the test set of the mimic-cxr dataset and theiu-x-ray dataset.
cmcl is taken from the “co-attention w/ cmcl” in table 1. in this table, the red and bluecolored numbers denote the best and second best results across all approaches, respectively..vs. modelscnn-hlstm (jing et al., 2018)†co-attention (jing et al., 2018)†.
baseline wins tie.
‘w/ cmcl’ wins.
1524.
2835.
5741.table 3: we invite 2 professional clinicians to conductthe human evaluation for comparing our method withbaselines.
all values are reported in percentage (%)..r2gen (chen et al., 2020), for comparison.
forthese selected models, we directly quote the re-sults from the original paper for iu-xray, and fromchen et al.
(2020) for mimic-cxr.
as we cansee, based on the co-attention (chen et al., 2020),our approach cmcl achieves competitive resultswith these state-of-the-art models on major metrics,which further demonstrate the effectiveness of theproposed approach..6.5 human evaluation.
in this section, to verify the effectiveness of ourapproach in clinical practice, we invite two profes-sional clinicians to evaluate the perceptual qual-ity of 100 randomly selected reports generatedby “baselines” and “baselines w/ cmcl”.
forthe baselines, we choose a representative model:cnn-hlstm and a state-of-the-art model: co-attention.
the clinicians are unaware of which.
model generates these reports.
in particular, tohave more documents examined, we did not usethe same documents for both clinicians and checkthe agreements between them.
that is to say, thedocuments for different clinicians do not overlap.
the results in table 3 show that our approach isbetter than baselines in clinical practice with win-ning pick-up percentages.
in particular, all invitedprofessional clinicians found that our approach cangenerate ﬂuent reports with more accurate descrip-tions of abnormalities than baselines.
it indicatesthat our approach can help baselines to efﬁcientlyalleviate the data bias problem, which also can beveriﬁed in section 6.7..6.6 quantitative analysis.
analysis on the difﬁculty metricsin this sec-tion, we conduct an ablation study by only usinga single difﬁculty metric during the curriculumlearning, i.e., single difﬁculty-based curriculumlearning, to investigate the contribution of eachdifﬁculty metric in our framework and the resultsare shown in table 4. settings (a-d) show thatevery difﬁculty metric can boost the performanceof baselines, which verify the effectiveness of ourdesigned difﬁculty metrics.
in particular, 1) the.
3008visual difﬁculty.
heuristicmetric.
modelconﬁdence.
textual difﬁcultymodelconﬁdence.
heuristicmetric.
routestrategy.
settings.
baseline.
(a)(b)(c)(d).
(e)(f)(g).
(h)(i)(j).
-√.
---√√√.
√√√.
-.
-√.
--√√√.
√√√.
-.
--√.
-.
-√√.
√√√.
dataset: iu-xray (demner-fushman et al., 2016)baseline: cnn-hlstm (jing et al., 2018).
b-1.
0.435.
0.4380.4470.4430.454.
0.4500.4550.462.
0.4400.4570.459.b-2.
0.280.
0.2830.2880.2870.290.
0.2890.2900.293.
0.2820.2910.290.b-3.
0.187.
0.1880.1950.1920.201.
0.1960.1990.207.
0.1900.1990.203.b-4.
0.131.
0.1320.1430.1350.148.
0.1440.1450.155.
0.1340.1460.150.m.0.173.
0.1730.1750.1750.177.
0.1760.1760.179.
0.1740.1780.176.r-l.0.346.
0.3480.3540.3510.357.
0.3550.3570.360.
0.3490.3580.354.
-.
---√.
--√.
√√√.
-.
----.
dynamicallydynamicallydynamically.
fuserandomlysequentially.
table 4: quantitative analysis of our approach, which includes four designed difﬁculty metrics (see section 4) andthe route strategy (see section 5.2).
we conduct the analysis on the widely-used baseline model cnn-hlstm(jing et al., 2018).
the setting (g) also denotes our full proposed approach..model conﬁdence in both visual and textual difﬁ-culties achieves better performance than the heuris-tic metrics.
it shows that the model conﬁdenceis the more critical in neural models.
2) both themodel conﬁdence and heuristic metrics in the tex-tual difﬁculty achieve better performance than theircounterparts in the visual difﬁculty, which indi-cates that the textual data bias is the more criticalin textual report generation task.
when progres-sively incorporate each difﬁculty metric, the per-formance will increase continuously (see settings(e-g)), showing that integrating different difﬁcultymetrics can bring the improvements from differentaspects, and the advantages of all difﬁculty metricscan be united as an overall improvement..analysis on the route strategy as stated insection 5.2, to implement the multiple difﬁculty-based curriculum learning, three simple and nat-ural ways is to: 1) fuse multiple difﬁculty met-rics directly as a single mixed difﬁculty metric,d1 + d2 + d3 + d4; 2) randomly choose a cur-ricula and 3) sequentially choose a curricula (i.e.,1→2→3→4→1) to train the model.
table 4 (h-j)show the results of the three implementations.
aswe can see, all route strategies are viable in prac-tice with improved performance of medical reportgeneration, which proves the effectiveness and ro-bustness of our cmcl framework.
besides, allof them perform worse than our approach (setting(g)), which conﬁrms the effectiveness of dynami-cally learning strategy at each training step..6.7 qualitative analysis.
in figure 1, we give two intuitive examples to bet-ter understand our approach.
as we can see, ourapproach generates structured and robust reports,which show signiﬁcant alignment with ground truth.
reports and are supported by accurate abnormaldescriptions.
for example, the generated reportcorrectly describes “blunting of right costophrenic”in the ﬁrst example and “scoliosis is present” in thesecond example.
the results prove our argumentsand verify the effectiveness of our proposed cmclin alleviating the data bias problem by enablingthe model to gradually proceed from easy to morecomplex instances in training..7 conclusion.
in this paper, we propose the novel competence-based multimodal curriculum learning framework(cmcl) to alleviate the data bias by efﬁciently uti-lizing the limited medical data for medical reportgeneration.
to this end, considering the difﬁcultyof accurately capturing and describing the abnor-malities, we ﬁrst assess four sample difﬁcultiesof training data from the visual complexity andthe textual complexity, resulting in four differentcurricula.
next, cmcl enables the model to betrained with the appropriate curricula and graduallyproceed from easy samples to more complex onesin training.
experimental results demonstrate theeffectiveness and the generalization capabilities ofcmcl, which consistently boosts the performanceof the baselines under most metrics..acknowledgments.
this work is partly supported by tencent medicalai lab, beijing, china.
we would like to sincerelythank the clinicians xiaoxia xie and jing zhang ofthe harbin chest hospital in china for providingthe human evaluation.
we sincerely thank all theanonymous reviewers for their constructive com-ments and suggestions that substantially improvedthis paper..3009ethical considerations.
in this work, we focus on helping a wide range ofexisting medical report generation systems allevi-ate the data bias by efﬁciently utilizing the limitedmedical data for medical report generation.
ourwork can enable the existing systems to graduallyproceed from easy samples to more complex onesin training, which is similar to the learning curveof radiologist: (1) ﬁrst start from simple and easy-written reports; (2) and then attempt to consumeharder reports, which consist of rare and diverseabnormalities.
as a result, our work can promotethe usefulness of existing medical report generationsystems in better assisting radiologists in clinicaldecision-makings and reducing their workload.
inparticular, for radiologists, given a large amountof medical images, the systems can automaticallygenerate medical reports, the radiologists only needto make revisions rather than write a new reportfrom scratch.
we conduct the experiments on thepublic mimic-cxr and iu-xray datasets.
allprotected health information was de-identiﬁed.
de-identiﬁcation was performed in compliance withhealth insurance portability and accountabilityact (hipaa) standards in order to facilitate publicaccess to the datasets.
deletion of protected healthinformation (phi) from structured data sources(e.g., database ﬁelds that provide patient name ordate of birth) was straightforward.
all necessary pa-tient/participant consent has been obtained and theappropriate institutional forms have been archived..references.
peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attention forimage captioning and vqa.
in cvpr..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in iee-valuation@acl..yoshua bengio, j´erˆome louradour, ronan collobert,and jason weston.
2009. curriculum learning.
inicml..xinlei chen, hao fang, tsung-yi lin, ramakr-ishna vedantam, saurabh gupta, piotr doll´ar, andc. lawrence zitnick.
2015. microsoft coco cap-tions: data collection and evaluation server.
arxivpreprint arxiv:1504.00325..zhihong chen, yan song, tsung-hui chang, and xi-.
ang wan.
2020. generating radiology reports viamemory-driven transformer.
in emnlp..louke delrue,.
bart.
ilsen,robert gosselin,an van landeghem, johan de mey, and philippeduyck.
2011. difﬁculties in the interpretation ofchest radiography.
comparative interpretation ofct and standard radiography of the chest, pages27–49..dina demner-fushman, marc d. kohli, marc b.rosenman, sonya e. shooshan, laritza rodriguez,sameer k. antani, george r. thoma, and clement j.mcdonald.
2016. preparing a collection of radiol-ogy examinations for distribution and retrieval.
j.am.
medical informatics assoc., 23(2):304–310..jia deng, wei dong, richard socher, li-jia li, kaili, and fei-fei li.
2009. imagenet: a large-scalehierarchical image database.
in cvpr..jeffrey l elman.
1993. learning and development inneural networks: the importance of starting small.
cognition, 48(1):71–99..stacy k goergen, felicity j pool, tari j turner,jane e grimm, mark n appleyard, carmel crock,michael c fahey, michael f fay, nicholas j fer-ris, susan m liew, et al.
2013. evidence-basedguideline for the written radiology report: methods,recommendations and implementation challenges.
journal of medical imaging and radiation oncology,57(1):1–7..philipp harzig, yan-ying chen, francine chen, andrainer lienhart.
2019. addressing data bias prob-lems for chest x-ray image report generation.
inbmvc..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for image recog-nition.
in cvpr..sepp hochreiter and j¨urgen schmidhuber.
1997. longshort-term memory.
neural comput., 9(8):1735–1780..jeremy irvin, pranav rajpurkar, michael ko, yi-fan yu, silviana ciurea-ilcus, chris chute, hen-rik marklund, behzad haghgoo, robyn l. ball,katie s. shpanskaya, jayne seekins, david a.mong, safwan s. halabi,jesse k. sandberg,ricky jones, david b. larson, curtis p. langlotz,bhavik n. patel, matthew p. lungren, and andrew y.ng.
2019. chexpert: a large chest radiographdataset with uncertainty labels and expert compari-son.
in aaai..baoyu jing, zeya wang, and eric p. xing.
2019. show,describe and conclude: on exploiting the structureinformation of chest x-ray reports.
in acl..baoyu jing, pengtao xie, and eric p. xing.
2018. onthe automatic generation of medical imaging reports.
in acl..3010alistair e. w. johnson, tom j. pollard, seth j.berkowitz, nathaniel r. greenbaum, matthew p.lungren, chih-ying deng, roger g. mark, andsteven horng.
2019. mimic-cxr: a large pub-licly available database of labeled chest radiographs.
arxiv preprint arxiv:1901.07042..diederik p. kingma and jimmy ba.
2014. adam: a.method for stochastic optimization.
in iclr..tom kocmi and ondrej bojar.
2017. curriculum learn-ing and minibatch bucketing in neural machine trans-lation.
in ranlp..jonathan krause, justin johnson, ranjay krishna, andli fei-fei.
2017. a hierarchical approach for gener-ating descriptive image paragraphs.
in cvpr..alex krizhevsky, ilya sutskever, and geoffrey e. hin-ton.
2012. imagenet classiﬁcation with deep convo-lutional neural networks.
in nips..gaurav kumar, george f. foster, colin cherry, andmaxim krikun.
2019.learningbased curriculum optimization for neural machinetranslation.
in naacl-hlt..reinforcement.
christy y. li, xiaodan liang, zhiting hu, and eric p.xing.
2019. knowledge-driven encode, retrieve,paraphrase for medical image report generation.
inaaai..qing li, siyuan huang, yining hong, and song-chunzhu.
2020. a competence-aware curriculum for vi-insual concepts learning via question answering.
eccv..yuan li, xiaodan liang, zhiting hu, and eric p. xing.
2018. hybrid retrieval-generation reinforced agentfor medical image report generation.
in neurips..xiaodan liang, zhiting hu, hao zhang, chuang gan,and eric p. xing.
2017. recurrent topic-transitiongan for visual paragraph generation.
in iccv..chin-yew lin.
2004. rouge: a package for auto-.
matic evaluation of summaries.
in acl..fenglin liu, yuanxin liu, xuancheng ren, xiaodonghe, and xu sun.
2019a.
aligning visual regions andtextual concepts for semantic-grounded image repre-sentations.
in neurips..fenglin liu, xuancheng ren, yuanxin liu, kai lei,and xu sun.
2019b.
exploring and distilling cross-modal information for image captioning.
in ijcai..fenglin liu, xuancheng ren, yuanxin liu, houfengwang, and xu sun.
2018. simnet: stepwise image-topic merging network for generating detailed andcomprehensive image captions.
in emnlp..fenglin liu, xian wu, shen ge, wei fan, and yuexianzou.
2021a.
exploring and distilling posterior andprior knowledge for radiology report generation.
incvpr..fenglin liu, changchang yin, xian wu, shen ge, pingzhang, and xu sun.
2021b.
contrastive attentionfor automatic chest x-ray report generation.
in acl(findings)..guanxiong liu, tzu-ming harry hsu, matthew b. a.mcdermott, willie boag, wei-hung weng, peterszolovits, and marzyeh ghassemi.
2019c.
clin-ically accurate chest x-ray report generation.
inmlhc..xuebo liu, houtim lai, derek f. wong, and lidia s.chao.
2020b.
norm-based curriculum learning forneural machine translation.
in acl..justin r. lovelace and bobak mortazavi.
2020. learn-ing to generate clinically coherent chest x-ray re-ports.
in emnlp (findings)..jiasen lu, caiming xiong, devi parikh, and richardsocher.
2017. knowing when to look: adaptive at-tention via a visual sentinel for image captioning.
incvpr..yasuhide miura, yuhao zhang, emily bao tsai, cur-tis p. langlotz, and dan jurafsky.
2021. improvingfactual completeness and consistency of image-to-text radiology report generation.
in naacl-hlt..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl..emmanouil antonios platanios, otilia stretcu, grahamneubig, barnab´as p´oczos, and tom m. mitchell.
2019. competence-based curriculum learning forneural machine translation.
in naacl-hlt..steven j. rennie, etienne marcheret, youssef mroueh,jarret ross, and vaibhava goel.
2017. self-criticalsequence training for image captioning.
in cvpr..hoo-chang shin, kirk roberts, le lu, dina demner-fushman, jianhua yao, and ronald m. summers.
2016. learning to read chest x-rays: recurrent neu-ral cascade model for automated image annotation.
in cvpr..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in cvpr..fenglin liu, xuancheng ren, xian wu, shen ge, weifan, yuexian zou, and xu sun.
2020a.
prophet at-tention: predicting attention with future attention.
in neurips..xiaosong wang, yifan peng, le lu, zhiyong lu, andronald m. summers.
2018. tienet: text-image em-bedding network for common thorax disease classi-ﬁcation and reporting in chest x-rays.
in cvpr..3011yiru wang, weihao gan, jie yang, wei wu, and junjieyan.
2019. dynamic curriculum learning for imbal-anced data classiﬁcation.
in iccv..daphna weinshall, gad cohen, and dan amir.
2018.curriculum learning by transfer learning: theoryand experiments with deep networks.
in icml..chen xu, bojie hu, yufan jiang, kai feng, zeyangwang, shen huang, qi ju, tong xiao, and jingbozhu.
2020. dynamic curriculum learning for low-resource neural machine translation.
in coling..kelvin xu,.
jimmy ba, ryan kiros, kyunghyuncho, aaron c. courville, ruslan salakhutdinov,richard s. zemel, and yoshua bengio.
2015. show,attend and tell: neural image caption generationwith visual attention.
in icml..yuan xue, tao xu, l. rodney long, zhiyun xue,sameer k. antani, george r. thoma, and xiaoleihuang.
2018. multimodal recurrent model with at-tention for automated radiology report generation.
in miccai..haonan yu, jiang wang, zhiheng huang, yi yang, andwei xu.
2016. video paragraph captioning usinghierarchical recurrent neural networks.
in cvpr..jianbo yuan, haofu liao, rui luo, and jiebo luo.
2019. automatic radiology report generation basedon multi-view image fusion and medical concept en-richment.
in miccai..xuan zhang, gaurav kumar, huda khayrallah, kentonmurray, jeremy gwinnup, marianna j. martindale,paul mcnamee, kevin duh, and marine carpuat.
2018. an empirical exploration of curriculum learn-ing for neural machine translation.
arxiv preprintarxiv:1811.00739..yixiao zhang, xiaosong wang, ziyue xu, qihang yu,alan l. yuille, and daguang xu.
2020a.
when radi-ology report generation meets knowledge graph.
inaaai..yuhao zhang, derek merck, emily bao tsai, christo-pher d. manning, and curtis langlotz.
2020b.
op-timizing the factual correctness of a summary: astudy of summarizing radiology reports.
in acl..mingjun zhao, haijiang wu, di niu, and xiaoliwang.
2020. reinforced curriculum learning on pre-trained neural machine translation models.
in aaai..3012