unnatural language inference.
koustuv sinha1,2,3, prasanna parthasarathi1,2, joelle pineau1,2,3 and adina williams31 school of computer science, mcgill university, canada2 montreal institute of learning algorithms (mila), canada3 facebook ai research (fair){koustuv.sinha, prasanna.parthasarathi, jpineau, adinawilliams}@{mail.mcgill.ca, mail.mcgill.ca, cs.mcgill.ca, fb.com}.
abstract.
premise.
hypothesis.
predictedlabel.
large-scale.
state-of-the-art.
recent investigations into the inner-workingsofpre-trainedtransformer-based natural language under-theystanding (nlu) models indicate thatappear to know humanlike syntax, at leastto some extent.
we provide novel evidencethat complicates this claim: we ﬁnd thatstate-of-the-art natural language inference(nli) models assign the same labels topermuted examples as they do to the original,i.e.
they are largely invariant to random word-order permutations.
this behavior notablydiffers from that of humans; we struggle withungrammatical sentences.
to measure theseverity of this issue, we propose a suite ofmetrics and investigate which properties ofparticular permutations lead models to bein the mnli dataset,word-order invariant.
for example, we ﬁnd almost all(98.7%)least one permutationexamples contain atwhich elicits the gold label.
models aresometimes even able to assign gold labelsto permutations that they originally failed topredict correctly.
we provide a comprehensiveempirical evaluation of this phenomenon, andfurther show that this issue exists for bothtransformers and pre-transformer rnn /convnet based encoders, as well as acrossmultiple languages (english and mandarinchinese).
our code and data are available athttps://github.com/facebookresearch/unlu..1.introduction.
of late, large scale pre-trained transformer-based(vaswani et al., 2017) models—such as roberta(liu et al., 2019), bart (lewis et al., 2020), andgpt-2 and -3 (radford et al., 2019; brown et al.,2020)—have exceeded recurrent neural networks’performance on many nlu tasks (wang et al.,2018, 2019).
several papers have even suggestedthat transformers pretrained on a language model-ing (lm) objective can capture syntactic informa-.
boats in daily use lie withinfeet of the fashionable barsand restaurants..there are boats closeto bars and restaurants..restaurants and use feet offashionable lie the in boatswithin bars daily ..restaurants arebarsthere and to closeboats ..his.
and.
heweren’t operating atlevel of metaphor..associatesthe.
he and his associateswere operating at thelevel of the metaphor..his at and metaphor theof were he operating asso-ciates n’t level ..his the and metaphorlevel the were he at as-sociates operating of ..e.e.c.c.table 1: examples from the mnli matched develop-ment set.
both the original example and the permutedone elicit the same classiﬁcation label (entailment andcontradiction respectively) from roberta (large).
asimple demo is provided in an associated google co-lab notebook..tion (hewitt and manning, 2019; jawahar et al.,2019; warstadt and bowman, 2020; wu et al.,2020), with their self-attention layers being capa-ble of surprisingly effective learning (rogers et al.,2020).
in this work, we question such claims thatcurrent models “know syntax”..since there are many ways to investigate “syn-tax”, we must be clear on what we mean by theterm.
knowing the syntax of a sentence meansbeing sensitive to the order of the words in that sen-tence (among other things).
humans are sensitiveto word order, so clearly, “language is not merely abag of words” (harris, 1954, p.156).
moreover, itis easier for us to identify or recall words presentedin canonical orders than in disordered, ungram-matical sentences; this phenomenon is called the“sentence superiority effect” (cattell 1886; scheerer1981; toyota 2001; baddeley et al.
2009; snell andgrainger 2017, 2019; wen et al.
2019, i.a.).
in ourestimation then, if one wants to claim that a model“knows syntax”, then they should minimally showthat the model is sensitive to word order (at least.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages7329–7346august1–6,2021.©2021associationforcomputationallinguistics7329for e.g.
english or mandarin chinese)..generally, knowing the syntax of a sentence istaken to be a prerequisite for understanding whatthat sentence means (heim and kratzer, 1998).
models should have to know the syntax ﬁrst then, ifperforming any particular nlu task that genuinelyrequires a humanlike understanding of meaning (cf.
bender and koller 2020).
thus, if our models areas good at nlu as our current evaluation methodssuggest, we should expect them to be sensitive toword order (see table 1).
we ﬁnd, based on a suiteof permutation metrics, that they are not..we focus here on textual entailment, one of thehallmark tasks used to measure how well mod-els understand language (condoravdi et al., 2003;dagan et al., 2005).
this task, often also callednatural language inference (nli; bowman et al.
2015, i.a.
), typically consists of two sentences:a premise and a hypothesis.
the objective is topredict whether the premise entails the hypothe-sis, contradicts it, or is neutral with respect to it.
we ﬁnd rampant word order insensitivity in pur-portedly high performing nli models.
for nearlyall premise-hypothesis pairs, there are many per-muted examples that fool the models into pro-viding the correct prediction.
in case of mnli,for example, the current state-of-the-art of 90.5%can be increased to 98.7% merely by permutingthe word order of test set examples.
we even ﬁnddrastically increased cross-dataset generalizationwhen we reorder words.
this is not just a matter ofchance—we show that the model output probabili-ties are signiﬁcantly different from uniform..we verify our ﬁndings with three popular en-glish nli datasets—snli (bowman et al., 2015),multinli (williams et al., 2018b) and anli (nieet al., 2020))—and one chinese one, ocnli (huet al., 2020a).
it is thus less likely that our ﬁndingsresult from some quirk of english or a particular to-kenization strategy.
we also observe the effect forvarious transformer architectures pre-trained on lan-guage modeling (bert, roberta, distilbert),and non-transformers, including a convnet, an in-fersent model, and a bilstm..our contributions are as follows: (i) we proposea suite of metrics (permutation acceptance) formeasuring model insensitivity to word order (§3),(ii) we construct multiple permuted test datasetsfor measuring nli model performance at a largescale (§5), (iii) we show that nli models focuson words more than word order, but can partially.
reconstruct syntactic information from words alone(§6), (iv) we show the problem persists on out-of-domain data, (v) we show that humans strugglewith unnatural language inference, underscoringthe non-humanlikeness of sota models (§7), (vi)ﬁnally, we explore a simple maximum entropy-based method (§8) to encourage models not to ac-cept permuted examples..2 related work.
researchers in nlp have realized the importanceof syntactic structure in neural networks going backto tabor (1994).
an early hand annotation efforton pascal rte (dagan et al., 2006) suggestedthat “syntactic information alone was sufﬁcient tomake a judgment” for roughly one third of exam-ples (vanderwende and dolan, 2005).
anecdotally,large generative language models like gpt-2 or -3exhibit a seemingly humanlike ability to generateﬂuent and grammatical text (goldberg, 2019; wolf,2019).
however, the jury is still out as to whethertransformers genuinely acquire syntax..models appear to have acquired syntax.
whenresearchers have peeked inside transformer lm’spretrained representations, familiar syntactic struc-ture (hewitt and manning, 2019; jawahar et al.,2019; lin et al., 2019; warstadt and bowman,2020; wu et al., 2020), or a familiar order oflinguistic operations (jawahar et al., 2019; ten-ney et al., 2019), has appeared.
there is alsoevidence, notably from agreement attraction phe-nomena (linzen et al., 2016) that transformer-based models pretrained on lm do acquire someknowledge of natural language syntax (gulordavaet al., 2018; chrupała and alishahi, 2019; jawaharet al., 2019; lin et al., 2019; manning et al., 2020;hawkins et al., 2020; linzen and baroni, 2021).
results from other phenomena (warstadt and bow-man, 2020) such as npi licensing (warstadt et al.,2019a) lend additional support.
the claim thatlms acquire some syntactic knowledge has beenmade not only for transformers, but also for convo-lutional neural nets (bernardy and lappin, 2017),and rnns (gulordava et al., 2018; van schijndeland linzen, 2018; wilcox et al., 2018; zhang andbowman, 2018; prasad et al., 2019; ravfogel et al.,2019)—although there are many caveats (e.g., rav-fogel et al.
2018; white et al.
2018; davis andvan schijndel 2020; chaves 2020; da costa andchaves 2020; kodner and gupta 2020)..7330models appear to struggle with syntax.
sev-eral works have cast doubt on the extent to whichnli models in particular know syntax (althougheach work adopts a slightly different idea of what“knowing syntax” entails).
for example, mccoyet al.
(2019) argued that the knowledge acquiredby models trained on nli (for at least some pop-ular datasets) is actually not as syntactically so-phisticated as it might have initially seemed; sometransformer models rely mainly on simpler, non-humanlike heuristics.
in general, transformer lmperformance has been found to be patchy and vari-able across linguistic phenomena (dasgupta et al.,2018; naik et al., 2018; an et al., 2019; ravichan-der et al., 2019; jeretic et al., 2020).
this is espe-cially true for syntactic phenomena (marvin andlinzen, 2018; hu et al., 2020b; gauthier et al.,2020; mccoy et al., 2020; warstadt et al., 2020),where transformers are, for some phenomena andsettings, worse than rnns (van schijndel et al.,2019).
from another angle, many have exploredarchitectural approaches for increasing a network’ssensitivity to syntactic structure (chen et al., 2017;li et al., 2020).
williams et al.
(2018a) showedthat learning jointly to perform nli and to parse re-sulted in parse trees that match no popular syntacticformalisms.
furthermore, models trained explicitlyto differentiate acceptable sentences from unaccept-able ones (i.e., one of the most common syntactictests used by linguists) have, to date, come nowherenear human performance (warstadt et al., 2019b)..insensitivity to perturbation.
most relatedly,several concurrent works (pham et al., 2020; alle-man et al., 2021; gupta et al., 2021; sinha et al.,2021; parthasarathi et al., 2021) investigated theeffect of word order permutations on transformernns.
pham et al.
(2020) is very nearly a proper sub-set of our work except for investigating additionaltasks (i.e.
from the glue benchmark of wanget al.
2018) and performing a by-layer-analysis.
gupta et al.
(2021) also relies on the glue bench-mark, but additionally investigates other types of“destructive” perturbations.
our contribution differsfrom these works in that we additionally includethe following: we (i) outline theoretically-informedpredictions for how models should be expected toreact to permuted input (we outline a few options),(ii) show that permuting can “ﬂip” an incorrect pre-diction to a correct one, (iii) show that the problemisn’t speciﬁc to transformers, (iv) show that theproblem persists on out of domain data, (v) offer.
a suite of ﬂexible metrics, and (vi) analyze whymodels might be accepting permutations (bleuand pos-tag neighborhood analysis).
finally, wereplicate our ﬁndings in another language.
whileour work (and pham et al.
; gupta et al.)
only per-mutes data during ﬁne-tuning and/or evaluation,recently sinha et al.
explored the sensitivity dur-ing pre-training, and found that models trained onn-gram permuted sentences perform remarkablyclose to regular mlm pre-training.
in the contextof generation, parthasarathi et al.
(2021) craftedlinguistically relevant perturbations (on the basisof part-of-speech tagging and dependency parsing)to evaluate whether permutation hinders automaticmachine translation models.
relatedly, but not fortranslation, alleman et al.
(2021) investigated asmaller inventory of perturbations with emphasison phrasal boundaries and the effects of n-gramperturbations on different layers in the network..nli models are very sensitive to words.
nlimodels often over-attend to particular words to pre-dict the correct answer (gururangan et al., 2018;clark et al., 2019).
wallace et al.
(2019) show thatsome short sequences of non-human-readable textcan fool many nlu models, including nli modelstrained on snli, into predicting a speciﬁc label.
in fact, ettinger (2020) observed that for one ofthree test sets, bert loses some accuracy in word-perturbed sentences, but that there exists a subsetof examples for which bert’s accuracy remainsintact.
if performance isn’t affected (or if permu-tation helps, as we ﬁnd it does in some cases), itsuggests that these state-of-the-art models actuallyperform somewhat similarly to bag-of-words mod-els (blei et al., 2003; mikolov et al., 2013)..3 our approach.
as we mentioned, linguists generally take syntac-tic structure to be necessary for humans to knowwhat sentences mean.
many also ﬁnd the nli taskto a very promising approximation of human nat-ural language understanding, in part because it isrooted in the tradition of logical entailment.
inthe spirit of propositional logic, sentence mean-ing is taken to be truth-conditional (frege, 1948;montague, 1970; chierchia and mcconnell-ginet,1990; heim and kratzer, 1998).
that is to say thatunderstanding a sentence is equivalent to know-ing the actual conditions of the world under whichthe sentences would be (judged) true (wittgenstein,1922).
if grammatical sentences are required for.
7331sentential inference, as per a truth conditional ap-proach (montague, 1970), then permuted sentencesshould be meaningless.
put another way, the mean-ings of highly permuted sentences (if they exist)are not propositions, and thus those sentences don’thave truth conditions.
only from their truth condi-tions of sentences can we tell if a sentence entailsanother.
in short, the textual entailment task istechnically undeﬁned in our “unnatural” setting..since existing deﬁnitions don’t immediately ex-tend to unnatural language inference (unli), weoutline several hypothetical systematic ways that amodel might perform, had it been sensitive to wordorder.
we hypothesize two models that operate onthe ﬁrst principles of nli, and one that doesn’t.
inthe ﬁrst case, model a deems permuted sentencesmeaningless (devoid of truth values), as formal se-mantic theories of human language would predict.
thus, it assigns “neutral” to every permuted ex-ample.
next, model b does not deem permutedsentences meaningless, and attempts to understandthem.
humans ﬁnd understanding permuted sen-tences difﬁcult (see our human evaluations in §7).
model b could also similarly struggle to decipherthe meaning, and just equally sample labels foreach example (i.e., assigns equal probability massto the outcome of each label).
finally, we hypoth-esize a non-systematic model, model c, which at-tempts to treat permuted sentences as though theyweren’t permuted at all.
this model could operatesimilarly as bag-of-words (bow), and thus alwaysassign the same label to the permuted examples asit would to the un-permuted examples.
if the modelfailed to assign the original gold label to the origi-nal unpermuted examples, it will also fail to assignthe original gold label to its permutations; it willnever get higher accuracy on permuted examplesthan on unpermuted ones..we ﬁnd in our experiments that the state-of-the-art transformer-based nli models (as well as pre-transformer class of models) do not perform likeany of the above hypothetical models.
they per-form closest to model c, but are, in some cases,actually able to achieve higher accuracy on per-muted examples.
to better quantitatively describethis behaviour, we introduce our suite of permuta-tion acceptance metrics that enable us to quantifyhow accepting models are of permuted sentences..4 methods.
constructing the permuted dataset.
for agiven dataset d having splits dtrain and dtest, weﬁrst train an nli model m on dtrain to achievecomparable accuracy to what was reported in theoriginal papers.
we then construct a randomizedversion of dtest, which we term as ˆdtest such that:for each example (pi, hi, yi) ∈ dtest (where pi andhi are the premise and hypothesis sentences of theexample respectively and yi is the gold label), weuse a permutation operator f that returns a list( ˆpi, ˆhi) of q permuted sentences (ˆpi and ˆhi), whereq is a hyperparameter.
f essentially permutes allpositions of the words in a given sentence (i.e., ei-ther in premise or hypothesis) with the restrictionthat no words maintain their original position.
inour initial setting, we do not explicitly control theplacement of the words relative to their originalneighbors, but we analyze clumping effects in §5.
ˆdtest now consists of |dtest|×q examples, with qdifferent permutations of hypothesis and premisefor each original test example pair.
if a sentence s(e.g., hi) contains w words, then the total numberof available permutations of s are (w − 1)!, thus(cid:1) permu-making the output of f a list of (cid:0)(w−1)!
tations in this case.
for us, the space of possibleoutputs is larger, since we permute pi and hi sepa-rately (and ignore examples for which any |s|≤ 5)..q.deﬁning permutation acceptance.
the choiceof q naturally allows us to analyze a statistical viewof the predictability of a model on the permutedsentences.
to that end, we deﬁne the followingnotational conventions.
let a be the original ac-curacy of a given model m on a dataset d, and cbe the number of examples in a dataset which aremarked as correct according to the standard formu-lation of accuracy for the original dataset (i.e., theyare assigned the ground truth label).
typically a isgiven by.
c|dtest| or.
c|ddev| ..let prm ( ˆpi, ˆhi)cor then be the percentage of qpermutations of an example (pi, hi) assigned theground truth label yi by m :.
( ˆpi, ˆhi)cor =.
prm.(cid:88).
1q.
(ˆpj ∈ ˆpi,ˆhj ∈ ˆhi).
((m (ˆpj, ˆhj) = yi) → 1).
(1).
to get an overall summary score, we let ωx be thepercentage of examples (pi, hi) ∈ dtest for whichprm ( ˆpi, ˆhi)cor exceeds a predetermined threshold0 < x < 1. concretely, a given example will count.
7332examples were originally correct (dc) and whenthe examples were originally incorrect (df ) as pera (hence, ﬂipped) respectively..p c =.
m ( ˆpi, ˆhi)cor.
(3).
1|dc|.
|dc|(cid:88).
i=0.
p f is deﬁned similarly by replacing dc by df .
note that for a classic bow model, p c = 100 andp f = 0, because it would rely on the words alone(not their order) to make its classiﬁcation decision.
since permuting removes no words, bow modelsshould come to the same decisions for permutedexamples as for the originals..5 results.
we present results for two types of models:(a) transformer-based models and (b) non-in (a), we investigatetransformer models.
the state-of-the-art pre-trained models such asroberta-large (liu et al., 2019), bart-large(lewis et al., 2020) and distilbert (sanh et al.,2020).
for (b) we consider several recurrent andconvolution based neural networks, such as in-fersent (conneau et al., 2017), bidirectional lstm(collobert and weston, 2008) and convnet (zhaoet al., 2015).
we train all models on mnli,and evaluate on in-distribution (snli and mnli)and out-of-distribution datasets (anli).
we in-dependently verify results of (a) using both ourﬁne-tuned model using huggingface transform-ers (wolf et al., 2020) and pre-trained checkpointsfrom fairseq (ott et al., 2019) (using pytorchmodel hub).
for (b), we use the infersent code-base.
we sample q = 100 permutations for eachexample in dtest, and use 100 seeds for each ofthose permutations to ensure full reproducibility.
we drop examples from test sets where we areunable to compute all unique randomizations, typi-cally these are examples with sentences of lengthof less than 6 tokens.
2.models accept many permuted examples.
weﬁnd ωmax is very high for models trained and evalu-ated on mnli (in-domain generalization), reaching98.7% on mnli dev.
and test sets (in roberta,compared to a of 90.6% (table 2).
recall, hu-man accuracy is approximately 92% on mnli dev.,nangia and bowman 2019).
this shows that thereexists at least one permutation (usually many more).
figure 1: graphical representation of the permutationacceptance class of metrics.
given a sample test setdtest with six examples, three of which originally pre-dicted correctly (model predicts gold label), three incor-rectly (model fails to predict gold label), with n = 6permutations, ωmax,ωrand, ω1.0, p c and p f are pro-vided.
green boxes indicate permutations accepted bythe model.
blue boxes mark examples that crossedeach threshold and were used to compute the corre-sponding metric..as correct according to ωx if more than x percentof its permutations ( ˆpi and ˆhi) are assigned yi bythe model m .
mathematically,.
ωx =.
(cid:88).
1| dtest |.
((prm(pi,hi)∈dtest.
( ˆpi, ˆhi)cor > x) → 1)..(2).
there are two speciﬁc cases of ωx that we are mostinterested in.
first, we deﬁne ωmax or the max-imum accuracy, where x = 1/|dtest|.
in short,ωmax gives the percentage of examples (pi, hi) ∈dtest for which there is at least one permutation( ˆpj, ˆhj) that model m assigns the gold label yi1. second, we deﬁne ωrand, or random baselineaccuracy, where x = 1/m or chance probability(for balanced m-way classiﬁcation, where m = 3in nli).
this metric is less stringent than ωmax,as it counts an example if at least one third of itspermutations are assigned the gold label (henceprovides a lower-bound relaxation).
see figure 1for a graphical representation of ωx..we also deﬁne df to be the list of examplesoriginally marked incorrect according to a, but arenow deemed correct according ωmax.
dc is the listof examples originally marked correct according toa. thus, we should expect df < dc for modelsthat have high accuracy.
additionally, we deﬁne p cand p f , as the dataset average percentage of per-mutations which predicted the gold label, when the.
1theoretically, ωmax → 1 if the number of permutations q.
2code, data, and model checkpoints will be available at.
is large.
thus, in our experiments we set q = 100..https://github.com/facebookresearch/unlu..73330.50.330.660.01.00.660.50.330.660.01.00.66ωmaxωrand0.50.330.660.01.00.66ω1.0𝒫f𝒫c0.83originally correctoriginally incorrectexamplespermutations0.660.160.550.50dtestmodel.
a ωmax.
p c.p f ωrand.
roberta-large.
bart-large.
distilbert.
infersent.
convnet.
bilstm.
mean.
0.668.
0.953.
0.592.
0.351.
0.634.mean.
0.652.
0.948.
0.611.
0.351.
0.623.eval.
dataset.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
mnli m devmnli mm devsnli devsnli testa1*a2*a3*.
0.9060.9010.8790.8830.4560.2710.268.
0.9020.9000.8860.8880.4550.3160.327.
0.8000.8110.7320.7380.2510.3000.312.
0.6580.6690.5560.5600.3160.3100.300.
0.6310.6400.5060.5010.2710.3070.306.
0.6620.6810.5470.5520.2620.2970.304.
0.9870.9870.9880.9880.8970.8890.902.
0.9890.9860.9910.9900.8940.8870.931.
0.9680.9680.9560.9500.7500.7600.830.
0.9040.9050.8200.8260.6690.6620.677.
0.9260.9260.8190.8210.7080.7250.798.
0.9250.9240.8600.8620.6710.7280.731.
0.7070.7070.7680.7600.3920.4650.480.
0.6890.6950.7620.7620.3790.4280.428.
0.7750.7750.7670.7700.5110.6190.559.
0.8420.8440.8210.8240.4250.6890.675.
0.7730.7820.8130.8090.6480.7030.688.
0.8000.8090.7620.7710.6480.6720.656.
0.3830.3870.3930.4070.2860.2920.308.
0.3930.3990.3630.3700.2950.3030.333.
0.3430.3460.3070.3120.2670.2650.259.
0.3590.3680.3230.3210.3950.2490.236.
0.3400.3430.3390.3410.2180.2240.234.
0.3510.3440.3510.3630.2710.2090.219.
0.7940.7900.8260.8280.3640.3590.397.
0.7840.7880.8340.8360.3740.3970.424.
0.7790.7860.7310.7250.3000.3430.363.
0.7120.7230.5870.6000.3130.3300.332.
0.6840.6940.5970.5960.3160.3560.388.
0.7110.7240.5980.6070.3400.3280.331.mean.
0.481.
0.780.
0.731.
0.322.
0.514.mean.
0.452.
0.817.
0.745.
0.291.
0.519.mean.
0.564.
0.883.
0.682.
0.300.
0.575.mean.
0.472.
0.814.
0.731.
0.301.
0.520.table 2:statistics for transformer-based modelstrained on mnli corpus (williams et al., 2018b).
thehighest values are bolded (red indicates the model mostinsensitive to permutation) per metric and per modelclass (transformers and non-transformers).
a1*, a2*and a3* refer to the anli dev.
sets (nie et al., 2020)..model.
roberta-largeinfersentconvnetbilstm.
a.
0.7840.5730.4070.566.ωmax.
0.9880.9310.7520.963.p c.p f.0.7260.7710.8080.701.
0.3390.2650.1990.271.ωrand.
0.7730.6150.4260.611.table 3: results on evaluation on ocnli dev set.
allmodels are trained on ocnli corpus (hu et al., 2020a).
bold marks the highest value per metric (red shows themodel is insensitive to permutation)..figure 2: average entropy of model conﬁdenceson permutations that yielded the correct results fortransformer-based models (top) and non-transformer-based models (bottom).
results are shown for dc (or-ange) and df (blue).
the boxes show the quartiles ofthe entropy distributions..for almost all examples in dtest such that modelm predicts the gold label.
we also observe highωrand at 79.4%, showing that there are many ex-amples for which the models outperform even arandom baseline in accepting permuted sentences(see appendix e for more ω values.).
evaluating out-of-domain generalization withanli dataset splits resulted in an ωmax value that isnotably higher than a (89.7% ωmax for robertacompared to 45.6% a).
as a consequence, we en-counter many ﬂips, i.e., examples where the modelis unable to predict the gold label, but at least onepermutation of that example is able to.
however, re-call this analysis expects us to know the gold labelupfront, so this test can be thought of as running aword-order probe test on the model until the modelpredicts the gold label (or give up by exhaustingour set of q permutations).
for out-of-domain gen-eralization, ωrand decreases considerably (36.4%ωrand on a1), which means fewer permutations areaccepted by the model.
next, recall that a classicbag-of-words model would have p c = 100 andp f = 0. no model performs strictly like a classicbag of words although they do perform somewhatbow-like (p c >> p f for all test splits, figure 5).
we ﬁnd this bow-likeness to be higher for certainnon-transformer models, (infersent) as they ex-hibit higher p c (84.2% for infersent compared to70.7% for roberta on mnli)..models are very conﬁdent.
the phenomenonwe observe would be of less concern if the correctlabel prediction was just an outcome of chance,.
7334which could occur when the entropy of the logprobabilities of the model output is high (suggest-ing uniform probabilities on entailment, neutraland contradiction labels, recall model b from §3).
we ﬁrst investigate the model probabilities for thetransformer-based models on the permutations thatlead to the correct answer in figure 2. we ﬁndoverwhelming evidence that model conﬁdences onin-distribution datasets (mnli, snli) are highlyskewed, resulting in low entropy, and it variesamong different model types.
bart proves to bethe most skewed transformer-based model.
thisskewness is not a property of model capacity, aswe observe distilbert log probabilities to havesimilar skewness as roberta (large) model, whileexhibiting lower a, ωmax, and ωrand..for non-transformers whose accuracy a islower, the ωmax achieved by these models are alsopredictably lower.
we observe roughly the samerelative performance in the terms of ωmax (figure 5and appendix table 2) and average entropy (fig-ure 2).
however, while comparing the averagedentropy of the model predictions, it is clear thatthere is some beneﬁt to being a worse model—non-transformer models are not as overconﬁdent onrandomized sentences as transformers are.
highconﬁdence of transformer models can be attributedto the overthinking phenomenon commonly ob-served in deep neural networks (kaya et al., 2019)and bert-based models (zhou et al., 2020)..similar artifacts in chinese nlu.
we ex-tended the experiments to the original chinesenli dataset (hu et al., 2020a, ocnli), and re-used the pre-trained roberta-large and infersent(non-transformer) models on ocnli.
our ﬁnd-ings are similar to the english results (table 3),thereby suggesting that the phenomenon is not justan artifact of english text or tokenization..other results.
we investigated the effect of sen-tence length (which correlates with number of pos-sible permutations; appendix a), and hypothesis-only randomization (models exhibit similar phe-nomenon even when only hypothesis is permuted;appendix c)..6 analyzing syntactic structure.
associated with tokens.
a natural question to ask following our ﬁndings:what is it about particular permutations that leadsmodels to accept them?
since the permutation oper-.
figure 3: bleu-2 score versus acceptability of per-muted sentences across all test datasets.
roberta andbart performance is similar but differs considerablyfrom the performance of non-transformer-based mod-els, such as infersent and convnet..ation is drastic and only rarely preserves local wordrelations, we ﬁrst investigate whether there existsa relationship between permutation acceptancescores and local word order preservation.
con-cretely, we compare bi-gram word overlap (bleu-2) with the percentage of permutations that aredeemed correct (figure 3).3 although the probabil-ity of a permuted sentence to be predicted correctlydoes appear to track bleu-2 score (figure 3), thepercentage of examples which were assigned thegold label by the transformer-based models is stillhigher than we would expect from permutationswith lower bleu-2 (66% for the lowest bleu-2range of 0 − 0.15), suggesting preserved relativeword order alone cannot explain the high permuta-tion acceptance rates..thus, we ﬁnd that local order preservationdoes correlate with permutation acceptance, butit doesn’t fully explain the high permutation ac-ceptance scores.
we now further ask whether ω isrelated to a more abstract measure of local wordrelations, i.e., part-of-speech (pos) neighborhood.
many syntactic formalisms, like lexical func-tional grammar (kaplan and bresnan, 1995; bres-nan et al., 2015, lfg), head-drive phrase structuregrammar (pollard and sag, 1994, hpsg) or lex-icalized tree adjoining grammar (schabes et al.,1988; abeille, 1990, ltag), are “lexicalized”, i.e.,.
3we observe, due to our permutation process, the maxi-mum bleu-3 and bleu-4 scores are negligibly low (< 0.2bleu-3 and < 0.1 bleu-4), already calling into questionthe hypothesis that n-grams are the sole explanation for ourﬁnding.
because of this, we only compare bleu-2 scores.
detailed experiments on specially constructed permutationsthat cover the entire range of bleu-3 and bleu-4 is providedin appendix d..7335individual words or morphemes bear syntactic fea-tures telling us which other words they can com-bine with.
for example, “buy” could be associatedwith (at least) two lexicalized syntactic structures,one containing two noun phrases (as in kim boughtcheese), and another with three (as in lee boughtlogan cheese).
we speculate that our nli mod-els might accept permuted examples at high rates,because they are (perhaps noisily) reconstructingthe original sentence from abstract, word-anchoredinformation about common neighbors..to test this, we pos-tagged dtrain using 17 uni-versal part-of-speech tags (using spacy, honni-bal et al.
2020).
for each wi ∈ si, we com-pute the occurrence probability of pos tags ontokens in the neighborhood of wi.
the neigh-borhood is speciﬁed by the radius r (a symmet-rical window r tokens from wi ∈ si to the leftand right).
we denote this sentence level prob-ability of neighbor pos tags for a word wi asψr{wi,si} ∈ r17 (see an example in figure 7 inthe appendix).
sentence-level word pos neighborscores can be averaged across dtrain to get a type{wi,dtrain} ∈ r17, ∀wi ∈ dtrain.
then,level score ψrfor a sentence si ∈ dtest, for each word wi ∈ si,we compute a pos mini-tree overlap score:.
βk{wi,si} =.
| argmaxkψr.
1k.{wi,dtrain}∩argmaxkψr.
{wi,si} |.
(4).
(cid:80).
wi∈si.
concretely, βk.
{wi,si} computes the overlap of top-k pos tags in the neighborhood of a word wi ins with that of the train statistic.
if a word has thesame mini-tree in a given sentence as it has in thetraining set, then the overlap would be 1. for agiven sentence si, the aggregate βk{si} is deﬁnedby the average of the overlap scores of all its words:{si} = 1βkβk{wi,si}, and we call it a pos|si|minitree signature.
we can also compute the posminitree signature of a permuted sentence ˆsi tohave βk.
if the permuted sentence pos signaturecomes close to that of the true sentence, then theirratio (i.e., βk{si}) will be close to 1. also,since pos signature is computed with respect tothe train distribution, a ratio of > 1 indicates thatthe permuted sentence is closer to the overall trainstatistic than to the original unpermuted sentencein terms of pos signature.
if high overlap with thetraining distribution correlates with percentage ofpermutations deemed correct, then our models treatwords as if they project syntactic minitrees../βk.
{ ˆsi}.
{ ˆsi}.
figure 4: pos tag mini tree overlap score and per-centage of permutations which the models assigned thegold-label..we investigate the relationship with percentage.
{ ˆsi}.
/βk.
of permuted sentences accepted with βk{si}in figure 4. we observe that the pos tag minitreehypothesis holds for transformer-based models,roberta, bart and distilbert, where the per-centage of accepted pairs increase as the sentenceshave higher overlap with the un-permuted sentencein terms of pos signature.
for non-transformermodels such as infersent, convnet, and bilstmmodels, the pos signature ratio to percentage ofcorrect permutation remains the same or decreases,suggesting that the reasoning process employedby these models does not preserve local abstractsyntax structure (i.e., pos neighbor relations)..7 human evaluation.
we expect humans to struggle with unli, givenour intuitions and the sentence superiority ﬁnd-ings (but see mollica et al.
2020).
to test this,we presented two experts in nli (one a linguist)with permuted sentence pairs to label.4 concretely,we draw equal number of examples from mnlimatched dev set (100 examples where robertapredicts the gold label, dc and 100 examples whereit fails to do so, df ), and then permute these exam-ples using f. the experts were given no additionalinformation (recall that it is common knowledgethat nli is a roughly balanced 3-way classiﬁcationtask).
unbeknownst to the experts, all permutedsentences in the sample were actually acceptedby the roberta (large) model (trained on mnlidataset).
we observe that the experts performed.
4concurrent work by gupta et al.
(2021) found that un-trained crowdworkers accept nli examples that have beensubjected to different kinds of perturbations at roughly mostfrequent class levels—i.e., only 35% of the time..7336evaluator accuracy.
macro f1 acc on dc.
acc on df.
eval dataset.
a (v) a (me) ωmax (v) ωmax (me).
xy.
0.581 ±0.0680.378 ±0.064.
0.4540.378.
0.649 ±0.1020.411 ±0.098.
0.515 ±0.0890.349 ±0.087.
table 4: human (expert) evaluation on 200 permutedexamples from the mnli matched development set.
half of the permuted pairs contained shorter sentencesand the other, longer ones.
all permuted exampleswere assigned the gold label by roberta-large..much worse than roberta (table 4), althoughtheir accuracy was a bit higher than random.
wealso ﬁnd that for both experts, accuracy on per-mutations from dc was higher than on df , whichveriﬁes ﬁndings that showed high word overlap cangive hints about the ground truth label (dasguptaet al., 2018; poliak et al., 2018; gururangan et al.,2018; naik et al., 2019)..8 training by maximizing entropy.
we propose an initial attempt to mitigate the effectof correct prediction on permuted examples.
as weobserve in §5, model entropy on permuted exam-ples is signiﬁcantly lower than expected.
neuralnetworks tend to output higher conﬁdence than ran-dom for even unknown inputs (gandhi and lake,2020), which might be an underlying cause of thehigh permutation acceptance..an ideal model would be ambivalent about ran-domized ungrammatical sentences.
thus, we trainnli models baking in the principle of mutual ex-clusivity (gandhi and lake, 2020) by maximizingmodel entropy.
concretely, we ﬁne-tune robertaon mnli while maximizing the entropy (h) ona subset of n randomized examples ((ˆpi, ˆri), foreach example (p, h) in mnli.
we modify the lossfunction as follows:.
l = argmin.
y log(p(y|(p, h); θ)).
(cid:88).
θ.
((p,h),y).
(cid:16).
h.y|(ˆpi, ˆhi); θ.
(cid:17).
(5).
+.
n(cid:88).
i=1.
using this maximum entropy method (n = 1), weﬁnd that the model improves considerably with re-spect to its robustness to randomized sentences, allwhile taking no hit to accuracy (table 5).
we ob-serve that no model reaches a ωmax score close to 0,suggesting further room to explore other methodsfor decreasing models’ permutation acceptance.
similar approaches have also proven useful (guptaet al., 2021) for other tasks as well..mnli m devmnli mm devsnli testsnli devanli r1 devanli r2 devanli r3 dev.
0.9050.9010.8820.8790.4560.2710.268.
0.9080.9030.8880.8870.4700.2580.243.
0.9840.9850.9830.9840.8900.8800.892.
0.3280.3290.3290.3330.3330.3330.334.table 5: nli accuracy (a) and permutation accep-tance metrics (ωmax) of roberta when trained onmnli dataset using vanilla (v) and maximum ran-dom entropy (me) method..9 future work & conclusion.
we show that state-of-the-art models do not rely onsentence structure the way we think they should:nli models (transformer-based models, rnns,and convnets) are largely insensitive to permuta-tions of word order that corrupt the original syntax.
we also show that reordering words can cause mod-els to ﬂip classiﬁcation labels.
we do ﬁnd thatmodels seem to have learned some syntactic infor-mation as is evidenced by a correlation betweenpreservation of abstract pos neighborhood infor-mation and rate of acceptance by models, but theseresults do not discount the high rates of permuta-tion acceptance, and require further veriﬁcation.
coupled with the ﬁnding that humans cannot per-form unli at all well, the high rate of permutationacceptance that we observe leads us to concludethat current models do not yet “know syntax” inthe fully systematic and humanlike way we wouldlike them to..a few years ago, manning (2015) encouragednlp to consider “the details of human language,how it is learned, processed, and how it changes,rather than just chasing state-of-the-art numbers ona benchmark task.” we expand upon this view, andsuggest one particular future direction: we shouldtrain models not only to do well on clean test data,but also to not to overgeneralize to corrupted input..acknowledgments.
thanks to omar agha, dzmitry bahdanau, sambowman, hagen blix, ryan cotterell, emily di-nan, michal drozdal, charlie lovering, nikita nan-gia, alicia parrish, grusha prasad, roy schwartz,shagun sodhani, anna szabolsci, alex warstadt,jackie chi-kit cheung, timothy o’donnell andmembers of mcgill mcqll lab for many invalu-able comments and feedback on early drafts..7337references.
anne abeille.
1990. lexical and syntactic rules in ain 28th annual meet-tree adjoining grammar.
ing of the association for computational linguistics,pages 292–298, pittsburgh, pennsylvania, usa.
as-sociation for computational linguistics..matteo alleman, jonathan mamou, miguel a del rio,hanlin tang, yoon kim, and sueyeon chung.
2021.syntactic perturbations reveal representational cor-relates of hierarchical phrase structure in pretrainedlanguage models.
arxiv preprint arxiv:2104.07578..aixiu an, peng qian, ethan wilcox, and roger levy.
2019. representation of constituents in neural lan-guage models: coordination phrase as a case study.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2888–2899, hong kong, china.
association for computa-tional linguistics..alan d baddeley, graham j hitch, and richard j allen.
2009. working memory and binding in sentence re-call.
journal of memory and language..emily m. bender and alexander koller.
2020. climb-ing towards nlu: on meaning, form, and under-in proceedings of thestanding in the age of data.
58th annual meeting of the association for compu-tational linguistics, pages 5185–5198, online.
as-sociation for computational linguistics..jean-phillipe bernardy and shalom lappin.
2017. us-ing deep neural networks to learn syntactic agree-ment.
in linguistic issues in language technology,volume 15, 2017. csli publications..david m. blei, andrew y. ng, and michael i. jordan..2003. latent dirichlet allocation.
jmlr..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large anno-tated corpus for learning natural language inference.
in proceedings of the 2015 conference on empiri-cal methods in natural language processing, pages632–642, lisbon, portugal.
association for compu-tational linguistics..joan bresnan, ash asudeh, ida toivonen, and stephenwechsler.
2015. lexical-functional syntax.
john wi-ley & sons..tom brown, benjamin mann, nick ryder, melaniesubbiah,jared d kaplan, prafulla dhariwal,arvind neelakantan, pranav shyam, girish sastry,amanda askell, sandhini agarwal, ariel herbert-voss, gretchen krueger, tom henighan, rewonchild, aditya ramesh, daniel ziegler, jeffrey wu,clemens winter, chris hesse, mark chen, ericsigler, mateusz litwin, scott gray, benjamin chess,jack clark, christopher berner, sam mccandlish,alec radford, ilya sutskever, and dario amodei.
in2020. language models are few-shot learners..advances in neural information processing systems,volume 33, pages 1877–1901.
curran associates,inc..james mckeen cattell.
1886. the time it takes to see.
and name objects.
mind, os-xi(41):63–65..rui chaves.
2020. what don’t rnn language modelslearn about ﬁller-gap dependencies?
in proceedingsof the society for computation in linguistics 2020,pages 1–11, new york, new york.
association forcomputational linguistics..qian chen, xiaodan zhu, zhen-hua ling, si wei, huijiang, and diana inkpen.
2017. enhanced lstmin proceedings offor natural language inference.
the 55th annual meeting of the association for com-putational linguistics (volume 1: long papers),pages 1657–1668, vancouver, canada.
associationfor computational linguistics..gennaro chierchia and sally mcconnell-ginet.
1990.meaning and grammar: an introduction to seman-tics.
cambridge, ma: mit press..grzegorz chrupała and afra alishahi.
2019. corre-lating neural and symbolic representations of lan-in proceedings of the 57th annual meet-guage.
ing of the association for computational linguis-tics, pages 2952–2962, florence, italy.
associationfor computational linguistics..kevin clark, urvashi khandelwal, omer levy, andchristopher d. manning.
2019. what does bertin pro-look at?
an analysis of bert’s attention.
ceedings of the 2019 acl workshop blackboxnlp:analyzing and interpreting neural networks fornlp, pages 276–286, florence, italy.
associationfor computational linguistics..ronan collobert and jason weston.
2008. a uniﬁedarchitecture for natural language processing: deepneural networks with multitask learning.
in icml..cleo condoravdi, dick crouch, valeria de paiva, rein-hard stolle, and daniel g. bobrow.
2003. entail-ment, intensionality and text understanding.
in pro-ceedings of the hlt-naacl 2003 workshop on textmeaning, pages 38–45..alexis conneau, douwe kiela, holger schwenk, lo¨ıcbarrault, and antoine bordes.
2017. supervisedlearning of universal sentence representations fromnatural language inference data.
in proceedings ofthe 2017 conference on empirical methods in nat-ural language processing, pages 670–680, copen-hagen, denmark.
association for computationallinguistics..jillian da costa and rui chaves.
2020. assessing theability of transformer-based neural models to repre-in pro-sent structurally unbounded dependencies.
ceedings of the society for computation in linguis-tics 2020, pages 12–21, new york, new york.
asso-ciation for computational linguistics..7338ido dagan, oren glickman, and bernardo magnini.
2005. the pascal recognising textual entailmentchallenge.
in machine learning challenges work-shop.
springer..ido dagan, oren glickman, and bernardo magnini.
2006. the pascal recognising textual entailmentin machine learning challenges.
evalu-challenge.
ating predictive uncertainty, visual object classiﬁca-tion, and recognising tectual entailment.
springer..ishita dasgupta, demi guo, andreas stuhlm¨uller,samuel j gershman, and noah d goodman.
2018.evaluating compositionality in sentence embed-dings.
in proceedings of annual meeting of the cog-nitive science society..forrest davis and marten van schijndel.
2020. recur-rent neural network language models always learnenglish-like relative clause attachment.
in proceed-ings of the 58th annual meeting of the associationfor computational linguistics, pages 1979–1990,online.
association for computational linguistics..allyson ettinger.
2020. what bert is not: lessonsfrom a new suite of psycholinguistic diagnostics forlanguage models.
transactions of the associationfor computational linguistics, 8:34–48..gottlob frege.
1948. sense and reference.
the philo-.
sophical review..kanishk gandhi and brenden m lake.
2020. mutualexclusivity as a challenge for deep neural networks.
in advances in neural information processing sys-tems, volume 33, pages 14182–14192.
curran asso-ciates, inc..jon gauthier, jennifer hu, ethan wilcox, peng qian,and roger levy.
2020. syntaxgym: an onlineplatform for targeted evaluation of language models.
in proceedings of the 58th annual meeting of theassociation for computational linguistics: systemdemonstrations, pages 70–76, online.
associationfor computational linguistics..yoav goldberg.
2019. assessing bert’s syntactic.
abilities.
arxiv preprint arxiv:1901.05287..kristina gulordava, piotr bojanowski, edouard grave,tal linzen, and marco baroni.
2018. colorlessgreen recurrent networks dream hierarchically.
inproceedings of the 2018 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long papers), pages 1195–1205, neworleans, louisiana.
association for computationallinguistics..ashim gupta, giorgi kvernadze, and vivek srikumar.
2021. bert & family eat word salad: experimentswith text understanding.
aaai..suchin gururangan, swabha swayamdipta, omerlevy, roy schwartz, samuel bowman, and noah a..smith.
2018. annotation artifacts in natural lan-in proceedings of the 2018guage inference data.
conference of the north american chapter of theassociation for computational linguistics: humanlanguage technologies, volume 2 (short papers),pages 107–112, new orleans, louisiana.
associa-tion for computational linguistics..zellig s harris.
1954. distributional structure.
word..robert hawkins, takateru yamakoshi, thomas grif-ﬁths, and adele goldberg.
2020. investigating rep-resentations of verb bias in neural language models.
in proceedings of the 2020 conference on empiricalmethods in natural language processing (emnlp),pages 4653–4663, online.
association for computa-tional linguistics..irene heim and angelika kratzer.
1998. semantics in.
generative grammar.
blackwell oxford..john hewitt and christopher d. manning.
2019. astructural probe for ﬁnding syntax in word repre-sentations.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4129–4138, minneapolis, minnesota.
associ-ation for computational linguistics..matthew honnibal,.
ines montani, soﬁe van lan-deghem,spacy:and adriane boyd.
2020.industrial-strength natural language processing inpython..hai hu, kyle richardson, liang xu, lu li, sandrak¨ubler, and lawrence moss.
2020a.
ocnli: orig-inal chinese natural language inference.
in find-ings of the association for computational linguis-tics: emnlp 2020, pages 3512–3526, online.
as-sociation for computational linguistics..jennifer hu, jon gauthier, peng qian, ethan wilcox,and roger levy.
2020b.
a systematic assessmentof syntactic generalization in neural language mod-in proceedings of the 58th annual meetingels.
of the association for computational linguistics,pages 1725–1744, online.
association for compu-tational linguistics..ganesh jawahar, benoˆıt sagot, and djam´e seddah.
2019. what does bert learn about the structurein proceedings of the 57th annualof language?
meeting of the association for computational lin-guistics, pages 3651–3657, florence, italy.
associa-tion for computational linguistics..paloma jeretic, alex warstadt, suvrat bhooshan, andadina williams.
2020. are natural language infer-ence models imppressive?
learning implicatureand presupposition.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8690–8705, online.
associationfor computational linguistics..7339ronald m kaplan and joan bresnan.
1995. formal sys-tem for grammatical representation.
formal issuesin lexical-functional grammar..yi˘gitcan kaya, sanghyun hong, and tudor dumitras.
2019. shallow-deep networks: understanding andin proceedingsmitigating network overthinking.
of the 2019 international conference on machinelearning (icml), long beach, ca..jordan kodner and nitish gupta.
2020. overestima-tion of syntactic representation in neural languagein proceedings of the 58th annual meet-models.
ing of the association for computational linguistics,pages 1757–1762, online.
association for computa-tional linguistics..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..peiguang li, hongfeng yu, wenkai zhang, guangluanxu, and xian sun.
2020. sa-nli: a supervised at-tention based framework for natural language infer-ence.
neurocomputing..yongjie lin, yi chern tan, and robert frank.
2019.open sesame: getting inside bert’s linguisticknowledge.
in proceedings of the 2019 acl work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 241–253, florence,italy.
association for computational linguistics..tal linzen and marco baroni.
2021. syntactic struc-ture from deep learning.
annual review of linguis-tics..ria..tal linzen, emmanuel dupoux, and yoav goldberg.
2016. assessing the ability of lstms to learnsyntax-sensitive dependencies.
transactions of theassociation for computational linguistics, 4:521–535..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretraining ap-proach.
arxiv..christopher d manning.
2015. computational linguis-tics and deep learning.
computational linguistics..christopher d. manning, kevin clark, john hewitt,urvashi khandelwal, and omer levy.
2020. emer-gent linguistic structure in artiﬁcial neural networkstrained by self-supervision.
proceedings of the na-tional academy of sciences, 117(48):30046–30054..rebecca marvin and tal linzen.
2018. targeted syn-in proceed-tactic evaluation of language models.
ings of the 2018 conference on empirical methodsin natural language processing, pages 1192–1202,brussels, belgium.
association for computationallinguistics..r. thomas mccoy, junghyun min, and tal linzen.
2020. berts of a feather do not generalize to-gether: large variability in generalization acrossin pro-models with similar test set performance.
ceedings of the third blackboxnlp workshop on an-alyzing and interpreting neural networks for nlp,pages 217–227, online.
association for computa-tional linguistics..tom mccoy, ellie pavlick, and tal linzen.
2019.right for the wrong reasons: diagnosing syntacticheuristics in natural language inference.
in proceed-ings of the 57th annual meeting of the associationfor computational linguistics, pages 3428–3448,florence, italy.
association for computational lin-guistics..tom´as mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-in 1st international con-tations in vector space.
ference on learning representations, iclr 2013,scottsdale, arizona, usa, may 2-4, 2013, workshoptrack proceedings..francis mollica, matthew siegelman, evgeniia di-achek, steven t piantadosi, zachary mineroff,richard futrell, hope kean, peng qian, and evelinafedorenko.
2020. composition is the core driverof the language-selective network.
neurobiology oflanguage, 1(1):104–134..richard montague.
1970. universal grammar.
theo-.
aakanksha naik, abhilasha ravichander, carolynrose, and eduard hovy.
2019. exploring numeracyin word embeddings.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 3374–3380, florence, italy.
asso-ciation for computational linguistics..aakanksha naik, abhilasha ravichander, normansadeh, carolyn rose, and graham neubig.
2018.stress test evaluation for natural language inference.
in proceedings of the 27th international conferenceon computational linguistics, pages 2340–2353,santa fe, new mexico, usa.
association for com-putational linguistics..nikita nangia and samuel r. bowman.
2019. humanvs. muppet: a conservative estimate of human per-formance on the glue benchmark.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 4566–4575, flo-rence, italy.
association for computational linguis-tics..7340yixin nie, adina williams, emily dinan, mohitbansal, jason weston, and douwe kiela.
2020. ad-versarial nli: a new benchmark for natural lan-guage understanding.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 4885–4901, online.
associationfor computational linguistics..myle ott, sergey edunov, alexei baevski, angelafan, sam gross, nathan ng, david grangier, andfairseq: a fast, extensiblemichael auli.
2019.in proceedings oftoolkit for sequence modeling.
the 2019 conference of the north american chap-ter of the association for computational linguistics(demonstrations), pages 48–53, minneapolis, min-nesota.
association for computational linguistics..prasanna parthasarathi, koustuv sinha, joelle pineau,and adina williams.
2021. sometimes we wanttranslationese.
arxiv preprint arxiv:2104.07623..thang m pham, trung bui, long mai, and anhnguyen.
2020. out of order: how important isthe sequential order of words in a sentence in nat-ural language understanding tasks?
arxiv preprintarxiv:2012.15180..adam poliak, jason naradowsky, aparajita haldar,rachel rudinger, and benjamin van durme.
2018.hypothesis only baselines in natural language in-in proceedings of the seventh joint con-ference.
ference on lexical and computational semantics,pages 180–191, new orleans, louisiana.
associa-tion for computational linguistics..carl pollard and ivan a sag.
1994. head-drivenphrase structure grammar.
university of chicagopress..grusha prasad, marten van schijndel, and tal linzen.
2019. using priming to uncover the organization ofsyntactic representations in neural language models.
in proceedings of the 23rd conference on computa-tional natural language learning (conll), pages66–76, hong kong, china.
association for compu-tational linguistics..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog..shauli ravfogel, yoav goldberg, and tal linzen.
2019.studying the inductive biases of rnns with syn-in proceed-thetic variations of natural languages.
ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 3532–3542, min-neapolis, minnesota.
association for computationallinguistics..shauli ravfogel, yoav goldberg, and francis tyers.
2018. can lstm learn to capture agreement?
thecase of basque.
in proceedings of the 2018 emnlp.
workshop blackboxnlp: analyzing and interpret-ing neural networks for nlp, pages 98–107, brus-sels, belgium.
association for computational lin-guistics..abhilasha ravichander, aakanksha naik, carolynrose, and eduard hovy.
2019. equate: a bench-mark evaluation framework for quantitative reason-in proceedingsing in natural language inference.
of the 23rd conference on computational naturallanguage learning (conll), pages 349–361, hongkong, china.
association for computational lin-guistics..anna rogers, olga kovaleva, and anna rumshisky.
2020. a primer in bertology: what we knowabout how bert works.
transactions of the associ-ation for computational linguistics, 8:842–866..victor sanh, lysandre debut, julien chaumond, andthomas wolf.
2020. distilbert, a distilled version ofbert: smaller, faster, cheaper and lighter..yves schabes, anne abeille, and aravind k. joshi.
1988. parsing strategies with ‘lexicalized’ gram-mars: application to tree adjoining grammars.
incoling budapest 1988 volume 2: international con-ference on computational linguistics..eckart scheerer.
1981. early german approaches toexperimental reading research: the contributions ofwilhelm wundt and ernst meumann.
psychologicalresearch..marten van schijndel and tal linzen.
2018. a neuralmodel of adaptation in reading.
in proceedings ofthe 2018 conference on empirical methods in nat-ural language processing, pages 4704–4710, brus-sels, belgium.
association for computational lin-guistics..marten van schijndel, aaron mueller, and tal linzen.
2019. quantity doesn’t buy quality syntax within proceedings of theneural language models.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5831–5837, hong kong,china.
association for computational linguistics..koustuv sinha, robin jia, dieuwke hupkes, joellepineau, adina williams, and douwe kiela.
2021.masked language modeling and the distributionalhypothesis: order word matters pre-training for lit-tle.
arxiv preprint arxiv:2104.06644..joshua snell and jonathan grainger.
2017. the sen-.
tence superiority effect revisited.
cognition..joshua snell and jonathan grainger.
2019. word po-sition coding in reading is noisy.
psychonomic bul-letin & review, 26(2):609–615..whitney tabor.
1994. syntactic innovation: a connec-.
tionist model.
ph.d. thesis..7341ian tenney, dipanjan das, and ellie pavlick.
2019.inbert rediscovers the classical nlp pipeline.
proceedings of the 57th annual meeting of the asso-ciation for computational linguistics, pages 4593–4601, florence, italy.
association for computationallinguistics..hiroshi toyota.
2001. changes in the constraints ofsemantic and syntactic congruity on memory acrossthree age groups.
perceptual and motor skills..masatoshi tsuchiya.
2018..performance impactcaused by hidden bias of training data for recog-in proceedings of thenizing textual entailment.
eleventh international conference on language re-sources and evaluation (lrec 2018), miyazaki,japan.
european language resources association(elra)..lucy vanderwende and william b dolan.
2005. whatsyntax can contribute in the entailment task.
in ma-chine learning challenges workshop.
springer..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30. curran associates, inc..eric wallace, shi feng, nikhil kandpal, matt gardner,and sameer singh.
2019. universal adversarial trig-gers for attacking and analyzing nlp.
in proceed-ings of the 2019 conference on empirical methodsin natural language processing and the 9th inter-national joint conference on natural language pro-cessing (emnlp-ijcnlp), pages 2153–2162, hongkong, china.
association for computational lin-guistics..alex wang, yada pruksachatkun, nikita nangia,amanpreet singh, julian michael, felix hill, omerlevy, and samuel r. bowman.
2019. superglue: astickier benchmark for general-purpose language un-derstanding systems.
in neurips..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 353–355, brussels, belgium.
association for computational linguistics..alex warstadt and samuel r bowman.
2020. can neu-ral networks acquire a structural bias from raw lin-in proceedings of the 42nd annualguistic data?
virtual meeting of the cognitive science society..alex warstadt, yu cao, ioana grosu, wei peng, ha-gen blix, yining nie, anna alsop, shikha bordia,haokun liu, alicia parrish, sheng-fu wang, jasonphang, anhad mohananey, phu mon htut, palomajeretic, and samuel r. bowman.
2019a.
investi-gating bert’s knowledge of language: five anal-in proceedings of theysis methods with npis..2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2877–2887, hong kong,china.
association for computational linguistics..alex warstadt, alicia parrish, haokun liu, anhad mo-hananey, wei peng, sheng-fu wang, and samuel r.bowman.
2020. blimp: the benchmark of linguis-tic minimal pairs for english.
transactions of the as-sociation for computational linguistics, 8:377–392..alex warstadt, amanpreet singh, and samuel r. bow-man.
2019b.
neural network acceptability judg-ments.
transactions of the association for compu-tational linguistics, 7:625–641..yun wen, joshua snell, and jonathan grainger.
2019.parallel, cascaded, interactive processing of wordsduring sentence reading.
cognition..aaron steven white, rachel rudinger, kyle rawlins,and benjamin van durme.
2018. lexicosyntacticin proceedings of theinference in neural models.
2018 conference on empirical methods in natu-ral language processing, pages 4717–4724, brus-sels, belgium.
association for computational lin-guistics..ethan wilcox, roger levy, takashi morita, andrichard futrell.
2018. what do rnn languagemodels learn about ﬁller–gap dependencies?
inproceedings of the 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp, pages 211–221, brussels, belgium.
association for computational linguistics..adina williams, andrew drozdov, and samuel r.bowman.
2018a.
do latent tree learning modelsidentify meaningful structure in sentences?
trans-actions of the association for computational lin-guistics, 6:253–267..adina williams, nikita nangia, and samuel bowman.
2018b.
a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the 2018 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume1 (long papers), pages 1112–1122, new orleans,louisiana.
association for computational linguis-tics..ludwig wittgenstein.
1922..tractatus logico-.
philosophicus.
harcourt, brace & company, inc..thomas wolf.
2019. some additional experiments ex-tending the tech report” assessing berts syntacticabilities” by yoav goldberg.
technical report, hug-gingface..thomas wolf, julien chaumond, lysandre debut, vic-tor sanh, clement delangue, anthony moi, pier-ric cistac, morgan funtowicz, joe davison, samshleifer, et al.
2020. transformers: state-of-the-art natural language processing.
in emnlp: systemdemonstrations..7342zhiyong wu, yun chen, ben kao, and qun liu.
2020.perturbed masking: parameter-free probing for ana-lyzing and interpreting bert.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 4166–4176, online.
as-sociation for computational linguistics..kelly zhang and samuel bowman.
2018. languagemodeling teaches you more than translation does:lessons learned through auxiliary syntactic taskanalysis.
in proceedings of the 2018 emnlp work-shop blackboxnlp: analyzing and interpreting neu-ral networks for nlp, pages 359–361, brussels, bel-gium.
association for computational linguistics..han zhao, zhengdong lu, and pascal poupart.
2015.in pro-self-adaptive hierarchical sentence model.
ceedings of the 24th international conference on ar-tiﬁcial intelligence, pages 4069–4076..wangchunshu zhou, canwen xu, tao ge, julianmcauley, ke xu, and furu wei.
2020. bert loses pa-tience: fast and robust inference with early exit.
inadvances in neural information processing systems,volume 33, pages 18330–18341.
curran associates,inc..7343sentences, we compute a ratio between the originaltest overlap score and an overlap score calculatedinstead from the permuted test.
in the figure 7,‘river’ would have a pos tag minitree score of 0.75..figure 5: comparison of ωmax,ωrand,p c and p f withthe model accuracy a on multiple datasets, where allmodels are trained on the mnli corpus (williams et al.,2018b)..a effect of length on permutation.
acceptance.
we investigate the effect of length on permutationacceptance in figure 6. we observe that shortersentences in general have a somewhat higher prob-ability of acceptance for examples which was orig-inally predicted correctly—since shorter sentenceshave fewer unique permutations.
however, for theexamples which were originally incorrect, the trendis not present..figure 6: length and permutation acceptancebytransformer-based models..b example of pos minitree.
in §6, we developed a pos signature for each wordin at least one example in a test set, then comparethat signature to the distribution of the same wordin the training set.
figure 7 provides a snapshota word “river” from the test set and shows howthe pos signature distribution of the word in aparticular example match with that of aggregatedtraining statistic.
in practice, we select the topk pos tags for the word in the test signature aswell as the train, and calculate their overlap.
whencomparing the model performance with permuted.
figure 7: example pos signature for the word ‘river’,calculated with a radius of 2. probability of each neigh-bor pos tag is provided.
orange examples come fromthe permuted test set, and blue come from the originaltraining data..c effect of hypothesis-only.
randomization.
in recent years, the impact of the hypothesis sen-tence (gururangan et al., 2018; tsuchiya, 2018;poliak et al., 2018) on nli classiﬁcation has been atopic of much interest.
as we deﬁne in §3, logicalentailment can only be deﬁned for pairs of propo-sitions.
we investigated one effect where we ran-domize only the hypothesis sentences while keep-ing the premise intact.
figure 9(a) shows that theωmax value is almost the same for the two schemes;randomizing the hypothesis alone also leads themodel to accept many permutations..d effect of clumped words in random.
permutations.
since our original permuted dataset consists of ex-tremely randomized words, we observe very lowbleu-3 (< 0.2) and bleu-4 scores (< 0.1).
tostudy the effect of overlap across a wider rangeof permutations, we devised an experiment wherewe clump certain words together before perform-ing random permutations.
concretely, we clump25%, 50% and 75% of the words in a sentence andthen permute the remaining words and the clumpedword as a whole.
this type of clumped-permutationallows us to study the full range of bleu-2/3/4scores, which we present in figure 10. as expected,the acceptability of permuted sentences increaselinearly with bleu score overlap..7344figure 8: ωx threshold for all datasets with varying x and computing the percentage of examples that fall withinthe threshold.
the top row consists of in-distribution datasets (mnli, snli) and the bottom row contains out-of-distribution datasets (anli).
e effect of the threshold of ωx in various.
eval data.
test splits.
we deﬁned two variations of ωx, ωmax and ωrand,but theoretically it is possible to deﬁne any arbi-trary threshold percentage x to evaluate the unnat-ural language inference mechanisms of differentmodels.
in figure 8 we show the effect of differentthresholds, including ωmax where x = 1/|dtest|and ωrand where x = 0.34. we observe for in-distribution datasets (top row, mnli and snlisplits), in the extreme setting when x = 1.0, thereare more than 10% of examples available, and morethan 25% in case of infersent and distilbert.
for out-of-distribution datasets (bottom row, anlisplits) we observe a much lower trend, suggestinggeneralization itself is the bottleneck in permutedsentence understanding..f training with permuted examples.
in this section, we hypothesize that if the nlumodels are mostly insensitive to word order, thentraining using permuted examples should also yieldthe same or comparable accuracy as training us-ing grammatically correct data (i.e., the standardsetup).
to test this, we train transformer-basedmodels on top of ˆdtrain, which is computed by ap-plying f on each example of dtrain for q = 1 times.
this ensures a control case where we keep the sameamount of training data as the standard setup (such.
robertaˆaa.
0.9060.9010.8790.8830.4560.2710.268.
0.8770.8780.8700.8730.3670.2790.271.bart.
a.
0.9020.9000.8860.8880.4550.3160.327.
ˆa.
0.8620.8690.8540.8590.3360.2930.309.distilbertˆaa.
0.8000.8110.7320.7380.2510.3000.312.
0.7600.7690.7190.7190.2500.2900.312.mnli matchedmnli mismatchedsnli devsnli testanli r1 (dev)anli r2 (dev)anli r3 (dev).
table 6: statistics for transformer-based models whentrained on permuted mnli corpus.
we compare theaccuracy for both models trained on unpermuted data(a) and the permuted data ( ˆa).
we use original testsets during inference..that models does not beneﬁt from data augmen-tation).
we also ensure that we use the same hy-perparameters while training as with the standardsetup.
concretely, ˆdtrain consists of n hypothesis-premise pairs from mnli training data, where eachexample is a permuted output of the original pair..we present the results of such training in ta-ble 6, and compare the accuracy ( ˆa) with that ofthe standard setup (a).
note, during inference forall the models we use the un-permuted examples.
as we can see, models perform surprisingly closeto the original accuracy a even when trained withungrammatical sentences.
this adds further proofto the bow nature of nlu models..7345(a).
(b).
figure 9: comparing the effect between randomizing both premise and hypothesis and only hypothesis on twotransformer-based models, roberta and bart (for more comparisons please refer to appendix).
in 9(a), weobserve the difference of ωmax is marginal in in-distribution datasets (snli, mnli), while hypothesis-only ran-domization is worse for out-of-distribution datasets (anli).
in 9(b), we compare the mean number of permutationswhich elicited correct response, and naturally the hypothesis-only randomization causes more percentage of ran-domizations to be correct..g reproducibility checklist.
as per the prescribed reproducibility checklist,we provide the information of the following:.
• a clear description of the mathematical set-ting, algorithm and/or model: we providedetails of models used in §5.
• description of the computing infrastructureused: we used 8 nvidia v100 32gb gpusto train the models and perform all necessaryinferences.
we didn’t run hyperparameter tun-ing for transformer-based models as we usedthe published hyperparameters from the origi-nal models..• average runtime for each approach: on an av-erage, each model inference experiment con-sistine of 100 permutations for each exampletakes roughly 1 hour to complete..• relevant statistics of the datasets used: weprovide the statistics of the datasets used intable 7..• explanation of any data that were excluded,and all pre-processing steps: we excludeexamples where either the hypothesis andpremise consists of less than 6 tokens.
thisway, we ensure that we have 100 unique per-mutations for each example..code:.
• link to downloadable version of dataanddownload-able version of our data and code athttps://github.com/facebookresearch/unlu..provide.
we.
figure 10: relation of bleu-2/3/4 scores against theacceptability of clumped-permuted sentences accrossall test datasets on all models..dataset.
test examples used examples.
mnli matchedmnli mismatchedsnli devsnli testanli r1 (dev)anli r2 (dev)anli r3 (dev).
9815983298429824100010001200.
6655744936973671756709754.table 7: dataset statistics used in this paper for in-ference.
‘used examples’ provides the number ofpremise-hypothesis pairs for the dataset which we se-lected for inference (i.e., examples where at least 100unique permutations were possible)..7346