semface: pre-training encoder and decoder with a semantic interfacefor neural machine translation.
shuo ren†‡∗, long zhou‡, shujie liu‡, furu wei‡, ming zhou‡, shuai ma††sklsde lab, beihang university, beijing, china‡microsoft research asia, beijing, china†{shuoren,mashuai}@buaa.edu.cn §{long.zhou,shujliu,fuwei,mingzhou}@microsoft.com.
abstract.
while pre-training techniques are workingvery well in natural language processing, howto pre-train a decoder and effectively lever-age it for neural machine translation (nmt)still remains a tricky issue.
the main rea-son is that the cross-attention module betweenthe encoder and decoder cannot be pre-trained,and the combined encoder-decoder model can-not work wellin the ﬁne-tuning stage be-cause the inputs of the decoder cross-attentioncome from unknown encoder outputs.
inthis paper, we propose a better pre-trainingmethod for nmt by deﬁning a semantic in-terface (semface) between the pre-trained en-coder and the pre-trained decoder.
speciﬁ-cally, we propose two types of semantic in-including cl-semface which re-terfaces,gards cross-lingual embeddings as an inter-face, and vq-semface which employs vec-tor quantized embeddings to constrain the en-coder outputs and decoder inputs in the samelanguage-independent space.
we conduct mas-sive experiments on six supervised translationpairs and three unsupervised pairs.
experimen-tal results demonstrate that our proposed sem-face can effectively connect the pre-trained en-coder and decoder, and achieves signiﬁcant im-provement by 3.7 and 1.5 bleu points on thetwo tasks respectively compared with previouspre-training-based nmt models..1.introduction.
in recent years, pre-trained language models (pe-ters et al., 2018; devlin et al., 2018; radford et al.,2019; yang et al., 2019; raffel et al., 2020) signif-icantly boost the performances of various naturallanguage processing (nlp) tasks, receiving exten-sive attention in nlp communities.
following theidea of unsupervised pre-training methods in thenlp area, several approaches (lample and con-neau, 2019; zhu et al., 2020; lewis et al., 2020;.
liu et al., 2020) have been proposed to improveneural machine translation (nmt) models with pre-training by leveraging the large-scale monolingualcorpora.
the typical training process usually con-sists of two stages: pre-training an encoder and adecoder separately with a large monolingual corpusin a self-supervised manner, and then ﬁne-tuning onspeciﬁc nmt tasks (lample and conneau, 2019).
the above method essentially pre-trains a bert-like (devlin et al., 2019) transformer encoder, anduses it to initialize both the encoder and decoder.
although it shows promising results, pre-trainingdecoder beneﬁts little in their results.
the po-tential reason is that the cross-attention betweenthe encoder and decoder is not pre-trained, whichis randomly initialized when they are connectedfor ﬁne-tuning, resulting in a lack of semantic in-terfaces between the pre-trained encoder and de-coder.
another line of work attempts to pre-train asequence-to-sequence model directly, e.g., mass(song et al., 2019) and bart (lewis et al., 2020).
but these methods usually use monolingual denois-ing auto-encoder as the main training objective, andcannot learn the corss-lingual mapping betweensource and target languages explicitly..in parallel to the idea of dall·e1 which de-ﬁnes the cross-modality interface of image and text,we propose to pre-train the encoder and decoderwith a language-independent semantic interface(semface) for neural machine translation.
withthe semantic interface, the encoder is pre-trained toextract features to this space, and the decoder is pre-trained to generate contents with features providedby it.
by deﬁning this interface, we can decouplethe encoder-decoder network and pre-train themseparately.
during the decoder pre-training, thecross-attention module is also pre-trained, thus thepre-trained encoder and decoder can be naturally.
∗contribution during internship at msra..1https://openai.com/blog/dall-e/.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages4518–4527august1–6,2021.©2021associationforcomputationallinguistics4518figure 1: overview of our method (top: pre-training; bottom: ﬁne-tuning).
the training steps of pre-trainingencoder and decoder are separated, therefore the training samples of them are not necesarrily the same.
(in theﬁgure, the training sample for pre-training the encoder is x1 = x11..x61) and the training sample for pre-trainingthe decoder is x2 = x12).
for mt ﬁne-tuning, we use the parallel training sample {x1, y1} from the parallelcorpus or generated from back-translation..2..x6.
1x2.
2x2.
connected for mt ﬁne-tuning.
we propose twotypes of semantic interfaces, namely cl-semfaceand vq-semface.
the former takes the trained un-supervised cross-lingual embeddings (artetxe et al.,2018) as the interface for encoder and decoder pre-training.
inspired by the success of neural discreterepresentation learning (van den oord et al., 2017),the latter uses language-independent vector quan-tized (vq) embeddings (semantic unites) as theinterface to map encoder outputs and decoder in-puts into the shared vq space.
experiments con-ducted on both supervised and unsupervised trans-lation tasks demonstrate that semface effectivelyconnects the pre-trained encoder and decoder, andachieves a signiﬁcant improvement by 3.7 and 1.5bleu points on the two tasks respectively.
our contributions are listed as follows:.
• to the best of our knowledge, this is the ﬁrstwork to investigate and deﬁne a semantic in-terface between encoder and decoder for themt pre-train-ﬁnetune framework..• we design and compare two effective typesof semantic interfaces, which utilize cross-lingual embeddings and vector quantized em-beddings respectively..• we extensively verify the effectiveness of ourproposed model on supervised and unsuper-vised nmt tasks.
particularly, our proposedcl-semface and vq-semface lead to signif-icant improvements of 3.38 and 3.76 blue.
points on low-resource language pairs..2 semface.
2.1 pre-training both encoder and decoder.
the overview of our proposed semface is illus-trated in figure 1. as shown in this ﬁgure, ourmethod can be divided into two steps.
first, we usemonolingual data to pre-train encoder and decoderseparately with a semantic interface between them.
the encoder is pre-trained to map the input fromthe monolingual semantic space into the interface,while the decoder is pre-trained to use the contentfrom the interface via the cross attention moduleto ﬁnish decoding.
the parameters of the encoderand the decoder are updated independently, thustheir pre-training processes can be either jointlyor separately done.
then, we remove the seman-tic interface, and connect the pre-trained encoderand decoder with the pre-trained cross-attention asa sequence-to-sequence model for the subsequentmachine translation ﬁne-tuning.
note that in fig-ure 1, the input to the encoder and decoder includestoken representations, language embeddings andpositional embeddings..there are three types of semantic interface.
theﬁrst is the default output space of pre-trained en-coder with the masked language model (mlm)training loss.
in fact, previous work (song et al.,2019; lewis et al., 2020; liu et al., 2020) adoptsthis default settings in their pre-training methodfor machine translation.
the second one is cl-.
4519figure 2: cl-semface, which regards a pre-trained cross-lingual embeddings as a semantic interface..algorithm 1: pre-training with semface.
input: monolingual corpora dx and dy for two.
languagesoutput: the mt model mθ.
1 randomly initialize the parameters of the encoderθenc and the decoder θdec as well as the semanticinterface θsf.
2 initialize θsf with pre-trained cross-lingual.
embeddings (for cl-semface).
while not convergence do.
3.
4.
5.
6.
7.sample a batch b from dx or dypass b through the encoder with semfaceupdate θenc and θsfpass b through the decoder with semfaceupdate θdec.
8 return mθ = {θenc, θdec}.
semface (sec.
2.2), which uses the pre-trainedcontext-free cross-lingual embedding space as thesemantic interface.
the third is vq-semface (sec.
2.3), which automatically learns a context-awarevector quantized (vq) embedding space as the in-terface during pre-training.
the last two types de-ﬁne a language-independent interface, enforcingthe pre-trained encoder and the decoder to generateor leverage the language-independent information.
they can provide a better initialization for the fol-lowing mt ﬁne-tuning.
we give our pre-trainingalgorithm in alg.
1. note that the parameters ofthe cross-attention are included in θdec.
next, wewill introduce our proposed cl-semface and vq-semface in detail..2.2 cl-semface.
cl-semface uses the cross-lingual embeddingspace as the interface between the encoder andthe decoder during pre-training.
we ﬁrst concate-nate the monolingual corpora of two languages andlearn joint bpe, and then train cross-lingual bpeembeddings with vecmap (artetxe et al., 2018)..as shown in figure 2, on the encoder side, weinitialize the linear projection weights (output em-beddings) before the softmax with the pre-trainedbpe embeddings, and pre-train the encoder with.
two training objectives.
the ﬁrst is the commonlyused masked language model (mlm) (devlinet al., 2018) lmlm, and the other is the mse losslmse between the encoder output hiddens and thecorresponding output embeddings.
the latter con-trols the scale of the encoder outputs to be the sameas the cross-lingual embeddings, in order to matchthe encoder outputs and the cross-attention inputs.
to stabilize training, we calculate the mse lossbefore the last normalization layer of the encoder.
formally, given an input sample x, the encoderpre-training loss function is:.
lenc =lmlm + lmse(cid:88).
=.
[− log p(xi|ln(hi(x))).
(1).
i+ (wi − hi(x))2].
where xi is the masked tokens in the input sentence,hi is the activation of the ﬁnal layer of the encoderbut before the ﬁnal layer normalization ln, wi isthe output embedding of the ground-truth token,and p is the output probability of the softmax..when pre-training the decoder, we attempt to usethe content from the semantic interface to simulateencoder outputs.
to achieve that, given a monolin-gual training sample x, we ﬁrst add some noise1into it to get the noisy sample c(x)), then we passit through an embedding layer initialized with thepre-trained bpe embeddings to get the language-independent representations e(c(x)).
the train-ing target of the decoder is either the mlm or thecasual language model (clm) (lample and con-neau, 2019).
different from them, in our work,the decoder is trained to generate contents withthe language-independent representations from thesemantic interface.
during this process, the param-eters of the enc-dec attention (cross-attention) canalso be pre-trained, which is critical to the subse-quent machine translation ﬁne-tuning.
formally,.
1the noise here includes words dropping and swapping as.
in lample et al.
(2018)..4520figure 3: vq-semface, which utilizes vector quantized embeddings as a semantic interface..the decoder pre-training loss functions is:.
(cid:88).
j.
(cid:88).
j.ldec mlm =.
− log p[yj|(sj(x)), e(c(x))].
(2).
or.
ldec clm =.
− log p(yj|(s<j(x)), e(c(x))].
(3)where s is the ﬁnal output hidden of the decoderand p is the output probability of the softmax..2.3 vq-semface.
the cl semantic space is constrained with thecross-lingual word embedding, which is context-independent, meaning that the different meaningsof the same word share the same embedding, andthe number of semantic units should be the samewith the size of the vocabulary.
in order to learncontext-dependent semantic units freely, we alsopropose another interface type, vector quantizedembeddings, inspired by the recent success of vq-based speech pre-training (baevski et al., 2020).
the concept of vector quantized (vq) representa-tions is ﬁrst proposed in van den oord et al.
(2017).
the method uses a learnable code-book combinedwith the nearest neighbor search to train the dis-crete latent variable model.
the code-book is es-sentially a group of learnable embeddings (codes){z}k1 .
the nearest neighbor search is performedbetween the encoder outputs and the embedding ofthe latent code using the l2 distance metric.
for-mally, given the encoder output h(x), the discretelatent variable assignment is given by.
zi = arg minj∈[k].
||h(x) − zj||2.
(4).
the arg min operation is not differentiable.
fol-lowing baevski et al.
(2020), we use the gumbel-softmax (gumbel, 1954; jang et al., 2016) to selectdiscrete codebook variables in a fully differentiableway and we use the straight-through estimator ofjang et al.
(2016).
given the encoder output h(x),we apply a linear layer followed by a relu andanother linear which outputs l ∈ rk logits for thegumbel-softmax.
during inference, we simplypick the largest index in l. during training, theoutput probability to choose the j-th code is.
pj =.
exp(lj + vj)/τk=1 exp(lk + vk)/τ.
(cid:80)k.(5).
where v = − log(− log(u)) and u are uniformsamples from u(0, 1).
in the forward pass, onlythe embedding in the code-book with the largestprobability is used, which means the output of thevq layer is zi, where i = arg maxi pi, while inthe backward pass, the gradient is passed to all thegumbel-softmax outputs..the vq layer groups the context-aware hid-den states into limited semantic units (codes), andthe space of these codes can be used as our sec-ond language-independent semantic interface.
asshown in figure 3, for the encoder, we add a vqlayer between the encoder output and the predictionlayer of mlm.
the training loss is the combinationof the original mlm loss and two auxiliary lossesas used in baevski et al.
(2020).
the ﬁrst is thediversity loss ld to encourage the model to use thecode-book entries equally often by maximizing theentropy of the averaged softmax distribution overthe codes across a batch of utterances as.
ld =.
¯pk log ¯pk.
(6).
1k.k(cid:88).
k=1.
where k is the number of codes in the code-book,zj is j-th quantized vector in the code-book.
thatmeans, zi is the output of the vq layer correspond-ing to h(x).
the main issue of this method is that.
where ¯pk is the averaged probability of choosingthe k-th code in the code-book across a batch, andpk is calculated by eq.(5).
the second auxiliaryloss is an l2 penalty to stabilize the training, which.
4521is applied to the activations of the ﬁnal encode layerbut before the last normalization of the encoder.
therefore, the total loss of encoder pre-training islenc = lmlm + ld + l2..for the decoder, similar to cl-semface, we alsouse the content from the vq interface to simulatethe encoder output during pre-training.
to get thevq output, given a training sample, we ﬁrst feedit into an embedding layer and then pass the read-out embeddings to a two-layer transformer, whichcan be regarded as a feature extractor.
we use thetransformer output as the representations of eachword and ﬁnd the corresponding codes in the code-book according to eq.(5).
the readout codes arethe simulated encoder output, and they will be fedinto the decoder via the cross-attention.
note thatin the decoder pre-training, the vq code-book isﬁxed.
the training goal of the decoder is the sameas that in cl-semface, i.e., ldec mlm or ldec clm..2.4 fine-tuning.
the semantic interface acts as a bridge to connectthe encoder and decoder during pre-training.
theencoder is pre-trained to project the input to thefeatures in the semantic interface space, while thedecoder is pre-trained to leverage the features fromthe interface space through the cross-attention togenerate outputs.
with this method, we can pre-train all the parameters of the whole sequence-to-sequence model, including the cross-attentionbetween the encoder and the decoder.
after pre-training, we connect the encoder and the decodervia the cross-attention directly by removing thesemantic interface as shown in figure 1 (bottom).
we then ﬁne-tune the model on low-resource su-pervised nmt tasks and unsupervised nmt tasks.
for the low-resource settings, we use the standardcross-entropy loss − log p(y|x) given the paralleltraining sample {x, y}, and for the unsupervisedsettings, we use the denoising auto-encoder anditerative back-translation as the objectives as inlample and conneau (2019)..3 experiment.
3.1 setup.
3.1.1 datasetthe languages we choose for our experiments areenglish (en), french (fr), german (de), romanian(ro), finnish (ﬁ), estonian (et), latvian (lv), lithua-nian (lt), gujarati (gu), and kazakh (kk).
the de-tails of the datasets and statistics for each language.
pair are listed in table 1. all the data is provided bythe recent wmt translation tasks.
“para data” inthis table means the number of training samples of“x-en”.
the language pairs with parallel data in thetable are chosen for the low-resource supervisedsettings, while those with only monolingual dataare chosen for the unsupervised scenario only.
forthe language with more than 50 million monolin-gual data, we randomly sample 50 million from thecorpus.
we choose the corresponding developmentand test sets for each language pair from wmttranslation tasks, as listed in table 2..lang mono data source.
#sent.
para data.
enfrderoﬁetlvltgukk.
ncncncncnc, ccnc, cc, benc, ccnc, cc, wikinc, cc, wikinc, cc, wiki.
50m50m50m21m50m50m38m50m4.3m12.7m.
----2.7m1.9m4.5m2.1m10k91k.
table 1: the datasets used in our experiments.
lang:language; mono: monolingual; para: parallel; #sent:number of sentences in the monolingual corpus; nc:newscrawl; cc: commoncrawl; be: bigest estoniancorpus; wiki: wiki dumps..language-pair.
dev set.
test set.
en-fren-deen-roen-ﬁen-eten-lven-lten-guen-kk.
newstest2013newstest2013newsdev2016newsdev2015newsdev2018newsdev2017newsdev2019newsdev2019newsdev2019.
newstest2014newstest2016newstest2016newstest2017newstest2018newstest2017newstest2019newstest2019newstest2019.
table 2: development and test sets for each pair..3.1.2 baselineswe compare our method with two baselines.
theﬁrst is xlm (lample and conneau, 2019), whichpre-trains a transformer encoder with the mlm orclm loss and then initializes the encoder and thedecoder with the pre-trained model.
the param-eters of the cross-attention module are randomlyinitialized.
the second baseline is mbart (liuet al., 2020), which pre-trains the whole sequence-to-sequence architecture with the denoising auto-encoder loss on the multilingual corpus.
for a fair.
4522method.
en-ﬁ.
en-et.
en-lt.en-lv.
en-gu.
en-kk.
→ ← → ← → ← → ← → ← → ←.
transformer.
20.3.
21.7.
17.7.
22.4.
12.2.
18.1.
12.7.
15.4.xlmmbart.
cl-semfacevq-semface.
21.121.9.
22.722.1.
25.426.7.
25.125.3.
20.620.8.
21.821.6.
24.925.8.
26.627.0.
14.514.7.
15.915.4.
20.720.4.
21.822.3.
14.214.6.
15.915.4.
17.818.7.
19.720.1.
0.0.
0.00.1.
0.51.7.
0.1.
0.00.3.
1.92.6.
0.2.
1.72.1.
2.73.8.
0.8.
4.56.3.
7.69.4.avg..11.80.
(+1.98)(+2.57).
(+3.38)(+3.76).
table 3: bleu scores of the low-resource language pairs.
baseline results are based on our reproduction.
the lastrow means the averaged improvement of each method compared with the basic transformer without pre-training..comparison, we use their pre-training method onthe concatenated corpora of each language pair,i.e., mbart02 in their paper.
for the low-resourcesupervised settings, we also compare our methodwith the basic transformer without pre-training.
if there is a parallel corpus for a certain languagepair, we use the parallel data to ﬁne-tune the pre-trained models in the two baselines.
if there is onlya monolingual corpus, we use the denoising auto-encoder and iterative back-translation to ﬁne-tunethe pre-trained models..implementation details.
3.1.3we implement our method based on the code re-leased by lample and conneau (2019).
for eachlanguage pair, we ﬁrst lower-case all the case-sensitive languages by default and pre-process theconcatenated corpora of each language pair with60,000 joint bpe codes.
for both encoder anddecoder, we use 6-layer transformers with the em-bedding and hidden dimensions of 1024, 8 atten-tion heads, and a dropout rate of 0.1. the maxi-mum sequence length is 256 and the batch size is128. we use the adam optimizer (kingma and ba,2014) for both pre-training and ﬁne-tuning.
duringpre-training, the learning rate is 0.0001 constantly.
during mt ﬁne-tuning, the learning rate is 0.0001with 4,000 warm-up steps, and then decayed basedon the inverse square root of the update number.
the loss of the denoising auto-encoder objectiveis weighted by a coefﬁcient α, and it is linearlydecreased to 0.1 in the ﬁrst 100,000 steps and de-creased to 0 in the next 200,000 steps.
for vq-semface, the code-book contains 102,400 codeswith their dimensions being 1024..3.2 main results.
in this section, we report the result of our pre-training method ﬁne-tuned with neural machinetranslation.
we have two settings.
the ﬁrst set-ting is low-resource supervised machine translation,.
which uses additional parallel corpus to ﬁne-tunethe pre-trained encoder and decoder.
the secondis unsupervised neural machine translation, whichuses the two objectives of denoising auto-encoderand back-translation to ﬁne-tune the model..3.2.1 low-resource language pairs.
the results on the low-resource language pairs areshown in table 3. from the table, we see that ourproposed methods cl-semface and vq-semfacesigniﬁcantly outperform the non-pre-training trans-former with an average improvement of over 3bleu scores.
compared with the strong baselinembart, our methods also outperform it by 0.8 to1.2 bleu scores.
for most translation directions,vq-semface is better than cl-semface, maybedue to the lower quality of cross-lingual languageembeddings of these language pairs, especially forthe distant language pairs (en-gu and en-kk).
thisalso shows the shortcomings of the cl-semfacethat it depends on the quality of the cross-lingualembeddings.
if the quality is not good, the seman-tic interface will be far from language-independent,posing difﬁculties for the splicing of the pre-trainedencoder and the pre-trained decoder.
by contrast,vq-semface gets rid of the constraints of cross-lingual embeddings and learns a context-dependentsemantic space shared across languages, which canhandle those language pairs with low-quality cross-lingual embeddings better..3.2.2 unsupervised language pairs.
we also report the results of three unsupervisedlanguage pairs in table 4. from the table, we ﬁndour proposed methods also signiﬁcantly outperformthe baseline xlm over 1 bleu score.
comparedwith mbart, we also obtain an improvement ofnearly 0.9 bleu score (cl-semface).
contrary tothe result of low-resource pairs in table 3, for thelanguage pairs in table 4, we see the performanceof cl-semface is better than vq-semface.
this.
4523method.
xlmmbart.
cl-semfacevq-semface.
en-fr.
en-de→ ← → ← → ←.
en-ro.
avg..33.033.1.
34.334.2.
33.432.9.
35.034.5.
26.429.8.
28.828.6.
34.334.0.
35.234.8.
33.133.7.
34.533.9.
31.530.9.
32.932.5.
31.95(+0.45).
(+1.50)(+1.13).
table 4: bleu scores of three unsupervised language pairs.
baseline results are based on our reproduction.
thelast row means the averaged improvement of each method compared with the xlm..may be because the cross-lingual embeddings ofthese rich-resource language pairs are of higherquality, thus the semantic interface is initializedbetter during the pre-training..ilar effect that stabilizes the training, contributingabout 1 bleu score in the ﬁnal result.
for decoderpre-training, the performance of the two losses iscomparable, with the mlm slightly better..3.3 discussion.
3.3.1 ablation study.
in this subsection, we ﬁrst investigate the inﬂuenceof the encoder losses (eq.
1) by removing eachof them independently in the encoder pre-training.
besides, note that there are two types of loss usedin our decoder pre-training, mlm and clm, asshown in eq.
(2,3), so we also compare the resultswith different losses in decoder pre-training, takingthe supervised pair en-ﬁ and unsupervised pair en-ro as examples..method.
en-ﬁ.
en-ro.
→ ← → ←.
avg..encoder pre-training loss.
cl-semface-lmsevq-semface-ld-l2.
28.8022.7 25.1 34.5 32.921.3 24.6 33.3 31.6 (-1.10)22.1 25.3 33.9 32.528.4519.7 17.4 29.8 29.6 (-4.33)21.4 24.5 32.5 31.5 (-0.97).
decoder pre-training loss.
22.4 25.1 34.5 32.9cl-semface (mlm)22.7 24.7 33.9 32.1cl-semface (clm)vq-semface (mlm) 22.1 25.1 33.9 32.521.9 25.3 33.2 31.9vq-semface (clm).
28.7328.3528.4028.08.table 5: ablation study of each loss in pre-training..from the table, we ﬁnd that for vq-semface un-der encoder pre-training, the most inﬂuential auxil-iary loss is the diversity loss ld, which contributes4.33 bleu scores in the ﬁnal results, which isdesigned to encourage the model to use the code-book entries equally often.
according to our ob-servation, without ld, the model only uses a smallgroup of codes in the code-book (< 30%), whichindeed shrinks the vq semantic space and leadsto the bad performance.
lmse and l2 have a sim-.
3.3.2.inﬂuence of parallel data.
in this section, we investigate the inﬂuence of thedata quantity in the experiments.
the languagepair we choose is de-en, which has a large paral-lel corpus and makes it possible to conduct ourinvestigation.
we compare the performance of themodel with our pre-training method and the modelwithout pre-training.
note that we do not use anymonolingual data in the training so the result hereis not comparable with that in table 4..figure 4: test bleu of de-en wt./wto.
pre-training.
the horizontal axis is log10 of the used parallel data..as shown in figure 4, when the number of par-allel training data is less than 106.7 ≈ 5m, themodel with pre-training signiﬁcantly outperformsthe non-pre-training model by about 3 to 5 bleuscores.
however, when the training samples in-crease to over 10m, there is almost no differencein performance between the two models..3.3.3 analysis about vq.
as mentioned in sec.2.3, vq space could be re-garded as a language-independent semantic inter-face for the encoder and decoder pre-training.
totest whether vq space is trained to contain cross-lingual representations, we carry out an analysiswith a parallel sample of de-en.
for each token pair.
4524(wen, wde) in the two sentences, we collect top-100codes according to eq.
(5), and calculate how muchthe codes overlapped, as code100(wen)∩code100(wde).
as shown in figure 5, the translated tokens sharemuch of the codes chosen from the vq code-book,which veriﬁes our motivation that vq could actlike a language-independent semantic interface..100.figure 5: the percentage of the overlapping codes cho-sen for each token pair.
the red numbers denote thetranslated tokens..4 related work.
pre-training has been widely used in nlp tasks tolearn better language representations (peters et al.,2018; devlin et al., 2018; lample and conneau,2019; radford et al., 2019; yang et al., 2019; donget al., 2019; lewis et al., 2020).
typically, thesemethods ﬁrst pre-train neural networks on large-scale unlabeled corpora, and then ﬁne-tune themodels on downstream tasks (devlin et al., 2018).
the early pre-training techniques mainly focusedon the natural language understanding tasks suchas the glue benchmark (wang et al., 2018) , andlater it was gradually extended to the natural lan-guage generation tasks, e.g., nmt..recently, a prominent line of work has been pro-posed to improve nmt with pre-training.
thesetechniques can be broadly classiﬁed into two cate-gories.
the ﬁrst category usually uses pre-trainedmodels as feature extractors of a source language,or initializes the encoder and decoder with pre-trained models separately (lample and conneau,2019; ren et al., 2019; yang et al., 2020a; zhuet al., 2020).
for example, lample and conneau(2019) proposed a cross-lingual language modelwith a supervised translation language modelingobjective, and used mlm or clm to pre-train.
the encoder and decoder of nmt.
however, thecombined encoder-decoder model, where the cross-attention is randomly initialized, often does notwork well because of the lack of semantic inter-faces between the pre-trained encoder and decoder.
there is also some work trying to leverage bert-like pre-trained models for mt with an adapter(guo et al., 2020) or an apt framework (wenget al., 2020).
the former deﬁnes additional layersin the pre-trained encoder and decoder during ﬁne-tuning, while the last adopts a fusion mechanismor knowledge distillation to leverage knowledge inbert for mt.
different from them, we enable theencoder and decoder to interact with a semanticinterface during pre-training, and they can be con-nected directly for the mt ﬁne-tuning without anyother additional layers or training loss..the second category methods pre-train a wholesequence-to-sequence model for nmt.
mass(song et al., 2019) employed the encoder-decoderframework to reconstruct a sentence fragment giventhe remaining part of the sentence.
bart (lewiset al., 2020) adopted a similar framework andtrained the model as a denoising auto-encoder.
mbart (liu et al., 2020) trained bart modelon large-scale monolingual corpora in many lan-guages.
although the above work can pre-trainthe cross-attention of decoder, they are learnedon monolingual denoising auto-encoding and can-not learn the corss-lingual transformation betweensource and target languages.
there is also somework trying to explicitly introduce cross-lingualinformation in a code-switch way during thesequence-to-sequence pre-training, such as csp(yang et al., 2020b) and mrasp (lin et al., 2020).
however, their methods need a lexicon or phrasetranslation table, which is inferred from unsuper-vised cross-lingual embeddings.
therefore, theydepend on the quality of the dictionary..the most similar work to ours is probably theone of dall·e and clip (radford et al., 2020).
dall·e is a transformer language model that re-ceives both the text and the image as a singlestream of data.
the core idea is to deﬁne thecross-modality interface of image and text, whichcan generate images from text descriptions.
inthis paper, to address the above limitations of pre-training methods for nmt, we attempt to deﬁnea cross-lingual semantic interface to connect thepre-trained encoder and decoder..45255 conclusion.
we propose semface, a better pre-training methodfor neural machine translation.
the key point is touse a semantic interface to connect the pre-trainedencoder and decoder.
by deﬁning this interface, wecan pre-train the encoder and decoder separatelywith the same intermediate language-independentspace.
the cross-attention can also be pre-trainedwith our method so that we can naturally combinethe pre-trained encoder and decoder for ﬁne-tuning.
we introduce and compare two semantic interfaces,e.g., cl-semface and vq-semface, which lever-age unsupervised cross-lingual embeddings andvector quantized embeddings as the intermediateinterfaces respectively.
massive experiments on su-pervised and unsupervised nmt translation tasksshow that our proposed semface obtains substan-tial improvements over the state-of-the-art baselinemodels.
in the future, we will design and test moresemantic interface types for extensions..acknowledgments.
this work is supported in part by national keyr&d program of china 2018aaa0102301, andnsfc 61925203..references.
mikel artetxe, gorka labaka, and eneko agirre.
2018.a robust self-learning method for fully unsupervisedcross-lingual mappings of word embeddings.
arxivpreprint arxiv:1805.06297..alexei baevski, yuhao zhou, abdelrahman mohamed,and michael auli.
2020. wav2vec 2.0: a frame-work for self-supervised learning of speech represen-tations.
advances in neural information processingsystems, 33..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2018. bert: pre-training of deepbidirectional transformers for language understand-ing.
arxiv preprint arxiv:1810.04805..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..li dong, nan yang, wenhui wang, furu wei, xi-aodong liu, yu wang, jianfeng gao, ming zhou,and hsiao-wuen hon.
2019. uniﬁed language.
model pre-training for natural language understand-ing and generation.
in advances in neural informa-tion processing systems, pages 13063–13075..emil julius gumbel.
1954. statistical theory of ex-treme values and some practical applications: a se-ries of lectures, volume 33. us government print-ing ofﬁce..junliang guo, zhirui zhang, linli xu, hao-ran wei,incor-boxing chen, and enhong chen.
2020.porating bert into parallel sequence decoding withadapters.
arxiv preprint arxiv:2010.06138..eric jang, shixiang gu, and ben poole.
2016. categor-ical reparameterization with gumbel-softmax.
arxivpreprint arxiv:1611.01144..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
arxiv preprintarxiv:1901.07291..guillaume lample, alexis conneau, ludovic denoyer,and marc’aurelio ranzato.
2018. unsupervised ma-chine translation using monolingual corpora only.
ininternational conference on learning representa-tions..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 7871–7880, online.
associationfor computational linguistics..zehui lin, xiao pan, mingxuan wang, xipeng qiu,jiangtao feng, hao zhou, and lei li.
2020. pre-training multilingual neural machine translation byleveraging alignment information.
arxiv preprintarxiv:2010.03142..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020. multilingual denoisingpre-training for neural machine translation.
arxivpreprint arxiv:2001.08210..matthew e peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word repre-sentations.
arxiv preprint arxiv:1802.05365..alec radford, jong wook kim, chris hallacy, adityaramesh, gabriel goh, sandhini agarwal, girishsastry, amanda askell, pamela mishkin, jack clark,et al.
2020. learning transferable visual modelsfrom natural language supervision.
image, 2:t2..4526alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog, 1(8):9..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..shuo ren, yu wu, shujie liu, ming zhou, and shuaima.
2019. explicit cross-lingual pre-training forunsupervised machine translation.
arxiv preprintarxiv:1909.00180..kaitao song, xu tan, tao qin, jianfeng lu, and tie-yan liu.
2019. mass: masked sequence to sequencepre-training for language generation.
arxiv preprintarxiv:1905.02450..aaron van den oord, oriol vinyals, et al.
2017. neu-in advancesral discrete representation learning.
in neural information processing systems, pages6306–6315..alex wang, amanpreet singh, julian michael, felixhill, omer levy, and samuel r bowman.
2018.glue: a multi-task benchmark and analysis platformfor natural language understanding.
arxiv preprintarxiv:1804.07461..rongxiang weng, heng yu, shujian huang, shanbocheng, and weihua luo.
2020. acquiring knowl-edge from pre-trained modelto neural machinetranslation.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 34, pages 9266–9273..jiacheng yang, mingxuan wang, hao zhou, chengqizhao, weinan zhang, yong yu, and lei li.
2020a.
towards making the most of bert in neural machinetranslation.
in proceedings of the aaai conferenceon artiﬁcial intelligence, volume 34, pages 9378–9385..zhen yang, bojie hu, ambyera han, shen huang, andqi ju.
2020b.
csp: code-switching pre-training forin proceedings of theneural machine translation.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 2624–2636..zhilin yang, zihang dai, yiming yang, jaime car-bonell, russ r salakhutdinov, and quoc v le.
2019.xlnet: generalized autoregressive pretraining forlanguage understanding.
in advances in neural in-formation processing systems, pages 5753–5763..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
2020. incorporating bert into neural machine trans-lation.
arxiv preprint arxiv:2002.06823..4527