cross-modal memory networks for radiology report generation.
zhihong chen‚ô†‚ô•, yaling shen‚ô†, yan song‚ô†‚ô•‚Ä†, xiang wan‚ô•‚ô†the chinese university of hong kong (shenzhen)‚ô•shenzhen research institute of big data‚ô†{zhihongchen,yalingshen}@link.cuhk.edu.cn‚ô†songyan@cuhk.edu.cn ‚ô•wanxiang@sribd.cn.
abstract.
medical imaging plays a signiÔ¨Åcant role inclinical practice of medical diagnosis, wherethe text reports of the images are essential inunderstanding them and facilitating later treat-ments.
by generating the reports automati-cally, it is beneÔ¨Åcial to help lighten the burdenof radiologists and signiÔ¨Åcantly promote clin-ical automation, which already attracts muchattention in applying artiÔ¨Åcial intelligence tomedical domain.
previous studies mainly fol-low the encoder-decoder paradigm and focuson the aspect of text generation, with few stud-ies considering the importance of cross-modalmappings and explicitly exploit such map-pings to facilitate radiology report generation.
in this paper, we propose a cross-modal mem-ory networks (cmn) to enhance the encoder-decoder framework for radiology report gen-eration, where a shared memory is designed torecord the alignment between images and textsso as to facilitate the interaction and generationacross modalities.
experimental results illus-trate the effectiveness of our proposed model,where state-of-the-art performance is achievedon two widely used benchmark datasets, i.e.,iu x-ray and mimic-cxr.
further analysesalso prove that our model is able to better aligninformation from radiology images and textsso as to help generating more accurate reportsin terms of clinical indicators.1.
1.introduction.
interpreting radiology images (e.g., chest x-ray)and writing diagnostic reports are essential oper-ations in clinical practice and normally requiresconsiderable manual workload.
therefore, radi-ology report generation, which aims to automat-ically generate a free-text description based on aradiograph, is highly desired to ease the burden of.
‚Ä†corresponding author.
1our code and the best performing models are released at.
https://github.com/cuhksz-nlp/r2gencmn..figure 1: a chest x-ray image and its report includ-ing Ô¨Åndings and impression, where aligned visual andtextual features are marked in different colors.
radiologists while maintaining the quality of healthcare.
recently, substantial progress has been madetowards research on automated radiology reportgeneration models (jing et al., 2018; li et al., 2018;johnson et al., 2019; liu et al., 2019; jing et al.,2019).
most existing studies adopt a conventionalencoder-decoder architecture, with convolutionalneural networks (cnns) as the encoder and recur-rent (e.g., lstm/gru) or non-recurrent networks(e.g., transformer) as the decoder following the im-age captioning paradigm (vinyals et al., 2015; an-derson et al., 2018).
although these methods haveachieved remarkable performance, they are still re-strained in fully employing the information acrossradiology images and reports, such as the mappingsdemonstrated in figure 1 that aligned visual andtextual features point to the same content.
the rea-son for the restraint comes from both the limitationof annotated correspondences between image andtext for supervised learning as well as the lack ofgood model design to learn the correspondences.
unfortunately, few studies2 are dedicated to solv-ing the restraint.
therefore, it is expected to havea better solution to model the alignments acrossmodalities and further improve the generation abil-ity, although promising results are continuouslyacquired by other approaches (li et al., 2018; liuet al., 2019; jing et al., 2019; chen et al., 2020)..2along this research track, recently there is only jing et al.
(2018) studying on a multi-task learning framework with a co-attention mechanism to explicitly explore information linkingparticular parts in a radiograph and its corresponding report..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages5904‚Äì5914august1‚Äì6,2021.¬©2021associationforcomputationallinguistics5904findingsthere is no focal consolidation, pleural eff-usion or pneumothorax.
bilateral nodular opacities that most likely represent nipple shadows.
the cardiomediastinal silhouette is normal.
clips project over the left lung, potentially within the breast.
the imaged upper abdomen is unremarkable.impressionno acute cardiopulmonary process.
figure 2: the overall architecture of our proposed approach, where the visual extractor, encoder and decoder areshown in gray dash boxes with the details omitted.
the cross-modal memory networks are illustrated in blue dashboxes with presenting the detailed process of memory querying and responding..in this paper, we propose an effective yet simpleapproach to radiology report generation enhancedby cross-modal memory networks (cmn), which isdesigned to facilitate the interactions across modal-ities (i.e., images and texts).
in detail, we use amemory matrix to store the cross-modal informa-tion and use it to perform memory querying andmemory responding for the visual and textual fea-tures, where for memory querying, we extract themost related memory vectors from the matrix andcompute their weights according to the input visualand textual features, and then generate responsesby weighting the queried memory vectors.
after-wards, the responses corresponding to the inputvisual and textual features are fed into the encoderand decoder, so as to generate reports enhancedby such explicitly learned cross-modal information.
experimental results on two benchmark datasets,iu x-ray and mimic-cxr, conÔ¨Årm the validityand effectiveness of our proposed approach, wherestate-of-the-art performance is achieved on bothdatasets.
several analyses are also performed toanalyze the effects of different factors affecting ourmodel, showing that our model is able to generatereports with meaningful image-text mapping whilerequiring few extra parameters in doing so..2 the proposed approach.
we regard radiology report generation as an image-to-text generation task, for which there exist sev-.
eral solutions (vinyals et al., 2015; xu et al., 2015;anderson et al., 2018; cornia et al., 2019).
al-though images are organized as 2-d format, we fol-low the standard sequence-to-sequence paradigmfor this task as that performed in chen et al.
in detail, the source sequence is x =(2020).
{x1, x2, ..., xs, ..., xs}, where xs ‚àà rd are ex-tracted by visual extractors from a radiology imagei and the target sequence are the correspondingreport y = {y1, y2, ..., yt, ..., yt }, where yt ‚àà vare the generated tokens, t the length of the reportand v the vocabulary of all possible tokens.
theentire generation process is thus formalized as arecursive application of the chain rule.
p(y|i) =.
p(yt|y1, ..., yt‚àí1, i).
(1).
t(cid:89).
t=1.
the model is then trained to maximize p(y|i)through the negative conditional log-likelihood ofy given the i:.
Œ∏‚àó = arg max.
Œ∏.t(cid:88).
t=1.
log p(yt|y1, ..., yt‚àí1, i; Œ∏) (2).
where Œ∏ is the parameters of the model.
anoverview of the proposed model is demonstratedin figure 2, with cross-modal memories empha-sized.
the details of our approach are described infollowing subsections regarding to its three majorcomponents, i.e., the visual extractor, the cross-modal memory networks and the encoder-decoderprocess enhanced by the memory..5905visual extractorpatch featuresimagetransformerencoder layertransformerdecoder layer√ón√ónlinearsoftmaxyùíï"ùüèrespondingqueryingoutput embedding‚Äúheart‚Äù‚Äúis‚Äù‚Ä¶xùíî‚Ä¶‚Ä¶ùíìùíöùíï"ùüèùíìùíôùë∫ùë¥ùë∞2.1 visual extractor.
to generate radiology reports, the Ô¨Årst step is to ex-tract the visual features from radiology images.
inour approach, the visual features x of a radiologyimage i are extracted by pre-trained convolutionalneural networks (cnn), such as vgg (simonyanand zisserman, 2015) or resnet (he et al., 2016).
normally, an image is decomposed into regions ofequal size3, i.e., patches, and the features (represen-tations) of them are extracted from the last convo-lutional layer of cnn.
once extracted, the featuresin our study are expanded into a sequence by con-catenating them from each row of the patches onthe image.
the resulted representation sequence isused as the source input for all subsequent modulesand the process is formulated as.
{x1, x2, ..., xs, ..., xs} = fv(i).
(3).
where fv(¬∑) refers to the visual extractor..2.2 cross-modal memory networks.
to model the alignment between image and text,existing studies tend to map between images andtexts directly from their encoded representations(e.g., jing et al.
(2018) used a co-attention to do so).
however, this process always suffers from the limi-tation that the representations across modalities arehard to be aligned, so that an intermediate mediumis expected to enhance and smooth such mapping.
to address the limitation, we propose to use cmnto better model the image-text alignment, so as tofacilitate the report generation process..with using the proposed cmn, the mapping andencoding can be described in the following pro-cedure.
given a source sequence {x1, x2, ..., xs}(features extracted from the visual extractor) froman image, we feed itto this module to ob-tain the memory responses of the visual features{rx1, rx2, ..., rxs }.
similarly, given a generatedsequence {y1, y2, ..., yt‚àí1} with its embedding{y1, y2, ..., yt‚àí1}, it is also fed to the cross-modalmemory networks to output the memory responsesof the textual features {ry1, ry2, ..., ryt‚àí1}.
in do-ing so, the shared information of visual and textualfeatures can be recorded in the memory so that theentire learning process is able to explicitly mapbetween the images and texts.
speciÔ¨Åcally, thecross-modal memory networks employs a matrixto preserve information for encoding and decodingprocess, where each row of the matrix (i.e., a mem-.
ory vector) records particular cross-modal informa-tion connecting images and texts.
we denote thematrix as m = {m1, m2, ..., mi, ..., mn }, wheren represents the number of memory vectors andmi ‚àà rd the memory vector at row i with d refer-ring to its dimension.
during the process of reportgeneration, cmn is operated with two main steps,namely, querying and responding, whose detailsare described as follows.4.
memory querying we apply multi-thread5 query-ing to perform this operation, where in each threadthe querying process follows the same proceduredescribed as follows..in querying memory vectors, the Ô¨Årst step isto ensure the input visual and textual features arein the same representation space.
therefore, weconvert each memory vector in m as well as inputfeatures through linear transformation by.
where wk and wq are trainable weights for theconversion.
then we separately extract the mostrelated memory vector to visual and textual featuresaccording to their distances dsi and dti through.
ki = mi ¬∑ wkqs = xs ¬∑ wqqt = yt ¬∑ wq.
dsi =.
dti =.
qs ¬∑ k(cid:62)i‚àödqt ¬∑ k(cid:62)i‚àöd.(4).
(5).
(6).
(7).
(8).
where the number of extracted memory vectors canbe controlled by a hyper-parameter k to regularizehow much memory is used.
we denote the queriedmemory vectors as {ks1, ks2, ..., ksj , ..., ksk} and{kt1, kt2, ..., ktj , ..., ktk}.
afterwards, the impor-tance weight of each memory vector with respectto visual and textual features are obtained by nor-malization over all distances by.
wsi =.
wti =.
exp(dsi)j=1exp(dsj )exp(dti)j=1exp(dtj ).
œÉk.
œÉk.
(9).
(10).
note that the above steps are applied in each threadto allow memory querying from different memoryrepresentation subspaces..4note that these two steps are performed in both trainingand inference stages, where in inference, all textual featuresare obtained along with the generation process..
3e.g., vgg/resnet uses region size 32 √ó 32 (in pixels)..5thread number can be arbitrarily set in experiments..5906memory responding the responding process isalso conducted in a multi-thread manner corre-sponding to the query process.
for each thread,we Ô¨Årstly perform a linear transformation on thequeried memory vector via.
dataset.
iu x-ray.
mimic-cxr.
train val test train val test.
image #report #patient #avg.
len..5.2k 0.7k 1.5k 369.0k 3.0k 5.2k2.8k 0.4k 0.8k 222.8k 1.8k 3.3k2.8k 0.4k 0.8k 64.6k 0.5k 0.3k66.437.6 36.8.
53.0 53.1.
33.6.vi = mi ¬∑ wv.
(11).
where wv is the trainable weight for mi.
so thatall memory vectors {vs1, vs2, ..., vsj , ..., vsk} aretransferred into {vt1, vt2, ..., vtj , ..., vtk}.
then,we obtain the memory responses for visual andtextual features by weighting over the transferredmemory vectors by.
rxs = œÉkryt = œÉk.
i=1wsivsii=1wtivti.
(12).
(13).
where wsi and wti are the weights obtained frommemory querying.
similar to memory querying,we apply memory responding to all the threadsso as to obtain responses from different memoryrepresentation subspaces..2.3 encoder-decoder.
since the quality of input representation plays animportant role in model performance (penningtonet al., 2014; song et al., 2017, 2018; peters et al.,2018; song and shi, 2018; devlin et al., 2019; songet al., 2021), the encoder-decoder in our model isbuilt upon standard transformer (which is a pow-erful architecture that achieved state-of-the-art inmany tasks), where memory responses of visualand textual features are functionalized as the in-put of the encoder and decoder so as to enhancethe generation process.
in detail, as the Ô¨Årst step,the memory responses {rx1, rx2, ..., rxs } for vi-sual features are fed into the encoder through.
{z1, z2, ..., zs} = fe(rx1, rx2, ..., rxs ).
(14).
where fe(¬∑) represents the encoder.
then the re-sulted intermediate states {z1, z2, ..., zs} are sentto the decoder at each decoding step, jointly withthe memory responses {ry1, ry2, ..., ryt‚àí1} for thetextual features of generated tokens from previoussteps, so as to generate the current output yt by.
yt = fd(z1, z2, ..., zs, ry1, ry2, ..., ryt‚àí1).
(15).
table 1: the statistics of the two benchmark datasetsw.r.t.
their training, validation and test sets, includingthe numbers of images, reports and patients, and theaveraged word-based length (avg.
len.)
of reports..3 experiment settings.
3.1 datasets.
we employ two conventional benchmark datasetsin our experiments, i.e., iu x-ray (demner-fushman et al., 2016)6 from indiana universityand mimic-cxr (johnson et al., 2019)7 fromthe beth israel deaconess medical center.
theformer is a relatively small dataset with 7,470 chestx-ray images and 3,955 corresponding reports; thelatter is the largest public radiography dataset with473,057 chest x-ray images and 206,563 reports.
following the experiment settings from previousstudies (li et al., 2018; jing et al., 2019; chen et al.,2020), we only generate the Ô¨Åndings section andexclude the samples without the Ô¨Åndings section forboth datasets.
for iu x-ray, we use the same split(i.e., 70%/10%/20% for train/validation/test set) asthat stated in li et al.
(2018) and for mimic-cxrwe adopt its ofÔ¨Åcial split.
table 1 show the statisticsof all datasets in terms of the numbers of images,reports, patients and the average length of reportswith respect to train/validation/test set..3.2 baseline and evaluation metrics.
to examine our proposed model, we use the follow-ing ones as the main baselines in our experiments:‚Ä¢ base:this is the backbone encoder-decoderused in our full model, i.e., a three-layer trans-former model with 8 heads and 512 hidden unitswithout other extensions..‚Ä¢ base+mem: this is the transformer model withthe same architecture of base where two mem-ory networks are separately applied to image andtext, respectively.
this baseline aims to providea reference to the cross-modal memory..to further demonstrate the effectiveness of ourmodel, we compare it with previous studies, includ-.
where fd(¬∑) refers to the decoder.
as a result, togenerate a complete report, the above process isrepeated until the generation is Ô¨Ånished..6https://openi.nlm.nih.gov/7https://physionet.org/content/.
mimic-cxr/2.0.0/.
5907data model.
bl-1 bl-2 bl-3 bl-4 mtr rg-l avg.
‚àÜ p.nlg metrics.
ce metricsr.f1.
iux-ray.
base0.396 0.254 0.179 0.135 0.164 0.342+mem 0.443 0.270 0.191 0.144 0.172 0.3510.475 0.309 0.222 0.170 0.191 0.375+cmn.
-6.6%.
--19.6% -.
---.
---.
mimic-cxr.
base0.314 0.192 0.127 0.090 0.125 0.265+mem 0.340 0.209 0.140 0.100 0.135 0.2730.353 0.218 0.148 0.106 0.142 0.278+cmn.
-.
0.331 0.224 0.2288.2% 0.322 0.255 0.26113.1% 0.334 0.275 0.278.table 2: nlg and ce evaluations of different models on the test sets of iu x-ray and mimic-cxr datasets.
bl-n denotes bleu score using up to 4-grams; mtr and rg-l denote meteor and rouge-l, respectively.
the average improvement over all nlg metrics compared to base is also presented in the ‚Äúavg.
‚àÜ‚Äù column..ing conventional image captioning models, e.g.,st (vinyals et al., 2015), att2in (rennie et al.,2017), adaatt (lu et al., 2017), topdown (an-derson et al., 2018), and the ones proposed for themedical domain, e.g., coatt (jing et al., 2018),hrgr (li et al., 2018), cmas-rl (jing et al.,2019) and r2gen (chen et al., 2020)..following chen et al.
(2020), we evaluate theabove models by two types of metrics, conventionalnatural language generation (nlg) metrics andclinical efÔ¨Åcacy (ce) metrics8.
the nlg metrics9include bleu (papineni et al., 2002), meteor(denkowski and lavie, 2011) and rouge-l (lin,2004).
for ce metrics, the chexpert (irvin et al.,2019)10 is applied to label the generated reportsand compare the results with ground truths in 14different categories related to thoracic diseases andsupport devices.
we use precision, recall and f1 toevaluate model performance for ce metrics..3.3.implementation details.
to ensure consistency with the experiment settingsof previous work (li et al., 2018; chen et al., 2020),we use two images of a patient as input for re-port generation on iu x-ray and one image formimic-cxr.
for visual extractor, we adopt theresnet101 (he et al., 2016) pretrained on ima-genet (deng et al., 2009) to extract patch featureswith 512 dimensions for each feature.
for theencoder-decoder backbone, we use a transformerstructure with 3 layers and 8 attention heads, 512dimensions for hidden states and initialize it ran-domly.
for the memory matrix in cmn, its dimen-.
8note that ce metrics only apply to mimic-cxr be-cause the labeling schema of chexpert is designed formimic-cxr, which is different from that of iu x-ray..9https://github.com/tylin/coco-caption10https://github.com/mit-lcp/mimic-cxr/.
tree/master/txt/chexpert.
sion and the number of memory vectors n are setto 512 and 2048, respectively, and also randomlyinitialized.
for memory querying and responding,thread number and the k are set to 8 and 32, re-spectively.
we train our model under cross entropyloss with adam optimizer (kingma and ba, 2015).
the learning rates of the visual extractor and otherparameters are set to 5 √ó 10‚àí5 and 1 √ó 10‚àí4, re-spectively, and we decay them by a 0.8 rate perepoch for all datasets.
for the report generationprocess, we set the beam size to 3 to balance the ef-fectiveness and efÔ¨Åciency of all models.
note thatthe optimal hyper-parameters mentioned above areobtained by evaluating the models on the validationsets from the two datasets..4 results and analyses.
4.1 effect of cross-modal memory.
the main experimental results on the two afore-mentioned datasets are shown in table 2, wherebase+cmn represents our model (same below).
there are several observations drawn from differentaspects.
first, both base+mem and base+cmnoutperform the vanilla transformer (base) on bothdatasets with respect to nlg metrics, which con-Ô¨Årms the validity of incorporating memory to intro-duce more knowledge into the transformer back-bone.
such knowledge may come from the hiddenstructures and regularity patterns shared among ra-diology images and their reports, so that the mem-ory modules are able to explicitly and reasonablymodel them to promote the recognition of diseases(symptoms) and the generation of reports.
sec-ond, the comparison between base+cmn and twobaselines on different metrics conÔ¨Årms the effec-tiveness of our proposed model with signiÔ¨Åcant im-provement.
particularly, base+cmn outperformsbase+mem by a large margin, which indicates the.
5908data.
model.
nlg metricsbl-1 bl-2 bl-3 bl-4 mtr rg-l.ce metricsrp.f1.
iux-ray.
mimic-cxr.
st‚Ä°att2in‚Ä°adaatt‚Ä°coatt‚Ä°hrgr‚Ä°cmas-rl‚Ä°r2gen‚Ä°.
(cid:51).
ours (cmn)st(cid:51)att2inadaatttopdownr2gen‚Ä°.
(cid:51).
(cid:51).
0.2160.2240.220.
0.4550.4380.4640.470.
0.2990.3250.2990.3170.353.
0.1240.1290.127.
0.2880.2980.3010.304.
0.1840.2030.1850.1950.218.
0.0870.0890.089.
0.2050.2080.2100.219.
0.1210.1360.1240.1300.145.
0.0660.0680.068.
0.1540.1510.1540.165.
0.0840.0960.0880.0920.103.
---.
---0.187.
0.1240.1340.1180.1280.142.
0.3060.3080.308.
0.3690.3220.3620.371.
0.2630.2760.2660.2670.277.
0.475.
0.309.
0.222.
0.170.
0.191.
0.375.
---.
----.
-.
---.
----.
-.
---.
----.
-.
0.2490.3220.2680.3200.333.
0.2030.2390.1860.2310.273.
0.2040.2490.1810.2380.276.ours (cmn).
0.353.
0.218.
0.148.
0.106.
0.142.
0.278.
0.334.
0.275.
0.278.table 3: comparisons of our proposed model with previous studies on the test sets of iu x-ray and mimic-cxr with respect to nlg and ce metrics.
‚Ä° refers to that the result is directed cited from the original paper and(cid:51) represents our replicated results by their released codes..usefulness of cmn in learning cross-modal fea-tures with a shared structure rather than separateones.
third, when comparing between datasets,the performance gains from base+cmn over twobaselines (i.e., base and base+mem) on mimic-cxr are larger than that of iu x-ray.
this ob-servation owes to the fact that mimic-cxr isrelatively larger, which helps the learning of thealignment between images and texts so that cmnhelps more on report generation on mimic-cxr.
third, when compared between datasets, the per-formace gain from base+cmn over two baselines(i.e., base and base+mem) on iu x-ray arelarger than that of mimic-cxr.
this observationowes to the fact that iu x-ray is relatively smalland has less complicated visual-textual mappings,thus easier for generation by cmn.
moreover, thissize effect also helps that our model shows thesame trend on the ce metrics on mimic-cxr asthat for nlg metrics, where it outperforms all itsbaselines in terms of precision, recall and f1..4.2 comparison with previous studies.
to further demonstrate the effectiveness, we furthercompare our model with existing models on thesame datasets, with their results reported in table 3on both nlg and ce metrics.
we have followingobservations.
first, cross-modal memory shows itseffectiveness in this task, where our model outper-.
forms coatt, although both of them improve thereport generation by the alignment of visual andtextual features.
the reason behind might be thatour model is able to use a shared memory matrixas the medium to softly align the visual and tex-tual features instead of direct alignment using theco-attention mechanism, thus uniÔ¨Åes cross-modalfeatures within same representation space and fa-cilitate the alignment process.
second, our modelconÔ¨Årms its superiority of simplicity when com-paring with those complicated models.
for exam-ple, hrgr uses manually extracted templates andcmas-rl utilizes reinforcement learning with acareful design of adaptive rewards and our modelachieves better results with a rather simpler method.
third, applying memory to both the encoding anddecoding can further improve the generation abil-ity of transformer when compared with r2genwhich only uses memory in decoding.
this obser-vation complies with our intuition that the cross-modal operation tightens the encoding and decod-ing so that our model generates higher quality re-ports.
fourth, note that although there are othermodels (i.e., coatt and hrgr) with exploitingextra information (such as private datasets for vi-sual extractor pre-training), our model still achievesthe state-of-the-art performance without requiringsuch information.
it reveals that in this task, thehidden structures among the images and texts and a.
5909figure 3: the bleu-4 score and the number of param-eters from base+cmn against the memory size (i.e.,number of memory vectors) when the model is trainedand tested on mimic-cxr dataset..good solution of exploiting them are more essentialin promoting the report generation performance..4.3 analysis.
memory size to analyze the impacts of memorysize, we train our model with different numbers ofmemory vectors, i.e., n ranges from 32 to 4096,with the results on mimic-cxr shown in fig-ure 3. it is observed that, Ô¨Årst, enlarging memoryby the number of vectors results in better overallperformance when the entire memory matrix is rel-atively small (n ‚â§ 1024), which can be explainedby that, within a certain memory capacity, largermemory size helps store more cross-modal infor-mation; second, when the memory matrix is largerthan a threshold, increasing memory vectors is notable to continue promising a better outcome.
anexplanation to this observation may be that, whenthe matrix is getting to large, the memory vectorscan not be fully updated so they do not help thegeneration process other than being played as noise.
more interestingly, it is noted that even if we usea rather large memory size (i.e., n = 4096), only3.34% extra parameters are added to the modelcompared to base, which justiÔ¨Åes that introducingmemory to report generation process through ourmodel can be done with small price..number of queried memory vectors to ana-lyze how querying impacts report generation, wetry cmn with different numbers of queried vec-tors, i.e., k ranges from 1 to 512, and show theresults in figure 4. it is found that the number ofqueried vectors should be neither too small nor toobig, where enlarging k leads to better results whenk ‚â§ 32 and after this threshold the performance.
figure 4: the bleu-4 score from base+cmn whentested on the mimic-cxr test set against differentnumbers of queried memory vectors..starts to drop.
the reason behind might be theoverÔ¨Åtting of memory updating since the memorymatrix is sparsely updated in each iteration whenk is small, i.e., it is hard to be overÔ¨Åt under thisscenario, while more queried vectors should causeintensive updating on the matrix and some of theessential vectors are over-updated accordingly.
asa result, it is interesting to Ô¨Ånd the optimal num-ber (i.e., 32) of queried vectors and this is a usefulguidance to further improve report generation withcontrolling the querying process..case study to further qualitatively investigatehow our model learns from the alignments betweenthe visual and textual information, we perform acase study on the generated reports from differentmodels regarding to an input chest x-ray imagechosen from mimic-cxr.
figure 5 shows theimage with ground-truth report, and different re-ports with selected mappings from visual (somepart of the image) and textual features (some wordsand phrases),11 where the mapped areas on theimage are highlighted with different colors.
in gen-eral, base+cmn is able to generate more accuratedescriptions (in terms of better visual-textual map-ping) in the report while other baselines are inferiorin doing so.
for instance, normal medical condi-tions and abnormalities presented in the chest x-rayimage are covered by the generated report frombase+cmn (e.g., ‚Äúsevere cardiomegaly‚Äù, ‚Äúpul-monary edema‚Äù and ‚Äúpulmonary arteries‚Äù) and therelated regions on the image are precisely locatedregarding to the texts, while the areas highlightedon the image from other models are inaccurate..11the representations of the textual features are extracted.
from the Ô¨Årst layer of the decoder..591032641282565121024204840960.0880.0920.0960.1000.1040.108 base base+mem base+cmn parametermemory sizebl-462.8m63.2m63.6m64.0m64.4m64.8m parameters12481632641282565120.0880.0920.0960.1000.1040.108 base base+mem base+cmnqueried vectorsbl-463m63m63m64m64mparametersfigure 5: visualizations of image-text mappings between particular regions (indicated by colored weights) of achest x-ray image and words/phrases from its reports generated by base, base+mem and base+cmn, respec-tively.
the color spectrum indicates the value of weight from low to high in the range of [0, 1]..among these studies, the most related study fromcornia et al.
(2019) also proposed to leverage mem-ory matrices to learn a priori knowledge for visualfeatures using memory networks (weston et al.,2015; sukhbaatar et al., 2015; zeng et al., 2018;santoro et al., 2018; nie et al., 2020; diao et al.,2020; tian et al., 2020b, 2021; chen et al., 2021),but such operation is only performed during theencoding process.
different from this work, thememory in our model is designed to align the visualand textual features, and the memory operations(i.e., querying and responding) are performed inboth the encoding and decoding process..recently, many advanced nlp techniques (e.g.,pre-trained language models) have been applied totasks in the medical domain (pampari et al., 2018;zhang et al., 2018; wang et al., 2018; alsentzeret al., 2019; tian et al., 2019, 2020a; wang et al.,2020; lee et al., 2020; song et al., 2020).
beingone of the applications and extensions of imagecaptioning to the medical domain, radiology re-port generation aims to depicting radiology imageswith professional reports.
existing methods weredesigned and proposed to better align images andtexts or to exploit highly-patternized features oftexts.
for the former studies, jing et al.
(2018)proposed a co-attention mechanism to simultane-ously explore visual and semantic information witha multi-task learning framework.
for the latter stud-ies, li et al.
(2018) introduced a template databaseto incorporate patternized information and chenet al.
(2020) improved the performance of radi-.
figure 6: t-sne visualization of memory vectors withan example input image and its partial generated reportfrom mimic-cxr test set.
the queried vectors forvisual and textual features are indicated by arrows..to further illustrate how the alignment worksbetween visual and textual features, we perform a t-sne visualization on the memory vectors linking toan image and its generated report from the mimic-cxr test set.
it is observed that the word ‚Äúlung‚Äùin the report and the visual feature for the regionof lung on the image query similar memory vec-tors from cmn, where similar observation is alsodrawn for ‚Äúhemidiaphragms‚Äù and its correspond-ing regions on the image.
this case conÔ¨Årms thatmemory vector is effective intermediate medium tointeract between image and text features..5 related work.
in general, the most popular related task to oursis image captioning, a cross-modal task involv-ing natural language processing and computer vi-sion, which aims to describe images in sentences(vinyals et al., 2015; xu et al., 2015; andersonet al., 2018; wang et al., 2019; cornia et al., 2019)..5911mild hyperinflated lungs‚Ä¶‚Ä¶ with flattening hemidiaphragms.00.20.40.60.8100.20.40.60.81ology report generation by applying a memory-driven transformer to model patternized informa-tion.
compared to these studies, our model offersan effective yet simple alternative to generating ra-diology reports, where a soft intermediate layer isprovided to facilitate the mappings between visualand textual features, so that more accurate descrip-tions are produced for generation..6 conclusion.
in this paper, we propose to generate radiology re-ports with cross-modal memory networks, wherea memory matrix is employed to record the align-ment and interaction between images and texts,with memory querying and responding performedto obtain the shared information across modalities.
experimental results on two benchmark datasetsdemonstrate the effectiveness of our model, whichachieves the state-of-the-art performance.
furtheranalyses investigate the effects of hyper-parametersin our model and show that our model is able to bet-ter align information from images and texts, so asto generate more accurate reports, especially withthe fact that enlarging the memory matrix does notsigniÔ¨Åcantly affect the entire model size..acknowledgments.
this work is supported by chinese key-area re-search and development program of guangdongprovince (2020b0101350001) and nsfc under theproject ‚Äúthe essential algorithms and technolo-gies for standardized analytics of clinical texts‚Äù(12026610)..references.
emily alsentzer, john murphy, william boag, wei-hung weng, di jindi, tristan naumann, andmatthew mcdermott.
2019.publicly availableclinical bert embeddings.
in proceedings of the2nd clinical natural language processing work-shop, pages 72‚Äì78..peter anderson, xiaodong he, chris buehler, damienteney, mark johnson, stephen gould, and leizhang.
2018. bottom-up and top-down attentionfor image captioning and visual question answer-ing.
in proceedings of the ieee conference on com-puter vision and pattern recognition, pages 6077‚Äì6086..guimin chen, yuanhe tian, yan song, and xiang wan.
2021. relation extraction with type-aware mapin findings ofmemories of word dependencies..the association for computational linguistics: acl-ijcnlp 2021..zhihong chen, yan song, tsung-hui chang, and xi-ang wan.
2020. generating radiology reports viamemory-driven transformer.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 1439‚Äì1449..marcella cornia, matteo stefanini, lorenzo baraldi,and rita cucchiara.
2019. m2: meshed-memorytransformer for image captioning.
arxiv preprintarxiv:1912.08226..dina demner-fushman, marc d kohli, marc b rosen-man, sonya e shooshan, laritza rodriguez, sameerantani, george r thoma, and clement j mcdon-ald.
2016. preparing a collection of radiology ex-journalaminations for distribution and retrieval.
of the american medical informatics association,23(2):304‚Äì310..jia deng, wei dong, richard socher, li-jia li, kai li,and li fei-fei.
2009. imagenet: a large-scale hier-archical image database.
in 2009 ieee conferenceon computer vision and pattern recognition, pages248‚Äì255..michael denkowski and alon lavie.
2011. meteor1.3: automatic metric for reliable optimization andevaluation of machine translation systems.
in pro-ceedings of the sixth workshop on statistical ma-chine translation, pages 85‚Äì91..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171‚Äì4186..shizhe diao, yan song, and tong zhang.
2020.keyphrase generation with cross-document atten-tion.
arxiv preprint arxiv:2004.09800..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2016. deep residual learning for imagerecognition.
in proceedings of the ieee conferenceon computer vision and pattern recognition, pages770‚Äì778..jeremy irvin, pranav rajpurkar, michael ko, yifan yu,silviana ciurea-ilcus, chris chute, henrik mark-lund, behzad haghgoo, robyn ball, katie shpan-skaya, et al.
2019. chexpert: a large chest ra-diograph dataset with uncertainty labels and ex-pert comparison.
in proceedings of the aaai con-ference on artiÔ¨Åcial intelligence, volume 33, pages590‚Äì597..baoyu jing, zeya wang, and eric xing.
2019. show,describe and conclude: on exploiting the structureinformation of chest x-ray reports.
in proceedingsof the 57th annual meeting of the association forcomputational linguistics, pages 6570‚Äì6580..5912baoyu jing, pengtao xie, and eric xing.
2018. on theautomatic generation of medical imaging reports.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 2577‚Äì2586..alistair ew johnson, tom j pollard, seth j berkowitz,nathaniel r greenbaum, matthew p lungren, chih-ying deng, roger g mark, and steven horng.
2019. mimic-cxr: a large publicly availablearxivlabeled chestdatabase ofpreprint arxiv:1901.07042..radiographs..diederik p kingma and jimmy ba.
2015. adam:corr,.
a method for stochastic optimization.
abs/1412.6980..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020.biobert: a pre-trained biomedical language representation modelbioinformatics,for biomedicaltext mining.
36(4):1234‚Äì1240..yuan li, xiaodan liang, zhiting hu, and eric pxing.
2018. hybrid retrieval-generation rein-forced agent for medical image report generation.
in advances in neural information processing sys-tems, pages 1530‚Äì1540..chin-yew lin.
2004. rouge: a package for auto-matic evaluation of summaries.
in text summariza-tion branches out, pages 74‚Äì81..guanxiong liu, tzu-ming harry hsu, matthew mc-dermott, willie boag, wei-hung weng, peterszolovits, and marzyeh ghassemi.
2019. clinicallyin ma-accurate chest x-ray report generation.
chine learning for healthcare conference, pages249‚Äì269..jiasen lu, caiming xiong, devi parikh, and richardsocher.
2017. knowing when to look: adaptiveattention via a visual sentinel for image caption-in proceedings of the ieee conference oning.
computer vision and pattern recognition, pages 375‚Äì383..yuyang nie, yuanhe tian, yan song, xiang ao, andxiang wan.
2020. improving named entity recog-nition with attentive ensemble of syntactic infor-mation.
in proceedings of the 2020 conference onempirical methods in natural language processing:findings, pages 4231‚Äì4245..anusri pampari, preethi raghavan, jennifer liang, andjian peng.
2018. emrqa: a large corpus for ques-tion answering on electronic medical records.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages2357‚Äì2368..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automaticevaluation of machine translation.
in proceedingsof the 40th annual meeting on association for com-putational linguistics, pages 311‚Äì318..jeffrey pennington, richard socher, and christo-glove: global vectorspher manning.
2014.in proceedings of thefor word representation.
2014 conference on empirical methods in naturallanguage processing (emnlp), pages 1532‚Äì1543,doha, qatar..matthew peters, mark neumann, mohit iyyer, mattgardner, christopher clark, kenton lee, and lukezettlemoyer.
2018. deep contextualized word rep-resentations.
in proceedings of the 2018 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long papers), pages 2227‚Äì2237, new orleans, louisiana..steven j rennie, etienne marcheret, youssef mroueh,jerret ross, and vaibhava goel.
2017. self-criticalin pro-sequence training for image captioning.
ceedings of the ieee conference on computer vi-sion and pattern recognition, pages 7008‚Äì7024..adam santoro, ryan faulkner, david raposo, jackrae, mike chrzanowski, theophane weber, daanwierstra, oriol vinyals, razvan pascanu, and timo-thy lillicrap.
2018. relational recurrent neural net-works.
in advances in neural information process-ing systems, pages 7299‚Äì7310..karen simonyan and andrew zisserman.
2015. verydeep convolutional networks for large-scale im-age recognition.
corr, abs/1409.1556..yan song, chia-jung lee, and fei xia.
2017. learn-ing word representations with regularization fromprior knowledge.
in proceedings of the 21st confer-ence on computational natural language learning(conll 2017), pages 143‚Äì152..yan song and shuming shi.
2018. complementarylearning of word embeddings.
in proceedings ofthe twenty-seventh international joint conferenceon artiÔ¨Åcial intelligence, ijcai-18, pages 4368‚Äì4374..yan song, shuming shi, jing li, and haisong zhang.
2018. directional skip-gram: explicitly distin-guishing left and right context for word embed-dings.
in proceedings of the 2018 conference of thenorth american chapter of the association for com-putational linguistics: human language technolo-gies, volume 2 (short papers), pages 175‚Äì180..yan song, yuanhe tian, nan wang, and fei xia.
2020.summarizing medical conversations via identifyingin proceedings of the 28thimportant utterances.
international conference on computational linguis-tics, pages 717‚Äì729..yan song, tong zhang, yonggang wang, and kai-fulee.
2021. zen 2.0: continue training and adap-tion for n-gram enhanced text encoders.
arxivpreprint arxiv:2105.01279..5913yuhao zhang, daisy yi ding, tianpei qian, christo-pher d manning, and curtis p langlotz.
2018.learning to summarize radiology findings.
in pro-ceedings of the ninth international workshop onhealth text mining and information analysis, pages204‚Äì213..sainbayar sukhbaatar, jason weston, rob fergus, et al.
in ad-2015. end-to-end memory networks.
vances in neural information processing systems,pages 2440‚Äì2448..yuanhe tian, guimin chen, and yan song.
2021. en-hancing aspect-level sentiment analysis with wordin proceedings of the 16th confer-dependencies.
ence of the european chapter of the associationfor computational linguistics: main volume, pages3726‚Äì3739, online..yuanhe tian, weicheng ma, fei xia, and yan song.
2019. chimed: a chinese medical corpus forin proceedings of the 18thquestion answering.
bionlp workshop and shared task, pages 250‚Äì260..yuanhe tian, wang shen, yan song, fei xia, minhe, and kenli li.
2020a.
improving biomedicalnamed entity recognition with syntactic informa-tion.
bmc bioinformatics, 21:1471‚Äì2105..yuanhe tian, yan song, fei xia, tong zhang, andyonggang wang.
2020b.
improving chinese wordsegmentation with wordhood memory networks.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages8274‚Äì8285..oriol vinyals, alexander toshev, samy bengio, anddumitru erhan.
2015. show and tell: a neural im-age caption generator.
in proceedings of the ieeeconference on computer vision and pattern recogni-tion, pages 3156‚Äì3164..nan wang, yan song, and fei xia.
2018. coding struc-tures and actions with the costa scheme in med-in proceedings of the bionlpical conversations.
2018 workshop, pages 76‚Äì86..nan wang, yan song, and fei xia.
2020. studyingchallenges in medical conversation with structuredin proceedings of the first workshopannotation.
on natural language processing for medical con-versations, pages 12‚Äì21, online..weixuan wang, zhihong chen, and haifeng hu.
2019.hierarchical attention network for image caption-ing.
in proceedings of the aaai conference on arti-Ô¨Åcial intelligence, volume 33, pages 8957‚Äì8964..jason weston, sumit chopra, and antoine bordes.
2015. memory networks.
corr, abs/1410.3916..kelvin xu, jimmy ba, ryan kiros, kyunghyun cho,aaron courville, ruslan salakhudinov, rich zemel,and yoshua bengio.
2015. show, attend and tell:neural image caption generation with visual atten-tion.
in international conference on machine learn-ing, pages 2048‚Äì2057..jichuan zeng,.
jing li, yan song, cuiyun gao,michael r lyu, and irwin king.
2018. topic mem-ory networks for short text classiÔ¨Åcation.
in pro-ceedings of the 2018 conference on empirical meth-ods in natural language processing, pages 3120‚Äì3131..5914