taming pre-trained language models with n-gram representationsfor low-resource domain adaptationshizhe diao♦, ruijia xu♦, hongjin su♣, yilei jiang♣yan song♠♥, tong zhang♦♦the hong kong university of science and technology{sdiaoaa, rxuaq, tongzhang}@ust.hk♣the chinese university of hong kong♠the chinese university of hong kong (shenzhen)♥shenzhen research institute of big datasongyan@cuhk.edu.cn.
abstract.
large pre-trained models such as bert areknown to improve different downstream nlptasks, even when such a model is trained ona generic domain.
moreover, recent studieshave shown that when large domain-speciﬁccorpora are available, continued pre-trainingon domain-speciﬁc data can further improvethe performance of in-domain tasks.
how-ever, this practice requires signiﬁcant domain-resourcesspeciﬁc data and computationalwhich may not always be available.
in thispaper, we aim to adapt a generic pretrainedmodel with a relatively small amount ofdomain-speciﬁc data.
we demonstrate that byexplicitly incorporating the multi-granularityinformation of unseen and domain-speciﬁcwords via the adaptation of (word based) n-grams, the performance of a generic pretrainedmodel can be greatly improved.
speciﬁcally,we introduce a transformer-based domain-aware n-gram adaptor, t-dna, to effectivelylearn and incorporate the semantic represen-tation of different combinations of words inthe new domain.
experimental results illus-trate the effectiveness of t-dna on eight low-resource downstream tasks from four domains.
we show that t-dna is able to achieve sig-niﬁcant improvements compared to existingmethods on most tasks using limited data withlower computational costs.
moreover, furtheranalyses demonstrate the importance and ef-fectiveness of both unseen words and the in-formation of different granularities.1.
1.introduction.
pre-trained language models have achieved greatsuccess and shown promise in various applica-tion scenarios across natural language understand-ing (devlin et al., 2019; liu et al., 2019; tian et al.,2020a) and generation (lewis et al., 2020; zhang.
1our code is available at https://github.com/.
shizhediao/t-dna..et al., 2020; yang et al., 2020).
normally applyingpre-trained language models to different applica-tions follows a two-stage paradigm: pre-training ona large unlabeled corpus and then ﬁne-tuning on adownstream task dataset.
however, when there aredomain gaps between pre-training and ﬁne-tuningdata, previous studies (beltagy et al., 2019; leeet al., 2020) have observed a performance dropcaused by the incapability of generalization to newdomains.
towards ﬁlling the gaps, the main re-search stream (beltagy et al., 2019; alsentzer et al.,2019; huang et al., 2019; lee et al., 2020) onadapting pre-trained language models starts froma generic model (e.g., bert, roberta) and thencontinues pre-training with similar objectives ona large-scale domain-speciﬁc corpus.
however,without providing sufﬁcient understanding of thereason for the performance drop during the domainshift, it is prone to failure of adaptation.
there-fore, many aspects of continuous pre-training areexpected to be enhanced.
first, although genericpre-trained models offer better initialization forcontinuous pre-training models, it still costs con-siderable time (and money) that are beyond thereach of many institutions.2 second, it is clumsyto pre-train domain-speciﬁc models repeatedly foreach domain on large-scale corpora.3 therefore, itis helpful to have an efﬁcient and ﬂexible methodfor being able to adapt pre-trained language modelsto different domains requiring limited resources..starting from the observed vocabulary mismatchproblem (gururangan et al., 2020), we further showempirically that the domain gap is largely caused bydomain-speciﬁc n-grams.4 motivated by this ﬁnd-.
2for example, biobert (lee et al., 2020), initialized bygeneric bert, was trained on biomedical corpora for 23 dayson eight nvidia v100 gpus..3for example, scibert (beltagy et al., 2019) needs tobe trained from scratch if one wants to use a domain-speciﬁcvocabulary (i.e., scivocab in their paper)..4we explain it in detail in the following section..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3336–3349august1–6,2021.©2021associationforcomputationallinguistics3336ing, we propose a light-weight transformer-baseddomain-aware n-gram adaptor (t-dna) by in-corporating n-gram representations to bridge thedomain gap between source and target vocabulary.
speciﬁcally, the proposed model is able to explic-itly learn and incorporate better representations ofdomain-speciﬁc words and phrases (in the form ofn-grams) by the adaptor networks with only requir-ing small pieces of data.
with this adaptor, onceentering a new domain, one can choose to train theadaptor alone or train it with a transformer-basedbackbone (e.g., bert) together, where the jointtraining paradigm could provide more improve-ment.
in addition, although it is designed for a low-resource setting, the adaptor is still able to workwith enough data, which ensures its generalizationability in different scenarios..experimental results demonstrate that t-dnasigniﬁcantly improves domain adaptation perfor-mance based on a generic pre-trained model andoutperforms all baselines on eight classiﬁcationtasks (on eight datasets).
the results conﬁrm thatincorporating domain-speciﬁc n-grams with theproposed t-dna is an effective and efﬁcient solu-tion to domain adaptation, showing that the infor-mation carried by larger text granularity is highlyimportant for language processing across domains.
moreover, further analyses investigate the factorsthat may inﬂuence the performance of our model,such as the amount of available data, the train-ing time cost and efﬁciency, and the granularityof domain-speciﬁc information, revealing the bestway and setting for using the model..2 the motivation.
as observed in gururangan et al.
(2020), the trans-fer gain of domain-speciﬁc pre-training becomesincreasingly signiﬁcant when the source and tar-get domain are vastly dissimilar in terms of thevocabulary overlap.
motivated by this associationbetween transfer gain and vocabulary distribution,we further investigate the shift of words and phrasesacross domains and attempt to alleviate the degra-dation of language models without large domain-speciﬁc corpora..in particular, we start with a roberta-basemodel from the generic domain and then ﬁne-tuneit on the imdb (maas et al., 2011) dataset.
weinvestigate the outputs predicted by the [cls] em-bedding on the imdb development set and dividethem into two categories: correct predictions (true.
figure 1: the proportion of domain-speciﬁc n-gramsin correct predictions and false predictions over 10 dif-ferent random seeds..positive/negative) and false predictions (false pos-itive/false negative).
to examine the vocabularymismatch problem during the domain shift, weextract the top 1k most frequent n-grams5 fromthese two categories respectively.
we identify then-grams not in the top 10k most frequent n-gramsof source data6 as domain-speciﬁc n-grams.
asrevealed in figure 1, a larger proportion of domain-speciﬁc n-grams are captured when the model ismisled to make wrong predictions, which suggeststhat the shifts in semantic meaning for both wordsand phrases might account for the domain shift.
furthermore, we conjecture that the representationsof domain-speciﬁc n-grams are unreliable, whichexacerbates the model degradation.
while moredetails will be presented in §6.3, we brieﬂy men-tion here that the tokens usually improperly attendto other tokens in the sentence but omit the mostimportant words and phrases..in light of this empirical evidence, we are moti-vated to design a framework to not only capture thedomain-speciﬁc n-grams but also reliably embedthem to extrapolate in the novel domain..3 the t-dna.
our approach follows the standard recipe of pre-training and ﬁne-tuning a language model, whichreceives a sentence x = t1t2 · · · ti · · · tt withti indicating the i-th token, and outputs the rep-resentation of each token.
the overall architec-ture of our approach is shown in figure 2.inthe middle, a generic pre-trained encoder, such.
5here we set n to 5.
6we sample a subset from english wikipedia..33371-gram2-gram3-gram4-gram5-gramgranularity405060708090100ratiolabelcorrectfalsefigure 2: the overall architecture of our model..as bert or roberta, provides a representationat the subword-level without any target domainknowledge.
the right-hand side shows the pro-posed t-dna to enhance the backbone pre-trainedencoder, where word based n-grams in x are ex-tracted from a pre-constructed lexicon l, and arerepresented through n-gram attention module.
theleft-hand side shows the n-gram matching matrixand the integrating process of domain-speciﬁc rep-resentation and generic encoding..in this section, we start with a detailed descrip-tion of lexicon construction, then introduce ourn-gram encoding module and how to integrate n-gram encoding with the backbone model to getdomain-aware representation, and end with an il-lustration of two training strategies..3.1 lexicon construction and n-gram.
extraction.
to better represent and incorporate unseen anddomain-speciﬁc n-grams, we ﬁrst need to ﬁnd andextract them.
here we propose to use an unsuper-vised method, pointwise mutual information (pmi),to ﬁnd domain-speciﬁc words and phrases by col-locations and associations between words..given a sentence x = x1x2 · · · xk with kwords, for any two adjacent words (e.g., ¯x, (cid:101)x).
within the sentence, their pmi is calculated by.
p m i(¯x, (cid:101)x) = log.
p(¯x(cid:101)x)p(¯x)p((cid:101)x).
,.
(1).
where p(x) is the probability of an n-gram x. whena high pmi score is detected between the adja-cent ¯x and (cid:101)x, it suggests they are good collocationpairs, because they have a high probability of co-occurrence and are more likely to form an n-gram.
on the contrary, a delimiter is inserted betweenthe two adjacent words if their p m i(¯x, (cid:101)x) is lessthan a threshold σ, i.e., x = x1x2 · · · ¯x/(cid:101)x · · · xk.
as a result, those consecutive words without a de-limiter are identiﬁed as candidate domain-speciﬁcn-grams.
after using pmi to segment each sen-tence in the training set of a target task, we couldselect among candidate n-grams to obtain the ﬁnaln-gram lexicon l, where each n-gram appears witha frequency of at least f ..in light of this lexicon, for each training in-put sentence x = t1t2 · · · ti · · · tt with t tokens,where ti denotes the i-th token of x , we extractthose sub-strings of x that exist in the lexiconto form domain-speciﬁc n-gram sequence s =s1s2, · · · , sj, · · · , sn , with sj indicating the j-thn-gram of x .
at the same time, an n-gram match-ing matrix, m ∈ rt ×n , can be built to record the.
3338tokenembeddinglayerinputsubjectiveeffects,psychomotortaskperformance,andphysiologicalmeasureswere…subsubjectiveeffectsjectiveeffects,psychomotorsubjectivepsychomotorphychomotortask…physiologicalmeasuresphysiologicalpsychomotortaskperformancetask…tokenizationpositional encoding+add & normfeedforwardadd & normmulti-head attention●●+!add & normfeedforwardadd & normmulti-head attention●●n-gramextractionmodulen-gramembeddinglayersubjectivepsychomotorphysiologicalpsychomotortaskperformancesubjectiveeffects……domainlexicon"…!′tokenizationperformance…positions of the extracted domain-speciﬁc n-gramset and its associated tokens, where mij = 1 forti ∈ sj and mij = 0 for ti /∈ sj.
the matchingmatrix is shown in the left hand size of figure 2..3.2 domain-aware representation.
the backbone pre-trained encoder is a transformerarchitecture (vaswani et al., 2017) with l layers, sself-attention heads and h hidden dimensions ini-tialized from any pre-trained encoder (e.g., bertor roberta).
the input sentence is passed throughit, resulting in a generic hidden state hi for eachinput token xi.
to get the domain-aware hiddenrepresentation, the n-gram adaptor network is im-plemented by a transformer encoder with l layers,s self-attention heads and h hidden dimensions.
first, the embeddings of domain-speciﬁc n-gramscould be obtained by an n-gram embedding layerand then they are fed into the n-gram encoder toget a sequence of hidden states g via a multi-headattention mechanism.
the n-gram encoder is ableto model the interactions among all extracted n-grams and dynamically weighs n-grams to empha-size truly useful n-grams and ignores noisy infor-mation.
the combination of the generic representa-tion and domain-speciﬁc n-gram representation arecomputed by.
h(cid:48)i = hi +.
(cid:88).
gi,k,.
k.(2).
where h(cid:48)i is the desired domain-aware representa-tion, and gi,k is the resulting hidden state for thei-th token and the k-th n-gram associated with thistoken according to the matching matrix m. the n-gram encoding process and hidden state integrationis repeated layer-by-layer along with the genericencoder for l layers from the bottom..3.3 training strategies.
several training strategies could be used and weadopt two in our experiments: ﬁne-tuning (ft)and task-adaptive pre-training (tapt).
for ﬁne-tuning, we operate on the hidden state of the specialclassiﬁcation token [cls].
following the traditioncitation, we simply add a fully-connected layeras a classiﬁer on top of the model and obtain theprobabilities via a softmax layer.
the classiﬁer andthe whole model are ﬁne-tuned on the labeled taskdata in the target domain with cross-entropy loss.
to inject unsupervised target domain knowledge,we leverage the task-adaptive pre-training proposed.
in (gururangan et al., 2020) which strips the labelsin downstream task training data and trains themodel on this unlabeled data.
we use the maskedlanguage model (mlm) as our objective and donot include the next sentence prediction (nsp) taskfollowing liu et al.
(2019); lan et al.
(2020)..note that, our model also supports other train-ing strategies such as domain-adaptive pre-training,which proves to be effective in gururangan et al.
(2020).
one can pre-train our model on a far largerdomain corpus (normally beyond 10gb) at the be-ginning, and then do the task-adaptive pre-trainingand ﬁne-tuning.
because our main goal is to adaptour model in a low-resource setting in terms of datasize and time cost, we leave it for future research.7.
4 experiment settings.
in this section, we ﬁrst introduce eight benchmark-ing datasets.
then the baseline models, evaluationmetrics, and implementation details are presentedin the following three subsections, respectively..4.1 datasets.
following gururangan et al.
(2020), we conductour experiments on eight classiﬁcation tasks fromfour domains including biomedical sciences, com-puter science, news and reviews.
the datasets aredescribed as follows.
• chemprot (kringelum et al., 2016), a man-ually annotated chemical–protein interactiondataset extracted from 5,031 abstracts for rela-tion classiﬁcation..• rct (dernoncourt and lee, 2017), which con-tains approximately 200,000 abstracts from pub-lic medicine with the role of each sentenceclearly identiﬁed..• citationintent (jurgens et al., 2018), whichcontains around 2,000 citations annotated fortheir function..• scierc (luan et al., 2018), which consistsof 500 scientiﬁc abstracts annotated for relationclassiﬁcation..• hyperpartisan (kiesel et al., 2019), whichcontains 645 articles from hyperpartisan newswith either extreme left-wing or right-wing stand-point used for partisanship classiﬁcation..• agnews (zhang et al., 2015), consisting of127,600 categorized articles from more than2000 news source for topic classiﬁcation..7we show some analyses and discussion of data size in.
section 6.2..3339biomed.
cs.
news.
reviews.
cp.
rct.
hp.
ag.
am imdb.
domain.
dataset.
train.
dev.
test.
classes.
s#t#o.s#o.t#s#t#s#t#.
4.1k895k4.1k895k2.4k547k3.4k773k13.
1.8k267k180k27.4m30k4.6m30k4.6m5.ci.
1.6k376k1.6k376k11424k13931k6.se.
3.2k619k3.2k619k45589k974187k7.
5161.7m516.
1.1k1.0m115k.
1.1k213k115k.
2.0k2.6m20k1.7m 21.4m 98.9m 25.9m5k5k5k6.6m4.4m929k7.6k25k25k1.4m 21.5m 31.8m2.
64194k65238k2.
4.
2.table 1: the statistics of the eight task datasets in four target domains.
to limit the computational resources andmaintain all datasets on thousand-level, we only take 10% of imdb training set, and 1% of rct, ag and amtraining sets.
o.s# and o.t# refer to the number of sentences and the number of tokens in the original datasets,respectively.
s# denotes the number of sentences and t# is the number of tokens.
cp, ci, se, hp, ag and amdenote chemprot, citationintent, scierc, hyperpartisan,agnews and amazon, respectively..• amazon (mcauley et al., 2015), consisting of145,251 reviews on women’s and men’s cloth-ing & accessories, each representing users’ im-plicit feedback on items with a binary label sig-nifying whether the majority of customers foundthe review helpful..• imdb (maas et al., 2011), 50,000 balancedpositive and negative reviews from the internetmovie database for sentiment classiﬁcation.
to create a low-resource setting, we constrainthe size of all datasets into thousand-level.
to do so,we randomly select a subset for rct, ag, amazon,imdb with the ratio 1%, 1%, 1%, 10%, respec-tively.
the details can be found in table 1..4.2 baselines.
in our experiments, the following two models serveas the main baselines.
• roberta+ft:.
off-the-shelf.
ﬁne-tuned.
roberta-base model for downstream tasks.
• roberta+tapt: task-adaptive pre-trainedon unlabeled task data starting from robertaand then ﬁne-tuned on labeled data..multi-class classiﬁcation setup, micro-f1 is prefer-able if there is class imbalance, which is true forchemprot and rct..4.4.implementation.
we implement the roberta-base architecture andinitialize it with pre-trained weights by hugging-face’s transformers library8.
in order to obtaina fast and warm start for n-gram representations,we utilize fasttext (bojanowski et al., 2017) to ini-tialize n-gram embeddings.
considering the smallamount of data and based on our experience, thenumber of n-gram encoding layers l is set to 1..for unsupervised task-adaptive pre-training(tapt), the batch size is set to 16 and trainingepochs range from 10 to 15. we adopt adam(kingma and ba, 2015) as the optimizer , where thecorresponding learning rates of different datasetscan be found in our code.
the dropout rate is set to0.5. for the task-speciﬁc ﬁne-tuning (ft), we usesimilar hyperparameter settings and the details areelaborated in the appendix.
all the experimentsare implemented on nvidia v100 gpus..4.3 evaluation metrics.
5 experimental results.
following beltagy et al.
(2019), we adopt macro-f1 for citationintent, scierc, hyperpartisan,agnews, amazon, imdb, and micro-f1 forchemprot and rct as evaluation metrics.
macro-f1 will compute the f1 metric independently foreach class and then take the average, whereasmicro-f1 will aggregate the contributions of allin aclasses to compute the average metric..we compare the performance of the robertamodel with and without t-dna on the aforemen-tioned datasets.
in both ﬁne-tuning and task adap-tive pre-training experiments, t-dna shows sig-niﬁcant improvements over the pre-trained genericroberta..8https://github.com/huggingface/transformers.
3340domaincpdataset81.100.70roberta+ft82.660.31+t-dnaroberta+tapt 82.241.3383.890.76+t-dna.
biomed.
cs.
news.
reviews.
rct80.720.4081.520.4182.730.2383.940.27.ci56.745.4764.954.9863.442.3069.732.87.se74.065.2578.612.0077.851.1279.400.48.hp88.151.5192.490.6992.700.7393.911.48.ag88.600.0188.910.0688.840.0189.050.03.am63.040.6963.920.6264.130.2264.360.34.imdb92.290.2392.910.7192.770.2593.130.15.table 2: the overall performance of t-dna and the comparison against existing models on eight target down-stream datasts.
we report average scores across ﬁve random seeds, with standard deviations as subscripts..5.1 fine-tuning.
the results of ﬁne-tuning on eight datasets are re-ported in table 4. in general, the roberta modelwith t-dna outperforms that without t-dna onall datasets, clearly indicating the effectiveness oft-dna by emphasizing multi-granularity infor-mation.
on average, t-dna is able to bring animprovement of performance by around 2.66%..across all eight datasets, it is observed that t-dna achieves the greatest improvement (8.21%)on the citationintent dataset and the least improve-ment on the agnews dataset.
one reasonable ex-planation for different improvements is that thedomain gap between the roberta pre-trainingdomain and the cs domain is the greatest so thatfar more gains could be obtained by an effectiveadaptation strategy.
to conﬁrm this, we follow gu-rurangan et al.
(2020) to characterize the domainsimilarity by analyzing vocabulary overlap and wedraw the same conclustion that roberta’s pre-training domain has a similar vocabulary to newsand reviews, but far more dissimilar vocabulary tobiomed and cs.
in light of this observation, werecognize that the proposed method is more appli-cable when the domain gap is large.
in this sce-nario, the potential of incorporating multi-grainedinformation by domain-speciﬁc n-grams is greatlyexploited to boost the performance of adaptation.
when comparing the improvements over fourdomains, t-dna is able to offer 1.18%, 6.38%,2.33%, 0.75% gains on biomed, cs, news, re-views, respectively.
the improvement on the csdomain is the best while on the reviews domainit is the poorest, which is consistent with previousanalyses across datasets for similar reasons..5.2 task-adaptive pre-training.
in the previous section, we show that t-dna ishelpful in ﬁne-tuning.
additionally, we would liketo explore whether t-dna is complementary tomore training strategies, such as task-adaptive pre-training (tapt).
tapt has been shown useful for.
figure3:(n=0,1,2,3)..effects.
of different granularities.
pre-trained models in previous studies (howardand ruder, 2018; gururangan et al., 2020), by pre-training on the unlabeled task dataset drawn fromthe task distribution.
the experimental results oftwo models with and without t-dna are reportedin the bottom two rows in table 4. from the re-sults, we can clearly see that the model with t-dna achieves better performance on all datasetscompared to the generic roberta model with-out t-dna.
the t-dna helps to improve theperformance by approximately 1.59% on average,which shows that the effectiveness of t-dna doesnot vanish when combined with tapt.
instead,it further leads to a large performance boost forpre-trained models, indicating that t-dna is acomplementary approach, where explicitly model-ing domain-speciﬁc information helps the unsuper-vised learning of representations (i.e., the maskedlanguage model (mlm) pre-training objective)..overall, for both ft and tapt experiments, theresults show that t-dna signiﬁcantly improvesdomain adaptation performance based on a genericpre-trained model.
we attribute this improvementto the essential domain-speciﬁc semantic informa-tion that is carried by n-grams and the valid repre-sentation of n-grams from the t-dna network..6 analyses.
we analyze several aspects of t-dna, includingthe effects of different granularities and the effects.
33410-gram1-gram2-gram3-gramgranularity of n-grams5560657075808590performancecprctcisehpagamimdbtaskw.omodel80.7810%85.2220%87.1050%100% 87.31.rct.
w.82.23↑1.4586.16↑0.9487.69↑0.5987.69↑0.38.
w.o90.1191.7192.1793.75.ag.
w.92.01↑1.9092.14↑0.4392.58↑0.4194.00↑0.25.
am.
imdb.
w.o63.1364.0165.5266.79.w.64.10↑0.9765.12↑1.1166.10↑0.5867.14↑0.35.
w.o92.2992.1193.1394.34.w.92.91↑0.6292.89↑0.7893.32↑0.1994.81↑0.47.
table 3: performance gains of t-dna w.r.t.
different sampling ratios of rct, ag, am and imdb datasets.
w.and w.o indicate whether the model is equipped with t-dna or not.
the uparrow marks where a positive gain isobtained..of data size.
in addition, we examine the attentionmechanism to verify the effects of n-gram repre-sentations during the domain shift.
the details areillustrated in this section..6.1 effects of different granularities.
the lexical unit in roberta is a subword obtainedfrom byte pair encoding (bpe) (sennrich et al.,2016) tokenization, resulting in a smaller tokenspace and more training data for each token.
ourapproach provides coarse-grained information car-ried by the larger lexical units, n-gram..to verify the contribution of larger granularityinformation, we compare the improvement broughtby t-dna with information of different granular-ities, for n from 0 to 3. note that here n meansthat we extract and incorporate all n-grams with alength smaller or equal to n (within a certain granu-larity).
for example, n = 3 means that we includeall unigrams, bigrams and trigrams.
two consis-tent observations could be made.
first, addingonly 1-gram is able to bring improvements over0-gram (i.e., without t-dna) on all eight datasets,as shown in figure 3. as we know, the tokens in thegeneric encoder are at the subword-level and ourunigrams are at the word-level, which can be seenas a combination of subwords.
therefore, the re-sults suggest that adding unseen words through ouradaptor network is effective, which could enhancethe interaction between subwords of the same word,especially for the new words in the target domain..moreover, based on 1-gram, involving largergranularity offer further gains.
comparing 2-gramand 3-gram v.s.
1-gram, the consistent improve-ments of t-dna demonstrate that the potentialboundary information presented by n-grams playsan essential role in learning representations by pro-viding explicit and better guidance..6.2 effects of data size.
in the previous section, we explored the virtueof incorporating multi-grained information underresource-limited settings, where only a small sub-set of speciﬁc datasets can be accessed.
in addition,we are curious whether t-dna could work wellon a larger scale.
to this end, we sample differ-ent ratios (i.e., 10%, 20%, 50%, 100%) of fourdatasets (i.e., rct, agnews, amazon and imdb)and investigate how t-dna performs at differentdata scales.
as shown in table 3, the model witht-dna always outperforms that without t-dnaw.r.t.
any subsets of four datasets.
this demon-strates that models with t-dna could easily adaptto any size of dataset with the help of domain-speciﬁc n-gram information.
however, it is alsonoted that the performance gains of our methoddecayed with the increase of the amount of trainingdata, dropping from 1.24% (proportion=10%) to0.36% (proportion=100%).
it is not surprising be-cause with adequate data, a model is able to learn agood representation with supervised learning with-out the need of prior knowledge.
however, sincesufﬁcient data normally could not be accessed in re-ality, especially labeled data, we argue that t-dnais desirable and necessary for domain adaptation..6.3 visualization of n-gram representations.
to verify the effects of n-gram representations dur-ing the domain shift, we examine the attentionmechanism of roberta and t-dna by plottingthe attention maps and salience maps using thelit tool (tenney et al., 2020).
in the attentionmap of roberta without t-dna, we found thatthe tokens usually improperly attend to other to-kens in the sentence.
for example, in figure 4,“barbie” attributes more attentions to “animated”and “scary” but omits “creepy” and fails to capture“scary as hell” as an integrated phase.
in contrast,when the model is equipped with t-dna, this vari-ant will shift its attention to include “creepy” and.
3342figure 4: the visualization of attention maps and salience maps of roberta and roberta+t-dna.
the upperregion of each row shows the attention map, where thicker lines denote higher attention weights.
the bottom regionillustrates the salience map, where the darker color box denotes the more dominant weights for the prediction..force the model to focus on the informative phrase“scary as hell”.
furthermore, the salience mapof roberta without t-dna suggests that “an-imated” and “scary” dominate its prediction while“creepy” and “scary as hell” are captured by our t-dna, which is consistent with the decision processof human beings..due to the space limitations, more visualizedexamples are not shown here.
however, based onconsiderable empirical evidence, we conclude thatthe unreliable representations of domain-speciﬁcn-grams (words and phrases) might be one of themain causes for model degradation..7 related work.
a large performance drop of pre-trained modelscaused by domain shift has been observed andmany domain-speciﬁc bert models (beltagy et al.,2019; alsentzer et al., 2019; huang et al., 2019;lee et al., 2020) have been introduced to bridge thedomain gap.
for example, scibert (beltagy et al.,2019) is trained on 1.14m scientiﬁc papers fromsemantic scholar corpus (ammar et al., 2018) for7 days on tpu v3-8 machine and biobert (leeet al., 2020) is trained on pubmed abstracts andpmc full text articles for 23 days on eight nvidiav100 gpus.
clinicalbert (alsentzer et al., 2019)is trained on about 2 million notes in the mimic-iii.
v1.4 database (johnson et al., 2016) for 17-18 dayson a single geforce gtx titan x 12 gb gpu.
however, they all incur a huge computational cost,which is not affordable for many university labsor institutions.
this is precisely why we believethat our efﬁcient adaptor is useful to the commu-nity.
although gururangan et al.
(2020) introducedtask-adaptive pre-training (tapt) to save time bytraining on unlabeled downstream task data, wedemonstrate that our plug-in adaptor is faster andmore effective because of the explicit learning strat-egy and efﬁcient model architecture..out of vocabulary (oov) words refer to thosewords that are not in the vocabulary list and havereceived a lot of attention in recent years.
one wayto handle oov words is to simply utilize and learnan “unknown” embedding during training.
anotherway is to add in-domain words into the original vo-cabulary list and learn their representation by pre-training from scratch (beltagy et al., 2019; gu et al.,2020), which requires substantial resources andtraining data.
moreover, scibert (beltagy et al.,2019) found that in-domain vocabulary is helpfulbut not signiﬁcant while we attribute it to the inefﬁ-ciency of implicit learning of in-domain vocabulary.
to represent oov words in multilingual settings,the mixture mapping method (wang et al., 2019)utilized a mixture of english subwords embedding,but it has been shown useless for domain-speciﬁc.
3343modelattentionmapsandsaliencemapspredictionlabelrobertapositivenegativeroberta+t-dnanegativenegativethatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernowthatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernow..thatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernow.thatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernowthatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernow..thatcreepyanimatedbarbieisscaryashell!iwanttostoptalkingabouthernow.
words by tai et al.
(2020).
exbert (tai et al.,2020) applied an extension module to adapt an aug-menting embedding for the in-domain vocabularybut it still needs large continuous pre-training.
sim-ilar to our work, they highlight the importance ofthe domain-speciﬁc words but all of these work nei-ther explore the understanding of performance dropduring a domain shift nor examine the importanceof multi-grained information.
large granularitycontextual information carried by spans or n-gramshas proven to be helpful to enhance text representa-tion for chinese (song et al., 2009; song and xia,2012; ouyang et al., 2017; kim et al., 2018; penget al., 2018; higashiyama et al., 2019; tian et al.,2020e,b; li et al., 2020; diao et al., 2020; songet al., 2021) and english (joshi et al., 2020; xiaoet al., 2020; tian et al., 2020c,d).
in addition to textencoders on pre-training, the knn-lm (khandel-wal et al., 2019) proposes to augment the languagemodel for effective domain adaptation, by varyingthe nearest neighbor datastore of similar contextswithout further training.
however, all of the previ-ous studies focused on either general pre-trainingprocedures or different tasks (e.g., language model-ing), and did not explore the effectiveness of multi-grained information for domain adaptation.
wehence view them as orthogonal to our work..8 conclusion.
in this work, we ﬁrst reveal a novel discovery be-hind the performance drop during a domain shift,demonstrating that an unreliable representation ofdomain-speciﬁc n-grams causes the failure of adap-tation.
to this end, we propose an innovativeadaptor network for generic pre-trained encoders,supporting many training strategies such as task-adaptive pre-training and ﬁne-tuning, both leadingto signiﬁcant improvements to eight classiﬁcationdatasets from four domains (biomedical, computerscience, news and reviews).
our method is easyto implement, simple but effective, implying thatexplicitly representing and incorporating domain-speciﬁc n-grams offer large gains.
in addition, fur-ther analyses consistently demonstrate the impor-tance and effectiveness of both unseen words andthe information carried by coarse-grained n-grams..acknowledgments.
for their great support.
y. song was supported bynsfc under the project “the essential algorithmsand technologies for standardized analytics ofclinical texts” (12026610) and shenzhen instituteof artiﬁcial intelligence and robotics for societyunder the project “automatic knowledge enhancednatural language understanding and its applica-tions” (ac01202101001).
r. xu was supported bythe hong kong phd fellowship scheme (hkpfs)..references.
emily alsentzer, john murphy, william boag, wei-hung weng, di jindi, tristan naumann, andmatthew mcdermott.
2019.publicly availableclinical bert embeddings.
in proceedings of the2nd clinical natural language processing work-shop, pages 72–78..waleed ammar, dirk groeneveld, chandra bhagavat-ula, iz beltagy, miles crawford, doug downey, ja-son dunkelberger, ahmed elgohary, sergey feld-man, vu ha, et al.
2018. construction of the lit-erature graph in semantic scholar.
in proceedingsof the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 3 (industrypapers), pages 84–91..iz beltagy, kyle lo, and arman cohan.
2019. scib-ert: a pretrained language model for scientiﬁcin proceedings of the 2019 conference ontext.
empirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages3606–3611..piotr bojanowski, édouard grave, armand joulin, andtomáš mikolov.
2017. enriching word vectors withsubword information.
transactions of the associa-tion for computational linguistics, 5:135–146..franck dernoncourt and ji young lee.
2017. pubmed200k rct: a dataset for sequential sentence clas-in proceedings ofsiﬁcation in medical abstracts.
the eighth international joint conference on natu-ral language processing (volume 2: short papers),pages 308–313..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186..this work was supported by the general researchfund (grf) of hong kong (no.
16201320).
theauthors also want to thank the sinovation ventures.
shizhe diao, jiaxin bai, yan song, tong zhang, andyonggang wang.
2020. zen: pre-training chinesetext encoder enhanced by n-gram representations.
in proceedings of the 2020 conference on empirical.
3344methods in natural language processing: findings,pages 4729–4740..yu gu, robert tinn, hao cheng, michael lucas,naoto usuyama, xiaodong liu, tristan naumann,jianfeng gao, and hoifung poon.
2020. domain-speciﬁc language model pretraining for biomed-ical natural language processing.
arxiv e-prints,pages arxiv–2007..suchin gururangan, ana marasovi´c,.
swabhaswayamdipta, kyle lo, iz beltagy, doug downey,and noah a smith.
2020. don’t stop pretraining:adapt language models to domains and tasks.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages8342–8360..shohei higashiyama, masao utiyama, eiichiro sumita,masao ideuchi, yoshiaki oida, yohei sakamoto,and isaac okada.
2019. incorporating word atten-tion into character-based word segmentation.
inproceedings of the 2019 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,volume 1 (long and short papers), pages 2699–2709, minneapolis, minnesota..jeremy howard and sebastian ruder.
2018. universallanguage model fine-tuning for text classiﬁcation.
in proceedings of the 56th annual meeting of theassociation for computational linguistics (volume1: long papers), pages 328–339, melbourne, aus-tralia..kexin huang, jaan altosaar, and rajesh ranganath.
2019. clinicalbert: modeling clinical notes andarxiv preprintpredicting hospital readmission.
arxiv:1904.05342..ae johnson, tj pollard, l shen, lw lehman, m feng,m ghassemi, b moody, p szolovits, la celi, andrg mark.
2016. mimic-iii, a freely accessiblecritical care database.
scientiﬁc data, 3:160035–160035..mandar joshi, danqi chen, yinhan liu, daniel s weld,luke zettlemoyer, and omer levy.
2020. spanbert:improving pre-training by representing and predict-ing spans.
transactions of the association for com-putational linguistics, 8:64–77..david jurgens, srijan kumar, raine hoover, dan mc-farland, and dan jurafsky.
2018. measuring theevolution of a scientiﬁc field through citationframes.
transactions of the association for com-putational linguistics, 6:391–406..urvashi khandelwal, omer levy, dan jurafsky, lukezettlemoyer, and mike lewis.
2019. generalizationthrough memorization: nearest neighbor languagein international conference on learningmodels.
representations..johannes kiesel, maria mestre, rishabh shukla, em-manuel vincent, payam adineh, david corney,benno stein, and martin potthast.
2019. semeval-2019 task 4: hyperpartisan news detection.
inproceedings of the 13th international workshop onsemantic evaluation, pages 829–839..geewook kim, kazuki fukui, and hidetoshi shi-modaira.
2018. word-like character n-gram em-bedding.
in proceedings of the 2018 emnlp work-shop w-nut: the 4th workshop on noisy user-generated text, pages 148–152..diederik p. kingma and jimmy ba.
2015. adam: ain interna-.
method for stochastic optimization.
tional conference on learning representations..jens kringelum, sonny kim kjaerulff, søren brunak,ole lund, tudor i oprea, and olivier taboureau.
2016. chemprot-3.0: a global chemical biologydiseases mapping.
database, 2016..zhenzhong lan, mingda chen, sebastian goodman,kevin gimpel, piyush sharma, and radu soricut.
2020. albert: a lite bert for self-supervisedlearning of language representations.
in interna-tional conference on learning representations..jinhyuk lee, wonjin yoon,.
sungdong kim,donghyeon kim, sunkyu kim, chan ho so,and jaewoo kang.
2020. biobert: a pre-trainedbiomedical language representation modelbioinformatics,for biomedical text mining.
36(4):1234–1240..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, transla-in proceedings of thetion, and comprehension.
58th annual meeting of the association for compu-tational linguistics, pages 7871–7880..xiaonan li, hang yan, xipeng qiu, and xuanjinghuang.
2020.flat: chinese ner using flat-lattice transformer.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 6836–6842, online..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019.roberta: a robustly optimized bert pretrain-ing approach.
arxiv preprint arxiv:1907.11692..yi luan, luheng he, mari ostendorf, and hannanehhajishirzi.
2018. multi-task identiﬁcation of en-tities, relations, and coreference for scientiﬁcknowledge graph construction.
in proceedings ofthe 2018 conference on empirical methods in natu-ral language processing, pages 3219–3232..andrew l. maas, raymond e. daly, peter t. pham,dan huang, andrew y. ng, and christopher potts..33452011. learning word vectors for sentiment analy-sis.
in proceedings of the 49th annual meeting ofthe association for computational linguistics: hu-man language technologies, pages 142–150, port-land, oregon, usa.
association for computationallinguistics..julian mcauley, christopher targett, qinfeng shi, andanton van den hengel.
2015. image-based recom-mendations on styles and substitutes.
in proceed-ings of the 38th international acm sigir confer-ence on research and development in information re-trieval, pages 43–52..en ouyang, yuxi li, ling jin, zuofeng li, and xi-aoyan zhang.
2017. exploring n-gram characterpresentation in bidirectional rnn-crf for chineseclinical named entity recognition.
in ceur work-shop proc, volume 1976, pages 37–42..haiyun peng, yukun ma, yang li, and erik cambria.
2018. learning multi-grained aspect target se-quence for chinese sentiment analysis.
knowledge-based systems, 148:167–176..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare wordswith subword units.
in proceedings of the 54th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1715–1725..yan song, chunyu kit, and xiao chen.
2009. translit-eration of name entity via improved statisticaltranslation on character sequences.
in proceedingsof the 2009 named entities workshop: shared taskon transliteration (news 2009), pages 57–60, sun-tec, singapore..yan song and fei xia.
2012. using a goodness mea-surement for domain adaptation: a case study onchinese word segmentation.
in lrec, pages 3853–3860..yan song, tong zhang, yonggang wang, and kai-fulee.
2021. zen 2.0: continue training and adap-tion for n-gram enhanced text encoders.
arxivpreprint arxiv:2105.01279..wen tai, ht kung, xin luna dong, marcus comiter,and chang-fu kuo.
2020. exbert: extending pre-trained models with domain-speciﬁc vocabularyunder constrained training resources.
in proceed-ings of the 2020 conference on empirical methodsin natural language processing: findings, pages1433–1439..ian tenney, james wexler, jasmijn bastings, tolgabolukbasi, andy coenen, sebastian gehrmann,ellen jiang, mahima pushkarna, carey radebaugh,the language inter-emily reif, et al.
2020.pretability tool: extensible, interactive visualiza-tions and analysis for nlp models.
arxiv preprintarxiv:2008.05122..yuanhe tian, yan song, xiang ao, fei xia, xi-aojun quan, tong zhang, and yonggang wang.
2020a.
joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8286–8296..yuanhe tian, yan song, xiang ao, fei xia, xi-aojun quan, tong zhang, and yonggang wang.
2020b.
joint chinese word segmentation and part-of-speech tagging via two-way attentions of auto-analyzed knowledge.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics, pages 8286–8296, online..yuanhe tian, yan song, and fei xia.
2020c.
supertag-ging combinatory categorial grammar with attentivegraph convolutional networks.
in proceedings of the2020 conference on empirical methods in naturallanguage processing (emnlp), pages 6037–6044..yuanhe tian, yan song, fei xia, and tong zhang.
2020d.
improving constituency parsing with spanin findings of the 2020 conference onattention.
empirical methods in natural language process-ing..yuanhe tian, yan song, fei xia, tong zhang, andyonggang wang.
2020e.
improving chinese wordsegmentation with wordhood memory networks.
in proceedings of the 58th annual meeting of theassociation for computational linguistics, pages8274–8285, online..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, łukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, pages 5998–6008..hai wang, dian yu, kai sun, jianshu chen, andimproving pre-trained multilin-dong yu.
2019.gual model with vocabulary expansion.
in proceed-ings of the 23rd conference on computational natu-ral language learning (conll), pages 316–327..dongling xiao, yu-kun li, han zhang, yu sun,hao tian, hua wu, and haifeng wang.
2020.ernie-gram: pre-training with explicitly n-grammasked language modeling for natural languageunderstanding.
arxiv preprint arxiv:2010.12148..ze yang, wei wu, can xu, xinnian liang, jiaqibai, liran wang, wei wang, and zhoujun li.
2020.styledgpt: stylized response generation with pre-in proceedings of thetrained language models.
2020 conference on empirical methods in naturallanguage processing: findings, pages 1548–1559..xiang zhang, junbo zhao, and yann lecun.
2015.character-level convolutional networks for textclassiﬁcation.
advances in neural information pro-cessing systems, 28:649–657..3346yizhe zhang, siqi sun, michel galley, yen-chun chen,chris brockett, xiang gao, jianfeng gao, jingjingliu, and william b dolan.
2020. dialogpt:large-scale generative pre-training for conversa-tional response generation.
in proceedings of the58th annual meeting of the association for compu-tational linguistics: system demonstrations, pages270–278..3347a description of computing infrastructure.
all the experiments are implemented on nvidia v100 gpus with 32gb memory..b run time.
biomeddomaincp rct cidataset374095roberta+ft4039+t-dna93117132roberta+tapt 300114128320+t-dna.
cs.
news.
reviews.
se hp ag am imdb7472234240.
102104389390.
114113392394.
130131402400.
5052285290.table 4: running time per epoch of models, in the unit of second..c validation performance.
biomeddomaincpdataset80.08roberta+ft+t-dna81.17roberta+tapt 81.2782.58+t-dna.
rct81.2182.0080.9883.24.cs.
news.
ci58.0662.9860.1167.89.se75.3379.6277.0880.69.hp93.5091.8193.5093.74.reviewsam imdb93.0492.8392.3893.11.ag88.70 62.5088.64 63.4088.90 64.3089.31 64.27.table 5: the validation performance..d evaluation measures.
we use manual tuning and adopt macro-f1 for citationintent, scierc, hyperpartisan, agnews, amazon,imdb, and micro-f1 for chemprot and rct as evaluation metrics.
macro-f1 will compute the f1 metricindependently for each class and then take the average, whereas micro-f1 will aggregate the contributionsof all classes to compute the average metric.
in a multi-class classiﬁcation setup, micro-f1 is preferable ifthere is class imbalance, which is true for chemprot and rct..e bounds of hyperparameters.
hyperparameternumber of epochspatiencebatch sizelearning ratedropoutclassiﬁcation layerlearning rate optimizeradam epsilonadam betalearning rate optimizer.
assaignment3(ft) or 15(tapt)1[4,8,16,32,64][1e-5,1e-4]0.5[1,2]adam1e-80.9, 0.999adam.
table 6: bounds of hyperparameters..3348f conﬁguration of best model.
hyperparameternumber of epochspatiencebatch sizelearning ratedropoutclassiﬁcation layerlearning rate optimizeradam epsilonadam betalearning rate optimizer.
assaignment3(ft) or 15(tapt)1324e-50.51adam1e-80.9, 0.999adam.
table 7: conﬁguration of the best model..3349