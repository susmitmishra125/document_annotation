neural semi-markov crf for monolingual word alignment.
wuwei lan(cid:70)1, chao jiang(cid:70)2, wei xu21 department of computer science and engineering, ohio state university2 school of interactive computing, georgia institute of technologylan.105@osu.edu {chao.jiang, wei.xu}@cc.gatech.edu.
abstract.
monolingual word alignment is important forstudying ﬁne-grained editing operations (i.e.,deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrasegeneration, text simpliﬁcation, neutralizing bi-ased language, etc.
in this paper, we presenta novel neural semi-markov crf alignmentmodel, which uniﬁes word and phrase align-ments through variable-length spans.
we alsocreate a new benchmark with human annota-tions that cover four different text genres toevaluate monolingual word alignment modelsin more realistic settings.
experimental resultsshow that our proposed model outperformsall previous approaches for monolingual wordalignment as well as a competitive qa-basedbaseline, which was previously only appliedto bilingual data.
our model demonstratesgood generalizability to three out-of-domaindatasets and shows great utility in two down-stream applications: automatic text simpliﬁ-cation and sentence pair classiﬁcation tasks.1.
1.introduction.
it.
monolingual word alignment aims to align wordsor phrases with similar meaning in two sentencesthat are written in the same language.
isuseful for improving the interpretability in nat-ural language understanding tasks, including se-mantic textual similarity (li and srikumar, 2016)and question answering (yao, 2014).
monolin-gual word alignment can also support the anal-ysis of human editing operations (figure 1) andimprove model performance for text-to-text gen-eration tasks, such as text simpliﬁcation (mad-dela et al., 2021) and neutralizing biased language(pryzant et al., 2020).
it has also been shown to behelpful for data augmentation and label projection.
1our code and data will be available at: https://.
github.com/chaojiang06/neural-jacana.
(cid:70)authors contributed equally..figure 1: an example that illustrates monolingual wordalignment (shown as arrows) can support analysis ofhuman editing process and training of text generationmodels (§6.1), such as for simplifying complex sen-tences for children to read..(culkin et al., 2021) when combined with para-phrase generation..one major challenge for automatic alignment isthe need to handle not only alignments betweenwords and linguistic phrases (e.g., a dozen ↔more than 10), but also non-linguistic phrases thatare semantically related given the context (e.g.,tensions ↔ relations being strained in figure 3).
in this paper, we present a novel neural semi-markov crf alignment model, which uniﬁesboth word and phrase alignments though variable-length spans, calculates span-based semantic sim-ilarities, and takes alignment label transitions intoconsideration.
we also create a new manuallyannotated benchmark, multi-genre monolingualword alignment (multimwa), which consists offour datasets across different text genres and islarge enough to support the training of neural-based models (table 1).
it addresses the short-comings of existing datasets for monolingual wordalignment: mtreference (yao, 2014) was an-notated by crowd-sourcing workers and containsmany obvious errors (more details in §4); ists(agirre et al., 2016) and spade/espada (araseand tsujii, 2018, 2020) were annotated based onchunking and parsing results, which may restrictthe granularity and ﬂexibility of the alignments..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6815–6828august1–6,2021.©2021associationforcomputationallinguistics6815with canadian collaborators, lloyd went on to conduct laboratory  simulations of his model.
lloyd performed successful laboratory experiments of his model.deletiondeletionsubstitutioninsertionour experimental results show that the proposedsemi-markov crf model achieves state-of-the-artperformance with higher precision, in comparisonto the previous monolingual word alignment mod-els (yao et al., 2013a,b; sultan et al., 2014), aswell as another very competitive span-based neu-ral model (nagata et al., 2020) that had previouslyonly applied to bilingual data.
our model exceeds90% f1 in the in-domain evaluation and also hasvery good generalizability on three out-of-domaindatasets.
we present a detailed ablation and er-ror analysis to better understand the performancegains.
finally, we demonstrate the utility of mono-lingual word alignment in two downstream appli-cations, namely automatic text simpliﬁcation andsentence pair classiﬁcation..2 related work.
word alignment has a long history and wasﬁrst proposed for statistical machine translation.
the most representative ones are the ibm mod-els(brown et al., 1993), which are a sequence ofunsupervised models with increased complexityand implemented the giza++ toolkit (och andney, 2003).
many more works followed, such asfastalign (dyer et al., 2013).
dyer et al.
(2011)also used a globally normalized log-linear modelfor discriminative word alignment.
bansal et al.
(2011) proposed a hidden semi-markov model tohandle both continuous and noncontinuous phrasealignment.
these statistical methods promotedthe development of monolingual word alignment(maccartney et al., 2008; thadani and mcke-own, 2011; thadani et al., 2012).
yao et al.
(2013a) proposed a crf aligner following (blun-som and cohn, 2006), then extended it to a semi-crf model for phrase-level alignments (yao et al.,2013b).
sultan et al.
(2014) designed a simple sys-tem with heuristic rules based on word similarityand contextual evidence..neural methods have been explored in the pastdecade primarily for bilingual word alignment.
some early attempts (yang et al., 2013; tamuraet al., 2014) did not match the performance ofgiza++, but recent transformer-based modelsstarted to outperform.
garg et al.
(2019) pro-posed a multi-task framework for machine trans-lation and word alignment, while zenkel et al.
(2020) designed an alignment layer on top oftransformer for machine translation.
both canbe trained without word alignment annotations but.
rely on millions of bilingual sentence pairs.
as forsupervised methods, stengel-eskin et al.
(2019)extracted representations from the transformer-based mt system, then used convolutional neu-ral network to incorporate neighboring words foralignment.
nagata et al.
(2020) proposed a spanprediction method and formulated bilingual wordalignment as a squad-style question answeringtask, then solved it by ﬁne-tuning multilingualbert.
we adapt their method to monolingualword alignment as a new state-of-the-art baseline(§5.1).
some monolingual neural models have dif-ferent settings from this work.
ouyang and mcke-own (2019) introduced pointer networks for long,sentence- or clause-level alignments.
arase andtsujii (2017, 2020) utilized constituency parsersfor compositional and non-compositional phrasealignments.
culkin et al.
(2021) considered spanalignment for framenet (baker et al., 1998) anno-tations and treated each span pair as independentprediction..3 neural semi-crf alignment model.
in this section, we ﬁrst describe the problem for-mulation for monolingual word alignment, thenpresent the architecture of our neural semi-crfword alignment model (figure 2)..3.1 problem formulation.
we formulate word alignment as a sequence tag-ging problem following previous works (blunsomand cohn, 2006; yao et al., 2013b).
given a sourcesentence s and a target sentence t of the same lan-guage, the span alignment a consists of a sequenceof tuples (i, j), which indicates that span si in thesource sentence is aligned with span tj in the tar-get sentence.
more speciﬁcally, ai = j meanssource span si is aligned with target span tj.
weconsider all spans up to a maximum length of dwords.
given a source span si of d (d ≤ d) words[swbi+d−1], where bi is the beginningbiword index, its corresponding label ai means ev-ery word within the span si is aligned to the tar-get span tai.
that is, the word-level alignmentsawbi+d−1 have the same value j. webiuse aw to denote the label sequence of alignmentsbetween words and swto denote the bith word inbithe source sentence.
there might be cases wherespan si is not aligned to any words in the targetsentence, then ai = [null].
when d ≥ 2, themarkov property would no longer hold for word-.
bi+1, ..., aw.
bi+1, ..., sw., aw.
, sw.6816figure 2: illustration of our neural semi-crf word alignment model..level alignment labels, but for span-level labels.
that is, ai depends on awbi−1, the position in thetarget sentence where the source span (with endingword index bi − 1) that precedes the current spansi is aligned to.
we therefore design a discrimina-tive model using semi-markov conditional randomﬁelds (sarawagi and cohen, 2005) to segment thesource sentence and ﬁnd the best span alignment,which we present below.
one unique aspect of oursemi-markov crf model is that it utilizes a variedset of labels for each sentence pair..3.2 our model.
the conditional probability of alignment a givena sentence pair s and t is deﬁned as follows:.
p(a|s, t) =.
eψ(a,s,t)a(cid:48)∈a eψ(a(cid:48),s,t).
(cid:80).
where the set a denotes all possible alignmentsbetween the two sentences.
the potential functionψ can be decomposed into:.
consistent with gold labels.
function υ and τ areimplemented as two neural components which wedescribe below..span representation layer.
first, source andtarget sentences are concatenated together and en-coded by the pre-trained spanbert (joshi et al.,2020) model.
the hidden representations in thelast layer of the encoder are extracted for eachwordpiece token, then averaged to form the wordrepresentations.
following previous work (joshiet al., 2020), the span is represented by a self-attention vector computed over the representationsof each word within the span, concatenated withthe transformer output states of two endpoints..span interaction layer.
the semantic similar-ity score between source span si and target span tjis calculated by a 2-layer feed-forward neural net-work ffsim with parametric relu (prelu) (heet al., 2015),2 after applying layer normalizationto each span representation:.
ψ(a, s, t) =.
υ(si,tai) + τ (aw.
bi−1, ai)+.
υ(si, tj) = ffsim([hs.
i ; ht.
j; |hs.
i − ht.
j|; hs.
i ◦ ht.
j]).
(3).
(1).
(2).
(cid:88).
i.cost(a, a∗).
where i denotes the indices of a subset of sourcespans that are involved in the alignment a; a∗represents the gold alignment sequence at span-level.
the potential function ψ consists of threeelements, of which the ﬁrst two compose nega-tive log-likelihood loss: the span interaction func-tion υ, which accounts the similarity between asource span and a target span; the markov tran-sition function τ , which models the transition ofalignment labels between adjacent source spans;the cost is implemented with hamming loss to en-courage the predicted alignment sequence to be.
where [; ] is concatenation and ◦ is element-wisemultiplication.
we use hsj to denote therepresentation of source span si and target spantj, respectively..i and ht.
markov transition layer.
monolingual wordalignment moves along the diagonal direction inmost cases.
to incorporate this intuition, we pro-pose a scoring function to model the transition be-tween the adjacent alignment labels awbi−1 and ai.
the main feature we use is the distance betweenthe beginning index of current target span and the.
2we also compared relu and gelu, and found prelu.
works slightly better..6817[null]stockswallstreetstocksfellsharplywall_street⋯fell_sharply⋯stocks_fell_sharplyslumponwallstreetwallstreetstocksfellsharply[sep]stocksslumponwallstreetpretrained spanbert encoderv(si,tj)span interactionalignment label transitionbidirectional trainingtarget ()tsource ()sas2t=argmaxap(a|s,t)⋯⋯span  representation[null]wallstocksslumponwallstreetstocks_slump⋯wall_street⋯on_wall_streetstreetstocksfellsharplyat2s=argmaxap(a|t,s)end index of the target span that the prior sourcespan is aligned to.
the distance is binned into 1of 13 buckets with the following boundaries [-11,-6, -4, -3, -2, -1, 0, 1, 2, 3, 5, 10], and each bucketis encoded by a 128-dim randomly initialized em-bedding.
it is then transformed into a real-valuescore by a 1-layer feed forward neural network..training and inference.
during training, weminimizes the negative log-likelihood of the goldalignment a∗, and the model is trained from bothdirections (source to target, target to source):.
(cid:80)(s,t,a∗).
−log p(a∗.
s2t|s, t) − log p(a∗.
t2s|t, s).
(4).
s2t and a∗where a∗labels from both directions..t2s represent the gold alignment.
during inference, we use the viterbi algorithmto ﬁnd the optimal alignment.
there are differ-ent strategies to merge the outputs from two di-rections, including intersection, union, grow-diag(koehn, 2009), bidi-avg (nagata et al., 2020), etc.
it can be seen as a hyper-parameter and decidedbased on the dev set.
in this work, we use intersec-tion in our semi-crf model for all experiments..3.3.implementation details.
we implement our modelin pytorch (paszkeet al., 2017).
we use the adam optimizer and setboth the learning rate and weight decay as 1e-5.
we set the maximum span size to 3 for our neu-ral semi-crf model, which can converge within 5epochs.
the neural semi-crf model has ∼2 hourtraining time per epoch for multimwa-mtref,measured on a single geforce gtx 1080 ti gpu..4 a multi-genre benchmark formonolingual word alignment.
in this section, we present the manually annotatedmulti-genre monolingual word alignment (mul-timwa) benchmark that consists of four datasetsof different text genres.
as summarized in table1, our new benchmark is the largest to date andof higher quality compared to existing datasets.
to ists (agirre et al., 2016) andin contrastspade/espada (arase and tsujii, 2018, 2020),our annotation does not rely on external chunkingor parsing that may introduce errors or restrict thegranularity and ﬂexibility.
our benchmark con-tains both token alignments and a signiﬁcant por-tion of phrase alignments as they are semantically.
equivalent as a whole.
our benchmark also con-tains a large portion of semantically similar but notstrictly equivalent sentence pairs, which are com-mon in text-to-text generation tasks and thus im-portant for evaluating the monolingual word align-ment models under this realistic setting..for all four datasets, we closely follow the stan-dard 6-page annotation guideline3 from (callison-burch et al., 2006) and further extend it to improvethe phrase-level annotation consistency (more de-tails in appendix b.1).
we describe each of thefour datasets below..multimwa-mtref.
we create this dataset byannotating 3,998 sentence pairs from the mtref-erence (yao, 2014), which are human referencesused in a machine translation task.
the orig-inal labels in mtreference were annotated bycrowd-sourcing workers on amazon mechani-cal turk following the guideline from (callison-burch et al., 2006).
in an early pilot study, we dis-covered that these crowd-sourced annotations arenoisy and contain many obvious errors.
it onlygets 73.6/96.3/83.4 for precision/recall/f1 on arandom sample of 100 sentence pairs, when com-pared to the labels we manually corrected..to address the lack of reliable annotation, wehire two in-house annotators to correct the originallabels using goldalign4 (gokcen et al., 2016), anannotation tool for monolingual word alignment.
both annotators have linguistic background andextensive nlp annotation experience.
we pro-vide a three-hour training session to the the anno-tators, during which they are asked to align 50 sen-tence pairs and discuss until consensus.
followingprevious work, we calculate the inter-annotatoragreement as 84.2 of f1 score for token-level non-identical alignments by comparing one annotator’sannotation against the other’s.
the alignments be-tween identical words are usually easy for humanannotators.
after merging the the labels from bothannotators, we create a new split of 2398/800/800for train/dev/test set.
to ensure the quality, an ad-judicator further exams the dev and test sets andconstructs the ﬁnal labels..multimwa-newsela.
newsela(xuet al., 2015b) consists of 1,932 english newsarticles and their simpliﬁed versions written by.
corpus.
3http://www.cs.jhu.edu/˜ccb/.
publications/paraphrase_guidelines.pdf.
4https://github.com/ajdagokcen/.
goldalign-repo.
6818datasets.
#train #dev #test length %aligned %word/phrase %id/non-id.
genre.
external license.
existing monolingual word alignment datesets.
msr rte (brockett, 2007)edinburgh++ (thadani et al., 2012).
800714.ists (agirre et al., 2016).
1,506.
––.
–.
800 29 / 11306 22 / 22.
750.
9 / 9.spade / espada† (2018; 2020).
1,916.
50.
151 23 / 23.
90.0 / 10.077.7 / 22.3.
76.6 / 23.467.2 / 32.8.
6.5 / 93.5.
23.3 / 76.7.
44.0 / 56.0.
72.3 / 27.7.misc.
misc.
newsimage captionsnews.
chunking free.
parsing.
ldc.
our multi-genre monolingual word alignment (multimwa) benchmark.
multimwa-mtrefmultimwa-wikimultimwa-newselamultimwa-arxivtotal.
2,3982,514––.
800800 22 / 17533 1,052 30 / 29500 27 / 23–200 29 / 28–4,912 1,333 2,552 26 / 23.
62.0 / 38.095.6 / 4.474.6 / 25.496.6 / 3.479.3 / 20.7.
52.6 / 47.394.1 / 5.967.1 / 32.993.4 / 6.673.8 / 26.2.newswikipedianewsscientiﬁc writingall above.
––.
–––––.
freefree.
freefreefree∗freefree.
37.985.7.
74.0.
81.9.
88.691.876.587.889.4.table 1: statistics of our new multiwma benchmark and existing datasets.
length of the longer/shorter sentencein each pair is measured by the number of tokens.
%aligned is the percentage of aligned words among all words.
%word/phrase denotes the percentage of word alignment and phrasal alignment.
%id/non-id speciﬁes the per-centage of identical (e.g., lloyd ↔ lloyd) and non-identical (e.g., conduct ↔ performed) alignments.
externelindicates whether the annotation relies on additional linguistic information.
†espada (train) has not been releasedat the time of writing; statistics are based on the spade (dev/test) dataset.
∗newsela data is free for academicresearch but license needs to be requested at: https://newsela.com/data..it has been widely usedprofessional editors.
in text simpliﬁcation research (xu et al., 2016;zhang and lapata, 2017; zhong et al., 2020).
werandomly select 500 complex-simple sentencepairs from the test set of newsela-auto (jianget al., 2020),5 which is the newest sentence-aligned version of newsela.
214 of these 500pairs contain sentence splitting.
an in-house an-notator6 labels the word alignment by correctingthe outputs from giza++ (och and ney, 2003)..multimwa-arxiv.
the arxiv7is an open-access platform that stores more than 1.7 mil-lion research papers with their historical versions.
it has been used to study paraphrase generation(dong et al., 2021) and statement strength (tanand lee, 2014).
we ﬁrst download the latexsource code for 750 randomly sampled papersand their historical versions, then use opendetex8package to extract plain text from them.
we usethe trained neural crf sentence alignment model(jiang et al., 2020) to align sentences between dif-ferent versions of the papers and sample 200 non-identical aligned sentence pairs for further annota-tion.
the word alignment is annotated in a similarprocedure to that of the multimwa-wiki..multimwa-wiki.
wikipedia has been widelyused in text-to-text tasks, including text simpli-.
5more speciﬁcally, we sample from the exact test set used.
in table 2 in maddela et al.
(2021)..6this annotator has annotated multimwa-mtref.
7https://arxiv.org/8https://github.com/pkubowicz/.
opendetex.
ﬁcation (jiang et al., 2020), sentence splitting(botha et al., 2018), and neutralizing bias lan-guage (pryzant et al., 2020).
we follow themethod in (pryzant et al., 2020) to extract par-allel sentences from wikipedia revision historydump (dated 01/01/2021) and randomly sample4,099 sentence pairs for further annotation.
weﬁrst use an earlier version of our neural semi-crfword aligner (§3) to automatically align words forthe sentence pairs, then ask two in-house anno-tators to correct the aligner’s outputs.
the inter-annotator agreement is 98.1 at token-level mea-sured by f1.9 we split the data into 2514/533/1052sentence pairs for train/dev/test sets..5 experiments.
in this section, we present both in-domain and out-of-domain evaluations for different word align-ment models on our multiwma benchmark.
wealso provide a detailed error analysis of our neuralsemi-crf model and an ablation study to analyzethe importance of each component..5.1 baselines.
we introduce a novel state-of-the-art baseline byadapting the qa-based method in (nagata et al.,2020), which has not previously applied to mono-lingual word alignment but only bilingual wordalignment.
this method treats the word alignmentproblem as a collection of independent predictions.
9the inter-annotator agreement is much higher comparedto that of multimwa-mtref, as the parallel sentences ex-tracted from wikipedia revision history have more overlap..6819models.
jacanatoken (yao et al., 2013a).
jacanaphrase (yao et al., 2013b).
pipelinealigner (sultan et al., 2014).
qa-based aligner.
neural crf aligner.
neural semi-crf aligner.
multimwa-mtrefsure multimwa-mtrefsure+p ossppi / pn.
em p.f1f1 i / f1n.
f1f1i / f1n.
rri / rn.
rri / rn.
pi / pn.
em.
multimwa-wiki.
ppi / pn.
rri / rn.
f1f1i / f1 n.87.994.4 / 65.184.494.1 / 58.596.098.1 / 78.9.
88.498.2 / 76.387.697.3 / 74.290.698.9 / 78.9.
72.294.7 / 41.372.495.3 / 40.767.793.3 / 30.6.
92.399.2 / 83.991.699.5 / 82.290.398.9 / 79.1.
79.394.6 / 50.578.094.7 / 48.079.495.6 / 44.1.
90.398.7 / 79.989.598.4 / 78.090.598.9 / 79.0.
2.6.
1.9.
2.5.
82.893.3 / 61.782.893.3 / 61.497.198.3 / 82.9.
98.5 / 84.1.
14.0 91.310.8 91.514.1 94.7.
98.5 / 83.4.
99.3 / 89.1.
70.596.7 / 43.670.096.2 / 42.560.892.9 / 23.9.
92.999.2 / 86.990.299.2 / 82.190.298.7 / 82.3.
76.295.0 / 51.175.894.8 / 50.374.895.5 / 37.1.
92.198.9 / 85.590.898.8 / 82.792.499.0 / 85.5.
1.3.
1.4.
1.0.
21.3.
16.9.
23.3.
98.899.3 / 77.192.898.5 / 44.499.599.6 / 66.2.
97.499.5 / 82.396.599.3 / 80.697.799.6 / 82.8.
95.799.5 / 71.697.099.8 / 49.194.999.6 / 60.0.
97.999.8 / 81.997.699.6 / 80.697.599.7 / 80.8.
97.299.4 / 74.394.999.2 / 46.697.199.6 / 62.9.
97.699.7 / 82.197.199.4 / 80.697.6.
99.7 / 81.8∗ 68.5.em.
59.8.
27.4.
53.4.
67.4.
63.5.table 2:in-domain evaluation of different monolingual word alignment models on the multimwa benchmark.
we report the precision (p), recall (r), f1, and exact match (em), which is the percentage of sentence pairs forwhich model predictions are exactly same as gold labels for the entire sentence.
for each metric, we also reportthe performance on identical alignments (pi, ri, f1i) and non-identical alignments (pn, rn, f1n) separately.
∗multimwa-wiki contains only about 5% non-identical alignment..models.
jacanatoken (yao et al., 2013a).
jacanaphrase (yao et al., 2013b).
pipelinealigner (sultan et al., 2014).
qa-based aligner.
neural crf aligner.
neural semi-crf aligner.
multimwa-newselappi / pn.
f1f1 i / f1n.
rri / rn.
85.591.2 / 60.184.391.3 / 53.995.296.9 / 64.4.
84.895.3 / 69.488.295.3 / 72.389.496.7 / 76.1.
74.997.5 / 39.775.097.4 / 38.669.495.3 / 25.4.
87.999.1 / 71.485.099.0 / 66.385.098.4 / 66.5.
79.894.3 / 47.979.494.3 / 45.080.396.1 / 36.5.
86.297.1 / 70.486.697.1 / 69.187.297.6 / 71.0.multimwa-arxiv.
multimwa-wiki.
em p.pi / pn11.0 94.997.3 / 72.690.997.1 / 53.2.
8.210.0 98.5.
98.8 / 68.3.
98.0 / 70.7.
16.2 93.915.6 92.921.6 96.2.
96.4 / 62.9.
98.9 / 79.3.rri / rn.
f1f1i / f1n.
96.899.5 / 73.496.699.1 / 64.794.699.0 / 62.4.
94.395.0 / 79.998.799.8 / 73.398.499.6 / 83.0.
95.898.4 / 73.093.798.1 / 58.496.598.9 / 65.2.
94.196.5 / 75.095.798.0 / 67.797.399.3 / 81.1.em p.pi / pn49.0 94.731.5 92.949.0 99.5.
98.5 / 44.9.
98.4 / 51.2.
99.6 / 66.2.
99.3 / 76.2.
27.0 96.143.5 96.162.5 97.2.
99.1 / 70.5.
99.6 / 80.4.rri / rn.
f1f1i / f1 n.96.999.9 / 50.196.999.8 / 49.694.999.6 / 60.0.
98.299.8 / 78.398.099.9 / 71.997.699.5 / 79.5.
95.899.2 / 50.694.999.1 / 47.197.199.6 / 62.9.
97.299.5 / 77.397.094.5 / 71.297.499.5 / 79.9.em.
33.3.
28.0.
53.4.
57.8.
52.1.
64.8.table 3: out-of-domain evaluation of different monolingual word alignment models on the multimwa bench-mark.
all the models in this table are trained on the multimwa-mtrefsure+p oss dataset..from every token in the source sentence to a spanin the target sentence, which is then solved by ﬁne-tuning multilingual bert (devlin et al., 2019)similarly as for squad-style question answeringtask.
taking the sentence pair in figure 1 as anexample, the word to be aligned is marked by ¶in the source sentence and concatenated with theentire target sentence to form the input as “withcanadian · · · ¶conduct¶ · · · his model.
lkoydperformed · · · his model.
”.
a span predictionmodel based on ﬁne-tuning multilingual bert isthen expected to extract performed from the tar-get sentence.
the predictions from both directions(source to target, target to source) are symmetrizedto produce the ﬁnal alignment, using a probabilitythreshold of 0.4 instead of the typical 0.5..we change to use standard bert in this modelfor monolingual alignment and ﬁnd that the 0.4threshold chosen by nagata et al.
(2020) is al-most optimalin maximizing the f1 score onour multimwa-mtref dataset.
this qa-basedmethod alone outperforms all existing models formonolingual word alignment, including: jacana-.
token aligner (yao et al., 2013a), which is acrf model using hand-crafted features and exter-nal resources; jacanaphrase aligner (yao et al.,2013b), which is a semi-crf model relying onfeature templates and external resources; pipelin-ealigner (sultan et al., 2014), which is a pipelinesystem that utilizes word similarity and contextualinformation with heuristic algorithms.
we alsocreate a variation of our model, a neural crfaligner, in which all modules remain the same butthe max span length is set to 1, to evaluate the ben-eﬁts of span-based alignments..5.2 experimental results.
following the literature (thadani et al., 2012;yao et al., 2013a,b), we present results underboth sure and sure + p oss settings for themultimwa-mtref dataset.
sure + p oss settingincludes all the annotated alignments, and sureonly contains a subset of them which are agreed bymultiple annotators.
we consider sure + p oss asthe default setting for all the other three datasets.
the in-domain evaluation results are shown in.
6820neural semi-crf aligner.
f1.
em ∆f1 /∆em.
w/ spanbert.
w/ bert.
92.1 23.3.
0.0 / 0.0.
90.8 18.9 -1.3 / -4.4.w/o transition layer.
91.9 21.3 -0.2 / -2.0.w/ post-processing.
92.1 23.3.
0.0 / 0.0.w/ intersection.
w/ union.
w/ grow-diag.
92.0 21.8.
-0.1/ -1.5.
91.1 20.1 -1.0 / -3.2.
91.5 20.6 -0.6 / -2.7.table 4: ablation study of our neural semi-crfaligner with each component removed or swapped.
theresults are based on the dev set of mtrefsure+p oss..table 2. the neural models are working remark-ably well in comparison to the non-neural meth-ods, especially as measured by exact matches(em).
on both mtref and wiki datasets, ourneural semi-crf model achieves the best f1 andem.
qa-based aligner also has competitive per-formance with strong recall, however, its precisionis lower compared to our model.
it is worthy tonote that our model has a modular design, and canbe more easily adjusted than qa-based method tosuit different datasets and downstream tasks..table 3 presents the out-of-domain evaluationresults.
our neural models achieve the best perfor-mance across all three datasets.
this demonstratesthe generalization ability of our model, which canbe useful in the downstream applications..5.3 ablation study.
table 4 shows the ablation study for our neuralsemi-crf model.
f1 and em drops by 1.3 and4.4 points respectively after replacing spanbertwith bert, indicating the importance of opti-mized pre-trained representations.
markov transi-tion layer contributes mainly to the alignment ac-curacy (em).
we have experimented with differ-ent strategies to merge the outputs from two direc-tions:intersection yields better precision, grow-diag and union bias towards recall.
leverag-ing the span interaction matrix generated by ourmodel (details in §3.2), we design a simple post-processing rule to extend the phrasal alignmentto spans that are longer than 3 tokens.
adja-cent target words are gradually included if theyhave very high semantic similarity with the samesource span.
this rule further improves recall andachieves the best f1 on the multimwa-mtref..5.4 error analysis.
we sample 50 sentence pairs from the dev set ofmultimwa-mtref and analyze the errors undersure+poss setup.10 figure 4 shows how the per-formance of different alignment models would im-prove, if we resolve each of the 7 types of errors.
we discuss the categorization of errors and theirbreakdown percentages below:.
phrase boundary (58.6%).
the phrase bound-ary error (see 3 in figure 3 for an example)is the most prominent error in all models, at-tributing 7.6 points of f1 for jacanaphrase, 5.7for qa aligner, and 4.7 for neural semi-crfaligner.
for another example, instead of 3x2 align-ment funds for research ↔ research funding, ourfunds ↔model captures two 1x1 alignments,funding and research ↔ research.
this is largelydue to the fact that alignments are not limitedto linguistic phrases (e.g., noun phrases, verbphrases, etc.
), but rather, include non-linguisticphrases.
it could also be challenging to han-dle longer spans, such as keep his position ↔protect himself from being removed (more on thisin appendix b.2).
although we use spanbertfor better phrase representation, there is still roomfor improvement..function words (19.1%).
function words canbe tricky to align when rewording and reorderinghappens, such as 2 .
adding on the complexity,same function word may appear more than oncein one sentence.
this type of error is common inall the models we experiment with.
it attributes 4.7points of f1 for jacanaphrase, 1.3 for qa aligner,and 1.5 for our neural semi-crf aligner..content words (14.2%).
similar to functionwords, content words (e.g., security bureau ↔defense ministry) can also be falsely aligned ormissed, but the difference between neural and non-neural model is much more signiﬁcant.
this errortype attributes 7.7 points of f1 score for jacanaaligner, but only 1.1 and 0.8 for neural semi-crfaligner and qa aligner, respectively..context implication (5.6%).
some words orphrases that are not strictly semantically equiva-lent can also be aligned if they appear in a similarcontext.
for example, given the source sentence.
10the strict sure only labels exclude many alignments thatare critical for certain applications, such as label projection.
we thus focus on the sure+poss labels for error analysis..68211.
2.
3.fang’sbeingdetainedoncemadechina-usrelationsbeingstrained..t.a.o.r.r.f.h.e.e.f.f.h.a.u.a.te.c.te.a.s.m.u.m.n.g.st.b.c.a.n.et.p.s.o.e.sio.w.n.d.hin.e.a.r.d.a.n.e.s.n.th.u.e.n...state.it.
e.s.d.in.g.ril.
y.figure 3: error examples of the semi-crf word align-ment model on mtref data.
black-ﬁlled boxes denotetrue positives, boxes ﬁlled with blue diagonal lines arefalse negatives, and red slant lines are false positives..‘gaza international airport was put into operationthe day before’ and the target sentence ‘the air-port began operations one day before’, the phrasepair was put into ↔ began can be aligned.
thistype is related to 2.8 f1 score improvement for ja-cana aligner, but only 0.4 and 0.2 for neural semi-crf and qa-based aligners, respectively..debatable labels (1.9%).
word alignment an-notation can be subjective sometimes.
take phrasealignment two days of ↔ a two-day for example,it can go either way to include the function word‘a’ in the alignment, or not..name variations (0.6%).
while our neuralsemi-crf model is designed to handle spellingvariations or name abbreviations, it fails some-times as shown by 1 in figure 3 as an example.
some cases can be very difﬁcult, such as saws ↔the state’s supervision and control bureau of safeproduction, where saws stands for state adminis-tration of work safety..skip alignment (0.0%).
non-contiguous to-kens can be aligned to the same target token orphrase (e.g., owes ... to ↔ is a result of), posing achallenging situation for monolingual word align-ers.
however, this error is rare, as only 0.6% of allalignments in mtref dev set are discontinuous..6 downstream applications.
in this section, we apply our monolingual wordaligner to some downstream applications, includ-ing both generation and understanding tasks..figure 4: performance comparison on mtref dev setfor 3 different aligners after resolving each error type..6.1 automatic text simpliﬁcation.
text simpliﬁcation aims to improve the readabilityof text by rewriting complex sentences with sim-pler language.
we propose to incorporate wordalignment information into the state-of-the-art ed-itnts model (dong et al., 2019) to explicitly learnthe edit operations, including addition, deletionand paraphrase.
the editnts model uses a neu-ral programmer-interpreter architecture, which de-rives the add, keep and delete operation se-quence based on the edit-distance measurementsduring training time.
we instead construct this editsequence based on the neural semi-crf aligner’soutputs (trained on mtrefsure+p oss) with an ad-ditional replace tag to train the editnts model(more details in appendix a)..table 5 presents the text simpliﬁcation resultson two benchmark datasets, newsela-auto andwikipedia-auto (jiang et al., 2020), where we im-prove the sari score (xu et al., 2016) by 0.9and 0.6, respectively.
the sari score averagesthe f1/precision of n-grams inserted (add), kept(keep) and deleted (del) when compared to hu-man references.
we also calculate the bleu scorewith respect to the input (s-bl), the percentage ofnew words (%new) added, and the percentage ofsystem outputs being identical to the input (%eq)to show the paraphrasing capability.
we manu-ally inspect 50 sentences sampled from newsela-auto test set and ﬁnd that both models (editntsand editnts+aligner) generate the same outputfor 10 sentences.
for the remaining 40 sentences,the original editnts only attempts to paraphrase4 times (2 are good).
our modiﬁed model (edit-nts+aligner) is more aggressive, generating 25paraphrases (11 are good).
with the help of wordaligner, the modiﬁed model also produces a highernumber of good deletions (20 vs. 13) and a lowernumber of bad deletions (6 vs. 12), which is con-sistent with the better keep and del scores..6822datasets.
newsela-auto.
wikipedia-auto.
0.0.sari add keep del fk slen olen cr %split s-bl %new %eqmodels100.024.835.511.8complex (input)0.0 12.3 24.80.013.386.9 84.7 78.4 97.6 6.5simple (reference)13.31.214.332.9 75.7 7.5editnts36.614.333.4 77.9 7.2editnts + aligner 37.50.814.314.3100.022.60.0 13.4 22.674.624.9complex (input)16.221.781.7 66.2 97.5 81.5 12.2 21.7simple (reference)0.623.668.4 39.8 12.8 23.6editnts36.869.5 40.9 12.7 23.6editnts + aligner 37.42.823.6.
100.025.750.249.1100.064.069.774.4.
1.00.630.660.661.00.971.061.05.
0.033.56.57.60.014.812.410.2.
2.00.82.41.50.85.41.70.6.
1.11.30.0.
2.11.9.table 5: downstream application on text simpliﬁcation.
by incorporating our monolingual word aligner into theeditnts (dong et al., 2019) model, we improve the performance measured by sari score (the main automaticmetric for simpliﬁcation) and its three parts: precision for delete (del), f1 scores for add and keep operations..models.
rte mrpc sts-b sts14 wikiqa sick pit8k2.5k11kracc83.665.3bert83.7bert + aligner 67.3.snli549kmap/mrr acc max f1 max f1 map/mrr acc acc m/acc mm acc90.581.8/83.083.2/84.490.4.
5.7kr/ρ86.7/85.886.8/86.0.
trecqa qqp363k.
84.4/89.685.1/87.8.
3.5kf188.288.9.
84.8/83.184.8/83.5.
mnli392k.
url42k.
78.778.5.
75.075.5.
86.287.2.
90.890.9.
10k.
53k.
8k.
table 6: downstream applications on natural language inference (rte, sick, mnli, snli), paraphrase identiﬁ-cation (mrpc, pit, url, qqp), question answering (wikiqa, trecqa), and semantic textual similarity (sts-b,sts14) tasks.
the datasets in this table are ordered by the size of their training set, as shown in the second row..6.2 sentence pair modeling.
7 conclusion.
we can utilize our neural aligner in sentence pairclassiﬁcation tasks (lan and xu, 2018), addingconditional alignment probability p(a|s, t) as anextra feature.
we concatenate it with the [cls]representation in ﬁne-tuned bert and apply thesoftmax layer for prediction.
we experiment withon different datsets for various tasks, including:natural language inference on snli (bowmanet al., 2015), mnli (williams et al., 2018), sick(marelli et al., 2014), and rte (giampiccoloet al., 2007) from the glue benchmark (wanget al., 2018); semantic textual similarity on sts-b(cer et al., 2017) and sts14 (agirre et al., 2014);question answering on wikiqa (yang et al., 2015)and trecqa (wang et al., 2007); paraphrase iden-tiﬁcation on mrpc (dolan and brockett, 2005),url (lan et al., 2017), pit (xu et al., 2015a),and qqp (iyer et al., 2017)..we implement the ﬁne-tuned bertbase modelusing huggingface’s library (wolf et al., 2019).
table 6 shows performance improvement on small(2k-15k) datasets, which include sick, sts-b,mrpc, rte, wikiqa, and pit, but little or no im-provement on large (40k-550k) datasets, such assnli, mnli, and qqp.
we hypothesize that thetransformer model can potentially learn the latentword alignment through self-attentions, but not aseffectively for small data size..in this work, we present the ﬁrst neural semi-crf word alignment model which achieves com-petitive performance on both in-domain and out-of-domain evaluations.
we also create a man-ually annotated multi-genre monolingual wordalignment (multimwa) benchmark which is thelargest and of higher quality compared to existingdatasets..acknowledgement.
we thank yang chen, sarthak garg, and anony-mous reviewers for their helpful comments.
wealso thank sarah flanagan, yang zhong, pa-nya bhinder, kenneth kannampully for help-ing with data annotation.
this research is sup-ported in part by the nsf awards iis-2055699,odni and iarpa via the better program con-tract 19051600004, aro and darpa via thesocialsim program contract w911nf-17-c-0095,and criteo faculty research award to wei xu.
the views and conclusions contained herein arethose of the authors and should not be inter-preted as necessarily representing the ofﬁcial poli-cies, either expressed or implied, of nsf, odni,iarpa, aro, darpa or the u.s. government.
the u.s. government is authorized to reproduceand distribute reprints for governmental purposesnotwithstanding any copyright annotation therein..6823references.
eneko agirre, carmen banea, claire cardie, danielcer, mona diab, aitor gonzalez-agirre, weiweiguo, rada mihalcea, german rigau, and janycewiebe.
2014. semeval-2014 task 10: multilingualin proceedings of thesemantic textual similarity.
8th international workshop on semantic evaluation(semeval)..inigoeneko agirre, aitor gonzalez agirre,lopez-gazpio, montserrat maritxalar,ger-man rigau claramunt, and larraitz uria.
2016.semeval-2016 task 2: interpretable semantic textualsimilarity.
in proceedings of the 10th internationalworkshop on semantic evaluation (semeval)..yuki arase and junichi tsujii.
2017. monolingualphrase alignment on parse forests.
in proceedings ofempirical methods in natural language processing(emnlp)..yuki arase and junichi tsujii.
2018. spade: eval-uation dataset for monolingual phrase alignment.
in proceedings of the international conference onlanguage resources and evaluation (lrec)..yuki arase and jun’ichi tsujii.
2020. compositionalin proceedings ofphrase alignment and beyond.
empirical methods in natural language processing(emnlp)..collin f baker, charles j fillmore, and john b lowe.
1998. the berkeley framenet project.
in 36th an-nual meeting of the association for computationallinguistics (acl) and 17th international confer-ence on computational linguistics (coling)..mohit bansal, chris quirk, and robert moore.
2011.gappy phrasal alignment by agreement.
in proceed-ings of the association for computational linguis-tics (acl)..phil blunsom and trevor cohn.
2006. discrimina-tive word alignment with conditional random ﬁelds.
in proceedings of the 21st international conferenceon computational linguistics (coling) and the44th annual meeting of the association for compu-tational linguistics (acl)..jan a. botha, manaal faruqui, john alex, jasonbaldridge, and dipanjan das.
2018. learning toinsplit and rephrase from wikipedia edit history.
proceedings of empirical methods in natural lan-guage processing (emnlp)..samuel r. bowman, gabor angeli, christopher potts,and christopher d. manning.
2015. a large an-notated corpus for learning natural language infer-ence.
in proceedings of empirical methods in nat-ural language processing (emnlp)..chris brockett.
2007. aligning the rte 2006 corpus.
intechnical report msr-tr-2007-77, microsoft re-search..peter f. brown, stephen a. della-pietra, vin-cent j. della-pietra, and robert l. mercer.
1993.the mathematics of statistical machine translation.
computational linguistics (cl)..chris callison-burch, trevor cohn, and mirella la-pata.
2006. annotation guidelines for paraphrasealignment.
technical report..daniel cer, mona diab, eneko agirre, inigo lopez-gazpio, and lucia specia.
2017. semeval-2017task 1: semantic textual similarity-multilingual andcross-lingual focused evaluation.
in proceedings ofthe 10th international workshop on semantic eval-uation (semeval)..ryan culkin, j. edward hu, elias stengel-eskin,guanghui qin, and benjamin van durme.
2021. it-erative paraphrastic augmentation with discrimina-tive span alignment.
transactions of the associa-tion for computational linguistics (tacl)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-in proceedings of the conference ofderstanding.
the north american chapter of the association forcomputational linguistics: human language tech-nologies (naacl)..william b dolan and chris brockett.
2005. automati-cally constructing a corpus of sentential paraphrases.
in proceedings of the third international workshopon paraphrasing (iwp)..qingxiu dong, xiaojun wan, and yue cao.
2021.parasci: a large scientiﬁc paraphrase dataset forlonger paraphrase generation.
in proceedings of theeuropean chapter of the association for computa-tional linguistics (eacl)..yue dong, zichao li, mehdi rezagholizadeh, andjackie chi kit cheung.
2019. editnts: an neu-ral programmer-interpreter model for sentence sim-in proceed-pliﬁcation through explicit editing.
ings of the association for computational linguis-tics (acl)..chris dyer, victor chahuneau, and noah a. smith.
2013. a simple, fast, and effective reparameteriza-in proceedings of the con-tion of ibm model 2.ference of the north american chapter of the asso-ciation for computational linguistics: human lan-guage technologies (naacl)..chris dyer, jonathan h. clark, alon lavie, andnoah a. smith.
2011. unsupervised word align-ment with arbitrary features.
in proceedings of theassociation for computational linguistics (acl)..matt gardner, joel grus, mark neumann, oyvindtafjord, pradeep dasigi, nelson f. liu, matthew pe-ters, michael schmitz, and luke zettlemoyer.
2018.allennlp: a deep semantic natural language pro-in proceedings of workshop forcessing platform.
nlp open source software (nlp-oss)..6824sarthak garg, stephan peitz, udhyakumar nallasamy,jointly learning toand matthias paulik.
2019.align and translate with transformer models.
inproceedings of empirical methods in natural lan-guage processing and the international joint con-ference on natural language processing (emnlp-ijcnlp)..danilo giampiccolo, bernardo magnini, ido dagan,and bill dolan.
2007. the third pascal recogniz-ing textual entailment challenge.
in proceedings ofthe acl-pascal workshop on textual entailmentand paraphrasing..ajda gokcen, evan jaffe, johnsey erdmann, michaelwhite, and douglas danforth.
2016. a corpus ofword-aligned asked and anticipated questions in ain proceedingsvirtual patient dialogue system.
of the international conference on language re-sources and evaluation (lrec)..kaiming he, xiangyu zhang, shaoqing ren, and jiansun.
2015. delving deep into rectiﬁers: surpass-ing human-level performance on imagenet classiﬁ-cation.
in proceedings of the international confer-ence on computer vision (iccv)..shankar iyer, nikhil dandekar, and korn´el cser-first quora dataset release: ques-in https://data.quora.com/first-quora-.
nai.
2017.tion pairs.
dataset-release-question-pairs..chao jiang, mounica maddela, wuwei lan, yangzhong, and wei xu.
2020. neural crf model forin pro-sentence alignment in text simpliﬁcation.
ceedings of the association for computational lin-guistics (acl)..mandar joshi, danqi chen, yinhan liu, daniel s.weld, luke zettlemoyer, and omer levy.
2020.spanbert: improving pre-training by representingand predicting spans.
transactions of the associa-tion for computational linguistics (tacl)..philipp koehn.
2009. statistical machine translation..cambridge university press..empirical methods in natural language processing(emnlp)..mounica maddela, fernando alva-manchego, and weixu.
2021. controllable text simpliﬁcation with ex-plicit paraphrasing.
proceedings of the conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies (naacl)..marco marelli, stefano menini, marco baroni, luisabentivogli, raffaella bernardi, and roberto zam-parelli.
2014. a sick cure for the evaluationof compositional distributional semantic models.
in proceedings of the international conference onlanguage resources and evaluation (lrec)..masaaki nagata, katsuki chousa,.
and masaakinishino.
2020.a supervised word alignmentmethod based on cross-language span prediction us-in proceedings of em-ing multilingual bert.
pirical methods in natural language processing(emnlp)..franz josef och and hermann ney.
2003. a systematiccomparison of various statistical alignment models.
computational linguistics (cl)..jessica ouyang and kathy mckeown.
2019. neuralnetwork alignment for sentential paraphrases.
inproceedings of the association for computationallinguistics (acl)..adam paszke, sam gross, soumith chintala, gre-gory chanan, edward yang, zachary devito, zem-ing lin, alban desmaison, luca antiga, and adamlerer.
2017. automatic differentiation in pytorch.
in nips autodiff workshop..reid pryzant, richard diehl martinez, nathan dass,sadao kurohashi, dan jurafsky, and diyi yang.
2020. automatically neutralizing subjective bias intext.
in proceedings of the aaai conference on ar-tiﬁcial intelligence (aaai)..wuwei lan, siyu qiu, hua he, and wei xu.
2017.a continuously growing dataset of sentential para-in proceedings of empirical methods inphrases.
natural language processing (emnlp)..sunita sarawagi and william w cohen.
2005. semi-markov conditional random ﬁelds for informationin proceedings of advances in neuralextraction.
information processing systems (neurips)..wuwei lan and wei xu.
2018. neural network modelsfor paraphrase identiﬁcation, semantic textual simi-larity, natural language inference, and question an-in proceedings of international confer-swering.
ence on computational linguistics (coling)..tao li and vivek srikumar.
2016. exploiting sentencesimilarities for better alignments.
in proceedings ofempirical methods in natural language processing(emnlp)..bill maccartney, michel galley, and christopher dmanning.
2008. a phrase-based alignment modelin proceedings offor natural language inference..elias stengel-eskin, tzu-ray su, matt post, and ben-jamin van durme.
2019. a discriminative neu-ral model for cross-lingual word alignment.
inproceedings of empirical methods in natural lan-guage processing and the international joint con-ference on natural language processing (emnlp-ijcnlp)..md arafat sultan, steven bethard, and tamara sum-ner.
2014. back to basics for monolingual align-ment: exploiting word similarity and contextual ev-idence.
transactions of the association for compu-tational linguistics (tacl)..6825nan yang, shujie liu, mu li, ming zhou, and neng-hai yu.
2013. word alignment modeling with con-in proceed-text dependent deep neural network.
ings of the association for computational linguis-tics (acl)..yi yang, wen-tau yih, and christopher meek.
2015.wikiqa: a challenge dataset for open-domain ques-tion answering.
in proceedings of empirical meth-ods in natural language processing (emnlp)..xuchen yao.
2014. feature-driven question answer-ing with natural language alignment.
ph.d. thesis,johns hopkins university..xuchen yao, benjamin van durme, chris callison-burch, and peter clark.
2013a.
a lightweight andhigh performance monolingual word aligner.
inproceedings of the association for computationallinguistics (acl)..xuchen yao, benjamin van durme, chris callison-semi-markovburch, and peter clark.
2013b.
in proceed-phrase-based monolingual alignment.
ings of empirical methods in natural languageprocessing (emnlp)..thomas zenkel, joern wuebker, and john denero.
2020. end-to-end neural word alignment outper-forms giza++.
in proceedings of the association forcomputational linguistics (acl)..xingxing zhang and mirella lapata.
2017. sentencesimpliﬁcation with deep reinforcement learning.
inproceedings of empirical methods in natural lan-guage processing (emnlp)..yang zhong, chao jiang, wei xu, and junyi jessy li.
2020. discourse level factors for sentence deletionin proceedings of the aaaiin text simpliﬁcation.
conference on artiﬁcial intelligence (aaai)..akihiro tamura, taro watanabe, and eiichiro sumita.
2014. recurrent neural networks for word align-ment model.
in proceedings of the association forcomputational linguistics (acl)..chenhao tan and lillian lee.
2014. a corpus ofsentence-level revisions in academic writing: a steptowards understanding statement strength in com-in proceedings of the association formunication.
computational linguistics (acl)..kapil thadani, scott martin, and michael white.
2012.a joint phrasal and dependency model for para-in proceedings of internationalphrase alignment.
conference on computational linguistics (col-ing)..kapil thadani and kathleen mckeown.
2011. optimaland syntactically-informed decoding for monolin-gual phrase-based alignment.
in proceedings of theassociation for computational linguistics (acl)..alex wang, amanpreet singh, julian michael, fe-lix hill, omer levy, and samuel bowman.
2018.glue: a multi-task benchmark and analysis plat-in pro-form for natural language understanding.
ceedings ofthe 2018 emnlp workshop black-boxnlp: analyzing and interpreting neural net-works for nlp..mengqiu wang, noah a smith, and teruko mitamura.
2007. what is the jeopardy model?
a quasi-in conference onsynchronous grammar for qa.
empirical methods in natural language process-ing and computational natural language learning(emnlp-conll)..adina williams, nikita nangia, and samuel bowman.
2018. a broad-coverage challenge corpus for sen-tence understanding through inference.
in proceed-ings of the conference of the north american chap-ter of the association for computational linguistics:human language technologies (naacl)..thomas wolf, lysandre debut, victor sanh, julienchaumond, clement delangue, anthony moi, pier-ric cistac, tim rault, r´emi louf, morgan fun-towicz, et al.
2019. huggingface’s transformers:state-of-the-art natural language processing.
arxivpreprint arxiv:1910.03771..wei xu, chris callison-burch, and william b. dolan.
2015a.
semeval-2015 task 1: paraphrase and se-mantic similarity in twitter (pit).
in proceedings ofthe 9th international workshop on semantic evalu-ation (semeval)..wei xu, chris callison-burch, and courtney napoles.
2015b.
problems in current text simpliﬁcation re-search: new data can help.
transactions of the as-sociation for computational linguistics (tacl)..wei xu, courtney napoles, ellie pavlick, quanzechen, and chris callison-burch.
2016. optimizingstatistical machine translation for text simpliﬁcation.
transactions of the association for computationallinguistics (tacl)..6826complex sentence:[‘with’,‘canadian’, ‘collaborators,’, ‘lloyd’, ‘went’, ‘on’, ‘to’, ‘conduct’, ‘laboratory’, ‘simulations’, ‘of’, ‘his’,‘model.’]simple sentence:[‘lloyd’, ‘performed’, ‘successful’, ‘laboratory’, ‘experiments’, ‘of’, ‘his’, ‘model.’]expert program from editnts:[del, del, del, keep, add(‘performed’), add(‘successful’), del, del, del, del,add(‘experiments’), del, keep, keep, keep]expert program from editnts with aligner:[del, del, del, keep, add(‘performed’), add(‘successful’), del, del, del, del, ‘keep’, ‘replace-s’,add(‘experiments’), ‘replace-e’, keep, keep, keep].
‘keep’,.
table 7: expert program comparison between the original editnts and our modiﬁed version with word alignmentfor the example in figure 1..0.0.sari add keep del fk slen olen cr %split s-bl %new %eqmodels100.024.811.835.5complex (input)0.013.386.9 84.7 78.4 97.6 6.5simple (reference)1.214.332.9 75.7 7.5editnts (original)36.61.514.132.6 75.9 7.4editnts (original) + aligner 36.633.8 75.8 8.30.716.336.9editnts (new)33.4 77.9 7.237.50.814.3editnts (new) + aligner.
0.0 12.3 24.813.314.314.116.314.3.
100.025.750.249.356.549.1.
1.00.630.660.650.730.66.
0.033.56.56.05.77.6.
2.00.82.42.21.61.5.
1.11.21.21.3.table 8: comparison experiments on newsela-auto dataset with different versions of editnts model.
+ alignermeans using the neural semi-crf aligner output, editnts (new) means adding the replace-s/e tags to theoriginal editnts model..a editnts with aligner.
the original editnts model constructs expertprogram with the shortest edit path from complexsentence to simple sentence, speciﬁcally, it calcu-lates the levenshtein distances without substitu-tions and recovers the edit path with three labels:add, keep and del.
since edit distance relieson word identity to match the sentence pair, it can-not produce lexical paraphrases (e.g.
conduct ↔performed and simulations ↔ experiments in fig-ure 1,).
the ﬁnal edit sequence will mix para-phrase words (performed and experiments) andnormal added words (successful) together underthe same add label.
in order to differentiate thesetwo types of added words, we introduced spe-cial tags (replace-s and replace-e) to re-fer to lexical paraphrases speciﬁcally.
during theedit label construction process, after checking theword pair identity for keep label, we addition-ally check whether they are aligned by our neuralsemi-crf aligner, if so, we produce replace-s/e tags, otherwise we do normal add/del tags.
see table 7 for a speciﬁc example.
word align-ment can arbitrarily align any words in the tar-get sentence, this can break the sequential de-.
pendency of the edit labels, we therefore discardsome lexical paraphrases to guarantee such pro-priety (conduct ↔ performed in table 7)..in order to show the effectiveness of our mod-iﬁed model, we compared two more versions ofeditnts in table 8: editnts (original) + aligner,where we directly add word alignment informa-tion to the original editnts model without anyreplace tags; editnts (new), where we keepthe replace tags but don’t use any word align-ments.
the results show that editnts model withreplace tags can improve the performance, butit is not signiﬁcant.
after adding the word align-ment information, we can further improve thesari score signiﬁcantly, which can demonstratethe effectiveness of our modiﬁed editnts withaligner..b more details for multimwa.
benchmark.
b.1 updated annotation guideline.
after the ﬁrst round of annotation, we discoverythat the deﬁnition of phrasal alignment can be am-biguous, which will hinder the development anderror analysis for word alignemnt models.
there-.
6827• noun1 should be only aligned to noun1 in the.
27.
0.
42.
0.
0.fore, we further extend the standard 6-page an-notation guideline11 from (callison-burch et al.,2006) to cover three linguistics phenomena to im-prove the phrase-level annotation consistency..• “a/an/the + noun” should be aligned together.
with noun if both nouns are same..phrase “noun1 and noun2”..• noun should be only aligned to noun in the.
“adjective + noun” phrase..utilizing the constituency parser implementedin the allennlp package (gardner et al., 2018),we ﬁrst write a script to implement these rulesand apply them to all the training/dev/test setsof multimwa-mtref.
then, we manually gothrough both dev and test sets to further ensure theannotation consistency..b.2 statistics of alignment shape.
we also analyze the shape of alignment in eachdataset, and the statistics can be found in table9. statistical result showes that the dev and testof multimwa-mtref contain a similar portionof phrasal alignment, and less than the trainingset.
there even exists 1×10 alignment annota-tions in multimwa-mtref, which are actuallycorrect based on our manual inspection.
bothmultimwa-newsela and multimwa-arxiv con-tain signiﬁcantly larger portion of 1×1 alignment,especially the latter one contains only 3.2% ofphrasal alignment..6×6.
8×5.
7×5.
5×5.
9×4.
8×4.
6×4.
5×4.
4×4.
9×3.
8×3.
7×3.
6×3.
5×3.
4×3.
3×3.
01×2.
9×2.
8×2.
7×2.
6×2.
5×2.
4×2.
3×2.
2×2.
01×1.
9×1.
8×1.
7×1.
6×1.
5×1.
4×1.
3×1.
2×1.
1×1.fo%.
1×1.sriap#.
27.
08.
07.
52.
27.
23.
081.
671.
72.
42.
36.
891.
063.
216.
945.
02.
81.
61.
48.
402.
072.
639.
6351.
8401.
211.
891.
035.
8461.
3243.
6597.
%05.
95.
17303.
8932.niart.0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
04.
04.
02.
0.
84.
46.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
0.
36.
0.
0.
63.
81.
0.
501.
651.
531.
57.
54.
0.
441.
18.
081.
711.
21.
0.
0.
0.
0.
0.
0.
0.
81.
81.
0.
0.
61.
0.
0.
0.
82.
0.
84.
27.
42.
0.
06.
05.
04.
0.
441.
002.
441.
0.
474.
615.
672.
21.
803.
213.
251.
8.
41.
82.
53.
0.
03.
63.
84.
0.
001.
571.
08.
0.
804.
678.
4342.
%56.
56.
05301.
404.
2901.
0562.
%71.
36.
92301.
081.
384.
838.
%28.
47.
8.
03.
87.
%08.
69.
4648.
0205.
008.
008.
005.
002.ved.tset.fertmawm.-.
itl.um.aleswenawm.-.
itl.um.vixra-awm.itl.um.01.
01.
0.
0.
0.
0.
9.
0.
63.
0.
84.
61.
42.
8.
0.,elpmaxe.rof..
epahs.cﬁicepshti.w.tnemngila.lasarhprofdedulcni.era.stnemngiladrowynamwohstneserper.rebmunhcae..
tesatadhcaeni.sepahs.tnemngila.foscitsitats.:9elbat...wor.eht.fomus.eht.revo.
1×1.yb.detaluclac.si.
1×1.fo%.
.
stnemngila.drowxis.ot.etubirtnoc.lli.w.tnemngila.lasarhp3×2.eno.
11http://www.cs.jhu.edu/˜ccb/.
publications/paraphrase_guidelines.pdf.
6828