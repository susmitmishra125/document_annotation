accelerating text communication via abbreviated sentence input.
jiban adhikary1jiban@mtu.edu.
jamie berger2jamieberger16@gmail.com1michigan technological university, houghton, michigan, usa2washington leadership academy, washington, dc, usa.
keith vertanen1vertanen@mtu.edu.
abstract.
typing every character in a text message mayrequire more time or effort than strictly neces-sary.
skipping spaces or other characters maybe able to speed input and reduce a user’s phys-ical input effort.
this can be particularly im-portant for people with motor impairments.
ina large crowdsourced study, we found workersfrequently abbreviated text by omitting mid-word vowels.
we designed a recognizer opti-mized for expanding noisy abbreviated inputwhere users often omit spaces and mid-wordvowels.
we show using neural language mod-els for selecting conversational-style trainingtext and for rescoring the recognizer’s n-bestsentences improved accuracy.
on noisy touch-screen data collected from hundreds of users,we found accurate abbreviated input was pos-sible even if a third of characters was omitted.
finally, in a study where users had to dwell fora second on each key, sentence abbreviated in-put was competitive with a conventional key-board with word predictions.
after practice,users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute..1 introduction.
experienced desktop and touchscreen typists canoften achieve fast and accurate text input by simplytyping all the characters in their desired text.
how-ever, for some users, such quick and precise input isdifﬁcult due to a motor disability.
such users mayuse a virtual touchscreen keyboard, but their touchlocations may be slow and inaccurate, e.g.
peo-ple with cerebral palsy.
other users may need toclick keys by pointing at them with a head- or eye-tracker and dwelling for a ﬁxed time, e.g.
peoplewith amyotrophic lateral sclerosis (als)..when a person’s typing is slow or inaccurate,word completions may provide more efﬁcient in-put.
word completions predict the most probable.
words based on the current typed preﬁx.
however,monitoring predictions carries a cognitive cost andmay not always improve performance (trnka et al.,2009).
further, monitoring predictions can be difﬁ-cult without visual feedback.
eyes-free text inputcan be slow for users who are visually-impaired(nicolau et al., 2019), and even slower for userswho are motor- and visually-impaired (nel et al.,2019).
finally, eyes-free text input may be neededin future augmented reality (ar) interfaces wherevisual feedback is limited or non-existent (e.g.
dueto lighting or device limitations).
in audio-onlyar, it is still possible to type on an invisible virtualkeyboard (vertanen et al., 2013; zhu et al., 2018).
all these cases motivate our interest in explor-ing alternatives to conventional word completion.
here we investigate accelerating input by allowingusers to skip typing spaces and mid-word vow-els.
we decided to abbreviate in this manner basedon past results on touchscreen text input withoutspaces (vertanen et al., 2015, 2018), and a studywe present here in which 200 people abbreviatedemail messages.
our interaction approach of abbre-viation is similar to features in commercial assis-tive interfaces (e.g.
grid 3, nuvoice, lightwriter).
our whole utterance prediction approach is simi-lar to features in touchscreen phone keyboards andin commercial assistive interfaces (e.g.
dwell-freesentence input in tobii communicator 5)..we modiﬁed a probabilistic recognizer to accu-rately expand abbreviated input by 1) improvingour language models by selecting well-matchedtraining data via a neural network, 2) modifyingthe search to model the insertion of mid-word vow-els, and 3) adding a neural language model to thesearch.
we validate our method in computationalexperiments on over six thousand sentences typedon touchscreen devices.
we found that even when28% of letters were omitted, we recognized sen-tences with no errors 70% of the time.
selecting.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6574–6588august1–6,2021.©2021associationforcomputationallinguistics6574from the top three sentences, user could obtain theirintended sentence 80% of the time..finally, we compare word completion and abbre-viated sentence input in a user study.
in this study,users had to dwell for one second to trigger a tap.
we found sentence input was slightly slower thanusing word completions, but still saved substantialtime compared to typing all the characters.
usersobtained their desired sentence 68% of the time..2 related work.
abbreviated input.
demasco and mccoy (1992)investigated expanding uninﬂected words (e.g.
“ap-ple eat john”) into syntactic sentences (e.g.
“theapple is eaten by john”).
gregory et al.
(2006)created abbreviation codes (e.g.
“rmb” = “remem-ber”).
users selected words from a menu or bytyping a code’s letters.
typing codes was the mostefﬁcient.
pini et al.
(2010) detected abbreviatedphrases using a support vector machine and ex-panded them via a hidden markov model (hmm).
their detector and expander were 90% and 95%accurate respectively.
users decreased keystrokesand input time by 32% and 26% respectively..shieber and nelken (2007) allowed users to dropnon-initial vowels and repeated consonants.
thisdeleted 26% of the total characters.
using an n-gram word language model and a spelling trans-ducer for each word, they expanded abbreviatedtext at an error rate of 3.3%.
our work differsin that we: 1) removed spaces between words, 2)did not remove consecutive consonants, 3) used acharacter language model with no ﬁxed vocabulary.
tanaka-ishii et al.
(2001) explored japanese textinput with digits.
they used an hmm to expanda sequence of digits into characters.
users saved35% of keystrokes typing on a mobile phone.
hanet al.
(2009) also used an hmm to expand abbre-viations learned from a corpus of java code.
theirapproach did not require memorizing abbreviationsand provided incremental feedback while typing..in two studies with 31 users, willis et al.
(2002,2005) identiﬁed common abbreviation behaviorssuch as vowel deletion, phonetic replacement, andword truncation.
they did not release their dataand it was on a relatively small number of people.
based on their work, we conducted an abbreviationstudy with 200 users and also share our data..developed to address this problem.
lin et al.
(1997),gao et al.
(2002), and yasuda et al.
(2008) used lan-guage modeling and in-domain perplexity to selecttraining data.
in this approach, a language model istrained on a small in-domain dataset.
training in-stances from an out-of-domain dataset are selectedif they are below some perplexity threshold..other work has investigated data selection usingcross-entropy or cross-entry difference between in-and out-of-domain datasets (axelrod et al., 2011;moore and lewis, 2010; schwenk et al., 2012;rousseau, 2013; mansour et al., 2011; vertanenand kristensson, 2011b).
in this approach, an in-domain and out-of-domain language models areﬁrst trained.
sentences are selected based on across-entropy threshold or cross entropy differencecalculated from the two language models..hildebrand et al.
(2005) and l¨u et al.
(2007) ap-plied information retrieval based techniques to se-lect data.
other method include selecting based oninfrequent n-gram occurrences (gasc´o et al., 2012;parcheta et al., 2018), or levenshtein distance andword vectors (chinea-rios et al., 2018)..duh et al.
(2013) employed the data selectionmethod of axelrod et al.
(2011), which builds uponmoore and lewis (2010)’s approach.
the main dis-tinction is that they used neural language modelsfor selection rather than n-gram models.
chen andhuang (2016), peris et al.
(2017), and chen et al.
(2016) selected based on convolutional and bidirec-tional long short-term memory neural networks..bidirectional neural models like bert (devlinet al., 2019) has proven effective in many naturallanguage tasks.
ma et al.
(2019) used bert fordomain-discriminative data selection.
hur et al.
(2020) used bert for domain adaptation and in-stance selection for disease classiﬁcation.
our se-lection method is similar to these methods but fo-cuses on selecting conversational-style sentences.
decoding noisy input.
text entry interfaces of-ten use a probabilistic decoder to infer a user’s textfrom time sequence data (vertanen et al., 2015;kristensson and zhai, 2004; zhai et al., 2002; zhaiand kristensson, 2008).
typically, a keyboard like-lihood model and a language model prior are usedto infer a user’s text from input with incorrect, miss-ing, or extra characters.
to date, these approacheshave mostly used n-gram language models..data selection.
mismatch between the trainingand target text domains can lead to sub-optimallanguage models.
a variety of methods have been.
ghosh and kristensson (2017) corrected typosin tweets to a low character error rate of 2.4% byusing a character convolutional neural network, an.
65753 free-form abbreviation study.
figure 1: error rate of automatic expansion with in-creasing abbreviation of the input..encoder with gated recurrent units, and a decoderwith attention.
the twitter typo data contained se-quences with a similar number of characters to thetarget.
in our work, we show acceptable charactererror rate can be achieved on input not only withtypos, but also with missing spaces and mid-wordvowels.
we show the advantage of using a recurrentneural network language model (rnnlm) directlyin the decoder’s search or to rescore hypotheses..to better understand how people do free-form ab-breviation, we conducted a study on amazon me-chanical turk.
as a pilot, we had 26 workers abbre-viate an email from the enron mobile data set (ver-tanen and kristensson, 2011a).
we designed ourinstructions based on willis et al.
(2005).
workersabbreviated the same email three times.
each timethe worker was asked to abbreviate in three ways:heavily, as little as possible, or as they saw ﬁt..in our pilot, we found workers abbreviated simi-larly regardless of instructions.
thus, we designeda single set of instructions for our main study thatasked workers to imagine they were using artiﬁ-cially intelligent (ai) software that was good atguessing their intended text from an abbreviatedform.
they were told to shorten words by remov-ing or changing letters, but they should avoid short-ening words that might be hard for the system toguess and that they should not omit words entirely.
see the appendix for our instructions.
our supple-mentary data contains all the data from the study.
we recruited 200 workers who each abbreviatedten emails.
in our analysis, we used 1,308 of the2,000 emails.
we ﬁltered out emails that did nothave the same number of words as their originalemails.
this ﬁltering helped us to align the sen-tences by word.
punctuation was removed exceptapostrophes and at signs.
we lowercased the text..3.1 abbreviation behavior.
we found 90% of abbreviated words were an in-order subsets of their full spelling.
on average,21% of a word’s letters were deleted.
of these,16% were consonants and 42% were vowels.
in theset of six common letters in english, e t o a i n,consonants were less likely to be deleted than vow-els.
surprisingly, the six least common letters,z q x j v k were often deleted.
considering let-ter position in words, 14% of ﬁrst letters, 35% oflast letters, and 90% of middle letters were deleted..our study conﬁrmed some of our initial beliefsabout how people would do free-form abbreviation.
we found people deleted vowels more frequentlythan consonants and people usually retained theﬁrst letter of words.
other aspects we found sur-prising such as the frequent deletion of uncommonletters.
the percentage of middle letters deletedwas high.
one reason for this was some workerspersistently only used the ﬁrst letter of each word..3.2 initial automatic expansion experiment.
we selected 564 passages where each word wasan in-order subset of the full word.
we imple-mented a search that proposed inserting all char-acters at all positions in words in workers’ input.
the search was guided by the language models de-scribed in vertanen et al.
(2015).
we used beamsearch to keep the search tractable.
see the ap-pendix for example input and the expanded output.
we measured accuracy using character error rate(cer).
cer is the number of insertions, substi-tutions, and deletions required to transform theexpanded text into the original text (typically multi-plied by 100).
as shown in figure 1, the expansionhad a cer of less than 5% for compression ofup to 30%.
beyond that, much of the input wasonly the ﬁrst letter of each word and our algorithmsimply imagined probable text consistent with theprovided letters.
we think these results are promis-ing given our search simply proposed the insertionof all characters at all positions..4 conversational language modeling.
we think abbreviated input may most beneﬁt userswith slow input.
from this point on, we focus onoptimizing our system for use by augmentative andalternative communication (aac) users.
aacusers may not be able to speak due to a condition.
6576●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●0204060800204060compression of input (%)error rate of expansion (cer %)such as als.
aac users slow input rate maketaking part in conversations difﬁcult (arnott et al.,1992).
sentence abbreviation may be particularlyuseful for short phrases with predictable language.
our search-based approach to abbreviation ex-pansion relies crucially on a well-trained languagemodel.
for a language model to work well it needsto be trained on data that is suited to the target do-main.
ideally we would train our language modelson large amounts of conversational communica-tions written by aac users.
for privacy and ethi-cal reasons, it is difﬁcult to ﬁnd large amounts ofsuch data.
therefore, in this section, we explore se-lecting training data from an out-of-domain datasetusing a small amount of in-domain aac-like data..4.1 selecting training data.
as our in-domain set, we used 29 k words of aac-like crowdsourced messages (vertanen and kris-tensson, 2011b).
for our out-of-domain trainingset, we used one billion words of web text fromcommon crawl1.
we only kept sentences consist-ing of a–z, apostrophes, spaces, commas, periods,question marks, and exclamation point.
we com-pared three ways to select training sentences:.
random selection.
we randomly selected sen-.
tences until we reached 100 million characters..cross entropy difference selection.
followingmoore and lewis (2010), we trained an in-domain4-gram word language model on our aac-like data,and an out-of-domain 4-gram model on a randomsubset of web text (disjoint from the training set).
we calculated the cross-entropy difference of train-ing sentences using the in- and out-of-domain mod-els.
we selected the highest scoring sentences untilwe reached 100 million characters..bert selection.
bert is a language represen-tation model built using self-attentive transformers(devlin et al., 2019).
we took the in- and out-of-domain data from the previous step and labeledeach sentence based on its set.
we then trained abinary classiﬁer using bert-base-uncased2.
we ran our classiﬁer on each sentence in the train-ing set yielding the probability of a sentence be-longing to the in-domain set.
we selected the topsentences until we reached 100 million characters..4.2 comparison of selection methods.
as shown in table 1, random sentences from com-mon crawl averaged 30 words.
the cross-entropy.
1https://commoncrawl.org/2https://github.com/google-research/bert/.
method words oov enron daily enronppl cer.
sent..(%).
ppl.
randomce diff.
bert.
29.814.311.1.
1.210.320.39.
4.814.574.53.
3.313.113.05.
7.046.005.75.table 1: impact of selection method on training sen-tences and performance of letter language models..difference and bert methods selected shorter sen-tences of 14 and 11 words respectively.
this islikely good given our goal of supporting short, con-versational messages.
for comparison, sentencesaveraged 13 words in the in-domain aac set and10 words in dailydialog (li et al., 2017).
dailydi-alog consists of two-sided everyday dialogues..we calculated the out-of-vocabulary (oov) ratewith respect to a vocabulary of 100 k words.
ourrandomly selected sentences had a much higher1.2% oov rate compared to cross-entropy andbert selected data at 0.3% and 0.4% respectively(table 1).
again this suits our purpose as we sus-pect abbreviated input is best suited for sentenceswithout uncommon words.
for comparison, theoov rates of dailydialog and our aac-like setwere both low at 0.2%.
see the appendix for sam-ples of sentences selected by each method..we trained 12-gram character language modelswith witten-bell smoothing on each 100 millioncharacter training set.
we trained without countcutoffs and did not prune the models.
the binaryberkeleylm (pauls and klein, 2011) size of therandom, cross-entropy difference, and bert mod-els were 1.7 gb, 1.3 gb, and 1.2 gb respectively.
we evaluated these character language modelson the enron mobile (vertanen and kristensson,2011a) and dailydialog (li et al., 2017) datasets.
before evaluation, we split each dialog turn indailydialog into single sentences and randomizedtheir order.
we calculated the average per-characterperplexity of these two datasets.
as shown in table1, the cross-entropy and bert models had perplex-ities around 6% lower than the random model withthe bert model having the lowest perplexity..we also compared the recognition accuracy ofthe three language models using the recognizer anddata to be described in the next section.
as shownin table 1 (right column), these perplexities reduc-tions did translate into improvements in recognitionaccuracy on touchscreen input where spaces and.
657750% of mid-word vowels were removed..5 recognizing noisy abbreviated input.
we now describe how we used our optimized lan-guage models to recognize noisy abbreviated input..5.1 decoder details and improvements.
we extended the velocitap touchscreen keyboarddecoder (vertanen et al., 2015).
velocitap searchesfor the most likely text given a sequence of 2d taps.
each tap has a likelihood under a 2d gaussianscentered at each key.
taps can be deleted with-out generating a character by incurring a deletionpenalty.
adding characters to a hypothesis incurpenalties based on a character language model..the decoder can insert characters without con-suming a tap.
a general insertion penalty allowsall possibles characters to be inserted.
the decoderalso has separate space and apostrophe insertionpenalties.
we extend this further by adding a vowelinsertion penalty for inserting the vowels: a, e, i,o, u. however, this penalty is only used if the priorcharacter is not a space.
this models that vowelsshould not be skipped at the start of words..the search is performed in parallel, with differ-ent threads extending partial hypotheses.
whena hypothesis consumes all taps, it is added to ann-best list.
to keep the search tractable, a conﬁg-urable beam controls whether partial hypothesesare pruned.
a wider beam searches more thor-oughly, but at the cost of more time and memory..to date, velocitap has only used n-gram lan-guage models.
we extend the decoder to use a re-current neural network language model (rnnlm)either as a replacement for the character n-gramduring search, or to rescore the n-best list.
whenused for rescoring, we compute the log probabilityof each sentence under the rnnlm.
we multiplythis probability by an rnnlm scale factor and addthe result to a hypothesis’ log probability..we trained an rnnlm on the bert-selectedtraining data.
after a hyperparameter search, wesettled on 512 lstm units, a character embeddingsize of 64, two hidden layers, a learning rate of0.001, and a dropout probability of 0.5. we trainedusing the adam optimizer.
on the enron mobileand dailydialog test sets, our rnnlm had a per-plexity of 4.50 and 2.64 respectively..to allow efﬁcient hypothesis extension duringrnnlm-based search, we augmented our partialhypotheses to track the state of the neural network..however, as we will see, rnnlm search requiredsubstantial memory and computation time.
whilewe experimented with using a gpu for rnnlmqueries, we found parallel cpu search was faster..5.2 touchscreen data and simulation details.
we tested our improvements on noisy, abbreviated,touchscreen keyboard input.
we wanted noisy in-put to ensure our system was robust to mistakesaac users may make when typing (e.g.
when us-ing a mouth stick or an eye-tracker).
we createda test and development set using data collected ontouchscreen phones (vertanen et al., 2015, 2013)and watches (vertanen et al., 2018, 2019).
we lim-ited our data to sentences from the enron mobileset.
we concatenated taps to create single sentencesequences without spaces.
we removed sentenceswhere the number of taps did not match the lengthof its reference.
this resulted in a test and develop-ment set of 6,631 and 731 sentences respectively.
we played back taps to our decoder, deletingmid-word vowels with a given vowel drop proba-bility.
we tested drop probabilities of 0.5 and 1.0.in our test set, 17.7% of characters were spaces.
with a drop probability of 0.5, 27.9% of charac-ters (including spaces) were deleted.
if all mid-word vowels were dropped, 38.2% of characterswere saved.
for the n-gram search and rnnlmrescoring setups and two drop probabilities, wetuned decoder parameters to minimize cer on thedevelopment set.
tuning used a random restarthill-climbing approach.
we tuned each of the foursetups for 600 cpu hours.
due to the computa-tional costs, we used the parameters found for then-gram search for the rnnlm search..we report the character error rate (cer), as wellas word error rate (wer), and sentence error rate(ser) on our test set.
we also report the top-5 serwhich is the lowest ser of the top ﬁve hypotheses.
we searched in parallel using 24 threads on a dualxeon e5-2697 v2 server.
this large number ofthreads mainly sped up the rnnlm search..5.3 recognition results.
as shown in table 2, using the rnnlm in thesearch instead of the n-gram model reduced errorrates by 23% and 12% relative for a vowel dropprobability of 0.5 and 1.0 respectively.
this how-ever came at a much higher cost with decodingtaking much longer and requiring more memory.
using the n-gram model for search and rescoringwith the rnnlm resulted in similar error rates.
6578decodersearch.
dropprob..cer(%).
wer(%).
ser top-5 ser decode memory(gb)(%).
time (s).
(%).
n-gram search.
+ rnnlm rescore.
rnnlm search.
0.5 5.7 ± 0.3 12.4 ± 0.5 35.1 ± 1.19.5 ± 0.5 27.7 ± 1.10.5 4.4 ± 0.29.3 ± 0.5 27.6 ± 1.10.5 4.3 ± 0.2.n-gram search.
+ rnnlm rescore.
rnnlm search.
1.0 9.5 ± 0.4 19.0 ± 0.7 45.5 ± 1.21.0 8.0 ± 0.3 15.5 ± 0.6 38.5 ± 1.21.0 8.2 ± 0.3 15.8 ± 0.6 41.5 ± 1.2.
22.016.515.4.
30.324.226.3.
0.210.3424.05.
0.030.091.09.
40.752.1353.2.
41.452.952.4.table 2: error rates and decoder performance using different search methods and vowel drop probabilities.
± values denote sentence-wise 95% bootstrap conﬁdence intervals (bisani and ney, 2004)..6 user study.
thus far, we tested abbreviated sentence input onlyin ofﬂine experiments.
to see if our method offerscompetitive performance in practice, we conducteda user study using a touchscreen web application..6.1 design.
we designed a touchscreen keyboard that runs in amobile web browser.
the keyboard has two modes:word — this mode has the keys a–z, apostro-phe, spacebar, and backspace (figure 2, left).
thekeyboard has three prediction slots above the key-board.
the left slot shows the exact letters typed.
the center and the right slots show predictionsbased on a user’s taps and any previous text.
pre-dictions and recognition occur after each key press.
pressing the spacebar normally selects the left slot.
similar to the iphone keyboard, if a user’s input isnoisy and we predict an auto-correction with highprobability, we highlight this slot instead.
in thiscase, pressing spacebar selects the auto-correction.
a done button signals completion of a sentence..sentence — this mode is similar but has nospacebar or suggestion slots (figure 2, right).
inputis recognized only after the done button is pressed.
to simulate users with a slow input rate, usershad to dwell on a key for one second to click it.
we chose one second because this is a common de-fault setting in dwell-based eye typing, for example,1.2 seconds in tobii communicator.
we displaya progress circle around a user’s ﬁnger locationshowing the dwell time.
after a click, the keyboardborder ﬂashes and the nearest key is added to thetext area above the keyboard..due to memory and computation requirements,we ran our decoder on a server at our university.
the keyboard client makes requests to the serverto recognize input.
in word mode, at the start of.
figure 2: the word (left) and sentence (right) key-board modes from our user study.
the circle is centeredon the user’s touch location with a green arc showingprogress towards the one second dwell time..to searching with the rnnlm, but only causedmodest increases in decode time and memory..dropping half of vowels, we recognized thecorrect sentence 72% of the time using rnnlmrescoring.
if we assume an interface allowing selec-tion from the top ﬁve results, this increased to 85%.
dropping all vowels was harder; we recognized thecorrect sentence only 59% of the time.
providingthe top ﬁve sentences increased this to 74%..interestingly, our vowel drop probability 1.0 se-tups were faster.
we investigated this by varyingthe tuned beams, measuring cer on the develop-ment set.
we found for drop 0.5, a narrower beamincreased cer while a wider beam provided nogain.
for drop 1.0, a narrower beam also increasedcer, but even a modestly wider beam increasedcer slightly (3% relative).
the tuned penalty forvowel insertion was small (0.8 probability).
weobserved in sentences with errors at a narrow beam,a wider beam sometimes resulted in more insertedvowels.
this may have allowed more probable text,but ultimately a higher cer.
this suggests we mayneed a more nuanced model of how users abbrevi-ate, e.g.
by penalizing contiguous vowel insertions..6579metricword9.9 ± 1.5 [6.6, 12.4] 9.0 ± 1.5 [5.7, 11.5] t(27) = -3.92, r = 0.60, p < 0.001entry rate (wpm)error rate (cer %) 0.3 ± 0.5 [0.0, 2.5] 7.2 ± 5.4 [1.0, 23.6] t(27) = 6.72, r = 0.79, p < 0.001.statistical test.
sentence.
table 3: user performance in each condition in our user study.
results formatted as: mean ± sd [min, max]..figure 3: entry and error rate in our user study..figure 4: entry rates for each block of four phrases..each key press, we request predictions for the key-board slots.
in sentence mode, we request sentencerecognition at the start of pressing the done button.
by making the server request at the start of a keypress, we effectively eliminated the need to waitfor predictions.
the average round trip time for re-quests in our user study was 0.41 s (sd 0.21) in theword mode and 0.58 s (sd 0.29) in sentence mode..6.2 procedure.
we recruited 28 amazon mechanical turk workers.
the study took 30–40 minutes.
workers were paid$10.
we also offered a $5 bonus for the fastest 10%of workers in each condition subject to having acer below 5%.
this was a within-subject experi-ment with two counterbalanced conditions: wordand sentence.
the conditions used the word andsentence mode of the keyboard respectively..workers typed 26 phrases in each condition.
theﬁrst two were practice phrases which we did notanalyze.
workers wrote phrases written by peoplewith als for voice banking purposes (costello,2014).
we used phrases with 3–6 words (1,182total phrases).
workers received a random set ofphrases and never wrote the same phrase twice..6.3 results.
table 3 and figure 3 show results and statisticaltests.
we calculated entry rate in words-per-minute(wpm).
we considered a word to be ﬁve charactersincluding space.
we measured the entry time froma worker’s ﬁrst tap until they ﬁnished dwelling onthe done button.
the entry rate in word was fasterat 9.9 wpm versus sentence at 9.0 wpm.
this.
difference was signiﬁcant (table 3)..as shown in figure 4, participants started outslower in sentence compared to word, but theentry rate gap closed as they wrote more phrases.
we averaged performance in the ﬁrst eight andlast eight phrases.
in word, the entry rate was9.7 wpm in the ﬁrst set and 9.9 wpm in the lastset.
in sentence, the entry rate was 8.6 wpm inthe ﬁrst set and 9.6 wpm in the last set.
this ispromising, as perhaps with more practice, sentenceabbreviation might achieve comparable speed butwithout requiring monitoring of word predictions.
participants were less accurate in sentencewith a cer of 7.2% versus 0.3% in word.
thisdifference was signiﬁcant (table 3).
participantsobtained a completely correct phrase 97% of thetime in word, but only 68% in sentence.
wethink the lower accuracy in sentence was mostlydue to some users abbreviating phrases too aggres-sively.
in phrases recognized completely correctly,the compression rate was 35%.
in phrases withrecognition errors, the compression rate was 43%.
we classiﬁed phrases in sentence accordingto their input length versus the reference length mi-nus spaces and mid-word vowels.
252 phrases hadthe correct length, 162 were longer, and 258 wereshorter.
these sets correspond to phrases that werelikely correctly abbreviated, under-abbreviated,and over-abbreviated.
the error rates of these setswere 3.2%, 2.1%, and 14.0% respectively.
wefound ﬁve workers over-abbreviated 20 or morephrases.
removing these workers lowered the over-all cer to 5.7%.
while not as accurate as wordinput, sentence input did have acceptable accuracy.
6580●●03691215wordsentenceentry rate (wpm)●●●024681012wordsentenceerror rate (cer %)●●●●●●●●●●●●0246810123456sentencewordentry rate (wpm)phrase blocksources, such sources lack visibility into how theuser actually produced the text (e.g.
did they useword completions?).
further, the propensity to ab-breviate may be inﬂuenced by the particular aacinterface used.
second, even if we could sourceaac abbreviated text, we would have no reliableway to determine the unabbreviated text.
we couldhave asked aac users to complete our abbrevia-tion study, but this would have introduced morenoise (incorrect key presses) that would have com-plicated our ﬁrst study’s goal of discovering naturalabbreviation behaviors.
it would have also limitedthe number of people we learned behaviors from.
in this phase our goal was to discover what lettershumans think are the most information carryingin a passage of text.
while we suspect abbrevia-tion strategies of aac users would be similar, thiswould beneﬁt from validation with aac users..we tested our method on touchscreen datarecorded in previous studies on phones andwatches, and in a web-based crowdsourced userstudy.
we think our method mainly would beneﬁtusers who have a slow input rate; fast typists mayonly be slowed by the cognitive overheads of de-ciding what letters to omit or by disrupting theirmuscle memory for typing familiar words.
this ledus to limiting the input rate in our study by requir-ing users dwell for one second.
while this studyallowed us to conﬁrm our abbreviation methodis competitive with a conventional keyboard withword predictions, this needs validation with userswith actual input rate limits.
aac user interac-tion may feature more imprecise key presses, moreaccidental key presses, and may introduce com-plications related to attending to word predictions(e.g.
the “midas touch” problem in eye tracking).
further, we only tested one input rate, it is possibleour method may be better or worse at different in-put speeds.
we think our approach may also offeradvantages for eyes-free text input, but this alsoneeds comparison against conventional eyes-freeinput approaches (e.g.
iphone’s voiceover feature)..we investigated abbreviation by omitting mid-word vowels.
we did not investigate other formsof abbreviation such as phonetic replacement(e.g.
“you” → ”u”) or removal of consonants.
ourmodel may beneﬁt from more sophisticated model-ing on how and when vowels are inserted (e.g.
pe-ideally im-nalizing repeated vowel insertions).
proved models would be based on data collected byusers engaged in actual abbreviated input.
as our.
figure 5: participants’ entry and error rate in each con-dition of the user study..when users abbreviated as instructed..individual user performance was variable (fig-ure 5).
16 participants achieved 0% cer in wordand all but two had a cer below 1%.
while insentence, no participant achieved 0% cer andﬁve participants had a high cer of over 10%..using backspace, participants could ﬁx incorrectletters or misrecognized words.
the number ofbackspaces per ﬁnal output character was low at0.02 in both conditions.
thus, it appears partici-pants precisely targeted keys, likely as a result ofthe slow input induced by the dwell time..7 discussion.
we set out to show we could accelerated the writ-ing of short and reasonably predictable phrases bycombining sentence-at-a-time recognition with ag-gressive abbreviation.
in our ﬁnal user study, wefound our method did not quite beat a conventionalkeyboard with word predictions.
however, users inour study likely had substantial experience doingword-at-a-time input on their phones.
it appearsusers got faster at abbreviated sentence input evenduring the brief study session.
by the last eightphrases, users were only 3% relative slower usingsentence abbreviated input compared to word-at-a-time input with word completions.
when usersprovided abbreviated input consisting of all the cor-rect letters except mid-word vowels, 90% of thesephrases were expanded correctly..we observed the abbreviation behaviour of alarge number of non-aac users and designed asystem supporting the most common behaviours.
while we could have tried to learn abbreviationbehaviors from actual aac user data, this presentsa number of issues.
first, actual text from aacusers is difﬁcult to obtain for ethical and privacyreasons.
while it may be possible to obtain suchtext via donations from aac users and from online.
6581●●●●●●●●●●●●●●●●●●●●●●●●●●●●7911●sentenceword05101520error rate (cer %)entry rate (wpm)results show, correctly inferring the intended sen-tences was challenging even when we asked usersto obey a few simple behaviours, namely remov-ing spaces and mid-word vowels.
while an idealsystem would support a wide-range of abbreviationbehaviors and even adapt to individuals, we suspectthis may be challenging given our current lack oftraining data on this task..in our initial study, participants abbreviatedemail text that was displayed visually.
an alter-native approach would be to play audio of the text.
while this might be a more realistic abbreviationtask, it also presents practical challenges to partici-pants such as remembering the text and spelling anydifﬁcult words.
perhaps an even more externallyvalid approach would be to have workers composenovel abbreviated sentences.
this would requireanother step to obtain the unabbreviated compo-sitions (vertanen and kristensson, 2014; gaineset al., 2021).
given we now have a competent ini-tial system, it would be interesting to undertakesuch a data collection effort..our results suggest a simple correction interfacebased on selecting from the top sentences wouldoften, but not always work.
designing an efﬁcientand easy-to-use interface for correcting a few wordswithin such sentence results would be interestingfuture work.
this might be especially challengingto design for users with diverse motor abilities..we used language models trained on only 100 mcharacters of text.
while this allowed us to com-pare the efﬁcacy of the language model types anddecoder conﬁgurations, substantially more trainingdata is available along with neural architecturesthat scale to large training sets, e.g.
gpt-2 (rad-ford et al., 2019).
we suspect further recognitionaccuracy gains are possible for abbreviated, noisyinput by incorporating such models.
further, wecould likely obtain additional improvements fromthe n-gram model by training on more data and thenpruning the model to reduce its size.
we avoideddoing this in this work to fairly compare the n-gram and rnn language models when trained onthe same amount of text..our language model training data was drawnfrom common crawl.
we used a corpus of aac-like crowdsourced messages to select training sen-tences from common crawl.
other sources oftraining data such as twitter or reddit are likelymore conversational in style.
it would be interest-ing to investigate whether data selecting from a.more targeted large-scale training source providesadditional improvements in language modeling..we did not speciﬁcally investigate how ourmethod would support text containing difﬁcultwords such as acronyms or proper names.
userscan often anticipate and alter their input behaviorto avoid auto-correct errors, e.g.
by force (weiret al., 2014), by long pressing a key (vertanenet al., 2019), or by switching to a precise inputmode (dudley et al., 2018).
similarly, our abbre-viated input method needs a way to specify wordsthat should not be expanded or auto-corrected..at the onset, we did not know that our proposedabbreviation technique would be competitive toconventional word completion.
the results fromour user study tell us we need to make further im-provements to our recognition, better train users toabbreviate in supported ways, and conduct a longi-tudinal evaluation.
further, testing an abbreviatedinput prototype with aac users will undoubtedlylead to new insights.
this paper is a ﬁrst step inproducing a viable prototype for testing with userswith rate-limited input abilities..8 conclusion.
we explored accelerating text communication byabbreviated sentence input.
we conducted auser study to learn how users abbreviate.
weshowed the efﬁcacy of a neural classiﬁer to selectconversational-style training instances from a largetext corpus.
we found that dropping spaces andmid-word vowels can provide compression of sen-tences from 28% to 38%.
such abbreviated andnoisy input can often be expanded correctly 59%to 72% of the time.
we also showed how the accu-racy of a statistical virtual keyboard decoder canbe improved by using a neural language model tore-rank the top recognition results.
finally, afterpractice, users wrote only slightly slower using sen-tence abbreviated input at 9.6 words-per-minutecompared to a conventional keyboard with wordpredictions at 9.9 words-per-minute.
if a phrasewas abbreviated by removing spaces and mid-wordvowels, our system expanded the abbreviated inputto the intended phrase 90% of the time..acknowledgments.
this material is based upon work supported by thensf under grant no.
iis-1750193..6582references.
john l. arnott, alan f. newell, and norman alm.
1992. prediction and conversational momentum inan augmentative communication system.
commu-nications of the acm, 35(5):46–57..amittai axelrod, xiaodong he, and jianfeng gao.
2011. domain adaptation via pseudo in-domainin proceedings of the 2011 con-data selection.
ference on empirical methods in natural languageprocessing, pages 355–362, edinburgh, scotland,uk.
association for computational linguistics..m. bisani and h. ney.
2004. bootstrap estimates forconﬁdence intervals in asr performance evalua-tion.
proceedings of the ieee conference on acous-tics, speech, and signal processing, pages 409–411..boxing chen and fei huang.
2016. semi-supervisedconvolutional networks for translation adaptationwith tiny amount of in-domain data.
in proceed-ings of the 20th signll conference on compu-tational natural language learning, pages 314–323, berlin, germany.
association for computa-tional linguistics..boxing chen, roland kuhn, george foster, colincherry, and fei huang.
2016. bilingual methodsfor adaptive training data selection for machinein proceedings of the association fortranslation.
machine translation in the americas, pages 93–103..mara chinea-rios, germ´an sanchis-trilles, and fran-cisco casacuberta.
2018. creating the best devel-opment corpus for statistical machine translationsystems.
in proceedings of the 21st annual confer-ence of the european association for machine trans-lation, pages 99–108.
european association for ma-chine translation..john m. costello.
2014. message banking, voice bank-ing and legacy messages.
boston children’s hospi-tal, boston, ma..patrick w. demasco and kathleen f. mccoy.
1992.generating text from compressed input: an intelli-gent interface for people with severe motor impair-ments.
communications of the acm, 35(5):68–78..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language un-derstanding.
in proceedings of the 2019 conferenceof the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..kevin duh, graham neubig, katsuhito sudoh, and ha-jime tsukada.
2013. adaptation data selection us-ing neural language models: experiments in ma-chine translation.
in proceedings of the 51st annualmeeting of the association for computational lin-guistics (volume 2: short papers), pages 678–683,soﬁa, bulgaria.
association for computational lin-guistics..dylan gaines, per ola kristensson, and keith verta-nen.
2021. enhancing the composition task in textentry studies: eliciting difﬁcult text and improvingin proceedings of the 2021error rate calculation.
chi conference on human factors in computingsystems, chi ’21, new york, ny, usa.
associationfor computing machinery..jianfeng gao, joshua goodman, mingjing li, and kai-fu lee.
2002. toward a uniﬁed approach to statis-tical language modeling for chinese.
acm trans-actions on asian language information processing,1(1):3–33..guillem gasc´o, martha-alicia rocha, germ´ansanchis-trilles, jes´us andr´es-ferrer, and franciscocasacuberta.
2012. does more data always yieldin proceedings of the 13thbetter translations?
conference of the european chapter of the asso-ciation for computational linguistics, eacl ’12,page 152–161, usa.
association for computationallinguistics..shaona ghosh and per ola kristensson.
2017. neu-ral networks for text correction and comple-arxiv preprinttion in keyboard decoding.
arxiv:1709.06429..ellyn gregory, melinda soderman, christy ward,david r beukelman, and karen hux.
2006. aacmenu interface: effectiveness of active versus pas-sive learning to master abbreviation-expansioncodes.
augmentative and alternative communica-tion, 22(2):77–84..sangmok han, david r. wallace, and robert c. miller.
2009. code completion from abbreviated input.
in proceedings ofthe 2009 ieee/acm interna-tional conference on automated software engineer-ing, ase ’09, page 332–343, usa.
ieee computersociety..almut silja hildebrand, matthias eck, stephan vo-gel, and alex waibel.
2005. adaptation of thetranslation model for statistical machine transla-in proceed-tion based on information retrieval.
ings of the 10th eamt conference: practical appli-cations of machine translation, pages 133–142, bu-dapest, hungary.
european association for machinetranslation..john j. dudley, keith vertanen, and per ola kristens-son.
2018. fast and precise touch-based text entryfor head-mounted augmented reality with variableocclusion.
acm transactions on computer-humaninteraction (tochi), 25(6)..brian hur, timothy baldwin, karin verspoor, laurahardefeldt, and james gilkerson.
2020. domainadaptation and instance selection for disease syn-drome classiﬁcation over veterinary clinical notes.
in proceedings of the 19th sigbiomed workshop on.
6583biomedical language processing, pages 156–166,online.
association for computational linguistics..per ola kristensson and shumin zhai.
2004. shark2:a large vocabulary shorthand writing system forin proceedings of the 17thpen-based computers.
annual acm symposium on user interface softwareand technology, uist ’04, pages 43–52, new york,ny, usa.
acm..yanran li, hui su, xiaoyu shen, wenjie li, ziqiangcao, and shuzi niu.
2017. dailydialog: a manu-ally labelled multi-turn dialogue dataset.
in pro-ceedings of the eighth international joint confer-ence on natural language processing (volume 1:long papers), pages 986–995, taipei, taiwan.
asianfederation of natural language processing..sung-chien lin, chi-lung tsai, lee-feng chien, ker-jiann chen, and lin-shan lee.
1997. chinese lan-guage model adaptation based on document clas-siﬁcation and multiple domain-speciﬁc languagein proceedings of european conferencemodels.
on speech communication and technology, pages1463–1466..yajuan l¨u, jin huang, and qun liu.
2007..improv-ing statistical machine translation performance byin pro-training data selection and optimization.
ceedings of the 2007 joint conference on empiricalmethods in natural language processing and com-putational natural language learning (emnlp-conll), pages 343–350, prague, czech republic.
association for computational linguistics..xiaofei ma, peng xu, zhiguo wang, ramesh nalla-pati, and bing xiang.
2019. domain adaptationwith bert-based domain classiﬁcation and dataselection.
in proceedings of the 2nd workshop ondeep learning approaches for low-resource nlp(deeplo 2019), pages 76–83, hong kong, china.
association for computational linguistics..saab mansour, joern wuebker, and hermann ney.
2011. combining translation and language modelscoring for domain-speciﬁc data filtering.
in in-ternational workshop on spoken language transla-tion (iwslt) 2011..robert c. moore and william lewis.
2010..intelli-gent selection of language model training data.
inproceedings of the acl 2010 conference short pa-pers, aclshort ’10, pages 220–224, stroudsburg,pa, usa.
association for computational linguis-tics..2019. the design space of nonvisual word com-in the 21st international acm sigac-pletion.
cess conference on computers and accessibility,assets ’19, page 249–261, new york, ny, usa.
association for computing machinery..zuzanna parcheta, germ´an sanchis-trilles, and fran-cisco casacuberta.
2018. data selection for nmtusing infrequent n-gram recovery.
in proceedingsof the 21st annual conference of the european as-sociation for machine translation, pages 219–227.
european association for machine translation..adam pauls and dan klein.
2011. faster and smallerin proceedings of then-gram language models.
49th annual meeting of the association for com-putational linguistics: human language technolo-gies - volume 1, hlt ’11, pages 258–267, strouds-burg, pa, usa.
association for computational lin-guistics..´alvaro peris, mara chinea-r´ıos,.
and franciscocasacuberta.
2017. neural networks classiﬁer fordata selection in statistical machine translation.
the prague bulletin of mathematical linguistics,108(1):283–294..stefano pini, sangmok han, and david r. wallace.
2010. text entry for mobile devices using ad-hocin proceedings of the internationalabbreviation.
conference on advanced visual interfaces, avi ’10,page 181–188, new york, ny, usa.
association forcomputing machinery..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
ope-nai blog, 1(8):9..anthony rousseau.
2013. xenc: an open-sourcetool for data selection in natural language pro-cessing.
the prague bulletin of mathematical lin-guistics, 100:73–82..holger schwenk, anthony rousseau, and mohammedattik.
2012. large, pruned or continuous spacelanguage models on a gpu for statistical machinein proceedings of the naacl-hlttranslation.
2012 workshop: will we ever really replace the n-gram model?
on the future of language modelingfor hlt, pages 11–19, montr´eal, canada.
associa-tion for computational linguistics..stuart m shieber and rani nelken.
2007. abbreviatedtext input using language modeling.
natural lan-guage engineering, 13(2):165–183..emli-mari nel, per ola kristensson, and david j. c.mackay.
2019. ticker: an adaptive single-switchtext entry method for visually impaired users.
ieee transactions on pattern analysis and machineintelligence, 41(11):2756–2769..kumiko tanaka-ishii, yusuke inutsuka, and masatotakeichi.
2001. japanese text input system within proceedings of the first internationaldigits.
conference on human language technology re-search..hugo nicolau, andr´e rodrigues, andr´e santos, tiagoguerreiro, kyle montague, and jo˜ao guerreiro..keith trnka, john mccaw, debra yarrington, kath-leen f. mccoy, and christopher pennington.
2009..6584tim willis, helen pain, and shari trewin.
2005. aprobabilistic flexible abbreviation expansion sys-tem for users with motor disabilities.
in proceed-ings of the 2005 international conference on acces-sible design in the digital world, accessible de-sign’05, page 4, swindon, gbr.
bcs learning &development ltd..tim willis, helen pain, shari trewin, and stepheninforming flexible abbreviation ex-clark.
2002.inpansion for users with motor disabilities.
proceedings of the 8th international conferenceon computers helping people with special needs,icchp ’02, page 251–258, berlin, heidelberg.
springer-verlag..keiji yasuda, ruiqiang zhang, hirofumi yamamoto,and eiichiro sumita.
2008. method of selectingtraining data to build a compact and efﬁcienttranslation model.
in proceedings of the third in-ternational joint conference on natural languageprocessing: volume-ii..shumin zhai and per ola kristensson.
2008. interlacedqwerty: accommodating ease of visual searchand input flexibility in shape writing.
in proceed-ings of the sigchi conference on human factorsin computing systems, chi ’08, page 593–596, newyork, ny, usa.
association for computing machin-ery..shumin zhai, alison sue, and johnny accot.
2002.movement model, hits distribution and learning invirtual keyboarding.
in proceedings of the sigchiconference on human factors in computing sys-tems, chi ’02, page 17–24, new york, ny, usa.
association for computing machinery..suwen zhu, tianyao luo, xiaojun bi, and shuminzhai.
2018.typing on an invisible keyboard.
in proceedings of the sigchi conference on hu-man factors in computing systems, chi ’18, pages439:1–439:13, new york, ny, usa.
acm..user interaction with word prediction: the effectsof prediction quality.
acm transactions on acces-sible computing, 1:17:1–17:34..keith vertanen, crystal fletcher, dylan gaines, jacobgould, and per ola kristensson.
2018. the impactof word, multiple word, and sentence input on vir-tual keyboard decoding performance.
in proceed-ings of the sigchi conference on human factors incomputing systems, chi ’18, pages 626:1–626:12,new york, ny, usa.
acm..keith vertanen, dylan gaines, crystal fletcher,alex m. stanage, robbie watling, and per ola kris-tensson.
2019. velociwatch: designing and evalu-ating a virtual keyboard for the input of challeng-in proceedings of the 2019 chi confer-ing text.
ence on human factors in computing systems, chi’19, page 1–14, new york, ny, usa.
associationfor computing machinery..keith vertanen and per ola kristensson.
2011a.
aversatile dataset for text entry evaluations basedon genuine mobile emails.
in proceedings of the13th international conference on human computerinteraction with mobile devices & services, mo-bilehci ’11, pages 295–298, new york, ny, usa.
acm..keith vertanen and per ola kristensson.
2011b.
theimagination of crowds: conversational aac lan-guage modeling using crowdsourcing and largein proceedings of the 2011 confer-data sources.
ence on empirical methods in natural languageprocessing, pages 700–711, edinburgh, scotland,uk.
association for computational linguistics..keith vertanen and per ola kristensson.
2014. com-plementing text entry evaluations with a composi-tion task.
acm transactions of computer humaninteraction, 21(2):8:1–8:33..keith vertanen, haythem memmi,.
justin emge,shyam reyal, and per ola kristensson.
2015. ve-locitap: investigating fast mobile text entry usingsentence-based decoding of touchscreen keyboardin proceedings of the sigchi conferenceinput.
on human factors in computing systems, chi ’15,pages 659–668, new york, ny, usa.
acm..keith vertanen, haythem memmi, and per ola kris-tensson.
2013. the feasibility of eyes-free touch-in proceedings of thescreen keyboard typing.
15th international acm sigaccess conference oncomputers and accessibility, assets ’13, pages69:1–69:2, new york, ny, usa.
acm..daryl weir, henning pohl, simon rogers, keith ver-tanen, and per ola kristensson.
2014. uncertaintext entry on mobile devices.
in proceedings of thesigchi conference on human factors in comput-ing systems, chi ’14, pages 2307–2316, new york,ny, usa.
acm..6585appendix.
a abbreviation study instructions.
in our ﬁrst crowdsourced study, we had 200 work-ers abbreviate a series of ten email messages.
fig-ure 6 shows the complete instructions we gave toworkers..e recognizing noisy abbreviated input.
table 6 shows some examples of recognition re-sults using the rnnlm rescoring conﬁguration.
acomplete list of recognition results is provided inour supplementary materials..figure 6: instructions given to workers in our crowd-sourced free-from abbreviation study..b error rate to compression.
in our ﬁnal crowdsourced study, we plotted par-ticipants’ character error rate against increasingabbreviation in the sentence condition.
with in-creasing compression, the error rate also increased(figure 7)..figure 7: error rate of automatic expansion with in-creasing abbreviation of the input in the ﬁnal study..c initial automatic expansion experiment.
table 4 shows some examples from our initial au-tomatic expansion experiment where the decoderinserted all characters at all possible positions in aworker’s input..d selecting training data.
table 5 shows a list of example sentences selectedusing three ways: random selection, cross-entropydifference selection, and bert selection..6586●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●025507510020406080compression of input (%)error rate of expansion (cer %)originalabbreviation wexpansion.
originalabbreviation dntexpansion.
originalabbreviation nexpansion.
we are off to the uk in a couple of days.
r.off t.th.
uk n.a cpl.
of dys.
we are off to the uk in a couple of days.
didn’t get a commitment just told them i thought it would be impossible.
gt.
a cmmt.
jst tld.
thm i tht.
it wd.
b.mpss.
don’t.
get a comment.
just told them i thought it would be impress.
no.
arrangements he.
just hasn’t had a good year on a comparative basis.
ats.
he.
j.h.h.a go.
ye.
on a ct.and that’s.
the job he.
has a good eye on a city.
b.but.
table 4: examples from the initial automatic expansion experiment in section 3.2. each example shows theoriginal text, workers’ abbreviated version, and the automatically expanded result.
errors are underlined..random selectionrandom 1:.
i’m a huge fan of your work it’s really well done..random 2:.
the main challenge is to integrate more and more qubits to silicon chips..cross-entropy difference selectiontop 1: what’s for dinner?
what’s for lunch?.
top 2: how’s things going?.
mid 1: want to know what happened during ﬁsh robert ed ﬁsh’s life?.
mid 2: do you want to work at a job or do you want to play at a passion..bottom 1:.
think super mario bros..bottom 2:.
is there a form applicants should submit, or should they just send an email with their resume?.
bert selectiontop 1: you don’t like doing homework?.
top 2: you need money?.
mid 1:.
i am very proud of you for what you are doing with your life right now..mid 2:.
imagine my terrible position!.
bottom 1:.
she has a bachelor’s degree in political science from lincoln university in oxford, pennsylvania..bottom 2: good relations are answers.
bad relations are disasters..table 5: examples of selected text data using three different approaches in section 4.1. for bert and cross-entropy difference selection top, mid, and bottom represent the absolute positions in the ordered list according totheir scores..6587vowel drop probability example.
0.50.50.5.
0.50.50.5.
0.50.50.5.
1.01.01.0.
1.01.01.0.
1.01.01.0.hopefully this can wait until monday.
reference:input:recognition: hopefully this can wait until monday.
hopeflltthisvamwwtujrlmndy.
let it rip.
reference:input:recognition: let it rip.
ltutrp.
should systems manage the migration.
reference:input:recognition: she’d systems manager he migration.
shldsystensmnferhemgratin.
could you see where this stands.
reference:input:cldyusewhtwrgsstndarecognition: could you see where the stands.
florida is great.
reference:input:recognition: florida is great.
flrdaushrt.
they are more efficiently pooled.
reference:input:recognition: they are more egg vinyl hold.
yhyare’rrefgvmyluplf.
table 6: examples of abbreviated and noisy input and the resulting recognition results for two different voweldrop probabilities in section 5.3. the input text represents the closest key to each tap observation in our data.
recognition errors are underlined..6588