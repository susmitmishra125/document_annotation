bridging subword gaps in pretrain-finetune paradigm fornatural language generationxin liu1,2 baosong yang3 dayiheng liu3 haibo zhang3 weihua luo3min zhang4 haiying zhang1,2 jinsong su1,2,5∗1school of informatics, xiamen university2institute of artiﬁcial intelligence, xiamen university.
3alibaba group 4soochow university, china.
5pengcheng lab, shenzhen.
liuxin@stu.xmu.edu.cn.
{yangbaosong.ybs,liudayiheng.ldyh,zhanhui.zhb,weihua.luowh}@alibaba-inc.com.
minzhang@suda.edu.cn.
{zhang2002,jssu}@xmu.edu.cn.
abstract.
a well-known limitation in pretrain-ﬁnetuneparadigm lies in its inﬂexibility caused bythe one-size-ﬁts-all vocabulary.
this poten-tially weakens the effect when applying pre-trained models into natural language genera-tion (nlg) tasks, especially for the subworddistributions between upstream and down-stream tasks with signiﬁcant discrepancy.
to-wards approaching this problem, we extendthe vanilla pretrain-ﬁnetune pipeline with anextra embedding transfer step.
speciﬁcally,a plug-and-play embedding generator is intro-duced to produce the representation of anyinput token, according to pre-trained embed-dings of its morphologically similar ones.
thus, embeddings of mismatch tokens indownstream tasks can also be efﬁciently ini-tialized.
we conduct experiments on a varietyof nlg tasks under the pretrain-ﬁnetune fash-ion.
experimental results and extensive anal-yses show that the proposed strategy offers usopportunities to feel free to transfer the vocab-ulary, leading to more efﬁcient and better per-formed downstream nlg models.
1.
1.introduction.
pretrain-ﬁnetune paradigm has been highly suc-cessful on tackling challenging problems in naturallanguage processing, e.g., domain adaptation (satoet al., 2020; yao et al., 2020), incremental learn-ing (khayrallah et al., 2018; wan et al., 2020), aswell as knowledge transferring (liu et al., 2020b).
the rise of large-scale pre-trained language mod-els further attracts increasing attention towards thisstrategy (devlin et al., 2019; edunov et al., 2019).
typically, these methods ﬁrst pretrain a universal.
1we.
atdeeplearnxmu/embedding-transfer.
release.
code.
the.
https://github.com/.
*jinsong su is the corresponding author.
this work wasdone when xin liu was interning at damo academy, alibabagroup..ce no zo ic pala eo hy dro dyn ami c.m-bertout-of-domain cen ozo ic pal a e o hydro dynamicthesis.
cenozoic palaeohydrodynamic.
table 1: segmentation of english sequence “cenozoicpalaeohydrodynamic” learned from different data dis-tribution as described in § 4. high frequent wordsin thesis domain are split into ﬁne-grained and under-represented tokens in pre-trained models..model using a large-scale corpus, which is thenﬁnetuned to various downstream tasks via a fewadjustments.
due to its simplicity yet impressiveperformance, pretrain-ﬁnetune paradigm becomesthe undoubtedly dominant solution for buildingstate-of-the-art models in many natural languageunderstanding tasks (xu et al., 2019; yang et al.,2019a; liu et al., 2020b)..in comparison, this strategy often achieves disap-pointing or barely satisfactory performance in natu-ral language generation (nlg) tasks.
for example,several studies observe that m-bert (devlin et al.,2019) fails to enhance the decoder of a translationmodel (edunov et al., 2019; zhu et al., 2020), whilerothe et al.
(2020) reach the same conclusion evenwhen adapting an autoregressive model gpt (rad-ford et al., 2019).
a natural problem arises: what isthe crucial bottleneck in current pretrain-ﬁnetuneframework and how to break it?.
in this paper, we provide the ﬁrst answer fromthe subword discrepancy aspect, namely, the sub-word vocabulary extracted according to the pre-training data distribution is insufﬁcient to copewith the downstream nlg tasks.
such inﬂexi-bility stems from the fact that downstream nlgmodels have to inherit the vocabulary from theirpre-trained counterparts.
in order to deal with theopen-vocabulary problem, it is de-facto standardfor pre-trained models to employ heuristic subwordsegmentation methods (sennrich et al., 2016; kudo.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6001–6011august1–6,2021.©2021associationforcomputationallinguistics6001and richardson, 2018).
however, the segmentationlearns on the upstream corpus other than the ﬁne-tuned data and is likely to be sub-optimal (cherryet al., 2018; provilkov et al., 2020)..we argue that these lead to subword discrep-ancy and bring two defects.
firstly, the pre-trainedmodel usually learns a ﬁne-grained subword seg-mentation to maintain the coverage of a largeamount of diverse vocabulary.
consequently, down-stream nlg models may suffer from more seri-ous exposure bias (bengio et al., 2015) and expen-sive computational cost caused by the increasedsequence lengths.
as one example, m-bert ex-ploits 100 thousand ﬁne-grained subwords to en-code hundreds of languages, while most of down-stream nlg tasks, in fact, require only one lan-guage and its associate tokens.
secondly, wordsthat are rare in upstream task but frequent in down-stream task may be segmented end up poorly un-derstood (provilkov et al., 2020).
considering theenglish sequence “cenozoic palaeohydrodynamic”shown in table 1, all the words are frequent ina thesis domain translation task and can be wellpreserved in its vocabulary.
nevertheless, theyare segmented into under-represented tokens bypre-trained models, preventing the ﬁnetuning stagefrom better learning their compositionality for gen-eration.
an alternative solution is reconstructingthe pre-trained model by exploiting either a task-speciﬁc vocabulary (nguyen and chiang, 2017;kocmi and bojar, 2018) or a subword regulariza-tion approach (provilkov et al., 2020).
however,retraining the upstream model from scratch for eachtask is time-consuming and unavailable for large-scale models like m-bert, gpt, etc..to this end, we propose a simple yet general-ized pretrain-ﬁnetune strategy, where an embed-ding transfer stage is inserted between pre-trainingand ﬁnetuning to eliminate their token granularitygaps.
unlike the prior strategy using a ﬁxed vocab-ulary, our vocabulary is changeable and its itemsincluding mismatched ones can be easily initializedby the pre-trained embeddings.
concretely, weequip the pre-trained model with a plug-and-playembedding generator, which is able to produce theembedding of any token by feeding its subwordsand hyperwords that appeared in pre-trained vocab-ulary.
to train this generator, we randomly split ormerge some tokens to replace their original embed-dings with those produced by the generator.
theparameters of the generator are optimized underthe vanilla pre-training framework to minimize the.
divergence before and after replacing the embed-dings.
accordingly, we can use a task-speciﬁc vo-cabulary for the downstream task, where commontokens are immediately initialized with pre-trainedembeddings while mismatched ones are initializedby our generator..we conduct experiments on various tasks undernlg context, in a range from domain adaptationto knowledge transferring, and from machine trans-lation to answer-aware question generation.
empir-ical results demonstrate the universal-effectivenessof the proposed strategy comparing with strongbaselines and related approaches.
quantitativeand qualitative analyses verify that tackling sub-word discrepancy can exactly alleviate the problemof exposure bias, large computational cost, andthe under-represented tokens in vanilla pretrain-ﬁnetune paradigm.
to summarize, the contribu-tions of our work are as follows:.
• through in-depth analyses, we point out andformally analyze subword discrepancy, affect-ing the conventional pretrain-ﬁnetune strategyin nlg tasks..• we propose a simple, ﬂexible, and generalizedpretrain-ﬁnetune training strategy, where anembedding generator is introduced to leveragethe knowledge of the pre-trained model toinitialize embeddings of any required tokens.
• extensive experiments show that our strategyis able to efﬁciently decrease the vocabularygaps in pretrain-ﬁnetune paradigm and signiﬁ-cantly boost the performance of nlg models..2 related work.
recent studies observe that pre-trained models suf-fer a bottleneck when they are applied to nlg tasks(edunov et al., 2019; zhu et al., 2020; rothe et al.,2020).
this problem has been attributed to manyreasons.
for example, yang et al.
(2019b) pointout pretrain-ﬁnetune discrepancy caused by the ab-sent masked frames in real data when adopting pre-trained masked language models.
chronopoulouet al.
(2019) investigate catastrophic forgetting inﬁnetuning stage.
it can be said that how to suc-cessfully employ pretrain-ﬁnetune to enhance nlgmodels remains a great challenge.
we explore thisproblem from another direction, i.e., the unsuitablesubword segmentation for downstream tasks..task-speciﬁc vocabulary a natural manner toaddress this issue is to adopt a task-speciﬁc vocabu-lary.
lewis et al.
(2020) ﬁrst replace the embedding.
6002layer with an independent encoder, of which vocab-ulary and parameters are learned from the down-stream corpus.
along this line, sato et al.
(2020) ex-ploit external monolingual data to construct a newembedding layer and achieve improvements in do-main adaptation.
this series of studies empiricallyconﬁrm the necessity of the suitable vocabulary forthe ﬁnetuning stage.
however, these methods haveto learn the task-speciﬁc embeddings separatelybefore each adaptation, which brings in additionalcomputational cost thus limiting their applicability.
besides, they completely discard the pre-trainedembeddings, which have been proved to be usefulby aji et al.
(2020).
extra encoder or embeddinglayer may fail to be well optimized with insufﬁ-cient downstream resources.
accordingly, rotheet al.
(2020) employ a task-speciﬁc vocabulary toretrain m-bert, which is then used to initializeneural machine translation (nmt) model.
consid-ering more robust approaches, kudo (2018) andprovilkov et al.
(2020) randomly sample segmenta-tions for each sentence at the training time.
unlikethe above methods, our goal is to build a plug-and-play component, that involves neither retrainingthe pre-trained model nor learning task-speciﬁcembeddings separately..embedding generator our work is also relatedto studies with respect to generating embeddingsfor out-of-vocabulary (oov) words.
in this con-text, researchers use embeddings of characters orsubwords to predict those of unseen words (pin-ter et al., 2017; zhao et al., 2018; sasaki et al.,2019; fukuda et al., 2020).
for example, zhaoet al.
(2018) train an embedding generator throughreconstructing the original representation of eachword from its bag of subwords.
sasaki et al.
(2019)progressively improve the generator using attentionmechanism.
fukuda et al.
(2020) further leveragesimilar words to enhance this procedure.
our worksigniﬁcantly differs from the above studies in twoaspects.
due to the vocabulary is ﬁxed once prede-ﬁned, the embedding reconstruction can be merelydrawn on a few of selected words.
by contrast,our generator is able to produce embeddings of anytokens, since these embeddings are directly embed-ded into the pre-trained model with an objective interms of minimizing the divergence.
moreover, pre-vious studies mainly focus on handling the problemof oov, while our work, to our best of knowledge,is the ﬁrst study that exploits embedding generatorto transfer granularity over subwords for pretrain-.
ﬁnetune paradigm..3 methodology.
in this section, we introduce our proposed pretrain-ﬁnetune strategy in detail..3.1 main steps in our strategy.
figure 1: illustration of our pretrain-ﬁnetune pipeline.
we pretrain an embedding generator for the initializa-tion of embeddings of unseen tokens.
thus, each down-stream model can adopt its suitable vocabulary insteadof the unchangeable one.
e(·) and g(·) indicate thepretrained and generated embedding, respectively..as shown in figure 1, we extend the priorpretrain-ﬁnetune paradigm with an embeddingtransfer stage.
speciﬁcally, we revise the conven-tional pretrain-ﬁnetune pipeline as follows:pretrain.
as usual, we ﬁrst construct a pre-trainedmodel using an existing large-scale corpus.
in ad-dition, we further pretrain an embedding generatorregardless of downstream tasks.
it’s expected toproduce the embedding of any required token, byfeeding pre-trained embeddings of its subwordsand hyperwords.
hence, it can be employed intoany downstream tasks for embedding transferring.
finetune.
we differently initialize the word em-beddings and the other parameters (inner layer) forthe downstream model, respectively.
for the for-mer, we use the downstream-task training corpusto learn a task-speciﬁc subword segmentation andcorresponding vocabulary.
for an unseen token,we apply the generator to produce its initial repre-sentation.
otherwise, we directly initialize it withthe corresponding pre-trained embeddings.
con-sidering the latter, we directly adapt inner-layerparameters of the pre-trained model to the down-stream model.
finally, we continue to train the.
6003inner layere(waiter)e(worker)e(moto)e(writer)e(##cycle) …pretrained modelvocabularywaiter, ##er, writer, worker, motocycle…g(motocycle)vocabularywaiter, ##er, writer, worker, motocycle…g(motocycle)pretraininitialized downstream models …extract task-specific vocabularyfinetunee(waiter)e(worker)+…e(motor)e(##cycle)+e(writer)g(##er)  g(motorcycle)embedding generatorvocabularywaiter, ##er,writer, worker,motocycle…inner layere(waiter)g(##er)e(moto)e(writer)g(motocycle) …downstream modelinner layere(waiter)g(##er)e(moto)e(writer)g(motocycle) …downstream modelinner layere(waiter) g(##er)e(writer)e(worker) g(motocycle)…downstream modelupstream corpusdownstream corpusapplyfeedinitializetraindownstream model using the ﬁnetuning data fol-lowing the common fashion..as seen, our strategy is lightweight and also ableto avoid the issue of subword discrepancy, sinceit does not require retraining for the pre-trainedmodel and can be quickly applied to various down-stream nlg models..3.2 constructing the embedding generator.
to make the word embedding generator applica-ble to all downstream nlg models, we designthe generator so that it can generate the embed-ding of any input token according to those of itsmorphologically similar tokens from the learnedpre-training vocabulary.
the basic intuition behindour design stems from this fact: if the input tokenis a complete word, like motorcycle, its semanticmeaning is related to those of its subwords, motorand ##cycle.
on the contrary, if the input token isa subword, such as ##er, the words that contain theinput token, which we call them hyperwords, e.g.,worker, writer and singer, can be exploited to learnits semantic meaning..concretely, given a mismatch token w, we bor-row the segmentation principle from pre-trainedmodel to split w into subwords based on the pre-training vocabulary, and traverse the pre-trainingvocabulary to select all longer tokens containingw. then, we combine the generated subwords andthe selected hyperwords to form the morpholog-ically similar token set of w, denoted by sm(w).
afterwards, we explore three kinds of generatorsto produce the embedding g(w) of w:.
avg-eg: averaging-based embedding gener-intuitively, we can simply deﬁne g(w) asatorthe average embedding of the words from sm(w):.
g(w) =.
1|sm(w)|.
(cid:88).
e(w(cid:48)),.
(1).
w(cid:48)∈sm(w).
where e(w(cid:48)) is the pre-trained embedding of thetoken w(cid:48).
in this way, our generator can be directlyused, without increasing the cost of training time..att-eg: attention-based embedding genera-tor another natural solution is to softly fuse in-formation from different morphologically similarwords using an attention mechanism (bahdanau.
et al., 2015).
the g(w) is formally expressed as:.
g(w) =.
1|sm(w)|.
(cid:88).
α(w(cid:48)) · e(w(cid:48)),.
α(w(cid:48)) =.
(cid:80).
w(cid:48)∈sm(w)exp(w(cid:62)e(w(cid:48)))w(cid:48)(cid:48)∈sm(w) exp(w(cid:62)e(w(cid:48)(cid:48))).
,.
(2).
where w ∈ r1×d indicates a learnable vector,d denotes the dimensionality of word embedding.
compared with the ﬁrst generator, this generatorcan be jointly trained with the pre-trained model,therefore it is capable of better quantifying the ef-fects of morphologically similar words in sm(w)..patt-eg: position-aware attention-basedembedding generator from the linguisticperspective, differentlocations of morphemesin a word reﬂect distinct semantic meaning.
consequently, we reﬁne the above attention-basedgenerator by considering six kinds of morphologyrelationships between w and w(cid:48) ∈ sm(w): if w(cid:48) isa subword of w, w(cid:48) can be the preﬁx/inﬁx/sufﬁxsubword of w. in turn, if w(cid:48) is a hyperword ofw, w can be the preﬁx/inﬁx/sufﬁx subword of w(cid:48).
formally, g(w) is produced in the following way:.
g(w) =.
1|sm(w)|.
(cid:88).
α(w(cid:48))e(w(cid:48)),.
α(w(cid:48)) =.
(cid:80).
w(cid:48)∈sm(w)exp(iwre(w(cid:48)))w(cid:48)(cid:48)∈sm(w) exp(iwre(w(cid:48)(cid:48))).
,.
(3).
where wr ∈ r6×d is a learnable parameter matrix,and i ∈ r1×6 is the one-hot vector indicating therelationship between w and w(cid:48)..note that, all the trainable generators are de-signed to lightweight architectures with a few ofparameters.
we believe this can achieve a more gen-eralizable model and speed up their convergence.
we will compare and investigate these generatorsin the subsequent experiment section..3.3 training the embedding generator.
one principle of our strategy is plug-and-play,which can be directly applied to initialize any un-seen tokens in all downstream nlg tasks, avoidingthe time cost of retraining the model.
to this end,we borrow the pre-trained model and its associatedcorpus to train our generator before ﬁnetuning..in the speciﬁc implementation, we ﬁrst prepro-cess the sentences of pre-training corpus, wheretwo kinds of preprocessing operations are applied.
6004figure 2: illustration of the knowledge distillation procedure.
our strategy ﬁrst performs a segmentation (differsfrom the pre-trained one) on the original sentence to create unseen tokens, of which embeddings can be producedby our embedding generator.
we ﬁx the inner layers of the pre-trained model and force our model to narrow thedistance between its output layer and the conventional one..to simulate unseen tokens: 1) randomly selectingsome consecutive subwords and combining theminto an unseen token; and 2) randomly choosinga token and splitting it into several consecutiveunseen tokens.
figure 2 provides an example ofsentence preprocessing, where the word nothingis randomly split into two unseen subwords nothand ##ing, while the subwords ima and ##gineare concatenated into an unseen token imagine.
through this data preprocessing, we can obtainlarge amounts of samples with unseen tokens in-volving various granularities, which facilitates therobustness of our generator..then, we embed our generator into the pre-trained model to encode unseen words, and ﬁxparameters of the pre-trained model to train thegenerator according to the following objectives:.
reusing pre-training loss the generated em-beddings should share the same latent space withthe existing embeddings, in the meanwhile, rep-resenting appropriate semantic meaning.
accord-ingly, we serve to minimize the vanilla loss of pre-trained model as the basic training objective ofour generator.
the loss function can be diverseaccording to the upstream tasks, which is denotedas lp(s(cid:48)) with s(cid:48) being the preprocessed trainingsentence..knowledge distillation we further exploitknowledge distillation (hinton et al., 2015) to nar-row the divergence between hidden states in thepre-trained model before and after applying thegenerated embeddings.
given a training examples, the vanilla pre-trained model and our generatorpreprocess it to sp and s(cid:48), respectively.
as shown.
in figure 2, we transfer the knowledge of the out-put layer in terms of sp to that of s(cid:48).
euclideandistance is adopted to measure the divergence be-tween representations output by vanilla pretrainedmodel hp(w) and that of our model h(cid:48)(w) with re-spect to the same word w. since each word may besplit into different sequences of tokens, we regardthe average hidden states of the corresponding to-ken sequence as its representation.
thus, the lossfunction can be deﬁned as:.
ld(sp, s(cid:48)) =.
||hp(w) − h(cid:48)(w)||2,.
(4).
1|s|.
(cid:88).
w∈s.
finally, we assign a hyper-parameter λ to quan-tify the effect of l(·) and ld(·), which is empiri-cally set to 0.5 as default:.
l(sp, s(cid:48)) = lp(s(cid:48)) + λld(sp, s(cid:48))..(5).
4 experiments.
in this section, we examine the effectiveness ofthe proposed strategy in a variety of nlg tasks.
we ﬁrst run a set of experiments to compare thevariants of our approach and the related methodson domain adaptation translation tasks.
then, weassess the superiority of our approach on transfer-ring the knowledge from m-bert (devlin et al.,2019) and m-bart (liu et al., 2020c) to two down-stream nlg tasks: machine translation (mt) andanswer-aware question generation (qg)..4.1 domain adaptation.
we conduct experiments on english-to-chinese(en⇒zh) domain adaptation translation tasks,where the pretrain-ﬁnetune paradigm resort as stan-dard.
the pre-training corpus is extracted from an.
6005…………g(noth)g(##ing)e(i)e(could)e(have)g(imagine)e(##d)ℎ′(noth)ℎ′(##ing)ℎ′(i)ℎ′(could)ℎ′(have)ℎ′(imagine)ℎ′(##d)ℎ!(nothing)ℎ!(i)ℎ!(could)ℎ!(have)ℎ!(ima)ℎ!(##d)ℎ!
(##gine)e(nothing)e(i)e(could)e(have)e(ima)e(##d)e(##gine)pretrained modeloriginal embeddingembedding transferpretrained modelldldldldldldout-of-domain dataset ldc†, in which 1.25m (m= million), 3k (k = thousand), 3k sentences pairsare randomly sampled as training, developmentand test set, respectively.
we verify the effective-ness of our strategy on two downstream domains:thesis and laws, of which data are collected fromum-corpus (tian et al., 2014).
we follow thesame settings as zeng et al.
(2018) and su et al.
(2021) to preprocess two corpus and train mod-els.
the translation quality is evaluated by casedbleu (papineni et al., 2002), which is caculatedby mteval-v13a.pl..implementation details all the compared meth-ods are re-implemented on top of fairseq‡ andbuilt on transformer (vaswani et al., 2017).
we ap-ply adam optimizer (kingma and ba, 2015) withβ1 and β2 being 0.9 and 0.999, respectively.
thedropout ratio is set to 0.3 and each iteration batchconsists of 25k tokens.
for both pre-training andﬁnetuning, we employ warm-up strategy where thelinear warm-up phase takes 4k steps, reaching itsmaximum learning rate to 5 × 10−4.
the trainingof each model is early-stopped to maximize bleuscore on the development set.
other hyperparame-ters are set following base setting in vaswani et al.
(2017).
we investigate the following methods: §.
• baseline: we design baselines under twobasic settings: single-run denotes that thetranslation model only trained on in-domaincorpus with the domain-speciﬁc vocabu-lary.
pretrain-finetune represents the well-known pipeline, i.e., pre-training using up-stream corpus, then ﬁnetuning on in-domaindataset via inheriting pre-training vocabulary.
• task-speciﬁc vocabulary: this group of meth-ods retrain the upstream model using a task-speciﬁc vocabulary, involving: the vocabularycollected from in-domain data (downstreamvocab, rothe et al., 2020), the joint vocabu-lary extracted from all corpus (joint vocab,nguyen and chiang, 2017), as well as thepre-trained vocabulary with a subword regu-larization process on upstream corpus for ro-bustness (bpe-drop, provilkov et al., 2020).
• embedding generator: we also examine sev-eral representatives of existing embeddinggenerators on pretrain-ﬁnetune paradigm.
we.
†including ldc2002e18, ldc2003e07, ldc2003e14,.
ldc2004t07, ldc2004t08 and ldc2005t06..‡https://github.com/pytorch/fairseq§hyperparameters that are not mentioned in our paper areset to the default according to the corresponding literatures..strategy.
thesis laws.
baseline.
single-runpretrain-finetune.
34.5130.21task-speciﬁc vocabulary31.7035.0132.41.downstream vocabjoint vocabbpe-drop.
embedding generator.
random initword2vecembedding recon.
36.3336.2136.25.
52.2152.12.
52.2352.7052.43.
53.1453.1153.01.new embedding layer.
independent encodercbow.
34.7736.12.
52.7352.93.our strategy.
avg-egatt-eg.
patt-eg.
+knowledge distillation.
+knowledge distillation.
37.0337.4037.5937.7237.90.
53.3053.3953.8753.8554.27.table 2: evaluation (bleu) of different pretrain-ﬁnetune strategies on en⇒zh domain translation tasks..assign the domain-speciﬁc vocabulary foreach downstream model, in which embed-dings of the seen tokens are reused, whilethe mismatched ones are: 1) randomly ini-tialized (random init, aji et al., 2020); 2)learned by word2vec (mikolov et al., 2013)using in-domain data; and 3) produced by agenerator trained via reconstructing embed-dings using bag-of-subwords (embeddingrecon, zhao et al., 2018)..• new embedding layer: these methods as-signed the domain-speciﬁc vocabulary foreach downstream model, but completely dis-card the embeddings of upstream models.
the new embeddings are produced from: 1)randomly initialized independent encoder(lewis et al., 2020); and 2) cbow modeltrained under the downstream corpus (satoet al., 2020)..• our strategy: our embedding generators aretrained using the setting of pre-trained modelwith one epoch, as described in § 3..note that, to eliminate the inﬂuence of control vari-ables, all the vocabulary transfers in above modelsare conducted on the decoder-side only..results table 2 lists our results on domain adap-tation tasks.
considering baseline models, imme-.
6006models.
random initw/ m-bert+ours.
w/ m-bart.
+ours.
wmt14 en⇒de.
squad v1.1 question generation.
bleu # param.
speed rouge-l bleu meteor # param.
speed382m 21.2326.08382m 21.5828.24242m 27.8629.77610m 12.6229.1330.15363m 14.41.
382m 25.64382m 26.51255m 49.54610m 19.65387m 25.79.
23.9825.8826.7648.0748.11.
2.913.313.5520.2020.27.
9.259.279.8624.2924.31.table 3: evaluation of our model on knowledge transferring tasks.
“w/” denotes “with”.
random init uses thesame architecture as “ w/ m-bert” while being initialized randomly.
“# param.” denotes the trainable parametersize of each model.
“speed” indicates the inference speed measured in sentences per second.¶.
diately ﬁnetuning a downstream model with out-of-domain vocabulary performs worse than merelytraining each model using in-domain data and task-speciﬁc vocabulary.
this is consistent with ﬁndingsin edunov et al.
(2019) and zhu et al.
(2020).
weobserve that there are over 13k and 11k tokensin the vocabulary in terms of out-of-domain aremismatched with that of thesis and laws respec-tively, indicating that subword discrepancy indeedharms the performance of downstream nlg mod-els.
when adapting task-speciﬁc vocabulary to re-train upstream models, all the translation qualitiesare improved, conﬁrming the necessity of bridgingsubword gaps between upstream and downstreammodels.
in addition, we also appraise several ex-isting embedding transfer strategies into pretrain-ﬁnetune pipeline.
interestingly, randomly initial-izing embeddings of unseen tokens yields evenslightly better results than utilizing “word2vec”and “embedding recon”.
we attribute this to thefact that the training of the latter two generatorsis individual regardless of the pre-trained model,resulting in unshared latent space between the gen-erated and pre-trained embeddings..our models surpass all baselines and relatedmethods on translation qualities.
most importantly,in contrast to existing approaches that have to eitherretrain the pre-trained model from scratch or learn aseparate embedding generator for each domain, ourstrategy can be immediately adopted to any down-stream tasks once ready.
speciﬁcally, patt-egachieves the best performance, conﬁrming our hy-pothesis that softly summarizing information frommorphologically similar tokens and consideringpositions of morphemes facilitate the embeddingtransferring.
besides, using knowledge distillationto narrow the divergence before and after apply-ing our generator can progressively improve theperformance.
accordingly, we use patt-eg +knowledge distillation as the default setting in.
subsequent experiments..4.2 knowledge transferring.
we test our method on transferring the knowledgefrom two advanced large-scale language models:non-autoregressive m-bert and autoregressivem-bart.
for computational efﬁciency, we ran-domly extract 4m samples from the conventionalpre-training corpus|| to train our embedding genera-tor using the conﬁgurations of pre-trained modelswith one epoch and 4,096 batch size.
comparisonsare conducted on machine translation and questiongeneration task.
the pre-trained model is employedon both of encoder and decoder.
same as conﬁgu-rations in domain adaptation, we merely performthe embedding transferring in decoder.
since thetwo language models exploit different segmenta-tion tools, i.e., wordpiece (wu et al., 2016) andsentencepiece (kudo, 2018), we set 32k and 10kas the number of word and sentence pieces fordownstream tasks, respectively..machine translation considering machinetranslation, we examine our method on the widelyused english-to-german (en⇒de) benchmarks:wmt14.
we follow rothe et al.
(2020) and liuet al.
(2020c) to deal this task..question generation we use the squad v1.1(rajpurkar et al., 2016) dataset for question gen-eration.
we follow the common setting to pre-process dataset and train our models (liu et al.,2020a).
the answer and the passage are taken asthe model input, while the question is the target out-put.
rouge-l (lin and hovy, 2003), bleu, andmeteor (banerjee and lavie, 2005) are treatedas the assessment metrics..results as illustrated in table 3, the randomlyinitialized nmt model yields comparable results.
¶single nvidia v100 gpu with batch size being 32.
||https://dumps.wikimedia.org.
6007figure 3: effects of different token granularities onen⇒de task.
as seen, the segmentation granularityremarkably affects inference speed and inference ece..figure 4: effects of the training steps of embeddinggenerators on bleu scores of downstream models..with the reported system with the same architec-ture (26.1 vs. 26.0, rothe et al., 2020), makingour subsequent experiments convincing.
our meth-ods signiﬁcantly boost nlg performances acrossdifferent pre-trained models, downstream tasks, lin-guistic resources, as well as segmentation tools,demonstrating its universal-effectiveness.
more-over, the embedding generator is able to decreasethe vocabulary size and the generated sentencelength, leading to less computational costs..5 analysis.
segmentedtoken.
source.
reference.
translations.
m-bert: s dan k barours: dankbarit’s very gratifying to have this kindof reception here.
ich bin sehr dankbar f¨ur denempfang hier.
m-bert: es ist sehr befriedigend,diese art von empfang hier zu haben.
ours: ich bin sehr dankbar f¨ur denempfang hier..table 4: the german word dankbar (gratifying) is oversegmented by m-bert, and mistranslated by its asso-ciated translation model.
our method can exactly ap-proach this problem via using a more suitable segmen-tation for downstream tasks..to better understand subword discrepancy andour method, we make in-depth analyses on wmten⇒de task to investigate three problems: q1:how subword granularity affects nlg models?
(§ 5.1) q2: how embedding transfer beneﬁts todownstream models?
(§ 5.2) q3: dose our strat-egy acquire large computational costs?
(§ 5.3) q4:can our strategy exactly handle under-representedtokens?
(§ 5.4).
5.1.impact of subword granularity.
figure 3 visualizes the inference speed and expo-sure bias (inference expected calibration error(ece), wang et al., 2020) of translation modelswith different token granularities in their vocabu-lary.
obviously, for a translation model, neithertoo small nor too large granularity regarding tosubwords can reach a satisfactory performance oninference speed.
at the same time, the granular-ity indeed affects the problem of exposure bias intranslation task.
the experiments conﬁrm the suit-able segmentation strategy can effectively alleviatethe problem of exposure bias..5.2.impact of embedding transfer.
we further investigate how the embedding trans-fer impacts the initialization of downstream mod-els.
we draw figure 4 to plot the bleu scoresof downstream models using the embedding gen-erators trained with different steps.
the x-axisindicates the training steps of the generator.
both“+ours” and “w/ m-bert” are fully ﬁnetuned, butthe latter doesn’t employ our embedding generator,resulting in an unchanged line.
it is encouragingto see that the bleu scores of downstream modelconverges very fast, indicating that our generatorcan be used with only a few of training steps.
weargue that the commonalities in word composition-ality lead to the fast transfer learning on generatingdifferent embeddings, and the simple architectureof our generator further speeds up such procedure..5.3 computational costs.
as shown in figure 4, our generator converges veryfast (around 20k steps).
the training process of ourgenerator takes about 2 hours under our experimen-tal setting.
as a reference, the vanilla wmt ﬁne-tuning process takes approximately 40 hours.
in.
60080.140.160.180.20.2289991091191293.33.53.73.94.1inference eceinference speed (sentence/s)averaged token lengthinference speedinference ece2828.428.829.229.630050100150200bleutraining stepsw/ m-bert+oursaddition, our generator only takes about 3 minutesfor producing 13k embeddings in thesis, which isalso insigniﬁcant compare to the ﬁnetuning time.
most importantly, once the embedding generatoris well-trained, it’s available for any downstreamtasks.
thus, we argue that the computational costsare not the obstacle to the extensibility of our ap-proach..5.4 qualitative analysis.
table 4 gives an example to show the effectivenessof our model on handling under-represented tokens.
the german word dankbar (gratifying) is over seg-mented by m-bert, and fail to be generated by themodel trained under conventional pipeline.
on thecontrary, our approach offers an opportunity for thedownstream model to preserve the word into vo-cabulary, thus better learning its semantic meaningand correctly predicting it during inference..6 conclusion.
in this paper, we point out that the one-size-ﬁts-allsubword vocabulary, despite its all-encompassingsuperiority, is not the preferred solution for thepopular pretrain-ﬁnetune paradigm.
it causes thesubword discrepancy among upstream and down-stream models, which is given concrete form tothe unsuitable granularity and under-representedwords.
consequently, we propose a novel embed-ding transfer strategy with a plug-and-play embed-ding generator.
empirical results suggest that: 1)our approach is universally effective on overcom-ing subword discrepancy; 2) embedding transfercan bring beneﬁts to computational efﬁciency; and3) embedding generator can be achieved via eitherdirectly averaging the input embeddings or apply-ing trainable components, the latter performs betterbut depends on few of training.
as our approach istransparent to model architectures and tasks, we be-lieve it can be widely applied and further raise theﬂexibility and applicability of pre-trained models.
in the future, we plan to investigate its effec-tiveness on other generation tasks, such as codegeneration (jiang et al., 2021; xie et al., 2021),summarization (shi et al., 2021) and so on..acknowledgments.
the project was supported by national natural sci-ence foundation of china (no.
62036004, no.
61672440), national key research and develop-ment program of china (no.
2018yfb1403202),.
natural science foundation of fujian province ofchina (no.
2020j06001), youth innovation fundof xiamen (no.
3502z20206059), and the funda-mental research funds for the central universities(no.
zk20720200077).
we also thank the review-ers for their insightful comments..references.
alham fikri aji, nikolay bogoychev, kennethheaﬁeld, and rico sennrich.
2020.in neural ma-chine translation, what does transfer learning trans-fer?
in acl 2020..dzmitry bahdanau, kyunghyun cho, and yoshua ben-gio.
2015. neural machine translation by jointlylearning to align and translate.
in iclr 2015..satanjeev banerjee and alon lavie.
2005. meteor:an automatic metric for mt evaluation with im-proved correlation with human judgments.
in acl2005..samy bengio, oriol vinyals, navdeep jaitly, andnoam shazeer.
2015. scheduled sampling for se-quence prediction with recurrent neural networks.
in nips 2015..colin cherry, george foster, ankur bapna, orhanfirat, and wolfgang macherey.
2018. revisitingcharacter-based neural machine translation with ca-pacity and compression.
in emnlp 2018..alexandra chronopoulou, christos baziotis,.
andalexandros potamianos.
2019. an embarrassinglysimple approach for transfer learning from pre-trained language models.
in naacl 2019..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-standing.
in acl 2019..sergey edunov, alexei baevski, and michael auli.
2019. pre-trained language model representationsfor language generation.
in acl 2019..nobukazu fukuda, naoki yoshinaga, and masaru kit-suregawa.
2020. robust backed-off estimation ofin emnlp find-out-of-vocabulary embeddings.
ings 2020..geoffrey e. hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
corr 2015, abs/1503.02531..hui jiang, chulun zhou, fandong meng, biao zhang,jie zhou, degen huang, qingqiang wu, and jinsongsu.
2021. exploring dynamic selection of branchexpansion orders for code generation.
in acl 2021..huda khayrallah, brian thompson, kevin duh, andphilipp koehn.
2018. regularized training objective.
6009for continued training for domain adaptation in neu-ral machine translation.
in proceedings of the 2ndworkshop on neural machine translation and gen-eration 2018..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in iclr 2015..tom kocmi and ondˇrej bojar.
2018. trivial transferlearning for low-resource neural machine translation.
in machine translation: research papers 2018..taku kudo.
2018. subword regularization: improvingneural network translation models with multiple sub-word candidates.
in acl 2018..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inemnlp 2018..mike lewis, yinhan liu, naman goyal, mar-jan ghazvininejad, abdelrahman mohamed, omerlevy, veselin stoyanov, and luke zettlemoyer.
2020. bart: denoising sequence-to-sequence pre-training for natural language generation, translation,and comprehension.
in acl 2020..chin-yew lin and eduard hovy.
2003..auto-matic evaluation of summaries using n-gram co-occurrence statistics.
in naacl 2003..dayiheng liu, yu yan, yeyun gong, weizhen qi,hang zhang, jian jiao, weizhu chen, jie fu, linjunshou, ming gong, pengcheng wang, jiusheng chen,daxin jiang, jiancheng lv, ruofei zhang, winniewu, ming zhou, and nan duan.
2020a.
glge: anew general language generation evaluation bench-mark.
corr 2020, abs/2011.11928..xin liu, kai liu, xiang li, jinsong su, yubin ge,bin wang, and jiebo luo.
2020b.
an iterativemulti-source mutual knowledge transfer frameworkfor machine reading comprehension.
in ijcai 2020..yinhan liu, jiatao gu, naman goyal, xian li, sergeyedunov, marjan ghazvininejad, mike lewis, andluke zettlemoyer.
2020c.
multilingual denoisingpre-training for neural machine translation.
tacl2020..tom´as mikolov, kai chen, greg corrado, and jeffreydean.
2013. efﬁcient estimation of word represen-tations in vector space.
in iclr 2013..toan q. nguyen and david chiang.
2017. transferlearning across low-resource, related languages forneural machine translation.
in ijcnlp 2017..ivan provilkov, dmitrii emelianenko, and elena voita.
2020. bpe-dropout: simple and effective subwordregularization.
in acl 2020..alec radford, jeffrey wu, rewon child, david luan,dario amodei, and ilya sutskever.
2019. languagemodels are unsupervised multitask learners.
openaiblog 2019, 1(8):9..pranav rajpurkar, jian zhang, konstantin lopyrev, andpercy liang.
2016. squad: 100,000+ questions formachine comprehension of text.
in emnlp 2016..sascha rothe, shashi narayan, and aliaksei severyn.
2020. leveraging pre-trained checkpoints for se-quence generation tasks.
tacl 2020..shota sasaki, jun suzuki, and kentaro inui.
2019.subword-based compact reconstruction of wordembeddings.
in naacl 2019..shoetsu sato, jin sakuma, naoki yoshinaga, masashitoyoda, and masaru kitsuregawa.
2020. vocabularyadaptation for domain adaptation in neural machinetranslation.
in emnlp 2020..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in acl 2016..tian shi, yaser keneshloo, naren ramakrishnan, andchandan k. reddy.
2021. neural abstractive textsummarization with sequence-to-sequence models.
trans.
data sci., 2(1):1:1–1:37..jinsong su,.
jiali zeng,.
jun xie, huating wen,yongjing yin, and yang liu.
2021. exploring dis-criminative word-level domain contexts for multi-domain neural machine translation.
ieee transac-tions on pattern analysis and machine intelligence,43(5):1530–1545..liang tian, derek f. wong, lidia s. chao, pauloquaresma, francisco oliveira, yi lu, shuo li, yim-ing wang, and longyue wang.
2014. um-corpus:a large english-chinese parallel corpus for statisti-cal machine translation.
in lrec 2014..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n. gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in nips 2017..yu wan, baosong yang, derek f. wong, yikai zhou,lidia s. chao, haibo zhang, and boxing chen.
2020. self-paced learning for neural machine trans-lation.
in emnlp 2020, pages 1074–1080..kishore papineni, salim roukos, todd ward, and wei-jing zhu.
2002. bleu: a method for automatic eval-uation of machine translation.
in acl 2002..shuo wang, zhaopeng tu, shuming shi, and yang liu.
2020. on the inference calibration of neural ma-chine translation.
in acl 2020..yuval pinter, robert guthrie, and jacob eisenstein.
2017. mimicking word embeddings using subwordrnns.
in emnlp 2017..yonghui wu, mike schuster, zhifeng chen, quoc v.le, mohammad norouzi, wolfgang macherey,maxim krikun, yuan cao, qin gao, klaus.
6010macherey, jeff klingner, apurva shah, melvin john-son, xiaobing liu, lukasz kaiser, stephan gouws,yoshikiyo kato, taku kudo, hideto kazawa, keithstevens, george kurian, nishant patil, wei wang,cliff young, jason smith, jason riesa, alex rud-nick, oriol vinyals, greg corrado, macduff hughes,google’s neural ma-and jeffrey dean.
2016.chine translation system: bridging the gap be-tween human and machine translation.
corr 2016,abs/1609.08144..binbin xie, jinsong su, xiang li, yubin ge, jianweicui, junfeng yao, and bin wang and.
2021. improv-ing tree-structured decoder training for code genera-tion via mutual learning.
in aaai 2021..hu xu, bing liu, lei shu, and philip yu.
2019. bertpost-training for review reading comprehension andaspect-based sentiment analysis.
in acl 2019..an yang, quan wang, jing liu, kai liu, yajuan lyu,hua wu, qiaoqiao she, and sujian li.
2019a.
en-hancing pre-trained language representations withrich knowledge for machine reading comprehension.
in acl 2019..zhilin yang, zihang dai, yiming yang, jaime g.carbonell, ruslan salakhutdinov, and quoc v. le.
2019b.
xlnet: generalized autoregressive pretrain-ing for language understanding.
in neurips 2019..liang yao, baosong yang, haibo zhang, boxing chen,and weihua luo.
2020. domain transfer based dataaugmentation for neural query translation.
in col-ing 2020..jiali zeng, jinsong su, huating wen, yang liu, junxie, yongjing yin, and jianqiang zhao.
2018. multi-domain neural machine translation with word-leveldomain context discrimination.
in emnlp 2018..jinman zhao, sidharth mudgal, and yingyu liang.
2018. generalizing word embeddings using bag ofsubwords.
in emnlp 2018..jinhua zhu, yingce xia, lijun wu, di he, tao qin,wengang zhou, houqiang li, and tie-yan liu.
2020.incorporating bert into neural machinetranslation.
in iclr 2020..6011