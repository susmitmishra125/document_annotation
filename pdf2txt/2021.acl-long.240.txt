unitedqa: a hybrid approach for open domain question answering.
hao cheng1∗, yelong shen2∗, xiaodong liu1, pengcheng he2,weizhu chen2, jianfeng gao11 microsoft research 2 microsoft azure ai{chehao,yeshe,xiaodl,penhe,wzchen,jfgao}@microsoft.com.
abstract.
recent work under.
to date, most oftheretrieval-reader framework for open-domainqa focuses on either extractive or generativereader exclusively.
in this paper, we study ahybrid approach for leveraging the strengthsof both models.
we apply novel techniques toenhance both extractive and generative readersbuilt upon recent pretrained neural languagemodels, and ﬁnd that proper training meth-ods can provide large improvements over pre-vious state-of-the-art models.
we demonstratethat an hybrid approach by combining answersfrom both readers can effectively take advan-tages of extractive and generative answer in-ference strategies and outperform single mod-els as well as homogeneous ensembles.
ourapproach outperforms previous state-of-the-artmodels by 3.3 and 2.7 points in exact match onnaturalquestions and triviaqa respectively..1.introduction.
open-domain question answering (qa) has been along standing problem in natural language under-standing, information retrieval, and related ﬁelds(chen and yih, 2020).
an typical open-domainqa system follows the retrieval-reader framework(chen et al., 2017; guu et al., 2020; karpukhinet al., 2020), where the relevant passages are ﬁrstretrieved from a large text corpus, and a reader mod-ule then navigates multiple passages for answerinference.
in this work, we study two paradigmsof reader modules, i.e.
extractive (karpukhin et al.,2020; guu et al., 2020) and generative (lewis et al.,2020; izacard and grave, 2021) readers.
the ex-tractive reader extracts contiguous spans from theretrieved passages whereas the generative readersequentially decodes the answer string which mightnot be contained in the retrieved passages..∗equal contribution.
recent work on open-domain qa (karpukhinet al., 2020; guu et al., 2020; lewis et al., 2020;izacard and grave, 2021) explores either an extrac-tive reader or a generative reader exclusively.
wehypothesize that extractive and generative readersadopt different answer inference strategies, thus ahybrid extractive/generative reader can be a bet-ter option for open-domain qa tasks.
as shownin figure 1, compared with prediction agreementamong only generative or extractive readers (top-left and bottom-right), the cross prediction agree-ment between extractive and generative readers(bottom-left) is relatively low (<50%).
it indicatesthat answers produced by those two types of mod-els are different and they can be complementary toeach other.
therefore, we propose a hybrid readerapproach, unitedqa, which is a simple ensembleapproach to combine the predictions from extrac-tive and generative readers.
it achieves state-of-the-art results on naturalquestions (kwiatkowski et al.,2019) and triviaqa (joshi et al., 2017)..in unitedqa, the extractive reader (unitedqa-e) and generative reader (unitedqa-g) are builtupon the pretrained language models, electra(clark et al., 2020) and t5 (raffel et al., 2020),respectively.
for the unitedqa-e, we adopt aweakly-supervised training objective to address thenoisy supervision issue caused by the heuristics-based labeling and incorporate the posterior differ-ential regularization (pdr) (cheng et al., 2021) toimprove the model robustness.
the unitedqa-gfollows the t5 fusion-in-decoder (fid) (izacardand grave, 2021) and we make two improvements:ﬁrst, we add a group of attention bias parametersinto the decoder cross-attention block to featurethe ranking information of retrieved contexts; sec-ond, we add the adversarial training (ju et al., 2019;jiang et al., 2020; pereira et al., 2021) to improvethe model generalization ability..the experimental results highlight the effec-.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3080–3090august1–6,2021.©2021associationforcomputationallinguistics3080for a given question.
for bm25, passages are en-coded as bag of words (bow), and inverse docu-ment frequencies are used as the ranking function.
for dpr, passages and questions are representedas dense vectors based on two bert (devlin et al.,2019) models.
the relevance score is then com-puted based on the dot production between thequery and passage vectors.
in this paper, we adoptthe same implementation as karpukhin et al.
(2020)for retrieving passages.
speciﬁcally, the englishwikipedia dump from dec. 20, 2018 is used as thesource documents for retrieval, with the removal ofsemi-structured data, such as tables or lists.
eachdocument is split into disjoint 100-word passagesas the basic retrieval unit.
the top-100 passagesare then passed for reading.
reading we combine the generative reader and theextractive reader to produce answer candidates overthe retrieved passages.
here, we only give a high-level description of our approach.
more detailsregarding our improved extractive and generativemodels are presented in §2.1 and §2.2 respectively.
the generative reader is based on a sequence-to-sequence model pre-trained in a forward-generation fashion on a large corpus, i.e.
t5 (raffelet al., 2020).
similar to izacard and grave (2021),the model takes the question and its relevant pas-sages as input, and then generates the answer stringtoken by token.
speciﬁcally, the concatenation ofall retrieved passages and the corresponding ques-tion is used as the encoder input.
then, the decoderperforms reasoning over the concatenation of allevidence through an attention mechanism..following state-of-the-art extractive qa mod-els (devlin et al., 2019; karpukhin et al., 2020),our extractive reader is based on a transformerneural network pre-trained with a cloze style self-supervised objective, i.e.
electra (clark et al.,2020).
here, a pair of a given question and a sup-port passage is jointly encoded into neural text rep-resentations.
these representations are then usedto deﬁne scores or probabilities of possible answerbegin and end positions, which are in turn usedto deﬁne probabilities over possible answer spans.
finally, the answer string probabilities are basedon the aggregation over all possible answer spansfrom the entire set of support passages..2.1 unitedqa-e.in §2.1.2, we give the problem deﬁnition of open-domain qa for extractive reader.
then, we detail.
figure 1: pairwise prediction agreement ratio.
g-1,g-2, g-3 and e-1, e-2, e-3 are three differentgenerative and extractive readers respectively.
all read-ers achieve similar performance (≈ 52% exact match)on naturalquestions.
higher agreement (>50%) inred and lower agreement (<50%) in gray.
the agree-ment is calculated based on exact string match..tiveness of the simple hybrid approach of unit-edqa.
with both improved extractive and genera-tive readers, unitedqa sets new state-of-the-art re-sults on two popular open-domain qa datasets, i.e.
54.7 and 70.3 in exact match on naturalquestions(kwiatkowski et al., 2019) and triviaqa (joshiet al., 2017), respectively.
it is worth noting thatour unitedqa model not only outperforms eachsingle model but also brings more pronounced im-provements over homogeneous ensembles of eitherextractive or generative readers.
last, based onour analyses, unitedqa-e and unitedqa-g haveadvantages in different cases, suggesting they mayuse different reasoning strategies..2 method.
in this section, we present the overall pipeline ofthe unitedqa system, which consists of three com-ponents: retrieval, reading, and re-ranking.
first, the retrieval module fetches a list of rele-vant passages from a wikipedia dump for a givenquestion.
then, the module of hybrid readers pro-duces answer candidates from the set of retrievedpassages.
last, the re-ranking module combinesthe answer candidates with linear interpolation andproduce the ﬁnal answer.
retrieval following karpukhin et al.
(2020), weconsider two methods, bm25 and dense passageretrieval (dpr), for retrieving the support passages.
3081g-1g-2g-3e-1e-2e-3g-1g-2g-3e-1e-2e-30.40.50.60.70.80.91.0the improvements of unitedqa-e in §2.1.2..2.1.1 extractive readergiven a question q and a set of k retrieved pas-sages p1, .
.
.
, pk, a text encoder produces con-t ∈ rn fortextualized representations: hkthe question-passage pair (q, pk) in the form of“[cls]question [sep]passage [sep]”, where[cls]and [sep]are special tokens for encodinginputs, t is the maximum sequence length of theinput text, and hki indicates the contextualized em-bedding of the i-th token in (q, pk)..1, ...hk.
b hk.
the extractive reader computes the span-beginscore of the i-th token as sb(ik) = wti usinga weight vector wb ∈ rd.
the span-end scorese(jk) is deﬁned in the same way.
thus, the prob-abilities of a start position ik and an end positionjk are pb(ik) = exp(sb(ik)), pe(jk) = exp(se(jk)),where zb, ze are normalizing factors deﬁned bythe corresponding probability space.
the probabil-ity of an answer span from ik to jk is deﬁned asps(ik, jk) = pb(ik)pe(jk)..zb.
ze.
b = (cid:80).
ik∪null exp(sb(i)), ze = zk.
here, we consider two probability spaces, pas-sage level and multi-passage level, with the onlydifference in the computing of zb, ze.
specif-ically, the passage-level probability of each an-swer begin and end is computed by normalizingall possible positions in the respective passage, i.e.
zb = zke =(cid:80)ik∪null exp(se(j)), where i k is the set of allpossible positions from the k-th passage and nullindicates special positions if pk does not support an-swering the question.
similarly, the multi-passagelevel probability is computed by normalizing overeach answer positions across all k relevant pas-sages, i.e.
zb = z∗ik exp(sb(i)), ze =ke = (cid:80)z∗ik exp(se(j)), respectively.
since there are usually multiple plausible men-tions for open-domain qa, during training, it is typ-ical to maximize either the marginal log-likelihood(mml) of all correct spans (karpukhin et al., 2020)or the log-likelihood of the most likely correctspan (hardem) (min et al., 2019).
during infer-ence, the prediction is made based on the candi-date answer string score, obtaining as pa(y) =(cid:80)(i,j)∈y ps(i, j), where y is the set of spans cor-.
b = (cid:80).
(cid:80).
(cid:80).
k.responding to the answer string y..multi-objective for weakly-supervised qa themulti-objective formulation is introduced in chenget al.
(2020) for improving weakly superviseddocument-level qa.
different from cheng et al.
(2020) where only mml is considered for themulti-objective formulation, we found combin-ing hardem with mml is more effective foropen-domain qa based on our experiments (§4.1).
speciﬁcally, we combine a multi-passage hardemloss with k passage-level mml losses over a batchof k passages.
p m.s (i, j) +.
lext = log max(i,j)(cid:88).
1k.(cid:88).
log.
s (ik, jk),p p.(1).
k.(ik,jk).
s , p p.s is the multi-passage level and pas-.
where p msage level span probabilities respectively.
posterior differential regularization due to thenoisy supervision for open-domain qa (chen et al.,2017), we investigate the posterior differential reg-ularization (pdr) (cheng et al., 2021) to improvethe robustness of the extractive reader.
differentfrom cheng et al.
(2021) where only clean supervi-sion setting is considered, in this work, we applypdr to the weakly supervised open-domain qascenario.
given it is computationally expensive toenumerate all possible spans, we apply two sepa-rate regularization terms for the begin and end prob-abilities at the multi-passage level, respectively,.
lpdr = d(pb(i)|p (cid:48).
b(i)) + d(pe(j)|p (cid:48).
e(j)),.
(2).
b, p (cid:48).
where d(·|·) is the squared hellinger distance,and p (cid:48)e are the probabilities of start and endpositions with additive input noise to the tokenembeddings.
speciﬁcally, we sample noise vec-tors (cid:15)1, .
.
.
, (cid:15)t from n (0, c2i), and add themto the token embeddings as the noisy input, i.e.
v1 + (cid:15)1, .
.
.
, vt + (cid:15)t , where c is ﬁxed to 1e−3throughout our experiments..based on this, the overall training objective for.
the extractive reader is.
l1 = lext + γlpdr,.
(3).
where γ is a regularization scalar hyperparameter..2.1.2.improvement method.
2.2 unitedqa-g.in addition to better text representations from clarket al.
(2020), we consider two methods for improv-ing the training of the extractive reader..here, we ﬁrst formally deﬁne the setup of genera-tive reader for open-domain qa in § 2.2.1 and thenpresent our improvements in § 2.2.2..30822.2.1 generative readergiven a question q and a set of k retrieved pas-sages p1, .
.
.
, pk, the encoder model encodes each(q, pk) pair independently, and produces contextu-i ∈ rd foralized representation for each token: hkthe i-th token of the k-th pair.
the decoder thenperforms attention over the concatenation of therepresentations of all the retrieved passages, andgenerates the answer string..let x denote the input of the question and all re-trieved passages x = (cid:0)(q, p1), ..., (q, pk)(cid:1), and ythe answer string with its tokens as (y1, ..., yn ).
the generative reader is trained to maximize asequence-to-sequence objective for a given (x, y),.
l(x, y; θ) =.
logpθ(yi|x, y1:i−1),.
(4).
n(cid:88).
i.where θ is the model parameter.
during inference,a greedy decoding is used to produce the answer..improvement method.
2.2.2decoder attention bias the decoder in the t5transformer model adopts a cross-attention mecha-nism to compute attention scores between the de-coding answer tokens and all the retrieved passagetokens.
speciﬁcally, let yi ∈ rd be the queryj ∈ rdvector of the i-th decoding token1, and mkbe the key vector of the j-th token in ((q), pk).
the multi-head cross-attention scores in t5 (raffelet al., 2020) sk.
i,j is calculated asi,j = multiheadatt(yi, mksk.
j ) ∈ r|head|.
(5).
where |head| is the number of attention heads.
however, it doesn’t capture the relevance informa-tion of retrieved passages into the reader in (5).
toadd the relevance feature into the attention block,we revise (5) by incorporating the attention bias.
i,j = multiheadatt(yi, mksk.
j ) + bk,.
(6).
where bk ∈ r|head| is a trainable attention biasvector for all the tokens in the k-th retrieved pas-sage.
in the experiments, the maximum retrievedpassages is by default set to 100. thus, the decoderattention bias introduces additional 100 ∗ |head|parameters for each layer.
adversarial training adversarial training createsadversarial examples by adding small perturba-tions to the embedding layer.
assuming the word(-piece) embedding layer is parameterized by a ma-trix v ∈ r|v |×d, |v | is the vocabulary size, and d.1we omit the layer notation for simpliﬁcation.
dataset.
train.
dev.
test.
nqtriviaqaeffcientqa.
7916878785-.
875788371800.
361011313-.
table 1: number of questions in each qa dataset..is the embed-dimension.
the adversarial embed-ding matrix ˆv can be obtained by.
gv = −∇vl(x, y; θ),ˆv = v + sg((cid:15)gv/||gv||2),.
(7).
(8).
where sg(·) is the stop-gradient operation.
we usethe adversarial embedding matrix ˆv to replace theoriginal v in model parameters θ, and obtain ˆθ.
thus the adversarial loss can be calculated as.
lat(x, y; θ) = l(x, y; ˆθ)..(9).
therefore, the overall training objective of the.
generative reader is.
l2 = αl(x, y; θ) + βlat(x, y; θ),.
(10).
where α = 0.5, β = 0.5 in all of the exepriments..2.3 unitedqa system.
the unitedqa system combines outputs from bothextractive and generative models for a given ques-tion during inference.
since the output spaces ofextractive and generative models are different, weuse a simple linear interpolation based on best pre-dictions from each model2.
denote the predictedstrings from m extractive and n generative mod-els as yen , respectively.
thehybrid prediction y∗ is obtained by.
m and yg.
1 , ..., yg.
1 , ..., ye.
argmaxy∈y.
τ.m(cid:88).
m=1.
n(cid:88).
n=1.
1(y, ye.
m) + δ.
1(y, yg.
n ),.
(11).
where y is the set of all predicted strings, 1(y, y(cid:48))is an indicator function and τ = 0.6, δ = 0.4..3 experiments.
3.1 experiment setup.
we use two representative qa datasets and adoptthe same training/dev/testing splits as in previous.
2we have also tried a few more complex approaches forcombining the extractive and generative models.
for example,we ﬁrst train an extractive model, and then append the top-kanswer strings from the extractive model at the end of theinput for training a generative model.
none of them is as goodas the simple ensemble approach..3083model.
reader type reader size (m).
nq.
triviaqa.
realm(guu et al., 2020)rag(lewis et al., 2020).
dpr(karpukhin et al., 2020)t5-fidbase(izacard and grave, 2021)t5-fidlarge(izacard and grave, 2021).
unitedqa-ebase (ours)unitedqa-elarge (ours)unitedqa-glarge(ours).
unitedqa-elarge++ (ours)unitedqa-glarge++ (ours)unitedqa (ours).
extractivegenerative.
extractivegenerativegenerative.
extractiveextractivegenerative.
ensembleensemblehybrid.
110400.
110220770.
110330770.
3x3303x7702x770+330.
40.444.5.
41.548.251.4.
47.751.852.3.
52.453.354.7.n/a56.1.
57.965.067.6.
66.368.968.6.
69.669.270.5.table 2: comparison to state-of-the-art models on the test sets of naturualquestions (nq) and triviaqa.
exactmatch score is used for evaluation.
the overall best model is in box , the best single model is in bold, and thebest model with the smallest reader size is in underline..work (lee et al., 2019; karpukhin et al., 2020).
both datasets (see table 1 for statistics) have beenheavily studied in recent work (lee et al., 2019;min et al., 2019; karpukhin et al., 2020; guu et al.,2020).
we follow the standard evaluation protocoland use exact match (em) as the evaluation metric..naturalquestions (kwiatkowski et al., 2019) iscomposed of questions by real users to googlesearch, each with answers identiﬁed by human an-notators in wikipedia.
the open-domain versionof naturalquestions (lee et al., 2019) only con-sider questions with short answers, i.e.
answerswith less than 5 tokens.
in the naturualquestions,the questions are considered to be more informa-tion seeking given that the question askers didn’tknow the answer beforehand.
in addition, we useanother evaluation set, i.e.
the dev set introducedrecently by the efﬁcientqa competition (min et al.,2021), which is constructed in the same way as theoriginal naturalquestions dataset..triviaqa (joshi et al., 2017) contains triviaquestion-answer pairs that were scraped from theweb.
different from naturalquestions, the ques-tions here are written with known answers in mind.
speciﬁcally, the unﬁltered set has been used fordeveloping open-domain qa models..implementation details for a fair comparison, weuse the same retrieval module as karpukhin et al.
(2020) for naturalquestions and triviaqa to mit-igate the impact of retrieval difference.
speciﬁ-cally, we use dpr (single) for naturalquestionsand bm25+dpr (multi) for triviaqa because of.
their best end-to-end performance (karpukhin etal.
2020).
for all the experiments, we use 8 and16 v100-32gb for base and large model trainingrespectively.
we train our models with adam op-timizer of a linear scheduler with a warmup raitoof 0.1. the extractive models are trained for upto 8 epochs with a learning rate of 2e−5 and abatch passage size per question of 16. the genera-tive models are trained for up to 10 epochs with alearning rate of 1e−4, a batch size of 64, and 100retrieved passages per question for model training.
we select γ in {4, 8}.
after the best conﬁgurationis selected based on the dev set, we run our bestmodels 3 times independently with different ran-dom seeds and report the median performance onthe test set.
we also report ensemble results whichare based on the linear interpolation over answerpredictions from the 3 models..3.2 main results.
single model results: we ﬁrst compare our mod-els to two recent models, realm (guu et al.,2020) and rag (lewis et al., 2020), which areﬁrst pre-trained with different retrieval augmentedobjectives and then ﬁne-tuned for open-domainqa.
in addition, we include as baselines dpr(karpukhin et al., 2020) and t5-fid (izacard andgrave, 2021), both of which are based on the sameretriever as ours.
as shown in table 2, both ourextractive and generative models achieve new state-of-the-art results for both studied datasets.
com-pared with the recent state-of-the-art extractive.
3084model (dpr), our base model leads to pronounced15% relative improvements for both naturalques-tions (+6.2 absolute improvement) and triviaqa(+8.4 absolute improvement).
more importantly,unitedqa-ebase achieves comparable or even bet-ter performance with regard to generative modelsof larger size, i.e.
rag and t5-fidbase.
it high-lights the importance of proper training strategiesfor open-domain qa models..hybrid model results: in order to evaluate theadvantage of the hybrid of the extractive and gen-erative models (unitedqa), we include two ho-mogeneous ensemble baselines, one consisting ofonly extractive readers (unitedqa-e++) and theother ensemble of exclusively generative models(unitedqa-g++).
for homogeneous ensemblecases, the three-way majority prediction is used.
for the hybrid of extractive and generative read-ers, we select a three-model combination from theset of three generative and three extractive modelsbased on the dev set.
we observed that combiningpredictions from two generative models and one ex-tractive model results in the best hybrid model forboth datasets.
as expected, all ensemble modelsshow an improvement over their single model coun-terparts.
however, the two homogeneous ensem-ble baselines, unitedqa-e++ and unitedqa-g++,only provide marginal gains over the correspondingbest single models.
the signiﬁcant improvementbrought by our proposed hybrid approach indicatesthe beneﬁt of combining extractive and generativereaders for open-domain qa..discussion: although the proposed hybrid ap-proach has been shown to be highly effective foropen-domain qa, we point out that the improvedperformance comes with increased computationalcost.
the best combination requires approximatelythree times the computational cost of a single gen-erative model.
therefore, it would be interestingto explore more efﬁcient hybrid methods, such aseffective parameter sharing strategies or uniﬁed for-mulations.
another interesting future direction isto explore customized compression approaches forreducing the model size of retriever and reader sep-arately or jointly through pruning (han et al., 2016),quantization (hubara et al., 2018), and knowledgedistillation (hinton et al., 2015).
speciﬁcally, giventhat the hybrid model is more effective, it is likelythat a student model can learn more effectivelyfrom a hybrid teacher model via knowledge distil-lation for open-domain qa..model.
nq triviaqa.
(cheng et al., 2020) +pdr 43.344.2bertbase43.541.840.6.
-multi-obj-pdr-multi-obj & pdr.
unitedqa-ebase-multi-obj-pdr-multi-obj & pdr.
46.045.243.142.5.
60.162.261.360.258.5.
65.464.363.861.2.table 3: ablation experiments of the extractive modelon the dev sets of naturalquestions (nq) and trivi-aqa.
exact match score is reported.
the top and bot-tom models are built on bertbase and electrabase,respectively..4 analysis.
in this section, we ﬁrst carry out ablation studyon the extractive and generative model improve-ments.
moreover, we aim to take a deeper look andunderstand the difference between the two models..4.1 ablation study.
in table 3, we present ablation experiments on theeffectiveness of different textual representationsand methods for improving the extractive modelunitedqa-ebase.
here, we focus on base models,i.e.
bertbase and electrabase.
note that therow unitedqa-ebase is the corresponding basemodel reported in table 2. compared with themml-based multi-objective (cheng et al., 2020),we ﬁnd that a new multi-objective with hardemat the multi-passage level and mml at the passagelevel is more effective for open-domain qa.
in ad-dition to the multi-objective training, there is a no-ticeable improvement brought by the regularizationmethod (pdr) which indicates the importance ofproper regularization for learning with noisy super-vision.
last but not least, the large improvement ofelectra over bert indicates the importance ofderiving better text representations for weakly su-pervised nlp problems.
for the unitedqa-g, wepresent the ablation study on analyzing the effec-tiveness of decoder attention bias component andadversarial training mechanism in table 4. bothtechniques contribute to decent improvements overt5-fid with more pronounced gains brought byadversarial training..3085model.
nq triviaqa.
t5-fidlargeunitiedqa-glarge.
51.452.3-adv training52.0-attention bias 51.8.
67.668.668.268.1.table 4: ablation experiments of the generative modelon the test sets of naturalquestions (nq) and trivi-aqa.
exact match score is reported..top-20 top-100 ∆.
nq.
triviaqa.
retrievalunited-eunited-g.retrievalunited-eunited-g.78.449.849.3.
79.967.165.4.
85.451.852.3.
84.468.968.6.
+9%+4%+6%.
+6%+3%+5%.
table 5: retieval top-k accuracy and end-to-end qaextact match scores on the test sets of naturalquestions(nq) and triviaqa.
united-e and united-g stand forour extractive and generative models respectively..their superior performance, we again only con-sider our improved extractive and generative mod-els, i.e.
unitedqa-elarge and unitedqa-g respec-tively.
the evaluation is summarized in table 6. incomparison to their corresponding overall perfor-mance, both the extractive and generative modelsachieve much better performance on the “overlap”categories (i.e.
“question overlap” and “answeroverlap”) for both naturalquestions and trivaqa,which indicates that both models perform wellfor question and answer memorization.
differentfrom question and answer memorization, there isa pronounced performance drop for both modelson the“answer overlap only” category where cer-tain amount of relevance inference capability isrequired to succeed.
lastly, we see that both extrac-tive and generative models suffer some signiﬁcantperformance degradation for the “no overlap” col-umn which highlights model’s generalization eval-uation.
nevertheless, the extractive model demon-strate a better qa generalization by achieving abetter overall performance on the “no overlap”category for both datasets..4.2.impact of retrieval accuracy.
4.4 error analysis.
here, we vary the number of retrieved passagesduring inference and report the evaluation resultsin terms of end-to-end qa exact match score ofunitedqa-e and unitedqa-g along with the cor-responding top-k retrieval accuracy.
the resultsare summarized in table 5. as expected, whenthe number of retrieved passages increases, bothtop-k retrieval accuracy and the end-to-end qa per-formance improve.
however, there is a noticeablegap between the improvement of retrieving morepassages (i.e., recall) and that of the correspond-ing end-to-end qa performance, especially for theextractive reader.
this is likely caused by addi-tional noise introduced with improved retrieval re-call.
speciﬁcally, only half of the retriever improve-ment can be effectively utilized by the extractivemodel while the generative model can beneﬁt morefrom retrieving more passages.
this suggests thatby concatenating all passages in vector space, thegenerative model are more effective in de-noisingin comparison to the extractive model..4.3 breakdown evaluation.
following lewis et al.
(2021), we carry out a break-down evaluation of model performance over thenaturalquestions and triviaqa test sets.
given.
here, we conduct analyses into prediction errorsmade by the extractive and generative models basedon automatic evaluation.
for this study, we use theefﬁcientqa dev set (min et al., 2021) which isconstructed in the same way as the original natu-ralquestions dataset.
speciﬁcally, we group pre-diction errors into three categorizes: 1) commonprediction errors made by both the extractive andgenerative models, 2) prediction errors made bythe extractive model, 3) prediction errors producedby the generative model.
in the following, we ﬁrstcarry out a manual inspection into the common er-rors.
then, we compare the prediction errors madeby extractive and generative models, respectively.
first of all, there is an error rate of 29% of thoseconsensus predictions made by both extractive andgenerative models according to the automatic eval-uation.
based on 30 randomly selected examples,we ﬁnd that around 30% of those predictions areactually valid answers as shown in the top part oftable 7. in addition to predictions that are answersat different granularity or semantically equivalentones, some of those prediction errors are likelycaused by the ambiguity in questions.
as the givenexample in table 7, based on the speciﬁcity, themodel prediction is also a valid answer.
this high-.
3086dataset.
model.
total.
questionoverlap.
noquestionoverlap.
answeroverlap.
answeroverlaponly.
nooverlap.
nq.
triviaqa.
unitedqa-g 52.351.8unitedqa-e.unitedqa-g 68.668.9unitedqa-e.72.269.4.
88.489.3.
40.541.5.
62.562.7.
62.760.1.
78.178.6.
45.445.1.
69.670.6.
34.037.6.
44.544.3.table 6: breakdown evaluation on naturalquestions (nq) and triviaqa based on test splits deﬁned in (lewiset al., 2021).
exact match scores are reported.
unitedqa-e and unitedqa-g denote our extractive and generativemodels respectively..valid answers.
different granularity.
semantically equivalent.
ambiguity question.
q: when was harry potter and the deathly hallows part 2 movie releasedprediction: 2011 / gold: 15 july 2011q: minimum age limit for chief justic of indiaprediction: 65 / gold: 65 yearsq: who won her ﬁrst tennis grand slam in 2018prediction: carolin wozniacki / gold: simona halep.
wrong answers.
part as whole error.
entity confusion.
event confusion.
q: the ofﬁcial u.s. poverty line is based on the cost of whatprediction: food / gold: icp purchasing powerq: actor who played tommy in terms of endearmentprediction: jeff daniels / gold: troy bishopq: when did the saskatchewan roughriders last won the grey cupprediction: 2007 / gold: 2013.table 7: examples of prediction errors as judged by the automatic evaluation..lights the limitation of the current evaluation met-ric, which does not accurately estimate the existingopen-domain qa system capabilities.
as shown inthe bottom part of table 7, most of representativeerrors are due to the confusion of related concepts,entities or events that are mentioned frequently to-gether with the corresponding gold answers..next, all questions from the dev set are catego-rized based the wh question word, i.e.
what, which,when, who, how, where.
we then report the relativeperformance change of each wh category for bothextractive and generative models over their corre-sponding overall prediction accuracy in figure 2.first, it is easy to see that both extractive and gen-erative models achieve the best performance forentity related who questions, which is likely to bethe result of high ratio of samples of this type seenduring training.
in contrast, the answers to whatquestions can play a much richer syntactic role incontext, making it more difﬁcult for both extractive.
and generative models to perform well.
interest-ingly, the generative model exhibits the strength fortemporal reasoning, whereas the extractive modeldoes not.
this difference suggests that it is worthexploring better temporal modeling strategies toimprove the extractive model in the future..5 related work.
open-domain qa open-domain qa requires asystem to answer questions based on evidenceretrieved from a large corpus such as wikipedia(voorhees, 2000; chen et al., 2017).
recentprogress has been made towards improving evi-dence retrieval through both sparse vector modelslike tf-idf or bm25 (chen et al., 2017; min et al.,2019), and dense vector models based on bert(lee et al., 2019; karpukhin et al., 2020; guu et al.,2020; qu et al., 2021).
generally, the dense repre-sentations complement the sparse vector methodsfor passage retrieval as they can potentially give.
3087tive open-domain qa, where the input passages aregiven by a retrieval model and are typically fromdifferent documents..6 conclusion.
in this study, we propose a hybrid model for open-domain qa, called unitedqa, which combinesthe strengths of extractive and generative readers.
we demonstrate the effectiveness of unitedqa ontwo popular open-domain qa benchmarks, natu-ralquestions and triviaqa.
our results show thatthe proposed unitedqa model signiﬁcantly outper-forms single extractive and generative models aswell as their corresponding homogeneous ensem-bles, and sets new state-of-the-art on both bench-marks.
we also perform a comprehensive empiricalstudy to investigate the relative contributions of dif-ferent components of our model and the techniqueswe use to improve the readers..for future work, it would be interesting to ex-plore model compression approaches for reducingthe model size of retriever and reader separately orjointly through pruning, quantization, and knowl-edge distillation..acknowledgments.
we would like to thank the anonymous reviewersfor valuable suggestions, yuning mao for valuablediscussions and comments, and microsoft researchtechnology engineering team for computing sup-port..references.
danqi chen, adam fisch, jason weston, and antoinebordes.
2017. reading wikipedia to answer open-in proceedings of the 55th an-domain questions.
nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1870–1879. association for computational linguistics..danqi chen and wen-tau yih.
2020. open-domainquestion answering.
in proceedings of the 58th an-nual meeting of the association for computationallinguistics: tutorial abstracts, pages 34–37, on-line.
association for computational linguistics..improved models.
hao cheng, ming-wei chang, kenton lee, andprobabilistic assump-kristina toutanova.
2020.for distantly-tions matter:supervised document-level question answering.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5657–5667, online.
association for computational lin-guistics..figure 2: relative accuracy of different wh questions.
the relative accuracy is the relative change of a whcategory accuracy to the overall model accuracy..high similarity to semantically related text pairs,even without exact lexical overlap.
unlike mostwork focusing on a pipeline model, lee et al.
(2019)propose a pre-training objective for jointly trainingboth the retrieval encoder and reader.
it is fur-ther extended by guu et al.
(2020) with a dynamicupdate of the passage index during the training.
in-stead, in this work, we focus on a hybrid readerapproach for open-domain qa.
by simply comb-ing answer predictions from extractive and gener-ative models, our unitedqa achieves signiﬁcantimprovements over state-of-the-art models.
reading comprehension with noisy labelsthere has been a line of work on improvingdistantly-supervised reading comprehension mod-els by developing learning methods and model ar-chitectures that can better use noisy labels.
mostof them focus on the document-level qa, whereall paragraphs share the same document context.
clark and gardner (2018) propose a paragraph-pair ranking objective for learning with multipleparagraphs so that the model can distinguish rele-vant paragraphs from irrelevant ones.
in (lin et al.,2018), a coarse-to-ﬁne model is proposed to han-dle label noise by aggregating information fromrelevant paragraphs and then extracting answersfrom selected ones.
min et al.
(2019) propose ahard em learning scheme where only passage-levelloss is considered for document-level qa.
more re-cently, different probabilistic assumptions with cor-responding training and inference methods are ex-amined in (cheng et al., 2020) again for document-level qa with distant supervision.
in our work,we further extend the multi-objective formulationproposed in (cheng et al., 2020) with the hard emlearning (min et al., 2019) for enhancing extrac-.
3088whatwhichwhenwhohowwhere0.100.050.000.050.10relative accuracygenerativewhatwhichwhenwhohowwhereextractivehao cheng, xiaodong liu, lis pereira, yaoliang yu,and jianfeng gao.
2021. posterior differential regu-larization with f-divergence for improving model ro-bustness.
in proceedings of the 2021 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 1078–1089, online.
association forcomputational linguistics..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledregularized optimization.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, pages 2177–2190, online.
asso-ciation for computational linguistics..christopher clark and matt gardner.
2018. simpleand effective multi-paragraph reading comprehen-sion.
in proceedings of the 56th annual meeting ofthe association for computational linguistics (vol-ume 1: long papers), pages 845–855.
associationfor computational linguistics..kevin clark, minh-thang luong, quoc v. le, andchristopher d. manning.
2020. electra: pre-training text encoders as discriminators rather thanin international conference on learn-generators.
ing representations (iclr)..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..kelvin guu, kenton lee, zora tung, panupong pa-supat, and mingwei chang.
2020. retrieval aug-in proceed-mented language model pre-training.
ings of the 37th international conference on ma-chine learning, volume 119 of proceedings of ma-chine learning research, pages 3929–3938.
pmlr..song han, huizi mao, and william j. dally.
2016.deep compression: compressing deep neural net-work with pruning, trained quantization and huff-in 4th international conference onman coding.
learning representations, iclr 2016, san juan,puerto rico, may 2-4, 2016, conference track pro-ceedings..geoffrey hinton, oriol vinyals, and jeffrey dean.
2015. distilling the knowledge in a neural network.
in nips deep learning and representation learn-ing workshop..itay hubara, matthieu courbariaux, daniel soudry,ran el-yaniv, and yoshua bengio.
2018. quantizedneural networks: training neural networks with lowprecision weights and activations.
journal of ma-chine learning research, 18(187):1–30..mandar joshi, eunsol choi, daniel weld, and lukezettlemoyer.
2017. triviaqa: a large scale dis-tantly supervised challenge dataset for reading com-prehension.
in proceedings of the 55th annual meet-ing of the association for computational linguistics(volume 1: long papers), pages 1601–1611, van-couver, canada.
association for computational lin-guistics..ying ju, fubang zhao, shijie chen, bowen zheng,xuefeng yang, and yunfeng liu.
2019. technicalreport on conversational question answering..vladimir karpukhin, barlas oguz, sewon min, patricklewis, ledell wu, sergey edunov, danqi chen, andwen-tau yih.
2020. dense passage retrieval foropen-domain question answering.
in proceedings ofthe 2020 conference on empirical methods in nat-ural language processing (emnlp), pages 6769–6781, online.
association for computational lin-guistics..tom kwiatkowski, jennimaria palomaki, olivia red-ﬁeld, michael collins, ankur parikh, chris al-berti, danielle epstein, illia polosukhin, jacob de-vlin, kenton lee, kristina toutanova, llion jones,matthew kelcey, ming-wei chang, andrew m. dai,jakob uszkoreit, quoc le, and slav petrov.
2019.natural questions: a benchmark for question an-swering research.
transactions of the associationfor computational linguistics, 7:453–466..kenton lee, ming-wei chang, and kristina toutanova.
2019. latent retrieval for weakly supervised opendomain question answering.
in proceedings of the57th annual meeting of the association for compu-tational linguistics, pages 6086–6096.
associationfor computational linguistics..patrick lewis, ethan perez, aleksandra piktus, fabiopetroni, vladimir karpukhin, naman goyal, hein-rich küttler, mike lewis, wen-tau yih, tim rock-täschel, sebastian riedel, and douwe kiela.
2020.retrieval-augmented generation for knowledge-in advances in neural infor-intensive nlp tasks.
mation processing systems, volume 33, pages 9459–9474. curran associates, inc..gautier izacard and edouard grave.
2021. leveragingpassage retrieval with generative models for opendomain question answering.
in proceedings of the16th conference of the european chapter of the as-sociation for computational linguistics: main vol-ume, pages 874–880, online.
association for com-putational linguistics..patrick lewis, pontus stenetorp, and sebastian riedel.
2021. question and answer test-train overlap inin pro-open-domain question answering datasets.
ceedings of the 16th conference of the europeanchapter of the association for computational lin-guistics: main volume, pages 1000–1008, online.
association for computational linguistics..3089yankai lin, haozhe ji, zhiyuan liu, and maosong sun.
2018. denoising distantly supervised open-domainquestion answering.
in proceedings of the 56th an-nual meeting of the association for computationallinguistics (volume 1: long papers), pages 1736–1745..sewon min,.
jordan boyd-graber, chris alberti,danqi chen, eunsol choi, michael collins, kelvinguu, hannaneh hajishirzi, kenton lee, jenni-maria palomaki, colin raffel, adam roberts, tomkwiatkowski, patrick lewis, yuxiang wu, hein-rich küttler, linqing liu, pasquale minervini, pon-tus stenetorp, sebastian riedel, sohee yang, min-joon seo, gautierizacard, fabio petroni, lu-cas hosseini, nicola de cao, edouard grave,ikuya yamada, sonse shimaoka, masatoshi suzuki,shumpei miyawaki, shun sato, ryo takahashi, junsuzuki, martin fajcik, martin docekal, karel on-drej, pavel smrz, hao cheng, yelong shen, xi-aodong liu, pengcheng he, weizhu chen, jian-feng gao, barlas oguz, xilun chen, vladimirkarpukhin, stan peshterliev, dmytro okhonko,michael schlichtkrull, sonal gupta, yashar mehdad,and wen tau yih.
2021. neurips 2020 efﬁcientqacompetition: systems, analyses and lessons learned..sewon min, danqi chen, hannaneh hajishirzi, andluke zettlemoyer.
2019. a discrete hard em ap-proach for weakly supervised question answering.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 2851–2864, hong kong, china.
association for computa-tional linguistics..lis pereira, xiaodong liu, hao cheng, hoifung poon,jianfeng gao, and ichiro kobayashi.
2021. tar-geted adversarial training for natural language under-standing.
in proceedings of the 2021 conference ofthe north american chapter of the association forcomputational linguistics: human language tech-nologies, pages 5385–5393, online.
association forcomputational linguistics..yingqi qu, yuchen ding, jing liu, kai liu, ruiyangren, wayne xin zhao, daxiang dong, hua wu,and haifeng wang.
2021. rocketqa: an opti-mized training approach to dense passage retrievalin proceed-for open-domain question answering.
ings of the 2021 conference of the north ameri-can chapter of the association for computationallinguistics: human language technologies, pages5835–5847, online.
association for computationallinguistics..colin raffel, noam shazeer, adam roberts, kather-ine lee, sharan narang, michael matena, yanqizhou, wei li, and peter j. liu.
2020. exploringthe limits of transfer learning with a uniﬁed text-to-text transformer.
journal of machine learning re-search, 21(140):1–67..ellen voorhees.
2000. the trec-8 question answer-.
ing track report..3090