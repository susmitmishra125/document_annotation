multimodal multi-speaker merger & acquisitionfinancial modeling: a new task, dataset, and neural baselines.
ramit sawhney†∗, mihir goyal†∗, prakhar goel†∗, puneet mathur‡, rajiv ratn shah†† department of computer engineering, iiit delhi‡ university of maryland, college park{ramits, mihir17166, prakhar17306, rajivratn}@iiitd.ac.in‡puneetm@umd.edu.
abstract.
risk prediction is an essential task in ﬁnan-cial markets.
merger and acquisition (m&a)calls provide key insights into the claims madeby company executives about the restructuringof the ﬁnancial ﬁrms.
extracting vocal andtextual cues from m&a calls can help modelthe risk associated with such ﬁnancial activi-ties.
to aid the analysis of m&a calls, wecurate a dataset of conference call transcriptsand their corresponding audio recordings forthe time period ranging from 2016 to 2020.we introduce m3anet, a baseline architecturethat takes advantage of the multimodal multi-speaker input to forecast the ﬁnancial risk asso-ciated with the m&a calls.
empirical resultsprove that the task is challenging, with the pro-posed architecture performing marginally bet-ter than strong bert-based baselines.
we re-lease the m3a dataset and benchmark modelsto motivate future research on this challengingproblem domain..1.introduction.
mergers and acquisitions (m&as)1 conferencecalls are events preceding ﬁnancial transactionsinvolving two or more entities such that eitherone of the participant companies takes over theother(s) and establishes itself as the owner (termedas ”acquisition”) or when one company combineswith another to become a joint entity (termed as”merger”).
in these m&a conference calls, theparticipating companies’ management makes a pre-sentation to the call participants, such as marketanalysts, media personnel, and other stakeholders,explaining the rationale for the deal and possibleroadblocks to deal completion (dasgupta et al.,2020).
following the presentation segment, thereis a q&a segment in which the call participantsask questions to which the management responds..∗ equal contribution1https://www.investopedia.com/mergers-and-acquisitions.
figure 1: a schematic of our proposed approach (m3a)that leverages three types of input modalities: text utter-ances from the call transcripts, audio clips, and speakerspeciﬁc input, for ﬁnancial modeling tasks..building on the important.
information thatm&as provide, academic research, the ﬁnancialpress, and other media give a great deal of atten-tion.
one of these discussions’ principal aspectslies in how the deals may affect the company’svaluation (moeller et al., 2003; fraunhoffer et al.,2018) and future growth.
a signiﬁcant focus inﬁnancial and economic literature has been on un-derstanding whether m&as create or destroy value.
consequently, shareholders critically analyze thedeals to estimate the potential stock price and stockprice volatility post the m&a conference call..identifying the gap in natural language process-ing (nlp) literature on the lack of resources tostudy m&a conference calls with their text tran-scripts and audio recordings, we take the ﬁrst stepin multimodal ﬁnancial modeling in the m&aspace.
such data can allow academicians to studym&a calls further, especially with the rich multi-modal data.
it shall enable studies that focus notonly on the words spoken in the call but also in themanner they were spoken, a relatively unexploredﬁeld in ﬁnancial forecasting, as shown in figure 1.a salient aspect of conference calls is that, unliketext reports, the company’s management interactswith external stakeholders and asks questions.
this.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages6751–6762august1–6,2021.©2021associationforcomputationallinguistics6751figure 2: m&a calls have a q&a session where ﬁnancial stakeholders can ask questions to the company execu-tives.
in such sessions, company executives have to be impromptu with their responses, allowing informal words toseep in.
this example q&a session is from the call regarding the acquisition of 21st century fox by disney, datedjune 20, 2018. in the example, an analyst poses a few questions to the company executives (depicted in yellow).
the ceo of disney responds to these questions, where we notice some cases of informal speech (depicted in pur-ple).
the executive’s response however mainly focused on speciﬁc objects or entities (depicted in red) intermixedwith some time-based information (depicted in green)..interaction presents an opportunity of analyzing notjust the management’s claims but also the way theyexpress them.
in figure 2, we highlight the variouscomponents in a short q&a interaction.
often,both the transcript and the audios of the calls areavailable to the public..vocal cues play a critical role in verbal commu-nication as they can provide support or discreditthe verbal message that is being spoken (jiang andpell, 2017).
for example, consider if the ceo ofthe acquiring company exhibits conﬁdence in thestatement - ”we are conﬁdent that this acquisitionwill bring us proﬁts,” however, displays nervous-ness while justifying technical details of the deal,we may infer contradiction in the claims of a suc-cessful m&a.
vocal cues have been proven indica-tors of emotions like deceit and nervousness (belinet al., 2017; sporer and schwandt, 2006).
past re-search (qin and yang, 2019; sawhney et al., 2020c)shows that the addition of vocal cues has helpedwith the task of ﬁnancial predictions and enrich thelearned representations..our contributions can be summarized as:.
• we curate a public dataset m3a2 (multimodalmulti-speaker merger & acquisition call fi-.
2the source code, processed features, and details on ac-quiring raw data are available at https://github.com/midas-research/m3a-acl.
nancial forecasting dataset) that consists of816 m&a conference calls spanning over 545hours between 2016 to 2020 with their tran-scripts and audio recordings, segmented byutterances and aligned with the audio..• we accompany the dataset with neural base-line architectures that use the multimodalmulti-speaker input to predict stock volatil-ity and price movement..• to the best of our knowledge, no such m&aconference call dataset exists in academia, andour proposed methodology, m3anet is theﬁrst deep learning approach for ﬁnancial pre-dictions on m&a conference calls..2 related work.
m&a conference calls financial reports andconference calls have been shown to have a corre-lation with the stock market and improve ﬁnancialpredictions (bowen et al., 2001; kogan et al., 2009).
studies have also been carried out speciﬁcally form&a calls, showing their effect on the market(dasgupta et al., 2020; hu et al., 2018).
however,there exists a gap in leveraging neural predictivemodeling on using verbal and vocal cues pertainingto m&a calls for ﬁnancial forecasting..6752financial forecasting research has shown his-torical pricing data to be useful in predicting ﬁ-nancial risk modeling (kristjanpoller et al., 2014;zheng et al., 2019; dumas et al., 2009).
it alsoconsiders volatility as an indicator of uncertainty,which helps make decisions regarding investments(heston, 1993; johnson and shanno, 1987; scott,1987).
previous work often use numerical features(liu and chen, 2019; nikou et al., 2019) in ap-proaches like neural networks (kim et al., 2019;luo et al., 2017), graph neural networks (sawhneyet al., 2020b), and time-series models (bollerslev,1986; engle, 1981).
on the other hand, we areinterested in analyzing multimodal data like textand audio, which can hold completely differentinformation for predictive models..natural language processing and financefor any system using human interactions to de-termine ﬁnancial risk or stock movements, it isnecessary to determine the relationship betweenthe various words to determine the speaker’s sen-timent.
advances in nlp have been utilized inmany approaches to show ﬁnancial informationsigniﬁcantly improving performance in forecast-ing tasks like volatility and stock price prediction(wang et al., 2013; ding et al., 2015; mittermayerand knolmayer, 2007).
research has also shownthat social media affects the stock market (bollenet al., 2010; oliveira et al., 2017; sawhney et al.,2020a).
machine learning methods using simplebag-of-words features to represent the ﬁnancialdocuments used in previous research (kogan et al.,2009; rekabsaz et al., 2017) largely ignore theinter-dependencies between the sentences.
to ﬁllthe gap, recent approaches have moved towardsnewer models such as transformers (yang et al.,2020) and reinforcement learning (sawhney et al.,2021b) over natural language data for ﬁnancial fore-casting..multimodality and financial forecasting re-search shows that psychological and behavioralelements are often indicators of stock price move-ment (malkiel, 2003).
vocal cues have been proveneffective in portraying these elements (wurm et al.,2010; hobson et al., 2011; jiang and pell, 2017).
thus, it is no surprise that multimodal architec-tures that use these cues for ﬁnancial predictionshave seen signiﬁcant improvements in their perfor-mances (yang et al., 2020; sawhney et al., 2020d)..speaker context encoding pastresearch(zhang et al., 2019; li et al., 2020) in ﬁeldslike emotion recognition have seen the improvedperformance on their prediction tasks with theaddition of speaker context.
models with datarelated to spoken text beneﬁt when the input isenriched with information about who spoke what..3 problem formulation.
consider an m&a call χ ∈ {χ1, χ2, .
.
.
, χm },which comprises multimodal components: χ =[t; a].
here, t is the sequence of textual utterances(sentences)3 of the call transcript and can be rep-resented as [t1, t2...tn ] where ti is the ith utter-ance of the call and n is the maximum numberof utterances in any call.
similarly, a is the se-quence of corresponding call audios for the textualutterances (sentences) and can be represented as[a1, a2...an ] where ai is the ith call audio.
thecall’s utterances are annotated with speaker infor-mation s = [s1, s2...sn ], where si is the speakerof the ith utterance and where each speaker in thecall may have spoken one or more utterances.
eachm&a conference call may have two or more partic-ipating companies, with at least one publicly-tradedcompany with publicly available stock price infor-mation.
we limit the scope of the problem beingsolved by forecasting predictions for just one ofthe participant companies with the larger marketvaluation (in case of a merger) or the acquiringcompany (in case of an acquisition).
we now de-scribe the two prediction tasks that we utilize totrain m3anet on..measuring stock volatility following (koganet al., 2009), we formulate stock volatility as aregression problem.
for a given stock with a closeprice of pk on the trading day k, we calculate theaverage log volatility as the natural log of the stan-dard deviation of return prices r in a window of τdays as:.
v[0,τ ] = ln.
(cid:32)(cid:114) (cid:80)τ.
(cid:33).
k=1(rk − ¯r)2τ.
(1).
where rk = pk−pk−1.
is the return price on day kfor a given stock, and ¯r is the average return priceover a period of τ days..pk−1.
3we restrict the scope of segmentation to a sentence levelas opposed to a more granular level such as the word levelowing to the higher complexity and noise involved in word-level segmentation for long m&a calls..6753s.ll.ac.forebmun.200.
175.
150.
125.
100.
75.
50.
25.
0.mergersacquisitions.
mergersacquisitions.
mergersacquisitions.
secnarett.u.fo.rebmunegareva.
115.
110.
105.
100.
95.
90.srekaeps.fo.rebmunegareva.
12.0.
11.5.
11.0.
10.5.
10.0.
9.5.
9.0.
46.
44.
42.
40.
38.i.odua.fo.htgnel.egareva.mergersacquisitions.
2016.
2017.
2018.
2019.
2020.
2016.
2017.
2018.
2019.
2020.
2016.
2017.
2018.
2019.
2020.
2016.
2017.
2018.
2019.
2020.
(a) yearly frequency of calls..(b) mean # of utterances..(c) mean # of speakers..(d) mean audio length..figure 3: statistics pertaining to the m3a dataset across modalities, types of calls, and years..formalizing price movement prediction fol-lowing (xu and cohen, 2018), we deﬁne pricemovement yd−τ,d over a period of τ days as a bi-nary classiﬁcation task.
for a given stock, we em-ploy its close price, which can either rise or fallon a day d compared to a previous day d − τ , toformulate the classiﬁcation task as:.
y[d−τ,d] =.
(cid:26)1, pd+τ > pd,0, pd+τ ≤ pd.
(2).
given an acquisition conference call χ, our learn-ing objective is to predict the average negative logvolatility v[0,τ ] and price movement y[d−τ,d] usingthe conference call data χ = [t; a]..be 40.15 ± 15.15 minutes and a maximum lengthof 98.15 minutes for the audio clips.
we providefurther statistics in figure 3. looking at year-wisetrends, we see that acquisitions are consistentlymore frequent that mergers every year.
further,we note that mergers see a decreasing trend in thenumber of utterances and acquisitions have a con-sistent number of speakers in m&a calls.
we alsonote that acquisitions conference calls seem to beincreasing in length as the years progress..we chronologically divide our dataset into atrain, validation, and test set in the ratio of 70 :10 : 20, respectively.
such a split ensures thatfuture data is not used for forecasting past data..4 curating m3a: dataset creation.
4.2 call segmentation and alignment.
4.1 data acquisition.
we curate our dataset, m3a, by acquiring audiorecords and text transcripts from the bloombergterminal.4 since the conference calls were reliablyavailable from 2016, we ﬁlter and list all m&acalls between 2016 and 2020. to limit the scope,we ensured the calls were in english, had theirdomicile as the u.s.a., and had ’merger’ or ’ac-quisition’ in their title.
the bloomberg terminaloften only provides the stock ticker for the acquir-ing company (in case of an acquisition) and thecompany with a more prominent marker valuation(in case of a merger).
to maintain uniformity, wedecide only to use the given stock information.
wepull the adjusted closing price data from yahoofinance.5.
the dataset comprises 816 conference calls.
the mean number of speakers across the calls is10.68 ± 4.17, with a maximum of 31 speakers.
the mean number of utterances across the calls is100.54 ± 38.32 utterances and a maximum of 284utterances in a call.
the mean length comes out to.
4https://bba.bloomberg.net/5https://in.finance.yahoo.com/.
each transcript of the dataset begins with the com-pany’s details with the larger market valuation (incase of a merger) or the acquiring company (in caseof an acquisition).
these details include the com-pany’s name, stock ticker, and the date of the call.
the transcript then lists the speakers in the call andtheir position in the companies, if any.
the callcontents follow the list of speakers.
the contentsare separated by utterances and are annotated withthe utterances’ speakers..given our dataset, we have the option to choosebetween transcript-level, utterance-level, and word-level embeddings.
we decide to use utterance-levelembeddings.6 we select utterances with at least tenwords to ensure better parsing of the transcript andparse the texts to extract all valid utterances..since we are working with audio ﬁles, it be-comes essential that we can segment them suchthat we can align them with their correspondingutterances in the text transcript.
to achieve thisalignment, we have used the aeneas7 library to per-.
6transcript-level embeddings are too coarse for our task.
we experimented with word-level embeddings but found thatthe performance degraded..7https://www.readbeyond.it/aeneas/.
6754figure 4: data pipeline: an overview of the processing involved with each data point including segmentation,encoding of modalities, speaker information augmentation and prediction..form the forced alignment.
the forced alignmentalgorithm takes as input a text ﬁle divided into frag-ments and an unfragmented audio ﬁle.
it processesthe input to output a synchronization map, whichautomatically associates a time interval in the au-dio ﬁle to its corresponding text fragment.
aeneasuses the sakoe-chiba band dynamic time warp-ing (dtw) (sakoe and chiba, 1978) forced align-ment algorithm, which has been proven to improvediscrimination between words and has superior per-formance over other conventional algorithms..5 methodology.
5.1 text and audio encoding.
text encoding we compute an utterance’s textualencoding as the arithmetic mean of all its wordvectors.
bert is well known as an effective pre-trained language-based model for extracting word-embeddings (biswas et al., 2020) for a variety oflanguage modeling tasks.
we use uncased basebert (devlin et al., 2019) to extract the wordembeddings.
for each call, we represent the textutterances as [t1, t2, .
.
.
, tn ].
as seen from figure4, we embed each text utterance ti to get its corre-sponding 768-dimensional text encoding gi usingbert such that gi = bert(ti) for each i ∈ [1, n ]..audio encoding we use the opensmile8 li-brary to extract the audio features at a sampling rateof 10ms and choose the set of 62 gemaps featuresdescribed in (eyben et al., 2016).
this set includesfeatures like pitch, jitter, loudness, etc., which haveproven to be effective in audio analysis tasks (chaoet al., 2015).
for each call, we represent the audioclips of the utterances as [a1, a2, .
.
.
, an ].
we em-bed each audio utterance ai to its corresponding 62-dimensional encoding hi using opensmile suchthat hi = opensmile(ai) for each i ∈ [1, n ]..8https://pypi.org/project/opensmile/.
motivation for speaker information infusionthe audio encodings help decipher the vocal cuesin the text transcript’s context to support or dis-credit the speaker’s claims.
however, it is criticalfor the system to recognize the importance of theutterance’s speaker to gauge its impact on ﬁnancialpredictions.
this requires the information about thespeaker of each utterance to be augmented to theinput.
prior research (zhang et al., 2019; li et al.,2020) shows the addition of speaker context helpsimprove prediction performance on tasks involvingdatasets with spoken texts..m&a calls have utterances spoken by the com-pany’s management (the decision-making force ofthe company), by analysts (who want to gauge therisk in the company’s decisions), or even just theoperator (often an impartial person).
capturing thisspeaker context will allow us to decide how muchimpact a speciﬁc utterance can have on a com-pany’s stock price.
thus, we extract the speakerinformation for each utterance.
we parse the listof speakers from the transcripts and assign an idto each of the speakers.
the ids start from 1 andare assigned incrementally to each speaker in theorder in which they are listed.
the operator of thecall is assigned the id 0. we then annotate eachof the utterances based on who spoke it.
finally,we use one-hot encoding to represent the speakerencoding s of each utterance in the call..5.2 m3anet: speaker transformer.
the transformer (vaswani et al., 2017) uses multi-head attention and position embeddings to learnthe relationship between different utterances.
themultimodal input requires the model to learn theinter-dependencies between the audio and the textfeatures.
m3anet can then use the audio cues toafﬁrm or discredit the spoken message and makean informed prediction.
the idea behind m3anetis to use attention to weigh the importance of eachmodality at different timestamps.
we then aug-.
6755ment the data with the speaker encoding and allowthe transformer to extract the multimodal inter-dependencies for performing the prediction tasks..attention-fusion before we can fuse the inputs,we need to linearly transform the text embeddingsto ensure the multimodal embeddings’ sizes arethe same.
we then extract the attention weightsto calculate the attended inputs similar to (horiet al., 2017).
these attention weights describe theimportance of a speciﬁc modality concerning theother modality.
we multiply the text and audiofeatures by their attention weights wt and warespectively to get the attended input, followed byfusing them.
the following equations formalizethe attention mechanism used:.
wt = softmax(gwwt + bwt)wa = softmax(hwwa + bwa).
wt =.
wtwt + waxf used = gwt + hwa.
, wa =.
wawt + wa.
(3).
(4).
(5).
(6).
where wwt and bwt represent the text attentionlayer, wwa and bwa represent the audio attentionlayer and + represents addition..sentence-level transformer to model the se-quence of textual and audio embeddings of them&a calls, we augment the fused multimodal em-beddings xf used with position embeddings pos byaddition and the speaker information by concate-nation (represented by ⊕).
pos has the same di-mensions as xf used, posj,ind represents the valueof the positional embedding for the jth utteranceat index ind.
the augmentation is summarised asfollows:.
posj,2l, posj,2l+1 = sin.
(cid:19).
(cid:18) j10.
8ld., cos.(cid:19).
(cid:18) j10.
8ld.xf inal = (xf used + pos) ⊕ s.(7).
(8).
the transformer block uses the augmented fea-ture set for further processing, following which theintermediate tensors are passed through two con-secutive dense layers to output the task predictionas follows:.
o1 = relu (wl1i1 + bl1)y = σ(wl2o1 + bl2).
(9).
(10).
and o1 represent the input to the ﬁrst and seconddense layer after being passed through the sentencetransformer, while σ represents the ﬁnal activationfunction and y represents the ﬁnal prediction fromthe activation corresponding to the task.
we userelu for the ﬁnal prediction in the volatility pre-diction task and sigmoid for the price predictiontask.
we then use mean squared error (mse)and binary cross-entropy (bce) losses to trainthe output for volatility prediction and stock pricemovement prediction, respectively..6 experimental setup.
6.1 baselines.
we compare m3anet against modern baselinesacross modalities for both the tasks.
we employglove (pennington et al., 2014), finbert (araci,2019) and roberta (liu et al., 2019) to embed thetext and choose an lstm + dense layer architec-ture as a benchmark for both volatility and pricemovement prediction.
we also use all three (text,audio, and multimodal) variants of the multimodaldeep regression model (mdrm) (qin and yang,2019) as baselines..6.2 training setup.
we tune m3anet’s hyper-parameters using gridsearch.
we summarize the range of hyperparam-eters tuned on: size of the transformer’s feed-forward layer and size of the linear layers ∈ {16,32, 64}, dropout δ ∈ {0.0, 0.1, 0.25, 0.5}, batchsize b ∈ {32, 64, 128} and learning rate e ∈ {0.1,0.01, 0.001, 0.0001}.
the experiment results in thefollowing optimal choices of the hyper-parameters:b = 64, e = 0.001, feed forward network size(volatility) = 16, hidden layer size (volatility)= 16 and δ (volatility) = 0.1, , feed forward net-work size (movement) = 64, hidden layer size(movement) = 32, δ (movement) = 0.0..we implement all methods with keras9 andgoogle colab.10, using relu as our hidden layeractivation function and optimize using adam.
wechoose the highest performing model during thetraining phase on our validation set and chosenevaluation metrics as our best model.
we zero-padthe calls that have less than the maximum numberof utterances/speakers for efﬁcient batching.
weexperiment with trading periods τ ∈ {3, 7, 15}.
where, wl1 and bl1 represent the ﬁrst linear layer,wl2 and bl2 represent the second linear layer, i1.
9https://keras.io/10https://research.google.com/.
colaboratory/.
6756model.
mse3.
roberta + lstm 0.78 (0.009)glove + lstm0.80 (0.005)finbert + lstm 0.78 (0.008)0.79 (0.003)mdrm (t)0.79 (0.004)mdrm (a)0.78 (0.005)mdrm (t+a)m3anet (ours)0.77 (0.018)*.
volatility predictionmse70.58 (0.009)0.60 (0.004)0.60 (0.004)0.59 (0.003)0.60 (0.002)0.58 (0.003)0.57 (0.016)*.
mse150.47 (0.006)0.48 (0.005)0.47 (0.005)0.47 (0.002)0.47 (0.003)0.46 (0.002)0.46 (0.011)*.
f130.570.550.580.580.240.590.59.f170.580.560.580.560.360.580.59.price predictionf115 mcc3 mcc7 mcc150.490.420.480.480.120.460.50*.
0.100.020.060.120.000.110.13.
0.220.220.210.190.170.190.19.
0.190.190.200.200.020.190.19.table 1: mean τ -day volatility mse and price movement prediction results (mean and stdev.
of 5 runs for eachapproach).
* indicates that the result is signiﬁcantly better than the mdrm (t+a).
bold denotes best performance..model.
transformer (t)transformer (a)transformer (t+a: concat)transformer (t+a: att.
fusion)m3anet (ours).
mse30.79 (0.0130)0.82 (0.0180)0.80 (0.0006)0.76 (0.0180)0.77 (0.0180).
volatility predictionmse70.62 (0.0310)0.61 (0.0140)0.61 (0.0006)0.58 (0.0140)0.57 (0.0160).
mse150.47 (0.004)0.49 (0.013)0.48 (0.0003)0.47 (0.0090)0.46 (0.0110).
f130.500.530.090.570.59.f170.540.590.160.610.58.price predictionf115 mcc3 mcc7 mcc150.400.500.060.550.50.
0.160.180.010.180.17.
0.130.130.000.160.18.
0.110.130.010.120.13.table 2: effect of multimodality and multi-speaker inputs (mean and stdev.
of 5 runs for each approach)..days allowing experimentation across both shortand medium-term periods..sophisticated models may result in greater improve-ments in the performance on m3a..similar to prior work (sawhney et al., 2020d;theil et al., 2019; yang et al., 2020), we evaluatepredicted volatility using the mean squared error(mse) for each hold period, n ∈ {3,7,15}.
forthe classiﬁcation task, we report the f1 score andmathew’s correlation coefﬁcient (mcc) for theclassiﬁcation task (matthews, 1975).
we use mccbecause, unlike the f1 score, mcc avoids bias dueto any data skew that may be present as it does notdepend on the choice of the positive class.
for a(cid:18) tpf p.given confusion matrix.
f ntn.
(cid:19):.
m cc =.
tp × tn − f p × f n(cid:112)(tp + f p)(tp + f n)(tn + f p)(tn + f n).
(11).
7 results and analysis.
7.1 performance comparison.
as shown in table 1, m3anet achieves the bestperformance for both the volatility prediction andthe price prediction task.
we observe improve-ments using m3anet (table 2) that leverages thetext and audio modalities along with speaker in-formation.
this improvement can be attributedto attention to emphasize the importance of eachmodality throughout the series of utterances.
itcan also be observed that the improvements ourarchitecture results in are not quite large in mag-nitude.
we attribute this to the difﬁculty that thetask inherently possesses.
further research in more.
7.2 multimodal and multi-speaker learning.
from table 1 and table 2, we see that in both themdrm and transformer models, the multimodalmodels performed much better than the unimodalcounterparts.
this performance improvement fol-lows from previous research (qin and yang, 2019)with respect to volatility prediction.
similar ob-servations validate our hypothesis that audio cuesprovide additional information that helps make abetter prediction.
it is also apparent from table 2that adding speaker context improves the predictionresult consistently.
thus, we infer that speaker in-formation does play an essential part in forecastingand adds to the data’s richness..7.3 ablation study: fusion.
we experiment with fusion by concatenation andfusion by attention for the transformer and ﬁndthe latter performing better in most cases (table2).
we believe this happens because simple fu-sion techniques cannot produce features that ef-fectively capture the individual modalities’ impor-tance.
however, attention fusion uses weights forboth the modalities, learned by the architecture, todetermine the importance of each modality withrespect to its counterpart.
using these weights toperform a weighted addition gives a much betterrepresentation of both the modalities and their par-ticular importance in a fused vector..6757−4.
−220sentence number.
4.
−4.
−220sentence number.
4.
0.
10.
20.
30.
40.sentences.
(a) qa1: the ceo answers a questionabout the company’s competitors.
sen-tence 2: the ceo invites questions..(b) qa2.
the analyst has a spike in theirmean audio pitch while the ceo’s meanaudio pitch is stable..(c) qa3.
the mean audio pitch of theaudio clips..sentence 1sentence 2.ceo utterancesanalyst’s utterances.
hcti.pnae.m.220.
200.
180.
160.
140.
240.
220.
200.
180.
160.
140.
120.hcti.pnae.m.figure 5: qualitative analysis.
acquisitionsmergers.
210.
200.
190.
180.
170.
160.hcti.pnae.m.2.5.
1.5.
2.
1.
0.5.rorrederauqsnae.m.7.5 merger & acquisition transfer.
we experiment by training m3anet on mergersand acquisitions calls separately, and testing bothmodels on each set of calls separately.
from table3, it can be observed that both models predict theprice movement better for their respective sets asexpected.
it is surprising to see that the models pre-dict volatility of acquisition calls relatively betterthan that of merger calls.
this suggests that ac-quisition conference calls lead to a volatility that’srelatively easier to predict and seems to be an av-enue for further research..7.6 qualitative analysis.
call 1: acquisition of shape security by f5 net-works inc following the call, f5 networks incsuffered a price drop of up to 5.2% within the nextmonth.
studying the call’s vocal cues, we notice(figure 5a) the ceo had sudden peaks in the meanpitch of his audio while answering questions.
sim-ilar peaks occurred when a participant asks theceo about their fraud protection when comparedto their competitors.
prior research on audio analy-sis (jiang and pell, 2017) proves a high mean pitchmay indicate a lack of conﬁdence in the speaker.
it was later ascertained that f5 had overpaid to ac-quire shape security without proper due diligenceof fraud protection plans sold by shape security.
we observe how m3anet successfully predicts thedecrease in price for all choices of τ while theunimodal models fail to do the same each time.
though the text reveals no lack of conﬁdence, theaudio cues likely allow the model to make a suc-cessful prediction..call 2: merger of ak steel holding corpo-ration and cleveland-cliffs inc following the.
2.
4.
6.
8days after the call.
10.
12.
14.figure 6: drift in predicted stock volatility over time;the line graph represents the mean mse while theshaded regions represent the performance over 10 runs.
trained on.
acquisitionsmergers.
tested on acquisitions only tested on mergers onlymcc3mse30.0150.650.200.85.mcc30.120.03.mse31.471.01.f130.560.47.f130.660.28.table 3: ablation study: performance of m3a, whentrained on acquisitions and mergers separately.
7.4 performance drift over time.
as observed in previous works (sawhney et al.,2020d) using earnings calls, figure 6 shows thatshort-term stock volatility prediction is more com-plex, possibly due to the erratic price ﬂuctuationsafter a m&a call.
we hypothesize that these priceﬂuctuations settle as more time elapses, similarto the phenomenon of pead (post earnings an-nouncement drift) (bernard and thomas, 1989;bhushan, 1994; sadka, 2006).
this saturation inperformance improvement can be attributed to thedilution of cues extracted from the calls, as we’drift’ away from them.
however, it can be notedthat a similar trend may not necessarily be true forprice movement prediction..6758merger call, cleveland-cliffs inc saw an increasein their stock price up to 17.9% in the next ﬁvedays.
similar to the ﬁrst call, we notice spikes andsudden increases in the audios’ mean pitch fromfigure 5b.
however, the difference exists in the factthat these high pitch patterns come from an analystin the call and not someone holding an inﬂuentialposition in the companies involved.
m3anet candifferentiate between the speakers and correctlypredicts the price going up, unlike the transformervariant without speaker embeddings.
this showshow the augmentation of the multimodal data withthe speaker embedding likely beneﬁts the predic-tive power of m3anet..call 3: acquisition of plateau excavation incby sterling construction company inc wenow analyze this acquisition as an error analysiswhere m3anet predicts incorrectly.
we see thetext transformer performing well on this exampleand accurately predicting the increase in the stockprice for sterling construction company inc. onthe other hand, our multimodal multi-speaker isunable to do the same.
observing the audio cues(figure 5c), we ﬁnd a great deal of variance in themean audio pitch.
we attribute the erroneous per-formance to the potential overﬁtting of the modelor noise in the audio cues..8 conclusion.
we present a dataset of m&a calls that can be uti-lized to predict ﬁnancial risk following m&a calls.
we also present a strong baseline model usingmultimodal multi-speaker inputs from the m&acalls to perform ﬁnancial forecasting.
m3anetuses attention-based fusion to leverage the inter-dependency between the verbal message and thevocal cues.
further, the approach uses speaker in-formation to enrich the input data to determine ifthe speakers’ vocal cues or verbal messages conﬂictwith others and accounts for the same.
experimentson m3a display the effectiveness of m3anet.
wehope our m3a can enable more academic progressin the ﬁeld of ﬁnancial forecasting..ethical considerations and limitations.
examining a speaker’s tone and speech in confer-ence calls is a well-studied task in past literature(qin and yang, 2019; chariri, 2009).
our workfocuses only on calls for which companies publiclyrelease transcripts and audio recordings.
the data.
used in our study corresponds to m&a conferencecalls of companies in the nasdaq stock exchange.
we acknowledge the presence of gender bias in ourstudy, given the imbalance in the gender ratio ofspeakers of the calls.
we also acknowledge thedemographic bias (sawhney et al., 2021a) in ourstudy as the companies are organizations withinthe public stock market of united states of amer-ica and may not generalize directly to non-nativespeakers..references.
dogu araci.
2019..finbert: financial sentimentanalysis with pre-trained language models.
arxiv,abs/1908.10063..pascal belin, bibi boehme, and phil mcaleer.
2017.the sound of trustworthiness: acoustic-based mod-ulation of perceived voice personality.
plos one,12:e0185651..victor l. bernard and jacob k. thomas.
1989. post-earnings-announcement drift: delayed price re-journal of accountingsponse or risk premium?
research, 27:1–36..ravi bhushan.
1994. an informational efﬁciency per-spective on the post-earnings announcement drift.
journal of accounting and economics, 18(1):45 –65..e. biswas, m. e. karabulut, l. pollock, and k. vijay-shanker.
2020. achieving reliable sentiment anal-ysis in the software engineering domain using bert.
in 2020 ieee international conference on softwaremaintenance and evolution (icsme), pages 162–173..johan bollen, huina mao, and xiao-jun zeng.
2010.twitter mood predicts the stock market.
journal ofcomputational science, 2..tim bollerslev.
1986. generalized autoregressive con-journal of economet-.
ditional heteroskedasticity.
rics, 52:5–59..robert bowen, angela davis, and dawn matsumoto.
2001. do conference calls affect analyst forecasts.
the accounting review, 77..linlin chao, jianhua tao, minghao yang, ya li, andzhengqi wen.
2015. long short term memory recur-rent neural network based multimodal dimensionalemotion recognition.
in proceedings of the 5th inter-national workshop on audio/visual emotion chal-lenge, avec ’15, page 65–72, new york, ny, usa.
association for computing machinery..anis chariri.
2009. ethical culture and ﬁnancial re-porting: understanding ﬁnancial reporting practicewithin javanese perspective*.
issues in social andenvironmental accounting, 3..6759sudipto dasgupta, jarrad harford, fangyuan ma,daisy wang, and haojun xie.
2020. mergers un-der the microscope: analysing conference call tran-scripts.
available at ssrn 3528016..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, volume 1 (long and short papers),pages 4171–4186, minneapolis, minnesota.
associ-ation for computational linguistics..xiao ding, yue zhang, t. liu, and junwen duan.
2015.deep learning for event-driven stock prediction.
inijcai..bernard dumas, alexander kurshev, and raman up-pal.
2009. equilibrium portfolio strategies in thepresence of sentiment risk and excess volatility.
thejournal of finance, 64:579 – 629..robert engle.
1981. autoregressive conditional het-eroscedasticity with estimates of the variance ofunited kingdom inﬂation.
econometrica, 50..f. eyben, k. r. scherer, b. w. schuller, j. sund-berg, e. andr´e, c. busso, l. y. devillers, j. epps,p. laukka, s. s. narayanan, and k. p. truong.
2016. the geneva minimalistic acoustic parameterset (gemaps) for voice research and affective com-puting.
ieee transactions on affective computing,7(2):190–202..r. fraunhoffer, h. kim, and d. schiereck.
2018. valuecreation in ma transactions, conference calls, andshareholder protection.
international journal of fi-nancial studies, 6:1–21..steven heston.
1993. a closed-form solution for op-tions with stochastic volatility with applications tobond and currency options.
review of financialstudies, 6:327–43..jessen hobson, william mayew, and mohan venkat-achalam.
2011. analyzing speech to detect ﬁnancialmisreporting.
journal of accounting research, 50..chiori hori, takaaki hori, teng-yok lee, zimingzhang, bret harsham, john r. hershey, tim k.marks, and kazuhiko sumi.
2017. attention-basedmultimodal fusion for video description.
in proceed-ings of the ieee international conference on com-puter vision (iccv)..wenyao hu, thomas shohﬁ, and runzu wang.
2018.what’s really in a deal?
evidence from textual anal-ysis.
ssrn electronic journal..xiaoming jiang and marc d. pell.
2017. the soundof conﬁdence and doubt.
speech communication,88:106 – 126..herb johnson and david shanno.
1987. option pricingwhen the variance is changing.
journal of financialand quantitative analysis, 22:143–151..raehyun kim, chan ho so, minbyul jeong, sanghoonlee, jinkyu kim, and jaewoo kang.
2019. hats: ahierarchical graph attention network for stock move-ment prediction..shimon kogan, dimitry levin, bryan r. routledge,pre-jacob s. sagi, and noah a. smith.
2009.dicting risk from ﬁnancial reports with regression.
in proceedings of human language technologies:the 2009 annual conference of the north americanchapter of the association for computational lin-guistics, pages 272–280, boulder, colorado.
associ-ation for computational linguistics..werner kristjanpoller, anton fadic, and marcel min-utolo.
2014. volatility forecast using hybrid neuralnetwork models.
expert systems with applications,41:2437–2442..qingbiao li, chunhua wu, zhe wang, and kangfengzheng.
2020. hierarchical transformer network forutterance-level emotion recognition.
applied sci-ences, 10:4447..jiexi liu and songcan chen.
2019. non-stationarymultivariate time series prediction with selectiverecurrent neural networks, pages 636–649..y. liu, myle ott, naman goyal, jingfei du, mandarjoshi, danqi chen, omer levy, m. lewis, lukezettlemoyer, and veselin stoyanov.
2019. roberta:a robustly optimized bert pretraining approach.
arxiv, abs/1907.11692..rui luo, weinan zhang, xiaojun xu, and jun wang..2017. a neural stochastic volatility model..burton malkiel.
2003. the efﬁcient market hypothesisand its critics.
journal of economic perspectives,17:59–82..b.w.
matthews.
1975..the pre-dicted and observed secondary structure of t4 phagelysozyme.
biochimica et biophysica acta (bba) -protein structure, 405(2):442 – 451..comparison of.
marc-andre mittermayer and g.f. knolmayer.
2007.newscats: a news categorization and trading sys-tem.
pages 1002 – 1007..sara moeller, frederik schlingemann, and rene stulz.
2003. do shareholders of acquiring ﬁrms gain fromacquisitions?
ssrn electronic journal..mahla nikou,.
gholamreza mansourfar,.
andj. bagherzadeh.
2019.stock price predictionusing deep learning algorithm and its comparisonintelligentwith machine learning algorithms.
systems in accounting, finance and management,26..6760nuno oliveira, p. cortez, and nelson areal.
2017. theimpact of microblogging data for stock market pre-diction: using twitter to predict returns, volatility,trading volume and survey sentiment indices.
ex-pert syst.
appl., 73:125–144..jeffrey pennington, richard socher, and christophermanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the 2014 confer-ence on empirical methods in natural languageprocessing (emnlp), pages 1532–1543, doha,qatar.
association for computational linguistics..yu qin and yi yang.
2019. what you say and how yousay it matters: predicting stock volatility using ver-bal and vocal cues.
in proceedings of the 57th an-nual meeting of the association for computationallinguistics, pages 390–401, florence, italy.
associ-ation for computational linguistics..navid rekabsaz, mihai lupu, artem baklanov,alexander d¨ur, linda andersson, and allan han-bury.
2017. volatility prediction using ﬁnancial dis-closures sentiments with word embedding-based irin proceedings of the 55th annual meet-models.
ing of the association for computational linguistics(volume 1: long papers), pages 1712–1721, van-couver, canada.
association for computational lin-guistics..ronnie sadka.
2006. momentum and post-earnings-announcement drift anomalies: the role of liquidityrisk.
journal of financial economics, 80(2):309 –349..h. sakoe and s. chiba.
1978. dynamic programmingalgorithm optimization for spoken word recognition.
ieee transactions on acoustics, speech, and signalprocessing, 26(1):43–49..ramit sawhney, shivam agarwal, arnav wadhwa, andrajiv ratn shah.
2020a.
deep attentive learning forstock movement prediction from social media textin proceedings of theand company correlations.
2020 conference on empirical methods in naturallanguage processing (emnlp), pages 8415–8426,online.
association for computational linguistics..ramit sawhney, shivam agarwal, arnav wadhwa, andrajiv ratn shah.
2020b.
spatiotemporal hypergraphconvolution network for stock movement forecast-in 2020 ieee international conference oning.
data mining (icdm), pages 482–491..ramit sawhney, arshiya aggarwal, and rajiv ratnshah.
2021a.
an empirical investigation of bias inthe multimodal analysis of ﬁnancial earnings calls.
in proceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 3751–3757, online.
association for compu-tational linguistics..ramit sawhney, piyush khanna, arshiya aggarwal,taru jain, puneet mathur, and rajiv ratn shah..2020c.
voltage: volatility forecasting via text au-dio fusion with graph convolution networks for earn-ings calls.
in proceedings of the 2020 conferenceon empirical methods in natural language process-ing (emnlp), pages 8001–8013, online.
associa-tion for computational linguistics..ramit sawhney, puneet mathur, ayush mangal, piyushkhanna, r. shah, and roger zimmermann.
2020d.
multimodal multi-task ﬁnancial risk forecasting.
proceedings of the 28th acm international confer-ence on multimedia..ramit sawhney, arnav wadhwa, shivam agarwal, andrajiv ratn shah.
2021b.
quantitative day tradingfrom natural language using reinforcement learning.
in proceedings of the 2021 conference of the northamerican chapter of the association for computa-tional linguistics: human language technologies,pages 4018–4030, online.
association for compu-tational linguistics..louis scott.
1987. option pricing when the variancechanges randomly: theory, estimation, and an appli-cation.
journal of financial and quantitative anal-ysis, 22:419–438..siegfried sporer and barbara schwandt.
2006. paraver-bal indicators of deception: a meta-analytic synthe-sis.
applied cognitive psychology, 20:421 – 446..kilian theil, samuel broscheit, and h. stucken-schmidt.
2019. profet: predicting the risk of ﬁrmsfrom event transcripts.
in ijcai..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..chuan-ju wang, ming-feng tsai, tse liu, and chin-ting chang.
2013. financial sentiment analysis forrisk prediction..lee wurm, douglas vakoch, maureen strasser, robertcalin-jageman, and shannon ross.
2010. speechperception and vocal expression of emotion.
cogni-tion emotion, 15:831–852..yumo xu and shay b cohen.
2018. stock movementprediction from tweets and historical prices.
in pro-ceedings of the 56th annual meeting of the associa-tion for computational linguistics (volume 1: longpapers), pages 1970–1979..linyi yang, tin lok james ng, barry smyth, and rui-hai dong.
2020. html: hierarchical transformer-based multi-task learning for volatility prediction.
proceedings of the web conference 2020..dong zhang, liangqing wu, changlong sun,shoushan li, qiaoming zhu, and guodong zhou.
2019. modeling both context- and speaker-sensitivedependence for emotion detection in multi-speakerconversations.
pages 5415–5421..6761jie zheng, andi xia, lin shao, tao wan, andzengchang qin.
2019. stock volatility predictionbased on self-attention networks with social informa-tion.
pages 1–7..6762