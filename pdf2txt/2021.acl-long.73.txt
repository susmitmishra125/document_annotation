xlpt-amr: cross-lingual pre-training via multi-task learning forzero-shot amr parsing and text generation.
dongqin xu1.
junhui li1∗ muhua zhu2.
min zhang1.
guodong zhou1.
1school of computer science and technology, soochow university, suzhou, china2tencent news, beijing, chinaxdqck@live.com, {lijunhui, minzhang, gdzhou}@suda.edu.cnzhumuhua@gmail.com.
abstract.
tasks,.
due to the scarcity of annotated data, abstractmeaning representation (amr) research isrelatively limited and challenging for lan-guages other than english.
upon the avail-ability of english amr dataset and english-to-x parallel datasets, in this paper we proposea novel cross-lingual pre-training approachvia multi-task learning (mtl) for both zero-shot amr parsing and amr-to-text genera-tion.
speciﬁcally, we consider three typesof relevantincluding amr parsing,amr-to-text generation, and machine transla-tion.
we hope that knowledge gained whilelearning for english amr parsing and textgeneration can be transferred to the counter-parts of other languages.
with properly pre-trained models, we explore four different ﬁne-tuning methods, i.e., vanilla ﬁne-tuning with asingle task, one-for-all mtl ﬁne-tuning, tar-geted mtl ﬁne-tuning, and teacher-student-based mtl ﬁne-tuning.
experimental re-sults on amr parsing and text generation ofmultiple non-english languages demonstratethat our approach signiﬁcantly outperforms astrong baseline of pre-training approach, andgreatly advances the state of the art.
in detail,on ldc2020t07 we have achieved 70.45%,71.76%, and 70.80% in smatch f1 for amrparsing of german, spanish, and italian, re-spectively, while for amr-to-text generationof the languages, we have obtained 25.69,31.36, and 28.42 in bleu respectively.
wemake our code available on github https://github.com/xdqkid/xlpt-amr..1.introduction.
abstract meaning representation (amr) (ba-narescu et al., 2013) is a widely used formalismthat represents the semantics of a sentence witha directed and acyclic graph.
figure 1 (b) showsan example amr graph where the nodes such as.
∗corresponding author: junhui li..“doctor” and “give-01” represent concepts, and theedges such as “:arg0” and “:arg1” stand for se-mantic relations between two connected concepts.
recent studies on amr mainly fall in two direc-tions: amr parsing which converts a sentence intoan amr graph (flanigan et al., 2014; wang et al.,2015a; konstas et al., 2017, to name a few) and itsinverse, i.e., amr-to-text generation that producesa sentence from an amr graph (flanigan et al.,2016; song et al., 2017, 2018, to name a few)..restricted by the availability of annotated cor-pora, most of previous studies on amr focuson english while very few studies are for chi-nese and portuguese (wang et al., 2018; sobre-villa cabezudo et al., 2019; anchiˆeta and pardo,2020).
cross-lingual amr research, however, hasreceived relatively less attention.
in fact, cross-lingual amr has mainly been studied in the scopeof annotation works (xue et al., 2014; hajiˇc et al.,2014).
till recently, damonte and cohen (2018)demonstrate that amr annotated for english canbe used as cross-lingual semantic representations,and propose to conduct cross-lingual amr pars-ing via annotation projection and machine transla-tion.
blloshmi et al.
(2020) follow the same lineand create large-scale silver data to boost the per-formance of cross-lingual amr parsing.
fan andgardent (2020) focus on multilingual amr-to-textgeneration for twenty one different languages.
theaforementioned studies consider amr parsing andamr-to-text generation separately..in this paper, we formalize both amr pars-ing and amr-to-text generation as sequence-to-sequence (seq2seq) learning and propose a noveland effective approach to cross-lingual amr,which is illustrated in figure 1. upon the avail-ability of the english amr dataset and english-to-x parallel datasets (x ∈ {german, spanish, italian} inthis paper), our purpose is to boost the performanceof zero-shot amr parsing and text generation in.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages896–907august1–6,2021.©2021associationforcomputationallinguistics896of another relevant yet stronger task.
experi-mental results on the cross-lingual amr dataset(ldc2020t07) show that the proposed approachgreatly advances the state of the art of cross-lingualamr..overall, we make the following contributions..• we propose an effective cross-lingual pre-training approach for zero-shot amr parsingand amr-to-text generation.
our pre-trainedmodels could be used for both amr parsingand amr-to-text generation..• we explore and compare different ﬁne-tuningmethods.
we also propose a teacher-student-based ﬁne-tuning method that achieves thebest performance..• we evaluate our approach in three zero-shotlanguages of amr and our approach greatlyadvances the state of the art..2 related work.
we describe related studies on amr from threeperspectives: english amr parsing, english amr-to-text generation, and cross-lingual amr..english amr parsing.
amr parsing is a taskthat translates a sentence into a directed and acyclicgraph (banarescu et al., 2013).
according to theapproaches to modeling the structure in amrgraphs, previous studies on amr parsing for en-glish can be broadly grouped into several cate-gories, which are tree-based approaches (wanget al., 2015b; groschwitz et al., 2018), graph-basedapproaches (flanigan et al., 2014; werling et al.,2015; cai and lam, 2019), transition-based ap-proaches (zhou et al., 2016; damonte et al., 2017;ballesteros and al-onaizan, 2017; guo and lu,2018; zhou et al., 2021), sequence-to-sequence(seq2seq) approaches (peng et al., 2017; van no-ord and bos, 2017; konstas et al., 2017; ge et al.,2019; xu et al., 2020; bevilacqua et al., 2021), andsequence-to-graph (seq2graph) approaches (lyuand titov, 2018; zhang et al., 2019a,b; cai andlam, 2020a)..english amr-to-text generation.
as an in-verse task of amr parsing, amr-to-text gener-ation aims to write a sentence from an amr graph.
early studies on this task rely on grammar-based ap-proaches (flanigan et al., 2016; song et al., 2017).
more recent studies propose to regard amr-to-text generation as a machine translation or seq2seq.
figure 1: illustration of cross-lingual amr parsing andamr-to-text generation: (a) sentences in different lan-guages sharing the same meaning; (b) amr graph ofthe sentences..x-language.
to this end, we borrow the idea ofjoint pre-training from xu et al.
(2020) and explorethree types of relevant tasks, including machinetranslation tasks, amr parsing and amr-to-textgeneration tasks.
we conjecture that knowledgegained while learning for english amr parsing andtext generation could be helpful to the x-languagecounterparts, and machine translation tasks couldact as a good regularizer (xu et al., 2020).
to thebest of our knowledge, this is the ﬁrst study that uti-lizes such a pre-training approach in cross-lingualamr research..we also explore and compare four different ﬁne-tuning methods to answer the question that whethercombining amr parsing and amr-to-text gener-ation tasks in ﬁne-tuning stage will achieve betterperformance.
moreover, inspired by the teacher-student mechanism (kim and rush, 2016; chenet al., 2017), we extend the ﬁne-tuning methodto improve a target ﬁne-tuning task with the help.
897(a) parallel sentencesenglish the doctors gave her medication and it's made her much better.germansie bekam medikamente und nun geht es ihr viel besser.spanishlos médicos le dieron medicación y ha mejorado mucho.italiani medici le hanno dato un farmaco che la fa stare molto meglio.amr parsingamr-to-textandgive-01doctorgood-02moremake-02muchmedicationshe:degree:quant :arg1:arg1:arg2(b)amr graphtask (pourdamghani et al., 2016; ferreira et al.,2017; konstas et al., 2017; cao and clark, 2019).
however, seq2seq approaches tend to lose struc-tural information in amr graphs since they simplylinearize amr graphs into sequences before feed-ing them into the models.
to prevent informationloss caused by linearization, a variety of graph-to-sequence approaches have been proposed to bettermodel structural information (song et al., 2018;beck et al., 2018; damonte and cohen, 2019; guoet al., 2019; ribeiro et al., 2019; zhu et al., 2019;cai and lam, 2020b; zhao et al., 2020; song et al.,2020; yao et al., 2020; bai et al., 2020).
by takingadvantages of strong pre-trained language models,recent studies achieve new state of the art (mageret al., 2020; harkous et al., 2020; ribeiro et al.,2020; bevilacqua et al., 2021) ..cross-lingual amr.
all above related studiesfocus on english amr research.
relatively limitedefforts have been put on other languages due tothe lack of language-speciﬁc amr corpora.
actu-ally, whether amr can act as an interlingua is anopen question (xue et al., 2014; hajiˇc et al., 2014).
till lately , damonte and cohen (2018) demon-strate that a simpliﬁed amr can be used acrosslanguages and for the ﬁrst time they study cross-lingual amr parsing for languages rather than en-glish.
blloshmi et al.
(2020) employ large-scalesilver parallel amr data to bridge the gap betweendifferent languages and greatly advance the perfor-mance of cross-lingual amr parsing.
sheth et al.
(2021) explore annotation projection to leverage ex-isting english amr and overcome resource short-age in the target language.
furthermore, fan andgardent (2020) explore cross-lingual amr-to-textbased on pre-trained cross-lingual language model(xlm) (lample and conneau, 2019).
in this paperwe build strong cross-lingual pre-trained modelsfor both amr parsing and amr-to-text generation.
moreover, a nice property of our approach is thatfor amr parsing, unlike related studies (damonteand cohen, 2018; blloshmi et al., 2020), we donot need to perform lemmatization, pos tagging,ner, or re-categorization of entities, thus requireno language speciﬁc toolkits in pre-processing..3 cross-lingual pre-training.
in this section, we ﬁrst present the background ofour pre-training approach (section 3.1), followedby the description of cross-lingual pre-trainingtasks (section 3.2).
then we present our joint.
pre-training (section 3.3).
for simplicity, in thefollowing we use german as a representative to de-scribe our approach to german amr parsing andamr-to-text generation..3.1 background.
transformer-based seq2seq learning.
ourmodels are built on the transformerframe-work (vaswani et al., 2017).
the encoder in trans-former consists of a stack of multiple identical lay-ers, each of which has two sub-layers: one imple-ments the multi-head self-attention mechanism andthe other is a position-wise fully-connected feed-forward network.
the decoder is also composed ofa stack of multiple identical layers.
each layer inthe decoder consists of the same sub-layers as in theencoder plus an additional sub-layer that performsmulti-head attention to the distributional represen-tation produced by the encoder.
see vaswani et al.
(2017) for more details..amr graph linearization and recovering.
to make transformer applicable to amr parsingand amr-to-text generation, on the one hand wefollow van noord and bos (2017) to linearize amrgraphs into sequences by removing variables, wikilinks and duplicating the co-referring nodes.
on theother hand, for amr parsing we need to recoverthe graph representation from linearized amrsby assigning a unique variable to each concept,pruning duplicated and redundant materials, restor-ing co-referring nodes, ﬁxing incomplete conceptsand performing wikiﬁcation.1 in this paper, weadopt linearization and recovering scripts providedby van noord and bos (2017).2.
3.2 cross-lingual pre-training tasks.
due to the unavailability of gold training data ofgerman amr parsing and amr-to-text genera-tion, we view english as a pivot and hope thatknowledge gained while learning for english amrparsing and text generation could be helpful forthe german counterparts.
speciﬁcally, given anen-de parallel dataset (cid:0)t en , t de(cid:1), we use anenglish amr parser trained on annotated englishamrs (i.e., amr2.0) to parse the english sen-tences into amr graphs, thus obtain a trilingualparallel dataset t = (cid:0)t en , t de, t am r(cid:1).
then.
1we extract a term-wiki list from english amr trainingdataset.
when performing wikiﬁcation, we simply just lookup the list..2https://github.com/rikvn/amr.
898on the trilingual parallel dataset, we propose cross-lingual pre-training via multi-task learning.
weconsider three types of tasks, i.e., amr parsing,amr-to-text generation, and machine translation..amr parsing tasks, which include bothenglish amr parsing on the training data(cid:0)t en , t am r(cid:1) and german amr parsing on(cid:0)t de, t am r(cid:1).
note that both amr parsing tasksare trained on silver amr graphs..amr-to-text generation tasks, which in-clude both english amr-to-text generation andsimilar togerman amr-to-text generation.
amr parsing,these two amr-to-text genera-tion tasks are also trained on silver amr graphs(cid:0)t am r, t en (cid:1) and (cid:0)t am r, t de(cid:1), respectively..machine translation tasks, which includeboth english-to-german and german-to-englishmachine translation tasks on (cid:0)t en , t de(cid:1).
theadvantage of including the bi-directional transla-tion tasks is three-fold.
first, english-to-germantranslation will enable the decoder to generate ﬂu-ent german sentence, which is beneﬁcial to ger-man amr-to-text generation.
second, german-to-english translation will enable the encoder to cap-ture syntax and semantic information from germansentences, which is beneﬁcial to german amrparsing.
third, translation tasks can serve as reg-ularization to the training of amr parsing andamr-to-text generation, both of which are apt tooverﬁt to the training data..overall speaking, in our pre-training there ex-ist three types of (six) pre-training tasks in total.
the pre-training is conducted on a trilingual paral-lel dataset (cid:0)t en , t de, t am r(cid:1), where t en andt de are parallel gold sentence pairs while t am ris the set of corresponding silver amr graphs..3.3.jointly mtl pre-training.
to train the above six pre-training tasks with a sin-gle model, we follow the strategy used in xu et al.
(2020) and add preceding language tags to bothsource and target sides of training data to distin-guish the inputs and outputs of each training task.
as illustrated in table 1, we use <en>, <de>, and<amr> as the tags of begin-of-sentence for en-glish sentences, german sentences, and linearizedamrs, respectively..our joint pre-training on multiple tasks falls intothe paradigm of multi-task learning (mtl).
in thetraining stage, we take turns to load the training.
english <en> english sentencegerman <de> german sentenceamr <amr> linearized amr.
table 1: preceding tags as the symbol of begin-of-sentence to distinguish languages..data of these pre-training tasks.
for example, weupdate model parameters on a batch of traininginstances from the ﬁrst task, and then update pa-rameters on a batch of training instances of thesecond task, and the process repeats.
we also notethat, according to our preliminary experimentation,the effect of different orders of carrying out thesepre-training tasks is negligible..4 fine-tuning methods.
to ﬁne-tune a pre-trained model, we create aﬁne-tuning dataset from english annotated amrs(i.e.,amr2.0).
given english-amr parallel data(cid:0)f en , f am r(cid:1), we use an english-to-germantranslator to translate the english sentences intogerman sentences, thus obtain trilingual paralleldataset f = (cid:0)f en , f de, f am r(cid:1).
as our goalis to improve the performance of zero-shot amrparsing and amr-to-text generation, our primaryﬁne-tuning tasks are german amr parsing andamr-to-text generation.
moreover, we could in-clude the other four ﬁne-tuning tasks as auxiliarytasks when necessary, i.e., english amr parsingand amr-to-text generation, as well as english-to-german and german-to-english translation..once the ﬁne-tuning dataset is ready, we can ﬁne-tune a pre-trained model with different methods.
the vanilla ﬁne-tuning method that ﬁne-tunes a pre-trained model on the dataset of a primary task is anatural choice.
we can also ﬁne-tune a pre-trainedmodel jointly over all ﬁne-tuning tasks, or over theprimary tasks plus speciﬁcally chosen ﬁne-tuningtasks that are relevant.
in the following we exploreand compare four different ﬁne-tuning methods..4.1 vanilla fine-tuning.
given a pre-trained model, vanilla ﬁne-tuning up-dates the parameters of the pre-trained model solelyon the dataset of the downstream task.
for exam-ple, for german amr parsing, we ﬁne-tune thepre-trained model on the ﬁne-tuning dataset of thegerman amr parsing task.
in other words, vanillaﬁne-tuning involves only a single-task learning..8994.2 one-for-all mtl fine-tuning.
we ﬁne-tune a pre-trained model synchronously forall six ﬁne-tuning tasks, which are the same as thepre-training tasks.
related studies (li and hoiem,2018; xu et al., 2020) have shown that it is im-portant to optimize for high accuracy of a primaryﬁne-tuning task while preserving the performanceof other tasks.
preserving the performance of var-ious pre-training tasks could be viewed as a regu-larizer for each ﬁne-tuning task.
similarly to jointpre-training, we take turns to load the ﬁne-tuningdata of these ﬁne-tuning tasks.
consequently, weobtain a single ﬁne-tuned model for all tasks..4.3 targeted mtl fine-tuning.
rather than including all ﬁne-tuning tasks withina single model, we can selectively choose relevantﬁne-tuning tasks.
for german amr parsing, weuse amr parsing on german as the primary ﬁne-tuning task and german-to-english translation asan auxiliary ﬁne-tuning task.
the auxiliary taskwill enhance the encoder to capture semantic in-formation from german sentences.
this is alsoconsistent with the ﬁne-tuning tasks designed forenglish amr parsing in (xu et al., 2020).
for ger-man amr-to-text generation, we choose english-to-german as the auxiliary ﬁne-tuning task, whichis beneﬁcial for the decoder to generate ﬂuent ger-man sentences..4.4 teacher-student-based mtl.
fine-tuning.
one notable property of the ﬁne-tuning datasetis that the german sentences are produced auto-matically through machine translation.
noises insuch silver ﬁne-tuning dataset may degrade the per-formance of ﬁne-tuned models.
inspired by theteacher-student framework (kim and rush, 2016;chen et al., 2017), we propose to solve this prob-lem by using a stronger ﬁne-tuning task to helpimprove ﬁne-tuning tasks on such noisy data.
forexample, we can use english amr parsing (as theteacher) to help german amr parsing (as the stu-dent), since english amr parsing that is ﬁne-tunedon gold data tends to have stronger performance..fine-tuning for german amr parsing.
weuse e, g, a to denote english-side, german-side,and amr-side, respectively, and (e, g, a) as atriple instance.
for german amr parsing (i.e.,g → a), we regard english amr parsing (i.e.,.
|a|(cid:88).
i=1.
|a|(cid:88).
=.
=.
e → a) as its teacher and assume that the prob-ability of generating a target amr token ai fromg should be close to that from its counterpart e,given the already obtained partial amr a<i.
onthis assumption, the student model can acquireknowledge from the teacher by applying word-levelknowledge distillation for multi-class cross-entropywith the following joint training objective:.
j (θg→a) =.
(cid:88).
(cid:16).
j.
(e,g,a).
e, g, a, ˆθe→a, θg→a.
(cid:17).
+ lθg→a (a | g) ,.
(1).
where (e, g, a) ∈ de,g,a, i.e., (cid:0)f en , f de, f am r(cid:1),the ﬁne-tuning data for english/german amrparsing, ˆθe→a denotes the already learned modelparameters for english amr parsing,3 andlθg→a (a | g) denotes the log-likelihood functionfor translating g into a. the function j in eq.
1 isdeﬁned as:.
(cid:16).
j.e, g, a, ˆθe→a, θg→a.
(cid:17).
(cid:16).
(cid:17)p (a|e, a<i; ˆθe→a) (cid:107) p (a|g, a<i; θg→a).
kl.
(cid:88).
p (a|e, a<i; ˆθe→a) log.
p (a|e, a<i; ˆθe→a)p (a|g, a<i; θg→a).
,.
i=1.
a∈va.
(2)where kl (· (cid:107) ·) denotes the kl divergence betweentwo distributions, and va is the vocabulary set.4.
to sum up, in mtl ﬁne-tuning we use eq.
1 asthe objective for the ﬁne-tuning task of germanamr parsing while we still use the log-likelihoodfunction for the auxiliary ﬁne-tuning task, i.e.,german-to-english translation..fine-tuning for german amr-to-text genera-tion.
considering the fact that the performanceof english-to-german translation is also better thanthat of german amr-to-text generation, we viewenglish-to-german translation as the teacher andassume that the probability of generating a targetgerman token gi from a should be close to thatfrom its counterpart e, given the already obtainedpartial german sentence g<i.
the joint trainingobjective for german amr-to-text generation issimilar to the aforementioned objective functionfor german amr parsing.
due to limited space,we omit deﬁnition details of the objective function..3the english amr parser is learned by ﬁne-tuning the pre-trained model on ﬁne-tuning tasks of english amr parsingand english-to-german translation..4to avoid overﬁtting, the method additionally ﬁne-tunes.
80k steps on the pre-training dataset at the beginning..9005 experimentation.
in this section, we report the performance of ourapproach to amr parsing and amr-to-text gener-ation for non-english languages, including german(de), spanish (es), and italian (it).
the modelsare pre-trained and ﬁne-tuned on english data andone of either de, es, or it, and are evaluated inthe target language..5.1 experimental settings.
pre-training datasets.
for german, we usethe wmt14 english-german translation dataset 5which consists of 3.9m sentence pairs after pre-processing.
for spanish and italian, we use eu-roparl parallel datasets,6 which consist of 1.9menglish-spanish and 1.9m english-italian sen-tence pairs, respectively.
the english sentencesof all the datasets are all parsed into amr graphsvia an english amr parser trained on amr 2.0(ldc2017t10) (appendix a provides more detailson the english amr parser).
we merge english,german (spanish/italian) sentences and linearizedamrs together and segment all the tokens intosubwords by byte pair encoding (bpe) (sennrichet al., 2016) with 40k (or 30k for both spanishand italian) operations..in addition, we also train nmt models to trans-late english into german, spanish, and italian onabove parallel datasets with transformer-big set-tings (vaswani et al., 2017).
these nmt modelswill be used in preparing ﬁne-tuning datasets (ap-pendix b provides more implementation details onthe nmt models)..fine-tuning datasets.
we use english amr2.0which contains 36,521, 1,368, and 1,371 english-amr pairs for training, development, and testing,respectively.
we translate the english sentencesinto german, spanish, and italian, respectively.
wesegment all the tokens into subwords by using thebpe model trained on pre-training datasets..pre-training and fine-tuning model settings.
we implement above pre-trained models based onopennmt-py (klein et al., 2017).
7 for simplicity,we use the same hyperparameter settings to trainall the models in both pre-training and ﬁne-tuning.
5https://www.statmt.org/wmt14/.
translation-task.html.
6https://www.statmt.org/europarl/index..html.
7https://github.com/opennmt/opennmt-py.
by just following the settings for the transformer-base model in vaswani et al.
(2017).
the numberof layers in encoder and decoder is 6 while thenumber of heads is 8. both the embedding sizeand the hidden state size are 512 while the size offeedforward network is 2048. moreover, we useadam optimizer (kingma and ba, 2015) with β1of 0.9 and β2 of 0.98. warm up step, learning rate,dropout rate, and label smoothing epsilon are setto 16000, 2.0, 0.1 and 0.1 respectively.
we set thebatch size to 4,096 (8,196) in pre-training (ﬁne-tuning).
we pre-train (ﬁne-tune) the models for250k (10k) steps and save them at every 10k (1k)steps.
finally, we obtain ﬁnal pre-trained (ﬁne-tuned) models by averaging the last 10 checkpoints..evaluation.
we evaluate on ldc2020t07 (da-monte and cohen, 2018), a corpus containing hu-man translations of the test portion of 1371 sen-tences from the amr 2.0, in german, spanish,italian, and chinese.
this data is designed for usein cross-lingual amr research.
following fan andgardent (2020), we only evaluate on languages ofgerman, spanish and italian where we have train-ing data from europarl.
for amr parsing eval-uation, we utilize smatch and other ﬁne-grainedmetrics (cai and knight, 2013; damonte et al.,2017).
for amr-to-text generation, we report per-formance in bleu (papineni et al., 2002)..5.2 baseline systems.
we compare the performance of our approachagainst two baseline systems..baselinescratch.
to build this baseline system, wedirectly train models from scratch on the ﬁne-tuning datasets.
taking german amr parsingas example, we train the model on its ﬁne-tuningdataset (cid:0)f de, f amr(cid:1) to get baselinescratch..baselinepre-trained.
rather than training modelsfrom scratch, we pre-train the models on large-scale silver datasets.
taking german amr parsingas example, we ﬁrst pre-train the model on the pre-training dataset, i.e., (cid:0)t de, t amr(cid:1), then we ﬁne-tune the pre-trained model on the correspondingﬁne-tuning dataset, i.e., (cid:0)f de, f amr(cid:1)..5.3 main results.
table 2 shows the performance of amr parsingand amr-to-text generation for german (de),spanish (es), and italian (it)..901approach.
de58.1064.9048.9766.8867.4068.3170.45.amr parsinges60.65baselinescratch68.05baselinepre-trainedxlpt-amrnone59.5269.86xlpt-amrvanilla69.85xlpt-amrone4all70.10xlpt-amrtargeted71.76xlpt-amrt-sprevious works on cross-lingual amr parsingdamonte and cohen (2018)†blloshmi et al.
(2020)‡sheth et al.
(2021)‡previous works on cross-lingual amr-to-text generationfan and gardent (2020)‡.
it58.6766.5458.1369.1369.2669.6470.80.
58.058.167.4.
60.058.067.9.
57.053.062.7.
-.
-.
-.
---.
amr-to-textes17.8327.1721.1729.1431.1730.8331.36.de13.1119.3210.6323.1123.3724.1525.69.it13.5924.1316.5627.5628.2628.2728.42.
---.
---.
15.3.
21.7.
19.8.table 2: performance of amr parsing in smatch f1 and amr-to-text generation in bleu for german (de),spanish (es), and italian (it).
here, xlpt-amrnone denotes that we test the pre-trained models without ﬁne-tuning them.
xlpt-amrone4all, xlpt-amrtargeted, and xlpt-amrt-s indicate that we use one-for-all, targetedand teacher-student as mtl ﬁne-tuning method, respectively.
† is for using google translator while ‡ for pre-trained models..from the performance comparison of the twobaseline approaches, it is not surprising to ﬁndout that pre-training on silver datasets is a veryeffective way to boost performance (konstas et al.,2017; xu et al., 2020).
by using silver datasets,we obtain improvements of 6.80 ∼ 7.87 smatchf1, and 6.21 ∼ 10.54 bleu for parsing and textgeneration, respectively..with any of our ﬁne-tuning methods, ourcross-lingual pre-training approach further im-proves the performance over the strong baselinebaselinepre-trained in both parsing and gener-ation tasks over all languages.
it shows that likeother ﬁne-tuning methods, vanilla ﬁne-tuning sig-niﬁcantly boosts the performance of both parsingand generation.
however, it still underperformsany of the mtl ﬁne-tuning methods.
this con-ﬁrms that it is important to optimize for high accu-racy of a certain ﬁne-tuning task while preservingthe performance of other pre-training.
the perfor-mance comparison between xlpt-amrone4alland xlpt-amrtargeted suggests that selectivelychoosing relevant ﬁne-tuning tasks, rather than in-cluding all ﬁne-tuning tasks, could further boostparsing and generation performance with the ex-ception of spanish generation task..the xlpt-amrt-s models perform the best,which reveals that using the teacher-student frame-work to guide the decoding process also helps thestudent task.
this is owing to fact that the teacher.
models achieve better performance than the studentmodels.
see more in section 5.4 for performancecomparison of teacher and student models..finally, we compare our approach to the previ-ous studies.
among them, both blloshmi et al.
(2020) and fan and gardent (2020) adopt pre-trained models which cover either the encoder part,or the decoder part.
from the results we can seeeven our baseline baselinepre-trained outper-forms them by pre-training the encoder and thedecoder simultaneously.
the results also showthat our xlpt-amrt-s models greatly advancethe state of art.
for example, our xlpt-amrt-smodels outperform sheth et al.
(2021) by 3.4∼7.8smatch f1 on amr parsing of the three languageswhile surpass fan and gardent (2020) by around10 bleu on amr-to-text generation..table 3 compares the performance of ﬁne-grained metrics for amr parsing.
it shows thatour xlpt-amrt-s models achieve the best perfor-mance on all the metrics with the only exceptionof concepts for italian amr parsing.
it showsthat like english amr parsing, all models predictreentrancies poorly (szubert et al., 2020).
italso demonstrates that negations is another met-ric which is hard to predict.
in future work, we willpay particular attention to the two metrics..902metric.
smatchunlabeledno wsdconceptsnamed ent.
negationswikiﬁcationreentranciessrl.
blloshmi et al.
(2020)de53.057.753.258.066.011.760.939.947.9.it58.163.458.464.764.729.267.046.154.7.es58.063.058.465.965.923.463.146.655.2.baselinepre-traineditesde66.5468.0564.9071.1672.4969.5366.7868.4065.1678.2173.0668.7968.4281.3479.1248.5751.9342.6971.0569.4067.4044.1046.2042.4063.8065.2060.50.xlpt-amrt-ses71.7675.8672.1476.2984.0957.1973.3248.4068.50.de70.4574.5770.7073.4285.9552.4874.0545.7064.90.it70.8075.0771.1174.8683.3554.9573.7347.9067.30.table 3: fine-grained f1 scores of amr parsing..5.4 discussion.
in this section, we try to answer the following threequestions:.
• first, what is the performance of teacher mod-els when we use teacher models to guide stu-dent ones in teacher-student-based mtl ﬁne-tuning?.
• second, what is the effect of the two machine.
translation tasks in pre-training?.
• third, in our approach we take english aspivot language by taking advantage of largescale english-to-german (or spanish, italian)dataset.
what is the performance of englishamr parsing and amt-to-text generation?.
performance ofteacher models in teacher-student-based mtl ﬁne-tuning.
table 4 com-pares the performance of teacher and student mod-els.
it shows that the performance of teacher mod-els for english amr parsing and english-to-xtranslation is much higher than the counterparts ofstudent models (i.e., stu.
(before) in the table).
thetable also shows that the student models beneiftfrom receiving guidance from the teachers.
forexample, while the english amr parsing model(i.e., the teacher) achieves 78.62 smatch f1 on thetest set, it improves the performance of the germanamr parsing model (i.e., the student) from 68.31smatch f1 to 70.45. similarly, while the english-to-german model (i.e., the teacher) achieves 39.40bleu on the test set, it boosts the performance ofthe german amr-to-text generation model (i.e.,the student) from 24.15 bleu to 25.69..effect of machine translation tasks in pre-training.
we use german as a representative..note that when machine translation tasks are notinvolved in pre-training, the targeted mtl ﬁne-tuning method is not applicable since we cannotuse machine translation as the auxiliary task.
there-fore, we use the vanilla ﬁne-tuning method to ﬁne-tune the pre-trained models.
table 5 comparesthe performance with/without machine translationtasks in pre-training.
from it, we observe that in-cluding machine translation tasks in pre-trainingachieves improvements of 2.77 smatch f1 and 2.46bleu on german amr parsing and text genera-tion, respectively.
this suggests the necessity tohave machine translation tasks in pre-training..performance of english amr parsing andamr-to-text generation.
based on the pre-trained models, we take the targeted mtl ﬁne-tuning method (section 4.3) as a representative.
speciﬁcally, for english amr parsing, we chooseenglish-to-x (x ∈ {german, spanish, italian}) as theauxiliary ﬁne-tuning task while for english testgeneration, we choose x-to-english as the auxil-iary task..table 6 shows that the performance of englishparsing and generation is much higher than that ofother languages.
moreover, we ﬁnd that the resultsof english amr parsing are quite close when com-bining english with any of other languages whereasthe results of english amr-to-text generation areconsiderably different.
one possible reason for thephenomenon is that english amr-to-text genera-tion is relevant to the sizes of machine translationdatasets used in pre-training (i.e., 3.9m for en-detranslation whereas 1.9m for both en-es and en-it, respectively) while english parsing seems tobe less affected by the sizes of (silver) datasets.
itindicates that with more english sentences in pre-training, it helps the generation models to generate.
903model.
teacherstu.
(before)stu.(after).
amr parsinges78.1670.1071.76.de78.6268.3170.45.it78.5869.6470.80.amr-to-textes40.4130.8331.36.de39.4024.1525.69.it36.6728.2728.42.table 4: performance comparison of teacher and student models.
note that the performance of teacher models isfor english amr parsing, and english-to-x translation, respectively..pre-training tasks amr parsing amr-to-text66.88all- mt tasks64.11.
23.1120.65.references.
table 5:performance comparison for germanwith/without machine translation tasks in pre-training..language amr parsing amr-to-textdeeneseniten.
24.1540.8930.8332.2928.2731.98.
68.3178.6270.1078.1669.6478.58.table 6: performance comparison for amr parsingand amr-to-text generation for english and otherthree zero-shot languages..more ﬂuent and correct english sentences..6 conclusions.
in this paper we proposed a cross-lingual pre-training approach via multi-task learning for zero-shot amr parsing and amr-to-text generation.
upon english amr dataset and english-to-x par-allel datasets, we pre-trained models on three typesof relevant tasks, including amr parsing, amr-to-text generation, and machine translation.
wealso explored and compared four different ﬁne-tuning methods.
experimentation on the multilin-gual amr dataset shows that our approach greatlyadvances the state of the art..acknowledgments.
this work wassupported by the nationalkey r&d program of china under grant no.
2020aaa0108600 and by the national naturalscience foundation of china under grant no.
61876120..rafael anchiˆeta and thiago pardo.
2020. semanticallyinspired amr alignment for the portuguese language.
in proceedings of emnlp, pages 1595–1600..xuefeng bai, linfeng song, and yue zhang.
2020. on-line back-parsing for amr-to-text generation.
inproceedings of emnlp, pages 1206–1219..miguel ballesteros and yaser al-onaizan.
2017. amrin proceedings of.
parsing using stack-lstms.
emnlp, pages 1269–1275..laura banarescu, claire bonial, shu cai, madalinageorgescu, kira grifﬁtt, ulf hermjakob, kevinknight, philipp koehn, martha palmer, and nathanschneider.
2013. abstract meaning representationfor sembanking.
in proceedings of the 7th linguis-tic annotation workshop and interoperability withdiscourse, pages 178–186..daniel beck, gholamreza haffari, and trevor cohn.
graph-to-sequence learning using gatedin proceedings of acl,.
2018.graph neural networks.
pages 273–283..michele bevilacqua, rexhina blloshmi, and robertonavigli.
2021. one spring to rule them both: sym-metric amr semantic parsing and generation withouta complex pipeline.
in proceedings of aaai..rexhina blloshmi, rocco tripodi, and roberto nav-igli.
2020. xl-amr: enabling cross-lingual amrin pro-parsing with transfer learning techniques.
ceedings of emnlp, pages 2487–2500..deng cai and wai lam.
2019. core semantic ﬁrst: ain proceed-.
top-down approach for amr parsing.
ings of emnlp, pages 3799–3809..deng cai and wai lam.
2020a.
amr parsing viain proceed-.
graph(cid:10)sequence iterative inference.
ings of acl, pages 1290–1301..deng cai and wai lam.
2020b.
graph transformerin proceedings of.
for graph-to-sequence learning.
aaai, pages 7464–7471..shu cai and kevin knight.
2013. smatch: an evalu-ation metric for semantic feature structure.
in pro-ceedings of acl, pages 748–752..904kris cao and stephen clark.
2019. factorising amrin proceedings of.
generation through syntax.
naacl, pages 2157–2163..yun chen, yang liu, yong cheng, and victor o.k.
li.
2017. a teacher-student framework for zero-resource neural machine translation.
in proceedingsof acl, pages 1925–1935..hamza harkous, isabel groves, and amir saffari.
2020.have your text and use it too!
end-to-end neuraldata-to-text generation with semantic ﬁdelity.
inproceedings of coling, pages 2410–2424..yoon kim and alexander m. rush.
2016. sequence-in proceedings of.
level knowledge distillation.
emnlp, pages 1317–1327..marco damonte and shay b. cohen.
2018. cross-lingual abstract meaning representation parsing.
inproceedings of naacl, pages 1146–1155..diederik p. kingma and jimmy ba.
2015. adam: amethod for stochastic optimization.
in proceedingsof iclr..marco damonte and shay b. cohen.
2019. structuralneural encoders for amr-to-text generation.
in pro-ceedings of naacl, pages 3649–3658..marco damonte, shay b. cohen, and giorgio satta.
2017. an incremental parser for abstract meaningrepresentation.
in proceedings of eacl, pages 536–546..angela fan and claire gardent.
2020. multilingualamr-to-text generation.
in proceedings of emnlp,pages 2889–2901..thiago castro ferreira, iacer calixto, sander wubben,and emiel krahmer.
2017. linguistic realisation asmachine translation: comparing different mt modelsfor amr-to-text generation.
in proceedings of inlg,pages 1–10..jeffrey flanigan, chris dyer, noah a. smith, andjaime carbonell.
2016. generation from abstractinmeaning representation using tree transducers.
proceedings of naacl, pages 731–739..jeffrey flanigan, sam thomson, jaime carbonell,chris dyer, and noah a. smith.
2014. a discrim-inative graph-based parser for the abstract meaningrepresentation.
in proceedings of acl, pages 1426–1436..donglai ge, junhui li, muhua zhu, and shoushanli.
2019. modeling source syntax and semanticsfor neural amr parsing.
in proceedings of ijcai,pages 4975–4981..jonas groschwitz, matthias lindemann, meaghanfowlie, mark johnson, and alexander koller.
2018.amr dependency parsing with a typed semantic al-gebra.
in proceedings of acl, pages 1831–1841..zhijiang guo and wei lu.
2018. better transition-based amr parsing with a reﬁned search space.
inproceedings of emnlp, pages 1712–1722..guillaume klein, yoon kim, yuntian deng, jean senel-lart, and alexander m. rush.
2017. opennmt:open-source toolkit for neural machine translation.
in proceedings of acl, system demonstrations,pages 67–72..ioannis konstas, srinivasan iyer, mark yatskar, yejinchoi, and luke zettlemoyer.
2017. neural amr:sequence-to-sequence models for parsing and gen-eration.
in proceedings of acl, pages 146–157..guillaume lample and alexis conneau.
2019. cross-lingual language model pretraining.
in proceedingsof neurips..zhizhong li and derek hoiem.
2018. learning with-out forgetting.
ieee transactions on pattern analy-sis and machine intelligence, 40(12):2935–2947..chunchuan lyu and ivan titov.
2018. amr parsing asgraph prediction with latent alignment.
in proceed-ings of acl, pages 397–407..manuel mager, ram´on fernandez astudillo, tahiranaseem, md arafat sultan, young-suk lee, raduflorian, and salim roukos.
2020. gpt-too: alanguage-model-ﬁrst approach for amr-to-text gen-eration.
in proceedings of acl, pages 1846–1852..rik van noord and johan bos.
2017. neural seman-tic parsing by character-based translation: experi-ments with abstract meaning representation.
com-putational linguistics in the netherlands journal,7:93–108..kishore papineni, salim roukos, ward todd, and wei-jing zhu.
2002. bleu: a method for automatic evalu-ation of machine translation.
in proceedings of acl,pages 311–318..xiaochang peng, chuang wang, daniel gildea, andnianwen xue.
2017. addressing the data sparsityin proceedings ofissue in neural amr parsing.
eacl, pages 366–375..zhijiang guo, yan zhang, zhiyang teng, and wei lu.
2019. densely connected graph convolutional net-works for graph-to-sequence learning.
tacl, 7:297–312..nima pourdamghani, kevin knight, and ulf herm-jakob.
2016. generating english from abstractin proceedings of inlg,meaning representations.
pages 21–25..jan hajiˇc, ondˇsov´a bojar, and zdeˇnka ureˇsov´a.
2014.comparing czech and english amrs.
in proceed-ings of workshop on lexical and grammatical re-sources for language processing, pages 55–64..leonardo f. r. ribeiro, claire gardent, and irynagurevych.
2019. enhancing amr-to-text genera-tion with dual graph representations.
in proceedingsof emnlp-ijcnlp, pages 3183–3194..905dongqin xu, junhui li, muhua zhu, min zhang, andimproving amr parsingguodong zhou.
2020.with sequence-to-sequence pre-training.
in proceed-ings of emnlp, pages 2501–2511..nianwen xue, ondˇsov´a bojar, jan hajiˇc, marthapalmer, zdeˇnka ureˇsov´a, and xiuhong zhang.
2014. not an interlingua, but close: comparison ofenglish amrs to chinese and czech.
in proceed-ings of lrec, pages 1765–1772..shaowei yao, tianming wang, and xiaojun wan.
2020. heterogeneous graph transformer for graph-to-sequence learning.
in proceedings of acl, pages7145–7154..sheng zhang, xutai ma, kevin duh, and ben-jamin van durme.
2019a.
amr parsing assequence-to-graph transduction.
in proceedings ofacl, pages 80–94..sheng zhang, xutai ma, kevin duh, and benjaminvan durme.
2019b.
broad-coverage semantic pars-in proceedings of emnlp-ing as transduction.
ijcnlp, pages 3786–3798..yanbin zhao, lu chen, zhi chen, ruisheng cao,su zhu, and kai yu.
2020. line graph enhancedamr-to-text generation with mix-order graph atten-tion networks.
in proceedings of acl, pages 732–741..jiawei zhou, tahira naseem, ram´on fernandez as-tudillo, and radu florian.
2021. amr parsingwith action-pointer transformer.
in proceedings ofnaacl, pages 5585–5598..junsheng zhou, feiyu xu, hans uszkoreit, weiguangqu, ran li, and yanhui gu.
2016. amr parsingwith an incremental joint model.
in proceedings ofemnlp, pages 680–689..jie zhu, junhui li, muhua zhu, longhua qian, minzhang, and guodong zhou.
2019. modeling graphstructure in transformer for better amr-to-text gen-eration.
in proceedings of emnlp-ijcnlp, pages5459–5468..leonardo f. r. ribeiro, martin schmitt, hinrichsch¨utze, and iryna gurevych.
2020.investigat-ing pretrained language models for graph-to-textin computing research repository,generation.
arxiv:2007.08426..rico sennrich, barry haddow, and alexandra birch.
2016. neural machine translation of rare words withsubword units.
in proceedings of acl, pages 1715–1725..janaki sheth, young-suk lee, ram´on fernandez as-tudillo, tahira naseem, radu florian, salim roukos,and todd ward.
2021. bootstrapping multilingualamr with contextual word alignments.
in proceed-ings of eacl, pages 394–404..marco antonio sobrevilla cabezudo, simon mille, andthiago pardo.
2019. back-translation as strategy totackle the lack of corpus in natural language genera-tion from semantic representations.
in proceedingsof msr, pages 94–103..linfeng song, xiaochang peng, yue zhang, zhiguowang, and daniel gildea.
2017. amr-to-text gener-ation with synchronous node replacement grammar.
in proceedings of acl, pages 7–13..linfeng song, ante wang, jinsong su, yue zhang,kun xu, yubin ge, and dong yu.
2020. structuralinformation preserving for graph-to-text generation.
in proceedings of acl, pages 7987–7998..linfeng song, yue zhang, zhiguo wang, and danielgildea.
2018. a graph-to-sequence model for amr-in proceedings of acl, pagesto-text generation.
1616–1626..ida szubert, marco damonte, shay b. cohen, andmark steedman.
2020. the role of reentrancies inin find-abstract meaning representation parsing.
ings of emnlp, pages 2198–2207..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n.gomez, lukaszkaiser, and illia polosukhin.
2017. attention is allin proceedings of nips, pages 5998–you need.
6008..chuan wang, bin li, and nianwen xue.
2018.transition-based chinese amr parsing.
in proceed-ings of naacl, pages 247–252..chuan wang, nianwen xue, and sameer pradhan.
2015a.
boosting transition-based amr parsingwith reﬁned actions and auxiliary analyzers.
in pro-ceedings of acl, pages 857–862..chuan wang, nianwen xue, and sameer pradhan.
2015b.
a transition-based algorithm for amr pars-ing.
in proceedings of naacl, pages 366–375..keenon werling, gabor angeli, and christoerpher d.manning.
2015. robust subgraph generation im-proves abstract meaning representation parsing.
inproceedings of acl, pages 982–991..906averaging the last 5 (20 for both en-es and en-it)checkpoints..for evaluation, we use case-sensitive bleu mea-sured by multi-bleu script.
table 7 shows the per-formance of the three translation models on thetest sets, i.e., newstest2014 for en-de and new-stest2009 for both en-es and en-it..tasken-deen-esen-it.
bleu28.6726.5426.79.table 7: performance in bleu score for the three trans-lation tasks..a english amr parser on amr 2.0.our english amr parser is learned in a seq2seqframework and trained on amr2.0, which con-sists of 36,521 training amrs, 1,368 developmentamrs and 1,371 testing amrs.
we share vocab-ulary for the input and the output by segmentingtokens into pieces by byte pair encoding (bpe)with 20k merge operations..we use opennmt-py as the implementation oftransformer.
in model setting, we use transformerbase model setting.
we use adam with β1 = 0.9,β2 = 0.98 for optimization.
batch size, learningrate, warm-up step, and dropout rate are set to 4096,2.0, 16000 and 0.1 respectively.
we train the modelfor 250k steps on 1 gpus and save models every10k steps.
finally, we obtain ﬁnal model by aver-aging the last 10 checkpoints..the english amr parser achieves 73.68 and73.24 smatch f1 on the dev and test set, respec-tively..b nmt models for english-to-german,english-to-spanish, english-to-italian.
in pre-processing, we tokenize all of mt corpuswith moses scripts.8 then we segment words intopieces by bpe with 32k (30k) bpe merge opera-tions for en-de (both en-es and en-it).
afterﬁltering long and imbalanced pairs, we get 3.9mparallel sentence pairs for en-de and 1.9m forboth en-es and en-it..we again use opennmt-py as the implemen-tation of transformer.
in model setting, we usetransformer big model setting.
we use adam withβ1 = 0.9, β2 = 0.998 for optimization.
batch size,learning rate, warm-up step, and dropout rate areset to 8192, 2.0, 8000 (16000 for both en-es anden-it) and 0.1, respectively.
we train the modelfor 100k (110k for en-es and 150k for en-it)steps on 4 gpus and save models very 5000 steps.
for each translation task, we obtain ﬁnal model by.
8https://github.com/moses-smt/.
mosesdecoder.
907