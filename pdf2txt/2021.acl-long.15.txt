gl-gin: fast and accurate non-autoregressive model for joint multipleintent detection and slot filling.
libo qin, fuxuan wei, tianbao xie, xiao xu, wanxiang che∗, ting liuresearch center for social computing and information retrievalharbin institute of technology, china{lbqin,fuxuanwei,tianbaoxie,xxu,car,tliu}@ir.hit.edu.cn.
abstract.
multi-intent slu can handle multiple intentsin an utterance, which has attracted increas-ing attention.
however, the state-of-the-artjoint models heavily rely on autoregressive ap-proaches, resulting in two issues: slow infer-ence speed and information leakage.
in thispaper, we explore a non-autoregressive modelfor joint multiple intent detection and slot ﬁll-ing, achieving more fast and accurate.
specif-ically, we propose a global-locally graphinteraction network (gl-gin) where a localslot-aware graph interaction layer is proposedto model slot dependency for alleviating unco-ordinated slots problem while a global intent-slot graph interaction layer is introduced tomodel the interaction between multiple intentsand all slots in the utterance.
experimen-tal results on two public datasets show thatour framework achieves state-of-the-art perfor-mance while being 11.5 times faster..1.introduction.
spoken language understanding (slu) (younget al., 2013) is a critical component in spokendialog systems, which aims to understand user’squeries.
it typically includes two sub-tasks: intentdetection and slot ﬁlling (tur and de mori, 2011).
since intents and slots are closely tied, dominantsingle-intent slu systems in the literature (gooet al., 2018; li et al., 2018; liu et al., 2019b; eet al., 2019; qin et al., 2019; teng et al., 2021;qin et al., 2021b,c) adopt joint models to considerthe correlation between the two tasks, which haveobtained remarkable success..multi-intent slu means that the system can han-dle an utterance containing multiple intents, whichis shown to be more practical in the real-world sce-nario, attracting increasing attention.
to this end,.
∗corresponding author..figure 1: (a) autoregressive model generates outputsword by word from left-to-right direction.
the graycolor denotes the unseen information when model de-(b) non-autoregressivecodes for the word denver.
model can produce outputs in parallel.
an denotesairport name..xu and sarikaya (2013) and kim et al.
(2017) be-gin to explore the multi-intent slu.
however, theirmodels only consider the multiple intent detectionwhile ignoring slot ﬁlling task.
recently, gangad-haraiah and narayanaswamy (2019) make the ﬁrstattempt to propose a multi-task framework to jointmodel the multiple intent detection and slot ﬁlling.
qin et al.
(2020b) further propose an adaptive inter-action framework (agif) to achieve ﬁne-grainedmulti-intent information integration for slot ﬁlling,obtaining state-of-the-art performance..though achieving the promising performance,the existing multi-intent slu joint models heav-ily rely on an autoregressive fashion, as shown infigure 1(a), leading to two issues:.
• slow inference speed.
the autoregressivemodels make the generation of slot outputsmust be done through the left-to-right pass,which cannot achieve parallelizable, leadingto slow inference speed..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages178–188august1–6,2021.©2021associationforcomputationallinguistics178(a)howfarisdenverairport(b)howfarisdenverairportooob-ani-anooob-ani-an• information leakage.
autoregressive modelspredict each word slot conditioned on the pre-viously generated slot information (from left-to-right), resulting in leaking the bidirectionalcontext information..in this paper, we explore a non-autoregressiveframework for joint multiple intent detection andslot ﬁlling, with the goal of accelerating inferencespeed while achieving high accuracy, which isshown in figure 1(b).
to this end, we proposea global-locally graph-interaction network (gl-gin) where the core module is a proposed lo-cal slot-aware graph layer and global intent-slotinteraction layer, which achieves to generate in-tents and slots sequence simultaneously and non-autoregressively.
in gl-gin, a local slot-awaregraph interaction layer where each slot hiddenstates connect with each other is proposed to ex-plicitly model slot dependency, in order to alle-viate uncoordinated slot problem (e.g., b-singerfollowed by i-song) (wu et al., 2020) due to thenon-autoregressive fashion.
a global intent-slotgraph interaction layer is further introduced to per-form sentence-level intent-slot interaction.
un-like the prior works that only consider the token-level intent-slot interaction, the global graph is con-structed of all tokens with multiple intents, achiev-ing to generate slots sequence in parallel and speedup the decoding process..experimental results on two public datasetsmixsnips (coucke et al., 2018) and mix-atis (hemphill et al., 1990) show that our frame-work not only obtains state-of-the-art performancebut also enables decoding in parallel.
in addition,we explore the pre-trained model (i.e., roberta (liuet al., 2019c)) in our framework..in summary, the contributions of this work canbe concluded as follows: (1) to the best of ourknowledge, we make the ﬁrst attempt to explorea non-autoregressive approach for joint multipleintent detection and slot ﬁlling; (2) we proposea global-locally graph-interaction network, wherethe local graph is used to handle uncoordinatedslots problem while a global graph is introducedto model sequence-level intent-slot interaction; (3)experiment results on two benchmarks show thatour framework not only achieves the state-of-the-art performance but also considerably speeds upthe slot decoding (up to ×11.5); (4) finally, weexplore the pre-trained model in our framework.
with the pre-trained model, our model reaches a.new state-of-the-art level..for.
this pa-reproducibility, our code forper is publicly available at https://github.com/yizhen20133868/gl-gin..2 problem deﬁnition.
multiple intent detection given input se-quence x = (x1, .
.
.
, xn), multiple intent detec-tion can be deﬁned as a multi-label classiﬁca-tion task that outputs a sequence intent label oi= (oim), where m is the number of intentsin given utterance and n is the length of utterance..1, .
.
.
, oi.
slot filling slot ﬁlling can be seen as a sequencelabeling task that maps the input utterance x into aslot output sequence os = (os.
1 , .
.
.
, osn)..3 approach.
as shown in figure 2(a), we describe the pro-posed framework, which consists of a shared self-attentive encoder (§3.1), a token-level intent de-tection decoder (§3.2) and a global-local graph-interaction graph decoder for slot ﬁlling (§3.3).
both intent detection and slot ﬁlling are optimizedsimultaneously via a joint learning scheme..3.1 self-attentive encoder.
following qin et al.
(2019), we utilize a self-attentive encoder with bilstm and self-attentionmechanism to obtain the shared utterance repre-sentation, which can incorporate temporal featureswithin word orders and contextual information..bilstm thebidirectional lstm (bil-stm) (hochreiter and schmidhuber, 1997) havebeen successfully applied to sequence labelingtasks (li et al., 2020, 2021).
we adopt bilstmto read the input sequence {x1, x2, .
.
.
, xn}forwardly and backwardly to produce context-sensitive hidden states h = {h1, h2, .
.
.
, hn}, byrepeatedly applying the hi = bilstm (φemb(xi),hi−1, hi+1), where φemb is embedding function..self-attention following vaswani et al.
(2017),we map the matrix of input vectors x ∈ rn×d (drepresents the mapped dimension) to queries q,keys k and values v matrices by using differentlinear projections.
then, the self-attention outputc ∈ rn×d is a weighted sum of values:.
c = softmax.
(1).
(cid:19).
(cid:18) qk(cid:62)√dk.
v ..179figure 2: the overﬂow of model architecture (a) and global-locally graph interaction layer (b)..we concatenate the output of bilstm and self-.
attention as the ﬁnal encoding representation:.
e = h || c,.
(2).
where e = {e1, .
.
.
, en} ∈ rn×2d and || is con-catenation operation..3.2 token-level intent detection decoder.
inspired by qin et al.
(2019), we perform a token-level multi-label multi-intent detection, where wepredict multiple intents on each token and the sen-tence results are obtained by voting for all tokens.
speciﬁcally, we ﬁrst feed the contextual encodinge into an intent-aware bilstm to enhance itstask-speciﬁc representations:.
t = bilstm (cid:0)et, hihi.
t−1, hi.
t+1.
(cid:1) ..(3).
then, hi.
t is used for intent detection, using:.
it = σ(w i (leakyrelu(w h hi.
t +bh))+bi ), (4).
where it denotes the intent results at the t-th word;σ denotes the sigmoid activation function; w h andw i are the trainable matrix parameters.
finally, the sentence intent results oi.
k can be ob-.
tained by:.
oi = {oi.
k|(.
1[i(i,k) > 0.5]) > n/2},.
(5).
n(cid:88).
i=1.
where i(i,k) represents the classiﬁcation result oftoken i for oik..we predict the label as the utterance intent whenit gets more than half positive predictions in all ntokens.
for example, if i1 = {0.9, 0.8, 0.7, 0.1},i2 = {0.8, 0.2, 0.7, 0.4}, i3 = {0.9, 0.3, 0.2, 0.3},from three tokens, we get {3, 2, 1, 0} positive votes(> 0.5) for four intents respectively.
thus the indexwhere more than half of the votes ( > 3/2 ) were3, we predict intents oi =obtained was oi{oi3}..1 and oi.
1, oi.
3.3 slot filling decoder.
one main advantage of our framework is the pro-posed global-locally graph interaction network forslot ﬁlling, which is a non-autoregressive paradigm,achieving the slot ﬁlling decoding in parallel.
in thefollowing, we ﬁrst describe the slot-aware lstm(§3.3.1) to obtain the slot-aware representations,and then show how to apply the global-locallygraph interaction layer (§3.3.2) for decoding..3.3.1 slot-aware lstmwe utilize a bilstm to produce the slot-awarehidden representation s = (s1, .
.
.
, sn).
at eachdecoding step t, the decoder state st calculating by:.
st = bilstm (cid:0)i t || et, st−1, st+1.
(cid:1) ,.
(6).
where et denotes the aligned encoder hiddenstate and i t denotes the predicted intent informa-tion..180self-attentive encoder𝑥3𝑥4𝑥2𝑒1𝑒2𝑒3𝑒4𝐼1𝐼2𝐼3𝐼4𝐼1𝐼2𝐼3𝐼4𝑠1𝑠2𝑠3𝑠4𝑒1𝑒2𝑒3𝑒4token-level intent decoder𝑜1𝐼𝑜2𝐼global-locally graph interaction layer𝑜1𝑠𝑜2𝑠𝑜3𝑠𝑜4𝑠intent detectionslot filling𝑥1𝑠1𝑠2𝑠3𝑠4local slot-aware interaction layer𝑠1𝑠2𝑠3𝑠4𝑜1𝐼𝑜2𝐼global intent-slot interaction layer𝑜1𝑠𝑜2𝑠𝑜3𝑠𝑜4𝑠(b) global-locally graph interaction layer(a) model frameworkbilstmbilstm3.3.2 global-locally graph interaction layerthe proposed global-locally graph interaction layerconsists of two main components: one is a localslot-aware graph interaction network to model de-pendency across slots and another is the proposedglobal intent-slot graph interaction network to con-sider the interaction between intents and slots..in this section, we ﬁrst describe the vanilla graphattention network.
then, we illustrate the localslot-aware and global intent-slot graph interactionnetwork, respectively..vanilla graph attention network a graph at-tention network (gat) (veliˇckovi´c et al., 2018)is a variant of graph neural network, which fusesthe graph-structured information and node featureswithin the model.
its masked self-attention layersallow a node to attend to neighborhood features andlearn different attention weights, which can auto-matically determine the importance and relevancebetween the current node with its neighborhood..in particular, for a given graph with n nodes,one-layer gat take the initial node features ˜h ={˜h1, .
.
.
, ˜hn }, ˜hn ∈ rf as input, aiming atproducing more abstract representation, ˜h=(cid:48)(cid:48){˜h1, .
.
.
, ˜hn ∈ rf (cid:48), as its output.
the at-tention mechanism of a typical gat can be sum-marized as below:.
(cid:48)n }, ˜h.
(cid:48).
˜h.
(cid:48)i.k=1 σ(cid:0) (cid:80).
˜hj= ||kijw kαkh˜hi(cid:107)w hexp(leakyrelu(a(cid:62)[w hexp (leakyrelu.
(cid:1),˜hj ]))(cid:48)˜h˜hi(cid:107)w hj ].
a(cid:62)[w h.j∈ni.
(cid:16).
(7).
,(8).
(cid:17)).
αij =.
(cid:80).
j(cid:48)∈ni.
where w h ∈ rf (cid:48)×f and a ∈ r2f (cid:48)are the train-able weight matrix; ni denotes the neighbors ofnode i (including i); αij is the normalized atten-tion coefﬁcients and σ represents the nonlinearityactivation function; k is the number of heads..local slot-aware graph interaction layergiven slot decode hidden representations s = (s1,.
.
.
, sn), we construct a local slot-aware graphwhere each slot hidden node connects to otherslots.
this allows the model to achieve to modelthe dependency across slots, alleviating the unco-ordinated slots problem.
speciﬁcally, we constructthe graph g = (v, e) in the following way,.
vertices we deﬁne the v as the vertices set.
each word slot is represented as a vertex.
eachvertex is initialized with the corresponding slothidden representation.
thus, the ﬁrst layer statesvector for all nodes is s1 = s = (s1, .
.
.
, sn)..edges since we aim to model dependencyacross slots, we construct a slot-aware graph inter-action layer so that the dependency relationship canbe propagated from neighbor nodes to the currentnode.
each slot can connect other slots with a win-dow size.
for node si, only {si−m, .
.
.
, si+m}will be connected where m is a hyper-parameterdenotes the size of sliding window that controls thelength of utilizing utterance context..information aggregation the aggregation.
process at l-th layer can be deﬁned as:.
i = σ(cid:0) (cid:88)sl+1.
αijw lslj.
(cid:1),.
(9).
j∈ni.
where ni is a set of vertices that denotes the con-nected slots..after stacking l layer, we obtain the con-textual slot-aware local hidden features sl+1={sl+11., .
.
.
, sl+1.
}.
n.global slot-intent graph interaction layerto achieve sentence-level intent-slot interaction,we construct a global slot-intent interaction graphwhere all predicted multiple intents and sequenceslots are connected, achieving to output slot se-quences in parallel.
speciﬁcally, we construct thegraph g = (v, e) in the following way,.
vertices as we model the interaction betweenintent and slot token, we have n + m number ofnodes in the graph where n is the sequence lengthand m is the number of intent labels predicted bythe intent decoder.
the input of slot token fea-ture is g[s,1] = sl+1 ={sl+1} whichis produced by slot-aware local interaction graphnetwork while the input intent feature is an embed-ding g[i,1] = {φemb(oim)} whereφemb is a trainable embedding matrix.
the ﬁrstlayer states vector for slot and intent nodes is g1= {g[i,1] , g[s,1] } = {φemb(oim),sl+1}1.
1), .
.
.
, φemb(oi.
1), .
.
.
, φemb(oi.
, .
.
.
, sl+1.
, .
.
.
, sl+1.
n.n.1.edges there are three types of connections in.
this graph network..• intent-slot connection: since slots and intentsare highly tied, we construct the intent-slotconnection to model the interaction betweenthe two tasks.
speciﬁcally, each slot connectsall predicted multiple intents to automaticallycapture relevant intent information..181• slot-slot connection: we construct the slot-slot connection where each slot node connectsother slots with the window size to furthermodel the slot dependency and incorporatethe bidirectional contextual information..• intent-intent connection: following qin et al.
(2020b), we connect all the intent nodes toeach other to model the relationship betweeneach intent, since all of them express the sameutterance’s intent..information aggregation the aggregationprocess of the global gat layer can be formulatedas:.
g[s,l+1]i.
= σ(.
αijw gg[s,l].
j +.
αijw gg[i,l].
),.
j.
(cid:88).
j∈gs.
(cid:88).
j∈gi.
(10)where gs and gi are vertices sets which denotesthe connected slots and intents, respectively..3.3.3 slot predictionafter l layers’ propagation, we obtain the ﬁnal slotrepresentation g[s,l+1] for slot prediction..4 experiments.
4.1 datasets.
we conduct experiments on two publicly avail-able multi-intent datasets.1 one is the mix-atis (hemphill et al., 1990; qin et al., 2020b),which includes 13,162 utterances for training, 756utterances for validation and 828 utterances for test-ing.
another is mixsnips (coucke et al., 2018;qin et al., 2020b), with 39,776, 2,198, 2,199 utter-ances for training, validation and testing..4.2 experimental settings.
the dimensionality of the embedding is 128 and64 on atis and snips, respectively.
the dimen-sionality of the lstm hidden units is 256. thebatch size is 16. the number of the multi head is 4and 8 on mixatis and mixsnips dataset, respec-tively.
all layer number of graph attention networkis set to 2. we use adam (kingma and ba, 2015)to optimize the parameters in our model.
for allthe experiments, we select the model which worksthe best on the dev set and then evaluate it on thetest set.
all experiments are conducted at geforcertx 2080ti and titan xp..(cid:16).
w sg[s,l+1]t.(cid:17).
,.
ys.
t = softmaxt = arg max(ysos.
t ),.
(11).
(12).
4.3 baselines.
where w s is a trainable parameter and ospredicted slot if the t-th token in an utterance..t is the.
3.4.joint training.
following goo et al.
(2018), we adopt a joint train-ing model to consider the two tasks and updateparameters by joint optimizing.
the intent detec-tion objective is:.
ce(ˆy, y) = ˆy log (y) + (1 − ˆy) log (1 − y) , (13)ni(cid:88).
n(cid:88).
ce(ˆy(j,i)i., y(j,i)i.)
.
(14).
l1 (cid:44) −.
i=1.
j=1.
similarly, the slot ﬁlling task objective is:.
l2 (cid:44) −.
n(cid:88).
ns(cid:88).
i=1.
j=1.
ˆy(j,s)i.log.
(cid:16).
y(j,s)i.
(cid:17).
,.
(15).
where ni is the number of single intent labels andns is the number of slot labels..the ﬁnal joint objective is formulated as:.
l = αl1 + βl2,.
(16).
where α and β are hyper-parameters..we compare our model with the following best(1) attention birnn.
liu andbaselines:lane (2016) propose an alignment-based rnnfor joint slot ﬁlling and intent detection;(2)slot-gated atten.
goo et al.
(2018) pro-pose a slot-gated joint model, explicitly consideringthe correlation between slot ﬁlling and intent detec-tion; (3) bi-model.
wang et al.
(2018) proposethe bi-model to model the bi-directional betweenthe intent detection and slot ﬁlling; (4) sf-idnetwork.
e et al.
(2019) proposes the sf-id net-work to establish a direct connection between thetwo tasks; (5) stack-propagation.
qin et al.
(2019) adopt a stack-propagation framework toexplicitly incorporate intent detection for guidingslot ﬁlling; (6) joint multiple id-sf.
gan-gadharaiah and narayanaswamy (2019) propose amulti-task framework with slot-gated mechanismfor multiple intent detection and slot ﬁlling; (7)agif qin et al.
(2020b) proposes an adaptive in-teraction network to achieve the ﬁne-grained multi-.
1we adopt the cleaned verison that removes the repeatedsentences in original dataset, which is available at https://github.com/looperxx/agif..182model.
mixatisoverall(acc) slot(f1).
mixsnipsintent(acc) overall(acc) slot(f1).
attention birnn (liu and lane, 2016)slot-gated (goo et al., 2018)bi-model (wang et al., 2018)sf-id (e et al., 2019)stack-propagation (qin et al., 2019)joint multiple id-sf (gangadharaiah and narayanaswamy, 2019)agif (qin et al., 2020b)gl-gin.
39.135.534.434.940.136.140.843.5*.
86.487.783.987.487.884.686.788.3*.
74.663.970.366.272.173.474.476.3*.
59.555.463.459.972.962.974.275.4*.
89.487.990.790.694.290.694.294.9*.
intent(acc)95.494.695.695.096.095.195.195.6.table 1: main results.
the numbers with * indicate that the improvement of our framework over all baselines isstatistically signiﬁcant with p < 0.05 under t-test..modelstack-propagationjoint multiple id-sfagifgl-gin.
decode latency(s) speedup.
34.545.348.54.2.
8.2×10.8×11.5×1.0×.
table 2: speed comparison.
speedup is based on theratio of the time taken by the slot decoding part of dif-ferent models to run an epoch on the mixatis datasetwith batch size set to 32..intent information integration, achieving state-of-the-art performance..4.4 main results.
following goo et al.
(2018) and qin et al.
(2020b),we evaluate the performance of slot ﬁlling usingf1 score, intent prediction using accuracy, thesentence-level semantic frame parsing using over-all accuracy.
overall accuracy measures the ratioof sentences for which both intent and slot are pre-dicted correctly in a sentence..table 1 shows the results, we have the followingobservations: (1) on slot ﬁlling task, our frame-work outperforms the best baseline agif in f1scores on two datasets, which indicates the pro-posed local slot-aware graph successfully modelsthe dependency across slots, so that the slot ﬁllingperformance can be improved.
(2) more impor-tantly, compared with the agif, our frameworkachieves +2.7% and 1.2% improvements for mix-atis and mixsnips on overall accuracy, respec-tively.
we attribute it to the fact that our proposedglobal intent-slot interaction graph can better cap-ture the correlation between intents and slots, im-proving the slu performance..4.5 analysis.
4.5.1 speedupone of the core contributions of our frameworkis that the decoding process of slot ﬁlling canbe signiﬁcantly accelerated with the proposed.
non-autoregressive mechanism.
we evaluate thespeed by running the model on the mixatistest data in an epoch, ﬁxing the batch size to32. the comparison results are shown in ta-ble 2. we observe that our model achieves the×8.2, ×10.8 and ×11.5 speedup compared withsota models stack-propagation, jointmultiple id-sf and agif.
this is becausethat their model utilizes an autoregressive architec-ture that only performs slot ﬁlling word by word,while our non-autoregressive framework can con-duct slot ﬁlling decoding in parallel.
in addition,it’s worth noting that as the batch size gets larger,gl-gin can achieve better acceleration whereour model could achieve ×17.2 speedup comparedwith agif when batch size is 64..4.5.2 effectiveness of the local slot-aware.
graph interaction layer.
we study the effectiveness of the local slot-awareinteraction graph layer with the following ablation.
we remove the local graph interaction layer anddirectly feed the output of the slot lstm to theglobal intent-slot graph interaction layer.
we referit to w/o local gal in tabel 3. we can clearlyobserve that the slot f1 drops by 1.5% and 1.2% onmixatis and mixsnips datasets.
we attribute thisto the fact that local slot-aware gal can capturethe slot dependency for each token, which helpsto alleviate the slot uncoordinated problems.
aqualitative analysis can be founded at section 4.5.6..4.5.3 effectiveness of global slot-intentgraph interaction layer.
in order to verify the effectiveness of slot-intentglobal interaction graph layer, we remove theglobal interaction layer and utilizes the output oflocal slot-aware gal module for slot ﬁlling.
it isnamed as w/o global intent-slot gal in table 3.we can observe that the slot f1 drops by 0.9%,1.3%, which demonstrates that intent-slot graph in-.
183model.
w/o local slot-aware galw/o global intent-slot gal.
+ more parametersw/o global-locally galgl-gin.
overall(acc)41.140.941.940.543.5.mixatisslot(f1)86.887.487.786.388.3.intent(acc)74.075.575.075.276.3.overall(acc)71.471.773.070.275.4.mixsnipsslot(f1)93.793.693.892.994.9.intent(acc)95.295.595.595.095.6.table 3: ablation experiment..figure 3: visualization.
we use the green color to indi-cate the attention value..teraction layer can capture the correlation betweenmultiple intents, which is beneﬁcial for the seman-tic performance of slu system..following qin et al.
(2020b), we replace multi-ple lstm layers (2-layers) as the proposed global-locally graph layer to verify that the proposedglobal-locally graph interaction layer rather thanthe added parameters works.
table 3 (more pa-rameters) shows the results.
we observe that ourmodel outperforms more parameters by 1.6% and2.4% overall accuracy in two datasets, which showsthat the improvements come from the proposedglobal-locally graph interaction layer rather thanthe involved parameters..4.5.4 effectiveness of the global-locallygraph interaction layer.
instead of using the whole global-locally graph in-teraction layer for slot ﬁlling, we directly leveragethe output of slot-aware lstm to predict each to-ken slot to verify the effect of the global-locallygraph interaction layer.
we name the experimentas w/o global-locally gal in tabel 3. from theresults, we can observe that the absence of globalgat module leads to 3.0% and 5.2% overall accu-racy drops on two datasets.
this indicates that the.
figure 4:roberta..overall accuracy performances with.
global-locally graph interaction layer encouragesour model to leverage slot dependency and intentinformation, which can improve slu performance..4.5.5 visualizationto better understand how global-local graph inter-action layer affects and contributes to the ﬁnal re-sult, we visualize the attention value of the globalintent-slot gal.
as is shown in figure 3, we visu-alize the dependence of the word “6” on contextand intent information.
we can clearly observe thattoken “6” obtains information from all contextualtokens.
the information from “and 10” helpsto predict the slot, where the prior autoregressivemodels cannot be achieved due to the generationword by word from left to right..4.5.6 qualitative analysiswe conduct qualitative analysis by providing a casestudy that consists of two sequence slots whichare generated from agif and our model.
fromtable 4, for the word “6”, agif predicts its slotlabel as “o” incorrectly.
this is because that agifonly models its left information, which makes ithard to predict “6” is a time slot.
in contrast,our model predicts the slot label correctly.
weattribute this to the fact that our proposed globalintent-slot interaction layer can model bidirectionalcontextual information.
in addition, our frameworkpredicts the word slot “am” correctly while agifpredicts it incorrectly (i-airport name follows b-depart time), indicating that the proposed local slot-.
184whatairlinesofffromlovefieldbetween6and10amonjunesixthoooob-ariport_namei-ariport_nameob-start_timeob-end_timei-end_timeob-month_nameb-day_numberintent : atis_airlinemixatismixsnips02040608040.874.243.575.450.080.753.682.6agifgl-ginagif + robertagl-gin + robertatexts.
what.
airlines.
off.
from.
agif.
gl-gin.
o.o.o.o.o.o.o.o.loveb-fromlocairport nameb-fromlocairport name.
ﬁeldi-fromlocairport namei-fromlocairport name.
between.
o.o.
6.o.b-depart timestart time.
and.
o.o.
10b-depart timeend timeb-depart timeend time.
am.
i-tolocairport namei-depart timeend time.
on.
o.o.juneb-depart datemonth nameb-depart datemonth name.
sixthb-depart dateday numberb-depart dateday number.
table 4: case study.
predicted slots sequence about utterance “what airlines off from love ﬁeld between 6 and10 am on june sixth”.
aware graph layer has successfully captured the slotdependency..4.5.7 effect of pre-trained model.
following qin et al.
(2019), we explore the pre-trained model in our framework.
we replace theself-attentive encoder by roberta (liu et al., 2019c)with the ﬁne-tuning approach.
we keep other com-ponents identical to our framework and follow qinet al.
(2019) to consider the ﬁrst subword label if aword is broken into multiple subwords..figure 4 gives the result comparison of agif,gl-gin and two models with roberta on twodatasets.
we have two interesting observations.
first, the roberta-based model remarkablywell on two datasets.
we attribute this to the factthat pre-trained models can provide rich semanticfeatures, which can help slu.
second, gl-gin +roberta outperforms agif+roberta on bothdatasets and reaches a new state-of-the-art perfor-mance, which further veriﬁes the effectiveness ofour proposed framework..5 related work.
slot filling and intent detection recently,joint models (zhang and wang, 2016; hakkani-t¨ur et al., 2016; goo et al., 2018; li et al., 2018;xia et al., 2018; e et al., 2019; liu et al., 2019b;qin et al., 2019; zhang et al., 2019; wu et al., 2020;qin et al., 2021b; ni et al., 2021) are proposed toconsider the strong correlation between intent de-tection and slot ﬁlling have obtained remarkablesuccess.
compared with their work, we focus onjointly modeling multiple intent detection and slotﬁlling while they only consider the single-intentscenario..more recently, multiple intent detection can han-dle utterances with multiple intents, which has at-tracted increasing attention.
to the end, xu andsarikaya (2013) and kim et al.
(2017) begin to ex-plore the multiple intent detection.
gangadharaiahand narayanaswamy (2019) ﬁrst apply a multi-taskframework with a slot-gate mechanism to jointlymodel the multiple intent detection and slot ﬁll-.
ing.
qin et al.
(2020b) propose an adaptive interac-tion network to achieve the ﬁne-grained multipleintent information integration for token-level slotﬁlling, achieving the state-of-the-art performance.
their models adopt the autoregressive architecturefor joint multiple intent detection and slot ﬁlling.
in contrast, we propose a non-autoregressive ap-proach, achieving parallel decoding.
to the bestof our knowledge, we are the ﬁrst to explore anon-autoregressive architecture for multiple intentdetection and slot ﬁlling..graph neural network for nlp graph neuralnetworks that operate directly on graph structuresto model the structural information, which has beenapplied successfully in various nlp tasks.
linmeiet al.
(2019) and huang and carley (2019) exploregraph attention network (gat) (veliˇckovi´c et al.,2018) for classiﬁcation task to incorporate the de-pendency parser information.
cetoli et al.
(2017)and liu et al.
(2019a) apply graph neural networkto model the non-local contextual information forsequence labeling tasks.
yasunaga et al.
(2017)and feng et al.
(2020a) successfully apply a graphnetwork to model the discourse information forthe summarization generation task, which achievedpromising performance.
graph structure are suc-cessfully applied for dialogue direction (feng et al.,2020b; fu et al., 2020; qin et al., 2020a, 2021a).
in our work, we apply a global-locally graph inter-action network to model the slot dependency andinteraction between the multiple intents and slots..6 conclusion.
in this paper, we investigated a non-autoregressivemodel for joint multiple intent detection and slotﬁlling.
to this end, we proposed a global-locallygraph interaction network where the uncoordinated-slots problem can be addressed with the proposedlocal slot-aware graph while the interaction be-tween intents and slots can be modeled by theproposed global intent-slot graph.
experimentalresults on two datasets show that our frameworkachieves state-of-the-art performance with ×11.5times faster than the prior work..185acknowledgements.
this work was supported by the national key r&dprogram of china via grant 2020aaa0106501 andthe national natural science foundation of china(nsfc) via grant 61976072 and 61772153. thiswork was also supported by the zhejiang lab’sinternational talent fund for young professionals..references.
alberto cetoli, stefano bragaglia, andrew o’harney,and marc sloan.
2017. graph convolutional net-in proceed-works for named entity recognition.
ings of the 16th international workshop on tree-banks and linguistic theories, pages 37–45, prague,czech republic..alice coucke, alaa saade, adrien ball, th´eodorebluche, alexandre caulier, david leroy, cl´ementdoumouro, thibault gisselbrecht, francesco calta-girone, thibaut lavril, et al.
2018. snips voice plat-form: an embedded spoken language understandingsystem for private-by-design voice interfaces.
arxivpreprint arxiv:1805.10190..haihong e, peiqing niu, zhongfu chen, and meinasong.
2019. a novel bi-directional interrelatedmodel for joint intent detection and slot ﬁlling.
inproceedings oftheassociation for computational linguistics, pages5467–5471, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
xiachong feng, xiaocheng feng, bing qin, xinweigeng, and ting liu.
2020a.
dialogue discourse-aware graph convolutional networks for abstractivemeeting summarization..xiachong feng, xiaocheng feng, bing qin, and tingliu.
2020b.
incorporating commonsense knowl-edge into abstractive dialogue summarization viaarxiv preprintheterogeneous graph networks.
arxiv:2010.10044..qiankun fu, yue zhang, jiangming liu, and meis-han zhang.
2020. drts parsing with structure-aware encoding and decoding.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 6818–6828, online.
as-sociation for computational linguistics..rashmi.
gangadharaiahandbalakrishnanjoint multiple intentnarayanaswamy.
2019.detection and slot labeling for goal-oriented dialog.
in proceedings ofthenorth american chapter ofthe association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages564–569, minneapolis, minnesota.
association forcomputational linguistics..the 2019 conference of.
chih-wen goo, guang gao, yun-kai hsu, chih-lihuo, tsung-chieh chen, keng-wei hsu, and yun-nung chen.
2018. slot-gated modeling for jointslot ﬁlling and intent prediction.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 753–757, new orleans, louisiana.
as-sociation for computational linguistics..dilek hakkani-t¨ur, gokhan tur, asli celikyilmaz,yun-nung vivian chen, jianfeng gao, li deng, andye-yi wang.
2016. multi-domain joint semanticframe parsing using bi-directional rnn-lstm.
in pro-ceedings of the 17th annual meeting of the interna-tional speech communication association (inter-speech 2016).
isca..charles t. hemphill, john j. godfrey, and george r.doddington.
1990. the atis spoken language sys-tems pilot corpus.
in speech and natural language:proceedings of a workshop held at hidden valley,pennsylvania, june 24-27,1990..sepp hochreiter and j¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735–1780..binxuan huang and kathleen carley.
2019. syntax-level sentiment classiﬁcation withaware aspectin proceedings of thegraph attention networks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5469–5477, hong kong,china.
association for computational linguistics..byeongchang kim, seonghan ryu, and gary geunbaelee.
2017. two-stage multi-intent detection for spo-ken language understanding.
multimedia tools andapplications, 76(9):11377–11390..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..changliang li, liang li, and ji qi.
2018. a self-attentive model with gate mechanism for spoken lan-in proceedings of the 2018guage understanding.
conference on empirical methods in natural lan-guage processing, pages 3824–3833, brussels, bel-gium.
association for computational linguistics..yangming li, han li, kaisheng yao, and xiaolong li.
2020. handling rare entities for neural sequencelabeling.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6441–6451, online.
association for computa-tional linguistics..yangming li,.
lemao liu, and shuming shi.
2021.empirical analysis of unlabeled entity problem innamed entity recognition.
in international confer-ence on learning representations..186hu linmei, tianchi yang, chuan shi, houye ji, andxiaoli li.
2019. heterogeneous graph attention net-works for semi-supervised short text classiﬁcation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4821–4830, hong kong, china.
association for computa-tional linguistics..bing liu and ian lane.
2016. attention-based recur-rent neural network models for joint intent detectionand slot ﬁlling.
in interspeech 2016, pages 685–689..pengfei liu, shuaichen chang, xuanjing huang, jiantang, and jackie chi kit cheung.
2019a.
contextu-alized non-local neural networks for sequence learn-ing.
proceedings of the aaai conference on artiﬁ-cial intelligence, 33(01):6762–6769..yijin liu, fandong meng, jinchao zhang, jie zhou,yufeng chen, and jinan xu.
2019b.
cm-net: anovel collaborative memory network for spokenin proceedings of thelanguage understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1051–1060, hong kong,china.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019c.
roberta: a robustly optimized bert pretrain-arxiv preprint arxiv:1907.11692,ing approach.
abs/1907.11692..jinjie ni, tom young, vlad pandelea, fuzhao xue,vinay adiga, and erik cambria.
2021. recent ad-vances in deep learning based dialogue systems: asystematic survey..libo qin, wanxiang che, yangming li, haoyang wen,and ting liu.
2019. a stack-propagation frame-work with token-level intent detection for spokenin proceedings of thelanguage understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2078–2087, hong kong,china.
association for computational linguistics..libo qin, wanxiang che, minheng ni, yangming li,and ting liu.
2021a.
knowing where to leverage:context-aware graph convolutional network with anadaptive fusion layer for contextual spoken languageieee/acm transactions on audio,understanding.
speech, and language processing, 29:1280–1289..libo qin, zhouyang li, wanxiang che, minheng ni,and ting liu.
2020a.
co-gat: a co-interactive graphattention network for joint dialog act recognition andsentiment classiﬁcation..libo qin, tailu liu, wanxiang che, bingbing kang,a co-sendong zhao, and ting liu.
2021b.
interactive transformer for joint slot ﬁlling and intentdetection..libo qin, tianbao xie, wanxiang che, and ting liu.
2021c.
a survey on spoken language understanding:recent advances and new frontiers..libo qin, xiao xu, wanxiang che, and ting liu.
2020b.
agif: an adaptive graph-interactive frame-work for joint multiple intent detection and slot ﬁll-in findings of the association for computa-ing.
tional linguistics: emnlp 2020, pages 1807–1816,online.
association for computational linguistics..dechuang teng, libo qin, wanxiang che, sendongzhao, and ting liu.
2021. injecting word informa-tion with multi-level word adapter for chinese spo-ken language understanding..gokhan tur and renato de mori.
2011. spoken lan-guage understanding: systems for extracting seman-tic information from speech.
john wiley & sons..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, ł ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998–6008.
cur-ran associates, inc..petar veliˇckovi´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
international2018. graph attention networks.
conference on learning representations.
acceptedas poster..yu wang, yilin shen, and hongxia jin.
2018. a bi-model based rnn semantic frame parsing model forin proceedings ofintent detection and slot ﬁlling.
the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 309–314, new orleans, louisiana.
as-sociation for computational linguistics..di wu, liang ding, fan lu, and jian xie.
2020.slotreﬁne: a fast non-autoregressive model forin proceed-joint intent detection and slot ﬁlling.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1932–1937, online.
association for computationallinguistics..congying xia, chenwei zhang, xiaohui yan,yi chang, and philip yu.
2018. zero-shot userintent detection via capsule neural networks.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages3090–3099, brussels, belgium.
association forcomputational linguistics..puyang xu and ruhi sarikaya.
2013. convolutionalneural network based triangular crf for joint intentdetection and slot ﬁlling.
in 2013 ieee workshop.
187on automatic speech recognition and understand-ing..michihiro yasunaga, rui zhang, kshitijh meelu,ayush pareek, krishnan srinivasan, and dragomirradev.
2017. graph-based neural multi-documentsummarization.
in proceedings of the 21st confer-ence on computational natural language learning(conll 2017), pages 452–462, vancouver, canada.
association for computational linguistics..steve young, milica gaˇsi´c, blaise thomson, and ja-son d williams.
2013. pomdp-based statistical spo-ken dialog systems: a review.
proceedings of theieee, 101(5):1160–1179..chenwei zhang, yaliang li, nan du, wei fan, andphilip yu.
2019. joint slot ﬁlling and intent detec-tion via capsule neural networks.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 5259–5267, florence,italy.
association for computational linguistics..xiaodong zhang and houfeng wang.
2016. a jointmodel of intent determination and slot ﬁlling forspoken language understanding.
in proceedings ofthe twenty-fifth international joint conference onartiﬁcial intelligence, ijcai’16, page 2993–2999.
aaai press..188