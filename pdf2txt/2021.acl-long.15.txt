gl-gin: fast and accurate non-autoregressive model for joint multipleintent detection and slot filling.
libo qin, fuxuan wei, tianbao xie, xiao xu, wanxiang cheâˆ—, ting liuresearch center for social computing and information retrievalharbin institute of technology, china{lbqin,fuxuanwei,tianbaoxie,xxu,car,tliu}@ir.hit.edu.cn.
abstract.
multi-intent slu can handle multiple intentsin an utterance, which has attracted increas-ing attention.
however, the state-of-the-artjoint models heavily rely on autoregressive ap-proaches, resulting in two issues: slow infer-ence speed and information leakage.
in thispaper, we explore a non-autoregressive modelfor joint multiple intent detection and slot ï¬ll-ing, achieving more fast and accurate.
specif-ically, we propose a global-locally graphinteraction network (gl-gin) where a localslot-aware graph interaction layer is proposedto model slot dependency for alleviating unco-ordinated slots problem while a global intent-slot graph interaction layer is introduced tomodel the interaction between multiple intentsand all slots in the utterance.
experimen-tal results on two public datasets show thatour framework achieves state-of-the-art perfor-mance while being 11.5 times faster..1.introduction.
spoken language understanding (slu) (younget al., 2013) is a critical component in spokendialog systems, which aims to understand userâ€™squeries.
it typically includes two sub-tasks: intentdetection and slot ï¬lling (tur and de mori, 2011).
since intents and slots are closely tied, dominantsingle-intent slu systems in the literature (gooet al., 2018; li et al., 2018; liu et al., 2019b; eet al., 2019; qin et al., 2019; teng et al., 2021;qin et al., 2021b,c) adopt joint models to considerthe correlation between the two tasks, which haveobtained remarkable success..multi-intent slu means that the system can han-dle an utterance containing multiple intents, whichis shown to be more practical in the real-world sce-nario, attracting increasing attention.
to this end,.
âˆ—corresponding author..figure 1: (a) autoregressive model generates outputsword by word from left-to-right direction.
the graycolor denotes the unseen information when model de-(b) non-autoregressivecodes for the word denver.
model can produce outputs in parallel.
an denotesairport name..xu and sarikaya (2013) and kim et al.
(2017) be-gin to explore the multi-intent slu.
however, theirmodels only consider the multiple intent detectionwhile ignoring slot ï¬lling task.
recently, gangad-haraiah and narayanaswamy (2019) make the ï¬rstattempt to propose a multi-task framework to jointmodel the multiple intent detection and slot ï¬lling.
qin et al.
(2020b) further propose an adaptive inter-action framework (agif) to achieve ï¬ne-grainedmulti-intent information integration for slot ï¬lling,obtaining state-of-the-art performance..though achieving the promising performance,the existing multi-intent slu joint models heav-ily rely on an autoregressive fashion, as shown infigure 1(a), leading to two issues:.
â€¢ slow inference speed.
the autoregressivemodels make the generation of slot outputsmust be done through the left-to-right pass,which cannot achieve parallelizable, leadingto slow inference speed..proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages178â€“188august1â€“6,2021.Â©2021associationforcomputationallinguistics178(a)howfarisdenverairport(b)howfarisdenverairportooob-ani-anooob-ani-anâ€¢ information leakage.
autoregressive modelspredict each word slot conditioned on the pre-viously generated slot information (from left-to-right), resulting in leaking the bidirectionalcontext information..in this paper, we explore a non-autoregressiveframework for joint multiple intent detection andslot ï¬lling, with the goal of accelerating inferencespeed while achieving high accuracy, which isshown in figure 1(b).
to this end, we proposea global-locally graph-interaction network (gl-gin) where the core module is a proposed lo-cal slot-aware graph layer and global intent-slotinteraction layer, which achieves to generate in-tents and slots sequence simultaneously and non-autoregressively.
in gl-gin, a local slot-awaregraph interaction layer where each slot hiddenstates connect with each other is proposed to ex-plicitly model slot dependency, in order to alle-viate uncoordinated slot problem (e.g., b-singerfollowed by i-song) (wu et al., 2020) due to thenon-autoregressive fashion.
a global intent-slotgraph interaction layer is further introduced to per-form sentence-level intent-slot interaction.
un-like the prior works that only consider the token-level intent-slot interaction, the global graph is con-structed of all tokens with multiple intents, achiev-ing to generate slots sequence in parallel and speedup the decoding process..experimental results on two public datasetsmixsnips (coucke et al., 2018) and mix-atis (hemphill et al., 1990) show that our frame-work not only obtains state-of-the-art performancebut also enables decoding in parallel.
in addition,we explore the pre-trained model (i.e., roberta (liuet al., 2019c)) in our framework..in summary, the contributions of this work canbe concluded as follows: (1) to the best of ourknowledge, we make the ï¬rst attempt to explorea non-autoregressive approach for joint multipleintent detection and slot ï¬lling; (2) we proposea global-locally graph-interaction network, wherethe local graph is used to handle uncoordinatedslots problem while a global graph is introducedto model sequence-level intent-slot interaction; (3)experiment results on two benchmarks show thatour framework not only achieves the state-of-the-art performance but also considerably speeds upthe slot decoding (up to Ã—11.5); (4) finally, weexplore the pre-trained model in our framework.
with the pre-trained model, our model reaches a.new state-of-the-art level..for.
this pa-reproducibility, our code forper is publicly available at https://github.com/yizhen20133868/gl-gin..2 problem deï¬nition.
multiple intent detection given input se-quence x = (x1, .
.
.
, xn), multiple intent detec-tion can be deï¬ned as a multi-label classiï¬ca-tion task that outputs a sequence intent label oi= (oim), where m is the number of intentsin given utterance and n is the length of utterance..1, .
.
.
, oi.
slot filling slot ï¬lling can be seen as a sequencelabeling task that maps the input utterance x into aslot output sequence os = (os.
1 , .
.
.
, osn)..3 approach.
as shown in figure 2(a), we describe the pro-posed framework, which consists of a shared self-attentive encoder (Â§3.1), a token-level intent de-tection decoder (Â§3.2) and a global-local graph-interaction graph decoder for slot ï¬lling (Â§3.3).
both intent detection and slot ï¬lling are optimizedsimultaneously via a joint learning scheme..3.1 self-attentive encoder.
following qin et al.
(2019), we utilize a self-attentive encoder with bilstm and self-attentionmechanism to obtain the shared utterance repre-sentation, which can incorporate temporal featureswithin word orders and contextual information..bilstm thebidirectional lstm (bil-stm) (hochreiter and schmidhuber, 1997) havebeen successfully applied to sequence labelingtasks (li et al., 2020, 2021).
we adopt bilstmto read the input sequence {x1, x2, .
.
.
, xn}forwardly and backwardly to produce context-sensitive hidden states h = {h1, h2, .
.
.
, hn}, byrepeatedly applying the hi = bilstm (Ï†emb(xi),hiâˆ’1, hi+1), where Ï†emb is embedding function..self-attention following vaswani et al.
(2017),we map the matrix of input vectors x âˆˆ rnÃ—d (drepresents the mapped dimension) to queries q,keys k and values v matrices by using differentlinear projections.
then, the self-attention outputc âˆˆ rnÃ—d is a weighted sum of values:.
c = softmax.
(1).
(cid:19).
(cid:18) qk(cid:62)âˆšdk.
v ..179figure 2: the overï¬‚ow of model architecture (a) and global-locally graph interaction layer (b)..we concatenate the output of bilstm and self-.
attention as the ï¬nal encoding representation:.
e = h || c,.
(2).
where e = {e1, .
.
.
, en} âˆˆ rnÃ—2d and || is con-catenation operation..3.2 token-level intent detection decoder.
inspired by qin et al.
(2019), we perform a token-level multi-label multi-intent detection, where wepredict multiple intents on each token and the sen-tence results are obtained by voting for all tokens.
speciï¬cally, we ï¬rst feed the contextual encodinge into an intent-aware bilstm to enhance itstask-speciï¬c representations:.
t = bilstm (cid:0)et, hihi.
tâˆ’1, hi.
t+1.
(cid:1) ..(3).
then, hi.
t is used for intent detection, using:.
it = Ïƒ(w i (leakyrelu(w h hi.
t +bh))+bi ), (4).
where it denotes the intent results at the t-th word;Ïƒ denotes the sigmoid activation function; w h andw i are the trainable matrix parameters.
finally, the sentence intent results oi.
k can be ob-.
tained by:.
oi = {oi.
k|(.
1[i(i,k) > 0.5]) > n/2},.
(5).
n(cid:88).
i=1.
where i(i,k) represents the classiï¬cation result oftoken i for oik..we predict the label as the utterance intent whenit gets more than half positive predictions in all ntokens.
for example, if i1 = {0.9, 0.8, 0.7, 0.1},i2 = {0.8, 0.2, 0.7, 0.4}, i3 = {0.9, 0.3, 0.2, 0.3},from three tokens, we get {3, 2, 1, 0} positive votes(> 0.5) for four intents respectively.
thus the indexwhere more than half of the votes ( > 3/2 ) were3, we predict intents oi =obtained was oi{oi3}..1 and oi.
1, oi.
3.3 slot filling decoder.
one main advantage of our framework is the pro-posed global-locally graph interaction network forslot ï¬lling, which is a non-autoregressive paradigm,achieving the slot ï¬lling decoding in parallel.
in thefollowing, we ï¬rst describe the slot-aware lstm(Â§3.3.1) to obtain the slot-aware representations,and then show how to apply the global-locallygraph interaction layer (Â§3.3.2) for decoding..3.3.1 slot-aware lstmwe utilize a bilstm to produce the slot-awarehidden representation s = (s1, .
.
.
, sn).
at eachdecoding step t, the decoder state st calculating by:.
st = bilstm (cid:0)i t || et, stâˆ’1, st+1.
(cid:1) ,.
(6).
where et denotes the aligned encoder hiddenstate and i t denotes the predicted intent informa-tion..180self-attentive encoderğ‘¥3ğ‘¥4ğ‘¥2ğ‘’1ğ‘’2ğ‘’3ğ‘’4ğ¼1ğ¼2ğ¼3ğ¼4ğ¼1ğ¼2ğ¼3ğ¼4ğ‘ 1ğ‘ 2ğ‘ 3ğ‘ 4ğ‘’1ğ‘’2ğ‘’3ğ‘’4token-level intent decoderğ‘œ1ğ¼ğ‘œ2ğ¼global-locally graph interaction layerğ‘œ1ğ‘ ğ‘œ2ğ‘ ğ‘œ3ğ‘ ğ‘œ4ğ‘ intent detectionslot fillingğ‘¥1ğ‘ 1ğ‘ 2ğ‘ 3ğ‘ 4local slot-aware interaction layerğ‘ 1ğ‘ 2ğ‘ 3ğ‘ 4ğ‘œ1ğ¼ğ‘œ2ğ¼global intent-slot interaction layerğ‘œ1ğ‘ ğ‘œ2ğ‘ ğ‘œ3ğ‘ ğ‘œ4ğ‘ (b) global-locally graph interaction layer(a) model frameworkbilstmbilstm3.3.2 global-locally graph interaction layerthe proposed global-locally graph interaction layerconsists of two main components: one is a localslot-aware graph interaction network to model de-pendency across slots and another is the proposedglobal intent-slot graph interaction network to con-sider the interaction between intents and slots..in this section, we ï¬rst describe the vanilla graphattention network.
then, we illustrate the localslot-aware and global intent-slot graph interactionnetwork, respectively..vanilla graph attention network a graph at-tention network (gat) (veliË‡ckoviÂ´c et al., 2018)is a variant of graph neural network, which fusesthe graph-structured information and node featureswithin the model.
its masked self-attention layersallow a node to attend to neighborhood features andlearn different attention weights, which can auto-matically determine the importance and relevancebetween the current node with its neighborhood..in particular, for a given graph with n nodes,one-layer gat take the initial node features Ëœh ={Ëœh1, .
.
.
, Ëœhn }, Ëœhn âˆˆ rf as input, aiming atproducing more abstract representation, Ëœh=(cid:48)(cid:48){Ëœh1, .
.
.
, Ëœhn âˆˆ rf (cid:48), as its output.
the at-tention mechanism of a typical gat can be sum-marized as below:.
(cid:48)n }, Ëœh.
(cid:48).
Ëœh.
(cid:48)i.k=1 Ïƒ(cid:0) (cid:80).
Ëœhj= ||kijw kÎ±khËœhi(cid:107)w hexp(leakyrelu(a(cid:62)[w hexp (leakyrelu.
(cid:1),Ëœhj ]))(cid:48)ËœhËœhi(cid:107)w hj ].
a(cid:62)[w h.jâˆˆni.
(cid:16).
(7).
,(8).
(cid:17)).
Î±ij =.
(cid:80).
j(cid:48)âˆˆni.
where w h âˆˆ rf (cid:48)Ã—f and a âˆˆ r2f (cid:48)are the train-able weight matrix; ni denotes the neighbors ofnode i (including i); Î±ij is the normalized atten-tion coefï¬cients and Ïƒ represents the nonlinearityactivation function; k is the number of heads..local slot-aware graph interaction layergiven slot decode hidden representations s = (s1,.
.
.
, sn), we construct a local slot-aware graphwhere each slot hidden node connects to otherslots.
this allows the model to achieve to modelthe dependency across slots, alleviating the unco-ordinated slots problem.
speciï¬cally, we constructthe graph g = (v, e) in the following way,.
vertices we deï¬ne the v as the vertices set.
each word slot is represented as a vertex.
eachvertex is initialized with the corresponding slothidden representation.
thus, the ï¬rst layer statesvector for all nodes is s1 = s = (s1, .
.
.
, sn)..edges since we aim to model dependencyacross slots, we construct a slot-aware graph inter-action layer so that the dependency relationship canbe propagated from neighbor nodes to the currentnode.
each slot can connect other slots with a win-dow size.
for node si, only {siâˆ’m, .
.
.
, si+m}will be connected where m is a hyper-parameterdenotes the size of sliding window that controls thelength of utilizing utterance context..information aggregation the aggregation.
process at l-th layer can be deï¬ned as:.
i = Ïƒ(cid:0) (cid:88)sl+1.
Î±ijw lslj.
(cid:1),.
(9).
jâˆˆni.
where ni is a set of vertices that denotes the con-nected slots..after stacking l layer, we obtain the con-textual slot-aware local hidden features sl+1={sl+11., .
.
.
, sl+1.
}.
n.global slot-intent graph interaction layerto achieve sentence-level intent-slot interaction,we construct a global slot-intent interaction graphwhere all predicted multiple intents and sequenceslots are connected, achieving to output slot se-quences in parallel.
speciï¬cally, we construct thegraph g = (v, e) in the following way,.
vertices as we model the interaction betweenintent and slot token, we have n + m number ofnodes in the graph where n is the sequence lengthand m is the number of intent labels predicted bythe intent decoder.
the input of slot token fea-ture is g[s,1] = sl+1 ={sl+1} whichis produced by slot-aware local interaction graphnetwork while the input intent feature is an embed-ding g[i,1] = {Ï†emb(oim)} whereÏ†emb is a trainable embedding matrix.
the ï¬rstlayer states vector for slot and intent nodes is g1= {g[i,1] , g[s,1] } = {Ï†emb(oim),sl+1}1.
1), .
.
.
, Ï†emb(oi.
1), .
.
.
, Ï†emb(oi.
, .
.
.
, sl+1.
, .
.
.
, sl+1.
n.n.1.edges there are three types of connections in.
this graph network..â€¢ intent-slot connection: since slots and intentsare highly tied, we construct the intent-slotconnection to model the interaction betweenthe two tasks.
speciï¬cally, each slot connectsall predicted multiple intents to automaticallycapture relevant intent information..181â€¢ slot-slot connection: we construct the slot-slot connection where each slot node connectsother slots with the window size to furthermodel the slot dependency and incorporatethe bidirectional contextual information..â€¢ intent-intent connection: following qin et al.
(2020b), we connect all the intent nodes toeach other to model the relationship betweeneach intent, since all of them express the sameutteranceâ€™s intent..information aggregation the aggregationprocess of the global gat layer can be formulatedas:.
g[s,l+1]i.
= Ïƒ(.
Î±ijw gg[s,l].
j +.
Î±ijw gg[i,l].
),.
j.
(cid:88).
jâˆˆgs.
(cid:88).
jâˆˆgi.
(10)where gs and gi are vertices sets which denotesthe connected slots and intents, respectively..3.3.3 slot predictionafter l layersâ€™ propagation, we obtain the ï¬nal slotrepresentation g[s,l+1] for slot prediction..4 experiments.
4.1 datasets.
we conduct experiments on two publicly avail-able multi-intent datasets.1 one is the mix-atis (hemphill et al., 1990; qin et al., 2020b),which includes 13,162 utterances for training, 756utterances for validation and 828 utterances for test-ing.
another is mixsnips (coucke et al., 2018;qin et al., 2020b), with 39,776, 2,198, 2,199 utter-ances for training, validation and testing..4.2 experimental settings.
the dimensionality of the embedding is 128 and64 on atis and snips, respectively.
the dimen-sionality of the lstm hidden units is 256. thebatch size is 16. the number of the multi head is 4and 8 on mixatis and mixsnips dataset, respec-tively.
all layer number of graph attention networkis set to 2. we use adam (kingma and ba, 2015)to optimize the parameters in our model.
for allthe experiments, we select the model which worksthe best on the dev set and then evaluate it on thetest set.
all experiments are conducted at geforcertx 2080ti and titan xp..(cid:16).
w sg[s,l+1]t.(cid:17).
,.
ys.
t = softmaxt = arg max(ysos.
t ),.
(11).
(12).
4.3 baselines.
where w s is a trainable parameter and ospredicted slot if the t-th token in an utterance..t is the.
3.4.joint training.
following goo et al.
(2018), we adopt a joint train-ing model to consider the two tasks and updateparameters by joint optimizing.
the intent detec-tion objective is:.
ce(Ë†y, y) = Ë†y log (y) + (1 âˆ’ Ë†y) log (1 âˆ’ y) , (13)ni(cid:88).
n(cid:88).
ce(Ë†y(j,i)i., y(j,i)i.)
.
(14).
l1 (cid:44) âˆ’.
i=1.
j=1.
similarly, the slot ï¬lling task objective is:.
l2 (cid:44) âˆ’.
n(cid:88).
ns(cid:88).
i=1.
j=1.
Ë†y(j,s)i.log.
(cid:16).
y(j,s)i.
(cid:17).
,.
(15).
where ni is the number of single intent labels andns is the number of slot labels..the ï¬nal joint objective is formulated as:.
l = Î±l1 + Î²l2,.
(16).
where Î± and Î² are hyper-parameters..we compare our model with the following best(1) attention birnn.
liu andbaselines:lane (2016) propose an alignment-based rnnfor joint slot ï¬lling and intent detection;(2)slot-gated atten.
goo et al.
(2018) pro-pose a slot-gated joint model, explicitly consideringthe correlation between slot ï¬lling and intent detec-tion; (3) bi-model.
wang et al.
(2018) proposethe bi-model to model the bi-directional betweenthe intent detection and slot ï¬lling; (4) sf-idnetwork.
e et al.
(2019) proposes the sf-id net-work to establish a direct connection between thetwo tasks; (5) stack-propagation.
qin et al.
(2019) adopt a stack-propagation framework toexplicitly incorporate intent detection for guidingslot ï¬lling; (6) joint multiple id-sf.
gan-gadharaiah and narayanaswamy (2019) propose amulti-task framework with slot-gated mechanismfor multiple intent detection and slot ï¬lling; (7)agif qin et al.
(2020b) proposes an adaptive in-teraction network to achieve the ï¬ne-grained multi-.
1we adopt the cleaned verison that removes the repeatedsentences in original dataset, which is available at https://github.com/looperxx/agif..182model.
mixatisoverall(acc) slot(f1).
mixsnipsintent(acc) overall(acc) slot(f1).
attention birnn (liu and lane, 2016)slot-gated (goo et al., 2018)bi-model (wang et al., 2018)sf-id (e et al., 2019)stack-propagation (qin et al., 2019)joint multiple id-sf (gangadharaiah and narayanaswamy, 2019)agif (qin et al., 2020b)gl-gin.
39.135.534.434.940.136.140.843.5*.
86.487.783.987.487.884.686.788.3*.
74.663.970.366.272.173.474.476.3*.
59.555.463.459.972.962.974.275.4*.
89.487.990.790.694.290.694.294.9*.
intent(acc)95.494.695.695.096.095.195.195.6.table 1: main results.
the numbers with * indicate that the improvement of our framework over all baselines isstatistically signiï¬cant with p < 0.05 under t-test..modelstack-propagationjoint multiple id-sfagifgl-gin.
decode latency(s) speedup.
34.545.348.54.2.
8.2Ã—10.8Ã—11.5Ã—1.0Ã—.
table 2: speed comparison.
speedup is based on theratio of the time taken by the slot decoding part of dif-ferent models to run an epoch on the mixatis datasetwith batch size set to 32..intent information integration, achieving state-of-the-art performance..4.4 main results.
following goo et al.
(2018) and qin et al.
(2020b),we evaluate the performance of slot ï¬lling usingf1 score, intent prediction using accuracy, thesentence-level semantic frame parsing using over-all accuracy.
overall accuracy measures the ratioof sentences for which both intent and slot are pre-dicted correctly in a sentence..table 1 shows the results, we have the followingobservations: (1) on slot ï¬lling task, our frame-work outperforms the best baseline agif in f1scores on two datasets, which indicates the pro-posed local slot-aware graph successfully modelsthe dependency across slots, so that the slot ï¬llingperformance can be improved.
(2) more impor-tantly, compared with the agif, our frameworkachieves +2.7% and 1.2% improvements for mix-atis and mixsnips on overall accuracy, respec-tively.
we attribute it to the fact that our proposedglobal intent-slot interaction graph can better cap-ture the correlation between intents and slots, im-proving the slu performance..4.5 analysis.
4.5.1 speedupone of the core contributions of our frameworkis that the decoding process of slot ï¬lling canbe signiï¬cantly accelerated with the proposed.
non-autoregressive mechanism.
we evaluate thespeed by running the model on the mixatistest data in an epoch, ï¬xing the batch size to32. the comparison results are shown in ta-ble 2. we observe that our model achieves theÃ—8.2, Ã—10.8 and Ã—11.5 speedup compared withsota models stack-propagation, jointmultiple id-sf and agif.
this is becausethat their model utilizes an autoregressive architec-ture that only performs slot ï¬lling word by word,while our non-autoregressive framework can con-duct slot ï¬lling decoding in parallel.
in addition,itâ€™s worth noting that as the batch size gets larger,gl-gin can achieve better acceleration whereour model could achieve Ã—17.2 speedup comparedwith agif when batch size is 64..4.5.2 effectiveness of the local slot-aware.
graph interaction layer.
we study the effectiveness of the local slot-awareinteraction graph layer with the following ablation.
we remove the local graph interaction layer anddirectly feed the output of the slot lstm to theglobal intent-slot graph interaction layer.
we referit to w/o local gal in tabel 3. we can clearlyobserve that the slot f1 drops by 1.5% and 1.2% onmixatis and mixsnips datasets.
we attribute thisto the fact that local slot-aware gal can capturethe slot dependency for each token, which helpsto alleviate the slot uncoordinated problems.
aqualitative analysis can be founded at section 4.5.6..4.5.3 effectiveness of global slot-intentgraph interaction layer.
in order to verify the effectiveness of slot-intentglobal interaction graph layer, we remove theglobal interaction layer and utilizes the output oflocal slot-aware gal module for slot ï¬lling.
it isnamed as w/o global intent-slot gal in table 3.we can observe that the slot f1 drops by 0.9%,1.3%, which demonstrates that intent-slot graph in-.
183model.
w/o local slot-aware galw/o global intent-slot gal.
+ more parametersw/o global-locally galgl-gin.
overall(acc)41.140.941.940.543.5.mixatisslot(f1)86.887.487.786.388.3.intent(acc)74.075.575.075.276.3.overall(acc)71.471.773.070.275.4.mixsnipsslot(f1)93.793.693.892.994.9.intent(acc)95.295.595.595.095.6.table 3: ablation experiment..figure 3: visualization.
we use the green color to indi-cate the attention value..teraction layer can capture the correlation betweenmultiple intents, which is beneï¬cial for the seman-tic performance of slu system..following qin et al.
(2020b), we replace multi-ple lstm layers (2-layers) as the proposed global-locally graph layer to verify that the proposedglobal-locally graph interaction layer rather thanthe added parameters works.
table 3 (more pa-rameters) shows the results.
we observe that ourmodel outperforms more parameters by 1.6% and2.4% overall accuracy in two datasets, which showsthat the improvements come from the proposedglobal-locally graph interaction layer rather thanthe involved parameters..4.5.4 effectiveness of the global-locallygraph interaction layer.
instead of using the whole global-locally graph in-teraction layer for slot ï¬lling, we directly leveragethe output of slot-aware lstm to predict each to-ken slot to verify the effect of the global-locallygraph interaction layer.
we name the experimentas w/o global-locally gal in tabel 3. from theresults, we can observe that the absence of globalgat module leads to 3.0% and 5.2% overall accu-racy drops on two datasets.
this indicates that the.
figure 4:roberta..overall accuracy performances with.
global-locally graph interaction layer encouragesour model to leverage slot dependency and intentinformation, which can improve slu performance..4.5.5 visualizationto better understand how global-local graph inter-action layer affects and contributes to the ï¬nal re-sult, we visualize the attention value of the globalintent-slot gal.
as is shown in figure 3, we visu-alize the dependence of the word â€œ6â€ on contextand intent information.
we can clearly observe thattoken â€œ6â€ obtains information from all contextualtokens.
the information from â€œand 10â€ helpsto predict the slot, where the prior autoregressivemodels cannot be achieved due to the generationword by word from left to right..4.5.6 qualitative analysiswe conduct qualitative analysis by providing a casestudy that consists of two sequence slots whichare generated from agif and our model.
fromtable 4, for the word â€œ6â€, agif predicts its slotlabel as â€œoâ€ incorrectly.
this is because that agifonly models its left information, which makes ithard to predict â€œ6â€ is a time slot.
in contrast,our model predicts the slot label correctly.
weattribute this to the fact that our proposed globalintent-slot interaction layer can model bidirectionalcontextual information.
in addition, our frameworkpredicts the word slot â€œamâ€ correctly while agifpredicts it incorrectly (i-airport name follows b-depart time), indicating that the proposed local slot-.
184whatairlinesofffromlovefieldbetween6and10amonjunesixthoooob-ariport_namei-ariport_nameob-start_timeob-end_timei-end_timeob-month_nameb-day_numberintent : atis_airlinemixatismixsnips02040608040.874.243.575.450.080.753.682.6agifgl-ginagif + robertagl-gin + robertatexts.
what.
airlines.
off.
from.
agif.
gl-gin.
o.o.o.o.o.o.o.o.loveb-fromlocairport nameb-fromlocairport name.
ï¬eldi-fromlocairport namei-fromlocairport name.
between.
o.o.
6.o.b-depart timestart time.
and.
o.o.
10b-depart timeend timeb-depart timeend time.
am.
i-tolocairport namei-depart timeend time.
on.
o.o.juneb-depart datemonth nameb-depart datemonth name.
sixthb-depart dateday numberb-depart dateday number.
table 4: case study.
predicted slots sequence about utterance â€œwhat airlines off from love ï¬eld between 6 and10 am on june sixthâ€.
aware graph layer has successfully captured the slotdependency..4.5.7 effect of pre-trained model.
following qin et al.
(2019), we explore the pre-trained model in our framework.
we replace theself-attentive encoder by roberta (liu et al., 2019c)with the ï¬ne-tuning approach.
we keep other com-ponents identical to our framework and follow qinet al.
(2019) to consider the ï¬rst subword label if aword is broken into multiple subwords..figure 4 gives the result comparison of agif,gl-gin and two models with roberta on twodatasets.
we have two interesting observations.
first, the roberta-based model remarkablywell on two datasets.
we attribute this to the factthat pre-trained models can provide rich semanticfeatures, which can help slu.
second, gl-gin +roberta outperforms agif+roberta on bothdatasets and reaches a new state-of-the-art perfor-mance, which further veriï¬es the effectiveness ofour proposed framework..5 related work.
slot filling and intent detection recently,joint models (zhang and wang, 2016; hakkani-tÂ¨ur et al., 2016; goo et al., 2018; li et al., 2018;xia et al., 2018; e et al., 2019; liu et al., 2019b;qin et al., 2019; zhang et al., 2019; wu et al., 2020;qin et al., 2021b; ni et al., 2021) are proposed toconsider the strong correlation between intent de-tection and slot ï¬lling have obtained remarkablesuccess.
compared with their work, we focus onjointly modeling multiple intent detection and slotï¬lling while they only consider the single-intentscenario..more recently, multiple intent detection can han-dle utterances with multiple intents, which has at-tracted increasing attention.
to the end, xu andsarikaya (2013) and kim et al.
(2017) begin to ex-plore the multiple intent detection.
gangadharaiahand narayanaswamy (2019) ï¬rst apply a multi-taskframework with a slot-gate mechanism to jointlymodel the multiple intent detection and slot ï¬ll-.
ing.
qin et al.
(2020b) propose an adaptive interac-tion network to achieve the ï¬ne-grained multipleintent information integration for token-level slotï¬lling, achieving the state-of-the-art performance.
their models adopt the autoregressive architecturefor joint multiple intent detection and slot ï¬lling.
in contrast, we propose a non-autoregressive ap-proach, achieving parallel decoding.
to the bestof our knowledge, we are the ï¬rst to explore anon-autoregressive architecture for multiple intentdetection and slot ï¬lling..graph neural network for nlp graph neuralnetworks that operate directly on graph structuresto model the structural information, which has beenapplied successfully in various nlp tasks.
linmeiet al.
(2019) and huang and carley (2019) exploregraph attention network (gat) (veliË‡ckoviÂ´c et al.,2018) for classiï¬cation task to incorporate the de-pendency parser information.
cetoli et al.
(2017)and liu et al.
(2019a) apply graph neural networkto model the non-local contextual information forsequence labeling tasks.
yasunaga et al.
(2017)and feng et al.
(2020a) successfully apply a graphnetwork to model the discourse information forthe summarization generation task, which achievedpromising performance.
graph structure are suc-cessfully applied for dialogue direction (feng et al.,2020b; fu et al., 2020; qin et al., 2020a, 2021a).
in our work, we apply a global-locally graph inter-action network to model the slot dependency andinteraction between the multiple intents and slots..6 conclusion.
in this paper, we investigated a non-autoregressivemodel for joint multiple intent detection and slotï¬lling.
to this end, we proposed a global-locallygraph interaction network where the uncoordinated-slots problem can be addressed with the proposedlocal slot-aware graph while the interaction be-tween intents and slots can be modeled by theproposed global intent-slot graph.
experimentalresults on two datasets show that our frameworkachieves state-of-the-art performance with Ã—11.5times faster than the prior work..185acknowledgements.
this work was supported by the national key r&dprogram of china via grant 2020aaa0106501 andthe national natural science foundation of china(nsfc) via grant 61976072 and 61772153. thiswork was also supported by the zhejiang labâ€™sinternational talent fund for young professionals..references.
alberto cetoli, stefano bragaglia, andrew oâ€™harney,and marc sloan.
2017. graph convolutional net-in proceed-works for named entity recognition.
ings of the 16th international workshop on tree-banks and linguistic theories, pages 37â€“45, prague,czech republic..alice coucke, alaa saade, adrien ball, thÂ´eodorebluche, alexandre caulier, david leroy, clÂ´ementdoumouro, thibault gisselbrecht, francesco calta-girone, thibaut lavril, et al.
2018. snips voice plat-form: an embedded spoken language understandingsystem for private-by-design voice interfaces.
arxivpreprint arxiv:1805.10190..haihong e, peiqing niu, zhongfu chen, and meinasong.
2019. a novel bi-directional interrelatedmodel for joint intent detection and slot ï¬lling.
inproceedings oftheassociation for computational linguistics, pages5467â€“5471, florence, italy.
association for compu-tational linguistics..the 57th annual meeting of.
xiachong feng, xiaocheng feng, bing qin, xinweigeng, and ting liu.
2020a.
dialogue discourse-aware graph convolutional networks for abstractivemeeting summarization..xiachong feng, xiaocheng feng, bing qin, and tingliu.
2020b.
incorporating commonsense knowl-edge into abstractive dialogue summarization viaarxiv preprintheterogeneous graph networks.
arxiv:2010.10044..qiankun fu, yue zhang, jiangming liu, and meis-han zhang.
2020. drts parsing with structure-aware encoding and decoding.
in proceedings of the58th annual meeting of the association for compu-tational linguistics, pages 6818â€“6828, online.
as-sociation for computational linguistics..rashmi.
gangadharaiahandbalakrishnanjoint multiple intentnarayanaswamy.
2019.detection and slot labeling for goal-oriented dialog.
in proceedings ofthenorth american chapter ofthe association forcomputational linguistics: human language tech-nologies, volume 1 (long and short papers), pages564â€“569, minneapolis, minnesota.
association forcomputational linguistics..the 2019 conference of.
chih-wen goo, guang gao, yun-kai hsu, chih-lihuo, tsung-chieh chen, keng-wei hsu, and yun-nung chen.
2018. slot-gated modeling for jointslot ï¬lling and intent prediction.
in proceedings ofthe 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 753â€“757, new orleans, louisiana.
as-sociation for computational linguistics..dilek hakkani-tÂ¨ur, gokhan tur, asli celikyilmaz,yun-nung vivian chen, jianfeng gao, li deng, andye-yi wang.
2016. multi-domain joint semanticframe parsing using bi-directional rnn-lstm.
in pro-ceedings of the 17th annual meeting of the interna-tional speech communication association (inter-speech 2016).
isca..charles t. hemphill, john j. godfrey, and george r.doddington.
1990. the atis spoken language sys-tems pilot corpus.
in speech and natural language:proceedings of a workshop held at hidden valley,pennsylvania, june 24-27,1990..sepp hochreiter and jÂ¨urgen schmidhuber.
1997.long short-term memory.
neural computation,9(8):1735â€“1780..binxuan huang and kathleen carley.
2019. syntax-level sentiment classiï¬cation withaware aspectin proceedings of thegraph attention networks.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 5469â€“5477, hong kong,china.
association for computational linguistics..byeongchang kim, seonghan ryu, and gary geunbaelee.
2017. two-stage multi-intent detection for spo-ken language understanding.
multimedia tools andapplications, 76(9):11377â€“11390..diederik p. kingma and jimmy ba.
2015. adam: ain 3rd inter-method for stochastic optimization.
national conference on learning representations,iclr 2015, san diego, ca, usa, may 7-9, 2015,conference track proceedings..changliang li, liang li, and ji qi.
2018. a self-attentive model with gate mechanism for spoken lan-in proceedings of the 2018guage understanding.
conference on empirical methods in natural lan-guage processing, pages 3824â€“3833, brussels, bel-gium.
association for computational linguistics..yangming li, han li, kaisheng yao, and xiaolong li.
2020. handling rare entities for neural sequencelabeling.
in proceedings of the 58th annual meet-ing of the association for computational linguistics,pages 6441â€“6451, online.
association for computa-tional linguistics..yangming li,.
lemao liu, and shuming shi.
2021.empirical analysis of unlabeled entity problem innamed entity recognition.
in international confer-ence on learning representations..186hu linmei, tianchi yang, chuan shi, houye ji, andxiaoli li.
2019. heterogeneous graph attention net-works for semi-supervised short text classiï¬cation.
in proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing (emnlp-ijcnlp), pages 4821â€“4830, hong kong, china.
association for computa-tional linguistics..bing liu and ian lane.
2016. attention-based recur-rent neural network models for joint intent detectionand slot ï¬lling.
in interspeech 2016, pages 685â€“689..pengfei liu, shuaichen chang, xuanjing huang, jiantang, and jackie chi kit cheung.
2019a.
contextu-alized non-local neural networks for sequence learn-ing.
proceedings of the aaai conference on artiï¬-cial intelligence, 33(01):6762â€“6769..yijin liu, fandong meng, jinchao zhang, jie zhou,yufeng chen, and jinan xu.
2019b.
cm-net: anovel collaborative memory network for spokenin proceedings of thelanguage understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 1051â€“1060, hong kong,china.
association for computational linguistics..yinhan liu, myle ott, naman goyal, jingfei du, man-dar joshi, danqi chen, omer levy, mike lewis,luke zettlemoyer, and veselin stoyanov.
2019c.
roberta: a robustly optimized bert pretrain-arxiv preprint arxiv:1907.11692,ing approach.
abs/1907.11692..jinjie ni, tom young, vlad pandelea, fuzhao xue,vinay adiga, and erik cambria.
2021. recent ad-vances in deep learning based dialogue systems: asystematic survey..libo qin, wanxiang che, yangming li, haoyang wen,and ting liu.
2019. a stack-propagation frame-work with token-level intent detection for spokenin proceedings of thelanguage understanding.
2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 2078â€“2087, hong kong,china.
association for computational linguistics..libo qin, wanxiang che, minheng ni, yangming li,and ting liu.
2021a.
knowing where to leverage:context-aware graph convolutional network with anadaptive fusion layer for contextual spoken languageieee/acm transactions on audio,understanding.
speech, and language processing, 29:1280â€“1289..libo qin, zhouyang li, wanxiang che, minheng ni,and ting liu.
2020a.
co-gat: a co-interactive graphattention network for joint dialog act recognition andsentiment classiï¬cation..libo qin, tailu liu, wanxiang che, bingbing kang,a co-sendong zhao, and ting liu.
2021b.
interactive transformer for joint slot ï¬lling and intentdetection..libo qin, tianbao xie, wanxiang che, and ting liu.
2021c.
a survey on spoken language understanding:recent advances and new frontiers..libo qin, xiao xu, wanxiang che, and ting liu.
2020b.
agif: an adaptive graph-interactive frame-work for joint multiple intent detection and slot ï¬ll-in findings of the association for computa-ing.
tional linguistics: emnlp 2020, pages 1807â€“1816,online.
association for computational linguistics..dechuang teng, libo qin, wanxiang che, sendongzhao, and ting liu.
2021. injecting word informa-tion with multi-level word adapter for chinese spo-ken language understanding..gokhan tur and renato de mori.
2011. spoken lan-guage understanding: systems for extracting seman-tic information from speech.
john wiley & sons..ashish vaswani, noam shazeer, niki parmar, jakobuszkoreit, llion jones, aidan n gomez, Å‚ ukaszkaiser, and illia polosukhin.
2017. attention is allyou need.
in advances in neural information pro-cessing systems, volume 30, pages 5998â€“6008.
cur-ran associates, inc..petar veliË‡ckoviÂ´c, guillem cucurull, arantxa casanova,adriana romero, pietro li`o, and yoshua bengio.
international2018. graph attention networks.
conference on learning representations.
acceptedas poster..yu wang, yilin shen, and hongxia jin.
2018. a bi-model based rnn semantic frame parsing model forin proceedings ofintent detection and slot ï¬lling.
the 2018 conference of the north american chap-ter of the association for computational linguistics:human language technologies, volume 2 (short pa-pers), pages 309â€“314, new orleans, louisiana.
as-sociation for computational linguistics..di wu, liang ding, fan lu, and jian xie.
2020.slotreï¬ne: a fast non-autoregressive model forin proceed-joint intent detection and slot ï¬lling.
ings of the 2020 conference on empirical methodsin natural language processing (emnlp), pages1932â€“1937, online.
association for computationallinguistics..congying xia, chenwei zhang, xiaohui yan,yi chang, and philip yu.
2018. zero-shot userintent detection via capsule neural networks.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, pages3090â€“3099, brussels, belgium.
association forcomputational linguistics..puyang xu and ruhi sarikaya.
2013. convolutionalneural network based triangular crf for joint intentdetection and slot ï¬lling.
in 2013 ieee workshop.
187on automatic speech recognition and understand-ing..michihiro yasunaga, rui zhang, kshitijh meelu,ayush pareek, krishnan srinivasan, and dragomirradev.
2017. graph-based neural multi-documentsummarization.
in proceedings of the 21st confer-ence on computational natural language learning(conll 2017), pages 452â€“462, vancouver, canada.
association for computational linguistics..steve young, milica gaË‡siÂ´c, blaise thomson, and ja-son d williams.
2013. pomdp-based statistical spo-ken dialog systems: a review.
proceedings of theieee, 101(5):1160â€“1179..chenwei zhang, yaliang li, nan du, wei fan, andphilip yu.
2019. joint slot ï¬lling and intent detec-tion via capsule neural networks.
in proceedings ofthe 57th annual meeting of the association for com-putational linguistics, pages 5259â€“5267, florence,italy.
association for computational linguistics..xiaodong zhang and houfeng wang.
2016. a jointmodel of intent determination and slot ï¬lling forspoken language understanding.
in proceedings ofthe twenty-fifth international joint conference onartiï¬cial intelligence, ijcaiâ€™16, page 2993â€“2999.
aaai press..188