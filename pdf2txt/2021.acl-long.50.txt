understanding and countering stereotypes: a computational approachto the stereotype content model.
kathleen c. fraser, isar nejadgholi, and svetlana kiritchenkonational research council canadaottawa, canada{kathleen.fraser,isar.nejadgholi,svetlana.kiritchenko}@nrc-cnrc.gc.ca.
abstract.
stereotypical language expresses widely-heldbeliefs about different social categories.
manystereotypes are overtly negative, while othersmay appear positive on the surface, but stilllead to negative consequences.
in this work,we present a computational approach to inter-preting stereotypes in text through the stereo-type content model (scm), a comprehensivecausal theory from social psychology.
thescm proposes that stereotypes can be under-stood along two primary dimensions: warmthand competence.
we present a method fordeﬁning warmth and competence axes in se-mantic embedding space, and show that thefour quadrants deﬁned by this subspace accu-rately represent the warmth and competenceconcepts, according to annotated lexicons.
wethen apply our computational scm model totextual stereotype data and show that it com-pares favourably with survey-based studies inthe psychological literature.
furthermore, weexplore various strategies to counter stereo-typical beliefs with anti-stereotypes.
isknown that countering stereotypes with anti-stereotypical examples is one of the most ef-fective ways to reduce biased thinking, yet theproblem of generating anti-stereotypes has notbeen previously studied.
thus, a better under-standing of how to generate realistic and effec-tive anti-stereotypes can contribute to address-ing pressing societal concerns of stereotyping,prejudice, and discrimination..it.
1.introduction.
stereotypes are widely-held beliefs about traits orcharacteristics of groups of people.
while we tendto think of stereotypes as expressing negative viewsof groups, some stereotypes actually express posi-tive views (e.g.
all women are nurturing).
however,even so-called ‘positive’ stereotypes can be harm-ful, as they dictate particular roles that individuals.
are expected to fulﬁll, regardless of whether theyhave the ability or desire to do so (kay et al., 2013).
the existence of stereotypes in our society – in-cluding in entertainment, the workplace, public dis-course, and even legal policy – can lead to a numberof harms.
timmer (2011) organizes these harmsinto three main categories: (1) misrecognition ef-fects: harms caused by denying members of partic-ular groups an equal place in society, diminishingtheir human dignity, or other forms of marginaliza-tion.
(2) distribution effects: harms resulting fromunfair allocation of resources, either by increas-ing the burden placed on a group, or decreasinga group’s access to a beneﬁt.
(3) psychologicaleffects: the distress and unhappiness caused by anawareness and internalization of the stereotypedbiases against one’s identity group.
additionally,the internalization of these negative stereotypes canlead to anxiety and underachievement.
to reducethese harms and promote a more egalitarian so-ciety, we must identify and counter stereotypicallanguage when it occurs..evidence from the psychological literature sug-gests that one of the most effective methods forreducing stereotypical thinking is through expo-sure to counter-stereotypes, or anti-stereotypes.
finnegan et al.
(2015) showed participants stereo-typical and anti-stereotypical images of highlysocially-gendered professions (e.g., a surgeon isstereotypically male, and a nurse is stereotypi-cally female; the genders were reversed in theanti-stereotypical images), and then measured theirgender bias in a judgement task.
exposure to anti-stereotypical images signiﬁcantly reduced genderbias on the task.
blair et al.
(2001) used a mentalimagery task and reported that participants in theanti-stereotypical condition subsequently showedsigniﬁcantly weaker effects on the implicit associ-ation test (iat).
dasgupta and greenwald (2001)showed a similar effect by exposing participants to.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages600–616august1–6,2021.©2021associationforcomputationallinguistics600anti-stereotypical exemplars (e.g.
admired blackcelebrities, and disliked white individuals).
whenlai et al.
(2014) compared 17 interventions aimedat reducing stereotypical thinking, methods involv-ing anti-stereotypes were most successful overall.
thus, creating technology that enables users toidentify stereotypical language when it occurs, andthen counter it with anti-stereotypes, could help toreduce biased thinking.
however, the idea of whatconstitutes an anti-stereotype remains ill-deﬁned.
is an anti-stereotype simply the semantic oppositeof a stereotype?
or can anything that is not a stereo-type serve as an anti-stereotype?
if two groups arestereotyped similarly, do they have an identical anti-stereotype?
can an anti-stereotype actually reﬂectan equally harmful view of a target group (e.g.
thecold-hearted career woman as an anti-stereotypeto the nurturing housewife)?.
here, we begin to untangle some of these ques-tions using the stereoset dataset (nadeem et al.,2020).
we begin by analyzing the stereotypes ex-pressed in this dataset.
one widely-accepted modelof stereotypes, prejudice, and inter-group relation-ships from social psychology is the “stereotypecontent model” or scm (fiske et al., 2002).
thescm proposes two fundamental and universal di-mensions of social stereotypes: warmth and com-petence.
by deﬁning the warm–cold, competent–incompetent axes in the semantic embedding space,we are able to cluster and interpret stereotypes withrespect to those axes.
we can then examine the as-sociated anti-stereotypes and their relation to boththe stereotyped description and the target group.
thus, our contributions are as follows:.
• to develop a computational method for automati-cally mapping textual information to the warmth–competence plane as proposed in the stereotypecontent model..• to validate the computational method and opti-mize the choice of word embedding model usinga lexicon of words known to be associated withpositive and negative warmth and competence..• to compare the stereotypes in stereoset withthose reported in the survey-based social psy-chology literature..• to analyze human-generated anti-stereotypes asa ﬁrst step towards automatically generating anti-stereotypes, as a method of countering stereo-types in text with constructive, alternative per-spectives..2 related work.
we provide more details on the stereotype con-tent model and its practical implications, and thenbrieﬂy review the nlp research on computationalanalysis of stereotypical and abusive content.
stereotype content model: stereotypes, and therelated concepts of prejudice and discrimination,have been extensively studied by psychologists forover a century (dovidio et al., 2010).
concep-tual frameworks have emerged which emphasizetwo principle dimensions of social cognition.
thestereotype content model (scm) refers to thesetwo dimensions as warmth (encompassing sociabil-ity and morality) and competence (encompassingability and agency) (fiske et al., 2002).
when form-ing a cognitive representation of a social group toanticipate probable behaviors and traits, people arepredominantly concerned with the others’ intent—are they friends or foes?
this intent is captured inthe primary dimension of warmth.
the competencedimension determines if the others are capable toenact that intent.
a key ﬁnding of the scm hasbeen that, in contrast to previous views of prejudiceas a uniformly negative attitude towards a group,many stereotypes are actually ambivalent; that is,they are high on one dimension and low on theother..further, the scm proposes a comprehensivecausal theory, linking stereotypes with social struc-ture, emotions, and discrimination (fiske, 2015).
according to this theory, stereotypes are affected bya perceived social structure of interdependence (co-operation versus competition), corresponding to thewarmth dimension, and status (prestige and power),determining competence.
stereotypes then predictemotional response or prejudices.
for example,groups perceived as unfriendly and incompetent(e.g., homeless people, drug addicts) evoke disgustand contempt, groups allegedly high in warmth butlow in competence (e.g., older people, people withdisabilities) evoke pity, and groups perceived ascold and capable (e.g., rich people, businesspeople)elicit envy..finally, the emotions regulate the actions (activeor passive help or harm).
thus, low warmth–lowcompetence groups often elicit active harm andpassive neglect, whereas low warmth–high compe-tence groups may include envied out-groups whoare subjects of passive help in peace times butcan become targets of attack during social unrest(cuddy et al., 2007)..601the scm has been supported by extensive quan-titative and qualitative analyses across cultures andtime (fiske, 2015; fiske and durante, 2016).
toour knowledge, the current work presents the ﬁrstcomputational model of the scm..stereotypes in language models: an active lineof nlp research is dedicated to quantifying andmitigating stereotypical biases in language models.
early works focused on gender and racial bias andrevealed stereotypical associations and commonprejudices present in word embeddings through as-sociation tests (bolukbasi et al., 2016; caliskanet al., 2017; manzini et al., 2019).
to discoverstereotypical associations in contextualized wordembeddings, may et al.
(2019) and kurita et al.
(2019) used pre-deﬁned sentence templates.
sim-ilarly, bartl et al.
(2020) built a template-basedcorpus to quantify bias in neural language models,whereas nadeem et al.
(2020) and nangia et al.
(2020) used crowd-sourced stereotypical and anti-stereotypical sentences for the same purpose.
incontrast to these studies, while we do use wordembeddings to represent our data, we aim to iden-tify and categorize stereotypical views expressed intext, not in word embeddings or language models..abusive content detection: stereotyping, explic-itly or implicitly expressed in communication, canhave a detrimental effect on its target, and can beconsidered a form of abusive behavior.
onlineabuse, including hate speech, cyber-bullying, on-line harassment, and other types of offensive andtoxic behaviors, has been a focus of substantialresearch effort in the nlp community in the pastdecade (e.g.
see surveys by schmidt and wiegand(2017); fortuna and nunes (2018); vidgen et al.
(2019)).
most of the successes in identifying abu-sive content have been reported on text containingexplicitly obscene expressions; only recently haswork started on identifying more subtly expressedabuse, such as stereotyping and micro-aggressions(breitfeller et al., 2019).
for example, fersini et al.
(2018) and chiril et al.
(2020) examined gender-related stereotypes as a sub-category of sexist lan-guage, and price et al.
(2020) annotated ‘unfairgeneralizations’ as one attribute of unhealthy on-line conversation.
sap et al.
(2020) employed large-scale language models in an attempt to automati-cally reconstruct stereotypes implicitly expressedin abusive social media posts.
their work showedthat while the current models can accurately predictwhether the online post is offensive or not, they.
struggle to effectively reproduce human-writtenstatements for implied meaning..counter-narrative: counter-narrative (or coun-terspeech) has been shown to be effective inconfronting online abuse (benesch et al., 2016).
counter-narrative is a non-aggressive response toabusive content that aims to deconstruct and dele-gitimize the harmful beliefs and misinformationwith thoughtful reasoning and fact-bound argu-ments.
several datasets of counter narratives, spon-taneously written by regular users or carefullycrafted by experts, have been collected and ana-lyzed to discover common intervention strategies(mathew et al., 2018; chung et al., 2019).
pre-liminary experiments in automatic generation ofcounter-narrative demonstrated the inadequacy ofcurrent large-scale language models for generatingeffective responses and the need for a human-in-the-loop approach (qian et al., 2019; tekiro˘gluet al., 2020).
countering stereotypes through expo-sure to anti-stereotypical exemplars is based on asimilar idea of deconstructing harmful beliefs withcounter-facts..3 data and methods.
we develop our computational scm using la-belled data from nicolas et al.
(2020) and thepolar framework for interpretable word embed-dings (mathew et al., 2020), and then apply it tostereotype and anti-stereotype data from stereoset(nadeem et al., 2020).
details are provided in thefollowing sections..3.1 warmth-competence lexicons.
to construct and validate our model, we makeuse of the supplementary data from nicolas et al.
(2020) (https://osf.io/yx45f/).
they providea list of english seed words, captured from the psy-chological literature, associated with the warmthand competence dimensions; speciﬁcally, associ-ated with sociability and morality (warmth), andability and agency (competence).
they then usewordnet to generate an extended lexicon of en-glish words either positively or negatively asso-ciated with aspects of warmth and competence.
some examples from the seed data and extendedlexicon are given in table 1..3.2 stereoset.
human-generated.
foranti-stereotype data, we use the publicly-available.
stereotype.
and.
602dimensionwarmth.
componentsociability.
competence agency.
morality.
ability.
signposnegposnegposnegposneg.
seed word examplesfriendly, warm, pleasantcold, repellent, dislikedtrustworthy, sincere, honestdishonest, selﬁsh, unfairconﬁdent, assertive, securefearful, lazy, inactivesmart, intelligent, ablestupid, ignorant, incapable.
nseed3432404935313329.extended lexicon examplesamusing, brother, fundetached, grim, surlydonor, justice, modestcheat, dreadful, henchmanbravery, decisive, stubbornfollow, minion, quitteranalytic, ﬂuency, thoroughforgetful, silly, unﬁt.
nextended4824234601750444265579301.table 1: examples of words from the training data (seed words) and validation data (extended lexicon), for eachof the components comprising the warmth and competence dimensions..portion of the stereoset dataset (nadeem et al.,this english-language dataset was2020).
constructed to testlanguage model bias, andpart of the data is kept hidden as the test setfor a leaderboard on language model fairness(https://stereoset.mit.edu/).
instead, we usethe development set, which contains stereotypedata for 79 target groups across four broaddemographic domains: gender, race or nationality,profession, and religion..in stereoset, there are two experimental condi-tions: intra-sentence and inter-sentence.
here, wefocus on the intra-sentence data only.
the datawas collected from crowd-workers as follows (seenadeem et al.
(2020) for more detail): given atarget group label, the annotator is asked to gener-ate a stereotypical word associated with that group,as well as an anti-stereotypical word and an un-related word.
they then construct a context sen-tence containing the target group label, and a blankwhich can be ﬁlled with the stereotypical or anti-stereotypical word.
for example, if the target groupwas women, the annotator might come up withemotional and rational as the stereotype and anti-stereotype words respectively, and then constructa sentence like women are known for being overly(cid:104)blank(cid:105).
for our current analysis, we consideronly the stereotype and anti-stereotype words, anddiscard the context sentence.
we also exclude anytargets that do not directly refer to groups of peo-ple (e.g., we discard norway but keep norwegian).
this results in 58 target groups with an average of25 stereotype and anti-stereotype word pairs each..3.3 constructing warmth and competence.
dimensions.
we consider several possible representations forthe words in our dataset, including glove (pen-nington et al., 2014), word2vec (mikolov et al.,.
2013), and fasttext (mikolov et al., 2018).1 in allcases, the key question is how to project the higher-dimensional word embedding onto the warmth–competence plane..rather than using an unsupervised approachsuch as pca, we choose the polar frameworkintroduced by mathew et al.
(2020).
this frame-work seeks to improve the interpretability of wordembeddings by leveraging the concept of ‘semanticdifferentials,’ a psychological rating scale whichcontrasts bipolar adjectives, e.g.
hot–cold, or good–bad.
given word embeddings that deﬁne thesepolar opposites for a set of concepts, all other wordembeddings in the space are projected onto the‘polar embedding space,’ where each dimension isclearly associated with a concept..for our purposes,.
the polar opposites arewarmth–coldness and competence–incompetence,as deﬁned by the sets of seed words from nicolaset al.
(2020).
to reduce the dimensionality of thespace to 2d, we average the word vectors for allseed words associated with each dimension andpolarity.
that is, to deﬁne the warmth direction, wetake the mean of all words in the seed dictionarywhich are positively associated with warmth.
givenvector deﬁnitions for warmth, coldness, compe-tence, and incompetence, we can then use a simplematrix transformation to project any word embed-ding to the 2d subspace deﬁned by these basis vec-tors (mathematical details are given in appendixa)..4 model validation.
we ﬁrst evaluate the model’s ability to accuratelyplace individual words from the lexicons along the.
1we consider here only noncontextual word embeddings,in line with mathew et al.
(2020).
because the polar frame-work is based on linear algebraic computations, it is not im-mediately obvious whether it will extend directly to contextu-alized embeddings, which are notably anisotropic (ethayarajh,2019)..603embedding modelfasttext-crawl-subword-300fasttext-wiki-news-subword-300word2vec-googlenews-300glove-twitter-200glove-wiki-gigaword-300.
warmth comp.
85.884.872.674.277.9.
85.084.980.272.878.7.table 2: accuracy of the word embedding models onpredicting the correct labels for the extended lexicon..warmth and competence dimensions.
we then ex-plore whether we can reproduce ﬁndings describingwhere certain target groups are typically located inthe warmth–competence plane, based on the previ-ous survey-based social psychology literature..4.1 comparison with existing lexicons.
as described above, we use the extended lexiconfrom nicolas et al.
(2020) to validate our model.
we remove any words in the lexicon which appearin the seed dictionary and any words which do nothave representations in all the pretrained embed-ding models, leaving a total of 3,159 words forvalidation..in the extended lexicon, the words are annotatedwith either +1 or -1 to indicate a positive or nega-tive association with the given dimension.
we passthe same words through our system, and observewhether the model labels the word as being pos-itively or negatively associated with the relevantdimension.
our evaluation metric is accuracy; i.e.
the proportion of times our system agrees with thelexicon.
note that all words are associated with ei-ther warmth or competence, and therefore we canonly evaluate one dimension at a time..we evaluate a number of pre-trained word em-beddings in the gensim library ( ˇreh˚uˇrek and sojka,2010), with the results given in table 2. the fast-text embeddings generally outperform the otherembeddings on this task, with the 2m word modeltrained on 600b tokens in the common crawl lead-ing to the highest accuracy.
therefore, we use thisembedding model in the analysis that follows..4.2 comparison with psychological surveys.
we now address the question of whether our model,in conjunction with the stereoset data, is able to re-produce ﬁndings from psychological surveys.
weproject stereotypes from the stereoset data onto thewarmth–competence space for the 24 target groupsthat meet both of the following criteria: (1) theyare included in the publicly available portion ofthe stereoset data, and (2) they have been previ-.
ously studied for stereotyping in the psychologicalliterature.
based on the ﬁndings from psychologi-cal surveys, we expect these target groups will bemapped to the following quadrants:2.
• warm-competent:.
(‘healthcare professions’)2010), researcher (’professor’) (eckes, 2002)..nurse,.
psychologist(brambilla et al.,.
• warm-incompetent: grandfather (‘elderly’),mommy, mother (‘traditional women’) (cuddyet al., 2008), schoolboy, schoolgirl (‘children’)(fiske, 2018)..• cold-competent: male, gentleman (‘man’)(glick et al., 2004), japanese (lee and fiske,2006), commander (cuddy et al., 2011), man-ager, entrepreneur (fiske, 2010), mathematician,physicist, chemist, engineer (‘scientist’) (loshet al., 2008), software developer (‘technical ex-pert’) (fiske, 2018)..• cold-incompetent: african, ethiopian, gha-nian, eritrean, hispanic (lee and fiske, 2006),arab (fiske et al., 2006)..to locate each target group on the plane, we gen-erate word embeddings for each of the stereotypewords associated with the target group, ﬁnd themean, and project the mean to the polar embed-ding space.
as we aim to identify commonly-heldstereotypes, we use a simple cosine distance ﬁl-ter to remove outliers, heuristically deﬁned hereas any words which are greater than a distance of0.6 from the mean of the set of words.
we alsoremove words which directly reference a demo-graphic group (e.g., black, white) as these wordsare vulnerable to racial bias in the embeddingmodel and complicate the interpretation.
a com-plete list of the words in each stereotype clustercan be found in the appendix b..figure 1 conﬁrms many of the ﬁndings pre-dicted by the literature.
most (67%) of the stereo-types lie in the predicted quadrant, including grand-father and schoolgirl in the paternalistic warm–incompetent quadrant; nurse and psychologist inthe admired warm–competent quadrant, managerand male in the envied cold–competent quadrant,and african and hispanic in the cold–cold quad-rant..other stereotypes lie in locations which seem.
2note that these research ﬁndings simply report stereotypi-cal beliefs which are prevalent in north american society; wein no way aim to perpetuate, conﬁrm, or promote these views..6045 stereotypes and anti-stereotypes.
the scm presents a concise theory to explainstereotypes and resulting prejudiced behaviour;however,it does not generate any predictionsabout anti-stereotypes.
here, we explore the anti-stereotypes in stereoset within the context of thescm, ﬁrst at the level of individual annotators, andthen at the level of target groups (combining wordsfrom multiple annotators).
we then discuss how wemight use information about warmth and compe-tence to generate anti-stereotypes with the speciﬁcgoal of reducing biased thinking..5.1 anti-stereotypes in stereoset.
in this section, we investigate the question: whatdo human annotators come up with when asked toproduce an anti-stereotype?
one possibility is thatthey simply produce the antonym of their stereo-type word.
to test this hypothesis, for all 58 groupsand each pair of stereotype and anti-stereotypewords, we obtain a list of antonyms for the stereo-type word using the python library pydictionary.
we additionally search all the synonyms for thestereotype word, and add all of their antonyms tothe list of antonyms as well.
then, if the lemmaof the anti-stereotype matches the lemma of any ofthe retrieved antonyms, we consider it a match..however, as seen in table 3, the strategy ofsimply producing a direct antonym is only used23% of the time.
we consider four other broadpossibilities: (1) that the annotator generates ananti-stereotype word that lies in the opposite quad-rant from the stereotype word, e.g., if the stereo-type word is low-competence, low-warmth (lc-lw), then the anti-stereotype word should be high-competence, high-warmth (hc-hw); (2) that theannotator chooses a word with the opposite warmthpolarity (i.e.
ﬂips warmth), while keeping the com-petence polarity the same; (3) that the annotatorchooses a word with the opposite competence po-larity (i.e.
ﬂips competence), while keeping thewarmth polarity the same; (4) that the annotatorchooses a word that lies in the same quadrant as thestereotype word.
we report the proportion of timesthat each strategy is observed; ﬁrst overall, then foreach quadrant individually.
the choice of whetherto modify warmth or competence might also de-pend on which of those dimensions is most salientfor a given word, and so we consider separatelywords for which the absolute value of competenceis greater than the absolute value of warmth, and.
figure 1: validating known stereotypes..reasonable on examination of the underlying data.
for example, while men are typically stereotypedas being competent yet cold in the psychologicalliterature, the speciﬁc keyword gentlemen evokes acertain subset of men (described with words suchas polite, respectful, and considerate), which rankshigher on the warmth dimension than the targetword male (example words: dominant, aggressive).
we also observe that while children have gener-ally been labelled as warm–incompetent in previ-ous work (fiske, 2018), this dataset distinguishesbetween male and female schoolchildren, and, asexpected based on studies of gender, schoolboysare ranked as lower warmth than schoolgirls.
thewords used to describe schoolboys include refer-ences to the ‘naughty’ schoolboy stereotype, whilethe words describing schoolgirls focus on their in-nocence and naivety..it is also notable that arab, predicted to lie in thecold–incompetent quadrant, is here mapped to thecold–competent quadrant instead.
we hypothesizethat this is due to the use of stereotype words likedangerous and violent, which suggest a certain de-gree of agency and the ability to carry out goals.
incontrast, the target group african as well as thoseassociated with african countries are stereotypedas poor and uneducated, and thus low on the com-petence dimension..in general, we conclude that in most cases thecomputational approach is successful in mappingstereotyped groups onto the predicted areas ofthe warmth–competence plane, and that the caseswhich diverge from ﬁndings in the previous litera-ture do appear to be reasonable, based on an exami-nation of the text data.
having validated the model,we can now apply it to the rest of the stereotypedata in stereoset, as well as the anti-stereotypes..605strategy.
direct antonymopposite quadrantflip warmthflip competencesame quadrant.
overalln = 89523.429.620.616.79.6.hc-hw lc-hw lc-lw hc-lw |c| > |w|n = 428n = 176n = 19227.227.826.028.130.226.129.512.314.622.813.124.09.63.45.2.n = 34415.038.316.416.713.5.n = 18332.615.526.512.712.7.
|w| > |c|n = 46719.231.229.810.19.6.table 3: the percentage of times each of the hypothesized strategies of anti-stereotype generation is used forstereotypes, overall and in each quadrant.
quadrants are labelled as hc-hw, lc-hw, lc-lw, and hc-lw, wherehc/lc denotes high/low competence, and hw/lw denotes high/low warmth.
we also consider separately thosestereotypes which have competence as the most salient dimension (|c| > |w|), and those which have warmth asthe most salient dimension (|w| > |c|)..vice versa.
the results are given in table 3..while no single strategy dominates, we canmake a few observations.
in general, it is morelikely that people select an anti-stereotype which isnot a direct antonym, but which lies in the oppositequadrant in the warmth-competence plane.
flip-ping only one axis is less frequent, although we seein the last two columns that it is more likely thatthe competence will be ﬂipped when competenceis the salient dimension for a word, and similarlyfor warmth.
finally, choosing another word in thesame quadrant is rare, but more common in theambivalent quadrants..while it is not possible to know what thoughtprocess the annotators followed to produce anti-stereotypes, we consider the following possibleexplanation.
just as we have here conceptualizeda stereotype as being deﬁned not by a single word,but by a set of words, perhaps each annotator alsomentally represents each stereotype as a set ofwords or ideas.
then, the anti-stereotype word theyproduce sometimes reﬂects a different componentof their mental image than the initial stereotypeword.
to give a concrete example from the data,one annotator stereotypes hispanic people as ag-gressive, but then comes up with hygienic as ananti-stereotype, suggesting that unhygienic is alsopart of their multi-dimensional stereotype concept.
the choice of whether to select a direct antonym,or whether to negate some other component of thestereotype, may depend on the availability of a fa-miliar lexical antonym, the context sentence, or anynumber of other factors.
in short, it appears that theprocess by which human annotators generate pairsof stereotype and anti-stereotype words is complexand not easily predicted by the scm..we then examine how these pairs of stereotypeand anti-stereotype words combine to produce anoverall anti-stereotype for the target group in ques-tion.
taking the same approach as in the previous.
targetafricanhispanicmothernursecommandermoverfootball player.
stereotype antonym anti-stereotypepoorpoorcaringcaringstrongstrongdumb.
richhardworkinghatefulrudestupidweakweak.
richrichuncaringuncaringweakweaksmart.
table 4: examples comparing stereotypes with their di-rect antonym and the anti-stereotype from stereoset..section, we average the anti-stereotype word vec-tors to determine the location of the anti-stereotypein the warmth–competence plane.
for each targetgroup, we then select the word closest to the meanfor both the stereotype and anti-stereotype clusters.
similarly to when we look at individual word pairs,in 22% of cases, the mean of the anti-stereotypeis the direct antonym of the stereotype mean.
inthe other cases, 45% of the anti-stereotype meanslie in the opposite quadrant to the stereotypes, in16% of cases the warmth polarity is ﬂipped, in 10%of cases the competence polarity is ﬂipped, and inonly 7% cases (4 target groups), the anti-stereotypelies in the same quadrant as the stereotype..in table 4, we offer a few examples of caseswhere the anti-stereotype means agree and disagreewith the direct antonyms of the stereotypes.
asin the pairwise analysis, in many cases the anti-stereotypes appear to be emphasizing a supposedcharacteristic of the target group which is not cap-tured by the stereotype mean; for example, theanti-stereotype for ‘dumb football player’ is notsmart, but weak – demonstrating that strength isalso part of the football player stereotype.
this isalso seen clearly in the fact that two target groupswith the same stereotype mean are not always as-signed the same anti-stereotype: for example, bothafricans and hispanics are stereotyped as poor,but africans are assigned the straightforward anti-stereotype rich, while hispanics are assigned hard-.
606working (perhaps implying that their poverty is dueto laziness rather than circumstance)..the general conclusion from these experimentsis that stereotypes are indeed multi-dimensional,and the anti-stereotypes must be, also.
hence itis not enough to generate an anti-stereotype sim-ply by taking the antonym of the most representa-tive word, nor is it sufﬁcient to identify the mostsalient dimension of the stereotype and only adjustthat.
when generating anti-stereotypes, annotators(individually, in the pairwise comparison, and onaverage) tend to invert both the warmth and com-petence dimensions, taking into account multiplestereotypical characteristics of the target group..5.2 anti-stereotypes for social good.
the anti-stereotypes in stereoset were generatedwith the goal of evaluating language model bias.
ultimately, our goal is quite different: to reducebiased thinking in humans.
in particular, we wantto generate anti-stereotypes that emphasize the pos-itive aspects of the target groups..as underscored by cuddy et al.
(2008), manystereotypes are ambivalent: they take the form ’xbut y’.
women are nurturing but weak, scientistsare intelligent but anti-social.
when we simplytake the antonym of the mean, we focus on the sin-gle most-representative word; i.e., the x. however,in the examples we can observe that it’s actuallywhat comes after the “but ...” that is the prob-lem.
therefore, in generating anti-stereotypes forthese ambivalent stereotypes, we hypothesize thata better approach is not to take the antonym ofthe primary stereotype (i.e., women are uncaring,scientists are stupid), but rather to challenge thesecondary stereotype (women can be nurturing andstrong, scientists can be intelligent and social)..as a ﬁrst.
step towards generating anti-stereotypes for such ambivalent stereotypes, wepropose the following approach: ﬁrst identify themost positive aspect of the stereotype (e.g., if thestereotype mean lies in the incompetent–warmquadrant, the word expressing the highest warmth),then identify the most negative aspect of the stereo-type in the other dimension (in this example, theword expressing the lowest competence).
then thestereotype can be phrased in the x but y construc-tion, where x is the positive aspect and y is the neg-ative aspect.3 to generate a positive anti-stereotype.
3a similar method can be used for warm–competent andcold–incompetent stereotypes, although if all words are pos-itive, an anti-stereotype may not be needed, and if all words.
which challenges stereotypical thinking while notpromoting a negative view of the target group, takethe antonym only of the negative aspect.
someexamples are given in table 5. a formal evaluationof these anti-stereotypes would involve carryingout a controlled psychological study in which theanti-stereotypes were embedded in an implicit biastask to see which formulations are most effectiveat reducing bias; for now, we simply present themas a possible way forward..as shown in the table, taking into account theambivalent aspects of stereotypes can result inmore realistic anti-stereotypes than either takingthe mean of the crowd-sourced anti-stereotypes,or simply generating the semantic opposite of thestereotype.
for example, the group grandfatheris mostly stereotyped as old, and then counter-intuitively anti-stereotyped as young.
it is moreuseful in terms of countering ageism to combat theunderlying stereotype that grandfathers are feeblerather than denying that they are often old.
sim-ilarly, it does not seem helpful to oppose biasedthinking by insisting that entrepreneurs can be lazy,engineers and developers can be dumb, and moth-ers can be uncaring.
rather, by countering only thenegative dimension of ambivalent stereotypes, wecan create realistic and positive anti-stereotypes..6 discussion and future work.
despite their prevalence, stereotypes can be hard torecognize and understand.
we tend to think aboutother people on a group level rather than on anindividual level because social categorization, al-though harmful, simpliﬁes the world for us andleads to cognitive ease.
however, psychologistshave shown that we can overcome such ways ofthinking with exposure to information that con-in this exploratory study,tradicts those biases.
we present a computational implementation of thestereotype content model to better understand andcounter stereotypes in text..a computational scm-based framework can bea promising tool for large-scale analysis of stereo-types, by mapping a disparate set of stereotypes tothe 2d semantic space of warmth and competence.
we described here our ﬁrst steps towards devel-oping and validating this framework, on a highlyconstrained dataset: in stereoset, the annotatorswere explicitly instructed to produce stereotypicalideas, the target groups and stereotypical words.
are negative, then an antonym may be more appropriate..607targetgrandfatherentrepreneurengineermommysoftware developer.
stereotypeoldsavvysmartlovingnerdy.
anti-stereotypeyounglazydumbuncaringdumb.
x but y constructionkind but feebleinventive but ruthlessintelligent but egotisticalcaring but childishintelligent but unhealthy.
x and ¬y anti-stereotypekind and stronginventive and compassionateintelligent and altruisticcaring and matureintelligent and healthy.
table 5: examples of positive anti-stereotypes created by identifying positive and negative words along each ofthe dimensions, and taking the antonym only of the negative words..are clearly speciﬁed, and every stereotype has anassociated anti-stereotype generated by the sameannotator.
in future work, this method should befurther assessed by using different datasets and sce-narios.
for example, it may be possible to collectstereotypical descriptions of target groups ‘in thewild’ by searching large corpora from social mediaor other sources.
we plan to extend this frameworkto analyze stereotypes on the sentence-level andconsider the larger context of the conversations.
working with real social media texts will introducea number of challenges, but will offer the possi-bility of exploring a wider range of marginalizedgroups and cultural viewpoints..related to this, we reiterate that only a por-tion of the stereoset dataset is publicly available.
therefore, the data does not include the full set ofcommon stereotypical beliefs for social groups fre-quently targeted by stereotyping.
in fact, some ofthe most affected communities (e.g., north ameri-can indigenous people, lgbtq+ community, peo-ple with disabilities, etc.)
are completely missingfrom the dataset.
in this work, we use this datasetonly for illustration purposes and preliminary eval-uation of the proposed methodology.
future workshould examine data from a wide variety of subpop-ulations differing in language, ethnicity, culturalbackground, geographical location, and other char-acteristics..from a technical perspective, with largerdatasets it will be possible to implement a clus-ter analysis within each target group to reveal thedifferent ways in which a given group can be stereo-typed.
a classiﬁcation model may additionallyimprove the accuracy of the warmth–competencecategorization, although we have chosen the po-lar framework here for its interpretability andease of visualization..we also examined how we might leverage the de-veloped computational model to challenge stereo-typical thinking.
our analysis did not reveal a sim-ple, intuitive explanation for the anti-stereotypesproduced by the annotators, suggesting they ex-.
ploited additional information beyond what wasstated in the stereotype word.
this extra infor-mation may not be captured in a single pair ofstereotype–anti-stereotype words, but by consider-ing sets of words, we can better characterize stereo-types as multi-dimensional and often ambivalentconcepts, consistent with the established view inpsychology.
this also allows us to suggest anti-stereotypes which maintain positive beliefs about agroup, while challenging negative beliefs..we propose that this methodology may poten-tially contribute to technology that assists humanprofessionals, such as psychologists, educators, hu-man rights activists, etc., in identifying, tracking,analyzing, and countering stereotypes at large scalein various communication channels.
there area number of ways in which counter-stereotypescan be introduced to users (e.g., through mentionsof counter-stereotypical members of the group orfacts countering the common beliefs) with the goalof priming users to look at others as individualsand not as stereotypical group representatives.
anscm-based approach can provide the psycholog-ical basis and the interpretation of automatic sug-gestions to users..since our methodology is intended to be part ofa technology-in-the-loop approach, where the ﬁnaldecision on which anti-stereotypes to use and inwhat way will be made by human professionals,we anticipate few instances where incorrect (i.e.,not related, unrealistic, or ineffective) automati-cally generated anti-stereotypes would be dissem-inated.
in most such cases, since anti-stereotypesare designed to be positive, no harm is expected tobe incurred on the affected group.
however, it ispossible that a positive, seemingly harmless anti-stereotypical description can have a detrimentaleffect on the target group, or possibly even intro-duce previously absent biases into the discourse.
further work should investigate the efﬁciency andpotential harms of such approaches in real-life so-cial settings..608ethical considerations.
data: we present a method for mapping a set ofwords that represent a stereotypical view of a so-cial category held by a given subpopulation ontothe two-dimensional space of warmth and compe-tence.
the stereotype content model, on whichthe methodology is based, has been shown to be ap-plicable across cultures, sub-populations, and time(fiske, 2015; fiske and durante, 2016).
therefore,the methodology is not speciﬁc to any subpopula-tion or any target social group..in the current work, we employ the publiclyavailable portion of the stereoset dataset (nadeemet al., 2020).
this english-only dataset has beencreated through crowd-sourcing us workers onamazon mechanical turk.
since mechanical turkus workers tend to be younger and have on averagelower household income than the general us pop-ulation (difallah et al., 2018), the collected datamay not represent the stereotypical views of thewider population.
populations from other parts ofthe world, and even sub-populations in the us, mayhave different stereotypical views of the same so-cial groups.
furthermore, as discussed in section 6,the stereoset dataset does not include stereotypedata for a large number of historically marginalizedgroups.
future work should examine data both re-ferring to, and produced by, a wider range of socialand cultural groups.
potential applications: as discussed previously,the automatically proposed anti-stereotypes canbe utilized by human professionals in a vari-ety of ways, e.g., searching for or creating anti-stereotypical images, writing counter-narratives,creating educational resources, etc.
one potentialconcern which has not received attention in the re-lated literature is the possibility that the processof generating counter-stereotypes may itself intro-duce new biases into the discourse, particularly ifthese counter-stereotypes are generated automati-cally, perhaps even in response to adversarial data.
we emphasize the importance of using counter-stereotypes not to deﬁne new, prescriptive boxesinto which groups of people must ﬁt (e.g., fromtable 3, that all software developers should be in-telligent and healthy, or that all entrepreneurs mustbe inventive and compassionate).
rather, counter-stereotypes should weaken common stereotypicalassociations by emphasizing that any social groupis not actually homogenous, but a group of individ-uals with distinct traits and characteristics.
in most.
cases, the algorithm-in-the-loop approach (with au-tomatic suggestions assisting human users) shouldbe adopted to reduce the risk of algorithmic biasesbeing introduced into the public discourse..often, harmful stereotyping is applied to minor-ity groups.
work on identifying and analyzingstereotypes might propagate the harmful beliefsfurther, and it is possible that collections of stereo-typical descriptions could be misused as informa-tion sources for targeted campaigns against vulner-able populations.
however, this same informationis needed to understand and counter stereotypicalviews of society.
we also note that although wetake advantage of word embedding models in ourapproach, we do not use the representations of tar-get group names.
previous work has shown thatbiased thinking is encoded in these models, andusing them to represent groups can be harmful tospeciﬁc demographics..identifying demographic characteristics: theproposed methodology deals with societal-levelstereotypical and anti-stereotypical representationsof groups of people and does not attempt to iden-tify individual user/writer demographic charac-teristics.
however, work on stereotyping andanti-stereotyping entails, by deﬁnition, namingand deﬁning social categories of people.
label-ing groups not only deﬁnes the category bound-aries, but also positions them in a hierarchicalsocial-category taxonomy (beukeboom and burg-ers, 2019).
we emphasize that our goal is not tomaintain and reproduce existing social hierarchies,as cautioned by blodgett et al.
(2020), but ratherto help dismantle this kind of categorical thinkingthrough the use of anti-stereotypes..energy resources: the proposed scm-basedmethod is computationally low-cost, and all ex-periments were performed on a single cpu.
oncethe pretrained vectors are loaded, the projectionand analysis is completed in less than a minute..references.
marion bartl, malvina nissim, and albert gatt.
2020.unmasking contextual stereotypes: measuring andin proceedingsmitigating bert’s gender bias.
of the second workshop on gender bias in natu-ral language processing, pages 1–16, barcelona,spain (online).
association for computational lin-guistics..susan benesch, derek ruths, kelly p dillon, haji mo-hammad saleem, and lucas wright.
2016. counter-.
609speech on twitter: a ﬁeld study.
a report for publicsafety canada under the kanishka project..camiel j. beukeboom and christian burgers.
2019.how stereotypes are shared through language: a re-view and introduction of the social categories andstereotypes communication (scsc) framework.
re-view of communication research, 7:1–37..irene v blair, jennifer e ma, and alison p lenton.
imagining stereotypes away: the moder-2001.ation of implicit stereotypes through mental im-agery.
journal of personality and social psychol-ogy, 81(5):828..su lin blodgett, solon barocas, hal daum´e iii, andhanna wallach.
2020. language (technology) ispower: a critical survey of “bias” in nlp.
in pro-ceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 5454–5476, online.
association for computational lin-guistics..tolga bolukbasi, kai-wei chang, james y zou,venkatesh saligrama, and adam t kalai.
2016.man is to computer programmer as woman is toin ad-homemaker?
debiasing word embeddings.
vances in neural information processing systems,pages 4349–4357..marco brambilla, simona sacchi, federica castellini,and paola riva.
2010. the effects of status on per-ceived warmth and competence.
social psychology,41(2):82–87..luke breitfeller, emily ahn, david jurgens, and yu-lia tsvetkov.
2019. finding microaggressions in thewild: a case for locating elusive phenomena in so-cial media posts.
in proceedings of the 2019 con-ference on empirical methods in natural languageprocessing and the 9th international joint confer-ence on natural language processing (emnlp-ijcnlp), pages 1664–1674, hong kong, china.
as-sociation for computational linguistics..aylin caliskan,.
and arvindjoanna j bryson,narayanan.
2017. semantics derived automaticallyfrom language corpora contain human-like biases.
science, 356(6334):183–186..patricia chiril, v´eronique moriceau, farah benamara,alda mari, gloria origgi, and marl`ene coulomb-gully.
2020. an annotated corpus for sexism de-in proceedings of thetection in french tweets.
12th language resources and evaluation confer-ence, pages 1397–1403..yi-ling chung, elizaveta kuzmenko, serra sinemtekiroglu, and marco guerini.
2019. conan -counter narratives through nichesourcing: a mul-tilingual dataset of responses to ﬁght online hatein proceedings of the 57th annual meet-speech.
ing of the association for computational linguis-tics, pages 2819–2829, florence, italy.
associationfor computational linguistics..amy j. c. cuddy, susan t. fiske, and peter glick.
2007. the bias map: behaviors from intergroupaffect and stereotypes.
journal of personality andsocial psychology, 92(4):631..amy j. c. cuddy, susan t. fiske, and peter glick.
2008. warmth and competence as universal dimen-sions of social perception: the stereotype contentmodel and the bias map.
advances in experimen-tal social psychology, 40:61–149..amy j. c. cuddy, peter glick, and anna beninger.
2011. the dynamics of warmth and competencejudgments, and their outcomes in organizations.
re-search in organizational behavior, 31:73–98..nilanjana dasgupta and anthony g greenwald.
2001.on the malleability of automatic attitudes: combat-ing automatic prejudice with images of admired anddisliked individuals.
journal of personality and so-cial psychology, 81(5):800–814..djellel difallah, elena filatova, and panos ipeirotis.
2018. demographics and dynamics of mechanicalturk workers.
in proceedings of the acm interna-tional conference on web search and data mining,pages 135–143..john f. dovidio, miles hewstone, peter glick, and vic-toria m. esses.
2010. the sage handbook of preju-dice, stereotyping and discrimination.
sage publica-tions..thomas eckes.
2002. paternalistic and envious genderstereotypes: testing predictions from the stereotypecontent model.
sex roles, 47(3):99–114..kawin ethayarajh.
2019. how contextual are contextu-alized word representations?
comparing the geome-try of bert, elmo, and gpt-2 embeddings.
arxivpreprint arxiv:1909.00512..elisabetta fersini, debora nozza, and paolo rosso.
2018. overview of the evalita 2018 task on au-tomatic misogyny identiﬁcation (ami).
evalitaevaluation of nlp and speech tools for italian,12:59..eimear finnegan, jane oakhill, and alan garnham.
2015.counter-stereotypical pictures as a strat-egy for overcoming spontaneous gender stereotypes.
frontiers in psychology, 6:1291..susan t. fiske.
2010. venus and mars or downto earth: stereotypes and realities of gender dif-ferences.
perspectives on psychological science,5(6):688–692..susan t. fiske.
2015..intergroup biases: a focus onstereotype content.
current opinion in behavioralsciences, 3:45–50..susan t. fiske.
2018. stereotype content: warmth andcompetence endure.
current directions in psycho-logical science, 27(2):67–73..610susan t. fiske, amy j. c. cuddy, and peter glick.
2006. universal dimensions of social cognition:warmth and competence.
trends in cognitive sci-ences, 11(2):77–83..susan t. fiske, amy j. c. cuddy, peter glick, and junxu.
2002. a model of (often mixed) stereotype con-tent: competence and warmth respectively followfrom perceived status and competition.
journal ofpersonality and social psychology, 82(6):878–902..susan t. fiske and federica durante.
2016. stereotypecontent across cultures: variations on a few themes.
in m. j. gelfand, c.-y.
chiu, and y.-y.
hong, edi-tors, handbook of advances in culture and psychol-ogy, pages 209–258.
oxford university press, newyork, ny..paula fortuna and s´ergio nunes.
2018. a survey on au-tomatic detection of hate speech in text.
acm com-puting surveys (csur), 51(4):1–30..peter glick, maria lameiras, susan t. fiske, thomaseckes, barbara masser, chiara volpato, anna mariamanganelli, jolynn c. x. pek, li-li huang, nuraysakalli-u˘gurlu, et al.
2004. bad but bold: ambiva-lent attitudes toward men predict gender inequalityin 16 nations.
journal of personality and social psy-chology, 86(5):713..aaron c kay, martin v day, mark p zanna, anda david nussbaum.
2013.the insidious (andironic) effects of positive stereotypes.
journal of ex-perimental social psychology, 49(2):287–291..keita kurita, nidhi vyas, ayush pareek, alan w black,and yulia tsvetkov.
2019. measuring bias in contex-tualized word representations.
in proceedings of thefirst workshop on gender bias in natural languageprocessing, pages 166–172, florence, italy.
associ-ation for computational linguistics..calvin k lai, maddalena marini, steven a lehr, carlocerruti, jiyun-elizabeth l shin, jennifer a joy-gaba, arnold k ho, bethany a teachman, sean pwojcik, spassena p koleva, et al.
2014. reducingimplicit racial preferences: a comparative investiga-tion of 17 interventions.
journal of experimentalpsychology: general, 143(4):1765..chapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 615–621, minneapo-lis, minnesota.
association for computational lin-guistics..binny mathew, navish kumar, pawan goyal, animeshmukherjee, et al.
2018. analyzing the hate andcounter speech accounts on twitter.
arxiv preprintarxiv:1812.02712..binny mathew, sandipan sikdar, florian lemmerich,and markus strohmaier.
2020. the polar frame-work: polar opposites enable interpretability of pre-in proceedings of thetrained word embeddings.
web conference 2020, pages 1548–1558..chandler may, alex wang, shikha bordia, samuel r.bowman, and rachel rudinger.
2019. on measur-ing social biases in sentence encoders.
in proceed-ings of the 2019 conference of the north americanchapter of the association for computational lin-guistics: human language technologies, volume 1(long and short papers), pages 622–628, minneapo-lis, minnesota.
association for computational lin-guistics..tomas mikolov, kai chen, greg corrado, and jef-efﬁcient estimation of wordarxiv preprint.
frey dean.
2013.representations in vector space.
arxiv:1301.3781..tomas mikolov, edouard grave, piotr bojanowski,christian puhrsch, and armand joulin.
2018. ad-vances in pre-training distributed word representa-in proceedings of the international confer-tions.
ence on language resources and evaluation (lrec2018)..moin nadeem, anna bethke,.
and siva reddy.
stereoset: measuring stereotypical biasarxiv preprint.
2020.in pretrained language models.
arxiv:2004.09456..nikita nangia, clara vania, rasika bhalerao, andsamuel r. bowman.
2020. crows-pairs: a chal-lenge dataset for measuring social biases in maskedlanguage models.
in proceedings of the conferenceon empirical methods in natural language process-ing (emnlp)..tiane l. lee and susan t. fiske.
2006. not an out-group, not yet an ingroup: immigrants in the stereo-type content model.
international journal of inter-cultural relations, 30(6):751–768..gandalf nicolas, xuechunzi bai, and susan t. fiske.
2020. comprehensive stereotype content dictionar-ies using a semi-automated method.
european jour-nal of social psychology..susan c losh, ryan wilke, and margareta pop.
2008.some methodological issues with “draw a scientisttests” among young children.
international journalof science education, 30(6):773–792..thomas manzini, lim yao chong, alan w black,and yulia tsvetkov.
2019. black is to criminalas caucasian is to police: detecting and removingin proceed-multiclass bias in word embeddings.
ings of the 2019 conference of the north american.
jeffrey pennington, richard socher, and christopher dmanning.
2014. glove: global vectors for wordrepresentation.
in proceedings of the conference onempirical methods in natural language processing(emnlp), pages 1532–1543..ilan price, jordan gifford-moore, jory flemming, saulmusker, maayan roichman, guillaume sylvain,nithum thain, lucas dixon, and jeffrey sorensen.
six attributes of unhealthy conversations.
2020..611in proceedings of the fourth workshop on onlineabuse and harms, pages 114–124, online.
associa-tion for computational linguistics..jing qian, anna bethke, yinyin liu, elizabeth beld-ing, and william yang wang.
2019. a bench-mark dataset for learning to intervene in online hatespeech.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on natu-ral language processing (emnlp-ijcnlp), pages4755–4764, hong kong, china.
association forcomputational linguistics..radim ˇreh˚uˇrek and petr sojka.
2010. software frame-work for topic modelling with large corpora.
in pro-ceedings of the lrec 2010 workshop on new chal-lenges for nlp frameworks, pages 45–50, valletta,malta.
elra..maarten sap, saadia gabriel, lianhui qin, dan ju-rafsky, noah a. smith, and yejin choi.
2020. so-cial bias frames: reasoning about social and powerin proceedings of theimplications of language.
58th annual meeting of the association for compu-tational linguistics, pages 5477–5490, online.
as-sociation for computational linguistics..anna schmidt and michael wiegand.
2017. a surveyon hate speech detection using natural language pro-in proceedings of the fifth internationalcessing.
workshop on natural language processing for so-cial media, pages 1–10..serra sinem tekiro˘glu, yi-ling chung, and marcoguerini.
2020.generating counter narrativesagainst online hate speech: data and strategies.
inproceedings of the 58th annual meeting of the asso-ciation for computational linguistics, pages 1177–1190, online.
association for computational lin-guistics..alexandra timmer.
2011. toward an anti-stereotypingapproach for the european court of human rights.
human rights law review, 11(4):707–738..bertie vidgen, alex harris, dong nguyen, rebekahtromble, scott hale, and helen margetts.
2019.challenges and frontiers in abusive content detec-tion.
in proceedings of the third workshop on abu-sive language online, pages 80–93, florence, italy.
association for computational linguistics..612a constructing polar dimensions.
b stereotype data.
in contrast to the standard polar framework in-troduced by mathew et al.
(2020), we do not have aset of polar opposite word pairs, each representinga different interpretable dimension, but rather a setof words for each of the concepts warmth, coldness,competence, and incompetence from nicolas et al.
(2020).
therefore, we use a slightly different for-mulation to obtain the polar directions associatedwith warmth and competence.4−→wa3, ...,1,.
−→wav ] ∈ rv ×d de-note the set of pretrained d-dimensional embed-ding vectors, trained with algorithm a, where v isthe size of the vocabulary andi is a unit vectorrepresenting the ith word in the vocabulary..let d = [.
−→wa2,.
−−→wa.
−→wa.
w+, ..., pn1.
in this work, we use four sets of seed words;a set of n1 words associated with positivewarmth pw+ = {p1w+, p2w+}, a setof n2 words associated with negative warmth,pw− = {p1w−}, a set of n3 wordsassociated with positive competence, pc+ ={p1c+ }, and a set of n4 words as-sociated with negative competence, pc− ={p1c− }.
in order to ﬁnd the two po-lar opposites, we obtain the following directions:.
w−, ..., pn2.
c−, ..., pn4.
c+, ..., pn3.
w−, p2.
c−, p2.
c+, p2.
n1(cid:88).
−−→dir1 =.
1n1.
wapiw+.
−.
−−→dir2 =.
i=1.
n3(cid:88).
i=1.
1n3.
wapic+.
−.
1n2.
1n4.
n2(cid:88).
i=1.
n4(cid:88).
i=1.
wapiw−.
wapic−.
(a.1).
where waυ represents the vector of the word υ. thetwo direction vectors are stacked to form dir ∈r2×d, which represents the change of basis matrixfor the new two-dimensional embedding subspacee. in the new subspace, a word υ is represented by−→e υ, which is calculated using the following lineartransformation:.
−→e υ = (dirt )−1wa.
υ.
(a.2).
in our experiments, we showed that, as expected,the two dimensions of e are associated withwarmth and competence..here we present all of the words contributing toeach stereotype for each target group.
in additionto 37 tokens which did not have vector represen-tations in the pre-trained embeddings, here weretwo reasons why words were discarded from theanalysis, as described in the paper.
first, if a worddirectly referenced another demographic category,it was discarded.
this was to avoid, as much aspossible, including effects of language model biasin our model.
for example, a number of anno-tators used the word black to describe prisoners;however, if the language model has some racialbias involving the word black, then it would af-fect the placement of the word prisoner on thewarmth-competence plane.
while acknowledgingthat stereotypical associations between groups areproblematic and worth of study in their own right(including this disturbing example involving raceand incarceration), it is beyond the scope of thecurrent analysis..additional words were discarded in a ﬁlteringstep, where words greater than a cosine distance of0.6 from the mean or centroid of the group werediscarded.
as people’s views towards differentgroups naturally vary, this was done to prevent out-lier words from impacting the analysis, which isfocused here on the most widespread or prevalentstereotypes of a given group.
while heuristicallychosen, the threshold value appears to be accept-able in many cases (see table b.1).
however, othertimes a large number of words are discarded, whichappear in some cases to represent a second, coher-ent cluster of stereotype words for a given group(see, for example, the words for policeman, whichsplit into two clusters alternately characterizing thegroup as corrupt and racist, or strong and heroic).
as mentioned in the discussion, future work will ex-amine how we can identify clusters of stereotypesin larger datasets..in the following table, for each target group we.
present three lists of words:• included: the words included in the analysis.
this list is ranked according to distance to themean, and thus most ‘representative’ words occurﬁrst.
words which occur more than once havethe frequency given in parentheses..• discarded: the words discarded as outliers by.
the thresholding step..4we use the same notation as mathew et al.
(2020) to.
explain our method..• demographic: the words discarded as referring.
primarily to a demographic characteristic..613targetgroup.
words.
mother.
mommy.
schoolgirl.
schoolboy.
male.
gentlemen.
herself.
himself.
sister.
muslim.
target groups from the ‘gender’ category.
included: caring(6), loving(4), nurturing(5), comforting, protective, kind(2), love, strong, feminine, over-bearing(2), children, housekeeper discarded : old demographic: [].
included: loving(4), caring(4), nurturing(5), doting, sweet, protective, understanding, child, sexy, overbear-ing, mama, childish, busy discarded: kindly, nagging demographic: female.
included: smart(2), naive, immature(2), young, innocent(2), sexy, studious, artsy, girly, small, hopeful, thin,hardworker discarded: innnocent, short, cellphone, does demographic: [].
included: naughty(2), mischevious(3), immature(2), innocent, cute, energetic, horny, young, troublesome,brainy, troublemaker, pranks discarded: nerd, weak, athletic, pretentious, strong, student, touched, mean,football, scholar, tag, uniform demographic: [].
included: strong(8), dominant(3), aggressive, muscular, masculine(2), violent, strength, brute, abusive,stupid, dumb discarded: blue, messy, ignorant, demanding, alpha, manager, sports, mansplaining demo-graphic: [].
included: polite(3), respectful, classy(2), considerate, gentle, kind, cordial, honorable, mannerable, hand-some, proper(2), haughty, nicest, respect, bossy discarded: nerdy, open, wealthy, wealthy, understanding,were, ﬂask demographic: [].
included: emotional(2), kind, dumb, negative, upbeat, beautiful, unsure, incapable, mean girl discarded:lonely, uptight, created, dress, pink, slap, cake, beating, shopping, alone, needlepoint, walked demographic:[].
included: athletic, progressive, brawny, strong, runner, workout, egotistical, gaming male, boy discarded:funny, quiet, doctors, safe, job, aggression, admire, yardwork, enjoy, push, opinions, alone demographic: [].
grandfather.
included: old(8), antiquated, feeble, boring, stubborn, kind discarded: wise(4), smart, sexist, slow(2), hero,sleeping, protected, watch, spoil, ﬁght demographic: [].
included: caring(2), loving, nurturing, talkative, sassy, bossy, empathy, fragile, friend discarded: annoy-ing(2), close, small, bonded, protect, argumentative, naggy, noisy, mean(2), college, catty, rival demo-graphic: girl.
target groups from the ‘religion’ category.
included: violent(2), terrorist(6), dangerous(2), evil(2), radical(2), hateful, religious, aggressive, crazy,misogynist, intollerant, oppression, legalistic, prejudice, unprecedented discarded: religion, quaran, hijabdemographic: men.
brahmin.
included: religious(3), sacred, ancient, foreign, temples, nobleman, loving discarded: prideful, snobby,arrogant, haughty, wise, unknown, despised, dirty(2), prestigeous, creator(2) demographic: muslim, hindu.
target groups from the ‘race or nationality’ category.
eritrean.
included: poor(6), impoverished, lazy(2), needy, dirty(2), rich, starving, weird, aggressive, unsustainablediscarded: proud, gang, crime, submissive, stole, huts, artwork, rug demographic: black(2).
norweigan.
included:blond(7), pale, tall, attractive discarded: beautiful, wealthy(2), cold(2), strong, serious, nice,sincere, quiet demographic: white(4).
saudi ara-bian.
included: rich(4), greedy(2), poor, spoiled, oppressed, conservative discarded: cruel, oil, docile, pre-tentious, sandy, racist, hateful, prince, mecca, camel, extremist, turbans, terrorist(2), isis demographic:muslim(4).
japanese.
russian.
ethiopian.
columbian.
crimean.
iranian.
included: friendly, smart, quiet, polite, playful, peaceful, wise, clean(2), sneaky, loyal, passive, educated, shy,exotic, proud, xenophobic discarded: sexist, short, academic, work, kamikaze, sushi, seafood, workaholicsdemographic: black.
included: drunk(4), alcoholic, drinking, angry, stupid, vodka, cold, homophobic discarded: corrupt,suspicious, scary, outspoken, communist, brutal, spy, deceitful, spies, slavic, shady, strong, tough, stern,mean(2), scammer, control, communism demographic: [].
included: skinny(7), starving(5), hungry(2), emaciated, thin christian discarded: empty, unfortunate,corrupt, somber, stews, save demographic: [].
included: drugs(8), cocaine(2), druggie, coffee(2), criminal, druglord discarded: football, hot, sexy(2),local, educated, poor, voluptuous, undeveloped, ﬁery demographic: [].
included: poor, impoverished, weak, nonexistent, corrupt, harsh, indigenous, boring, wartorn, native,abundance, drunk discarded: radical, communist, exotic, independent, controversial, stolen, combative,crime, war, lawlessness, dumber, ukraine demographic: islam, russian, black, white.
included: dangerous(4), violent, scary, hateful, bloodthirsty, evil, foolish, dumb, terrorist(2), religious,pretentious, conservative, rogue, greasy discarded: terrorists(3), panicked, war, fragrant, traditional,hookah demographic: [].
614persian peo-ple.
included: greedy(2), rich(2), uneducated(2), poor, extravagant, disgusting, intelligent, mysterious, ﬂashydiscarded: hairy, smelly, violent, religious, smoke, strict, sand, naan demographic: black, muslim.
arab.
african.
ghanaian.
ukrainian.
bengali.
hispanic.
included: violent, dangerous(2), evil, terrorist(2), radical, crazy, religious(2), extremism, extreme, bad, cruel,terrorists(2), dirty(2), conservative, misogynist, war, diverse discarded: rich(2), oil, turban demographic:muslim.
included: poor(3), impoverished, starving(2), dirty, stupid, harsh, dark, fast(2), violent discarded: savages,criminal, crackhead, stealing, run, atheltic demographic: black(6).
included: poor(4), dangerous(2), dark(2), lazy, strange, reckless, brutish, fun, starving discarded: friendly,athletic, criminals, tribal, less, gold, rapists, tall, dated demographic: black(2), indian.
included: stupid, dumb, poor(3), dirty, hateful, cynical, evil, unintelligent, pretty, cheap, powerless, unclean,exotic, criminal, interesting discarded:religious, isolationist, quiet, freezing, gangster, discernible,vodka(3), allies, borscht demographic: jewish, white, russian.
included: dirty(2), stinky, poor(2), dangerous, smelly(2), cold, primitive, wild, tough, rich, smart, inde-cent, adventorous discarded: macho, dancing, conservative, curry, rice, tiger, eastern, india, tigers(2)demographic: [].
included: poor(3), lazy(2), dirty(2), nasty, uneducated, shady, illegal(2), aggressive, hardworking(2),beautiful, greasy, undocumented discarded: loud, smelly, immigrants, farmworker, spicy, spanish, gardener,season, equator demographic: catholic.
target groups from the ‘profession’ category.
psychologist.
included: intelligent(3), smart(3), empathetic(3), compassionate, witty, analytical(3), educated(2), opin-ionated, careful, observant, curious, interesting, calm, rigid discarded: calming, nosy, wackjobs, helpsdemographic: [].
manager.
guitarist.
included: bossy(8), overbearing, rude, selfcentered, boss, lazy, authoritative, leader discarded: strict(2),controlling, boring, knowledgeable, corrupt, organized(3), mean, seriousness demographic: white, man(2),he(2).
included: energetic, passionate, talented(2), musical(3), young, sensitive, emotional, skilled, cool, stimulat-ing, attitude, lazy, hippie discarded: skinny, drunk, rock, stoned, dedication, druggies demographic: male,caucasian.
civil servant.
included: lazy, tired, overworked(2), poor, bad, careless, selﬁsh, bored, stressed, corrupt, smart, dependablediscarded: bureaucratic, kind, competent, honorable, bureaucrat(2), disrespected, democrat, state, job,volunteering demographic: [].
butcher.
commander.
chessplayer.
performingartist.
plumber.
tailor.
chemist.
musician.
included: brutal, bloody(2), violent, bloodthirsty, ﬁlthy, strong(2), sloppy, dumb, burly, macho, cutsdiscarded: meat, fat(3), knives, foreign, uneducated(2), experienced, overweight demographic: he(2),white, man, male.
included: strong, ruthless, harsh, wise, friendly, honest, intelligent, brave, hard, trustworthy, loud(2),important, sharp, protective, strict(2), dominant discarded: leading, experienced, leader(2), organized,bossy(2), controlling demographic: male(2), him.
included: smart(4), strategic(4), clever, brilliant, analytical, methodical, careful, nerdy, strategy, quiet, dorky,calm, geek discarded: reserved, frail, calculated, glasses demographic: asian(3), man, russian.
included: talented(4), charismatic, creative, skilled, egotistical(2), opinionated, attractive, expressive,outgoing, dancer discarded: selﬁsh, ballerina, musical, big, emotional, ostentatious, broke demographic:gay.
included: uneducated(3), lazy, illiterate, dirty(2), uncouth, fat(3), unkempt, crooked, redneck, lackadaisical,hardworking discarded: greasy, expensive(2), lower, ﬁx, installs demographic: male, italian.
included: precise(2), meticulous, detailed(2), careful, professional(2), neat, skilled(2), creative, quiet(2),short discarded: snarky, crafty, foreigner, james demographic: italian(2), gay, asian, man, jew, female.
included: smart(5), intelligent(3), nerdy(3), geeky(2), studious, crafty, analytical, introverted, careful,brillant discarded: shy, science, laboratory demographic: white, man, he.
included: creative(4), artistic(2), talented(4), professional, good, emotional, cute, intense, informal dis-carded: emotion, poor, employed, ﬂakey, famous, handsome, shiftless, addict, unreliable, broke demo-graphic: [].
software de-veloper.
included: nerdy(7), geeky(2), smart(6), nerd, intelligent(2), genius, math, unhealthy discarded: boys,rich(2), coder, glasses, skilled demographic: male.
mover.
footballplayer.
included: strong(18), burly, hardworking, dumb, tall discarded: creepy, careless demographic: men(2),mexican.
included: dumb(4), stupid(3), strong(5), tough(2), aggressive(2), fast, talented discarded: bully, muscular,hulking, athletic(3) injured, buff, athlete, doping demographic: black (2), male.
615producer.
politician.
prosecutor.
bartender.
physicist.
assistant.
historian.
researcher.
included: shrewd, smart, egotistical, selﬁsh, ﬂashy, industrious, aloof, rich, visionary, knowledgable,businesslike, creepy, busy, critical, cheap discarded: predatory, tired, leader, ideas, elitist, demanding(3),ﬁlm demographic: white, male(2).
included: dishonest(3), corrupt(8), deceptive, crooked, liars(3), shifty, lying(2), disingenuine, cheating, fake,slippery, argumentative discarded: [] demographic: female, male.
included: aggressive(5), ruthless, agressive, vindictive, harsh(2), tough, unfair, unrelenting, aggressively,sneaky, biased, contentious, persuasive, racist, corrupt, conﬁdent discarded: educated, guilty(2), lawyer,justice demographic: male(2).
included: friendly(3), sociable(2), personable, gregarious, outgoing(2), beautiful, sexy, lovely, quick, fast,stupid discarded: uneducated, criminal, alcoholic(2), drunk(3), listens, drinker, provocatively, listener,listeners demographic: [].
included: smart(7), intelligent(4), nerdy(5), brilliant, eccentric, genius(2), educated, dynamic, focuseddiscarded: pety demographic: man, his.
included: helpful(4) efﬁcient, diligent, organized, helper discarded: harried, independent, meek, young,obediant, slutty, meak, unnoticed, clerical, coffee, grunt, inferior, coward demographic: female.
included: bookish(4), nerdy, studious, bookworm, smart(3), academic, professorial, library discarded:boring(5), dull, educated, ancient, wise, correct demographic: [].
included: studious, nerdy(3), inquisitive, smart, analytical(2), careful, meticulous, thorough, introverted,competent, dedicated(2), awkward, shy, inquiring discarded: boring, coldhearted, educator, overworked,intern, library, computer demographic: man, asian.
deliveryman.
included: young(3), hardworking(2), poor, careless(2), stupid, teenager, strong, late(2), athletic discarded:kind, ﬁt, screwing, efﬁcient, rushed, timely, distribute demographic: white, black, male.
entrepreneur.
included: savvy(3), ambitious(2), smart, greedy(3), ruthless, young(2), creative(2), industrious, wise,rich(2), successful, inventive, talkative, sleazy, educated, extroverted discarded: hardworker, driven, free,boss demographic: male.
policeman.
engineer.
included: corrupt(4), racist(2), megalomaniacal, bad, aggressive discarded: fat, murderer, strong, safe,bully, macho, agressive, brave, hero, assaults, stern, mean, helpful demographic: white(2).
included: smart(6), intelligent, nerdy(3), geeky, knowledgeable, egotistical, wealthy, awkward, rich, boring,intellegant, methodical discarded: technical, antisocial, update demographic: man, chinese, he, male.
mathemati-cian.
included: smart(6), intelligent(3), nerdy(4), geek, analytical, analytical, nerds, good, geniuses, intelligence,meek discarded: logic, antisocial, introvert, numbers, algebra demographic: he, man(2).
nurse.
included: caring(8), compassionate, hardworking, supportive, patient, kind, dedicated discarded: over-worked, profession, nice, busy, tired, hot, underqualiﬁed demographic: woman(2), her, female(2), she,male.
prisoner.
included: violent(5), dangerous(2), brutal, cruel, evil(2), criminal, bad, dishonest, untrustworthy, hopeless,thug, lazy, perpetrator discarded: smelly, guilty(2), mean(2) demographic: black(6).
table b.1: target groups and associated stereotype words in stereoset.
words which occur morethan once for a given group have their frequency indicated in parentheses.
words that are includedin the analysis are ranked by closeness to the cluster mean; thus the ﬁrst words in the list are mostrepresentative of the stereotype for that group..616