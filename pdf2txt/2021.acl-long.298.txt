label-speciﬁc dual graph neural networkfor multi-label text classiﬁcation.
qianwen ma1,2, chunyuan yuan1,2, wei zhou1* and songlin hu1,21 institute of information engineering, chinese academy of sciences2 school of cyber security, university of chinese academy of sciences{maqianwen,yuanchunyuan,zhouwei,husonglin}@iie.ac.cn.
abstract.
multi-label text classiﬁcation is one of the fun-damental tasks in natural language process-ing.
previous studies have difﬁculties to dis-tinguish similar labels well because they learnthe same document representations for differ-ent labels, that is they do not explicitly ex-tract label-speciﬁc semantic components fromdocuments.
moreover, they do not fully ex-plore the high-order interactions among thesesemantic components, which is very helpful topredict tail labels.
in this paper, we proposea novel label-speciﬁc dual graph neural net-work (ldgn), which incorporates categoryinformation to learn label-speciﬁc componentsfrom documents, and employs dual graphconvolution network (gcn) to model com-plete and adaptive interactions among thesecomponents based on the statistical label co-occurrence and dynamic reconstruction graphin a joint way.
experimental results on threebenchmark datasets demonstrate that ldgnsigniﬁcantly outperforms the state-of-the-artmodels, and also achieves better performancewith respect to tail labels..1.introduction.
automatically labeling multiple labels of docu-ments is a fundamental and practical task in nat-urallanguage processing.
recently, with thegrowth of data scale, multi-label text classiﬁca-tion(mltc) has attracted more attention, since itis often applied to many ﬁelds such as sentimentanalysis (liu and chen, 2015; li et al., 2016),emotion recognition (wang et al., 2016; jabreeland moreno, 2019), web page tagging (jain et al.,2016) and so on.
however, the number of labelsand documents and the complex relations of labelsrender it an unsolved and challenging task..existing studies for multi-label text classiﬁca-tion mainly focus on learning enhanced document.
*corresponding author.
representation (liu et al., 2017) and modeling la-bel dependency (zhang et al., 2018; yang et al.,2018; tsai and lee, 2019) to improve classiﬁca-tion performance.
although they have exploredthe informative words in text content, or consid-ered the label structure and label semantics to cap-ture label correlations, these models cannot dis-tinguish similar labels well (e.g., the categoriesprices vs consumer prices in reuters news)..the main reason is that most of them neglectthe semantic connections between labels and in-put documents and they learn the same documentrepresentations for different labels, which cannotissue the label similarity problem.
more specif-ically, they do not explicitly consider the corre-sponding semantic parts of each label in the docu-ment..recently, some studies (you et al., 2019; xiaoet al., 2019; du et al., 2019) have used attentionmechanism to explore the above semantic connec-tions, and learn a label-speciﬁc document repre-sentation for classiﬁcation.
these methods haveobtained promising results in mltc, which showsthe importance of exploring semantic connections.
however, they did not further study the interac-tions between label-speciﬁc semantic componentswhich can be guided by label correlations, andthus these models cannot work well on predict-ing tail labels which is also a challenging issue inmltc.
to handle these issues, a common way toexplore the semantic interactions between label-speciﬁc parts in document is to utilize the statisti-cal correlations between categories to build a labelco-occurrence graph for guiding interactions..nevertheless, statistical correlations have threedrawbacks.
first, the co-occurrence patterns be-tween label pairs obtained from training data areincomplete and noisy.
speciﬁcally, the label co-occurrences that appear in the test set but do notappear in the training set may be ignored, while.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3855–3864august1–6,2021.©2021associationforcomputationallinguistics3855some rare label co-occurrences in the statisticalcorrelations may be noise.
second, the label co-occurrence graph is built in global, which maybe biased for rare label correlations.
and thusthey are not ﬂexible to every sample document.
third, statistical label correlations may form along-tail distribution, i.e., some categories are verycommon while most categories have few of doc-uments.
this phenomenon may lead to modelsfailing to predict low-frequency labels.
thus, ourgoal is to ﬁnd a way to explore the complete andadaptive interactions among label-speciﬁc seman-tic components more accurately..in this paper, we investigate: (1) how to explic-itly extract the semantic components related to thecorresponding labels from each document; and (2)how to accurately capture the more complete andmore adaptive interactions between label-speciﬁcsemantic components according to label depen-dencies.
to solve the ﬁrst challenge, we ex-ploit the attention mechanism to extract the label-speciﬁc semantic components from the text con-tent, which can alleviate the label similar problem.
to capture the more accurate high-order interac-tions between these semantic components, we ﬁrstemploy one graph convolution network (gcn)to learn component representations using the sta-tistical label co-occurrence to guide the informa-tion propagation among nodes (components) ingcn.
then, we use the component representa-tions to reconstruct the adjacency graph dynami-cally and re-learn the component representationswith another gcn, and thus we can capture thelatent interactions between these semantic compo-nents.
finally, we exploit ﬁnal component repre-sentations to predict labels.
we evaluate our modelon three real-world datasets, and the results showthat the proposed model ldgn outperforms allthe comparison methods.
further studies demon-strate our ability to effectively alleviate the tail la-bels problem, and accurately capture the mean-ingful interactions between label-speciﬁc seman-tic components..the contributions of this paper are as follows:.
• we propose a novel label-speciﬁc dual graphneural network (ldgn), which incorporatescategory information to extract label-speciﬁccomponents from documents, and exploresthe interactions among these components..• to model the accurate and adaptive interac-tions, we jointly exploit global co-occurrence.
patterns and local dynamic relations.
tomake up the deﬁciency of co-occurrences, weemploy the local reconstruction graph whichis built by every document dynamically..• we conduct a series of experiments on threepublic datasets, and experimentalresultsdemonstrate that our model ldgn signiﬁ-cantly outperforms the state-of-the-art mod-els, and also achieves better performancewith respect to tail labels..2 model.
as depicted in figure 1, our model ldgnis composed of two major modules: 1) label-speciﬁc document representation 2) dual graphneural network for semantic interaction learn-ing.
speciﬁcally, label-speciﬁc document repre-sentation learning describes how to extract label-speciﬁc semantic components from the mixtureof label information in each document; and thedual graph neural network for semantic interactionlearning illustrates how to accurately explore thecomplete interactions among these semantic com-ponents under the guidance of the prior knowledgeof statistical label co-occurrence and the posteriorinformation of dynamic reconstruction graph.
problem formulation: let d = {xi, yi}nbe the set of documents, which consists of ndocument xi and its corresponding label yi ∈{0, 1}|c|, where |c| denotes the total number oflabels.
each document xi contains j words xi =wi1, wi2, .
.
.
, wij .
the target of multi-label textclassiﬁcation is to learn the mapping from inputtext sequence to the most relevant labels..2.1 label-speciﬁc document representation.
given a document x with j words, we ﬁrst em-bed each word wj in the text into a word vectorewj ∈ rd, where d is the dimensionality of wordembedding vector.
to capture contextual infor-mation from two directions of the word sequence,we ﬁrst use a bidirectional lstm to encode word-level semantic information in document represen-tation.
and we concatenate the forward and back-ward hidden states to obtain the ﬁnal word se-quence vector h ∈ r|j|×d..after that, to explicitly extract the correspond-ing semantic component related to each label fromdocuments, we use a label guided attention mech-anism to learn label-speciﬁc text representation..3856figure 1: the architecture of the proposed network ldgn..firstly, we randomly initialize the label represen-tation c ∈ r|c|×dc, and compute the label-awareattention values.
then, we can induce the label-speciﬁc semantic components based on the labelguided attention.
the formula is as follows:.
i.
(cid:1)exp (cid:0)hjctj exp (cid:0)hjctαijhj ,.
i.
(cid:80).
(cid:88).
(cid:1) ,.
αij =.
ui =.
j.
(1).
(2).
where αij indicates how informative the j-th textfeature vector is for the i-th label.
ui ∈ rd de-notes the semantic component related to the labelci in this document..2.2 dual graph neural network.
interaction learning with statistical label co-occurrenceto capture the mutual interactionsbetween the label-speciﬁc semantic components,we build a label graph based on the prior knowl-edge of label co-occurrence, each node in whichcorrelates to a label-speciﬁc semantic componentui.
and then we apply a graph neural network topropagate message between nodes..formally, we deﬁne the label graph g = (v, e),where nodes refer to the categories and edges re-fer to the statistical co-occurrence between nodes(categories).
speciﬁcally, we compute the proba-bility between all label pairs in the training set andget the matrix as ∈ r|c|×|c|, where asij denotesthe conditional probability of a sample belongingto category ci when it belongs to category cj..then, we utilize gcn (kipf and welling, 2017)to learn the deep relationships between label-speciﬁc semantic components guided by the statis-tical label correlations.
gcns are neural networks.
operating on graphs, which are capable of enhanc-ing node representations by propagating messagesbetween neighboring nodes..in multi-layer gcn, each gcn layer takes thecomponent representations from previous layer hlas inputs and outputs enhanced component repre-sentations, i.e., hl+1.
the layer-wise propagationrule is as follows:.
hl+1 = σ.
(cid:16).
(cid:98)ashlwl(cid:17).
,.
(3).
where σ (·) denotes leakyrelu (maas et al.,2013) activation function.
wl ∈ rd×d(cid:48)is atransformation matrix to be learned.
(cid:98)a denotesthe normalized adjacency matrix, and the normal-ization method (kipf and welling, 2017) is:.
(cid:98)a = d− 1.
2 ad− 12 ,.
(4).
where d is a diagonal degree matrix with entriesdij = σjaij.
depending on how many convolutional layersare used, gcn can aggregate information onlyabout immediate neighbors (with one convolu-tional layer) or any nodes at most k-hops neigh-bors (if k layers are stacked).
see (kipf andwelling, 2017) for more details about gcn..we use a two-layer gcn to learn the interac-tions between label-speciﬁc components.
the ﬁrstlayer takes the initialized component representa-tions u ∈ r|c|×d in equation 2 as inputs h0;and the last layer outputs h2 ∈ r|c|×d(cid:48)with d(cid:48)denoting the dimensionality of ﬁnal node repre-sentations..however, the statistical label correlations ob-tained by training data are incomplete and noisy..3857(cid:50)(cid:71)(cid:72)(cid:75)(cid:82)(cid:3)(cid:56)(cid:75)(cid:86)(cid:88)(cid:75)(cid:89)(cid:75)(cid:84)(cid:90)(cid:71)(cid:90)(cid:79)(cid:85)(cid:84)(cid:3)(cid:857)(cid:857)(cid:58)(cid:82)(cid:85)(cid:71)(cid:53)(cid:72)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)w1w2wj(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:856)(cid:40)(cid:79)(cid:50)(cid:57)(cid:58)(cid:51)(cid:258)(cid:410)(cid:410)(cid:286)(cid:374)(cid:410)(cid:349)(cid:381)(cid:374)(cid:45)(cid:41)(cid:52)(cid:95)(cid:460)(cid:23)(cid:3)(cid:95)(cid:460)(cid:24)(cid:95)(cid:460)(cid:73)(cid:857)(cid:68)(cid:62)(cid:87)(cid:44)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:3)(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:3)(cid:54)(cid:87)(cid:68)(cid:87)(cid:76)(cid:86)(cid:87)(cid:76)(cid:70)(cid:68)(cid:79)(cid:3)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:3)(cid:38)(cid:82)(cid:16)(cid:82)(cid:70)(cid:70)(cid:88)(cid:85)(cid:85)(cid:72)(cid:81)(cid:70)(cid:72)(cid:50)(cid:71)(cid:72)(cid:75)(cid:82)(cid:19)(cid:89)(cid:86)(cid:75)(cid:73)(cid:79)(cid:76)(cid:79)(cid:73)(cid:3)(cid:58)(cid:75)(cid:94)(cid:90)(cid:3)(cid:56)(cid:75)(cid:86)(cid:88)(cid:75)(cid:89)(cid:75)(cid:84)(cid:90)(cid:71)(cid:90)(cid:79)(cid:85)(cid:84)(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79)(cid:3)(cid:79)(cid:82)(cid:86)(cid:86)(cid:47)(cid:68)(cid:69)(cid:72)(cid:79)(cid:16)(cid:86)(cid:83)(cid:72)(cid:70)(cid:76)(cid:73)(cid:76)(cid:70)(cid:3)(cid:38)(cid:82)(cid:80)(cid:83)(cid:82)(cid:81)(cid:72)(cid:81)(cid:87)(cid:86)h14h24hc4(cid:56)(cid:75)(cid:73)(cid:85)(cid:84)(cid:89)(cid:90)(cid:88)(cid:91)(cid:73)(cid:90)(cid:79)(cid:85)(cid:84)(cid:45)(cid:88)(cid:71)(cid:86)(cid:78)(cid:45)(cid:41)(cid:52)(cid:50)(cid:71)(cid:72)(cid:75)(cid:82)(cid:3)(cid:41)(cid:85)(cid:19)(cid:85)(cid:73)(cid:73)(cid:91)(cid:88)(cid:88)(cid:75)(cid:84)(cid:73)(cid:75)(cid:42)(cid:91)(cid:71)(cid:82)(cid:3)(cid:45)(cid:88)(cid:71)(cid:86)(cid:78)(cid:3)(cid:52)(cid:75)(cid:91)(cid:88)(cid:71)(cid:82)(cid:3)(cid:52)(cid:75)(cid:90)(cid:93)(cid:85)(cid:88)(cid:81)(cid:3)(cid:76)(cid:85)(cid:88)(cid:57)(cid:75)(cid:83)(cid:71)(cid:84)(cid:90)(cid:79)(cid:73)(cid:3)(cid:47)(cid:84)(cid:90)(cid:75)(cid:88)(cid:71)(cid:73)(cid:90)(cid:79)(cid:85)(cid:84)(cid:3)(cid:50)(cid:75)(cid:71)(cid:88)(cid:84)(cid:79)(cid:84)(cid:77)(cid:53)(cid:72)(cid:16)(cid:79)(cid:72)(cid:68)(cid:85)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:39)(cid:92)(cid:81)(cid:68)(cid:80)(cid:76)(cid:70)(cid:53)(cid:72)(cid:70)(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)(cid:70)(cid:87)(cid:76)(cid:82)(cid:81)(cid:42)(cid:85)(cid:68)(cid:83)(cid:75)u1u2uch12h2(cid:21)hc2and the co-occurrence patterns between labelpairs may form a long-tail distribution.
re-learning with dynamic reconstructiongraphto capture the more complete andadaptive interactions between these components,we exploit the above component representationsh2 to reconstruct the adjacency graph dynam-ically, which can make up the deﬁciency ofco-occurrence matrix.
and then we re-learn theinteractions among the label-speciﬁc componentsguided by the posterior information of dynamicreconstruction graph..speciﬁcally, we apply two 1×1 convolution lay-ers and dot product to get the dynamic reconstruc-tion graph ad as follows:.
ad = f.(cid:16)(cid:0)wa ∗ h2(cid:1)t (cid:0)wb ∗ h2(cid:1)(cid:17).
,.
(5).
where wa ∈ rd1×d(cid:48)and wb ∈ rd1×d(cid:48)are the weights of two convolution layers, f isthe sigmoid activation function.
and then wenormalize the reconstruction adjacency matrix asequation 4, and obtain the normalized adjacencymatrix (cid:98)ad of reconstruction graph..in a similar way as equation 3, we apply an-other 2-layer gcn to learn the deep correlationsbetween components with the dynamic recon-struction graph.
the ﬁrst layer of this gcn takesthe component representations h2 as inputs, andthe last layer outputs the ﬁnal component repre-sentations h4 ∈ r|c|×d(cid:48)..2.3 multi-label text classiﬁcation.
after the above procedures, we concatenate thetwo types of component representations ho =[h2, h4] and feed it into a fully connected layerfor prediction: (cid:98)y = σ(w1ho) , where w1 ∈r2d(cid:48)×1 and σ is the sigmoid function..we use y ∈ r|c| to represent the ground-truthlabel of a document, where yi = 0, 1 denoteswhether label i appears in the document or not.
the proposed model ldgn is trained with themulti-label cross entropy loss:.
l =.
yc log ((cid:98)yc) + (1 − yc) log (1 − (cid:98)yc) ..c(cid:88).
c=1.
3 experiment.
3.1 experimental setup.
tion datasets, which are aapd (yang et al.,2018), eur-lex (mencia and f¨urnkranz, 2008)and rcv1 (lewis et al., 2004).
the statistics ofthese three datasets are listed in table 1..dataset ntrain ntestrcv1aapd 54,840eur-lex 11,585.
23,149 781,265 101 3.18 259.472.41 163.423,954 5.32 1225.2.
1,0003,865.l w.54.l.table 1: statistics of the datasets.
ntrain and ntestdenote the number of training and testing samples re-spectively.
l is the total number of classes, l is theaverage number of labels per sample and w is the av-erage number of words per sample.
evaluation metricfollowing the settings ofprevious work (you et al., 2019; xiao et al., 2019),we use precision at top k (p@k) and normalizeddiscounted cumulated gains at top k (ndcg@k)for performance evaluation.
the deﬁnition of twometrics can be referred to you et al.
(2019).
implementation detailsfor a fair compari-son, we apply the same dataset split as previouswork (xiao et al., 2019), which is also the origi-nal split provided by dataset publisher (yang et al.,2018; mencia and f¨urnkranz, 2008)..the word embeddings in the proposed networkare initialized with the 300-dimensional word vec-tors, which are trained on the datasets by skip-gram (mikolov et al., 2013) algorithm.
the hid-den sizes of bi-lstm and gcns are set to 300and 512, respectively.
we use the adam optimiza-tion method (kingma and ba, 2014) to minimizethe cross-entropy loss, the learning rate is initial-ized to 1e-3 and gradually decreased during theprocess of training.
we select the best parameterconﬁguration based on performance on the valida-tion set and evaluate the conﬁguration on the testset.
our code is available on github1..3.2 baselines.
we compare the proposed model with recentdeep learning based methods for mltc, includingseq2seq models, deep embedding models, and la-bel attention based models.
and it should be notedthat, because of different application scenarios, wedid not choose the label tree-based methods andextreme text focused methods as baseline models..(6).
datasetswe evaluate the proposed modelon three benchmark multi-label text classiﬁca-.
• xml-cnn (liu et al., 2017): a cnn-based.
1https://github.com/makwen1995/ldgn mltc.
3858models.
aapd.
eur-lex.
p@1 p@3 p@5 n@3 n@5.p@1 p@3 p@5 n@3 n@5.xml-cnnsgmdxml.
74.38 53.84 37.79 71.12 75.9375.67 56.75 35.65 72.36 75.3580.54 56.30 39.16 77.23 80.99attentionxml 83.02 58.72 40.56 78.01 82.3183.26 59.77 40.66 79.10 82.7985.28 61.12 41.84 80.84 84.7886.24 61.95 42.29 83.32 86.85.examlsanldgn.
70.40 54.98 44.86 58.62 53.1070.45 60.37 43.88 60.72 55.2475.63 60.13 48.65 63.96 53.6067.34 52.52 47.72 56.21 50.7874.40 61.93 50.98 65.12 59.4379.17 64.99 53.67 68.32 62.4781.03 67.79 56.36 71.81 66.09.table 2: comparisons with state-of-the-art methods on both aapd and eur-lex datasets.
the experimentalresults of all baseline models are directly cited from paper (xiao et al., 2019)..model which uses cnn and a dynamic poolinglayer to extract high-level feature for mltc..• sgm (yang et al., 2018): a sequence generationmodel which models label correlations as an or-dered sequence..• dxml (zhang et al., 2018): a deep embeddingmethod which models the feature space and la-bel graph structure simultaneously..• attentionxml (you et al., 2019): a label tree-based deep learning model which uses a prob-abilistic label tree and multi-label attention tocapture informative words in extreme-scale data..• exam (du et al., 2019): a novel framework thatleverages the label information to compute theword-level interactions..• lsan (xiao et al., 2019): a label-speciﬁc atten-tion network model based on self-attention andlabel-attention mechanism..the sota model (i.e., lsan) used bilstmmodel for text representations.
for a fair compar-ison, we also take bilstm as text encoder in ourmodel..3.3 experimental results and analysis.
table 2 and table 3 demonstrate the performanceof all the compared methods based on the threedatasets.
for fair comparison, the experimentalresults of baseline models are directly cited fromprevious studies (xiao et al., 2019).
we also boldthe best result of each column in all tables..from the table 2 and table 3, we can observethat the proposed ldgn outperforms all other.
models.
rcv1.
p@1 p@3 p@5 n@3 n@5.sgmdxml.
xml-cnn 95.75 78.63 54.94 89.89 90.7795.37 81.36 53.06 91.76 90.6994.04 78.65 54.38 89.83 90.21attentionxml 96.41 80.91 56.38 91.88 92.7093.67 75.80 52.73 86.85 87.7196.81 81.89 56.92 92.83 93.4397.12 82.26 57.29 93.80 95.03.examlsanldgn.
table 3: comparisons with state-of-the-art methods onthe rcv1 dataset.
the experimental results of base-lines are directly cited from (xiao et al., 2019)..baselines on three datasets.
the outstanding re-sults conﬁrm the effectiveness of label-speciﬁc se-mantic interaction learning with dual graph neuralnetwork, which include global statistical patternsand local dynamic relations..it is observed that the performance of xml-cnn is worse than other comparison methods.
the reason is that it only exploits the text contentof documents for classiﬁcation but ignores the la-bel correlations which have been proven very im-portant for multi-label classiﬁcation problem..the label.
tree-based model attentionxmlperforms better than the seq2seq method (sgm)and the deep embedding method (dxml).
al-though both dxml and sgm employ a labelgraph or an ordered sequence to model the rela-tionship between labels, they ignore the interac-tions between labels and document content.
andattentionxml uses multi-label attention whichcan focus on the most relevant parts in content andextract different semantic information for each la-bel..compared with other label attention based.
3859(a) rcv1.
(b) aapd.
(c) eur-lex.
figure 2: performance on tail labels..methods (attentionxml, exam), lsan per-forms the best because ittakes the semanticcorrelations between document content and la-bel text into account simultaneously, which ex-ploits an adaptive fusion to integrate self-attentionand label-attention mechanisms to learn the label-speciﬁc document representation..in conclusion,.
the proposed network ldgnoutperforms sequence-to-sequence models, deepembedding models, and label attention basedmodels, and the metrics p @k and ndcg@kof multi-label text classiﬁcation obtain signiﬁcantimprovement.
speciﬁcally, on the aapd dataset,ldgn increases the p @1 of the lsan method(the best baseline) from 85.28% to 86.24%, andincreases ndcg@3 and ndcg@5 from 80.84%to 83.33%, 84.78% to 86.85% , respectively.
onthe eur-lex dataset, the metric p @1 is boostedfrom 79.17% to 81.03%, and p @5 and ndcg@5are increased from 53.67% to 56.36%, 62.47%to 66.09%, respectively.
on the rcv1 dataset,the p @k of our model increased by 0.3% at av-erage, and ldgn achieves 1% and 1.6% abso-lute improvement on ndcg@3, 5 compared withlsan.
the improvements of the proposed ldgnmodel demonstrate that the semantic interactionlearning with joint global statistical relations andlocal dynamic relations are generally helpful andeffective, and ldgn can capture the deeper cor-relations between categories than lsan..3.4 ablation test.
we perform a series of ablation experiments toexamine the relative contributions of dual graph-based semantic interactions module.
to this end,ldgn is compared with its three variants:(1)s:graph-based semantic interactions only with sta-tistical label co-occurrence; (2)d: graph-based se-mantic interactions only with dynamic reconstruc-(3)no-g:removing the dual graphtion graph;.
(a) aapd.
(b) eur-lex.
figure 3: ablation test of ldgn on two datasets..neural network.
for a fair comparison, both s andd use 4-layer gcn which is the same as ldgn.
as presented in figure 3, s and d perform bet-ter than no-g, which demonstrates that exploringeither statistical relations or dynamic relations cancorrectly capture the effective semantic interac-tions between label-speciﬁc components.
d per-forms better than s, indicating the model with lo-cal dynamic relations is adaptive to data and hasbetter stability and robustness, which also showsthat the model with local dynamic relations cancapture semantic dependencies more effectivelyand accurately.
the performance of s+d (i.e.,ldgn) combining two aspect relations obtainssigniﬁcant improvement, which shows dynamicrelations can make up the deﬁciency of statisticalco-occurrence and correct the bias of global corre-lations.
thus, it is necessary to explore their jointeffects to further boost the performance..3.5 performance on tail labels.
in order to prove the effectiveness of the proposedldgn in alleviating the tail labels problem, weevaluate the performance of ldgn by propensityscored precision at k (psp@k), which is calcu-.
3860707580859095psp@1psp@3psp@5lsan   ldgn747678808284868890psp@1psp@3psp@5lsan ldgn4244464850525456psp@1psp@3psp@5lsan ldgn82838485868788p@1n@5sdno-gs+d606570758085p@1n@5sdno-gs+dfigure 4: the visualization of label attention weights.
(the attention weights of ’physics.soc’ for words are shadedin blue, and the attention scores of class cs.cy and cs.ce are shaded in green and yellow color respectively.
darker color represents higher weight score.).
lated as follow:.
p sp @k =.
k(cid:88).
1k.yrank(l)prank(l).
,.
(7).
l=1where prank(l) is the propensity score (jainet al., 2016) of label rank(l).
figure 2 shows theresults of ldgn and lsan on three datasets..as shown in figure 2(a), figure 2(b) and fig-the proposed ldgn performs betterure 2(c),in predicting tail labels than the lsan model(the best baseline) on three datasets.
specif-ically, on the rcv1 dataset, ldgn achieves0.97% and 1.35% absolute improvement in termof p sp @3 and p sp @5 compared with lsan.
on the aapd dataset, the p sp @k increased byat least 0.63% up to 0.90%.
and on the eur-lexdataset, ldgn achieves 1.94%, 3.64% and 4.93%absolute improvement on p sp @1, 3, 5 comparedwith lsan.
the reason for the improvement inthe eur-lex dataset is more obvious is that thesemantic interactions learning is more useful tocapture related information in the case of a largenumber of labels..the results prove that ldgn can effectively al-.
leviate the problem of predicting tail labels..3.6 case study.
to further verify the effectiveness of our label at-tention module and dual graph neural network inldgn, we present a typical case and visualizethe attention weights on the document words andthe similarity scores between label-speciﬁc com-ponents.
we show a test sample from originalaapd dataset, and the document belongs to threecategories, ‘physics and society’ (physics.soc),‘computers and society’ (cs.cy) and ‘computa-tional engineering, finance, and science’ (cs.ce).
visualization of attentionwe can observefrom the figure 4 that different labels focus ondifferent parts in the document text, and each la-bel has its own concerned words.
for example,.
figure 5: the visualization of two adjacency matricesof dual gnn.
darker color represents higher weight..the more important parts in the ‘physics.soc’ cate-gory are ‘digitalization power grid’, ‘energy man-agement’.
and the words that the ‘cs.ce’ cate-gory focuses on are ‘consuming systems’, ‘vary-ing prices’, ‘laying foundations’, ‘lower ’ and etc.
for class ‘cs.cy’, the concerned words are ‘sam-ples dutch distribution’, ‘evolutions’ and ‘topolo-gies’.
the corresponding related words of thethree categories can reﬂect the semantics of thecategories.
visualization of interactionsto gain a clearerview of the importance of our dual graph-basedinteractions learning module, we display two.
3861smart grid digitalization power grid visionary acceptation model energy management users engaged producing energy consuming systems aware energy demand response network dynamically varying prices natural question smart grid reality distribution grid updated assume positive answer question lower layers medium low voltage change previous analyzed samples dutch distribution grid previous considered evolutions synthetic topologies modeled studies complex systems technological domains previous paper extra step defining methodology evolving existing physical power grid smart grid model laying foundations decision support system utilities governmental organizations evolution strategies apply dutch distribution gridheatmaps in figure 5 to visualize the partial graphstructure of dual gcn.
the edge weights shownin the heatmaps are obtained by global label co-occurrence and local dynamic relations (i.e., com-puted by equation 5), respectively..as presented in heatmaps, different relationsbetween categories are captured by dual gcn.
inglobal statistical relations, ‘cs.cy’ is highly linked‘nlin.ao’,with ‘physics.soc’ and wrong labelwhile the true label ‘cs.ce’ is isolated.
and in lo-cal dynamic relations, ‘cs.cy’ is more related to‘cs.ce’, and the correlations between wrong label‘nlin.ao’ and true labels are reduced.
this demon-strates that local dynamic relations can capture thelatent relations that do not appear in global rela-tions, and correct the bias of global correlations..4 related work.
multi-label text classiﬁcationthe existingmethods for mltc mainly focus on learning en-hanced document representation (liu et al., 2017)and modeling label dependency (nam et al., 2017;yang et al., 2018; tsai and lee, 2019) to improvethe classiﬁcation performance..with the wide application of neural networkmethods for text representation, some innova-tive models have been developed for this task,which include traditional deep learning methodsand seq2seq based methods.
liu et al.
(2017)employed cnns and dynamic pooling to learnthe text representation for mltc.
however, theytreated all words equally and cannot explored theinformative words in documents.
the seq2seqmethods, such as mlc2seq (nam et al., 2017)and sgm (yang et al., 2018), employed a rnnto encode the input text and an attention basedrnn decoder to generate predicted labels se-quentially.
although they used attention mecha-nism to capture the informative words in text con-tent, these models cannot distinguish similar la-bels well.
there is a big reason that most of themneglect the semantic connections between labelsand document, and learn the same document rep-resentations for different labels..recently, some studies (you et al., 2019; xiaoet al., 2019; du et al., 2019) have used atten-tion mechanism to explore the interactions be-tween words and labels, and learned a label-speciﬁc document representation for classiﬁca-tion.
these methods have obtained promising re-sults in mltc, which shows the importance of ex-.
ploring semantic connections.
however, they didnot further study the interactions between label-speciﬁc semantic components which can help topredict low-frequency labels..to handle these issues, a common way to ex-plore the semantic interactions between label-speciﬁc parts in document, is to utilize the labelgraph based on statistical co-occurrences.
mlc with label graphin order to capturethe deep correlations of labels in a graph struc-ture, many researches in image classiﬁcation applynode embedding and graph neural network modelsto the task of multi-label image classiﬁcation.
leeet al.
(2018) incorporated knowledge graphs fordescribing the relationships between labels, andthe information propagation can model the de-pendencies between seen and unseen labels formulti-label zero-shot learning.
chen et al.
(2019)learned label representations with prior label cor-relation matrix in gcn, and mapped the label rep-resentations to inter-dependent classiﬁers, whichachieved superior performance..however,.
there were few related approachestext.
zhangfor multi-label classiﬁcation ofet al.
(2018) established an explicitlabel co-occurrence graph to explore label embedding inlow-dimension latent space..furthermore,.
the statistical label correlationsobtained by training data are incomplete andnoisy.
and the co-occurrence patterns between la-bel pairs may form a long-tail distribution..thus, our goal is to ﬁnd a way to explore thecomplete and adaptive interactions among label-speciﬁc semantic components more accurately..5 conclusion.
in this paper, we propose a graph-based networkldgn to capture the semantic interactions re-lated to corresponding labels, which jointly ex-ploits global statistical patterns and local dynamicrelations to derive complete and adaptive depen-dencies between different label-speciﬁc semanticparts.
we ﬁrst exploit multi-label attention to ex-tract the label-speciﬁc semantic components fromdocuments.
then, we employ gcn to learn com-ponent representations using label co-occurrencesto guide the information propagation among com-ponents.
after that, we use the learned componentrepresentations to compute the adjacency graphdynamically and re-learn with gcn based on thereconstruction graph.
extensive experiments con-.
3862ducted on three public datasets show that the pro-posed ldgn model outperforms other state-of-the-art models on multi-label text classiﬁcationtask and also demonstrates much higher effective-ness to alleviate the tail label problem.
in the fu-ture, we will improve the proposed model in efﬁ-ciency, for example we could construct a dynamicgraph for a few samples rather than each sample.
and besides, we will explore more informationabout labels for mlc classiﬁcation..acknowledgement.
we gratefully thank the anonymous reviewers fortheir insightful comments.
this research is sup-ported by the strategic priority research programof the chinese academy of sciences under grantno.
xdc02060400..references.
zhao-min chen, xiu-shen wei, peng wang, and yan-wen guo.
2019. multi-label image recognition withgraph convolutional networks.
in proceedings of theieee conference on computer vision and patternrecognition, pages 5177–5186..cunxiao du, zhaozheng chen, fuli feng, lei zhu,tian gan, and liqiang nie.
2019. explicit inter-in pro-action model towards text classiﬁcation.
ceedings of the aaai conference on artiﬁcial intel-ligence, volume 33, pages 6359–6366..mohammed jabreel and antonio moreno.
2019. adeep learning-based approach for multi-label emo-applied sciences,tion classiﬁcation in tweets.
9(6):1123..himanshu jain, yashoteja prabhu, and manik varma.
2016. extreme multi-label loss functions for rec-ommendation, tagging, ranking & other missing la-bel applications.
in proceedings of the 22nd acmsigkdd international conference on knowledgediscovery and data mining, pages 935–944..diederik p kingma and jimmy ba.
2014. adam: amethod for stochastic optimization.
arxiv preprintarxiv:1412.6980..thomas n kipf and max welling.
2017..semi-supervised classiﬁcation with graph convolutionalnetworks.
in international conference on learningrepresentations (iclr)..chung-wei lee, wei fang, chih-kuan yeh, and yu-chiang frank wang.
2018. multi-label zero-shotlearning with structured knowledge graphs.
in pro-ceedings of the ieee conference on computer visionand pattern recognition, pages 1576–1585..david d lewis, yiming yang, tony g rose, and fanli.
2004. rcv1: a new benchmark collection forjournal of machinetext categorization research.
learning research, 5(apr):361–397..xin li, haoran xie, yanghui rao, yanjia chen,xuebo liu, huan huang, and fu lee wang.
2016.weighted multi-label classiﬁcation model for senti-ment analysis of online news.
in 2016 internationalconference on big data and smart computing (big-comp), pages 215–222.
ieee..jingzhou liu, wei-cheng chang, yuexin wu, andyiming yang.
2017. deep learning for extrememulti-label text classiﬁcation.
in proceedings of the40th international acm sigir conference on re-search and development in information retrieval,pages 115–124..shuhua monica liu and jiun-hung chen.
2015. amulti-label classiﬁcation based approach for senti-ment classiﬁcation.
expert systems with applica-tions, 42(3):1083–1093..andrew l maas, awni y hannun, and andrew y ng.
2013. rectiﬁer nonlinearities improve neural net-in proc.
icml, volume 30,work acoustic models.
page 3..eneldo loza mencia and johannes f¨urnkranz.
2008.efﬁcient pairwise multilabel classiﬁcation for large-in jointscale problems in the legal domain.
european conference on machine learning andknowledge discovery in databases, pages 50–65.
springer..tomas mikolov, ilya sutskever, kai chen, greg s cor-rado, and jeff dean.
2013. distributed representa-tions of words and phrases and their compositional-in advances in neural information processingity.
systems, pages 3111–3119..jinseok nam, eneldo loza menc´ıa, hyunwoo j kim,and johannes f¨urnkranz.
2017. maximizing subsetaccuracy with recurrent neural networks in multi-label classiﬁcation.
in advances in neural informa-tion processing systems, pages 5413–5423..che-ping tsai and hung-yi lee.
2019. order-freelearning alleviating exposure bias in multi-labelclassiﬁcation.
arxiv preprint arxiv:1909.03434..yaqi wang, shi feng, daling wang, ge yu, and yifeizhang.
2016. multi-label chinese microblog emo-tion classiﬁcation via convolutional neural network.
in asia-paciﬁc web conference, pages 567–580.
springer..lin xiao, xin huang, boli chen, and liping jing.
2019. label-speciﬁc document representation formulti-label text classiﬁcation.
in proceedings of the2019 conference on empirical methods in natu-ral language processing and the 9th internationaljoint conference on natural language processing(emnlp-ijcnlp), pages 466–475..3863pengcheng yang, xu sun, wei li, shuming ma, weiwu, and houfeng wang.
2018. sgm: sequence gen-eration model for multi-label classiﬁcation.
arxivpreprint arxiv:1806.04822..ronghui you, zihan zhang, ziye wang, suyang dai,hiroshi mamitsuka, and shanfeng zhu.
2019. at-tentionxml: label tree-based attention-aware deepmodel for high-performance extreme multi-labeltext classiﬁcation.
in advances in neural informa-tion processing systems, pages 5820–5830..wenjie zhang, junchi yan, xiangfeng wang, andhongyuan zha.
2018. deep extreme multi-labellearning.
in proceedings of the 2018 acm on inter-national conference on multimedia retrieval, pages100–107..3864