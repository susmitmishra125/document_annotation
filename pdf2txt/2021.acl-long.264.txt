consistency regularization for cross-lingual fine-tuning.
bo zheng†∗, li dong‡, shaohan huang‡, wenhui wang‡, zewen chi‡∗saksham singhal‡, wanxiang che†, ting liu†, xia song‡, furu wei‡†harbin institute of technology‡microsoft corporation{bzheng,car,tliu}@ir.hit.edu.cn{lidong1,shaohanh,wenwan,saksingh,xiaso,fuwei}@microsoft.com.
abstract.
fine-tuning pre-trained cross-lingual languagemodels can transfer task-speciﬁc supervisionfrom one language to the others.
in this work,we propose to improve cross-lingual ﬁne-tuning with consistency regularization.
specif-ically, we use example consistency regulariza-tion to penalize the prediction sensitivity tofour types of data augmentations,i.e., sub-word sampling, gaussian noise, code-switchsubstitution, and machine translation.
in ad-dition, we employ model consistency to regu-larize the models trained with two augmentedversions of the same training set.
experimen-tal results on the xtreme benchmark showthat our method1 signiﬁcantly improves cross-lingual ﬁne-tuning across various tasks, includ-ing text classiﬁcation, question answering, andsequence labeling..1.introduction.
pre-trained cross-lingual language models (con-neau and lample, 2019; conneau et al., 2020a;chi et al., 2020) have shown great transferabilityacross languages.
by ﬁne-tuning on labeled datain a source language, the models can generalize toother target languages, even without any additionaltraining.
such generalization ability reduces therequired annotation efforts, which is prohibitivelyexpensive for low-resource languages..recent work has demonstrated that data aug-mentation is helpful for cross-lingual transfer, e.g.,translating source language training data into targetlanguages (singh et al., 2019), and generating code-switch data by randomly replacing input words inthe source language with translated words in tar-get languages (qin et al., 2020).
by populatingthe dataset, their ﬁne-tuning still treats training.
∗contribution during internship at microsoft research.
1the code is available at https://github.com/.
bozheng-hit/xtune..instances independently, without considering theinherent correlations between the original input andits augmented example.
in contrast, we propose toutilize consistency regularization to better leveragedata augmentation for cross-lingual ﬁne-tuning.
in-tuitively, for a semantic-preserving augmentationstrategy, the predicted result of the original inputshould be similar to its augmented one.
for ex-ample, the classiﬁcation predictions of an englishsentence and its translation tend to remain consis-tent..in this work, we introduce a cross-lingual ﬁne-tuning method xtune that is enhanced by con-sistency regularization and data augmentation.
first, example consistency regularization enforcesthe model predictions to be more consistent forsemantic-preserving augmentations.
the regular-izer penalizes the model sensitivity to different sur-face forms of the same example (e.g., texts writtenin different languages), which implicitly encour-ages cross-lingual transferability.
second, we in-troduce model consistency to regularize the mod-els trained with various augmentation strategies.
speciﬁcally, given two augmented versions of thesame training set, we encourage the models trainedon these two datasets to make consistent predic-tions for the same example.
the method enforcesthe corpus-level consistency between the distribu-tions learned by two models..under the proposed ﬁne-tuning framework, westudy four strategies of data augmentation, i.e., sub-word sampling (kudo, 2018), code-switch substi-tution (qin et al., 2020), gaussian noise (agha-janyan et al., 2020), and machine translation.
weevaluate xtune on the xtreme benchmark (huet al., 2020), including three different tasks onseven datasets.
experimental results show thatour method outperforms conventional ﬁne-tuningwith data augmentation.
we also demonstratethat xtune is ﬂexible to be plugged in various.
proceedingsofthe59thannualmeetingoftheassociationforcomputationallinguisticsandthe11thinternationaljointconferenceonnaturallanguageprocessing,pages3403–3417august1–6,2021.©2021associationforcomputationallinguistics3403tasks, such as classiﬁcation, span extraction, andsequence labeling..we summarize our contributions as follows:.
the target language.
besides, qin et al.
(2020) ﬁne-tuned models on multilingual code-switch data,which achieves considerable improvements..• we propose xtune, a cross-lingual ﬁne-tuning method to better utilize data augmenta-tions based on consistency regularization..• we study four types of data augmentationsthat can be easily plugged into cross-lingualﬁne-tuning..• we give instructions on how to apply xtuneto various downstream tasks, such as classiﬁ-cation, span extraction, and sequence labeling..• we conduct extensive experiments to showthat xtune consistently improves the perfor-mance of cross-lingual ﬁne-tuning..2 related work.
cross-lingual transfer besides learning cross-lingual word embeddings (mikolov et al., 2013;faruqui and dyer, 2014; guo et al., 2015; xuet al., 2018; wang et al., 2019), most recent workof cross-lingual transfer is based on pre-trainedcross-lingual language models (conneau and lam-ple, 2019; conneau et al., 2020a; chi et al., 2020).
these models generate multilingual contextualizedword representations for different languages with ashared encoder and show promising cross-lingualtransferability..cross-lingual data augmentation machinetranslation has been successfully applied to thecross-lingual scenario as data augmentation.
acommon way to use machine translation is to ﬁne-tune models on both source language training dataand translated data in all target languages.
further-more, singh et al.
(2019) proposed to replace a seg-ment of source language input text with its transla-tion in another language.
however, it is usually im-possible to map the labels in source language datainto target language translations for token-leveltasks.
zhang et al.
(2019) used code-mixing to per-form the syntactic transfer in cross-lingual depen-dency parsing.
fei et al.
(2020) constructed pseudotranslated target corpora from the gold-standard an-notations of the source languages for cross-lingualsemantic role labeling.
fang et al.
(2020) pro-posed an additional kullback-leibler divergenceself-teaching loss for model training, based on auto-generated soft pseudo-labels for translated text in.
consistency regularization one strand ofwork in consistency regularization focused on reg-ularizing model predictions to be invariant to smallperturbations on image data.
the small perturba-tions can be random noise (zheng et al., 2016),adversarial noise (miyato et al., 2019; carmonet al., 2019) and various data augmentation ap-proaches (hu et al., 2017; ye et al., 2019; xie et al.,2020).
similar ideas are used in the natural lan-guage processing area.
both adversarial noise (zhuet al., 2020; jiang et al., 2020; liu et al., 2020) andsampled gaussian noise (aghajanyan et al., 2020)are adopted to augment input word embeddings.
another strand of work focused on consistencyunder different model parameters (tarvainen andvalpola, 2017; athiwaratkun et al., 2019), which iscomplementary to the ﬁrst strand.
we focus on thecross-lingual setting, where consistency regulariza-tion has not been fully explored..3 methods.
conventional cross-lingual ﬁne-tuning trains a pre-trained language model on the source language anddirectly evaluates it on other languages, which isalso known as the setting of zero-shot cross-lingualﬁne-tuning.
speciﬁcally, given a training corpus din the source language (typically in english), and amodel f (·; θ) that predicts task-speciﬁc probabilitydistributions, we deﬁne the loss of cross-lingualﬁne-tuning as:.
ltask(d, θ) =.
(cid:96)(f (x; θ), g(x)),.
(cid:88).
x∈d.
where g(x) denotes the ground-truth label of ex-ample x, (cid:96)(·, ·) is the loss function depending onthe downstream task..apart from vanilla cross-lingual ﬁne-tuning onthe source language, recent work shows that dataaugmentation is helpful to improve performanceon the target languages.
for example, conneauand lample (2019) add translated examples to thetraining set for better cross-lingual transfer.
leta(·) be a cross-lingual data augmentation strategy(such as code-switch substitution), and da = d ∪{a(x) | x ∈ d} be the augmented training corpus,the ﬁne-tuning loss is ltask(da, θ).
notice that it isnon-trivial to apply some augmentations for token-level tasks directly.
for instance, in part-of-speech.
3404figure 1: overview of our two-stage ﬁne-tuning algorithm.
the model parameters f (·; θ∗) in the second stage arecopied from the ﬁrst stage..tagging, the labels of source language examples cannot be mapped to the translated examples becauseof the lack of explicit alignments..3.1 xtune: cross-lingual fine-tuning with.
consistency regularization.
we propose to improve cross-lingual ﬁne-tuningwith two consistency regularization methods, sothat we can effectively leverage cross-lingual dataaugmentations..3.1.1 example consistency regularizationin order to encourage consistent predictions for anexample and its semantically equivalent augmenta-tion, we introduce example consistency regulariza-tion, which is deﬁned as follows:.
r1(d, θ, a) =.
kls(f (x; θ)(cid:107)f (a(x); θ)),.
(cid:88).
x∈d.
kls(p, q) = kl(stopgrad(p )(cid:107)q)+.
kl(stopgrad(q)(cid:107)p ).
where kls(·) is the symmertrical kullback-leiblerdivergence.
the regularizer encourages the pre-dicted distributions f (x; θ) and f (a(x); θ) toagree with each other.
the stopgrad(·) operation2is used to stop back-propagating gradients, whichis also employed in (jiang et al., 2020; liu et al.,2020).
the ablation studies in section 4.2 empiri-cally show that the operation improves ﬁne-tuningperformance..2implemented by .detach() in pytorch..3.1.2 model consistency regularizationwhile the example consistency regularization isconducted at the example level, we propose themodel consistency to further regularize the modeltraining at the corpus level.
the regularization isconducted at two stages.
first, we obtain a ﬁne-tuned model θ∗ on the training corpus d:.
θ∗ = arg min.
ltask(d, θ1)..θ1.
in the second stage, we keep the parameters θ∗ﬁxed.
the regularization term is deﬁned as:(cid:88).
kl(f (x; θ∗)(cid:107)f (x; θ)).
r2(da, θ, θ∗) =.
x∈da.
where da is the augmented training corpus, andkl(·) is kullback-leibler divergence.
for each ex-ample x of the augmented training corpus da, themodel consistency regularization encourages theprediction f (x; θ) to be consistent with f (x; θ∗).
the regularizer enforces the corpus-level consis-tency between the distributions learned by twomodels..an unobvious advantage of model consistencyregularization is the ﬂexibility with respect to dataaugmentation strategies.
for the example of part-of-speech tagging, even though the labels can notbe directly projected from an english sentence toits translation, we are still able to employ the reg-ularizer.
because the term r2 is put on the sameexample x ∈ da, we can always align the token-level predictions of the models θ and θ∗..3405cross-lingual language model, while it applies sub-word tokenization directly on raw text data usingsentencepiece (kudo and richardson, 2018) witha unigram language model (kudo, 2018).
as oneof our data augmentation strategies, we apply theon-the-ﬂy subword sampling algorithm in the uni-gram language model to generate multiple subwordsequences..3.2.2 gaussian noise.
most data augmentation strategies in nlp changeinput text discretely, while we directly add randomperturbation noise sampled from gaussian distribu-tion on the input embedding layer to conduct dataaugmentation.
when combining this data augmen-tation with example consistency r1, the method issimilar to the stability training (zheng et al., 2016),random perturbation training (miyato et al., 2019)and the r3f method (aghajanyan et al., 2020).
wealso explore gaussian noise’s capability to gener-ate new examples on continuous input space forconventional ﬁne-tuning..3.2.3 code-switch substitution.
anchor points have been shown useful to improvecross-lingual transferability.
conneau et al.
(2020b)analyzed the impact of anchor points in pre-trainingcross-lingual language models.
following qin et al.
(2020), we generate code-switch data in multiplelanguages as data augmentation.
we randomly se-lect words in the original text in the source lan-guage and replace them with target language wordsin the bilingual dictionaries to obtain code-switchdata.
intuitively, this type of data augmentationexplicitly helps pre-trained cross-lingual modelsalign the multilingual vector space by the replacedanchor points..3.2.4 machine translation.
figure 2: cross-lingual data augmentation strategies..3.1.3 full xtune fine-tuningas shown in figure 1, we combine example con-sistency regularization r1 and model consistencyregularization r2 as a two-stage ﬁne-tuning pro-cess.
formally, we ﬁne-tune a model with r1 inthe ﬁrst stage:.
θ∗ = arg min.
ltask(d, θ1) + r1(d, θ1, a∗).
θ1.
where the parameters θ∗ are kept ﬁxed for r2 inthe second stage.
then the ﬁnal loss is computedvia:.
lxtune = ltask(da, θ).
+ λ1r1(da, θ, a(cid:48))+ λ2r2(da, θ, θ∗).
where λ1 and λ2 are the corresponding weightsof two regularization methods.
notice that thedata augmentation strategies a, a(cid:48), and a∗ canbe either different or the same, which are tuned ashyper-parameters..3.2 data augmentation.
we consider four types of data augmentation strate-gies in this work, which are shown in figure 2. weaim to study the impact of different data augmenta-tion strategies on cross-lingual transferability..3.2.1 subword samplingrepresenting a sentence in different subword se-quences can be viewed as a data augmentation strat-egy (kudo, 2018; provilkov et al., 2020).
we utilizexlm-r (conneau et al., 2020a) as our pre-trained.
machine translation has been proved to be an ef-fective data augmentation strategy (singh et al.,2019) under the cross-lingual scenario.
however,the ground-truth labels of translated data can beunavailable for token-level tasks (see section 3),which disables conventional ﬁne-tuning on the aug-mented data.
meanwhile, our proposed model con-sistency r2 can not only serve as consistency regu-larization but also can be viewed as a self-trainingobjective to enable semi-supervised training on theunlabeled target language translations..3406i love to eat apples.ich mag es, äpfel zu essenj'adoremanger des pommes.我喜欢吃苹果。_i/_love/_to/_eat/_apple/s/._i/_love/_to/_e/a/t/_app/l/es/._/i/_lo/ve/_to/_e/at/_app/l/es/.i 喜欢to essenapples.i liebeto eat 苹果.ich喜欢to eat pommes.gaussian noisemachinetranslationcode-switchsubwordsamplingembedding layeri love to eat apples.
3.3 task adaptation.
we give instructions on how to apply xtune tovarious downstream tasks, i.e., classiﬁcation, spanextraction, and sequence labeling.
by default, weuse model consistency r2 in full xtune.
wedescribe the usage of example consistency r1 asfollows..3.3.1 classiﬁcation.
for classiﬁcation task, the model is expected topredict one distribution per example on nlabel types,i.e., model f (·; θ) should predict a probability dis-tribution pcls ∈ rnlabel.
thus we can directly useexample consistency r1 to regularize the consis-tency of the two distributions for all four types ofour data augmentation strategies..3.3.2 span extraction.
for span extraction task, the model is expected topredict two distributions per example pstart, pend ∈rnsubword, indicating the probability distribution ofwhere the answer span starts and ends, nsubword de-notes the length of the tokenized input text.
forgaussian noise, the subword sequence remains un-changed so that example consistency r1 can bedirectly applied to the two distributions.
since sub-word sampling and code-switch substitution willchange nsubword, we control the ratio of words tobe modiﬁed and utilize example consistency r1on unchanged positions only.
we do not use theexample consistency r1 for machine translationbecause it is impossible to explicitly align the twodistributions..3.3.3 sequence labeling.
recent pre-trained language models generate rep-resentations at the subword-level.
for sequencelabeling tasks, these models predict label distribu-tions on each word’s ﬁrst subword.
therefore, themodel is expected to predict nword probability dis-tributions per example on nlabel types.
unlike spanextraction, subword sampling, code-switch substi-tution, and gaussian noise do not change nword.
thus the three data augmentation strategies willnot affect the usage of example consistency r1.
although word alignment is a possible solutionto map the predicted label distributions betweentranslation pairs, the word alignment process willintroduce more noise.
therefore, we do not employmachine translation as data augmentation for theexample consistency r1..4 experiments.
4.1 experiment setup.
datasets for our experiments, we select threetypes of cross-lingual understanding tasks fromxtreme benchmark (hu et al., 2020), includingtwo classiﬁcation datasets: xnli (conneau et al.,2018), paws-x (yang et al., 2019), three span ex-traction datasets: xquad (artetxe et al., 2020),mlqa (lewis et al., 2020), tydiqa-goldp (clarket al., 2020), and two sequence labeling datasets:ner (pan et al., 2017), pos (nivre et al., 2018).
the statistics of the datasets are shown in the sup-plementary document..fine-tuning settings we consider two typicalﬁne-tuning settings from conneau et al.
(2020a)and hu et al.
(2020) in our experiments, whichare (1) cross-lingual transfer: the models are ﬁne-tuned on english training data without translationavailable, and directly evaluated on different tar-get languages; (2) translate-train-all: translation-based augmentation is available, and the models areﬁne-tuned on the concatenation of english trainingdata and its translated data on all target languages.
since the ofﬁcial xtreme repository3 does notprovide translated target language data for pos andner, we use google translate to obtain transla-tions for these two datasets..language model..implementation details we utilize xlm-r (conneau et al., 2020a) as our pre-trainedcross-lingualthe bilingualdictionaries we used for code-switch substitutionare from muse (lample et al., 2018).4 forlanguages that cannot be found in muse, weignore these languages since other bilingualdictionaries might be of poorer quality.
for thepos dataset, we use the average-pooling strategyon subwords to obtain word representation sincepart-of-speech is related to different parts ofwords, depending on the language.
we tune thehyper-parameter and select the model with the bestaverage results over all the languages’ developmentset.
there are two datasets without developmentset in multi-languages.
for xquad, we tunethe hyper-parameters with the development setof mlqa since they share the same training setand have a higher degree of overlap in languages.
for tydiqa-goldp, we use the english test set.
3github.com/google-research/xtreme4github.com/facebookresearch/muse.
3407model.
metrics.
pair sentence.
structure prediction.
question answering.
xnli paws-x pos.
acc..acc..f1.
ner.
f1.
xquad mlqa.
tydiqa.
f1/em.
f1/em.
f1/em avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
mbertxlmx-stilts (phang et al., 2020)veco (luo et al., 2020)xlm-rlargextune.
veco (luo et al., 2020)filter (fang et al., 2020)xlm-rlargextune.
65.469.180.479.979.282.6.
83.083.982.684.8.
81.980.987.788.786.489.8.
91.191.490.491.6.
70.370.174.475.172.678.5.
75.176.2-79.3.
62.261.263.465.765.469.3.
65.767.7-69.9.
64.5/49.4 61.4/44.2 59.7/43.959.8/44.3 48.5/32.6 43.6/29.177.2/61.3 72.3/53.5 76.0/59.577.3/61.8 71.7/53.2 67.6/49.176.6/60.8 71.6/53.2 65.1/45.079.4/64.4 74.4/56.2 74.8/59.4.
79.9/66.3 73.1/54.9 75.0/58.982.4/68.0 76.2/57.7 68.3/50.980.2/65.9 72.8/54.3 66.5/47.782.5/69.0 75.0/57.1 75.4/60.8.
63.158.672.371.470.074.9.
74.174.4-76.5.translate-train-all (translation-based augmentation is available for english training data).
table 1: evaluation results on the xtreme benchmark.
results of mbert (devlin et al., 2019), xlm (conneauand lample, 2019) and xlm-rlarge (conneau et al., 2020a) are taken from (hu et al., 2020).
results of xlm-rlargeunder the translate-train-all setting are from filter (fang et al., 2020).
the results of xtune are from the bestmodels selected with the performance on the corresponding development set..model.
metrics.
pair sentence.
structure prediction.
question answering.
xnli paws-x pos.
acc..acc..f1.
xquad mlqa.
tydiqa.
f1/em.
f1/em.
f1/em.
cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
translate-train-all (translation-based augmentation is available for english training data).
xlm-rbasextunewith only example consistency r1with only model consistency r2.
xlm-rbasextunewith only example consistency r1with only model consistency r2remove stopgrad in r1.
74.977.777.676.6.
78.880.680.578.980.2.
84.987.587.286.3.
88.489.489.388.589.1.
75.676.576.376.3.
-77.8-76.676.8.
71.9/56.4 65.0/47.1 55.4/38.373.9/59.0 68.1/50.2 61.2/45.273.6/58.6 67.6/49.7 60.7/44.473.2/58.1 66.7/49.0 59.2/42.3.
75.2/61.4 67.8/50.1 63.7/47.778.1/64.4 69.7/52.1 65.9/51.176.1/62.5 69.1/51.6 65.1/50.377.4/63.4 68.7/51.1 64.5/48.777.3/63.4 69.9/52.1 65.1/50.5.
ner.
f1.
61.863.062.463.0.
-63.7-63.563.4.table 2: ablation studies on the xtreme benchmark.
all numbers are averaged over ﬁve random seeds..as the development set.
in order to make a faircomparison, the ratio of data augmentation in dais all set to 1.0. the detailed hyper-parameters areshown in the supplementary document..4.2 results.
table 1 shows our results on xtreme.
for thecross-lingual transfer setting, we outperform pre-vious works on all seven cross-lingual languageunderstanding datasets.5 compared to xlm-rlargebaseline, we achieve an absolute 4.9-point improve-ment (70.0 vs. 74.9) on average over seven datasets.
for the translate-train-all setting, we achieved state-of-the-art results on six of the seven datasets.
com-.
pared to filter,6 we achieve an absolute 2.1-point improvement (74.4 vs. 76.5), and we donot need english translations during inference..table 2 shows how the two regularization meth-ods affect the model performance separately.
forthe cross-lingual transfer setting, xtune achievesan absolute 2.8-point improvement compared toour implemented xlm-rbase baseline.
meanwhile,ﬁne-tuning with only example consistency r1 andmodel consistency r2 degrades the averaged re-sults by 0.4 and 1.0 points, respectively..for the translate-train-all setting, our proposedmodel consistency r2 enables training on pos andner even if labels of target language translations.
5x-stilts (phang et al., 2020) uses additional squadv1.1 english training data for the tydiqa-goldp dataset,while we prefer a cleaner setting here..6filter directly selects the best model on the test setof xquad and tydiqa-goldp.
under this setting, we canobtain 83.1/69.7 for xquad, 75.5/61.1 for tydiqa-goldp..3408model.
en.
ar.
bg.
de.
el.
es.
fr.
hi.
ru.
sw th.
tr.
ur.
vi.
zh avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
r3f (aghajanyan et al., 2020) 89.4 80.6 84.6 83.7 83.6 85.1 84.2 77.3 82.3 72.6 79.4 80.7 74.2 81.1 80.1 81.2r4f (aghajanyan et al., 2020) 89.6 80.5 84.6 84.2 83.6 85.2 84.7 78.2 82.5 72.7 79.2 80.3 73.9 80.9 80.6 81.488.7 77.2 83.0 82.5 80.8 83.7 82.2 75.6 79.1 71.2 77.4 78.0 71.7 79.3 78.2 79.2xlm-rlarge89.6 81.6 85.9 84.8 84.3 86.5 85.4 80.5 82.8 73.3 80.3 82.1 77.1 83.0 82.3 82.6xtune.
translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020)xlm-rlargextune.
89.5 83.6 86.4 85.6 85.4 86.6 85.7 81.1 83.7 78.7 81.7 83.2 79.1 83.9 83.8 83.988.6 82.2 85.2 84.5 84.5 85.7 84.2 80.8 81.8 77.0 80.2 82.1 77.7 82.6 82.7 82.689.9 84.0 87.0 86.5 86.2 87.4 86.6 83.2 85.2 80.0 82.7 84.1 79.6 84.8 84.3 84.8.table 3: xnli accuracy scores for each language.
xlm-rlarge under the cross-lingual transfer setting are from (huet al., 2020).
results of xlm-rlarge under the translate-train-all setting are from (fang et al., 2020)..method.
model.
xnli.
pos mlqa.
model.
tatoeba bucc.
baseline.
xlm-rbase.
subwordsampling.
gaussiannoise.
code-switch.
machinetranslation.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
74.9.
75.376.575.8.
74.776.375.5.
76.577.676.8.
78.879.778.9.
75.6.
75.876.376.3.
75.675.776.2.
75.175.876.1.
--76.6.
65.0/47.1.
64.7/46.767.4/49.566.7/49.0.
64.2/46.166.7/48.966.3/48.5.
63.8/45.967.6/49.766.3/48.6.
67.8/50.1-68.7/51.1.
table 4: comparison between different data augmen-tation strategies.
“data aug.” uses data augmentationfor conventional ﬁne-tuning.
“xtuner1” denotes ﬁne-tuning with only example consistency r1.
“xtuner2 ”denotes ﬁne-tuning with only model consistency r2..are unavailable in these two datasets.
to make afair comparison in the translate-train-all setting, weaugment the english training corpus with target lan-guage translations when ﬁne-tuning with only ex-ample consistency r1.
otherwise, we only use theenglish training corpus in the ﬁrst stage, as shownin figure 1(a).
compared to xtune, the perfor-mance drop on two classiﬁcation datasets under thissetting is relatively small since r1 can be directlyapplied between translation-pairs in any languages.
however, the performance is signiﬁcantly degradedin three question answering datasets, where wecan not align the predicted distributions betweentranslation-pairs in r1.
we use subword samplingas the data augmentation strategy in r1 for thissituation.
fine-tuning with only model consistencyr2 degrades the overall performance by 1.1 points.
these results demonstrate that the two consistencyregularization methods complement each other.
be-.
xlm-rbase (cross-lingual transfer)xlm-rbase (translate-train-all)xtune (translate-train-all)with only example consistency r1with only model consistency r2.
74.279.782.382.079.5.
78.279.782.282.179.0.table 5: results of cross-lingual retrieval with the mod-els ﬁne-tuned on xnli..sides, we observe that removing stopgrad degradesthe overall performance by 0.5 points..table 3 provides results of each language on thexnli dataset.
for the cross-lingual transfer setting,we utilize code-switch substitution as data augmen-tation for both example consistency r1 and modelconsistency r2.
we utilize all the bilingual dictio-naries, except for english to swahili and englishto urdu, which muse does not provide.
resultsshow that our method outperforms all baselines oneach language, even on swahili (+2.2 points) andurdu (+5.4 points), indicating our method can begeneralized to low-resource languages even with-out corresponding machine translation systems orbilingual dictionaries.
for translate-train-all setting,we utilize machine translation as data augmenta-tion for both example consistency r1 and modelconsistency r2.
we improve the xlm-rlarge base-line by +2.2 points on average, while we still have+0.9 points on average compared to filter.
it isworth mentioning that we do not need correspond-ing english translations during inference.
com-plete results on other datasets are provided in thesupplementary document..4.3 analysis.
it is better to employ data augmentation forconsistency regularization than for conven-tional ﬁne-tuning.
as shown in table 4, com-.
3409figure 3: t-sne visualization of 100 examples in four languages from the xnli development set (best viewed incolor).
we ﬁne-tune the xlm-rbase model on xnli and use the hidden states of [cls] symbol in the last layer.
examples with different labels are represented with different colors.
examples in different languages are repre-sented with different markers.
the red lines connect english examples and their translations in target languages..pared to employing data augmentation for conven-tional ﬁne-tuning (data aug.), our regularizationmethods (xtuner1, xtuner2) consistently im-prove the model performance under all four dataaugmentation strategies.
since there is no labeleddata on translations in pos and the issue of distri-bution alignment in example consistency r1, whenmachine translation is utilized as data augmenta-tion, the results for data aug. and xtuner1 inpos, as well as xtuner1 in mlqa, are unavail-able.
we observe that data aug. can enhancethe overall performance for coarse-grained taskslike xnli, while our methods can further improvethe results.
however, data aug. even causes theperformance to degrade for ﬁne-grained tasks likemlqa and pos.
in contrast, our proposed twoconsistency regularization methods improve theperformance by a large margin (e.g., for mlqaunder code-switch data augmentation, data aug.decreases baseline by 1.2 points, while xtuner1increases baseline by 2.6 points).
we give detailedinstructions on how to choose data augmentationstrategies for xtune in the supplementary docu-ment..xtune improves cross-lingual retrieval.
weﬁne-tune the models on xnli with different set-tings and compare their performance on two cross-lingual retrieval datasets.
following chi et al.
(2020) and hu et al.
(2020), we utilize represen-tations averaged with hidden-states on the layer8 of xlm-rbase.
as shown in table 5, we ob-serve signiﬁcant improvement from the translate-train-all baseline to ﬁne-tuning with only exampleconsistency r1, this suggests regularizing the task-speciﬁc output of translation-pairs to be consistentalso encourages the model to generate language-.
invariant representations.
xtune only slightly im-proves upon this setting, indicating r1 betweentranslation-pairs is the most important factor to im-prove cross-lingual retrieval task..xtune improves decision boundaries as wellas the ability to generate language-invariantrepresentations.
as shown in figure 3, wepresent t-sne visualization of examples from thexnli development set under three different set-tings.
we observe the model ﬁne-tuned withxtune signiﬁcantly improves the decision bound-aries of different labels.
besides, for an englishexample and its translations in other languages, themodel ﬁne-tuned with xtune generates more sim-ilar representations compared to the two baselinemodels.
this observation is also consistent withthe cross-lingual retrieval results in table 5..5 conclusion.
in this work, we present a cross-lingual ﬁne-tuningframework xtune to make better use of data aug-mentation.
we propose two consistency regular-ization methods that encourage the model to makeconsistent predictions for an example and its se-mantically equivalent data augmentation.
we ex-plore four types of cross-lingual data augmentationstrategies.
we show that both example and modelconsistency regularization considerably boost theperformance compared to directly ﬁne-tuning ondata augmentations.
meanwhile, model consis-tency regularization enables semi-supervised train-ing on the unlabeled target language translations.
xtune combines the two regularization methods,and the experiments show that it can improve theperformance by a large margin on the xtremebenchmark..3410(a)cross-lingualtransfer(b)translate-train-all(c)xtunelabelneutralcontradictionentailmentlanguagedeenfrzhacknowledgments.
wanxiang che is the corresponding author.
thiswork was supported by the national key r&dprogram of china via grant 2020aaa0106501 andthe national natural science foundation of china(nsfc) via grant 61976072 and 61772153..references.
armen aghajanyan, akshat shrivastava, anchit gupta,naman goyal, luke zettlemoyer, and sonal gupta.
2020. better ﬁne-tuning by reducing representa-tional collapse.
corr, abs/2008.03156..mikel artetxe, sebastian ruder, and dani yogatama.
2020. on the cross-lingual transferability of mono-lingual representations.
in proceedings of the 58thannual meeting of the association for computa-tional linguistics, acl 2020, online, july 5-10,2020, pages 4623–4637.
association for computa-tional linguistics..ben athiwaratkun, marc finzi, pavel izmailov, and an-drew gordon wilson.
2019. there are many consis-tent explanations of unlabeled data: why you shouldaverage.
in 7th international conference on learn-ing representations, iclr 2019, new orleans, la,usa, may 6-9, 2019. openreview.net..yair carmon, aditi raghunathan, ludwig schmidt,john c. duchi, and percy liang.
2019. unlabeleddata improves adversarial robustness.
in advancesin neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, 8-14 december 2019,vancouver, bc, canada, pages 11190–11201..zewen chi, li dong, furu wei, nan yang, sak-sham singhal, wenhui wang, xia song, xian-lingmao, heyan huang, and ming zhou.
2020.in-foxlm: an information-theoretic framework forcross-lingual language model pre-training.
corr,abs/2007.07834..jonathan h. clark, jennimaria palomaki, vitaly niko-laev, eunsol choi, dan garrette, michael collins,and tom kwiatkowski.
2020. tydi qa: a bench-mark for information-seeking question answering intypologically diverse languages.
trans.
assoc.
com-put.
linguistics, 8:454–470..alexis conneau, kartikay khandelwal, naman goyal,vishrav chaudhary, guillaume wenzek, franciscoguzm´an, edouard grave, myle ott, luke zettle-moyer, and veselin stoyanov.
2020a.
unsupervisedcross-lingual representation learning at scale.
inproceedings of the 58th annual meeting of the as-sociation for computational linguistics, acl 2020,online, july 5-10, 2020, pages 8440–8451.
associa-tion for computational linguistics..alexis conneau and guillaume lample.
2019. cross-in advanceslingual language model pretraining.
in neural information processing systems 32: an-nual conference on neural information processingsystems 2019, neurips 2019, 8-14 december 2019,vancouver, bc, canada, pages 7057–7067..alexis conneau, ruty rinott, guillaume lample, ad-ina williams, samuel r. bowman, holger schwenk,and veselin stoyanov.
2018. xnli: evaluatingcross-lingual sentence representations.
in proceed-ings of the 2018 conference on empirical methodsin natural language processing, brussels, belgium,october 31 - november 4, 2018, pages 2475–2485.
association for computational linguistics..alexis conneau, shijie wu, haoran li, luke zettle-moyer, and veselin stoyanov.
2020b.
emergingcross-lingual structure in pretrained language mod-els.
in proceedings of the 58th annual meeting ofthe association for computational linguistics, acl2020, online, july 5-10, 2020, pages 6022–6034.
association for computational linguistics..jacob devlin, ming-wei chang, kenton lee, andkristina toutanova.
2019. bert: pre-training ofdeep bidirectional transformers for language under-in proceedings of the 2019 conferencestanding.
of the north american chapter of the associationfor computational linguistics: human languagetechnologies, naacl-hlt 2019, minneapolis, mn,usa, june 2-7, 2019, volume 1 (long and short pa-pers), pages 4171–4186.
association for computa-tional linguistics..yuwei fang, shuohang wang, zhe gan, siqi sun,and jingjing liu.
2020. filter: an enhanced fu-sion method for cross-lingual language understand-ing.
corr, abs/2009.05166..manaal faruqui and chris dyer.
2014. improving vec-tor space word representations using multilingualin proceedings of the 14th confer-correlation.
ence of the european chapter of the association forcomputational linguistics, eacl 2014, april 26-30,2014, gothenburg, sweden, pages 462–471.
the as-sociation for computer linguistics..hao fei, meishan zhang, and donghong ji.
2020.cross-lingual semantic role labeling with high-in proceedingsquality translated training corpus.
of the 58th annual meeting of the association forcomputational linguistics, acl 2020, online, july5-10, 2020, pages 7014–7026.
association for com-putational linguistics..jiang guo, wanxiang che, david yarowsky, haifengwang, and ting liu.
2015. cross-lingual depen-dency parsing based on distributed representations.
in proceedings of the 53rd annual meeting of theassociation for computational linguistics and the7th international joint conference on natural lan-guage processing of the asian federation of naturallanguage processing, acl 2015, july 26-31, 2015,beijing, china, volume 1: long papers, pages 1234–1244. the association for computer linguistics..3411junjie hu, sebastian ruder, aditya siddhant, gra-ham neubig, orhan firat, and melvin johnson.
2020. xtreme: a massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation.
in proceedings of the 37th internationalconference on machine learning, icml 2020, 13-18 july 2020, virtual event, volume 119 of proceed-ings of machine learning research, pages 4411–4421. pmlr..weihua hu, takeru miyato, seiya tokui, eiichi mat-sumoto, and masashi sugiyama.
2017. learningdiscrete representations via information maximiz-ing self-augmented training.
in proceedings of the34th international conference on machine learning,icml 2017, sydney, nsw, australia, 6-11 august2017, volume 70 of proceedings of machine learn-ing research, pages 1558–1567.
pmlr..haoming jiang, pengcheng he, weizhu chen, xi-aodong liu, jianfeng gao, and tuo zhao.
2020.smart: robust and efﬁcient ﬁne-tuning for pre-trained natural language models through principledin proceedings of theregularized optimization.
58th annual meeting of the association for compu-tational linguistics, acl 2020, online, july 5-10,2020, pages 2177–2190.
association for computa-tional linguistics..taku kudo.
2018. subword regularization: improvingneural network translation models with multiple sub-word candidates.
in proceedings of the 56th annualmeeting of the association for computational lin-guistics, acl 2018, melbourne, australia, july 15-20, 2018, volume 1: long papers, pages 66–75.
as-sociation for computational linguistics..taku kudo and john richardson.
2018. sentencepiece:a simple and language independent subword tok-enizer and detokenizer for neural text processing.
inproceedings of the 2018 conference on empiricalmethods in natural language processing, emnlp2018: system demonstrations, brussels, belgium,october 31 - november 4, 2018, pages 66–71.
as-sociation for computational linguistics..guillaume lample, alexis conneau, ludovic denoyer,and marc’aurelio ranzato.
2018. unsupervised ma-chine translation using monolingual corpora only.
in 6th international conference on learning rep-resentations, iclr 2018, vancouver, bc, canada,april 30 - may 3, 2018, conference track proceed-ings.
openreview.net..patrick s. h. lewis, barlas oguz, ruty rinott, sebas-tian riedel, and holger schwenk.
2020. mlqa:evaluating cross-lingual extractive question answer-ing.
in proceedings of the 58th annual meeting ofthe association for computational linguistics, acl2020, online, july 5-10, 2020, pages 7315–7330.
association for computational linguistics..xiaodong liu, hao cheng, pengcheng he, weizhuchen, yu wang, hoifung poon, and jianfeng gao.
2020. adversarial training for large neural languagemodels.
corr, abs/2004.08994..fuli luo, w. wang, jiahao liu, yijia liu, bin bi,songfang huang, fei huang, and l. si.
2020.veco: variable encoder-decoder pre-training forcross-lingual understanding and generation.
arxiv,abs/2010.16046..tomas mikolov, quoc v. le, and ilya sutskever.
2013.exploiting similarities among languages for ma-chine translation.
corr, abs/1309.4168..takeru miyato, shin-ichi maeda, masanori koyama,and shin ishii.
2019. virtual adversarial training:a regularization method for supervised and semi-ieee trans.
pattern anal.
supervised learning.
mach.
intell., 41(8):1979–1993..joakim nivre, rogier blokland, niko partanen, andmichael rießler.
2018. universal dependencies 2.2..xiaoman pan, boliang zhang, jonathan may, joelnothman, kevin knight, and heng ji.
2017. cross-lingual name tagging and linking for 282 languages.
in proceedings of the 55th annual meeting of the as-sociation for computational linguistics, acl 2017,vancouver, canada, july 30 - august 4, volume1: long papers, pages 1946–1958.
association forcomputational linguistics..jason phang, phu mon htut, yada pruksachatkun,haokun liu, clara vania, katharina kann, iacercalixto, and samuel r. bowman.
2020. englishintermediate-task training improves zero-shot cross-lingual transfer too.
corr, abs/2005.13013..ivan provilkov, dmitrii emelianenko, and elena voita.
2020. bpe-dropout: simple and effective subwordin proceedings of the 58th annualregularization.
meeting of the association for computational lin-guistics, acl 2020, online, july 5-10, 2020, pages1882–1892.
association for computational linguis-tics..libo qin, minheng ni, yue zhang, and wanxiang che.
2020. cosda-ml: multi-lingual code-switchingdata augmentation for zero-shot cross-lingual nlp.
in proceedings of the twenty-ninth internationaljoint conference on artiﬁcial intelligence, ijcai2020, pages 3853–3860.
ijcai.org..jasdeep singh, bryan mccann, nitish shirish keskar,caiming xiong, and richard socher.
2019. xlda:cross-lingual data augmentation for naturallan-guage inference and question answering.
corr,abs/1905.11471..antti tarvainen and harri valpola.
2017. mean teach-ers are better role models: weight-averaged consis-tency targets improve semi-supervised deep learningresults.
in 5th international conference on learningrepresentations, iclr 2017, toulon, france, april24-26, 2017, workshop track proceedings.
openre-view.net..yuxuan wang, wanxiang che, jiang guo, yijia liu,and ting liu.
2019. cross-lingual bert trans-information for zero-shot dependency parsing..3412proceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 5720–5726. association for computational linguistics..qizhe xie, zihang dai, eduard h. hovy, thang luong,and quoc le.
2020. unsupervised data augmenta-tion for consistency training.
in advances in neuralinformation processing systems 33: annual con-ference on neural information processing systems2020, neurips 2020, december 6-12, 2020, virtual..ruochen xu, yiming yang, naoki otani, and yuexinwu.
2018. unsupervised cross-lingual transfer ofword embedding spaces.
in proceedings of the 2018conference on empirical methods in natural lan-guage processing, brussels, belgium, october 31 -november 4, 2018, pages 2465–2474.
associationfor computational linguistics..yinfei yang, yuan zhang, chris tar, and jasonpaws-x: a cross-lingual ad-baldridge.
2019.versarial dataset for paraphrase identiﬁcation.
inproceedings of the 2019 conference on empiricalmethods in natural language processing and the9th international joint conference on natural lan-guage processing, emnlp-ijcnlp 2019, hongkong, china, november 3-7, 2019, pages 3685–3690. association for computational linguistics..mang ye, xu zhang, pong c. yuen, and shih-fuchang.
2019. unsupervised embedding learning viain ieeeinvariant and spreading instance feature.
conference on computer vision and pattern recog-nition, cvpr 2019, long beach, ca, usa, june 16-20, 2019, pages 6210–6219.
computer vision foun-dation / ieee..meishan zhang, yue zhang, and guohong fu.
2019.cross-lingual dependency parsing using code-mixedtreebank.
in proceedings of the 2019 conference onempirical methods in natural language processingand the 9th international joint conference on nat-ural language processing, emnlp-ijcnlp 2019,hong kong, china, november 3-7, 2019, pages 997–1006. association for computational linguistics..stephan zheng, yang song, thomas leung, and ian j.improving the robustness ofgoodfellow.
2016.deep neural networks via stability training.
in 2016ieee conference on computer vision and patternrecognition, cvpr 2016, las vegas, nv, usa, june27-30, 2016, pages 4480–4488.
ieee computer so-ciety..chen zhu, yu cheng, zhe gan, siqi sun, tom gold-stein, and jingjing liu.
2020. freelb: enhancedadversarial training for natural language understand-in 8th international conference on learninging.
representations, iclr 2020, addis ababa, ethiopia,april 26-30, 2020. openreview.net..appendix.
a statistics of xtreme datasets.
task.
dataset.
|train|.
|lang|.
classiﬁcation.
xnli392kpaws-x 49.4k.
structuredprediction.
posner.
21k20k.
questionanswering.
xquad 87k87kmlqatydiqa 3.7k.
157.
3340.
1179.table 6: statistics for the datasets in the xtremebenchmark.
we report the number of training examples(|train|), and the number of languages (|lang|)..b hyper-parameters.
for xnli, paws-x, pos and ner, we ﬁne-tune10 epochs.
for xquad and mlqa, we ﬁne-tune4 epochs.
for tydiqa-goldp, we ﬁne-tune 20epochs and 10 epochs for base and large model,respectively.
we select λ1 in [1.0, 2.0, 5.0], λ2in [0.3, 0.5, 1.0, 2.0, 5.0].
for learning rate, weselect in [5e-6, 7e-6, 1e-5, 1.5e-5] for large models,[7e-6, 1e-5, 2e-5, 3e-5] for base models.
we usebatch size 32 for all datasets and 10% of total train-ing steps for warmup with a linear learning rateschedule.
our experiments are conducted with asingle 32gb nvidia v100 gpu, and we use gradi-ent accumulation for large-size models.
the otherhyper-parameters for the two-stage xtune train-ing are shown in table 7 and table 8..c results for each dataset and.
language.
we provide detailed results for each dataset andlanguage below.
we compare our method againstxlm-rlarge for cross-lingual transfer setting, fil-ter (fang et al., 2020) for translate-train-all set-ting..d how to select data augmentation.
strategies in xtune.
we give instructions on selecting a proper data aug-mentation strategy depending on the correspondingtask..3413variable xnli paws-x pos ner xquad mlqa tydiqa.
table 7: the best hyper-parameters used for xtune under the cross-lingual transfer setting.
“ss”, “cs”, “mt”denote the data augmentation methods: subword sampling, code-switch substitution, and machine translation,respectively..stage 1.stage 2.hyper-parameters.
stage 1.stage 2.hyper-parameters.
a∗.
aa(cid:48).
λ1λ2.
a∗.
aa(cid:48).
λ1λ2.
cs.
cscs.
5.05.0.mt.
mtmt.
5.01.0.cs.
cscs.
5.02.0.mt.
mtmt.
5.01.0.ss.
ssss.
5.00.3.ss.
ssss.
5.05.0.ss.
ss.
mt mtssss.
5.00.3.
5.01.0.cs.
ssss.
5.05.0.cs.
mtss.
5.00.1.cs.
ssss.
5.05.0.cs.
mtss.
5.00.5.ss.
ssss.
5.05.0.ss.
mtss.
5.00.3.variable xnli paws-x pos ner xquad mlqa tydiqa.
table 8: the best hyper-parameters used for xtune under the translate-train-all setting.
“ss”, “cs”, “mt” denotethe data augmentation methods subword sampling, code-switch substitution, and machine translation, respectively..method.
model.
xnli pos mlqa avg..d.2 span extraction.
-.
xlm-rbase.
10.6.
20.8.subwordsampling.
gaussiannoise.
code-switch.
machinetranslation.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
data aug.xtuner1xtuner2.
10.510.210.6.
10.810.510.8.
9.29.18.8.
7.26.97.2.
20.520.220.1.
20.620.720.2.
21.120.720.2.
--19.6.
20.3.
20.219.619.8.
19.819.819.7.
20.519.420.0.
17.9-17.1.
17.2.
17.116.716.8.
17.117.016.9.
16.916.416.3.
--14.6.table 9: cross-lingual transfer gap, i.e., averaged per-formance drop between english and other languagesin zero-shot transfer.
a smaller gap indicates bettertransferability.
for mlqa, we report the average off1-scores and exact match scores..d.1 classiﬁcation.
the two distribution in example consistency r1can always be aligned.
therefore, we recommendusing machine translation as data augmentation ifthe machine translation systems are available.
oth-erwise, the priority of our data augmentation strate-gies is code-switch substitution, subword samplingand gaussian noise..the two distribution in example consistency r1can not be aligned in translation-pairs.
therefore, itis impossible to use machine translation as data aug-mentation in example consistency r1.
we preferto use code-switch when applying example consis-tency r1 individually.
however, when the trainingcorpus is augmented with translations, since thebilingual dictionaries between arbitrary languagepairs may not be available, we recommend usingsubword sampling in example consistency r1..d.3 sequence labeling.
similar to span extraction, the two distributionin example consistency r1 can not be aligned intranslation-pairs.
therefore, we do not use machinetranslation in example consistency r1.
unlike clas-siﬁcation and span extraction, sequence labelingrequires ﬁner-grained information and is more sen-sitive to noise.
we found code-switch is worse thansubword sampling as data augmentation in both ex-ample consistency r1 and model consistency r2,it will even degrade performance for certain hyper-parameters.
thus we recommend using subwordsampling in example consistency r1, and use ma-chine translation to augment the english trainingcorpus if machine translation systems are available,otherwise subword sampling..3414e cross-lingual transfer gap.
as shown in table 9, the cross-lingual transfergap can be reduced under all four data augmen-tation strategies.
meanwhile, we observe machinetranslation and code-switch substitution achieve asmaller cross-lingual transfer gap than the othertwo data augmentation methods.
this suggeststhe data augmentation methods with cross-lingualknowledge have a greater improvement in cross-lingual transferability.
although code-switch sig-niﬁcantly reduces the transfer gap on xnli, theimprovement is relatively small on pos and mlqaunder the cross-lingual transfer setting, indicatingthe noisy code-switch substitution will harm thecross-lingual transferability on ﬁner-grained tasks..3415model.
en.
de.
es.
fr.
ja.
ko.
zh.
avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
translate-train-all (translation-based augmentation is available for english training data).
xlm-rlargextune.
filter (fang et al., 2020)xtune.
94.796.0.
95.996.1.
89.792.5.
92.892.6.
90.192.2.
93.093.1.
90.492.7.
93.793.9.
78.784.9.
87.487.8.
79.084.2.
87.689.0.
82.386.6.
89.688.8.
86.489.8.
91.591.6.table 10: pawsx results (accuracy scores) for each language..model.
en.
ar.
de.
el.
es.
hi.
ru.
th.
tr.
vi.
zh.
avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
86.5/75.7 68.6/49.0 80.4/63.4 79.8/61.7 82.0/63.9 76.7/59.7 80.1/64.3 74.2/62.8 75.9/59.3 79.1/59.0 59.3/50.0 76.6/60.888.9/78.6 77.1/60.0 83.1/67.2 82.6/66.0 83.0/65.1 77.8/61.8 80.8/64.8 73.5/62.1 77.6/62.0 81.8/62.5 67.7/58.4 79.4/64.4.
translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 86.4/74.6 79.5/60.7 83.2/67.0 83.0/64.6 85.0/67.9 83.1/66.6 82.8/67.4 79.6/73.2 80.4/64.4 83.8/64.7 79.9/77.0 82.4/68.088.8/78.1 79.7/63.9 83.7/68.2 83.0/65.7 84.7/68.3 80.7/64.9 82.2/66.6 81.9/76.1 79.3/65.0 82.7/64.5 81.3/78.0 82.5/69.0xtune.
table 11: xquad results (f1/em scores) for each language..model.
en.
ar.
de.
es.
hi.
vi.
zh.
avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
83.5/70.6 66.6/47.1 70.1/54.9 74.1/56.6 70.6/53.1 74.0/52.9 62.1/37.0 71.6/53.285.2/72.6 67.9/47.7 72.2/56.8 75.5/57.9 73.2/55.1 75.9/54.7 71.1/48.6 74.4/56.2.
translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 84.0/70.8 72.1/51.1 74.8/60.0 78.1/60.1 76.0/57.6 78.1/57.5 70.5/47.0 76.2/57.785.3/72.9 69.7/50.1 72.3/57.3 76.3/58.8 74.0/56.0 76.5/55.9 70.8/48.3 75.0/57.1xtune.
table 12: mlqa results (f1/em scores) for each language..model.
en.
ar.
bn.
ﬁ.id.
ko.
ru.
sw.te.
avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
71.5/56.8 67.6/40.4 64.0/47.8 70.5/53.2 77.4/61.9 31.9/10.9 67.0/42.1 66.1/48.1 70.1/43.6 65.1/45.075.3/63.6 77.4/60.3 72.4/58.4 75.5/60.2 81.5/68.5 68.6/58.3 71.1/48.8 73.3/56.7 78.4/60.1 74.8/59.4.
translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 72.4/59.1 72.8/50.8 70.5/56.6 73.3/57.2 76.8/59.8 33.1/12.3 68.9/46.6 77.4/65.7 69.9/50.4 68.3/50.973.8/61.6 77.8/60.2 73.5/61.1 77.0/62.2 80.8/68.1 66.9/56.5 72.1/51.9 77.9/65.3 77.6/60.7 75.3/60.8xtune.
table 13: tydiqa-golp results (f1/em scores) for each language..3416model.
af.
ar.
bg.
de.
el.
en.
es.
et.
eu.
fa.
ﬁ.fr.
he.
hi.
hu.
id.
it.
cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
89.8 67.5 88.1 88.5 86.3 96.1 88.3 86.5 72.5 70.6 85.8 87.2 68.3 76.4 82.6 72.4 89.490.4 72.8 89.0 89.4 87.0 96.1 88.8 88.1 73.1 74.7 87.2 89.5 83.5 77.7 83.6 73.2 90.5.translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 88.7 66.1 88.5 89.2 88.3 96.0 89.1 86.3 78.0 70.8 86.1 88.9 64.9 76.7 82.6 72.6 89.890.7 74.2 89.9 90.2 87.4 96.1 90.5 88.4 75.9 74.2 87.9 90.2 85.9 79.3 83.2 73.3 91.0xtune.
model.
ja.
kk.
ko mr.nl.
pt.
ru.
ta.
te.
th.
tl.
tr.
ur.
vi.
yo.
zh avg..cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
15.9 78.1 53.9 80.8 89.5 87.6 89.5 65.2 86.6 47.2 92.2 76.3 70.3 56.8 24.6 25.7 73.862.7 78.3 55.7 82.4 90.2 88.5 90.5 63.6 88.3 61.8 94.5 76.9 72.0 57.8 24.4 69.4 78.5.fine-tune multilingual model on all target language target language training sets (translate-train-all).
filter (fang et al., 2020) 40.4 80.4 53.3 86.4 89.4 88.3 90.5 65.3 87.3 57.2 94.1 77.0 70.9 58.0 43.1 53.1 76.965.3 79.8 56.0 85.5 89.7 89.3 90.8 65.7 85.5 61.4 93.8 78.3 74.0 57.5 27.9 68.8 79.3xtune.
table 14: pos results (accuracy) for each language..model.
en.
af.
ar.
bg.
bn.
de.
el.
es.
et.
eu.
fa.
ﬁ.fr.
he.
hi.
hu.
id.
it.
ja.
jv.
cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
84.7 78.9 53.0 81.4 78.8 78.8 79.5 79.6 79.1 60.9 61.9 79.2 80.5 56.8 73.0 79.8 53.0 81.3 23.2 62.585.0 80.4 59.1 84.8 79.1 80.5 82.0 78.1 81.5 64.5 65.9 82.2 81.9 62.0 75.0 82.8 55.8 83.1 30.5 65.9.translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 83.5 80.4 60.7 83.5 78.4 80.4 80.7 74.0 81.0 66.9 71.3 80.2 79.9 57.4 74.3 82.2 54.0 81.9 24.3 63.584.4 81.7 59.7 85.3 80.8 80.9 82.0 74.1 83.4 69.9 63.6 82.5 80.6 64.0 76.3 83.8 57.9 83.3 26.5 69.8xtune.
model.
ka.
kk.
ko ml mr ms my.
nl.
pt.
ru.
sw.ta.
te.
th.
tl.
tr.
ur.
vi.
yo.
zh.
cross-lingual-transfer (models are ﬁne-tuned on english training data without translation available).
xlm-rlargextune.
71.6 56.2 60.0 67.8 68.1 57.1 54.3 84.0 81.9 69.1 70.5 59.5 55.876.7 57.5 65.9 68.1 73.3 67.2 63.7 85.3 84.0 73.6 70.1 66.1 60.1.
1.31.8.
73.2 76.1 56.4 79.4 33.6 33.176.9 83.6 76.0 80.3 44.4 38.7.translate-train-all (translation-based augmentation is available for english training data).
filter (fang et al., 2020) 71.0 51.1 63.8 70.2 69.8 69.3 59.0 84.6 82.1 71.1 70.6 64.3 58.776.3 56.9 67.1 72.6 71.5 72.5 66.7 85.8 82.1 75.2 72.4 66.0 61.8xtune.
2.41.1.
74.4 83.0 73.4 75.8 42.9 35.477.5 83.7 75.6 80.8 44.9 36.5.table 15: ner results (f1 scores) for each language..3417